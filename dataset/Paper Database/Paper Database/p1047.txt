Grouped Convolutional Neural Networks for Multivariate Time Series
Subin Yi 1 Janghoon Ju 1 Man-Ki Yoon 2 Jaesik Choi 1
Analyzing multivariate time series data is important for many applications such as automated
control, fault diagnosis and anomaly detection.
One of the key challenges is to learn latent features automatically from dynamically changing
multivariate input. In visual recognition tasks,
convolutional neural networks (CNNs) have been
successful to learn generalized feature extractors with shared parameters over the spatial domain. However, when high-dimensional multivariate time series is given, designing an appropriate CNN model structure becomes challenging because the kernels may need to be extended
through the full dimension of the input volume.
To address this issue, we present two structure
learning algorithms for deep CNN models. Our
algorithms exploit the covariance structure over
multiple time series to partition input volume into
groups. The ﬁrst algorithm learns the group CNN
structures explicitly by clustering individual input sequences. The second algorithm learns the
group CNN structures implicitly from the error
backpropagation. In experiments with two realworld datasets, we demonstrate that our group
CNNs outperform existing CNN based regression methods.
1. Introduction
Advances in computing technology has made many complicated systems such as automobile, avionics, and industrial control systems more sophisticated and sensitive. Analyzing multiple variables that compose such systems accurately is therefore becoming more important for many applications such as automated control, fault diagnosis, and
anomaly detection.
In complex systems, one of the key requirements is to
maintain integrity of the sensor data so that it can be moni-
1Ulsan National Institute of Science and Technology, Ulsan, 44919, Republic of Korea 2University of Illinois at Urbana-
Champaign, Urbana, IL 61801. Correspondence to: Jaesik Choi
< >.
tored and analyzed in a trusted manner. Previously, sensor
integrity has been analyzed by feedback controls and nonparametric Bayesian
methods . However, regression models based on control theory and nonparametric Bayesian are
highly sensitive to the model parameters. Thus, ﬁnding the
best model parameter for the regression models is challenging with high-dimensional multivariate sequences.
Artiﬁcial neural network models also have been used
to handle multivariate time series data.
Autoencoders
 train model parameters in an unsupervised manner by specifying the same
input and output values. Recurrent neural networks (RNN)
(Rumelhart et al.) and long-short term memory (LSTM)
 represent changes of
time series data by learning recurrent transition function
between time steps.
Unfortunately, existing neural network models for time series data assume fully connected
networks among time series under the Markov assumption.
Thus, such models are often not precise enough to address
high-dimensional multivariate regression problems.
To address this issue, we present two structure learning algorithms for deep convolutional neural networks (CNNs).
Both of our algorithms partition input volume into groups
by exploiting the covariance structure for multiple time series so that the input CNN kernels process only one of the
grouped time series. Due to this partitioning of the input
time series, we can avoid the CNN kernels being extended
through the full dimension. In this reason, we denote the
CNN models as Group CNN (G-CNN) which can exploit
latent features from multiple time-series more efﬁciently
by utilizing structural covariance of the input variables.
The ﬁrst structure learning algorithm learns the CNN structure explicitly by clustering input sequences with spectral
clustering . The second algorithm learns
the CNN structures implicitly with the error backpropagation which will be explained in Section 3.2.
Our model design principle is to reduce model parameters
by sharing parameters when necessary. In multivariate time
series regression tasks, our hypotheses on the parameter
sharing scheme (or parameter tying) are as follow: (1) convolutions on a group of correlated signals are more robust
to signal noises; and (2) convolutions operators on groups
 
Grouped Recurrent Convolutional Layers for Multivariate Time Series
Figure 1: Building blocks of CNN and RCNN.
of signals are more feasible to learn when a large number
of time series is given. In experiments, we show that G-
CNN make the better predictive performance on challenging regression tasks compared to the existing CNN based
regression models.
2. Background
2.1. Convolutional Neural Network
A convolutional neural network (CNN) is a multi-layer artiﬁcial neural network that has been successful recognizing visual patterns. The most common architecture of the
CNNs is a stack of three types of multiple layers: convolutional layer, sub-sampling layer, and fully-connected layer.
Conventionally, A CNN consists of alternate layers of convolutional layers and sub-sampling layers on the bottom
and several fully-connected layers following them.
First, an unit of a convolutional layer receives inputs from
a set of neighboring nodes of the previous layer similarly
with animal visual cortex cell. The local weights of convolutional layers are shared with the nodes in the same layer.
Such local computations in the layer reduce the memory
burden and improve the classiﬁcation performance.
None-linear down-sapling layer, which is the second type
of CNN layers, is another important characteristic of
CNNs. The idea of local sub-sampling is that once a feature has been detected, its location itself is not as important as its relative location with other features. By reducing
the dimensionality, it reduces the local sensitivity of the
network and computational complexity .
The last type of the layers is the fully-connected layer.
It computes a full matrix calculation with all activations
and nodes same as regular neural networks.
After convolutional layers and sub-sampling layers extract features,
fully-connected layers implements reasoning and gives the
actual output. Then the model is trained in the way minimizing the error between the actual output of the model
and the target output values by backpropagation method.
CNN has been very effective for solving many computer
vision problems such as classiﬁcation , object detection and semantic segmentation . It has been
also applied to other problems such as natural language
processing . Recently, variants of CNN are applied to analyzing
various kinds of time-series such as sensor values and EEG
(electroencephalogram) signals .
2.2. Recurrent Convolutional Neural Network (RCNN)
Recurrent Convolutional Neural Network (RCNN) is a type
of CNN with its convolutional layers being replaced with
recurrent convolutional layers.
It improves the expressive power of the convolutional layer by exploiting multiple convolutional layers that share the parameters. RCNN
has been applied to not only the image processing problem but
also other tasks that require temporal analysis . RCNN can effectively extract invariant features in
the temporal domain regarding the time-series data as a 2dimensional data with one of the dimensions is one. with
one of the dimensions is 1, RCNN can effectively extract
invariant features in the temporal domain.
2.2.1. RECURRENT CONVOLUTIONAL LAYER
Recurrent Convolutional Layer (RCL), which is the most
representative building block of an RCNN, is the composition of l intermediate convolutional layers that shares the
same parameters. The ﬁrst convolutional layer of an RCL
carries the convolution on the input x, resulting in the output σ(W ∗x) where W is the convolutional ﬁlter, * is a convolution operator, and σ(·) is an activation function. Then
the next convolutional layer recursively processes the summation of the original input and the output of the previous
layer, x + σ(W ∗x), as an input. After some iterations of
this process, an RCL gives the result of the ﬁnal intermediate convolutional layer as its output.
During the error backpropagation, the parameters are updated l times. In each update, the parameters are changed
to ﬁx the error made by itself from the previous layer.
RCL can also be regarded as a skip-layer connection .
Skip-layer connection represents connecting layers skipping intermediate
layers as in Figure 1’s RCL. The main motivation is that
the deeper networks show a better performance in many
cases but they are also harder to train in actual applications
due to vanishing gradients and degradation problem .
 designed such layers with skip-layer connection, named as residual learning. The idea was that if
Grouped Recurrent Convolutional Layers for Multivariate Time Series
one can hypothesizes that multiple nonlinear layers can estimate an underlying mapping H(x), it is equivalent to estimating an residual function F(x) := H(x) −x. If the
residual F(x) is approximately a zero mapping, H(x) is
an optimal identity mapping.
2.3. Spectral Clustering
The goal of clustering data points x1, ..., xN is to partition
the data points into some groups such that the points in the
same group are similar and points in different groups are
dissimilar in a certain similarity measure sij between xi
and xj. Spectral clustering is the clustering method that
solves this problem from the graph-cut point of view.
From the graph-cut point of view, data points are represented as a similarity graph G = (V, E).
a weighted undirected graph with the vertex set V
{v1, ..., vN} where each vertex vi represents a data point
xi and the weighted adjacency matrix W = {wij|i, j =
1, ..., N} where wij represents the similarity sij. Let the
degree of a vertex vi ∈V be di = Pn
j=1 wij and deﬁne
a degree matrix D as the diagonal matrix with the degrees
d1, ..., dN on the diagonal.
Then, clustering can be reformulated to ﬁnd a partition of
the graph such that the edges between different groups have
very low weights and the edges within a group have high
One of the most intuitive way to solve this problem is to
solve the min-cut problem . Min-cut
problem is to choose a partition A1, ..., AK for a given
number K that minimizes the equation (2.3.1) given as:
cut(A1, ..., AK) := 1
link(Ai, ¯Ai)
s.t. link(A, B) :=
wij for disjoint A, B ⊂A.
Here, 1/2 is introduce for normalizing as otherwise each
edge will be counted twice. The algorithm of explicitly requests the sets be large enough where the
size of a subset A ⊂V is measured by:
Then, ﬁnd the following normalized cut, Ncut:
Ncut(A1, ..., AK) := 1
link(Ai, ¯Ai)
The denominator of the Ncut tries to balance the size of the
clusters and the numerator ﬁnds the minimum cut of the
Figure 2: Grouped layers for CNN and RCNN.
given graph. Then to ﬁnd the partition A1, ..., AK is same
as to solve the following optimization problem:
H∈RNxK trace(HT LH)
vol(Vj), vi ∈Vj
0, otherwise
i Lhi=cut(Ai, ¯Ai)/vol(Ai),
i Dhi=1 where the indicator vector hj is the j-th
column of the matrix H.
Unfortunately, introducing the additional term to the mincut problem has proven to make the problem NP-hard
 so solves the relaxed problem, which gives the solution H that consists of the eigenvectors corresponding to
the K smallest eigenvalues of the matrix Lrw := D−1L or
the K smallest generalized eigenvectors of Lu = λDu.
Given an NxN similarity matrix, the spectral clustering
algorithm runs eigenvalue decomposition (EVD) on the
graph Laplacian matrix and the eigenvectors corresponding
to the K smallest eigenvalues are clustered by a clustering
algorithm representing the graph vertices. The K eigenvectors are also the eigenvectors of the similarity matrix
whereas corresponding K largest eigenvalues, which can be
considered as an encoding of the graph similarity matrix.
3. Grouped Time Series
In this section, we present two algorithms build group CNN
structure. The ﬁrst method builds the group structure explicitly from Spectral clustering. The second method build
the group structure through the error backpropagation.
3.1. Learning the Structure by Spectral Clustering
Our group CNN structure receives both the input variables X = [x1, ..., xN] and their cluster information C =
[c1, ..., cN] where ci represents the membership of the variable xi. Unlike usual convolutional layers, the grouped
Grouped Recurrent Convolutional Layers for Multivariate Time Series
(a) Convolutional Neural Network (CNN).
(b) CNN with grouped convolutional layers.
Figure 3: Comparison of the general CNN model and CNN with grouped layers.
convolutional layers divide the input volume based on the
cluster membership ci and performs the convolution operations over the input variables that belong to the same cluster
as described in the Figure 2. Formally, the k-th group of the
layer H is deﬁned as:
W k · xi + bk
where (·) is the convolution operation, i
{1, ..., N}, cj = k}, W k is the weight matrix, and bk is
the bias vector of the k-th group.
As in the CNN models, the input variables X are processed
throughout multiple grouped convolutional layers and subsampling layers, ﬂattened into one-dimensional layer followed by fully-connected layers, and produces the output
y = {y1, ..., yP } (Figure 3).
Given the target output t = {t1, ..., tP }, we can also train
this model using gradient descent solving the optimization
with respect to the trainable parameter θ = {W, b}. The
error is backpropagated to each group separately, training
the CNN structure explicitly.
This model requires signiﬁcantly less number of parameters compared to the vanilla CNN model. For example,
to process 100 input variables producing 100 output channel, existing CNN model needs 100 kernels of size (width,
height, 100). However, if the input variables consist of 5
clusters, each with 20 variables, it requires 5x20 kernels
of size (width, height, 20), which is 5 times less than the
vanilla model. It could make the CNN model more compact by eliminating redundant parameters.
Example of a neural network that receives
grouped variables.
3.2. Neural Networks with Clustering Coefﬁcient
Assuming that the input time series are correlated with each
other, we group those variables explicitly to make use of
such correlations as CNN utilizes local connectivity of an
image. It can be considered to ﬁnd the local connectivity
and correlations within channels of CNN.
Given an input data X which consists of N variables where
each variables are D dimensional real valued vectors, i.e.
X = [x1, ..., xN], we wish to group these variables into K
clusters introducing a matrix U = [ui,j; i ∈{1, ..., N}, j ∈
{1, ..., K}] where ui,j ∈ , PK
j ui,j = 1 whose element ui,j is the clustering coefﬁcient which represents the
portion of the j-th cluster takes for the variable xi as in
multinomial distribution. In this paper, we use boldface
letters to represent D-dimensional real valued column vectors.Then, hj, the node that represents the j-th cluster is
deﬁned as :
Grouped Recurrent Convolutional Layers for Multivariate Time Series
i,j is the i-th row’s j-th column of the weight matrix W 1 of size NxK, b1
j is the bias, and σ(·) is the activation function. By multiplying ui,j, variables can proportionally participate to each cluster.
Suppose that an example of two layered neural network is
given as shown in the Figure 4. The output node y is also
deﬁned as :
Given the true target value t, this network can be trained
by gradient descent method solving the below optimization
Assuming linear activation function, gradient of the Err
with respect to ui,j is :
I{j′ ̸= j}
= (y −t)(KxT
where j′ is the cluster out of the j-th cluster and I is an
indicator function. Intuitively, the parameter update ui,j
includes (K times of loss from the j-th cluster - loss from
all clusters ; KxT
i,j), ui,j value that
gives smaller loss increase ﬁnding the optimal values while
minimizing the error by the gradient descent method.
Figure 5: Convolutional layer with clustering coefﬁcient.
To implement the clustering coefﬁcient to out model, we
added a new layer which works as the Figure 5 on the
bottom of the model, before the layer 1 of the Figure 3
(b). This layer receives N input variables and computes
channel-wise convolution throughout the variables using
the same weight and bias in the group (group parameter
sharing). This channel-wise convolution is repeated for K
groups with different parameters for each groups. Therefore, the i-th channel of the k-th group is deﬁned as:
i = σ(ui,kW k · xi + bk)
Then the output is processed by the same process with the
model from the Section 3.1 with explicit clustering.
4. Related Work
Recently, deep learning methods are making good results in
solving a variety of problems such as visual pattern recognition, signal processing, and others. Consequently there
has been researches for applying those methods to analyzing complex multivariate systems.
Neural networks that are composed of fully-connected layers only are not appropriate for handling sequential data
since they need to process the whole sequence of input.
More speciﬁcally, such networks are too inefﬁcient in terms
of both memory usage and learning efﬁciency.
One of the popular choices for processing time-series is
a recurrent neural network (RNN). An RNN processes sequential data with recurrent connections to represent transition models over time. so that it can store temporal information within the network. RNN models have been successfully used for processing sequential data .
 used dynamic RNN to forecast nonstationary hydrological time-series and 
used stacked LSTM network as a predictor over a number
of time steps and detected anomalies that has high prediction error in time series.
Convolutional neural network (CNN) is also commonly
used to analyze temporal data. 
used CNN for speech recognition problem and proposed multi-channels deep convolutional
neural network for multivariate time series classiﬁcation.
Recurrent Convolutional Neural Network (RCNN), which
can be considered as a variant of a CNN, is recently proposed and shows state-of-the-art performance on classifying multiple time series . When a small number of
time series is given, multiple signals can be handled individually in a straightforward manner by using polling operators or fully connected linear operators on signals. However, it is not clear how to model the covariance structure
of large number of multiple sequences explicitly for deep
neural network models.
Grouped Recurrent Convolutional Layers for Multivariate Time Series
5. Experimental Results
In experiments, we compare the regression performance
of several CNN-based models on two real-world highdimensional multivariate datasets, groundwater level data
and drone ﬂight data. Groundwater data and drone data
respectively have 88 and 148 variables.
5.1. Settings
To evaluate the regression performance, we picked one of
the variables, say xp, from the dataset randomly and constructed the variable’s values as a target at time t, y =
xp(t), by seeing its correlated variables’ values from time
t−T to t without including variable, i.e. X = ∪i,i̸=p[xi(t−
T), ..., xi(t)].
We trained our models with 90% of the whole dataset and
tested on the other 10%. Then, the regression performance
were compared with other regression models: linear regression, ridge regression, CNN and RCNN. The regression
performance was measured on the scale of the standardized root mean square error (SRMSE), which is deﬁned as
the equation 5.1.1 when ¯t is the mean value of the target
t=1(tt −yt)2/N
5.2. Datasets
5.2.1. GROUNDWATER DATA
We used daily collected groundwater data provided by the
United States Geological Survey (USGS)1. Dataset is composed of various parameters from the US territories and
we used depth to water level from the regions other than
Hawaii and Alaska. Regions where the data was collected
over 28 years were selected and those that
have unrecorded periods longer than two months were excluded. Empty records shorter than two months were ﬁlled
by interpolation. Final dataset contains records from 88
sites of 10,228 days.
5.2.2. DRONE DATA
We used a quadcopteras our experimental platform to collect ﬂight sensor data. Quadcopters are aerodynamically
unstable and their actuators, i.e., the motors, must be controlled directly by an on-board computer for stable ﬂight.
We used the Pixhawk2 as the autopilot hardware for our
quadcopter. It has on-board sensors such as inertial mea-
1 
2 
(a) quadcopter used for the test.
(b) drone’s ﬂight path.
Figure 6: Drone.
surement unit (IMU), compass, and barometer.
the open-source PX4 autopilot software suite3 on the ARM
Cortex M4F processor on the Pixhawk. It combines sensor
data and ﬂight commands to compute correct outputs to the
motors, which then controls the vehicle’s orientation and
We collected ﬂight data of the quadcopter using PX4’s autopilot logging facility. Each ﬂight data is composed of
time-stamped sensor and actuator measurements, ﬂight set
points (attitude, position), and other auxiliary information
(radio input, battery status, etc.). We collected data sets by
ﬂying the quadcopter in an autonomous mode, in which it
ﬂies along a pre-deﬁned path. We obtained three sets of
logs by varying the path as shown in Figure 6. In total, we
used 148 sensors of 12,654 time points excluding those that
do not show any change during the ﬂight and have missing
5.3. Results
We built group CNN and group RCNN using both spectral
clustering method (explicit) and the clustering coefﬁcient
method (coeff), and compared the performance with corresponding vanilla CNNs and vanilla RCNNs. The deep
CNN model architecture are shown in Table 1.
learning parameters such as the learning rate and weight
3 
Grouped Recurrent Convolutional Layers for Multivariate Time Series
(1x64x100)54
RCNN3 & exp
(1x64x100)5
CNN & coeff 6
(1x64x100)5
RCNN & coeff
(1x64x100)5
(1x64x50)155
(1x16x50)15
(1x4x50)15
(1x1x50)15
RCNN3 & exp
(1x64x50)15
(1x16x50)15
(1x4x50)15
(1x1x50)15
CNN & coeff 6
(1x64x50)15
(1x16x50)15
(1x4x50)15
(1x1x50)15
RCNN & coeff
(1x64x50)15
(1x16x50)15
(1x4x50)15
(1x1x50)15
1Groundwater Dataset. 2Drone Dataset. 3Layer 1, Layer 2, Layer 3 are RCLs with iteration 2. 4K=5. 5K=15.
6Models with clustering coefﬁcient has an additional layer (Figure 5) before the Layer 1.
Table 1: Architecture of tested deep CNN models.
initialization parameters are matched. Every models were
trained for 200 epochs and the best results were chosen.
Linear Regression
CNN & explicit grouping
CNN & clustering coeff
(a) Drone IMU GyroZ sensor.
Linear Regression
CNN & explicit grouping
CNN & clustering coeff
(b) Groundwater site 1673.
Figure 7: Reconstruction examples on the test data.
Experiment results are shown in the following tables, Table 2 and Table 3. In general, our group CNN models outperform in the groundwater dataset. RCNN with clustering
coefﬁcient model performs best with 0.754 SRMSE compared to 0.985 of vanilla RCNN model. Our group CNN
models also tend to perform better than the vanilla CNN
models in the drone ﬂight dataset. RCNN with spectral
clustering model performs best with 0.438 SRMSE compared to 0.464 SRMSE of vanilla CNN model. The values
predicted by our models are shown in Figure 7.
Linear Regression
Ridge Regression
CNN & explicit
RCNN1 & explicit
CNN & coeff
RCNN & coeff
1RCNN with three RCLs of iteration 2-2-2.
Table 2: SRMSE of regression on Groundwater data.
Linear Regression
Ridge Regression
CNN & explicit
RCNN1 & explicit
CNN & coeff
RCNN & coeff
1RCNN with three RCLs of iteration 2-2-2.
Table 3: SRMSE of regression on Drone data.
6. Conclusion
In this paper, we presented two structure learning algorithms for deep CNN models. Our algorithms exploited
Grouped Recurrent Convolutional Layers for Multivariate Time Series
the covariance structure over multiple time series to partition input volume into groups. The ﬁrst algorithm learned
the group CNN structures explicitly by clustering individual input sequences.
The second algorithm learned the
group CNN structures implicitly from the error backpropagation. In the experiments with two real-world datasets,
we demonstrate that our group CNN models outperformed
the existing CNN based regression methods.
Acknowledgements
The authors would like to thank Bo Liu at the Intelligent
Robotics Laboratory, University of Illinois, for helping
with collecting drone sensor data and the anonymous reviewers for their helpful and constructive comments. This
work was supported by the National Research Foundation
of Korea (NRF) grant funded by the Korea government
(Ministry of Science, ICT & Future Planning, MSIP) (No.
2014M2A8A2074096).