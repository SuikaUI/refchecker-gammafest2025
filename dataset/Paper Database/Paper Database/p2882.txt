Rapid Quality Assurance with Requirements Smells
Henning Femmera,∗, Daniel Méndez Fernándeza, Stefan Wagnerb, Sebastian Edera
aSoftware & Systems Engineering, Technische Universität München, Germany
bInstitute of Software Technology, University of Stuttgart, Germany
Context: Bad requirements quality can cause expensive consequences during the software development
lifecycle, especially if iterations are long and feedback comes late. Objectives: We aim at a light-weight
static requirements analysis approach that allows for rapid checks immediately when requirements are written
down. Method: We transfer the concept of code smells to Requirements Engineering as Requirements Smells.
To evaluate the beneﬁts and limitations, we deﬁne Requirements Smells, realize our concepts for a smell
detection in a prototype called Smella and apply Smella in a series of cases provided by three industrial and
a university context. Results: The automatic detection yields an average precision of 59% at an average
recall of 82% with high variation. The evaluation in practical environments indicates beneﬁts such as an
increase of the awareness of quality defects. Yet, some smells were not clearly distinguishable. Conclusion:
Lightweight smell detection can uncover many practically relevant requirements defects in a reasonably
precise way. Although some smells need to be deﬁned more clearly, smell detection provides a helpful means
to support quality assurance in Requirements Engineering, for instance, as a supplement to reviews.
Requirements Engineering, Quality Assurance, Automatic Defect Detection, Requirements
Introduction
Related work
The notion of smells in software engineering . . . . . . . . . . . . . . . . . .
Quality assurance of software requirements
. . . . . . . . . . . . . . . . . .
Discussion . . . . . . . . . . . . . . . .
Requirements Smells
Requirements Smell terminology
Requirements Smells based on ISO 29148
Smella: A prototype for Requirements
Smell detection
Requirements parsing
. . . . . . . . .
Language annotation . . . . . . . . . .
Findings identiﬁcation . . . . . . . . .
Findings presentation
. . . . . . . . .
Requirements Smell detection in the
process of quality assurance
Evaluation
Case study design
. . . . . . . . . . .
Research questions . . . . . . .
Case and subjects selection . .
Data collection procedure . . .
Analysis procedure . . . . . . .
Validity procedure . . . . . . .
Results . . . . . . . . . . . . . . . . . .
Case and subjects description .
 
 
RQ 1: How many Requirements
Smells are present in the artifacts? 21
RQ 2.1: How accurate is the
smell detection?
. . . . . . . .
RQ 2.2: Which of these smells
are practically relevant in which
context? . . . . . . . . . . . . .
Which requirements
quality defects can be detected
with smells? . . . . . . . . . . .
RQ 4: How could smells help
in the QA process? . . . . . . .
Evaluation of validity
Conclusion
Summary of conclusions . . . . . . . .
Relation to existing evidence
Impact/Implications . . . . . . . . . .
Limitations . . . . . . . . . . . . . . .
Future work . . . . . . . . . . . . . . .
Appendix A
Requirements Checklist 41
1. Introduction
Defects in requirements, such as ambiguities or incomplete requirements, can lead to time and cost
overruns in a project .
Some of the issues require speciﬁc domain knowledge to be uncovered. For
example, it is very diﬃcult to decide whether a requirements artifact is complete without domain knowledge. Other issues, however, can be detected more
easily: If a requirement states that a sensor should
work with suﬃcient accuracy without detailing what
suﬃcient means in that context, the requirement is
vague and consequently not testable. The same holds
for other pitfalls such as loopholes: Phrasing that a
certain property of the software under development
should be fulﬁlled as far as possible leaves room for
subjective (mis-)interpretation and, thus, can have
severe consequences during the acceptance phase of a
product .
To detect such quality defects, quality assurance
processes often rely on reviews. Reviews of requirements artifacts, however, need to involve all relevant
stakeholders , who must manually read and understand each requirements artifact. Moreover, they
are diﬃcult to perform. They require a high domain
knowledge and expertise from the reviewers and
the quality of their outcome depends on the quality of
the reviewer . On top of all this, reviewers could
be distracted by superﬁcial quality defects such as the
aforementioned vague formulations or loopholes. We
therefore argue that reviews are time-consuming and
Therefore, quality assurance processes would beneﬁt
from faster feedback cycles in requirements engineering (RE), which support requirements engineers and
project participants in immediately discovering certain types of pitfalls in requirements artifacts. Such
feedback cycles could enable a lightweight quality
assurance, e.g., as a complement to reviews.
Since requirements in industry are nearly exclusively written in natural language and natural
language has no formal semantics, quality defects in
requirements artifacts are hard to detect automatically. To face this challenge of fast feedback and the
imperfect knowledge of a requirement’s semantics, we
created an approach that is based on what we call
Requirements (Bad) Smells. These are concrete symptoms for a requirement artifact’s quality defect for
which we enable rapid feedback through automatic
smell detection.
In this paper, we contribute an analysis of whether
and to what extent Requirements Smell analysis can
support quality assurance in RE. To this end, we
1. deﬁne the notion of Requirements Smells and integrate the Requirements Smells1 concept into an
analysis approach to complement (constructive
and analytical) quality assurance in RE,
2. present a prototypical realization of our smell
detection approach, which we call Smella, and
3. conduct an empirical investigation of our approach to better understand the usefulness of
1In context of our studies, we use the ISO/IEC/IEEE
29148:2011 standard (in the following: ISO 29148) as
basis for deﬁning requirements quality. The standard supplies
a list of so-called Requirements Language Criteria, such as
loopholes or ambiguous adverbs, which we use to deﬁne eight
smells (see also the smell deﬁnition in Sect. 3.2).
a Requirements Smell analysis in quality assurance.
Our empirical evaluation involves three industrial
contexts: The companies Daimler AG as a representative for the automotive sector, Wacker Chemie AG
as a representative for the chemical sector, and Tech-
Divison GmbH as an agile-specialized company. We
complement the industrial contexts with an academic
one, where we apply Smella to 51 requirements artifacts created by students. With our evaluations, we
aim at discovering the accuracy of our smell analysis
taking both a technical and a practical perspective
that determines the context-speciﬁc relevance of the
detected smells. We further analyze which requirements quality defects can be detected with smells, and
we conclude with a discussion of how smell detection
could help in the (industrial) quality assurance (QA)
Previously published material
This article extends our previously published workshop paper in the following aspects: We provide
a richer discussion on the notion of Requirements
Smell and give a precise deﬁnition. We introduce
our (extended) tool-supported realization of our smell
analysis approach and outline its integration into the
QA process. We extend our ﬁrst two case studies with
another industrial one as well as with an investigation
in an academic context to expand our initial empirical
investigations by
1. investigating the accuracy of our smell detection
including precision, recall, and relevance from a
practical perspective,
2. analyzing which quality defects can be detected
with smells and
3. gathering practitioner’s feedback on how they
would integrate smell detection in their QA process considering both formal and agile process
environments.
The remainder of this paper is structured as follows.
In Sect. 2, we describe previous work in the area. In
Sect. 3, we deﬁne the concept of Requirements Smells
and describe how we derived a set of Requirements
Smells from ISO 29148. We introduce the tool realization in Sect. 4 and discuss the integration of smell
detection in context of quality assurance in Sect. 5.
In Sect. 6, we report on the empirical study that we
set up to evaluate our approach, before concluding
our paper in Sect. 7.
2. Related work
In the following, we discuss work relating to the
concept of natural language processing and smells in
general, followed by quality assurance in RE, before
critically discussing currently open research gaps.
2.1. The notion of smells in software engineering
The concept of code smells was, to the best of our
knowledge, ﬁrst proposed by Fowler and Beck 
to answer the question at which point the quality of
code is so low that it must be refactored. According
to Fowler and Beck, the answer cannot be objectively
measured, but we can look for certain concrete, visible
symptoms, such as duplicated code as an indicator
for bad maintainability . This concept of smells,
as well as the list that Fowler and Beck proposed, led
to a large ﬁeld of research. Zhang et al. provide
an in-depth analysis of the state of the art in code
smells. The metaphor of smells as concrete symptoms
has since then been transferred to quality of other
artifacts including (unit) test smells and smells for
system tests in natural language . Ciemniewska
et al. , further characterize diﬀerent defects of use
cases through the term use case smell. In our work,
we extend the notion of smells to the broader context
of requirements engineering and introduce a concrete
deﬁnition for the term Requirements Smell.
2.2. Quality assurance of software requirements
The concept of Requirements Smells is located in
the context of RE quality assurance (QA), which is
performed either manually or automatically.
Manual QA. Various authors have worked on QA for
software requirements by applying manual techniques.
Some put their focus on the classiﬁcation of quality
into characteristics , others develop comprehensive checklists . Regarding QA, some
develop constructive QA approaches, such as creating
new RE languages, e.g. , to prevent issues up front,
others develop approaches to make analytic QA, such
as reviews, more eﬀective . In a recent empirical
study on analytical QA, Parachuri et al. manually investigate the presence of defects in use cases.
To sum it up, these works on manual QA provide
analytical and constructive methods, as well as (varying) lists for defects. They strengthen our conﬁdence
that today’s requirements artifacts are vulnerable to
quality defects.
Automatic QA. Various publications discuss the automatic detection of quality violations in RE. We
summarize existing approaches and tools, their publications, and empirical evaluations in Table 2. We
also created an in-depth analysis of in total 27 related
publications evaluating which quality defects or smells
the approaches opt for in their described detection.
In the following, we will ﬁrst explain two related areas (automatic QA for redundancy and for controlled
languages), before discussing automatic QA for ambiguity in general. For ambiguity, we ﬁrst describe those
approaches that conducted empirical evaluations of
precision or recall of quality defects related, but not
identical to, the ones of ISO 29148. Afterwards, we focus on publications that mention the same criteria as
in the ISO 29148 (see Table 1 for this list and their respective empirical evaluations) and discuss the chosen
approaches and results. We publish the complete list
of each quality defect that is detected by each of the
27 papers, as well as the precision and recall (where
provided), online as supplementary material .
Automatic QA for redundancy. One speciﬁc area of
QA is avoiding redundancy and cloning. Whereas
Juergens et al. use ConQAT to search for syntactic identity resulting from a copy-and-paste reuse,
Falessi et al. aim at detecting similar content,
therefore using methods from information retrieval
(such as Latent Semantic Analysis ).
al. extend this work speciﬁcally for use cases.
Their tool, ReqAlign, classiﬁes each step with a semantic abstraction of the step. These publications
analyze the performance of their approaches, and depending on the artifact and methods achieve precision
and recall close to 1 (see Table 2).
Automatic QA for controlled languages. Another speciﬁc area is the application of controlled language and
the QA of controlled language. RETA speciﬁcally
analyzes requirements that are written via certain
requirements patterns (such as with the EARS template ). Their goal is to detect both conformance
to the template but also some of the ambiguities as de-
ﬁned by Berry et al . The authors report on a case
study where they look at the template conformance
in depth, indicating that template conformance can
be classiﬁed with various NLP suites to a high accuracy (Precision > 0.85, Recall > 0.9), both with and
without glossaries. However, the performance of ambiguity detection (such as the detection of pronouns)
is not further discussed in the publication. Similarly,
AQUSA analyzes requirements written in user
story format (c.f. for a detailed introduction into
user stories), and detects various defects, such as missing rationales, where they achieve a precision of 0.63-1.
Circe is a further tool that assumes that requirements are written in such requirements patterns
and detects violations of context- and domain-speciﬁc
quality characteristics by building logical models. The
authors report on six exemplary ﬁndings, which were
detected in a NASA case study. However, despite
their value to automatic QA, such approaches require
very speciﬁc requirements structure.
Automatic QA for ambiguity in general. The remaining approaches listed in Table 2 aim at detecting
ambiguities in unconstraint natural language. Since
the quality defects detected by the approaches by
Ciemniewska et al. , Kof , HeRA by Knauss et
al. , Kiyavitskaya et al. , RESI by Körner
et al. , and Alpino by DeBruijn et al. 
are not the ones discussed in ISO 29148 and since we
could not ﬁnd an evaluation of precision and recall of
these approaches, we omit discussing these approaches
in-depth here. An analysis of what these approaches
focus on in detail as well as their evaluation can be
found in short in Table 2 and in full length in our supplementary material online . In the following, we
ﬁrst report on those publications that focus on criteria
diﬀerent from ISO 29148, but which report precision
or recall. Afterwards, we describe publications that
aim at detecting quality violations of ISO 29148 (see
First, Chantree et al. target the speciﬁc grammatical issue of coordination ambiguity (detecting
problems of ambiguous references between parts of a
sentence), mostly through statistical methods, such
as occurrence and co-occurrence of words. In a case
study, they report on a precision of their approach
mostly between 54% and 75%.
even though they
do not explicitly diﬀerentiate between the detected
ambiguities and the concept of pronouns. Second,
Gleich et al. base their approach on the ambiguity handbook, as deﬁned by Berry et al. , as well
as company-speciﬁc guidelines. They compare their
dictionary- and POS-based approach against a gold
standard which they created by letting people highlight ambiguities in requirements sentences. The gold
standard deviates substantially, however, from what is
considered high quality in their guidelines. Therefore,
they create an additional gold standard, mostly based
on the guideline rules. Consequently, their precision2
varies between 34% for the pure experts opinion, to
97% for a more guideline-based gold standard. Third,
Krisch and Houdek , focus on the detection of
passive voice and so-called weak words. They present
their dictionary- and POS-based approach to practitioners and ﬁnd many false positives, similar to our
RQ 3. In average, a precision of 12% is reported for
the weak words detection. These approaches focus on
very related, but not identical quality violations or
Automatic QA for ISO 29148 criteria. Lastly, we
speciﬁcally focus on those approaches that report
to detect the criteria from the ISO 29148 standard.
Table 1 provides an overview of these works and their
respective evaluations.
2Gleich et al. calculate their metrics based on the combination of all ambiguities; unfortunately, they do not diﬀerentiate,
e.g. by the type of ambiguity. Also, to our knowledge, the
gold standard does not diﬀerentiate between the types. This
prevents a direct comparison to their work.
The ARM tool deﬁnes quality in terms of the
(now superseeded) IEEE 830 standard and proposes generic metrics, instead of giving feedback directly to requirements engineers. The metrics are
calculated through counting how often a set of predeﬁned terms (per metric) occurs in a document, including a metric of what we call Loopholes. Even
though they report on a case study with 46 speciﬁcations from NASA, only a quantitative overview is
reported3. The QuARS tool is based on the
author’s experience. Bucchiarone et al. describe
the use of QuARS in a case study with Siemens and
show some exemplary ﬁndings. SyTwo adopts
the quality model of QuARS and applies it to use
cases. Loopholes and Subjectivity are part of the
QuARS quality model. Also RQA is built on a different, proprietary quality model, as described by
Génova et al. , which includes negative terms as
well as pronouns as quality defects. These works also
built upon extending natural language with NLP annotations, such as POS tags and searching through
dictionaries for certain problematic phrases. However,
we could not ﬁnd a detailed empirical investigation
of these tools, e.g. with regards to precision and recall. SREE is an approach by Tjong and Berry ,
which aims at detection of ambiguities with a recall
of 100%. Therefore, they completely avoid all NLP
approaches (since they come with imprecision), and
build large dictionaries of words. The tool includes
detection of loopholes, as well as pronouns; however,
they report only on an aggregated precision for all
the diﬀerent types of ambiguities (66-68%) from two
case studies. In our previous paper , we searched
for violations of ISO 29148, yet we provided only a
quantitative analysis, as well as qualitative examples.
As mentioned before, RETA also issues warnings for
pronouns, however, the evaluation in their paper 
focusses on template conformance.
2.3. Discussion
Previous work has led to many valuable contributions to our ﬁeld. To explore open research gaps, we
now critically reﬂect on previous contributions from
3See also our RQ 1 in Sect. 6.
Table 1: Related work on criteria of ISO-29148 standard, detailed supplementary material can be found
online 
Ambiguous Adv. & Adj.
Comparatives
Loopholes (or Options)
Negative Terms
Non-Veriﬁable Terms
Subjectivity
Superlatives
Legend: O=No empirical analysis, E=Examples from Case, Q=Quantiﬁcation, P=Precision analyzed,
R=Recall analyzed, *=Aggregated over multiple smells
an evaluation, a quality deﬁnition and a technical
perspective.
First, one gap in existing automatic QA approaches is the lack of empirical evidence, especially under
realistic conditions. Only few of the introduced contributions were evaluated using industrial requirements
artifacts.
Those who do apply their approach on
such artifacts focus on quantitative summaries explaining which ﬁnding was detected and how often
it was detected. Some authors also give examples of
ﬁndings, but only few works analyze this aspect in
depth with precision and recall, especially in the fuzzy
domain of ambiguity (see Table 2). When looking at
the characteristics that are described in ISO 29148,
we have not seen a quantitative analysis of precision
and recall. Furthermore, reported evidence does not
include qualitative feedback from engineers who are
supposed to use the approach, which could reveal
many insights that cannot be captured by numbers
alone. However, we postulate that the accuracy of
quality violations very much depends on the respective
context. This is especially true for the fuzzy domain of
natural language where it is important to understand
the (context-speciﬁc) impact of a ﬁnding to rate its
detection for appropriateness and eventually justify
resolving the issue.
Second, the existing approaches are based on proprietary deﬁnitions of quality, based on experience
or, sometimes, simply on what can be directly measured. The ARM tool is loosely based on the
IEEE 830 standard. However, as the recent literature survey by Schneider and Berenbach states:
“the ISO/IEC/IEEE 29148:2011 is actually the standard that every requirements engineer should be familiar with”. We are not aware of an approach that
evaluates the current ISO 29148 standard in this
respect. As Table 1 shows, for most language quality
defects of ISO 29148, there has not yet been a tool
to detect these quality defects. To all our knowledge,
for neither of these factors, there is an diﬀerentiated
empirical analysis of precision and recall. Yet, many
other quality models (most notably from the ambiguity handbook by Berry et al. ) and quality violations
could lead to Requirements Smells, as far as they comply with the deﬁnition given in the next section.
Finally, taking a more technical perspective, our
Requirements Smell detection approach does not fundamentally diﬀer from existing approaches. Similar
to previous works, we apply existing NLP techniques,
such as lemmatization and POS tagging, as well as
dictionaries. For the rules of the ISO 29148 standard,
no parsing or ontologies (as used in other approaches)
were required. However, to detect superlatives and
comparatives in German, we added a morphological
analysis, which have not yet seen in related work.
In summary, in our contribution, we extend the
current state of reported evidence on automatic QA
for requirements artifacts via systematic studies in
terms of distribution, precision, recall, and relevance,
as well as by means of a systematic evaluation with
Table 2: Related approaches and tools, and their evaluation, detailed supplementary material can be found online 
Tool/Approach
Purpose (unless ambiguity det.)
Publications
Evaluation
Redundancy
Redundancy
Redundancy
Structured Language Rules
User Story Rules
Structured Language Rules
(Ciemniewska)
(Kiyavitskaya)
 
(Chantree)
E/Q*/P*/R*
RE Artifact Metrics
QuARS / SyTwo
 
0.66-0.68*
Legend: O=No empirical analysis, E=Examples from Case, Q=Quantiﬁcation, P=Precision analyzed, R=Recall analyzed,
*=Aggregated over multiple smells
practitioners under realistic conditions. We perform
this on both existing, as well as new quality defects
taken from the ISO 29148. Therefore, we extend our
previously published ﬁrst empirical steps to close
these gaps by thorough empirical evaluation.
3. Requirements Smells
We ﬁrst introduce the terminology on Requirements
Smells as used in this paper. In a second step, we
deﬁne those smells we derived from ISO 29148 and
which we use in our studies, before describing the tool
realization in the next section.
3.1. Requirements Smell terminology
Code smells are supposed to be an imprecise indication for bad code quality .
We apply this
concept of smells to requirements and deﬁne it as
follows: A Requirements Smell is an indicator of a
quality violation, which may lead to a defect, with a
concrete location and a concrete detection mechanism.
In detail, we consider a smell as having the following
characteristics:
1. A Requirements Smell is an indicator for a quality violation of a requirements artifact. For this
deﬁnition, we understand requirements quality in
terms of quality-in-use, meaning that bad requirements artifact quality is deﬁned by its (potential)
negative eﬀects on activities in the software lifecycle that rely on these requirements artifacts
(see also ).
2. A Requirements Smell does not necessarily lead
to a defect and, thus, has to be judged by the context (supported e.g. by (counter-/)indications).
Whether a Requirements Smell ﬁnding is or is
not a problem in a certain context must be individually decided for that context and is subject
to reviews and other follow-up quality assurance
activities.
3. A Requirements Smell has a concrete location
in an entity of the requirements artifact itself,
e.g. a word or a sequence. Requirements Smells
always provide a pointer to a certain location that
QA must inspect. In this regard, it diﬀers from
general quality characteristics, e.g. completeness,
that only provide abstract criteria.
4. A Requirements Smell has a concrete detection
mechanism. Due to its concrete nature, Requirements Smells oﬀer techniques for detection of the
smells. These techniques can, of course, be more
or less accurate.
Furthermore, we deﬁne a quality defect as a concrete
instance or manifestation of a quality violation in the
artifact, in contrast to a ﬁnding which is an instance
of a smell. However, like a smell indicates for a quality
violation, the ﬁnding indicates for a defect. Fig. 1
visualizes the relation of these terms.
Quality Model
Requirements Smells
indicates for
supported by
indicates for
automated by
present in
instance of
Quality-inuse
Figure 1: Terminology of Requirements Smells (simpliﬁed)
In the following, we will focus on natural language
Requirements Smells, since requirements are mostly
written in natural language . Furthermore, the real
beneﬁts of smell detection in practice should come
with automation. Therefore, the remainder of the
paper discusses only Requirements Smells where the
detection mechanism can be executed automatically
(i.e. it requires no manual creation of intermediate or
supporting artifacts).
3.2. Requirements Smells based on ISO 29148
We develop a set of Requirements Smells based on
an existing deﬁnition of quality. For the investigations in scope of this paper, we take the ISO 29148
requirements engineering standard as a baseline.
The reasons for this are two-fold.
First, the ISO 29148 standard was created to harmonize a set of existing standards, including the
IEEE 830:1998 standard. It diﬀerentiates between
quality characteristics for a set of requirements, such
as completeness or consistency, and quality characteristics for individual requirements, such as unambiguity
and singularity. The standard furthermore describes
the usage of requirements in diﬀerent project phases
and gives exemplary contents and structure for requirements artifacts. Therefore, we argue that this
standard is based on a broad agreement and acceptance. Recent literature studies come to the same
conclusion .
Second, the standard provides readers with a list
of so-called requirements language criteria which support the choice of proper language for requirements
artifacts. The authors of the standard argue that
violating the criteria results “in requirements that are
often diﬃcult or even impossible to verify or may allow
for multiple interpretations" [33, p.12]. For deﬁning
our smells, which we describe next, we refer to this
section of the standard and use all the deﬁned requirements language criteria. We employ those criteria
as a starting point and deﬁne the smells by adding
the aﬀected entities (e.g. a word) and an explanation. Here, we do not discuss the impact smells have
on the quality-in-use. Essentially, smells hinder the
understandability of requirements and consequently
their subsequent handling and their veriﬁcation (for
a richer discussion, see also previous work in ).
Our current understanding is based on the examples given by the standard.
A subset of
the language criteria,
Subjective Language,
Ambiguous Adverbs and Adjectives and
Non-verifiable Terms,
as deﬁned in ,
strongly related, essentially since subjective language
is a special type of ambiguity, which may lead to issues during veriﬁcation. Since the intention of this
work is to start with the standard as a deﬁnition of
quality, in the following, we will remain with the provided deﬁnition based on the language criteria and
leave the development of a precise and complete set
of Requirements Smells to future work. In detail, we
use the requirements language criteria to derive the
smells summarized next.
Smell Name:
Subjective Language
Explanation:
Subjective Language refers to
words of which the semantics is
not objectively deﬁned, such as
user friendly, easy to use, cost
The architecture as well as the
programming must ensure a simple and eﬃcient maintainability.
Smell Name:
Ambiguous Adverbs and Adjectives
Adverb, Adjective
Explanation:
Ambiguous Adverbs and Adjectives refer to certain adverbs and
adjectives that are unspeciﬁc by
nature, such as almost always, signiﬁcant and minimal.
If the (...) quality is too low, a
fault must be written to the error
Smell Name:
Explanation:
Loopholes refer to phrases that
express that the following requirement must be fulﬁlled only to a
certain, imprecisely deﬁned extent.
As far as possible, inputs are
checked for plausibility.
Smell Name:
Open-ended, Non-verifiable
Explanation:
Open-ended, non-veriﬁable terms
are hard to verify as they oﬀer a
choice of possibilities, e.g. for the
developers.
The system may only be activated, if all required sensors (...)
work with suﬃcient measurement accuracy.
Smell Name:
Superlatives
Adverb, Adjective
Explanation:
Superlatives refer to requirements
that express a relation of the system to all other systems.
The system must provide the signal in the highest resolution that
is desired by the signal customer.
Smell Name:
Comparatives
Adverb, Adjective
Explanation:
Comparatives are used in requirements that express a relation of
the system to speciﬁc other systems or previous situations.
The display (...)
contains the
ﬁelds A, B, and C, as well as
more exact build infos.
Smell Name:
Negative Statements
Explanation:
Negative Statements are “statements of system capability not
to be provided" . Some argue
that negative statements can lead
to underspeciﬁcation, such as lack
of explaining the system’s reaction on such a case.
The system must not sign oﬀ
users due to timeouts.
Smell Name:
Vague Pronouns
Explanation:
Vague Pronouns are unclear relations of a pronoun.
The software must implement
services for applications, which
communicate
controller applications deployed on
other controllers.
Smell Name:
Incomplete References
Text reference
Explanation:
Incomplete References are references that a reader cannot follow
(e.g. no location provided).
 “Unknown white paper". Peter Miller.
4. Smella:
A prototype for Requirements
Smell detection
Requirements Smell detection, as presented in this
paper, serves to support manual quality assurance
tasks (see also the next section). The smell detection
is implemented on top of the software quality analysis
toolkit ConQAT,4 a platform for source code analysis,
which we extended with the required NLP features.
In the following, we introduce the process for the automatic part of the approach, i.e. the detection and
reporting of Requirements Smells. To the best of our
knowledge, there is no tool, other than the ones mentioned in related work, that detect and present these
smells in natural language requirements documents.
The process takes requirements artifacts in various formats (MS Word, MS Excel, PDF, plain text,
comma-separated values) and consists of four steps
(see also Fig. 2):
1. Requirements parsing of the requirements artifacts into single items (e.g. sections or rows),
resulting in plain texts, one for each item
4 
Requirements
Annotation
Identiﬁcation
POS Tagging
Morphologic Analysis
Lemmatization
Presentation
Figure 2: The overall smell detection process
2. Language annotation of the requirements with
meta-information
3. Findings identiﬁcation in the requirements, based
on the language annotations
4. Presentation of a human-readable visualization
of the ﬁndings as well as a summary of the results
The techniques behind these steps are explained in
the following section.
4.1. Requirements parsing
Our current tool is able to process several ﬁle formats: MS Word, MS Excel, PDF, plain text and
comma-separated values (CSV). Depending on the
format, the ﬁles are parsed in diﬀerent ways. Plain
text and PDF are taken as is and parsed ﬁle by ﬁle.
Microsoft Word ﬁles are grouped by their sections.
For Microsoft Excel and CSV ﬁles, we deﬁne those
columns that represent the IDs or names (if there are
any), and those columns should be used as text input
to detect smells.
If a ﬁle is written in a known template, such as a
common template for use cases, we can make use of
this template to understand structural defects, such
as lacking content items in a template. In the remainder of this paper, however, we focus on the natural
language Requirements Smells as provided by the ISO
4.2. Language annotation
For the annotation and smell detection steps, we
employ three techniques from Natural Language Processing (NLP) . Table 3 additionally shows which
of the techniques we use for which smell.
POS Tagging: For two smells, we use part-of-speech
(POS) tagging. Given a sentence in natural language, it determines the role and function of
each single word in the sentence. The output is
a so-called tag for each word indicating, for instance, whether a word is an adjective, a particle,
or a possessive pronoun. We used the Stanford
NLP library and the RFTagger for this.
Both are statistical, probabilistic taggers that
train models similar to Hidden Markov Models
based on existing databases of tagged texts. A
detailed introduction into the technical details of
POS tagging is beyond the scope of this paper
but can be found, for example, in . We use
POS tagging to determine so-called substituting
pronouns. These are pronouns that do not repeat the original noun and, thus, need a human’s
interpretation of its dependency.
Morphological Analysis: Based on POS tagging,
we perform a more detailed analysis of text and
determine a word’s inﬂection. This includes, inter
alia, determining a verb’s tense or an adjective’s
comparison. We use this technique to analyze if
adjectives or adverbs are used in their comparative or superlative form.
Dictionaries & Lemmatization: For the remaining ﬁve smells, we use dictionaries based on the
proposals of the standard and on our experiences from ﬁrst experiments in a previous
work . We furthermore apply lemmatization
for these words, which is a normalization technique that reproduces the original form of a word.
In other words, if a lemmatizer is applied to the
words were, is or are, the lemmatizer will return for all three the word be. Lemmatization
is in its purpose very similar to stemming (see,
e.g. the Porter Algorithm ), yet not based
on heuristics but on the POS tag as well as the
word’s morphological form. For Requirements
Smells, the diﬀerence is signiﬁcant: For example,
the words use and useful stem to the same word
origin (use), but to diﬀerent lemmas (i.e. meanings; use and useful). Whereas the lemma use is
mostly clear to all stakeholders, the lemma useful
is easily misinterpreted.
4.3. Findings identiﬁcation
Based on the aforementioned information, we identify ﬁndings. This step actually ﬁnds the parts of an
artifact that exhibit bad smells. Dependent on the
actual smell, we use diﬀerent techniques, as shown in
Table 3. If the smell relates to a grammatical aspect,
we search through the information from POS tagging
and morphological analyses. For example, for the
Superlatives Smell, we report a ﬁnding if an adjective is, according to morphologic analysis, inﬂected
in its superlative form. If the smell does not relate
to grammatical aspects but rather the semantics of
the requirements, we identify the smell by matching
the lemma of a word against a set of words from predeﬁned dictionaries. Since the requirements under
analysis in our cases did not contain references, incomplete references are not part of our tool at present.
4.4. Findings presentation
We implemented the presentation of ﬁndings in
a prototype, which we call Smella (Smell Analysis).
Smella is a web-based tool that enables viewing, reviewing and blacklisting ﬁndings as well as a hotspot
analysis at an artifact level. In the Smella presentation, we display the complete requirements artifact
and annotate ﬁndings in a spell checker style. This
follows the idea of smells as only indications that must
be evaluated in their context. Lastly, the system gives
detailed information when a user hovers a ﬁnding (see
Fig. 3). In the following, we shortly describe the features of Smella in detail to provide the reader with a
rough understanding of the prototype.
View ﬁndings: At the level of a single artifact, we
present the text of the artifact and its structure.
We mark all ﬁndings in the text. With a click on
the markers, more information about the ﬁnding
is displayed. The tool provides an explanation of
the rationale behind this smell and possible improvements for the ﬁnding depending on the smell
(every smell has a message for improvements).
Review ﬁndings: We allow the user to write a review and to set a status for each ﬁnding, both
supporting feedback mechanisms within and between project teams. A user has the possibility
to accept or reject a ﬁnding but also to set a custom state, for example under review. Accepting a
ﬁnding means the ﬁnding needs to be addressed.
If a ﬁnding is rejected, the ﬁnding does not need
to be addressed. The semantics of the custom
status is open to the reviewer.
Blacklist ﬁndings: Smells are only indicators for
issues. Therefore, users can reject ﬁndings. If
a ﬁnding is rejected by the user, the ﬁnding is
removed from the visualization and will not be
presented to the user anymore.
Disable smells: Often, users are interested in only a
subset of smells or even just one smell. Therefore,
we allow the user to hide all ﬁndings of particular
smells and to select the smells she wants to display
in the artifact view.
Analyze hotspots: In this view, we present all artifacts in a colored treemap (see Fig. 4). Every
box in the treemap is one artifact, with the color
Figure 3: A sample output from the smell detection tool (detailed artifact view) with some smells disabled
and some ﬁndings blacklisted
Figure 4: A sample output from the smell detection tool (hotspot analysis view)
Table 3: Detection techniques for smells
Smell Name
Detection Mechanism
Subjective Language
Dictionary
Ambiguous Adverbs and Adjectives
Dictionary
Dictionary
Open-ended, non-veriﬁable terms
Dictionary
Superlatives
Morphological analysis or POS tagging
Comparatives
Morphological analysis or POS tagging
Negative Statements
POS tagging and dictionary
Vague Pronouns
POS tagging: Substituting pronouns.
Incomplete References
Not in scope of this study
of the box indicating the number of ﬁndings: the
more red an artifacts is, the more ﬁndings it contains (the more it “smells” bad). The artifacts
are grouped by their folder structure. The tool
provides a summarized treemap for all smells
as well as a separate treemap for all individual
smells. With these treemaps, users can identify
artifacts or groups of artifacts exhibiting a high
number of ﬁndings – for one single smell but also
for all smells together. This feature supports the
identiﬁcation of candidates for in-depth reviews.
5. Requirements Smell detection in the process of quality assurance
The Requirements Smell detection approach described in previous sections serves the primary purpose
of supporting quality assurance in RE. The detection
process itself is, however, not restricted to particular quality assurance tasks, nor does it depend on a
particular (software) process model as we will show
in Sect. 6. Hence, a smell detection, similar to the
notion of quality itself, always depends on the views
in a socio-economic context. Thus, how to integrate
smell detection into quality assurance needs to be answered according to the particularities of that context.
In the following, we therefore brieﬂy outline the role
smell detection can generally take in the process of
quality assurance. More concrete proposals on how
to integrate it into speciﬁc contexts are given in our
case studies in Sect. 6.
We postulate the applicability of the Requirements
Smell detection in the process of both constructive
and analytical quality assurance (see Fig. 5). From
the perspective of a constructive quality assurance,
authors can use the smell detection to increase their
awareness of potential smells in their requirements
artifacts and to remove smells before releasing an artifact for, e.g., an inspection. External reviewers in
turn, can then use the smell detection to prepare analytical, potentially cost-intensive, quality assurance
tasks, such as a Fagan inspection . Such an inspection involves several reviewers and would beneﬁt
from making potential smells visible in advance. Iterative inspection approaches are also known as phased
inspections, as deﬁned by Knight and Myers .
We furthermore believe that one major advantage
is that the scope of our smell detection is not to
enforce resolving a potential smell but to increase
the awareness of the like and to make transparent
later reasoning why certain decisions have been taken.
Please note that two diﬀerent roles (e.g. requirements
engineer and QA engineer) can take two diﬀerent viewpoints on the same smell, respectively its criticality
and whether it should be resolved or not. In addition,
a ﬁnding could be unambiguous to the author, but
unclear to the target group of readers (represented by
the reviewers). Therefore, one contribution of our toolsupported smell detection is also to actively foster the
communication between reviewers and authors and to
enable continuous feedback between both roles. For
this reason, we enable stakeholders in Smella to com-
Smell Detection
Constructive QA
Analytical QA
View ﬁndings &
Review ﬁndings
View ﬁndings &
Review ﬁndings
Figure 5: A suggestion for applying Requirements Smell detection in QA
ment on detected smells and make explicit whether
they need to be resolved or whether and why they
have been accepted or rejected.
6. Evaluation
For a better, empirical understanding of smells in
requirements artifacts, we conducted an exploratory
multi-case study with both industrial and academic
cases. We particularly rely on case study research
over other techniques, such as controlled experiments,
because we want to evaluate our approach in practical
settings under realistic conditions. For the design
and reporting of the case study, we largely follow the
guidelines of Runeson and Höst .
6.1. Case study design
Our overall research objective is as follows:
Research Objective: Analyze whether automatic
analysis of Requirements Smells helps in requirements
artifact quality assurance.
To reach this aim, we formulate four research questions (RQ). In the following, we introduce those research questions, the procedures for the case and
subjects selection, the data collection and analysis,
and the validity procedures.
6.1.1. Research questions
RQ 1: How many smells are present in requirements artifacts? To see if the automatic detection
of smells in requirements artifacts could help in QA,
we ﬁrst need to verify that Requirements Smells exist
in the real world. The answer to this question fosters
the understanding how widespread the smells under
analysis are in industrial and academic requirements
artifacts.
RQ 2: How many of these smells are relevant?
Not only the number of detected smells is important.
If many of the detected smells are false positives and
not relevant for the requirements engineers and developers, it would hinder QA more than it would help.
As relevancy is a rather broad concept, we break down
RQ 2 into two sub-questions.
RQ 2.1: How accurate is the smell detection? The ﬁrst sub-question looks at the more
technical view on relevance. We want to ﬁnd
false positives and false negatives to determine
the precision and recall of the analysis in terms
of correct detection of the deﬁned smell.
RQ 2.2: Which of these smells are practically relevant in which context? This second sub-question is concerned with practical relevance.
We investigate whether practitioners
would react and change the requirement when
confronted with the ﬁndings.
RQ 3: Which requirements quality defects can
be detected with smells? After we understood how
relevant the analyzed Requirements Smells are, we
want to understand their relation to existing quality
defects in requirements artifacts. Hence, we need to
check whether, and if so, which defects in requirements
artifacts correspond to smells, as we understand smell
ﬁndings as indicators for defects.
RQ 4: How could smells help in the QA process? Finally, we collect general feedback from practitioners whether (and how) smell detection could be
a useful addition to QA for requirements artifacts and
whether as well as how they would integrate the smell
detection into their QA process.
6.1.2. Case and subjects selection
Our case and subject selection is opportunistic but
in a way that maximizes variation and, hence, evaluates the smell detection in very diﬀerent contexts.
This is particularly important for investigating requirements artifacts under realistic conditions, also
due to the large variation in how these artifacts manifest themselves in practice. A prerequisite for our
selection is the access to the necessary data. To get
a reasonable quantitative analysis of the number of
smells (RQ 1) and qualitative analysis of the relation of smells and defects (RQ 3), we complement
our three industrial cases with a case in an academic
setting. There, various student teams are asked to
provide software with a certain set of (identical) functionality for a customer as part of a practical course.
This is also a realistic setting but provides us with a
higher number of speciﬁcations and reviews than in
the industrial cases.
We will refer to the subjects of the industrial cases
as practitioners and we will call the latter subjects
6.1.3. Data collection procedure
We used a 6-step procedure to collect the data
necessary for answering the research questions.
1. Collect requirements artifact(s) for each case. We
retrieved the requirements artifacts to be analyzed in each case. For one case, the requirements were stored in Microsoft Word Documents.
For the other cases, this involved extracting the
requirements from other systems, either a proprietary requirements management tool (resulting
in a list of html ﬁles), or the online task management system JIRA, which led to a set of commaseparated values ﬁles. For the student projects,
the students handed in their ﬁnal artifacts either
as a single PDF or as a PDF with the general
artifact and another PDF with the use cases.
Where authors explicitly structured requirements
in numbered requirements, user stories or use
cases, we counted these artifacts.
2. Run the smell detection via Smella. We applied
our detection tool as introduced in Sect. 4.4 on
the given requirements artifacts, which generated
a list of smells per artifact.
3. Classify false positives. For all cases in which we
wanted to present our results to practitioners, we
reviewed each detected ﬁnding. In pairs of researchers, we classiﬁed the ﬁndings as either true
or false positive. We classiﬁed a ﬁnding as false
positive if the ﬁnding was not an instance of the
smell, e.g. because the results of the linguistic
analysis was incorrect.5
For artifacts containing more than 10 ﬁndings of a smell, we only
inspected a set of 10 random ﬁndings (of that
5For example, if the linguistic analysis incorrectly classiﬁed
the word provider in the sentence “As a provider, I want [. . . ]”
as a comparative adjective.
smell) per artifact. The same holds for Case D,
where we inspected 10 random ﬁndings of each
category for the whole case.
4. Inspect documents for false negatives.
calculate the recall of the smell detection,
for each case we randomly selected one artifact that a pair of researchers inspected
for false negatives.
To ease the manual
inspection,
we grouped the smells Subjective Language,
Ambiguous Adverbs and Adjectives, Loopholes, Non-verifiable Terms
(as Ambiguity-related smells). We classiﬁed
whether a ﬁnding is a true or false negative based
on the same conditions as in the previous step.
One common cause for false negatives for
dictionary-based smells can be that an ambiguous phrase is not part of the dictionary. Since
we developed the dictionaries based on existing
dictionaries, such as the standard, these dictionaries are not yet complete and must be further
developed. However, since this is an issue that is
not a problem of the smell detection approach in
general, but rather a conﬁguration task, we did
not take these ﬁndings into consideration for the
5. Get rating by practitioners. We selected a subset
of the true positive ﬁndings so that we cover all
smells with a minimum of two ﬁndings per smell
as far as the artifacts allowed. When we found
repeating or similar ﬁndings, e.g. multiple similar
sentences with the same smell, we also included
one of these ﬁndings into the set.
We presented this subset to the practitioners and interviewed them, ﬁnding by ﬁnding,
through three closed questions (see also Table 9):
Q1: Would you consider this smell as relevant?
Q2: Have you been aware of this ﬁnding before?
Q3: Would you resolve the ﬁnding? Of these,
the former two must be answered with yes or
no. For the last question, we also needed to take
the criticality into account. Therefore, in case
practitioners answered that they would resolve a
ﬁnding, we also asked whether they would resolve
it immediately, in a short time (i.e. within this
project iteration) or in a long time (e.g. if it happens again). In addition to these three questions,
we took notes of qualitative feedback, such as
discussions.
6. Interview practitioners. In addition to the ratings, we performed open interviews with practitioners about their experience with the smell
detection and how they might include it in their
quality assurance process. We took notes of the
7. Get review results from students. Lastly, the students performed reviews of the artifacts of other
student teams. They documented and classiﬁed
found problems according to a checklist (see Table A.11) without awareness of the smell ﬁndings
in their artifacts. We then collected the review
reports from the students.
6.1.4. Analysis procedure
We structure our analysis procedure into seven
steps. Each step leads to the results necessary for
answering one of our research questions.
1. Calculate ratios of ﬁndings per artifact. To understand whether smells are a common issue in
requirements artifacts, we compared the quantitative summaries of smells in the various artifacts
and domains. To enable a comparison between
diﬀerent types of requirement artifacts, we used
the number of words in each artifact as a measure
of size. Hence, we ﬁnally reported the ratio of
ﬁndings per 1000 words for each smell and all
smells in total. This provided answers for RQ 1.
2. Calculate ratios of ﬁndings for parts of user stories. In one case, we had a common structure of
the requirements, because they were formulated
as user stories. To get a deeper insight into the
distribution of smells and ﬁndings, we calculated
the ratios of ﬁndings per 1000 words for each
part. We divided the user stories into the parts
role (“As a. . . ”), feature (“I want to. . . ”) and
reason (“so that. . . ”) using regular expressions.
We counted the words and ﬁndings in each part.
This provided further insights into the answer for
3. Calculate ratios of false positives. After a rough
overview obtained under the umbrella of RQ 1
describing the number of ﬁndings for each smell
of the varying artifacts, we wanted to better understand the smell’s relevance. The ﬁrst step
was to calculate the ratios of false positive as we
classiﬁed them in Step 3 of the data collection.
We reported false positive rates overall and for
each smell. This provides the ﬁrst part of the
answer to RQ 2.1.
4. Calculate ratios of false negatives. The precision of a smell detection is tightly coupled with
the recall. Therefore, we calculated the ratio of
detected smell ﬁndings to all existing ﬁndings,
according to our manual inspection, as described
in Step 4 of the data collection procedure. This
provides the second part of the answer to RQ 2.1.
5. Calculate ratio of irrelevant smells. We were not
only interested in errors in the linguistic analysis
but also in how relevant the correct analyses
were for the practitioners. Hence, we calculated
and reported the ratios of ﬁndings considered
irrelevant by the practitioners.
This answers
6. Compare defects from reviews with ﬁndings. From
the students, we received review reports for each
artifact. As the eﬀort to check them all would
have been overwhelming, we took a random sample of 20% of the artifacts. For each of the defects
detected in the review, we checked if there is a
corresponding ﬁnding from a smell. This answers
7. Interpret interview notes. To answer ﬁnally RQ 4,
we analyze the interview transcripts and code the
answers given by the interviewees manually.
6.1.5. Validity procedure
First, we used peer debrieﬁng in the sense that all
data collection and analyses were done by at least two
researchers. Analysis results were also checked by all
researchers. This researcher triangulation especially
increases the internal validity. Furthermore, we kept
an audit trail in a Subversion system to capture all
changes to documents and analyses.
Second, we performed all the classiﬁcations of ﬁndings into true and false positives in pairs. This already
helped to avoid misclassiﬁcations. To further check
our classiﬁcations, we afterwards did an independent
re-classiﬁcation of randomly selected 10% of the ﬁndings and calculated the inter-rater agreement. We
discussed to clarify which ﬁndings we consider false
positives and repeated the classiﬁcations until we
reached an acceptable agreement. The same procedure held for the inspection of artifacts to detect false
negatives, which we also conducted in pairs. Furthermore, we also independently re-classiﬁed one of the
artifacts to understand the inter-rater agreement on
the false negatives. Overall, our analysis for false positives and relevance of the ﬁndings is also a validity
procedure in the sense that we check in RQ 2 the
results from RQ 1.
Third, we discussed with the practitioners what
relevance of smells means in the context of the study
to avoid misinterpretations. Furthermore, we gave the
students review guidelines to give them an indication
what quality defects in requirements artifacts might
be. Both serve in particular as mitigation to threats
to the internal and the construct validity.
Fourth, we performed the analysis of the correspondence between smells and defects with a pair of
researchers. This pair derived a classiﬁcation of the
found and not found defects. Both other researchers
reviewed the classiﬁcation, and we improved it iteratively until we reached a joint agreement.
Fifth, we performed member checking by showing
our transcriptions and interpretations for RQ 4 to the
interviewed practitioners and incorporating feedback.
Finally, to support the external validity of the results of our study, we aimed at selecting cases with
maximum variation in their domains, sizes, and how
they document requirements.
6.2. Results
In the following, we report on the results of our case
studies. We ﬁrst describe the cases and subjects under
analysis, before we answer the research questions. We
end by evaluating the validity of the cases.
6.2.1. Case and subjects description
The ﬁrst three cases contain requirements produced
in diﬀerent industrial contexts: embedded systems
in the automotive industry, business information systems for the chemical domain and agile development
of web-based systems. While the ﬁrst two represent
rather classical approaches to Requirements Engineering, the third case applies the concept of user stories,
as it is popular in agile software development. The
fourth case is in an academic background and employs
both use cases and textual requirements. Regarding
subject selection, for each industrial case we selected
practitioners involved in the company, domain and
speciﬁcation. We executed the ﬁndings rating (Step 5)
and the interviews regarding the QA process (Step 6)
with the same experts, so that their answer in Step 6
is based on their experience with practical, real examples. In the following, we describe the cases, as
well as the experts or students for each case. Table 4
provides a quantitative overview of the cases.
Case A: Daimler AG. Daimler AG is a multinational
automotive corporation headquartered in Stuttgart,
Germany. At Daimler, we analyzed six diﬀerent requirements artifacts (A1–A6) which were written by
various authors. The requirements artifacts describe
functionality in diﬀerent domains of engine control as
well as driving information. In this case, requirements
are written down in the form of sentences, identiﬁed
by an ID. The authors are domain experts who are
coached on writing requirements.
The requirements artifacts A1–A6 consist of 323
requirements in total (see Table 4). All of the artifacts
of Daimler analyzed in our study were created by
domain experts in a pilot phase after a change in the
requirements engineering process as part of a software
process improvement endeavour.
For RQ 2.2., we
reviewed 22 ﬁndings with an external coach who works
as a consultant for requirements engineering and has
tightly collaborated with the group for many years.
Case B: Wacker Chemie AG. In the second case, we
analyzed requirements artifacts of business information systems from Wacker Chemie AG. Wacker is a
globally active company working in the chemical sector and headquartered in Munich, Germany. The
systems that we analyzed fulﬁl company-internal purposes, such as systems for access to Wacker buildings
or support systems for document management.
We analyzed three Wacker requirements artifacts
that were written by ﬁve diﬀerent authors. At Wacker,
functional requirements are written as use cases (including ﬁelds for Name, Description, Role and Precondition) whereas non-functional requirements are
described in simple sentences. The artifacts consisted
of 53 use cases and 13 numbered requirements (see
Table 4). For the reviews of the ﬁndings in RQ 2.2,
we selected 18 ﬁndings and discussed them with the
Chief Software Architect, who also has several years
of experience in quality assurance.
Case C: TechDivision. For the third case, we analyzed the requirements of the agile software engineering company TechDivision GmbH. TechDivision has
around 70 employees, working in 3 locations in Germany. They focus mainly on web development, i.e.
creating product portals and e-commerce solutions for
a variety of companies, as well as web consulting, especially focusing on search engine optimizations. Many
of their products involve customisation of Magento6
or Typo37 frameworks.
In their projects, TechDivision follows an agile software development process using either Scrum or
Kanban methodologies. For their requirements,
TechDivision applies user stories , which they write
and manage in Atlassian JIRA8. User stories at Tech-
Divison follow the common Connextra format: As a
[Role], I want [Feature], so that [Reason]. We will
also follow this terminology here.
The systems under analysis consist of two online
shopping portals, a customer-relationship system and
a content-management system, all of which we cannot
name for non-disclosure-agreement reasons. In total,
we analyzed over 1,000 user stories containing roughly
28,000 words. For RQ 2.2, we met with an experienced
Scrum Master and a long-term developer, who have
worked on several projects for TechDivision.
6 
7 
8 
Case D: University of Stuttgart. The requirements of
Case D were created by 52 groups of three 2nd-year
students each during a compulsory practical course in
the software engineering programme at the University
of Stuttgart. We removed one artifact, because it was
incorrectly encoded, thus resulting in 51 requirements
artifacts for this analysis.
Figure 6: Variation of size of requirements artifacts
in Case D in words
The resulting requirements artifacts diﬀer vastly in
style; hence, we were unable to count them in terms of
requirements, but instead only counted the structured
use cases as provided by the authors, and quantiﬁed
the artifacts by word size.
The average size of a
requirements artifact was 4,471 words (min: 1,425,
max: 8,807, see Fig. 6) and contained 19 use cases
(min: 6, max: 39), thus creating a set of artifacts of
nearly a quarter of a million words, including more
than 950 use cases.
For practical reasons, we could not evaluate each
research question in each case: For example, RQ 3
depends on the existence of reviews with documented
results, which is often not existent in practice. Furthermore, depending the answers of RQ 4 on the potentially less experienced students from Case D would
introduce a threat to the validity of our evaluation.
Table 5 shows the mapping between research questions and study objects. The interviews for RQ 2.2
and RQ 4 lasted 60 minutes for each Case A and B
and 120 minutes for Case C.
Table 5: Study objects usage in research questions
RQ 1: Distribution
RQ 2.1: Precision
RQ 2.1: Recall
RQ 2.2: Relevance
RQ 3: Defect Types
RQ 4: QA Process
A: Daimler
C: TechDivision
D: Univ. of Stuttgart
6.2.2. RQ 1: How many Requirements Smells are
present in the artifacts?
Under this research question, we quantify the number of ﬁndings that appear in requirements. Table 6
shows the number of ﬁndings for each case, each requirements artifact and each smell and also puts these
numbers in relation to the size of the artifact. We
analyzed requirements of the size of more than 250k
words, on which the smell detection produced in total
more than 11k ﬁndings, thus revealing roughly 44
ﬁndings per thousand words.
Table 6 shows that all requirements artifacts contain
ﬁndings of Requirements Smells. They vary from 5
ﬁndings for the smallest9 case (A3) up to 572 for the
largest case (C4). The number of ﬁndings strongly
correlates with the size of the artifact (see Fig. 7,
Spearman correlation of 0.9). Hence, in the remainder,
we normalize the number of ﬁndings by the size of the
The artifacts of Daimler have an average of 26 ﬁndings per thousand words, in contrast to 41 for both
9in terms of total number of words
Number of Words in Artifact
Number of Findings in Artifact
TechDivision
Figure 7: Number of ﬁndings strongly correlates with size of artifact (for readability reasons, for the Stuttgart cases (blue) only
IDs of less correlating artifacts are displayed).
Wacker and TechDivision and 43 for the artifacts produced by the students. Best to analyze the variance
within a requirements artifact seems Case D, in which
multiple teams had a similar background and project
size. Fig. 8 shows the variance between the artifacts
of Case D with an average of 44 ﬁndings, a minimum
of 26 ﬁndings (D11) and a maximum of 75 ﬁndings
(D32) per 1,000 words.
Figure 8: Number of ﬁndings per 1,000 words in Case
When inspecting the diﬀerent Requirements Smells,
we can see that the most common smells are vague
pronouns with 25 ﬁndings per 1,000 words, followed
by the negative words smell with 6 ﬁndings and
the loophole smell with 4 ﬁndings.
The least often smells are non-verifiable terms with 1 ﬁnding
per 1,000 words, and ambiguous adverbs and adjectives with 0.25 ﬁndings per 1,000 words. In fact,
the most common smell, vague pronouns, appears
100 times more often than the ambiguous adverbs
and adjectives. To analyze the variance in depth,
we again take the students’ artifacts for reference.
Fig. 9 shows the relative number of ﬁndings across
the projects.
Interpretation. We
quantitative
overview along three variables: projects, contexts and
the diﬀerent Requirements Smells.
Projects When comparing at project level, we see
that Cases A1–A6 (with outlier A5) and C1–C4
(with outlier C3) show quite similar numbers.
In contrast B1 to B3 vary between 28 and 68
ﬁndings per 1,000 words.
When looking into
the most extreme outliers B3 and D32, we see a
systematic error that creates a large number of
ﬁndings: Both projects repeatedly explain what
the system should10 do instead of what it must
do. 16 of 19 loopholes ﬁndings in B3 and 29
of 37 loophole ﬁndings in D32 root from this
problem. This can lead to diﬃcult issues in contracting as requirements that are phrased with
a should are commonly understood as optional
(see e.g. RFC2119 for a detailed explanation).
Hence, we could see a surprising consistency in
two of three industrial case studies. The Wacker
data varies, so does the students case. In both
cases, the negative extremes point at issues that
potentially have expensive consequences.
Context The four cases diﬀer strongly in their context: They write down requirements in diﬀerent forms, vary in their software development
methodology and also produce software for different domains. When comparing the ﬁndings
at the domain level, we see that Daimler artifacts with an average of 26 ﬁndings per thousand
words contain less ﬁndings than both Wacker and
TechDivision with 41 ﬁndings and the artifacts
produced by the students with 43 ﬁndings.
Our partners reported that there have been trainings for the authors of the cases A1–A6 recently,
which could explain the diﬀerence. Another reason could be the strong focus that the automotive
domain puts on requirements and requirements
quality in contrast to the other domains. Lastly,
also the strict process in this domain could be a
10Soll is a German modal verb that is less strict than an
English must.
Subjective
Superlatives
Comparatives
Non−verifiable
Amb. Adverbs
Figure 9: Variation of smells per 1,000 words in Case D
reason for this striking diﬀerence of the Daimler
requirements. Unsurprisingly, the students’ requirements form the lower end of the scale, yet
not by much.
Requirements Smells When comparing the eight
smells, we see a strong variance between the number of ﬁndings, both in absolute as well as relative
values. A qualitative inspection indicates reasons
for the most occurring smells. First, the smell
detection for vague pronouns ﬁnds all substituting pronouns in the requirements. Especially
in German, in many sentences the reference of
the pronoun can sometimes be derived from gender and grammatical case of the word, thus correctly detecting pronouns, but not vague pronouns. RQ 2.1 quantiﬁes this issue. Second, the
most common indication for loophole ﬁndings
is the aforementioned use of the word should. We
discuss this case in-depth with practitioners in
RQ 2.2. Third, we will also inspect reasons for
the high number of negative words ﬁndings in
RQ 2.1 and RQ 2.2.
Answer to RQ 1. The number of ﬁndings in requirements artifacts strongly correlates with the size of
the artifact. There are roughly 44 ﬁndings per 1,000
words and some contexts show a striking similarity
in the number of ﬁndings for their artifacts. In our
cases, the automotive requirements had a lower number of ﬁndings whereas student artifacts contained
a higher number of ﬁndings relative to the size of
the artifacts. The most common ﬁndings are for the
smells loopholes and vague pronouns.
6.2.3. RQ 2.1: How accurate is the smell detection?
To understand the capabilities of the smell detection, we need to understand precision as metric indicating how many of the detected ﬁndings are correct,
as well as recall as a metric indicating how many of
the correct ﬁndings are detected.
Precision. To understand to which extent the numbers of ﬁndings for certain smells in RQ 1 are caused
by the detection mechanism, we inspected a random
sample of 616 ﬁndings by taking equivalent sets of
ﬁndings from each project and manually classifying
whether the ﬁnding fulﬁlls the smell deﬁnition. We
could not inspect the same number of ﬁndings of each
smell for each project, because some projects only had
few or even no ﬁndings of a certain smell (see number
of ﬁndings per project in Table 6).
Table 7 and Fig. 10 show the summary of this analysis: The precision of the detection of the subjective language smell revealed only three false positives in total, thus leading to a precision of 0.96.
Non-verifiable words, loophole, and ambiguous
adverbs and adjectives smells range between 0.70
and 0.81, hence leading to roughly one mistake in four
suggestions. Comparative and superlative smells
range around 0.5 which would mean that every second ﬁnding is correct. At the rear end of the list
are the negative words and vague pronouns smells
with one correct ﬁnding in three to four suggestions.
Across all smells, the precision is between 0.48 (over all
inspections) and 0.59, if we take the varying number
of inspected ﬁndings between the smells into account.
To understand these numbers, we qualitatively inspected the false positive classiﬁcations, revealing the
following main reasons for false positives:
Grammatical errors in real world language.
The ﬁrst issue that creates false positives is the
fact that our study analyzes real world language.
Some of the requirements, especially in Case
C, contained a number of grammatical ﬂaws as
well as dialectal phrases, which lead to wrong
results in the automatic morphologic analysis
and automatic POS tagging and consequently
also to false positives during smell detection.
Vague pronouns. The smell detection for vague
pronouns showed the lowest precision. In the
detection of this smell, we look for substituting
pronouns, which are pronouns where the noun
is not repeated after the pronoun11, of which we
characterize only every fourth ﬁnding as a defect.
The reason behind this poor performance, besides a number of false positives due to the poor
grammar mentioned before, is the comparably
11E.g. The father of these. vs The father of these kids.
large number of grammatical exponents of the
German language. In addition to number and
three grammatical genders, the German language
also has four grammatical cases. Therefore, in
various instances of substituting pronouns, there
is only one grammatical possibility of what the
pronoun could refer to.
Findings in conditions. A third reason for false
positives is that the smell detection, so far, takes
very little context into account. For example, the
comparatives smell aims at detecting requirements that deﬁne properties of the system relative to other systems or circumstances12. When
searching for grammatical comparatives in requirements, roughly 48% of the cases are of the
aforementioned kind. In roughly the same number of cases, however, the comparative describes a
condition. For example, if the requirement states
that if the system takes more than 1 second to respond [. . . ], the comparison is not against another
system or circumstance but against absolute numbers. Therefore, in this case, the comparative
does not indicate a problem (one could even argue
that this is an indicator for good quality).
A similar problem holds for the negative
phrases smell: The smell detection aims at revealing statements of what the system should not
do. Often, however, the negative is mentioned
in conditions. For example, if the requirements
express what to do if the user input is not zero
[. . . ], the negation relates to a condition and not
to a property of the system.
Recall. When analyzing the accuracy of an automatic
detection, we must look not only at precision, but
also at recall, i.e. the ratio of all detected ﬁndings to
all defects of a certain type in an artifact. To this
end, we inspected one artifact of each case, in total a
12As discussed in Sect. 3.2, the problem of comparatives in
requirements is validation: How can we understand whether
a system fulﬁlls a requirements if that requirement is stated
in a relative instead of an absolute way? What if the system
in comparison changes its properties, would this render the
requirement suddenly unfulﬁlled?
set of roughly 16,200 words, and manually identiﬁed
the ﬁndings in each artifact. Due to the problems of
distinguishing the various ambiguity-related smells,
we analyzed the recall of these four smells as if it
was one smell, without further diﬀerentiation (see
Section 6.1.3).
The manual inspection revealed 200 ﬁndings in this
artifact sample and an average recall of 0.82. Table 8
and Fig. 10 show the summary of the results: The comparison shows a recall between 0.84 and 0.95 for four
of the ﬁve investigated smells. The highest recall was
achieved by the Comparative Requirements Smell,
with 0.95, which means that the smell detection missed
one in 20 ﬁndings. The ﬁfth smell, with the lowest
recall, is Superlative Requirements Smell with a
recall of 0.5. However, this smell is one of the rarest
of the smells, as one can also see in the results to RQ
1. Therefore our analysis of the recall of this smell is
based on few data points. Hence, we suggest to take
the recall of this smell with care, and suggest that
future studies should investigate this issue in more
A further analysis of the false negatives shows that
the smell detection missed ﬁndings because of imprecisions in the NLP libraries (i.e. Stanford NLP for
Lemmatization and POS Tagging and RFTagger 
for morphologic analysis). For the dictionary-based
smells, the lemmatization did not correctly deduce
the correct lemma, e.g. it did not understand that
a certain word was a plural of a lemma.
the lemmatized version of the word, i.e. the singular
form, is in the dictionary, then the smell detector
does not correctly identify the smell. In the false negative cases for the Comparative and Superlative
Requirements Smell, RFTagger did not correctly
classify the inﬂection.
Interpretation. The study revealed that the precision
strongly varies between the diﬀerent smells. Qualitative analysis provided further insights described
We can now explain the high number of ﬁndings
for vague pronouns in RQ 1. If we assume that a
quarter of the ﬁndings are correct, the number of
ﬁndings in this category is closer to the remaining
smells. Also, we could see that while there are certain
reasons of impreciseness that root from the study
objects themselves and are, thus, unavoidable, there
is plenty of space for optimization. First, existing
techniques from NLP could be applied to improve
certain smells, such as the vague pronouns. Second,
from the examples we have seen, we would argue that
the application of heuristics could heavily improve the
precision of existing smell detection techniques. For
example, if we exploit the information available from
POS tagging, we can ﬁnd out whether a comparison
refers to a number or numerical expression.
Regarding recall, our analysis shows only a slight
variance between the smells, with the only outlier
being the Superlative Requirements Smell; however, since this is a very rare smell, this recall is based
on only few data points, therefore, we must consider
this result with care. When inspecting the reasons
for false negatives, we found that optimizations could
be made through the lemmatizer. Future research in
this direction should compare whether the accuracy of
lemmatizers as reported in the ﬁeld of computational
linguistics also holds for requirements engineering artifacts. Furthermore, we analyzed requirements in
German language where lemmatization is a more dif-
ﬁcult problem than in English, since the language
makes stronger use of inﬂections (e.g. with cases or
gender). Hence, smell detectors based on lemmatization for the English language might work better than
the results indicate in our analysis.
In general, the precision and recall are therefore
comparable to other approaches with related purposes
(see Sect. 2). However, is it suﬃcient for an application
of Requirements Smells in practice?
First, when looking at precision, we must take into
account that the current state of practice consists
still of manual work and that the cost for running
an automatic analysis is virtually zero. Nevertheless,
checking a false positive ﬁnding takes eﬀort which an
inspector could rather spend in reading the document
in more detail. However, as we see a high variation in
the precision over diﬀerent smells, we need to discuss
these separately. Several of the smells have a precision of 0.7 and higher which is considered acceptable
in static code analysis . For other Requirements
Smells, the precision is below 0.5. This means that
every other ﬁnding will be a false positive. This can
be critical in the eﬀort spent in vain and annoy a
user of the smell detection. Yet, we follow Menzies et
al. that a low precision can be still useful “When
there is little or no cost in checking false alarms.” In
our experience, the cost of checking a ﬁnding is often
just a few seconds.
Second, when looking at recall, most of the smell
detections reach a recall of more than 80%. Various
publications, most prominently Kiyavitskaya and
Berry et al. , argue that a recall close to 100% is a
basic requirement for any tool for automatic QA in
RE. The core argument is that with a lower recall, reviewers stop checking these aspects and consequently
miss defects, and that reviewers need to check the
complete artifact anyway. However, if taking the example of spell checkers and grammar checks, these are
still used on a daily basis, although they are far away
from 100% recall. Therefore, one could consequently
also argue that the precision is more important than
the recall.
In any case, whether the reported precision and recall are suﬃcient in industry needs further research in
the future. As mentioned above, it mainly depends on
two factors: the required investment versus the gained
beneﬁt (similar to the concept of technical debt). For
the required investment, we argue that, based on our
experience of analyzing the various cases presented
here, one can quickly iterate through the detected
ﬁndings with low investment. To further support this
discussion, the following research question analyzes
the aspect of the beneﬁts to practitioners in more
Answer to RQ 2.1. As shown in Tables 7 and 8, and
as shown in Fig. 10, the precision is on average around
59%, with an average recall of 82%, but both vary
between smells. We consider this reasonable for a task
that is usually performed manually. However, this also
depends on the relevance of ﬁndings to practitioners,
which we analyze in RQ 2.2. The study also reveals
improvements for future work through the application
of deeper NLP.
Subjective
Adjectives
verifiable
Superlative
Requirements
Comparative
Requirements
Figure 10: Precision and recall of the discussed smell detection approaches.
Table 8: Recall of smell detection within sample of 4
artifacts (16,271 words)
Findings in artifacts
Findings identiﬁed correctly
Ambiguity-related S.
Superlative Requirements S.
Comparative Requirements S.
Negative Words S.
Vague Pronouns S.
6.2.4. RQ 2.2: Which of these smells are practically
relevant in which context?
To understand whether the Requirements Smells
help detecting relevant problems, we ﬁrst performed
a pre-study, in which we confronted practitioners of
Daimler and Wacker with ﬁndings. The pre-study,
which we reported in Femmer et al. , aimed at
receiving qualitative and tacit feedback. It showed
that Requirements Smells can in fact indicate relevant
In contrast, in this study we analyze relevance in
speciﬁc categories by interviewing practitioners at
TechDivision on their opinion on the ﬁndings in terms
of relevance, awareness, and whether these practitioners would resolve the suggested ﬁnding.
Quantitative observations. Table 9 reports the 20 ﬁndings that we discussed with TechDivision. In summary,
we can see that they considered 65% of the ﬁndings
as relevant for their context. Furthermore, they have
not been aware of 45% of the ﬁndings. Lastly, they
would act on 50% of the presented ﬁndings and on
40% even immediately.
Qualitative observations (true positives). The ﬁndings that the tool produces mostly constituted forms
of underspeciﬁcation. For example, in Finding #1 (see
Table 9): "As a searcher, I want to see the checkboxes
in the diﬀerent categories displayed more clearly, so
that. . . " (for similar examples, see Findings 3, 4, 14,
16, and 20). In this case, as in many of the other examples, the practitioners stated that no developer could
implement this story properly. They also recalled
various discussions in estimation meetings on what
was to be done to complete these types of stories13.
In the previous research questions, we have seen that
Requirements Smells are able to detect loopholes in
requirements, such as the usage of the word should. To
understand the relevance of this ﬁnding in the context
of an agile company, we also discussed the loophole
in Finding #6. When we pointed out the ﬁnding, they
responded that they considered expressing what the
system should do in user stories problematic. They
considered this defect a low risk, as the developers
understood ("If you are told that you should take out
the trash, you understand that it is an imperative.")
and their user stories did never turn out to be of legal
relevance. They concluded that they want to avoid
this, but it has no immediate urgency in a project
situation.
ISO 29148 discusses the use of negative statements ("capabilities not to be provided"). In a previous study practitioners expressed their reluctance
of this criterion. In contrast, in this study, practitioners said they would act upon 2 out of 3 of the negative
statements (Findings #9–11) that we presented to
them as they revealed unclear requirements. In one
case they even remembered that this led to discussions
about the implementation during the sprint. Table 9
shows many more, similar examples.
Qualitative observations (false positives). Also interesting are those cases that practitioners considered not
13Note that discussions can have diﬀerent objectives, i.e. what
is to be implemented and how. For these, how to implement a
story is the team’s task and thus discussions can help ﬁnding
the best way. In contrast, what the product owner wants is
outside of the team’s scope and therefore should not be a matter
of discussion.
relevant in their context or where practitioners said
they would not act upon. Summarized, the reasons
were the following:
Domain and context knowledge: Some
that were unclear to outsiders were understandable for someone knowing the system under
consideration. For example, in user story #18 it
was unclear to the ﬁrst and second author what
their refers to. It was clear, however, to both
practitioners with knowledge about the system.
Process requirement: In Finding #8, the smell reveals another conspicuous ﬁnding: The developer
should put as low eﬀort as possible into the implementation of this story. In the discussion, the
reason for this was that the customer did not
want to pay much for this implementation. Thus
the story should only be fulﬁlled if it was possible
to be fulﬁlled cheaply. While the practitioners
told us they would not change anything about
this story, they agreed that the smell pointed
out something that violates common user story
Finding in reason part: In four cases, the practitioners agreed to the ﬁnding but considered it
irrelevant as the ﬁnding was inside the reason
part of the user story. This is due to this part of
the user story only serving as additional information. This reason part is not used in testing nor
is the information directly relevant for implementation. The main purpose is to understand the
business value and to indicate the major goal to
the team, similar to goals and goal modeling in
traditional requirements engineering .
Answer to RQ 2.2. In summary, the practitioners
expressed that 65% of the discussed ﬁndings were
relevant, as they lead to lengthy discussions and unnecessary iterations in estimation. They also saw the
problem of legal binding, but in contrast to the practitioners of Case A and B, they considered these ﬁndings less relevant. Due to these results, they expressed
their strong interest in exploring smell detection for
projects; we will explain the results of this discussion
Further observations of quality defects in diﬀerent
parts of a user story
We considered especially the last explanation for rejecting ﬁndings (ﬁnding in reason part of a user story)
particularly interesting. We had noticed that the reason part was often written in a rather imprecise way.
To be able to quantify this aspect, we automatically
split user stories according to the language patterns
and quantiﬁed the distribution of words as well as
ﬁndings over the diﬀerent parts of user stories.
Table 10 shows the results of this analysis. The
number of words is roughly distributed as follows: 11%
of the words of a user story describe the role, 55% of
the words describe the feature and 34% describe the
reason. Of the 1,082 user stories, 290 had no reason
part at all. Due to this uneven distribution, similar as
in the previous analyses, we normalize the number of
ﬁndings by the number of words in each part resulting
in the number of ﬁndings per 1,000 words.
Only 1% of the ﬁndings are located in the role
part. In fact, when we inspected these ﬁndings, they
were false positives due to the grammatical problems
described in the previous section. The absence of
ﬁndings in this section is expected, as this part of
the user story only names the role and does not oﬀer
many chances for smells as described in Sect. 3.2. For
the remainder, 46% of the ﬁndings are located in the
feature and 53% are located in the reason part. In
relation to its size, the diﬀerence is striking: With 64
ﬁndings per 1,000 words, the reason has nearly double
the number of ﬁndings of the feature part and nearly
70% more ﬁndings than the average requirement, as
analyzed in Sect. 6.2.2.
In summary, the reason part of user stories is particularly prone to smells, but the qualitative analysis
in RQ 2.2 reveals that practitioners consider ﬁndings
in this section to be less relevant. This investigation
could support further application of Requirements
Smells in practice by helping to prioritize smells according to their location.
6.2.5. RQ 3: Which requirements quality defects can
be detected with smells?
For 44 of the 51 requirements artifacts the students provided technical reviews. We qualitatively
analyzed the results of 10 randomly selected reviews
(around 20%). The inspected reviews were conducted
by 5–7 reviewers (mean: 5.6), took 90 minutes and
resulted in 18–69 defects (mean: 38.1). We iterated
through the 381 defects documented in the reviews
and evaluated whether the smell detection produced
ﬁndings indicating these defects. If no smell indicated
the defect, we openly classiﬁed the defects. We did
not quantify these results, because the resulting numbers would assume and suggest that the distribution
of defects is representative for regular projects, which
we are unsure about (i.e. because of a high number of
spelling and grammatical issues).
The classiﬁcation of the defects and their comparison with the detected smells resulted in the following
list of of defects indicated by Requirements Smells:
Sentence not understandable. In some instances,
when the defect suggested changing the sentence
to improve understandability, these sentences
were highlighted especially by the vague pronouns and negative statements smells.
Improper legal binding. Various requirements artifacts had issues with improper legal binding.
In one case, the reviewers recognized this and
demanded the use of the term must. The loopholes smell pinpointed at this issue.
Unspeciﬁed/unmeasurable NFRs. Various
especially
superlatives smell,
indicated at defects of underspeciﬁcation within
non-functional requirements.
The remaining defects were not indicated by Requirements Smells.
Interpretation. The quantitative distribution of defects is not necessarily representative for industry
projects and, thus, has not been not analyzed. The reviews clearly show that manual inspection discovered
the same defects as in the previous research question:
Understandability, legally binding terminology and
underspeciﬁed requirements. These are issues with regards to representation but also the content described
in the artifact. We argue that these issues are common for requirements artifacts. Requirements Smells
can therefore indicate relevant defects from multiple,
independent sources (manual inspection, interviews
with practitioners, independent manual reviews) for
multiple, independent cases.
Answer to RQ 3. Automatic smell detection can point
to issues in both representation (e.g. improper legal
binding) and content (underspeciﬁed/unmeasurable
NFRs). The analysis of the reported defects indicates
that more defects could be automatically detected
(see section further discussion on detectability of defects described next). Nevertheless, just as for static
code analysis, we see that automatic analysis can not
indicate all defects and thus must be accompanied by
reviews . The fourth research question aims at
analyzing this aspect in depth.
Further discussion on detectability of defects. During
the analysis, if no smells indicated the defect, we
openly classiﬁed the defects. While discussing the
resulting list of defects and the degree to which they
are detectable within the group of authors, we came
up with a classiﬁcation which is broader as initially
planned while designing the study. This classiﬁcation
considers whether a defect:
• Already can be detected
• Could be detected, but is not implemented yet
in our detection
• Cannot be detected at the moment, but should be
• Cannot be detected at all and probably won’t be
This classiﬁcation is purely based on our knowledge
of existing related work and our subjective expectations gained during the data analysis process. The
classiﬁcation yielded in a map visualised in Fig. 11.
The ﬁgure is structured in two dimensions: On the
vertical axis, we group the defects into defects relating
to the content, and defects relating to representation.
Furthermore, on the horizontal axis, we map the items
according to the expected precision and completeness
we believe the detection could be (i.e. the classiﬁcation above). The further left an item, the more
precise and complete we expect a smell detection to
be; the items on the right we assume to be close to
impossible to detect in a general case.
by&Requirements&Smells
Sentence not understandable
Unspeciﬁed/unmeasurable NFRs
Improper legal binding
Incorrect information
Unintuitive Use Case ﬂow or diagrams
Language'seman*cs
Semantic clones
Missing mandatory items
Terminology
Presenta*on'and'Structure
Representa5on
Singularity in UC
Unnatural itemizations
Unintuitive structure of table
Unappealing image
Unreadable image
Underspeciﬁed terms
Unnecessary terms in glossary
Naming violating convention
Undeﬁned domain-speciﬁc terms
Inconsistent usage of terms
Incomplete information
Language mixture
Wrong word (language)
Semantically contradicting information
Detectable
by&Requirements&Smells
Rather&not&detectable
by&Requirements&Smells
Wrong word (domain)
Structural redundancy / Cloning
Structurally inconsistent diagrams
Figure 11: Findings in requirements reviews, classiﬁed by content/representation and detection
With the defects that our current approach does not
reveal, this research question shows that more defects
could be detected: These are namely defects with
terminology, singularity in use cases and structural
issues focusing on the content such as the absence
of mandatory elements in the artifact , structural
redundancy or structural inconsistency between
content. It remains unclear how far more enhanced
language analysis with more sophisticated NLP and
ontologies can enable to understand language. In any
case, when a defect remains subtle and vague in its
deﬁnition, such as an unintuitive structuring or design,
we only see potential for automation if a defect can
be deﬁned precisely. For problems relating to the
domain itself (e.g. incomplete information about the
domain or incorrect information with regards to the
domain), we consider it impossible to detect issues
unless formalizing the concepts of the domain.
6.2.6. RQ 4: How could smells help in the QA process?
After the interviews and analysis, we asked all involved practitioners whether or not they think requirements smell detection is a helpful support, and
whether and how they would integrate it in their
context. We asked those questions openly and transcribed the answers for validation by the interviewees
and later coding. In the following, we report on the
results structured by topics. Where applicable, we
provide the verbatim answers in relation to their cases
(A, B or C).
Overall Evaluation. In general,
all practitioners
agreed on the usefulness of the smell detection even if
considering diﬀerent perspectives that arise from their
process setting. One practitioner (Case C) reports
that he expects one beneﬁt in using smell detection is
that it would lead to a reduction of the time spent for
eﬀort estimations (in context of agile methods), as the
product owner could beneﬁt from the smell detection
on the ﬂy and, thus, avoid misinterpretations later.
Quotes on Overall Evaluation
“I think that smells can help to analyze a
speciﬁcation.”
“The method of Requirements Smells is a
valuable extension in the area of requirements engineering and gives helpful input
concerning the quality of speciﬁed requirements in early development phases.”
“I think such a smell detection is of high
value to make sure that our team is confronted with already quality assured [user]
stories. This can reduce the time in our effort estimations, because the product owner
would directly notice on the ﬂy what could
lead to misinterpretations later.”
Integration into Process. When asked for how the
practitioners would integrate the smell detection into
their process setting, we got varying answers depending on the process. The practitioner relying more
on rich process models (Case B) could imagine using
a smell detection either as a support for the person
writing the requirements or as part of a more fundamental QA method for the company. But also the
practitioner relying more on the agile methods (Case
C) could imagine using Requirements Smells as a support for the person writing the requirements or in
context of analytical QA. In addition, one potential
use is seen in context of problem management. Importantly, all practitioners see the full potential of a
smell detection only if integrated in their existing tool
chain (see also quotes on constraints and limitations).
Quotes on Integration into Process
“I like to compare Requirements Smells to
the “check spelling aid" known e.g. from
Microsoft Word. So for me Requirements
Smells are intuitive and lightweight and
should be used and integrated within requirements engineering and quality assurance
processes.”
“As a product owner, I would use a smell
detection on the ﬂy [...]. In addition, smell
detection could help in analytical QA, as it
could reveal when a problem occurs repeatedly, either in a project or in the company
as a whole.”
Constraints and Limitations. One facet we consider
especially interesting when using qualitative data is
the chance to reveal further ﬁelds of improvement.
We therefore concentrate now on the constraints that
would hamper the usage of a smell detection. One
facet we believe to be important is that practitioners
want to avoid additional eﬀort when using smell detection in their context. Furthermore, the practitioner
of Case A believes that the automatic smell detection
requires a common understanding on the notion of RE
quality. He further indicates that the smell detection
should explicitly take into account that some criteria
cannot be met at every stage of a project.
Quotes on Constraints and Limitations
“First, the people who need to write the
speciﬁcation received training which gives
the required performance criteria. Second,
abstraction levels must be taken into account during the smell detection process,
since at higher abstraction levels diﬀerent
criteria cannot be met (e.g. vague pronouns
or subjective language).”
“As a product owner, I would use a smell
detection on the ﬂy provided that it would
not mean additional eﬀort [such as by having to use another tool].”
Answer to RQ 4. Our practitioners provided a general
agreement on potential beneﬁts of using smell detection a quality assurance context. When asked how
they would integrate the requirements smell detection,
they see possibility for both analytical and constructive QA, provided, however, this integration would
not increase the required eﬀort, e.g. by integrating
the detection into existing tool chains.
6.2.7. Evaluation of validity
We use the structure of threats to validity from 
to discuss the evaluation of the validity of our study.
Construct validity. In our evaluation, we analyzed
Requirements Smells in the terms of false positives,
relevance and relation to quality defects. There are
threats that the understanding of these terms varies
and, thus, the results are not repeatable. Yet, we are
conﬁdent that our validity procedures described in
Sect. 6.1.5 reduced this threat. For the false positives,
we classiﬁed a subset of the ﬁndings independently,
and afterwards compared (inter-rater agreement Cohen’s kappa: 0.53) and discussed the results.
subsequently reclassiﬁed a diﬀerent subset of ﬁndings
again, which lead to an inter-rater agreement (Cohen’s
kappa) of 0.72. For the classiﬁcation of false negatives,
we reclassiﬁed one document separately, calculating
the percentage of agreement on false positives14. This
lead to an agreement of 88%.
We consider both of these substantial agreements,
especially in the inherently ambiguous and complex
domain of RE. Thus, we consider this threat as suﬃciently controlled.
Internal validity. A threat to the internal validity of
our results is that the experience of the students as
well as the practitioners might play a role in their ratings of relevance or detection of quality defects. We
mitigated this threat by choosing only practitioners
for the ratings and interviews who had several years
of experience. The students are only in the second
year. We cannot mitigate this threat but consider the
eﬀect to be small. There might be some defects not
found by the students that could have been indicated
by a smell as well as unfound defects undetectable
by smells. Hence, future studies will add to the classiﬁcation but are unlikely to change it substantially.
Personal pride could potentially have an impact on
the answers to a RQ 2.2, if practitioners are not able
to professionally discuss their own work products. In
our cases, however, all practitioners openly accepted
the discussions (as can be seen in their answers). Even
though we carefully supervised this threat, we have
not found signs of personal bias in the cases involved.
Finally, the students might also have been inﬂuenced
by the review guidelines we provided. Yet, none of
the investigated smells was explicitly listed in the
14We did not employ Cohen’s kappa here, since the number
of true positives (non-smell words) would strongly dominate the
result and therefore skew the inter-rater agreement. Instead,
we calculated the ratio of ﬁndings which both rating teams
independently classiﬁed as false positive to the number of
ﬁndings which only one of the teams classiﬁed false positive.
guidelines. Instead, the guideline contained rather
high-level aspects such as “unambiguity”. Although
we consider this threat to be a minor one, it is still
External validity. As requirements engineering is a
diverse ﬁeld, the main threat to the external validity
of our results is that we do not cover all domains
and ways of specifying requirements. We mitigated
this threat to some degree by covering at least several
diﬀerent domains and study objects, of which some are
purely textual requirements artifacts, some use cases,
and some user stories. We argue that this represents
a large share of today’s requirements practices.
Reliability. Our study contains several classiﬁcations
and ratings performed by people. This constitutes a
threat to the reliability of our results. We are conﬁdent, however, that the peer debrieﬁng and member
checking procedures helped to reduce this threat.
7. Conclusion
In this paper, we deﬁned Requirements Smells and
presented an approach to the detection of Requirements Smells which we empirically evaluated in a
multi-case study.
In the following, we summarize
our conclusions, relate it to existing evidence on the
detection of natural language quality defects in requirements artifacts, and we discuss the impact and
limitations of our approach and its evaluation. We
close with outlining future work.
7.1. Summary of conclusions
First, we proposed a light-weight approach to detect Requirements Smells. It is based on the natural
language criteria of ISO 29148 and serves to rapidly
detect Requirements Smells. We deﬁne the term Requirement Smell as an indicator of a quality violation,
which may lead to a defect, with a concrete location and a detection mechanism, and we also give
deﬁnitions of a concrete set of smells.
Second, we developed an implementation that is
able to detect Requirements Smells by using part-ofspeech (POS) tagging, morphological analysis and
dictionaries. We found that it is possible to provide
such tool support and outlined how such a tool could
be integrated into quality assurance.
Third, in the empirical evaluation, our approach
showed to support us in automatically analysing requirements of the size of 250k words. Findings were
present throughout all cases but in varying frequencies
between 22 and 67 ﬁndings per 1,000 words. Outliers
indicated serious issues. An investigation of the detection precision showed an average precision around
0.59 over all smells, again varying between 0.26 and
0.96. The recall was on average 0.82, but also varied
between 0.5 and 0.95. To improve the accuracy, we
described concrete improvement potential based on
real world, practical examples.
A further analysis of reviews and practitioner’s opinions strengthen our conﬁdence that smells indicate
quality defects in requirements.
For these quality
defects, practitioners explicitly stated the negative
impact of discovered ﬁndings on estimation and implementation in projects.
The study also showed,
however, that while Requirements Smell detection
can help during QA presumedly in a broad spectrum
of methodologies followed (including agile ones), the
relevance of Requirements Smells varies between cases.
Hence, it is necessary to tailor the detection to the
context of a project or company. We analyzed this
factor in depth, demonstrating that the reason part of
a user story contains most ﬁndings (absolutely and relatively), but practitioners consider these ﬁndings less
relevant as they argue that this part is not commonly
used in implementation or testing. This raises the
question of the relevance of this part at all, at least
from a quality assurance perspective, which should
be investigated in future work.
Our comparison with defects found in reviews furthermore showed that the Requirements Smell detection partly overlaps with results from reviews. As a
result, we provide a map of defects in requirements
artifacts in which we give a ﬁrst indication where
Requirements Smells can provide support and where
they cannot.
Therefore, we provide empirical evidence from multiple, independent sources (manual inspection, interviews with practitioners, independent manual reviews)
for multiple, independent cases, showing that Requirements Smells can indicate relevant defects across dif-
ferent forms of requirements, diﬀerent domains, and
diﬀerent methodologies followed.
7.2. Relation to existing evidence
Existing approaches in the direction of automatic
QA for RE are based on various quality models, including the ambiguity handbook by Berry et al. ,
the now superseeded IEEE 830 standard and proprietary models. Yet, according to a recent literature
review by Schneider and Berenbach , ISO 29148
is the current standard in RE “that every requirements engineer should be familiar with”. However,
no detailed empirical studies (see Table 1) exist for
the quality violations described in ISO 29148. When
comparing to similar, related quality violations, also
few empirical, industrial case studies exist (see Table 2). Gleich et al. and Chantree et al. report
for conceptually similar problems, a precision of the
detection between 34% and 75% (97% in a special
case), and a recall between 2% and 86%. Krisch and
Houdek report a lower precision in an industrial
setting. The precision and recall for the detection of
the smells, which we developed based on the description in the standard, are in a similar range to the
aforementioned. In summary, this work provides a
detailed empirical evaluation on the quality factors of
ISO 29148, including a deeper understanding of both
existing and novel factors.
We also take a ﬁrst step from the opposite perspective: So far, to all our knowledge, all related work
starts from a certain quality model and goes into
automation. Our results to RQ 3 provides a bigger
picture for understanding in how far quality defects in
requirements could be addressed through automatic
analysis in general.
Our results to RQ 2.2 furthermore provides evidence for the claim by Gervasi and Nuseibeh that
“Lightweight validation can discover subtle errors in
requirements.”
More precisely, our work indicates
that automatic analysis can ﬁnd a set of relevant defects in requirements artifacts by providing evidence
from multiple case studies in various domains and
approaches. The responses by practitioners to the
ﬁndings do, to some extent, contradict the claim by
Kiyavitskaya et al. who state that “any tool [...]
should have 100% recall”. Practitioners responded
very positively on our ﬁrst prototype and the smells
it ﬁnds. Yet, obviously, more detailed and broader
evaluations, especially conducted independently by
other researchers not involved in the development of
Smella, should follow.
7.3. Impact/Implications
For practitioners, Requirements Smells provide a
way to ﬁnd certain issues in a requirements artifact
without expensive review cycles. We see three main
beneﬁts of this approach: First, the approach, just
as static analysis for code, can enable project leads
to keep a basic hygiene for their requirements artifacts. Second, the review team can avoid discussing
obvious issues and focus on the important, diﬃcult,
domain-speciﬁc aspects in the review itself. Third,
the requirements engineers receive a tool for immediate feedback, which can help them to increase their
awareness for certain quality aspects and establish
common guidelines for requirements artifacts.
Yet, the low precision for some of the smells might
cause unnecessary work checking and rejecting ﬁndings from the automatic smell detection. Hence, at
least for now, it is advisable to concentrate on the
highly accurate smells.
For researchers, this work sharpens the term Requirements Smell by providing a deﬁnition and a taxonomy. By implementing and rating concrete smell
ﬁndings, we also came to the conclusion, however, that
not all of the requirements defects from ISO/IEC/-
IEEE 29148 can be clearly distinguished as Requirements Smells. In particular, the diﬀerence between
Subjective Language, Ambiguous Adverbs and Adjectives, Non-veriﬁable Terms, and Loopholes was not always clear to us during our investigations (see RQ 2.1).
Therefore, we, as a community, can take our smell
taxonomy as a starting point, but we also need to
critically reﬂect on some smells to further reﬁne the
Finally, empirical evidence in RE is, in general, dif-
ﬁcult to obtain because many concepts depend on
subjectivity . One issue increasing the level of
diﬃculty in evidence-based research in RE remains
that most requirements speciﬁcations are written in
natural language. Therefore, they do not lend themselves for automated analyses. Requirements Smell
detection provides us with a means to quantify the
extent of certain defects in a large sample of requirements artifacts while explicitly taking into account
the sensitivity of ﬁndings to their context. Hence,
this allows us to consider a whole new spectrum of
questions worth studying in an empirical manner.
7.4. Limitations
We concentrated on a ﬁrst set of concrete Requirements Smells based on our interpretation of the sometimes imprecise language criteria of ISO/IEC/IEEE
29148. There are more smells, also with diﬀerent characteristics than the ones we proposed and analyzed.
In addition, even though we diversiﬁed our study objects over domains, methods and diﬀerent types of
requirements, we cannot generalize our ﬁndings to
all applicable contexts. We therefore consider the
presented results only a ﬁrst step towards the continuous application of Requirements Smells in software
engineering projects.
7.5. Future work
Our work focuses on Requirements Smells based
on ISO/IEC/IEEE 29148. Future work needs to clarify and extend this taxonomy based on related work
and experience in practice. This also includes the
development of other Requirements Smell detection
techniques to increase our understanding about which
defects can be revealed by Requirements Smells and
which defects cannot.
Second, this ﬁrst study gained ﬁrst insights into the
usefulness of Requirements Smells for QA. We furthermore sketched an integration of Requirements Smells
into a QA process. Yet, a full integration and the consequences must be analyzed in depth. In particular,
we need to understand whether smell detection as a
supporting tool, similar to spell checking, as pointed
out by on of our participants, enables requirements
engineers to improve their requirements artifacts.
Lastly, Requirements Smells focus on the detection
of issues in requirements artifacts. They require a
thorough understanding of the impact of a quality
defect, which is hence also part of the requirements
smell taxonomy. This link must be carefully evaluated
and analyzed in practice. Our preliminary works on
this topic provide ﬁrst ideas in that direction.
Acknowledgments
We would like to thank Elmar Juergens, Michael
Klose, Ilona Zimmer, Joerg Zimmer, Heike Frank,
Jonas Eckhardt as well as the software engineering
students of Stuttgart University for their support
during the case studies and feedback on earlier drafts
of this paper.
This work was performed within the project Q-
Eﬀekt; it was partially funded by the German Federal
Ministry of Education and Research (BMBF) under
grant no. 01IS15003 A-B. The authors assume responsibility for the content.
Bibliography