Generative Adversarial Network in Medical Imaging: A Review
Xin Yia,∗, Ekta Waliaa,b, Paul Babyna
aDepartment of Medical Imaging, University of Saskatchewan, 103 Hospital Dr, Saskatoon, SK, S7N 0W8 Canada
bPhilips Canada, 281 Hillmount Road, Markham, Ontario, ON L6C 2S3, Canada
Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data
generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides
a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful
in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted
researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as
image reconstruction, segmentation, detection, classiﬁcation, and cross-modality synthesis. Based on our observations, this trend
will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme
with the hope of beneﬁting researchers interested in this technique.
Keywords: Deep learning, Generative adversarial network, Generative model, Medical imaging, Review
1. Introduction
With the resurgence of deep learning in computer vision
starting from 2012 , the adoption of
deep learning methods in medical imaging has increased dramatically. It is estimated that there were over 400 papers published in 2016 and 2017 in major medical imaging related conference venues and journals . The wide
adoption of deep learning in the medical imaging community
is due to its demonstrated potential to complement image interpretation and augment image representation and classiﬁcation.
In this article, we focus on one of the most interesting recent
breakthroughs in the ﬁeld of deep learning - generative adversarial networks (GANs) - and their potential applications in the
ﬁeld of medical imaging.
GANs are a special type of neural network model where
two networks are trained simultaneously, with one focused on
image generation and the other centered on discrimination. The
adversarial training scheme has gained attention in both academia
and industry due to its usefulness in counteracting domain shift,
and eﬀectiveness in generating new image samples. This model
has achieved state-of-the-art performance in many image generation tasks, including text-to-image synthesis ,
super-resolution , and image-to-image translation .
Unlike deep learning which has its roots traced back to the
1980s , the concept of adversarial training is relatively new with signiﬁcant recent progress . This paper presents a general overview of
∗Corresponding author
Email addresses: (Xin Yi),
 (Ekta Walia), Paul.Babyn
@saskhealthauthority.ca (Paul Babyn)
GANs, describes their promising applications in medical imaging, and identiﬁes some remaining challenges that need to be
solved to enable their successful application in other medical
imaging related tasks.
To present a comprehensive overview of all relevant works
on GANs in medical imaging, we searched databases including
PubMed, arXiv, proceedings of the International Conference on
Medical Image Computing and Computer Assisted Intervention
(MICCAI), SPIE Medical Imaging, IEEE International Symposium on Biomedical Imaging (ISBI), and International conference on Medical Imaging with Deep Learning (MIDL). We also
incorporated cross referenced works not identiﬁed in the above
search process. Since there are research publications coming
out every month, without losing generality, we set the cut oﬀ
time of the search as January 1st, 2019. Works on arXiv that
report only preliminary results are excluded from this review.
Descriptive statistics of these papers based on task, imaging
modality and year can be found in Figure 1.
The remainder of the paper is structured as follows. We
begin with a brief introduction of the principles of GANs and
some of its structural variants in Section 2. It is followed by
a comprehensive review of medical image analysis tasks using
GANs in Section 3 including but not limited to the ﬁelds of radiology, histopathology and dermatology. We categorize all the
works according to canonical tasks: reconstruction, image synthesis, segmentation, classiﬁcation, detection, registration, and
others. Section 4 summarizes the review and discusses prospective applications and identiﬁes open challenges.
 
September 5, 2019
 
Reconstruction
Segmentation
Classiﬁcation
Registration
Proportion of publications (%)
Histopathology
Retinal Fundus Imaging
Ultrasound
Dermoscopy
Proportion of publications (%)
Number of publications
Figure 1: (a) Categorization of GAN related papers according to canonical tasks. (b) Categorization of GAN related papers according to imaging modality. (c)
Number of GAN related papers published from 2014. Note that some works performed various tasks and conducted evaluation on datasets with diﬀerent modalities.
We counted these works multiple times in plotting these graphs. Works related to cross domain image transfer were counted based on the source domain. The
statistics presented in ﬁgure (a) and (b) are based on papers published on or before January 1st, 2019.
real or fake
xg, xr ∈Rc×w×h
y1 ∈{0, 1}
Figure 2: Schematic view of the vanilla GAN for synthesis of lung nodule on
CT images. Top of the ﬁgure shows the network conﬁguration. The part below
shows the input, output and the internal feature representations of the generator
G and discriminator D. G transforms a sample z from p(z) into a generated
nodule xg. D is a binary classiﬁer that diﬀerentiates the generated and real
images of lung nodule formed by xg and xr respectively.
2. Background
2.1. Vanilla GAN
The vanilla GAN is a generative
model that was designed for directly drawing samples from the
desired data distribution without the need to explicitly model
the underlying probability density function. It consists of two
neural networks: the generator G and the discriminator D. The
input to G, z is pure random noise sampled from a prior distribution p(z), which is commonly chosen to be a Gaussian or a
uniform distribution for simplicity. The output of G, xg is expected to have visual similarity with the real sample xr that is
drawn from the real data distribution pr(x). We denote the nonlinear mapping function learned by G parametrized by θg as
xg = G(z; θg). The input to D is either a real or generated sample. The output of D, y1 is a single value indicating the probability of the input being a real or fake sample. The mapping
learned by D parametrized by θd is denoted as y1 = D(x; θd).
The generated samples form a distribution pg(x) which is desired to be an approximation of pr(x) after successful training.
The top of Figure 2 shows an illustration of a vanilla GAN’s
conﬁguration. G in this example is generating a 2D CT slice
depicting a lung nodule.
D’s objective is to diﬀerentiate these two groups of images
whereas the generator G is trained to confuse the discriminator D as much as possible. Intuitively, G could be viewed as a
forger trying to produce some quality counterfeit material, and
D could be regarded as the police oﬃcer trying to detect the
forged items. In an alternative view, we can perceive G as receiving a reward signal from D depending upon whether the
generated data is accurate or not. The gradient information is
back propagated from D to G, so G adapts its parameters in order to produce an output image that can fool D. The training
objectives of D and G can be expressed mathematically as:
D Exr∼pr(x)
 log D(xr) + Exg∼pg(x)
 log(1 −D(xg)),
G Exg∼pg(x)
 log(1 −D(xg)).
As can be seen, D is simply a binary classiﬁer with a maximum log likelihood objective. If the discriminator D is trained
to optimality before the next generator G updates, then minimizing LGAN
is proven to be equivalent to minimizing the Jensen–
Shannon (JS) divergence between pr(x) and pg(x) . The desired outcome after training is that samples formed by xg should approximate the real data distribution
2.2. Challenges in optimizing GANs
The above GAN training objective is regarded as a saddle
point optimization problem and the training is often accomplished by gradient-based methods. G and
D are trained alternately from scratch so that they may evolve
together. However, there is no guarantee of balance between
the training of G and D with the JS divergence. As a consequence, one network may inevitably be more powerful than the
other, which in most cases is D. When D becomes too strong
as opposed to G, the generated samples become too easy to be
separated from real ones, thus reaching a stage where gradients
from D approach zero, providing no guidance for further training of G. This happens more frequently when generating high
resolution images due to the diﬃculty of generating meaningful
high frequency details.
Another problem commonly faced in training GANs is mode
collapse, which, as the name indicates, is a case when the distribution pg(x) learned by G focuses on a few limited modes of
the data distribution pr(x). Hence instead of producing diverse
images, it generates a limited set of samples.
2.3. Variants of GANs
2.3.1. Varying objective of D
In order to stabilize training and also to avoid mode collapse, diﬀerent losses for D have been proposed, such as fdivergence (f-GAN) , least-square (LS-
GAN) , hinge loss , and
Wasserstein distance (WGAN, WGAN-GP) . Among these, Wasserstein distance is arguably the most popular metric. As an alternative to
the real/fake discrimination scheme, Springenberg proposed an entropy based objective where real data is encouraged to make conﬁdent class predictions (CatGAN, Figure 3
b). In EBGAN and BEGAN (Figure 3 c), the commonly used encoder architecture for
discriminator is replaced with an autoencoder architecture. D’s
objective then becomes matching autoencoder loss distribution
rather than data distribution.
GANs themselves lack the mechanism of inferencing the
underlying latent vector that is likely to encode the input. Therefore, in ALI and BiGAN (Figure 3 d), a separate encoder network is incorporated. D’s objective then becomes separating joint samples
(xg, zg) and (xr, zr). In InfoGAN (Figure 3 e), the discriminator outputs the latent vector that encodes part of the semantic features of the generated image. The discriminator maximizes the mutual information between the generated image and
the latent attribute vector the generated image is conditioned
upon. After successful training, InfoGAN can explore inherent
data attributes and perform conditional data generation based
on these attributes. The use of class labels has been shown to
further improve generated image’s quality and this information
can be easily incorporated into D by enforcing D to provide
class probabilities and use cross entropy loss for optimization
such as used in ACGAN (Figure 3 f).
2.3.2. Varying objective of G
In the vanilla GAN, G transforms noise z to sample xg =
G(z). This is usually accomplished by using a decoder network
to progressively increase the spatial size of the output until the
desired resolution is achieved as shown in Figure 2.
et al. proposed a variational autoencoder network (VAE)
as the underlying architecture of G (VAEGAN, Figure 3 g),
where it can use pixel-wise reconstruction loss to enforce the
decoder part of VAE to generate structures to match the real
The original setup of a GAN does not have any restrictions
on the modes of data it can generate. However, if auxiliary
information were provided during the generation, the GAN can
be driven to output images with desired properties. A GAN in
this scenario is usually referred as a conditional GAN (cGAN)
and the generation process can be expressed as xg = G(z, c).
One of the most common conditional inputs c is an image.
pix2pix, the ﬁrst general purpose GAN based image-to-image
translation framework was proposed by Isola et al. (Figure 4 a). Further, task related supervision was introduced to the
generator. For example, reconstruction loss for image restoration and Dice loss for segmentation. This
form of supervision requires aligned training pairs. Zhu et al.
 ; Kim et al. relaxed this constraint by stitching
two generators together head to toe so that images can be translated between two sets of unpaired samples (Figure 4 b). For
the sake of simplicity, we chose CycleGAN to represent this
idea in the rest of this paper. Another model named UNIT (Figure 4 c) can also perform unpaired image-to-image transform
by combining two VAEGANs together with each one responsible for one modality but sharing the same latent space . These image-to-image translation frameworks
are very popular in the medical imaging community due to their
general applicability.
Other than image, the conditional input can be class labels (CGAN, Figure 3 h) , text descriptions , object locations , surrounding image context , or
sketches . Note that ACGAN mentioned
in the previous section also has a class conditional generator.
2.3.3. Varying architecture
Fully connected layers were used as the building block in
vanilla GAN but later on, were replaced by fully convolutional
downsampling/upsampling layers in DCGAN . DCGAN demonstrated better training stability hence
quickly populated the literature. As shown in Figure 2, the generator in DCGAN architecture works on random input noise
vector by successive upsampling operations eventually generating an image from it. Two of its important ingredients are
BatchNorm for regulating the extracted feature scale, and LeakyRelu for preventing dead gradients. Very recently, Miyato et al. proposed a spectral normalization layer that normalized weights
in the discriminator to regulate the scale of feature response
values. With the training stability improved, some works have
(a) Vanilla GAN
(b) CatGAN
(c) EBGAN/BEGAN
(d) ALI/BiGAN
(e) InfoGAN
(g) VAEGAN
(i) LAPGAN/SGAN
(cascade or stack of GANs)
real or fake sample
certain or uncertain class prediction
real or fake reconstruction loss
Figure 3: A schematic view of variants of GAN. c represents the conditional vector. In CGAN and ACGAN, c is the discrete categorical code (e.g. one hot vector)
that encodes class labels and in InfoGAN it can also be continuous code that encodes attributes. xg generally refers to the generated image but can also be internal
representations as in SGAN.
(a) pix2pix
(b) CycleGAN
real image
generated fake image
Aligned training sample
r ||p: target consistency
Unaligned training sample
||G2(G1(xa
r ||p + ||G1(G2(xb
r ||p: cycle consistency
Figure 4: cGAN frameworks for image-to-image translation. pix2pix requires aligned training data whereas this constraint is relaxed in CycleGAN but usually
suﬀers from performance loss. Note that in (a), we chose reconstruction loss as an example of target consistency. This supervision is task related and can take many
other diﬀerent forms. (c) It consists of two VAEGANs with shared latent vector in the VAE part.
also incorporated residual connections into both the generator and discriminator and experimented with much deeper networks . The work
in Miyato and Koyama proposed a projection based way
to incorporate the conditional information instead of direct concatenation and found it to be beneﬁcial in improving the generated image’s quality.
Directly generating high resolution images from a noise vector is hard, therefore some works have proposed tackling it in
a progressive manner. In LAPGAN (Figure 3 i), Denton et al.
 proposed a stack of GANs, each of which adds higher
frequency details into the generated image. In SGAN, a cascade of GANs is also used but each GAN generates increasingly
lower level representations
 , which are
compared with the hierarchical representations extracted from a
discriminatively trained model. Karras et al. adopted an
alternate way where they progressively grow the generator and
discriminator by adding new layers to them rather than stacking another GAN on top of the preceding one (PGGAN). This
progressive idea was also explored in conditional setting . More recently, Karras et al. proposed a
style-based generator architecture (styleGAN) where instead of
directly feeding the latent code z to the input of the generator, they transformed this code ﬁrst to an intermediate latent
space and then use it to scale and shift the normalized image
feature responses computed from each convolution layer. Similarly, Park et al. proposed SPADE where the segmentation mask was injected to the generator via a spatially adaptive
normalization layer. This conditional setup was found to better
preserve the semantic layout of the mask than directly feeding
the mask to the generator.
Schematic illustrations of the most representative GANs are
shown in Figure 3. They are GAN, CatGAN, EBGAN/BEGAN,
ALI/BiGAN, InfoGAN, ACGAN, VAEGAN, CGAN, LAPGAN,
SGAN. Three popular image-to-image translation cGANs (pix2pix,
CycleGAN, and UNIT) are shown in Figure 4. For a more indepth review and empirical evaluation of these diﬀerent variants
of GAN, we refer the reader to .
3. Applications in Medical Imaging
There are generally two ways GANs are used in medical
imaging. The ﬁrst is focused on the generative aspect, which
can help in exploring and discovering the underlying structure
of training data and learning to generate new images. This property makes GANs very promising in coping with data scarcity
and patient privacy. The second focuses on the discriminative
aspect, where the discriminator D can be regarded as a learned
prior for normal images so that it can be used as regularizer or
detector when presented with abnormal images. Figure 5 provides examples of GAN related applications, with examples (a),
(b), (c), (d), (e), (f) that focus on the generative aspect and example (g) that exploits the discriminative aspect. In the following subsections, in order to help the readers ﬁnd applications
of their interest, we categorized all the reviewed articles into
canonical tasks: reconstruction, image synthesis, segmentation,
classiﬁcation, detection, registration, and others.
3.1. Reconstruction
Due to constraints in clinical settings, such as radiation dose
and patient comfort, the diagnostic quality of acquired medical
images may be limited by noise and artifacts. In the last decade,
we have seen a paradigm shift in reconstruction methods changing from analytic to iterative and now to machine learning based
methods. These data-driven learning based methods either learn
to transfer raw sensory inputs directly to output images or serve
as a post processing step for reducing image noise and removing artifacts. Most of the methods reviewed in this section are
borrowed directly from the computer vision literature that formulate post-processing as an image-to-image translation problem where the conditioned inputs of cGANs are compromised
in certain forms, such as low spatial resolution, noise contamination, under-sampling, or aliasing. One exception is for MR
images where the Fourier transform is used to incorporate the
raw K-space data into the reconstruction.
The basic pix2pix framework has been used for low dose
CT denoising , MR reconstruction , and PET denoising . A pretrained VGG-net was further
incorporated into the optimization framework to ensure perceptual similarity . Yi
and Babyn introduced a pretrained sharpness detection
network to explicitly constrain the sharpness of the denoised
CT especially for low contrast regions. Mahapatra computed a local saliency map to highlight blood vessels in superresolution process of retinal fundus imaging. A similar idea was
explored by Liao et al. in sparse view CT reconstruction. They compute a focus map to modulate the reconstructed
output to ensure that the network focused on important regions.
Besides ensuring image domain data ﬁdelity, frequency domain
data ﬁdelity is also imposed when raw K-space data is available
in MR reconstruction .
Losses of other kinds have been used to highlight local image structures in the reconstruction, such as the saliency loss to
reweight each pixel’s importance based on its perceptual relevance and the style-content loss in PET
denoising . In image reconstruction
of moving organs, paired training samples are hard to obtain.
Therefore, Rav`ı et al. proposed a physical acquisition
based loss to regulate the generated image structure for endomicroscopy super resolution and Kang et al. proposed to
use CycleGAN together with an identity loss in the denoising
of cardiac CT. Wolterink et al. found that in low dose
CT denoising, meaningful results can still be achieved when removing the image domain ﬁdelity loss from the pix2pix framework, but the local image structure can be altered. Papers relating to medical image reconstruction are summarized in Table 1.
It can be noticed that the underlying methods are almost the
same for all the reconstruction tasks. MR is special case as it
has a well deﬁned forward and backward operation, i.e. Fourier
transform, so that raw K-space data can be incorporated. The
same methodology can potentially be applied to incorporate the
sinogram data in the CT reconstruction process but we have not
seen any research using this idea as yet probably because the
sinogram data is hard to access. The more data used, either
raw K-space or image from other sequence, the better are the
reconstructed results. In general, using adversarial loss produces more visually appealing results than using pixel-wise reconstruction loss alone. But using adversarial loss to match the
generated and real data distribution may make the model hallucinate unseen structures. Pixel-wise reconstruction loss helps
to combat this problem if paired samples are available, and if
the model was trained on all healthy images but employed to
reconstruct images with pathologies, the hallucination problem
will still exist due to domain mismatch. Cohen et al. 
have conducted extensive experiments to investigate this problem and suggest that reconstructed images should not be used
for direct diagnosis by radiologists unless the model has been
properly veriﬁed.
However, even though the dataset is carefully curated to
match the training and testing distribution, there are other problems in further boosting performance. We have seen various
diﬀerent losses introduced to the pix2pix framework as shown
in Table 2 to improve the reconstructed ﬁdelity of local structures. There is, however, no reliable way of comparing their
eﬀectiveness except for relying on human observer or downstream image analysis tasks.
Large scale statistical analysis
by human observer is currently lacking for GAN based reconstruction methods. Furthermore, public datasets used for image reconstruction are not tailored towards further medical image analysis, which leaves a gap between upstream reconstruction and downstream analysis tasks. New reference standard
datasets should be created for better comparison of these GANbased methods.
(a) low dose CT denoising
(b) Cross modality transfer (MR→CT)
(c) Vessel to fundus image
(d) Skin lesion synthesis
(e) Organ segmentation
(f) Domain adaptation
(g) Abnormality Detection
Figure 5: Example applications using GANs. Figures are directly cropped from the corresponding papers. (a) Left side shows the noise contaminated low dose CT
and right side shows the denoised CT that well preserved the low contrast regions in the liver . (b) Left side shows the MR image and right side
shows the synthesized corresponding CT. Bone structures were well delineated in the generated CT image . (c) The generated retinal fundus
image have the exact vessel structures as depicted in the left vessel map . (d) Randomly generated skin lesion from random noise (a mixture of
malignant and benign) . (e) An organ (lung and heart) segmentation example on adult chest X-ray. The shapes of lung and heart are regulated by
the adversarial loss . (f) The third column shows the domain adapted brain lesion segmentation result on SWI sequence without training with the
corresponding manual annotation . (g) Abnormality detection of optical coherence tomography images of the retina .
3.2. Medical Image Synthesis
Depending on institutional protocols, patient consent may
be required if diagnostic images are intended to be used in a
publication or released into the public domain . GANs are widely for medical image
synthesis. This helps overcome the privacy issues related to diagnostic medical image data and tackle the insuﬃcient number
of positive cases of each pathology. Lack of experts annotating medical images poses another challenge for the adoption
of supervised training methods. Although there are ongoing
collaborative eﬀorts across multiple healthcare agencies aiming to build large open access datasets, e.g. Biobank, the National Biomedical Imaging Archive (NBIA), The Cancer Imaging Archive (TCIA) and Radiologist Society of North America
(RSNA), this issue remains and constrains the number of images researchers might have access to.
Traditional ways to augment training sample include scaling, rotation, ﬂipping, translation, and elastic deformation . However, these transformations do not account
for variations resulting from diﬀerent imaging protocols or sequences, not to mention variations in the size, shape, location
and appearance of speciﬁc pathology. GANs provide a more
generic solution and have been used in numerous works for
augmenting training images with promising results.
3.2.1. Unconditional Synthesis
Unconditional synthesis refers to image generation from random noise without any other conditional information. Techniques commonly adopted in the medical imaging community
include DCGAN, WGAN, and PGGAN due to their good training stability. The ﬁrst two methods can handle an image resolution of up to 256 × 256 but if higher resolution images are
desired, the progressive technique proposed in PGGAN is a
choice. Realistic images can be generated by directly using
the author released code base as long as the variations between
images are not too large, for example, lung nodules and liver
lesions. To make the generated images useful for downstream
tasks, most studies trained a separate generator for each individual class; for example, Frid-Adar et al. used three DC-
GANs to generate synthetic samples for three classes of liver
lesions (cysts, metastases, and hemangiomas); generated samples were found to be beneﬁcial to the lesion classiﬁcation task
Table 1: Medical image reconstruction publications. In the second column, * following the method denotes some modiﬁcations on the basic framework either on
the network architecture or on the employed losses. A brief description of the losses, quantitative measures and datasets can be found in Table 2, 3 and 7. In the
last column, symbol Ë and é denotes whether the corresponding literature used paired training data or not. All studies were performed in 2D unless otherwise
mentioned.
Publications
Quantitative Measure
Wolterink et al. 
[é] [3D] Denoising
Yi and Babyn 
M12, 13, 24, 25
[Ë] Denoising
Yang et al. 
M12, 13, 25
[Ë] [3D] [Abdomen] Denoising
Kang et al. 
[é] [Coronary] Denoising CT
You et al. 
M1, 11, 12, 13, 25
[Ë] [3D] Denoising
Tang et al. 
[Ë] Denoising, contrast enhance
Shan et al. 
M9, 10, 12, 13
[é] [3D] Denoising transfer from 2D
Liu et al. 
[Ë] Denoising, using adjacent slice
Liao et al. 
M11, 12, 13
[Ë] Sparse view CT reconstruction
Wang et al. 
[Ë] Metal artefact reduction cochlear implants
You et al. 
M12, 13, 16
[Ë] Superresolution, denoising
GANs 
[Ë] Sparse view CT reconstruction
Armanious et al. 
L1, 2, 8, 11
M11, 12, 13, 15,
[Ë] Inpainting
Quan et al. 
D11, 12, 13
M11, 12, 13
[Ë] Under-sampled K-space
Mardani et al. 
[Ë] Under-sampled K-space
Yu et al. 
M11, 12, 13, 24
[Ë] Under-sampled K-space
Yang et al. 
L1, 2, 8, 15
M11, 12, 13, 24
[Ë] Under-sampled K-space
Sanchez and Vilaplana 
[Ë] [3D] Superresolution
Chen et al. 
M11, 12, 13
[Ë] [3D] Superresolution
Kim et al. 
M1, 11, 13, 26
[Ë] Superresolution
Dar et al. 
D11, 19, 22
[Ë] Under-sampled K-space
Shitrit and Raviv 
[Ë] Under-sampled K-space
Ran et al. 
[Ë] [3D] Denoising
Seitzer et al. 
M1, 12, 23
[Ë] Two stage
Abramian and Eklund 
[Ë] Facial anonymization problem
Armanious et al. 
L1, 2, 8, 11
M11, 12, 13, 15
[Ë] Inpainting
Oksuz et al. 
M11, 12, 13
[Ë] Motion correction
Zhang et al. 
L1, 2, 8, 12
[Ë] Directly in complex-valued k-space data
Armanious et al. 
L1, 2, 8, 11
M13, 14, 15, 18
[Ë] Motion correction
Wang et al. 
cascade cGAN
M11, 12, 27
Armanious et al. 
L1, 2, 8, 11
M1, 11, 12, 13, 14, 15, 18
Retinal fundus imaging
Mahapatra 
L1, 2, 8, 17
M11, 12, 13
[Ë] Superresolution
Endomicroscopy
Rav`ı et al. 
L1, 18, 19
[é] Superresolution
Table 2: A brief summary of diﬀerent losses used in the reviewed publications in Tables 1 and 5. The third column speciﬁes conditions to be fulﬁlled in order to use
the corresponding loss. L in the ﬁrst column stands for loss.
Requirement
Ladversarial
Adversarial loss introduced by the discriminator, can take the form of cross entropy loss, hinge loss, least square loss etc. as discussed in Section 2.3.1
Aligned training pair
Element-wise data ﬁdelity loss in image domain to ensure structure similarity to the target when aligned training pair is provided
Element-wise loss to ensure self-similarity during cycled transformation when unaligned training pair is provided
Aligned training pair
Element-wise loss in the gradient domain to emphasize edges
Aligned training pair
Similar to Lgradient but using gradient feature map as a weight to image pixels
Aligned training pair
Element-wise loss in a feature domain computed from a pre-trained network, which is expected to be the image sharpness with focus on low contrast regions
Lshape, Lseg
Annotated pixel-wise label
Loss introduced by a segmentor to ensure faithful reconstruction of anatomic regions
Lperceptual
Aligned training pair
Element-wise loss in a feature domain computed from a pre-trained network which expected to conform to visual perception
Lstructure
Aligned training pair
Patch-wise loss in the image domain computed with SSIM which claims to better conform to human visual system
Lstructure2
Aligned pair
MIND as used in image registration for two images with the same content from diﬀerent modality
Lstyle-content
Aligned training pair
Style and content loss to ensure similarity of image style and content. Style is deﬁned as the Gram matrix which is basically the correlation of low-level
Element-wise loss in image domain to ensure structure similarity to the input. Useful in denoising since the two have similar underlying structure
Aligned training pair
Element-wise loss in a feature domain which is computed from steerable ﬁlters with focus on vessel-like structures
Aligned image-wise label
Loss introduced by a classiﬁer to get semantic information
Lfrequency
Aligned training pair
Element-wise loss in frequency domain (K-space) used in MR image reconstruction
Kullback–Leibler divergence which is commonly seen in variational inference to ensure closer approximation to the posterior distribution
Aligned training pair
Element-wise loss in a feature domain which is expected to be the saliency map
Physical model
Loss introduced by a physical image acquisition model
Lregulation
Regulate the generated image contrast by keeping the mean value across row and column unchanged
with both improved sensitivity and speciﬁcity when combined
with real training data.
Bermudez et al. claimed that
neuroradiologists found generated MR images to be of comparable quality to real ones, however, there were discrepancies
in anatomic accuracy. Papers related to unconditional medical
image synthesis are summarized in Table 4.
3.2.2. Cross modality synthesis
Cross modality synthesis (such as generating CT-like images based on MR images) is deemed to be useful for multiple
reasons, one of which is to reduce the extra acquisition time
and cost. Another reason is to generate new training samples
with the appearance being constrained by the anatomical structures delineated in the available modality. Most of the methods
reviewed in this section share many similarities to those in Section 3.1. pix2pix-based frameworks are used in cases where different image modality data can be co-registered to ensure data
ﬁdelity. CycleGAN-based frameworks are used to handle more
general cases where registration is challenging such as in cardiac applications. In a study by Wolterink et al. for
brain CT image synthesis from MR image, the authors found
that training using unpaired images was even better than using aligned images. This most likely resulted from the fact
that rigid registration could not very well handle local alignment in the throat, mouth, vertebrae, and nasal cavities. Hiasa
et al. further incorporated gradient consistency loss in
Table 3: A brief summary of quantitative measures used in the reviewed publications listed in Tables 1, 4, 5 and 6.
Overall image quality without reference
Human observer
Gold standard but costly and hard to scale
M2 Breuleux et al. 
Kernel density function
Estimate the probability density of the generated data and compute the log likelihood of real test data under this distribution
M3 Salimans et al. 
Inception score
Measure the generated images’ diversity and visual similarity to the real images with the pretrained Inception model
M4 Goodfellow et al. 
JS divergence
Distance measure between two distributions (used for comparison between normalized color histogram computed from a large
batch of image samples)
Wasserstein distance
Distance measure between two distributions (used for comparison between normalized color histogram computed from a large
batch of image samples)
M6 Matkovic et al. 
Global contrast factor
M7 K¨ohler et al. 
Vessel-based quality metric (noise and blur) for fundus image
M8 Niemeijer et al. 
Image structure clustering, a trained classiﬁer based to diﬀerentiate normal from low quality fundus images
M9 Shan et al. 
Perceptual loss
Diﬀerence of features extracted from a pre-trained VGG net
M10 Shan et al. 
Texture loss
Gram matrix which is basically the correlation of low-level features, deﬁned as style in style transfer literature
Overall image quality with respect to a groundtruth
NMSE/MAE/MSE
(Normalized) mean absolute/square error with respect to a given groundtruth
(Peak) signal to noise ratio with respect to a given groundtruth
M13 Wang et al. 
Structural similarity with respect to a given groundtruth
M14 Sheikh and Bovik 
Visual information ﬁdelity with regard to a given groundtruth
M15 Wang and Bovik 
Universal quality index with regard to a given groundtruth
M16 Sheikh et al. 
Information Fidelity Criterion
M17 Zhang et al. 
A low-level feature based image quality assessment metric with regard to a given groundtruth
M18 Zhang et al. 
Learned perceptual image patch similarity
M19 Pluim et al. 
Mutual information
Commonly used in cross modality registration in evaluating the alignment of two images
(Normalized) median intensity, used to measure color consistancy of histology images
M21 Lee Rodgers and Nicewander 
Cross correlation
Global correlation between two images
M22 Low 
Clinical measure
Dose diﬀerence, gamma analysis for CT
M23 Seitzer et al. 
Semantic interpretability score, essentially the dice loss of a pre-trained downstream segmentor
Local image quality
Line proﬁle
Measure the loss of spatial resolution
Noise level
Standard deviation of intensities in a local smooth region
Contrast to background ratio, measure the local contrast loss
M27 Kinahan and Fletcher 
Standard uptake value, a clinical measure in oncology for local interest region, should not vary too much in reconstruction
Noise power spectrum
Image quality analysis by auxiliary task
Task speciﬁc statistics
Down stream task (e.g. for coronary calcium quantiﬁcation)
Classiﬁcation
Down stream task
Down stream task (e.g. for lesion/hemorrhage)
Segmentation
Down stream task
Cross modality registration
Down stream task
Depth estimation
Down stream task
the training to improve accuracy at the boundaries. Zhang et al.
 found that using only cycle loss in the cross modality
synthesis was insuﬃcient to mitigate geometric distortions in
the transformation. Therefore, they employed a shape consistency loss that is obtained from two segmentors (segmentation
network). Each segmentor segments the corresponding image
modality into semantic labels and provides implicit shape constraints on the anatomy during translation. To make the whole
system end-to-end trainable, semantic labels of training images from both modalities are required.
Zhang et al. 
and Chen et al. proposed using a segmentor also in the
cycle transfer using labels in only one modality. Therefore, the
segmentor is trained oﬄine and ﬁxed during the training of the
image transfer network. As reviewed in Section 2, UNIT and
CycleGAN are two equally valid frameworks for unpaired cross
modality synthesis. It was found that these two frameworks performed almost equally well for the transformation between T1
and T2-weighted MR images . Papers
related to cross modality medical image synthesis are summarized in Table 5.
3.2.3. Other conditional synthesis
Medical images can be generated by constraints on segmentation maps, text, locations or synthetic images etc. This is useful to synthesize images in uncommon conditions, such as lung
nodules touching the lung border . Moreover, the conditioned segmentation maps can also be generated
from GANs or from a pretrained segmentation network , by making the generation
a two stage process.
Mok and Chung used cGAN to
augment training images for brain tumour segmentation. The
generator was conditioned on a segmentation map and generated brain MR images in a coarse to ﬁne manner. To ensure
the tumour was well delineated with a clear boundary in the
generated image, they further forced the generator to output the
tumour boundaries in the generation process. The full list of
synthesis works is summarized in Table 6.
3.3. Segmentation
Generally, researchers have used pixel-wise or voxel-wise
loss such as cross entropy for segmentation. Despite the fact
that U-net was used to combine both
low-level and high-level features, there is no guarantee of spatial consistency in the ﬁnal segmentation map. Traditionally,
conditional random ﬁeld (CRF) and graph cut methods are usually adopted for segmentation reﬁnement by incorporating spatial correlation. Their limitation is that they only take into account pair-wise potentials which might cause serious boundary
leakage in low contrast regions. On the other hand, adversarial
losses as introduced by the discriminator can take into account
high order potentials . In this case, the discriminator can be regarded as a shape regulator. This regularization eﬀect is more prominent when the object of interest has
a compact shape, e.g. for lung and heart mask but less useful for deformable objects such as vessels and catheters. This
regulation eﬀect can be also applied to the internal features of
the segmentor to achieve domain (diﬀerent scanners, imaging
protocols, modality) invariance 
[Lung nodule]
Frid-Adar et al. 
DCGAN /ACGAN
[Liver lesion] Generating each lesion class separately (with DCGAN) is than generating all classes at once (using ACGAN)
Bowles et al. 
[Brain] Joint learning of image and segmentation map
Calimeri et al. 
Zhang et al. 
Semi-Coupled-GAN
[Heart] Two generators coupled with a single discriminator which outputted both a distribution over the image data source and class
Han et al. 
Beers et al. 
Bermudez et al. 
Mondal et al. 
[Brain] Semi-supervised training with labeled, unlabeled, generated data
Bowles et al. 
[Brain] Joint learning of image and segmentation map
Salehinejad et al. 
[Chest] Five diﬀerent GANs to generate ﬁve diﬀerent classes of chest disease
Madani et al. 
[Chest] Semi-supervised DCGAN can achieve performance comparable with a traditional supervised CNN with an order of magnitude less labeled data
Madani et al. 
[Chest] Two GANs to generate normal and abnormal chest X-rays separately
Mammography
Korkinof et al. 
Histopothology
Hu et al. 
WGAN+infoGAN
Cell level representation learning
Retinal fundus imaging
Beers et al. 
Lahiri et al. 
Semi-supervised DCGAN can achieve performance comparable with a traditional supervised CNN with an order of magnitude less
labeled data
Lahiri et al. 
Extend the above work by adding an unsupervised loss into the discriminator
Dermoscopy
Baur et al. 
Baur et al. 
Yi et al. 
CatGAN+ WGAN
Semi-supervised skin lesion feature representation learning
et al., 2018). The adversarial loss can also be viewed as a adaptively learned similarity measure between the segmented outputs and the annotated groundtruth. Therefore, instead of measuring the similarity in the pixel domain, the discriminative network projects the input to a low dimension manifold and measures the similarity there. The idea is similar to the perceptual
loss. The diﬀerence is that the perceptual loss is computed from
a pre-trained classiﬁcation network on natural images whereas
the adversarial loss is computed from a network that trained
adaptively during the evolvement of the generator.
Xue et al. used a multi-scale L1 loss in the discriminator where features coming from diﬀerent depths are compared. This was demonstrated to be eﬀective in enforcing the
multi-scale spatial constraints on segmentation maps and the
system achieved state-of-the-art performance in the BRATS 13
and 15 challenges. Zhang et al. proposed to use both
annotated and unannotated images in the segmentation pipeline.
The annotated images are used in the same way as in where both element-wise loss and adversarial loss are applied. The unannotated images on the other
hand are only used to compute a segmentation map to confuse
the discriminator. Li and Shen combined pix2pix with
ACGAN for segmentation of ﬂuorescent microscopy images of
diﬀerent cell types. They found that the introduction of the auxiliary classiﬁer branch provides regulation to both the discriminator and the segmentor.
Unlike these aforementioned segmentation works where adversarial training is used to ensure higher order structure consistency on the ﬁnal segmentation maps, the adversarial training
scheme in enforces network invariance to
small perturbations of the training samples in order to reduce
overﬁtting on small dataset. Papers related to medical image
segmentation are summarized in Table 8.
3.4. Classiﬁcation
Classiﬁcation is arguably one of the most successful tasks
where deep learning has been applied. Hierarchical image features can be extracted from a deep neural network discriminatively trained with image-wise class labels. GANs have been
used for classiﬁcation problems as well, either using part of
the generator and discriminator as a feature extractor or directly using the discriminator as a classiﬁer (by adding an extra
class corresponding to the generated images). Hu et al. 
used combined WGAN and InfoGAN for unsupervised celllevel feature representation learning in histopathology images
whereas Yi et al. combined WGAN and CatGAN for
unsupervised and semi-supervised feature representation learning for dermoscopy images. Both works extract features from
the discriminator and build a classiﬁer on top.
Madani et al.
 , Lahiri et al. and Lecouat et al. adopted
the semi-supervised training scheme of GAN for chest abnormality classiﬁcation, patch-based retinal vessel classiﬁcation
and cardiac disease diagnosis respectively.
They found that
the semi-supervised GAN can achieve performance compara-
Table 5: Cross modality image synthesis publications. In the second column, * following the method denotes some modiﬁcations on the basic framework either
on the network architecture or on the employed losses. A brief description of the losses, quantitative evaluation measures and datasets can be found in Tables 2, 3
and 7. In the last column, symbol Ë and é denotes whether the corresponding literature used paired training data or not.
Pulications
Nie et al. 
Cascade GAN
[Ë]Brain; Pelvis
Emami et al. 
M11, 12, 13
Jin et al. 
Jiang et al. 
L1, 2, 3, 7, 8
Chartsias et al. 
Zhang et al. 
[é][3D] Heart
Huo et al. 
[é] Spleen
Chartsias et al. 
Hiasa et al. 
[é] Musculoskeletal
Wolterink et al. 
Huo et al. 
[é] Abdomen
Yang et al. 
L1, 2, 3, 10
M11, 12, 13
Maspero et al. 
[Ë] Pelvis
Bi et al. 
Ben-Cohen et al. 
M11, 12, 31
Armanious et al. 
L1, 2, 8, 11
M11, 12, 13, 14, 15, 18
Wei et al. 
cascade cGAN
Pan et al. 
3D CycleGAN
Choi and Lee 
Synthetic →Real
Hou et al. 
synthesizer+cGAN
[Ë] Histopathology
Real →Synthetic
Mahmood et al. 
[é] Endocsocpy
Zhang et al. 
Domain adaption
Chen et al. 
Dar et al. 
D11, 19, 22
Yang et al. 
M11, 12, 19, 32, 33
Welander et al. 
CycleGAN, UNIT
M11, 12, 19
Liu 
T1 →FLAIR MR
Yu et al. 
M11, 12, 32
[Ë] [3D] Brain
T1, T2 →MRA
Olut et al. 
Nie et al. 
Cascade GAN
Histopathology color normalization
Bentaieb and Hamarneh 
cGAN+classiﬁer
D37, 38, 39
Zanjani et al. 
L1, 2, 12, 16
Shaban et al. 
M12, 13, 17, 30
Hyperspectral histology →H&E
Bayramoglu et al. 
Table 6: Other conditional image synthesis publications categorized by imaging modality. * following the method denotes some modiﬁcations on the basic
framework either on the network architecture or on the employed losses. A brief description of the losses, quantitative evaluation measures and datasets can be
found in Tables 2, 3 and 7
Publications
Conditional information
Evaluation
Jin et al. (lung nodule)
VOI with removed central region
[3D] pix2pix* (L1 loss considering nodule context)
Mok and Chung 
Segmentation map
Coarse-to-ﬁne boundary-aware
Shin et al. 
Segmentation map
D16, 19, 21
Gu et al. 
Lau et al. 
Segmentation map
Cascade cGAN
Hu et al. 
Gleason score
Ultrasound
Hu et al. (fetus)
Probe location
Tom and Sheet 
Segmentation map
cascade cGAN
Retinal fundus imaging
Zhao et al. 
Vessel map
D41, 43, 45
Guibas et al. 
Vessel map
Costa et al. 
Vessel map
Segmentor+pix2pix
Costa et al. 
Vessel map
Adversarial VAE+cGAN
Appan and Sivaswamy 
Vessel map; Lesion map
D46, 47, 48
Iqbal and Ali 
Vessel map
Histopathology
Senaras et al. 
Segmentation map
Galbusera et al. 
Diﬀerent view; segmentation map
pix2pix/CycleGAN
Mahapatra et al. 
segmentation map+X-ray
pix2pix* (content loss encourage dissimilarity)
Oh and Yun 
X-ray (for bone supression)
pix2pix* (Haar wavelet decomposion)
M11, 12, 13, 28
Table 7: Common datasets used in the reviewed literature. In the ﬁrst column, D stands for Dataset.
D1 Yi and Babyn 
Whole body
D2 McCollough et al. 
MICCAI2013
Organ segmentation
Abdomen, Pelvis
D4 Armato III et al. 
Lung cancer detection and diagnosis
D5 Yan et al. 
DeepLesion
Lesion segmentation
Liver tumor segmentation
D7 Glocker et al. 
Vertebrate localization
D8 Aerts et al. 
NSCLC-Radiomics
D9 Zhuang and Shen 
Whole heart segmentation
D10 Pace et al. 
HVSMR 2016
Whole heart and great vessel segmentation
Heart, Vessel
Analysis of brain development
End-systolic/diastolic volumes measurement
MRI reconstruction
Cartilage and bone segmentation
D15 Crimi et al. 
Lesion segmentation
Alzheimers disease neuroimaging Initiative
Brain structure segmentation
Gliomas segmentation
Gliomas segmentation
Gliomas segmentation
Gliomas segmentation, overall survival prediction
D22 Bullitt et al. 
Assessing the eﬀects of healthy aging
D23 Resnick et al. 
Baltimore longitudinal study of aging
D24 Van Essen et al. 
Human connectome project
D25 Wang et al. 
Infant brain tissue segmentation
UK Biobank
Health research
Brain, Heart, Body
D27 Gutman et al. 
Skin lesion analysis
Dermoscopy
D28 Codella et al. 
Skin lesion analysis
Dermoscopy
Skin lesion analysis
Dermoscopy
D30 Mendonca et al. 
Skin lesion analysis
Dermoscopy
D31 Ballerini et al. 
Skin lesion analysis
Dermoscopy
D32 Jaeger et al. 
Montgomery
Pulmonary disease detection
D33 Shiraishi et al. 
Pulmonary nodule detection
Cancer screening trial for
Prostate, lung, colorectal and ovarian (PLCO)
X-ray; Digital pathology
Segmentation of nuclei
Digital pathology
Segmentation of nuclei
Digital pathology
MITOS-ATYPIA
Mitosis detection; Nuclear atypia score evaluation
Digital pathology
D38 Sirinukunwattana et al. 
Gland segmentation
Digital pathology
D39 K¨obel et al. 
Carcinoma subtype prediction
Digital pathology
Camelyon16
Lymph node metastases detection
Digital pathology
D41 Bayramoglu et al. 
Virtual H&E staining
Digital pathology
D42 Kainz et al. 
CellDetect
Cell detection
Bone marrow
Digital pathology
D43 Staal et al. 
Blood vessels segmentation
Fundus imaging
Structural analysis of the retina
Fundus Imaging
D45 Budai et al. 
Image quality assessment, segmentation
Fundus Imaging
D46 Decenci`ere et al. 
Segmentation in retinal ophthalmology
Fundus Imaging
D47 Prentasic et al. 
Diabetic retinopathy detection
Fundus Imaging
D48 K¨alvi¨ainen and Uusitalo 
Diabetic retinopathy detection
Fundus Imaging
D49 Fumero et al. 
Optic nerve head segmentation
Fundus Imaging
D50 Hobson et al. 
HEp-2 cell classiﬁcation
Fluorescent microscopy
HEp-2 cell segmentation
Fluorescent microscopy
D52 Balocco et al. 
Vessel inner and outer wall border detection
Blood Vessel
Ultrasound
D53 Moreira et al. 
Mass segmentation
Mammography
D54 Heath et al. 
Mass segmentation
Mammography
ble with a traditional supervised CNN with an order of magnitude less labeled data. Furthermore, Madani et al. have
also shown that the adversarial loss can reduce domain overﬁtting by simply supplying unlabeled test domain images to the
discriminator in identifying cardiac abnormalities in chest Xray. A similar work in addressing domain variance in whole
slide images (WSI) has been conducted by Ren et al. .
Most of the other works that used GANs to generate new
training samples have been already mentioned in Section 3.2.1.
These studies applied a two stage process, with the ﬁrst stage
learned to augment the images and the second stage learned to
perform classiﬁcation by adopting the traditional classiﬁcation
network. The two stages are trained disjointedly without any
communication in between. The advantage is that these two
components can be replaced easily if more advanced unconditional synthesis architectures are proposed whereas the downside is that the generation has to be conducted for each class
separately (N models for N classes), which is not memory and
computation eﬃcient. A single model that is capable of performing conditional synthesis of multiple categories is an active
research direction . Surprisingly, Frid-Adar
et al. found that using separate GAN (DCGAN) for each
Table 8: Segmentation publications. A brief description of the datasets can be found in Table 7.
Publications
Yang et al. 
[3D] [Liver] Generator is essentially a U-net with deep supervisions
Dou et al. 
Ensure that the feature distribution of images from both domains (MR and CT) are indistinguishable
Rezaei et al. 
Additional reﬁnement network, patient-wise batchNorm, recurrent cGAN to ensure temporal consistancy
Sekuboyina et al. 
Adversarial training based on EBGAN; Butterﬂy shape network to combine two views
Xue et al. 
A multi-scale L1 loss in the discriminator where features coming from diﬀerent depth are compared
Rezaei et al. 
The generator takes heterogenous MR scans of various contrast as provided by BRATS 17 challenge
Rezaei et al. 
A cascade of cGANs in segmenting myocardium and blood pool
Li et al. 
The generator takes heterogenous MR scans of various contrast as provided by BRATS 17 challenge
Moeskops et al. 
Kohl et al. 
[Prostate] Improved sensitivity
Huo et al. 
[Spleen] Global convolutional network (GCN) with a large receptive ﬁeld as the generator
Kamnitsas et al. 
Regulate the learned representation so that the feature representation is domain invariant
Dou et al. 
Ensure that the feature distribution of images from both domains (MR and CT) are indistinguishable
Rezaei et al. 
Additional reﬁnement network, patient-wise batchNorm, recurrent cGAN to ensure temporal consistency
Xu et al. 
Joint learning (segmentation and quantiﬁcation); convLSTM in the generator for spatial-temporal processing; Bi-LSTM in the discriminator to learn relation between tasks
Han et al. 
Local-LSTM in the generator to capture spatial correlations between neighbouring structures
Zhao et al. 
Deep supervision; Discriminate segmentation map based on features extracted from a pre-trained network
Retinal fundus imaging
Son et al. 
Deep architecture is better for discriminating whole images and has less false positives with ﬁne vessels
Zhang et al. 
Use both annotated and unannotated images in the segmentation pipeline
Shankaranarayana et al. 
Dai et al. 
Adversarial loss is able to correct the shape inconsistency
Histopothology
Wang et al. 
Basal membrane segmentation
ﬂuorescent microscopy
Li and Shen 
pix2pix + ACGAN; Auxiliary classiﬁer branch provides regulation to both the discriminator and the segmentor
Dermoscopy
Izadi et al. 
Adversarial training helps to reﬁne the boundary precision
Mammography
Zhu et al. 
Enforce network invariance to small perturbations of the training samples in order to reduce overﬁtting on small size dataset
Ultrasound
Tuysuzoglu et al. 
Joint learning (landmark localization + prostate contour segmentation); Contour shape prior imposed by the discriminator
lesion class resulted in better performance in lesion classiﬁcation than using a uniﬁed GAN (ACGAN) for all classes. The
underlying reason remains to be explored. Furthermore, Finlayson et al. argue that images generated from GANs
may serve as an eﬀective augmentation in the medium-data
regime, but may not be helpful in a high or low-data regime.
3.5. Detection
The discriminator of GANs can be utilized to detect abnormalities such as lesions by learning the probability distribution
of training images depicting normal pathology. Any image that
falls out of this distribution can be deemed as abnormal. Schlegl
et al. used the exact idea to learn a manifold of normal
anatomical variability and proposed a novel anomaly scoring
scheme based on the ﬁtness of the test image’s latent code to
the learned manifold. The learning process was conducted in
an unsupervised fashion and eﬀectiveness was demonstrated
by state-of-the-art performance of anomaly detection on optical coherence tomography (OCT) images.
Alex et al. 
used GAN for brain lesion detection on MR images. The generator was used to model the distribution of normal patches
and the trained discriminator was used to compute a posterior
probability of patches centered on every pixel in the test image.
Chen and Konukoglu used an adversarial auto-encoder
to learn the data distribution of healthy brain MR images. The
lesion image was then mapped to an image without a lesion by
exploring the learned latent space, and the lesion could be highlighted by computing the residual of these two images. We can
see that all the detection studies targeted for abnormalities that
are hard to enumerate.
In the image reconstruction section, it has been observed
that if the target distribution is formed from medical images
without pathology, lesions within an image could be removed
in the CycleGAN-based unpaired image transfer due to the distribution matching eﬀect. However, it can be seen here that if
the target and source domain are of the same imaging modality diﬀering only in terms of normal and abnormal tissue, this
adverse eﬀect can actually be exploited for abnormality detection Sun et al. .
3.6. Registration
cGAN can also be used for multi-modal or uni-modal image registration. The generator in this case will either generate
transformation parameters, e.g. 12 numbers for 3D aﬃne transformation, deformation ﬁeld for non-rigid transformation or directly generate the transformed image. The discriminator then
discriminates aligned image pairs from unaligned image pairs.
A spatial transformation network or a
deformable transformation layer is usually
plugged in between these two networks to enable end-to-end
training. Yan et al. performed prostate MR to transrectal ultrasound (TRUS) image registration using this framework.
The paired training data was obtained through manual registration by experts.
Yan et al. employed a discriminator
to regularize the displacement ﬁeld computed by the generator
and found this approach to be more eﬀective than the other regularizers in MR to TRUS registration. Mahapatra et al. 
used CycleGAN for multi-modal (retinal) and uni-modal (MR)
deformable registration where the generator produces both the
transformed image and the deformation ﬁeld. Mahapatra et al.
 took one step further and explored the idea of joint
segmentation and registration with CycleGAN and found their
method performs better than the separate approaches for lung
X-ray images.
Tanner et al. employed CycleGAN for
deformable image registration between MR and CT by ﬁrst
transforming the source domain image to the target domain and
then employing a mono-modal image similarity measure for the
registration. They found this method can achieve at best similar performance with the traditional multi-modal deformable
registration methods.
3.7. Other works
In addition to the tasks described in the aforementioned sections, GANs have also been applied in other tasks discussed
here. For instance, cGAN has been used for modelling patient
speciﬁc motion distribution based on a single preoperative image , highlighting regions most accountable
for a disease and re-colorization
of endoscopic video data . In pix2pix was used for treatment planning in radiotherapy by predicting the dose distribution map from CT image.
WGAN has also been used for modelling the progression of
Alzheimer’s disease (AD) in MRI. This is achieved by isolating
the latent encoding of AD and performing arithmetic operation
in the latent space .
4. Discussion
In the years 2017 and 2018, the number of studies applying
GANs has risen signiﬁcantly. The list of these papers reviewed
for our study can be found on our 1GitHub repository.
About 46% of these papers studied image synthesis, with
cross modality image synthesis being the most important application of GANs. MR is ranked as the most common imaging
modality explored in the GAN related literature. We believe
one of the reasons for the signiﬁcant interest in applying GANs
for MR image analysis is due to the excessive amount of time
spent on the acquisition of multiple sequences. GANs hold the
potential to reduce MR acquisition time by faithfully generating
certain sequences from already acquired ones. A recent study
in image synthesis across diﬀerent MR sequences using Colla-
GAN shows the irreplaceable nature of exogenous contrast sequence, but reports the synthesis of endogenous contrast such
as T1, T2, from each other with high ﬁdelity .
A second reason for the popularity of GANs in MR might be
because of large number of publicly available MR datasets as
shown in Table 7.
1 
Another 37% of these studies fall into the group of reconstruction and segmentation due to the popularity of image-toimage translation frameworks.
Adversarial training in these
cases imposes a strong shape and texture regulation on the generator’s output which makes it very promising in these two tasks.
For example, in liver segmentation from 3D CT volumes, the
incorporation of adversarial loss signiﬁcantly improves the segmentation performance on non-contrast CT (has fuzzy liver boundary) than graph cut and CRF .
Further 8% of these studies are related to classiﬁcation. In
these studies, the most eﬀective use case was to combat domain
shift. For the studies that used GAN for data augmentation in
classiﬁcation, most focused on generating tiny objects that can
be easily aligned, such as nodules, lesions and cells. We believe it is partly due to the relatively smaller content variation of
these images compared to the full context image which makes
the training more stable with the current technique. Another
reason might be related to the computation budget of the research since training on high resolution images requires a lot
of GPU time. Although there are studies that applied GAN on
synthesizing whole chest-X-ray , the
eﬀectiveness has only been shown on fairly easy tasks, e.g.
cardiac abnormality classiﬁcation and on a medium size data
regime, e.g. a couple of thousand images. With the advent
of large volume labeled datasets, such as the CheXpert , it seems there is diminishing return in the employment of GANs for image generation, especially for classiﬁcation. We would like to argue that GANs are still useful in the
following two cases. First, nowadays the training of a deep neural network heavily relies on data augmentation to improve the
network’s generalizability on unseen test data and reduce over-
ﬁtting. However, existing data augmentation operations are all
manually designed operations, e.g. rotation, color jittering, and
can not cover the whole variation of the data.
Cubuk et al.
 recently proposed to learn an augmentation policy with
reinforcement learning but the search space still consisted of basic hand-crafted image processing operations. GANs, however,
can allow us to sample the whole data distribution which oﬀers
much more ﬂexibility in augmenting the training data . For example, styleGAN, is able to generate high
resolution realistic face images with unprecedented level of details. This could be readily applied to chest X-ray datasets to
generate images of a pathology class that has suﬃcient number
of cases. Second, it is well known that medical data distribution is highly skewed with its largest mass centered on common
diseases. It is impossible to accumulate enough training data
for rare diseases, such as rheumatoid arthritis, sickle cell disease. But radiologists have been trained to detect these diseases
in the long tail. Thus, another potential of GANs will be in
synthesizing uncommon pathology cases, most likely through
conditional generation with the conditioned information being
speciﬁed by medical experts either through text description or
hand drawn ﬁgures.
The remaining studies pertaining to detection, registration
and other applications are so limited that it is hard to draw any
conclusion.
4.1. Future challenges
Alongside many positive utilities of GANs, there are still
challenges that need to be resolved for their employment to
medical imaging. In image reconstruction and cross modality
image synthesis, most works still adopt traditional shallow reference metrics such as MAE, PSNR, or SSIM for quantitative
evaluation. These measures, however, do not correspond to the
visual quality of the image. For example, direct optimization of
pixel-wise loss produces a suboptimal (blurry) result but provides higher numbers than using adversarial loss. It becomes
increasingly diﬃcult to interpret these numbers in horizontal
comparison of GAN-based works especially when extra losses
as shown in Table 2 are incorporated. One way to alleviate this
problem is to use down stream tasks such as segmentation or
classiﬁcation to validate the quality of the generated sample.
Another way is to recruit domain experts but this approach is
expensive, time consuming and hard to scale. Recently, Zhang
et al. proposed learned perceptual image path similarity (LPIPS), which outperforms previous metrics in terms of
agreement with human judgements.
It has been adopted in
MedGAN for evaluation of the generated image quality but it would be interesting to see its eﬀectiveness for diﬀerent types of medical images as compared to subjective measures from experienced human observers in a more
extensive study. For natural images, the unconditional generated sample quality and diversity is usually measured by inception score , the mean MS-SSIM metric
among randomly chosen synthetic sample pairs , or Fr´echet Inception distance (FID) .
The validity of these metrics for medical images remains to be
Cross domain image-to-image translation can be achieved
with both paired and unpaired training data and it oﬀers many
prospective applications in medical imaging as has already been
seen in section 3.2.2. Unpaired training does not have the data
ﬁdelity loss term therefore there is no guarantee of preservation of small abnormality regions during the translation process.
Cohen et al. warn against the use of generated images for
direct interpretation by doctors. They observe that trained CycleGAN networks (for unpaired data) can be subject to bias due
to matching the generated data to the distribution of the target
domain. This system bias comes into being when target domain images in the training set have an over or under representation of certain classes. As an example of exploitation of this
eﬀect, Mirsky et al. demonstrate the possibility of malicious tampering of 3D medical imaging using 3D conditional
GANs to remove and inject solitary pulmonary nodule into patient’s CT scans. This system bias also exists in paired cross
domain image-to-image translation with the data ﬁdelity loss
but only happens when the model was trained on normal images but tested on abnormal images. Cautions should be taken
in training of the translation model and new methods should be
proposed to faithfully preserve local abnormal regions.
4.2. Interesting future applications
Similar to other deep learning neural network models, various applications of GANs demonstrated in this paper have di-
rect bearing on improving radiology workﬂow and patient care.
The strength of GANs however lies in their ability to learn in
an unsupervised and/or weakly-supervised fashion. In particular, we perceive that image-to-image translation achieved by
cGANs can have various other useful applications in medical
imaging. For example, restoration of MR images acquired with
certain artifacts such as motion, especially in a pediatric setting,
may help reduce the number of repeated exams.
Exploring GANs for image captioning task 
may lead to semi-automatic generation of medical imaging reports potentially reducing image reporting
times. Success of adversarial text classiﬁcation 
also prompts potential utility of GANs in improving performance of such systems for automatic MR protocol generation
from free-text clinical indications . Automated systems may improve MRI wait times which have been
on the rise as well as enhance patient care. cGANs,
speciﬁcally CycleGAN applications, such as makeup removal , can be extended to medical imaging with applications in improving bone x-ray images by removal of artifacts
such as casts to facilitate enhanced viewing.
This may aid
radiologists in assessing ﬁne bony detail, potentially allowing
for enhanced detection of initially occult fractures and helping assess the progress of bone healing more eﬃciently. The
success of GANs in unsupervised anomaly detection can help achieve the task of detecting abnormalities in medical images in an unsupervised manner. This has the
potential to be further extended for detection of implanted devices, e.g. staples, wires, tubes, pacemaker and artiﬁcial valves
on X-rays. Such an algorithm can also be used for prioritizing radiologists’ work lists, thus reducing the turnaround time
for reporting critical ﬁndings . We also expect to witness the utility of GANs in medical image synthesis from text descriptions , especially for rare
cases, so as to ﬁll in the gap of training samples required for
training supervised neural networks for medical image classi-
ﬁcation tasks. The recent work on styleGAN shows the capability to control the high level attributes
of the synthesized image by manipulating the scale and bias
parameters of the AdaIN layer .
Similarly, the SPADE controls the semantic
layout of the synthesized image by a spatially adaptive normalization layer. Imagine in the future the desired attribute can be
customized and speciﬁed in prior and manipulated in a localized fashion. We may then be able to predict the progression of
disease, measure the impact of drug trial as suggested in Bowles
et al. but with more ﬁne-grained controls.
Diﬀerent imaging modalities work by exploiting tissue response to a certain physical media, such as x-rays or a magnetic
ﬁeld, and thus can provide complementary diagnostic information to each other. As a common practice in supervised deep
learning, images of one modality type are labelled to train a
network to accomplish a desired task. This process is repeated
when switching modalities even if the underlying anatomical
structure is the same, resulting in a waste of human eﬀort. Adversarial training, or more speciﬁcally unpaired cross modality translation, enables reuse of the labels in all modalities and
opens new ways for unsupervised transfer learning .
Finally, we would like to point out that, although there have
many promising results reported in the literature, the adoption
of GANs in medical imaging is still in its infancy and there is
currently no breakthrough application as yet adopted clinically
for GANs-based methods.