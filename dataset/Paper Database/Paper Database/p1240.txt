Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning
Yinbo Chen
UC San Diego
Zhuang Liu
UC Berkeley
Huijuan Xu
Penn State University
Trevor Darrell
UC Berkeley
Xiaolong Wang
UC San Diego
Meta-learning has been the most common framework for
few-shot learning in recent years. It learns the model from
collections of few-shot classiﬁcation tasks, which is believed
to have a key advantage of making the training objective
consistent with the testing objective.
However, some recent works report that by training for whole-classiﬁcation,
i.e. classiﬁcation on the whole label-set, it can get comparable or even better embedding than many meta-learning
algorithms.
The edge between these two lines of works
has yet been underexplored, and the effectiveness of metalearning in few-shot learning remains unclear. In this paper,
we explore a simple process: meta-learning over a wholeclassiﬁcation pre-trained model on its evaluation metric.
We observe this simple method achieves competitive performance to state-of-the-art methods on standard benchmarks. Our further analysis shed some light on understanding the trade-offs between the meta-learning objective and
the whole-classiﬁcation objective in few-shot learning. Our
code is available at 
few-shot-meta-baseline.
1. Introduction
While humans have shown incredible ability to learn
from very few examples and generalize to many different
new examples, the current deep learning approaches still
rely on a large scale of training data. To mimic this human ability of generalization, few-shot learning is
proposed for training networks to understand a new concept based on a few labeled examples. While directly learning a large number of parameters with few samples is very
challenging and most likely leads to overﬁtting, a practical
setting is applying transfer learning: train the network on
common classes (also called base classes) with sufﬁcient
samples, then transfer the model to learn novel classes with
a few examples.
The meta-learning framework for few-shot learning follows the key idea of learning to learn. Speciﬁcally, it samples few-shot classiﬁcation tasks from training samples belonging to the base classes and optimizes the model to perform well on these tasks. A task typically takes the form
of N-way and K-shot, which contains N classes with K
support samples and Q query samples in each class. The
goal is to classify these N × Q query samples into the
N classes based on the N × K support samples. Under
this framework, the model is directly optimized on fewshot classiﬁcation tasks. The consistency between the objectives of training and testing is considered as the key advantage of meta-learning. Motivated by this idea, many recent works focus on improving
the meta-learning structure, and few-shot learning itself has
become a common testbed for evaluating meta-learning algorithms.
However, some recent works ﬁnd that training for wholeclassiﬁcation, i.e. classiﬁcation on the whole training labelset (base classes), provides the embedding that is comparable or even better than many recent meta-learning algorithms.
The effectiveness of whole-classiﬁcation models
has been reported in both prior works and some concurrent works . Meta-learning makes the form of
training objective consistent with testing, but why it turns
out to learn even worse embedding than simple wholeclassiﬁcation?
While there are several possible reasons,
e.g. optimization difﬁculty or overﬁtting, the answer has
not been clearly studied yet. It remains even unclear that
whether meta-learning is still effective compared to wholeclassiﬁcation in few-shot learning.
In this work, we aim at exploring the edge between
whole-classiﬁcation and meta-learning by decoupling the
discrepancies. We start with Classiﬁer-Baseline: a wholeclassiﬁcation method that is similarly proposed in concurrent works . In Classiﬁer-Baseline, we ﬁrst train
a classiﬁer on base classes, then remove the last fullyconnected (FC) layer which is class-dependent. During test
time, it computes mean embedding of support samples for
each novel class as their centroids, and classiﬁes query samples to the nearest centroid with cosine distance. We observe this baseline method outperforms many recent metalearning algorithms.
In order to understand whether meta-learning is still ef-
 
fective compared to whole-classiﬁcation, a natural experiment is to see what happens if we perform further metalearning over a converged Classiﬁer-Baseline on its evaluation metric (i.e. cosine nearest-centroid). As a resulting
method, it is similar to MatchingNet or ProtoNet 
with an additional classiﬁcation pre-training stage.
observe that meta-learning can still improve Classiﬁer-
Baseline, and it achieves competitive performance to stateof-the-art methods on standard benchmarks. We call this
simple method Meta-Baseline.
We highlight that as a
method, all the individual components of Meta-Baseline
have been proposed in prior works, but to the best of our
knowledge, it has been overlooked that none of the prior
works studies them as a whole. We further decouple the
discrepancies by evaluating on two types of generalization: base class generalization denotes performance on
few-shot classiﬁcation tasks from unseen data in the base
classes, which follows the common deﬁnition of generalization (i.e. evaluated in the training distribution); and novel
class generalization denotes performance on few-shot classiﬁcation tasks from data in novel classes, which is the
goal of the few-shot learning problem. We observe that:
(i) During meta-learning, improving base class generalization can lead to worse novel class generalization; (ii) When
training Meta-Baseline from scratch (i.e. without wholeclassiﬁcation training), it achieves higher base-class generalization but much lower novel class generalization.
Our observations suggest that there could be a tradeoff between the objectives of meta-learning and wholeclassiﬁcation. It is likely that meta-learning learns the embedding that works better for N-way K-shot tasks, while
whole-classiﬁcation learns the embedding with stronger
class transferability.
We ﬁnd that the main advantage
of training for whole-classiﬁcation before meta-learning is
likely to be improving class transferability. Our further experiments provide a potential explanation of what makes
Meta-Baseline a strong baseline: by inheriting one of the
most effective evaluation metrics of the whole-classiﬁcation
model, it maximizes the reusing of the embedding with
strong class transferability. From another perspective, our
results also rethink the comparison between meta-learning
and whole-classiﬁcation from the perspective of datasets.
When base classes are collected to cover the distribution of
novel classes, novel-class generalization should converge to
base-class generalization and the strength of meta-learning
may overwhelm the strength of whole-classiﬁcation.
In summary, our contributions are as following:
• We present a simple Meta-Baseline that has been overlooked in prior work. It achieves competitive performance to state-of-the-art methods on standard benchmarks and is easy to follow.
• We observe a trade-off between the objectives of metalearning and whole-classiﬁcation, which potentially
explains the success of Meta-Baseline and rethinks the
effectiveness of both objectives in few-shot learning.
2. Related Work
Most recent approaches for few-shot learning follow the
meta-learning framework. The various meta-learning architectures for few-shot learning can be roughly categorized into three groups. Memory-based methods are based on the idea to train a meta-learner
with memory to learn novel concepts (e.g.
an LSTMbased meta-learner). Optimization-based methods 
follows the idea of differentializing an optimization process over support-set within the meta-learning framework:
MAML ﬁnds an initialization of the neural network that
can be adapted to any novel task using a few optimization steps. MetaOptNet learns the feature representation that can generalize well for a linear support vector
machine (SVM) classiﬁer. Besides explicitly considering
the dynamic learning process, metric-based methods 
meta-learn a deep representation with a metric in feature
space. For example, Prototypical Networks compute
the average feature for each class in support-set and classify query samples by the nearest-centroid method. They
use Euclidean distance since it is a Bregman divergence.
Relation Networks further generalizes this framework
by proposing a relation module as a learnable metric jointly
trained with deep representations. TADAM proposes to
use a task conditioned metric resulting in a task-dependent
metric space.
While signiﬁcant progress is made in the meta-learning
framework, some recent works challenge the effectiveness
of meta-learning with simple whole-classiﬁcation, i.e.
classiﬁcation model on the whole training label-set. Cosine classiﬁer and Baseline++ perform wholeclassiﬁcation training by replacing the top linear layer with
a cosine classiﬁer, and they adapt the classiﬁer to a fewshot classiﬁcation task of novel classes by performing nearest centroid or ﬁne-tuning a new layer respectively. They
show these whole-classiﬁcation models can achieve competitive performance compared to several popular metalearning models. Another recent work studies on a transductive setting. Along with these baseline methods, more
advanced meta-learning methods are proposed
and they set up new state-of-the-art results. The effectiveness of whole-classiﬁcation is then revisited in two of the
concurrent works with improved design choices.
By far, the effectiveness of meta-learning compared to
whole-classiﬁcation in few-shot learning is still unclear,
since the edge between whole-classiﬁcation models and
meta-learning models remains underexplored. The goal of
this work is to explore the insights behind the phenomenons.
Our experiments show a potential trade-off between the
Whole-classiﬁcation training
Meta-learning
Matching Networks 
no / yes (large models)
attention + cosine
Prototypical Networks 
centroid + Euclidean
Baseline++ 
yes (cosine classiﬁer)
ﬁne-tuning
Meta-Baseline (ours)
centroid + cosine (∗τ)
Table 1: Overview of method comparison. We summarize the differences between Meta-Baseline and prior methods.
meta-learning and whole-classiﬁcation objectives, which
provides a more clear understanding of the comparison between both objectives for few-shot learning.
As a method, similar ideas to Classiﬁer-Baseline are concurrently reported in recent works . Unlike some
prior works , Classiﬁer-Baseline does not replace the
last layer with cosine classiﬁer during training, it trains
the whole-classiﬁcation model with a linear layer on the
top and applies cosine nearest-centroid metric during the
test time for few-shot classiﬁcation on novel classes. The
Meta-Baseline is meta-learning over a converged Classiﬁer-
Baseline on its evaluation metric (cosine nearest-centroid).
It is similar (with inconspicuous and important differences
as shown in Table 1) to those simple and classical metricbased meta-learning methods . The main purpose of
Meta-Baseline in this paper is to understand the comparison
between whole-classiﬁcation and meta-learning objectives,
but we ﬁnd it is also a simple meta-learning baseline that
has been overlooked. While every individual component in
Meta-Baseline is not novel, to the best of our knowledge,
none of the prior works studies them as a whole.
3.1. Problem deﬁnition
In standard few-shot classiﬁcation, given a labeled
dataset of base classes Cbase with a large number of images, the goal is to learn concepts in novel classes Cnovel
with a few samples. In an N-way K-shot few-shot classiﬁcation task, the support-set contains N classes with K
samples per class, the query-set contains samples from the
same N classes with Q samples per class, and the goal is to
classify the N × Q query images into N classes.
3.2. Classiﬁer-Baseline
Classiﬁer-Baseline is a whole-classiﬁcation model, i.e.
a classiﬁcation model trained for the whole label-set. It
refers to training a classiﬁer with classiﬁcation loss on all
base classes and performing few-shot tasks with the cosine
nearest-centroid method. Speciﬁcally, we train a classiﬁer
on all base classes with standard cross-entropy loss, then remove its last FC layer and get the encoder fθ, which maps
the input to embedding. Given a few-shot task with the
support-set S, let Sc denote the few-shot samples in class
c, it computes the average embedding wc as the centroid of
then for a query sample x in a few-shot task, it predicts the
probability that sample x belongs to class c according to the
cosine similarity between the embedding of sample x and
the centroid of class c:
p(y = c | x) =
 ⟨fθ(x), wc⟩
 ⟨fθ(x), wc′⟩
where ⟨·, ·⟩denotes the cosine similarity of two vectors.
Similar methods to Classiﬁer-Baseline have also been
proposed in concurrent works . Compared to Baseline++ , the Classiﬁer-Baseline does not use the cosine
classiﬁer for training or perform ﬁne-tuning during testing,
while it performs better on standard benchmarks. In this
work, we choose Classiﬁer-Baseline as the representative of
whole-classiﬁcation models for few-shot learning. For simplicity and clarity, we do not introduce additional complex
techniques for this whole-classiﬁcation training.
3.3. Meta-Baseline
Figure 1 visualizes the Meta-Baseline. The ﬁrst stage
is the classiﬁcation training stage, it trains a Classiﬁer-
Baseline, i.e. training a classiﬁer on all bases classes and
remove its last FC layer to get fθ. The second stage is the
meta-learning stage, which optimizes the model on the evaluation metric of Classiﬁer-Baseline. Speciﬁcally, given the
classiﬁcation-trained feature encoder fθ, it samples N-way
K-shot tasks (with N × Q query samples) from training
samples in base classes. To compute the loss for each task,
in support-set it computes the centroids of N classes de-
ﬁned in Equation 1, which are then used to compute the
predicted probability distribution for each sample in queryset deﬁned in Equation 2. The loss is a cross-entropy loss
computed from p and the labels of the samples in the queryset. During training, each training batch can contain several
tasks and the average loss is computed.
Since cosine similarity has the value range of [−1, 1],
when it is used to compute the logits, it can be helpful to
Meta-Baseline
Classifier-Baseline / Meta-Baseline
evaluation
support-set
Classifier-Baseline
classification
on base classes
Meta-Learning Stage
Classification Training Stage
representation
Figure 1: Classiﬁer-Baseline and Meta-Baseline. Classiﬁer-Baseline is to train a classiﬁcation model on all base classes
and remove its last FC layer to get the encoder fθ. Given a few-shot task, it computes the average feature for samples of each
class in support-set, then it classiﬁes a sample in query-set by nearest-centroid with cosine similarity as distance. In Meta-
Baseline, it further optimizes a converged Classiﬁer-Baseline on its evaluation metric, and an additional learnable scalar τ is
introduced to scale cosine similarity.
scale the value before applying Softmax function during
training (a common practice in recent work ). We
multiply the cosine similarity by a learnable scalar τ, and
the probability prediction in training becomes:
p(y = c | x) =
 τ · ⟨fθ(x), wc⟩
 τ · ⟨fθ(x), wc′⟩
In this work, the main purpose of Meta-Baseline is to
investigate whether the meta-learning objective is still effective over a whole-classiﬁcation model.
As a method,
while every component in Meta-Baseline has been proposed
in prior works, we ﬁnd none of the prior works studies them
as a whole. Therefore, Meta-Baseline should also be an important baseline that has been overlooked.
4. Results on Standard Benchmarks
4.1. Datasets
The miniImageNet dataset is a common benchmark
for few-shot learning. It contains 100 classes sampled from
ILSVRC-2012 , which are then randomly split to 64,
16, 20 classes as training, validation, and testing set respectively. Each class contains 600 images of size 84 × 84.
The tieredImageNet dataset is another common
benchmark proposed more recently with much larger scale.
It is a subset of ILSVRC-2012, containing 608 classes from
34 super-categories, which are then split into 20, 6, 8 supercategories, resulting in 351, 97, 160 classes as training, validation, testing set respectively. The image size is 84 × 84.
This setting is more challenging since base classes and
novel classes come from different super-categories.
In addition to the datasets above, we evaluate our model
on ImageNet-800, which is derived from ILSVRC-2012 1K
classes by randomly splitting 800 classes as base classes
and 200 classes as novel classes. The base classes contain
the images from the original training set, the novel classes
contain the images from the original validation set. This
larger dataset aims at making the training setting standard
as the ImageNet 1K classiﬁcation task .
4.2. Implementation details
We use ResNet-12 that follows the most of recent
works on miniImageNet and tieredImageNet, and we use ResNet-18, ResNet-50 on ImageNet-
800. For the classiﬁcation training stage, we use the SGD
optimizer with momentum 0.9, the learning rate starts from
0.1 and the decay factor is 0.1. On miniImageNet, we train
100 epochs with batch size 128 on 4 GPUs, the learning
rate decays at epoch 90. On tieredImageNet, we train 120
epochs with batch size 512 on 4 GPUs, the learning rate de-
Matching Networks 
43.56 ± 0.84
55.31 ± 0.73
Prototypical Networks 
48.70 ± 1.84
63.11 ± 0.92
Prototypical Networks (re-implement)
53.81 ± 0.23
75.68 ± 0.17
Activation to Parameter 
59.60 ± 0.41
73.74 ± 0.19
61.76 ± 0.08
77.59 ± 0.12
Baseline++ 
51.87 ± 0.77
75.68 ± 0.63
SNAIL 
55.71 ± 0.99
68.88 ± 0.92
AdaResNet 
56.88 ± 0.62
71.94 ± 0.57
TADAM 
58.50 ± 0.30
76.70 ± 0.30
61.20 ± 1.80
75.50 ± 0.80
MetaOptNet 
62.64 ± 0.61
78.63 ± 0.46
SLA-AG 
62.93 ± 0.63
79.63 ± 0.47
ProtoNets + TRAML 
60.31 ± 0.48
77.94 ± 0.57
ConstellationNet 
64.89 ± 0.23
79.95 ± 0.17
Classiﬁer-Baseline (ours)
58.91 ± 0.23
77.76 ± 0.17
Meta-Baseline (ours)
63.17 ± 0.23
79.26 ± 0.17
Table 2: Comparison to prior works on miniImageNet. Average 5-way accuracy (%) with 95% conﬁdence interval.
51.67 ± 1.81
70.30 ± 1.75
Prototypical Networks* 
53.31 ± 0.89
72.69 ± 0.74
Relation Networks* 
54.48 ± 0.93
71.32 ± 0.78
66.33 ± 0.05
81.44 ± 0.09
MetaOptNet 
65.99 ± 0.72
81.56 ± 0.53
Classiﬁer-Baseline (ours)
68.07 ± 0.26
83.74 ± 0.18
Meta-Baseline (ours)
68.62 ± 0.27
83.74 ± 0.18
Table 3: Comparison to prior works on tieredImageNet. Average 5-way accuracy (%) with 95% conﬁdence interval.
cays at epoch 40 and 80. On ImageNet-800, we train 90
epochs with batch size 256 on 8 GPUs, the learning rate decays at epoch 30 and 60. The weight decay is 0.0005 for
ResNet-12 and 0.0001 for ResNet-18 or ResNet-50. Standard data augmentation is applied, including random resized crop and horizontal ﬂip. For meta-learning stage, we
use the SGD optimizer with momentum 0.9. The learning
rate is ﬁxed as 0.001. The batch size is 4, i.e. each training
batch contains 4 few-shot tasks to compute the average loss.
The cosine scaling parameter τ is initialized as 10.
We also apply consistent sampling for evaluating the performance. For the novel class split in a dataset, the sampling
of testing few-shot tasks follows a deterministic order. Consistent sampling allows us to get a better model comparison
with the same number of sampled tasks. In the following
sections, when the conﬁdence interval is omitted in the table, it indicates that a ﬁxed set of 800 testing tasks are sampled for estimating the performance.
4.3. Results
Following the standard-setting, we conduct experiments
on miniImageNet and tieredImageNet, the results are shown
in Table 2 and 3 respectively. To get a fair comparison to
prior works, we perform model selection according to the
validation set. On both datasets, we observe that the Meta-
Baseline achieves competitive performance to state-of-theart methods. We highlight that many methods for comparison introduce more parameters and architecture designs
(e.g. self-attention in ), while Meta-Baseline has the
minimum parameters and the simplest design. We also notice that the simple Classiﬁer-Baseline can achieve competitive performance when compared to meta-learning methods, especially in 5-shot tasks. We observe that the metalearning stage consistently improves Classiﬁer-Baseline on
Classiﬁer-Baseline (ours)
83.51 ± 0.22
94.82 ± 0.10
Meta-Baseline (ours)
86.39 ± 0.22
94.82 ± 0.10
Classiﬁer-Baseline (ours)
86.07 ± 0.21
96.14 ± 0.08
Meta-Baseline (ours)
89.70 ± 0.19
96.14 ± 0.08
Table 4: Results on ImageNet-800. Average 5-way accuracy (%) is reported with 95% conﬁdence interval.
5-way acc (%)
miniImageNet, 1-shot
miniImageNet, 5-shot
tieredImageNet, 1-shot
tieredImageNet, 5-shot
Meta-Baseline, Meta-Learning Stage (ResNet-12)
base class generalization
novel class generalization
Figure 2: Objective discrepancy of meta-learning on miniImageNet and tieredImageNet. Each epoch contains 200
training batches. Average 5-way accuracy (%) is reported.
5-way acc (%)
ResNet-50, 1-shot
ResNet-50, 5-shot
base class generalization
novel class generalization
Figure 3: Objective discrepancy of meta-learning on
ImageNet-800. Each epoch contains 500 training batches.
Average 5-way accuracy (%) is reported.
miniImageNet. Compared to miniImageNet, we ﬁnd that
the gap between Meta-Baseline and Classiﬁer-Baseline is
smaller on tieredImageNet, and the meta-learning stage
does not improve 5-shot in this case.
We further evaluate our methods on the larger dataset
ImageNet-800.
In this larger-scale experiment, we ﬁnd
freezing the Batch Normalization layer (set to eval
mode) is beneﬁcial.
The results are shown in Table 4.
From the results, we observe that in this large dataset Meta-
Baseline improves Classiﬁer-Baseline in 1-shot, while it is
not improving the performance in 5-shot.
5. Observations and Hypothesis
5.1. Objective discrepancy in meta-learning
improvements
meta-learning
Classiﬁer-Baseline, we observe the test performance drops
during the meta-learning stage. While a common assumption for this phenomenon is overﬁtting, we observe that this
issue seems not to be mitigated on larger datasets. To further
locate the issue, we propose to evaluate base class generalization and novel class generalization. Base class generalization is measured by sampling tasks from unseen images
in base classes, while novel class generalization refers to the
performance of few-shot tasks sampled from novel classes.
The base class generalization is the generalization in the input distribution for which the model is trained, it decouples
the commonly deﬁned generalization and class-level transfer performance, which helps for locating the reason for the
performance drop.
Figure 2 and 3 demonstrate the meta-learning stage of
Meta-Baseline on different datasets. We ﬁnd that during the
meta-learning stage, when the base class generalization is
increasing, the novel class generalization can be decreasing
instead. This fact indicates that over a converged wholeclassiﬁcation model, the meta-learning objective itself, i.e.
making the embedding generalize better in few-shot tasks
from base classes, can have a negative effect on the performance of few-shot tasks from novel classes. It also gives a
mini-tiered
mini-shufﬂed
full-tiered
full-shufﬂed
Classiﬁer-Baseline
Meta-Baseline
Classiﬁer-Baseline
Meta-Baseline
Table 5: Effect of dataset properties. Average 5-way accuracy (%), with ResNet-12.
Novel gen.
Table 6: Comparison on Meta-Baseline training from
scratch. Average 5-way accuracy (%), with ResNet-12 on
miniImageNet. ClsTr: classiﬁcation training stage.
Classiﬁer-Baseline
Classiﬁer-Baseline (Euc.)
Meta-Baseline
Meta-Baseline (Euc.)
Table 7: Importance of inheriting a good metric. Average 5-way accuracy (%), with ResNet-12 on miniImageNet.
possible explanation for why such phenomenon is not mitigated on larger datasets, as this is not sample-level over-
ﬁtting, but class-level overﬁtting, which is caused by the
objective discrepancy that the underlying training class distribution is different from testing class distribution.
This observation suggests that we may reconsider the
motivation of the meta-learning framework for few-shot
learning. In some settings, optimizing towards the training objective with a consistent form as the testing objective
(except the inevitable class difference) may have an even
negative effect. It is also likely that the whole-classiﬁcation
learns the embedding with stronger class transferability, and
meta-learning makes the model perform better at N-way
K-shot tasks but tends to lose the class transferability.
5.2. Effect of whole-classiﬁcation training before
meta-learning
According to our hypothesis, the whole-classiﬁcation
pre-trained model has provided extra class transferability for the meta-learning model, therefore, it is natural to
compare Meta-Baseline with and without the classiﬁcation
training stage. The results are shown in Table 6. We observe
that Meta-Baseline trained without classiﬁcation training
stage can actually achieve higher base class generalization,
but its novel class generalization is much lower when compared to Meta-Baseline with whole-classiﬁcation training.
These results support our hypothesis, that the wholeclassiﬁcation training provides the embedding with stronger
class transferability, which signiﬁcantly helps novel class
generalization.
Interestingly, TADAM ﬁnds that
co-training the meta-learning objective with a wholeclassiﬁcation task is beneﬁcial, which may be potentially
related to our hypothesis. While our results show it is likely
that the key effect of the whole-classiﬁcation objective is
improving the class transferability, it also indicates a potential trade-off that the whole-classiﬁcation objective can
have a negative effect on base class generalization.
5.3. What makes Meta-Baseline a strong baseline?
As a method with a similar objective as ProtoNet ,
Meta-Baseline achieves nearly 10% higher accuracy on 1shot in Table 2. The observations and hypothesis in previous sections potentially explain its strength, as it starts with
the embedding of a whole-classiﬁcation model which has
stronger class transferability.
We perform further experiments, that in Meta-Baseline
(with classiﬁcation training stage) we replace the cosine
distance with the squared Euclidean distance proposed in
ProtoNet . To get a fair comparison, we also include
the learnable scalar τ with a proper initialization value 0.1.
The results are shown in Table 7.
While ProtoNet 
ﬁnds that squared Euclidean distance (as a Bregman divergence) works better than cosine distance when performing meta-learning from scratch, here we start meta-learning
from Classiﬁer-Baseline and we observe that cosine similarity works much better. A potential reason is that, as
shown in Table 7, cosine nearest-centroid works much better than nearest-centroid with squared Euclidean distance in
Classiﬁer-Baseline (note that this is just the evaluation metric and has no changes in training). Inheriting a good metric
for Classiﬁer-Baseline might be the key that makes Meta-
Baseline strong. According to our hypothesis, the embedding from the whole-classiﬁcation model has strong class
transferability, inheriting a good metric potentially minimizes the future modiﬁcations on the embedding from the
whole-classiﬁcation model, thus it can keep the class transferability better and achieve higher performance.
5.4. Effect of dataset properties
We construct four variants from the tieredImageNet
dataset. Speciﬁcally, full-tiered refers to the original tiered-
ImageNet, full-shufﬂed is constructed by randomly shuf-
ﬂing the classes in tieredImageNet and re-splitting the
classes into training, validation, and test set.
The minitiered and mini-shufﬂed datasets are constructed from fulltiered and full-shufﬂed respectively, their training set is constructed by randomly selecting 64 classes with 600 images
from each class in the full training set, while the validation
set and the test set remain unchanged. Since tieredImageNet
separates training classes and testing classes into different
super categories, shufﬂing these classes will mix the classes
in different super categories together and make the distribution of base classes and novel classes closer.
Our previous experiments show that base class generalization is always improving, if novel classes are covered
by the distribution of base classes, the novel class generalization should also keep increasing. From Table 5, we
can see that from mini-tiered to mini-shufﬂed, and from
full-tiered to full-shufﬂed, the improvement achieved by the
meta-learning stage gets signiﬁcantly larger, which consistently supports our hypothesis. Therefore, our results indicate it is likely that meta-learning is mostly effective over
whole-classiﬁcation training when novel classes are similar
to base classes.
We also observe that other factors may affect the improvement of meta-learning. From mini-tiered to full-tiered
and from mini-shufﬂed to full-shufﬂed, when the dataset
gets larger the improvements become less. A potential hypothesis could be that the class transferability advantage of
whole-classiﬁcation training becomes more obvious when
trained on large datasets. From the results of our experiments in Table 2, 3, 4, 5, we observe that the improvement
of the meta-learning stage in 5-shot is less than 1-shot. We
hypothesize this is because when there are more shots, taking average embedding becomes a more reasonable choice
to estimate the class center in Classiﬁer-Baseline, therefore
the advantage of meta-learning becomes less.
5.5. The trade-off between meta-learning and
whole-classiﬁcation
All of the experiments in previous sections support a key
hypothesis, that there exists a trade-off: the meta-learning
objective learns better embedding for N-way K-shot tasks
(in the same distribution), while the whole-classiﬁcation objective learns embedding with stronger class transferability. Optimizing towards one objective may hurt the strength
of another objective. With this hypothesis, Meta-Baseline
balances this trade-off by choosing to calibrate the wholeclassiﬁcation embedding with meta-learning and inherit the
metric with high initial performance.
The discrepancy between base class generalization and
novel class generalization also considers the effectiveness
of meta-learning and whole-classiﬁcation from the perspective of datasets. Speciﬁcally, when novel classes are similar
enough to base classes or the base classes are sufﬁcient to
cover the distribution of novel classes, novel class generalization should converge to base class generalization. In
practice, this can be potentially achieved by collecting base
classes that are similar to the target novel classes. In this
case, it may be possible that the novel meta-learning algorithms outperform the whole-classiﬁcation baselines again.
6. Additional Results on Meta-Dataset
Meta-Dataset is a new benchmark proposed for fewshot learning, it consists of diverse datasets for training and
evaluation. They also propose to generate few-shot tasks
with a variable number of ways and shots, for having a
setting closer to the real world. We follow the setting in
Meta-Dataset and use ResNet-18 as the backbone, with
the original image size of 126×126, which is resized to be
128×128 before feeding into the network. For the classiﬁcation training stage, we apply the training setting similar to
our setting in ImageNet-800. For the meta-learning stage,
the model is trained for 5000 iterations with one task in each
iteration.
The left side of Table 8 demonstrates the models trained
with samples in ILSVRC-2012 only. We observe that the
Meta-Baseline does not signiﬁcantly improve Classiﬁer-
Baseline under this setting in our experiments, possibly due
to the average number of shots are high.
The right side of Table 8 shows the results when the
models are trained on all datasets, except Trafﬁc Signs and
MSCOCO which have no training samples. The Classiﬁer-
Baseline is trained as a multi-dataset classiﬁer, i.e. an encoder together with multiple FC layers over the encoded
feature to output the logits for different datasets. The classiﬁcation training stage has the same number of iterations
as training on ILSVRC only, to mimic the ILSVRC training, a batch has 0.5 probability to be from ILSVRC and 0.5
probability to be uniformly sampled from one of the other
datasets. For Classiﬁer-Baseline, comparing to the results
on the left side of Table 8, we observe that while the performance on ILSVRC is worse, the performances on other
datasets are mostly improved due to having their samples in
training. It can be also noticed that the cases where Meta-
Baseline improves Classiﬁer-Baseline are mostly on the
Trained on ILSVRC
Trained on all datasets
fo-Proto-MAML
Classiﬁer/Meta
fo-Proto-MAML
Quick Draw
VGG Flower
Trafﬁc Signs
Table 8: Additional results on Meta-Dataset. Average accuracy (%), with variable number of ways and shots. The fo-Proto-
MAML method is from Meta-Dataset , Classiﬁer and Meta refers to Classiﬁer-Baseline and Meta-Baseline respectively,
1000 tasks are sampled for evaluating Classiﬁer or Meta. Note that Trafﬁc Signs and MSCOCO have no training set.
datasets which are “less relevant” to ILSVRC (the dataset
“relevance” could be shown in Dvornik et al. ). A potential reason is that the multi-dataset classiﬁcation training stage samples ILSVRC with 0.5 probability, similar
to ILSVRC training, the meta-learning stage is hard to
improve on ILSVRC, therefore those datasets relevant to
ILSVRC will have similar properties so that it is hard to
improve on them.
7. Conclusion and Discussion
In this work, we presented a simple Meta-Baseline that
has been overlooked for few-shot learning. Without any additional parameters or complex design choices, it is competitive to state-of-the-art methods on standard benchmarks.
Our experiments indicate that there might be an objective discrepancy in the meta-learning framework for fewshot learning, i.e. a meta-learning model generalizing better
on unseen tasks from base classes might have worse performance on tasks from novel classes. This provides a possible
explanation that why some complex meta-learning methods
could not get signiﬁcantly better performance than simple
whole-classiﬁcation. While most recent works focus on improving the meta-learning structures, many of them did not
explicitly address the issue of class transferability. Our observations suggest that the objective discrepancy might be a
potential key challenge to tackle.
While many novel meta-learning algorithms are proposed and some recent works report that simple wholeclassiﬁcation training is good enough for few-shot learning, we show that meta-learning is still effective over
whole-classiﬁcation models. We observe a potential tradeoff between the objectives of meta-learning and wholeclassiﬁcation.
From the perspective of datasets, we
demonstrate how the preference between meta-learning and
whole-classiﬁcation changes according to class similarity
and other factors, indicating that these factors may need
more attention for model comparisons in future work.
Acknowledgements. This work was supported, in part, by grants from
DARPA LwLL, NSF 1730158 CI-New: Cognitive Hardware and Software
Ecosystem Community Infrastructure (CHASE-CI), NSF ACI-1541349
CC*DNI Paciﬁc Research Platform, and gifts from Qualcomm, TuSimple and Picsart. Prof. Darrell was supported, in part, by DoD including
DARPA’s XAI, LwLL, and/or SemaFor programs, as well as BAIR’s industrial alliance programs. We thank Hang Gao for the helpful discussions.