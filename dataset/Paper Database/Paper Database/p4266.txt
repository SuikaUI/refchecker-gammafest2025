The MVGC multivariate Granger causality toolbox: a new approach to
Granger-causal inference
Lionel Barnett, Anil Seth
Publication date
15-02-2014
This work is made available under the Copyright not evaluated licence and should only be used in accordance
with that licence. For more information on the specific terms, consult the repository record for this item.
Document Version
Submitted version
Citation for this work (American Psychological Association 7th edition)
Barnett, L., & Seth, A. . The MVGC multivariate Granger causality toolbox: a new approach to Grangercausal inference (Version 1). University of Sussex.
 
Journal of Neuroscience Methods
Link to external publisher version
 
Copyright and reuse:
This work was downloaded from Sussex Research Open (SRO). This document is made available in line with publisher policy
and may differ from the published version. Please cite the published version where possible. Copyright and all moral rights to the
version of the paper presented here belong to the individual author(s) and/or other copyright owners unless otherwise stated. For
more information on this work, SRO or to report an issue, you can contact the repository administrators at .
Discover more of the University’s research at 
The MVGC Multivariate Granger Causality Toolbox:
A New Approach to Granger-causal Inference
Lionel Barnett∗, Anil K. Seth
Sackler Centre for Consciousness Science and School of Informatics,
University of Sussex, Brighton BN1 9QJ, UK
Background:
Wiener-Granger causality (“G-causality”) is a statistical notion of causality applicable to time series data, whereby
cause precedes, and helps predict, eﬀect. It is deﬁned in both time and frequency domains, and allows for the conditioning out of
common causal inﬂuences. Originally developed in the context of econometric theory, it has since achieved broad application in
the neurosciences and beyond. Prediction in the G-causality formalism is based on VAR (Vector AutoRegressive) modelling.
New Method:
The MVGC Matlab c⃝Toolbox approach to G-causal inference is based on multiple equivalent representations
of a VAR model by (i) regression parameters, (ii) the autocovariance sequence and (iii) the cross-power spectral density of the
underlying process. It features a variety of algorithms for moving between these representations, enabling selection of the most
suitable algorithms with regard to computational eﬃciency and numerical accuracy.
In this paper we explain the theoretical basis, computational strategy and application to empirical G-causal inference of
the MVGC Toolbox. We also show via numerical simulations the advantages of our Toolbox over previous methods in terms of
computational accuracy and statistical inference.
Comparison with Existing Method(s):
The standard method of computing G-causality involves estimation of parameters for both
a full and a nested (reduced) VAR model. The MVGC approach, by contrast, avoids explicit estimation of the reduced model, thus
eliminating a source of estimation error and improving statistical power, and in addition facilitates fast and accurate estimation of
the computationally awkward case of conditional G-causality in the frequency domain.
Conclusions:
The MVGC Toolbox implements a ﬂexible, powerful and eﬃcient approach to G-causal inference.
Keywords: Granger causality, vector autoregressive modelling, time series analysis
1. Introduction
Wiener-Granger causality (G-causality) is an increasingly popular method for
identifying “causal” connectivity in neural time series data
 .
It can be traced conceptually to
Wiener and was operationalised by Granger in
terms of linear autoregressive modelling of stochastic processes
 .
G-causality is based on predictability and
precedence. Put simply, a variable X is said to G-cause a variable Y if the past of X contains information that helps predict
the future of Y over and above information already in the past
of Y. Importantly, G-causality is a measure of directed functional connectivity in terms of providing a statistical description of observed responses. In contrast, methods for identifying
∗Corresponding author
Email addresses: (Lionel Barnett),
 (Anil K. Seth)
eﬀective connectivity aim to elucidate the “the simplest possible circuit diagram explaining observed responses” from
time series data, both unconditional and conditional, in the time
and frequency domains. It supersedes and extends the GCCA
(Granger Causal Connectivity Analysis) Toolbox .
Based on advanced VAR (Vector AutoRegressive) model theory , it is designed for computational eﬃciency and accuracy. Improving upon standard
approaches to G-causal inference, it avoids separate full and reduced regressions (see Section 2), thus eliminating a common
source of statistical inaccuracy and computational ineﬃciency.
It also facilitates estimation of fully multivariate conditional Gcausality in the frequency domain.
The MVGC toolbox has been designed with application to
empirical neuroscience data in mind. Several issues commonly
 
October 28, 2013
faced in applying G-causal inference to such data are discussed
later in the paper (Section 4). However, the software is not restricted to this application domain. G-causal inference is a very
general method which has been productively applied in many
areas, though caution is always needed in ensuring that the
data satisfy the assumptions underpinning the method (see Section 2.1 and also Section 3.3). Thus, while the MVGC software
is designed to be intuitive and straightforward to use, to get the
most out of it some understanding of the theoretical basis of Gcausal inference, as well as approaches to its numerical computation, is recommended. To facilitate this, this paper presents
the conceptual and theoretical basis of G-causality (substantially extending previous presentations), and details the computational implementation of the MVGC toolbox. Although not
intended as a user guide—the integrated help system and walkthrough demonstration scripts fulﬁl that purpose—Section 3
provides a helpful overview of the toolbox rationale and design principles, Section 3.1 explains the MVGC computational
strategy while Appendix A.1 and Appendix A.2 detail the core
computational algorithms.
2. G-causality: theory, estimation and inference
Assume two jointly distributed vector-valued stochastic processes (“variables”) X = X1, X2, . . ., Y = Y1, Y2, . . .. We say
that Y does not G-cause X if and only if X, conditional on its
own past, is independent of the past of Y; intuitively, past values
of Y yield no information about the current value of X beyond
information already contained in the past of X itself. If, conversely, the past of Y does convey information about the future
of X above and beyond all information contained in the past
of X then we say that Y G-causes X. Much has been made
of the fact that this notion of “causality” does not necessarily
tally with more conventional (and perhaps more physically intuitive) notions (note, for example, that the
processes must be strictly non-deterministic for the above deﬁnition even to make sense). We do not engage this debate here;
throughout this paper the term “causal” is used exclusively in
the Wiener-Granger sense just described.
Testing for Granger (non-)causality requires establishing—in
a statistical sense—a conditional (non-)dependency. This may
seem to invite an information-theoretic approach, since mutual information is a natural measure of statistical dependence.
However in empirical settings a drawback with informationtheoretic quantities is the diﬃculty of estimation in sample
and lack of known theoretical distributions for informationtheoretic estimators, complicating statistical inference . An alternative is therefore to
invoke a model-based or parametric approach for which conditional dependency estimators are more eﬃcient and (preferably)
have known sampling distributions, thus facilitating inference.
In fact, G-causality is frequently identiﬁed with a model-based
viewpoint, which may also yield intuitive predictive interpretations of G-causality; for predictive models, the statement “X,
conditional on its own past, is independent of the past of Y”
may be reframed as “the past of Y does not help predict X beyond the degree to which X may be predicted by its own past”.
The most common operationalisation of G-causality, and
the one on which the MVGC Toolbox is based, utilises VAR
modelling of time series data.
A multivariate time series
u1, u2, . . . , um, where for each time t ut is a real-valued ndimensional (column) vector with components u1t, u2t, . . . , unt,
is considered as a realisation of length m of a discrete-time stationary1 vector stochastic process U1, U2, . . . (the “universe of
variables”). A pth order vector autoregressive model for the
process—a VAR(p)—takes the form2
Ak · Ut−k + εt
Here p is the model order, which may be inﬁnite. The n × n
real-valued matrices Ak are the regression coeﬃcients, and the
n-dimensional stochastic process εt the residuals, which are independently and identically distributed (iid) and serially uncorrelated; that is, they constitute a white noise process. The parameters of the model are the coeﬃcients Ak and the n×n residuals covariance matrix Σ ≡cov(εt) which, by stationarity, does
not depend on the time t. Interpreted as a predictive model, (1)
thus models the value of the process at current time t in terms of
its past values at times t−1, . . . , t−p. The regression coeﬃcients
represent the predictable structure of the data, the residuals the
unpredictable.
The determination of a VAR model (1) should not be taken
to imply that the time series data modelled by the stochastic
process Ut was actually generated by a linear autoregressive
scheme. In comparison to eﬀective connectivity techniques like
dynamic causal modelling [DCM] , which
make very explicit assumptions about the generative mechanism underlying the observed data , the
VAR models underlying G-causality are “generic”, in the sense
that they make no assumptions about the mechanism that produced the data, beyond that a VAR model actually exists. Standard theory yields that a rather general class
of covariance-stationary multivariate process—including many
nonlinear processes—may be modelled as VARs, albeit of theoretically inﬁnite order (see also the next Section). Note that this
belies a common misapprehension that VAR-based G-causal
analysis is restricted to linear processes. In practice, given empirical time-series data, a ﬁnite model order p is selected on theoretical principles, suﬃciently high so as to capture predictable
variation but not so high as to overﬁt unpredictable variation in
the data (Section 2.4).
1G-causal inference is not necessarily limited to stationary processes. Under
some circumstances—notably for multi-trial data or for near-stationary data
that may be “windowed”—the stationarity requirement may be relaxed. This is
addressed later on.
2Throughout, bold symbols represent vector quantities and normal typeface
represents scalar quantities. Capitalised symbols represent random variables
or matrices, according to context. Thus we write Ut for the random vector
representing the value of the process U1, U2, . . . at time t, u1, . . . , um for a realisation of the process of length m, uit for the ith component of ut, etc. A
superscript ⊺denotes matrix transpose, and a superscript ∗denotes (complex)
matrix conjugate-transpose. |M| denotes the determinant of the square matrix
2.1. VAR process theory
The MVGC toolbox exploits some advanced aspects of VAR
process theory, which we outline here.
For valid G-causality analysis, the VAR coeﬃcients in
(1) must be square summable and stable .
Square summability requires that Pp
k=1 ∥Ak∥2 < ∞which means intuitively that the
coeﬃcients do not “blow up” even for inﬁnite model order p
(for ﬁnite p square summability is satisﬁed trivially.) Stability
means that the VAR coeﬃcients deﬁne a covariance-stationary
process. This is related to the the characteristic polynomial for
the coeﬃcients sequence Ak, which is:
for the variable z deﬁned in the complex plane C. By standard theory the coeﬃcients Ak deﬁne a stable VAR iﬀthe characteristic polynomial is invertible on the
unit disc |z| ≤1 in the complex plane. Deﬁning the spectral
radius of the VAR as
ϕA(z)=0{|z|−1}
an equivalent condition for stability—i.e. that (1) deﬁne a
covariance-stationary process—is
The autocovariance sequence Γk for a covariance-stationary
(not necessarily VAR) stochastic process Ut is deﬁned as the
sequence of n × n matrices
Γk ≡cov(Ut, Ut−k)
k = . . . , −2, −1, 0, 1, 2, . . .
Note that by stationarity the Γk do not depend on the time t, and
that Γ−k = Γk⊺for all lags k. For a VAR process of the form (1),
it may be shown [Appendix A.1, (A5)] that in the long run,
the autocovariance sequence decays exponentially with lag k,
the rate of decay being just the spectral radius ρ(A). Note, importantly, that this implies that if the autocovariance sequence
for a process does not decay (sub-)exponentially then the process may not be modelled as a VAR (see Section 3.3). This has
some importance for analysis of neural data, which we return
to in Section 4.2.
For a VAR process (1), the autocovariance sequence is related to the VAR parameters via the Yule-Walker equations 
AℓΓk−ℓ+δk0Σ
k = . . . , −2, −1, 0, 1, 2, . . . (6)
Numerically, there are eﬃcient algorithms available for solving
(6) for the Γk in terms of (Ak, Σ) [Appendix A.1, (A5)] and,
conversely, for (Ak, Σ) in terms of the Γk [Appendix A.1, (A6)].
The cross-power spectral density (CPSD) is deﬁned as the
(two-sided) Fourier transform of the autocovariance sequence3:
The S (λ) are n × n Hermitian positive semi-deﬁnite matrices.
The inverse Fourier transform recovers the autocovariance sequence:
S (λ)eiλk dλ
k = . . . , −2, −1, 0, 1, 2, . . .
Numerically, (7) and (8) may be eﬃciently computed by a (discrete) Fast Fourier Transform (FFT) and Inverse Fast Fourier
Transform (IFFT) respectively.
For a VAR process, the CPSD admits a unique spectral factorisation 
S (λ) = H(λ)ΣH(λ)∗
where the transfer function H(λ) is deﬁned as the inverse matrix
of the Fourier transform of the regression coeﬃcients:
I −
While there is no known closed-form solution of (9) for H(λ), Σ
in terms of S (λ), a classical result 
states that, given a CPSD S (λ) of a VAR process, a unique solution exists4 and may be computed numerically [Appendix A.1,
(A7)]. According to (10) the VAR coeﬃcients Ak may then be
recovered from H(λ) by a matrix inversion and inverse Fourier
transform.
A further technical condition for valid G-causality analysis
is that the CPSD be uniformly bounded away from zero almost
everywhere . Explicitly, there must exist
a c > 0 such that for almost all 0 ≤λ ≤2π
where for A, B square matrices, A ⃝
≤B denotes that B −A
is positive semideﬁnite. Importantly, condition (11) guarantees square summability of the regression coeﬃcients .
Equations (6-10) relate the VAR parameters (Ak, Σ), the autocovariance sequence Γk and the CPSD S (λ). Importantly, these
three mathematical objects specify entirely equivalent representations for a VAR and any of them may be estimated from empirical time series data. As we shall see (Section 3), the MVGC
toolbox makes extensive use of these equivalences to furnish
accurate and eﬃcient computational pathways for the calculation of G-causality in the time and frequency domains.
3For simplicity, all spectral quantities are deﬁned on the normalised frequency range 0 ≤λ ≤2π. In an empirical scenario—and indeed in the MVGC
implementation—the natural frequency range of deﬁnition is 0 ≤λ ≤f where
f is the sampling frequency in Hertz (Hz). In fact, to take advantage of inherent symmetries, the MVGC computes all spectral entities only up to the Nyqvist
frequency f/2.
4More precisely, given S (λ) there is a unique holomorphic matrix function
˜H(z) on the complex plane with ˜H(0) = I, and a unique symmetric positive
deﬁnite matrix Σ, such that (9) holds for H(λ) ≡˜H(e−iλ).
2.2. Unconditional G-causality in the time domain
In its simplest (unconditional, time-domain) form, Gcausality is motivated as follows: suppose that Ut is split into
two jointly-distributed (i.e. inter-dependent) multivariate processes:
Under a predictive interpretation (cf. Section 2), the G-causality
from Y to X, written FY→X, stands to quantify the “degree to
which the past of Y helps predict X, over and above the degree to which X is already predicted by its own past”. In the
VAR formulation, this notion is operationalised as follows: the
VAR(p) (1) decomposes as
and the residuals covariance matrix as
The x-component of the regression (13) is
Axx,k · Xt−k +
Axy,k · Yt−k + εx,t
from which we see that the dependence of X on the past of Y,
given its own past, is encapsulated in the coeﬃcients Axy,k; in
particular, there is no conditional dependence of X on the past
of Y iﬀAxy,1 = Axy,2 = . . . = Axy,p = 0. This leads to consideration of the reduced (or restricted) regression—as opposed to
the full regression (15)—given by omitting the “historic” (past)
Y dependency:
xx,k · Xt−k + ε′
so that now X is predicted by its own past only. Here A′
the reduced regression coeﬃcients and ε′
x,t the reduced regression residuals, with covariance matrix Σ′
then, stands to quantify the degree to which the full regression
(15) represents a “better” model of the data than the restricted
regression (16). Maximum likelihood (ML) theory furnishes a natural framework for the analysis of parametric data modelling. In this framework, an appropriate comparative measure for nested models of the form (15, 16) is the
likelihood ratio statistic or, for its more convenient statistical
properties, its logarithm5. Technically, it is a test statistic for
the null hypothesis of zero causality
H0 : Axy,1 = Axy,2 = . . . = Axy,p = 0
5Alternative test statistics which appear in the G-causality literature are
the Wald or Lagrange multiplier estimators . An advantage
of the likelihood ratio is that is has pleasant invariance properties under a
broad class of transformations 
(Section 2.7), a natural spectral decomposition (Section 2.6)
and an information-theoretic interpretation as a transfer entropy . This motivates the deﬁnition of the G-causality statistic
as the appropriate log-likelihood ratio. Now standard ML theory tells us that the likelihood for a realisation of length m of a
VAR model of the form (1) is proportional to |Σ|−(m−p)/2, where
|Σ|, the generalised variance of the model ,
is the determinant of the residuals covariance matrix. Thus the
(unconditional, time-domain) G-causality from Y to X is de-
ﬁned to be the log-likelihood ratio
where Σxx = cov εx,t
are the residuals
covariance matrices of the VAR models (15) and (16) respectively. A predictive interpretation of (18) is that the generalised
variance of a regression model may be viewed as a measure
of model prediction error6. From this perspective, G-causality
(18) thus quantiﬁes the reduction in prediction error when the
past of the process Y is included in the explanatory variables of
a VAR model for X.
An important and frequently misunderstood point is that Gcausality is often perceived to lack quantitative meaning beyond
its interpretation as a hypothesis-test statistic appropriate only
for signiﬁcance testing (Section 2.5); that is, that its magnitude is essentially meaningless, and in particular should not
be compared with other G-causality statistics. However, Gcausality magnitudes have a natural interpretation in terms of
information-theoretic bits-per-unit-time. This is because, for
a large class of joint processes, G-causality and informationtheoretic transfer entropy are asymptotically
equivalent [in the Gaussian
case the equivalence is exact ]. Note that
transfer entropy is conceptually isomorphic to G-causality and
is often described as a measure of information ﬂow . Thus, G-causalities may be
meaningfully compared, and magnitudes cited, albeit with due
caution to statistical signiﬁcance (Section 2.5).
2.3. Conditional and pairwise-conditional G-causality
The unconditional G-causality statistic introduced above has
the undesirable characteristic that if there are joint (possibly
historical) dependencies between X and Y and a third set of
variables, Z say, then spurious causalities may be reported.
Thus, for instance, if there is no direct causal inﬂuence Y →X
but there are (possibly lagged) dependencies of X and Y on
Z then a spurious Y →X causality may be reported. These
spurious causalities may be eliminated by “conditioning out”
the common dependencies – provided they are available in the
data. If, however, there are dependencies on unknown (exogenous) or unrecorded (latent) variables, then it will in general be
impossible to eliminate entirely their potentially confounding
6It may seem intuitive to use the mean square error trace(Σ) (the “total error”) as a measure of prediction error. However, on the grounds of transformation invariance, frequency decomposition and information-theoretic interpretation, as well as for consistency with the ML formulation, the generalised
variance is more appropriate. See Barrett et al. for a fuller discussion.
eﬀect on causal inference, although attempts have been made
to mitigate their impact [e.g. “partial” G-causality (Guo et al.,
To illustrate the conditional case, suppose that the universe U
of known, recorded variables splits into three jointly-distributed
(i.e. inter-dependent) multivariate processes


and we wish to eliminate any joint eﬀect of Z on the inference
of the G-causality from Y to X. Again the VAR(p) (1) splits
analogously to (13) and we may consider the full and reduced
regressions
Axx,k · Xt−k +
Axy,k · Yt−k +
Axz,k · Zt−k + εx,t
xx,k · Xt−k +
xz,k · Zt−k + ε′
analogous to (15, 16), but with the conditioning variables Zt
included in both regressions. The null hypothesis to be tested is
still (17) and the causality Y →X conditioned on Z, which we
write F Y→X | Z, is again as in (18):
F Y→X | Z ≡ln
but now the inclusion of Z in both regressions accounts for its
joint eﬀect. F Y→X | Z may thus be read as “the degree to which
the past of Y helps predict X, over and above the degree to
which X is already predicted by its own past and the past of
Note that in this (and the previous) Section, the source, target and conditioning variables X, Y, Z may themselves be multivariate; that is, they may represent groups of variables. It is in
this sense that we use the term “multivariate” G-causality. Gcausality is thus able to account for group interactions. This is
signiﬁcant, since elements in a multivariate system may function cooperatively or competitively, or interact generally in a
more complex fashion than traditional bivariate analysis can accomodate .
A case of particular importance is that of pairwiseconditional G-causality. Given a universe U of variables comprising n (known, recorded) jointly-distributed univariate processes U1t, . . . , Unt, it is frequently of interest to estimate the
G-causalities between pairs of variables Ui, U j, i , j. The traditional bivariate pairwise G-causalities are the FU j→Ui. Since
these are prone to spurious eﬀects through joint dependencies
7Note that there are a number of errors in this paper: ﬁrstly, it is stated
erroneously that partial G-causality may be negative. Secondly, the sampling
distribution given is incorrect. See Roelstraete and Rosseel and also
Barrett et al. .
as described above, however, it is generally preferable to consider rather the pairwise-conditional causalities
Gi j(U) ≡F U j→Ui | U[ij]
where the subscript [ij] denotes omission of the ith and jth variables in the multivariate universe U. Thus, when considering
the causality U j →Ui the joint dependencies of all remaining
known variables are conditioned out8. The quantities Gi j(U),
i , j may be considered as a weighted directed graph, which
we shall sometimes refer to as the (G-)causal graph.
2.4. Estimation from time series data
So far we have discussed time series data in terms of abstract
stochastic processes; that is, we have assumed that our empirical time series data behaves like a realisation of some VAR
process. We now turn to the crucial issues of estimation, under this assumption, of F Y→X | Z from9 a numerical time series
u = u1, . . . , um. The ﬁrst stage is to determine an appropriate
model order for the regression (1). This may be achieved via
standard techniques such as the Akaike or Bayesian information criteria, or cross-validation .
The idea of model order selection is to balance the number of
parameters (in the VAR case this is determined by the maximum lag p) so as to achieve the best model ﬁt—perhaps in a
ML or error-minimisation sense—while at the same time avoiding overﬁtting a ﬁnite data sequence.
The next stage is to obtain estimates of the model parameters
which maximise the likelihood function for the respective VAR
models (equivalently, minimise model error). As mentioned
(Section 1), the MVGC toolbox obviates the need to to estimate the reduced model parameters separately from the data.
Here there is a choice of techniques yielding estimates asymptotically equivalent to the ML estimate, notably ordinary least
squares (OLS) and various multivariate extensions of Durbin recursion (frequently under the banner of
“LWR algorithms”) - see Appendix A.1,
(A2). Once we have estimates of all relevant parameters for
both the full and reduced models, the G-causality sample estimator bF Y→X | Z(u) is obtained via (22) with the residuals covariance matrices Σxx, Σ′
xx replaced by their respective estimators ˆΣxx(u), ˆΣ′
xx(u). Another route to estimation of causalities in
the time domain is via spectral G-causality (Section 2.6).
An important but rarely considered issue in G-causality
model estimation is that, given that the full process Ut is a
VAR (1), it is not clear that the subprocess Xt will always be
a well-deﬁned VAR. In fact condition (11) guarantees that it
is10. However, even if the full VAR (1) is of ﬁnite order, the
8Confusingly, the term “multivariate” G-causality is sometimes applied in
the literature to this case.
9This Section and the following applies equally to the unconditional case
(Section 2.2); we need simply take Z to be empty; i.e. of dimension 0.
10As noted by Geweke , since the full CPSD S (λ) is assumed to satisfy condition (11), so too does the CPSD S xx(λ) of the process
Xt alone, from which it follows that the coeﬃcients of the autoregression of Xt
are square summable, and the reduced VAR is thus well-deﬁned. See Geweke
 for full details.
reduced VAR (16) will generally be of inﬁnite order11. (Similar
remarks apply to the reduced regression (21).) Since under the
ML formalism the full and reduced model orders should be the
same, this implies that the appropriate (ﬁnite) empirical model
order p should really be estimated for the reduced, rather than
the full regression model. Failure to do so has potentially serious implications for G-causal inference (Section 2.5 - see also
Section 3.1.2). The MVGC toolbox overcomes this issue in a
natural way by only requiring estimation of the full regression
(Section 3.1).
2.5. Statistical inference
The next task is to establish the statistical signiﬁcance of
the estimated causality against the null hypothesis (17) of zero
causality, and, if desired, conﬁdence intervals for its magnitude.
As a likelihood ratio test, standard large-sample theory applies to the time-domain Gcausality estimator in both the conditional and unconditional
cases. If dim(X) = nx, dim(Y) = ny and dim(Z) = nz (with
nx+ny+nz = n) then the diﬀerence in the number of parameters
between the full model (20) and the nested reduced model (21)
is just d ≡pnxny. Thus under the null hypothesis (17) of zero
causality, (m −p) bF Y→X | Z(u), the G-causality estimator scaled
by sample size has an asymptotic χ2(d) distribution. Under the
alternative hypothesis the scaled estimator has an asymptotic
noncentral-χ2(d; ν) distribution, with noncentrality parameter
ν = (m −p) F Y→X | Z equal to the scaled actual causality (which
may, for the purpose of constructing conﬁdence intervals, be
replaced by its estimator).
In the case of a univariate causal target [i.e. dim(X) = nx = 1]
an alternative asymptotic sampling distribution is available for
the R2-like statistic exp(F Y→X | Z) −1: namely, under the null
hypothesis, exp(bF Y→X | Z(u))−1 scaled by d2/d1 has an asymptotic F(d1, d2) distribution where d1 = pny and d2 = m−p(n+1),
and under the alternative hypothesis an asymptotic noncentral-
F(d1, d2; ν) distribution where again ν is equal to the actual
scaled value of the causality. For small samples in particular,
the F-distribution may be preferable (it has a fatter tail than the
corresponding χ2 distribution). The MVGC toolbox makes both
tests available and defaults to the F-test if the target variable is
univariate.
Since G-causality is non-negative, bF Y→X | Z(u) will generally be positively biased, and under the alternative hypothesis
the non-central asymptotic distributions will be more accurate
the closer the actual causality is to zero . Some techniques for dealing with bias are discussed in
Section 3.4.
For very small samples the theoretical asymptotic distributions may not be suﬃciently accurate and nonparametric dataderived empirical sampling distributions may be preferable for
statistical inference. For this purpose the MVGC toolbox supplies permutation test and nonparametric bootstrap routines for signiﬁcance
11In fact if Ut is a ﬁnite-order VAR, then a subprocess Xt will in general be
a ﬁnite-order VARMA process.
testing and computation of conﬁdence intervals respectively;
the toolbox also features routines suitable for simulation of surrogate time series data [Appendix A.1, (A3)].
Finally, if multiple causalities are to be simultaneously considered, in view of the multiple null hypotheses we should take
into account family-wise error rates or false discovery rates
 ; again, this functionality is
available in the MVGC toolbox.
2.6. G-causality in the frequency domain
A powerful feature of G-causality is that it may be decomposed in a natural way by frequency . The
resulting spectral G-causality integrates to the time-domain
causality previously introduced, which may thus be considered
an average over all frequencies of the spectral causality. The
decomposition in the unconditional case is derived as follows:
a split of U into sub-processes X, Y as in (12) induces a decomposition
of the cross-power spectral density (Section 2.1) and a similar
decomposition for the transfer function H(λ). Now S xx(λ) is
just the CPSD of the sub-process X, and from (9) we may derive
S xx(λ) = Hxx(λ)ΣxxHxx(λ)∗+ 2 Re
Hxx(λ)ΣxyHxy(λ)∗o
+ Hxy(λ)ΣyyHxy(λ)∗
Following Geweke , we note that in the special case that
Σxy ≡0, which may always be eﬀected by a linear transformation of variables leaving FY→X invariant ,
(25) takes the simpler form
S xx(λ) = Hxx(λ)ΣxxHxx(λ)∗+ Hxy(λ)ΣyyHxy(λ)∗
whereby the CPSD of X splits into an “intrinsic” term and a
“causal” term. This motivates deﬁnition of the (unconditional)
spectral G-causality from Y to X as
fY→X(λ) ≡ln
S xx(λ) −Hxy(λ)ΣyyHxy(λ)∗
or, in terms of untransformed variables (i.e., where Σxy , 0),
fY→X(λ) ≡ln
S xx(λ) −Hxy(λ)Σy|xHxy(λ)∗
where the partial covariance matrix Σy|x is deﬁned by
Σy|x ≡Σyy −ΣyxΣ−1
Geweke then establishes the fundamental spectral decomposition of G-causality in the unconditional case12
fY→X(λ) dλ = FY→X
Ayy(λ) −ΣyxΣ−1
, 0 is satisﬁed on 0 < λ ≤2π; otherwise it
should be replaced by ≤. In practice, according to Geweke , the equality
condition is “almost always” satisﬁed.
so that time domain causality may be thought of as the average
over all frequencies of spectral causality.
Sample estimation of fY→X(λ) admits at least two possible
approaches. A straightforward procedure is to estimate parameters Ak, Σ for the VAR(p) (1) as before (after selecting a suitable
model order). The transfer function may then by calculated by
(fast) Fourier transform of the estimated regression coeﬃcients
according to (10), the CPSD from (9) and the partial covariance
matrix from (29). An estimate for the unconditional spectral
G-causality is then obtained by plugging the appropriate estimates into (28). Note that no reduced regression is required.
An alternative procedure is to estimate the CPSD S (λ) directly from the data - many standard
techniques are available to this end. Then, as mentioned previously (Section 2.1), H(λ) and Σ may be computed numerically
and an estimate for the spectral G-causality is again obtained
from (28). The MVGC toolbox facilitates both techniques; we
recommend the former due to improved numerical accuracy and
computational eﬃciency (Section 3.1 and Appendix A.1). As
regards statistical inference, in contrast to the time-domain case
there are (to our knowledge) no known (asymptotic) distributions for the sample distribution of ˆfY→X(λ) and nonparametric techniques are best deployed for signiﬁcance testing and derivation
of conﬁdence intervals13.
The conditional case is less straightforward. Suppose that U
splits into sub-processes X, Y, Z as per (19). We then consider
the reduced regression [cf. (21)]
to deﬁne the residuals X†, Z† as new variables.
 the identity
F Y→X | Z ≡FY⊕Z†→X†
is established, where Y ⊕Z†
, which expresses the conditional time-domain G-causality F Y→X | Z as an unconditional
causality in terms of the new variables X†, Y⊕Z†. Accordingly,
the spectral causality in the unconditional case is deﬁned as
f Y→X | Z(λ) ≡fY⊕Z†→X†(λ)
and the spectral decomposition
f Y→X | Z(λ) dλ = F Y→X | Z
again obtains.
To estimate ˆf Y→X | Z(λ) in sample, a separate reduced
regression—which, as has been pointed out (Chen et al.,
13Our unpublished simulation studies suggest that, in fact, the sampling distribution for ˆfY→X(λ) at any ﬁxed frequency λ is actually the same as for the
time-domain causality bFY→X, although we do not yet have a theoretical justiﬁcation for this claim. This appears also to hold in the conditional case.
may lead to substantial inaccuracies14—may be
avoided as follows: after estimation of VAR parameters for (1)
(or spectral estimation), it may be shown [Appendix A.1, (A11)
and (A12)] that the transformation of variables X, Z →X†, Z†
deﬁned by (31) induces a transformation of the autocovariance
sequence and CPSD which may be eﬀected computationally.
Then the conditional spectral causality may be calculated as for
the unconditional case via (33). Again, little is known of the
sampling distribution of f Y→X | Z(λ) so that nonparametric techniques must be used for statistical inference.
As in the time-domain case, pairwise-conditional spectral
causalities may be calculated as
gi j(U; λ) ≡f U j→Ui | U[ij](λ)
2.7. Filter invariance of G-causality
We have previously shown that multivariate G-causality, conditional and unconditional and in both time and frequency domains, is theoretically invariant under the application of (almost) arbitrary stable, invertible15 digital ﬁlters . Empirically, however, ﬁltering
of stationary time series data may severely compromise statistical inference of causalities estimated in sample. This is due
primarily to an increase in empirical VAR model order following ﬁltering. An important implication is that G-causality restricted to particular frequency bands cannot be measured by
ﬁrst ﬁltering the data within the desired bands. While the computed values may change following ﬁltering, this likely reﬂects
not the desired “band-limited” G-causality but rather spurious
values arising from inaccuracies in model ﬁtting and statistical
inference. The solution proposed in Barnett and Seth 
is to avoid ﬁltering (of stationary data - see below) altogether.
Then if spectral causalities are required, values outside the frequency range of interest may simply be ignored, while appropriate time-domain causalities may be obtained by averaging
spectral causalities (28,33) over the frequency range B of prior
interest [cf. (30,34)] to obtain band-limited G-causality:
fY→X(λ) dλ
F X→Y | Z(B) ≡
f Y→X | Z(λ) dλ
where µ(B) ≡
B dλ is the measure (length) of B. Once again,
nonparametric methods must be used for statistical inference of
band-limited G-causality.
In practice, some ﬁltering may still be appropriate in cases
where the data are not stationary to begin with, in which case
14The “block decomposition” technique proposed in Chen et al. appears, according to our unpublished simulations, to yield inaccurate results. In
particular, the integral equality (34) generally fails even for simple test data in
large samples. It is possible that the analytic derivation in Chen et al. 
may be ﬂawed.
15In Barnett and Seth it is erroneously stated that to guarantee invariance a ﬁlter need only be causal—i.e. not contain a pure lag. In fact full
invertibility (minimum phase ﬁltering) is required to ensure that the condition
(11) is not violated for the ﬁltered process. We are grateful to Victor Solo (private communication) for pointing this out.
G-causal analysis is likely anyway to fail or deliver spurious
results (Section 3.3). In these cases ﬁltering may be a valid
and useful tool to improve stationarity; e.g. notch-ﬁltering of
line-noise in electronically recorded time series data , or ﬁnite diﬀerencing to eliminate drift . In the latter case, ﬁlters of the form ˜ut = ut −ut−1 may be
applied iteratively until (approximate) stationarity is achieved;
but note that diﬀerencing constitutes a non-invertible ﬁlter, so
that stationary data should not be diﬀerenced. An important
application of ﬁlter invariance in neuroscience (Section 4) is
to G-causal analysis of fMRI (functional Magnetic Resonance
Imaging) data, where serious confounds have been previously
suspected; this is discussed further in Section 4.3.
3. MVGC Toolbox design
Central to the design of the MVGC toolbox is the equivalence
(Section 2.1) of the VAR parameters (Ak, Σ), the autocovariance
sequence Γk and the cross-power spectral density S (λ) as representations for a VAR process. The MVGC toolbox exploits
these equivalences to provide numerically accurate algorithms
for moving ﬂexibly between the alternative VAR representations, thus furnishing computationally eﬃcient pathways for
calculation of G-causalities, conditional and unconditional, in
the time and frequency domains. A schematic of computational
pathways implemented in the toolbox is given in Fig. 1.
3.1. Computational strategy
The basic operating principle of estimation of G-causalities
via the MVGC toolbox is that (after determination of a suitable
model order) a VAR (1) is ﬁtted to the time series data just once
(A2) and all subsequent calculations are based on the estimated
model parameters ˆAk, ˆΣ. This approach is theoretically consistent with the fundamental assumption of G-causal estimation,
that the time series data is a realisation of a stationary VAR
(1). In principle, any of the equivalent VAR representations
might be chosen for initial estimation—e.g. via sample autocovariance (A1 →A6) or spectral estimation (A4 →A7)—but
empirically direct estimation (A2) proves to be the most stable,
accurate and computationally eﬃcient; nonetheless the MVGC
toolbox allows implementation of alternative methods.
Having acquired the VAR model, (conditional) G-causality
in the time (22) and frequency (33) domains involves estimation of VAR parameters for the reduced regressions (21) and
(31) respectively. There is no simple algorithm (to our knowledge) to compute reduced regression VAR parameters directly
from full regression parameters. Indeed, the standard approach
[e.g., as implemented in Seth ] is to perform the reduced
regressions explicitly from the data (A2); however this leads
to inaccuracies that may be particularly serious in the spectral
case . Fortunately, numerical algorithms are available for obtaining reduced regression parameters from the autocovariance sequence (A7) via solution of the Yule-Walker equations (6), or from the CPSD (A7)
via spectral factorisation (9); the autocovariance sequence and
CPSD may themselves be computed from the (full) VAR parameters by “reverse” Yule-Walker solution (A5) and straightforward spectral calculation (Fourier transform) (A8) respectively. The recommended pathway for the MVGC Toolbox is
to use the autocovariance sequence. This is on the grounds
that, as revealed by empirical testing over a broad range of VAR
parameters, system size and model orders, Whittle’s algorithm
(A6) for Yule-Walker solution is faster, more
stable and more accurate than Wilson’s spectral factorisation algorithm (A7) ; it also has additional useful properties (see below). The toolbox thus uses the autocovariance sequence as the dominant representation for all G-causality routines (A13, A14) although, again, the alternative spectral pathways may be implemented if desired. Note that a useful “reality check” for the accuracy of G-causality computations is that
time-domain causalities should integrate with reasonable accuracy to their spectral counterparts (34) (A15).
3.1.1. Reduced model order, spectral resolution and autocovariance lags
As noted previously (Section 2.4), even if the model order
for the full regression (20) is ﬁnite (and in practice a ﬁnite full
model order will, of course, need to be selected), the model
order for the reduced regressions (21) and (31) will be in theory
inﬁnite. If, as described above, reduced VAR parameters are
calculated from the autocovariance sequence (or CPSD) how
do we choose a suitable ﬁnite model order?
First, we note that spectral factorisation (9) speciﬁes the
transfer function H(λ) at frequencies λ for which the CPSD
S (λ) is speciﬁed. Now the VAR coeﬃcients Ak are calculated
by matrix inversion of the transfer function followed by Fourier
transform. Since the Fast Fourier Transform (FFT) will be used, this implies that to calculate
VAR coeﬃcients from the CPSD, the CPSD must be speciﬁed
at some set of frequencies λ1, . . . λq evenly-spaced on [0, f],
where f is the sampling frequency—indeed, Wilson’s algorithm (A7) assumes this, and approximates the H(λk) accordingly. Then the Ak will naturally be derived for k = 1, . . . , q;
i.e. the spectral resolution q will, eﬀectively, be the model order for the reduced regression. Unfortunately, for the spectral
factorisation approach there is no obvious way to choose q. Furthermore (in contrast to Whittle’s algorithm - see below), given
a CPSD S (λ) for a stable VAR process evaluated, for a given
spectral resolution q at evenly-spaced λk, there is no guarantee
that VAR parameters derived by Wilson’s algorithm will actually be stable. These problems do not aﬄict the autocovariance
approach, described next.
For the autocovariance route, there is a natural answer for
the choice of q, based on speciﬁc properties of VAR processes
and Whittle’s Yule-Walker solution algorithm (A6). Given an
autocovariance sequence Γk for k = 0, 1, . . . q lags, Whittle’s
algorithm calculates Ak for k = 1, . . . q.
As noted in Section 2.1, for a stationary VAR process the autocovariance sequence Γk decays exponentially; thus for large enough k, Γk
will become numerically insigniﬁcant. This implies that we
only need calculate the Γk for k ≤some q, where the truncation decision is based on a speciﬁed numerical tolerance (which
Sample autocovariance estimation
time series
VAR parameter estimation
VAR coeﬃcients
VAR simulation (for testing)
Residuals covariance matrix
Sample spectral estimation
Autocovariance sequence
Yule-Walker reverse solution
Yule-Walker solution
G-causality (time domain)
VAR spectral factorisation
G-causality (frequency domain)
VAR spectral calculation
Fourier transform of autocovariance sequence
Inverse Fourier transform of CPSD
Autocovariance transform for reduced regression
Spectral transform for reduced regression
Time-domain G-causality calculation
Spectral (unconditional) G-causality calculation
Integration of spectral G-causality
Figure 1: Schematic of computational pathways for the MVGC Toolbox. The “inner triangle” (shaded circles) represents the equivalence between the VAR
representations outlined in Section 2.1. Bold arrows represent recommended (useful and computationally eﬃcient) pathways, while blue arrows represent actual
MVGC calculation.
might be determined e.g. by machine precision). Noting that the
exponential decay factor is just the spectral radius ρ—easily
calculated from the (assumed known) full VAR parameters—
the MVGC Toolbox reverse Yule-Walker (A5) implementation
core/var to autocov calculates q so that ρq < a given tolerance,
which defaults to 10−8 (empirically this yields good accuracy).
Since the CPSD is calculated by FFT from the autocovariance sequence (A9), q also becomes the spectral resolution for
any subsequent spectral calculations. Thus a principled value
for model order for the reduced regression is obtained. A further pleasant feature of Whittle’s algorithm is that given the ﬁnite autocovariance sub-sequence Γk of a stable VAR model up
to any given number of lags q, the derived VAR parameters
are—in contrast to Wilson’s algorithm—guaranteed to be stable. We emphasise that the full model order p as derived by
standard model order selection criteria, and the reduced model
order q as derived from autocovariance decay, are eﬀectively
independent; as implemented by the MVGC Toolbox, the latter
depends only on the spectral radius ρ of the ﬁtted VAR model.
3.1.2. Computational eﬃciency and accuracy
The MVGC Matlab implementation (Appendix
been designed with great regard to eﬃciency. All algorithms
are wherever possible completely vectorised and designed to
exploit Matlab’s computational strengths, including the use,
where appropriate, of key methods which invoke machineoptimised, multi-threaded libraries, such as linear solvers (LA-
PACK) and Fast Fourier Transform (FFTW)16. Detailed benchmarking is beyond the scope of this article, but in testing we
have not in general found performance to be a major issue, even
with large, highly multivariate datasets.
The most performance-critical algorithm in the MVGC
workﬂow under normal usage scenarios (Section 3.2) is
Yule-Walker
core/autocov to var). The performance of this algorithm depends critically on the number of autocovariance lags deemed
necessary for given numerical accuracy - which, in turn, depends entirely on the spectral radius of the estimated VAR (see
previous Section). Thus performance in practice is closely tied
to the actual dataset being analysed. An implication is that,
if the spectral radius ρ is close to 1—i.e. the VAR estimate
is “near unstable”—then the single-regression approach of the
MVGC may well be more computationaly intensive than the
traditional dual-regression method (“GCCA mode”, in the toolbox parlance - see Section 3.3) and it may be necessary to limit
the number of autocovariance lags17. This may be achieved
either explicitly (via the acmaxlags parameter) or by adjusting the decay tolerance (via the acdectol parameter) in the
routine A6, core/autocov to var, eﬀectively trading-oﬀperfor-
16It is diﬃcult, in lieu of published information, to deduce the computational
complexity and scaling of Matlab’s core algorithms, particularly regarding the
impact of potential parallelisation.
17This is mitigated by the observation that model order estimates based on
autocovariance decay for Whittle’s algorithm may be pessimistically large; in
general, the Ak tend to become negligibly small (and the residuals covariance
matrix Σ converges) for k ≪number of autocovariance lags.
mance for numerical accuracy (but note that statistical inference may be compromised - see below).
In another usage
scenario—smaller spectral radius coupled with very long timeseries lengths—the MVGC approach may actually be more ef-
ﬁcient, since here VAR estimation (A2, core/tsdata to var) may
become a computational bottleneck, and a single regression will
thus be preferable.
Crucially however, in arguably the most important usage
case—multivariate conditional G-causality in the frequency domain (Section 2.6)—performance is not the issue at stake. This
calculation is especially relevant in neuroscience applications
(Section 4), where results are required (a) to exclude common cross-variable inﬂuences by conditioning and (b) to be restricted to speciﬁc frequency bands. The traditional approach
here has been to pre-ﬁlter the data, ostensibly to restrict causal
estimates to frequencies of prior interest; however, as already
described (Section 2.7), this approach is fatally ﬂawed, insofar as it not only fails to restrict G-causality as desired, but in
addition seriously compromises statistical inference. Thus it
becomes essential to compute conditional spectral causalities
where, as has been established , the traditional dual-regression approach is known to produce spurious
results, including negative causal estimates. The MVGC singleregression method handles this essential case correctly (and
here too, the performance/accuracy trade-oﬀdescribed above
is available).
Regarding accuracy, as previously mentioned (Section 2.4)
the traditional dual-regression method fails to take account of
the theoretically inﬁnite model order of the reduced regression,
with adverse eﬀects on statistical inference. We illustrate this
with a comparative analysis of the MVGC single-regression
method versus the traditional dual-regression method for the
minimal VAR(1)
Xt = aXt−1 + cYt−1 + εx,t
bYt−1 + εy,t
Here the residuals εx,t, εy,t are normally distributed, uncorrelated and unit-variance white noise, a, b represent decay parameters (stability requires that |a| < 1, |b| < 1) and c controls the
strength of G-causality from Y →X. G-causalities for (38) may
be fully solved analytically [Appendix B; see also Barnett and
Seth ]. Evidently FX→Y ≡0 and in Appendix B we calculate expressions for F (∞)
Y→X, G-causality as approximated by
the MVGC single-regression method where inﬁnite lags are assumed for the reduced regression, and for F (1)
Y→X, G-causality as
approximated by the traditional dual-regression method, with
model order 1 (i.e. 1 autoregression lag) for both full and reduced regressions.
We simulated the process (38) 10, 000 times with time series lengths of 100 time steps, for a = 0.8, b = 0.9 and varying causal coeﬃcient c, calculating sample distributions for Gcausality estimators for both the statistically signiﬁcant causality Y →X and the statistically non-signiﬁcant (“null”) causality
X →Y. Results are displayed in Fig. 2. We see [Fig. 2 (a)] that,
due to ﬁnite-sample eﬀects, bF
Y→X slightly underestimates the
true causality F (∞)
Y→X on average, while bF
Y→X slightly under-
causal coefficient
G-causality mean (causal)
causal coefficient
G-causality mean (null)
causal coefficient
G-causality std. dev. (causal)
causal coefficient
G-causality std. dev (null)
Figure 2: MVGC (single-regression) vs. GCCA (dual-regression) accuracy for the minimal causal VAR(1) (38), with a = 0.8, b = 0.9 and varying causal coeﬃcient
c (x-axis). Sample statistics estimated over 10,000 runs of 100 time steps each. (a) sample mean signiﬁcant G-causality bFY→X, as estimated by MVGC and GCCA
methods. F (∞) plots the theoretical (inﬁnite lags) G-causality as estimated by the MVGC method, while F (1) plots the theoretical (1-lag) causality as estimated by
the GCCA method; (b) standard deviation of signiﬁcant G-causality; (c) sample mean null G-causality bFX→Y; (d) standard deviation of null G-causality. See text
for details.
estimates the 1-lag causality F (1)
Y→X - which itself substantially
overestimates the true causality. The latter may be explained by
the fact that the model order 1 reduced regression fails to take
into account the full explanatory power of the history of the
process Xt on itself. We also see [Fig. 2 (b)] that the dispersion
(as measured by standard deviation) of the sample distribution
is signiﬁcantly greater for bF
Y→X; this may be explained by the
additional sampling error incurred by the extra regression involved. In the non-causal X →Y direction, we see that both the
mean [Fig. 2 (c)] and standard deviation [Fig. 2 (d)] are signiﬁcantly greater for bF
X→Y than for bF
X→Y; while the latter decays
with increasing causal strength c, the former remains approximately constant.
Next we investigated the robustness of G-causal inference to
measurement noise under the two estimation techniques: we
considered the process
˜Xt = Xt + νηx,t
˜Yt = Yt + νηy,t
where ηx,t, ηy,t are normally distributed unit-variance white
noise terms uncorrelated with the Xt, Yt, representing additive measurement noise of intensity ν. The addition of noise
is known to degrade the ability to detect G-causalities [Solo
 18]. Again we simulated the process (39) 10, 000 times
with time series lengths of 100 time steps, for a = 0.8, b = 0.9,
c = 1 and varying noise intensity ν. Results are displayed in
Fig. 3. Again we see that mean estimated causalities as well
as dispersion are higher for the dual-regression estimates than
the single-regression estimates, most severely in the null (noncausal) X →Y direction. Here we also calculated Type II [false
negative, Fig. 3 (c)] and Type I [false positive, Fig. 3 (f)] error
rates, using the asymptotic null F-distribution (Section 2.5), at
a signiﬁcance level of α = 0.05. We see that while the impact
of dual regression on Type II errors is negligible, the impact
on Type I errors is quite severe in comparison with the singleregression MVGC method.
3.2. MVGC work-ﬂow
A typical work-ﬂow for calculation of (conditional) Gcausalities from empirical time series data, as exempliﬁed in
the demo/mvgc demo.m toolbox demonstration script, is illustrated as follows:
1. Model order estimation: For each model order p up to
a chosen maximum, ﬁt a VAR model to the full “universe
of data” time series u1, . . . , um (A2) and calculate the likelihood L ∝|Σ−(m−p)/2|. Use this to calculate the chosen
model order criterion (AIC or BIC). Select the best model
order p according to the criterion.
2. VAR model estimation: Estimate the corresponding VAR
model parameters (Ak, Σ) for the selected model order
18Solo’s “weak linear GC” corresponds to our G-causality, as opposed to his
“strong linear GC”, which corresponds to what has been independently termed
“partial Granger causality” (see Section 2.3).
(A2) and check that the spectral radius (3) is < 1 (other
statistical tests on the VAR parameters and residuals may
be performed at this stage).
3. Autocovariance calculation: Calculate the autocovariance sequence Γk from the VAR parameters (A5), to a suitable number of lags (as described above). This involves
“reverse solution” of the Yule-Walker equations (6).
4. Time domain: For each conditional causality F Y→X | Z required [possibly for the entire causal graph (23)]:
(a) Calculate the VAR parameters for both the full and
reduced regressions (20,21) from the autocovariance
sequence Γk (A6). This involves solution of the Yule-
Walker equations (6).
(b) Calculate the time-domain conditional G-causality
according to (22) (A13).
(c) Test the resulting causalities for signiﬁcance at a
given level using the analytical sampling distribution (Section 2.5), taking care to adjust for multiple
hypotheses, and construct conﬁdence intervals if desired.
5. Frequency domain:
For each conditional spectral
causality f Y→X | Z(λ) required [possibly for all pairwiseconditional spectral causalities]:
(a) Transform the autocovariance sequence for X, Z
to the autocovariance sequence for X†
then, as per (33), calculate the unconditional spectral
causality fY⊕Z†→X†(λ).
(b) Calculate the VAR parameters for the full regression
(15) from the transformed autocovariance sequence
(c) Calculate the transfer function H(λ) by Fourier transformation of the regression coeﬃcients (10), and
then the CPSD S (λ) according to (9) (A8).
(d) Calculate the partial residuals covariance (29) and
then the conditional spectral causality according to
(28) (A14).
(e) Test the spectral causalities for signiﬁcance at a given
level by permutation test (Section 2.5), and construct
conﬁdence intervals, if desired, by non-parametric
bootstrap.
• If both time and frequency domain causalities are required,
we can alternatively calculate the time-domain causalities
by integrating their spectral counterparts (34) (A15).
3.3. Potential problems and some solutions
Users of the GCCA (Granger Causal Connectivity Analysis) Toolbox , which the MVGC Toolbox supersedes, may occasionally ﬁnd that time series data which appeared to yield satisfactory results using the GCCA software
noise intensity
G-causality mean (causal)
noise intensity
G-causality mean (null)
noise intensity
G-causality std. dev. (causal)
noise intensity
G-causality std. dev (null)
noise intensity
Type II error rate
noise intensity
Type I error rate
Figure 3: MVGC (single-regression) vs. GCCA (dual-regression) accuracy for the minimal causal VAR(1) with additive noise (39), with a = 0.8, b = 0.9, c = 1 and
varying noise intensity ν (x-axis). Sample statistics estimated over 10,000 runs of 100 time steps each. (a) sample mean signiﬁcant G-causality bFY→X, as estimated
by MVGC and GCCA methods. F (∞) plots the theoretical (inﬁnite lags) G-causality as estimated by the MVGC method, while F (1) plots the theoretical (1-lag)
causality as estimated by the GCCA method; (b) standard deviation of signiﬁcant G-causality; Type II error rate (proportion of false negatives); (d) sample mean
null G-causality bFX→Y; (e) standard deviation of null G-causality; (f) Type I error rate (proportion of false positives). Error rates are calculated using the asymptotic
null F-distribution (Section 2.5) at a signiﬁcance level of α = 0.05. See text for details.
trigger errors or warnings when analysed via the MVGC Toolbox.
This will typically be because the MVGC Toolbox is
more stringent in its requirements and performs more thorough
error-checking.
There may be situations (see below) where
the more robust GCCA approach can be useful. The MVGC
toolbox may optionally be deployed in “GCCA mode” (see
demo/mvgc demo GCCA), which takes the more traditional
approach of separate full and reduced regressions (calculation
of conditional spectral causalities will not be available). Generally, however, data which yield apparently reasonable results
in GCCA mode but fail in standard MVGC mode should be
treated with caution.
Likely reasons for reported problems with time series data
are (i) colinearity, (ii) non-stationarity, (iii) long-term memory,
(iv) strong moving average component and (v) heteroscedasticity. We brieﬂy discuss each in turn.
(i) Colinearity occurs when there are linear relationships between variables (i.e. between individual time series) in
multivariate time series data.
In this case there is an
ambiguity in the VAR representation of the data.
Colinearity (or near-colinearity) will most likely be detected in the VAR estimation stage (A2) and reported as
“rank-deﬁcient” or “ill-conditioned” regressions by the
core/tsdata to var routine; this should be tested for by the
caller (see demo/mvgc demo). One solution is to eliminate linear dependencies, possibly via a Principal Component Analysis (PCA) or factor model
approach , or a signal separation technique such as Independent Component Analysis (ICA)
 .
(ii) The principal stationarity check performed by the MVGC
Toolbox (explicitly by the routine utils/var specrad or,
more usually, as part of the standard workﬂow [Section 3.2, step 3] by core/var to autocov) is that the
spectral radius of the estimated full VAR model (1)
is less than one (4).
If this condition is not satis-
ﬁed then analysis cannot proceed.
Note that the routine core/var to autocov performs exhaustive error checking and also produces useful diagnostics:
the utility utils/var info reports all errors, warnings and diagnostics generated by core/var to autocov (see e.g.
demo/mvgc demo).
Non-stationary data may be dealt with in several ways.
Stationarity can sometimes be achieved by standard techniques such as de-trending. As mentioned earlier (Section 2.7), pre-ﬁltering (e.g. notch ﬁltering of electrical
line noise or ﬁnite diﬀerencing) may also be useful, albeit at the potential cost of undermining model ﬁtting and
statistical inference. For unit root processes (processes
exhibiting random walk-like nonstationarities) , a more principled route to G-causal analysis is via
co-integration .
The MVGC Toolbox is not currently able to deal directly
with co-integrated processes.
An alternative approach is to divide the data into (overlapping or non-overlapping) windows, on the logic that
shorter windows are more likely to be approximately
stationary.
This implies a tradeoﬀbetween likelihood
of stationarity (shorter is better) and accuracy of model
ﬁt (longer is better).
An advantage of windowing—
assuming there is suﬃcient data—is that time-varying Gcausality can be analysed .
particularly useful given a large number of temporallyaligned trials: then so-called “vertical regression” can
be implemented using extremely short windows.
method depends on an assumption (which requires justiﬁcation) that each trial is an independent realisation of
the same underlying stochastic generative process; see the
demo/mvgc demo nonstationary script for an example.
(iii) The spectral radius condition may also fail if the time series, although stationary, has long-term memory—that is
the autocorrelation does not decay exponentially.
may arise in particular for iEEG/LFP data (Section 4.2).
In this case, the data is fundamentally unsuited to VAR
modelling (cf. Section 2.1), which may silently yield spurious results. The MVGC Toolbox is not currently able
to deal with such data.
Models for long-term memory processes do exist, e.g. VARFIMA (Vector AutoRegressive Fractionally Integrated Moving Average) models , but G-causal inference
for such models is diﬃcult and the theory underdeveloped.
Pre-analysis to determine the presence of longterm memory may be performed via checks on sample autocovariance (core/var to autocov) and/or CPSD
(core/var to cpsd), where long-term memory typically
manifests itself as power-law behaviour.
(iv) A problem may arise when the data, although stationary,
contain a strong (and in particular a “slow”) moving average component, violating the condition (11). In this case,
the routine core/var to autocov may report warnings or errors (speciﬁcally, it may fail to solve the associated 1-lag
problem - see Appendix A.1, A5) and it may become
necessary to use the more robust GCCA mode described
above. This may in particular be the case for fMRI BOLD
data (Section 4.3). A recent study (see
also 4.3) indicates that empirically, even if square summability [or the condition (11)] is violated, G-causal analysis may still, under some circumstances, yield meaningful
causal magnitudes and directionality. Clearly, however, if
the coeﬃcients of a VAR are not square summable, then
in an empirical setting they cannot be well-approximated
at ﬁnite model order, so that the estimated VAR will inevitably be misspeciﬁed. This may result in diminished
accuracy of causal estimates and compromised statistical
inference (Section 2.5).
In rare cases it is possible that, as regards the reverse
Yule-Walker calculation (A5) (Whittle’s algorithm), the
reduced model order q, as determined by autocovariance
decay, may turn out to be too small for the VAR coef-
ﬁcients Ak to decay suﬃciently, preventing the residuals
covariance matrix Σ from converging. This is unlikely to
occur given a valid VAR model and a reasonably small
choice of autocovariance decay tolerance.
(v) Finally, under standard VAR model scenarios, it is usually
assumed that the variance of the residual terms does not
depend on the actual values of the process. If this condition is violated, the process is said to be heteroscedastic. Note that heteroscedasticity does not in itself violate
stationarity. However, while a stationary heteroscedastic
process might well be modellable in theory as an (inﬁnite order) VAR, such a model will not be parsimonious
and statistical inference is likely to suﬀer; indeed, it is
well-known that heteroscedasticity can invalidate standard statistical signiﬁcance tests, and may confound Gcausal inference . While there is extensive research (mainly in the econometrics literature) into
GARCH (Generalised AutoRegressive Conditional Heteroscedastic) models ,
which autoregress residuals variances on their own history
and/or the history of the process itself, G-causal analysis
of such models is somewhat fragmented. In the neurosciences (Section 4) the literature on detection and functional analysis of heteroscedasticity is limited.
Future iterations of the MVGC will address some or all of
the above challenges. These iterations will take advantage of
our recent work establishing a
very general equivalence of G-causality and transfer entropy for
a broad class of predictive models (e.g. VARMA, VARFIMA,
GARCH, co-integration) within a maximum likelihood framework, enabling the derivation of analytic (asymptotic) sampling
distributions for statistical inference.
3.4. Debiasing of G-causality magnitudes
G-causality is by deﬁnition > 0 and hence any empirical estimation is subject to bias. This is not important when assessing
statistical signiﬁcance but may be important if the objective is
to accurately quantify the magnitude of a G-causality interaction in terms of bits ). To ensure accurate magnitude estimation, debiasing is
recommended. As illustrated in Barrett et al. , a useful
approach to debiasing is to generate surrogate distributions by
dividing the data into windows and (many times) randomly rearranging these windows independently for each variable. The
mean G-causality across this surrogate data set should give a
good estimate of the bias, which can then be subtracted from
the sample estimate obtained from the non-shuﬄed data.
4. Application to neuroscience time series data
Although G-causality is an entirely general method for analysis of time series data, the MVGC toolbox has been developed
with application to neuroscience data in mind. Methodological development in this application domain is rapidly advancing and a full review is beyond the present scope . This section summarises
some of the main issues involved in application of G-causality
(as implemented by the MVGC toolbox) to some of the more
common varieties of neuroscience time-series data.
4.1. Application to surface EEG and MEG data
M/EEG data is well suited for analysis by G-causality in
virtue of the high time resolution, fast sampling, and its typically stochastic nature. For analysis of steady-state M/EEG the
primary consideration is to ensure that the time-series are covariance stationary. As mentioned (Section 3.3), non-stationary
data can be treated in a variety of ways which may achieve
stationarity. First, time-series can be diﬀerenced (Section 2.7)
which is useful for removing long-term trends or drifts; however the interpretation of any subsequent analysis may be altered (linear or piecewise de-trending provide a simpler and less
problematic alternative that may be eﬀective). Second, the data
can be windowed (Section 3.3). Finally, minimal pre-ﬁltering
may be applied to help achieve stationarity (see Section 2.7),
for example by removing electrical line noise by notch ﬁltering
at 50 (or 60) Hz .
For evoked data, as exempliﬁed by event-related potentials
(ERPs), nonstationarity is likely to be a common issue. Given a
large number of trials, “vertical regression” as outlined in Section 3.3 can be implemented using extremely short windows.
An alternative (complementary) approach is to subtract the ensemble average ERP from each trial, which—assuming low
inter-trial ERP variation—should result in stationary residual
time series reﬂecting the induced response .
However, this method (i) risks “throwing out the baby with the
bathwater” by dispensing with the ERP itself, and (ii) will fail
if there is any substantial inter-trial ERP variability, which is
common .
Volume conduction in EEG may lead to excessive colinearity (Section 3.3) among EEG time series in sensor space (this is
less of a problem for MEG). Some useful approaches here are
application of a surface Laplacian transform, which has the effect of spatially decorrelating the data , use of factor models or projection
of sensor-level VAR model coeﬃcients onto the locations of
neural sources . The validity of application to source-localised EEG data depends on the method of
source-localisation and is beyond the present scope . Finally, as emphasised
earlier, spectral G-causality of M/EEG data should be measured
by the band-limited approach (Section 2.7) and not by preﬁltering into the desired band-pass and then applying time-domain
G-causality.
4.2. Application to intracranial EEG and LFP data
Intracranially recorded EEG (iEEG) and local ﬁeld potential (LFP) data also provide data well suited for G-causality
analysis. Unlike surface M/EEG this data is usually spatially
highly precise while sharing the advantages of high temporal resolution and sampling rate. However, perhaps counterintuitively, LPF and iEEG data can sometimes appear to be “too
clean” inasmuch as the stochastic variance on which VAR modelling depends is overshadowed by non-stationary deﬂections
and ﬂuctuations which may have both neural and non-neural
(e.g., due to electrode movement) origins, and which may re-
ﬂect long-term memory eﬀects (Section 2.1) which are diﬃcult to accommodate within a VAR framework. As well as the
techniques described above (for M/EEG), one additional possibility for iEEG/LFP is to transform the data using a bipolar
montage/reference which may reduce the impact of common
sources and drifts, emphasising the residual stochastic activity.
However, a fully adequate treatment of inherently nonstationary (and especially long-term memory) data is likely to require
substantially adapted methods, e.g. VARFIMA modelling (Section 3.3). Despite these caveats, G-causality analysis of stationary iEEG/LFP data is likely to be highly informative; see
Gaillard et al. for an illuminating example.
4.3. Application to fMRI BOLD data
Application of G-causality to fMRI BOLD data has been
highly controversial for apparently good reasons; e.g., David
et al. ; Valdes-Sosa et al. .
First, the BOLD
signal (as captured by the hemodynamic response function,
HRF) is an indirect, sluggish, and variable (inter-regionally and
inter-subjectively), transformation of underlying neural activity .
Second, typical fMRI protocols involve severe downsampling with sample intervals (repetition times, TRs) normally ranging from 1 - 3 sec, substantially longer than typical inter-neuron delays.
Inter-regional
HRF variation has been argued to be particularly devastating
for G-causality analysis: if X is causally driving Y at the neural
level, but if the BOLD response to X peaks later than the BOLD
response to Y, the suspicion is that G-causality analysis would
falsely infer that Y is driving X . Perhaps
surprisingly, this is not the case. In fact, G-causality of fMRI
BOLD data is robust to a wide variety of changes in HRF properties, including notably their time-to-peak .
This is because the HRF is eﬀectively a slow, moving average
ﬁlter, to which G-causality is in principle invariant. . Notably, these simulations include
detailed population-based spiking neuron generative models
coupled to the biomechanically realistic Balloon-Windkessel
model of hemodynamic responses, eliminating concerns that
the invariance properties described above depend on simpliﬁed
VAR-based generative models of neuronal responses.
19The proof turns on the identiﬁcation of the HRF convolution as an invertible ﬁlter and the consequent ﬁlter-invariance of G-causality (Section 2.7).
While there is good evidence that the HRF convolution is causal , whether it is fully invertible (i.e. minimum-phase) for typical HRF
parameters requires further research.
Unfortunately, the severe downsampling imposed by fMRI,
together with measurement noise, still undermine G-causal inference of BOLD data in many applications. When confounding HRFs are combined with these other factors, false Gcausality inferences are indeed likely. This precludes naive application of G-causality to fMRI data generally. While technological developments supporting ultra-low TRs and de-noising 
promise to alleviate these problems, in the absence of these
developments a conservative methodology is recommended.
Useful strategies include (i) using as short a TR as possible
by compromising on coverage; (ii) examining changes in Gcausality between experimental conditions, rather than attempting to identify “ground truth” G-causality patterns; (iii) correlating changes in G-causality magnitude with behavioural
variables such as reaction times across trials (or trial blocks)
 , and (iv) computing the so-called “diﬀerence of inﬂuence” term which may
provide some robustness to HRF variation. Alternative promising approaches include estimation of state-space models which
jointly parameterize functional connectivity and hemodynamic
responses or blind deconvolution of the HRF
to retrieve the underlying neuronal processes . In general, G-causality of fMRI BOLD
data should be treated with caution and may best be interpreted
as exploratory . See Seth et al. for
further discussion of this important application domain.
4.4. Application to spiking (i.e., point process) data
Point process data obtained from direct neural recordings
represents the other end of the spatiotemporal scale from fMRI,
but carries its own challenges with respect to G-causality analysis. The main challenge is that point process spike train data
is not suitable for modelling by linear VAR models. A theoretically principled approach is to replace the VAR modelling step
with ﬁtting of full and reduced point process models, within a
framework of maximum likelihood estimation . The MVGC toolbox does not support this in its current
version. A simpler (although arguably less principled) approach
is to obtain an estimate of the point process spectrum of spiking neural data; G-causal analysis may then proceed via spectral factorisation (Section 2.1), eﬀectively treating the estimated
spectrum as if it derived from a VAR process . A “quick and dirty” alternative is to convolve each spike
train with a Gaussian or half-Gaussian (causal) kernel, choosing the kernel width by some appropriate function of the mean
inter-spike interval - see e.g. Cadotte et al. ; Kispersky
et al. . Assuming the resulting time series data satisfy
stationarity tests, G-causality can then be applied. Due caution
is needed in any interpretation of the results since this, again, is
not a theoretically principled solution.
5. Conclusions
The MVGC toolbox provides a comprehensive set of Matlab routines for implementing G-causality analysis in the time
and frequency domains and in both conditional and unconditional cases. It optimises computational eﬃciency, numerical
accuracy and statistical inference by leveraging multiple equivalent representations of a VAR model by regression parameters, the autocovariance sequence, and the cross-power spectral
density of the underlying process. By this approach, it is able
to compute G-causality quantities by a “one shot” regression,
without requiring separate “reduced” regressions which reduce
statistical power and model estimation accuracy; the awkward
case of conditional spectral G-causality is also ﬂuently catered
for. The toolbox provides extensive functionality for statistical
signiﬁcance testing, model order estimation, and error checking. It seamlessly integrates into the Matlab environment, offering multiple demonstration scripts and extensive documentation throughout.
As with any advanced statistical method, G-causality analysis via the MVGC toolbox should be implemented with care
and with a good understanding of the underlying statistical
principles and practical constraints, as described in this paper
and elsewhere. This being the case, we hope that the MVGC
toolbox will help address a major challenge for current neuroscience, namely the deciphering of the functional organisation
of neural systems. We also anticipate its fruitful application in
other contexts involving complex dynamic systems, both within
biology and beyond.
Acknowledgements
We are grateful to Adam Barrett for helpful discussions and
comments, and for testing pre-release versions of the MVGC
toolbox. For ﬁnancial support we are grateful to the EPSRC
(G/700543/1) and to the Dr. Mortimer and Dame Theresa Sackler Foundation, which supports the work of the Sackler Centre
for Consciousness Science.
Software availability
The MVGC toolbox will shortly be available for download
from www.sussex.ac.uk/sackler. The software is freely available for non-proﬁt academic usage under the GNU General
Public License (GPL), version 3 or optionally any later version;
see www.gnu.org/licenses.
Appendix A. MVGC algorithms
Appendix A.1. Core algorithms
We now review the algorithms A1-A15 implementing the
MVGC toolbox key computational pathways (Fig. 1). All multivariate time series u1, . . . , um in what follows are n-variable of
length m and assumed already de-meaned; i.e. the sample mean
has been subtracted from each sample ut (see stats/demean).
Although all MVGC routines that reference time series data
generally accept/return multi-trial data, here for clarity we assume single-trial data. A model order p is assumed to have been
A1 Sample autocovariance estimation
The sample autocorrelation sequence [cf. (5)] is estimated as
k = 0, 1, 2, . . .
The factor of 1/(m−k−1) rather than 1/(m−k) is used to obtain
an unbiased estimate. We do not recommend this as a computational pathway since (auto)covariance estimates can be unacceptably biased by error in estimation of the mean (A.1). An
experimental routine implementing a potentially more accurate
algorithm due to Shkolnisky et al. is
included, but we nonetheless do not recommend this pathway.
Another drawback is that it is diﬃcult to ascertain how many
lags k of autocovariance will be appropriate to avoid overﬁtting
while adequately modelling variation in the data.
core/tsdata to autocov,
experimental/tsdata to autocov debias
A2 VAR parameter estimation
The MVGC toolbox supplies two algorithms for ﬁtting parameters (Ak, Σ) for a VAR model (1). Both yield VAR parameter
estimates that are asymptotically equivalent to the corresponding ML estimate.
The ﬁrst method is a standard OLS, which computes a leastsquares estimate for the sample estimators ˆAk. Given a sample
time series u1, . . . , um and estimated regression coeﬃcient matrices ˆA1, . . . , ˆAp, the residual errors for the regression are
ˆεt = ut −
t = p + 1, . . . , m
In an OLS, the ˆAk are chosen so as to minimise the mean
squared error E2 =
t=p+1 ∥ˆεt∥2 where ∥·∥denotes the L2
(Euclidean) vector norm. In the MVGC toolbox the OLS is
implemented by the Matlab “/” (mrdivide) operator, which
solves the overdetermined linear system Pp
k=1 ˆAkut−k = ut, t =
p + 1, . . . , m in the least-squares sense via QR decomposition.
An estimate for the residuals covariance matrix is then obtained
as the unbiased sample covariance of the residual errors [cf.
eq. (A.2)]:
The second supported method is a variant of the so called
“LWR algorithm”—a term generally referring to a class of
multivariate extensions to Durbin recursion —
due to Morf et al. . The Morf variant estimates regression coeﬃcient matrices ˆAk recursively for k =
1, 2, . . . from the time series data ut. It is widely acknowledged
to be very stable, and has the advantage that not only VAR coeﬃcients but also estimates ˆΣ for the residuals covariance matrix (and hence the VAR maximum likelihood estimator) are
computed recursively. It is thus extremely eﬃcient for use in
likelihood-based model selection criteria such as the AIC or
BIC , in comparison with OLS estimation, which needs to be recomputed for each model order.
In practice, for a single estimate, the Morf algorithm may be
faster or slower than an equivalent OLS, depending on number
of variables, length of time series and model order. We refer the
reader to Morf et al. for details of the algorithm.
Key routines: core/tsdata to var, core/tsdata to infocrit
A3 VAR simulation
The routine core/var to tsdata returns simulated multi-trial,
multivariate VAR(p) test data u1, . . . , um according to (1) with
normally distributed iid residuals εt, for given VAR parameters (A1, . . . , Ap, Σ). Initial samples will generally contain nonstationary transients, and may thus be truncated. Truncation
may be performed automatically; a suﬃcient number of initial
samples to approximate stationarity is calculated according to
the estimated number of time steps for autocovariance to decay
to near zero (A5).
Key routine: core/var to tsdata
A4 Spectral estimation
Although we do not recommend spectral estimation as a starting point for MVGC estimation examination of cross-power spectra
may be a useful preliminary step in time series data analysis.
The routine core/tsdata to cpsd implements two diﬀerent
methods of estimating the cross-power spectral density from
time series data (further algorithms may be added in future).
The ﬁrst algorithm uses Welch’s method which splits the data into overlapping
“windows”, computes modiﬁed periodograms of the overlapping segments, and averages the resulting periodograms to produce the power spectral density estimates. It is implemented via
the Matlab Signal Processing Toolbox functions pwelch and
cpsd. Window length and overlap parameters may be speciﬁed
by the user. The second algorithm uses a multi-taper method
 adapted from the Chronux neural
data analysis package . In addition to
window length and overlap parameters, a time bandwidth parameter and the number of tapers may also be set by the user.
Discrete prolate spheroidal (Slepian) sequences for the multitaper method are calculated using the function dpss from the
Matlab Signal Processing Toolbox.
Key routine: core/tsdata to cpsd
A5 Yule-Walker reverse solution
Given VAR(p) parameters (A1, . . . , Ap, Σ), we wish to solve the
Yule-Walker equations (6) for the autocovariance sequence Γk.
Firstly, we note that if the Γk are known for k = 0, 1, . . . , p −1,
then since (6) expresses Γk in terms of Γk−1, . . . , Γk−p, the Γk
may be calculated recursively for k ≥p. Thus if we can solve
(6) up to k = p−1 then we can calculate Γk up to arbitrary lags.
Consider now the case p = 1. Setting A ≡A1, the ﬁrst
two Yule-Walker equations are Γ0 = AΓ1⊺+Σ and Γ1 = AΓ0,
leading to (note that Γ0 is a symmetric matrix)
Γ0 = AΓ0A⊺+Σ
This is a discrete-time Lyapunov equation for Γ0, for which eﬃcient numerical solvers are available.
The Matlab Control System Toolbox function dlyap solves
equations of the form (A.5); if the Control System Toolbox is
not available, then the MVGC toolbox function utils/dlyap aitr
implements an eﬃcient iterative solver. Thus we may calculate Γ0 for a VAR(1). We now use a standard trick to express a VAR(p) as a VAR(1) on a new set of variables. Given the VAR(p) (1), we obtain the VAR(1)






and the residuals covariance matrix is


We ﬁnd that the covariance matrix of Up
t is given by


so that solving the discrete-time Lyapunov equation Γp
0 Ap⊺+Σp for Γp
0 yields Γ1, . . . , Γp−1 and the Γk may be
calculated recursively up to arbitrary lags as described above.
The autocovariance for a VAR decays exponentially with lag
(Section 2.1); more precisely, the Yule-Walker equations for the
VAR(1) (A.6) yield
k = (Ap)kΓp
k = 1, 2, . . .
Now it is easy to show that the spectral radius ρ(A) of the
original VAR(p) is just the largest modulus of the eigenvalues of Ap, so that
decays with rate ρ(A) where ∥·∥is
any consistent matrix norm. In the MVGC Toolbox routine
core/var to autocov, the autocovariance sequence is calculated
up to a maximum of κ lags such that ρ(A)κ is smaller than a
given speciﬁed numerical tolerance (which defaults to 10−8) beyond which it is presumed that the Γk will be negligibly small.
Thus the maximum number of lags required will depend on the
spectral radius of the VAR, and may potentially become very
large as the VAR approaches the unstable regime ρ(A) →1.
Key routine: core/var to autocov
A6 Yule-Walker solution
Given an autocovariance sequence Γk the Yule-Walker equations (6) may be solved for VAR parameters (A1, . . . , Aq, Σ) under the assumption that Γk is the autocovariance sequence of
a VAR(q). The equations for k = 1, . . . , q may be written as
and the k = 0 equation then gives Σ = Γ0 −
⊺. In fact, if the Γk are estimated in sample (cf.
A1), then (A.13) is just the OLS solution for VAR(q) coeﬃcients (cf. A2).
However, the solution (A.13) has the practical disadvantage
that for n variables it requires inversion of an nq × nq matrix,
which may be computationally prohibitive if q is large (cf. A5).
Furthermore, if the true VAR model is stable and of order p,
the Ak computed from (A.13) are not guaranteed to be stable,
even if q = p. In practice, the true model order will not in
general be known exactly; indeed, in the MVGC approach, the
Γk will generally be calculated as in (A5) up to q large enough
to ensure negligible autocovariance for k > q.
The MVGC Toolbox function core/autocov to var uses instead an LWR algorithm due to Whittle to
solve (6) recursively for VAR parameters (A1, . . . , Aq, Σ). Importantly, Whittle’s algorithm, unlike the OLS solution (A.13),
guarantees that if the true VAR is stable of order p, then the
calculated model (A1, . . . , Aq, Σ) is also stable, even if not of
the correct order; i.e. if q , p. For n variables the algorithm
requires 2q separate n × n matrix inversions, and is thus likely
to be computationally tractable up to far higher model orders.
The reader is referred to Whittle for details.
Key routine: core/autocov to var
A7 VAR spectral factorisation
Although as mentioned previously (Section 2.1) there is no
known closed-form solution of (9) for H(λ), Σ in terms of S (λ),
it is known that a unique solution to the spectral factorisation
problem exists. The routine core/cpsd to var deploys an iterative algorithm due to Wilson to achieve this
numerically. The implementation is based on code kindly provided by G. Rangarajan . The
VAR coeﬃcients Ak may then be recovered from H(λ) by a matrix inversion and inverse Fourier transform; the utility function
utils/trfun2var performs this calculation.
We remark that this is not a recommended MVGC computational pathway, since our tests indicate that, numerically, over a
wide range of scenarios, spectral factorisation may be more ef-
ﬁciently and accurately calculated by transforming to the time
domain (i.e. to the autocovariance sequence (A10)), and using
Whittle’s LWR algorithm (A6).
Key routine: core/cpsd to var
A8 VAR spectral calculation
The function core/var to cpsd implements the relations (9,10)
directly to compute a VAR CPSD S (λ) from VAR coeﬃcients
(Ak, Σ); a fast Fourier transform (FFT) is used to calculate the
transfer function H(λ) from the VAR coeﬃcients.
Key routine: core/var to cpsd
Transforming between the autocovariance sequence and cross-power spectral density
Calculating the CPSD from the autocovariance sequence entails the Fourier transform (7), implemented in the toolbox as
an FFT. In terms of normalised frequencies, i.e. with period 2π,
the (discrete) FFT of a sequence Gk approximates
F [G](λ) =
From (7) we thus ﬁnd:
S (λ) = F [Γ](λ) + F [Γ](λ)∗−Γ0
0 ≤λ ≤2π (A.15)
Transforming in the other direction entails the inverse Fourier
transform (8), implemented in the toolbox as an inverse fast
Fourier transform (IFFT). Since the (discrete) IFFT of a function P(λ) approximates
F −1[P]k = 1
P(λ)eiλk dλ
k = 0, 1, 2, . . .
we want to represent (8) as an integral over [0, 2π], rather than
over [−π, π]. We may calculate that
Γk = (−1)kF −1[P]k
k = 0, 1, 2, . . .
P(λ) = S (λ −π) =
in terms of S (λ) on [0, π], which is what we require since
the MVGC toolbox represents all spectral quantities up to the
Nyqvist frequency, which corresponds to [0, π] for normalised
frequencies (cf. Section 2.1).
Key routines: core/autocov to cpsd, core/cpsd to autocov
Autocovariance and spectral transforms for
reduced regression
Let Γk be the autocovariance sequence and S (λ) the CPSD for
a universe Ut of variables. We assume that we may take Γk = 0
for k > q (i.e. we take q large enough that the autocovariance
decays to some tolerance, as in A5). Suppose that Ut splits as
and we have a reduced regression
BkXt−k + X†
with (white noise) residuals X†
t . Note that we we may restrict
the reduced regression to q lags, since from the Yule-Walker
equations (6) it may be seen that Γk = 0 for k > q implies that
Bk = 0 for k > q. Considering the residuals X†
t as new variables, the problem is to calculate the autocovariance sequence
Γ†k and CPSD S †(λ) for the transformed universe U†
Now since the X†
t are residuals of a regression they are white
noise (i.e. iid and serially uncorrelated), so that Γ†xx,k = δk0Σ†xx,
where Σ†xx = cov
. We also have Γ†yy,k = Γyy,k and from
(A.19) it follows that the Γ†
yx are derived by convolving
corresponding Γ terms with B:

ℓ=1 BℓΓxy,k−ℓ
ℓ=1 Γyx,k+ℓBℓ⊺

for k = 0, 1, 2, . . .. We thus need to calculate Γ†xy,k to k =
2q lags (although in practice it seems that q lags is suﬃcient)
and Γ†yx,k to q lags. In the spectral domain, again since X† is
white noise, the CPSD of X† is just the ﬂat spectrum S †xx(λ) =
Σ†xx, and we also have S †yy(λ) = S yy(λ). From (A.20), and the
Convolution Theorem , we have:

B(λ)S xy(λ)
S yx(λ)B(λ)∗

where B(λ) ≡I −Pq
k=1 Bke−ikλ is the Fourier transform of the
reduced regression coeﬃcients.
The principal application of the transformations is to the reduced regression (31) in the calculation of conditional spectral
MVGCs. Although the recommended pathway for this calculation is via the autocovariance sequence, in fact the spectral
transformation (A.21) is more eﬃcient in practice (especially if
q is large), since fast Fourier transformation may be deployed;
in fact the default method in the routine core/autocov xform
is to convert to the CPSD (A11), apply (A.21) and then convert back to the autocovariance sequence (A12). The function
core/cpsd xform is also implemented for completeness.
Key routines: core/autocov xform, core/cpsd xform
Appendix A.2. G-causality algorithms
A13 Time-domain G-causality calculation
Time domain causalities are calculated from the autocovariance
sequence Γk—most likely obtained via core/var to autocov
parameters—by
gc/autocov to mvgc according to (18) in the unconditional and
(22) in the conditional case. The routine core/autocov to var
(A6) is used to calculate the residuals covariance matrices for both the full and reduced regressions.
The routine gc/autocov to pwcgc calculates similarly the pairwiseconditional causalities (23).
Key routines: gc/autocov to mvgc, gc/autocov to pwcgc
A14 Frequency-domain (unconditional) G-causality calculation
Spectral causalities are calculated from the autocovariance sequence Γk—again most likely obtained via core/var to autocov
(A5) from estimated VAR parameters—by the routines
gc/autocov to smvgc
gc/autocov to spwcgc
pairwise-conditional case.
In the unconditional case (28),
VAR parameters (Ak, Σ) for the full regression are ﬁrst calculated by core/autocov to var (A6), and the CPSD S (λ) and
transfer function H(λ) are then calculated by core/var to cpsd
In the conditional case, VAR parameters are ﬁrstly obtained
from the autocovariance sequence Γk for the reduced regression
(31) by core/autocov to var (A6), and are used to transform the
autocovariance sequence by core/autocov xform (A11). The
conditional spectral causalities are then calculated from the
transformed autocovariance sequence Γ†
k as unconditional spectral causalities according to (33). There is in fact a simpliﬁcation due to the fact that [cf. A12, eq. (A.21)] the power spectrum S †
xx(λ) for the transformed target variable is ﬂat and will
thus already have been calculated as the covariance matrix Σ†
Key routines: gc/autocov to smvgc, gc/autocov to spwcgc
A15 Integration of spectral G-causality
The routine gc/smvgc to mvgc implements the spectral MVGC
integrals (30,34) and the band-limited variants (36,37); see Sections 2.6 and 2.7 respectively. Numerical integration is performed by a simple trapezoidal rule quadrature.
Key routines: gc/smvgc to mvgc
Appendix B. Exact solution of a minimal causal VAR(1)
We consider the VAR(1) of (38):
Xt = aXt−1 + cYt−1 + εx,t
bYt−1 + εy,t
where |a| < 1, |b| < 1 and the residuals εx,t, εy,t are unit-variance
uncorrelated white noise. The coeﬃcients matrix of (B.1) is
and we may easily calculate the transfer matrix as
(1 −az)(1 −bz)
with z = e−iλ. The CPSD of the joint (Xt, Yt) process (B.1) may
thus be calculated from (9) as
|1 −az|2|1 −bz|2
|1 −bz|2 + c2
cz(1 −a¯z)
c¯z(1 −az)
for |z| = 1 in the complex plane [cf. Barnett and Seth ,
Section 4]. By inspection, the x-component S xx(z) of the CPSD
may be factorised explicitly in the form
|1 −bz|2 + c2
|1 −az|2|1 −bz|2 = σ2h(z)h(z)∗
where the transfer function is of the form
(1 −az)(1 −bz)
and σ2 is just the residuals variance of the process Xt considered as a VAR(∞). Note that this implies that, although the
joint process (Xt, Yt) is VAR(1), the process Xt alone is actually
VARMA(2, 1) (cf. Section 2.4). In order to satisfy (B.5,B.6),
the unknowns σ2 and r must satisfy
σ2|1 −rz|2 = |1 −bz|2 + c2
for all z = e−iλ on the unit circle in the complex plane, which
σ2(1 + r2) = ∆≡1 + b2 + c2
We may then solve for σ2:
From (18) [note that by assumption Σxx = var εx,t
 ≡1, and that
xx = σ2] the (inﬁnite-lag) G-causality in the Y →X direction
Y→X = ln σ2 = ln
[cf. Barnett and Seth , Eq. (45)].
To calculate the 1-lag G-causality, we consider the 1-lag reduced regression
Xt = a′Xt−1 + ε′
which we need to solve for the residuals variance σ′2 ≡var ε′
in the least-squares (or ML) sense. The OLS solution is given
by the partial covariance 
σ′2 = var(Xt) −cov(Xt, Xt−1)2 var(Xt−1)−1 = γ2
where γk ≡cov(Xt, Xt−k) is the autocovariance sequence for Xt.
Now from the Yule-Walker equations (6) for the joint process
(Xt, Yt) with k = 0, 1 some straightforward algebra yields
γ0 = 1 + (1 + ab)δ
γ1 = a + (a + b)δ
(1 −ab)(1 −b2)
leading to
Y→X = ln σ′2 = ln
"1 + 2δ + (1 −b2)δ2
1 + (1 + ab)δ