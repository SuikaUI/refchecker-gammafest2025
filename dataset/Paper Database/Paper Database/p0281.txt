Unifying Human and Statistical Evaluation for
Natural Language Generation
Tatsunori B. Hashimoto*1,2
Hugh Zhang*1
Percy Liang1,2
(* equal contribution)
1Department of Computer Science
2Department of Statistics
Stanford University
{thashim,hughz}@stanford.edu
 
How can we measure whether a natural language generation system produces both high
quality and diverse outputs?
Human evaluation captures quality but not diversity, as it
does not catch models that simply plagiarize
from the training set. On the other hand, statistical evaluation (i.e., perplexity) captures diversity but not quality, as models that occasionally emit low quality samples would be insufﬁciently penalized. In this paper, we propose a uniﬁed framework which evaluates both
diversity and quality, based on the optimal error rate of predicting whether a sentence is
human- or machine-generated.
We demonstrate that this error rate can be efﬁciently estimated by combining human and statistical
evaluation, using an evaluation metric which
we call HUSE. On summarization and chitchat dialogue, we show that (i) HUSE detects
diversity defects which fool pure human evaluation and that (ii) techniques such as annealing
for improving quality actually decrease HUSE
due to decreased diversity.
Introduction
Generating text is a core part of many NLP tasks
such as image captioning , opendomain dialogue , story generation , and summarization
 . However, proper evaluation of natural language generation has proven
difﬁcult . A good evaluation metric
should not only capture the quality of generation,
but also the diversity of generation, which is especially crucial for creative, open-ended tasks like
dialogue or story generation.
Human evaluation, which is often viewed as the
gold standard evaluation, captures quality but fails
to capture diversity. As an example, for language
Model Probability (pmodel)
Agassi bows out of Australian open
Agassi withdraws from Australian open
Sharon has stroke for stroke
Cleared coach facing another
grilling from British swim bosses
Model Generations
Human Judgment
Figure 1. HUSE is twice the classiﬁcation error
of distinguishing reference and generated text based
on human judgment scores and model probabilities. HUSE identiﬁes samples with defects in quality (Sharon has stroke . . .) and diversity (Cleared
coach facing . . .).
modeling, a model that directly plagiarizes sentences from the training set would pass the human quality bar but would have zero generalization ability and thus have inadequate diversity. On
the other hand, statistical evaluation—i.e., perplexity on a reference test set—captures diversity,
as it ensures a model must assign reasonable probability to novel sentences, but perplexity provides
an inadequate measure of quality . For example, modifying a perfect model by
removing its ability to generate even a single test
sentence results in inﬁnite perplexity even though
the model is still near-perfect. Automatic metrics
such as BLEU and ROUGE
 capture quality better than
perplexity but still correlate poorly with human
evaluation and fail to capture diversity .
Existing approaches to combining statistical
and human evaluation have been ad-hoc, leading to misleading performance measures. A common approach is to measure diversity through the
perplexity of a probabilistic model and quality
through human evaluation on beam-searched outputs. This gives the illusion that a single model is
 
high-quality and diverse, while the reality is that it
shows we can have either a diverse model (when
sampling from the distribution used to compute
perplexity) or a high-quality model (when beamsearching).
In this paper, we deﬁne the idealized evaluation
metric as twice the error of the optimal discriminator for classifying sentences as coming from the
reference distribution or the model (Section 2). If
a model generates gibberish (low quality), the optimal discriminator can classify these accurately as
coming from the model. If the reference distribution contains sentences the model cannot generate
(low diversity), the optimal discriminator can classify these accurately as coming from the reference.
Unfortunately, the optimal discriminator is unavailable. Human discriminators cannot capture
diversity effectively, and learned discriminators—
e.g., from a Generative Adversarial Network
 or one trained on human
judgments —are too unreliable
to use for rigorous evaluation.
Our key result (Section 3) is based on the observation that the optimal classiﬁer depends only on
two numbers: the probability of a sentence under
the model and the probability under the reference
distribution. The former can be computed directly
from the model, and we show that the latter can
be well-approximated by human judgment scores.
The resulting two-dimensional space is illustrated
in Figure 1. We apply a simple k-nearest neighbor classiﬁer in this space and deﬁne Human Uni-
ﬁed with Statistical Evaluation (HUSE) as twice
the leave-one-out error of this classiﬁer.
We apply HUSE to four natural language generation tasks (Section 5):
language modeling,
chitchat dialogue, story generation, and summarization.
First, we show that human evaluation
alone is insufﬁcient to discriminate model generations from the references, leading to inﬂated estimates of model performance. In contrast, HUSE is
able to reveal deﬁciencies of current models. We
also show that common techniques for improving sample quality such as annealing actually increase distinguishability between the model and
reference due to losses in diversity.
Optimal Discriminator
Consider a natural language generation task where
the model is given a context x (e.g., a dialogue history) drawn from some prior p(x) and must output
a distribution over possible sentences pmodel(y |
x). We deﬁne an idealized evaluation metric based
on whether pmodel is close to a reference distribution pref, which is generally human-generated.1
Speciﬁcally, consider a random variable y drawn
from either the reference or the model based on an
indicator z ∼Bernoulli
y | x, z ∼
pref(y | x)
pmodel(y | x)
Deﬁne L∗to be twice the lowest possible error
over any discriminator f that attempts to determine z based on x and y:
f P[f(x, y) ̸= z].
L∗measures similarity between pmodel and pref; it
is 0 if pmodel and pref are disjoint and 1 if they are
identical.2
Obstacles.
Unfortunately, L∗is unattainable because it requires computing the optimal discriminator. In the spirit of the Turing Test, we could
consider using the error rate of a human discriminator fhum instead, often considered the gold
standard for evaluation. However, while humans
might have knowledge of pref, they do not have full
knowledge of pmodel and thus would have difﬁculties determining which sentences a model cannot
As a concrete example, suppose pref placed a
uniform distribution over some set S.
knowledge of pmodel the most sensible discriminator is to predict z = 1 (reference) when y ∈S.
This discriminator achieves the same classiﬁcation
error of 0.5 for both the perfect model pmodel =
pref and one which can only return a single y ∈S.
We could try to reveal pmodel to humans by showing multiple samples simultaneously, but this is
expensive and, as we will later see, unnecessary.
Another option is to learn f over an expressive
class of functions such as neural networks on data
1 While some tasks only care about quality and thus only
require pmodel to place mass on some high quality y, we demand that pmodel places mass on all high quality y as given by
pref. This diversity is important for open-ended tasks such as
dialogue or story generation. Also note that pref need not be
the human distribution, or match the training distribution. It
can be deﬁned as the distribution given by experts.
variational
divergence:
x,y p(x) |pmodel(y | x) −pref(y | x)|
Appendix A.1 for details.
sampled from pmodel and pref. This is analogous to
learning the discriminator in a Generative Adversarial Network (GAN) 
or learning an evaluation metric from human judgments . However, as (x, y) are
high-dimensional objects, training a good classi-
ﬁer is extremely difﬁcult (and perhaps not significantly easier than solving the original generation
problem). Indeed, learned evaluation metrics do
not generalize very well . Unlike these approaches which
seek to replace human evaluation, our focus will
instead be on combining human and automatic statistical evaluation to estimate the optimal classiﬁer
Human Uniﬁed with Statistical
Evaluation (HUSE)
Our key result is that the optimal discriminator depends on (x, y) only through a two-dimensional
sufﬁcient statistic (Section 3.1), motivating an approximation which we call HUSE (Section 3.2).
For any feature map φ that maps (x, y) to
φ(x, y) ∈Rd, deﬁne the evaluation score L(φ) to
be twice the error rate of the optimal discriminator
that depends on (x, y) only through φ:
f P[f(φ(x, y)) ̸= z].
Note that the evaluation score L(φ) given by
a feature map φ optimizes over all functions that
depend on φ (3). Thus, the more information φ
contains, the lower L(φ) is. This has two implications: First, any feature map φ yields an (optimistic) upper bound on L∗(2), meaning that L(φ)
might be able detect when a model is poor but cannot certify that it is good. Second, adding features
to φ can only improve this bound.
Two features sufﬁce
Let us consider the following two-dimensional
feature map:
φopt(x, y) def
= [pref(y | x), pmodel(y | x)] .
From the arguments above,
it is clear that
L(φopt) ≥L∗, but perhaps more surprisingly, we
actually have equality:
Proposition 1. The two-dimensional feature map
φopt achieves the optimal discriminator score:
L(φopt) = L∗.
We compute the true posterior over z
given x, y. Since p(z = 1) = p(z = 0) =
p(y | x, z = 1) = pref(y | x) and p(y | x, z =
0) = pmodel(y | x), by Bayes’ rule:
p(z = 1 | x, y) =
pref(y | x)
pref(y | x) + pmodel(y | x).
The optimal discriminator simply predicts z = 1
if pref(y | x) > pmodel(y | x) and z = 0 otherwise.
In other words, the decision boundary is given by
φopt(x, y)1 > φopt(x, y)2.
More generally, we can obtain this equality with a
wider class of φ. It will hold exactly for any invertible transformation of φopt (Appendix Corollary 1), and approximately for any φ which has
high mutual information with φopt (Appendix Theorem 1). This means that we can substitute pref
with noisy, possibly un-normalized estimates and
still obtain accurate estimates of L∗.
HUSE features
While we can directly compute pmodel(y | x) for
many probabilistic models, pref(y | x) is unattainable, so L(φopt) is not computable. However, the
wisdom of the crowds suggests that pooling together the
judgments of many humans can often produce surprisingly reliable estimates of real-world probabilities such as pref(y | x), even if no individual human is particularly reliable. With this motivation,
we ask Amazon Mechanical Turk workers to rate a
sentence from 1–5 based on how “typical” it is as
a way to estimate pref(y | x). (see Appendix A.3
for more details). We deﬁne HJ(x, y) to be the average response over 20 crowdworkers. Figure 2
shows that for a language modeling task on the
Reddit corpus,3 HJ(x, y) strongly correlates with
the actual log-frequency of y in the corpus. The
high correlation suggests that human judgments
HJ(x, y) are a good surrogate for log pref.
In addition, we found that rather than using
the model probability pmodel(y | x) directly as
a feature, normalizing by sentence length len(y)
yielded lower (tighter) scores. We therefore deﬁne
the HUSE features as follows:
φhuse(x, y) def
log pmodel(y | x)
, HJ(x, y)
3We used the Reddit corpus due to crowdworker familiarity, corpus size, and short average sentence length, which
results in a wide range of sentence frequencies.
Frequency in Reddit corpus (log scale)
Human Judgment (HJ) Score
Figure 2. On the Reddit corpus, human judgment
(HJ) of the “typicality” of a sentence y correlates
strongly (r = 0.92) with its frequency in the corpus,
suggesting that HJ is a good surrogate for log pref.
Error bars at the 90% conﬁdence interval.
and deﬁne the (population) HUSE score as
Guarantees derived from HUSE
We now show that the HUSE score satisﬁes two
nice properties: (i) HUSE does at least as well as
human evaluation and (ii) a low HUSE score is
sufﬁcient to show that a model is far from the reference distribution.
To show (i), consider a feature map that only includes human evaluation: φhj(x, y) def
= [HJ(x, y)].
Because φhuse also incorporates human evaluation,
L(φhuse) is always tighter (lower) than the human
discriminator error L(φhj):
Proposition 1 (Relationship between HUSE, human evaluation, and optimal scores).
L(φhj) ≥L(φhuse) ≥L∗.
Furthermore,
the main difference between
L(φhuse) and L∗is that the former uses HJ(x, y)
and the latter uses pref. But as we argued using
Figure 2, HJ(x, y) is strongly correlated with pref,
and good approximations to pref provide approximation guarantees for L(φhuse) (Appendix Theorem 1).
Evaluating models with HUSE
In this section, we show how we can estimate the
error rate L(φ) from ﬁnite data (Section 4.1). We
then show how the HUSE estimate (ˆL(φhuse)) can
be decomposed into a score that measures quality (HUSE-Q) and a score that measures diversity (HUSE-D), which allows us to study qualitydiversity tradeoffs (Section 4.2).
Learning a discriminator
For any feature map φ, we show how to produce
an estimate of L(φ). Fix n contexts x1, . . . , xn.
First, we draw n examples y1, . . . , yn from the reference distribution pref(y | x), which are usually
human-generated sentences from a test set. We
also draw n examples y′
1, . . . , y′
n from the model
pmodel(y | x) we wish to evaluate. Next, for each
of the 2n examples (x, y), we compute the feature
map φ(x, y), which might involve evaluating the
model probability pmodel(y | x) as well as collecting human judgments HJ(x, y) from crowdworkers.
Finally, we compute the leave-one-out error of
a classiﬁer that tries to predict whether a given example (x, y) comes from the reference distribution
(z = 1) or the model (z = 0).
The classiﬁcation problems for HUSE are twodimensional, which allows us to accurately estimate error rates using a k-nearest neighbors classiﬁer. We opt to use nearest neighbors classiﬁers
as they are simple, require no training, and can
asymptotically capture arbitrary continuous decision boundaries. Speciﬁcally, we set k = 16 and
deﬁne neighbors using L2 distances over the feature vectors φ(x, y) scaled componentwise to have
unit variance. The overall procedure for computing the estimate ˆL(φ) is formally deﬁned in Algorithm 1.
Algorithm 1 Estimating error rates under φ
Require: Feature map φ, number of neighbors k
Contexts x1, . . . , xn
Reference outputs y1, . . . , yn
Model outputs y′
1, . . . , y′
1: Construct dataset:
{(φ(xi, yi), 1), (φ(xi, y′
2: ˆL(φ) def
= leave-one-out error of k-NN on D
Quality-diversity decomposition
We now deﬁne the (empirical) HUSE score using
the feature map φhuse:
= ˆL(φhuse).
We deﬁne the quality component of HUSE
(HUSE-Q) similarly using human judgments
HUSE-Q def
= ˆL(φhj).
Since humans can detect quality defects in
a model, any increase in error from removing
pmodel must come from a model’s lack of diversity. Therefore, we deﬁne the diversity component
(HUSE-D) as follows:
HUSE-D def
= 1 + HUSE −HUSE-Q,
which implies the decomposition (1−HUSE-D)+
(1 −HUSE-Q) = 1 −HUSE. As long as the discriminators are non-degenerate (obtaining better
performance than chance and HUSE > HUSE-Q),
all scores are contained in . Here, HUSE-D =
1 implies that the model suffers no diversity defects, while HUSE-D = 0 indicates that the examples could be discriminated perfectly due to a lack
of diversity.
Experiments
Experimental setup
We use HUSE to evaluate three different types of
single-sentence natural language generation tasks:
(i) unconditional and high entropy (language modeling); (ii) conditional and high entropy (story
generation, chit-chat dialogue); and (iii) conditional and low entropy (summarization). We show
that HUSE provides a direct and interpretable
measure of diversity on high-entropy tasks, while
also serving as a useful model diagnostic on lowentropy ones.
The four tasks along with the datasets and models are as follows:
• Summarization: Giganews story to headline dataset and the pre-trained model from
Gehrmann et al. .
The dataset consists of 3.8 million news story-headline pairs.
Examples from this dataset are shown in Table 2.
• Story generation: Last sentence generation
for ROC stories 
consisting of 96,198 examples of partially
written four-sentence stories as input, and a
single sentence which completes the story as
the target.
We use a standard OpenNMT
model with global attention . The task consists
of generating a single sentence from the one
billion word newswire text distribution.
• Chit-chat dialogue: Two-turn chit-chat dialogue dataset consisting of 37.3 million
comment-response pairs from Reddit (Appendix A.4). Comments are generally short
(5–15 tokens) and cover a single topic (e.g.
given “wow how did i not notice that”, the response is “you were focusing on other things
its understandable”).
We train a convolutional model using fairseq (Gehring et al.,
For all the tasks, we train neural models and
evaluate their diversity-quality tradeoffs as we
change the decoding scheme for generation. Our
primary evaluation concerns diversity trade-offs
involving temperature annealing which is a generation technique applicable to any probabilistic model that generates words sequentially.
temperature annealed models, we sample a word
w proportional to p1/t(w) where p is the model
probability of w given previous words and t is
the temperature parameter.
We excluded beam
search since it qualitatively behaves similarly to
temperature annealing with low temperatures and
HUSE ≈0 due to beam search being extremely
under diverse.
As a non-neural baseline, we also consider retrieval based models based on Apache solr on
a few tasks. For this approach, we retrieve the
single most relevant response from the training
set using the BM25 similarity metric on inputs.
Such models are known to perform well in tasks
with complex outputs such as program generation
 and
style transfer .
For cost reasons, we did not measure certain
combinations of task and generation mechanisms.
We did not measure retrieval for chit-chat dialogue, as we observed its outputs were lower quality than a low-temperature neural model. We also
did not anneal language models, as the generation
quality from the language model was already high,
and our goal was to show that they achieved high
HUSE. Our set of measurements, while not comprehensive, generally covers the available qualitydiversity tradeoffs for conditional tasks.
Summarization
Story generation
Chit-chat dialogue
Table 1. Performance achieved by the best models on the four tasks, as measured by overall goodness-of-ﬁt (HUSE),
sample quality (HUSE-Q) and diversity (HUSE-D). The scale for HUSE and HUSE-Q ranges from 0.0 (completely
distinguishable from reference) to 1.0 (indistinguishable from reference) where the implied classiﬁcation error is
HUSE/2. HUSE-D may exceed 1.0 with small sample sizes when HUSE-Q > HUSE.
Quality (HUSE-Q)
Diversity (HUSE-D)
Tradeoffs between HUSE-D and
Points are models and color indicates
task. Neural models (circle) generate using temperature annealing (point labels indicate temperature).
Models closer to the top right are superior, and gray
diagonal lines indicate equivalent HUSE. A shaded
region for a task indicates models which are strictly
dominated (worse HUSE with the same HUSE-D-
HUSE-Q proportion). Annealing can trade-off between diversity and quality but cannot easily increase the underlying model performance (HUSE).
Finally, we collect human judgments HJ(x, y)
as per Section 4.1 where we query 20 Amazon
Mechanical Turk crowdworkers for typicality ratings on 100 reference and 100 model sentences.
Since our models generate UNK (unknown and
out-of-vocabulary) tokens, we instructed crowdworkers to treat UNK tokens as rare, but appropriate words for the context.
Overall results
The HUSE scores across the four tasks vary
widely. Table 1 shows that single-sentence language models are nearly indistinguishable, with
HUSE = 0.86 and implied discriminator error of
In contrast, both summarization and dialogue
are highly distinguishable (HUSE ≈0.5) with relatively low quality when sampled from t = 1.0.
Human evaluation alone (HUSE-Q) would suggest that using temperature annealing (t = 0.7) to
emphasize high-probability outputs substantially
improves the model (HUSE-Q goes from 0.58 to
0.92 for summarization and 0.56 to 0.92 for dialogue). However, we ﬁnd that this increase in sample quality comes at the cost of diversity (HUSE-D
goes from 0.95 to 0.34 for summarization and 1.0
to 0.57 for dialogue). Examining the achievable
HUSE and diversity tradeoffs in Figure 3 shows
that mechanisms such as annealing which improve
sample quality actually degrade HUSE due to severe losses in diversity.
We ﬁnd that all generation schemes and models
are inadequate for story generation on ROC stories. The original model (t = 1.0) is very easily
distinguishable by a human (HUSE-Q = 0.15),
corresponding to a discriminator error of 7%. The
retrieval models can improve this to HUSE-Q =
0.47, but this comes at the expense of diversity.
Finally, we observe that directly sampling from
the model (t = 1.0) is always diverse.
suggests that human evaluation is an appropriate
evaluation for generation systems that are directly
sampled (rather than beam-searched).
Model error analysis with HUSE
Since HUSE is estimated from a two-dimensional
classiﬁcation problem, we can directly visualize
the classiﬁcation problem to understand defects in
both model quality and diversity.
φhuse(xi, yi) (blue squares) and model points
φhuse(xi, y′
i) (red circles) for the summarization
The shaded areas indicate the decision
boundary of the 16-nearest neighbor classiﬁer.
At temperature t = 1.0, we ﬁnd that the classiﬁcation boundary is mostly horizontal, implying
that human judgment alone can distinguish model
outputs from references. There is a cluster of sentences with high HJ and high pmodel which are essentially indistinguishable. Examining the samples in this top-right region reveals that these are
news stories with short headlines such as “Nadal
pulls out of Sydney International” which can be
Figure 4. The two-dimensional classiﬁcation problem in Algorithm 1 on the summarization task with different
softmax temperatures (three panels). Each point represents a reference sentence φhuse(xi, yi) or model-generated
sentence φhuse(xi, y′
i). The color denotes the source of the sentence (z), shading is the classiﬁcation conﬁdence of the
nearest neighbor classiﬁer.
reliably generated even at t = 1.0.
the model frequently generates low quality samples that can easily be distinguished such as “two
new vaccines in the poor countries were effective
against go-it-alone study says” (Table 2).
At lower temperatures of t = 0.9 and t = 0.7,
the boundary shifts towards becoming diagonal.
Although the distribution is no longer directly separable on human judgment, the two distributions
are clearly separable with the inclusion of pmodel.
Using Figure 4, we can identify individual examples which were correctly and incorrectly classiﬁed based on pmodel and HJ. Table 2 shows examples of both quality failures and diversity failures identiﬁed by HUSE. For example, the “diversity failure” table shows that the summarization
model (t = 0.7) has an extremely low probability
of generating some reference sentences (“NFL’s
bills shake up front ofﬁce”) and is thus underdiverse. Closer examination of the model shows
that the probability of generating “front ofﬁce”
is low, since it is an unusual way to refer to the
president and general manager. Improving these
models on the diversity failures will require that
the model understand more subtle paraphrases.
We can also identify model successes, where the
model outputs are indistinguishable from the reference in terms of quality (“Agassi bows out of
Australian Open after injury”), and the model assigns high probability to the reference (“Agassi
withdraws from Australian Open”).
HUSE stability
Since HUSE depends on human crowdworker annotations, one might ask if it is possible to reduce
either the number of annotated examples, or number of distinct crowdworkers for each example.
We show that for low-quality models, substantially
fewer annotations are needed.
Figure 5. Estimates of HUSE are robust to small
test set size, but generally require ≈20 crowdworker measurements for each example.
Figure 5 shows the result of subsampling our
original data of 200 sentences and 20 crowdworkers and estimating HUSE. First, we ﬁnd that using 50 test set examples (Figure 5, left) is often sufﬁcient to give accurate estimates of HUSE.
Next, we ﬁnd that the necessary number of crowdworkers per example depends heavily on the task.
Easily distinguishable tasks (story generation), require only 10 crowdworkers, while less distinguishable tasks (summarization) require more than
20 crowdworkers to obtain accurate estimates.
Related work
The current state of NLG evaluation.
approaches to NLG evaluation use a hodgepodge
mix of quality and diversity measures. Out of the
26 NLG papers at ACL 2018, six perform only human evaluation, fourteen measure human evaluation and a diversity metric such as perplexity or
n-gram diversity, and six do not evaluate using human judgments.
While perplexity and n-gram counts can in
principle evaluate diversity, their practical implementations suffer from serious drawbacks. When
human evaluation and perplexity are both evaluated, they are almost always done on separate
Quality failure
log pmodel
Two new vaccines have been shown effective against rotavirus, which is responsible for a
half-million infant deaths in poor countries each year, research studies published Wednesday said.
Two new vaccines in the poor countries were effective against go-it-alone study says
New vaccines for key <UNK> virus shown effective
Diversity failure
The Buffalo Bills sacked Tom Donahoe as president and general manager on Wednesday,
fulﬁlling expectations of a shake-up after another failure to make the National Football
League playoffs.
Bills sack <UNK> as president GM and general manager
NFL’s Bills shake up front ofﬁce.
Model is indistinguishable
US veteran and eight-time Grand Slam winner Andre Agassi has withdrawn from this
month’s Australian Open due to a nagging ankle injury, his management team announced
Agassi bows out of Australian Open after injury.
Agassi withdraws from Australian Open.
Table 2. Example reference and model outputs (capitalization added for readability) corresponding to Figure 4
(summarization task) that were shown to crowdworkers (left column). Crowdworkers were shown samples from the
model (including the <UNK> token) and returned human judgments (right column). Using human judgments and the
model probability, we can identify several types of failures. Quality failures are examples that are classiﬁed by human
judgment. Diversity failures are examples that are classiﬁed by model probabilities. Finally some examples are not
easily classiﬁed, as they have similar human judgment and model probability scores.
models—human evaluations are done on beamsearched output, while perplexity is computed on
the softmax outputs. This makes it appear as if the
models can simultaneously generate high quality
outputs while also being diverse, when in fact they
can only be one at a time based on whether they
sample or run beam search.
On the other hand, n-gram diversity was proposed by Li et al. to identify models with
the generic utterance problem where models repeat phrases such as ‘I don’t know’.
Unfortunately, n-gram diversity is computed across contexts by counting the number of unique n-grams
generated, and so does not measure a model’s ability to generate multiple valid utterances at any single context.
In particular, a model which only
outputs a single memorized utterance per context
(e.g., via memorization or retrieval) can still have
high n-gram diversity as long as the memorized
sentences differ across contexts.
Finally, all existing diversity measures are computed separately from human evaluation.
results in two incomparable evaluation metrics,
which prevent us from reasoning about tradeoffs
between diversity and quality. In contrast, HUSE
allows us to make precise statements about the
tradeoffs between model quality and diversity because it is a single metric which decomposes into
diversity and quality terms.
Related evaluations of diversity.
The importance of diverse responses has previously been acknowledged for summarization and information retrieval . Our work differs in considering a single
evaluation measure that captures quality and diversity applicable to any generation task.
Automated metrics based on n-gram overlap
such as BLEU, METEOR, ROUGE work well for machine translation but
do not generalize well to domains with a diverse
spectrum of correct responses.
While variants
 have adapted such metrics
to high entropy generative environments, they are
still signiﬁcantly inferior to the human judgments
they attempt to mimic.
Caccia et al. recently examined the diversity and quality tradeoffs for different language
model architectures on synthetic datasets. However, as their approach relies on measuring loglikelihoods under both the model and reference
distributions, it cannot be applied to real data
where pref is unavailable. Our main conceptual
contribution overcomes this by showing that HJ is
an acceptable proxy for pref.
Sajjadi et al. also examines diversity and
quality (which they call precision and recall) in
the context of generative image models.
However, they rely on assuming that pref and pmodel
can be estimated accurately using the Fr´echet
Inception Distance (FID) .
HUSE avoids such assumptions and instead directly leverages human judgments, resulting in a
simple and reliable metric more suitable for use as
a gold-standard.
Estimating optimal classiﬁcation error.
Evaluating a model by estimating its optimal classiﬁcation error has been considered by several earlier
works .
However, these methods have focused on classifying sentences directly,
which is quite challenging to do reliably. Existing adversarial evaluation methods do not yet reliably outperform human classiﬁcation .
We propose the use of both human evaluation and
model probabilities as part of the adversarial evaluation framework, and demonstrate that the resulting classiﬁer reliably outperforms humans and
captures both the sample quality and diversity of a
Distributional
divergence
estimation.
proposed evaluation metric is closely related to
the total variation distance which has been studied
extensively in the distribution testing literature.
It is known that total variation distance estimates
have pessimistic minimax estimation rates in
high dimensions (Balakrishnan and Wasserman,
Our work overcomes this by utilizing
pmodel and an estimate of pref. Other approaches to
distributional testing include the maximum mean
discrepancy (MMD) and Wasserstein distances,
but these approaches require knowledge of a
ground truth metric or kernel space . Although such
divergences are easier to estimate than the total
variation distance from samples, the implied convergence rates are still too slow to be practically
Discussion
In this paper, we demonstrate that the current gold
standard of human evaluation does not penalize
under-diverse models. To remedy this, we propose HUSE, a general purpose evaluation strategy which can be applied to any model for which
we can calculate a model’s sampling probabilities.
HUSE is an upper bound on the optimal classiﬁcation error of distinguishing reference and modelgenerated text, and never does worse than human
classiﬁcation. HUSE leverages both model probabilities and human judgments, ensuring that models which do well on the metric are both highquality and diverse.
Our work can be viewed as a “superhuman version” of the classic Turing Test .
Instead of relying on just a human classiﬁer, we
approximate the optimal classiﬁer, which can utilize information about the model in addition to the
reference. We also modify the classiﬁcation problem and seek to identify whether a sample comes
from a (potentially superhuman) reference distribution, rather than the human distribution. These
two changes lead to tractable, rigorous estimators
which can quantify tradeoffs between model quality and diversity on a wide range of generation
Acknowledgements. We would like to thank
Arun Chaganty, Robin Jia, and Peng Qi for extensive comments and feedback on the paper. This
work was funded by DARPA CwC program under
ARO prime contract no. W911NF-15-1-0462.
Reproducibility.
All code, data, and experiments
 
codalab.org/worksheets/
0x88644b5ee189402eb19d39d721d1005c.