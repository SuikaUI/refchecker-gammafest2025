Image biomarker standardisation initiative
Reference manual
 
The image biomarker
standardisation initiative
The image biomarker standardisation initiative (IBSI) is an independent international collaboration
which works towards standardising the extraction of image biomarkers from acquired imaging for
the purpose of high-throughput quantitative image analysis (radiomics). Lack of reproducibility
and validation of radiomic studies is considered to be a major challenge for the ﬁeld. Part of
this challenge lies in the scantiness of consensus-based guidelines and deﬁnitions for the process
of translating acquired imaging into high-throughput image biomarkers. The IBSI therefore seeks
to provide standardised image biomarker nomenclature and deﬁnitions, a standardised general
image processing workﬂow, tools for verifying radiomics software implementations and reporting
guidelines for radiomic studies.
Permanent identiﬁers
The IBSI uses permanent identiﬁers for image biomarker deﬁnitions and important related
concepts such as image processing. These consist of four-character codes and may be used for
reference. Please do not use page numbers or section numbers as references, as these are subject
to change.
This work is a copy-edited version of the ﬁnal (v10) pre-print version of the IBSI reference
manual, which was licensed under the Creative Commons Attribution 4.0 International License
(CC-BY). The original work may be cited as: Zwanenburg A, Leger S, Valli`eres M, L¨ock S. Image
biomarker standardisation initiative. arXiv preprint arXiv:1612.07003.
This document is licensed under the Creative Commons Attribution 4.0 International License.
To view a copy of this license, visit or send a letter
to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.
The digital phantom (see section 5.1) is licensed under the Creative Commons Attribution 4.0
International License. To view a copy of this license, visit 
or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.
The radiomics phantom (see section 5.2), which is based on a human lung cancer computed
tomography image and published by cancerdata.org , is licensed
under the Creative Commons Attribution-NonCommercial 3.0 Unported Licence. To view a copy
of this license, visit or send a letter to Creative
Commons, PO Box 1866, Mount View, CA 94042, USA. This license pertains to both the original
DICOM set, as well as the same data in NIfTI format released by the IBSI.
Copyright information regarding the reference data sets may be found on GitHub: 
Citation information
To cite the document or the digital phantom, please use the following citation:
1. Zwanenburg A, Leger S, Valli`eres M, L¨ock S. Image biomarker standardisation initiative.
arXiv preprint arXiv:1612.07003.
Additionally, when using the radiomics phantom originally published on cancerdata.org, please
include the following citation:
1. Lambin P, Leijenaar RT, Deist TM, Peerlings J, de Jong EE, van Timmeren J, Sanduleanu S,
Larue RT, Even AJ, Jochems A, van Wijk Y. Radiomics: the bridge between medical imaging
and personalized medicine. Nature Reviews Clinical Oncology. 2017 Dec;14(12):749.
2. Lambin P. Radiomics Digital Phantom, CancerData , DOI:10.17195/candat.2016.08.1
Dr. Alex Zwanenburg
 
Dr. Martin Valli`eres
 
IBSI contributors
Mahmoud A. Abdalah
Image Response Assessment Team Core Facility, Moﬃtt Cancer
Center, Tampa (FL), USA
Hugo J.W.L. Aerts
Dana-Farber Cancer Institute, Brigham and Women’s Hospital,
and Harvard Medical School, Harvard University, Boston (MA),
Vincent Andrearczyk
Institute of Information Systems, University of Applied Sciences
Western Switzerland (HES-SO), Sierre, Switzerland
Aditya Apte
Department of Medical Physics, Memorial Sloan Kettering Cancer
Center, New York (NY), USA
Saeed Ashraﬁnia
Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore (MD), USA and Department of Radiology and Radiological Science, Johns Hopkins University, Baltimore (MD), USA
Spyridon Bakas
Center for Biomedical image Computing and Analytics (CBICA),
University of Pennsylvania, Philadelphia (PA), USA and Department of Radiology, Perelman School of Medicine, University of
Pennsylvania, Philadelphia (PA), USA and Department of Pathology and Laboratory Medicine, Perelman School of Medicine, University of Pennsylvania, Philadelphia (PA), USA
Roelof J. Beukinga
Department of Nuclear Medicine and Molecular Imaging, University of Groningen, University Medical Center Groningen (UMCG),
Groningen, The Netherlands
Ronald Boellaard
Department of Nuclear Medicine and Molecular Imaging, University of Groningen, University Medical Center Groningen (UMCG),
Groningen, The Netherlands and Radiology and Nuclear Medicine, VU University Medical Centre (VUMC), Amsterdam, The
Netherlands
continued on next page
Marta Bogowicz
Department of Radiation Oncology, University Hospital Zurich,
University of Zurich, Zurich, Switzerland
Luca Boldrini
Fondazione Policlinico Universitario ””A. Gemelli”” IRCCS,
Roma, Italia
Ir`ene Buvat
Imagerie Mol´eculaire In Vivo, CEA, Inserm, Univ Paris Sud,
CNRS, Universit Paris Saclay, Orsay, France
Gary J.R. Cook
Cancer Imaging Dept, School of Biomedical Engineering and Imaging Sciences, Kings College London, London, United Kingdom
Christos Davatzikos
Center for Biomedical image Computing and Analytics (CBICA),
University of Pennsylvania, Philadelphia (PA), USA and Department of Radiology, Perelman School of Medicine, University of
Pennsylvania, Philadelphia (PA), USA
Adrien Depeursinge
Institute of Information Systems, University of Applied Sciences
Western Switzerland (HES-SO), Sierre, Switzerland and Department of Nuclear Medicine and Molecular Imaging, Lausanne University Hospital, Lausanne, Switzerland
Marie-Charlotte Desseroit
Laboratory of Medical Information Processing (LaTIM) - team
ACTION (image-guided therapeutic action in oncology), IN-
SERM, UMR 1101, IBSAM, UBO, UBL, Brest, France
Nicola Dinapoli
Fondazione Policlinico Universitario ””A. Gemelli”” IRCCS,
Roma, Italia
Cuong Viet Dinh
Department of Radiation Oncology, the Netherlands Cancer Institute (NKI), Amsterdam, The Netherlands
Sebastian Echegaray
Department of Radiology, Stanford University School of Medicine,
Stanford (CA), USA
Issam El Naqa
Department of Radiation Oncology, Physics Division, University
of Michigan, Ann Arbor (MI), USA and Medical Physics Unit,
McGill University, Montr´eal, Qubec, Canada
Andriy Y. Fedorov
Surgical Planning Laboratory, Brigham and Women’s Hospital
and Harvard Medical School, Harvard University, Boston (MA),
Roberto Gatta
Fondazione Policlinico Universitario ””A. Gemelli”” IRCCS,
Rome, Italy
Robert J. Gillies
Department of Cancer Imaging and Metabolism, Moﬃtt Cancer
Center, Tampa (FL), USA
Cancer Imaging Dept, School of Biomedical Engineering and Imaging Sciences, Kings College London, London, United Kingdom
Matthias Guckenberger
Department of Radiation Oncology, University Hospital Zurich,
University of Zurich, Zurich, Switzerland
Michael G¨otz
Department of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
Sung Min Ha
Center for Biomedical image Computing and Analytics (CBICA),
University of Pennsylvania, Philadelphia (PA), USA and Department of Radiology, Perelman School of Medicine, University of
Pennsylvania, Philadelphia (PA), USA
Mathieu Hatt
Laboratory of Medical Information Processing (LaTIM) - team
ACTION (image-guided therapeutic action in oncology), IN-
SERM, UMR 1101, IBSAM, UBO, UBL, Brest, France
continued on next page
Fabian Isensee
Department of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
Philippe Lambin
The D-Lab, Department of Precision Medicine, GROW-School
for Oncology and Developmental Biology, Maastricht University
Medical Centre+, Maastricht, The Netherlands
Baptiste Laurent
Laboratory of Medical Information Processing (LaTIM) - team
ACTION (image-guided therapeutic action in oncology), IN-
SERM, UMR 1101, IBSAM, UBO, UBL, Brest, France
Stefan Leger
National Center for Tumor Diseases (NCT), Partner Site Dresden,
German Cancer Research Center (DKFZ), Heidelberg, Germany; Faculty of Medicine and University Hospital Carl
Gustav Carus, Technische Universit¨at Dresden, Dresden, Germany, and; Helmholtz Association / Helmholtz-Zentrum Dresden
- Rossendorf (HZDR), Dresden, Germany and OncoRay – National Center for Radiation Research in Oncology, Faculty of
Medicine and University Hospital Carl Gustav Carus, Technische
Universitt Dresden, Helmholtz-Zentrum Dresden - Rossendorf,
Dresden, Germany and German Cancer Consortium (DKTK),
Partner Site Dresden, and German Cancer Research Center
(DKFZ), Heidelberg, Germany
Ralph T.H. Leijenaar
The D-Lab, Department of Precision Medicine, GROW-School
for Oncology and Developmental Biology, Maastricht University
Medical Centre+, Maastricht, The Netherlands
Jacopo Lenkowicz
Fondazione Policlinico Universitario ””A. Gemelli”” IRCCS,
Rome, Italy
Fiona Lippert
Section for Biomedical Physics, Department of Radiation Oncology, University of T¨ubingen, Germany
Are Losneg˚ard
Department of Clinical Medicine, University of Bergen, Bergen,
Steﬀen L¨ock
OncoRay – National Center for Radiation Research in Oncology,
Faculty of Medicine and University Hospital Carl Gustav Carus,
Technische Universit¨at Dresden, Helmholtz-Zentrum Dresden -
Rossendorf, Dresden, Germany and German Cancer Consortium
(DKTK), Partner Site Dresden, and German Cancer Research
Center (DKFZ), Heidelberg, Germany and Department of Radiotherapy and Radiation Oncology, Faculty of Medicine and University Hospital Carl Gustav Carus, Technische Universitt Dresden,
Dresden, Germany
Klaus H. Maier-Hein
Department of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
Sarah A. Mattonen
Department of Medical Biophysics, The University of Western
Ontario, London (ON), Canada
Olivier Morin
Department of Radiation Oncology, University of California, San
Francisco (CA), USA
Henning M¨uller
Institute of Information Systems, University of Applied Sciences
Western Switzerland (HES-SO), Sierre, Switzerland and University of Geneva, Geneva, Switzerland
continued on next page
Sandy Napel
Department of Radiology, Stanford University School of Medicine, Stanford (CA), USA and Department of Electrical Engineering, Stanford University, Stanford (CA), USA and Department
of Medicine (Biomedical Informatics Research), Stanford University School of Medicine, Stanford (CA), USA
Christophe Nioche
Imagerie Mol´eculaire In Vivo, CEA, Inserm, Univ Paris Sud,
CNRS, Universit Paris Saclay, Orsay, France
Fanny Orlhac
Imagerie Mol´eculaire In Vivo, CEA, Inserm, Univ Paris Sud,
CNRS, Universit Paris Saclay, Orsay, France
Sarthak Pati
Center for Biomedical image Computing and Analytics (CBICA),
University of Pennsylvania, Philadelphia (PA), USA and Department of Radiology, Perelman School of Medicine, University of
Pennsylvania, Philadelphia (PA), USA
Elisabeth A.G. Pfaehler
Department of Nuclear Medicine and Molecular Imaging, University of Groningen, University Medical Center Groningen (UMCG),
Groningen, The Netherlands
Arman Rahmim
Departments of Radiology and Physics, University of British
Columbia, Vancouver (BC), Canada and Department of Radiology and Radiological Science, Johns Hopkins University, Baltimore (MD), USA
Arvind U.K. Rao
Department of Computational Medicine and Bioinformatics, University of Michigan, Ann Arbor (MI), USA and Department of Radiation Oncology, Physics Division, University of Michigan, Ann
Arbor (MI), USA
Christian Richter
OncoRay – National Center for Radiation Research in Oncology,
Faculty of Medicine and University Hospital Carl Gustav Carus,
Technische Universit¨at Dresden, Helmholtz-Zentrum Dresden -
Rossendorf, Dresden, Germany and German Cancer Consortium
(DKTK), Partner Site Dresden, and German Cancer Research
Center (DKFZ), Heidelberg, Germany and Department of Radiotherapy and Radiation Oncology, Faculty of Medicine and University Hospital Carl Gustav Carus, Technische Universitt Dresden,
Dresden, Germany and Helmholtz-Zentrum Dresden - Rossendorf,
Institute of Radiooncology OncoRay, Dresden, Germany
Jonas Scherer
Department of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany
Muhammad Musib Siddique
Cancer Imaging Dept, School of Biomedical Engineering and Imaging Sciences, Kings College London, London, United Kingdom
Nanna M. Sijtsema
Department of Radiation Oncology, University of Groningen,
University Medical Center Groningen (UMCG), Groningen, The
Netherlands
Jairo Socarras Fernandez
Section for Biomedical Physics, Department of Radiation Oncology, University of T¨ubingen, Germany
Emiliano Spezi
School of Engineering, CardiﬀUniversity, Cardiﬀ, United Kingdom and Department of Medical Physics, Velindre Cancer Centre,
Cardiﬀ, United Kingdom
Roel J.H.M Steenbakkers
Department of Radiation Oncology, University of Groningen,
University Medical Center Groningen (UMCG), Groningen, The
Netherlands
continued on next page
Stephanie Tanadini-Lang
Department of Radiation Oncology, University Hospital Zurich,
University of Zurich, Zurich, Switzerland
Daniela Thorwarth
Section for Biomedical Physics, Department of Radiation Oncology, University of T¨ubingen, Germany
Esther G.C. Troost
OncoRay – National Center for Radiation Research in Oncology,
Faculty of Medicine and University Hospital Carl Gustav Carus,
Technische Universit¨at Dresden, Helmholtz-Zentrum Dresden -
Rossendorf, Dresden, Germany and National Center for Tumor
Diseases (NCT), Partner Site Dresden, Germany: German Cancer
Research Center (DKFZ), Heidelberg, Germany; Faculty of Medicine and University Hospital Carl Gustav Carus, Technische Universitt Dresden, Dresden, Germany, and; Helmholtz Association
/ Helmholtz-Zentrum Dresden - Rossendorf (HZDR), Dresden,
Germany and German Cancer Consortium (DKTK), Partner
Site Dresden, and German Cancer Research Center (DKFZ),
Heidelberg, Germany and Department of Radiotherapy and Radiation Oncology, Faculty of Medicine and University Hospital Carl
Gustav Carus, Technische Universitt Dresden, Dresden, Germany
and Helmholtz-Zentrum Dresden - Rossendorf, Institute of Radiooncology OncoRay, Dresden, Germany
Taman Upadhaya
Department of Nuclear Medicine, CHU Mil´etrie, Poitiers, France
and Laboratory of Medical Information Processing (LaTIM) team ACTION (image-guided therapeutic action in oncology), IN-
SERM, UMR 1101, IBSAM, UBO, UBL, Brest, France
Vincenzo Valentini
Fondazione Policlinico Universitario ””A. Gemelli”” IRCCS,
Rome, Italy
Martin Valli`eres
Medical Physics Unit,
McGill University,
Montr´eal,
Lisanne V. van Dijk
Department of Radiation Oncology, University of Groningen,
University Medical Center Groningen (UMCG), Groningen, The
Netherlands
Joost van Griethuysen
Department of Radiology,
the Netherlands Cancer Institute
(NKI), Amsterdam, The Netherlands and GROW-School for Oncology and Developmental Biology, Maastricht University Medical
Center, Maastricht, The Netherlands and Department of Radiation Oncology, Dana-Farber Cancer Institute, Brigham and Womens Hospital, Harvard Medical School, Boston (MA), USA
Floris H.P. van Velden
Department of Radiology, Leiden University Medical Center
(LUMC), Leiden, The Netherlands
Philip Whybra
School of Engineering, CardiﬀUniversity, Cardiﬀ, United Kingdom
continued on next page
Alex Zwanenburg
OncoRay – National Center for Radiation Research in Oncology,
Faculty of Medicine and University Hospital Carl Gustav Carus,
Technische Universit¨at Dresden, Helmholtz-Zentrum Dresden -
Rossendorf, Dresden, Germany and National Center for Tumor
Diseases (NCT), Partner Site Dresden, Germany: German Cancer
Research Center (DKFZ), Heidelberg, Germany; Faculty of Medicine and University Hospital Carl Gustav Carus, Technische Universitt Dresden, Dresden, Germany, and; Helmholtz Association
/ Helmholtz-Zentrum Dresden - Rossendorf (HZDR), Dresden,
Germany and German Cancer Consortium (DKTK), Partner Site
Dresden, and German Cancer Research Center (DKFZ), Heidelberg, Germany
Table 1 — Alphabetical list of IBSI contributors.
Introduction
Image processing
Data conversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Post-acquisition processing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Interpolation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Re-segmentation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
ROI extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Intensity discretisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Feature calculation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Image features
Morphological features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Local intensity features
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Intensity-based statistical features
. . . . . . . . . . . . . . . . . . . . . . . . . . .
Intensity histogram features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Intensity-volume histogram features
. . . . . . . . . . . . . . . . . . . . . . . . . .
Grey level co-occurrence based features
. . . . . . . . . . . . . . . . . . . . . . . .
Grey level run length based features
. . . . . . . . . . . . . . . . . . . . . . . . . .
Grey level size zone based features . . . . . . . . . . . . . . . . . . . . . . . . . . .
Grey level distance zone based features . . . . . . . . . . . . . . . . . . . . . . . . .
3.10 Neighbourhood grey tone diﬀerence based features . . . . . . . . . . . . . . . . . .
3.11 Neighbouring grey level dependence based features . . . . . . . . . . . . . . . . . .
Radiomics reporting guidelines and nomenclature
Reporting guidelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Feature nomenclature
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Reference data sets
Digital phantom
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Lung cancer CT image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A Digital phantom texture matrices
A.1 Grey level co-occurrence matrix (2D) . . . . . . . . . . . . . . . . . . . . . . . . . .
A.2 Grey level co-occurrence matrix (2D, merged) . . . . . . . . . . . . . . . . . . . . .
A.3 Grey level co-occurrence matrix (3D) . . . . . . . . . . . . . . . . . . . . . . . . . .
A.4 Grey level co-occurrence matrix (3D, merged) . . . . . . . . . . . . . . . . . . . . .
A.5 Grey level run length matrix (2D)
. . . . . . . . . . . . . . . . . . . . . . . . . . .
A.6 Grey level run length matrix (2D, merged) . . . . . . . . . . . . . . . . . . . . . . .
A.7 Grey level run length matrix (3D)
. . . . . . . . . . . . . . . . . . . . . . . . . . .
A.8 Grey level run length matrix (3D, merged) . . . . . . . . . . . . . . . . . . . . . . .
A.9 Grey level size zone matrix (2D)
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.10 Grey level size zone matrix (3D)
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.11 Grey level distance zone matrix (2D) . . . . . . . . . . . . . . . . . . . . . . . . . .
A.12 Grey level distance zone matrix (3D) . . . . . . . . . . . . . . . . . . . . . . . . . .
A.13 Neighbourhood grey tone diﬀerence matrix (2D)
. . . . . . . . . . . . . . . . . . .
A.14 Neighbourhood grey tone diﬀerence matrix (3D)
. . . . . . . . . . . . . . . . . . .
A.15 Neighbouring grey level dependence matrix (2D) . . . . . . . . . . . . . . . . . . .
A.16 Neighbouring grey level dependence matrix (3D) . . . . . . . . . . . . . . . . . . .
Introduction
A biomarker is ”a characteristic that is objectively measured and evaluated as an indicator of
normal biological processes, pathogenic processes, or pharmacologic responses to a therapeutic intervention”7. Biomarkers may be measured from a wide variety of sources, such as tissue samples,
cell plating, and imaging. The latter are often referred to as imaging biomarkers55. Imaging biomarkers consist of both qualitative biomarkers, which require expert interpretation, and quantitative biomarkers which are based on mathematical deﬁnitions. Calculation of quantitative imaging
biomarkers can be automated, which enables high-throughput analyses. We refer to such (highthroughput) quantitative biomarkers as image biomarkers to diﬀerentiate them from qualitative
imaging biomarkers. Image biomarkers characterise the contents of (regions of) an image, such
as volume or mean intensity.
Because of the historically close relationship with the computer
vision ﬁeld, image biomarkers are also referred to as image features. The term features, instead
of biomarkers, will be used throughout the remainder of the reference manual, as the contents are
generally applicable and not limited to life sciences and medicine only.
This work focuses speciﬁcally on the (high-throughput) extraction of image biomarkers from
acquired, reconstructed and stored imaging.
High-throughput quantitative image analysis (radiomics) has shown considerable growth in e.g. cancer research41, but the scarceness of consensus
guidelines and deﬁnitions has led to it being described as a ”wild frontier”13.
This reference
manual therefore presents an eﬀort to chart a course through part of this frontier by presenting
consensus-based recommendations, guidelines, deﬁnitions and reference values for image biomarkers and deﬁning a general radiomics image processing scheme. We hope use of this manual will
improve reproducibility of radiomic studies.
We opted for a speciﬁc focus on the computation of image biomarkers from acquired imaging.
Thus, validation of imaging biomarkers, either viewed in a broader framework such as the one
presented by O’Connor et al. 55, or within smaller-scope settings such as those presented by Caicedo
et al. 13 and by Lambin et al. 41, falls beyond the scope of this work. Notably, the issue of harmonising and standardising (medical) image acquisition and reconstruction is being addressed in a
more comprehensive manner by groups such as the Quantitative Imaging Biomarker Alliance53,68,
the Quantitative Imaging Network17,54, and task groups and committees of the American Association of Physicists in Medicine, the European Association for Nuclear Medicine11, the European
Society of Radiology (ESR)28, and the European Organisation for Research and Treatment of
Cancer (EORTC)55,86, among others. Where overlap does exists, the reference manual refers to
existing recommendations and guidelines.
This reference manual is divided into several chapters that describe processing of acquired and
reconstructed (medical) imaging for high-throughput computation of image biomarkers (Chapter
2); that deﬁne a diverse set of image biomarkers (Chapter 3); that describe guidelines for report-
CHAPTER 1. INTRODUCTION
ing on radiomic studies and provide nomenclature for image biomarkers (Chapter 4); and that
describe the data sets and image processing conﬁgurations used to ﬁnd reference values for image
biomarkers (Chapter 5).
Image processing
Image processing is the sequence of operations required to derive image biomarkers (features) from
acquired images. In the context of this work an image is deﬁned as a three-dimensional (3D) stack
of two-dimensional (2D) digital image slices. Image slices are stacked along the z-axis. This stack
is furthermore assumed to possess the same coordinate system, i.e. image slices are not rotated or
translated (in the xy-plane) with regard to each other. Moreover, digital images typically possess
a ﬁnite resolution. Intensities in an image are thus located at regular intervals, or spacing. In 2D
such regular positions are called pixels, whereas in 3D the term voxels is used. Pixels and voxels
are thus represented as the intersections on a regularly spaced grid. Alternatively, pixels and voxels
may be represented as rectangles and rectangular cuboids. The centers of the pixels and voxels
then coincide with the intersections of the regularly spaced grid. Both representations are used in
the document.
Pixels and voxels contain an intensity value for each channel of the image. The number of
channels depends on the imaging modality. Most medical imaging generates single-channel images,
whereas the number of channels in microscopy may be greater, e.g. due to diﬀerent stainings. In
such multi-channel cases, features may be extracted for each separate channel, a subset of channels,
or alternatively, channels may be combined and converted to a single-channel representation. In
the remainder of the document we consider an image as if it only possesses a single channel.
The intensity of a pixel or voxel is also called a grey level or grey tone, particularly in singlechannel images. Though practically there is no diﬀerence, the terms grey level or grey tone are
more commonly used to refer to discrete intensities, including discretised intensities.
Image processing may be conducted using a wide variety of schemes. We therefore designed
a general image processing scheme for image feature calculation based on schemes used within
scientiﬁc literature38. The image processing scheme is shown in ﬁgure 2.1. The processing steps
referenced in the ﬁgure are described in detail within this chapter.
Data conversion
Some imaging modalities require conversion of raw image data into a more meaningful presentation,
standardised uptake values (SUV)11.
This is performed during the data conversion step.
Assessment of data conversion methods falls outside the scope of the current work.
CHAPTER 2. IMAGE PROCESSING
Image data
Segmentation
Voxel interpolation
Feature data
Data conversion
Region of interest
Image interpolation
ROI interpolation
Re-segmentation
ROI extraction
Discretisation
Intensity mask
Morphological mask
Feature calculation
Calculation
local intensity
Calculation
Calculation
Calculation
Calculation
IH, IVH*, GLCM, GLRLM
GLSZM, NGTDM, NGLDM
morphological
statistical
post-acquisition
processing
Figure 2.1 — Image processing scheme for image feature calculation.
Depending on the speciﬁc
imaging modality and purpose, some steps may be omitted. The region of interest (ROI) is explicitly
split into two masks, namely an intensity and morphological mask, after interpolation to the same
grid as the interpolated image. Feature calculation is expanded to show the diﬀerent feature families
with speciﬁc pre-processing. IH: intensity histogram; IVH: intensity-volume histogram; GLCM: grey
level cooccurrence matrix; GLRLM: grey level run length matrix; GLSZM: grey level size zone matrix;
NGTDM: neighbourhood grey tone diﬀerence matrix; NGLDM: Neighbouring grey level dependence
matrix; GLDZM: grey level distance zone matrix; *Discretisation of IVH diﬀers from IH and texture
features, see section 3.5.
CHAPTER 2. IMAGE PROCESSING
Image post-acquisition processing
Images are post-processed to enhance image quality. For instance, magnetic resonance imaging
(MRI) contains both Gaussian and Rician noise33 and may beneﬁt from denoising. As another
example, intensities measured using MR may be non-uniform across an image and could require
correction9,61,84. FDG-PET-based may furthermore be corrected for partial volume eﬀects12,66 and
noise26,43. In CT imaging, metal objects, e.g. pacemakers and tooth implants, introduce artifacts
and may require artiﬁnterpact suppression32. Microscopy images generally beneﬁt from ﬁeld-ofview illumination correction as illumination is usually inhomogeneous due to the light-source or
the optical path13,62.
Evaluation and standardisation of various image post-acquisition processing methods falls outside the scope of the current work.
Note that vendors may provide or implement software to
perform noise reduction and other post-processing during image reconstruction. In such cases,
additional post-acquisition processing may not be required.
Segmentation
High-throughput image analysis, within the feature-based paradigm, relies on the deﬁnition of
regions of interest (ROI). ROIs are used to deﬁne the region in which features are calculated.
What constitutes an ROI depends on the imaging and the study objective. For example, in 3D
microscopy of cell plates, cells are natural ROIs. In medical imaging of cancer patients, the tumour
volume is a common ROI. ROIs can be deﬁned manually by experts or (semi-)automatically using
algorithms.
From a process point-of-view, segmentation leads to the creation of an ROI mask R, for which
every voxel j ∈R (Rj) is deﬁned as:
ROIs are typically stored with the accompanying image. Some image formats directly store
ROI masks as voxels (e.g. NIfTI, NRRD and DICOM Segmentation), and generating the ROI mask is
conducted by loading the corresponding image. In other cases the ROI is saved as a set of (x, y, z)
points that deﬁne closed loops of (planar) polygons, for example within DICOM RTSTRUCT or DICOM
SR ﬁles. In such cases, we should determine which voxel centers lie within the space enclosed by
the contour polygon in each slice to generate the ROI mask.
A common method to determine whether a point in an image slice lies inside a 2D polygon
is the crossing number algorithm, for which several implementations exist58. The main concept
behind this algorithm is that for any point inside the polygon, any line originating outside the
polygon will cross the polygon an uneven number of times. A simple example is shown in ﬁgure
2.2. The implementation in the example makes use of the fact that the ROI mask is a regular grid
to scan entire rows at a time. The example implementation consists of the following steps:
1. (optional) A ray is cast horizontally from outside the polygon for each of the n image rows.
As we iterate over the rows, it is computationally beneﬁcial to exclude polygon edges that
will not be crossed by the ray for the current row j. If the current row has y-coordinate yj,
and edge k has two vertices with y-coordinates yk1 and yk2, the ray will not cross the edge
if both vertices lie either above or below yj, i.e. yj < yk1, yk2 or yj > yk1, yk2. For each row
j, ﬁnd those polygon edges whose y-component of the vertices do not both lie on the same
CHAPTER 2. IMAGE PROCESSING
side of the row coordinate yj. This step is used to limit calculation of intersection points to
only those that cross a ray cast from outside the polygon – e.g. ray with origin (−1, yj) and
direction (1, 0). This an optional step.
2. Determine intersection points xi of the (remaining) polygon edges with the ray.
3. Iterate over intersection points and add 1 to the count of each pixel center with x ≥xi.
4. Apply the even-odd rule. Pixels with an odd count are inside the polygon, whereas pixels
with an even count are outside.
Note that the example represents a relatively naive implementation that will not consistently
assign voxel centers positioned on the polygon itself to the interior.
grid with polygon contour
1. find intersecting polygons
2. find ray-polygon intersection
3. count intersections along line
4. apply even-odd rule
Figure 2.2 — Simple algorithm to determine which pixels are inside a 2D polygon. The suggested
implementation consists of four steps: (1) Omit edges that will not intersect with the current row of
voxel centers. (2) Calculate intersection points of edges I and II with the ray for the current row. (3)
Determine the number of intersections crossed from ray origin to the row voxel centers. (4) Apply
even-odd rule to determine whether voxel centers are inside the polygon.
Interpolation
Texture feature sets require interpolation to isotropic voxel spacing to be rotationally invariant,
and to allow comparison between image data from diﬀerent samples, cohorts or batches. Voxel
interpolation aﬀects image feature values as many image features are sensitive to changes in voxel
size4,8,59,60,87. Maintaining consistent isotropic voxel spacing across diﬀerent measurements and
devices is therefore important for reproducibility. At the moment there are no clear indications
whether upsampling or downsampling schemes are preferable. Consider, for example, an image
stack of slices with 1.0 × 1.0 × 3.0 mm3 voxel spacing. Upsampling to 1.0 × 1.0 × 1.0 mm3 requires
inference and introduces artiﬁcial information, while conversely downsampling to the largest dimension (3.0×3.0×3.0 mm3) incurs information loss. Multiple-scaling strategies potentially oﬀer a
good trade-oﬀ78. Note that downsampling may introduce image aliasing artifacts. Downsampling
may therefore require anti-aliasing ﬁlters prior to ﬁltering49,90.
While in general 3D interpolation algorithms are used to interpolate 3D images, 2D interpolation within the image slice plane may be recommended in some situations. In 2D interpolation
voxels are not interpolated between slices. This may be beneﬁcial if, for example, the spacing
between slices is large compared to the desired voxel size, and/or compared to the in-plane spacing. Applying 3D interpolation would either require inferencing a large number of voxels between
CHAPTER 2. IMAGE PROCESSING
slices (upsampling), or the loss of a large fraction of in-plane information (downsampling). The
disadvantage of 2D interpolation is that voxel spacing is no longer isotropic, and as a consequence
texture features can only be calculated in-plane.
Interpolation algorithms
Interpolation algorithms translate image intensities from the original image grid to an interpolation grid. In such grids, voxels are spatially represented by their center. Several algorithms
are commonly used for interpolation, such as nearest neighbour, trilinear, tricubic convolution and
tricubic spline interpolation. In short, nearest neighbour interpolation assigns the intensity of the
most nearby voxel in the original grid to each voxel in the interpolation grid. Trilinear interpolation
uses the intensities of the eight most nearby voxels in the original grid to calculate a new interpolated intensity using linear interpolation. Tricubic convolution and tricubic spline interpolation
draw upon a larger neighbourhood to evaluate a smooth, continuous third-order polynomial at the
voxel centers in the interpolation grid. The diﬀerence between tricubic convolution and tricubic
spline interpolation lies in the implementation. Whereas tricubic spline interpolation evaluates the
smooth and continuous third-order polynomial at every voxel center, tricubic convolution approximates the solution using a convolution ﬁlter. Though tricubic convolution is faster, with modern
hardware and common image sizes, the diﬀerence in execution speed is practically meaningless.
Both interpolation algorithms produce similar results, and both are often referred to as tricubic
interpolation.
While no consensus exists concerning the optimal choice of interpolation algorithm, trilinear
interpolation is usually seen as a conservative choice. It does not lead to the blockiness produced
by nearest neighbour interpolation that introduces bias in local textures38. Nor does it lead to
out-of-range intensities which may occur due to overshoot with tricubic and higher order interpolations. The latter problem can occur in acute intensity transitions, where the local neighbourhood
itself is not suﬃciently smooth to evaluate the polynomial within the allowed range.
methods, however, may retain tissue contrast diﬀerences better. Particularly when upsampling,
trilinear interpolation may act as a low-pass ﬁlter which suppresses higher spatial frequencies
and cause artefacts in high-pass spatial ﬁlters. Interpolation algorithms and their advantages and
disadvantages are treated in more detail elsewhere, e.g. Th´evenaz et al. 70.
In a phantom study, Larue et al. 42 compared nearest neighbour, trilinear and tricubic interpolation and indicated that feature reproducibility is dependent on the selected interpolation
algorithm, i.e. some features were more reproducible using one particular algorithm.
Rounding image intensities after interpolation
Image intensities may require rounding after interpolation, or the application of cut-oﬀvalues.
For example, in CT images intensities represent Hounsﬁeld Units, and these do not take non-integer
values. Following voxel interpolation, interpolated CT intensities are thus rounded to the nearest
Partial volume eﬀects in the ROI mask
If the image on which the ROI mask was deﬁned, is interpolated after the ROI was segmented,
the ROI mask R should likewise be interpolated to the same dimensions. Interpolation of the ROI
mask is best conducted using either the nearest neighbour or trilinear interpolation methods, as
these are guaranteed to produce meaningful masks. Trilinear interpolation of the ROI mask leads
to partial volume eﬀects, with some voxels containing fractions of the original voxels. Since a ROI
CHAPTER 2. IMAGE PROCESSING
mask is a binary mask, such fractions need to be binarised by setting a partial volume threshold δ:
Rinterp,j ≥δ
Rinterp,j < δ
A common choice for the partial volume threshold is δ = 0.5. For nearest neighbour interpolation
the ROI mask does not contain partial volume fractions, and may be used directly.
Interpolation results depend on the ﬂoating point representation used for the image and ROI
masks. Floating point representations should at least be full precision (32-bit) to avoid rounding
Interpolation grid
Interpolated voxel centers lie on the intersections of a regularly spaced grid. Grid intersections
are represented by two coordinate systems. The ﬁrst coordinate system is the grid coordinate
system, with origin at (0.0, 0.0, 0.0) and distance between directly neighbouring voxel centers (spacing) of 1.0. The grid coordinate system is the coordinate system typically used by computers, and
consequentially, by interpolation algorithms. The second coordinate system is the world coordinate
system, which is typically found in (medical) imaging and provides an image scale. As the desired
isotropic spacing is commonly deﬁned in world coordinate dimensions, conversions between world
coordinates and grid coordinates are necessary, and are treated in more detail after assessing grid
alignment methods.
Grid alignment aﬀects feature values and is non-trivial. Three common grid alignments may
be identiﬁed, and are shown in ﬁgure 2.3:
1. Fit to original grid (58MB). In this case the interpolation grid is deformed so that the voxel
centers at the grid intersections overlap with the original grid vertices. For an original 4 × 4
voxel grid with spacing (3.00, 3.00) mm and a desired interpolation spacing of (2.00, 2.00) mm
we ﬁrst calculate the extent of the original voxel grid in world coordinates leading to an
extent of ((4 −1) 3.00, ((4 −1) 3.00) = (9.00, 9.00) mm. In this case the interpolated grid
will not exactly ﬁt the original grid. Therefore we try to ﬁnd the closest ﬁtting grid, which
leads to a 6 × 6 grid by rounding up (9.00/2.00, 9.00/2.00). The resulting grid has a grid
spacing of (1.80, 1.80) mm in world coordinates, which diﬀers from the desired grid spacing
of (2.00, 2.00) mm.
2. Align grid origins (SBKJ). A simple approach which conserves the desired grid spacing is
the alignment of the origins of the interpolation and original grids. Keeping with the same
example, the interpolation grid is (6 × 6). The resulting voxel grid has a grid spacing of
(2.00, 2.00) mm in world coordinates.
By deﬁnition both grids are aligned at the origin,
(0.00, 0.00).
3. Align grid centers (3WE3). The position of the origin may depend on image meta-data
deﬁning image orientation. Not all software implementations may process this meta-data the
same way. An implementation-independent solution is to align both grids on the grid center.
Again, keeping with the same example, the interpolation grid is (6 × 6). Thus, the resulting
voxel grid has a grid spacing of (2.00, 2.00) mm in world coordinates.
Align grid centers is recommended as it is implementation-independent and achieves the desired
voxel spacing. Technical details of implementing align grid centers are described below.
Interpolation grid dimensions
The dimensions of the interpolation grid are determined as follows. Let na be the number of
points along one axis of the original grid and sa,w their spacing in world coordinates. Then, let
CHAPTER 2. IMAGE PROCESSING
sb,w be the desired spacing after interpolation. The axial dimension of the interpolated mesh grid
Rounding towards inﬁnity guarantees that the interpolation grid exists even when the original grid
contains few voxels. However, it also means that the interpolation mesh grid is partially located
outside of the original grid. Extrapolation is thus required. Padding the original grid with the
intensities at the boundary is recommended. Some implementations of interpolation algorithms
may perform this padding internally.
Interpolation grid position
For the align grid centers method, the positions of the interpolation grid points are determined
as follows. As before, let na and nb be the dimensions of one axis in the original and interpolation
grid, respectively. Moreover, let sa,w be the original spacing and sb,w the desired spacing for the
same axis in world coordinates. Then, with xa,w the origin of the original grid in world coordinates,
the origin of the interpolation grid is located at:
xb,w = xa,w + sa(na −1) −sb(nb −1)
In the grid coordinate system, the original grid origin is located at xa,g = 0. The origin of the
interpolation grid is then located at:
na −1 −sb,w
Here the fraction sb,w/sa,w = sb,g is the desired spacing in grid coordinates. Thus, the interpolation
grid points along the considered axis are located at grid coordinates:
xb,g, xb,g + sb,g, xb,g + 2sb,g, . . . , xb,g + (nb −1)sb,g
Naturally, the above description applies to each grid axis.
CHAPTER 2. IMAGE PROCESSING
original grid
ﬁt to original grid
Interpolaon grid
Size: 6x6 points
Desired spacing: (2.00, 2.00)
Realised spacing: (1.80, 1.80)
Original grid
Size: 4x4 points
Spacing: (3.00, 3.00)
align grid origins
Interpolaon grid
Size: 6x6 points
Desired spacing: (2.00, 2.00)
Realised spacing: (2.00, 2.00)
align grid centers
Interpolaon grid
Size: 6x6 points
Desired spacing: (2.00, 2.00)
Realised spacing: (2.00, 2.00)
Figure 2.3 — Diﬀerent interpolation mesh grids based on an original 4 × 4 grid with (3.00, 3.00) mm
spacing. The desired interpolation spacing is (2.00, 2.00) mm. Fit to original grid creates an interpolation mesh grid that overlaps with the corners of the original grid. Align grid origins creates an
interpolation mesh grid that is positioned at the origin of the original grid. Align grid centers creates
an interpolation grid that is centered on the center of original and interpolation grids.
CHAPTER 2. IMAGE PROCESSING
1. original ROI
2. ROI aer re-segmentaon
with range 
outside ROI
inside ROI
re-segmented pixel
3a. re-segmented ROI
(morphological mask)
3b. re-segmented ROI
(intensity mask)
Figure 2.4 — Example showing how intensity and morphological masks may diﬀer due to resegmentation.
(1) The original region of interest (ROI) is shown with pixel intensities.
(2) Subsequently, the ROI is re-segmented to only contain values in the range . Pixels outside this range
are marked for removal from the intensity mask. (3a) Resulting morphological mask, which is identical
to the original ROI. (3b) Re-segmented intensity mask. Note that due to re-segmentation, intensity
and morphological masks are diﬀerent.
Re-segmentation
Re-segmentation entails updating the ROI mask R based on corresponding voxel intensities Xgl.
Re-segmentation may be performed to exclude voxels from a previously segmented ROI, and is
performed after interpolation. An example use would be the exclusion of air or bone voxels from
an ROI deﬁned on CT imaging.
Two common re-segmentation methods are described in this
section. Combining multiple re-segmentation methods is possible. In this case, the intersection of
the intensity ranges deﬁned by the re-segmentation methods is used.
Intensity and morphological masks of an ROI
Conventionally, an ROI consists of a single mask.
However, re-segmentation may lead to
exclusion of internal voxels, or divide the ROI into sub-volumes. To avoid undue complexity by
again updating the re-segmented ROI for a more plausible morphology, we deﬁne two separate
ROI masks.
The morphological mask (G5KJ) is not re-segmented and maintains the original morphology as
deﬁned by an expert and/or (semi-)automatic segmentation algorithms.
CHAPTER 2. IMAGE PROCESSING
range: [-50, 50] HU
outlier: 1.0 σ
range: [-50, 50] HU
outlier: 1.0 σ
CT image slice with mask
Figure 2.5 — Re-segmentation example based on a CT-image.
The masked region (blue) is resegmented to create an intensity mask (orange). Examples using three diﬀerent re-segmentation parameter sets are shown. The bottom right combines the range and outlier re-segmentation, and the
resulting mask is the intersection of the masks in the other two examples. Image data from Valli`eres
et al.16,76,77.
The intensity mask (SEFI) can be re-segmented and will contain only the selected voxels. For
many feature families, only this is important. However, for morphological and grey level distance
zone matrix (GLDZM) feature families, both intensity and morphological masks are used.
two-dimensional schematic example is shown in ﬁgure 2.4, and a real example is shown in ﬁgure
Range re-segmentation
Re-segmentation may be performed to remove voxels from the intensity mask that fall outside
of a speciﬁed range. An example is the exclusion of voxels with Hounsﬁeld Units indicating air
and bone tissue in the tumour ROI within CT images, or low activity areas in PET images. Such
ranges of intensities of included voxels are usually presented as a closed interval [a, b] or half-open
interval [a, ∞), respectively. For arbitrary intensity units (found in e.g. raw MRI data, uncalibrated
microscopy images, and many spatial ﬁlters), no re-segmentation range can be provided.
When a re-segmentation range is deﬁned by the user, it needs to be propagated and used for
the calculation of features that require a speciﬁed intensity range (e.g. intensity-volume histogram
features) and/or that employs ﬁxed bin size discretisation.
Recommendations for the possible
combinations of diﬀerent imaging intensity deﬁnitions, re-segmentation ranges and discretisation
algorithms are provided in Table 2.1.
CHAPTER 2. IMAGE PROCESSING
Intensity outlier ﬁltering
ROI voxels with outlier intensities may be removed from the intensity mask.
One method
for deﬁning outliers was suggested by Valli`eres et al. 76 after Collewet et al. 19. The mean µ and
standard deviation σ of grey levels of voxels assigned to the ROI are calculated. Voxels outside
the range [µ −3σ, µ + 3σ] are subsequently excluded from the intensity mask.
ROI extraction
Many feature families require that the ROI is isolated from the surrounding voxels. The ROI
intensity mask is used to extract the image volume to be studied. Excluded voxels are commonly
replaced by a placeholder value, often NaN. This placeholder value may then used to exclude these
voxels from calculations.
Voxels included in the ROI mask retain their original intensity.
example is shown in ﬁgure 2.6.
Intensity discretisation
Discretisation or quantisation of image intensities inside the ROI is often required to make calculation of texture features tractable88, and possesses noise-suppressing properties as well. An
example of discretisation is shown in ﬁgure 2.7.
Two approaches to discretisation are commonly used. One involves the discretisation to a ﬁxed
number of bins, and the other discretisation with a ﬁxed bin width. As we will observe, there
is no inherent preference for one or the other method. However, both methods have particular
characteristics (as described below) that may make them better suited for speciﬁc purposes. Note
that the lowest bin always has value 1, and not 0. This ensures consistency for calculations of
texture features, where for some features grey level 0 is not allowed .
Fixed bin number
In the ﬁxed bin number method, intensities Xgl are discretised to a ﬁxed number of Ng bins.
It is deﬁned as follows:
Xgl,k−Xgl,min
Xgl,max−Xgl,min
Xgl,k < Xgl,max
Xgl,k = Xgl,max
In short, the intensity Xgl,k of voxel k is corrected by the lowest occurring intensity Xgl,min in the
ROI, divided by the bin width (Xgl,max −Xgl,min) /Ng, and subsequently rounded down to the
apply mask
Figure 2.6 — Masking of an image by the ROI mask during ROI extraction. Intensities outside the
ROI are excluded. Image data from Valli`eres et al.16,76,77.
CHAPTER 2. IMAGE PROCESSING
ﬁxed bin number discretisation
ﬁxed bin size discretisation
Figure 2.7 — Discretisation of two diﬀerent 18F-FDG-PET images with SUVmax of 27.8 (A) and 6.6
(B). Fixed bin number discretisation adjust the contrast between the two images, with the number
of bins determining the coarseness of the discretised image. Fixed bin size discretisation leaves the
contrast diﬀerences between image A and B intact. Increasing the bin size increases the coarseness of
the discretised image. Image data from Valli`eres et al.16,76,77.
nearest integer (ﬂoor function).
The ﬁxed bin number method breaks the relationship between image intensity and physiological meaning (if any). However, it introduces a normalising eﬀect which may be beneﬁcial when
intensity units are arbitrary (e.g. raw MRI data and many spatial ﬁlters), and where contrast
is considered important. Furthermore, as values of many features depend on the number of grey
levels found within a given ROI, the use of a ﬁxed bin number discretisation algorithm allows for a
direct comparison of feature values across multiple analysed ROIs (e.g. across diﬀerent samples).
Fixed bin size
Fixed bin size discretisation is conceptually simple. A new bin is assigned for every intensity
interval with width wb; i.e. wb is the bin width, starting at a minimum Xgl,min. The minimum
intensity may be a user-set value as deﬁned by the lower bound of the re-segmentation range, or
data-driven as deﬁned by the minimum intensity in the ROI Xgl,min = min (Xgl). In all cases, the
method used and/or set minimum value must be clearly reported. However, to maintain consistency
between samples, we strongly recommend to always set the same minimum value for all samples
as deﬁned by the lower bound of the re-segmentation range (e.g. HU of -500 for CT, SUV of 0 for
PET, etc.). In the case that no re-segmentation range may be deﬁned due to arbitrary intensity
units (e.g. raw MRI data and many spatial ﬁlters), the use of the ﬁxed bin size discretisation
algorithm is not recommended.
The ﬁxed bin size method has the advantage of maintaining a direct relationship with the
original intensity scale, which could be useful for functional imaging modalities such as PET.
CHAPTER 2. IMAGE PROCESSING
Imaging intensity
Re-segmentation
calibrated
Table 2.1 — Recommendations for the possible combinations of diﬀerent imaging intensity deﬁnitions, re-segmentation ranges and discretisation algorithms. Checkmarks () represent recommended
combinations of re-segmentation range and discretisation algorithm, whereas crossmarks () represent
non-recommended combinations.
(1) PET and CT are examples of imaging modalities with calibrated intensity units (e.g. SUV and HU,
respectively), and raw MRI data of arbitrary intensity units.
(2) Fixed bin number (FBN) discretisation uses the actual range of intensities in the analysed ROI
(re-segmented or not), and not the re-segmentation range itself (when deﬁned).
(3) Fixed bin size (FBS) discretisation uses the lower bound of the re-segmentation range as the minimum set value. When the re-segmentation range is not or cannot be deﬁned (e.g. arbitrary intensity
units), the use of the FBS algorithm is not recommended.
Discretised intensities are computed as follows:
Xgl,k −Xgl,min
In short, the minimum intensity Xgl,min is subtracted from intensity Xgl,k in voxel k, and then
divided by the bin width wb. The resulting value is subsequently rounded down to the nearest
integer (ﬂoor function), and 1 is added to arrive at the discretised intensity.
Other methods
Many other methods and variations for discretisation exist, but are not described in detail here.
Valli`eres et al. 76 described the use of intensity histogram equalisation and Lloyd-Max algorithms
for discretisation. Intensity histogram equalisation involves redistributing intensities so that the
resulting bins contain a similar number of voxels, i.e.
contrast is increased by ﬂattening the
histogram as much as possible34. Histogram equalisation of the ROI imaging intensities can be
performed before any other discretisation algorithm (e.g. FBN, FSB, etc.), and it also requires the
deﬁnition of a given number of bins in the histogram to be equalised. The Lloyd-Max algorithm
is an iterative clustering method that seeks to minimise mean squared discretisation errors47,50.
Recommendations
The discretisation method that leads to optimal feature inter- and intra-sample reproducibility
is modality-dependent. Usage recommendations for the possible combinations of diﬀerent imaging
intensity deﬁnitions, re-segmentation ranges and discretisation algorithms are provided in Table
2.1. Overall, the discretisation choice has a substantial impact on intensity distributions, feature
values and reproducibility4,25,37,38,44,59,83.
Feature calculation
Feature calculation is the ﬁnal processing step where feature descriptors are used to quantify
characteristics of the ROI. After calculation such features may be used as image biomarkers by
CHAPTER 2. IMAGE PROCESSING
relating them to physiological and medical outcomes of interest. Feature calculation is handled in
full details in the next chapter.
Let us recall that the image processing steps leading to image biomarker calculations can be
performed in many diﬀerent ways, notably in terms of spatial ﬁltering, segmentation, interpolation
and discretisation parameters.
Furthermore, it is plausible that diﬀerent texture features will
better quantify the characteristics of the ROI when computed using diﬀerent image processing
parameters. For example, a lower number of grey levels in the discretisation process (e.g. 8 or
16) may allow to better characterize the sub-regions of the ROI using grey level size zone matrix
(GLSZM) features, whereas grey level co-occurence matrix (GLCM) features may be better modeled
with a higher number of grey levels (e.g. 32 or 64). Overall, these possible diﬀerences opens the
door to the optimization of image processing parameters for each diﬀerent feature in terms of a
speciﬁc objective. For the speciﬁc case of the optimization of image interpolation and discretisation
prior to texture analysis, Valli`eres et al.76 have named this process texture optimization.
authors notably suggested that the texture optimization process could have signiﬁcant inﬂuence
of the prognostic capability of subsequent features. In another study78, the authors constructed
predictive models using textures calculated from all possible combinations of PET and CT images
interpolated at four isotropic resolutions and discretised with two diﬀerent algorithms and four
numbers of grey levels.
Image features
In this chapter we will describe a set of quantitative image features together with the reference
values established by the IBSI. This feature set builds upon the feature sets proposed by Aerts
et al. 1 and Hatt et al. 38, which are themselves largely derived from earlier works. References to
earlier work are provided whenever they could be identiﬁed.
Reference values were derived for each feature. A table of reference values contains the values
that could be reliably reproduced, within a tolerance margin, for the reference data sets (see
Chapter 5). Consensus on the validity of each reference value is also noted. Consensus can have
four levels, depending on the number of teams that were able to produce the same value during the
standardization process: weak (< 3 matches), moderate (3 to 5 matches), strong (6 to 9 matches),
and very strong (≥10 matches). If consensus on a reference value was weak or if it could not be
reproduced by an absolute majority of teams, it was not considered standardized. Such features
do currently not have reference values, and should not be used.
The set of features can be divided into a number of families, of which intensity-based statistical, intensity histogram-based, intensity-volume histogram-based, morphological features, local
intensity, and texture matrix-based features are treated here. All texture matrices are rotationally and translationally invariant. Illumination invariance of texture matrices may be achieved by
particular image post-acquisition schemes, e.g. histogram matching. None of the texture matrices
are scale invariant, a property which can be useful in many (biomedical) applications. What the
presented texture matrices lack, however, is directionality in combination with rotation invariance.
These may be achieved by local binary patterns and steerable ﬁlters, which however fall beyond
the scope of the current work. For these and other texture features, see Depeursinge et al. 24.
Features are calculated on the base image, as well as images transformed using wavelet or
Gabor ﬁlters). To calculate features, it is assumed that an image segmentation mask exists, which
identiﬁes the voxels located within a region of interest (ROI). The ROI itself consists of two masks,
an intensity mask and a morphological mask. These masks may be identical, but not necessarily
so, as described in Section 2.5.
Several feature families require additional image processing steps before feature calculation.
Notably intensity histogram and texture feature families require prior discretisation of intensities
into grey level bins. Other feature families do not require discretisation before calculations. For
more details on image processing, see ﬁgure 2.1 in the previous chapter.
Below is an overview table that summarises image processing requirements for the diﬀerent
feature families.
CHAPTER 3. IMAGE FEATURES
Feature family
morphology
local intensity
intensity-based statistics
intensity histogram
intensity-volume histogram
grey level co-occurrence matrix
grey level run length matrix
grey level size zone matrix
grey level distance zone matrix
neighbourhood grey tone diﬀerence matrix
neighbouring grey level dependence matrix
Table 3.1 — Feature families and required image processing. For each feature family, the number of
features in the document, the required input of a morphological (morph.) and/or intensity (int.) ROI
mask, as well as the requirement of image discretisation (discr.) is provided.
a The entire image volume should be available when computing local intensity features.
b Image discretisation for the intensity-volume histogram is performed with ﬁner discretisation than
required for e.g. textural features.
Though image processing parameters aﬀect feature values, three other concepts inﬂuence feature
values for many features: distance, feature aggregation and distance weighting. These are described
Grid distances
Grid distance is an important concept that is used by several feature families, particularly
texture features. Grid distances can be measured in several ways. Let m = (mx, my, mz) be the
vector from a center voxel at k = (kx, ky, kz) to a neighbour voxel at k + m. The following norms
(distances) are used:
• ℓ1 norm or Manhattan norm (LIFZ):
∥m∥1 = |mx| + |my| + |mz|
• ℓ2 norm or Euclidean norm (G9EV):
m2x + m2y + m2z
• ℓ∞norm or Chebyshev norm (PVMT):
∥m∥∞= max(|mx|, |my|, |mz|)
An example of how the above norms diﬀer in practice is shown in ﬁgure 3.1.
CHAPTER 3. IMAGE FEATURES
(a) Manhattan norm
(b) Euclidean norm
(c) Chebyshev norm
Figure 3.1 — Grid neighbourhoods for distances up to 3 according to Manhattan, Euclidean and
Chebyshev norms. The orange pixel is considered the center pixel. Dark blue pixels have distance
δ = 1, blue pixels δ ≤2 and light blue pixels δ ≤3 for the corresponding norm.
Feature aggregation
Features from some families may be calculated from, e.g. slices. As a consequence, multip le
values for the same feature may be computed. These diﬀerent values should be combined into a
single value for many common purposes. This process is referred to as feature aggregation. Feature
aggregation methods depend on the family, and are detailed in the family description.
Distance weighting
Distance weighting is not a default operation for any of the texture families, but is implemented
in software such as PyRadiomics81. It may for example be used to put more emphasis on local
intensities.
CHAPTER 3. IMAGE FEATURES
Morphological features
Morphological features describe geometric aspects of a region of interest (ROI), such as area and
volume. Morphological features are based on ROI voxel representations of the volume. Three voxel
representations of the volume are conceivable:
1. The volume is represented by a collection of voxels with each voxel taking up a certain volume
2. The volume is represented by a voxel point set Xc that consists of coordinates of the voxel
centers (4KW8).
3. The volume is represented by a surface mesh (WRJH).
We use the second representation when the inner structure of the volume is important, and the third
representation when only the outer surface structure is important. The ﬁrst representation is not
used outside volume approximations because it does not handle partial volume eﬀects at the ROI
edge well, and also to avoid inconsistencies in feature values introduced by mixing representations
in small voxel volumes.
Mesh-based representation
A mesh-based representation of the outer surface allows consistent evaluation of the surface
volume and area independent of size. Voxel-based representations lead to partial volume eﬀects
and over-estimation of the surface area. The surface of the ROI volume is translated into a triangle
mesh using a meshing algorithm. While multiple meshing algorithms exist, we suggest the use of
the Marching Cubes algorithm45,48 because of its widespread availability in diﬀerent programming
languages and reasonable approximation of the surface area and volume67. In practice, meshbased feature values depend upon the meshing algorithm and small diﬀerences may occur between
implementations46.
Figure 3.2 — Meshing algorithms draw faces and vertices to cover the ROI. One face, spanned
by vertices a, b and c, is highlighted. Moreover, the vertices deﬁne the three edges ab = b −a,
bc = c−b and ca = a−c. The face normal n is determined using the right-hand rule, and calculated
as n = (ab × bc) /∥ab × bc∥, i.e. the outer product of edge ab with edge bc, normalised by its
Meshing algorithms use the ROI voxel point set Xc to create a closed mesh. Dependent on the
algorithm, a parameter is required to specify where the mesh should be drawn. A default level
of 0.5 times the voxel spacing is used for marching cube algorithms. Other algorithms require a
so-called isovalue, for which a value of 0.5 can be used since the ROI mask consists of 0 and 1
values, and we want to roughly draw the mesh half-way between voxel centers. Depending on
implementation, algorithms may also require padding of the ROI mask with non-ROI (0) voxels
CHAPTER 3. IMAGE FEATURES
to correctly estimate the mesh in places where ROI voxels would otherwise be located at the edge
of the mask.
The closed mesh drawn by the meshing algorithm consists of Nfc triangle faces spanned by
Nvx vertex points. An example triangle face is drawn in Figure 3.2. The set of vertex points is
The calculation of the mesh volume requires that all faces have the same orientation of the
face normal. Consistent orientation can be checked by the fact that in a regular, closed mesh, all
edges are shared between exactly two faces. Given the edge spanned by vertices a and b, the edge
must be ab = b −a for one face and ba = a −b for the adjacent face. This ensures consistent
application of the right-hand rule, and thus consistent orientation of the face normals. Algorithm
implementations may return consistently orientated faces by default.
ROI morphological and intensity masks
The ROI consists of a morphological and an intensity mask. The morphological mask is used
to calculate many of the morphological features and to generate the voxel point set Xc. Any holes
within the morphological mask are understood to be the result of segmentation decisions, and
thus to be intentional. The intensity mask is used to generate the voxel intensity set Xgl with
corresponding point set Xc,gl.
Aggregating features
By deﬁnition, morphological features are calculated in 3D (DHQ4), and not per slice.
Units of measurement
By deﬁnition, morphological features are computed using the unit of length as deﬁned in the
DICOM standard, i.e. millimeter for most medical imaging modalities1.
If the unit of length is not deﬁned by a standard, but is explicitly deﬁned as meta data, this
deﬁnition should be used. In this case, care should be taken that this deﬁnition is consistent across
all data in the cohort.
If a feature value should be expressed as a diﬀerent unit of length, e.g. cm instead of mm, such
conversions should take place after computing the value using the standard units.
Volume (mesh)
The mesh-based volume V is calculated from the ROI mesh as follows89. A tetrahedron is formed
by each face k and the origin. By placing the origin vertex of each tetrahedron at (0, 0, 0), the
signed volume of the tetrahedron is:
Vk = a · (b × c)
Here a, b and c are the vertex points of face k. Depending on the orientation of the normal,
the signed volume may be positive or negative. Hence, the orientation of face normals should be
consistent, e.g. all normals must be either pointing outward or inward. The volume V is then
calculated by summing over the face volumes, and taking the absolute value:
Fmorph.vol = V =
1DICOM PS3.3 2019a - Information Object Deﬁnitions, Section 10.7.1.3
CHAPTER 3. IMAGE FEATURES
In positron emission tomography, the volume of the ROI commonly receives a name related to
the radioactive tracer, e.g. metabolically active tumour volume (MATV) for 18F-FDG.
dig. phantom
very strong
3.58 × 105
very strong
3.58 × 105
3.67 × 105
3.67 × 105
3.67 × 105
Table 3.2 — Reference values for the volume (mesh) feature.
Volume (voxel counting)
In clinical practice, volumes are commonly determined by counting voxels. For volumes consisting
of a large number of voxels (1000s), the diﬀerences between voxel counting and mesh-based approaches are usually negligible. However for volumes with a low number of voxels (10s to 100s),
voxel counting will overestimate volume compared to the mesh-based approach. It is therefore only
used as a reference feature, and not in the calculation of other morphological features.
Voxel counting volume is deﬁned as:
Fmorph.approx.vol =
Here Nv is the number of voxels in the morphological mask of the ROI, and Vk the volume of voxel
dig. phantom
very strong
3.59 × 105
3.58 × 105
3.68 × 105
3.68 × 105
3.68 × 105
Table 3.3 — Reference values for the volume (voxel counting) feature.
Surface area (mesh)
The surface area A is also calculated from the ROI mesh by summing over the triangular face
surface areas1. By deﬁnition, the area of face k is:
Ak = |ab × ac|
CHAPTER 3. IMAGE FEATURES
As in Figure 3.2, edge ab = b −a is the vector from vertex a to vertex b, and edge ac = c −a
the vector from vertex a to vertex c. The total surface area A is then:
Fmorph.area = A =
dig. phantom
very strong
3.57 × 104
3.37 × 104
3.43 × 104
3.43 × 104
3.43 × 104
Table 3.4 — Reference values for the surface area (mesh) feature.
Surface to volume ratio
The surface to volume ratio is given as1:
Fmorph.av = A
Note that this feature is not dimensionless.
dig. phantom
very strong
Table 3.5 — Reference values for the surface to volume ratio feature.
Compactness 1
Several features (compactness 1 and 2, spherical disproportion, sphericity and asphericity) quantify
the deviation of the ROI volume from a representative spheroid. All these deﬁnitions can be derived
from one another. As a results these features are are highly correlated and may thus be redundant.
Compactness 1 1 is a measure for how compact, or sphere-like the volume is. It is deﬁned as:
Fmorph.comp.1 =
Compactness 1 is sometimes1 deﬁned using A2/3 instead of A3/2, but this does not lead to a
dimensionless quantity.
CHAPTER 3. IMAGE FEATURES
dig. phantom
Table 3.6 — Reference values for the compactness 1 feature. An unset value (—) indicates the lack
of a reference value.
Compactness 2
Like Compactness 1, Compactness 2 1 quantiﬁes how sphere-like the volume is:
Fmorph.comp.2 = 36π V 2
By deﬁnition Fmorph.comp.1 = 1/6π (Fmorph.comp.2)1/2.
dig. phantom
Table 3.7 — Reference values for the compactness 2 feature.
Spherical disproportion
Spherical disproportion 1 likewise describes how sphere-like the volume is:
Fmorph.sph.dispr =
(36πV 2)1/3
By deﬁnition Fmorph.sph.dispr = (Fmorph.comp.2)−1/3.
dig. phantom
Table 3.8 — Reference values for the spherical disproportion feature.
CHAPTER 3. IMAGE FEATURES
Sphericity
Sphericity 1 is a further measure to describe how sphere-like the volume is:
Fmorph.sphericity =
 36πV 21/3
By deﬁnition Fmorph.sphericity = (Fmorph.comp.2)1/3.
dig. phantom
very strong
Table 3.9 — Reference values for the sphericity feature.
Asphericity
Asphericity 6 also describes how much the ROI deviates from a perfect sphere, with perfectly
spherical volumes having an asphericity of 0. Asphericity is deﬁned as:
Fmorph.asphericity =
By deﬁnition Fmorph.asphericity = (Fmorph.comp.2)−1/3 −1
dig. phantom
Table 3.10 — Reference values for the asphericity feature.
Centre of mass shift
The distance between the ROI volume centroid and the intensity-weighted ROI volume is an
abstraction of the spatial distribution of low/high intensity regions within the ROI. Let Nv,m be
the number of voxels in the morphological mask. The ROI volume centre of mass is calculated
from the morphological voxel point set Xc as follows:
CoM geom =
CHAPTER 3. IMAGE FEATURES
The intensity-weighted ROI volume is based on the intensity mask. The position of each voxel
centre in the intensity mask voxel set Xc,gl is weighted by its corresponding intensity Xgl. Therefore, with Nv,gl the number of voxels in the intensity mask:
k=1 Xgl,k ⃗Xc,gl,k
The distance between the two centres of mass is then:
Fmorph.com = ||−−−→
CoM geom −−−−→
dig. phantom
very strong
Table 3.11 — Reference values for the centre of mass shift feature.
Maximum 3D diameter
The maximum 3D diameter 1 is the distance between the two most distant vertices in the ROI
mesh vertex set Xvx:
Fmorph.diam = max
|| ⃗Xvx,k1 −⃗Xvx,k2||2
k1 = 1, . . . , N
k2 = 1, . . . , N
A practical way of determining the maximum 3D diameter is to ﬁrst construct the convex hull
of the ROI mesh. The convex hull vertex set Xvx,convex is guaranteed to contain the two most
distant vertices of Xvx. This signiﬁcantly reduces the computational cost of calculating distances
between all vertices. Despite the remaining O(n2) cost of calculating distances between diﬀerent
vertices, Xvx,convex is usually considerably smaller in size than Xvx. Moreover, the convex hull is
later used for the calculation of other morphological features (3.1.25-3.1.26).
dig. phantom
Table 3.12 — Reference values for the maximum 3D diameter feature.
Major axis length
Principal component analysis (PCA) can be used to determine the main orientation of the ROI65.
On a three dimensional object, PCA yields three orthogonal eigenvectors {e1, e2, e3} and three
CHAPTER 3. IMAGE FEATURES
eigenvalues (λ1, λ2, λ3). These eigenvalues and eigenvectors geometrically describe a triaxial ellipsoid. The three eigenvectors determine the orientation of the ellipsoid, whereas the eigenvalues
provide a measure of how far the ellipsoid extends along each eigenvector. Several features make use
of principal component analysis, namely major, minor and least axis length, elongation, ﬂatness,
and approximate enclosing ellipsoid volume and area density.
The eigenvalues can be ordered so that λmajor ≥λminor ≥λleast correspond to the major,
minor and least axes of the ellipsoid respectively. The semi-axes lengths a, b and c for the major,
minor and least axes are then 2
λmajor, 2√λminor and 2√λleast respectively. The major axis
length is twice the semi-axis length a, determined using the largest eigenvalue obtained by PCA
on the point set of voxel centers Xc 39:
Fmorph.pca.major = 2a = 4
dig. phantom
very strong
very strong
Table 3.13 — Reference values for the major axis length feature.
Minor axis length
The minor axis length of the ROI provides a measure of how far the volume extends along the
second largest axis. The minor axis length is twice the semi-axis length b, determined using the
second largest eigenvalue obtained by PCA, as described in Section 3.1.12:
Fmorph.pca.minor = 2b = 4
dig. phantom
very strong
very strong
Table 3.14 — Reference values for the minor axis length feature.
Least axis length
The least axis is the axis along which the object is least extended. The least axis length is twice
the semi-axis length c, determined using the smallest eigenvalue obtained by PCA, as described in
CHAPTER 3. IMAGE FEATURES
Section 3.1.12:
Fmorph.pca.least = 2c = 4
dig. phantom
very strong
Table 3.15 — Reference values for the least axis length feature.
Elongation
The ratio of the major and minor principal axis lengths could be viewed as the extent to which a
volume is longer than it is wide, i.e. is eccentric. For computational reasons, we express elongation
as an inverse ratio. 1 is thus completely non-elongated, e.g. a sphere, and smaller values express
greater elongation of the ROI volume.
Fmorph.pca.elongation =
dig. phantom
very strong
Table 3.16 — Reference values for the elongation feature.
The ratio of the major and least axis lengths could be viewed as the extent to which a volume is
ﬂat relative to its length. For computational reasons, we express ﬂatness as an inverse ratio. 1 is
thus completely non-ﬂat, e.g. a sphere, and smaller values express objects which are increasingly
Fmorph.pca.ﬂatness =
CHAPTER 3. IMAGE FEATURES
dig. phantom
very strong
Table 3.17 — Reference values for the ﬂatness feature.
Volume density (axis-aligned bounding box)
Volume density is the fraction of the ROI volume and a comparison volume. Here the comparison
volume is that of the axis-aligned bounding box (AABB) of the ROI mesh vertex set Xvx or the
ROI mesh convex hull vertex set Xvx,convex. Both vertex sets generate an identical bounding box,
which is the smallest box enclosing the vertex set, and aligned with the axes of the reference frame.
Fmorph.v.dens.aabb =
This feature is also called extent 27,65.
dig. phantom
Table 3.18 — Reference values for the volume density (AABB) feature.
Area density (axis-aligned bounding box)
Conceptually similar to the volume density (AABB) feature, area density considers the ratio of the
ROI surface area and the surface area Aaabb of the axis-aligned bounding box enclosing the ROI
mesh vertex set Xvx 80. The bounding box is identical to the one used for computing the volume
density (AABB) feature. Thus:
Fmorph.a.dens.aabb =
CHAPTER 3. IMAGE FEATURES
dig. phantom
Table 3.19 — Reference values for the area density (AABB) feature.
Volume density (oriented minimum bounding box)
Note: This feature currently has no reference values and should not be used.
The volume of an axis-aligned bounding box is generally not the smallest obtainable volume
enclosing the ROI. By orienting the box along a diﬀerent set of axes, a smaller enclosing volume
may be attainable. The oriented minimum bounding box (OMBB) of the ROI mesh vertex set
Xvx or Xvx,convex encloses the vertex set and has the smallest possible volume. A 3D rotating
callipers technique was devised by O’Rourke 56 to derive the oriented minimum bounding box. Due
to computational complexity of this technique, the oriented minimum bounding box is commonly
approximated at lower complexity, see e.g. Barequet and Har-Peled 10 and Chan and Tan 14. Thus:
Fmorph.v.dens.ombb =
Here Vombb is the volume of the oriented minimum bounding box.
Area density (oriented minimum bounding box)
Note: This feature currently has no reference values and should not be used.
The area density (OMBB) is estimated as:
Fmorph.a.dens.ombb =
Here Aombb is the surface area of the same bounding box as calculated for the volume density
(OMBB) feature.
Volume density (approximate enclosing ellipsoid)
The eigenvectors and eigenvalues from PCA of the ROI voxel center point set Xc can be used
to describe an ellipsoid approximating the point cloud51, i.e. the approximate enclosing ellipsoid
(AEE). The volume of this ellipsoid is Vaee = 4π a b c/3, with a, b, and c being the lengths of the
ellipsoid’s semi-principal axes, see Section 3.1.12. The volume density (AEE) is then:
Fmorph.v.dens.aee =
CHAPTER 3. IMAGE FEATURES
dig. phantom
Table 3.20 — Reference values for the volume density (AEE) feature.
Area density (approximate enclosing ellipsoid)
The surface area of an ellipsoid can generally not be evaluated in an elementary form. However,
it is possible to approximate the surface using an inﬁnite series. We use the same semi-principal
axes as for the volume density (AEE) feature and deﬁne:
Aaee (a, b, c) = 4π a b
1 −b2/a2 and β =
1 −c2/a2 are eccentricities of the ellipsoid and Pν is the Legendre
polynomial function for degree ν. The Legendre polynomial series, though inﬁnite, converges, and
approximation may be stopped early when the incremental gains in precision become limited. By
default, we stop the series after ν = 20.
The area density (AEE) is then approximated as:
Fmorph.a.dens.aee =
dig. phantom
Table 3.21 — Reference values for the area density (AEE) feature.
Volume density (minimum volume enclosing ellipsoid)
Note: This feature currently has no reference values and should not be used.
The minimum volume enclosing ellipsoid (MVEE), unlike the approximate enclosing ellipsoid, is
the smallest ellipsoid that encloses the ROI. Direct computation of the MVEE is usually unfeasible,
and is therefore approximated. Various approximation algorithms have been described, e.g.2,72,
which are usually elaborations on Khachiyan’s barycentric coordinate descent method40.
The MVEE encloses the ROI mesh vertex set Xvx, and by deﬁnition Xvx,convex as well. Use
of the convex mesh set Xvx,convex is recommended due to its sparsity compared to the full vertex
CHAPTER 3. IMAGE FEATURES
set. The volume of the MVEE is deﬁned by its semi-axes lengths Vmvee = 4π a b c/3. Then:
Fmorph.v.dens.mvee =
For Khachiyan’s barycentric coordinate descent-based methods we use a default tolerance τ =
0.001 as stopping criterion.
Area density (minimum volume enclosing ellipsoid)
Note: This feature currently has no reference values and should not be used.
The surface area of an ellipsoid does not have a general elementary form, but should be approximated as noted in Section 3.1.22. Let the approximated surface area of the MVEE be Amvee.
Fmorph.a.dens.mvee =
Volume density (convex hull)
The convex hull encloses ROI mesh vertex set Xvx and consists of the vertex set Xvx,convex and
corresponding faces, see section 3.1.11. The volume of the ROI mesh convex hull set Vconvex is
computed in the same way as that of the volume (mesh) feature (3.1.1). The volume density can
then be calculated as follows:
Fmorph.v.dens.conv.hull =
This feature is also called solidity 27,65.
dig. phantom
Table 3.22 — Reference values for the volume density (convex hull) feature.
Area density (convex hull)
The area of the convex hull Aconvex is the sum of the areas of the faces of the convex hull, and is
computed in the same way as the surface area (mesh) feature (section 3.1.3). The convex hull is
identical to the one used in the volume density (convex hull) feature. Then:
Fmorph.a.dens.conv.hull =
CHAPTER 3. IMAGE FEATURES
dig. phantom
Table 3.23 — Reference values for the area density (convex hull) feature.
Integrated intensity
Integrated intensity is the average intensity in the ROI, multiplied by the volume. In the context
of 18F-FDG-PET, this feature is often called total lesion glycolysis 75. Thus:
Fmorph.integ.int = V
Nv,gl is the number of voxels in the ROI intensity mask.
dig. phantom
4.81 × 106
4.12 × 106
−1.8 × 107
−8.64 × 106
1.56 × 106
−8.31 × 106
Table 3.24 — Reference values for the integrated intensity feature.
Moran’s I index
Moran’s I index is an indicator of spatial autocorrelation21,52. It is deﬁned as:
Fmorph.moran.i =
k2=1 wk1k2
k2=1 wk1k2 (Xgl,k1 −µ) (Xgl,k2 −µ)
k=1 (Xgl,k −µ)2
As before Nv,gl is the number of voxels in the ROI intensity mask, µ is the mean of Xgl and wk1k2
is a weight factor, equal to the inverse Euclidean distance between voxels k1 and k2 of the point set
Xc,gl of the ROI intensity mask20. Values of Moran’s I close to 1.0, 0.0 and -1.0 indicate high spatial
autocorrelation, no spatial autocorrelation and high spatial anti-autocorrelation, respectively.
Note that for an ROI containing many voxels, calculating Moran’s I index may be computationally expensive due to O(n2) behaviour. Approximation by repeated subsampling of the ROI
may be required to make the calculation tractable, at the cost of accuracy.
CHAPTER 3. IMAGE FEATURES
dig. phantom
Table 3.25 — Reference values for the Moran’s I index feature.
Geary’s C measure
Geary’s C measure assesses spatial autocorrelation, similar to Moran’s I index21,31. In contrast
to Moran’s I index, Geary’s C measure directly assesses intensity diﬀerences between voxels and
is more sensitive to local spatial autocorrelation. This measure is deﬁned as:
Fmorph.geary.c =
k2=1 wk1k2
k2=1 wk1k2 (Xgl,k1 −Xgl,k2)2
k=1 (Xgl,k −µ)2
As with Moran’s I, Nv,gl is the number of voxels in the ROI intensity mask, µ is the mean of Xgl
and wk1k2 is a weight factor, equal to the inverse Euclidean distance between voxels k1 and k2 of
the ROI voxel point set Xc,gl 20.
Just as Moran’s I, Geary’s C measure exhibits O(n2) behaviour and an approximation scheme
may be required to make calculation feasible for large ROIs.
dig. phantom
Table 3.26 — Reference values for the Geary’s C measure feature.
CHAPTER 3. IMAGE FEATURES
Local intensity features
Voxel intensities within a deﬁned neighbourhood around a center voxel are used to compute local
intensity features. Unlike many other feature sets, local features do not draw solely on intensities
within the ROI. While only voxels within the ROI intensity map can be used as a center voxel,
the local neighbourhood draws upon all voxels regardless of being in an ROI.
Aggregating features
By deﬁnition, local intensity features are calculated in 3D (DHQ4), and not per slice.
Local intensity peak
The local intensity peak was originally devised for reducing variance in determining standardised
uptake values85. It is deﬁned as the mean intensity in a 1 cm3 spherical volume (in world coordinates), which is centered on the voxel with the maximum intensity level in the ROI intensity
To calculate Floc.peak.local, we ﬁrst select all the voxels with centers within a radius r =
1/3 ≈0.62 cm of the center of the maximum intensity voxel. Subsequently, the mean intensity of the selected voxels, including the center voxel, are calculated.
In case the maximum intensity is found in multiple voxels within the ROI, local intensity peak
is calculated for each of these voxels, and the highest local intensity peak is chosen.
dig. phantom
Table 3.27 — Reference values for the local intensity peak feature.
Global intensity peak
The global intensity peak feature Floc.peak.global is similar to the local intensity peak 29. However,
instead of calculating the mean intensity for the voxel(s) with the maximum intensity, the mean
intensity is calculated within a 1 cm3 neighbourhood for every voxel in the ROI intensity mask.
The highest intensity peak value is then selected.
Calculation of the global intensity peak feature may be accelerated by construction and application of an appropriate spatial spherical mean convolution ﬁlter, due to the convolution theorem.
In this case one would ﬁrst construct an empty 3D ﬁlter that will ﬁt a 1 cm3 sphere. Within this
context, the ﬁlter voxels may be represented by a point set, akin to Xc in section 3.1. Euclidean
distances in world spacing between the central voxel of the ﬁlter and every remaining voxel are
computed. If this distance lies within radius r =
1/3 ≈0.62 the corresponding voxel receives
a label 1, and 0 otherwise. Subsequent summation of the voxel labels yields Ns, the number of
voxels within the 1 cm3 sphere. The ﬁlter then becomes a spherical mean ﬁlter by dividing the
labels by Ns.
CHAPTER 3. IMAGE FEATURES
dig. phantom
Table 3.28 — Reference values for the global intensity peak feature.
CHAPTER 3. IMAGE FEATURES
Intensity-based statistical features
The intensity-based statistical features describe how intensities within the region of interest (ROI)
are distributed. The features in this set do not require discretisation, and may be used to describe
a continuous intensity distribution. Intensity-based statistical features are not meaningful if the
intensity scale is arbitrary.
The set of intensities of the Nv voxels included in the ROI intensity mask is denoted as Xgl =
{Xgl,1, Xgl,2, . . . , Xgl,Nv}.
Aggregating features
We recommend calculating intensity-based statistical features using the 3D volume (DHQ4). An
approach that computes intensity-based statistical features per slice and subsequently averages
them (3IDG) is not recommended.
Mean intensity
The mean intensity of Xgl is calculated as:
Fstat.mean = 1
dig. phantom
very strong
very strong
very strong
Table 3.29 — Reference values for the mean feature.
Intensity variance
The intensity variance of Xgl is deﬁned as:
Fstat.var = 1
(Xgl,k −µ)2
Note that we do not apply a bias correction when computing the variance.
CHAPTER 3. IMAGE FEATURES
dig. phantom
very strong
1.42 × 104
very strong
1.44 × 104
5.06 × 104
very strong
3.28 × 104
3.51 × 104
Table 3.30 — Reference values for the variance feature.
Intensity skewness
The skewness of the intensity distribution of Xgl is deﬁned as:
Fstat.skew =
k=1 (Xgl,k −µ)3
k=1 (Xgl,k −µ)23/2
Here µ = Fstat.mean. If the intensity variance Fstat.var = 0, Fstat.skew = 0.
dig. phantom
very strong
very strong
very strong
Table 3.31 — Reference values for the skewness feature.
(Excess) intensity kurtosis
Kurtosis, or technically excess kurtosis, is a measure of peakedness in the intensity distribution
Fstat.kurt =
k=1 (Xgl,k −µ)4
k=1 (Xgl,k −µ)22 −3
Here µ = Fstat.mean. Note that kurtosis is corrected by a Fisher correction of -3 to center it on 0
for normal distributions. If the intensity variance Fstat.var = 0, Fstat.kurt = 0.
CHAPTER 3. IMAGE FEATURES
dig. phantom
very strong
very strong
very strong
Table 3.32 — Reference values for the (excess) kurtosis feature.
Median intensity
The median intensity Fstat.median is the sample median of Xgl.
dig. phantom
very strong
very strong
very strong
Table 3.33 — Reference values for the median feature.
Minimum intensity
The minimum intensity is equal to the lowest intensity present in Xgl, i.e:
Fstat.min = min(Xgl)
dig. phantom
very strong
very strong
very strong
Table 3.34 — Reference values for the minimum feature.
10th intensity percentile
P10 is the 10th percentile of Xgl. P10 is a more robust alternative to the minimum intensity.
CHAPTER 3. IMAGE FEATURES
dig. phantom
very strong
very strong
very strong
Table 3.35 — Reference values for the 10th percentile feature.
90th intensity percentile
P90 is the 90th percentile of Xgl. P90 is a more robust alternative to the maximum intensity.
dig. phantom
very strong
Table 3.36 — Reference values for the 90th percentile feature.
Note that the 90th intensity percentile obtained for the digital phantom may diﬀer from the
above reference value depending on the software implementation used to compute it. For example,
some implementations were found to produce a value of 4.2 instead of 4.
Maximum intensity
The maximum intensity is equal to the highest intensity present in Xgl, i.e:
Fstat.max = max(Xgl)
dig. phantom
very strong
very strong
very strong
Table 3.37 — Reference values for the maximum feature.
Intensity interquartile range
The interquartile range (IQR) of Xgl is deﬁned as:
Fstat.iqr = P75 −P25
CHAPTER 3. IMAGE FEATURES
P25 and P75 are the 25th and 75th percentiles of Xgl, respectively.
dig. phantom
very strong
very strong
very strong
Table 3.38 — Reference values for the interquartile range feature.
Intensity range
The intensity range is deﬁned as:
Fstat.range = max(Xgl) −min(Xgl)
dig. phantom
very strong
very strong
1.33 × 103
very strong
1.24 × 103
1.09 × 103
Table 3.39 — Reference values for the range feature.
Intensity-based mean absolute deviation
Mean absolute deviation is a measure of dispersion from the mean of Xgl:
Fstat.mad = 1
|Xgl,k −µ|
Here µ = Fstat.mean.
dig. phantom
very strong
very strong
very strong
Table 3.40 — Reference values for the mean absolute deviation feature.
CHAPTER 3. IMAGE FEATURES
Intensity-based robust mean absolute deviation
The intensity-based mean absolute deviation feature may be inﬂuenced by outliers. To increase
robustness, the set of intensities can be restricted to those which lie closer to the center of the
distribution. Let
Xgl,10−90 = {x ∈Xgl|P10 (Xgl) ≤x ≤P90 (Xgl)}
Then Xgl,10−90 is the set of Nv,10−90 ≤Nv voxels in Xgl whose intensities fall in the interval
bounded by the 10th and 90th percentiles of Xgl. The robust mean absolute deviation is then:
Fstat.rmad =
Xgl,10−90,k −Xgl,10−90
Xgl,10−90 denotes the sample mean of Xgl,10−90.
dig. phantom
very strong
very strong
very strong
Table 3.41 — Reference values for the robust mean absolute deviation feature.
Intensity-based median absolute deviation
Median absolute deviation is similar in concept to the intensity-based mean absolute deviation, but
measures dispersion from the median intensity instead of the mean intensity. Thus:
Fstat.medad = 1
|Xgl,k −M|
Here, median M = Fstat.median.
dig. phantom
very strong
Table 3.42 — Reference values for the median absolute deviation feature.
CHAPTER 3. IMAGE FEATURES
Intensity-based coeﬃcient of variation
The coeﬃcient of variation measures the dispersion of Xgl. It is deﬁned as:
Fstat.cov = σ
Here σ = Fstat.var
1/2 and µ = Fstat.mean are the standard deviation and mean of the intensity
distribution, respectively.
dig. phantom
very strong
Table 3.43 — Reference values for the coeﬃcient of variation feature.
Intensity-based quartile coeﬃcient of dispersion
The quartile coeﬃcient of dispersion is a more robust alternative to the intensity-based coeﬃcient
of variance. It is deﬁned as:
Fstat.qcod = P75 −P25
P25 and P75 are the 25th and 75th percentile of Xgl, respectively.
dig. phantom
very strong
Table 3.44 — Reference values for the quartile coeﬃcient of dispersion feature.
Intensity-based energy
The energy 1 of Xgl is deﬁned as:
Fstat.energy =
CHAPTER 3. IMAGE FEATURES
dig. phantom
very strong
1.65 × 109
very strong
3.98 × 108
2.44 × 109
1.48 × 109
1.58 × 109
Table 3.45 — Reference values for the energy feature.
Root mean square intensity
The root mean square intensity feature1, which is also called the quadratic mean, of Xgl is deﬁned
Fstat.rms =
dig. phantom
very strong
very strong
Table 3.46 — Reference values for the root mean square feature.
CHAPTER 3. IMAGE FEATURES
Intensity histogram features
An intensity histogram is generated by discretising the original intensity distribution Xgl into
intensity bins. Approaches to discretisation are described in Section 2.7.
Let Xd = {Xd,1, Xd,2, . . . , Xd,Nv} be the set of Ng discretised intensities of the Nv voxels in
the ROI intensity mask. Let H =
n1, n2, . . . , nNg
be the histogram with frequency count ni of
each discretised intensity i in Xd. The occurrence probability pi for each discretised intensity i is
then approximated as pi = ni/Nv.
Aggregating features
We recommend calculating intensity histogram features using the 3D volume (DHQ4). An approach that computes features per slice and subsequently averages (3IDG) is not recommended.
Mean discretised intensity
The mean 1 of Xd is calculated as:
Fih.mean = 1
An equivalent deﬁnition is:
Fih.mean =
dig. phantom
very strong
Table 3.47 — Reference values for the mean feature.
Discretised intensity variance
The variance 1 of Xd is deﬁned as:
Fih.var = 1
(Xd,k −µ)2
Here µ = Fih.mean. This deﬁnition is equivalent to:
(i −µ)2 pi
Note that no bias-correction is applied when computing the variance.
CHAPTER 3. IMAGE FEATURES
dig. phantom
Table 3.48 — Reference values for the variance feature.
Discretised intensity skewness
The skewness 1 of Xd is deﬁned as:
Fih.skew =
k=1 (Xd,k −µ)3
k=1 (Xd,k −µ)23/2
Here µ = Fih.mean. This deﬁnition is equivalent to:
Fih.skew =
i=1 (i −µ)3 pi
i=1 (i −µ)2 pi
If the discretised intensity variance Fih.var = 0, Fih.skew = 0.
dig. phantom
very strong
Table 3.49 — Reference values for the skewness feature.
(Excess) discretised intensity kurtosis
Kurtosis 1, or technically excess kurtosis, measures the peakedness of the Xd distribution:
Fih.kurt =
k=1 (Xd,k −µ)4
k=1 (Xd,k −µ)22 −3
Here µ = Fih.mean. An alternative, but equivalent, deﬁnition is:
Fih.kurt =
i=1 (i −µ)4 pi
i=1 (i −µ)2 pi
Note that kurtosis is corrected by a Fisher correction of -3 to center kurtosis on 0 for normal
distributions. If the discretised intensity variance Fih.var = 0, Fih.kurt = 0.
CHAPTER 3. IMAGE FEATURES
dig. phantom
very strong
Table 3.50 — Reference values for the (excess) kurtosis feature.
Median discretised intensity
The median Fih.median is the sample median of Xd 1.
dig. phantom
very strong
Table 3.51 — Reference values for the median feature.
Minimum discretised intensity
The minimum discretised intensity 1 is equal to the lowest discretised intensity present in Xd, i.e.:
Fih.min = min(Xd)
For ﬁxed bin number discretisation Fih.min = 1 by deﬁnition, but Fih.min > 1 is possible for
ﬁxed bin size discretisation.
dig. phantom
very strong
Table 3.52 — Reference values for the minimum feature.
10th discretised intensity percentile
P10 is the 10th percentile of Xd.
CHAPTER 3. IMAGE FEATURES
dig. phantom
very strong
Table 3.53 — Reference values for the 10th percentile feature.
90th discretised intensity percentile
P90 is the 90th percentile of Xd and is deﬁned as Fih.P90.
dig. phantom
Table 3.54 — Reference values for the 90th percentile feature.
Note that the 90th discretised intensity percentile obtained for the digital phantom may diﬀer
from the above reference value depending on the software implementation used to compute it. For
example, some implementations were found to produce a value of 4.2 instead of 4 for this feature.
Maximum discretised intensity
The maximum discretised intensity 1 is equal to the highest discretised intensity present in Xd, i.e.:
Fih.max = max(Xd)
By deﬁnition, Fih.max = Ng.
dig. phantom
very strong
Table 3.55 — Reference values for the maximum feature.
CHAPTER 3. IMAGE FEATURES
Intensity histogram mode
The mode of Xd Fih.mode is the most common discretised intensity present, i.e. the value i for with
the highest count ni. The mode may not be uniquely deﬁned. When the highest count is found in
multiple bins, the value i of the bin closest to the mean discretised intensity is chosen as intensity
histogram mode. In pathological cases with two such bins equidistant to the mean, the bin to the
left of the mean is selected.
dig. phantom
very strong
Table 3.56 — Reference values for the mode feature.
Discretised intensity interquartile range
The interquartile range (IQR) of Xd is deﬁned as:
Fih.iqr = P75 −P25
P25 and P75 are the 25th and 75th percentile of Xd, respectively.
dig. phantom
very strong
Table 3.57 — Reference values for the interquartile range feature.
Discretised intensity range
The discretised intensity range 1 is deﬁned as:
Fih.range = max(Xd) −min(Xd)
For ﬁxed bin number discretisation, the discretised intensity range equals Ng by deﬁnition.
CHAPTER 3. IMAGE FEATURES
dig. phantom
very strong
Table 3.58 — Reference values for the range feature.
Intensity histogram mean absolute deviation
The mean absolute deviation 1 is a measure of dispersion from the mean of Xd:
Fih.mad = 1
Here µ = Fih.mean.
dig. phantom
very strong
Table 3.59 — Reference values for the mean absolute deviation feature.
Intensity histogram robust mean absolute deviation
Intensity histogram mean absolute deviation may be aﬀected by outliers. To increase robustness,
the set of discretised intensities under consideration can be restricted to those which are closer to
the center of the distribution. Let
Xd,10−90 = {x ∈Xd|P10 (Xd) ≤x ≤P90 (Xd)}
In short, Xd,10−90 is the set of Nv,10−90 ≤Nv voxels in Xd whose discretised intensities fall in the
interval bounded by the 10th and 90th percentiles of Xd. The robust mean absolute deviation is
Fih.rmad =
Xd,10−90,k −Xd,10−90
Xd,10−90 denotes the sample mean of Xd,10−90.
CHAPTER 3. IMAGE FEATURES
dig. phantom
very strong
Table 3.60 — Reference values for the robust mean absolute deviation feature.
Intensity histogram median absolute deviation
Histogram median absolute deviation is conceptually similar to histogram mean absolute deviation,
but measures dispersion from the median instead of mean. Thus:
Fih.medad = 1
Here, median M = Fih.median.
dig. phantom
very strong
Table 3.61 — Reference values for the median absolute deviation feature.
Intensity histogram coeﬃcient of variation
The coeﬃcient of variation measures the dispersion of the discretised intensity distribution. It is
deﬁned as:
Fih.cov = σ
Here σ = Fih.var
1/2 and µ = Fih.mean are the standard deviation and mean of the discretised
intensity distribution, respectively.
dig. phantom
very strong
Table 3.62 — Reference values for the coeﬃcient of variation feature.
CHAPTER 3. IMAGE FEATURES
Intensity histogram quartile coeﬃcient of dispersion
The quartile coeﬃcient of dispersion is a more robust alternative to the intensity histogram coef-
ﬁcient of variance. It is deﬁned as:
Fih.qcod = P75 −P25
P25 and P75 are the 25th and 75th percentile of Xd, respectively.
dig. phantom
very strong
Table 3.63 — Reference values for the quartile coeﬃcient of dispersion feature.
Discretised intensity entropy
Entropy 1 is an information-theoretic concept that gives a metric for the information contained
within Xd. The particular metric used is Shannon entropy, which is deﬁned as:
Fih.entropy = −
pi log2 pi
Note that entropy can only be meaningfully deﬁned for discretised intensities as it will tend to
−log2 Nv for continuous intensity distributions.
dig. phantom
very strong
very strong
Table 3.64 — Reference values for the entropy feature.
Discretised intensity uniformity
Uniformity 1 of Xd is deﬁned as:
Fih.uniformity =
For histograms where most intensities are contained in a single bin, uniformity approaches 1.
The lower bound is 1/Ng.
CHAPTER 3. IMAGE FEATURES
Note that this feature is sometimes referred to as energy.
dig. phantom
very strong
very strong
Table 3.65 — Reference values for the uniformity feature.
Maximum histogram gradient
The histogram gradient H′ of intensity histogram H can be calculated as:
(ni+1 −ni−1) /2
1 < i < Ng
nNg −nNg−1
Histogram H should be non-sparse, i.e. bins where ni = 0 should not be omitted. Ostensibly, the
histogram gradient can be calculated in diﬀerent ways. The method above has the advantages of
being easy to implement and leading to a gradient H′ with same size as H. This helps maintain a
direct correspondence between the discretised intensities in H and the bins of H′. The maximum
histogram gradient 80 is:
Fih.max.grad = max (H′)
dig. phantom
very strong
3.22 × 103
4.75 × 103
7.26 × 103
6.01 × 103
Table 3.66 — Reference values for the maximum histogram gradient feature.
Maximum histogram gradient intensity
The maximum histogram gradient intensity 80 Fih.max.grad.gl is the discretised intensity corresponding to the maximum histogram gradient, i.e. the value i in H for which H′ is maximal.
CHAPTER 3. IMAGE FEATURES
dig. phantom
Table 3.67 — Reference values for the maximum histogram gradient intensity feature.
Minimum histogram gradient
The minimum histogram gradient 80 is:
Fih.min.grad = min (H′)
dig. phantom
very strong
−1.01 × 104
−3.02 × 103
−4.68 × 103
−6.67 × 103
−6.11 × 103
Table 3.68 — Reference values for the minimum histogram gradient feature.
Minimum histogram gradient intensity
The minimum histogram gradient intensity 80 Fih.min.grad.gl is the discretised intensity corresponding to the minimum histogram gradient, i.e. the value i in H for which H′ is minimal.
dig. phantom
Table 3.69 — Reference values for the minimum histogram gradient intensity feature.
CHAPTER 3. IMAGE FEATURES
Intensity-volume histogram features
The (cumulative) intensity-volume histogram (IVH) of the set Xgl of voxel intensities in the ROI
intensity mask describes the relationship between discretised intensity i and the fraction of the
volume containing at least intensity i, ν 27.
Depending on the imaging modality, the calculation of IVH features requires discretising Xgl
to generate a new voxel set Xd,gl with discretised intensities.
Moreover, the total range G of
discretised intensities and the discretisation interval wd should be provided or determined. The
total range G determines the range of discretised intensities to be included in the IVH, whereas the
discretisation interval determines the intensity diﬀerence between adjacent discretised intensities
in the IVH.
Recommendations for discretisation parameters diﬀer depending on what type of data the image
represents, and how it is represented. These recommendations are described below.
Discrete calibrated image intensities
Some imaging modalities by default generate voxels with calibrated, discrete intensities – for
example CT. In this case, the discretised ROI voxel set Xd,gl = Xgl, i.e. no discretisation required.
If a re-segmentation range is provided (see Section 2.5), the total range G is equal to the resegmentation range. In the case of a half-open re-segmentation range, the upper limit of the range
is max(Xgl). When no range is provided, G = [min(Xgl), max(Xgl)]. The discretisation interval
is wd = 1.
Continuous calibrated image intensities
Imaging with calibrated, continuous intensities such as PET requires discretisation to determine
the IVH, while preserving the quantitative intensity information.
The use of a ﬁxed bin size
discretisation method is thus recommended, see Section 2.7. This method requires a minimum
intensity Xgl,min, a maximum intensity Xgl,max and the bin width wb. If a re-segmentation range
is deﬁned (see Section 2.5), Xgl,min is set to the lower bound of the re-segmentation range and
Xgl,max to the upper bound; otherwise Xgl,min = min(Xgl) and Xgl,max = max(Xgl) (i.e. the
minimum and maximum intensities in the imaging volume prior to discretisation). The bin width
wb is modality dependent, but should be small relative to the intensity range, e.g. 0.10 SUV for
18F-FDG-PET.
Next, ﬁxed bin size discretisation produces the voxel set Xd of bin numbers, which needs to be
converted to bin centers in order to maintain a direct relationship with the original intensities. We
thus replace bin numbers Xd with the intensity corresponding to the bin center:
Xd,gl = Xgl,min + (Xd −0.5) wb
The total range is then G = [Xgl,min + 0.5wb, Xgl,max −0.5wb]. In this case, the discretisation
interval matches the bin width, i.e. wd = wb.
Arbitrary intensity units
Some imaging modalities, such as many MRI sequences, produce arbitrary intensities. In such
cases, a ﬁxed bin number discretisation method with Ng = 1000 bins is recommended, see Section
2.7. The discretisation bin width is wb = (Xgl,max −Xgl,min) /Ng, with Xgl,max = max (Xgl) and
Xgl,min = min (Xgl), as re-segmentation ranges generally cannot be provided for non-calibrated
intensities. The ﬁxed bin number discretisation produces the voxel set Xd ∈{1, 2, . . . , Ng}. Because
of the lack of calibration, Xd,gl = Xd, and consequentially the discretisation interval is wd = 1 and
the total range is G = [1, Ng]
CHAPTER 3. IMAGE FEATURES
Table 3.70 — Example intensity-volume histogram evaluated at discrete intensities i of the digital
phantom. The total range G = , with discretisation interval w = 1. Thus γ is the intensity
fraction and ν is the corresponding volume fraction that contains intensity i or greater.
Calculating the IV histogram
We use Xd,gl to calculate fractional volumes and fractional intensities.
As voxels for the same image stack generally all have the same dimensions, we may deﬁne
fractional volume ν for discretised intensity i:
[Xd,gl,k < i]
Here [. . .] is an Iverson bracket, yielding 1 if the condition is true and 0 otherwise. In essence, we
count the voxels containing a discretised intensity smaller than i, divide by the total number of
voxels, and then subtract this volume fraction to ﬁnd νi.
The intensity fraction γ for discretised intensity i in the range G is calculated as:
i −min (G)
max (G) −min (G)
Note that intensity fractions are also calculated for discretised intensities that are absent in Xd,gl.
For example intensities 2 and 5 are absent in the digital phantom (see Chapter 5), but are still
evaluated to determine both the fractional volume and the intensity fraction. An example IVH for
the digital phantom is shown in Table 3.70.
Aggregating features
We recommend calculating intensity-volume histogram features using the 3D volume (DHQ4).
Computing features per slice and subsequently averaging (3IDG) is not recommended.
Volume at intensity fraction
The volume at intensity fraction Vx is the largest volume fraction ν that has an intensity fraction γ
of at least x%. This diﬀers from conceptually similar dose-volume histograms used in radiotherapy
planning, where V10 would indicate the volume fraction receiving at least 10 Gy planned dose. El
Naqa et al. 27 deﬁned both V10 and V90 as features. In the context of this work, these two features
are deﬁned as Fivh.V10 and Fivh.V90, respectively.
CHAPTER 3. IMAGE FEATURES
dig. phantom
very strong
Table 3.71 — Reference values for the volume fraction at 10% intensity feature.
dig. phantom
very strong
6.98 × 10−5
1.03 × 10−5
7.31 × 10−5
1.03 × 10−5
Table 3.72 — Reference values for the volume fraction at 90% intensity feature.
Intensity at volume fraction
The intensity at volume fraction Ix is the minimum discretised intensity i present in at most x%
of the volume. El Naqa et al. 27 deﬁned both I10 and I90 as features. In the context of this work,
these two features are deﬁned as Fivh.I10 and Fivh.I90, respectively.
dig. phantom
very strong
Table 3.73 — Reference values for the intensity at 10% volume feature.
CHAPTER 3. IMAGE FEATURES
dig. phantom
very strong
Table 3.74 — Reference values for the intensity at 90% volume feature.
Volume fraction diﬀerence between intensity fractions
This feature is the diﬀerence between the volume fractions at two diﬀerent intensity fractions, e.g.
V10 −V90 27. In the context of this work, this feature is deﬁned as Fivh.V10minusV90.
dig. phantom
very strong
Table 3.75 — Reference values for the volume fraction diﬀerence between 10% and 90% intensity
Intensity fraction diﬀerence between volume fractions
This feature is the diﬀerence between discretised intensities at two diﬀerent fractional volumes,
e.g. I10 −I90 27. In the context of this work, this feature is deﬁned as Fivh.I10minusI90.
dig. phantom
very strong
Table 3.76 — Reference values for the intensity diﬀerence between 10% and 90% volume feature.
Area under the IVH curve
Note: This feature currently has no reference values and should not be used.
The area under the IVH curve Fivh.auc was deﬁned by van Velden et al. 82. The area under the
IVH curve can be approximated by calculating the Riemann sum using the trapezoidal rule. Note
that if there is only one discretised intensity in the ROI, we deﬁne the area under the IVH curve
Fivh.auc = 0.
CHAPTER 3. IMAGE FEATURES
Grey level co-occurrence based features
In image analysis, texture is one of the deﬁning sets of features. Texture features were originally
designed to assess surface texture in 2D images. Texture analysis is however not restricted to
2D slices and can be extended to 3D objects. Image intensities are generally discretised before
calculation of texture features, see Section 2.7.
The grey level co-occurrence matrix (GLCM) is a matrix that expresses how combinations of
discretised intensities (grey levels) of neighbouring pixels, or voxels in a 3D volume, are distributed
along one of the image directions. Generally, the neighbourhood for GLCM is a 26-connected
neighbourhood in 3D and a 8-connected neighbourhood in 2D. Thus, in 3D there are 13 unique
direction vectors within the neighbourhood for Chebyshev distance δ = 1, i.e. (0, 0, 1), (0, 1, 0),
(1, 0, 0), (0, 1, 1), (0, 1, −1), (1, 0, 1), (1, 0, −1), (1, 1, 0), (1, −1, 0), (1, 1, 1), (1, 1, −1), (1, −1, 1) and
(1, −1, −1), whereas in 2D the direction vectors are (1, 0, 0), (1, 1, 0), (0, 1, 0) and (−1, 1, 0).
A GLCM is calculated for each direction vector, as follows. Let Mm be the Ng × Ng grey level
co-occurrence matrix, with Ng the number of discretised grey levels present in the ROI intensity
mask, and m the particular direction vector. Element (i, j) of the GLCM contains the frequency at
which combinations of discretised grey levels i and j occur in neighbouring voxels along direction
m+ = m and along direction m−= −m. Then, Mm = Mm+ + Mm−= Mm+ + MT
consequence the GLCM matrix Mm is symmetric. An example of the calculation of a GLCM is
shown in Table 3.77. Corresponding grey level co-occurrence matrices for each direction are shown
in Table 3.78.
(a) Grey levels
Table 3.77 — Grey levels (a) and corresponding grey level co-occurrence matrices for the 0◦(b)
and 180◦directions (c). In vector notation these directions are m+ = (1, 0) and m−= (−1, 0). To
calculate the symmetrical co-occurrence matrix Mm both matrices are summed by element.
GLCM features rely on the probability distribution for the elements of the GLCM. Let us
consider Mm=(1,0) from the example, as shown in Table 3.79. We derive a probability distribution
for grey level co-occurrences, Pm, by normalising Mm by the sum of its elements. Each element
pij of Pm is then the joint probability of grey levels i and j occurring in neighbouring voxels
along direction m. Then pi. = PNg
j=1 pij is the row marginal probability, and p.j = PNg
i=1 pij is the
column marginal probability. As Pm is by deﬁnition symmetric, pi. = p.j. Furthermore, let us
consider diagonal and cross-diagonal probabilities pi−j and pi+j 36,74:
pij [k = |i −j|]
k = 0, . . . , Ng −1
pij [k = i + j]
k = 2, . . . , 2Ng
Here, [. . .] is an Iverson bracket, which equals 1 when the condition within the brackets is true and
0 otherwise. In eﬀect we select only combinations of elements (i, j) for which the condition holds.
CHAPTER 3. IMAGE FEATURES
Table 3.78 — Grey level co-occurrence matrices for the 0◦(a), 45◦(b), 90◦(c) and 135◦(d)
directions. In vector notation these directions are m = (1, 0), m = (1, 1), m = (0, 1) and m = (−1, 1),
respectively.
It should be noted that while a distance δ = 1 is commonly used for GLCM, other distances are
possible. However, this does not change the number of For example, for δ = 3 (in 3D) the voxels
at (0, 0, 3), (0, 3, 0), (3, 0, 0), (0, 3, 3), (0, 3, −3), (3, 0, 3), (3, 0, −3), (3, 3, 0), (3, −3, 0), (3, 3, 3),
(3, 3, −3), (3, −3, 3) and (3, −3, −3) from the center voxel are considered.
Aggregating features
To improve rotational invariance, GLCM feature values are computed by aggregating information from the diﬀerent underlying directional matrices23. Five methods can be used to aggregate
GLCMs and arrive at a single feature value. A schematic example is shown in Figure 3.3. A feature
may be aggregated as follows:
1. Features are computed from each 2D directional matrix and averaged over 2D directions and
slices (BTW3).
2. Features are computed from a single matrix after merging 2D directional matrices per slice,
and then averaged over slices (SUJT).
3. Features are computed from a single matrix after merging 2D directional matrices per direction, and then averaged over directions (JJUI).
4. The feature is computed from a single matrix after merging all 2D directional matrices (ZW7Z).
5. Features are computed from each 3D directional matrix and averaged over the 3D directions
6. The feature is computed from a single matrix after merging all 3D directional matrices (IAZD).
In methods 2,3,4 and 6, matrices are merged by summing the co-occurrence counts in each matrix
element (i, j) over the diﬀerent matrices. Probability distributions are subsequently calculated for
the merged GLCM, which is then used to calculate GLCM features. Feature values may dependent
strongly on the aggregation method.
CHAPTER 3. IMAGE FEATURES
(a) Mm=(1,0) with margins
(b) Pm=(1,0) with margins
k = |i −j|
(c) Diagonal probability for Pm=(1,0)
(d) Cross-diagonal probability for Pm=(1,0)
Table 3.79 — Grey level co-occurrence matrix for the 0◦direction (a); its corresponding probability
matrix Pm=(1,0) with marginal probabilities pi. and p.j(b); the diagonal probabilities pi−j (c); and
the cross-diagonal probabilities pi+j (d).
Discrepancies in panels b, c, and d are due to rounding
errors caused by showing only two decimal places. Also, note that due to GLCM symmetry marginal
probabilities pi. and p.j are the same in both row and column margins of panel b.
Distances and distance weighting
The default neighbourhood includes all voxels within Chebyshev distance 1. The corresponding
direction vectors are multiplied by the desired distance δ. From a technical point-of-view, direction
vectors may also be determined diﬀerently, using any distance norm. In this case, direction vectors
are the vectors to the voxels at δ, or between δ and δ −1 for the Euclidean norm. Such usage is
however rare and we caution against it due to potential reproducibility issues.
GLCMs may be weighted for distance by multiplying M with a weighting factor w. By default
w = 1, but w may also be an inverse distance function to weight each GLCM, e.g. w = ∥m∥−1 or
w = exp(−∥m∥2)81, with ∥m∥the length of direction vector m. Whether distance weighting yields
diﬀerent feature values depends on several factors. When aggregating the feature values, matrices
have to be merged ﬁrst, otherwise weighting has no eﬀect. Also, it has no eﬀect if the default
neighbourhood is used and the Chebyshev norm is using for weighting. Nor does weighting have
an eﬀect if either Manhattan or Chebyshev norms are used both for constructing a non-default
neighbourhood and for weighting. Weighting may furthermore have no eﬀect for distance δ = 1,
dependent on distance norms. Because of these exceptions, we recommend against using distance
weighting for GLCM.
Joint maximum
Joint maximum 35 is the probability corresponding to the most common grey level co-occurrence
in the GLCM:
Fcm.joint.max = max(pij)
CHAPTER 3. IMAGE FEATURES
(a) 2D: by slice, without merging
(b) 2D: by slice, with merging by slice
(c) 2.5D: by slice, with merging by direction
(d) 2.5D: by slice, with full merging
(e) 3D: as volume, without merging
(f) 3D: as volume, with full merging
Figure 3.3 — Approaches to calculating grey level co-occurrence matrix-based features. M∆k are
texture matrices calculated for direction ∆in slice k (if applicable), and f∆k is the corresponding
feature value. In (b-d) and (e) the matrices are merged prior to feature calculation.
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
3D, averaged
CHAPTER 3. IMAGE FEATURES
3D, merged
Table 3.80 — Reference values for the joint maximum feature.
Joint average
Joint average 74 is the grey level weighted sum of joint probabilities:
Fcm.joint.avg =
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.81 — Reference values for the joint average feature.
Joint variance
The joint variance 74, which is also called sum of squares 36, is deﬁned as:
Fcm.joint.var =
(i −µ)2 pij
Here µ is equal to the value of Fcm.joint.avg, which was deﬁned above.
aggr. method
CHAPTER 3. IMAGE FEATURES
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.82 — Reference values for the joint variance feature.
Joint entropy
Joint entropy 36 is deﬁned as:
Fcm.joint.entr = −
pij log2 pij
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
CHAPTER 3. IMAGE FEATURES
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.83 — Reference values for the joint entropy feature.
Diﬀerence average
The diﬀerence average 74 for the diagonal probabilities is deﬁned as:
Fcm.diﬀ.avg =
By deﬁnition diﬀerence average is equivalent to the dissimilarity feature81.
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.84 — Reference values for the diﬀerence average feature.
CHAPTER 3. IMAGE FEATURES
Diﬀerence variance
The diﬀerence variance for the diagonal probabilities36 is deﬁned as:
Fcm.diﬀ.var =
(k −µ)2pi−j,k
Here µ is equal to the value of diﬀerence average.
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.85 — Reference values for the diﬀerence variance feature.
Diﬀerence entropy
The diﬀerence entropy for the diagonal probabilities36 is deﬁned as:
Fcm.diﬀ.entr = −
pi−j,k log2 pi−j,k
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
CHAPTER 3. IMAGE FEATURES
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.86 — Reference values for the diﬀerence entropy feature.
Sum average
The sum average for the cross-diagonal probabilities36 is deﬁned as:
Fcm.sum.avg =
By deﬁnition, Fcm.sum.avg = 2Fcm.joint.avg 81.
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
3D, averaged
CHAPTER 3. IMAGE FEATURES
3D, merged
Table 3.87 — Reference values for the sum average feature.
Sum variance
The sum variance for the cross-diagonal probabilities36 is deﬁned as:
Fcm.sum.var =
(k −µ)2pi+j,k
Here µ is equal to the value of sum average. Sum variance is mathematically identical to the
cluster tendency feature81.
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.88 — Reference values for the sum variance feature.
Sum entropy
The sum entropy for the cross-diagonal probabilities36 is deﬁned as:
Fcm.sum.entr = −
pi+j,k log2 pi+j,k
aggr. method
CHAPTER 3. IMAGE FEATURES
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.89 — Reference values for the sum entropy feature.
Angular second moment
The angular second moment 36, which represents the energy of P∆, is deﬁned as:
Fcm.energy =
This feature is also called energy 1,74 and uniformity 18.
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
CHAPTER 3. IMAGE FEATURES
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.90 — Reference values for the angular second moment feature.
Contrast assesses grey level variations36. Hence elements of M∆that represent large grey level
diﬀerences receive greater weight. Contrast is deﬁned as18:
Fcm.contrast =
(i −j)2 pij
Note that the original deﬁnition by Haralick et al. 36 is seemingly more complex, but rearranging
and simplifying terms leads to the above formulation of contrast.
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.91 — Reference values for the contrast feature.
CHAPTER 3. IMAGE FEATURES
Dissimilarity
Dissimilarity 18 is conceptually similar to the contrast feature, and is deﬁned as:
Fcm.dissimilarity =
|i −j| pij
By deﬁnition dissimilarity is equivalent to the diﬀerence average feature81.
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.92 — Reference values for the dissimilarity feature.
Inverse diﬀerence
Inverse diﬀerence is a measure of homogeneity18. Grey level co-occurrences with a large diﬀerence
in levels are weighed less, thus lowering the total feature value. The feature score is maximal if all
grey levels are the same. Inverse diﬀerence is deﬁned as:
Fcm.inv.diﬀ=
1 + |i −j|
The equation above may also be expressed in terms of diagonal probabilities81:
Fcm.inv.diﬀ=
CHAPTER 3. IMAGE FEATURES
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.93 — Reference values for the inverse diﬀerence feature.
Normalised inverse diﬀerence
Clausi 18 suggested normalising inverse diﬀerence to improve classiﬁcation ability. The normalised
feature is then deﬁned as:
Fcm.inv.diﬀ.norm =
1 + |i −j|/Ng
Note that in Clausi’s deﬁnition, |i−j|2/N 2
g is used instead of |i−j|/Ng, which is likely an oversight,
as this exactly matches the deﬁnition of the normalised inverse diﬀerence moment feature.
The equation may also be expressed in terms of diagonal probabilities81:
Fcm.inv.diﬀ.norm =
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
CHAPTER 3. IMAGE FEATURES
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.94 — Reference values for the normalised inverse diﬀerence feature.
Inverse diﬀerence moment
Inverse diﬀerence moment 36 is similar in concept to the inverse diﬀerence feature, but with lower
weights for elements that are further from the diagonal:
Fcm.inv.diﬀ.mom =
1 + (i −j)2
The equation above may also be expressed in terms of diagonal probabilities81:
Fcm.inv.diﬀ.mom =
This feature is also called homogeneity 74.
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
CHAPTER 3. IMAGE FEATURES
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.95 — Reference values for the inverse diﬀerence moment feature.
Normalised inverse diﬀerence moment
Clausi 18 suggested normalising inverse diﬀerence moment to improve classiﬁcation performance.
This leads to the following deﬁnition:
Fcm.inv.diﬀ.mom.norm =
1 + (i −j)2 /N 2g
The equation above may also be expressed in terms of diagonal probabilities81:
Fcm.inv.diﬀ.mom.norm =
1 + (k/Ng)2
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
3D, averaged
3D, merged
CHAPTER 3. IMAGE FEATURES
Table 3.96 — Reference values for the normalised inverse diﬀerence moment feature.
Inverse variance
The inverse variance 1 feature is deﬁned as:
Fcm.inv.var = 2
The equation above may also be expressed in terms of diagonal probabilities. Note that in this
case, summation starts at k = 1 instead of k = 081:
Fcm.inv.var =
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.97 — Reference values for the inverse variance feature.
Correlation
Correlation 36 is deﬁned as:
Fcm.corr =
−µi. µ.j +
CHAPTER 3. IMAGE FEATURES
i=1 i pi. and σi. =
i=1(i −µi.)2pi.
are the mean and standard deviation of row
marginal probability pi., respectively. Likewise, µ.j and σ.j are the mean and standard deviation
of the column marginal probability p.j, respectively. The calculation of correlation can be simpliﬁed
since P∆is symmetrical:
Fcm.corr = 1
An equivalent formulation of correlation is:
Fcm.corr =
(i −µi.) (j −µ.j) pij
Again, simplifying due to matrix symmetry yields:
Fcm.corr = 1
(i −µi.) (j −µi.) pij
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.98 — Reference values for the correlation feature.
CHAPTER 3. IMAGE FEATURES
Autocorrelation
Soh and Tsatsoulis 63 deﬁned autocorrelation as:
Fcm.auto.corr =
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
1.58 × 103
3D, merged
1.58 × 103
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.99 — Reference values for the autocorrelation feature.
Cluster tendency
Cluster tendency 1 is deﬁned as:
Fcm.clust.tend =
(i + j −µi. −µ.j)2 pij
Here µi. = PNg
i=1 i pi. and µ.j = PNg
j=1 j p.j. Because of the symmetric nature of P∆, the feature
can also be formulated as:
Fcm.clust.tend =
(i + j −2µi.)2 pij
Cluster tendency is mathematically equal to the sum variance feature81.
CHAPTER 3. IMAGE FEATURES
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.100 — Reference values for the cluster tendency feature.
Cluster shade
Cluster shade 74 is deﬁned as:
Fcm.clust.shade =
(i + j −µi. −µ.j)3 pij
As with cluster tendency, µi. = PNg
i=1 i pi. and µ.j = PNg
j=1 j p.j. Because of the symmetric nature
of P∆, the feature can also be formulated as:
Fcm.clust.shade =
(i + j −2µi.)3 pij
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
−1.04 × 103
CHAPTER 3. IMAGE FEATURES
2D, slice-merged
−1.05 × 103
2.5D, direction-merged
−1.49 × 103
2.5D, merged
−1.49 × 103
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
−1.06 × 104
3D, merged
−1.06 × 104
very strong
3D, averaged
−1.27 × 103
3D, merged
−1.28 × 103
3D, averaged
−2.07 × 103
3D, merged
−2.08 × 103
Table 3.101 — Reference values for the cluster shade feature.
Cluster prominence
Cluster prominence 74 is deﬁned as:
Fcm.clust.prom =
(i + j −µi. −µ.j)4 pij
As before, µi. = PNg
i=1 i pi. and µ.j = PNg
j=1 j p.j. Because of the symmetric nature of P∆, the
feature can also be formulated as:
Fcm.clust.prom =
(i + j −2µi.)4 pij
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
5.27 × 104
2D, slice-merged
5.28 × 104
2.5D, direction-merged
4.76 × 104
2.5D, merged
4.77 × 104
2D, averaged
2.94 × 104
2D, slice-merged
2.95 × 104
2.5D, direction-merged
2.52 × 104
2.5D, merged
2.53 × 104
3D, averaged
5.69 × 105
3D, merged
very strong
CHAPTER 3. IMAGE FEATURES
3D, averaged
3.57 × 104
3D, merged
3.57 × 104
3D, averaged
6.89 × 104
3D, merged
Table 3.102 — Reference values for the cluster prominence feature.
Information correlation 1
Information theoretic correlation is estimated using two diﬀerent measures36. For symmetric P∆
the ﬁrst measure is deﬁned as:
Fcm.info.corr.1 = HXY −HXY1
HXY = −PNg
j=1 pij log2 pij is the entropy for the joint probability. HX = −PNg
i=1 pi. log2 pi.
is the entropy for the row marginal probability, which due to symmetry is equal to the entropy of
the column marginal probability. HXY 1 is a type of entropy that is deﬁned as:
pij log2 (pi.p.j)
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.103 — Reference values for the information correlation 1 feature.
CHAPTER 3. IMAGE FEATURES
Information correlation 2
The second measure of information theoretic correlation 36 is estimated as follows for symmetric
Fcm.info.corr.2 =
1 −exp (−2 (HXY 2 −HXY ))
As earlier, HXY = −PNg
j=1 pij log2 pij. HXY 2 is a type of entropy deﬁned as:
pi.p.j log2 (pi.p.j)
aggr. method
dig. phantom
2D, averaged
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.104 — Reference values for the information correlation 2 feature.
CHAPTER 3. IMAGE FEATURES
Grey level run length based features
The grey level run length matrix (GLRLM) was introduced by Galloway 30 to deﬁne various texture features. Like the grey level co-occurrence matrix, GLRLM also assesses the distribution of
discretised grey levels in an image or in a stack of images. However, whereas GLCM assesses
co-occurrence of grey levels within neighbouring pixels or voxels, GLRLM assesses run lengths. A
run length is deﬁned as the length of a consecutive sequence of pixels or voxels with the same grey
level along direction m, which was previously deﬁned in Section 3.6. The GLRLM then contains
the occurrences of runs with length j for a discretised grey level i.
A complete example for GLRLM construction from a 2D image is shown in Table 3.105. Let
Mm be the Ng × Nr grey level run length matrix, where Ng is the number of discretised grey
levels present in the ROI intensity mask and Nr the maximal possible run length along direction
m. Matrix element rij of the GLRLM is the occurrence of grev level i with run length j. Then,
let Nv be the total number of voxels in the ROI intensity mask, and Ns = PNg
j=1 rij the sum
over all elements in Mm. Marginal sums are also deﬁned. Let ri. be the marginal sum of the runs
over run lengths j for grey value i, that is ri. = PNr
j=1 rij. Similarly, the marginal sum of the runs
over the grey values i for run length j is r.j = PNg
Aggregating features
To improve rotational invariance, GLRLM feature values are computed by aggregating information from the diﬀerent underlying directional matrices23. Five methods can be used to aggregate
GLRLMs and arrive at a single feature value. A schematic example was previously shown in Figure
3.3. A feature may be aggregated as follows:
1. Features are computed from each 2D directional matrix and averaged over 2D directions and
slices (BTW3).
2. Features are computed from a single matrix after merging 2D directional matrices per slice,
and then averaged over slices (SUJT).
3. Features are computed from a single matrix after merging 2D directional matrices per direction, and then averaged over directions (JJUI).
4. The feature is computed from a single matrix after merging all 2D directional matrices (ZW7Z).
5. Features are computed from each 3D directional matrix and averaged over the 3D directions
6. The feature is computed from a single matrix after merging all 3D directional matrices (IAZD).
In methods 2,3,4 and 6 matrices are merged by summing the run counts of each matrix element
(i, j) over the diﬀerent matrices.
Note that when matrices are merged, Nv should likewise be
summed to retain consistency. Feature values may dependent strongly on the aggregation method.
Distance weighting
GLRLMs may be weighted for distance by multiplying the run lengths with a weighting factor
By default w = 1, but w may also be an inverse distance function, e.g.
w = ∥m∥−1 or
w = exp(−∥m∥2)81, with ∥m∥the length of direction vector m.
Whether distance weighting
yields diﬀerent feature values depends on several factors. When aggregating the feature values,
matrices have to be merged ﬁrst, otherwise weighting has no eﬀect. It also has no eﬀect if the
Chebyshev norm is used for weighting. Distance weighting is non-standard use, and we caution
against it due to potential reproducibility issues.
CHAPTER 3. IMAGE FEATURES
(a) Grey levels
Run length j
Run length j
Run length j
Run length j
Table 3.105 — Grey level run length matrices for the 0◦(a), 45◦(b), 90◦(c) and 135◦(d) directions. In
vector notation these directions are m = (1, 0), m = (1, 1), m = (0, 1) and m = (−1, 1), respectively.
Short runs emphasis
This feature emphasises short run lengths30. It is deﬁned as:
Frlm.sre = 1
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
CHAPTER 3. IMAGE FEATURES
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.106 — Reference values for the short runs emphasis feature.
Long runs emphasis
This feature emphasises long run lengths30. It is deﬁned as:
Frlm.lre = 1
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.107 — Reference values for the long runs emphasis feature.
CHAPTER 3. IMAGE FEATURES
Low grey level run emphasis
This feature is a grey level analogue to short runs emphasis 15. Instead of short run lengths, low
grey levels are emphasised. The feature is deﬁned as:
Frlm.lgre = 1
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.108 — Reference values for the low grey level run emphasis feature.
High grey level run emphasis
The high grey level run emphasis feature is a grey level analogue to long runs emphasis 15. The
feature emphasises high grey levels, and is deﬁned as:
Frlm.hgre = 1
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
CHAPTER 3. IMAGE FEATURES
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
1.47 × 103
3D, merged
1.47 × 103
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.109 — Reference values for the high grey level run emphasis feature.
Short run low grey level emphasis
This feature emphasises runs in the upper left quadrant of the GLRLM, where short run lengths
and low grey levels are located22. It is deﬁned as:
Frlm.srlge = 1
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
3D, averaged
CHAPTER 3. IMAGE FEATURES
3D, merged
3D, averaged
3D, merged
Table 3.110 — Reference values for the short run low grey level emphasis feature.
Short run high grey level emphasis
This feature emphasises runs in the lower left quadrant of the GLRLM, where short run lengths
and high grey levels are located22. The feature is deﬁned as:
Frlm.srhge = 1
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.111 — Reference values for the short run high grey level emphasis feature.
Long run low grey level emphasis
This feature emphasises runs in the upper right quadrant of the GLRLM, where long run lengths
and low grey levels are located22. The feature is deﬁned as:
Frlm.lrlge = 1
CHAPTER 3. IMAGE FEATURES
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.112 — Reference values for the long run low grey level emphasis feature.
Long run high grey level emphasis
This feature emphasises runs in the lower right quadrant of the GLRLM, where long run lengths
and high grey levels are located22. The feature is deﬁned as:
Frlm.lrhge = 1
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
1.41 × 103
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
1.39 × 103
CHAPTER 3. IMAGE FEATURES
2D, slice-merged
1.38 × 103
2.5D, direction-merged
2.5D, merged
3D, averaged
5.59 × 103
3D, merged
5.53 × 103
3D, averaged
2.67 × 103
3D, merged
2.63 × 103
3D, averaged
3D, merged
1.89 × 103
Table 3.113 — Reference values for the long run high grey level emphasis feature.
Grey level non-uniformity
This feature assesses the distribution of runs over the grey values30. The feature value is low when
runs are equally distributed along grey levels. The feature is deﬁned as:
Frlm.glnu = 1
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
1.73 × 103
2.5D, direction-merged
9.85 × 103
2.5D, merged
3.94 × 104
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3.18 × 103
3D, merged
4.13 × 104
3D, averaged
3.29 × 103
3D, merged
4.28 × 104
3D, averaged
3D, merged
5.19 × 104
Table 3.114 — Reference values for the grey level non-uniformity feature.
CHAPTER 3. IMAGE FEATURES
Normalised grey level non-uniformity
This is a normalised version of the grey level non-uniformity feature. It is deﬁned as:
Frlm.glnu.norm =
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.115 — Reference values for the normalised grey level non-uniformity feature.
Run length non-uniformity
This features assesses the distribution of runs over the run lengths30. The feature value is low
when runs are equally distributed along run lengths. It is deﬁned as:
Frlm.rlnu = 1
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
CHAPTER 3. IMAGE FEATURES
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
1.65 × 103
2D, slice-merged
2.5D, direction-merged
4.27 × 104
2.5D, merged
1.71 × 105
2D, averaged
2D, slice-merged
1.46 × 103
2.5D, direction-merged
9.38 × 103
2.5D, merged
3.75 × 104
3D, averaged
3D, merged
2.34 × 105
3D, averaged
1.24 × 104
3D, merged
3D, averaged
1.66 × 104
3D, merged
2.15 × 105
Table 3.116 — Reference values for the run length non-uniformity feature.
Normalised run length non-uniformity
This is normalised version of the run length non-uniformity feature. It is deﬁned as:
Frlm.rlnu.norm =
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
CHAPTER 3. IMAGE FEATURES
3D, averaged
3D, merged
Table 3.117 — Reference values for the normalised run length non-uniformity feature.
Run percentage
This feature measures the fraction of the number of realised runs and the maximum number of
potential runs30. Strongly linear or highly uniform ROI volumes produce a low run percentage. It
is deﬁned as:
Frlm.r.perc = Ns
As noted before, when this feature is calculated using a merged GLRLM, Nv should be the sum
of the number of voxels of the underlying matrices to allow proper normalisation.
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.118 — Reference values for the run percentage feature.
Grey level variance
This feature estimates the variance in runs over the grey levels. Let pij = rij/Ns be the joint
probability estimate for ﬁnding discretised grey level i with run length j. Grey level variance is
CHAPTER 3. IMAGE FEATURES
then deﬁned as:
Frlm.gl.var =
(i −µ)2pij
Here, µ = PNg
j=1 i pij.
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.119 — Reference values for the grey level variance feature.
Run length variance
This feature estimates the variance in runs over the run lengths. As before let pij = rij/Ns. The
feature is deﬁned as:
Frlm.rl.var =
(j −µ)2pij
Mean run length is deﬁned as µ = PNg
j=1 j pij.
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
CHAPTER 3. IMAGE FEATURES
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
3D, averaged
3D, merged
Table 3.120 — Reference values for the run length variance feature.
Run entropy
Run entropy was investigated by Albregtsen et al. 3. Again, let pij = rij/Ns. The entropy is then
deﬁned as:
Frlm.rl.entr = −
pij log2 pij
aggr. method
dig. phantom
2D, averaged
very strong
dig. phantom
2D, slice-merged
dig. phantom
2.5D, direction-merged
dig. phantom
2.5D, merged
dig. phantom
3D, averaged
very strong
dig. phantom
3D, merged
very strong
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
2D, averaged
2D, slice-merged
2.5D, direction-merged
2.5D, merged
3D, averaged
3D, merged
very strong
3D, averaged
3D, merged
CHAPTER 3. IMAGE FEATURES
3D, averaged
3D, merged
Table 3.121 — Reference values for the run entropy feature.
CHAPTER 3. IMAGE FEATURES
Grey level size zone based features
The grey level size zone matrix (GLSZM) counts the number of groups (or zones) of linked voxels71.
Voxels are linked if the neighbouring voxel has an identical discretised grey level. Whether a voxel
classiﬁes as a neighbour depends on its connectedness. In a 3D approach to texture analysis we
consider 26-connectedness, which indicates that a center voxel is linked to all of the 26 neighbouring
voxels with the same grey level.
In the 2 dimensional approach, 8-connectedness is used.
potential issue for the 2D approach is that voxels which may otherwise be considered to belong
to the same zone by linking across slices, are now two or more separate zones within the slice
plane. Whether this issue negatively aﬀects predictive performance of GLSZM-based features or
their reproducibility has not been determined.
Let M be the Ng × Nz grey level size zone matrix, where Ng is the number of discretised grey
levels present in the ROI intensity mask and Nz the maximum zone size of any group of linked
voxels. Element sij of M is then the number of zones with discretised grey level i and size j.
Furthermore, let Nv be the number of voxels in the intensity mask and Ns = PNg
j=1 sij be the
total number of zones. Marginal sums can likewise be deﬁned. Let si. = PNz
j=1 sij be the number
of zones with discretised grey level i, regardless of size. Likewise, let s.j = PNg
i=1 sij be the number
of zones with size j, regardless of grey level. A two dimensional example is shown in Table 3.122.
Aggregating features
Three methods can be used to aggregate GLSZMs and arrive at a single feature value.
schematic example is shown in Figure 3.4. A feature may be aggregated as follows:
1. Features are computed from 2D matrices and averaged over slices (8QNN).
2. The feature is computed from a single matrix after merging all 2D matrices (62GR).
3. The feature is computed from a 3D matrix (KOBO).
Method 2 involves merging GLSZMs by summing the number of zones sij over the GLSZM for
the diﬀerent slices. Note that when matrices are merged, Nv should likewise be summed to retain
consistency. Feature values may dependent strongly on the aggregation method.
The default neighbourhood for GLSZM is constructed using Chebyshev distance δ = 1. Manhattan or Euclidean norms may also be used to construct a neighbourhood, and both lead to
a 6-connected (3D) and 4-connected (2D) neighbourhoods. Larger distances are also technically
possible, but will occasionally cause separate zones with the same intensity to be considered as
belonging to the same zone. Using diﬀerent neighbourhoods for determining voxel linkage is nonstandard use, and we caution against it due to potential reproducibility issues.
Note on feature references
GLSZM feature deﬁnitions are based on the deﬁnitions of GLRLM features71. Hence, references
may be found in the section on GLRLM (3.7).
CHAPTER 3. IMAGE FEATURES
(a) Grey levels
Zone size j
(b) Grey level size zone matrix
Table 3.122 — Original image with grey levels (a); and corresponding grey level size zone matrix
(GLSZM) under 8-connectedness (b). Element s(i, j) of the GLSZM indicates the number of times a
zone of j linked pixels and grey level i occurs within the image.
(a) 2D: by slice, without merging
(b) 2.5D: by slice, with merging
(c) 3D: as volume
Figure 3.4 — Approaches to calculating grey level size zone matrix-based features. Mk are texture
matrices calculated for slice k (if applicable), and fk is the corresponding feature value. In (b) the
matrices from the diﬀerent slices are merged prior to feature calculation.
Small zone emphasis
This feature emphasises small zones. It is deﬁned as:
Fszm.sze = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
CHAPTER 3. IMAGE FEATURES
Table 3.123 — Reference values for the small zone emphasis feature.
Large zone emphasis
This feature emphasises large zones. It is deﬁned as:
Fszm.lze = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
3.89 × 104
9.91 × 104
5.86 × 104
Table 3.124 — Reference values for the large zone emphasis feature.
Low grey level zone emphasis
This feature is a grey level analogue to small zone emphasis. Instead of small zone sizes, low grey
levels are emphasised. The feature is deﬁned as:
Fszm.lgze = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
CHAPTER 3. IMAGE FEATURES
Table 3.125 — Reference values for the low grey level emphasis feature.
High grey level zone emphasis
The high grey level zone emphasis feature is a grey level analogue to large zone emphasis. The
feature emphasises high grey levels, and is deﬁned as:
Fszm.hgze = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.126 — Reference values for the high grey level emphasis feature.
Small zone low grey level emphasis
This feature emphasises zone counts within the upper left quadrant of the GLSZM, where small
zone sizes and low grey levels are located. It is deﬁned as:
Fszm.szlge = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
CHAPTER 3. IMAGE FEATURES
Table 3.127 — Reference values for the small zone low grey level emphasis feature.
Small zone high grey level emphasis
This feature emphasises zone counts in the lower left quadrant of the GLSZM, where small zone
sizes and high grey levels are located. The feature is deﬁned as:
Fszm.szhge = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.128 — Reference values for the small zone high grey level emphasis feature.
Large zone low grey level emphasis
This feature emphasises zone counts in the upper right quadrant of the GLSZM, where large zone
sizes and low grey levels are located. The feature is deﬁned as:
Fszm.lzlge = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
CHAPTER 3. IMAGE FEATURES
Table 3.129 — Reference values for the large zone low grey level emphasis feature.
Large zone high grey level emphasis
This feature emphasises zone counts in the lower right quadrant of the GLSZM, where large zone
sizes and high grey levels are located. The feature is deﬁned as:
Fszm.lzhge = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
1.49 × 103
very strong
3.16 × 105
3.38 × 105
1.81 × 105
1.81 × 105
7.07 × 107
4.14 × 107
3.36 × 107
Table 3.130 — Reference values for the large zone high grey level emphasis feature.
Grey level non-uniformity
This feature assesses the distribution of zone counts over the grey values. The feature value is low
when zone counts are equally distributed along grey levels. The feature is deﬁned as:
Fszm.glnu = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
very strong
CHAPTER 3. IMAGE FEATURES
Table 3.131 — Reference values for the grey level non-uniformity feature.
Normalised grey level non-uniformity
This is a normalised version of the grey level non-uniformity feature. It is deﬁned as:
Fszm.glnu.norm =
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.132 — Reference values for the normalised grey level non-uniformity feature.
Zone size non-uniformity
This features assesses the distribution of zone counts over the diﬀerent zone sizes.
non-uniformity is low when zone counts are equally distributed along zone sizes. It is deﬁned as:
Fszm.zsnu = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
1.24 × 104
3.63 × 103
3.04 × 103
1.63 × 103
2.37 × 103
Table 3.133 — Reference values for the zone size non-uniformity feature.
CHAPTER 3. IMAGE FEATURES
Normalised zone size non-uniformity
This is a normalised version of zone size non-uniformity. It is deﬁned as:
Fszm.zsnu.norm =
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.134 — Reference values for the normalised zone size non-uniformity feature.
Zone percentage
This feature measures the fraction of the number of realised zones and the maximum number of
potential zones. Highly uniform ROIs produce a low zone percentage. It is deﬁned as:
Fszm.z.perc = Ns
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
very strong
Table 3.135 — Reference values for the zone percentage feature.
CHAPTER 3. IMAGE FEATURES
Grey level variance
This feature estimates the variance in zone counts over the grey levels. Let pij = sij/Ns be the
joint probability estimate for ﬁnding zones with discretised grey level i and size j. The feature is
then deﬁned as:
Fszm.gl.var =
(i −µ)2pij
Here, µ = PNg
j=1 i pij.
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.136 — Reference values for the grey level variance feature.
Zone size variance
This feature estimates the variance in zone counts over the diﬀerent zone sizes.
As before let
pij = sij/Ns. The feature is deﬁned as:
Fszm.zs.var =
(j −µ)2pij
Mean zone size is deﬁned as µ = PNg
j=1 j pij.
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
3.89 × 104
5.85 × 104
CHAPTER 3. IMAGE FEATURES
Table 3.137 — Reference values for the zone size variance feature.
Zone size entropy
Let pij = sij/Ns. Zone size entropy is then deﬁned as:
Fszm.zs.entr = −
pij log2 pij
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.138 — Reference values for the zone size entropy feature.
CHAPTER 3. IMAGE FEATURES
Grey level distance zone based features
The grey level distance zone matrix (GLDZM) counts the number of groups (or zones) of linked
voxels which share a speciﬁc discretised grey level value and possess the same distance to ROI
edge71. The GLDZM thus captures the relation between location and grey level. Two maps are
required to calculate the GLDZM. The ﬁrst is a grey level zone map, which is identical to the one
created for the grey level size zone matrix (GLSZM), see Section 3.8. The second is a distance
map, which will be described in detail later.
As with GSLZM, neighbouring voxels are linked if they share the same grey level value. Whether
a voxel classiﬁes as a neighbour depends on its connectedness. We consider 26-connectedness for
a 3D approach and 8-connectedness in the 2D approach.
The distance to the ROI edge is deﬁned according to 6 and 4-connectedness for 3D and 2D,
respectively. Because of the connectedness deﬁnition used, the distance of a voxel to the outer
border is equal to the minimum number edges of neighbouring voxels that need to be crossed to
reach the ROI edge. The distance for a linked group of voxels with the same grey value is equal
to the minimum distance for the respective voxels in the distance map.
Our deﬁnition deviates from the original by Thibault et al. 71. The original was deﬁned in a
rectangular 2D image, whereas ROIs are rarely rectangular cuboids. Approximating distance using
Chamfer maps is then no longer a fast and easy solution. Determining distance iteratively in 6 or
4-connectedness is a relatively eﬃcient solution, implemented as follows:
1. The ROI mask is morphologically eroded using the appropriate (6 or 4-connected) structure
2. All eroded ROI voxels are updated in the distance map by adding 1.
3. The above steps are performed iteratively until the ROI mask is empty.
A second diﬀerence with the original deﬁnition is that the lowest possible distance is 1 instead of
0 for voxels directly on the ROI edge. This prevents division by 0 for some features.
Let M be the Ng × Nd grey level size zone matrix, where Ng is the number of discretised
grey levels present in the ROI intensity mask and Nd the largest distance of any zone. Element
dij = d(i, j) of M is then number of zones with discretised grey level i and distance j. Furthermore,
let Nv be the number of voxels and Ns = PNg
j=1 dij be the total zone count. Marginal sums
can likewise be deﬁned. Let di. = PNd
j=1 dij be the number of zones with discretised grey level
i, regardless of distance. Likewise, let d.j = PNg
i=1 dij be the number of zones with distance j,
regardless of grey level. A two dimensional example is shown in Table 3.139.
Morphological and intensity masks.
The GLDZM is special in that it uses both ROI masks. The distance map is determined using
the morphological ROI mask, whereas the intensity mask is used for determining the zones, as
with the GLSZM.
Aggregating features
Three methods can be used to aggregate GLDZMs and arrive at a single feature value. A
schematic example was previously shown in Figure 3.4. A feature may be aggregated as follows:
1. Features are computed from 2D matrices and averaged over slices (8QNN).
2. The feature is computed from a single matrix after merging all 2D matrices (62GR).
CHAPTER 3. IMAGE FEATURES
(a) Grey levels
(c) Grey level
Table 3.139 — Original image with grey levels (a); corresponding distance map for distance to border
(b); and corresponding grey level distance zone matrix (GLDZM) under 4-connectedness (c). Element
d(i, j) of the GLDZM indicates the number of times a zone with grey level i and a minimum distance
to border j occurs within the image.
3. The feature is computed from a 3D matrix (KOBO).
Method 2 involves merging GLDZMs by summing the number of zones dij over the GLDZM for
the diﬀerent slices. Note that when matrices are merged, Nv should likewise be summed to retain
consistency. Feature values may dependent strongly on the aggregation method.
In addition to the use of diﬀerent distance norms to determine voxel linkage, as described in
section 3.8, diﬀerent distance norms may be used to determine distance of zones to the boundary.
The default is to use the Manhattan norm which allows for a computationally eﬃcient implementation, as described above. A similar implementation is possible using the Chebyshev norm, as
it merely changes connectedness of the structure element. Implementations using an Euclidean
distance norm are less eﬃcient as this demands searching for the nearest non-ROI voxel for each
of the Nv voxels in the ROI. An added issue is that Euclidean norms may lead to a wide range of
diﬀerent distances j that require rounding before constructing the grey level distance zone matrix
M. Using diﬀerent distance norms is non-standard use, and we caution against it due to potential
reproducibility issues.
Note on feature references
GLDZM feature deﬁnitions are based on the deﬁnitions of GLRLM features71. Hence, references
may be found in the section on GLRLM (3.7).
Small distance emphasis
This feature emphasises small distances. It is deﬁned as:
Fdzm.sde = 1
aggr. method
dig. phantom
dig. phantom
CHAPTER 3. IMAGE FEATURES
dig. phantom
very strong
Table 3.140 — Reference values for the small distance emphasis feature.
Large distance emphasis
This feature emphasises large distances. It is deﬁned as:
Fdzm.lde = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.141 — Reference values for the large distance emphasis feature.
Low grey level zone emphasis
This feature is a grey level analogue to small distance emphasis. Instead of small zone distances,
low grey levels are emphasised. The feature is deﬁned as:
Fdzm.lgze = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
CHAPTER 3. IMAGE FEATURES
Table 3.142 — Reference values for the low grey level emphasis feature.
High grey level zone emphasis
The high grey level zone emphasis feature is a grey level analogue to large distance emphasis. The
feature emphasises high grey levels, and is deﬁned as:
Fdzm.hgze = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.143 — Reference values for the high grey level emphasis feature.
Small distance low grey level emphasis
This feature emphasises runs in the upper left quadrant of the GLDZM, where small zone distances
and low grey levels are located. It is deﬁned as:
Fdzm.sdlge = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
CHAPTER 3. IMAGE FEATURES
Table 3.144 — Reference values for the small distance low grey level emphasis feature.
Small distance high grey level emphasis
This feature emphasises runs in the lower left quadrant of the GLDZM, where small zone distances
and high grey levels are located. Small distance high grey level emphasis is deﬁned as:
Fdzm.sdhge = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.145 — Reference values for the small distance high grey level emphasis feature.
Large distance low grey level emphasis
This feature emphasises runs in the upper right quadrant of the GLDZM, where large zone distances
and low grey levels are located. The feature is deﬁned as:
Fdzm.ldlge = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
CHAPTER 3. IMAGE FEATURES
Table 3.146 — Reference values for the large distance low grey level emphasis feature.
Large distance high grey level emphasis
This feature emphasises runs in the lower right quadrant of the GLDZM, where large zone distances
and high grey levels are located. The large distance high grey level emphasis feature is deﬁned as:
Fdzm.ldhge = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
7.01 × 104
7.95 × 104
1.06 × 104
1.16 × 104
1.34 × 104
2.62 × 103
4.85 × 103
Table 3.147 — Reference values for the large distance high grey level emphasis feature.
Grey level non-uniformity
This feature measures the distribution of zone counts over the grey values.
Grey level nonuniformity is low when zone counts are equally distributed along grey levels.
The feature is
deﬁned as:
Fdzm.glnu = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
CHAPTER 3. IMAGE FEATURES
Table 3.148 — Reference values for the grey level non-uniformity feature.
Normalised grey level non-uniformity
This is a normalised version of the grey level non-uniformity feature. It is deﬁned as:
Fdzm.glnu.norm =
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.149 — Reference values for the normalised grey level non-uniformity feature.
Zone distance non-uniformity
Zone distance non-uniformity measures the distribution of zone counts over the diﬀerent zone
distances. Zone distance non-uniformity is low when zone counts are equally distributed along
zone distances. It is deﬁned as:
Fdzm.zdnu = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
CHAPTER 3. IMAGE FEATURES
1.57 × 103
1.87 × 103
1.37 × 103
Table 3.150 — Reference values for the zone distance non-uniformity feature.
Normalised zone distance non-uniformity
This is a normalised version of the zone distance non-uniformity feature. It is deﬁned as:
Fdzm.zdnu.norm =
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.151 — Reference values for the normalised zone distance non-uniformity feature.
Zone percentage
This feature measures the fraction of the number of realised zones and the maximum number of
potential zones. Highly uniform ROIs produce a low zone percentage. It is deﬁned as:
Fdzm.z.perc = Ns
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
CHAPTER 3. IMAGE FEATURES
Table 3.152 — Reference values for the zone percentage feature.
Grey level variance
This feature estimates the variance in zone counts over the grey levels. Let pij = dij/Ns be the
joint probability estimate for ﬁnding zones with discretised grey level i at distance j. The feature
is then deﬁned as:
Fdzm.gl.var =
(i −µ)2pij
Here, µ = PNg
j=1 i pij.
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.153 — Reference values for the grey level variance feature.
Zone distance variance
This feature estimates the variance in zone counts for the diﬀerent zone distances. As before let
pij = dij/Ns. The feature is deﬁned as:
Fdzm.zd.var =
(j −µ)2pij
Mean zone size is deﬁned as µ = PNg
j=1 j pij.
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
CHAPTER 3. IMAGE FEATURES
Table 3.154 — Reference values for the zone distance variance feature.
Zone distance entropy
Again, let pij = dij/Ns. Zone distance entropy is then deﬁned as:
Fdzm.zd.entr = −
pij log2 pij
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.155 — Reference values for the zone distance entropy feature.
CHAPTER 3. IMAGE FEATURES
Neighbourhood grey tone diﬀerence based featuresIPET
Amadasun and King 5 introduced an alternative to the grey level co-occurrence matrix.
neighbourhood grey tone diﬀerence matrix (NGTDM) contains the sum of grey level diﬀerences
of pixels/voxels with discretised grey level i and the average discretised grey level of neighbouring pixels/voxels within a Chebyshev distance δ.
For 3D volumes, we can extend the original
deﬁnition by Amadasun and King. Let Xd,k be the discretised grey level of a voxel at position
k = (kx, ky, kz). Then the average grey level within a neighbourhood centred at (kx, ky, kz), but
excluding (kx, ky, kz) itself is:
Xd(kx+mx, ky+my, kz+mz)
(mx, my, mz) ̸= (0, 0, 0)
W = (2δ + 1)3 −1 is the size of the 3D neighbourhood. For 2D W = (2δ + 1)2 −1, and averages
are not calculated between diﬀerent slices. Neighbourhood grey tone diﬀerence si for discretised
grey level i is then:
|i −Xk| [Xd(k) = i and k has a valid neighbourhood]
Here, [. . .] is an Iverson bracket, which is 1 if the conditions that the grey level Xd,k of voxel k
is equal to i and the voxel has a valid neighbourhood are both true; it is 0 otherwise. Nv is the
number of voxels in the ROI intensity mask.
A 2D example is shown in Table 3.156. A distance of δ = 1 is used in this example, leading
to 8 neighbouring pixels. Entry s1 = 0 because there are no valid pixels with grey level 1. Two
pixels have grey level 2. The average value of their neighbours are 19/8 and 21/8. Thus s2 =
|2 −19/8| + |2 −21/8| = 1. Similarly s3 = |3 −19/8| = 0.625 and s4 = |4 −17/8| = 1.825.
We deviate from the original deﬁnition by Amadasun and King 5 as we do not demand that valid
neighbourhoods are completely inside the ROI. In an irregular ROI mask, valid neighbourhoods
may simply not exist for a distance δ. Instead, we consider a valid neighbourhood to exist if there
is at least one neighbouring voxel included in the ROI mask. The average grey level for voxel k
within a valid neighbourhood is then:
Xd(k + m)[m ̸= 0 and k + m in ROI]
The neighbourhood size Wk for this voxel is equal to the number of voxels in the neighbourhood
that are part of the ROI mask:
[m ̸= 0 and k + m in ROI]
Under our deﬁnition, neighbourhood grey tone diﬀerence si for discretised grey level i can be
directly expressed using neighbourhood size Wk of voxel k:
|i −Xk| [Xd(k) = i and Wk ̸= 0]
Consequentially, ni is the total number of voxels with grey level i which have a non-zero neigh-
CHAPTER 3. IMAGE FEATURES
(a) Grey levels
Neighbourhood
tone diﬀerence matrix
Table 3.156 — Original image with grey levels (a) and corresponding neighbourhood grey tone difference matrix (NGTDM) (b). The Nv,c pixels with valid neighbours at distance 1 are located within
the rectangle in (a). The grey level voxel count ni, the grey level probability pi = ni/Nv,c, and the
neighbourhood grey level diﬀerence si for pixels with grey level i are included in the NGTDM. Note
that our actual deﬁnition deviates from the original deﬁnition of Amadasun and King 5, which is used
here. In our deﬁnition complete neighbourhood are no longer required. In our deﬁnition the NGTDM
would be calculated on the entire pixel area, and not solely on those pixels within the rectangle of panel
bourhood size.
Many NGTDM-based features depend on the Ng grey level probabilities pi = ni/Nv,c, where Ng
is the number of discretised grey levels in the ROI intensity mask and Nv,c = P ni is total number
of voxels that have at least one neighbour. If all voxels have at least one neighbour Nv,c = Nv.
Furthermore, let Ng,p ≤Ng be the number of discretised grey levels with pi > 0. In the above
example, Ng = 4 and Ng,p = 3.
Aggregating features
Three methods can be used to aggregate NGTDMs and arrive at a single feature value. A
schematic example was previously shown in Figure 3.4. A feature may be aggregated as follows:
1. Features are computed from 2D matrices and averaged over slices (8QNN).
2. The feature is computed from a single matrix after merging all 2D matrices (62GR).
3. The feature is computed from a 3D matrix (KOBO).
Method 2 involves merging NGTDMs by summing the neighbourhood grey tone diﬀerence si and
the number of voxels with a valid neighbourhood ni and grey level i for NGTDMs of the diﬀerent
slices. Note that when NGTDMs are merged, Nv,c and pi should be updated based on the merged
NGTDM. Feature values may dependent strongly on the aggregation method.
Distances and distance weighting
The default neighbourhood is deﬁned using the Chebyshev norm. Manhattan or Euclidean
norms may be used as well. This requires a more general deﬁnition for the average grey level Xk:
Xd(k + m)[∥m∥≤δ and m ̸= 0 and k + m in ROI]
The neighbourhood size Wk is:
[∥m∥≤δ and m ̸= 0 and k + m in ROI]
CHAPTER 3. IMAGE FEATURES
As before, [. . .] is an Iverson bracket.
Distance weighting for NGTDM is relatively straightforward. Let w be a weight dependent on
m, e.g. w = ∥m∥−1 or w = exp(−∥m∥2). The average grey level is then:
w(m)Xd(k + m)[∥m∥≤δ and m ̸= 0 and k + m in ROI]
The neighbourhood size Wk becomes a general weight:
w(m)[∥m∥≤δ and m ̸= 0 and k + m in ROI]
Employing diﬀerent distance norms and distance weighting is considered non-standard use, and
we caution against them due to potential reproducibility issues.
Coarseness
Grey level diﬀerences in coarse textures are generally small due to large-scale patterns. Summing
diﬀerences gives an indication of the level of the spatial rate of change in intensity5. Coarseness
is deﬁned as:
Fngt.coarseness =
Because PNg
i=1 pi si potentially evaluates to 0, the maximum coarseness value is set to an arbitrary
number of 106. Amadasun and King originally circumvented this issue by adding a unspeciﬁed
small number ϵ to the denominator, but an explicit, though arbitrary, maximum value should allow
for more consistency.
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
9.06 × 10−5
3.3 × 10−6
Table 3.157 — Reference values for the coarseness feature.
CHAPTER 3. IMAGE FEATURES
Contrast depends on the dynamic range of the grey levels as well as the spatial frequency of
intensity changes5. Thus, contrast is deﬁned as:
Fngt.contrast =
Ng,p (Ng,p −1)
pi1pi2 (i1 −i2)2
Grey level probabilities pi1 and pi2 are copies of pi with diﬀerent iterators, i.e. pi1 = pi2 for
i1 = i2. The ﬁrst term considers the grey level dynamic range, whereas the second term is a
measure for intensity changes within the volume. If Ng,p = 1, Fngt.contrast = 0.
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.158 — Reference values for the contrast feature.
Textures with large changes in grey levels between neighbouring voxels are said to be busy5.
Busyness was deﬁned as:
Fngt.busyness =
i2=1 i1 pi1 −i2 pi2
pi1 ̸= 0 and pi2 ̸= 0
As before, pi1 = pi2 for i1 = i2. The original deﬁnition was erroneously formulated as the denominator will always evaluate to 0. Therefore we use a slightly diﬀerent deﬁnition38:
Fngt.busyness =
i2=1 |i1 pi1 −i2 pi2|
pi1 ̸= 0 and pi2 ̸= 0
If Ng,p = 1, Fngt.busyness = 0.
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
CHAPTER 3. IMAGE FEATURES
very strong
Table 3.159 — Reference values for the busyness feature.
Complexity
Complex textures are non-uniform and rapid changes in grey levels are common5. Texture complexity is deﬁned as:
Fntg.complexity =
|i1 −i2| pi1 si1 + pi2 si2
pi1 ̸= 0 and pi2 ̸= 0
As before, pi1 = pi2 for i1 = i2, and likewise si1 = si2 for i1 = i2.
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
1.81 × 103
very strong
Table 3.160 — Reference values for the complexity feature.
Amadasun and King 5 deﬁned texture strength as:
Fngt.strength =
i2=1 (pi1 + pi2) (i1 −i2)2
pi1 ̸= 0 and pi2 ̸= 0
As before, pi1 = pi2 for i1 = i2. If PNg
i=1 si = 0, Fngt.strength = 0.
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
CHAPTER 3. IMAGE FEATURES
Table 3.161 — Reference values for the strength feature.
CHAPTER 3. IMAGE FEATURES
Neighbouring grey level dependence based featuresREK0
Sun and Wee 69 deﬁned the neighbouring grey level dependence matrix (NGLDM) as an alternative
to the grey level co-occurrence matrix. The NGLDM aims to capture the coarseness of the overall
texture and is rotationally invariant.
NGLDM also involves the concept of a neighbourhood around a central voxel.
All voxels
within Chebyshev distance δ are considered to belong to the neighbourhood of the center voxel.
The discretised grey levels of the center voxel k at position k and a neighbouring voxel m at
k + m are said to be dependent if |Xd(k) −Xd(k + m)| ≤α, with α being a non-negative integer
coarseness parameter. The number of grey level dependent voxels j within the neighbourhood is
then counted as:
[|Xd(k) −Xd(k + m)| ≤α and m ̸= 0]
Here, [. . .] is an Iverson bracket, which is 1 if the aforementioned condition is fulﬁlled, and 0
otherwise. Note that the minimum dependence jk = 1 and not jk = 0. This is done because
some feature deﬁnitions require a minimum dependence of 1 or are undeﬁned otherwise. One may
therefore also simplify the expression for jk by including the center voxel:
[|Xd(k) −Xd(k + m)| ≤α]
Dependence jk is iteratively determined for each voxel k in the ROI intensity mask. M is then
the Ng×Nn neighbouring grey level dependence matrix, where Ng is the number of discretised grey
levels present in the ROI intensity mask and Nn = max(jk) the maximum grey level dependence
count found. Element sij of M is then the number of neighbourhoods with a center voxel with
discretised grey level i and a neighbouring voxel dependence j. Furthermore, let Nv be the number
of voxels in the ROI intensity mask, and Ns = PNg
j=1 sij the number of neighbourhoods.
Marginal sums can likewise be deﬁned. Let si. = PNn
j=1 be the number of neighbourhoods with
discretised grey level i, and let sj. = PNg
i=1 sij be the number of neighbourhoods with dependence
j, regardless of grey level. A two dimensional example is shown in Table 3.162.
The deﬁnition we actually use deviates from the original by Sun and Wee 69. Because regions
of interest are rarely cuboid, omission of neighbourhoods which contain voxels outside the ROI
mask may lead to inconsistent results, especially for larger distance δ. Hence the neighbourhoods
of all voxels in the within the ROI intensity mask are considered, and consequently Nv = Ns.
Neighbourhood voxels located outside the ROI do not add to dependence j:
[|Xd(k) −Xd(k + m)| ≤α and k + m in ROI]
Note that while α = 0 is a typical choice for the coarseness parameter, diﬀerent α are possible.
Likewise, a typical choice for neighbourhood radius δ is Chebyshev distance δ = 1 but larger values
are possible as well.
Aggregating features
Three methods can be used to aggregate NGLDMs and arrive at a single feature value. A
schematic example was previously shown in Figure 3.4. A feature may be aggregated as follows:
CHAPTER 3. IMAGE FEATURES
(a) Grey levels
dependence k
(b) Neighbouring grey
level dependence matrix
Table 3.162 — Original image with grey levels and pixels with a complete neighbourhood within the
square (a); corresponding neighbouring grey level dependence matrix for distance d =
2 and coarseness
parameter a = 0 (b). Element s(i, j) of the NGLDM indicates the number of neighbourhoods with a
center pixel with grey level i and neighbouring grey level dependence k within the image. Note that
in our deﬁnition a complete neighbourhood is no longer required. Thus every voxel is considered as
a center voxel with a neighbourhood, instead of being constrained to the voxels within the square in
panel (a).
1. Features are computed from 2D matrices and averaged over slices (8QNN).
2. The feature is computed from a single matrix after merging all 2D matrices (62GR).
3. The feature is computed from a 3D matrix (KOBO).
Method 2 involves merging NGLDMs by summing the dependence count sij by element over the
NGLDM of the diﬀerent slices. Note that when NGLDMs are merged, Nv and Ns should likewise be
summed to retain consistency. Feature values may dependent strongly on the aggregation method.
Distances and distance weighting
Default neighbourhoods are constructed using the Chebyshev norm, but other norms can be
used as well. For this purpose it is useful to generalise the dependence count equation to:
[∥m∥≤δ and |Xd(k) −Xd(k + m)| ≤α and k + m in ROI]
with m the vector between voxels k and m and ∥m∥its length according to the particular norm.
In addition, dependence may be weighted by distance. Let w be a weight dependent on m, e.g.
w = ∥m∥−1 or w = exp(−∥m∥2). The dependence of voxel k is then:
w(m)[∥m∥≤δ and |Xd(k) −Xd(k + m)| ≤α and k + m in ROI]
Employing diﬀerent distance norms and distance weighting is considered non-standard use, and
we caution against them due to potential reproducibility issues.
Note on feature references
The NGLDM is structured similarly to the GLRLM, GLSZM and GLDZM. NGLDM feature
deﬁnitions are therefore based on the deﬁnitions of GLRLM features, and references may be found
in Section 3.7, except for the features originally deﬁned by Sun and Wee 69.
CHAPTER 3. IMAGE FEATURES
Low dependence emphasis
This feature emphasises low neighbouring grey level dependence counts. Sun and Wee 69 refer to
this feature as small number emphasis. It is deﬁned as:
Fngl.lde = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
very strong
Table 3.163 — Reference values for the low dependence emphasis feature.
High dependence emphasis
This feature emphasises high neighbouring grey level dependence counts. Sun and Wee 69 refer to
this feature as large number emphasis. It is deﬁned as:
Fngl.hde = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.164 — Reference values for the high dependence emphasis feature.
CHAPTER 3. IMAGE FEATURES
Low grey level count emphasis
This feature is a grey level analogue to low dependence emphasis. Instead of low neighbouring grey
level dependence counts, low grey levels are emphasised. The feature is deﬁned as:
Fngl.lgce = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.165 — Reference values for the low grey level count emphasis feature.
High grey level count emphasis
The high grey level count emphasis feature is a grey level analogue to high dependence emphasis.
The feature emphasises high grey levels, and is deﬁned as:
Fngl.hgce = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
1.57 × 103
Table 3.166 — Reference values for the high grey level count emphasis feature.
CHAPTER 3. IMAGE FEATURES
Low dependence low grey level emphasis
This feature emphasises neighbouring grey level dependence counts in the upper left quadrant of
the NGLDM, where low dependence counts and low grey levels are located. It is deﬁned as:
Fngl.ldlge = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
1.2 × 10−5
Table 3.167 — Reference values for the low dependence low grey level emphasis feature.
Low dependence high grey level emphasis
This feature emphasises neighbouring grey level dependence counts in the lower left quadrant of
the NGLDM, where low dependence counts and high grey levels are located. The feature is deﬁned
Fngl.ldhge = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.168 — Reference values for the low dependence high grey level emphasis feature.
CHAPTER 3. IMAGE FEATURES
High dependence low grey level emphasis
This feature emphasises neighbouring grey level dependence counts in the upper right quadrant of
the NGLDM, where high dependence counts and low grey levels are located. The feature is deﬁned
Fngl.hdlge = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.169 — Reference values for the high dependence low grey level emphasis feature.
High dependence high grey level emphasis
The high dependence high grey level emphasis feature emphasises neighbouring grey level dependence counts in the lower right quadrant of the NGLDM, where high dependence counts and high
grey levels are located. The feature is deﬁned as:
Fngl.hdhge = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
7.54 × 103
7.21 × 103
7.97 × 103
2.27 × 105
9.28 × 104
Table 3.170 — Reference values for the high dependence high grey level emphasis feature.
CHAPTER 3. IMAGE FEATURES
Grey level non-uniformity
Grey level non-uniformity assesses the distribution of neighbouring grey level dependence counts
over the grey values. The feature value is low when dependence counts are equally distributed
along grey levels. The feature is deﬁned as:
Fngl.glnu = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
1.72 × 104
4.76 × 103
6.42 × 103
1.02 × 104
8.17 × 103
Table 3.171 — Reference values for the grey level non-uniformity feature.
Normalised grey level non-uniformity
This is a normalised version of the grey level non-uniformity feature. It is deﬁned as:
Fngl.glnu.norm =
The normalised grey level non-uniformity computed from a single 3D NGLDM matrix is equivalent
to the intensity histogram uniformity feature81.
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
very strong
Table 3.172 — Reference values for the normalised grey level non-uniformity feature.
CHAPTER 3. IMAGE FEATURES
Dependence count non-uniformity
This features assesses the distribution of neighbouring grey level dependence counts over the diﬀerent dependence counts. The feature value is low when dependence counts are equally distributed.
Sun and Wee 69 refer to this feature as number non-uniformity. It is deﬁned as:
Fngl.dcnu = 1
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
1.75 × 104
3.71 × 103
2.45 × 103
1.84 × 103
2.25 × 103
Table 3.173 — Reference values for the dependence count non-uniformity feature.
Normalised dependence count non-uniformity
This is a normalised version of the dependence count non-uniformity feature. It is deﬁned as:
Fngl.dcnu.norm =
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.174 — Reference values for the normalised dependence count non-uniformity feature.
CHAPTER 3. IMAGE FEATURES
Dependence count percentage
This feature measures the fraction of the number of realised neighbourhoods and the maximum
number of potential neighbourhoods. Dependence count percentage may be completely omitted
as it evaluates to 1 when complete neighbourhoods are not required, as is the case under our
deﬁnition. It is deﬁned as:
Fngl.dc.perc = Ns
aggr. method
dig. phantom
dig. phantom
dig. phantom
Table 3.175 — Reference values for the dependence count percentage feature.
Grey level variance
This feature estimates the variance in dependence counts over the grey levels. Let pij = sij/Ns be
the joint probability estimate for ﬁnding discretised grey level i with dependence j. The feature is
then deﬁned as:
Fngl.gl.var =
(i −µ)2pij
Here, µ = PNg
j=1 i pij.
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
very strong
CHAPTER 3. IMAGE FEATURES
Table 3.176 — Reference values for the grey level variance feature.
Dependence count variance
This feature estimates the variance in dependence counts over the diﬀerent possible dependence
counts. As before let pij = sij/Ns. The feature is deﬁned as:
Fngl.dc.var =
(j −µ)2pij
Mean dependence count is deﬁned as µ = PNg
j=1 j pij.
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.177 — Reference values for the dependence count variance feature.
Dependence count entropy
This feature is referred to as entropy by Sun and Wee 69. Let pij = sij/Ns. Dependence count
entropy is then deﬁned as:
Fngl.dc.entr = −
pij log2 pij
This deﬁnition remedies an error in the deﬁnition of Sun and Wee 69, where the term within the
logarithm is dependence count sij instead of count probability pij.
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
CHAPTER 3. IMAGE FEATURES
very strong
Table 3.178 — Reference values for the dependence count entropy feature.
Dependence count energy
This feature is called second moment by Sun and Wee 69. Let pij = sij/Ns. Then dependence
count energy is deﬁned as:
Fngl.dc.energy =
This deﬁnition also remedies an error in the original deﬁnition, where squared dependence count
ij is divided by Ns only, thus leaving a major volume dependency. In the deﬁnition given here,
ij is normalised by N 2
s through the use of count probability pij.
aggr. method
dig. phantom
dig. phantom
dig. phantom
very strong
Table 3.179 — Reference values for the dependence count energy feature.
Radiomics reporting guidelines
and nomenclature
Reliable and complete reporting is necessary to ensure reproducibility and validation of results. To
help provide a complete report on image processing and image biomarker extraction, we present
the guidelines below, as well as a nomenclature system to uniquely features.
Reporting guidelines
These guidelines are partially based on the work of Lambin et al. 41, Sanduleanu et al. 57, Sollini
et al. 64, Traverso et al. 73. Additionally, guidelines are derived from the image processing and
feature calculation steps described within this document. An earlier version was reported elsewhere79.
description
Region of interest1
Describe the region of interest that is being imaged.
Patient preparation
Describe speciﬁc instructions given to patients
prior to image acquisition, e.g.
fasting prior to
Describe administration of drugs to the patient
prior to image acquisition, e.g. muscle relaxants.
Describe the use of speciﬁc equipment for patient
comfort during scanning, e.g. ear plugs.
Radioactive tracer
PET, SPECT
Describe which radioactive tracer was administered to the patient, e.g. 18F-FDG.
PET, SPECT
Describe the administration method.
PET, SPECT
Describe the injected activity of the radioactive
tracer at administration.
PET, SPECT
Describe the uptake time prior to image acquisition.
continued on next page
1Also referred to as volume of interest.
CHAPTER 4. RADIOMICS REPORTING GUIDELINES AND NOMENCLATURE
description
PET, SPECT
Describe how competing substance levels were
controlled.2
Contrast agent
Describe which contrast agent was administered
to the patient.
Describe the administration method.
Describe the injected quantity of contrast agent.
Describe the uptake time prior to image acquisition.
Describe how competing substance levels were
controlled.
Comorbidities
Describe if the patients have comorbidities that
aﬀect imaging.3
Acquisition4
Acquisition protocol
Describe whether a standard imaging protocol
was used, and where its description may be found.
Scanner type
Describe the scanner type(s) and vendor(s) used
in the study.
Imaging modality
Clearly state the imaging modality that was used
in the study, e.g. CT, MRI.
Static/dynamic scans
State if the scans were static or dynamic.
Dynamic scans
Describe the acquisition time per time frame.
Dynamic scans
Describe any temporal modelling technique that
Scanner calibration
Describe how and when the scanner was calibrated.
Patient instructions
Describe speciﬁc instructions given to the patient
during acquisition, e.g. breath holding.
Anatomical motion correction
Describe the method used to minimise the eﬀect
of anatomical motion.
Scan duration
Describe the duration of the complete scan or the
time per bed position.
Tube voltage
Describe the peak kilo voltage output of the X-ray
Tube current
Describe the tube current in mA.
Time-of-ﬂight
State if scanner time-of-ﬂight capabilities are used
during acquisition.
Describe what kind RF coil used for acquisition,
incl. vendor.
Scanning sequence
Describe which scanning sequence was acquired.
Describe which sequence variant was acquired.
Describe which scan options apply to the current
sequence, e.g. ﬂow compensation, cardiac gating.
continued on next page
2An example is glucose present in the blood which competes with the uptake of 18F-FDG tracer in tumour
To reduce competition with the tracer, patients are usually asked to fast for several hours and a blood
glucose measurement may be conducted prior to tracer administration.
3An example of a comorbidity that may aﬀect image quality in 18F-FDG PET scans are type I and type II
diabetes melitus, as well as kidney failure.
4Many acquisition parameters may be extracted from DICOM header meta-data, or calculated from them.
CHAPTER 4. RADIOMICS REPORTING GUIDELINES AND NOMENCLATURE
description
Repetition time
Describe the time in ms between subsequent pulse
sequences.
Describe the echo time in ms.
Echo train length
Describe the number of lines in k-space that are
acquired per excitation pulse.
Inversion time
Describe the time in ms between the middle of the
inverting RF pulse to the middle of the excitation
Flip angle
Describe the ﬂip angle produced by the RF pulses.
Acquisition type
Describe the acquisition type of the MRI scan, e.g.
k-space traversal
Describe the acquisition trajectory of the k-space.
Number of averages/ excitations
Describe the number of times each point in kspace is sampled.
Magnetic ﬁeld strength
Describe the nominal strength of the MR magnetic ﬁeld.
Reconstruction5
In-plane resolution
Describe the distance between pixels, or alternatively the ﬁeld of view and matrix size.
Image slice thickness
Describe the slice thickness.
Image slice spacing
Describe the distance between image slices.6
Convolution kernel
Describe the convolution kernel used to reconstruct the image.
Describe settings pertaining to iterative reconstruction algorithms.
Describe the exposure (in mAs) in slices containing the region of interest.
Reconstruction method
Describe which reconstruction method was used,
e.g. 3D OSEM.
Describe the number of iterations for iterative reconstruction.
Describe the number of subsets for iterative reconstruction.
Describe if and how point-spread function modelling was performed.
Image corrections
Describe if and how attenuation correction was
performed.
Describe if and how other forms of correction were
performed, e.g. scatter correction, randoms correction, dead time correction etc.
Reconstruction method
Describe the reconstruction method used to reconstruct the image from the k-space information.
Describe any artifact suppression methods used
during reconstruction to suppress artifacts due to
undersampling of k-space.
continued on next page
5Many reconstruction parameters may be extracted from DICOM header meta-data.
6Spacing between image slicing is commonly, but not necessarily, the same as the slice thickness,.
CHAPTER 4. RADIOMICS REPORTING GUIDELINES AND NOMENCLATURE
description
Diﬀusion-weighted imaging
Describe the b-values used for diﬀusion-weighting.
Image registration
Registration method
Describe the method used to register multimodality imaging.
Image processing - data conversion
SUV normalisation
Describe which standardised uptake value (SUV)
normalisation method is used.
ADC computation
Describe how apparent diﬀusion coeﬃcient (ADC)
values were calculated.
Other data conversions
Describe any other conversions that are performed
to generate e.g. perfusion maps.
Image processing - post-acquisition processing
Anti-aliasing
Describe the method used to deal with antialiasing when down-sampling during interpolation.
Noise suppression
Describe methods used to suppress image noise.
Post-reconstruction
smoothing ﬁlter
Describe the width of the Gaussian ﬁlter (FWHM)
to spatially smooth intensities.
Skull stripping
MRI (brain)
Describe method used to perform skull stripping.
Non-uniformity
correction7
Describe the method and settings used to perform
non-uniformity correction.
Intensity normalisation
Describe the method and settings used to normalise intensity distributions within a patient or patient cohort.
post-acquisition
processing methods
Describe any other methods that were used to process the image and are not mentioned separately
in this list.
Segmentation
Segmentation method
Describe how regions of interest were segmented,
e.g. manually.
Describe the number of experts, their expertise
and consensus strategies for manual delineation.
Describe methods and settings used for semiautomatic and fully automatic segmentation.
Describe which image was used to deﬁne segmentation in case of multi-modality imaging.
Conversion to mask
Describe the method used to convert polygonal or
mesh-based segmentations to a voxel-based mask.
Image processing - image interpolation
Interpolation method
Describe which interpolation algorithm was used
to interpolate the image.
Describe how the position of the interpolation grid
was deﬁned, e.g. align by center.
continued on next page
7Also known as bias-ﬁeld correction.
CHAPTER 4. RADIOMICS REPORTING GUIDELINES AND NOMENCLATURE
description
Describe how the dimensions of the interpolation
grid were deﬁned, e.g. rounded to nearest integer.
Describe how extrapolation beyond the original
image was handled.
Voxel dimensions
Describe the size of the interpolated voxels.
Intensity rounding
Describe how fractional Hounsﬁeld Units are
rounded to integer values after interpolation.
Image processing - ROI interpolation
Interpolation method
Describe which interpolation algorithm was used
to interpolate the region of interest mask.
Partially masked voxels
Describe how partially masked voxels after interpolation are handled.
Image processing - re-segmentation
Re-segmentation
Describe which methods and settings are used to
re-segment the ROI intensity mask.
Image processing - discretisation
Discretisation method8
Describe the method used to discretise image intensities.
Describe the number of bins (FBN) or the bin size
(FBS) used for discretisation.
Describe the lowest intensity in the ﬁrst bin for
FBS discretisation.9
Image processing - image transformation
Image ﬁlter10
Describe the methods and settings used to ﬁlter
images, e.g. Laplacian-of-Gaussian.
Image biomarker computation
Biomarker set
Describe which set of image biomarkers is computed and refer to their deﬁnitions or provide
IBSI compliance
State if the software used to extract the set of
image biomarkers is able to reproduce the IBSI
feature reference values.11
Robustness
Describe how robustness of the image biomarkers
was assessed, e.g. test-retest analysis.
Software availability
Describe which software and version was used to
compute image biomarkers.
Image biomarker computation - texture parameters
Texture matrix aggregation
Deﬁne how texture-matrix based biomarkers were
computed from underlying texture matrices.
continued on next page
8Discretisation may be performed separately to create intensity-volume histograms. If this is indeed the case,
this should be described as well.
9This is typically set by range re-segmentation.
10The IBSI has not introduced image transformation into the standardised image processing scheme, and is in
the process of benchmarking various common ﬁlters. This section may therefore be expanded in the future.
11A software is compliant if and only if it is able to reproduce image biomarker reference values for the digital
phantom and for one or more image processing conﬁgurations using the radiomics CT phantom. Reviewers may
demand that you provide the IBSI compliance spreadsheet for your software.
CHAPTER 4. RADIOMICS REPORTING GUIDELINES AND NOMENCLATURE
description
Distance weighting
Deﬁne how CM, RLM, NGTDM and NGLDM
weight distances, e.g. no weighting.
CM symmetry
Deﬁne whether symmetric or asymmetric cooccurrence matrices were computed.
CM distance
Deﬁne the (Chebyshev) distance at which cooccurrence of intensities is determined, e.g. 1.
SZM linkage distance
Deﬁne the distance and distance norm for which
voxels with the same intensity are considered to
belong to the same zone for the purpose of constructing an SZM, e.g. Chebyshev distance of 1.
DZM linkage distance
Deﬁne the distance and distance norm for which
voxels with the same intensity are considered to
belong to the same zone for the purpose of constructing a DZM, e.g. Chebyshev distance of 1.
DZM zone distance norm
Deﬁne the distance norm for determining the distance of zones to the border of the ROI, e.g. Manhattan distance.
NGTDM distance
Deﬁne the neighbourhood distance and distance
norm for the NGTDM, e.g. Chebyshev distance
NGLDM distance
Deﬁne the neighbourhood distance and distance
norm for the NGLDM, e.g. Chebyshev distance
NGLDM coarseness
Deﬁne the coarseness parameter for the NGLDM,
Machine learning and radiomics analysis
Diagnostic
prognostic modelling
See the TRIPOD guidelines for reporting on diagnostic and prognostic modelling.
Comparison with known
Describe where performance of radiomics models
is compared with known (clinical) factors.
Multicollinearity
Describe where the multicollinearity between image biomarkers in the signature is assessed.
Model availability
Describe where radiomics models with the necessary pre-processing information may be found.
Data availability
Describe where imaging data and relevant metadata used in the study may be found.
Table 4.1 — Guidelines for reporting on radiomic studies. Not all items may be applicable.
Feature nomenclature
Image features may be extracted using a variety of diﬀerent settings, and may even share the same
name. A feature nomenclature is thus required. Let us take the example of diﬀerentiating the following features: i) intensity histogram-based entropy, discretised using a ﬁxed bin size algorithm
with 25 HU bins, extracted from a CT image; and ii) grey level run length matrix entropy, discretised using a ﬁxed bin number algorithm with 32 bins, extracted from a PET image. To refer to
both as entropy would be ambiguous, whereas to add a full textual description would be cumbersome. In the nomenclature proposed below, the features would be called entropyIH, CT, FBS:25HU
CHAPTER 4. RADIOMICS REPORTING GUIDELINES AND NOMENCLATURE
and entropyRLM, PET, FBN:32, respectively.
Features are thus indicated by a feature name and a subscript. As the nomenclature is designed
to both concise and complete, only details for which ambiguity may exist are to be explicitly
incorporated in the subscript. The subscript of a feature name may contain the following items to
address ambiguous naming:
1. An abbreviation of the feature family (required).
2. The aggregation method of a feature (optional).
3. A descriptor describing the modality the feature is based on, the speciﬁc channel (for microscopy images), the speciﬁc imaging data (in the case of repeat imaging or delta-features)
sets, conversions (such as SUV and SUL), and/or the speciﬁc ROI. For example, one could
write PET:SUV to separate it from CT and PET:SUL features (optional).
4. Spatial ﬁlters and settings (optional).
5. The interpolation algorithm and uniform interpolation grid spacing (optional).
6. The re-segmentation range and outlier ﬁltering (optional).
7. The discretisation method and relevant discretisation parameters, i.e. number of bins or bin
size (optional).
8. Feature speciﬁc parameters, such as distance for some texture features (optional).
Optional descriptors are only added to the subscript if there are multiple possibilities. For example,
if only CT data is used, adding the modality to the subscript is not required. Nonetheless, such
details must be reported as well (see section 4.1).
The sections below have tables with permanent IBSI identiﬁers for concepts that were deﬁned
within this document.
Abbreviating feature families
The following is a list of the feature families in this document and their suggested abbreviations:
feature family
abbreviation
morphology
local intensity
intensity-based statistics
intensity histogram
intensity-volume histogram
grey level co-occurrence matrix
grey level run length matrix
GLRLM, RLM
grey level size zone matrix
GLSZM, SZM
grey level distance zone matrix
GLDZM, DZM
neighbourhood grey tone diﬀerence matrix
neighbouring grey level dependence matrix
Abbreviating feature aggregation
The following is a list of feature families and the possible aggregation methods:
CHAPTER 4. RADIOMICS REPORTING GUIDELINES AND NOMENCLATURE
morphology, LI
features are 3D by deﬁnition
IS, IH, IVH
averaged over slices (rare)
calculated over the volume (default)
GLCM, GLRLM
averaged over slices and directions
2D:mrg, 2D:smrg
merged directions per slice and averaged
2.5D:avg, 2.5D:dmrg
merged per direction and averaged
2.5D:mrg, 2.5D:vmrg
merged over all slices
averaged over 3D directions
merged 3D directions
GLSZM, GLDZM, NGTDM, NGLDM
averaged over slices
merged over all slices
calculated from single 3D matrix
In the list above, ’–’ signiﬁes an empty entry which does not need to be added to the subscript.
The following examples highlight the nomenclature used above:
• joint maximumCM, 2D:avg: GLCM-based joint maximum feature, calculated by averaging the
feature for every in-slice GLCM.
• short runs emphasisRLM, 3D:mrg: RLM-based short runs emphasis feature, calculated from
an RLM that was aggregated by merging the RLM of each 3D direction.
• meanIS: intensity statistical mean feature, calculated over the 3D ROI volume.
• grey level varianceSZM, 2D: SZM-based grey level variance feature, calculated by averaging
the feature value from the SZM in each slice over all the slices.
Abbreviating interpolation
The following is a list of interpolation methods and the suggested notation. Note that # is the
interpolation spacing, including units, and dim is 2D for interpolation with the slice plane and 3D
for volumetric interpolation.
interpolation method
nearest neighbour interpolation
linear interpolation
cubic convolution interpolation
cubic spline interpolation
CSI:dim:#, SI3:dim:#
The dimension attribute and interpolation spacing may be omitted if this is clear from the
context. The following examples highlight the nomenclature introduced above:
• meanIS, LIN:2D:2mm: intensity statistical mean feature, calculated after bilinear interpolation
with the slice planes to uniform voxel sizes of 2mm.
CHAPTER 4. RADIOMICS REPORTING GUIDELINES AND NOMENCLATURE
• meanIH, NNB:3D:1mm: intensity histogram mean feature, calculated after trilinear interpolation to uniform voxel sizes of 1mm.
• joint maximumCM, 2D:mrg, CSI:2D:2mm: GLCM-based joint maximum feature, calculated by
ﬁrst merging all GLCM within a slice to single GLCM, calculating the feature and then averaging the feature values over the slices. GLCMs were determined in the image interpolated
within the slice plane to 2 × 2mm voxels using cubic spline interpolation.
Describing re-segmentation
Re-segmentation can be noted as follows:
re-segmentation method
outlier ﬁltering
In the table above # signify numbers. A re-segmentation range can be half-open, i.e. RS:[#,∞).
Re-segmentation methods may be combined, i.e. both range and outlier ﬁltering methods may
be used. This is noted as RS:[#,#]+#σ or RS:#σ+[#,#]. The following are examples of the
application of the above notation:
• meanIS, CT, RS:[-200,150]: intensity statistical mean feature, based on an ROI in a CT image
that was re-segmented within a [-200,150] HU range.
• meanIS, PET:SUV, RS:[3,∞): intensity statistical mean feature, based on an ROI in a PET image
with SUV values, that was re-segmented to contain only SUV of 3 and above.
• meanIS, MRI:T1, RS:3σ: intensity statistical mean feature, based on an ROI in a T1-weighted
MR image where the ROI was re-segmented by removing voxels with an intensity outside a
µ ± 3σ range.
Abbreviating discretisation
The following is a list of discretisation methods and the suggested notation. Note that # is the
value of the relevant discretisation parameter, e.g. number of bins or bin size, including units.
discretisation method
ﬁxed bin size
ﬁxed bin number
histogram equalisation
Lloyd-Max, minimum mean squared
LM:#, MMS:#
In the table above, # signify numbers such as the number of bins or their width. Histogram
equalisation of the ROI intensities can be performed before the ”none”, ”ﬁxed bin size”, ”ﬁxed bin
number” or ”Lloyd-Max, minimum mean squared” algorithms deﬁned above, with # specifying
the number of bins in the histogram to be equalised. The following are examples of the application
of the above notation:
• meanIH,PET:SUV,RS[0,∞],FBS:0.2: intensity histogram mean feature, based on an ROI in a
SUV-PET image, with bin-width of 0.2 SUV, and binning from 0.0 SUV.
CHAPTER 4. RADIOMICS REPORTING GUIDELINES AND NOMENCLATURE
• grey level varianceSZM,MR:T1,RS:3σ,FBN:64: size zone matrix-based grey level variance feature,
based on an ROI in a T1-weighted MR image, with 3σ re-segmentation and subsequent
binning into 64 bins.
Abbreviating feature-speciﬁc parameters
Some features and feature families require additional parameters, which may be varied. These are
the following:
grey level co-occurrence matrix
co-occurrence matrix symmetry
symmetrical co-occurrence matrices
asymmetrical co-occurrence matrices (not recommended)
δ:#, δ-∞:#
Chebyshev (ℓ∞) norm with distance # (default)
Euclidean (ℓ2) norm with distance #
Manhattan (ℓ1) norm with distance #
distance weighting
no weighting (default)
weighting with function f
grey level run length matrix
distance weighting
no weighting (default)
weighting with function f
grey level size zone matrix
linkage distance
δ:#, δ-∞:#
Chebyshev (ℓ∞) norm with distance (default) #
Euclidean (ℓ2) norm with distance #
Manhattan (ℓ1) norm with distance #
grey level distance zone matrix
linkage distance
δ:#, δ-∞:#
Chebyshev (ℓ∞) norm with distance (default) #
Euclidean (ℓ2) norm with distance #
Manhattan (ℓ1) norm with distance #
zone distance norm
Chebyshev (ℓ∞) norm
Euclidean (ℓ2) norm
Manhattan (ℓ1) norm (default)
neighbourhood grey tone diﬀerence matrix
δ:#, δ-∞:#
Chebyshev (ℓ∞) norm with distance # (default)
continued on next page
CHAPTER 4. RADIOMICS REPORTING GUIDELINES AND NOMENCLATURE
Euclidean (ℓ2) norm with distance #
Manhattan (ℓ1) norm with distance #
distance weighting
no weighting (default)
weighting with function f
neighbouring grey level dependence matrix
dependence coarseness
dependence coarseness parameter with value #
δ:#, δ-∞:#
Chebyshev (ℓ∞) norm with distance # (default)
Euclidean (ℓ2) norm with distance #
Manhattan (ℓ1) norm with distance #
distance weighting
no weighting (default)
weighting with function f
In the above table, # represents numbers.
Reference data sets
Reference values for features were obtained using a digital image phantom and the CT image of a lung cancer patient, which are described below.
The same data sets can be used to
verify radiomics software implementations.
The data sets themselves may be found here: sets.
Digital phantom
A small digital phantom was developed to derive image features manually and compare these values
with values obtained from radiomics software implementations. The phantom is shown in ﬁgure
5.1. The phantom has the following characteristics:
• The phantom consists of 5 × 4 × 4 (x, y, z) voxels.
• A slice consists of the voxels in (x, y) plane for a particular slice at position z. Slices are
therefore stacked in the z direction.
• Voxels are 2.0 × 2.0 × 2.0 mm in size.
• Not all voxels are included in the region of interest. Several excluded voxels are located on
the outside of the ROI, and one internal voxel was excluded as well. Voxels excluded from
the ROI are shown in blue in ﬁgure 5.1.
• Some intensities are not present in the phantom. Notably, grey levels 2 and 5 are absent. 1
is the lowest grey level present in the ROI, and 6 the highest.
Computing image features
The digital phantom was designed to not require image processing prior to calculating the features.
Thus, feature calculation is done directly on the phantom itself. The following should be taken
into account for calculating image features:
• Discretisation is not required. All features are to be calculated using the phantom as it is. Alternatively, one could use a ﬁxed bin size discretisation of 1 or ﬁxed bin number discretisation
of 6 bins, which does not alter the contents of the phantom.
• Grey level co-occurrence matrices are symmetrical and calculated for (Chebyshev) distance
CHAPTER 5. REFERENCE DATA SETS
• Neighbouring grey level dependence and neighbourhood grey tone diﬀerence matrices are
likewise calculated for (Chebyshev) distance δ = 1. Additionally, the neighbouring grey level
dependence coarseness parameter has the value α = 0.
• Because discretisation is lacking, most intensity-based statistical features will match their
intensity histogram-based analogues in value.
• The ROI morphological and intensity masks are identical for the digital phantom, due to
lack of re-segmentation.
Lung cancer CT image
A small data set of CT images from four non-small-cell lung carcinoma patients was made publicly
available to serve as radiomics phantoms . We use the image for
the ﬁrst patient (PAT1) to obtain feature reference values for diﬀerent conﬁgurations of the image
processing scheme, as detailed below.
The CT image set is stored as a stack of slices in DICOM format.
The image slices can be
identiﬁed by the DCM IMG preﬁx. The gross tumour volume (GTV) was delineated and is used as
the region of interest (ROI). Contour information is stored as an RT structure set in the DICOM
ﬁle starting with DCM RS. For broader use, both the DICOM set and segmentation mask have been
converted to the NIfTI format. When using the data in NIfTI format, both image stacks should
be converted to (at least) 32-bit ﬂoating point and rounded to the nearest integer before further
processing.
We deﬁned ﬁve image processing conﬁgurations to test diﬀerent image processing methods, see
Table 5.1. While most settings are self-explanatory, there are several aspects that require some
attention. Conﬁgurations are divided in 2D and 3D approaches. For the 2D conﬁgurations (A, B),
image interpolation is conducted within the slice, and likewise texture features are extracted from
the in-slice plane, and not volumetrically (3D). For the 3D conﬁgurations (C-E) interpolation is
conducted in three dimensions, and features are likewise extracted volumetrically. Discretisation
is moreover required for texture, intensity histogram and intensity-volume histogram features, and
both ﬁxed bin number and ﬁxed bin size algorithms are tested.
Notes on interpolation
Interpolation has a major inﬂuence on feature values. Diﬀerent implementations of the same interpolation method may ostensibly provide the same functionality, but may use diﬀerent interpolation
grids. It is therefore recommended to read the documentation of the particular implementation to
assess if the implementation allows or implements the following:
• The spatial origin of the original (input) grid in world coordinates matches the DICOM origin
by deﬁnition.
• The size of the interpolation grid is determined by rounding the fractional grid size towards
inﬁnity, i.e. a ceiling operation. This prevents the interpolation grid from disappearing for
very small images, but is otherwise an arbitrary choice.
• The centers of the interpolation and original image grids should be aligned, i.e. the interpolation grid is centered on the center of the original image grid. This prevents spacing
inconsistencies in the interpolation grid and avoids potential issues with grid orientation.
CHAPTER 5. REFERENCE DATA SETS
Figure 5.1 — Exploded view of the test volume. The number in each voxel corresponds with its grey
level. Blue voxels are excluded from the region of interest. The coordinate system is so that x increases
from left to right, y increases from back to front and z increases from top to bottom, as is indicated
by the axis deﬁnition in the top-left.
CHAPTER 5. REFERENCE DATA SETS
• The extent of the interpolation grid is, by deﬁnition, always equal or larger than that of the
original grid. This means that intensities at the grid boundary are extrapolated. To facilitate
this process, the image should be suﬃciently padded with voxels that take on the nearest
boundary intensity.
• The ﬂoating point representation of the image and the ROI masks aﬀects interpolation precision, and consequentially feature values. Image and ROI masks should at least be represented at full precision (32-bit) to avoid rounding errors. One example is the unintended
exclusion of voxels from the interpolated ROI mask, which occurs when interpolation yields
0.4999. . . instead of 0.5. When images and ROI masks are converted to full precision from
lower precision (e.g. 16-bit), values may require rounding if the original data were integer
values, such as Hounsﬁeld Units or the ROI mask labels.
More details are provided in Section 2.4.
Diagnostic features
Identifying issues with an implementation of the image processing sequence may be challenging.
Multiple steps follow one another and diﬀerences propagate. Hence we deﬁne a small number of
diagnostic features that describe how the image and ROI masks change with each image processing
step. These diagnostic features also have reference values that may be found in IBSI compliance
check spreadsheet.
Initial image stack.
The following features may be used to describe the initial image stack (i.e.
after loading image data for processing):
• Image dimensions. This describes the image dimensions in voxels along the diﬀerent image
• Voxel dimensions. This describes the voxel dimensions in mm. The dimension along the zaxis is equal to the distance between the origin voxels of two adjacent slices, and is generally
equal to the slice thickness.
• Mean intensity. This is the average intensity within the entire image.
• Minimum intensity. This is the lowest intensity within the entire image.
• Maximum intensity. This is the highest intensity within the entire image.
Interpolated image stack.
The above features may also be used to describe the image stack
after image interpolation.
Initial region of interest.
The following descriptors are used to describe the region of interest
(ROI) directly after segmentation of the image:
• ROI intensity mask dimensions. This describes the dimensions, in voxels, of the ROI intensity
• ROI intensity mask bounding box dimensions. This describes the dimensions, in voxels, of
the bounding box of the ROI intensity mask.
• ROI morphological mask bounding box dimensions. This describes the dimensions, in voxels,
of the bounding box of the ROI morphological mask.
CHAPTER 5. REFERENCE DATA SETS
• Number of voxels in the ROI intensity mask. This describes the number of voxels included
in the ROI intensity mask.
• Number of voxels in the ROI morphological mask.
This describes the number of voxels
included in the ROI intensity mask.
• Mean ROI intensity. This is the mean intensity of image voxels within the ROI intensity
• Minimum ROI intensity. This is the lowest intensity of image voxels within the ROI intensity
• Maximum ROI intensity. This is the highest intensity of image voxels within the ROI intensity mask.
Interpolated region of interest.
The same features can be used to describe the ROI after
interpolation of the ROI mask.
Re-segmented region of interest.
Again, the same features as above can be used to describe
the ROI after re-segmentation.
Computing image features
Unlike the digital phantom, the lung cancer CT image does require additional image processing,
which is done according to the processing conﬁgurations described in Table 5.1. The following
should be taken into account when calculating image features:
• Grey level co-occurrence matrices are symmetrical and calculated for (Chebyshev) distance
• Neighbouring grey level dependence and neighbourhood grey tone diﬀerence matrices are
likewise calculated for (Chebyshev) distance δ = 1. Additionally, the neighbouring grey level
dependence coarseness parameter α = 0.
• Intensity-based statistical features and their intensity histogram-based analogues will diﬀer
in value due to discretisation, in contrast to the same features for the digital phantom.
• Due to re-segmentation, the ROI morphological and intensity masks are not identical.
• Calculation of IVH feature: since by default CT contains calibrated and discrete intensities,
no separate discretisation prior to the calculation of intensity-volume histogram features is
required. This is the case for conﬁgurations A, B and D (i.e. ‘calibrated intensity units –
discrete case’). However, for conﬁgurations C and E, we re-discretise the ROI intensities
prior to calculation of intensity-volume histogram features to allow for testing of of these
methods. Conﬁguration C simulates the ‘calibrated intensity units – continuous case’, while
conﬁguration E simulates the ‘arbitrary intensity units’ case where the re-segmentation range
is not used. For details, please consult section 3.5.
CHAPTER 5. REFERENCE DATA SETS
sample identiﬁer
slice-wise or single volume (3D)
interpolation
resampled voxel spacing (mm)
2 × 2 (axial)
interpolation method
tricubic spline
intensity rounding
nearest integer
nearest integer
nearest integer
nearest integer
ROI interpolation method
ROI partial mask volume
re-segmentation
range (HU)
[−500, 400]
[−500, 400]
[−1000, 400]
[−1000, 400]
outlier ﬁltering
discretisation
texture and IH
FBS: 25 HU
FBN: 32 bins
FBS: 25 HU
FBN: 32 bins
FBN: 32 bins
FBS: 2.5 HU
FBN: 1000 bins
texture parameters
GLCM, NGTDM, NGLDM distance
GLSZM, GLDZM linkage distance
NGLDM coarseness
Table 5.1 — Diﬀerent conﬁgurations for image processing. For details, refer to the corresponding sections in chapter 2. ROI: region of interest; HU: Hounsﬁeld Unit;
IH: intensity histogram; FBS: ﬁxed bin size; FBN: ﬁxed bin number; IVH: intensity-volume histogram; GLCM: grey level co-occurrence matrix; NGTDM: neighborhood
grey tone diﬀerence matrix; NGLDM: neighbouring grey level dependence matrix; GLSZM: grey level size zone matrix; GLDZM: grey level distance zone matrix.
Appendix A
Digital phantom texture matrices
This section contains the texture matrices extracted from the digital phantom for reference purposes.
Grey level co-occurrence matrix (2D)
(a) x: (0,1,0)
slice: 1 of 4
(b) x: (0,1,0)
slice: 2 of 4
(c) x: (0,1,0)
slice: 3 of 4
(d) x: (0,1,0)
slice: 4 of 4
(e) x: (1,-1,0)
slice: 1 of 4
(f) x: (1,-1,0)
slice: 2 of 4
(g) x: (1,-1,0)
slice: 3 of 4
(h) x: (1,-1,0)
slice: 4 of 4
APPENDIX A. DIGITAL PHANTOM TEXTURE MATRICES
(i) d: (1,0,0)
slice: 1 of 4
(j) d: (1,0,0)
slice: 2 of 4
(k) d: (1,0,0)
slice: 3 of 4
(l) d: (1,0,0)
slice: 4 of 4
(m) d: (1,1,0)
slice: 1 of 4
(n) d: (1,1,0)
slice: 2 of 4
(o) d: (1,1,0)
slice: 3 of 4
(p) d: (1,1,0)
slice: 4 of 4
Table A.1 — Grey-level co-occurrence matrices extracted from the xy plane (2D) of the digital
phantom using Chebyshev distance 1. x indicates the direction in (x, y, z) coordinates.
Grey level co-occurrence matrix (2D, merged)
APPENDIX A. DIGITAL PHANTOM TEXTURE MATRICES
(a) slice: 1 of 4
(b) slice: 2 of 4
(c) slice: 3 of 4
(d) slice: 4 of 4
Table A.2 — Merged grey-level co-occurrence matrices extracted from the xy plane (2D) of the digital
phantom using Chebyshev distance 1.
Grey level co-occurrence matrix (3D)
(a) x: (0,0,1)
(b) x: (0,1,-1)
(c) x: (0,1,0)
(d) x: (0,1,1)
APPENDIX A. DIGITAL PHANTOM TEXTURE MATRICES
(e) x: (1,-1,-1)
(f) x: (1,-1,0)
(g) x: (1,-1,1)
(h) x: (1,0,-1)
(i) x: (1,0,0)
(j) x: (1,0,1)
(k) x: (1,1,-1)
(l) x: (1,1,0)
(m) x: (1,1,1)
Table A.3 — Grey-level co-occurrence matrices extracted volumetrically (3D) from the digital phantom
using Chebyshev distance 1. x indicates the direction in (x, y, z) coordinates.
APPENDIX A. DIGITAL PHANTOM TEXTURE MATRICES
Grey level co-occurrence matrix (3D, merged)
Table A.4 — Merged grey-level co-occurrence matrix extracted volumetrically (3D) from the digital
phantom using Chebyshev distance 1.
Grey level run length matrix (2D)
(a) x: (0,1,0)
slice: 1 of 4
(b) x: (0,1,0)
slice: 2 of 4
(c) x: (0,1,0)
slice: 3 of 4
(d) x: (0,1,0)
slice: 4 of 4
APPENDIX A. DIGITAL PHANTOM TEXTURE MATRICES
(e) x: (1,-1,0)
slice: 1 of 4
(f) x: (1,-1,0)
slice: 2 of 4
(g) x: (1,-1,0)
slice: 3 of 4
(h) x: (1,-1,0)
slice: 4 of 4
(i) x: (1,0,0)
slice: 1 of 4
(j) x: (1,0,0)
slice: 2 of 4
(k) x: (1,0,0)
slice: 3 of 4
(l) x: (1,0,0)
slice: 4 of 4
(m) x: (1,1,0)
slice: 1 of 4
(n) x: (1,1,0)
slice: 2 of 4
(o) x: (1,1,0)
slice: 3 of 4
(p) x: (1,1,0)
slice: 4 of 4
Table A.5 — Grey-level run length matrices extracted from the xy plane (2D) of the digital phantom.
x indicates the direction in (x, y, z) coordinates.
APPENDIX A. DIGITAL PHANTOM TEXTURE MATRICES
Grey level run length matrix (2D, merged)
(a) slice: 1 of 4
(b) slice: 2 of 4
(c) slice: 3 of 4
(d) slice: 4 of 4
Table A.6 — Merged grey-level run length matrices extracted from the xy plane (2D) of the digital
Grey level run length matrix (3D)
(a) x: (0,0,1)
(b) x: (0,1,-1)
(c) x: (0,1,0)
(d) x: (0,1,1)
APPENDIX A. DIGITAL PHANTOM TEXTURE MATRICES
(e) x: (1,-1,-1)
(f) x: (1,-1,0)
(g) x: (1,-1,1)
(h) x: (1,0,-1)
(i) x: (1,0,0)
(j) x: (1,0,1)
(k) x: (1,1,-1)
(l) x: (1,1,0)
(m) x: (1,1,1)
Table A.7 — Grey-level run length matrices extracted volumetrically (3D) from the digital phantom.
x indicates the direction in (x, y, z) coordinates.
APPENDIX A. DIGITAL PHANTOM TEXTURE MATRICES
Grey level run length matrix (3D, merged)
Table A.8 — Merged grey-level run length matrix extracted volumetrically (3D) from the digital
Grey level size zone matrix (2D)
(a) slice: 1 of 4
(b) slice: 2 of 4
(c) slice: 3 of 4
(d) slice: 4 of 4
Table A.9 — Grey level size zone matrices extracted from the xy plane (2D) of the digital phantom.
Grey level size zone matrix (3D)
Table A.10 — Grey level size zone matrix extracted volumetrically (3D) from the digital phantom.
APPENDIX A. DIGITAL PHANTOM TEXTURE MATRICES
Grey level distance zone matrix (2D)
(a) slice: 1 of 4
(b) slice: 2 of 4
(c) slice: 3 of 4
(d) slice: 4 of 4
Table A.11 — Grey level distance zone matrices extracted from the xy plane (2D) of the digital
Grey level distance zone matrix (3D)
Table A.12 — Grey level distance zone matrix extracted volumetrically (3D) from the digital phantom.
Neighbourhood grey tone diﬀerence matrix (2D)
(a) slice: 1 of 4
(b) slice: 2 of 4
(c) slice: 3 of 4
(d) slice: 4 of 4
Table A.13 — Neighbourhood grey tone diﬀerence matrices extracted from the xy plane (2D) of the
digital phantom using Chebyshev distance 1.
Neighbourhood grey tone diﬀerence matrix (3D)
APPENDIX A. DIGITAL PHANTOM TEXTURE MATRICES
Table A.14 — Neighbourhood grey tone diﬀerence matrix extracted volumetrically (3D) from the
digital phantom using Chebyshev distance 1.
Neighbouring grey level dependence matrix (2D)
(a) slice: 1 of 4
(b) slice: 2 of 4
(c) slice: 3 of 4
(d) slice: 4 of 4
Table A.15 — Neighbouring grey level dependence matrices extracted from the xy plane (2D) of the
digital phantom using Chebyshev distance 1 and coarseness 0.
Neighbouring grey level dependence matrix (3D)
APPENDIX A. DIGITAL PHANTOM TEXTURE MATRICES
Table A.16 — Neighbouring grey level dependence matrix extracted volumetrically (3D) from the
digital phantom using Chebyshev distance 1 and coarseness 0.
Bibliography
 H. J. W. L. Aerts, E. Rios-Velazquez, R. T. H. Leijenaar, C. Parmar, P. Grossmann,
S. Cavalho, J. Bussink, R. Monshouwer, B. Haibe-Kains, D. Rietveld, F. J. P. Hoebers, M. M.
Rietbergen, C. R. Leemans, A. Dekker, J. Quackenbush, R. J. Gillies, and P. Lambin. Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach.
Nature communications, 5:4006, 2014.
 S. D. Ahipa¸sao˘glu. Fast algorithms for the minimum volume estimator. Journal of Global
Optimization, 62(2):351–370, 2015.
 F. Albregtsen, B. Nielsen, and H. Danielsen. Adaptive gray level run length features from
class distance matrices. In Proceedings 15th International Conference on Pattern Recognition.
ICPR-2000, volume 3, pages 738–741. IEEE Comput. Soc, 2000.
 B. A. Altazi, G. G. Zhang, D. C. Fernandez, M. E. Montejo, D. Hunt, J. Werner, M. C.
Biagioli, and E. G. Moros. Reproducibility of F18-FDG PET radiomic features for diﬀerent
cervical tumor segmentation methods, gray-level discretization, and reconstruction algorithms.
Journal of applied clinical medical physics, 18(6):32–48, 2017.
 M. Amadasun and R. King. Textural features corresponding to textural properties. IEEE
Transactions on Systems, Man and Cybernetics, 19(5):1264–1273, 1989.
 I. Apostolova, I. G. Steﬀen, F. Wedel, A. Lougovski, S. Marnitz, T. Derlin, H. Amthauer,
R. Buchert, F. Hofheinz, and W. Brenner. Asphericity of pretherapeutic tumour FDG uptake
provides independent prognostic value in head-and-neck cancer. European radiology, 24(9):
2077–87, 2014.
 J. Atkinson A.J., W. Colburn, V. DeGruttola, D. DeMets, G. Downing, D. Hoth, J. Oates,
C. Peck, R. Schooley, B. Spilker, J. Woodcock, and S. Zeger. Biomarkers and surrogate endpoints: Preferred deﬁnitions and conceptual framework. Clinical Pharmacology and Therapeutics, 69(3):89–95, 2001.
 C. Bailly, C. Bodet-Milin, S. Couespel, H. Necib, F. Kraeber-Bod´er´e, C. Ansquer, and
T. Carlier.
Revisiting the Robustness of PET-Based Textural Features in the Context of
Multi-Centric Trials. PloS one, 11(7):e0159984, 2016.
 M. A. Balafar, A. R. Ramli, M. I. Saripan, and S. Mashohor. Review of brain MRI image
segmentation methods. Artiﬁcial Intelligence Review, 33(3):261–274, 2010.
 G. Barequet and S. Har-Peled. Eﬃciently Approximating the Minimum-Volume Bounding
Box of a Point Set in Three Dimensions. Journal of Algorithms, 38(1):91–109, 2001.
 R. Boellaard, R. Delgado-Bolton, W. J. G. Oyen, F. Giammarile, K. Tatsch, W. Eschner,
F. J. Verzijlbergen, S. F. Barrington, L. C. Pike, W. A. Weber, S. G. Stroobants, D. Delbeke,
K. J. Donohoe, S. Holbrook, M. M. Graham, G. Testanera, O. S. Hoekstra, J. M. Zijlstra,
E. P. Visser, C. J. Hoekstra, J. Pruim, A. T. Willemsen, B. Arends, J. Kotzerke, A. Bockisch,
BIBLIOGRAPHY
T. Beyer, A. Chiti, and B. J. Krause. FDG PET/CT: EANM procedure guidelines for tumour
imaging: version 2.0. European journal of nuclear medicine and molecular imaging, 42(2):328–
 N. Boussion, C. C. Le Rest, M. Hatt, and D. Visvikis. Incorporation of wavelet-based denoising
in iterative deconvolution for partial volume correction in whole-body PET imaging. European
journal of nuclear medicine and molecular imaging, 36(7):1064–75, 2009.
 J. C. Caicedo, S. Cooper, F. Heigwer, S. Warchal, P. Qiu, C. Molnar, A. S. Vasilevich, J. D.
Barry, H. S. Bansal, O. Kraus, M. Wawer, L. Paavolainen, M. D. Herrmann, M. Rohban,
J. Hung, H. Hennig, J. Concannon, I. Smith, P. A. Clemons, S. Singh, P. Rees, P. Horvath,
R. G. Linington, and A. E. Carpenter. Data-analysis strategies for image-based cell proﬁling.
Nature Methods, 14(9):849–863, 2017.
 C. Chan and S. Tan. Determination of the minimum bounding box of an arbitrary solid: an
iterative approach. Computers and Structures, 79(15):1433–1449, 2001.
 A. Chu, C. M. Sehgal, and J. F. Greenleaf. Use of gray value distribution of run lengths for
texture analysis. Pattern Recognition Letters, 11(6):415–419, 1990.
 K. Clark, B. Vendt, K. Smith, J. Freymann, J. Kirby, P. Koppel, S. Moore, S. Phillips,
D. Maﬃtt, M. Pringle, L. Tarbox, and F. Prior.
The Cancer Imaging Archive (TCIA):
maintaining and operating a public information repository. Journal of digital imaging, 26
(6):1045–57, 2013.
 L. P. Clarke, R. J. Nordstrom, H. Zhang, P. Tandon, Y. Zhang, G. Redmond, K. Farahani,
G. Kelloﬀ, L. Henderson, L. Shankar, J. Deye, J. Capala, and P. Jacobs. The Quantitative
Imaging Network: NCI’s Historical Perspective and Planned Goals. Translational oncology, 7
(1):1–4, 2014.
 D. A. Clausi. An analysis of co-occurrence texture statistics as a function of grey level quantization. Canadian Journal of Remote Sensing, 28(1):45–62, 2002.
 G. Collewet, M. Strzelecki, and F. Mariette. Inﬂuence of MRI acquisition protocols and image
intensity normalization methods on texture classiﬁcation. Magnetic resonance imaging, 22(1):
81–91, 2004.
 E. C. Da Silva, A. C. Silva, A. C. De Paiva, and R. A. Nunes. Diagnosis of lung nodule using
Moran’s index and Geary’s coeﬃcient in computerized tomography images. Pattern Analysis
and Applications, 11(1):89–99, 2008.
 M. R. T. Dale, P. Dixon, M.-J. Fortin, P. Legendre, D. E. Myers, and M. S. Rosenberg.
Conceptual and mathematical relationships among methods for spatial analysis. Ecography,
25(5):558–577, 2002.
 B. V. Dasarathy and E. B. Holder. Image characterizations based on joint gray level-run
length distributions. Pattern Recognition Letters, 12(8):497–502, 1991.
 A. Depeursinge and J. Fageot. Biomedical Texture Operators and Aggregation Functions. In
A. Depeursinge, J. Fageot, and O. Al-Kadi, editors, Biomedical texture analysis, chapter 3,
pages 63–101. Academic Press, London, UK, 1st edition, 2017.
 A. Depeursinge, A. Foncubierta-Rodriguez, D. Van De Ville, and H. M¨uller. Three-dimensional
solid texture analysis in biomedical imaging: review and opportunities. Medical image analysis,
18(1):176–96, 2014.
BIBLIOGRAPHY
 M.-C. Desseroit, F. Tixier, W. A. Weber, B. A. Siegel, C. Cheze Le Rest, D. Visvikis, and
M. Hatt. Reliability of PET/CT Shape and Heterogeneity Features in Functional and Morphologic Components of Non-Small Cell Lung Cancer Tumors: A Repeatability Analysis in a
Prospective Multicenter Cohort. Journal of nuclear medicine, 58(3):406–411, 2017.
 I. El Naqa. Image Processing and Analysis of PET and Hybrid PET Imaging, pages 285–301.
Springer International Publishing, Cham, 2017.
 I. El Naqa, P. W. Grigsby, A. Apte, E. Kidd, E. Donnelly, D. Khullar, S. Chaudhari, D. Yang,
M. Schmitt, R. Laforest, W. L. Thorstad, and J. O. Deasy. Exploring feature-based approaches
in PET images for predicting cancer treatment outcomes. Pattern recognition, 42(6):1162–
1171, 2009.
 European Society of Radiology (ESR). ESR statement on the stepwise development of imaging
biomarkers. Insights into imaging, 4(2):147–52, 2013.
 V. Frings, F. H. P. van Velden, L. M. Velasquez, W. Hayes, P. M. van de Ven, O. S. Hoekstra,
and R. Boellaard. Repeatability of metabolically active tumor volume measurements with
FDG PET/CT in advanced gastrointestinal malignancies: a multicenter study. Radiology,
273(2):539–48, 2014.
 M. M. Galloway. Texture analysis using gray level run lengths. Computer Graphics and Image
Processing, 4(2):172–179, 1975.
 R. C. Geary. The Contiguity Ratio and Statistical Mapping. The Incorporated Statistician, 5
(3):115–145, 1954.
 L. Gjesteby, B. De Man, Y. Jin, H. Paganetti, J. Verburg, D. Giantsoudi, and G. Wang. Metal
Artifact Reduction in CT: Where Are We After Four Decades? IEEE Access, 4:5826–5849,
 H. Gudbjartsson and S. Patz. The Rician distribution of noisy MRI data. Magnetic resonance
in medicine, 34(6):910–4, 1995.
 E. L. Hall, R. P. Kruger, J. Samuel, D. Dwyer, R. W. McLaren, D. L. Hall, and G. Lodwick. A
Survey of Preprocessing and Feature Extraction Techniques for Radiographic Images. IEEE
Transactions on Computers, C-20(9):1032–1044, 1971.
 R. M. Haralick. Statistical and structural approaches to texture. Proceedings of the IEEE, 67
(5):786–804, 1979.
 R. M. Haralick, K. Shanmugam, and I. Dinstein. Textural Features for Image Classiﬁcation.
IEEE Transactions on Systems, Man, and Cybernetics, 3(6):610–621, 1973.
 M. Hatt, M. Majdoub, M. Valli`eres, F. Tixier, C. C. Le Rest, D. Groheux, E. Hindi´e, A. Martineau, O. Pradier, R. Hustinx, R. Perdrisot, R. Guillevin, I. El Naqa, and D. Visvikis. 18F-
FDG PET uptake characterization through texture analysis: investigating the complementary
nature of heterogeneity and functional tumor volume in a multi-cancer site patient cohort.
Journal of nuclear medicine, 56(1):38–44, 2015.
 M. Hatt, F. Tixier, L. Pierce, P. E. Kinahan, C. C. Le Rest, and D. Visvikis. Characterization
of PET/CT images using texture analysis: the past, the present. . . any future?
journal of nuclear medicine and molecular imaging, 44(1):151–165, 2017.
 R. M. Heiberger and B. Holland. Statistical Analysis and Data Display. Springer Texts in
Statistics. Springer New York, New York, NY, 2015.
BIBLIOGRAPHY
 L. G. Khachiyan. Rounding of Polytopes in the Real Number Model of Computation. Mathematics of Operations Research, 21(2):307–320, 1996.
 P. Lambin, R. T. H. Leijenaar, T. M. Deist, J. Peerlings, E. E. C. de Jong, J. van Timmeren,
S. Sanduleanu, R. T. H. M. Larue, A. J. G. Even, A. Jochems, Y. van Wijk, H. Woodruﬀ, J. van
Soest, T. Lustberg, E. Roelofs, W. J. C. van Elmpt, A. L. A. J. Dekker, F. M. Mottaghy, J. E.
Wildberger, and S. Walsh. Radiomics: the bridge between medical imaging and personalized
medicine. Nature reviews. Clinical oncology, 14(12):749–762, 2017.
 R. T. H. M. Larue, J. E. van Timmeren, E. E. C. de Jong, G. Feliciani, R. T. H. Leijenaar,
W. M. J. Schreurs, M. N. Sosef, F. H. P. J. Raat, F. H. R. van der Zande, M. Das, W. J. C. van
Elmpt, and P. Lambin. Inﬂuence of gray level discretization on radiomic feature stability for
diﬀerent CT scanners, tube currents and slice thicknesses: a comprehensive phantom study.
Acta oncologica, pages 1–10, 2017.
 A. Le Pogam, H. Hanzouli, M. Hatt, C. Cheze Le Rest, and D. Visvikis. Denoising of PET
images by combining wavelets and curvelets for improved preservation of resolution and quantitation. Medical image analysis, 17(8):877–91, 2013.
 R. T. H. Leijenaar, G. Nalbantov, S. Carvalho, W. J. C. van Elmpt, E. G. C. Troost, R. Boellaard, H. J. W. L. Aerts, R. J. Gillies, and P. Lambin. The eﬀect of SUV discretization in
quantitative FDG-PET Radiomics: the need for standardized methodology in tumor texture
analysis. Scientiﬁc reports, 5(August):11075, 2015.
 T. Lewiner, H. Lopes, A. W. Vieira, and G. Tavares. Eﬃcient Implementation of Marching
Cubes’ Cases with Topological Guarantees. Journal of Graphics Tools, 8(2):1–15, 2003.
 E. J. Limkin, S. Reuz´e, A. Carr´e, R. Sun, A. Schernberg, A. Alexis, E. Deutsch, C. Fert´e, and
C. Robert. The complexity of tumor shape, spiculatedness, correlates with tumor radiomic
shape features. Sci. Rep., 9(1):4329, 2019.
 S. P. Lloyd. Least Squares Quantization in PCM. IEEE Transactions on Information Theory,
28(2):129–137, 1982.
 W. E. Lorensen and H. E. Cline. Marching cubes: A high resolution 3D surface construction
algorithm. ACM SIGGRAPH Computer Graphics, 21(4):163–169, 1987.
 D. Mackin, X. Fave, L. Zhang, J. Yang, A. K. Jones, C. S. Ng, and L. Court. Harmonizing
the pixel size in retrospective computed tomography radiomics studies. PLOS ONE, 12(9):
e0178524, 2017.
 J. Max. Quantizing for minimum distortion. IEEE Transactions on Information Theory, 6
(1):7–12, 1960.
 M. A. Mazurowski, N. M. Czarnek, L. M. Collins, K. B. Peters, and K. Clark. Predicting
outcomes in glioblastoma patients using computerized analysis of tumor shape: preliminary
data. In G. D. Tourassi and S. G. Armato, editors, SPIE Medical Imaging, volume 9785, page
97852T, 2016.
 P. A. P. Moran. Notes on continuous stochastic phenomena. Biometrika, 37:17–23, 1950.
 J. L. Mulshine, D. S. Gierada, S. G. Armato, R. S. Avila, D. F. Yankelevitz, E. A. Kazerooni,
M. F. McNitt-Gray, A. J. Buckler, and D. C. Sullivan. Role of the Quantitative Imaging
Biomarker Alliance in optimizing CT for the evaluation of lung cancer screen-detected nodules.
Journal of the American College of Radiology, 12(4):390–5, 2015.
 R. J. Nordstrom. The quantitative imaging network in precision medicine. Tomography, 2(4):
239, 2016.
BIBLIOGRAPHY
 J. P. B. O’Connor, E. O. Aboagye, J. E. Adams, H. J. W. L. Aerts, S. F. Barrington, A. J.
Beer, R. Boellaard, S. E. Bohndiek, M. Brady, G. Brown, D. L. Buckley, T. L. Chenevert,
L. P. Clarke, S. Collette, G. J. Cook, N. M. DeSouza, J. C. Dickson, C. Dive, J. L. Evelhoch,
C. Faivre-Finn, F. A. Gallagher, F. J. Gilbert, R. J. Gillies, V. Goh, J. R. Griﬃths, A. M.
Groves, S. Halligan, A. L. Harris, D. J. Hawkes, O. S. Hoekstra, E. P. Huang, B. F. Hutton,
E. F. Jackson, G. C. Jayson, A. Jones, D.-M. Koh, D. Lacombe, P. Lambin, N. Lassau, M. O.
Leach, T.-Y. Lee, E. L. Leen, J. S. Lewis, Y. Liu, M. F. Lythgoe, P. Manoharan, R. J. Maxwell,
K. A. Miles, B. Morgan, S. Morris, T. Ng, A. R. Padhani, G. J. M. Parker, M. Partridge,
A. P. Pathak, A. C. Peet, S. Punwani, A. R. Reynolds, S. P. Robinson, L. K. Shankar, R. A.
Sharma, D. Soloviev, S. Stroobants, D. C. Sullivan, S. A. Taylor, P. S. Tofts, G. M. Tozer,
M. van Herk, S. Walker-Samuel, J. Wason, K. J. Williams, P. Workman, T. E. Yankeelov,
K. M. Brindle, L. M. McShane, A. Jackson, and J. C. Waterton. Imaging biomarker roadmap
for cancer studies. Nature Reviews Clinical Oncology, 14(3):169–186, 2017.
 J. O’Rourke. Finding minimal enclosing boxes. International Journal of Computer and Information Sciences, 14(3):183–199, 1985.
 S. Sanduleanu, H. C. Woodruﬀ, E. E. C. de Jong, J. E. van Timmeren, A. Jochems, L. Dubois,
and P. Lambin.
Tracking tumor biology with radiomics: A systematic review utilizing a
radiomics quality score. Radiother. Oncol., 127(3):349–360, 2018.
 S. Schirra. How Reliable Are Practical Point-in-Polygon Strategies?
In Algorithms - ESA
2008, pages 744–755. Springer Berlin Heidelberg, Berlin, Heidelberg, 2008.
 M. Shaﬁq-Ul-Hassan, G. G. Zhang, K. Latiﬁ, G. Ullah, D. C. Hunt, Y. Balagurunathan, M. A.
Abdalah, M. B. Schabath, D. G. Goldgof, D. Mackin, L. E. Court, R. J. Gillies, and E. G.
Moros. Intrinsic dependencies of CT radiomic features on voxel size and number of gray levels.
Medical physics, 44(3):1050–1062, 2017.
 I. Shiri, A. Rahmim, P. Ghaﬀarian, P. Geramifar, H. Abdollahi, and A. Bitarafan-Rajabi. The
impact of image reconstruction settings on 18F-FDG PET radiomic features: multi-scanner
phantom and patient studies. European Radiology, 27(11):4498–4509, 2017.
 J. G. Sled, A. P. Zijdenbos, and A. C. Evans. A nonparametric method for automatic correction
of intensity nonuniformity in MRI data. IEEE transactions on medical imaging, 17(1):87–97,
 K. Smith, Y. Li, F. Piccinini, G. Csucs, C. Balazs, A. Bevilacqua, and P. Horvath. CIDRE:
An illumination-correction method for optical microscopy. Nature Methods, 12(5):404–406,
 L.-K. Soh and C. Tsatsoulis. Texture analysis of sar sea ice imagery using gray level cooccurrence matrices. IEEE Transactions on Geoscience and Remote Sensing, 37(2):780–795,
 M. Sollini, L. Cozzi, L. Antunovic, A. Chiti, and M. Kirienko. PET Radiomics in NSCLC:
state of the art and a proposal for harmonization of methodology. Scientiﬁc reports, 7(1):358,
 C. Solomon and T. Breckon. Features. In Fundamentals of Digital Image Processing, chapter 9,
pages 235–262. John Wiley & Sons, Ltd, Chichester, UK, 2011.
 M. Soret, S. L. Bacharach, and I. Buvat. Partial-volume eﬀect in PET tumor imaging. Journal
of nuclear medicine, 48(6):932–45, 2007.
BIBLIOGRAPHY
 P. Stelldinger, L. J. Latecki, and M. Siqueira. Topological equivalence between a 3D object and
the reconstruction of its digital image. IEEE transactions on pattern analysis and machine
intelligence, 29(1):126–40, 2007.
 D. C. Sullivan, N. A. Obuchowski, L. G. Kessler, D. L. Raunig, C. Gatsonis, E. P. Huang,
M. Kondratovich, L. M. McShane, A. P. Reeves, D. P. Barboriak, A. R. Guimaraes, R. L.
Wahl, and RSNA-QIBA Metrology Working Group. Metrology Standards for Quantitative
Imaging Biomarkers. Radiology, 277(3):813–25, 2015.
 C. Sun and W. G. Wee. Neighboring gray level dependence matrix for texture classiﬁcation.
Computer Vision, Graphics, and Image Processing, 23(3):341–352, 1983.
 P. Th´evenaz, T. Blu, and M. Unser. Image interpolation and resampling. In Handbook of
medical imaging, pages 393–420. Academic Press, Inc., 2000.
 G. Thibault, J. Angulo, and F. Meyer. Advanced statistical matrices for texture characterization: application to cell classiﬁcation. IEEE transactions on bio-medical engineering, 61(3):
630–7, 2014.
 M. J. Todd and E. A. Yldrm. On Khachiyan’s algorithm for the computation of minimumvolume enclosing ellipsoids. Discrete Applied Mathematics, 155(13):1731–1744, 2007.
 A. Traverso, L. Wee, A. Dekker, and R. Gillies. Repeatability and reproducibility of radiomic
features: A systematic review. Int. J. Radiat. Oncol. Biol. Phys., 102(4):1143–1158, 2018.
 M. Unser. Sum and diﬀerence histograms for texture classiﬁcation. IEEE transactions on
pattern analysis and machine intelligence, 8(1):118–125, 1986.
 M. Vaidya, K. M. Creach, J. Frye, F. Dehdashti, J. D. Bradley, and I. El Naqa. Combined
PET/CT image characteristics for radiotherapy tumor response in lung cancer. Radiotherapy
and oncology, 102(2):239–45, 2012.
 M. Valli`eres, C. R. Freeman, S. R. Skamene, and I. El Naqa. A radiomics model from joint
FDG-PET and MRI texture features for the prediction of lung metastases in soft-tissue sarcomas of the extremities. Physics in medicine and biology, 60(14):5471–96, 2015.
 M. Valli`eres, C. R. Freeman, S. R. Skamene, and I. El Naqa. Data from: A radiomics model
from joint FDG-PET and MRI texture features for the prediction of lung metastases in softtissue sarcomas of the extremities, 2015.
 M. Valli`eres, E. Kay-Rivest, L. J. Perrin, X. Liem, C. Furstoss, H. J. W. L. Aerts, N. Khaouam, P. F. Nguyen-Tan, C.-S. Wang, K. Sultanem, J. Seuntjens, and I. El Naqa. Radiomics
strategies for risk assessment of tumour failure in head-and-neck cancer. Scientiﬁc reports, 7:
10117, 2017.
 M. Vallieres, A. Zwanenburg, B. Badic, C. Cheze-Le Rest, D. Visvikis, and M. Hatt. Responsible radiomics research for faster clinical translation, 2017.
 L. V. van Dijk, C. L. Brouwer, A. van der Schaaf, J. G. Burgerhof, R. J. Beukinga, J. A.
Langendijk, N. M. Sijtsema, and R. J. Steenbakkers.
CT image biomarkers to improve
patient-speciﬁc prediction of radiation-induced xerostomia and sticky saliva. Radiotherapy
and Oncology, 122(2):185–191, 2017.
 J. J. van Griethuysen, A. Fedorov, C. Parmar, A. Hosny, N. Aucoin, V. Narayan, R. G.
Beets-Tan, J.-C. Fillion-Robin, S. Pieper, and H. J. Aerts. Computational radiomics system
to decode the radiographic phenotype. Cancer research, 77(21):e104–e107, 2017.