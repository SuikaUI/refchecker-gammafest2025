Introduction to
Semi-Supervised Learning
Synthesis Lectures on
Artiﬁcial Intelligence and
Machine Learning
Ronald J. Brachman, Yahoo! Research
Thomas Dietterich, Oregon State University
Introduction to Semi-Supervised Learning
Xiaojin Zhu and Andrew B. Goldberg
Action Programming Languages
Michael Thielscher
Representation Discovery using Harmonic Analysis
Sridhar Mahadevan
Essentials of Game Theory: A Concise Multidisciplinary Introduction
Kevin Leyton-Brown, Yoav Shoham
A Concise Introduction to Multiagent Systems and Distributed Artiﬁcial Intelligence
Nikos Vlassis
Intelligent Autonomous Robotics: A Robot Soccer Case Study
Peter Stone
© Springer Nature Switzerland AG 2022
Reprint of original edition © Morgan & Claypool 2009
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in
any form or by any means—electronic, mechanical, photocopy, recording, or any other except for brief quotations in
printed reviews, without the prior permission of the publisher.
Introduction to Semi-Supervised Learning
Xiaojin Zhu and Andrew B. Goldberg
ISBN: 978-3-031-00420-9
ISBN: 978-3-031-01548-9
DOI 10.1007/978-3-031-01548-9
A Publication in the Springer series
SYNTHESIS LECTURES ON ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING
Lecture #6
Series Editors: Ronald J. Brachman, Yahoo! Research
Thomas Dietterich, Oregon State University
Series ISSN
Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning
Print 1939-4608
Electronic 1939-4616
Introduction to
Semi-Supervised Learning
Xiaojin Zhu and Andrew B. Goldberg
University of Wisconsin, Madison
SYNTHESIS LECTURES ON ARTIFICIAL INTELLIGENCE AND
MACHINE LEARNING #6
Semi-supervised learning is a learning paradigm concerned with the study of how computers and
natural systems such as humans learn in the presence of both labeled and unlabeled data.Traditionally,
learning has been studied either in the unsupervised paradigm (e.g., clustering, outlier detection)
where all the data is unlabeled, or in the supervised paradigm (e.g., classiﬁcation, regression) where
all the data is labeled.The goal of semi-supervised learning is to understand how combining labeled
and unlabeled data may change the learning behavior, and design algorithms that take advantage
of such a combination. Semi-supervised learning is of great interest in machine learning and data
mining because it can use readily available unlabeled data to improve supervised learning tasks when
the labeled data is scarce or expensive.Semi-supervised learning also shows potential as a quantitative
tool to understand human category learning, where most of the input is self-evidently unlabeled.
In this introductory book, we present some popular semi-supervised learning models, including
self-training, mixture models, co-training and multiview learning, graph-based methods, and semisupervised support vector machines. For each model, we discuss its basic mathematical formulation.
The success of semi-supervised learning depends critically on some underlying assumptions. We
emphasize the assumptions made by each model and give counterexamples when appropriate to
demonstrate the limitations of the different models.In addition,we discuss semi-supervised learning
for cognitive psychology. Finally, we give a computational learning theoretic perspective on semisupervised learning, and we conclude the book with a brief discussion of open questions in the
semi-supervised learning, transductive learning, self-training, Gaussian mixture model,
expectation maximization (EM), cluster-then-label, co-training, multiview learning,
mincut,harmonic function,label propagation,manifold regularization,semi-supervised
support vector machines (S3VM), transductive support vector machines (TSVM), entropy regularization, human semi-supervised learning
To our parents
Yu and Jingquan
Susan and Steven Goldberg
with much love and gratitude.
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii
Introduction to Statistical Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
The Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
Unsupervised Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .2
Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
Overview of Semi-Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9
Learning from Both Labeled and Unlabeled Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9
How is Semi-Supervised Learning Possible? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
Inductive vs. Transductive Semi-Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . 12
Caveats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .13
Self-Training Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
Mixture Models and EM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Mixture Models for Supervised Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Mixture Models for Semi-Supervised Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . .25
Optimization with the EM Algorithm∗. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .26
The Assumptions of Mixture Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .28
Other Issues in Generative Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30
Cluster-then-Label Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
Co-Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .35
Two Views of an Instance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
Co-Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
The Assumptions of Co-Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
Multiview Learning∗. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
Graph-Based Semi-Supervised Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .43
Unlabeled Data as Stepping Stones. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .43
The Graph. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .43
Mincut . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
Harmonic Function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .47
Manifold Regularization∗. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
The Assumption of Graph-Based Methods∗. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
Semi-Supervised Support Vector Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
Support Vector Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
Semi-Supervised Support Vector Machines∗. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
Entropy Regularization∗. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
The Assumption of S3VMs and Entropy Regularization . . . . . . . . . . . . . . . . . . . . . . 65
Human Semi-Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .69
From Machine Learning to Cognitive Science . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .69
Study One: Humans Learn from Unlabeled Test Data. . . . . . . . . . . . . . . . . . . . . . . . .70
Study Two: Presence of Human Semi-Supervised Learning in a Simple Task . . . . 72
Study Three: Absence of Human Semi-Supervised Learning in a Complex Task
Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
Theory and Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
A Simple PAC Bound for Supervised Learning∗. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .79
A Simple PAC Bound for Semi-Supervised Learning∗. . . . . . . . . . . . . . . . . . . . . . . . 81
Future Directions of Semi-Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
Basic Mathematical Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
Semi-Supervised Learning Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .89
Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
Biography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .115
The book is a beginner’s guide to semi-supervised learning. It is aimed at advanced undergraduates, entry-level graduate students and researchers in areas as diverse as Computer Science,
Electrical Engineering, Statistics, and Psychology.The book assumes that the reader is familiar with
elementary calculus, probability and linear algebra. It is helpful, but not necessary, for the reader to
be familiar with statistical machine learning, as we will explain the essential concepts in order for
this book to be self-contained. Sections containing more advanced materials are marked with a star.
We also provide a basic mathematical reference in Appendix A.
Our focus is on semi-supervised model assumptions and computational techniques.We intentionally avoid competition-style benchmark evaluations.This is because, in general, semi-supervised
learning models are sensitive to various settings, and no benchmark that we know of can characterize
the full potential of a given model on all tasks. Instead, we will often use simple artiﬁcial problems to
“break” the models in order to reveal their assumptions. Such analysis is not frequently encountered
in the literature.
Semi-supervised learning has grown into a large research area within machine learning. For
example, a search for the phrase “semi-supervised” in May 2009 yielded more than 8000 papers in
Google Scholar. While we attempt to provide a basic coverage of semi-supervised learning, the selected topics are not able to reﬂect the most recent advances in the ﬁeld.We provide a“bibliographical
notes” section at the end of each chapter for the reader to dive deeper into the topics.
We would like to express our sincere thanks to Thorsten Joachims and the other reviewers for
their constructive reviews that greatly improved the book. We thank Robert Nowak for his excellent
learning theory lecture notes, from which we take some materials for Section 8.1. Our thanks also
go to Bryan Gibson, Tushar Khot, Robert Nosofsky, Timothy Rogers, and Zhiting Xu for their
valuable comments.
We hope you enjoy the book.
Xiaojin Zhu and Andrew B. Goldberg
Madison, Wisconsin