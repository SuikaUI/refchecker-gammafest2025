Transfer Learning by Structural Analogy
Huayan Wang
Computer Science Department
Stanford University
353 Serra Street, Stanford, CA, U.S.A.
Qiang Yang
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
Clearwater Bay, Kowloon, Hong Kong
Transfer learning allows knowledge to be extracted
from auxiliary domains and be used to enhance learning in a target domain. For transfer learning to be successful, it is critical to ﬁnd the similarity between auxiliary and target domains, even when such mappings
are not obvious. In this paper, we present a novel algorithm for ﬁnding the structural similarity between
two domains, to enable transfer learning at a structured
knowledge level. In particular, we address the problem of how to learn a non-trivial structural similarity
mapping between two different domains when they are
completely different on the representation level. This
problem is challenging because we cannot directly compare features across domains. Our algorithm extracts the
structural features within each domain and then maps
the features into the Reproducing Kernel Hilbert Space
(RKHS), such that the “structural dependencies” of features across domains can be estimated by kernel matrices of the features within each domain. By treating
the analogues from both domains as equivalent, we can
transfer knowledge to achieve a better understanding of
the domains and improved performance for learning.
We validate our approach on synthetic and real-world
Introduction and Motivation
Re-using knowledge across different learning tasks (domains) has long been addressed in the machine learning
literature . Existing research on this issue usually assume that the tasks are related on the low level, i.e.
they share the same feature space or the same parametric family of models, such that knowledge transfer can be
achieved by re-using weighted samples across tasks, ﬁnding
a shared intermediate representation, or learning constraints
(informative priors) on the model parameters.
However, examining knowledge transfer in human intelligence, we could ﬁnd that human beings do not rely on
such low-level relatedness to transfer knowledge across domains. Namely, we human beings are able to make analogy
across different domains by resolving the high level (structural) similarities even when the learning tasks (domains)
Copyright c⃝2011, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
are seemingly irrelevant. For example, we can easily understand the analogy between debugging for computer viruses
and diagnosing human diseases. Even though the computer
viruses (harmful codes) themselves have nothing in common
with bacteria or germs, and the computer systems is totally
different from our bodies, we can still make the analogy base
on the following structural similarities:
1. Computer viruses cause malfunction of computers. Diseases cause disfunction of the human body.
2. Computer viruses spread among computers through
the networks. Infectious diseases spread among people
through various interactions.
3. System updates help computers avoid certain viruses.
Vaccines help human beings avoid certain diseases.
Understanding of these structural similarities helps us abstract away the details speciﬁc to the domains, and build a
mapping between the abstractions (see Figure 1). The mapping builds on the high level structural relatedness of the two
domains, instead of their low level “literal similarities”. In
other words, the attributes of the “computer” and the “human” themselves do not matter to the mapping, whereas
their relationships to other entities in their own domains matter.
This is reminiscent of the learning-by-analogy paradigm
in early endeavors in intelligent planing and problem solving. However, many previous operational systems in computational analogy, such as case-based reasoning, have used a
simple similarity function between an old and new problem
domain, whereby the features in the two domains are identical, albeit weighted. This similarity measure cannot handle some more intuitive cases of human problem solving,
such as the above example, in which the similarity between
the domains should be measured on the structural level. And
such a “structural similarity” can only be determined if we
can correctly identify analogues across completely different
representation spaces.
On the other hand, in cognitive science, analogical learning indeed involves developing a set of mappings between
features from different domains. Such a need is captured
in structure mapping theory of analogical reasoning, which argued for deep relational similarity rather than superﬁcial similarity. However,
an operational computational theory has been lacking for
Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence
Malfunction
Disfunction
Figure 1: We can make the analogy between debugging
for computer viruses and diagnosing human diseases based
on structural similarities. The dash lines bridge analogues
across domains.
how to come up with the mapping function. We try to ﬁll
this gap in this paper.
In this paper, we present a framework of transfer learning by structural analogy, which builds on functional space
embedding of distributions . Speciﬁcally, we
address transfer learning in a setting that the source domain
and target domain are using completely different representation spaces. As we cannot directly compare features across
domains, we extract the structural information of the features within each domain by mapping the features into the
Reproducing Kernel Hilbert Space (RKHS), such that the
“structural dependencies” of features across domains can be
estimated by kernel matrices of the features within each domain . Hence the learning process is formulated as simultaneously selecting and associating features
from both domains to maximize the dependencies between
the selected features and response variables (labels), as well
as between the selected features from both domains. With
the learned cross-domain mapping, a structural similarity
between the two domains can be readily computed, which
can be used in place of simple similarity measures in computational analogy systems such as case based reasoning. By
treating the analogues from both domains as equivalent, we
can transfer knowledge to achieve a better understanding of
the domains, e.g. better accuracy in classiﬁcation tasks.
Related Work
The idea of re-using knowledge across learning tasks (domains) has been addressed in the machine learning literature in different terminologies, such as learning to learn,
multi-task learning, domain adaptation, and transfer learning . To the best of our knowledge,
among these works and are the
only ones that address transferring knowledge across different representations spaces. However, rely on cooccurrence observations that bridges the two feature spaces
(such as a dictionary, which consists of co-occurrence observations of two languages), such that the cross-domain relations of the features can be estimated straightforwardly.
In contrast, our work does not rely on the availability of
such co-occurrence data. proposed theoretical foundations for transfer learning between arbitrary
tasks based on Kolmogorov complexity. However they only
showed how to implement their framework in the context of
decision trees, whereas our framework of making structural
analogy between the features can be applied together with
many different learning algorithms.
Learning by analogy is one of the fundamental insights of
artiﬁcial intelligence. Humans can draw on the past experience to solve current problems very well. In AI, there has
been several early works on analogical reasoning, such as
Dynamic Memory . Using analogy in problem solving, pointed out
that analogical reasoning implies that the relationship between entities must be compared, not just the entity themselves, to allow effective recall of previous experiences.
 has argued for high-level structural similarity as a basis of analogical reasoning. has
developed a computational theory of analogical reasoning
using this strategy, when abstraction rules given as input that
allow the two instances to be mapped to a uniﬁed representation.
Analogical problem solving is the cornerstone for casebased reasoning (CBR), where many systems have been developed. For example, HYPO retrieves similar past cases in a legal case base to argue in support of
a claim or make counter-arguments. PRODIGY uses a collection of previous problem solving cases as
a case base, and retrieves the most similar cases for adaptation.
However, most operational systems of analogical reasoning, such as CBR systems , have relied on the assumption
the past instances and the new target problem be in the same
representational space. Most applications of CBR fall in this
case , where
the sets of feature that describe the old cases and new problems are the same. For example, cases for car diagnosis are
built on descriptions of automobile attributes such as battery
and engine size, although the values are allowed to be different between a past case and the current problem.
Estimating Structural Dependencies by HSIC
We aim at resolving the structural analogy between two domains with completely different low-level representations.
For the source domain we are provided with observations
and response variables (labels):
S = {(x(s)
1 ), (x(s)
2 ), · · · , (x(s)
Ns)} ⊂Xs × Ys,
where Xs is the source input domain and Ys is the source
output (label) domain. Similarity we have data for the target
T = {(x(t)
1 ), (x(t)
2 ), · · · , (x(t)
Nt)} ⊂Xt × Yt.
Note that Xt, Yt can be representation spaces that are completely different from Xs, Ys.
For both the source and the target domain, we denote their
feature domains as Φs and Φt. In practice, features are represented by their proﬁles1 in the training set:
2 , · · · , f (s)
2 , · · · , f (t)
For vector representations, (f (s)
2 , · · · , f (s)
S ) is simply
the transpose of (x(s)
2 , · · · , x(s)
N ). Nevertheless, our
framework is applicable to more sophisticated representations (such as graphs etc.) as it is kernelized, which accesses
data only through the kernel function.
Let Hs, Ht, Gs, Gt, Fs, and Ft be reproducing kernel
Hilbert spaces (RKHS) on the domains Xs, Xt, Ys, Yt, Φs
and Φt, with associated kernel functions ms, mt, ls, lt, ks
and kt respectively. Then we are able to estimate dependencies across domains using the Hilbert-Schmidt Independence Criterion (HSIC) ,
which is deﬁned as the square of the Hilbert-Schmidt norm
of the cross-covariance operator bridging the two RKHS.
Speciﬁcally, for the RKHS Fs and Ft on the feature domains Φs and Φt, in terms of the kernel functions ks, kt the
HSIC can be expressed as
D(Fs, Ft, Prst) = Ess′tt′[ks(s, s′)kt(t, t′)]
+Ess′[ks(s, s′)]Ett′[kt(t, t′)]
−2Est[Ex′[ks(s, s′)]Ey′[kt(t, t′)]],
where Prst is the joint distribution of source and target domain features over Φs ×Φt, and (s, t), (s′, t′) are distributed
independently according to the joint distribution.
Given a sample
F = {(f (s)
1 ), (f (s)
2 ), · · · , (f (s)
of the joint distribution Prst, HSIC can be estimated using
the kernel matrices :
D(Fs, Ft, F) =
W(W −3)[tr(KsKt)
+ 1⊤Ks11⊤Kt1
(W −1)(W −2) −
W −21⊤KsKt1],
where Ks(i, j) = (1 −δij)ks(f (s)
) and Kt(i, j) =
(1 −δij)kt(f (t)
j ) are the kernel matrices with diagonal
entries set to zero.
1The “proﬁle” of a feature is deﬁned as its feature value on all
instances of a dataset.
Similarly, we can estimate the dependencies across the
domains (Xs, Ys) and (Xt, Yt) by the corresponding kernel matrices Ms, Ls, Mt and Lt computed by the samples
S, T (in (1) and (2)) from the joint distributions Pr(s)
xy, where Ms(i, j) = (1−δij)ms(x(s)
j ), Ls(i, j) =
(1 −δij)ls(y(s)
j ), Mt(i, j) = (1 −δij)mt(x(t)
and Lt(i, j) = (1 −δij)lt(y(t)
Estimating dependencies by HSIC is a crucial component
in our learning framework, which requires estimating dependencies for the three pairs of domains, namely the source input and output domain (Xs, Ys), the target input and output
domain (Xt, Yt), and the source and target feature domain
Transfer Learning by Structural Analogy
The joint distributions Pr(s)
xy and Pr(t)
xy are well characterized
by the samples S and T. So estimating HSIC for (Xs, Ys)
and (Xt, Yt) can be carried out straightforwardly. However
we have no direct sample from the joint distribution Prst
because the samples in (3) and (4), i.e. the features from different domains, are not associated. Actually how to associate
the features depends on the structures of each domain, and
we therefore name the cross-domain dependency as “structural dependency”, which can only be determined if we understand the structural analogy across the domains.
For a given association of the source and target domain
features, as in (6), structural dependency between the domains can be estimated by (7). That means, by maximizing
the estimated structural dependency, we ﬁnd the “correct”
association of the features from both domains, i.e. we make
the analogy across domains.
Formally, given W ≤min(S, T), let σs and σt be injectives from {1, · · · , W} to {1, · · · , S} and {1, · · · , T} respectively, we could describe the learning problem as selecting a
ordered set of features
σs(1), f (s)
σs(2), · · · , f (s)
σs(W )}, and
σt(1), f (t)
σt(2), · · · , f (t)
from both the source and the target learning task, such
that the objective function combining dependencies between
(Xs, Ys), (Xt, Yt) and (Φs, Φt) is maximized:
(ˆσs, ˆσt) = arg max
σs,σt[D(Fs, Ft, F)
+λsD(Hs, Gs, S) + λtD(Ht, Gt, T)]
where F = {(f (s)
σs(1), f (t)
σt(1)), · · · , (f (s)
σs(W ), f (t)
σt(W ))} is the
pseudo-sample from the joint distribution Prst constructed
by associating the selected features from both domains. All
the three terms in (9) are estimated by the estimator (7) with
kernel matrices Ks, Kt, Ms, Ls, Mt and Lt computed using the selected features in (8). λs and λt are free parameters
controlling the relative inﬂuences the terms.
After determining σs and σt, each sample of the source
domain can be “translated” into a sample for the target domain by treating the features f (s)
σs(i) and f (t)
σt(i) (analogues)
as equivalent. Then standard supervised learning methods
can be applied to the expanded training set of the target domain. Computing the structural similarity between the domains also becomes straightforward. One can directly measure the structural similarity by D(Fs, Ft, F).
It is noticeable that the above described learning paradigm
bears some key features that can be viewed as prototype
models of the components in human’s learning by analogy:
1. The learner knows the key concepts in a familiar case
(source domain).
2. The learner identiﬁes key concepts in a new problem (target domain) by both analyzing the new problem itself and
making the analogy from a previous familiar case base on
their structural similarities.
3. The learner gains better understanding of the new problem
thanks to the knowledge transferred from the previous familiar case.
We have presented the general framework of learning by
structural analogy. However, ﬁnding the globally optimal solution to the optimization problem in (9) is not straightforward. In this paper, we present a simple algorithm to implement the framework by ﬁnding a local minimum of the
objective.
Our algorithm ﬁrst selects features from both domains
by maximizing D(Hs, Gs, S) and D(Ht, Gt, T) respectively, without considering relations between the two domains. Then we ﬁnd the analogy by sorting the selected features for the source domain to maximize D(Fs, Ft, F). One
advantage of this implementation is that we actually do not
have to determine the weights λs and λt as the corresponding terms are maximized in separate procedures.
For feature selection, we simply sort all the features according the estimated HSIC (as in (7)) using the kernel matrix computed by only one feature. And then selected the
top W features with largest estimated HSIC. This procedure ignores possible interactions between the features, but
achieves better efﬁciency especially when dealing with large
scale problems (such as the one in our real-world data experiment).
Then, sorting the selected features of the source domain to
“make the analogy” is achieved by the algorithm proposed in
 . Speciﬁcally, we aim to ﬁnd the optimal
permutation π∗from the permutation group ΠW :
π∗= arg max
π∈ΠW tr ¯Ktπ⊤¯Ksπ
where ¯Kt = HKtH, ¯Ks = HKsH and Hij = δij −W −1.
This optimization problem is solved iteratively by:
πi+1 = (1 −λ)πi + λ arg max
tr ¯Ktπ⊤¯Ksπi
Since tr ¯Ktπ⊤¯Ksπi = tr ¯Ksπi ¯Ktπ⊤, we end up solving a linear assignment problem (LAP) with the cost matrix
−¯Ksπi ¯Kt. A very efﬁcient solver of LAP can be found in
 .
The whole procedure is formalized in Algorithm 1.
Algorithm 1 Transfer Learning by Structural Analogy
Input: S and T.
Output: {f (s)
σs(1), f (s)
σs(2), · · · , f (s)
and {f (t)
σt(1), f (t)
σt(2), · · · , f (t)
Compute Ls and Lt;
for i = 1 to Ns do
Compute Ms using only f (s)
Estimate the HSIC D(Hs, Gs, S) using Ms and Ls;
Find W features from Φs with largest estimated HSIC;
for i = 1 to Nt do
Compute Mt using only f (t)
Estimate the HSIC D(Ht, Gt, T) using Mt and Lt;
Find W features from Φt with largest estimated HSIC;
Compute ¯Ks and ¯Kt with all selected features together;
Initialize permutation matrix π0;
for i = 0 to MAX −1 do
Compute cost matrix −¯Ksπi−1 ¯Kt;
Solve the LAP with the cost matrix;
Update permutation matrix as in (11);
if converged then
Experiments
Ohsumed Dataset
We apply our method to the Ohsumed text
dataset2. The Ohsumed dataset consists of documents on
medical issues covering 23 topics (classes) with ground truth
labels on each document. The preprocessed corpus is bagof-words data on a vocabulary of 30689 unique words (dimensions). We randomly picked 2 classes from the dataset,
namely “Respiratory Tract Diseases” and “Cardiovascular
Diseases”. For each class we randomly sampled 200 positive
examples and 200 negative examples, and we will try to automatically make analogy between these two domains, such
that knowledge can be transferred for classiﬁcation tasks.
We let W = 10 in our algorithm, and we automatically
learned the top 10 words in each domain that are supposed
to be “analogues” as in Table 1. We can see that the top
10 words selected from the two domains have almost no
overlap, i.e., they are in different low-level representations.
However, the structural relatedness enables us to ﬁnd analogues across domains. As we can see in Table 1, the automatically learned words indeed constitute several pairs of
plausible analogues. For example, “infect” and “pneumonia”
(“pneumonia” means infection in the lung); “valv” and “respiratori”; “cell” and “lung” (“cell” means an enclosed cavity in the heart); “aortic” and ”tract” (they are both major
passages in each sub-system of the body). Note that these
analogues are automatically discovered without making use
downloaded
 
Table 1: Learned analogy between the two domains
CARDI. DISEASES
RESPIRATORY TRACT DISEASES
RESPIRATORI
of any co-occurrence information between the words from
different domains.
To further justify the analogy found by our algorithm, we
trained a linear classiﬁer for the source domain documents,
and applied it to the target domain documents by treating
the analogues as equivalent. This procedure yields an accuracy of 80.50% on the target domain3, which justiﬁed that
the analogy found by our algorithm greatly helped in understanding the target domain.
Synthetic Learning Scenario
In order to further illustrate the importance of exploiting
“structural” information within each domain in making the
analogy. We show experimental results in a synthetic learning scenario.
Suppose that a learner is presented with a task of distinguishing two classes represented by two 10-dimensional
Gaussian distributions with full covariance. The two classes
are created such that they are hardly distinguishable in all
the individual dimensions, but can largely be separated by a
hyper-plane in the 10-dimensional space. For evaluation we
hold out a test set of 2000 samples with known ground truth
labels. The learner is required to learn a hyperplane to distinguish the two classes, without recourse to the test samples
(standard inductive learning).
In our scenario, the learner only have 100 unlabeled samples in the training phase. With standard machine learning
techniques, we ﬁrst cluster the samples with the K-means
algorithm, then train a classiﬁer on the resulted two clusters
(using standard linear discriminant analysis, i.e. ﬁts a multivariate Gaussian density to each group, with a pooled estimate of covariance). The classiﬁer obtained from this procedure yields an accuracy 72.35% on the test set.
The above scenario is quite difﬁcult for traditional machine learning methodologies since the learner is only provided with a small number of unlabeled samples, which implicates that the learner have very limited understanding of
the learning task. According to the learning-by-analogy philosophy, in such situations the learning should have recourse
to a previous familiar case in order to solve the current problem. But the “previous familiar case” could have different
representations from the current problem, which means the
3A non-informative classiﬁer would give an accuracy of 50% in
this setting.
Table 2: Experiment results on the synthetic task
ACCURACY ON TEST SET
WITHOUT PREVIOUS CASE
INAPPROPRIATE ANALOGY
OUR METHOD
learner has to make an analogy across the two domains instead of directly copying the previous solution.
We synthesize such a “previous familiar case” by randomly permutating the 10 dimensions together with 10 additional noise dimensions which bear no information to distinguish the two classes. The learner is presented with 1000 labeled samples (500 for each class) from the 20-dimensional
distribution, which indicates that the learner is quite familiar
with this “previous case”. However, such a “previous case”
is of no use in traditional machine learning techniques as the
feature space is different.
It is noticeable that there is rich “structural” information
among the dimensions for us to exploit (as the data are
generate from Gaussian distributions with full covariances).
Speciﬁcally we apply our framework (let W = 10) to make
an analogy between the 10 dimensions in the current problem and the 20 dimensions in the “previous familiar case”.
Note that the term D(Ht, Gt, T) vanishes as we have no labeled data for the target task, and Kt is estimated using the
100 unlabeled samples. After determined the “analogues”
of the current problem’s dimensions in the “previous familiar case”, we translate the 1000 labeled samples to the current problem by treating the “analogues” as equivalent. We
then apply standard linear discriminant analysis to the translated samples, and obtain a classiﬁer for the current problem,
which yields an accuracy of 94.25% on the test set.
Note that resolving the “structures” among the dimensions within each domain plays an essential role in successfully making the analogy. To verify this, we also tried to
ignore the term D(Fs, Ft, F) and merely rank the dimensions according to their relevances to the label. In this way
we obtain a classiﬁer which yields an accuracy of 76.40%.
As summarized in Table 2, we can conclude that,
1. We cannot achieve satisﬁable performance when we have
limited understanding of the current problem and do not
have recourse to previous cases.
2. We achieve little performance improvement if we have
recourse to a previous familiar case but do not carefully
analyze the structural of both domains and make an inappropriate analogy.
3. We ﬁnally achieve satisﬁable understanding of the current
problem through correctly making the analogy to a previous familiar case.
Conclusion
In this paper we addressed the problem of transfer learning
by structural analogy between two domains with completely
different low-level representations. By making use of statistical tools, we tried to bridge transfer learning and the old
paradigm of learning by analogy, and extend them to more
general settings. The current work and our future research
aim at automatically making structural analogies and determine the structural similarities with as few prior knowledge
and background restrictions as possible.
Acknowledgement
We thank the support of Hong Kong RGC/NSFC project
N HKUST 624/09 and RGC project 621010.