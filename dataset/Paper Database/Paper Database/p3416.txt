Learning Rich Features from RGB-D Images for
Object Detection and Segmentation
Saurabh Gupta1, Ross Girshick1, Pablo Arbel´aez 1,2, and Jitendra Malik1
{sgupta, rbg, arbelaez, malik}@eecs.berkeley.edu
1University of California, Berkeley, 2Universidad de los Andes, Colombia
Abstract. In this paper we study the problem of object detection for
RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height
above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works
better than using raw depth images for learning feature representations
with convolutional neural networks. Our ﬁnal object detection system
achieves an average precision of 37.3%, which is a 56% relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by
our detector. For this task, we propose a decision forest approach that
classiﬁes pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric
pose features. Finally, we use the output from our object detectors in an
existing superpixel classiﬁcation framework for semantic scene segmentation and achieve a 24% relative improvement over current state-of-the-art
for the object categories that we study. We believe advances such as those
represented in this paper will facilitate the use of perception in ﬁelds like
Keywords: RGB-D perception, object detection, object segmentation
Introduction
We have designed and implemented an integrated system (Figure 1) for scene
understanding from RGB-D images. The overall architecture is a generalization
of the current state-of-the-art system for object detection in RGB images, R-
CNN , where we design each module to make eﬀective use of the additional
signal in RGB-D images, namely pixel-wise depth. We go beyond object detection
by providing pixel-level support maps for individual objects, such as tables and
chairs, as well as a pixel-level labeling of scene surfaces, such as walls and ﬂoors.
Thus our system subsumes the traditionally distinct problems of object detection
and semantic segmentation. Our approach is summarized below (source code is
available at 
RGB-D contour detection and 2.5D region proposals: RGB-D images enable one to compute depth and normal gradients , which we combine with the
 
Saurabh Gupta, Ross Girshick, Pablo Arbel´aez, Jitendra Malik
Color and Depth
Image Pair
Contour Detection
Region Proposal
Generation
Geocentric Encoding
Features Extraction
RGB CNN Features
Extraction
Object Detection
Instance Segm
Semantic Segm
Fig. 1. Overview: from an RGB and depth image pair, our system detects contours,
generates 2.5D region proposals, classiﬁes them into object categories, and then infers
segmentation masks for instances of “thing”-like objects, as well as labels for pixels
belonging to “stuﬀ”-like categories.
structured learning approach in to yield signiﬁcantly improved contours. We
then use these RGB-D contours to obtain 2.5D region candidates by computing
features on the depth and color image for use in the Multiscale Combinatorial
Grouping (MCG) framework of Arbel´aez et al. . This module is state-of-theart for RGB-D proposal generation.
RGB-D object detection: Convolutional neural networks (CNNs) trained on
RGB images are the state-of-the-art for detection and segmentation . We
show that a large CNN pre-trained on RGB images can be adapted to generate
rich features for depth images. We propose to represent the depth image by three
channels (horizontal disparity, height above ground, and angle with gravity) and
show that this representation allows the CNN to learn stronger features than by
using disparity (or depth) alone. We use these features, computed on our 2.5D
region candidates, in a modiﬁed R-CNN framework to obtain a 56% relative
improvement in RGB-D object detection, compared to existing methods.
Instance segmentation: In addition to bounding-box object detection, we also
infer pixel-level object masks. We frame this as a foreground labeling task and
show improvements over baseline methods.
Semantic segmentation: Finally, we improve semantic segmentation performance (the task of labeling all pixels with a category, but not diﬀerentiating
between instances) by using object detections to compute additional features
for superpixels in the semantic segmentation system we proposed in . This
approach obtains state-of-the-art results for that task, as well.
Related Work
Most prior work on RGB-D perception has focussed on semantic segmentation
 , i.e. the task of assigning a category label to each pixel. While
Learning Rich Features from RGB-D Images for Detection and Segmentation
this is an interesting problem, many practical applications require a richer understanding of the scene. Notably, the notion of an object instance is missing from
such an output. Object detection in RGB-D images , in contrast,
focusses on instances, but the typical output is a bounding box. As Hariharan et
al. observe, neither of these tasks produces a compelling output representation. It is not enough for a robot to know that there is a mass of ‘bottle’ pixels
in the image. Likewise, a roughly localized bounding box of an individual bottle
may be too imprecise for the robot to grasp it. Thus, we propose a framework for
solving the problem of instance segmentation (delineating pixels on the object
corresponding to each detection) as proposed by .
Recently, convolutional neural networks were shown to be useful for
standard RGB vision tasks like image classiﬁcation , object detection ,
semantic segmentation and ﬁne-grained classiﬁcation . Naturally, recent
works on RGB-D perception have considered neural networks for learning representations from depth images . Couprie et al. adapt the multiscale
semantic segmentation system of Farabet et al. by operating directly on
four-channel RGB-D images from the NYUD2 dataset. Socher et al. and Bo
et al. look at object detection in RGB-D images, but detect small prop-like
objects imaged in controlled lab settings. In this work, we tackle uncontrolled,
cluttered environments as in the NYUD2 dataset. More critically, rather than
using the RGB-D image directly, we introduce a new encoding that captures
the geocentric pose of pixels in the image, and show that it yields a substantial
improvement over naive use of the depth channel.
2.5D Region Proposals
In this section, we describe how to extend multiscale combinatorial grouping
(MCG) to eﬀectively utilize depth cues to obtain 2.5D region proposals.
Contour Detection
RGB-D contour detection is a well-studied task . Here we combine
ideas from two leading approaches, and our past work in .
In , we used gPb-ucm and proposed local geometric gradients dubbed
NG−, NG+, and DG to capture convex, concave normal gradients and depth
gradients. In , Doll´ar et al. proposed a novel learning approach based on
structured random forests to directly classify a pixel as being a contour pixel
or not. Their approach treats the depth information as another image, rather
than encoding it in terms of geocentric quantities, like NG−. While the two
methods perform comparably on the NYUD2 contour detection task (maximum
F-measure point in the red and the blue curves in Figure 3), there are diﬀerences
in the the type of contours that either approach produces. produces better
localized contours that capture ﬁne details, but tends to miss normal discontinuities that easily ﬁnds (for example, consider the contours between the walls
and the ceiling in left part of the image Figure 2). We propose a synthesis of the
Saurabh Gupta, Ross Girshick, Pablo Arbel´aez, Jitendra Malik
two approaches that combines features from with the learning framework
from . Speciﬁcally, we add the following features.
Normal Gradients: We compute normal gradients at two scales (corresponding
to ﬁtting a local plane in a half-disk of radius 3 and 5 pixels), and use these as
additional gradient maps.
Geocentric Pose: We compute a per pixel height above ground and angle
with gravity (using the algorithms we proposed in . These features allow the
decision trees to exploit additional regularities, for example that the brightness
edges on the ﬂoor are not as important as brightness edges elsewhere.
Richer Appearance: We observe that the NYUD2 dataset has limited appearance variation (since it only contains images of indoor scenes). To make the
model generalize better, we add the soft edge map produced by running the
RGB edge detector of (which is trained on BSDS) on the RGB image.
Candidate Ranking
From the improved contour signal, we obtain object proposals by generalizing
MCG to RGB-D images. MCG for RGB images uses simple features based
on the color image and the region shape to train a random forest regressors to
rank the object proposals. We follow the same paradigm, but propose additional
geometric features computed on the depth image within each proposal. We compute: (1) the mean and standard deviation of the disparity, height above ground,
angle with gravity, and world (X, Y, Z) coordinates of the points in the region;
(2) the region’s (X, Y, Z) extent; (3) the region’s minimum and maximum height
above ground; (4) the fraction of pixels on vertical surfaces, surfaces facing up,
and surfaces facing down; (5) the minimum and maximum standard deviation
along a direction in the top view of the room. We obtain 29 geometric features
for each region in addition to the 14 from the 2D region shape and color image
already computed in . Note that the computation of these features for a region
decomposes over superpixels and can be done eﬃciently by ﬁrst computing the
ﬁrst and second order moments on the superpixels and then combining them
appropriately.
We now present results for contour detection and candidate ranking. We work
with the NYUD2 dataset and use the standard split of 795 training images and
654 testing images (we further divide the 795 images into a training set of 381
images and a validation set of 414 images). These splits are carefully selected
such that images from the same scene are only in one of these sets.
Contour detection: To measure performance on the contour detection task,
we plot the precision-recall curve on contours in Figure 3 and report the standard maximum F-measure metric (Fmax) in Table 1. We start by comparing the
performance of (Gupta et al. CVPR [RGBD]) and Doll´ar et al. (SE [RGBD])
 . We see that both these contour detectors perform comparably in terms of
Learning Rich Features from RGB-D Images for Detection and Segmentation
Fig. 2. Qualitative comparison of
contours: Top row: color image, contours from , bottom row: contours
from and contours from our proposed contour detector.
(63.15) gPb−ucm [RGB]
(65.77) Silberman et al. [RGBD]
(68.66) Gupta et al. CVPR [RGBD]
(68.45) SE [RGBD]
(70.25) Our(SE + all cues) [RGBD]
(69.46) SE+SH [RGBD]
(71.03) Our(SE+SH + all cues) [RGBD]
Precision-recall
boundaries on the NYUD2 dataset.
Table 1. Segmentation benchmarks on NYUD2. All numbers are percentages.
ODS (Fmax) OIS (Fmax)
Silberman et al. 
Gupta et al. CVPR 
Our(SE + normal gradients) RGB-D
Our(SE + all cues)
SE+SH 
Our(SE+SH + all cues)
Fmax. obtains better precision at lower recalls while obtains better precision in the high recall regime. We also include a qualitative visualization of the
contours to understand the diﬀerences in the nature of the contours produced
by the two approaches (Figure 2).
Switching to the eﬀect of our proposed contour detector, we observe that
adding normal gradients consistently improves precision for all recall levels and
Fmax increases by 1.2% points (Table 1). The addition of geocentric pose features
and appearance features improves Fmax by another 0.6% points, making our ﬁnal
system better than the current state-of-the-art methods by 1.5% points.1
Candidate ranking: The goal of the region generation step is to propose a
pool of candidates for downstream processing (e.g., object detection and segmentation). Thus, we look at the standard metric of measuring the coverage of
ground truth regions as a function of the number of region proposals. Since we
are generating region proposals for the task of object detection, where each class
1 Doll´ar et al. recently introduced an extension of their algorithm and report
performance improvements (SE+SH[RGBD] dashed red curve in Figure 3). We can
also use our cues with , and observe an analogous improvement in performance
(Our(SE+SH + all cues) [RGBD] dashed blue curve in Figure 3). For the rest of the
paper we use the Our(SE+all cues)[RGBD] version of our contour detector.
Saurabh Gupta, Ross Girshick, Pablo Arbel´aez, Jitendra Malik
Fig. 4. Region Proposal Quality: Coverage as a function of the number of region
proposal per image for 2 sets of categories:
ones which we study in this paper, and the
ones studied by Lin et al. . Our depth
based region proposals using our improved
RGB-D contours work better than Lin et
al.’s , while at the same time being more
general. Note that the X-axis is on a log
35 Object Classes from Gupta et al.
Number of candidates
Coverage (Average Jaccard Index over Classes)
Lin et al. NMS [RGBD]
Lin et al. All [RGBD]
MCG (RGB edges,
RGB feats.) [RGB]
MCG (RGBD edges,
RGB feats.) [RGBD]
Our (MCG (RGBD edges,
RGBD feats.)) [RGBD]
21 Object Classes from Lin et al.
Number of candidates
Coverage (Average Jaccard Index over Classes)
Lin et al. NMS [RGBD]
Lin et al. All [RGBD]
MCG (RGB edges,
RGB feats.) [RGB]
MCG (RGBD edges,
RGB feats.) [RGBD]
Our (MCG (RGBD edges,
RGBD feats.)) [RGBD]
is equally important, we measure coverage for K region candidates by
coverage(K) = 1
k∈[1...K] O
where C is the number of classes, Ni is the number of instances for class i,
O(a, b) is the intersection over union between regions a and b, Ii
j is the region
corresponding to the jth instance of class i, l (i, j) is the image which contains
the jth instance of class i, and Rl
k is the kth ranked region in image l.
We plot the function coverage(K) in Figure 4 (left) for our ﬁnal method,
which uses our RGB-D contour detector and RGB-D features for region ranking
(black). As baselines, we show regions from the recent work of Lin et al. with
and without non-maximum suppression, MCG with RGB contours and RGB
features, MCG with RGB-D contours but RGB features and ﬁnally our system
which is MCG with RGB-D contours and RGB-D features. We note that there
is a large improvement in region quality when switching from RGB contours
to RGB-D contours, and a small but consistent improvement from adding our
proposed depth features for candidate region re-ranking.
Since Lin et al. worked with a diﬀerent set of categories, we also compare on
the subset used in their work (in Figure 4 (right)). Their method was trained
speciﬁcally to return candidates for these classes. Our method, in contrast, is
trained to return candidates for generic objects and therefore “wastes” candidates trying to cover categories that do not contribute to performance on any
ﬁxed subset. Nevertheless, our method consistently outperforms , which highlights the eﬀectiveness and generality of our region proposals.
RGB-D Object Detectors
We generalize the R-CNN system introduced by Girshick et al. to leverage
depth information. At test time, R-CNN starts with a set of bounding box proposals from an image, computes features on each proposal using a convolutional
neural network, and classiﬁes each proposal as being the target object class or
not with a linear SVM. The CNN is trained in two stages: ﬁrst, pretraining it
Learning Rich Features from RGB-D Images for Detection and Segmentation
on a large set of labeled images with an image classiﬁcation objective, and then
ﬁnetuning it on a much smaller detection dataset with a detection objective.
We generalize R-CNN to RGB-D images and explore the scientiﬁc question:
Can we learn rich representations from depth images in a manner similar to
those that have been proposed and demonstrated to work well for RGB images?
Encoding Depth Images for Feature Learning
Given a depth image, how should it be encoded for use in a CNN? Should the
CNN work directly on the raw depth map or are there transformations of the
input that the CNN to learn from more eﬀectively?
We propose to encode the depth image with three channels at each pixel:
horizontal disparity, height above ground, and the angle the pixel’s local surface
normal makes with the inferred gravity direction. We refer to this encoding
as HHA. The latter two channels are computed using the algorithms proposed
in and all channels are linearly scaled to map observed values across the
training dataset to the 0 to 255 range.
The HHA representation encodes properties of geocentric pose that emphasize complementary discontinuities in the image (depth, surface normal and
height). Furthermore, it is unlikely that a CNN would automatically learn to
compute these properties directly from a depth image, especially when very limited training data is available, as is the case with the NYUD2 dataset.
We use the CNN architecture proposed by Krizhevsky et al. in and used
by Girshick et al. in . The network has about 60 million parameters and was
trained on approximately 1.2 million RGB images from the 2012 ImageNet Challenge . We refer the reader to for details about the network. Our hypothesis, to be borne out in experiments, is that there is enough common structure
between our HHA geocentric images and RGB images that a network designed
for RGB images can also learn a suitable representation for HHA images. As an
example, edges in the disparity and angle with gravity direction images correspond to interesting object boundaries (internal or external shape boundaries),
similar to ones one gets in RGB images (but probably much cleaner).
Augmentation with synthetic data: An important observation is the amount
of supervised training data that we have in the NYUD2 dataset is about one
order of magnitude smaller than what is there for PASCAL VOC dataset . To address this
issue, we generate more data for training and ﬁnetuning the network. There
are multiple ways of doing this: mesh the already available scenes and render
the scenes from novel view points, use data from nearby video frames available
in the dataset by ﬂowing annotations using optical ﬂow, use full 3D synthetic
CAD objects models available over the Internet and render them into scenes.
Meshing the point clouds may be too noisy and nearby frames from the video
sequence maybe too similar and thus not very useful. Hence, we followed the
third alternative and rendered the 3D annotations for NYUD2 available from
 to generate synthetic scenes from various viewpoints. We also simulated
Saurabh Gupta, Ross Girshick, Pablo Arbel´aez, Jitendra Malik
the Kinect quantization model in generating this data (rendered depth images
are converted to quantized disparity images and low resolution white noise was
added to the disparity values).
Experiments
We work with the NYUD2 dataset and use the standard dataset splits into
train, val, and test as described in Section 2.3. The dataset comes with semantic
segmentation annotations, which we enclose in a tight box to obtain bounding
box annotations. We work with the major furniture categories available in the
dataset, such as chair, bed, sofa, table (listed in Table 2).
Experimental setup: There are two aspects to training our model: ﬁnetuning
the convolutional neural network for feature learning, and training linear SVMs
for object proposal classiﬁcation.
Finetuning: We follow the R-CNN procedure from using the Caﬀe CNN library . We start from a CNN that was pretrained on the much larger ILSVRC
2012 dataset. For ﬁnetuning, the learning rate was initialized at 0.001 and decreased by a factor of 10 every 20k iterations. We ﬁnetuned for 30k iterations,
which takes about 7 hours on a NVIDIA Titan GPU. Following , we label
each training example with the class that has the maximally overlapping ground
truth instance, if this overlap is larger than 0.5, and background otherwise. All
ﬁnetuning was done on the train set.
SVM Training: For training the linear SVMs, we compute features either from
pooling layer 5 (pool5), fully connected layer 6 (fc6), or fully connected layer 7
(fc7). In SVM training, we ﬁxed the positive examples to be from the ground
truth boxes for the target class and the negative examples were deﬁned as boxes
having less than 0.3 intersection over union with the ground truth instances
from that class. Training was done on the train set with SVM hyper-parameters
C = 0.001, B = 10, w1 = 2.0 using liblinear . We report the performance
(detection average precision AP b) on the val set for the control experiments. For
the ﬁnal experiment we train on trainval and report performance in comparison
to other methods on the test set. At test time, we compute features from the fc6
layer in the network, apply the linear classiﬁer, and non-maximum suppression
to the output, to obtain a set of sparse detections on the test image.
We use the PASCAL VOC box detection average precision (denoted as AP b following the generalization introduced in ) as the performance metric. Results
are presented in Table 2. As a baseline, we report performance of the stateof-the-art non-neural network based detection system, deformable part models
(DPM) . First, we trained DPMs on RGB images, which gives a mean AP b
of 8.4% (column A). While quite low, this result agrees with .2 As a stronger
2 Wang et al. report impressive detection results on NYUD2, however we are
unable to compare directly with their method because they use a non-standard train-
Learning Rich Features from RGB-D Images for Detection and Segmentation
Table 2. Control experiments for object detection on NYUD2 val set. We
investigate a variety of ways to encode the depth image for use in a CNN for feature
learning. Results are AP as percentages. See Section 3.2.
DPM DPM CNN CNN
CNN CNN CNN CNN CNN
input channels RGB RGBD RGB RGB disparity disparity HHA HHA HHA HHA HHA RGB+HHA
synthetic data?
67.2 67.8 61.0
18.9 26.3 13.1
garbage-bin
monitor 27.4
night-stand
television 16.2
toilet 25.1
baseline, we trained DPMs on features computed from RGB-D images (by using
HOG on the disparity image and a histogram of height above ground in each
HOG cell in addition to the HOG on the RGB image). These augmented DPMs
(denoted RGBD-DPM) give a mean AP b of 21.7% (column B). We also report
results from the method of Girshick et al. , without and with ﬁne tuning on
the RGB images in the dataset, yielding 16.4% and 19.7% respectively (column
C and column D). We compare results from layer fc6 for all our experiments.
Features from layers fc7 and pool5 generally gave worse performance.
The ﬁrst question we ask is: Can a network trained only on RGB images
can do anything when given disparity images? (We replicate each one-channel
disparity image three times to match the three-channel ﬁlters in the CNN and
scaled the input so as to have a distribution similar to RGB images.) The RGB
network generalizes surprisingly well and we observe a mean AP b of 11.3% (column E). This results conﬁrms our hypothesis that disparity images have a similar
structure to RGB images, and it may not be unreasonable to use an ImageNettest split that they have not made available. Their baseline HOG DPM detection
results are signiﬁcantly higher than those reported in and this paper, indicating
that the split used in is substantially easier than the standard evaluation split.
Saurabh Gupta, Ross Girshick, Pablo Arbel´aez, Jitendra Malik
trained CNN as an initialization for ﬁnetuning on depth images. In fact, in our
experiments we found that it was always better to ﬁnetune from the ImageNet
initialization than to train starting with a random initialization.
We then proceed with ﬁnetuning this network (starting from the ImageNet
initialization), and observe that performance improves to 20.1% (column F),
already becoming comparable to RGBD-DPMs. However, ﬁnetuning with our
HHA depth image encoding dramatically improves performance (by 25% relative), yielding a mean AP b of 25.2% (column G).
We then observe the eﬀect of synthetic data augmentation. Here, we add
2× synthetic data, based on sampling two novel views of the given NYUD2
scene from the 3D scene annotations made available by . We observe an
improvement from 25.2% to 26.1% mean AP b points (column H). However, when
we increase the amount of synthetic data further (15× synthetic data), we see a
small drop in performance (column H to I). We attribute the drop to the larger
bias that has been introduced by the synthetic data. Guo et al.’s annotations
replace all non-furniture objects with cuboids, changing the statistics of the
generated images. More realistic modeling for synthetic scenes is a direction for
future research.
We also report performance when using features from other layers: pool5
(column J) and fc7 (column K). As expected the performance for pool5 is lower,
but the performance for fc7 is also lower. We attribute this to over-ﬁtting during
ﬁnetuning due to the limited amount of data available.
Finally, we combine the features from both the RGB and the HHA image
when ﬁnetuned on 2× synthetic data (column L). We see there is consistent
improvement from 19.7% and 26.1% individually to 32.5% (column L) mean
AP b. This is the ﬁnal version of our system.
We also experimented with other forms of RGB and D fusion - early fusion
where we passed in a 4 channel RGB-D image for ﬁnetuning but were unable
to obtain good results (AP b of 21.2%), and late fusion with joint ﬁnetuning
for RGB and HHA (AP b of 31.9%) performed comparably to our ﬁnal system
(individual ﬁnetuning of RGB and HHA networks) (AP b of 32.5%). We chose
the simpler architecture.
Test set performance: We ran our ﬁnal system (column L) on the test set, by
training on the complete trainval set. Performance is reported in Table 3. We
compare against a RGB DPM, RGBD-DPMs as introduced before. Note that our
RGBD-DPMs serve as a strong baseline and are already an absolute 8.2% better
than published results on the B3DO dataset (39.4% as compared to 31.2%
from the approach of Kim et al. , detailed results are in the supplementary
material ). We also compare to Lin et al. . only produces 8, 15 or
30 detections per image which produce an average F1 measure of 16.60, 17.88
and 18.14 in the 2D detection problem that we are considering as compared
to our system which gives an average Fmax measure of 43.70. Precision Recall
curves for our detectors along with the 3 points of operation from are in the
supplementary material .
Learning Rich Features from RGB-D Images for Detection and Segmentation
Fig. 5. Output of our system: We visualize some true positives (column one, two
and three) and false positives (columns four and ﬁve) from our bed, chair, lamp, sofa
and toilet object detectors. We also overlay the instance segmentation that we infer for
each of our detections. Some of the false positives due to mis-localization are ﬁxed by
the instance segmentation.
Result visualizations: We show some of the top scoring true positives and the
top scoring false positives for our bed, chair, lamp, sofa and toilet detectors in
Figure 5. More ﬁgures can be found in the supplementary material .
Instance Segmentation
In this section, we study the task of instance segmentation as proposed in .
Our goal is to associate a pixel mask to each detection produced by our RGB-D
object detector. We formulate mask prediction as a two-class labeling problem
(foreground versus background) on the pixels within each detection window. Our
proposed method classiﬁes each detection window pixel with a random forest
classiﬁer and then smoothes the predictions by averaging them over superpixels.
Model Training
Learning framework: To train our random forest classiﬁer, we associate each
ground truth instance in the train set with a detection from our detector. We
Saurabh Gupta, Ross Girshick, Pablo Arbel´aez, Jitendra Malik
select the best scoring detection that overlaps the ground truth bounding box
by more than 70%. For each selected detection, we warp the enclosed portion of
the associated ground truth mask to a 50×50 grid. Each of these 2500 locations
(per detection) serves as a training point.
We could train a single, monolithic classiﬁer to process all 2500 locations or
train a diﬀerent classiﬁer for each of the 2500 locations in the warped mask. The
ﬁrst option requires a highly non-linear classiﬁer, while the second option suﬀers
from data scarcity. We opt for the ﬁrst option and work with random forests
 , which naturally deal with multi-modal data and have been shown to work
well with the set of features we have designed . We adapt the open source
random forest implementation in to allow training and testing with on-the-ﬂy
feature computation. Our forests have ten decision trees.
Features: We compute a set of feature channels at each pixel in the original
image (listed in supplementary material ). For each detection, we crop and
warp the feature image to obtain features at each of the 50×50 detection window
locations. The questions asked by our decision tree split nodes are similar to those
in Shotton et al. , which generalize those originally proposed by Geman et
al. . Speciﬁcally, we use two question types: unary questions obtained by
thresholding the value in a channel relative to the location of a point, and binary
questions obtained by thresholding the diﬀerence between two values, at diﬀerent
relative positions, in a particular channel. Shotton et al. scale their oﬀsets
by the depth of the point to classify. We ﬁnd that depth scaling is unnecessary
after warping each instance to a ﬁxed size and scale.
Testing: During testing, we work with the top 5000 detections for each category (and 10000 for the chairs category, this gives us enough detections to get
to 10% or lower precision). For each detection we compute features and pass
them through the random forest to obtain a 50× 50 foreground conﬁdence map.
We unwarp these conﬁdence maps back to the original detection window and
accumulate the per pixel predictions over superpixels. We select a threshold on
the soft mask by optimizing performance on the val set.
To evaluate instance segmentation performance we use the region detection average precision AP r metric (with a threshold of 0.5) as proposed in , which
extends the average precision metric used for bounding box detection by replacing bounding box overlap with region overlap (intersection over union). Note that
this metric captures more information than the semantic segmentation metric
as it respects the notion of instances, which is a goal of this paper.
We report the performance of our system in Table 3. We compare against
three baseline methods: 1) box where we simply assume the mask to be the box
for the detection and project it to superpixels, 2) region where we average the
region proposals that resulted in the detected bounding box and project this to
superpixels, and 3) fg mask where we compute an empirical mask from the set of
ground truth masks corresponding to the detection associated with each ground
Learning Rich Features from RGB-D Images for Detection and Segmentation
Table 3. Test
set results for detection and instance segmentation on
NYUD2: First four rows correspond to box detection average precision, AP b, and we
compare against three baselines: RGB DPMs, RGBD-DPMs, and RGB R-CNN. The
last four lines correspond to region detection average precision, AP r. See Section 3.3
and Section 4.2.
mean bath bed book box chair count- desk door dress- garba- lamp monit- night pillow sink sofa table tele toilet
RGBD-DPM 23.9 19.3 56.0 17.5
22.8 34.2 17.2
RGB R-CNN 22.5 16.9 45.3 28.5
9.7 16.3 18.9
16.6 29.4 12.7
37.3 44.4 71.0 32.9
1.4 43.3 44.0 15.1 24.5 30.4
40.0 34.8 36.1 53.9 24.4 37.5 46.8
3.2 14.5 26.9
28.1 32.4 54.9
8.9 20.3 29.0
33.1 30.9 30.5 10.2
28.0 14.7 59.9
7.2 22.6 33.2
32.0 36.2 11.2
32.1 18.9 66.1 10.2
1.5 35.5 32.8 10.2 22.8 33.7
31.5 34.4 40.7 14.3 37.4 50.5
truth instance in the training set. We see that our approach outperforms all the
baselines and we obtain a mean AP r of 32.1% as compared to 28.1% for the best
baseline. The eﬀectiveness of our instance segmentor is further demonstrated by
the fact that for some categories the AP r is better than AP b, indicating that
our instance segmentor was able to correct some of the mis-localized detections.
Semantic Segmentation
Semantic segmentation is the problem of labeling an image with the correct category label at each pixel. There are multiple ways to approach this problem,
like that of doing a bottom-up segmentation and classifying the resulting superpixels or modeling contextual relationships among pixels and superpixels
Here, we extend our approach from , which produces state-of-the-art results on this task, and investigate the use of our object detectors in the pipeline
of computing features for superpixels to classify them. In particular, we design
a set of features on the superpixel, based on the detections of the various categories which overlap with the superpixel, and use them in addition to the features
preposed in .
We report our semantic segmentation performance in Table 4. We use the same
metrics as , the frequency weighted average Jaccard Index fwavacc3, but
also report other metrics namely the average Jaccard Index (avacc) and average
Jaccard Index for categories for which we added the object detectors (avacc*).
3 We calculate the pixel-wise intersection over union for each class independently as in
the PASCAL VOC semantic segmentation challenge and then compute an average of
these category-wise IoU numbers weighted by the pixel frequency of these categories.
Saurabh Gupta, Ross Girshick, Pablo Arbel´aez, Jitendra Malik
Table 4. Performance on the 40 class semantic segmentation task as proposed by : We report the pixel-wise Jaccard index for each of the 40 categories. We
compare against 4 baselines: previous approaches from , , (ﬁrst three rows),
and the approach in augmented with features from RGBD-DPMs ( +DPM)
(fourth row). Our approach obtains the best performance fwavacc of 47%. There is
an even larger improvement for the categories for which we added our object detector
features, where the average performance avacc* goes up from 28.4 to 35.1. Categories
for which we added detectors are shaded in gray (avacc* is the average for categories
with detectors).
picture counter
As a baseline we consider + DPM, where we replace our detectors with
RGBD-DPM detectors as introduced in Section 3.3. We observe that there is
an increase in performance by adding features from DPM object detectors over
the approach of , and the fwavacc goes up from 45.2 to 45.6, and further
increase to 47.0 on adding our detectors. The quality of our detectors is brought
out further when we consider the performance on just the categories for which
we added object detectors which on average goes up from 28.4% to 35.1%. This
24% relative improvement is much larger than the boost obtained by adding
RGBD-DPM detectors (31.0% only a 9% relative improvement over 28.4%).
Acknowledgements : This work was sponsored by ONR SMARTS MURI
N00014-09-1-1051, ONR MURI N00014-10-1-0933 and a Berkeley Fellowship.
The GPUs used in this research were generously donated by the NVIDIA Corporation. We are also thankful to Bharath Hariharan, for all the useful discussions.
We also thank Piotr Doll´ar for helping us with their contour detection code.
Learning Rich Features from RGB-D Images for Detection and Segmentation