Published as a conference paper at ICLR 2017
LOSSY IMAGE COMPRESSION WITH
COMPRESSIVE AUTOENCODERS
Lucas Theis, Wenzhe Shi, Andrew Cunningham& Ferenc Husz´ar
London, UK
{ltheis,wshi,acunningham,fhuszar}@twitter.com
We propose a new approach to the problem of optimizing autoencoders for lossy
image compression. New media formats, changing hardware technology, as well
as diverse requirements and content types create a need for compression algorithms which are more ﬂexible than existing codecs. Autoencoders have the potential to address this need, but are difﬁcult to optimize directly due to the inherent
non-differentiabilty of the compression loss. We here show that minimal changes
to the loss are sufﬁcient to train deep autoencoders competitive with JPEG 2000
and outperforming recently proposed approaches based on RNNs. Our network
is furthermore computationally efﬁcient thanks to a sub-pixel architecture, which
makes it suitable for high-resolution images. This is in contrast to previous work
on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.
INTRODUCTION
Advances in training of neural networks have helped to improve performance in a number of domains, but neural networks have yet to surpass existing codecs in lossy image compression. Promising ﬁrst results have recently been achieved using autoencoders – in particular on small images – and neural networks are already achieving state-of-the-art results in lossless image
compression .
Autoencoders have the potential to address an increasing need for ﬂexible lossy compression algorithms. Depending on the situation, encoders and decoders of different computational complexity
are required. When sending data from a server to a mobile device, it may be desirable to pair a powerful encoder with a less complex decoder, but the requirements are reversed when sending data in
the other direction. The amount of computational power and bandwidth available also changes over
time as new technologies become available. For the purpose of archiving, encoding and decoding
times matter less than for streaming applications. Finally, existing compression algorithms may be
far from optimal for new media formats such as lightﬁeld images, 360 video or VR content. While
the development of a new codec can take years, a more general compression framework based on
neural networks may be able to adapt much quicker to these changing tasks and environments.
Unfortunately, lossy compression is an inherently non-differentiable problem. In particular, quantization is an integral part of the compression pipeline but is not differentiable. This makes it difﬁcult
to train neural networks for this task. Existing transformations have typically been manually chosen
(e.g., the DCT transformation used in JPEG) or have been optimized for a task different from lossy
compression . In contrast
to most previous work, but in line with Ball´e et al. , we here aim at directly optimizing the
rate-distortion tradeoff produced by an autoencoder. We propose a simple but effective approach
for dealing with the non-differentiability of rounding-based quantization, and for approximating the
non-differentiable cost of coding the generated coefﬁcients.
Using this approach, we achieve performance similar to or better than JPEG 2000 when evaluated for
perceptual quality. Unlike JPEG 2000, however, our framework can be optimized for speciﬁc content
(e.g., thumbnails or non-natural images), arbitrary metrics, and is readily generalizable to other
 
Published as a conference paper at ICLR 2017
Stochastic rounding
Additive noise
Figure 1: Effects of rounding and differentiable alternatives when used as replacements in JPEG
compression. A: A crop of an image before compression . B: Blocking artefacts
in JPEG are caused by rounding of DCT coefﬁcients to the nearest integer. Since rounding is used
at test time, a good approximation should produce similar artefacts. C: Stochastic rounding to the
nearest integer similar to the binarization of Toderici et al. . D: Uniform additive noise .
forms of media. Notably, we achieve this performance using efﬁcient neural network architectures
which would allow near real-time decoding of large images even on low-powered consumer devices.
COMPRESSIVE AUTOENCODERS
We deﬁne a compressive autoencoder (CAE) to have three components: an encoder f, a decoder g,
and a probabilistic model Q,
f : RN →RM,
g : RM →RN,
Q : ZM → .
The discrete probability distribution deﬁned by Q is used to assign a number of bits to representations based on their frequencies, that is, for entropy coding. All three components may have
parameters and our goal is to optimize the tradeoff between using a small number of bits and having
small distortion,
−log2 Q ([f(x)])
Number of bits
+β · d (x, g([f(x)]))
Distortion
Here, β controls the tradeoff, square brackets indicate quantization through rounding to the nearest
integer, and d measures the distortion introduced by coding and decoding. The quantized output
of the encoder is the code used to represent an image and is stored losslessly. The main source
of information loss is the quantization (Appendix A.3). Additional information may be discarded
by the encoder, and the decoder may not perfectly decode the available information, increasing
distortion.
Unfortunately we cannot optimize Equation 2 directly using gradient-based techniques, as Q and [·]
are non-differentiable. The following two sections propose a solution to deal with this problem.
QUANTIZATION AND DIFFERENTIABLE ALTERNATIVES
The derivative of the rounding function is zero everywhere except at integers, where it is undeﬁned.
We propose to replace its derivative in the backward pass of backpropagation with the derivative of a smooth approximation, r, that is, effectively deﬁning the derivative to
dy [y] := d
Importantly, we do not fully replace the rounding function with a smooth approximation but only
its derivative, which means that quantization is still performed as usual in the forward pass. If we
replaced rounding with a smooth approximation completely, the decoder might learn to invert the
Published as a conference paper at ICLR 2017
smooth approximation, thereby removing the information bottle neck that forces the network to
compress information.
Empirically, we found the identity, r(y) = y, to work as well as more sophisticated choices. This
makes this operation easy to implement, as we simply have to pass gradients without modiﬁcation
from the decoder to the encoder.
Note that the gradient with respect to the decoder’s parameters can be computed without resorting
to approximations, assuming d is differentiable. In contrast to related approaches, our approach has
the advantage that it does not change the gradients of the decoder, since the forward pass is kept the
In the following, we discuss alternative approaches proposed by other authors. Motivated by theoretical links to dithering, Ball´e et al. proposed to replace quantization by additive uniform
[f(x)] ≈f(x) + u.
Toderici et al. , on the other hand, used a stochastic form of binarization .
Generalizing this idea to integers, we deﬁne the following stochastic rounding operation:
{y} ≈⌊y⌋+ ε,
ε ∈{0, 1},
P(ε = 1) = y −⌊y⌋,
where ⌊·⌋is the ﬂoor operator. In the backward pass, the derivative is replaced with the derivative
of the expectation,
dy {y} := d
dy E [{y}] = d
Figure 1 shows the effect of using these two alternatives as part of JPEG, whose encoder and decoder
are based on a block-wise DCT transformation . Note that the output
is visibly different from the output produced with regular quantization by rounding and that the
error signal sent to the autoencoder depends on these images. Whereas in Fig. 1B the error signal
received by the decoder would be to remove blocking artefacts, the signal in Fig. 1D will be to
remove high-frequency noise. We expect this difference to be less of a problem with simple metrics
such as mean-squared error and to have a bigger impact when using more perceptually meaningful
measures of distortion.
An alternative would be to use the latter approximations only for the gradient of the encoder but not
for the gradients of the decoder. While this is possible, it comes at the cost of increased computational and implementational complexity, since we would have to perform the forward and backward
pass through the decoder twice: once using rounding, once using the approximation. With our
approach the gradient of the decoder is correct even for a single forward and backward pass.
ENTROPY RATE ESTIMATION
Since Q is a discrete function, we cannot differentiate it with respect to its argument, which prevents
us from computing a gradient for the encoder. To solve this problem, we use a continuous, differentiable approximation. We upper-bound the non-differentiable number of bits by ﬁrst expressing the
model’s distribution Q in terms of a probability density q,
[−.5,.5[M q(z + u) du.
An upper bound is given by:
−log2 Q (z) = −log2
[−.5,.5[M q(z + u) du ≤
[−.5,.5[M −log2 q(z + u) du,
where the second step follows from Jensen’s inequality . An unbiased
estimate of the upper bound is obtained by sampling u from the unit cube [−.5, .5[M. If we use a
differentiable density, this estimate will be differentiable in z and therefore can be used to train the
Published as a conference paper at ICLR 2017
VARIABLE BIT RATES
In practice we often want ﬁne-gained control over the number of bits used. One way to achieve this
is to train an autoencoder for different rate-distortion tradeoffs. But this would require us to train
and store a potentially large number of models. To reduce these costs, we ﬁnetune a pre-trained
autoencoder for different rates by introducing scale parameters1 λ ∈RM,
−log2 q ([f(x) ◦λ] + u) + β · d (x, g([f(x) ◦λ] /λ)) .
Here, ◦indicates point-wise multiplication and division is also performed point-wise. To reduce
the number of trainable scales, they may furthermore be shared across dimensions. Where f and g
are convolutional, for example, we share scale parameters across spatial dimensions but not across
An example of learned scale parameters is shown in Figure 3A. For more ﬁne-grained control over
bit rates, the optimized scales can be interpolated.
RELATED WORK
Perhaps most closely related to our work is the work of Ball´e et al. . The main differences
lie in the way we deal with quantization (see Section 2.1) and entropy rate estimation. The transformations used by Ball´e et al. consist of a single linear layer combined with a form of contrast
gain control, while our framework relies on more standard deep convolutional neural networks.
Toderici et al. proposed to use recurrent neural networks (RNNs) for compression. Instead
of entropy coding as in our work, the network tries to minimize the distortion for a given number
of bits. The image is encoded in an iterative manner, and decoding is performed in each step to be
able to take into account residuals at the next iteration. An advantage of this design is that it allows
for progressive coding of images. A disadvantage is that compression is much more time consuming than in our approach, as we use efﬁcient convolutional neural networks and do not necessarily
require decoding at the encoding stage.
Gregor et al. explored using variational autoencoders with recurrent encoders and decoders
for compression of small images. This type of autoencoder is trained to maximize the lower bound
of a log-likelihood, or equivalently to minimize
log q(y)q(x | y)
where p(y | x) plays the role of the encoder, and q(x | y) plays the role of the decoder. While
Gregor et al. used a Gaussian distribution for the encoder, we can link their approach to the
work of Ball´e et al. by assuming it to be uniform, p(y | x) = f(x) + u. If we also assume
a Gaussian likelihood with ﬁxed variance, q(x | y) = N(x | g(y), σ2I), the objective function can
be written
−log q(f(x) + u) +
2σ2 ||x −g(f(x) + u)||2
Here, C is a constant which encompasses the negative entropy of the encoder and the normalization
constant of the Gaussian likelihood. Note that this equation is identical to a rate-distortion trade-off
with β = σ−2/2 and quantization replaced by additive uniform noise. However, not all distortions
have an equivalent formulation as a variational autoencoder . This only
works if e−d(x,y) is normalizable in x and the normalization constant does not depend on y, or
otherwise C will not be constant. An direct empirical comparison of our approach with variational
autoencoders is provided in Appendix A.5.
Ollivier discusses variational autoencoders for lossless compression as well as connections
to denoising autoencoders.
Published as a conference paper at ICLR 2017
mirror-pad
denormalize
Figure 2: Illustration of the compressive autoencoder architecture used in this paper. Inspired by
the work of Shi et al. , most convolutions are performed in a downsampled space to speed up
computation, and upsampling is performed using sub-pixel convolutions (convolutions followed by
reshaping/reshufﬂing of the coefﬁcients). To reduce clutter, only two residual blocks of the encoder
and the decoder are shown. Convolutions followed by leaky rectiﬁcations are indicated by solid
arrows, while transparent arrows indicate absence of additional nonlinearities. As a model for the
distributions of quantized coefﬁcients we use Gaussian scale mixtures. The notation C × K × K
refers to K × K convolutions with C ﬁlters. The number following the slash indicates stride in the
case of convolutions, and upsampling factors in the case of sub-pixel convolutions.
EXPERIMENTS
ENCODER, DECODER, AND ENTROPY MODEL
We use common convolutional neural networks for the encoder and the decoder
of the compressive autoencoder. Our architecture was inspired by the work of Shi et al. , who
demonstrated that super-resolution can be achieved much more efﬁciently by operating in the lowresolution space, that is, by convolving images and then upsampling instead of upsampling ﬁrst and
then convolving an image.
The ﬁrst two layers of the encoder perform preprocessing, namely mirror padding and a ﬁxed pixelwise normalization. The mirror-padding was chosen such that the output of the encoder has the same
spatial extent as an 8 times downsampled image. The normalization centers the distribution of each
channel’s values and ensures it has approximately unit variance. Afterwards, the image is convolved
and spatially downsampled while at the same time increasing the number of channels to 128. This is
followed by three residual blocks , where each block consists of an additional two
convolutional layers with 128 ﬁlters each. A ﬁnal convolutional layer is applied and the coefﬁcients
downsampled again before quantization through rounding to the nearest integer.
The decoder mirrors the architecture of the encoder (Figure 9). Instead of mirror-padding and valid
convolutions, we use zero-padded convolutions. Upsampling is achieved through convolution followed by a reorganization of the coefﬁcients. This reorganization turns a tensor with many channels
into a tensor of the same dimensionality but with fewer channels and larger spatial extent . A convolution and reorganization of coefﬁcients together form a sub-pixel
convolution layer. Following three residual blocks, two sub-pixel convolution layers upsample the
image to the resolution of the input. Finally, after denormalization, the pixel values are clipped to
1To ensure positivity, we use a different parametrization and optimize log-scales rather than scales.
Published as a conference paper at ICLR 2017
Log-scales
Interpolated
Figure 3: A: Scale parameters obtained by ﬁnetuning a compressive autoencoder (blue). More ﬁnegrained control over bit rates can be achieved by interpolating scales (gray). Each dot corresponds
to the scale parameter of one coefﬁcient for a particular rate-distortion trade-off. The coefﬁcients
are ordered due to the incremental training procedure. B: Comparison of incremental training versus non-incremental training. The learning rate was decreased after 116,000 iterations (bottom two
lines). Non-incremental training is initially less stable and shows worse performance at later iterations. Using a small learning rate from the beginning stabilizes non-incremental training but is
considerably slower (top line).
the range of 0 to 255. Similar to how we deal with gradients of the rounding function, we redeﬁne
the gradient of the clipping function to be 1 outside the clipped range. This ensures that the training
signal is non-zero even when the decoded pixels are outside this range (Appendix A.1).
To model the distribution of coefﬁcients and estimate the bit rate, we use independent Gaussian scale
mixtures (GSMs),
log2 q(z + u) =
πksN . We used 6 scales in each GSM. Rather than using
the more common parametrization above, we parametrized the GSM so that it can be easily used
with gradient based methods, optimizing log-weights and log-precisions rather than weights and
variances. We note that the leptokurtic nature of GSMs means that the
rate term encourages sparsity of coefﬁcients.
All networks were implemented in Python using Theano and Lasagne .
For entropy encoding of the quantized coefﬁcients, we ﬁrst created Laplace-smoothed histogram
estimates of the coefﬁcient distributions across a training set. The estimated probabilities were then
used with a publicly available BSD licensed implementation of a range coder2.
INCREMENTAL TRAINING
All models were trained using Adam applied to batches of 32 images 128×128
pixels in size. We found it beneﬁcial to optimize coefﬁcients in an incremental manner (Figure 3B).
This is done by introducing an additional binary mask m,
−log2 q ([f(x)] ◦m + u) + β · d (x, g([f(x)] ◦m)) .
Initially, all but 2 entries of the mask are set to zero. Networks are trained until performance improvements reach below a threshold, and then another coefﬁcient is enabled by setting an entry of
the binary mask to 1. After all coefﬁcients have been enabled, the learning rate is reduced from an
initial value of 10−4 to 10−5. Training was performed for up to 106 updates but usually reached
good performance much earlier.
After a model has been trained for a ﬁxed rate-distortion trade-off (β), we introduce and ﬁne-tune
scale parameters (Equation 9) for other values of β while keeping all other parameters ﬁxed. Here
2 
Published as a conference paper at ICLR 2017
JPEG (4:2:0, optimized)
Toderici et al. 
CAE (ensemble)
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Bit rate [bpp]
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Bit rate [bpp]
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Bit rate [bpp]
Figure 4: Comparison of different compression algorithms with respect to PSNR, SSIM, and MS-
SSIM on the Kodak PhotoCD image dataset. We note that the blue line refers to the results of
Toderici et al. achieved without entropy encoding.
we used an initial learning rate of 10−3 and continuously decreased it by a factor of τ κ/(τ + t)κ,
where t is the current number of updates performed, κ = .8, and τ = 1000. Scales were optimized
for 10,000 iterations. For even more ﬁne-grained control over the bit rates, we interpolated between
scales optimized for nearby rate-distortion tradeoffs.
NATURAL IMAGES
We trained compressive autoencoders on 434 high quality images licensed under creative commons
and obtained from ﬂickr.com. The images were downsampled to below 1536 × 1536 pixels and
stored as lossless PNGs to avoid compression artefacts. From these images, we extracted 128 ×
128 crops to train the network. Mean squared error was used as a measure of distortion during
training. Hyperparameters affecting network architecture and training were evaluated on a small set
of held-out Flickr images. For testing, we use the commonly used Kodak PhotoCD dataset of 24
uncompressed 768 × 512 pixel images3.
We compared our method to JPEG , JPEG 2000 , and the RNNbased method of 4. Bits for header information were not counted towards the
bit rate of JPEG and JPEG 2000. Among the different variants of JPEG, we found that optimized
JPEG with 4:2:0 chroma sub-sampling generally worked best (Appendix A.2).
While ﬁne-tuning a single compressive autoencoder for a wide range of bit rates worked well, optimizing all parameters of a network for a particular rate distortion trade-off still worked better. We
here chose the compromise of combining autoencoders trained for low, medium or high bit rates
(see Appendix A.4 for details).
For each image and bit rate, we choose the autoencoder producing the smallest distortion. This
increases the time needed to compress an image, since an image has to be encoded and decoded
multiple times. However, decoding an image is still as fast, since it only requires choosing and
running one decoder network. A more efﬁcient but potentially less performant solution would be
to always choose the same autoencoder for a given rate-distortion tradeoff. We added 1 byte to the
coding cost to encode which autoencoder of an ensemble is used.
Rate-distortion curves averaged over all test images are shown in Figure 4. We evaluated the different methods in terms of PSNR, SSIM , and multiscale SSIM . We used the implementation of van der Walt et al. for SSIM and the implementation of Toderici et al. for MS-SSIM. We ﬁnd that in terms of PSNR, our method performs
3 
4We used the code which was made available on 
tree/2390974a/compression. We note that at the time of this writing, this implementation does not
include entropy coding as in the paper of Toderici et al. .
Published as a conference paper at ICLR 2017
Toderici et al. 
0.245972 bpp
0.250468 bpp
0.248413 bpp
0.356608 bpp
0.359151 bpp
0.365438 bpp
0.480632 bpp
0.491211 bpp
0.486755 bpp
0.245626 bpp
0.249654 bpp
0.254415 bpp
0.499308 bpp
0.504496 bpp
0.505473 bpp
Figure 5: Closeups of images produced by different compression algorithms at relatively low bit
rates. The second row shows an example where our method performs well, producing sharper lines
than and fewer artefacts than other methods. The fourth row shows an example where our method
struggles, producing noticeable artefacts in the hair and discolouring the skin. At higher bit rates,
these problems disappear and CAE reconstructions appear sharper than those of JPEG 2000 (ﬁfth
row). Complete images are provided in Appendix A.6.
Published as a conference paper at ICLR 2017
Bit rate [bpp]
Mean Opinion Score ± 95% CI
Toderici et al., 2016b
JPEG (4:2:0, optimized)
Figure 6: Results of a mean opinion score test.
similar to JPEG 2000 although slightly worse at low and medium bit rates and slightly better at high
bit rates. In terms of SSIM, our method outperforms all other tested methods. MS-SSIM produces
very similar scores for all methods, except at very low bit rates. However, we also ﬁnd these results to be highly image dependent. Results for individual images are provided as supplementary
material5.
In Figure 5 we show crops of images compressed to low bit rates. In line with quantitative results,
we ﬁnd that JPEG 2000 reconstructions appear visually more similar to CAE reconstructions than
those of other methods. However, artefacts produced by JPEG 2000 seem more noisy than CAE’s,
which are smoother and sometimes appear G´abor-ﬁlter-like.
To quantify the subjective quality of compressed images, we ran a mean opinion score (MOS) test.
While MOS tests have their limitations, they are a widely used standard for evaluating perceptual
quality . Our MOS test set included the 24 full-resolution uncompressed originals
from the Kodak dataset, as well as the same images compressed using each of four algorithms at
or near three different bit rates: 0.25, 0.372 and 0.5 bits per pixel. Only the low-bit-rate CAE was
included in this test.
For each image, we chose the CAE setting which produced the highest bit rate but did not exceed
the target bit rate. The average bit rates of CAE compressed images were 0.24479, 0.36446, and
0.48596, respectively. We then chose the smallest quality factor for JPEG and JPEG 2000 for which
the bit rate exceeded that of the CAE. The average bit rates for JPEG were 0.25221, 0.37339 and
0.49534, for JPEG 2000 0.24631, 0.36748 and 0.49373. For some images the bit rate of the CAE
at the lowest setting was still higher than the target bit rate. These images were excluded from the
ﬁnal results, leaving 15, 21, and 23 images, respectively.
The perceptual quality of the resulting 273 images was rated by n = 24 non-expert evaluators. One
evaluator did not ﬁnish the experiment, so her data was discarded. The images were presented to
each individual in a random order. The evaluators gave a discrete opinion score for each image
from a scale between 1 (bad) to 5 (excellent). Before the rating began, subjects were presented an
uncompressed calibration image of the same dimensions as the test images (but not from the Kodak
dataset). They were then shown four versions of the calibration image using the worst quality setting
of all four compression methods, and given the instruction “These are examples of compressed
images. These are some of the worst quality examples.”
Figure 6 shows average MOS results for each algorithm at each bit rate. 95% conﬁdence intervals
were computed via bootstrapping. We found that CAE and JPEG 2000 achieved higher MOS than
JPEG or the method of Toderici et al. at all bit rates we tested. We also found that CAE
signiﬁcantly outperformed JPEG 2000 at 0.375 bpp (p < 0.05) and 0.5 bpp (p < 0.001).
5 
Published as a conference paper at ICLR 2017
DISCUSSION
We have introduced a simple but effective way of dealing with non-differentiability in training autoencoders for lossy compression. Together with an incremental training strategy, this enabled us to
achieve better performance than JPEG 2000 in terms of SSIM and MOS scores. Notably, this performance was achieved using an efﬁcient convolutional architecture, combined with simple roundingbased quantization and a simple entropy coding scheme. Existing codecs often beneﬁt from hardware support, allowing them to run at low energy costs. However, hardware chips optimized for
convolutional neural networks are likely to be widely available soon, given that these networks are
now key to good performance in so many applications.
While other trained algorithms have been shown to provide similar results as JPEG 2000 , to our knowledge this is the ﬁrst time that an end-to-end trained
architecture has been demonstrated to achieve this level of performance on high-resolution images.
An end-to-end trained autoencoder has the advantage that it can be optimized for arbitrary metrics.
Unfortunately, research on perceptually relevant metrics suitable for optimization is still in its infancy . While perceptual metrics exist which
correlate well with human perception for certain types of distortions , developing a perceptual metric which can be optimized is a more challenging
task, since this requires the metric to behave well for a much larger variety of distortions and image
In future work, we would like to explore the optimization of compressive autoencoders for different
metrics. A promising direction was presented by Bruna et al. , who achieved interesting superresolution results using metrics based on neural networks trained for image classiﬁcation. Gatys
et al. used similar representations to achieve a breakthrough in perceptually meaningful style
transfer. An alternative to perceptual metrics may be to use generative adversarial networks . Building on the work of Bruna et al. and Dosovitskiy & Brox
 , Ledig et al. recently demonstrated impressive super-resolution results by combining
GANs with feature-based metrics.
ACKNOWLEDGMENTS
We would like to thank Zehan Wang, Aly Tejani, Cl´ement Farabet, and Luke Alonso for helpful
feedback on the manuscript.