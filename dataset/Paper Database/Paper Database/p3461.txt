Evaluating Protein Transfer Learning with TAPE
Roshan Rao∗1
Nicholas Bhattacharya∗1
Neil Thomas∗1
John Canny1,3
Pieter Abbeel1,2
Yun S. Song1,4
Protein modeling is an increasingly popular area of machine learning research.
Semi-supervised learning has emerged as an important paradigm in protein modeling due to the high cost of acquiring supervised protein labels, but the current
literature is fragmented when it comes to datasets and standardized evaluation
techniques. To facilitate progress in this ﬁeld, we introduce the Tasks Assessing
Protein Embeddings (TAPE), a set of ﬁve biologically relevant semi-supervised
learning tasks spread across different domains of protein biology. We curate
tasks into speciﬁc training, validation, and test splits to ensure that each task tests
biologically relevant generalization that transfers to real-life scenarios. We benchmark a range of approaches to semi-supervised protein representation learning,
which span recent work as well as canonical sequence learning techniques. We
ﬁnd that self-supervised pretraining is helpful for almost all models on all tasks,
more than doubling performance in some cases. Despite this increase, in several
cases features learned by self-supervised pretraining still lag behind features extracted by state-of-the-art non-neural techniques. This gap in performance suggests
a huge opportunity for innovative architecture design and improved modeling
paradigms that better capture the signal in biological sequences. TAPE will help
the machine learning community focus effort on scientiﬁcally relevant problems.
Toward this end, all data and code used to run these experiments are available at
 
Introduction
New sequencing technologies have led to an explosion in the size of protein databases over the
past decades. These databases have seen exponential growth, with the total number of sequences
doubling every two years . Obtaining meaningful labels and annotations for these sequences
requires signiﬁcant investment of experimental resources, as well as scientiﬁc expertise, resulting in
an exponentially growing gap between the size of protein sequence datasets and the size of annotated
subsets. Billions of years of evolution have sampled the portions of protein sequence space that are
relevant to life, so large unlabeled datasets of protein sequences are expected to contain signiﬁcant
biological information . Advances in natural language processing (NLP) have shown that
self-supervised learning is a powerful tool for extracting information from unlabeled sequences ,
which raises a tantalizing question: can we adapt NLP-based techniques to extract useful biological
information from massive sequence datasets?
To help answer this question, we introduce the Tasks Assessing Protein Embeddings (TAPE), which
to our knowledge is the ﬁrst attempt at systematically evaluating semi-supervised learning on protein
sequences. TAPE includes a set of ﬁve biologically relevant supervised tasks that evaluate the
performance of learned protein embeddings across diverse aspects of protein understanding.
We choose our tasks to highlight three major areas of protein biology where self-supervision can
facilitate scientiﬁc advances: structure prediction, detection of remote homologs, and protein en-
∗Equal Contribution 1UC Berkeley 2covariant.ai 3Google 4Chan Zuckerberg Biohub
Correspondence to: {roshan_rao,nick_bhat,nthomas,yss}@berkeley.edu
 
gineering. We constructed data splits to simulate biologically relevant generalization, such as a
model’s ability to generalize to entirely unseen portions of sequence space, or to ﬁnely resolve small
portions of sequence space. Improvement on these tasks range in application, including designing
new antibodies , improving cancer diagnosis , and ﬁnding new antimicrobial genes hiding in the
so-called “Dark Proteome”: tens of millions of sequences with no labels where existing techniques
for determining protein similarity fail .
We assess the performance of three representative models (recurrent, convolutional, and attentionbased) that have performed well for sequence modeling in other ﬁelds to determine their potential for
protein learning. We also compare two recently proposed semi-supervised models (Bepler et al. ,
Alley et al. ). With our benchmarking framework, these models can be compared directly to one
another for the ﬁrst time.
We show that self-supervised pretraining improves performance for almost all models on all downstream tasks. Interestingly, performance for each architecture varies signiﬁcantly across tasks,
highlighting the need for a multi-task benchmark such as ours. We also show that non-deep alignmentbased features outperform features learned via self-supervision on secondary structure and
contact prediction, while learned features perform signiﬁcantly better on remote homology detection.
Our results demonstrate that self-supervision is a promising paradigm for protein modeling but
considerable improvements need to be made before self-supervised models can achieve breakthrough
performance. All code and data for TAPE are publically available1, and we encourage members of
the machine learning community to participate in these exciting problems.
Background
Protein Terminology
Proteins are linear chains of amino acids connected by covalent bonds. We encode amino acids
in the standard 25-character alphabet, with 20 characters for the standard amino acids, 2 for the
non-standard amino acids selenocysteine and pyrrolysine, 2 for ambiguous amino acids, and 1 for
when the amino acid is unknown . Throughout this paper, we represent a protein x of length L
as a sequence of discrete amino acid characters (x1, x2, . . . , xL) in this ﬁxed alphabet.
Beyond its encoding as a sequence (x1, . . . , xL), a protein has a 3D molecular structure. The different
levels of protein structure include primary (amino acid sequence), secondary (local features), and
tertiary (global features). Understanding how primary sequence folds into tertiary structure is a
fundamental goal of biochemistry . Proteins are often made up of a few large protein domains,
sequences that are evolutionarily conserved, and as such have a well-deﬁned fold and function.
Evolutionary relationships between proteins arise because organisms must maintain certain functions,
such as replicating DNA, as they evolve. Evolution has selected for proteins that are well-suited to
these functions. Though structure is constrained by evolutionary pressures, sequence-level variation
can be high, with very different sequences having similar structure . Two proteins with different
sequences but evolutionary or functional relationships are called homologs.
Quantifying these evolutionary relationships is very important for preventing undesired information
leakage between data splits. We mainly rely on sequence identity, which measures the percentage of
exact amino acid matches between aligned subsequences of proteins . For example, ﬁltering at a
25% sequence identity threshold means that no two proteins in the training and test set have greater
than 25% exact amino acid matches. Other approaches besides sequence identity ﬁltering also exist,
depending on the generalization the task attempts to test .
Modeling Evolutionary Relationships with Sequence Alignments
The key technique for modeling sequence relationships in computational biology is alignment
 . Given a database of proteins and a new protein at test-time, an alignment-based
method uses either carefully designed scoring systems to perform pairwise comparisons ,
Hidden Markov Model-like probabilistic models , or a combination to align the test protein
1 
against the database. If good alignments are found, information from the alignments is either directly
sufﬁcient for the task at hand, or can be fed into downstream models for further use .
Semi-supervised Learning
The ﬁelds of computer vision and natural language processing have been dealing with the question
of how to learn from unlabeled data for years . Images and text found on the internet generally
lack accompanying annotations, yet still contain signiﬁcant structure. Semi-supervised learning
tries to jointly leverage information in the unlabeled and labeled data, with the goal of maximizing
performance on the supervised task. One successful approach to learning from unlabeled examples is
self-supervised learning, which in NLP has taken the form of next-token prediction , masked-token
prediction , and next-sentence classiﬁcation . Analogously, there is good reason to believe
that unlabelled protein sequences contain signiﬁcant information about their structure and function
 . Since proteins can be modeled as sequences of discrete tokens, we test both next-token and
masked-token prediction for self-supervised learning.
Related Work
The most well-known protein modeling benchmark is the Critical Assessment of Structure Prediction
(CASP) , which focuses on structure modeling. Each time CASP is held, the test set consists of
new experimentally validated structures which are held under embargo until the competition ends.
This prevents information leakage and overﬁtting to the test set. The recently released ProteinNet
 provides easy to use, curated train/validation/test splits for machine learning researchers where
test sets are taken from the CASP competition and sequence identity ﬁltering is already performed.
We take the contact prediction task from ProteinNet. However, we believe that structure prediction
alone is not a sufﬁcient benchmark for protein models, so we also use tasks not included in the CASP
competition to give our benchmark a broader focus.
Semi-supervised learning for protein problems has been explored for decades, with lots of work
on kernel-based pretraining . These methods demonstrated that semi-supervised learning
improved performance on protein network prediction and homolog detection, but couldn’t scale
beyond hundreds of thousands of unlabeled examples. Recent work in protein representation learning
has proposed a variety of methods that apply NLP-based techniques for transfer learning to biological
sequences . In a related line of work, Riesselman et al. trained Variational Auto
Encoders on aligned families of proteins to predict the functional impact of mutations. Alley et al.
 also try to combine self-supervision with alignment in their work by using alignment-based
querying to build task-speciﬁc pretraining sets.
Due to the relative infancy of protein representation learning as a ﬁeld, the methods described above
share few, if any, benchmarks. For example, both Rives et al. and Bepler et al. report
transfer learning results on secondary structure prediction and contact prediction, but they differ
signiﬁcantly in test set creation and data-splitting strategies. Other self-supervised work such as
Alley et al. and Yang et al. report protein engineering results, but on different tasks and
datasets. With such varied task evaluation, it is challenging to assess the relative merits of different
self-supervised modeling approaches, hindering efﬁcient progress.
Here we describe our unsupervised pretraining and supervised benchmark datasets. To create
benchmarks that test generalization across large evolutionary distances and are useful in real-life
scenarios, we curate speciﬁc training, validation, and test splits for each dataset. Producing the
data for these tasks requires signiﬁcant effort by experimentalists, database managers, and others.
Following similar benchmarking efforts in NLP , we describe a set of citation guidelines in our
repository2 to ensure these efforts are properly acknowledged.
2 
(a) Secondary Structure
(b) Contact Prediction
Fold = Beta Barrel
(c) Remote Homology
Figure 1: Structure and Annotation Tasks on protein KgdM Porin (pdbid: 4FQE). (a) Viewing this
Porin from the side, we show secondary structure, with the input amino acids for a segment (blue) and
corresponding secondary structure labels (yellow and white). (b) Viewing this Porin from the front,
we show a contact map, where entry i, j in the matrix indicates whether amino acids at positions i, j in
the sequence are within 8 angstroms of each other. In green is a contact between two non-consecutive
amino acids. (c) The fold-level remote homology class for this protein.
Unlabeled Sequence Dataset
We use Pfam , a database of thirty-one million protein domains used extensively in bioinformatics,
as the pretraining corpus for TAPE. Sequences in Pfam are clustered into evolutionarily-related groups
called families. We leverage this structure by constructing a test set of fully heldout families (see
Section A.4 for details on the selected families), about 1% of the data. For the remaining data
we construct training and test sets using a random 95/5% split. Perplexity on the uniform random
split test set measures in-distribution generalization, while perplexity on the heldout families test
set measures out-of-distribution generalization to proteins that are less evolutionarily related to the
training set.
Supervised Datasets
We provide ﬁve biologically relevant downstream prediction tasks to serve as benchmarks. We
categorize these into structure prediction, evolutionary understanding, and protein engineering tasks.
The datasets vary in size between 8 thousand and 50 thousand training examples (see Table S1 for
sizes of all training, validation and test sets). Further information on data processing, splits and
experimental challenges is in Appendix A.1. For each task we provide:
(Deﬁnition) A formal deﬁnition of the prediction problem, as well as the source of the data.
(Impact) The impact of improving performance on this problem.
(Generalization) The type of understanding and generalization desired.
(Metric) The metric used in Table 2 to report results.
Task 1: Secondary Structure (SS) Prediction (Structure Prediction Task)
(Deﬁnition) Secondary structure prediction is a sequence-to-sequence task where each input amino
acid xi is mapped to a label yi ∈{Helix, Strand, Other}. See Figure 1a for illustration. The data
are from Klausen et al. .
(Impact) SS is an important feature for understanding the function of a protein, especially if the
protein of interest is not evolutionarily related to proteins with known structure . SS prediction
tools are very commonly used to create richer input features for higher-level models .
(Generalization) SS prediction tests the degree to which models learn local structure. Data splits
are ﬁltered at 25% sequence identity to test for broad generalization.
(Metric) We report accuracy on a per-amino acid basis on the CB513 dataset.
Task 2: Contact Prediction (Structure Prediction Task)
(Deﬁnition) Contact prediction is a pairwise amino acid task, where each pair xi, xj of input amino
acids from sequence x is mapped to a label yij ∈{0, 1}, where the label denotes whether the
amino acids are “in contact” (< 8Å apart) or not. See Figure 1b for illustration. The data are from
the ProteinNet dataset .
(Impact) Accurate contact maps provide powerful global information; e.g., they facilitate robust
modeling of full 3D protein structure . Of particular interest are medium- and long-range
contacts, which may be as few as twelve sequence positions apart, or as many as hundreds apart.
Train on nearby mutations
Test on further mutations
Full Local Landscape
(a) Fluorescence
Most Stable
Least Stable
Train on broad
sample of proteins
Test on small neighborhoods
of best proteins
(b) Stability
Figure 2: Protein Engineering Tasks. In both tasks, a parent protein p is mutated to explore the local
landscape. As such, dots represent proteins and directed arrow x →y denotes that y has exactly
one more mutation than x away from parent p. (a) The Fluorescence task consists of training on
small neighborhood of the parent green ﬂuorescent protein (GFP) and then testing on a more distant
proteins. (b) The Stability task consists of training on a broad sample of proteins, followed by testing
on one-mutation neighborhoods of the most promising sampled proteins.
(Generalization) The abundance of medium- and long-range contacts makes contact prediction an
ideal task for measuring a model’s understanding of global protein context. We select the data
splits that was ﬁltered at 30% sequence identity to test for broad generalization.
(Metric) We report precision of the L/5 most likely contacts for medium- and long-range contacts
on the ProteinNet CASP12 test set, which is a standard metric reported in CASP .
Task 3: Remote Homology Detection (Evolutionary Understanding Task)
(Deﬁnition) This is a sequence classiﬁcation task where each input protein x is mapped to a label
y ∈{1, . . . , 1195}, representing different possible protein folds. See Figure 1c for illustration.
The data are from Hou et al. .
(Impact) Detection of remote homologs is of great interest in microbiology and medicine; e.g., for
detection of emerging antibiotic resistant genes and discovery of new CAS enzymes .
(Generalization) Remote homology detection measures a model’s ability to detect structural similarity across distantly related inputs. We hold out entire evolutionary groups from the training set,
forcing models to generalize across large evolutionary gaps.
(Metric) We report overall classiﬁcation accuracy on the fold-level heldout set from Hou et al. .
Task 4: Fluorescence Landscape Prediction (Protein Engineering Task)
(Deﬁnition) This is a regression task where each input protein x is mapped to a label y ∈R,
corresponding to the log-ﬂuorescence intensity of x. See Figure 2a for illustration. The data are
from Sarkisyan et al. .
(Impact) For a protein of length L, the number of possible sequences m mutations away is O(Lm),
a prohibitively large space for exhaustive search via experiment, even if m is modest. Moreover,
due to epistasis (second- and higher-order interactions between mutations at different positions),
greedy optimization approaches are unlikely to succeed. Accurate computational predictions could
allow signiﬁcantly more efﬁcient exploration of the landscape, resulting in better optima. Machine
learning methods have already seen some success in related protein engineering tasks .
(Generalization) The ﬂuorescence prediction task tests the model’s ability to distinguish between
very similar inputs, as well as its ability to generalize to unseen combinations of mutations. The
train set is a Hamming distance-3 neighborhood of the parent green ﬂuorescent protein (GFP),
while the test set has variants with four or more mutations.
(Metric) We report Spearman’s ρ (rank correlation coefﬁcient) on the test set.
Task 5: Stability Landscape Prediction (Protein Engineering Task)
(Deﬁnition) This is a regression task where each input protein x is mapped to a label y ∈R measuring the most extreme circumstances in which protein x maintains its fold above a concentration
threshold (a proxy for intrinsic stability). See Figure 2b for illustration. The data are from Rocklin
et al. .
Table 1: Language modeling metrics
Random Families
Heldout Families
Perplexity
Perplexity
Transformer
Supervised LSTM 
UniRep mLSTM 
(Impact) Designing stable proteins is important to ensure, for example, that drugs are delivered
before they are degraded. More generally, given a broad sample of protein measurements, ﬁnding better reﬁnements of top candidates is useful for maximizing yield from expensive protein
engineering experiments.
(Generalization) This task tests a model’s ability to generalize from a broad sampling of relevant
sequences and to localize this information in a neighborhood of a few sequences, inverting the
test-case for ﬂuorescence above. The train set consists of proteins from four rounds of experimental
design, while the test set contains Hamming distance-1 neighbors of top candidate proteins.
(Metric) We report Spearman’s ρ on the test set.
Models and Experimental Setup
We examine two self-supervised losses that have seen success in NLP. The ﬁrst is nexttoken prediction , which models p(xi | x1, . . . , xi−1). Since many protein tasks are sequenceto-sequence and require bidirectional context, we apply a variant of next-token prediction which
additionally trains the reverse model, p(xi | xi+1, . . . , xL), providing full context at each position (assuming a Markov sequence). The second is masked-token prediction , which models
p(xmasked | xunmasked) by replacing the value of tokens at multiple positions with alternate tokens.
Protein-speciﬁc loss:
In addition to self-supervised algorithms, we explore another protein-speciﬁc
training procedure proposed by Bepler et al. . They suggest that further supervised pretraining of
models can provide signiﬁcant beneﬁts. In particular, they propose supervised pretraining on contact
prediction and remote homology detection, and show it increases performance on secondary structure
prediction. Similar work in computer vision has shown that supervised pretraining can transfer well
to other tasks, making this a promising avenue of exploration .
Architectures and Training:
We implement three architectures: an LSTM , a Transformer
 , and a dilated residual network (ResNet) . We use a 12-layer Transformer with a hidden
size of 512 units and 8 attention heads, leading to a 38M-parameter model. Hyperparameters for the
other models were chosen to approximately match the number of parameters in the Transformer. Our
LSTM consists of two three-layer LSTMs with 1024 hidden units corresponding to the forward and
backward language models, whose outputs are concatenated in the ﬁnal layer, similar to ELMo .
For the ResNet we use 35 residual blocks, each containing two convolutional layers with 256 ﬁlters,
kernel size 9, and dilation rate 2.
In addition, we benchmark two previously proposed architectures that differ signiﬁcantly from the
three above. The ﬁrst, proposed by Bepler et al. , is a two-layer bidirectional language model,
similar to the LSTM discussed above, followed by three 512 hidden unit bidirectional LSTMs. The
second, proposed by Alley et al. , is a unidirectional mLSTM with 1900 hidden units. Details
on implementing and training these architectures can be found in the original papers.
The Transformer and ResNet are trained with masked-token prediction, while the LSTM is trained
with next-token prediction. Both Alley et al. and Bepler et al. are trained with next-token prediction.
All self-supervised models are trained on four NVIDIA V100 GPUs for one week.
Table 2: Results on downstream supervised tasks
Evolutionary
Engineering
Fluorescence
No Pretrain
Transformer
Transformer
Supervised 
UniRep 
Baselines:
We evaluate learned features against two baseline featurizations. The ﬁrst is a one-hot
encoding of the input amino acid sequence, which provides a simple baseline. Additionally, most
current state-of-the-art algorithms for protein modeling take advantage of alignment or HMM-based
inputs (see Section 2.2). Alignments can be transformed into various features, such as mutation
probabilities or the HMM state-transition probabilities for each amino acid position. These
are concatenated to the one-hot encoding of the amino acid to form another baseline featurization.
For our baselines we use alignment-based inputs that vary per task depending on the inputs used by
the current state-of-the-art method. See Appendix A.2 for details on the alignment-based features
used. We do not use alignment-based inputs for protein engineering tasks - since proteins in the
engineering datasets differ by only a single amino acid, and alignment-based methods search for
proteins with high sequence identity, the alignment-based methods return the same set of features for
all proteins we wish to distinguish between.
Experimental Setup:
The goal of our experimental setup is to systematically compare all featurizations. For each task we select a particular supervised architecture, drawing from state-of-the-art
where available, and make sure that ﬁne-tuning on all language models is identical. See Appendix A.2
for details on supervised architectures and training.
Table 1 contains accuracy, perplexity, and exponentiated cross entropy (ECE) on the language
modeling task for the ﬁve architectures we trained with self-supervision as well as a random model
baseline. We report metrics on both the random split and the fully heldout families. Supervised
LSTM metrics are reported after language modeling pretraining, but before supervised pretraining.
Heldout family accuracy is consistently lower than random-split accuracy, demonstrating a drop in the
out-of-distribution generalization ability. Note that although some models have lower perplexity than
others on both random-split and heldout sets, this lower perplexity does not necessarily correspond to
better performance on downstream tasks. This replicates the ﬁnding in Rives et al. .
Table 2 contains results for all benchmarked architectures and training procedures on all downstream
tasks in TAPE. We report accuracy, precision, or Spearman’s ρ, depending on the task, so higher is
always better and each metric has a maximum value of 1.0. See Section 4 for the metric reported in
each task. Detailed results and metrics for each task are in Appendix A.6.
We see from Table 2 that self-supervised pretraining improves overall performance across almost
all models and all tasks. Further analysis reveals aspects of these tasks with more for signiﬁcant
improvement. In the ﬂuorescence task, the distribution is bimodal with a mode of bright proteins and
a mode of dark proteins (see Figure 3). Since one goal of using machine learning models in protein
engineering is to screen potential variants, it is important for these methods to successfully distinguish
between beneﬁcial and deleterious mutations. Figure 3 shows that the model does successfully
perform some clustering of ﬂuorescent proteins, but that many proteins are still misclassiﬁed.
Log Fluorescence
(a) Dark Mode
Log Fluorescence
(b) Bright Mode
(c) Embedding t-SNE
Figure 3: Distribution of training, test, and pretrained Transformer predictions on the dark and bright
modes, along with t-SNE of pretrained Transformer protein embeddings colored by log-ﬂuorescence.
(a) True Contacts
(c) LSTM Pretrain
(d) One Hot
(e) Alignment
Figure 4: Predicted contacts for chain 1A of a Bacterioferritin comigratory protein (pdbid: 3GKN).
Blue indicates true positive contacts while red indicates false positive contacts. Darker colors
represent more certainty from the model.
For the stability task, to identify which mutations a model believes are beneﬁcial, we use the parent
protein as a decision boundary and label a mutation as beneﬁcial if its predicted stability is higher
than the parent’s predicted stability. We ﬁnd that our best pretrained model achieves 70% accuracy in
making this prediction while our best non-pretrained model achieves 68% accuracy (see Table S8 for
full results). Improving the ability to distinguish beneﬁcial from deleterious mutations would make
these models much more useful in real protein engineering experiments.
In the contact prediction task, long-range contacts are of particular interest and can be hundreds of
positions apart. Figure 4 shows the predictions of several models on a protein where the longest
range contact occurs between the 8th and 136th amino acids. Pretraining helps the model capture
more long-range information and improves the overall resolution of the predicted map. However,
the hand-engineered alignment features result in a much sharper map, accurately resolving a smaller
number well-spaced of long-range contacts. This increased speciﬁcity is highly relevant in the
structure prediction pipeline and highlights a clear challenge for pretraining.
Discussion
Comparison to state of the art.
As shown in Table 2, alignment-based inputs can provide a
powerful signal that outperforms current self-supervised models on multiple tasks. Current stateof-the-art prediction methods for secondary structure prediction, contact prediction, and remote
homology classiﬁcation all take in alignment-based inputs. These methods combine alignment-based
inputs with other techniques (e.g. multi-task training, kernel regularization) to achieve an additional
boost in performance. For comparison, NetSurfP-2.0 achieves 85% accuracy on the CB513 
secondary structure dataset, compared to our best model’s 75% accuracy, RaptorX achieves 0.69
precision at L/5 on CASP12 contact prediction, compared to our best model’s 0.49, and DeepSF
 achieves 41% accuracy on remote homology detection compared to our best model’s 26%.
Need for multiple benchmark tasks.
Our results support our hypothesis that multiple tasks are
required to appropriately benchmark performance of a given method. Our Transformer, which per-
forms worst of the three models in secondary structure prediction, performs best on the ﬂuorescence
and stability tasks. The reverse is true of our ResNet, which ties the LSTM in secondary structure
prediction but performs far worse for the ﬂuorescence task, with a Spearman’s ρ of 0.21 compared to
the LSTM’s 0.67. This shows that performance on a single task does not capture the full extent of a
trained model’s knowledge and biases, creating the need for multi-task benchmarks such as TAPE.
Future Work
Protein representation learning is an exciting ﬁeld with incredible room for expansion, innovation,
and impact. The exponentially growing gap between labeled and unlabeled protein data means that
self-supervised learning will continue to play a large role in the future of computational protein
modeling. Our results show that no single self-supervised model performs best across all protein
tasks. We believe this is a clear challenge for further research in self-supervised learning, as there is a
huge space of model architecture, training procedures, and unsupervised task choices left to explore.
It may be that language modeling as a task is not enough, and that protein-speciﬁc tasks are necessary
to push performance past state of the art. Further exploring the relationship between alignment-based
and learned representations will be necessary to capitalize on the advantages of each. We hope that
the datasets and benchmarks in TAPE will provide a systematic model-evaluation framework that
allows more machine learning researchers to contribute to this ﬁeld.
Acknowledgments
We thank Philippe Laban, David Chan, Jeffrey Spence, Jacob West-Roberts,
Alex Crits-Cristoph, Aravind Srinivas, Surojit Biswas, Ethan Alley, Mohammed AlQuraishi and
Grigory Khimulya for valuable input on this paper. We thank the AWS Educate program for providing
us with the resources to train our models. Additionally, we acknowledge funding from Berkeley
Deep Drive, Chan-Zuckerberg Biohub, DARPA XAI, NIH, the Packard Fellowship for Science and
Engineering, and the Open Philanthropy Project.