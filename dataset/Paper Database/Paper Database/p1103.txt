International Journal of Neural Systems, Vol. 12, No. 6 1–23
c⃝World Scientiﬁc Publishing Company
ENSEMBLES OF DEEP LEARNING ARCHITECTURES FOR THE EARLY
DIAGNOSIS OF THE ALZHEIMER’S DISEASE
ANDR´ES ORTIZ
Communications Engineering Department. University of M´alaga
M´alaga, 29071/Spain
E-mail: 
www.uma.es
JORGE MUNILLA
Communications Engineering Department. University of M´alaga
M´alaga, 29071/Spain
E-mail: 
www.uma.es
JUAN M. G´ORRIZ*
Department of Signal Theory, Communications and Networking. University of Granada.
Granada, 18060/Spain
E-mail: 
www.ugr.es
JAVIER RAM´ıREZ
Department of Signal Theory, Communications and Networking. University of Granada.
Granada, 18060/Spain
E-mail: 
www.ugr.es
FOR THE ALZHEIMER’S DISEASE NEUROIMAGING INITIATIVE†
Computer aided diagnosis (CAD) constitutes an important tool for the early diagnosis of Alzheimer’s
Disease (AD), which, in turn, allows the application of treatments that can be simpler and more likely
to be eﬀective. This paper explores the construction of classiﬁcation methods based on deep learning
architectures applied on brain regions deﬁned by the Automated Anatomical Labelling (AAL). Gray
Matter (GM) images from each brain area have been split into 3D patches according to the regions
deﬁned by the AAL atlas and these patches are used to train diﬀerent deep belief networks. An ensemble
of deep belief networks is then composed where the ﬁnal prediction is determined by a voting scheme.
Two deep learning based structures and four diﬀerent voting schemes are implemented and compared,
giving as a result a potent classiﬁcation architecture where discriminative features are computed in
an unsupervised fashion. The resulting method has been evaluated using a large dataset from the
∗Department of Signal Theory, Communications and Networking. E.T.S. de Ingenier´ıas Inform´atica y de Telecomunicaci´on.
Periodista Daniel Saucedo Aranda s/n. 18014 - Granada (Spain)
†Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI)
database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of
ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: to apply/ADNI Acknowledgement List.pdf.
gev2b˙aortiz˙no˙marks
Andr´es Ortiz, Jorge Munilla, Juan M. G´orriz, Javier Ram´ırez
Alzheimer’s disease Neuroimaging Initiative (ADNI). Classiﬁcation results assessed by cross-validation
prove that the proposed method is not only valid for diﬀerentiate between controls (NC) and AD images,
but it also provides good performances when tested for the more challenging case of classifying Mild
Cognitive Impairment Subjects (MCI). In particular, the classiﬁcation architecture provides accuracy
values up to 0.90 and AUC of 0.95 for NC/AD classiﬁcation, 0.84 and AUC of 0.91 for stable MCI /AD
classiﬁcation and 0.83 and AUC of 0.95 for NC/MCI converters classiﬁcation.
Keywords: Deep Learning; Ensemble; Alzheimer’s Disease classiﬁcation
Introduction
Alzheimer’s Disease (AD) is the most common cause
of dementia among older people and a third of young
people with dementia have AD, aﬀecting 30 million people worldwide. Due to the increasing life expectancy and the ageing of the population in developed nations, it is expected that AD will aﬀect 60
million people worldwide over the next 50 years. It
is a slow neurodegenerative disease associated to the
production of β-amyloid peptide (Aβ) and its extracellular deposition as well as the ﬂame -shaped neuroﬁbrillary tangles of the microtubule binding protein tau.1 This causes the loss of nerve cells, whose
symptoms usually start with mild memory problems,
turning into severe brain damage in several years.
There is no cure for AD, and currently developed
drugs can only help to temporarily slow down the
progression of the disease.2 Thus, early diagnosis becomes the best way to have eﬀective treatments.
Since the AD neurodegeneration process progressively aﬀects diﬀerent brain functions,functional images such as Single Emission Computerized Tomography (SPECT)3–5 or Positron Emission Tomography (PET)6,7 have been extensively used in Computer Aided Diagnosis systems.8 Other works present
diﬀerent techniques that allow to discover alterations
in electroencephalography (EEG) patterns associated to AD9–11 that have been used for automated
diagnosis.9,12–15 For instance, in Ref. 16 and Ref. 17
a probabilistic neural network is used for classiﬁcation between NC and AD by means of conventional
and wavelet coherence-based features extracted from
AD also causes structural changes in the brain
and thus structural diﬀerences between controls and
AD patients can be revealed by analysis of Magnetic Resonance Images (MRI). In fact, MRI has
been used in many previous works for automatic diagnosis.18–21 These works use White Matter (WM)
or Grey Matter (GM) images on whole brain volume
to classify controls and AD images20,21 or to compute Regions of Interest (ROI). Other approaches
deﬁne weak classiﬁers on small enough regions.22,23
Speciﬁcally, Ref. 22 uses an ensemble of sparse representation classiﬁers (SRC) deﬁned on equally-sized
patches extracted from the GM image. By contrast,
Ref. 23 uses an ensemble of Support Vector Machines
(SVM) to classify separately each area deﬁned by
the Automated Anatomical Labelling Atlas (AAL).
Despite showing good classiﬁcation results, both proposals present diﬀerent drawbacks. The former splits
the brain into equally-sized patches, and instead of
computing discriminative features, performs the classiﬁcation directly using voxel values, in a similar way
to using Voxel-as-Features (VAF) method over small
regions, and therefore shares the curse of dimensionality problem. The latter extracts some ﬁrst order
statistics from each brain area to be used as features,
not considering the spatial relationship among voxels. Moreover, these methods use supervised learning
for both, computing the statistical relevance of each
brain region and training the classiﬁer, which could
be a problem whenever not all the training samples
are labelled, or the labels are not reliable enough to
use them as ground truth. This is a relatively common problem in AD labels, as they are assigned from
the Mini Mental State Examination (MMSE) score.
Additionally, there are works that show clear advantages of using a reduced number of discriminative
features, such as eigenbrains-based methods,5 multivariate Gaussian methods,7 codebook based methods18 or SVM-based methods.24 Nevertheless, (e.g.
Ref.24) use a downsized cohort of subjects in the
study, complicating the estimation of the generalization error. Speciﬁcally, we propose the use of deep
learning architectures to extract representative features from each brain area deﬁned by the AAL atlas in an unsupervised manner, avoiding the need
for a ground truth at this stage. We implement and
gev2b˙aortiz˙no˙marks
An ensemble of deep learning architectures for the early diagnosis of the Alzheimer’s Disease
compare here diﬀerent architectures to deﬁne ensembles of Deep Belief Networks (DBN). Each brain area
has been split into small three-dimensional patches
which act as input samples of these DBNs. Diﬀerent
voting schemes to combine the DBNs are analyzed,
and an alternative architecture where a SVM (Support Vector Machine) is used to fuse the DBN outcomes is presented. An important aspect of the latter
is that each unit in the ensemble is responsible not
only of classifying the corresponding patch but also
of extracting representative features for the diﬀerent
brain regions.
The organization of the rest of this paper is as
follows. Section 2 describes the database and the
methods used in this work. In particular, image preprocessing and brain parcellation are explained in
subsection 2.2, while backgrounds in Deep Belief
Networks and Support Vector Classiﬁers (SVC) are
given in Section 2.3 and Section 2.4, respectively.
Section 3 shows details on the experiments performed
and the results obtained using patient data from
the ADNI database. In this Section, classiﬁcation of
NC/AD subjects is performed, but also experiments
involving stable MCI /AD subjects and NC / MCI
converters are addressed to deal with early diagnosis.
Finally, the main conclusions are drawn in Section 4.
Materials and methods
Data used in the preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging
Initiative (ADNI) database (adni.loni.usc.edu). The
ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W.
Weiner, MD. The primary goal of ADNI has been
to test whether serial magnetic resonance imaging
(MRI), positron emission tomography (PET), other
biological markers, and clinical and neuropsychological assessment can be combined to measure the
progression of mild cognitive impairment (MCI) and
early Alzheimer’s disease (AD). For up-to-date information, see www.adni-info.org.
ADNI database collects a vast amount of MRI
and Positron Emission Tomography (PET) images,
as well as blood biomarkers and cerebrospinal ﬂuid
analyses, for three groups of subjects: healthy individuals (Controls, NC), Alzheimer disease patients (AD) and patients suﬀering from mild cognitive impairment symptoms (MCI). The database
that has been used in this work, contains 1075 T1weighted MRI images, comprising 229 NC, 401 MCI
(312 stable MCI and 86 progressive MCI) and 188
AD images. Speciﬁcally, we have used the database
ADNI1:Screening 1.5T (subjects who have a screening data). This database contains MRI data from 818
subjects and repeated scans in some cases. When
multiple scans of the same subject were available,
the ﬁrst one was selected. As a result, 818 MR images were ﬁrst selected for assessing our approach.
However, as our study includes multimodal data
(i.e. MRI and PET images) and PET data are not
available for all patients, we have only selected those
patients having MRI and PET images simultaneously and taken on the same date. This way, 68
NC, 70 AD, 111 MCI and 26 Late MCI (LMCI patients) were selected. Demographic data of patients
in the multimodal database is summarized in Tab. 1.
Demographic data of patients in the database
75.81 ± 4.93
29.06 ± 1.08
76.39 ± 6.96
26.68 ± 2.16
75.33 ± 7.17
22.84 ± 2.91
73.06 ± 7.08
27.27 ± 1.89
In the ADNI2 database, MCI patients are split
into two subclasses: Late MCI (LMCI in Tab. 1) and
Early MCI (MCI in Tab. 1). Details regarding these
groups can be found at Hereafter, we consider MCI patients
and these were taken into account when searching for converters (MCI patients who converted to
AD within 2 years) in the ADNI database. It is
also important to highlight that clinical labels in
ADNI database are assigned according to MMSE
values and are not 100% accurate. In other words,
the presence/absence of AD pathology in controls
and MCI patients is not veriﬁed either with cerebrospinal ﬂuid or amyloid-PET biomarkers. This
could cause that some of the controls subjects are
in the asymptomatic stage of AD, or that neurodey
gev2b˙aortiz˙no˙marks
Andr´es Ortiz, Jorge Munilla, Juan M. G´orriz, Javier Ram´ırez
generation/amyloid load is below the standardized
cut-oﬀvalues in some of the MCI subjects. As a
result, the use of ADNI clinical labels could underestimate the classiﬁcation performance.
It is worth noting that women are exposed to
a higher risk of AD and consequently, AD prevalence is higher in females.25 Gender correction in AD
prediction could improve the prediction of diﬀerent
AD-related biomarkers.25,26 However, our work is focused on classiﬁcation and previous works using MRI
have demonstrated that genders were not signiﬁcant
predictors for the group separation.27–29
Image preprocessing and brain
parcellation
MRI and PET images from the ADNI database have
been spatially normalized according to the PET and
VBM-T1 templates, respectively, ensuring each image voxel correspond to the same anatomical position. After image registration, all the MRI images
from ADNI database were resized to 121x145x121
voxels with voxel-sizes of 1.5 mm (Sagittal) x 1.5
mm (coronal) x 1.5 mm (axial), and PET images
were resized to 79x95x68 voxels with voxel-size of
3 mm (Sagittal) x 3 mm (Coronal) x 3 mm (Axial). Subsequently, MRI and PET images are treated
diﬀerently. MRI images are segmented into White
Matter (WM) and Grey Matter (GM) tissues using
the VBM toolbox for SPM30,31 . This process, which
provides information about GM and WM tissue distributions, is guided by means of tissue probability
maps of GM, WM or cerebro-spinal ﬂuid (CSF). A
nonlinear deformation ﬁeld is estimated that best
overlays the tissue probability maps on the individual subjects’ images. The tissue probability maps
provided by the International Consortium for Brain
Mapping (ICBM) are derived from 452 T1-weighted
scans, which were aligned with an atlas space, corrected for scan inhomogeneities, and classiﬁed into
GM, WM and CSF. The segmentation process produces values in the range , which denotes the
membership probability to a speciﬁc tissue.
PET images, for their part, are also normalized in intensity in order to compute comparable levels among
the images. Since the cerebellum is considered as a
constant activation region,32 intensity normalization
is performed by means of the mean cerebellum activation level, which is used as a normalization value.
More speciﬁcally, the normalization value applied to
each image is calculated as the mean of the 1% of
the voxels with a higher activation level in the cerebellum. This normalization method, which is commonly used in radiology,32,33 helps to homogenize
the activation levels making them comparable by using the same scale. It is important to mention that
studies combining FDG-PET and MRI data tend to
under- and over- estimate the tracer concentration if
partial volume correction (PVC) is not applied.34,35
However, PVC could also introduce an additional error depending on the anatomical position.36 On the
other hand, a recent work by Teipel et al.37 analyzes
the use of PVC in the classiﬁcation methods and concludes that PVC only provides a slight improvement
in the predictive performance of FDG-PET data. As
a consequence, since this work is focused on the classiﬁcation methods, PVC has not been applied.
Voxel preselection
Voxel preselection has been applied to each image
modality separately to remove low signiﬁcance voxels
and reduce the computational burden due to the high
dimensionality of the input space. This feature preselection was performed by means of Welch’s t-test
hypothesis testing separately for each image type.
Welch’s t-test allows to test the diﬀerence between the means of two populations (e.g. NC and
AD) when the variances are unequal, and can be calculated using the following expression
AD are the mean images for NC and
AD respectively, Iσ
AD are the variance images, and NNC, NAD are the number of NC and AD
images respectively. Mean images Iµ
computed as
and variance images Iσ
AD are computed as
It represent the image composed by the t-value provided by Welch’s t-test for each image voxel, which is
gev2b˙aortiz˙no˙marks
An ensemble of deep learning architectures for the early diagnosis of the Alzheimer’s Disease
a signiﬁcance measurement on the means diﬀerence.
Greater t-values correspond to lower p-values, where
p is the probability of observing the given value t, or
one more extreme, by chance if the null hypothesis,
which argues for equal means, is true. Hence, small
values of p lead to reject the null hypothesis. Thus,
depending on the threshold chosen for the p-values,
a diﬀerent number of voxels will be selected. More
speciﬁcally, the lower the threshold for the p-values
the fewer voxels will be selected. In our case, only
those voxels of the training set with p-value ≤0.05
(5% signiﬁcance level) have been selected to build
the ensembles.
Brain parcellation
A key aspect of this work consists in splitting the
brain into patches to be classiﬁed separately, and
therefore we use an atlas deﬁning the diﬀerent brain
regions. In particular, we have used the AAL atlas38
which deﬁnes 116 brain regions corresponding to different neuroanatomical areas.
PET and MRI atlases have been co-registered along
with the PET and MRI images respectively, so that
both atlas and images voxels correspond to the same
neuroanatomical position. This allows us to extract
the brain regions indicated in the atlas from the images, making it possible to process them separately.
Fig. 1 shows MRI and PET example images and the
corresponding atlases.
MRI image (a), MRI atlas (b), (c) PET image
and (d) PET atlas (same slice is shown in MRI and PET
Although all regions deﬁned by the atlas, comprised of brain regions and cerebellum regions, can
be used for classiﬁcation, we have discarded cerebellum regions as these, according to medical literature,32,39,40 do not contain discriminant information for the detection of the AD. Some works discard
even more brain areas (e.g. Ref. 41) neglecting their
inﬂuence in the Alzheimer’s disease, but we have preferred not to assume this and work with all the rest.
This way, as the cerebellum is split into 18 subregions in the AAL atlas, our samples are composed of
voxels belonging to the 98 remaining areas.
Deep Belief Networks
A Deep Belief Network (DBN) can be seen as a neural network composed by multiple hidden layers with
connections between the layers but not between units
within each layer.42 The core idea is not new and it
was already used in multilayer perceptrons or multilayer back-propagation networks. The multilayer architecture tries to mimic the bioinspired model, as
it is believed that human brain organizes the information in a hierarchical fashion, from simpler concepts to more abstract representations along with the
relationships between these layers. As a typical example, visual cortex model is split into four areas:
retina (stores the raw pixels), V1 area (which combines raw pixels and stores edges), V2 area (combining edges to form primitive shape detectors) and V4
area (storing higher level visual abstractions). Nevertheless, the main drawback when using deep architectures in the past stemmed from the training
process. In fact, until 2006, many researchers tried
to train deep architectures unsuccessfully, and as a
result, many of them abandoned the use of multilayer neural architectures in favour of Support Vector Machines (SVM).43 SVMs can be seen as a smart
type of perceptron which uses an optimization technique to compute the weight associated to each feature. This way, SVMs clearly outperformed the multilayer neural networks. However, since 2006, when
some speciﬁc training algorithms were devised,44–46
multilayer neural architectures have become popular again. These algorithms facilitate the construction of deep architectures while trained in an unsupervised context, by using unsupervised networks
such as Restricted Boltzman Machines (RBM)47 or
autoencoders48 as single-layer building blocks. More
speciﬁcally, RBMs are the basic building block of
DBNs, as eﬃcient algorithms have been devised to
train them unsupervisedly and eﬃciently.
A RBM is a speciﬁc type of Markov random ﬁeld
gev2b˙aortiz˙no˙marks
Andr´es Ortiz, Jorge Munilla, Juan M. G´orriz, Javier Ram´ırez
with a two-layer architecture that represents the density of the input data x ∈{0, 1}d (also called visible
units) using binary latent variables h ∈{0, 1}r (also
called hidden units). Its basic architecture is depicted
in Fig. 2, where ωij is the weight between the units
i and j. In RBM, units at one layer are connected
to all the units in the another layer without lateral
connections.
Restricted Boltzman Machine Architecture
However, binary units provide a very poor representation in the case of natural images, as the data
are real-valued. Fortunately, Ref. 49 generalized the
RBMs to exponential family distributions allowing
the use of real-valued data in the DBM learning process. These are the so-called Gaussian RBM. Binary
units are replaced by linear units with independent
Gaussian noise. This way, the probability of the state
of a visible unit can be reconstructed given the state
of the hidden units.
Finally, it is worth noting that the sigmoid or logistic function is used to estimate the activation of each
Unsupervised learning algorithm
A fast unsupervised learning method that starts setting the visible units to a training vector was proposed by Hinton,50 and called Contrastive Divergence (CD). This method updates the weights ωij
by computing the error between the train data and
its reconstruction using the current state of the hidden units, without using data labels. Thus, the core
equation of the weight updating is
∆ωij = η (⟨vihj⟩data −⟨vihj⟩rec)
where η is the learning rate and ⟨vihj⟩rec is the reconstruction error.50 This way, the network trained
on a set of examples learns to probabilistically reconstruct the inputs by a learning process that is
addressed as an iterative minimization problem.
Deep Belief Networks
A Deep Belief Network (DBN) consists of an stack
of RBM layers, which are trained using the greedy
layer-wise algorithm proposed by Hinton et al.,42
which allows to train one RBM layer at a time. The
core idea of the greedy layer-wise algorithm is to start
to train the ﬁrst RBM using the training data, and
continue training higher level RBMs using the current state of the hidden layer at the previous level.
This process, sketched in Fig. 3, learns diﬀerent levels
of features; low-level features are located at the bottom (i.e. visible layer), corresponding to raw data,
while features encoding higher abstraction levels are
hierarchically computed at higher levels of the network.
Deep Belief Network
Following this scheme, RMBs are trained unsupervisedly, as explained in the previous section (using the CD method), to minimize the reconstruction
error of the samples. Once this learning step has ﬁnished, a DBN can be further trained in a supervised
way to perform classiﬁcation by means of backpropagation algorithms.51,52 This allows ﬁne-tuning the
weights of the network in order to improve its discriminative capabilities.
The network devised for this work includes a layer on
the top for the labels when used as a feedforward network. Thus, the constructed network is trained using
the backpropagation algorithm and the gradient descent method, which basically consists in computing
the error for each sample according to the training
labels and then backpropagating it to the ﬁrst hidden
layer. The backpropagation algorithm applied to the
proposed network can be summarized as follows. Let
tk the target (desired) output of unit k, and yk the
actual output. The input layer is ﬁrst feedforwarded
gev2b˙aortiz˙no˙marks
An ensemble of deep learning architectures for the early diagnosis of the Alzheimer’s Disease
layer wise to the output. Thus, the activation of the
neuron k at the output layer is computed using a
sigmoid activation function of the weighted sum:
ωjkhj + θk
where ωjk is the weight of the connection between
units j and k and θk is a bias term.
The total sum of squared error is computed from
the target activation tk as
(tk −yk)2.
Subsequently, the update rule for the weights
between the output layer and the top most hidden
layer can be written as
ωj,k(n + 1) = ωj,k(n) + η · tk · δj
where n is the epoch number, η the learning rate and
δj denotes the backpropagated error, computed as
δj = (tk −yk) · yk(1 −tk),
or as follows in the case that unit j belongs to a
hidden layer:
· yk · (1 −yk)
This process is repeated in subsequent epochs
until ϵ is below a predeﬁned threshold (error tolerance).
Extracting Features using DBN
Although DBNs are usually used for classiﬁcation,
in this work we have also focused on their abilities
as feature extractors. This is addressed by using the
activation of the RBM units at diﬀerent levels as features that represent diﬀerent abstraction levels generated during the training process.
DBNs can be thus used to extract features in
an unsupervised way, due to the unsupervised training algorithms for RBMs. This approach has been
addressed in diﬀerent works such as Ref. 53, where
the Sparse Encoding Symmetric Machine (SESM)
is proposed to produce sparse overcomplete representations of the data. Moreover, unsupervised
feature learning is also addressed in Ref. 54 using
convolutional DBNs to learn feature representations
from unlabeled audio data, showing very good performance for diﬀerent audio classiﬁcation tasks. In
addition, Ref. 55 uses deep autoencoders to extract
features from image and CSF biomarkers as well as
from MMSE data. Alternatively, supervised training can be used to ﬁne-tune the features computed
at each layer by means of backpropagation which
aims to minimize the classiﬁcation error, improving the representation capabilities of the features.
This approach, consisting in a classiﬁcation DBN
with unsupervised pre-training is used in Ref. 56 to
classify audio data, showing that DBN computed
features from raw data performs similarly to MFCCbased features. Nevertheless, Ref. 56 uses the DBN
as a classiﬁer but not to extract features from a
speciﬁc layer. It is worth noting that features produced at diﬀerent layers represent the data at diﬀerent abstraction levels and eventually have diﬀerent
discriminative capabilities. Similarly, Ref. 57 uses
discriminative DBNs for visual data classiﬁcation.
In this work we not only consider the use of a discriminative DBN but also the features generated at
each DBN layer during the training stage, using a
SVM as classiﬁer. The entire learning architecture is
shown in Fig. 4.
Proposed architecture for a discriminative DBN
Support Vector Machines
Support Vector Machines (SVM) are a set of supervised learning methods widely used for classiﬁcation and regression,43,58–60 designed to separate a
gev2b˙aortiz˙no˙marks
Andr´es Ortiz, Jorge Munilla, Juan M. G´orriz, Javier Ram´ırez
set of binary-labelled data by means of a hyperplane.
Speciﬁcally, they use a smart optimization method to
compute the maximal margin hyperplane to achieve
maximum separation between classes, using a decision function in the form h : Rn −→{±1}, corresponding to n-dimensional training vectors and class
labels yi:
(f1, y1), (f2, y2), ..., (fs, ys) ∈Rn × {±1}
in such a way that g is able to correctly classify new
samples (f, y). Linear discriminant functions deﬁne
decision hyperplanes in a multidimensional feature
g(f) = ωT f + υ0
where ω is the weight vector and υ0 is a bias (threshold). This way, ωT f + υ0 ≥1 if class yi = +1 and
ωT f+υ0 ≤1 if class yi = −1, being the weight vector
ω orthogonal to the decision hyperplane. The optimization task ﬁnds the unknown parameters ω and
υ0 which deﬁne the decision hyperplane that separates the two classes optimally.
Additionally, a measure of the relative importance of each feature can be computed. In fact, let
Ns be the number of support vectors within the margin chosen during the training phase, the following
vector can be computed:
where yj are the labels, λj are the corresponding Lagrangian parameters, which are also optimized during the training phase, and fj the training samples.
The coordinate i of the vector W, Wi with 1 ≤i ≤n,
informs us about the relevance of the i-th dimension
of the feature vectors.61 More precisely, the higher
the |Wi|, the more the relevance of the i-th dimension in the feature vectors. By contrast, |Wi| = 0
indicates that the i-th feature does not have any in-
ﬂuence in the classiﬁcation process.
Ensemble of Deep Learning
Architectures
A combination of weak classiﬁers is generally considered to be more accurate than individual classi-
ﬁers.22,62 When the dimension of the feature space
is high, the use of weak classiﬁers fed with a reduced
number of features each can also help to avoid the
curse of dimensionality problem63,64 . Thus, the use
of weak classiﬁers have been previously used to leverage the performance in MRI classiﬁcation problems.
For instance, in Ref. 22, weak Sparse Representation Classiﬁers (SRC) are deﬁned using randomly
extracted 3D patches from GM MRI images. Then,
these classiﬁers are combined following a classical
rule based on the SRC residuals. In Ref. 65 an ensemble of SVM classiﬁers is used over all the brain
ROI deﬁned by an atlas. However, it is very important that weak classiﬁers are properly combined to
take full advantage of the ensemble: diﬀerent methods may be possible that can be more or less accurate depending on the speciﬁc individual classiﬁers
and their decision boundaries.66 In this work we de-
ﬁne an individual DBN for each brain region. Thus,
although DBNs cannot be considered as weak classiﬁers, the ensemble of DBNs aims to combine the
expertise of individual good classiﬁers, but specialized in separate domains (i.e. diﬀerent brain areas).
One of the most popular techniques to combine the
classiﬁers in order to compose the ensemble is majority voting. Nevertheless, this method does not weight
the individual decision of each classiﬁer, considering
all of them equally relevant in the ﬁnal prediction.
A more elaborated combination technique consists
in computing a relevance measure associated to each
classiﬁer in order to weight the individual decisions.
A similar method is used in Ref.22, where the residuals are averaged so that SRCs providing higher residuals have a lower weight in the ﬁnal decision.
A diﬀerent technique to combine the classiﬁers
consists in using a new classiﬁer which is fed with
the outputs of individual classiﬁers67 . For instance,
when using a SVM to fuse all the classiﬁers, the supervised optimization process executed on the training samples will compute the weights that determine
the relative importance of each classiﬁer in the ﬁnal
decision68,69 .
In this work two DBN-based classiﬁcation methods have been implemented and compared. The ﬁrst
of them is analyzed for four diﬀerent voting schemes.
These schemes are described next.
gev2b˙aortiz˙no˙marks
An ensemble of deep learning architectures for the early diagnosis of the Alzheimer’s Disease
Ensemble of DBN classiﬁers and voting stage. In this case, individual DBN are used as weak classiﬁers deﬁned
for each brain region: a) training Phase, b) classiﬁcation phase. The voter block implements one of the voting mechanisms
described in the text.
Ensemble of DBN classiﬁers plus
voting scheme (DBN-voting)
The ﬁrst of the DBN-based classiﬁcation method
consists of an ensemble of discriminative DBNs, in
which the top layer is composed of two neurons
(for the binary classiﬁcation problem treated here)
in combination with a voting scheme. Four diﬀerent
voting schemes have been compared:
• Majority Voting (MV). The classiﬁcation outcomes from each region are summed up, so that
each region contributes with one vote for a speciﬁc subject. The ﬁnal prediction is determined as
the class with the higher number of votes.
• Weighted Voting (WV). We devised a method trying to circumvent the problem that arises in majority voting, due to the fact that not all regions
have the same relevance in terms of their discriminative power. This way, a two-sample Welch’s test
is used to rank the voxels in each region by means
of the p-value derived from the hypothesis test.
Consequently, the number of voxels in each region
with p-value < 0.01 (corresponding to 1% of signiﬁcance level) is computed, ranking the i-region
using the score:
SCi = #voxelsp
i (PET) + #voxelsp
where #voxelsp
i (PET) and #voxelsp
i (GM) correspond to the number of voxels in the region i with
p-value < 0.01 for PET and MRI-GM images, respectively. The scores computed by this method
are used to weight the votes from each region. Latter, the weighted votes of each region are summed
up and the ﬁnal prediction is determined as the
gev2b˙aortiz˙no˙marks
Andr´es Ortiz, Jorge Munilla, Juan M. G´orriz, Javier Ram´ırez
class with the highest overall score. If the p-value
used increases, more voxels are included for the
computation of the weights and the scheme approaches to the majority voting.
• Classiﬁers fusion using SVM. This method fuses
the elements of the ensemble through a SVM. This
way, a SVM is trained with the predictions of each
DBN. The support vector weights generated during the training of the SVM will determine the
most relevant DBNs and indirectly weight the decision of each individual classiﬁer.
• Classiﬁers fusion using a DBN. In this case, a discriminative DBN is trained with the classiﬁcation
outcomes from the individual classiﬁers of the ensemble.
Fig. 5 depicts the block diagram of the ensemble
of DBNs. The voter block represents one of the voting mechanisms described above. It is worth noting
that training samples are only required in the voter
block for weighted voting, as this weights each vote
by means of the discriminative power of the corresponding region, computed by the Welch’s test on the
training samples. Speciﬁc details on the implementation of the weighted voting scheme are provided in
Section 3.2.
Extracting features using DBN
(FEDBN-SVM)
In our second approach we take a further step in order to leverage the classiﬁcation results provided by
the ensemble of DBN classiﬁers, and we use a diﬀerent implementation in which DBNs are not used as
weak classiﬁes but as weak feature extractors. Henceforth, we will denote this architecture as FEDBN,
while the term DBN will be used to refer to the former one.
In FEDBN, voxels extracted from each region are
used as training samples for a DBN composing a
DBN-per-region structure. The activations of the
neurons in a hidden layer are then computed and
used as features. Finally, the features extracted from
each region are concatenated into a unique feature
vector to train a SVM. This way, the SVM will compute the relative relevance of each feature during the
training stage and avoid the need of the voting phase.
The use of a DBN as a feature extractor is based on
the idea that it generates a diﬀerent model in each
hidden layer, encoding the sample features in a different number of new features corresponding to the
activations of the neurons in each hidden layer. As
previously explained, hidden layers in DBNs represent features at diﬀerent abstraction levels in such
a way that higher levels in the network represent
higher levels of abstraction (see Fig. 6). However,
there is no a priory way to determine the discriminative capability of the features generated at each
layer, and the one providing the best performances
has to be determined by testing.
Feature extraction from diﬀerent levels of each
Fig. 7 shows the block diagram of the proposed
ensemble of DBNs, each of them extracting features
from a diﬀerent brain region, according to the regions
deﬁned by the AAL atlas, and fused by a SVM.
Results and discussion
In this section, classiﬁcation outcomes using the proposed classiﬁcation methods are shown. These results
have been also compared with those obtained using
other methods. Classiﬁcation results are assessed
by k-fold (k=10) cross-validation, namely stratiﬁed
cross validation, ensuring that each fold has roughly
equal size and roughly the same class proportions
as in the data manifold. To avoid double dipping,
training and testing subsets are disjoint sets and
thus they do not share any sample. This process is
repeated for the 10 folds and the results provided
here are computed as the average of 10 evaluations
throughout 10 folds. The main purpose of crossvalidation is to estimate the generalization error,
ensuring that similar results will be obtained on new
data (i.e. low generalization error). In practice, this
gev2b˙aortiz˙no˙marks
An ensemble of deep learning architectures for the early diagnosis of the Alzheimer’s Disease
Block diagram of the proposed ensemble of DBNs for extracting features and classiﬁcation. In this case, each
individual DBN extract features from a speciﬁc brain region: a) training Phase, b) classiﬁcation phase.
error will always result in an overestimate of the true
prediction error, since the models obtained during
the training phase are not computed using all the
training set but k −1 folds. This overestimation will
depend on the slope of the learning curve of the
classiﬁer and reduces when k increases. Thus, the
leave-one-out cross-validation (k = N, with N the
number of available samples) has the lowest bias but
can have high variance because the training sets are
so similar to one another. Overall, ﬁve- or tenfold
cross-validation are recommended as a good compromise.
The ﬁrst experiments, used also to validate and compare diﬀerent conﬁgurations sets, have consisted in
classifying between Controls and AD patients. Then,
we have addressed the much more challenging case
that involves the classiﬁcation of mild cognitive impairment patients (MCI).70 In particular, we have
performed classiﬁcation experiments between stable
MCI (MCIs) and AD patients, and between controls and MCI converters (MCIc). MCI converters
are patients who were diagnosed as MCI but ﬁnally
converted to AD in the term of 2 years, while Stable
MCI are those who remain MCI after this period.
The latter case, involving MCIc, deals directly with
early AD diagnosis, which constitutes the most relevant issue in AD diagnosis due to its importance in
the treatment success.
This section compares the diﬀerent classiﬁcation
methods that have been implemented to determine
that which provides the best results.
Parameter set analysis for DBN
The performances of the DBN-based classiﬁcation
methods will obviously depend on the performances
gev2b˙aortiz˙no˙marks
Andr´es Ortiz, Jorge Munilla, Juan M. G´orriz, Javier Ram´ırez
of the DBNs. These, in turn, will depend on the used
parameter conﬁguration. It is well-known that the
number of hidden layers and the number of neurons
have a direct inﬂuence on the representation capabilities and the convergence of the network. Thus, to
increase the conﬁdence in our comparisons, we have
carried out a previous analysis to determine some
optimal values for the number of layers and hidden
neurons of the deep learning networks.
Increasing the number of hidden layers implies
more epochs to converge and consequently a higher
training time48,51,71 . It has been also shown that,
in practice, structures composed of more than 3 hidden layers slightly increases the performance of the
network. Therefore, for this work, we have assumed
networks with 3 hidden layers.
Although a rough range of possible values could
be inferred from the characteristics of the input, the
speciﬁc number of neurons in the hidden layer has to
be determined by testing. To limit the number of possible solutions and since our objective is to compare
the representation capabilities at each layer, corresponding to diﬀerent abstraction levels for the same
number of features, we assume here that the three
layers are equally sized. Fig. 8 graphs the classiﬁcation accuracies for the DBN-SVM (SVM as voting
mechanisms) and FEDBN-SVM architectures where
diﬀerent numbers of neurons are used. It seems that
400 hidden neurons per layer provide the best results. Consequently, networks with 400 units at each
hidden layers are considered hereafter in the experiments.
The contrastive divergence method used to train
each RBM requires a certain number of iterations to
converge. This number of iterations has to be chosen for a trade-oﬀbetween representation error and
computing time while avoiding over-ﬁtting the data,
which would decrease the generalization capabilities
of the network. Moreover, the supervised training
used to ﬁne-tune the weights by back-propagation
has to be also stopped before over-ﬁtting occurs.72 In
the experiments performed, we trained the network
across 20 and 200 epochs in the unsupervised (RBM
training) and supervised (backpropagation) stages.
Experiments conducted using 10 and 100 epochs (for
the unsupervised and supervised part, respectively)
provided lower accuracy values, while 30 and 300 iterations respectively, do not improve the accuracy.
As a conclusion, using more than 20 and 200 iterations could tend to over-ﬁt the data. Additionally,
the learning rates in both unsupervised and supervised stages control the portion of weight updating
during training. These were also tuned by experimentation and we eventually used the values 0.1 and
0.01 for the unsupervised and supervised phases, respectively. In general, lower learning rates tend to
increase the learning rate and the network could be
stuck in a local minima. On the contrary, higher
values speed up the training process but the network may not converge. Experiments performed using 0.01 and 0.001 for the unsupervised and supervised stages respectively, slowed down the training
stage and slightly decreased the classiﬁcation accuracy. Higher values tested (0.5 and 0.1) decreased
considerably the accuracy.
Accuracy obtained for diﬀerent number of units
in the hidden layer for both implementations, DBN-
SVM and FEDBN-SVM. The architecture used consists
of three equally sized hidden layers
In Table 2 we summarize the parameters used
to train the i-th component (DBN) of the ensemble (1 ≤i ≤98), corresponding to the i-th region,
including the learning rates used during the unsupervised (RBM training) and supervised (backpropagation) phases. Note that the only diﬀerence between
the diﬀerent DBNs is the number of neurons of the
input layer since this corresponds to the number of
preselected voxels of each region.
Additionally, when using the DBN as a feature
extractor (i.e. FEDBN-SVM), it is also important to
determine the layer providing the most discriminative features. As there is no way to know the discrimy
gev2b˙aortiz˙no˙marks
An ensemble of deep learning architectures for the early diagnosis of the Alzheimer’s Disease
inative power of the features at diﬀerent levels a priori, diﬀerent experiments were conducted to evaluate
these features. With this aim, diﬀerent tests have
been carried out to compare the classiﬁcation accuracies when features are extracted from the diﬀerent
hidden layers: L1, L2 or L3. The results are collected
in Tab. 3.
DBN Parameters. voxelsi refers to voxels of region
# hidden layers
# neurons per hidden layer
DBNi Structure
voxelsi-400-400-400-2
Unsupervised training epochs
Supervised training epochs
Unsupervised learning rate
Backpropagation learning rate
Although L1, L2 and L3 seem to provide representative features, we consider the use of L2 features as it provides the best performance in terms of
AUC metric, measured for a 5% of signiﬁcance level,
which measures the robustness of the classiﬁer taking
into account not only the accuracy but also sensitivity and speciﬁcity. Consequently, features extracted
from hidden layer 2 are used hereafter.
Accuracy, Sensitivity and Speciﬁcity obtained with
corresponding
NC/AD classiﬁcation with FEDBN-SVM. AUC values are
signiﬁcance
Sensitivity
Speciﬁcity
0.88 ± 0.08
0.84 ± 0.10
0.94 ± 0.17
0.90 ± 0.09
0.86 ± 0.12
0.94 ± 0.10
0.87 ± 0.09
0.79 ± 0.17
0.96 ± 0.10
Voting Methods comparison
This section compares the voting methods described
in Section 2.5. Experiments combining the architecture DBN with the four diﬀerent voting schemes are
carried out. Table 4 shows the results of these experiments. As previously explained, the DBN voter case
consists in fusing the decisions given by the individual classiﬁers67 composing the ensemble by means
of another DBN. In this case, by experimentation, a
98-100-100-100-2 network structure was selected.
Comparison
classiﬁcation
classiﬁers.
signiﬁcance
Voting Method
Majority Voting (MV)
Weighted voting (WV)
DBN voter (DBN-DBN)
SVM voter (DBN-SVM)
Statistical signiﬁcance test of the results aiming to state the best-performing method is addressed
by ANOVA73 analysis using the accuracy values.
This revealed that null hypothesis (H0) can be rejected, which means that at least one group mean
diﬀers from the rest. A multiple comparison test was
eventually performed to identify these diﬀerences by
means of the 95% conﬁdence intervals. Consequently,
DBN-SVM method outperforms the MV and DBN
methods (conﬁdence intervals of [-0.17, -0.06] and [-
0.11,-0.02], respectively). At the same time, the superiority of the DBN-SVM method over the WV
method cannot be statistically assessed at 5% of signiﬁcance (conﬁdence interval of [-0.10,-0.01]). However, as SVM voting method provides a higher AUC
computed for 5% of signiﬁcance level, it can be regarded as superior to the other methods.
Classiﬁcation Experiments
Once the best-performing architectures have been
determined, we have carried out the previously described classiﬁcation experiments between the different subject groups. Tab. 5 shows the classiﬁcation outcomes obtained for diﬀerent classiﬁcation approaches for NC/AD classiﬁcation. In the ﬁrst three
rows, the Voxel as Features (VAF) method,74 which
considers individual voxels as diﬀerent features, is
shown. For VAF experiments we used a linear SVM
trained with GM, PET and GM+PET data. Although SVMs are less prone to suﬀer from this issue than other classiﬁers,43 the main drawback of
this method is the curse of dimensionality problem.
Thus, dimensionality of raw data used in VAF experiments was reduced by means of Principal Comy
gev2b˙aortiz˙no˙marks
Andr´es Ortiz, Jorge Munilla, Juan M. G´orriz, Javier Ram´ırez
ponent Analysis (PCA)75,76 to 10 principal components (under PCA PET+GM row in Tab. 5). Furthermore, an ensemble of linear SVMs, labelled as
SVM-e, is included for comparison. In SVM-e each
SVM acts as a weak classiﬁer being trained with voxels from one brain region, and then, a new linear
SVM is trained using the outputs of each individual
SVM composing the ensemble to deliver the ﬁnal decision. Finally, the last two rows correspond to the
proposed DBN-based classiﬁcation architectures de-
ﬁned in the previous sections. That is, DBN with the
four diﬀerent voting mechanisms, and FEDBN-SVM,
where DBNs are not used as classiﬁers but as feature extractors from each brain region, which allows
deﬁning a new space composed of the concatenated
features extracted by each DBN. These are then used
to train a linear SVM.
Classiﬁcation performance is assessed by measuring the accuracy, sensitivity and speciﬁcity for
each method as this is a widely accepted method
to evaluate the classiﬁcation performance66,77,78 and
to estimate the generalizaton error.66 Moreover, the
ROC curve which graphically shows the ability to
discriminate between diﬀerent classes79 is also provided along with the AUC (Area Under Roc Curve)
metric measured for a 5% of signiﬁcance level, which
can be deﬁned as the probability of the classiﬁer to
rank a randomly chosen positive sample higher than
a randomly chosen negative sample.79,80 Thus, AUC
values fall in the range, where 1 indicates perfect discrimination between classes, 0.5 indicates no
ability to discriminate (random classiﬁer) and 0 indicates that negative data are always ranked higher
than positive data.66,78,79
In addition, the Receiver Operating Curve
(ROC)79 computed for the best-performing classi-
ﬁcation alternatives are also shown in Figure 9,
providing AUC values of 0.95, 0.94, 0.94 and 0.79
for FEDBN-SVM, SVM-e, DBN-SVM and VAF
PET+GM methods, respectively. AUC values in the
Tab. 5 are computed for a signiﬁcance level of 5%
(p < 0.05) in all cases. Thus, it can be observed that
FEDBN-SVM shows best results that the rest of classiﬁcation approaches.
Ranking ROIs
The proposed methods where a SVM is used to compose the ensemble, also allow us to rank atlas ROIs
according to their relative discriminant capabilities.
In fact, as explained in Section 2.4, a weighted sum
vector W of the Ns support vectors can be computed. This vector gives us information about the
relative importance of each feature.
ROC curves for diﬀerent NC/AD classiﬁcation
For the FEDBN-SVM, the selected architecture
consists of 400 units at each RBM layer, and therefore 400 features are extracted for each region (including MRI and PET information). The SVM which
fuses the features generated in each DBN to compose
the ensemble is thus fed with 400*98 features. The
SVM then computes 400*98 weights, and the weight
of the k-th region can be deﬁned as the sum of all the
weights of the features corresponding to that region:
where wk,i is the SVM weight corresponding to the
activation of the neuron i in the region k. These
values can be normalized by dividing them by the
maximum W k
max for 1 ≤k ≤98, so that all the
values are in the range .
For NC/AD classiﬁcation, Figure 10 and 11 illustrate the computed relative importance of the
ROIs in the axial and coronal planes, and the most
discriminative brain regions, respectively. These selected regions are according to the medical bibliography.
ws-ijns13tex˙R7-jorgev2b˙aortiz˙no˙marks
An ensemble of deep learning architectures for the early diagnosis of the Alzheimer’s Disease
Accuracy, Sensitivity, Speciﬁcity and Area Under ROC Curve (AUC) for diﬀerent classiﬁcation
methods. These results correspond to NC/AD classiﬁcation. AUC values are computed for a signiﬁcance level
of 5% (p < 0.05).
Sensitivity
Speciﬁcity
0.85 ± 0.09
0.89 ± 0.13
0.81 ± 0.12
0.82 ± 0.12
0.82 ± 0.18
0.81 ± 0.14
VAF PET+GM
0.86 ± 0.11
0.85 ± 0.13
0.87 ± 0.16
PCA PET+GM
0.87 ± 0.10
0.85 ± 0.15
0.90 ± 0.10
0.88 ± 0.08
0.84 ± 0.13
0.92 ± 0.11
0.84 ± 0.07
0.80 ± 0.14
0.88 ± 0.09
0.87 ± 0.09
0.84 ± 0.16
0.90 ± 0.12
0.88 ± 0.08
0.87 ± 0.14
0.90 ± 0.12
0.78 ± 0.05
0.99 ± 0.05
0.57 ± 0.16
FEDBN-SVM L2
0.90 ± 0.09
0.86 ± 0.12
0.94 ± 0.10
ROIs computed for NC/AD in the axial (a) and coronal (b) planes. Relative importance is shown in the colorbar
(red colour indicates the most discriminative regions).
Classiﬁcation of MCI subjects
In this section, we address the more complex problem
of MCI/AD classiﬁcation. As previously explained,
MCI can be considered as an intermediate state between controls and AD patients, and not all MCI
subjects have to develop AD necessarily. Those MCI
whose diagnostic changed to AD, according to the
MMSE value, within the next two years after being diagnosed as MCI are labelled as MCI converters. Otherwise, they are considered stable MCI. The
diﬀerences, functional and structural, between the
groups are here much more subtle.
MCIs/AD Classiﬁcation
We ﬁrst tackle the MCIs/AD classiﬁcation issue.
This is possible since ADNI database provides information to identify these patients in the MCI cohort.
Indeed, Tab. 6 shows the demographic data of the
MCI patients in the database when MCIs and MCIc
subjects are diﬀerentiated.
gev2b˙aortiz˙no˙marks
Andr´es Ortiz, Jorge Munilla, Juan M. G´orriz, Javier Ram´ırez
Most discriminative brain regions for NC/AD classiﬁcation, identiﬁed in axial (a), coronal (b) and sagittal
(c) planes. Sorted by discriminative capability, (38) Right Hippocampus, (90) Right Inferior temporal Gyrus, (37) Left
Hippocampus, (85) Left Middle Temporal gyrus, (86) Right Middle Temporal gyrus, (89) Inferior Temporal gyrus, (40)
Right ParaHippocampal gyrus, (39) Left ParaHippocampal gyrus, (42) Right Amygdala, (41) Left Amygdala.
Demographic data of MCIs and MCIc in the
76.46 ± 6.56
14.73 ± 12.58
77.02 ± 7.06
17.05 ± 12.41
We apply the the same classiﬁcation and analysis methods that those described for NC/AD subjects. Tab. 7 shows the classiﬁcation performances
obtained for the diﬀerent classiﬁcation approaches.
The results obtained shows that using the raw voxels
as features (VAF approach) and a unique classiﬁer is
not enough to diﬀerentiate between MCIs and AD
subjects due to the subtle diﬀerences between them.
Nevertheless, alternatives using ensembles of classi-
ﬁers clearly outperform the VAF approach. Specifically, the ensemble of SVMs provides similar performances to those provided by the FEDBN-SVM
In the same way as for NC/AD, Fig. 12 shows
the ROC curves corresponding to the best performing approaches, i.e. FEDBN-SVM, SVM-e, DBN-
SVM and VAF PET+GM, according to the AUC
value computed from the ROC curve for a 5% of signiﬁcance level (p < 0.05).
ROIs computed for the case of MCIs/AD classiﬁcation are shown in Fig. 13. Diﬀerent layers in
the axial and coronal planes are depicted to indicate the most discriminative regions. More in particular, the top ten most discriminative regions correspond to (according to the AAL atlas notation):
Left Angular Gyrus (65), Right Angular Gyrus (66),
Posterior Cingulate Gyrus (35), Left Amygdala (41),
Left Hippocampus (37), Right Hippocampus (38),
Parahippocampal gyrus (39), Left Inferior Parietal,
but supramarginal and Angular Gyri (61), Right
Posterior Cingulate Gyrus (36), Left Precuneus (67).
Structure or functionality associated to these regions
appear in medical literature to be aﬀected in diﬀerent stages of the AD development.32,39,40 Note that
ROIs appear at diﬀerent layers, making it diﬃcult to
provide a ﬁgure similar to Fig. 11.
ROC curves for diﬀerent MCIs/AD classiﬁcation methods.
NC/MCIc Classiﬁcation: early AD
A further step towards early AD diagnosis involves
diﬀerentiating between controls and patients who
converted to AD (MCIc) in subsequent evaluations.
For NC/MCIc classiﬁcation, Tab. 8 collects the
classiﬁcation results obtained for the diﬀerent apy
gev2b˙aortiz˙no˙marks
An ensemble of deep learning architectures for the early diagnosis of the Alzheimer’s Disease
Accuracy, Sensitivity, Speciﬁcity and Area Under ROC Curve (AUC) for diﬀerent classiﬁcation
methods. These results correspond to stable MCIs/AD classiﬁcation. AUC values are computed for a significance level of 5% (p < 0.05).
Sensitivity
Speciﬁcity
0.65 ± 0.13
0.67 ± 0.17
0.63 ± 0.17
0.55 ± 0.08
0.53 ± 0.14
0.57 ± 0.13
VAF PET+GM
0.66 ± 0.11
0.64 ± 0.19
0.69 ± 0.13
PCA PET+GM
0.70 ± 0.09
0.72 ± 0.11
0.69 ± 0.15
0.84 ± 0.10
0.80± 0.15
0.88 ± 0.14
0.84 ± 0.12
0.83 ± 0.18
0.86 ± 0.13
0.84 ± 0.10
0.82 ± 0.15
0.85 ± 0.15
0.86 ± 0.08
0.90 ± 0.12
0.81 ± 0.14
0.69 ± 0.12
1.00 ± 0.05
0.35 ± 0.20
FEDBN-SVM L2
0.84 ± 0.09
0.79 ± 0.12
0.89 ± 0.12
ROIs computed for MCIs/AD in the axial (a) and coronal (b) planes. Relative importance is shown in the
colorbar (red colour indicates the most discriminative regions). These regions include Left Angular Gyrus (65), Right Angular Gyrus (66), Posterior Cingulate Gyrus (35), Left Amygdala (41), Left Hippocampus (37), Right Hippocampus (38),
Parahippocampal gyrus (39), Left Inferior Parietal, bu supramarginal and Angular Gyri (61), Right Posterior Cingulate
Gyrus (36), Left Precuneus (67) as most discriminative
proaches, while Fig. 14 shows the ROC curve for
the best performing ones. The top ten most discriminative ROIs in this case are: Left Amygdala (41),
Right Parahippocampal gyrus (40), Right Amygdala
(42), Right Hippocampus (38), Right Angular Gyrus
(66), Left Hippocampus (37), Left Parahippocampal
Gyrus (39), Right Gyrus Rectus (28), Left Temporal
Pole: middle Temporal Gyrus (87), Middle Temporal
Gyrus (86). In general, as previously commented, information for AD diagnosis is available in GM due to
the atrophy produced by GM shrinkage at the time
brain ventricles grow larger. By contrast, only mild
brain changes are present in MCI patients and therefore most information is contained in PET data.
In this case, the superiority of the FEDBN-SVM
method can be stated according to the AUC value
computed for 5% of signiﬁcance level, showing that
FEDBN-SVM classiﬁer is more robust than the other
approaches, as it provides the highest AUC value.
Comparison with other published
alternatives
Finally, this section compares the FEDBN-SVM with
other classiﬁcation methods published in the litery
gev2b˙aortiz˙no˙marks
Andr´es Ortiz, Jorge Munilla, Juan M. G´orriz, Javier Ram´ırez
Accuracy, Sensitivity, Speciﬁcity and Area Under ROC Curve (AUC) for diﬀerent classiﬁcation
methods. These results correspond to NC/MCI converter classiﬁcation. AUC values are computed for a
signiﬁcance level of 5% (p < 0.05).
Sensitivity
Speciﬁcity
0.72 ± 0.14
0.79 ± 0.18
0.62 ± 0.21
0.63 ± 0.15
0.83 ± 0.18
0.35 ± 0.17
VAF PET+GM
0.71 ± 0.13
0.86 ± 0.18
0.48 ± 0.13
PCA PET+GM
0.71 ± 0.19
0.68 ± 0.19
0.75 ± 0.24
0.83 ± 0.07
0.81 ± 0.13
0.85 ± 0.12
0.83 ± 0.11
0.66 ± 0.23
0.95 ± 0.09
0.82 ± 0.10
0.60 ± 0.15
0.90 ± 0.15
0.85 ± 0.14
0.69 ± 0.28
0.96 ± 0.08
0.73 ± 0.12
0.95 ± 0.05
0.55 ± 0.21
FEDBN-SVM L2
0.83 ± 0.14
0.67 ± 0.26
0.95 ± 0.09
ature. Although these comparisons are always arguable as a means to identify the best option, since
it is almost impossible to replicate the same initial
conditions (i.e. input data), they can help to determine if our proposal is consistent with the state of
the art. With this aim, Tab. 10 compares the performances of FEDBN-SVM with others reported in the
bibliography. According to these data, FEDBN-SVM
outperforms slightly previous results, which, at least,
indicates that this proposal must be considered as a
serious classiﬁcation alternative. In addition, works
such as Ref. 55 propose the use of deep autoencoders
to learn features from image, CSF biomarkers and
MMSE data. However, it is worth noting that using
MMSE results along with the labels could boost the
classiﬁcation performance, as ADNI labels are based
on these MMSE values. On the contrary, our proposal tries to exploit all the information contained in
the image data by computing speciﬁc features from
each brain region by individual DBN-based feature
extractors that are eventually combined in an ensemble.
On the other hand, we show results obtained using the FEDBN-SVM method when classifying between MCIs and MCIc. At the same time, we also
provide CN vs. MCIs classiﬁcation results for completeness. The classiﬁcation performances using all
the groups exposed in this work are summarized in
ROC curves for diﬀerent NC/MCIc classiﬁcation methods.
Conclusions
Comment 8.1
This paper presents a method for AD and early
AD diagnosis by fusing functional and structural
imaging data based on the use of the Deep Learning
paradigm, and more speciﬁcally, Deep Belief Networks (DBN). A set of DBNs is trained using data
from each brain region, according to the AAL atlas,
composing an ensemble of DBNs. This concept is
used to implement and compare two diﬀerent DBNbased alternatives: DBN-voter and FEDBN-SVM.
The ﬁrst consists in the use of an ensemble of DBNs
classiﬁers, while the latter is based on the use of
DBNs as feature extractors, making use of their capability of representing the information at diﬀerent
abstraction layers.
Four diﬀerent methods to fuse the decisions of
gev2b˙aortiz˙no˙marks
An ensemble of deep learning architectures for the early diagnosis of the Alzheimer’s Disease
Classiﬁcation performance of FEDBN-SVM approach for diﬀerent groups
Subjects (NC/AD)
0.90 ± 0.09
0.83 ± 0.14
0.67 ± 0.26
0.95 ± 0.09
0.80 ± 0.12
0.60 ± 0.20
0.90 ± 0.10
0.84 ± 0.10
0.79 ± 0.12
0.89 ± 0.12
MCIs / MCIc
0.78 ± 0.10
0.61 ± 0.15
0.88 ± 0.13
Comparison of NC / AD classiﬁcation results reported in the
literature using MRI image data from the ADNI database
Subjects (NC/AD)
VAF(GM)/(LP) Boosting81
VAF(GM)/SVM21
93 ROI (GM)82
VAF(GM)/SRC-ensemble22
ROIs computed for NC/MCIc in the axial (a) and coronal (b) planes. Relative importance is shown in the
colorbar (red colour indicates the most discriminative regions).
the individual classiﬁers in the DBN method have
been analyzed, and our experiments showed that the
best results are obtained with the SVM voter; i.e.
DBN-SVM. The best classiﬁcation outcomes have
been, however, obtained using DBNs as feature extractors; i.e. with FEDBN-SVM. It provides higher
classiﬁcation performances in terms of AUC than
using discriminative DBNs as classiﬁers. In order to
compare it with other ensemble alternatives, diﬀerent options have been implemented. For the diﬀerent
classiﬁcation experiments, FEDBN-SVM proposal
outperforms the VAF technique and the results obtained using PCA to reduce the feature space, and
oﬀers similar performances to those provided by an
ensemble of linear SVMs (SVM-e).
Classiﬁcation experiments using diﬀerent groups
of subjects have been carried out. Firstly, experiments using the FEDBN-SVM between Controls
and AD patients reported an accuracy of 0.90±0.09
gev2b˙aortiz˙no˙marks
Andr´es Ortiz, Jorge Munilla, Juan M. G´orriz, Javier Ram´ırez
and an AUC of 0.95. The proposed classiﬁcation approach also allowed devising a method to determine
the most discriminative ROIs, by using the SVM
weights computed during the optimization process
that deﬁned the hyperplane; regions associated to
AD such as the Hippocampus, the Temporal Gyrus
and the Parahippocampal gyrus are pointed out as
discriminative by our method, which is according
to the medical literature. Next, taking advantage
of the possibilities of the ADNI database to identify MCIs and MCIc, regions allowing to diﬀerentiate between stable MCIs and AD patients, such as
Angular gyrus, Posterior cingulate gyrus, Parahippocampal gyrus and the Hippocampus, were also determined. Finally, classiﬁcation experiments between
NC and MCIc (early AD diagnosis) were performed.
The classiﬁcation outcome in this case reported accuracy of 0.84±0.14 and AUC of 0.95. Two facts
were corroborated. First, the classiﬁcation performance for NC/MCIc is higher than for MCIs/AD,
and second, the regions involved in early AD diagnosis (NC/MCIc case) include regions computed as
discriminative for NC/AD but with diﬀerent relative
importance.
Acknowledgments
This work was partly supported by the MICINN under the projects TEC2012-34306 and PSI2015-65848-
R, and the Consejer´ıa de Innovaci´on, Ciencia y Empresa (Junta de Andaluc´ıa, Spain) under the Excellence Projects P09-TIC-4530, P11-TIC-7103 and
the Universidad de M´alaga. Programa de fortalecimiento de las capacidades de I+D+I en las Universidades 2014-2015, de la Consejer´ıa de Econom´ıa,
Innovaci´on, Ciencia y Empleo, coﬁnanciado por el
fondo europeo de desarrollo regional (FEDER) under the project FC14-SAF30.
Data collection and sharing for this project was
funded by the Alzheimer’s Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant
U01 AG024904) and DOD ADNI (Department of
Defense award number W81XWH-12-2-0012). ADNI
is funded by the National Institute on Aging, the
National Institute of Biomedical Imaging and Bioengineering, and through generous contributions
from the following: AbbVie, Alzheimer’s Association; Alzheimer’s Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers
Squibb Company; CereSpir, Inc.; Eisai Inc.; Elan
Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoﬀmann-La Roche Ltd and its aﬃliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research & Development, LLC.;Johnson &
Johnson Pharmaceutical Research & Development
LLC.; Lumosity ; Lundbeck; Merck & Co., Inc.;
Meso Scale Diagnostics, LLC.; NeuroRx Research;
Neurotrack Technologies; Novartis Pharmaceuticals
Corporation; Pﬁzer Inc.; Piramal Imaging; Servier;
Takeda Pharmaceutical Company; and Transition
Therapeutics. The Canadian Institutes of Health Research is providing funds to support ADNI clinical
sites in Canada. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated
by the Alzheimer’s Disease Cooperative Study at the
University of California, San Diego. ADNI data are
disseminated by the Laboratory for Neuro Imaging
at the University of Southern California.