Learning and Generalization in Overparameterized Neural
Networks, Going Beyond Two Layers
Zeyuan Allen-Zhu
 
Microsoft Research AI
Yuanzhi Li
 
Stanford University
Yingyu Liang
 
University of Wisconsin-Madison
November 12, 2018
(version 6)∗
The fundamental learning theory behind neural networks remains largely open. What classes
of functions can neural networks actually learn? Why doesn’t the trained network overﬁt when
it is overparameterized?
In this work, we prove that overparameterized neural networks can learn some notable concept classes, including two and three-layer networks with fewer parameters and smooth activations. Moreover, the learning can be simply done by SGD (stochastic gradient descent) or its
variants in polynomial time using polynomially many samples. The sample complexity can also
be almost independent of the number of parameters in the network.
On the technique side, our analysis goes beyond the so-called NTK (neural tangent kernel)
linearization of neural networks in prior works. We establish a new notion of quadratic approximation of the neural network (that can be viewed as a second-order variant of NTK), and
connect it to the SGD theory of escaping saddle points.
∗V1 appears on this date, V2/V3/V4 polish writing and parameters, V5 adds experiments, and V6 reﬂects our
conference camera ready version. Authors sorted in alphabetical order. We would like to thank Greg Yang and
Sebastien Bubeck for many enlightening conversations.
Y. Liang was supported in part by FA9550-18-1-0166, and would also like to acknowledge that support for this
research was provided by the Oﬃce of the Vice Chancellor for Research and Graduate Education at the University
of Wisconsin-Madison with funding from the Wisconsin Alumni Research Foundation.
 
Introduction
Neural network learning has become a key machine learning approach and has achieved remarkable
success in a wide range of real-world domains, such as computer vision, speech recognition, and
game playing .
In contrast to the widely accepted empirical success, much less
theory is known. Despite a recent boost of theoretical studies, many questions remain largely open,
including fundamental ones about the optimization and generalization in learning neural networks.
One key challenge in analyzing neural networks is that the corresponding optimization is nonconvex and is theoretically hard in the general case . This is in sharp contrast to the fact
that simple optimization algorithms like stochastic gradient descent (SGD) and its variants usually
produce good solutions in practice even on both training and test data. Therefore,
what functions can neural networks provably learn?
Another key challenge is that, in practice, neural networks are heavily overparameterized
(e.g., ): the number of learnable parameters is much larger than the number of the training samples. It is observed that overparameterization empirically improves both optimization and
generalization, appearing to contradict traditional learning theory.1 Therefore,
why do overparameterized networks (found by those training algorithms) generalize?
What Can Neural Networks Provably Learn?
Most existing works analyzing the learnability of neural networks make unrealistic assumptions about the data distribution (such as being random
Gaussian), and/or make strong assumptions about the network (such as using linear activations).
Other works focusing on structured data such as sparse coding model and Li and
Liang shows that two-layer ReLU networks can learn classiﬁcation tasks when the data come
from mixtures of arbitrary but well-separated distributions.
A theorem without distributional assumptions on data is often more desirable. Indeed, how to
obtain a result that does not depend on the data distribution, but only on the concept class itself, lies
in the center of PAC-learning which is one of the foundations of machine learning theory . Also,
studying non-linear activations is critical because otherwise one can only learn linear functions,
which can also be easily learned via linear models without neural networks.
Brutzkus et al. prove that two-layer networks with ReLU activations can learn linearlyseparable data (and thus the class of linear functions) using just SGD. This is an (improper) PAClearning type of result because it makes no assumption on the data distribution. Andoni et al.
 proves that two-layer networks can learn polynomial functions of degree r over d-dimensional
inputs in sample complexity O(dr). Their learner networks use exponential activation functions,
where in practice the rectiﬁed linear unit (ReLU) activation has been the dominant choice across
vastly diﬀerent domains.
On a separate note, if one treats all but the last layer of neural networks as generating a random
feature map, then training only the last layer is a convex task, so one can learn the class of linear
functions in this implicit feature space . This result implies low-degree polynomials and
1For example, Livni et al. observed that on synthetic data generated from a target network, SGD converges
faster when the learned network has more parameters than the target. Perhaps more interestingly, Arora et al. 
found that overparameterized networks learned in practice can often be compressed to simpler ones with much fewer
parameters, without hurting their ability to generalize; however, directly learning such simpler networks runs into
worse results due to the optimization diﬃculty. We also have experiments in Figure 1(a).
compositional kernels can be learned by neural networks in polynomial time. Empirically, training
last layer greatly weakens the power of neural networks (see Figure 1).
Our Result. We prove that an important concept class that contains three-layer (resp. two-layer)
neural networks equipped with smooth activations can be eﬃciently learned by three-layer (resp.
two-layer) ReLU neural networks via SGD or its variants.
Speciﬁcally, suppose in aforementioned class the best network (called the target function or
target network) achieves a population risk OPT with respect to some convex loss function. We
show that one can learn up to population risk OPT + ε, using three-layer (resp. two-layer) ReLU
networks of size greater than a ﬁxed polynomial in the size of the target network, in 1/ε, and in
the “complexity” of the activation function used in the target network. Furthermore, the sample
complexity is also polynomial in these parameters, and only poly-logarithmic in the size of the
learner ReLU network.
We stress here that this is agnostic PAC-learning because we allow the target function to have
error (e.g., OPT can be positive for regression), and is improper learning because the concept class
consists of smaller neural networks comparing to the networks being trained.
Our Contributions.
We believe our result gives further insights to the fundamental questions
about the learning theory of neural networks.
• To the best of our knowledge, this is the ﬁrst result showing that using hidden layers of neural
networks one can provably learn the concept class containing two (or even three) layer neural
networks with non-trivial activation functions.2
• Our three-layer result gives the ﬁrst theoretical proof that learning neural networks, even
with non-convex interactions across layers, can still be plausible.
In contrast, in the twolayer case the optimization landscape with overparameterization is almost convex ;
and in previous studies on the multi-layer case, researchers have weakened the network by
applying the so-called NTK (neural tangent kernel) linearization to remove all non-convex
interactions .
• In fact, the techniques introduced in this work is later used in to show how multi-layer
neural networks can provably learn concept classes where no kernel methods can learn
eﬃciently, and used in to show how the initial large learning rate in training a neural
network can improve generalization, also not achievable by the convex kernel methods.
• To some extent we explain the reason why overparameterization improves testing accuracy:
with larger overparameterization, one can hope to learn better target functions with possibly
larger size, more complex activations, smaller risk OPT, and to a smaller error ε.
• We establish new tools to tackle the learning process of neural networks in general, which can
be useful for studying other network architectures and learning tasks. (E.g., the new tools
here have allowed researchers to study also the learning of recurrent neural networks .)
Other Related Works.
We acknowledge a diﬀerent line of research using kernels as improper
learners to learn the concept class of neural networks . This is very diﬀerent from us
because we use “neural networks” as learners. In other words, we study the question of “what can
neural networks learn” but they study “what alternative methods can replace neural networks.”
2In contrast, Daniely focuses on training essentially only the last layer (and the hidden-layer movement is
negligible). After this paper has appeared online, Arora et al. showed that neural networks can provably learn
two-layer networks with a slightly weaker class of smooth activation functions. Namely, the activation functions that
are either linear functions or even functions.
There is also a line of work studying the relationship between neural networks and NTKs
(neural tangent kernels) . These works study neural networks by considering
their “linearized approximations.” There is a known performance gap between the power of real
neural networks and the power of their linearized approximations. For instance, ResNet achieves
96% test error on the CIFAR-10 data set but NTK (even with inﬁnite width) achieves 77% .
We also illustrate this in Figure 1.
With suﬃcient overparameterization, for two-layer neural networks Li and Liang and Du
et al. show that gradient descent can perfectly ﬁt the training samples when the data is not
degenerated. Allen-Zhu et al. show that deep neural networks can also provably ﬁt the training
samples in polynomial time. These results do not cover generalization so are not relevant in studying
the learnability of neural networks.
Why Do Overparameterized Networks Generalize?
Our result above assumes that the learner network is suﬃciently overparameterized. So, why does
it generalize to the population risk and give small test error?
More importantly, why does it
generalize with a number of samples that is (almost) independent of the number of parameters?
This question cannot be studied under the traditional VC-dimension learning theory since
the VC dimension grows with the number of parameters. Several works explain
generalization by studying some other “complexity” of the learned networks. Most related to the
discussion here is where the authors prove a generalization bound in the norms (of weight
matrices) of each layer, as opposed to the number of parameters. There are two main concerns
with those results.
• Learnability = Trainability + Generalization. It is not clear from those results how a network
with both low “complexity” and small training loss can be found by the training method.
Therefore, they do not directly imply PAC-learnability for non-trivial concept classes (at least
for those concept classes studied by this paper).
• Their norms are “sparsity induced norms”: for the norm not to scale with the number of
hidden neurons m, essentially, it requires the number of neurons with non-zero weights not to
scale with m. This more or less reduces the problem to the non-overparameterized case.
At a high level, our generalization is made possible with the following sequence of conceptual steps.
• Good networks with small risks are plentiful: thanks to overparameterization, with high probability over random initialization, there exists a good network in the close neighborhood of
any point on the SGD training trajectory. (This corresponds to Section 6.2 and 6.3.)
• The optimization in overparameterized neural networks has benign properties: essentially
along the training trajectory, there is no second-order critical points for learning three-layer
networks, and no ﬁrst-order critical points for two-layer. (This corresponds to Section 6.4.)
• In the learned networks, information is also evenly distributed among neurons, by utilizing
either implicit or explicit regularization. This structure allows a new generalization bound
that is (almost) independent of the number of neurons. (This corresponds to Section 6.5 and
6.6, and we also empirically verify it in Section 7.1.)
Since practical neural networks are typically overparameterized, we genuinely hope that our
results can provide theoretical insights to networks used in various applications.
Test error
m = number of hidden neurons
(a) N = 1000 and vary m
Test error
N = number of samples
3layer(last)
2layer(last)
3layer(NTK)
(b) m = 2000 and vary N
Figure 1: Performance comparison.
3layer/2layer stands for training (hidden weights) in three and
two-layer neural networks. (last) stands for conjugate kernel , meaning training only the
output layer. (NTK) stands for neural tangent kernel with ﬁnite width. We also implemented
other direct kernels such as but they perform much worse.
We consider ℓ2 regression task on synthetic data where feature vectors x ∈R4
are generated as normalized random Gaussian, and label is generated by target function
F ∗(x) = (sin(3x1)+sin(3x2)+sin(3x3)−2)2 ·cos(7x4). We use N training samples, and SGD with
mini-batch size 50 and best tune learning rates and weight decay parameters. See Appendix 7 for
our experiment setup, how we choose such target function, and more experiments.
In the main body of this paper, we introduce notations in Section 2, present our main results and
contributions for two and three-layer networks in Section 3 and 4, and conclude in Section 5.
For readers interested in our novel techniques, we present in Section 6 an 8-paged proof sketch
of our three-layer result. For readers more interested in the practical relevance, we give more experiments in Section 7. In the appendix, we begin with mathematical preliminaries in Appendix A.
Our full three-layer proof is in Appendix C. Our two-layer proof is much easier and in Appendix B.
We use x = y ± z to denote that x ∈[y −z, y + z]. We use N(µ, σ) to denote Gaussian distribution
with mean µ and variance σ; or N(µ, Σ) to denote Gaussian vector with mean µ and covariance
Σ. We use IE or I[E] to denote the indicator function of the event E. We use σ(·) to denote the
ReLU function σ(x) = max{x, 0}. Given function f : R →R, we use f(x) = (f(x1), . . . , f(xm)) to
denote the same function over vectors.
We denote by ∥w∥2 and ∥w∥∞the Euclidean and inﬁnity norms of vectors w, and ∥w∥0 the
number of non-zeros of w. We also abbreviate ∥w∥= ∥w∥2 when it is clear from the context. We
denote the row ℓp norm for W ∈Rm×d (for p ≥1) as
i∈[m] ∥wi∥p
By deﬁnition, ∥W∥2,2 = ∥W∥F is the Frobenius norm of W. We use ∥W∥2 to denote the matrix
spectral norm. For a matrix W ∈Rm×d, we use Wi or sometimes wi to denote the i-th row of W.
We say f : Rd →R is L-Lipschitz continuous if |f(x)−f(y)| ≤L∥x−y∥2; is L-Lipschitz smooth
if ∥∇f(x)−∇f(y)∥2 ≤L∥x−y∥2; and is L-second-order smooth if ∥∇2f(x)−∇2f(y)∥2 ≤L∥x−y∥2.
For notation simplicity, with high probability (or w.h.p.) means with probability 1 −e−c log2 m
for a suﬃciently large constant c for two-layer network where m is the number of hidden neurons,
and 1 −e−c log2(m1m2) for three-layer network where m1, m2 are the number of hidden neurons for
the ﬁrst and second layer (to be precisely deﬁned later). In this paper, eO hides factors of polylog(m)
for two-layer networks, or polylog(m1, m2) for three-layer networks.
Function complexity.
The following notion measures the complexity of any smooth activation
function φ(z). Suppose φ(z) = P∞
i=0 cizi. Given a non-negative R, the complexity
(i + 1)1.75Ri|ci|
where C∗is a suﬃciently large constant (e.g., 104). Intuitively, Cs measures the sample complexity:
how many samples are required to learn φ correctly; while Cε bounds the network size: how much
over-parameterization is needed for the algorithm to eﬃciently learn φ up to ε error. It is always
true that Cs(φ, R) ≤Cε(φ, R) ≤Cs(φ, O(R)) × poly(1/ε).3 While for sin z, exp(z) or low degree
polynomials, Cs(φ, O(R)) and Cε(φ, R) only diﬀer by o(1/ε).
Example 2.1. If φ(z) = ec·z −1, φ(z) = sin(c · z), φ(z) = cos(c · z) for constant c or φ(z) is low
degree polynomial, then Cε(φ, 1) = o(1/ε) and Cs(φ, 1) = O(1). If φ(z) = sigmoid(z) or tanh(z),
we can truncate their Taylor series at degree Θ(log 1
ε) to get ε approximation. One can verify this
gives Cε(φ, 1) ≤poly(1/ε) and Cs(φ, 1) ≤O(1).
Result for Two-Layer Networks
We consider learning some unknown distribution D of data points z = (x, y) ∈Rd × Y, where x
is the input point and y is the label. Without loss of generality, assume ∥x∥2 = 1 and xd = 1
Consider a loss function L: Rk × R →Y such that for every y ∈Y, the function L(·, y) is nonnegative, convex, 1-Lipschitz continuous and 1-Lipschitz smooth and L(0, y) ∈ . This includes
both the cross-entropy loss and the ℓ2-regression loss (for bounded Y). 5
Concept class and target function F ∗(x).
Consider target functions F ∗: Rd →Rk of
1 , . . . , f∗
1,i, x⟩)⟨w∗
where each φi : R →R is inﬁnite-order smooth and the weights w∗
1,i ∈Rd, w∗
2,i ∈Rd and a∗
We assume for simplicity ∥w∗
1,i∥2 = ∥w∗
2,i∥2 = 1 and |a∗
r,i| ≤1.6 We denote by
Cε(φ, R) := maxj∈[p]{Cε(φj, R)}
Cs(φ, R) := maxj∈[p]{Cs(φj, R)}
C∗i ≤eO(log(1/ε)) =
poly(ε) for every i ≥1.
2 can always be padded to the last coordinate, and ∥x∥2 = 1 can always be ensured from ∥x∥2 ≤1 by padding
2. This assumption is for simplifying the presentation. For instance, xd =
2 allows us to focus only on
target networks without bias.
5In fact, the non-negativity assumption and the 1-Lipschitz smoothness assumption are not needed for our twolayer result, but we state all of them here for consistency.
6For general ∥w∗
1,i∥2 ≤B, ∥w∗
2,i∥2 ≤B, |a∗
r,i| ≤B, the scaling factor B can be absorbed into the activation
function φ′(x) = φ(Bx). Our results then hold by replacing the complexity of φ with φ′. As for how much this
aﬀects the complexity (i.e., polynomial in B or exponential in B), it varies across diﬀerent activation functions. The
complexity sometimes stays the same when B is a constant, see Example 2.1. In general, it is impossible to obtain a
polynomial dependency on B unless well-known hardness assumptions break. For example, learning a single sigmoid
with polynomially large weights agnostically is equivalent to agnostically learning a half space to a polynomially small
error, which has no polynomial time algorithms under the random K-XOR assumption .
Algorithm 1 SGD for two-layer networks
Input: Data set Z, initialization W (0), step size η
2: for t = 1, 2, . . . do
Randomly sample z = (x, y) from the data set Z
Update: Wt = Wt−1 −η ∂
∂W L(F(x; Wt−1 + W (0)), y)
5: end for
the complexity of F ∗and assume they are bounded.
In the agnostic PAC-learning language, our concept class consists of all functions F ∗in the form
of (3.1) with complexity bounded by threshold C and parameter p bounded by threshold p0. Let
OPT = E[L(F ∗(x), y)] be the population risk achieved by the best target function in this concept
class. Then, our goal is to learn this concept class with population risk OPT + ε using sample and
time complexity polynomial in C, p0 and 1/ε. In the remainder of this paper, to simplify notations,
we do not explicitly deﬁne this concept class parameterized by C and p. Instead, we equivalently
state our theorem with respect to any (unknown) target function F ∗with speciﬁc parameters C
and p satisfying OPT = E[L(F ∗(x), y)]. We assume OPT ∈ for simplicity.
Remark. Standard two-layer networks f∗
r (x) = Pp
1,i, x⟩) are special cases of (3.1) (by
setting w∗
2,i = (0, . . . , 0, 1) and φi = φ). Our formulation (3.1) additionally captures combinations
of correlations between non-linear and linear measurements of diﬀerent directions of x.
Learner network F (x; W ).
Using a data set Z = {z1, . . . , zN} of N i.i.d. samples from D, we
train a network F = (f1, · · · , fk): Rd →Rk with
ar,iσ(⟨wi, x⟩+ bi) = a⊤
r σ(Wx + b)
where σ is the ReLU activation, W = (w1, . . . , wm) ∈Rm×d is the hidden weight matrix, b ∈Rm is
the bias vector, and ar ∈Rm is the output weight vector. To simplify analysis, we only update W
and keep b and ar at initialization values. For such reason, we write the learner network as fr(x; W)
and F(x; W). We sometimes use b(0) = b and a(0)
= ar to emphasize they are randomly initialized.
Our goal is to learn a weight matrix W with population risk E
L(F(x; W), y)
Learning Process.
Let W (0) denote the initial value of the hidden weight matrix, and let
W (0) + Wt denote the value at time t. (Note that Wt is the matrix of increments.) The weights
are initialized with Gaussians and then W is updated by the vanilla SGD. More precisely,
• entries of W (0) and b(0) are i.i.d. random Gaussians from N(0, 1/m),
• entries of each a(0)
are i.i.d. random Gaussians from N(0, ε2
a) for some ﬁxed εa ∈(0, 1].7
At time t, SGD samples z = (x, y) ∼Z and updates Wt+1 = Wt −η∇L(F(x; W (0) +Wt), y). 8 The
algorithm is in Algorithm 1.
7We shall choose εa = eΘ(ε) in the proof due to technical reason. As we shall see in the three-layer case, if weight
decay is used, one can relax this to εa = 1.
8Strictly speaking, the objective does not have gradient everywhere due to the non-diﬀerentiability of ReLU.
Throughout the paper, we use ∇to denote the value computed by setting ∇σ(x) = I[x ≥0], which is what is used
in practical auto-diﬀerentiation softwares.
Main Theorem
For notation simplicity, with high probability (or w.h.p.) means with probability 1 −e−c log2 m for
a suﬃciently large constant c, and eO hides factors of polylog(m).
Theorem 1 (two-layer). For every ε ∈
, there exists
M0 = poly(Cε(φ, 1), 1/ε)
N0 = poly(Cs(φ, 1), 1/ε)
such that for every m ≥M0 and every N ≥eΩ(N0), choosing εa = ε/eΘ(1) for the initialization,
choosing learning rate η = eΘ
(Cs(φ, 1))2 · k3p2
with high probability over the random initialization, SGD after T iteration satisﬁes
E(x,y)∼DL(F(x; W (0) + Wt), y)
(Above, Esgd takes expectation with the randomness of the SGD. )
Remark. SGD only takes one example per iteration. Thus, sample complexity is also at most T.
Example 3.1. For functions such as φ(z) = ez, sin z, sigmoid(z), tanh(z) or low degree polynomials,
using Example 2.1, our theorem indicates that for target networks with such activation functions,
we can learn them using two-layer ReLU networks with
size m = poly(k, p)
and sample complexity min{N, T} = poly(k, p, log m)
We note sample complexity T is (almost) independent of m, the amount of overparametrization.
Our Interpretations
Overparameterization improves generalization. By increasing m, Theorem 1 supports more
target functions with possibly larger size, more complex activations, and smaller population risk
OPT. In other words, when m is ﬁxed, among the class of target functions whose complexities
are captured by m, SGD can learn the best function approximator of the data, with the smallest
population risk. This gives intuition how overparameterization improves test error, see Figure 1(a).
Large margin non-linear classiﬁer.
Theorem 1 is a nonlinear analogue of the margin theory
for linear classiﬁers. The target function with a small population risk (and of bounded norm) can
be viewed as a “large margin non-linear classiﬁer.” In this view, Theorem 1 shows that assuming
the existence of such large-margin classiﬁer, SGD ﬁnds a good solution with sample complexity
mostly determined by the margin, instead of the dimension of the data.
Inductive bias.
Recent works (e.g., ) show that when the network is heavily overparameterized (that is, m is polynomial in the number of training samples) and no two training samples
are identical, then SGD can ﬁnd a global optimum with 0 classiﬁcation error (or ﬁnd a solution with
ε training loss) in polynomial time. This does not come with generalization, since it can even ﬁt
random labels. Our theorem, combined with , conﬁrms the inductive bias of SGD for two-layer
networks: when the labels are random, SGD ﬁnds a network that memorizes the training data;
when the labels are (even only approximately) realizable by some target network, then SGD learns
and generalizes. This gives an explanation towards the well-known empirical observations of such
inductive bias (e.g., ) in the two-layer setting, and is more general than Brutzkus et al. in
which the target network is only linear.
Result for Three-Layer Networks
Concept class and target function F ∗(x).
This time we consider more powerful target functions F ∗= (f∗
1 , · · · , f∗
k) of the form
1,i,jφ1,j(⟨w∗
2,i,jφ2,j(⟨w∗
where each φ1,j, φ2,j, Φi : R →R is inﬁnite-order smooth, and the weights w∗
2,i ∈Rd, v∗
Rp2 and a∗
r,i ∈R satisfy ∥w∗
1,j∥2 = ∥w∗
2,j∥2 = ∥v∗
1,i∥2 = ∥v∗
2,i∥2 = 1 and |a∗
r,i| ≤1. Let
Cε(φ, R) = maxj∈[p2],s∈ {Cε(φs,j, R)},
Cε(Φ, R) = maxj∈[p1]{Cε(Φj, R)}
Cs(φ, R) = maxj∈[p2],s∈ {Cs(φs,j, R)},
Cs(Φ, R) = maxj∈[p1]{Cs(Φj, R)}
to denote the complexity of the two layers, and assume they are bounded.
Our concept class contains measures of correlations between composite non-linear functions and
non-linear functions of the input, there are plenty of functions in this new concept class that may
not necessarily have small-complexity representation in the previous formulation (3.1), and as we
shall see in Figure 1(a), this is the critical advantage of using three-layer networks compared
to two-layer ones or their NTKs.
The learnability of this correlation is due to the non-convex
interactions between hidden layers. As a comparison, studies the regime where the changes in
hidden layers are negligible thus can not show how to learn this concept class with a three-layer
Remark 4.1. Standard three-layer networks
are only special cases of (4.1). Also, even in the special case of Φi(z) = z, the target
1,i,jφ1(⟨w∗
2,i,jφ2(⟨w∗
captures combinations of correlations of combinations of non-linear measurements in diﬀerent directions of x. This we do not know how to compute using two-layer networks.
Remark 4.2. In fact, our results of this paper even apply to the following general form:
1,i,jφ1,j(⟨w∗
1,j, x⟩)⟨w∗
2,i,jφ2,j(⟨w∗
2,j, x⟩)⟨w∗
with the mild requirement that ⟨w∗
3,j⟩= 0 for each j ∈[p2]. We choose to present the slightly
weaker formulation (4.1) for cleaner proofs.
Learner network F (x; W, V ).
Our learners are three-layer networks F = (f1, . . . , fk) with
ar,iσ(ni(x) + b2,i)
where each ni(x) =
vi,jσ (⟨wj, x⟩+ b1,j)
The ﬁrst and second layers have m1 and m2 hidden neurons. Let W ∈Rm1×d and V ∈Rm2×m1
represent the weights of the ﬁrst and second hidden layers respectively, and b1 ∈Rm1 and b2 ∈Rm2
represent the corresponding bias vectors, ar ∈Rm2 represent the output weight vector.
Learning Process
Again for simplicity, we only update W and V . The weights are randomly initialized as:
• entries of W (0) and b1 = b(0)
are i.i.d. from N(0, 1/m1),
• entries of V (0) and b2 = b(0)
are i.i.d. from N(0, 1/m2),
• entries of each ar = a(0)
are i.i.d. from N(0, ε2
a) for εa = 1. 9
As for the optimization algorithm, we use SGD with weight decay and an explicit regularizer.
For some λ ∈(0, 1], we will use λF(x; W, V ) as the learner network, i.e., linearly scale F down
by λ. This is equivalent to replacing W, V with
λV , since a ReLU network is positive
homogenous. The SGD will start with λ = 1 and slowly decrease it, similar to weight decay.10
We also use an explicit regularizer for some λw, λv > 0 with11
Now, in each round t = 1, 2, . . . , T, we use (noisy) SGD to minimize the following stochastic
objective for some ﬁxed λt−1:
L2(λt−1; W ′, V ′)
 x; W (0) + W ρ + ΣW ′, V (0) + V ρ + V ′Σ
Above, the objective is stochastic because (1) z ∼Z is a random sample from the training set, (2)
W ρ and V ρ are two small perturbation random matrices with entries i.i.d. drawn from N(0, σ2
and N(0, σ2
v) respectively, and (3) Σ ∈Rm1×m1 is a random diagonal matrix with diagonals i.i.d.
uniformly drawn from {+1, −1}. We note that the use of W ρ and V ρ is standard for Gaussian
smoothing on the objective (and not needed in practice).12 The use of Σ may be reminiscent of the
Dropout technique in practice which randomly masks out neurons, and can also be removed.13
Algorithm 2 presents the details. Speciﬁcally, in each round t, Algorithm 2 starts with weight
matrices Wt−1, Vt−1 and performs Tw iterations. In each iteration it goes in the negative direction
of the gradient ∇W ′,V ′L2(λt; W ′, V ′) (with respect to a stochastic choice of z, W ρ, V ρ, Σ). Let the
ﬁnal matrices be Wt, Vt. At the end of this round t, Algorithm 2 performs weight decay by setting
λt = (1 −η)λt−1 for some η > 0.
Main Theorems
For notation simplicity, with high probability (or w.h.p.) means with probability 1 −e−c log2(m1m2)
and eO hides factors of polylog(m1, m2).
9Recall in our two-layer result we have chosen εa < 1 due to technical reasons; thanks to weight decay, we can
simply select εa = 1 in our three-layer case.
10We illustrate the technical necessity of adding weight decay. During training, it is easy to add new information
to the current network, but hard to forget “false” information that is already in the network. Such false information
can be accumulated from randomness of SGD, non-convex landscapes, and so on. Thus, by scaling down the network
we can eﬀectively forget false information.
11This ∥·∥2,4 norm on W encourages weights to be more evenly distributed across neurons. It can be replaced with
∥√λt−1Wt−1∥2+α
2,2+α for any constant α > 0 for our theoretical purpose. We choose α = 2 for simplicity, and observe
that in practice, weights are automatically spread out due to data randomness, so this explicit regularization may
not be needed. See Section 7.1 for an experiment.
12Similar to known non-convex literature or smooth analysis, we introduce Gaussian perturbation W ρ and V ρ
for theoretical purpose and it is not needed in practice. Also, we apply noisy SGD which is the vanilla SGD plus
Gaussian perturbation, which again is needed in theory but believed unnecessary for practice .
13In the full paper we study two variants of SGD. This present version is the “second variant,” and the ﬁrst variant
L1(λt−1; W ′, V ′) is the same as (4.2) by removing Σ. Due to technical diﬃculty, the best sample complexity we can
prove for L1 is a bit higher.
Algorithm 2 SGD for three-layer networks (second variant (4.2))
Input: Data set Z, initialization W (0), V (0), step size η, number of inner steps Tw, σw, σv, λw, λv.
1: W0 = 0, V0 = 0, λ1 = 1, T = Θ
 η−1 log log(m1m2)
2: for t = 1, 2, . . . , T do
Apply noisy SGD with step size η on the stochastic objective L2(λt−1; W, V ) for Tw steps; the starting
point is W = Wt−1, V = Vt−1 and suppose it reaches Wt, Vt.
⋄see Lemma A.9
λt+1 = (1 −η)λt.
⋄weight decay
5: end for
6: Randomly sample bΣ with diagonal entries i.i.d. uniform on {1, −1}
7: Randomly sample eΘ(1/ε2
0) many noise matrices
W ρ,j, V ρ,j
j∗= arg minj
 x; W (0) + W ρ,j + bΣWT , V (0) + V ρ,j + VT bΣ
8: Output W (out)
= W (0) + W ρ,j∗+ bΣWT , V (out)
= V (0) + V ρ,j∗+ VT bΣ.
Theorem 2 (three-layer, second variant). Consider Algorithm 2. For every constant γ ∈(0, 1/4],
every ε0 ∈(0, 1/100], every ε =
2Cs(Φ,p2Cs(φ,1))Cs(φ,1)2 , there exists
Cε(Φ, √p2Cε(φ, 1)), 1
such that for every m2 = m1 = m ≥M, and properly set λw, λv, σw, σv in Table 1, as long as
Cε(Φ, √p2Cε(φ, 1)) · Cε(φ, 1) · √p2p1k2
there is a choice η = 1/poly(m1, m2) and T = poly(m1, m2) such that with probability ≥99/100,
E(x,y)∼DL(λT F(x; W (out)
), y) ≤(1 + γ)OPT + ε0.
Remark 4.3. For general Φ, ignoring p2, the complexity of three-layer networks is essentially
Cε(Φ, Cε(φ, 1)). This is necessary in some sense: consider the case when φ(x) = Cx for a very large
parameter C, then Φ(φ(z)) is just a function Φ′(z) = Φ(Cz), and we have Cε(Φ′, 1) = Cε(Φ, C) ≈
Cε(Φ, Θ(Cε(φ, 1))).
Our Contributions
Our sample complexity N scales polynomially with the complexity of the target network, and is
(almost) independent of m, the amount of overparameterization. This itself can be quite surprising,
because recent results on neural network generalization require N to be polynomial
in m. Furthermore, Theorem 2 shows three-layer networks can eﬃciently learn a bigger concept
class (4.1) comparing to what we know about two-layer networks (3.1).
From a practical standpoint, one can construct target functions of the form (4.1) that cannot be
(eﬃciently) approximated by any two-layer target function in (3.1). If data is generated according
to such functions, then it may be necessary to use three-layer networks as learners (see Figure 1).
From a theoretical standpoint, even in the special case of Φ(z) = z, our target function can
capture correlations between non-linear measurements of the data (recall Remark 4.1). This means
Cε(Φ, Cε(φ, 1)√p2) ≈O(√p2Cε(φ, 1)), so learning it is essentially in the same complexity as learning
each φs,j. For example, a three-layer network can learn cos(100⟨w∗
1, x⟩) · e100⟨w∗
2,x⟩up to accuracy
ε in complexity poly(1/ε), while it is unclear how to do so using two-layer networks.
Technical Contributions. We highlight some technical contributions in the proof of Theorem 2.
In recent results on the training convergence of neural networks for more than two layers ,
the optimization process stays in a close neighborhood of the initialization so that, with heavy overparameterization, the network becomes “linearized” and the interactions across layers are negligible.
In our three-layer case, this means that the matrix W never interacts with V . They then argue
that SGD simulates a neural tangent kernel (NTK) so the learning process is almost convex .
In our analysis, we directly tackle non-convex interactions between W and V , by studying a
“quadratic approximation” of the network. See Remark 6.1 for a mathematical comparison. In
other words, we introduced a second-order version of NTK, and our new proof techniques could be
useful for future theoretical applications.
Also, for the results and our two-layer Theorem 1 to hold, it suﬃces to analyze a regime
where the “sign pattern” of ReLUs can be replaced with that of the random initialization. (Recall
σ(x) = Ix≥0 · x and we call Ix≥0 the “sign pattern.”) In our three-layer analysis, the optimization process has moved suﬃciently away from initialization, so that the sign pattern change can
signiﬁcantly aﬀect output. This brings in additional technical challenge because we have to tackle
non-convex interactions between W and V together with changing sign patterns.14
Comparison to Daniely . Daniely studies the learnability of multi-layer networks when
(essentially) only the output layer is trained, which reduces to a convex task. He shows that multilayer networks can learn a compositional kernel space, which implies two/three-layer networks can
eﬃciently learn low-degree polynomials. He did not derive the general sample/time complexity
bounds for more complex functions such as those in our concept classes (3.1) and (4.1), but showed
that they are ﬁnite.
In contrast, our learnability result of concept class (4.1) is due to the non-convex interaction
between hidden layers. Since Daniely studies the regime when the changes in hidden layers are
negligible, if three layer networks are used, to the best of our knowledge, their theorem cannot lead
to similar sample complexity bounds comparing to Theorem 2 by only training the last layer of a
three-layer network. Empirically, one can also observe that training hidden layers is better than
training the last layer (see Figure 1).
Conclusion and Discussion
We show by training the hidden layers of two-layer (resp. three-layer) overparameterized neural
networks, one can eﬃciently learn some important concept classes including two-layer (resp. threelayer) networks equipped with smooth activation functions. Our result is in the agnostic PAClearning language thus is distribution-free. We believe our work opens up a new direction in both
algorithmic and generalization perspectives of overparameterized neural networks, and pushing
forward can possibly lead to more understanding about deep learning.
Our results apply to other more structured neural networks. As a concrete example, consider
convolutional neural networks (CNN). Suppose the input is a two dimensional matrix x ∈Rd×s
which can be viewed as d-dimensional vectors in s channels, then a convolutional layer on top
of x is deﬁned as follows. There are d′ ﬁxed subsets {S1, S2, . . . , Sd′} of [d] each of size k′. The
output of the convolution layer is a matrix of size d′ ×m, whose (i, j)-th entry is φ(⟨wj, xSi⟩), where
xSi ∈Rk′×s is the submatrix of x with rows indexed by Si; wj ∈Rk′×s is the weight matrix of the
j-th channel; and φ is the activation function. Overparameterization then means a larger number
14For instance, the number of sign changes can be m0.999 for the second hidden layer (see Lemma 6.5). In this
region, the network output can be aﬀected by m0.499 since each neuron is of value roughly m−1/2. Therefore, if after
training we replace the sign pattern with random initialization, the output will be meaningless.
of channels m in our learned network comparing to the target. Our analysis can be adapted to
show a similar result for this type of networks.
One can also combine this paper with properties of recurrent neural networks (RNNs) to
derive PAC-learning results for RNNs , or use the existential tools of this paper to derive PAClearning results for three-layer residual networks (ResNet) . The latter gives a provable separation
between neural networks and kernels in the eﬃcient PAC-learning regime.
There are plenty of open directions following our work, especially how to extend our result to
larger number of layers. Even for three layers, our algorithm currently uses an explicit regularizer
R(W ′, V ′) on the weights to control the generalization.
In contrast, in practice, it is observed
that neural networks actually implicitly regularizes: even without any restriction to weights, the
network learned by an unconstraint overparameterized neural network still generalizes. It is an
interesting direction to explain this inductive bias for three layers and beyond. (This present paper
only explains inductive bias for two-layer networks.) As for a third direction, currently our result
does not directly apply to target networks with ReLU activations. While there are functions φ with
Cε(φ, 1) = 2O(1/ε) that approximate ReLU function in [−1, 1] with ε error, obtaining an polynomial
or sub-exponential bound on the sampling complexity would be of great interest. On the other
hand, the result by indicates that learning ReLU could also be hard for algorithms such as
Proof Sketch
We present key technical lemmas we used for proving the three-layer network theorems in
Section 6 so that interested readers do not need to go through the appendix. Many of them can be
of independent interests and have found further applications beyond this paper (such as for residual
networks and for recurrent networks ). The full proof is in Appendix C.
The two-layer result is based on similar ideas but simpler, and the full proof is in Appendix B.
We give more experiments in Section 7 on Page 21. Our appendix starts on page 24.
Main Lemmas for Three Layer Networks
In Section 6.1, we state the main theorem for the ﬁrst variant of the SGD, which we excluded from
the main body due to space limitation. In Section 6.2, we show the existence of some good “pseudo
network” that can approximate the target.
In Section 6.3, we present our coupling technique
between a real network and a pseudo network. In Section 6.4, we present the key lemma about the
optimization procedure. In Section 6.5, we state a simple generalization bound that is compatible
with our algorithm. These techniques together give rise to the proof of Theorem 3. In Section 6.6,
we present additional techniques needed to show Theorem 2.
First Variant of SGD
In the ﬁrst variant of SGD, in each round t = 1, 2, . . . , T, we use (noisy) SGD to minimize the
following stochastic objective for some ﬁxed λt−1:
L1(λt−1; W ′, V ′)
 x; W (0) + W ρ + W ′, V (0) + V ρ + V ′
Above, the objective is stochastic because (1) z ∼Z is a random sample from the training set, and
(2) W ρ and V ρ are two small perturbation matrices. This is only diﬀerent from (4.2) by removing
Σ. We include the pseudocode in Algorithm 3.
Algorithm 3 SGD for three-layer networks (ﬁrst variant (6.1))
Input: Data set Z, initialization W (0), V (0), step size η, number of inner steps Tw, σw, σv, λw, λv.
1: W0 = 0, V0 = 0, λ1 = 1, T = Θ
η−1 log log(m1m2)
2: for t = 1, 2, . . . , T do
Apply noisy SGD with step size η on the stochastic objective L1(λt−1; W, V ) for Tw steps;
the starting point is W = Wt−1, V = Vt−1 and suppose it reaches Wt, Vt.
see Lemma A.9
λt+1 = (1 −η)λt.
weight decay
5: end for
6: Randomly sample eΘ(1/ε2
0) many noise matrices
W ρ,j, V ρ,j
j∗= arg min
 x; W (0) + W ρ,j + WT , V (0) + V ρ,j + VT
7: Output W (out)
= W (0) + W ρ,j∗+ WT , V (out)
= V (0) + V ρ,j∗+ VT .
Below is the main theorem for using the ﬁrst variant of SGD to train three-layer networks.
Theorem 3 (three-layer, ﬁrst variant). Consider Algorithm 3. In the same setting as Theorem 2,
for every m2 = m1 = m ≥M, as long as
N ≥eΩ(Mm3/2)
there is choice η = 1/poly(m1, m2), T = poly(m1, m2) such that with probability at least 99/100,
E(x,y)∼DL(λT F(x; W (out)
), y) ≤(1 + γ)OPT + ε0.
As m goes large, this sample complexity N ≈m3/2 polynomially scales with m so may not be
very eﬃcient (we did not try hard to improve the exponent 3/2). Perhaps interesting enough, this
is already non-trivial, because N can be much smaller than m2, the number of parameters of the
network or equivalently the naive VC dimension bound. Recall in our second variant of SGD, the
sample complexity N only grows polylogarithmically in m.
We wish to show the existence of some good “pseudo network” that can approximate the target
network. In a pseudo network, each ReLU activation σ(x) = Ix≥0x is replaced with Ix(0)≥0x where
x(0) is the value at random initialization. Formally, let
• Dw,x ∈{0, 1}m1×m1 denote a diagonal sign matrix indicating the sign of the ReLU’s for the
ﬁrst layer at random initialization, that is, [Dw,x]i,i = I[⟨w(0)
i , x⟩+ b(0)
1,i ≥0], and
• Dv,x ∈{0, 1}m2×m2 denote the diagonal sign matrix of the second layer at random initialization.
Consider the output of a three-layer network at randomly initialized sign without bias as
r (x; W, V )
= arDv,xV Dw,xWx
G(0)(x; W, V )
1 , · · · , g(0)
Remark 6.1. The above pseudo network can be reminiscent of the simpler linearized NTK approximation of a network used in prior work , which in our language means
arDv,xV Dw,xW (0)x + arDv,xV (0)Dw,xWx .
In such linearization it is clear that the weight matrices W and V do not interact with each other. In
contrast, in our quadratic formula arDv,xV Dw,xWx, the matrices W and V are multiplied together,
resulting in a non-convex interaction after putting into the loss function. This can be viewed as a
second-order version of NTK (but is itself not NTK). We shall see in Section 6.3 that the pseudo
network can be made close to the real network in some sense.
Lemma 6.2 (existence). For every ε ∈
2Cs(Φ,p2Cs(φ,1))Cs(φ,1)2
, there exists
 Φ, √p2Cε(φ, 1)
C0 = Cε(Φ, √p2Cε(φ, 1)) · Cε(φ, 1) · eO(p1
such that if m1, m2 ≥M, then with high probability, there exists weights W ⋇, V ⋇with
∥W ⋇∥2,∞= max
∥V ⋇∥2,∞= max
r (x) −g(0)
r (x; W ⋇, V ⋇)
and hence,
L(G(0)(x; W ⋇, V ⋇), y)
In other words, at randomly initialized signs, there exist choices of W ⋇and V ⋇with small
norms so that G(0)(x; W ⋇, V ⋇) approximates the target.
Later, we will combine this with the
coupling lemma Section 6.3 to show a main structural property of overparameterized networks:
solutions with good population risks are dense in the parameter space, in the sense that with high
probability over the random initialization, there exists a good solution in the “close ” neighborhood
of the initialized weights.
Technical Ideas
We begin with a simple task to illustrate the main idea.
Let w∗∈Rd be a given vector and
suppose we want to approximate function φ(⟨w∗, x⟩) (over x) by designing a random function
I⟨w,x⟩+b0≥0h(⟨w, w∗⟩, b0) (over x) where w is a random Gaussian, b0 is a random bias, and h(·, ·) is
a function at our choice. The next lemma says that, we can design h with good property so that
the expectation of the random function I⟨w,x⟩+b0≥0h(⟨w, w∗⟩, b0) is close to φ(⟨w∗, x⟩).
Lemma 6.3 (indicator to function). For every smooth function φ, every ε ∈
exists a function h : R2 →[−Cε(φ, 1), Cε(φ, 1)] that is also Cε(φ, 1)-Lipschitz continuous on its ﬁrst
coordinate with the following two (equivalent) properties:
(a) For every x1 ∈[−1, 1]:
1+b0≥0h(α1, b0)
where α1, β1, b0 ∼N(0, 1) are independent random variables.
(b) For every w∗, x ∈Rd with ∥w∗∥2 = ∥x∥2 = 1:
I⟨w,x⟩+b0≥0h(⟨w, w∗⟩, b0)
−φ(⟨w∗, x⟩)
where w ∼N(0, I) is an d-dimensional Gaussian, b0 ∼N(0, 1).
Furthermore, h satisﬁes Eα1,b0∼N(0,1)
h(α1, b0)2
≤(Cs(φ, 1))2.
If one designs a vector w⋇=
 0, . . . , 0, 2h(⟨w, w∗⟩, b0)
, then using xd = 1/2, Lemma 6.3 implies
I⟨w,x⟩+b0≥0⟨w⋇, x⟩
−φ(⟨w∗, x⟩)
≤ε. Therefore, Lemma 6.3 corresponds to Lemma 6.2 in the
special case of a single neuron.
Remark. The reason Lemma 6.3b is equivalent to Lemma 6.3a consists of a few thinking steps.
Without loss of generality, one can assume w∗= (1, 0, 0, . . . , 0) and write φ(⟨w∗, x⟩) = φ(x1) and
write h(⟨w, w∗⟩, b0) = h(w1, b0) so it only depends on the ﬁrst coordinate of w and b0. Under
such simpliﬁcations, the second through last coordinates do not make any diﬀerence, so we can
assume without loss of generality that w∗, w, x are only in 2 dimensions, and write w = (α1, β1)
and x = (x1,
1). In sum, proving Lemma 6.3a suﬃces in establishing Lemma 6.3b.
Given Lemma 6.3, we can directly apply it to the two-layer case (see Appendix B.1) to show
the existence of good pseudo networks. As for the three-layer case, we need to apply Lemma 6.3
twice: once for (each neuron of) the second hidden layer and once for the output.
Consider the the input (without bias) to a single neuron of the second hidden layer at random
initialization. Without loss of generality, say the ﬁrst neuron, given as:
i , x⟩+ b(0)
Even though n1(x) is completely random, using Lemma 6.3, we can derive the following lemma
which rewrites n1(x) in the direction of an arbitrary function φ.
Lemma 6.4 (information out of randomness). For every smooth function φ, every w∗∈Rd with
∥w∗∥2 = 1, for every ε ∈
, there exists real-valued functions
1 , W (0), b(0)
1 ), B(x, v(0)
1 , W (0), b(0)
1 ), R(x, v(0)
1 , W (0), b(0)
1 ), and φε(x)
such that for every x:
1 , W (0), b(0)
1 , W (0), b(0)
1 , W (0), b(0)
Moreover, letting C = Cε(φ, 1) be the complexity of φ, and if v(0)
m2 ) and w(0)
i,j , b(0)
m1 ) are at random initialization, then we have
1. For every ﬁxed x, ρ
1 , W (0), b(0)
is independent of B
1 , W (0), b(0)
1 , W (0), b(0)
3. For every x with ∥x∥2 = 1, |φε(x) −φ(⟨w∗, x⟩)| ≤ε.
4. For every ﬁxed x with ∥x∥2 = 1, with high probability
1 , W (0), b(0)
1 , W (0), b(0)
Furthermore, there exists real-valued function eρ(v(0)
1 ) satisfying with high probability:
W2(ρ|W (0),b(0)
1 , eρ) ≤eO
Lemma 6.4 shows that, up to some small noise B and R, we can “view” the input to any neuron
of the second layer essentially as “a Gaussian variable ρ” times “the target function φ(⟨w∗, x⟩)” in
the ﬁrst layer of the target network. This allows us to apply Lemma 6.3 again for the output layer,
so as to construct for instance a composite function Φ(·) with φi as its inputs.15
Remark. Lemma 6.4 may sound weird at ﬁrst look because random initialization cannot carry any
information about the target. There is no contradiction here, because we will show, B is essentially
another Gaussian (with the same distribution as ρ) times
constant −φ2(⟨w∗, x⟩), thus n1(x) can
still be independent of the value of φ(⟨w∗, x⟩). Nevertheless, the decomposition in Lemma 6.4 shall
enable us to show that, when we start to modify the hidden weights W, V , the learning process will
start to discover this structure and make the weight of the term relating to φ(⟨w∗, x⟩) stand out
from other terms.
Using Lemma 6.4 and applying Lemma 6.3 once more, we can prove Lemma 6.2.
Coupling Between Real and Pseudo Networks
Suppose we are currently at weights W (0) + W ′ + W ρ, V (0) + V ′ + V ρ, where matrices W ρ, V ρ are
random Gaussian matrices such that:
i,j ∼N(0, σ2
i,j ∼N(0, σ2
for some σv, σw ∈[1/(m1m2), 1] to be speciﬁed later, and W ′, V ′ are matrices with bounded norms
that can depend on the randomness of W (0), b(0)
1 , V (0), b(0)
2 . Intuitively, W ′ and V ′ capture how
much the algorithm has moved away from the initialization, while W ρ, V ρ are introduced for adding
smoothness in the optimization, see Section 6.4.
Let us introduce the notion of pseudo networks at the current weights. Let
• Dw,x denote the diagonal sign matrix of the ﬁrst layer at random initialization W (0),
• Dv,x denote the diagonal sign matrix of the second layer at random initialization W (0), V (0),
• Dw,x + D′
w,x ∈{0, 1}m1×m1 denote the diagonal sign matrix of the ﬁrst layer at weights
W (0) + W ′ + W ρ and V (0) + V ′ + V ρ, i.e., [Dw,x + D′
w,x]i,i = I[⟨w(0)
, x⟩+ b(0)
v,x ∈{0, 1}m2×m2 denote the diagonal sign matrix of the second layer at these weights.
For a ﬁxed r ∈[k], let us denote row vector ar = (ar,i)i∈[m2]. Deﬁne the pseudo network (and its
semi-bias, bias-free versions) as
gr(x; W, V ) = ar(Dv,x + D′
 V (Dw,x + D′
w,x) (Wx + b1) + b2
r (x; W, V ) = ar(Dv,x + D′
v,x)V (Dw,x + D′
w,x)(Wx + b1)
(x; W, V ) = ar(Dv,x + D′
v,x)V (Dw,x + D′
As a sanity check, at W (0) + W ′ + W ρ, V (0) + V ′ + V ρ the pseudo network equals the true one:
x; W (0) + W ′ + W ρ, V (0) + V ′ + V ρ
x; W (0) + W ′ + W ρ, V (0) + V ′ + V ρ
We state Lemma 6.5 below.
15One technical issue is the following. When considering multiple neurons of the second layer, those Gaussian
variables ρ are not independent because they all depend on W (0).
The notion of eρ in Lemma 6.4 removes such
dependency across multiple neurons, at the expense of small Wasserstein distance.
Lemma 6.5 (coupling). Suppose τv ∈
and η > 0. Given ﬁxed unit vector x, and perturbation matrices W ′, V ′, W ′′, V ′′ (that may depend
on the randomness of W (0), b(0)
1 , V (0), b(0)
and x) satisfying
∥W ′∥2,4 ≤τw, ∥V ′∥F ≤τv, ∥W ′′∥2,4 ≤τw, ∥V ′′∥F ≤τv ,
and random diagonal matrix Σ with each diagonal entry i.i.d. drawn from {±1}, then with high
probability the following holds:
1. (Sparse sign change). ∥D′
w,x∥0 ≤eO(τ 4/5
m2 + τ 2/3
2. (Cross term vanish).
gr(x; W (0) + W ρ + W ′ + ηΣW ′′, V (0) + V ρ + V ′ + ηV ′′Σ)
x; W (0) + W ρ + W ′, V (0) + V ρ + V ′
(x; ηΣW ′′, ηV ′′Σ) + g′
where EΣ[g′
r(x)] = 0 and with high probability |g′
r(x)| ≤η eO
√m1 + √m2τw
The ﬁrst statement “sparse sign change” of Lemma 6.5 says that, if we move from random
initialization W (0), V (0) to W (0) + W ′ + W ρ, V (0) + V ′ + V ρ, then how many signs of the ReLUs
(in each layer) will change, as a function of the norms of W ′ and V ′. This calculation is similar to
 but slightly more involved due to the ∥· ∥2,4 norm that we use here.
The second statement “cross term vanish” of Lemma 6.5 studies, if we are currently at weights
(W, V ) = (W (0) + W ′ + W ρ, V (0) + V ′ + V ρ) and want to move to (W + ηΣW ′′, V + ηV ′′Σ) where
Σ is a diagonal matrix with diagonal entries i.i.d. uniformly chosen from {±1}, then how does the
function value change in the pseudo network.
Coupling + Existence
We quickly point out a corollary by applying the coupling and existential lemmas together.
Recall in the existential Lemma 6.2, we have studied a pseudo network G(0) where the signs are
determined at the random initialization W (0), V (0). Now, the coupling Lemma 6.5 says that the
amount of sign change from G(0)) to G(b,b) can be controlled. Therefore, if parameters are chosen
appropriately, the existential Lemma 6.2 should also apply to G(b,b). Formally,
Corollary 6.6 (existence after coupling). In the same setting as Lemma 6.2, given perturbation
matrices W ′, V ′ (that may depend on the randomness of the initialization and the data distribution
∥W ′∥2,4 ≤τw, ∥V ′∥F ≤τv .
Using parameter choices from Table 1, w.h.p. there exist W ⋇and V ⋇(independent of the randomness of W ρ, V ρ) satisfying
∥W ⋇∥2,∞= max
∥V ⋇∥2,∞= max
r (x) −g(b,b)
(x; W ⋇, V ⋇)
L(G(b,b)(x; W ⋇, V ⋇), y)
In the three-layer network results, we choose parameters
m1/2−0.005
m3/4−0.005
τv = m1/2−0.001
for C0 = Cε(Φ, √p2Cε(φ, 1)) · Cε(φ, 1) · eO(p1√p2k) and ε =
2Cs(Φ,p2Cs(φ,1))Cs(φ,1)2
Table 1: Three-layer parameter choices (the constant inside Θ can depend on γ).
λw, λv give the weights of the regularizer λw∥· ∥4
2,4 + λv∥· ∥2
2,2 in the objective (6.1) and (4.2).
σw, σv give the amount of Gaussian perturbation we add to the objective (for analysis purpose).
v are set so that if the regularizer is bounded, it satisﬁes ∥W ′∥2,4 ≤τ ′
w and ∥V ′∥F ≤τ ′
τw, τv are set so that the coupling lemma works whenever ∥W ′∥2,4 ≤τw and ∥V ′∥F ≤τv.
As we shall see later, Corollary 6.6 gives rise to W ⋇and V ⋇that shall be used as a descent
direction for the objective. (Corollary 6.6 did not use all the parameters inside Table 1, and some
of those parameters shall be used in later subsections.)
Optimization
The na ¨ivelyapproach is to
• use the property that solutions with good risks are dense in the parameter space (i.e., Corollary 6.6)
to show that the optimization landscape of the overparameterized three-layer neural network
is benign: it has no spurious local minimal or (more or less) even any second-order critical
points; and
• use existing theorems on escaping saddle points (such as ) to show that SGD will not be
stuck in saddle points and thus converges.
Key issue.
Unfortunately, before digging into the details, there is already a big hole in this
approach. ReLU networks are not second-order-diﬀerentiable: a ReLU activation does not have
a well-deﬁned Hessian/sub-Hessian at zero. One may na ¨ivelythink that since a ReLU network
is inﬁnite-order diﬀerentiable everywhere except a measure zero set, so we can safely ignore the
Hessian issue and proceed by pretending that the Hessian of ReLU is always zero. This intuition
is very wrong.
Following it, we could have run into the absurd conclusion that any piece-wise
linear function is convex, since the Hessian of it is zero almost everywhere. In other words, the
only non-smooth point of ReLU has a Hessian value equal to the Dirac δ-function, but such nonsmooth points, albeit being measure zero, are actually turning points in the landscape. If we want a
meaningful second-order statement of the ReLU network, we must not na ¨ivelyignore the “Hessian”
of ReLU at zeros.
Smoothing. To ﬁx the na ¨iveapproach, we use Gaussian smoothing. Given any bounded function
f : Rm3 →R, we have that Eρ∼N(0,σ2I)[f(x+ρ)] is a inﬁnite-order diﬀerentiable function in x as long
as σ > 0. Thus, we can consider the smoothed version of the neural network: F(x; W +W ρ, V +V ρ)
where W ρ, V ρ are random Gaussian matrices. We show that E[L(F(x; W +W ρ, V +V ρ), y)] also has
the desired property of essentially having no second-order critical points. Perhaps worth pointing
out, the Hessian of this smoothed function is signiﬁcantly diﬀerent from the original one.
example, Eρ∼N(0,1)[σ(x + ρ)] has a Hessian value ≈1 at all x = [−1, 1], while in the original ReLU
function σ, the Hessian is 0 almost everywhere.
In practice, since the solution Wt, Vt are found by stochastic gradient descent starting from
random initialization, they will have a non-negligible amount of intrinsic noise. Thus, the additional
smoothing in the algorithm might not be needed by an observation in . Smooth analysis 
might also be used for analyzing the eﬀect of such noise, but this is beyond the scope of this paper.
Actual algorithm.
Let us consider the following smoothed, and regularized objective:
L′(λt, Wt, Vt) = EW ρ,V ρ,(x,y)∼Z
x; W (0) + W ρ + Wt, V (0) + V ρ + Vt
where W ρ, V ρ are Gaussian random matrices with each entry i.i.d. from N(0, σ2
w) and N(0, σ2
respectively.
R(√λtWt, √λtVt) = λv∥√λtVt∥2
F + λw∥√λtWt∥4
2,4 and λv, λw are set such that
λv∥√λtV ⋇∥2
F ≤ε0 and λw∥√λtW ⋇∥4
2,4 ≤ε0 for every W ⋇and V ⋇coming from Lemma 6.2.
See Algorithm 3 (ﬁrst SGD variant) for the details. We prove the following lemma.
Lemma 6.7 (descent direction). For every ε0 ∈(0, 1) and ε =
2Cs(Φ,p2Cs(φ,1))Cs(φ,1)2 , for every
constant γ ∈(0, 1/4], consider the parameter choices in Table 1, and consider any λt, Wt, Vt (that
may depend on the randomness of W (0), b(0), V (0), b(1) and Z) with
 (ε/ log(m1m2))Θ(1), 1
L′(λt, Wt, Vt) ∈[(1 + γ)OPT + Ω(ε0/γ), eO(1)]
With high probability over the random initialization, there exists W ⋇, V ⋇with ∥W ⋇∥F , ∥V ⋇∥F ≤1
such that for every η ∈
poly(m1,m2)
L′  λt, Wt + √ηΣW ⋇, Vt + √ηV ⋇Σ
, L′ (1 −η)λt, Wt, Vt
≤(1 −ηγ/4)(L′(λt, Wt, Vt)) ,
where Σ ∈Rm1×m1 is a diagonal matrix with each diagonal entry i.i.d. uniformly drawn from {±1}.
Lemma 6.7 says one of the following two scenarios will happen. Either (1) there exist W ⋇, V ⋇
so that updating in a random direction (ΣW ⋇, V ⋇Σ) decreases the objective, or (2) performing
weight decay decreases the objective. It is a simple exercise to check that, if (1) happens, then
F ′(λ, ·, ·) has a very negative curvature in the Hessian (see Fact A.8) at the current point. Therefore,
Lemma 6.7 essentially says that after appropriate regularization, every second-order critical point
of L′ is approximately global minimum.
More interestingly, since (noisy) stochastic gradient descent is capable of ﬁnding approximate
second-order critical points with a global convergence rate, one can show the following ﬁnal convergence rate for minimizing L′ for Algorithm 3.
Lemma 6.8 (convergence). In the setting of Theorem 3, with probability at least 99/100, Algorithm 3
(the ﬁrst SGD variant) converges in TTw = poly (m1, m2) iterations to a point
L′(λT , WT , VT ) ≤(1 + γ)OPT + ε0.
Details of Smoothing
The next lemma shows that when doing a small update to the current weight, one can view the
sign pattern as ﬁxed for the smoothed objective, up to a small error. Speciﬁcally, for every input
x and every W = W (0) + W ′, V = V (0) + V ′, let Σ be a random diagonal matrix with ± entries,
• Dw,x,ρ denote the diagonal matrix with diagonals being 0-1 signs of the ﬁrst layer at W + W ρ;
• Dw,x,ρ,η denote of the ﬁrst layer at weights W + W ρ + ηΣW ′′;
• Dv,x,ρ denote that of the second layer at weights W + W ρ and V + V ρ; and
• Dv,x,ρ,η denote that of the second layer at weights W + W ρ + ηΣW ′′ and V + V ρ + ηΣV ′′.
For a ﬁxed r ∈[k], consider the real network Pρ,η and the pseudo network P ′
= fr(x; W + W ρ + ηΣW ′′, V + V ρ + ηV ′′Σ)
= arDv,x,ρ,η
(V + V ρ + ηV ′′Σ)Dw,x,ρ,η
 (W + W ρ + ηΣW ′′)x + b1
= gr(x; W + W ρ + ηΣW ′′, V + V ρ + ηV ′′Σ)
= arDv,x,ρ
(V + V ρ + ηV ′′Σ)Dw,x,ρ
 (W + W ρ + ηΣW ′′)x + b1
We prove the following lemma:
Lemma 6.9 (smoothed real vs pseudo). There exists η0 =
poly(m1,m2) such that, for every η ≤η0,
for every ﬁxed x with ∥x∥2 = 1, for every W ′, V ′, W ′′, V ′′ that may depend on the randomness of
the initialization and
∥W ′∥2,4 ≤τw,
∥V ′∥2,2 ≤τv,
∥W ′′∥2,∞≤τw,∞,
∥V ′′∥2,∞≤τv,∞
we have with high probability:
|Pρ,η −P ′
where Op hides polynomial factor of m1, m2.
In other words, when working with a Gaussian smoothed network (with output Pρ,η), it suﬃces
to study a pseudo network (with output P ′
ρ,η). In the proof of Lemma 6.7, this allows us to go from
the real (smoothed) network to the pseudo (smoothed) network, and then apply Lemma 6.5.
Generalization
In our ﬁrst variant of SGD Algorithm 3, we show a very crude Rademacher complexity bound that
can be derived from the contraction lemma (a.k.a. Talagrand’s concentration inequality).
Lemma 6.10 (generalization for LR = L1). For every τ ′
w ≥0, every σv ∈(0, 1/√m2], w.h.p.
for every r ∈[k] and every N ≥1, the empirical Rademacher complexity is bounded by
N Eξ∈{±1}N
∥V ′∥F ≤τ ′v,∥W ′∥2,4≤τ ′w
ξifr(xi; W (0) + W ρ + W ′, V (0) + V ρ + V ′)
wm1√m2 + τ ′
m1m2τ ′w(1/√m1 + τ ′w)
Since the population risk is bounded by the Rademacher complexity, combining this with Lemma 6.8,
one can easily prove Theorem 3.
Second Variant of SGD
In our second variant of SGD Algorithm 2, we have added the Dropout-type noise matrix Σ directly
into the objective L2 (see (4.2)). Our new stochastic objective is the following.
L′′(λt, Wt, Vt) = EW ρ,V ρ,Σ,x,y∼Z
W (0) + W ρ + ΣWt, V (0) + V ρ + VtΣ, x
To show that this gives a better sampling complexity bound, we need the following stronger
coupling lemma. It gives a somewhat better bound on the “cross term vanish” part comparing to
the old coupling Lemma 6.5.
Lemma 6.11 (stronger coupling). With high probability over the random initialization and over a
random diagonal matrix Σ with diagonal entries i.i.d. generated from {−1, 1}, it satisﬁes that for
every W ′, V ′ with ∥V ′∥2 ≤τv, ∥W ′∥2,4 ≤τw for τv ∈ and τw ∈
fr(x; W (0) + ΣW ′, V (0) + V ′Σ) = arDv,x(V (0)Dw,x(W (0)x + b1) + b2) + arDv,xV ′Dw,xW ′x
Under parameter choices Table 1, the last error term is at most ε/k.
For this SGD variant, we also have the following the stronger Rademacher complexity bound.
It relies on Lemma 6.11 to reduce the function class to pseudo networks, which are only linear
functions in W ′ and V ′, and then computes its Rademacher complexity.
Lemma 6.12 (generalization for LR = L2). For every τ ′
v ∈ , τ ′
σw ∈[0, 1/√m1] and σv ∈[0, 1/√m2], w.h.p. for every r ∈[k] and every N ≥1, we have by our
choice of parameters in Lemma 6.7, the empirical Rademacher complexity is bounded by
N Eξ∈{±1}N
∥V ′∥F ≤τ ′v,∥W ′∥2,4≤τ ′w
ξiEΣ[fr(xi; W (0) + W ρ + ΣW ′, V (0) + V ρ + V ′Σ)]
w)8/5m9/10
w)16/5m9/5
Under parameter choices in Table 1, this is at most eO
After plugging Lemma 6.11 and Lemma 6.12 into the ﬁnal proof, we can show Theorem 2.
Remark. In contrast, without the Dropout-type noise matrix Σ, the error term in the old coupling
Lemma 6.5 is too large so we cannot get better Rademacher complexity bounds.
Empirical Evaluations
We discuss our experiment details for Figure 1.
Recall that we generate synthetic data where the feature vectors x ∈R4 are generated as
random Gaussian then normalized to norm 1, and labels are generated by target function y =
F ∗(x) = (sin(3x1) + sin(3x2) + sin(3x3) −2)2 · cos(7x4). Intuitively, the constants 3 and 7 control
Test error
m = number of hidden neurons
(a) N = 1000 and vary m
Test error
N = number of samples
3layer(last)
2layer(last)
3layer(NTK)
(b) m = 2000 and vary N
Figure 2: Performance comparison.
3layer/2layer stands for training (hidden weights) in three and
two-layer neural networks. (last) stands for conjugate kernel , meaning training only the
output layer. (NTK) stands for neural tangent kernel with ﬁnite width.
We consider ℓ2 regression task on synthetic data where feature vectors x ∈R4
are generated as normalized random Gaussian, and label is generated by target function
y = F ∗(x) = (tanh(8x1) + tanh(8x2) + tanh(8x3) −2)2 · tanh(8x4).
the complexity of the activation functions in the target function, and we choose these values to
ensure that the two factors in the target function have roughly the same complexity.16
To be more consistent with our theorems, we implement fully connected neural networks and
train only hidden weights (namely, W in the two-layer case and W, V in the two-layer case). We
also implement NTK with respect to only hidden weights. For conjugate kernel, we only train the
last (output) layer, that is, the weights ar ∈Rm for r ∈[k] in the language of this paper.
To be consistent with our theorems, we choose random initialization as follows. Entries of ar
are i.i.d. from N(0, 1), and entries of W, V, b1, b2 are i.i.d. from N(0, 1
m). This ensures that the
output at random initialization is Θ(1).
We use the default SGD optimizer of pytorch, with momentum 0.9, mini-batch size 50, learning
rate lr and weight decay parameter wd. We carefully run each algorithm with respect to lr and wd
in the set {10−k, 2 · 10−k, 5 · 10−k : k ∈Z}, and presents the best one in terms of testing accuracy.
In each parameter setting, we run SGD for 800 epochs, and decrease lr by 10 on epoch 400.
In Figure 2, we provide an additional experiment for target function y = F ∗(x) = (tanh(8x1) +
tanh(8x2) + tanh(8x3) −2)2 · tanh(8x4).
Justiﬁcation of Our ∥W∥2,4 Regularizer
Recall our three-layer theorem requires a slightly unconventional regularizer, namely, the ∥W∥2,4
norm of the ﬁrst layer to encourage weights to be more evenly distributed across neurons. Is this
really necessary? To get some preliminary idea we run our aforementioned three-layer experiment
on the ﬁrst data set,
• once with the traditional weight decay on W (which corresponds to minimizing ∥W∥F ), and
• once with the ∥W∥2,4 norm without weight decay.
16Recall that our sample complexity in Theorem 2 has the form C(Φ, C(φ)) ignoring other parameters. But in fact,
one can be more careful on this target function and derive a sample complexity of the form C(Φ, C(φ1))·C(φ2), where
Φ(x) = (x −2)2, φ1(x) = sin(3x) and φ2(x) = cos(7x). Because of this, to show the best contrast between two and
three-layer networks, we choose constant 7 on φ2 so that C(φ2) ≈C(φ1)2.
500 1000 2000 5000
test error
m = number of hidden neurons
500 1000 2000 5000
norm ratio
m = number of hidden neurons
Figure 3: Empirical comparison between regularizers ∥W∥F and ∥W∥2,4.
In both cases we best tune the learning rates as well as the weight decay parameter (or the weight
of the ∥W∥4
2,4 regularizer). We present our ﬁndings in Figure 3.
In Figure 3(a), we observe that there is no real diﬀerence between the two regularizers in terms
of test error. In one case (m = 200) using ∥W∥2,4 even gives slightly better test accuracy (but we
do not want to claim this as a general phenomenon).
More importantly, since by Cauchy-Schwarz we have ∥W∥F ≥∥W∥2,4 ≥m−1/4∥W∥F . Let us
consider the following norm ratio:
If the ratio is close to 1, then one can argue that the more evenly distributed the rows (i.e., neurons)
of W ∈Rm×d are. In contrast, if ratio is close to m then the weights in W are concentrated to
a single row. (Of course, we cannot expect ratio to be really close to 1 since there may not exist
such solutions with good accuracy to begin with.)
In Figure 3(b), we see that ratio is roughly the same between two types of regularizers. This
means, even if we regularize only the Frobenius norm, weights are still suﬃciently distributed across
neurons comparing to what we can do by regularizing ∥W∥2,4. We leave it a future work to study
why SGD encourages such implicit regularization.
Appendix: Complete Proofs
We provide technical preliminaries in Appendix A, give our two-layer proofs in Appendix B,
and three-layer proofs in Appendix C.
Technical Preliminaries
Wasserstein distance.
The ℓ2 Wasserstein distance between random variables A, B is
(X,Y ) s.t. X∼A,Y ∼B E
where the inﬁmum is taken over all possible joint distributions over (X, Y ) where the marginal on
X (resp. Y ) is distributed in the same way as A (resp. B).
Slightly abusing notation, in this paper, we say a random variable X satisﬁes |X| ≤B with
high probability if (1) |X| ≤B w.h.p. and (2) W2(X, 0) ≤B. For instance, if g = N(0, 1/m), then
|g| ≤eO(1/√m) with high probability.
Probability
Lemma A.1 (Gaussian indicator concentration). Let (n1, α1, a1,1, a2,1), · · · , (nm, αm, a1,m, a2,m)
be m i.i.d. samples from some distribution, where within a 4-tuples:
• the marginal distribution of a1,i and a2,i is standard Gaussian N(0, 1);
• ni and αi are not necessarily independent;
• a1,i and a2,i are independent; and
• ni and αi are independent of a1,i and a2,i.
Suppose h: R →[−L, L] is a ﬁxed function. Then, for every B ≥1:
a1,ia2,iI[ni ≥0]h(αi)
≥BL(√m + B)
1,iI[ni ≥0]h(αi)
1,1I[n1 ≥0]h(α1)]
≥BL(√m + B)
≤4e−B2/8.
Proof of Lemma A.1. Let us consider a ﬁxed n1, α1, · · · , nm, αm, then since each |I[ni ≥0]h(αi)| ≤
L, by Gaussian chaos variables concentration bound (e.g., Example 2.15 in ) we have that
a1,ia2,iI[ni ≥0]h(αi)
≥BL(√m + B)
{ni, αi}i∈[m]
≤4e−B2/8.
Since this holds for every choice of {ni, αi}i∈[m] we can complete the proof. The second inequality
follows from sub-exponential concentration bounds.
Proposition A.2. If X1, X2 are independent, and X1, X3 are independent conditional on X2, then
X1 and X3 are independent.
Proof. For every x1, x2, x3:
Pr[X1 = x1, X3 = x3 | X2 = x2] = Pr[X1 = x1 | X2 = x2] Pr[X3 = x3 | X2 = x2]
= Pr[X1 = x1] Pr[X3 = x3 | X2 = x2].
Multiplying Pr[X2 = x2] on both side leads to:
Pr[X1 = x1, X3 = x3, X2 = x2] = Pr[X1 = x1] Pr[X3 = x3, X2 = x2].
Marginalizing away X2 gives Pr[X1 = x1, X3 = x3] = Pr[X1 = x1] Pr[X3 = x3], so X1 and X3 are
independent.
Central Limit Theorem
The following Wasserstein distance bound of central limit theorem is easy to derive from known
Lemma A.3 (new CLT). Let X1, · · · , Xm ∈R be m independent zero-mean random variables with
each |Xi| ≤C, P
i∈[m] E[X2
i ] = V , then there exists Z ∼N(0, V ) such that
i=1 Xi, Z) = O (C log m) .
Proof. Deﬁne Σi = E[X2
i ] and without loss of generality assume Σ1 ≥Σ2 ≥· · · ≥Σm. Let us apply
[60, Lemma 1.6] on each i, then for every t such that t ≥5C2
Σi , let Yt ∼N(0, tΣi) be independent
of Xi, we have
W2(Yt, Yt−1 + Xi) ≤5C
In other words—by choosing t = Ri/Σi— we have for every Ri ≥5C2, letting Zi ∼N(0, Ri),
Zi+1 ∼N(0, Ri + Σi) be independent gaussian (also independent of Xi), it satisﬁes
W2(Zi+1, Zi + Xi) ≤5CΣi
Repeatedly applying the above inequality and starting with Z1 ∼N(0, 5C2) and choosing Ri =
5C2 + Pi−1
j=1 Σj, we have Zi ∼N(0, 5C2 + Pi−1
j=1 Σj) and Zm+1 ∼N(0, 5C2 + V ). Using Σ1 ≥Σ2 ≥
· · · ≥Σm, we know that 5CΣi
i−1 for i ≥2. This implies that
i −1 + W2 (Z2, Z1 + X1) = O (C log m) + W2 (Z2, Z1 + X1)
Finally, since |X1| ≤C we have W2(X1, 0) ≤C and Σ2
1 ≤C2. By triangle inequality we have
W2 (Z2, Z1 + X1) ≤W2(X1, 0) + W2(Z1, 0) + W2(Z2, 0) = O(C)
There exists Z ∼N(0, V ) such that W2(Zm+1, Z) = O(C). All of these together imply
= O (C log m) .
Interval Partition
Lemma A.4 (Interval Partition). For every τ ≤
100, there exists a function s: [−1, 1] × R →
{−1, 0, 1} and a set I(y) ⊂[−2, 2] for every y ∈[−1, 1] such that, for every y ∈[−1, 1],
1. (Indicator). s(y, g) = 0 if g /∈I(y), and s(y, g) ∈{−1, 1} otherwise.
2. (Balanced). Prg∼N(0,1)[g ∈I(y)] = τ for every y ∈[−1, 1].
3. (Symmetric). Prg∼N(0,1) [s(y, g) = 1] = Prg∼N(0,1) [s(y, g) = −1].
4. (Unbiased). Eg∼N(0,1)[s(y, g)g | g ∈I(y)] = y.
5. (Bounded). maxx∈I(y){s(y, x)x} −minx∈I(y){s(y, x)x} ≤10τ.
6. (Lipschitz). |I(y1)△I(y2)| ≤O(|y2 −y1|) , where |I|
x∈I dx is the measure of set I ⊆R.
We refer to I(y) as an “interval” although it may actually consist of two disjoint closed intervals.
Proof of Lemma A.4. Let us just prove the case when y ≥0 and the other case is by symmetry.
It is clear that, since there are only two degrees of freedom, there is a unique interval I1(y) =
[y −a(y), y + b(y)] with a(y), b(y) ≥0 such that
1. (Half probability). Prg∼N(0,1)[g ∈I1(y)] = τ
2. (Unbiased). Eg∼N(0,1)[g | g ∈I1(y)] = y.
Next, consider two cases:
1. Suppose [y −a(y), y + b(y)] and [−y −b(y), −y + a(y)] are disjoint. In this case, we just deﬁne
= [y −a(y), y + b(y)] ∪[−y −b(y), −y + a(y)] and deﬁne
if g ∈[y −a(y), y + b(y)];
if g ∈[−y −b(y), −y + a(y)];
otherwise.
2. [y−a(y), y+b(y)] and [−y−b(y), −y+a(y)] intersect. In this case, consider the unique interval
I2(y) = [−e(y), e(y)]
where e(y) ≥0 is deﬁned so that
Eg∼N(0,1)[g | g ∈I2(y) ∧g > 0] = Eg∼N(0,1)[|g| | g ∈I2(y)] = y .
It must satisfy Prg∼N(0,1)[g ∈I2(y) ∧g > 0] < τ/2, because otherwise we must have y−a(y) ≥
0 and the two intervals should not have intersected.
Deﬁne τ ′(y) = Prg∼N(0,1)[g ∈I2(y)] < τ. Let c(y) > e(y) be the unique positive real such that
g∼N(0,1) [g ∈[e(y), c(y)]] = τ −τ ′(y)
Let d(y) ∈[e(y), c(y)] be the unique real such that
g∼N(0,1) [g ∈[e(y), d(y)]] =
g∼N(0,1) [g ∈[d(y), c(y)]] .
Finally, we deﬁne I(y) = [−c(y), c(y)] and
if g ∈[0, e(y)] ∪[e(y), d(y)] ∪[−d(y), −e(y)];
if g ∈[−e(y), 0] ∪[d(y), c(y)] ∪[−c(y), −d(y)];
otherwise.
In both cases, one can carefully verify that properties 1, 2, 3, 4 hold. Property 5 follows from the
standard property of Gaussian random variable under condition τ ≤1/100 and y ∈[−1, 1].
To check the ﬁnal Lipschitz continuity property, recall for a standard Gaussian distribution,
inside interval [−1
10] it behaves, up to multiplicative constant factor, similar to a uniform distribution. Therefore, the above deﬁned functions a(y) and b(y) are O(1)-Lipschitz continuous in y.
Let y0 ≥0 be the unique constant such that y−a(y) = 0 (it is unique because y−a(y) monotonically
decreases as y →0+. It is clear that for y0 ≤y1 ≤y2 it satisﬁes
|I(y1)△I(y2)| ≤O(y2 −y1) .
As for the turning point of y = y0, it is clear that
y→y0+ I(y) = [−y0 −b(y0), y0 + b(y0)] = [−e(y0), e(y0)] =
so the function I(·) is continuous at point y = y0. Finally, consider y ∈[−y0, y0]. One can verify
that e(y) is O(1)-Lipschitz continuous in y, and therefore the above deﬁned τ ′(y), d(y) and c(y)
are also O(1)-Lipschitz in y. This means, for −y0 ≤y1 ≤y2 ≤y0, it also satisﬁes
|I(y1)△I(y2)| ≤O(y2 −y1) .
This proves the Lipschitz continuity of I(y).
Hermite polynomials
Deﬁnition A.5. Let hi(i ≥0) denote the degree-i (probabilists’) Hermite polynomial
m!(i −2m)!
satisfying the orthogonality constraint
Ex∼N(0,1)[hi(x)hj(x)] =
where δi,j = 1 if i = j and δi,j = 0 otherwise. They have the following summation and multiplication
hi(x + y) =
xi−khk(y),
γi−2k(γ2 −1)k
k! 2−khi−2k(x).
Lemma A.6.
(a) For even i > 0, for any x1 ∈ and b,
Eα,β∼N(0,1)
pi = (i −1)!!exp(−b2/2)
(b) For odd i > 0, for any x1 ∈ and b,
Eα,β∼N(0,1)
pi = (i −1)!!exp(−b2/2)
r=0,r even
(Throughout this paper, the binomial
is deﬁned as
Γ(m+1)Γ(n+1−m), and this allows us to write
for instance
without notation change.)
Proof. Using the summation formula of Hermite polynomial, we have:
(αx1)i−khk
Using the multiplication formula of Hermite polynomial, we have:
k−2j  −x2
j! 2−jhk−2j(β).
For even k, since Eβ∼N(0,1)[hn(β)] = 0 for n > 0, we have
(k/2)!2−k/2,
and for odd k,
This implies
k=0,k even
(αx1)i−k  −x2
(k/2)!2−k/2
k=0,k even
(k/2)!(−2)−k/2.
Therefore,
Eα,β∼N(0,1)
k=0,k even
Eα∼N(0,1)[αi−kI[α ≥b]]
(k/2)!(−2)−k/2 .
Li,b := Eα∼N(0,1)[αiI[α ≥b]].
(a) Consider even i > 0. By Lemma A.7, we have for even i ≥0:
Li,b = (i −1)!!Φ(0, 1; b) + φ(0, 1; b)
Eα,β∼N(0,1)
k=0,k even
(k/2)!(−2)−k/2
k=0,k even
(i −k −1)!!Φ(0, 1; b)
(k/2)!(−2)−k/2
1φ(0, 1; b)
k=0,k even
(i −k −1)!!
(k/2)!(−2)−k/2
k=0,k even
(i −k −1)!!
(k/2)!(−2)−k/2 =
k=0,k even
i!(i −k −1)!!
(i −k)!(k/2)!
k=0,k even
(i −k)!!(k/2)!
= (i −1)!!
k=0,k even
(i −k)!!(k/2)!
= (i −1)!!
k=0,k even
we know that
Eα,β∼N(0,1)
1(i −1)!!φ(0, 1; b)
where cr is given by:
k=0,k even
(i −k −1)!!
(k/2)!(−2)−k/2
k=0,k even
j −i/2 −1
−i/2 + (i −1 −r)/2
(i −1 −r)/2
= (−1)(i−1−r)/2
(i −1 −r)/2
(b) Consider odd i > 0. By Lemma A.7, we have for odd i > 0:
Li,b = φ(0, 1; b)
j=0,j even
Eα,β∼N(0,1)
k=0,k even
(k/2)!(−2)−k/2
1φ(0, 1; b)
k=0,k even
j=0,j even
(i −k −1)!!
(k/2)!(−2)−k/2
1(i −1)!!φ(0, 1; b)
r=0,r even
where cr is given by:
k=0,k even
(i −k −1)!!
(k/2)!(−2)−k/2
by a similar calculation as in the even i case.
The proof is completed.
Lemma A.7. Deﬁne Li,b as:
Li,b := Eα∼N(0,1)[αiI[α ≥b]].
Then Li,b’s are given by the recursive formula:
L0,b = Φ(0, 1; b) :=
α∼N(0,1)[α ≥b],
L1,b = φ(0, 1; b) := Eα∼N(0,1)[αI[α ≥b]] = exp(−b2/2)
Li,b = bi−1φ(0, 1; b) + (i −1)Li−2,b.
As a result (with the convention that 0!! = 1 and (−1)!! = 1)
for even i ≥0:
Li,b = (i −1)!!Φ(0, 1; b) + φ(0, 1; b)
for odd i > 0:
Li,b = φ(0, 1; b)
j=0,j even
One can verify that for b ≥0,
Li,b ≤O(1)e−b2/2 ·
Proof. The base cases L0,b and L1,b are easy to verify. Then the lemma comes from induction. □
Optimization
Fact A.8. For every B-second-order smooth function f : Rd →R, every ε > 0, every η ∈
, every ﬁxed vector x ∈Rd, suppose there is a random vector x2 ∈Rd with E[x2] = 0 and
∥x2∥2 = 1 satisfying
Ex2[f (x + √ηx2)] ≤f(x) −ηε .
Then, λmin(∇2f(x)) ≤−ε, where λmin is the minimal eigenvalue.
Proof of Fact A.8. We know that
f (x + √ηx2) = f(x) + ⟨∇f(x), √ηx2⟩+ 1
2 (√ηx2)⊤∇2f(x) (√ηx2) ± O(Bη1.5).
Taking expectation, we know that
E[f (x + √ηx2)] = f(x) + η1
2 ∇2f(x)x2
± O(Bη1.5)
2 ∇2f(x)x2
≤−ε, which completes the proof.
We also recall the following convergence theorem of SGD for escaping saddle point.17
17The original proof of was for constant probability but it is easy to change it to 1−p at the expense of paying
a polynomial factor in 1/p. The original proof did not state the objective “non-increasing” guarantee but it is easy to
show it for mini-batch SGD. Recall η =
poly(d,B,1/ε,1/δ) in so in each iteration, the original SGD may increase the
objective by no more than η2B
if a batch size b is used. If b is polynomially large in 1/η, this goal is easily achievable.
In practice, however, using a very small batch size suﬃces.
Lemma A.9 (escape saddle points, Theorem 6 of ). Suppose a function f : Rd →R has its
stochastic gradient bounded by B in Euclidean norm, is absolutely bounded |f(x)| ≤B, is B-smooth,
and is B-second-order smooth, then for every δ > 0, every p ∈(0, 1), with probability at least 1 −p,
noisy SGD outputs a point xT after T = poly(d, B, 1/δ, 1/p) iterations such that
∇2f(xT ) ⪰−δI
f(xT ) ≤f(x0) + δ · poly(d, B, 1/p)
Rademacher Complexity
Let F be a set of functions Rd →R and X = (x1, . . . , xN) be a ﬁnite set of samples. Recall the
empirical Rademacher complexity with respect to X of F is
= Eξ∼{±1}N
Lemma A.10 (Rademacher generalization). Suppose X = (x1, . . . , xN) where each xi is generated
i.i.d. from a distribution D. If every f ∈F satisﬁes |f| ≤b, for every δ ∈(0, 1) with probability at
least 1 −δ over the randomness of Z, it satisﬁes
Ex∼D[f(x)] −1
≤2bR(Z; F) + O
Corollary A.11. If F1, . . . , Fk are k classes of functions Rd →R and Lx : Rk →[−b, b] is a
1-Lipschitz continuous function for any x ∼D, then
f1∈F1,...,fk∈Fk
Ex∼D[Lx(f1(x), . . . , fk(x))] −1
Proof. Let F′ be the class of functions by composing L with F1, . . . , Fk, that is, F′ = {Lx ◦
(f1, . . . , fk) | f1 ∈F1 · · · fk ∈Fk}. By the (vector version) of the contraction lemma of Rademacher
complexity18 it satisﬁes bR(Z; F′) ≤O(1) · Pk
r=1 bR(Z; Fr).
Proposition A.12. We recall some basic properties of the Rademacher complexity. Let σ: R →R
be a ﬁxed 1-Lipschitz function.
(a) (ℓ2 linear) Suppose ∥x∥2 ≤1 for all x ∈X. The class F = {x 7→⟨w, x⟩| ∥w∥2 ≤B} has
Rademacher complexity bR(X; F) ≤O( B
(b) (ℓ1 linear) Suppose ∥x∥∞≤1 for all x ∈X ⊆Rm. The class F = {x 7→⟨w, x⟩| ∥w∥1 ≤B}
has Rademacher complexity bR(X; F) ≤O(B√log m
(c) (addition) bR(X; F1 + F2) = bR(X; F1) + bR(X; F2).
(d) (contraction) bR(X; σ ◦F) ≤bR(X; F).
(e) Given F1, . . . , Fm classes of functions X →R and suppose w ∈Rm is a ﬁxed vector, then
j=1 wjσ(fj(x))
satisﬁes bR(X; F′) ≤2∥w∥1 maxj∈[m] bR(X; Fj).
18There are slightly diﬀerent versions of the contraction lemma in the literature.
For the scalar case without
absolute value, see [43, Section 3.8]; for the scalar case with absolute value, see [14, Theorem 12]; and for the vector
case without absolute value, see .
(f) Given F1, . . . , Fm classes of functions X →R and suppose for each j ∈[m] there exist a
function f(0)
∈Fj satisfying supx∈X |σ(f(0)
(x))| ≤R, then
j=1 vjσ(fj(x))
fj ∈Fj ∧∥v∥1 ≤B ∧∥v∥∞≤D
satisﬁes bR(X; F′) ≤2D P
j∈[m] bR(X; Fj) + O
  BR log m
Proof. The ﬁrst three are trivial, and the contraction lemma is classical (see for instance ). The
derivations of Lemma A.12e and Lemma A.12f are less standard.
For Lemma A.12e, let us choose an arbitrary f(0)
from each Fj. We write f ∈F to denote
(f1, . . . , fm) ∈F1 × · · · × Fm.
wjσ(fj(xi))
 σ(fj(xi)) −σ(f(0)
 σ(fj(xi)) −σ(f(0)
 σ(fj(xi)) −σ(f(0)
 σ(fj(xi)) −σ(f(0)
ξiσ(fj(xi))
≤2∥w∥1 · N max
bR(X; σ ◦Fj)
≤2N∥w∥1 · max
bR(X; Fj) .
Above, x is because ξiwjσ(f(0)
(xi)) is independent of fj and thus zero in expectation; y uses the
non-negativity of supfj∈Fj
 σ(fj(xi)) −σ(f(0)
; z is for the same reason as x; and {
is by Proposition A.12d.
For Lemma A.12f,
f∈F,∥v∥1≤B,∥v∥∞≤D
vjσ(fj(xi))
f∈F,∥v∥1≤B,∥v∥∞≤D
 σ(fj(xi)) −σ(f(0)
f∈F,∥v∥∞≤D
 σ(fj(xi)) −σ(f(0)
 σ(fj(xi)) −σ(f(0)
 σ(fj(xi)) −σ(f(0)
ξiσ(fj(xi))
bR(X; σ ◦Fj) + Eξ
bR(X; σ ◦Fj) + O(BR
N log m) .
Above, x is from the same derivation as Proposition A.12e; and y uses Proposition A.12b by
viewing function class
j=1 vj · σ(f(0)
as linear in v.
Proofs for Two-Layer Networks
Recall from (3.1) the target F ∗= (f∗
1 , · · · , f∗
k) for our two-layer case is
1,i, x⟩)⟨w∗
We consider another function deﬁned as G(x; W) = (g1(x; W), . . . , gk(x; W)) for the weight matrix
r,i (⟨wi, x⟩+ b(0)
i )I[⟨w(0)
i , x⟩+ b(0)
where wi is the r-th row of W and w(0)
is the r-th row of W (0) (our random initialization). We
call this G a pseudo network. For convenience, we also deﬁne a pseudo network G(b)(x; W) =
1 (x; W), . . . , g(b)
k (x; W)) without bias
r,i ⟨wi, x⟩I[⟨w(0)
i , x⟩+ b(0)
Our analysis begins with showing that w.h.p. over the random initialization, there
exists a pseudo network in the neighborhood of the initialization that can approximate the target
function (see Section B.1). We then show that, near the initialization, the pseudo network approximates the actual ReLU network F (see Section B.2). Therefore, there exists a ReLU network
near the initialization approximating the target. Furthermore, it means that the loss surface of the
ReLU network is close to that of the pseudo network, which is convex. This then allows us to show
the training converges (see Section B.3). Combined with a generalization bound localized to the
initialization, we can prove the ﬁnal theorem that SGD learns a network with small risk.
The proof is much simpler than the three-layer case. First, the optimization landscape is almost
convex, so a standard argument for convex optimization applies. While for three-layer case, the
optimization landscape is no longer this nice and needs an escaping-from-saddle-point argument,
which in turn requires several technicalities (smoothing, explicit regularization, weight decay, and
Dropout-like noise). Second, the main technical part of the analysis, the proof of the existential
result, only needs to deal with approximating one layer of neurons in the target, while in the threelayer case, a composition of the neurons needs to be approximating, requiring additional delicate
arguments. It is also similar with the generalization bounds. However, we believe that the analysis
in the two-layer case already shows some of the key ideas, and suggest the readers to read it before
reading that for the three-layer case.
Existential Result
The main focus of this subsection is to show that there exists a good pseudo network near the
initialization. (Combining with the coupling result of the next subsection, this translates to the
real network near the initialization.)
Lemma B.1. For every ε ∈(0,
pkCs(φ,1)), letting εa = ε/eΘ(1), there exists
M = poly(Cε(φ, 1), 1/ε)
such that if m ≥M, then with high probability there exists W ⋇= (w⋇
1 , . . . , w⋇
m) with ∥W ⋇∥2,∞≤
εam and ∥W ⋇∥F ≤eO(kpCs(φ,1)
r (x) −g(b)
r (x; W ⋇)
and consequently,
L(G(b)(x; W ⋇), y)
Corollary B.2. In the same setting as Lemma B.1, we have that w.h.p.
r (x) −gr(x; W (0) + W ⋇)
and consequently,
L(G(x; W (0) + W ⋇), y)
Proof of Lemma B.1. Recall the pseudo network without bias is given by
r (x; W) =
r,i (⟨wi, x⟩+ b(0)
i )I[⟨w(0)
i , x⟩+ b(0)
Also recall from Lemma 6.3 that, for each i ∈[p], there is function h(i) : R2 →R with |h(i)| ≤
Cε(φ, 1), satisfying
1+b0≥0h(i)(α1, b0)
where α1, β1, b0 ∼N(0, 1) are independent random Gaussians.
Fit a single function a∗
1,i, x⟩)⟨w∗
We ﬁrst ﬁx some r ∈[k] and i ∈[p] and
construct weights w⋇
j ∈Rd. Deﬁne
r,ih(i) √m⟨w(0)
1,i⟩, √mb(0)
where √m⟨w(0)
1,i⟩, √mb(0)
has the same distribution with α1, b0 in Lemma 6.3. By Lemma 6.3,
we have that
r,j I⟨w(0)
≥0h(i) √m⟨w(0)
1,i⟩, √mb(0)
1,i, x⟩)⟨w∗
2,i, x⟩± ε.
Fit a combination P
1,i, x⟩)⟨w∗
We can re-deﬁne (the norm grows by a
maximum factor of p)
r,ih(i) √m⟨w(0)
1,i⟩, √mb(0)
and the same above argument gives
r,j I⟨w(0)
1,i, x⟩)⟨w∗
2,i, x⟩± εp.
Fit multiple outputs.
If there are k outputs let us re-deﬁne (the norm grows by a maximum
factor of k)
r,ih(i) √m⟨w(0)
1,i⟩, √mb(0)
and consider the quantity
j , x⟩I[⟨w(0)
j , x⟩+ b(0)
By deﬁnition of the initialization, we know that for r′ ̸= r, E[a(0)
r′,j] = 0. Thus, for every r ∈[k],
it satisﬁes
1,j,...,a(0)
k,j [Ξr,j]
1,j,...,a(0)
r′,ih(i) √m⟨w(0)
1,i⟩, √mb(0)
r,ih(i) √m⟨w(0)
1,i⟩, √mb(0)
1,i, x⟩))⟨w∗
2,i, x⟩± pε = f∗
r (x) ± pε .
Now, re-scaling each w⋇
j by a factor of
m and re-scaling ε by
2pk, we can write
r (x; W ⋇) =
r (x; W ⋇)
Now, we apply the concentration from Lemma A.1, which implies for our parameter choice of m,
with high probability
r (x; W ⋇) −f∗
The above concentration holds for every ﬁxed x with high probability, and thus also holds in
expectation with respect to (x, y) ∼D. This proves the ﬁrst statement. As for the second statement
on L(G(b)(x; W ⋇), y), it follows from the Lipschitz continuity of L.
Norm on W ⋇. According to its deﬁnition in (B.2), we have for each j ∈[m], with high probability
(here the additional
m is because we have re-scaled w⋇
m). This means
∥W ⋇∥2,∞≤eO
. As for the Frobenius norm,
h(i) √m⟨w(0)
1,i⟩, √mb(0)
Now, for each i ∈[p], we know that P
j∈[m] h(i) √m⟨w(0)
1,i⟩, √mb(0)
is a summation of i.i.d.
random variables, each with expectation at most Cs(φ, 1)2 by Lemma 6.3. Applying concentration,
we have with high probability
h(i) √m⟨w(0)
1,i⟩, √mb(0)
≤m · Cs(φ, 1)2 + √m · C2
0 ≤2mCs(φ, 1)2
Putting this back to (B.3) we have ∥W ⋇∥2
F ≤eO(k2p2Cs(φ,1)2
Proof of Corollary B.2. Let W ⋇be the weights constructed in Lemma B.1 to approximate up to
error ε/2. Then
|gr(x; W (0) + W ⋇) −g(b)
r (x; W ⋇)| =
r,i (⟨w(0)
i , x⟩+ b(0)
i )I[⟨w(0)
i , x⟩+ b(0)
By standard concentration (which uses the randomness of w(0) together with the randomness of
r,1, . . . , a(0)
r,m), the above quantity is with high probability bounded by eO(εa) = ε/2. This is the
only place we need parameter choice εa.
Here we show that the weights after a properly bounded amount of updates stay close to the
initialization, and thus the pseudo network is close to the real network using the same weights.
Lemma B.3 (Coupling). For every unit vector x, w.h.p. over the random initialization, for every
time step t ≥1, we have the following. Denote τ = εaηt.
(a) For at most eO(τ
km) fraction of i ∈[m]:
i , x⟩+ b(0)
≥0] ̸= I[⟨w(t)
i , x⟩+ b(0)
(b) For every r ∈[k],
fr(x; W (0) + Wt) −gr(x; W (0) + Wt)
= eO(εakτ 2m3/2).
(c) For every y:
∂W L(F(x; W (0) + Wt), y) −
∂W L(G(x; W (0) + Wt), y)
≤eO(εakτm3/2 + ε2
ak2τ 2m5/2).
Proof of Lemma B.3. Let us recall
fr(x; W) =
r,i (⟨wi, x⟩+ b(0)
i )I[⟨wi, x⟩+ b(0)
gr(x; W) =
r,i (⟨wi, x⟩+ b(0)
i )I[⟨w(0)
, x⟩+ b(0)
(a) W.h.p. over the random initialization, there is B = eO(1) so that every |a(0)
r,i | ≤εaB. Thus, by
the 1-Lipschitz continuity of L, for every i ∈[m] and every t ≥0,
∂fr(x; W (0) + Wt)
∂L(F(x; W (0) + Wt), y)
which implies that
kBτ. Accordingly, deﬁne H
so it satisﬁes for every i ∈H,
which implies I[⟨w(0)
i , x⟩+ b(0)
≥0] = I[⟨w(t)
i , x⟩+ b(0)
Now, we need to bound the size of H. Since ⟨w(0)
i , x⟩∼N(0, 1/m), and b(0)
∼N(0, 1/m), by
standard property of Gaussian one can derive |H| ≥m
with high probability.
(b) It is clear from (B.4) that fr and gr only diﬀer on the indices i ̸∈H. For such an index i ̸∈H,
the sign of ⟨w(t)
i , x⟩+b(0)
is diﬀerent from that of ⟨w(0)
and their diﬀerence is at most
kBτ. This contributes at most εaB ·
kBτ diﬀerence between fr and gr. Then the bound
follows from that there are only eO
many i ̸∈H.
(c) Note that
L(F(x; W (0) + Wt), y) = ∇L(F(x; W (0) + Wt), y) ∂
F(x; W (0) + Wt)
By the Lipschitz smoothness assumption on L and Lemma B.3b, we have
∥∇L(F(x; W (0) + Wt), y) −∇L(G(x; W (0) + Wt), y)∥2 ≤eO(εak3/2τ 2m3/2) .
For i ∈H we have I[⟨w(0)
, x⟩+ b(0)
≥0] = I[⟨w(t)
i , x⟩+ b(0)
∂F(x;W (0)+Wt)
∂G(x;W (0)+Wt)
, and thus the diﬀerence is only caused by (B.6).
Using (B.5), each such i
contributes at most eO(ε2
ak2τ 2m3/2), totaling eO(ε2
ak2τ 2m5/2).
For i ̸∈H, it contributes at most
kεaB because of (B.5), and there are eO(τm
such i’s, totaling eO(εakτm3/2).
Optimization
Recall for z = (x, y)
LF (z; Wt)
= L(F(x; W (0) + Wt), y) ,
= L(G(x; W (0) + Wt), y) .
For the set of samples Z, deﬁne
L(F(x; W + W (0)), y) ,
L(G(x; W + W (0)), y) .
We show the following lemma:
Lemma B.4. For every ε ∈(0,
pkCs(φ,1)), letting εa = ε/eΘ(1) and η = eΘ
, there exists
M = poly(Cε(φ, 1), 1/ε)
k3p2 · Cs(φ, 1)2
such that, w.h.p., if m ≥M then
LF (Z; Wt) ≤OPT + ε.
Proof of Lemma B.4. Let W ⋇be constructed in Corollary B.2. Recall L(·, y) is convex and G(x; W)
is linear in W so LG(z; W) is convex in W. By such convexity, we have
LG(Z; Wt) −LG(Z; W ⋇) ≤⟨∇LG(Z; Wt), Wt −W ⋇⟩
≤∥∇LG(Z; Wt) −∇LF (Z; Wt)∥2,1∥Wt −W ⋇∥2,∞
+ ⟨∇LF (Z; Wt), Wt −W ⋇⟩.
We also have
∥Wt+1 −W ⋇∥2
F = ∥Wt −η∇LF (z(t), Wt) −W ⋇∥2
= ∥Wt −W ⋇∥2
F −2η⟨∇LF (z(t), Wt), Wt −W ⋇⟩
+ η2∥∇LF (z(t), Wt)∥2
LG(Z; Wt) −LG(Z; W ⋇) ≤∥∇LG(Z; Wt) −∇LF (Z; Wt)∥2,1∥Wt −W ⋇∥2,∞
+ ∥Wt −W ⋇∥2
F −Ez(t)[∥Wt+1 −W ⋇∥2
2∥∇LF (Wt, z(t))∥2
Recall from (B.5) that with high probability over the random initialization,
∥Wt∥2,∞= eO(
∥Wt −W ⋇∥2,∞= eO(
kεaηt + kpC0
∥∇LF (Wt, z(t))∥2
where C0 is as in Lemma B.1. By Lemma B.3c, w.h.p. we know
∥∇LG(Z; Wt) −∇LF (Z; Wt)∥2,1 ≤∆= eO(ε2
akηTm3/2 + ε4
ak2(ηT)2m5/2).
Therefore, averaging up (B.7) from t = 0 to T −1 we have that
Esgd[LG(Z; Wt)] −LG(Z; W ⋇) ≤eO
kεaηT∆+ kpC0
+ ∥W0 −W ⋇∥2
Note that ∥W0 −W ⋇∥2
F = ∥W ⋇∥2
  k2p2Cs(φ,1)2
from Lemma B.1. Also recall εa = Θ(ε). By
choosing η = eΘ
and T = eΘ
 k3p2Cs(φ, 1)2/ε2
we have ∆= eO(k6p4Cs(φ, 1)4m1/2/ε2). Thus
when m is large enough we have:
Esgd[LG(Z; Wt)] −LG(Z; W ⋇) ≤O(ε).
By the coupling in Lemma B.3b, we know that LF (Z; Wt) is o(ε)-close to LG(Z; Wt); by applying
Corollary B.2, we know that LG(Z; W ⋇) is ε-close to OPT. This ﬁnishes the proof.
Generalization
The generalization can be bounded via known Rademacher complexity results. Recall
fr(x; W (0) + W ′)
r,i σ(⟨w(0)
i, x⟩+ b(0)
We have the following simple lemma (see also [43, Theorem 43])19
Lemma B.5 (two-layer network Rademacher complexity). For every τw,∞≥0, w.h.p. for every
r ∈[k] and every N ≥1, we have the empirical Rademacher complexity bounded by
N Eξ∈{±1}N
∥W ′∥2,∞≤τw,∞
ξifr(xi; W (0) + W ′)
Proof. The proof consists of the following simple steps.
• {x 7→⟨w′
j, x⟩| ∥w′
j∥2 ≤τw,∞} has Rademacher complexity O(τw,∞
N ) by Proposition A.12a.
19We note that [43, Theorem 43] did not have W (0) but the same result holds with the introduction of W (0).
• {x 7→⟨w(0)
j, x⟩+bj | ∥w′
j∥2 ≤τw,∞} has Rademacher complexity O(τw,∞
N ) because singleton
class has zero complexity and adding it does not aﬀect complexity by Proposition A.12c.
• {x 7→fr(x; W (0) + W ′) | ∥W ′∥2,∞≤τw,∞} has Rademacher complexity eO(εamτw,∞
w.h.p. ∥a(0)
r ∥1 ≤eO(εam) and Proposition A.12e.
Theorem 1: Two-Layer
Proof of Theorem 1. First, we can apply Lemma B.4 to bound the training loss. That is
E(x,y)∈ZL(F(x; Wt + W (0)), y) ≤OPT + ε.
Also recall from (B.8) and our parameter choices for η, T that
poly(k, p, log m) · Cs(φ, 1)2
so we can choose τw,∞= O(poly(k,p,log m)·Cs(φ,1)2
). For each (x, y) ∼D, it is a simple exercise to
verify that |L(F(x; Wt + W (0)), y)| ≤O(poly(k,p,log m)·Cs(φ,1)2
) with high probability.20
can plug the Rademacher complexity Lemma B.5 together with b = O(poly(k,p,log m)·Cs(φ,1)2
standard generalization statement Corollary A.11. It gives
E(x,y)∈DL(F(x; Wt + W (0)), y) −E(x,y)∈ZL(F(x; Wt + W (0)), y)
≤O(poly(k, p, log m) · Cs(φ, 1)2
This completes the proof with large enough N.
Remark B.6. Strictly speaking, |gr(x; W)| ≤eO(εa) does not hold for every x in D, thus the loss
function L is not absolutely bounded so one cannot apply Corollary A.11 directly.21 We only have
the statement that for each sample x, the loss function |L(F(x; Wt + W (0)), y)| ≤b is bounded
by some parameter b with high probability. By union bound, with high probability this can hold
for all the training samples (but possibly not all the testing samples).
A simple ﬁx here is to
apply a truncation (for analysis purpose only) on the loss function L to make it always bounded
by b. Then, we can apply Corollary A.11: the population risk “E(x,y)∈DL(· · · )” in (B.9) becomes
truncated but the empirical risk “E(x,y)∈ZL(· · · )” in (B.9) stays unchanged. In other words, the
truncated population risk must be small according to Corollary A.11.
Finally, we can remove
truncation from the population risk, because in the rare event that |L(F(x; Wt + W (0)), y)| exceeds
b, it is at most poly(m) so becomes negligible when evaluating the expectation E(x,y)∈DL(· · · ).
Remark B.7. In the above proof, it appears that N scales with ε−4 which may seemingly be larger
than T which only scales with ε−2. We are aware of a proof that tightens N to be on the order
of ε−2. It uses standard (but complicated) martingale analysis and creates extra diﬃculty that is
irrelevant to neural networks in general. We choose not to present it for simplicity.
20Indeed, with high probability |gr(x; W)| ≤eO(εa) and since ∥Wt∥2,∞≤τw,∞we have |g(b)
r (x; Wt)| ≤eO(εaτw,∞m).
Together we have |gr(x; W + Wt)| ≤eO(εaτw,∞m). By the coupling Lemma B.3b, this implies |fr(x; W + Wt)| ≤
eO(εaτw,∞m) as well. Using L(0, y) ∈ and the 1-Lipschitz continuity ﬁnishes the proof.
21In some literature this issue was simply ignored or an absolute bound on L is imposed; however, the only globally
absolutely bounded convex function is constant.
Proofs for Three-Layer Networks
Our three-layer proofs follow the same structure as our proof overview in Section 6.
Existential Results
Lemma 6.3: Indicator to Function
Recall without loss of generality it suﬃces to prove Lemma 6.3a.
Lemma 6.3a (indicator to function). For every smooth function φ, every ε ∈
that there exists a function h : R2 →[−Cε(φ, 1), Cε(φ, 1)] such that for every x1 ∈[−1, 1]:
1+b0≥0h(α1, b0)
where α1, β1 ∼N(0, 1) and b0 ∼N(0, 1) are independent random variables. Furthermore:
• h is Cε(φ, 1)-Lipschitz on the ﬁrst coordinate.
• Eα1,b0∼N(0,1)
h(α1, b0)2
≤(Cs(φ, 1))2.
For notation simplicity, let us denote w0 = (α1, β1) and x = (x1,
1) where α1, β1 are two
independent random standard Gaussians.
Throughout the proof, we also take an alternative view of the randomness. We write ⟨w0, x⟩= α
and α1 = αx1 +
1β for two independent α, β ∼N(0, 1).22
We ﬁrst make a technical claim involving in ﬁtting monomials in x1. Its proof is in Section C.1.2.
Claim C.1. Recall hi(x) is the degree-i Hermite polynomial (see Deﬁnition A.5). For every integer
i ≥1 there exists constant p′
i with |p′
i| ≥(i−1)!!
200i2 such that
for even i :
Ew0∼N(0,I),b0∼N(0,1) [hi(α1) · I[0 < −b0 ≤1/(2i)] · I[⟨x, w0⟩+ b0 ≥0]]
for odd i :
Ew0∼N(0,I),b0∼N(0,1) [hi(α1) · I[|b0| ≤1/(2i)] · I[⟨x, w0⟩+ b0 ≥0]]
We next use Claim C.1 to ﬁt arbitrary functions φ(x1). By Taylor expansion, we have
φ(x1) = c0 +
i=1, odd i
i=2, even i
i · Eα,β,b0∼N(0,1)
hi(α1) · I[qi(b0)] · I[⟨x, w0⟩+ b0 ≥0]
i| ≤200i2 |ci|
 |b0| ≤1/(2i),
0 < −b0 ≤1/(2i),
i is even.
The next technical claim carefully bounds the absolute values of the Hermite polynomials. Its
proof is in Section C.1.2.
Claim C.2. Setting Bi
= 100i1/2 + 10
ε, we have
22This is possible for the following reason. Let x⊥= (
1, −x1) be unit vector orthogonal to x. We can write
w0 = αx + βx⊥where α, β ∼N(0, 1) are two independent Gaussians.
i| · Ez∼N(0,1)
|hi(z)| · I[|z| ≥Bi]
i| · Ez∼N(0,1)
|hi(Bi)| · I[|z| ≥Bi]
i| · Ez∼N(0,1)
|hi(z)| · I[|z| ≤Bi]
2Cε (φ, 1)
i| · Ez∼N(0,1)
· I[|z| ≤Bi]
2Cε (φ, 1)
Now, let us deﬁne bhi(α1)
= hi(α1) · I[|α1| ≤Bi] + hi(sign(α1)Bi) · I[|α1| > Bi] as the truncated
version of the Hermite polynomial hi(·).
Using Claim C.2, we have
φ(x1) = c0 + R′(x1) +
i · Eα,β,b0∼N(0,1)
bhi(α1) · I[qi(b0)] · I[⟨x, w0⟩+ b0 ≥0]
where |R′(x1)| < ϵ/4 uses Claim C.2a and Claim C.2b. In other words, if we deﬁne
i · bhi(α1) · I[qi(b0)]
then we have
Eα,β,b0∼N(0,1)
I[⟨x, w0⟩+ b0 ≥0] · h(α1, b0)
As for the range of h, we use Claim C.2b and Claim C.2c to derive that
|h(α1, b0)| ≤2c0 + ε
2Cε (φ, 1) ≤Cε (φ, 1) .
As for the Lipschitz continuity of h on its ﬁrst coordinate α1, we observe that for each i > 0,
bhi(z) has zero sub-gradient for all |z| ≥Bi. Therefore, it suﬃces to bound
for |z| < Bi.
Replacing the use of Claim C.2c by Claim C.2d immediately give us the same bound on the Lipschitz
continuity of h with respect to α1.
As for the expected square Eα1,b0∼N(0,1)
h(α1, b0)2
, we can write
h(α1, b0) = 2c0 +
i · bhi(α1) · I[qi(b0)] x= 2c0 +
i · hi(α1) · I[qi(b0)] ± ε
Above, x uses Claim C.2a and Claim C.2b. Using the othogonality condition of Hermite polynomials (that is, Ex∼N(0,1)[hi(x)hj(x)] =
2πj!δi,j from Deﬁnition A.5), we immediately have
Eα1,b0∼N(0,1)[h(α1, b0)2] ≤O(ε2 + c2
0) + O(1) ·
i)2(i!) · Eb0[I[qi(b0)]]
≤O(ε2 + c2
0) + O(1) ·
≤O(ε2 + c2
0) + O(1) ·
(i!) · i3 · |ci|2
((i −1)!!)2
≤O(ε2 + c2
0) + O(1) ·
i3.5 · |ci|2 ≤(Cs(φ, 1))2 .
Above, x uses inequality
((i−1)!!)2 ≤2
i for all i ≥1.
This ﬁnishes the proof of Lemma 6.3a.
Proofs of Claim C.1 and Claim C.2
Proof of Claim C.1. We treat the two cases separately.
By Lemma A.6, we know that
Ew0∼N(0,I),b0∼N(0,1) [hi(α1) · I[0 < −b0 ≤1/(2i)] · I[⟨x, w0⟩+ b0 ≥0]]
= Eb0∼N(0,1)
Eα,β∼N(0,1)
· I[α ≥−b0]
· I[0 < −b0 ≤1/(2i)]
= Eb0∼N(0,1) [pi · I[0 < −b0 ≤1/(2i)]] × xi
pi = (i −1)!!exp(−b2
We try to bound the coeﬃcient “Eb0∼N(0,1) [pi · I[0 < −b0 ≤1/(2i)]]” as follows. Deﬁne cr as:
cr := (−1)
Then, for 0 ≤−b0 ≤1
2i, we know that for all 1 < r ≤i −1, r odd:
|cr(−b0)r| ≤1
4|cr−2(−b0)r−2|,
which implies
3|c1b0| = 2
= sign(c1)
is independent of the randomness of b0. Therefore, using the formula of pi in (C.2):
Eb0∼N(0,1)[pi · I[0 ≤−b0 ≤1/(2i)]]
Eb0∼N(0,1)
(i −1)!!exp(−b2
cr(−b0)r · I[0 ≤−b0 ≤1/(2i)]
≥Eb0∼N(0,1)
(i −1)!!exp(−b2
3|b0| · I[0 ≤−b0 ≤1/(2i)]
Similarly, by Lemma A.6, we have
Ew0∼N(0,I),b0∼N(0,1)
hi(α1) · I[|b0| ≤1/(2i)] · I[⟨x, w0⟩+ b0 ≥0]
= Eb0∼N(0,1)
pi · I[|b0| ≤1/(2i)]
pi = (i −1)!!exp(−b2
r=0,r even
This time we bound the coeﬃcient “Eb0∼N(0,1) [pi · I[|b0| ≤1/(2i)]]” as follows. Deﬁne cr as:
cr := (−1)
Then, for |b0| ≤1
2i, we know that for all even r in 1 < r ≤i −1 it satisﬁes
|cr(−b0)r| ≤1
4|cr−2(−b0)r−2|,
which implies
r=0,r even
= sign(c0)
is independent of the randomness of b0. Therefore, using the formula of pi in (C.3):
Eb0∼N(0,1)
pi · I[|b0| ≤1/(2i)]
Eb0∼N(0,1)
(i −1)!!exp(−b2
cr(−b0)r · I[|b0| ≤1/(2i)]
≥Eb0∼N(0,1)
(i −1)!!exp(−b2
2i · I[|b0| ≤1/(2i)]
Proof of Claim C.2. By the deﬁnition of Hermite polynomial (see Deﬁnition A.5), we have that
j!(i −2j)!2j ≤
|x|i−2ji2j
Using our bound on c′
i (see (C.1)), we have
ihi(z)| ≤O(1)|ci| i4
|z|i−2ji2j
(a) Denote by b = Bi = 100i1/2θ for notational simplicity (for some parameter θ ≥1 that we shall
choose later). We have
i| · Ez∼N(0,1)[|hi(z)| · I[|z| ≥b]]| ≤2|c′
i| · Ez∼N(0,1)
|z|i−2ji2jI[z ≥b]
= O(1)|ci| i4
Li−2j,b · i2j
where recall from Lemma A.7 that
Li,b ≤O(1)e−b2/2 ·
≤O(1)e−b2/2 ·
≤O(1)e−b2/2 (100θ)i · (i −1)!! ·
≤O(1)e−b2/2 (200θ)i · (i −1)!!
(using Pi−1
Thus we have
Li−2j,b · i2j
≤O(1) (200θ)i e−b2/2
(i + 1 −2j)!! · i2j
≤O(1) (400θ)i e−b2/2
i(i−2j)/2 · i2j
= O(1) (400θ)i e−b2/2ii/2
≤O(1) (1200θ)i e−b2/2ii/2
≤O(ii/2) · 1200i ·
 θ · e−104θ2i
Above, inequality x uses Pi−1
j! ≤3i; inequality y uses our deﬁnition of b = Bi; inequality
z uses (θ · e−104θ2)i ≤
100000i for θ = 1 +
; and inequality { uses ε|ci| ≤1. Putting
this back to (C.6), we have
i| · Ez∼N(0,1)[|hi(z)| · I[|z| ≥b]]| ≤O(1)
100i ii/2 ≤ε
Above, in the last inequality we have used i4
i!!ii/2 ≤40 · 4i for i ≥1.
(b) Similar to the previous case, we calculate that
i| · Ez∼N(0,1)[|hi(b)| · I[|z| ≥b]]| ≤|c′
i| · Ez∼N(0,1)
≤O(1)|ci| i4
i!! · L0,bbi
b−2j · i2j
≤O(1)|ci| i4
i!! · e−b2/2bi
≤O(1)|ci| i4
i!! · e−b2/2(3b)i
i!! · e−b2/2(3b)i .
Above, inequality x uses b2j = B2j
≥(10i)j; and inequality y uses again Pi−1
j! ≤3i. Using
this and continue from (C.7) of the previous case, we ﬁnish the proof.
(c) Again denote by b = Bi = 100i1/2θ for notational simplicity. By Eq. (C.5), it holds that
i| · Ez∼N(0,1)
|hi(z)| · I[|z| ≤Bi]
|ci| (O(1)θ)i
2Cε (φ, 1) .
Here, in x we use the fact that ij
j! ≤10i; in y we use the fact that
b ≤ea for all b ∈[1, a].
(d) By the deﬁnition of Hermite polynomial (see Deﬁnition A.5), we can also bound
|x|i−2ji2j
which is the same upper bound comparing to (C.4). Therefore, the same proof of Claim C.2c
also applies to
Lemma 6.4: Information out of Randomness
Let us consider a single neural of the second layer at random initialization, given as:
i , x⟩+ b(0)
Lemma 6.4 (information out of randomness). For every smooth function φ, every w∗∈Rd with
∥w∗∥2 = 1, for every ε ∈
, there exists real-valued functions
1 , W (0), b(0)
1 ), B(x, v(0)
1 , W (0), b(0)
1 ), R(x, v(0)
1 , W (0), b(0)
1 ), and φε(x)
such that for every x:
1 , W (0), b(0)
1 , W (0), b(0)
1 , W (0), b(0)
Moreover, letting C = Cε(φ, 1) be the complexity of φ, and if v(0)
m2 ) and w(0)
i,j , b(0)
m1 ) are at random initialization, then we have
1. For every ﬁxed x, ρ
1 , W (0), b(0)
is independent of B
1 , W (0), b(0)
1 , W (0), b(0)
3. For every x with ∥x∥2 = 1, |φε(x) −φ(⟨w∗, x⟩)| ≤ε.
4. For every ﬁxed x with ∥x∥2 = 1, with high probability
1 , W (0), b(0)
1 , W (0), b(0)
Furthermore, there exists real-valued function eρ(v(0)
1 ) satisfying with high probability:
W2(ρ|W (0),b(0)
1 , eρ) ≤eO
Before going to proofs, we recall from Section A that we have overridden the notion of “with high
probability” so the above statement implies W2(R, 0) ≤O
and W2(B, 0) ≤eO
Proof of Lemma 6.4. Without loss of generality we assume w∗= e1. Recall that
i , x⟩+ b(0)
By Lemma 6.3, for every ε > 0, there exists a function h such that for every unit x with xd = 1
and every i ∈[m1]: 23
1,i ∼N(0, 1
i,1 , b(0)
, x⟩+ b(0)
= φε(⟨w∗, x⟩)
|φε(⟨w∗, x⟩) −φ(⟨w∗, x⟩)| ≤ε
i,1 , b(0)
∈ . (Here to apply Lemma 6.3, we have re-scaled h in Lemma 6.3 by
and re-scaled α1, β1, b0 in Lemma 6.3 by
23If one wants to work with the more general target function in Remark 4.2, we can assume without loss of generality
3 = ed (because we have ⟨w∗
3⟩= 0 in Remark 4.2).
Throughout the proof, we ﬁx some parameter τ (that we shall in the end choose τ =
us construct the sign function s: [−1, 1] × R →{−1, 0, 1} and the set function I : [−1, 1] ∋y 7→
I(y) ⊂R given in Lemma A.4. Now, for every w(0)
i,1 , deﬁne
i,1 , b(0)
Also deﬁne set
i ∈[m1]: √m2v(0)
We deﬁne what we call “eﬀective sign of v(0)
1,i ” to be
i,1 , b(0)
By the deﬁnition of Ii and s (see Lemma A.4), we claim that S and the “eﬀective sign” of those
1,i in this set i ∈S are independent of W (0). Indeed, for any ﬁxed choice of W (0), each i ∈[m1] is
in set S with probability τ, and for each i ∈S, si is ±1 each with half probability. In other words,
the following unit vector u ∈Rm1 is independent of W (0):
Since each i ∈S with probability τ, we also know with high probability:
|S| = τm1 ± O(√τm1) .
Conversely, conditioning on S = S0 and {si}i∈S = s being ﬁxed (or equivalently on u being
ﬁxed), the distribution of W (0) is also unchanged. Since the entries of W (0) are i.i.d. generated
from N(0, 1/m1), we can write W (0) ∈Rm1×d as
W (0) = αue⊤
= u⊤W (0)ed ∼N
and β ∈Rm1×d are two independent random variables given u
(the entries of β are not i.i.d.) This factorizes out the randomness of the last column of W (0) along
the direction u, and in particular,
• α is independent of u.
A simple observation here is that, although α ∼N
, if we ﬁx W (0) (or b(0)
1 ) then the distribution of α is not Gaussian. Fortunately, ﬁxing W (0) and b(0)
1 , we still have that the coordinates of
s = (s1, . . . , sm1) are i.i.d. (each si is zero with probability 1−τ, and si is ±1 each with probability
Let α|W (0),b(0)
denote the conditional distribution of α.
With high probability over W (0),
∥W (0)ed∥∞≤eO
. Fixing the support of u to be S, we know that for every i ∈S, ui is
|S|. This implies that ﬁxing W (0), b(0)
1 , S, the quantity
α = u⊤W (0)ed =
ui[W (0)ed]i
is a sum of |S| many independent, mean zero random variables with each |ui[W (0)ed]i| ≤eO
i∈S ui[W (0)ed]2
w.h.p. Applying any Wasserstein distance bound of central
limit theorem (see Lemma A.3), we know that there exists some random Gaussian g ∼N(0,
that is independent of W0 or b(0)
such that w.h.p.
W2(α|W (0),b(0)
1 , g) ≤eO
We can write
i , x⟩+ b(0)
i , x⟩+ b(0)
,W (0),b(0)
i , x⟩+ b(0)
By deﬁnition of B1, conditioning on the randomness of u, we know that B1 is independent of α—
because ⟨w(0)
i , x⟩+ b(0)
1,i = ⟨βi, x⟩+ b(0)
1,i for i ̸∈S. Since u and α are independent, we know that α
and B1 are independent by Proposition A.2. We continue to write
i , x⟩+ b(0)
1,i I[⟨w(0)
i , x⟩+ b(0)
xd + ⟨βi, x⟩+ b(0)
1,i I[⟨w(0)
i , x⟩+ b(0)
1,i ≥0] αsi
1,i I[⟨w(0)
i , x⟩+ b(0)
⟨βi, x⟩+ b(0)
First consider T3.
For each i ∈S we have
i,1 , b(0)
i,1 , b(0)
Above, the ﬁrst row is because √m2v(0)
1,i ∈Ii and the Bounded-ness property of the interval (see
Lemma A.4); and the second row by the Unbiased property of the interval (see Lemma A.4). By
concentration, for ﬁxed vector x, with high probability over the randomness of V (0):
i,1 , b(0)
i , x⟩+ b(0)
In other words,
i,1 , b(0)
i , x⟩+ b(0)
1,i ≥0] + R1
where R1 = R1
1 , W (0), b(0)
satisﬁes |R1| ≤eO
. We write
i,1 , b(0)
, x⟩+ b(0)
By (C.9) (i.e., the property of h), we know that for every ﬁxed x, using concentration bound, with
high probability over W (0) and b(0):
√m2C φε(⟨w∗, x⟩)
and thus by (C.10)
T5 −φε(⟨w∗, x⟩)
Let us deﬁne
1 , W (0), b(0)
1 , W (0), b(0)
· φε(⟨w∗, x⟩) + R1 + R2
1 , W (0), b(0)
where |R2| ≤eO
Note that α is independent of u so ρ is also independent of u. We can also deﬁne
and using (C.12) we can derive the desired bound on W2(ρ|W (0),b(0)
1 , eρ) in the statement of Lemma 6.4.
Next consider T4.
1,i I[⟨w(0)
i , x⟩+ b(0)
⟨βi, x⟩+ b(0)
For ﬁxed unit vector x, with high probability, we have that
and therefore
, x⟩+ b(0)
xd + ⟨βi, x⟩+ b(0)
1,i = ⟨βi, x⟩+ b(0)
By the above formula, for an index i ∈S to have I[⟨βi, x⟩+ b(0)
1,i ≥0] ̸= I[⟨w(0)
i , x⟩+ b(0)
1,i ≥0], it
must satisfy
, x⟩+ b(0)
Thus, for ﬁxed S, since W (0) is independent of S, with high probability over the randomness of
W (0), there are at most eO
many indices i ∈S satisfying (C.13). In other words, using
1,i | ≤eO(1/√m2) with high probability, we have
1,i I[⟨βi, x⟩+ b(0)
⟨βi, x⟩+ b(0)
with R3 = R3
1 , W (0), b(0)
satisfying |R3| ≤eO
with high probability.
To bound T6.
i ∈[m1]: √m2v(0)
i,1 , b(0)
and we can deﬁne a similar notion
i ∈[m1]: √m2v(0)
βi,1, b(0)
Observe that
i,1 , b(0)
βi,1 + ⟨αuied, e1⟩, b(0)
βi,1, b(0)
Therefore, we have S = S′ and can write
1,i I[⟨βi, x⟩+ b(0)
⟨βi, x⟩+ b(0)
= B2(x,v(0)
,W (0),b(0)
Again, conditioning on the randomness of u, we know that B2 is independent of α. Since u and
α are independent, we know that α and B2 are also independent (by Proposition A.2). In other
words, setting B = B1 + B2, B and α (and therefore ρ) are independent.
Let R = R1 + R2 + R3 be the residual term, setting τ =
100, we have |R| ≤eO
high probability.
As for the norm bound on B, recall
i , x⟩+ b(0)
and by our random initialization, n1(x) ∼N
 W (0)x + b(0)
. At the same time, with
high probability
 W (0)x + b(0)
2 = O(1). Therefore, we know |n1(x)| ≤eO(
√m2 ), and this
implies |B| ≤eO(
√m2 ) with high probability.
Lemma 6.2: Existence
Lemma 6.2 (existence). For every ε ∈
2Cs(Φ,p2Cs(φ,1))Cs(φ,1)2
, there exists
 Φ, √p2Cε(φ, 1)
C0 = Cε(Φ, √p2Cε(φ, 1)) · Cε(φ, 1) · eO(p1
such that if m1, m2 ≥M, then with high probability, there exists weights W ⋇, V ⋇with
∥W ⋇∥2,∞= max
∥V ⋇∥2,∞= max
r (x) −g(0)
r (x; W ⋇, V ⋇)
and hence,
L(G(0)(x; W ⋇, V ⋇), y)
Let us mostly focus on proving Lemma 6.2 for a single term
1,jφ1,j(⟨w∗
2,jφ2,j(⟨w∗
Extending it to multiple terms and multiple outputs, that is
1,i,jφ1,j(⟨w∗
2,i,jφ2,j(⟨w∗
is rather straightforward (and indeed a repetition of the proof of Lemma B.1 in the two-layer case).
The proof consists of several steps.
Step 1: Existence in expectation
Recall that the input (without bias) to each neuron at random initialization in the second hidden
r , x⟩+ b(0)
We ﬁrst use Lemma 6.4 to derive the following claim:
Claim C.3. For every ε ∈(0, 1/Cs(φ, 1)), there exists real-value functions φ1,j,ε(·) satisfying
|φ1,j,ε(⟨w∗
1,j, x⟩) −φ1,j(⟨w∗
1,j, x⟩)| ≤ε for all j ∈[p2] and ∥x∥2 = 1,
and the following holds. Denote by
= Cε(φ, 1),
C′ φ1,j,ε(⟨w∗
For every i ∈[m2], there exist independent Gaussians24
αi,j ∼N(0, 1/m2) and βi(x) ∼N
satisfying
αi,jφ1,j(x) + β(x)
Proof of Claim C.3. Let us deﬁne p2S many chunks of the ﬁrst layer, each chunk corresponds to a
set Sj,l of cardinality |Sj,l| = m1
p2S for j ∈[p2], l ∈[S], such that
+ (l −1) m1
Let us then denote vi[j, l] to be (vi,r)r∈Sj,l and W[j, l] to be (Wr)r∈Sj,l. Recall that the input
(without bias) to each neuron at random initialization in the second hidden layer is
r , x⟩+ b(0)
r , x⟩+ b(0)
24More speciﬁcally, αi,j = αi,j(v(0)
, W (0), b(0)
1 ) and βi = βi(x, v(0)
, W (0), b(0)
1 ) depend on the randomness of v(0)
W (0) and b(0)
For each j ∈[p2] and l ∈[S], let us apply Lemma 6.4 to the summation P
r∈Sj,l . . . in the above
formula, to approximate φ1,j(⟨w∗
1,j, x⟩). (We need to replace m1 with
p2S and scale up W (0) and
by √p2S before applying Lemma 6.4)). It tells us we can write ni(x) as:
j∈[p2],l∈[S]
[j, l], W (0)[j, l], b(0)
φ1,j,ε(⟨w∗
j∈[p2],l∈[S]
[j, l], W (0)[j, l], b(0)
[j, l], W (0)[j, l], b(0)
where random variables ρj,l
[j, l], W (0)[j, l], b(0)
100C2m2(p2S)) are independent Gaussian for diﬀerent j and l. Let C
= Cε(φ, 1). We know that
= 10C√p2 .
Moreover, |φ1,j,ε(⟨w∗
1,j, x⟩) −φ1,j(⟨w∗
1,j, x⟩)| ≤ε for each j ∈[p2].
Let us then denote
[j, l], W (0)[j, l], b(0)
[j, l], W (0)[j, l], b(0)
Lemma 6.4 tells us that random variables ρj are independent of Bj(x), and with high probability
j(x)| ≤S × eO
(m1/(p2S))m2
[j, l], W (0)[j, l], b(0)
Let us apply the Wasserstein distance version of the central limit theorem (see for instance
[23, Theorem 1]) 25: since Bs
j(x) is the summation of S i.i.d random variables, there is a Gaussian
random variable βj(x) only depending on the randomness of Bs
j(x) such that
j(x), βj(x)) ≤eO
Deﬁne β′(x) = P
j∈[p2] βj(x), we know that β′(x) is a Gaussian random variable independent of all
the ρj with
ρjφ1,j,ε(⟨w∗
1,j, x⟩) + β′(x)
25All the variables considered in this section is not absolutely bounded, but only with high probability with a
Gaussian tail. Strictly speaking, when apply this Theorem we should be ﬁrst replaced Bs
jIall Bj≤e
O(1/√m2p2S).
We choose to avoid writing this truncation in the paper to simply the presentation.
Let us slightly override notation and denote
φ1,j,ε(x) = 1
C′ φ1,j,ε(⟨w∗
We then have that variables αi,j
= C′ρj ∼N(0, 1/m2) are i.i.d. and
αi,jφ1,j,ε(x) + β′(x)
Since by our random initialization, ni(x) ∼N
W (0)x + b(0)
, and since for every
every unit vector x, with high probability
W (0)x + b(0)
2 = 1 ± eO
, we can write
W2(ni(x), g) ≤eO(
for g ∼N(0, 1
Since we can write g = P
j∈[p2] αi,jφ1,j,ε(x) + βi(x) for
being an independent from αi,1, . . . , αi,p2, we conclude that (by choosing S = (m1/p2)1/3)
αi,jφ1,j,ε(x) + βi(x)
This ﬁnishes the proof of Claim C.3.
Claim C.4. In the same notations as Claim C.3, there exists function h: R2 →[−C′′, C′′] for
C′′ = Cε (Φ, C′) such that for every i ∈[m2],
Ini(x)+b(0)
1,jαi,j, b(0)
2,jφ2,j(⟨w∗
1,jφ1,j(⟨w∗
2,jφ2,j(⟨w∗
2Cs(Φ, p2Cs(φ, 1))Cs(φ, 1)ε
Proof of Claim C.4. Let us slightly override notation and denote
= φ2,j(⟨w∗
We apply Lemma 6.3 again with φ chosen as Φ′(z) = Φ(C′x).26 We know there exists a function
26More speciﬁcally, we can choose w from Lemma 6.3 as
 αi,1, . . . , αi,p2, βi/
, choose x from
Lemma 6.3 as
 φ1,1,ε(x), . . . , φ1,p2,ε(x),
, choose w∗from Lemma 6.3 as (v∗
1,1, . . . , v∗
1,p2, 0), and
choose b0 from Lemma 6.3 as b(0)
h: R2 →[−C′′, C′′] for C′′ = Cε (Φ′, 1) = Cε (Φ, C′) such that
j∈[p2] αi,jφ1,j,ε(x)+βi(x)+b(0)
1,jαi,j, b(0)
2,jφ2,j(x)
1,jφ1,j,ε(x)
2,jφ2,j(x)
2,jφ2,j(x)
≤p2Cs(φ, 1) .
Next, we wish to use the Wasserstein bound from Claim C.3 to replace P
j∈[p2] αi,jφ1,j,ε(x) + βi(x)
with ni(x). We derive that
Ini(x)+b(0)
1,jαi,j, b(0)
2,jφ2,j(x)
Iαi,jφ1,j,ε(x)+βi(x)+b(0)
1,jαi,j, b(0)
2,jφ2,j(x)
αi,jφ1,j,ε(x) + βi(x)
√m2C′′′C′′
1,jφ1,j,ε(x)
2,jφ2,j(x)
εC′′′ + C′′C′′′p2/3
1,jφ1,j(⟨w∗
2,jφ2,j(⟨w∗
εC′′′ + C′′′ · (εp2)LΦ + C′′C′′′p2/3
1,jφ1,j(⟨w∗
2,jφ2,j(⟨w∗
2Cs(Φ, p2Cs(φ, 1))Cs(φ, 1)ε
Above, x uses the fact that ni ∼N
W (0)x + b(0)
W (0)x + b(0)
w.h.p. and |h| ≤C′′ is bounded. y uses Claim C.3 and (C.16). z uses C′φ1,j,ε(x) = φ1,j,ε(⟨w∗
φ2,j(x) = φ2,j(⟨w∗
2,j, x⟩), and denote by LΦ the Lipschitz continuity parameter of Φ (namely,
|Φ(x) −Φ(y)| ≤LΦ|x −y| for all x, y ∈
−p2Cs(φ, 1), p2Cs(φ, 1)
)). { uses LΦ ≤Cs(Φ, p2Cs(φ, 1))
and our assumption m1 ≥M. This proves Claim C.4.
Step 2: From expectation to ﬁnite neurons
Intuitively, we wish to apply concentration bound on Claim C.4 with respect to all neurons i ∈[m2]
on the second layer. Recall ai ∼N(0, εa) is the weight of the i-th neuron at the output layer. Our
main result of Step 2 is the following claim.
Claim C.5. In the same notation as Claim C.4,
Ini(x)+b(0)
1,jαi,j, b(0)
2,jφ2,j(⟨w∗
1,jφ1,j(⟨w∗
2,jφ2,j(⟨w∗
2Cs(Φ, p2Cs(φ, 1))Cs(φ, 1)ε
Proof of Claim C.5. Across diﬀerent choices of i, the values of ni(x) and αi,j can be correlated.
This makes it not a trivial thing to apply concentration. In the remainder of this proof, let us try
to modify the two quantities to make them independent across i.
First modify αi,j. Recall αi,j = C′ρj = C′ P
l∈[S] ρj,l where each ρj,l is a function on v(0)
[j, l], W (0)[j, l],
1 [j, l]. Now, using the eρ notion from Lemma 6.4, let us also deﬁne eρj,l = eρj
is in the same distribution as ρj,l except that it does not depend on W (0) or b(0)
1 . We can similarly
let eρj = P
l∈[S] eρj,l. From Lemma 6.4, we know that with high probability over W (0):27
W2(ρj,l|W (0),b(0), eρj,l) ≤eO
=⇒W2(ρj|W (0),b(0), eρj) ≤eO
According, we deﬁne eαi,j = C′eρj = C′ P
l∈[S] eρj,l.
Next modifty ni(x).
Recall ni(x) = P
r∈[m1] v(0)
r , x⟩+ b(0)
and accordingly we deﬁne
r∈[m1] v(0)
r , x⟩+ b(0)
where vector u
r , x⟩+ b(0)
r∈[m1]. By deﬁnition, we know
is a Gaussian variable and is independent of u. As a consequence, the quantities eni(x) are independent among diﬀerent choices of i ∈[m1]. Using standard concentration on ∥u∥2 (see the line
above (C.14)), we have for every x with ∥x∥2 = 1,
W2(ni(x), eni(x)) ≤eO
Concentration.
Using Claim C.4 of Step 1 we have
Ini(x)+b(0)
1,jαi,j, b(0)
2,jφ2,j(⟨w∗
1,jφ1,j(⟨w∗
2,jφ2,j(⟨w∗
2Cs(Φ, p2Cs(φ, 1))Cs(φ, 1)ε
27Recall from the proof of Claim C.3 that, we need to replace m1 with
p2S and scale up W (0) and b(0)
before applying Lemma 6.4.
Using the notions of en and eα and the Wasserstein distance bounds (C.18) and (C.19), it implies28
Ieni(x)+b(0)
1,j eαi,j, b(0)
2,jφ2,j(⟨w∗
1,jφ1,j(⟨w∗
2,jφ2,j(⟨w∗
2Cs(Φ, p2Cs(φ, 1))Cs(φ, 1)ε
Using Eai[a2
ε2a ] = 1 and applying standard concentration—and the independence of tuples (ai, eni(x), (eαi,j)j∈[p2])
with respect to diﬀerent choices of i— we know with high probability
Ieni(x)+b(0)
1,j eαi,j, b(0)
2,jφ2,j(⟨w∗
Ieni(x)+b(0)
1,j eαi,j, b(0)
2,jφ2,j(⟨w∗
Using again the Wasserstein distance bounds bounds (C.18) and (C.19), we can combine the above
two equations to derive that w.h.p.
Ini(x)+b(0)
1,jαi,j, b(0)
2,jφ2,j(⟨w∗
1,jφ1,j(⟨w∗
2,jφ2,j(⟨w∗
2Cs(Φ, p2Cs(φ, 1))Cs(φ, 1)ε
Step 3: From ﬁnite neurons to the network
We now construct the network and prove Lemma 6.2. Recall we focus on constructing
1,jφ1,j(⟨w∗
2,jφ2,j(⟨w∗
We shall explain towards the end how to extend it to multiple terms and multiple outputs.
For each φ2,j, applying Lemma 6.3b, we can construct hφ,j : R2 →[−C, C] satisfying
∀i′ ∈[m1]:
i′ ⟩, b(0)
i′ ,x⟩+b(0)
= φ2,j(⟨w∗
2,j, x⟩) ± ε .
Now consider an arbitrary vector v ∈Rm1 with vi ∈{−1, 1}.
• Deﬁne V ⋇∈Rm2×m1 as
V ⋇= (C0 · C′′/C)−1/2 a∗
1,jαi,j, b(0)
28We have skipped the details since it is analogous to (C.17).
where the notations of h: R2 →[−C′′, C′′] and αi,j come from Claim C.4. We have
• Deﬁne W ⋇∈Rm1×d as
W ⋇= (C0 · C′′/C)1/2
(If one wants the more general target in Remark 4.2, we can replace the use of ed with w∗
following the similar treatment as (B.1) in the two-layer existential proof.) We have
∥W ⋇∥2,∞≤2√p2
√C0 · C′′C
Given these weights, suppose the signs of ReLU’s are determined by the random initialization
(i.e., by W (0) and V (0)). We can consider the network output
g(0)(x; W ⋇, V ⋇)
= aDv,xV ⋇Dw,xW ⋇x
aiIni(x)+b(0)
i , x⟩I⟨w(0)
i′ ,x⟩+b(0)
1,jαi,j, b(0)
Ini(x)+b(0)
i′ ⟩, b(0)
i′ ,x⟩+b1,i′≥0
Above, we have used ⟨ed, x⟩= xd = 1/2. Since hφ,j ∈[−C′′, C′′], we can apply concentration on
(C.20) and get (recalling m1 is suﬃciently large) w.h.p
i′ ⟩, b(0)
i′ ,x⟩+b1,i′≥0 = φ2,j(⟨w∗
2,j, x⟩) ± 2ε
Using Claim C.5, we have
Ini(x)+b(0)
1,jαi,j, b(0)
2,jφ2,j(⟨w∗
1,jφ1,j(⟨w∗
2,jφ2,j(⟨w∗
2Cs(Φ, p2Cs(φ, 1))Cs(φ, 1)ε
Putting this into (C.21), and using a∗∈[−1, 1] and h ∈[−C′′, C′′], we know that with high
probability
g(0)(x; W ⋇, V ⋇) = a∗· Φ
1,jφ1,j(⟨w∗
2,jφ2,j(⟨w∗
2Cs(Φ, p2Cs(φ, 1))Cs(φ, 1)ε
Finally, scaling down the value of ε by factor p2
2Cs(Φ, p2Cs(φ, 1))Cs(φ, 1) we complete the proof
of Lemma 6.2 for a single output and for a single Φ.
Remark C.6. The proof generalizes to ﬁtting a combination of multiple functions
1,i,jφ1,j(⟨w∗
2,i,jφ2,j(⟨w∗
in the same way as the proof of Lemma B.1, if we choose the vector v ∈{−1, 1}m1 uniformly at
random and apply concentration. Note that we have to further scale down ε by
p1 for the same
reason as Lemma B.1, and the norm of V ⋇shall grow by a factor of p1.
Remark C.7. The proof generalizes to multiple outputs in the same way as Lemma B.1, using the
fact that weights ar,i are independent across diﬀerent outputs r. Note that we have to further scale
down ε by 1
k for the same reason as Lemma B.1, and the norm of V ⋇shall grow by a factor of k.
Finally, applying the two remarks above, we have
∥W ⋇∥2,∞≤2√p2
√C0 · C′′C
We thus ﬁnish the proof of Lemma 6.2 with our choice of C0.
Lemma 6.5: Coupling
Lemma 6.5 (coupling, restated). Suppose τv ∈
and η > 0. Given ﬁxed unit vector x, and perturbation matrices W ′, V ′, W ′′, V ′′
(that may depend on the randomness of W (0), b(0)
1 , V (0), b(0)
and x) satisfying
∥W ′∥2,4 ≤τw, ∥V ′∥F ≤τv, ∥W ′′∥2,4 ≤τw, ∥V ′′∥F ≤τv ,
and random diagonal matrix Σ with each diagonal entry i.i.d. drawn from {±1}, then with high
probability the following holds:
1. (Sparse sign change). ∥D′
w,x∥0 ≤eO(τ 4/5
m2 + τ 2/3
2. (Cross term vanish).
gr(x; W (0) + W ρ + W ′ + ηΣW ′′, V (0) + V ρ + V ′ + ηV ′′Σ)
x; W (0) + W ρ + W ′, V (0) + V ρ + V ′
(x; ηΣW ′′, ηV ′′Σ) + g′
where EΣ[g′
r(x)] = 0 and with high probability |g′
r(x)| ≤η eO
√m1 + m1/2
Part I, Sparsity
For notation simplicity, we only do the proof when there is no bias term. The proof with bias term
is analogous (but more notationally involved). Let us denote
= Dw,xW (0)x
= (Dw,x + D′
w,x)(W (0) + W ρ + W ′)x −Dw,xW (0)x .
• diagonal matrix Dw,x denotes the sign of ReLU’s at weights W (0),
• diagonal matrix Dw,x + D′′
w,x denotes the sign of ReLU’s at weights W (0) + W ρ, and
• diagonal matrix Dw,x + D′
w,x denotes the sign of ReLU’s at weights W (0) + W ρ + W ′.
Sign change in D′′
Each coordinate of W (0)x ∼N
and each coordinate of W ρx ∼
w). Thus, by standard property of Gaussian, for each i, we have Pr[|W ρ
i x| ≥|W (0)
. By concentration bound, with high probability, the number of sign changes of the
ReLU activations in the ﬁrst hidden layer caused by adding W ρ is no more than
w,x∥0 ≤eO(σwm3/2
Moreover, for each coordinate i with [D′′
w,x]i,i ̸= 0, we must have |(D′′
w,xW (0)x)i| ≤|(W ρx)i| ≤
eO(σw) with high probability, and thus
w,xW (0)x∥2 ≤eO
By our assumption σw ≤τw/m1/4
w,x∥0 ≤τwm5/4
w,xW (0)x∥2 ≤τ 3/2
Sign change in D′
Let s = ∥D′
w,x∥0 be the total number of sign changes of
the ReLU activations in the ﬁrst hidden layer caused by further adding W ′x. Observe that, the
total number of coordinates i where |((W (0) +W ρ)xi| ≤s′′ def
s1/4 is at most eO
probability. Now, if s ≥eΩ
, then W ′ must have caused the sign change of s
2 coordinates
each by absolute value at least s′′. Since ∥W ′∥2,4 ≤τw, this is impossible because s
2 × (s′′)4 > τ 4
Therefore, we must have
s ≤eO(s′′m3/2) = eO
w,x∥0 = s ≤eO
Next, for each coordinate i where (D′
w,x)i,i ̸= 0, we must have |((W (0) +W ρ)x)i| ≤|(W ′x)i|,
and since (W ′x)4
i must sum up to at most τw for those s coordinates, we have
(W (0) + W ρ)x∥2 ≤
i,(D′w,x−D′′w,x)i,i̸=0
i,(D′w,x−D′′w,x)i,i̸=0
Sum up: First Layer.
Combining (C.22) and (C.23) and using τw ≤m−1/4
from assumption,
By the 1-Lipschitz continuity of ReLU, we know that w.h.p.
(Dw,x + D′
w,x)(W (0) + W ρ + W ′)x −Dw,xW (0)x
σ(W (0) + W ρ + W ′)x) −σ(W (0)x)
≤∥(W ρ + W ′)x∥2 ≤∥W ′x∥2 + ∥W ρx∥2 ≤eO
where we have used our assumption σw ≤τwm−1/4
Second Layer Sign Change.
The sign change in the second layer is caused by input vector
changing from
V (0)z0 + V (0)z2 + V ρ(z0 + z2) + V ′(z0 + z2).
Here, using w.h.p. ∥z∥2 ≤eO(1), we have
∥V ρ(z0 + z2)∥∞≤eO(σv) · (∥z0∥2 + ∥z2∥2) ≤eO(σv)
∥V (0)z2 + V ′(z0 + z2)∥2 ≤eO (τv + ∥z2∥2) ≤eO
τv + τwm1/4
In comparison (at random initialization) we have V (0)z0 ∼N
with ∥z0∥2 = eΩ(1). Using
a careful two-step argument (see Claim C.8), we can bound
τv + τwm1/4
m2 + σvm3/2
Part II, Diagonal Cross Term
gr(x; W, V ) = ar(Dv,x + D′
 V (Dw,x + D′
w,x) (Wx + b1) + b2
r (x; W, V ) = ar(Dv,x + D′
v,x)V (Dw,x + D′
w,x)(Wx + b1)
(x; W, V ) = ar(Dv,x + D′
v,x)V (Dw,x + D′
and one can carefully check that
gr(x; W (0) + W ρ + W ′ + ηΣW ′′, V (0) + V ρ + V ′ + ηV ′′Σ)
x; W (0) + W ρ + W ′, V (0) + V ρ + V ′
(ηΣW ′′, ηV ′′Σ)
r (x; W (0) + W ρ + W ′, ηV ′′Σ) + g(b,b)
(x; ηΣW ′′, V (0) + V ρ + V ′)
error terms
We consider the last two error terms.
First error term.
The ﬁrst term is
r (x; W (0) + W ρ + W ′, ηV ′′Σ)
= ηar(Dv,x + D′
v,x)V ′′Σ(Dw,x + D′
w,x)((W (0) + W ρ + W ′)x + b1)
Clearly, it has zero expectation with respect to Σ. With high probability, we have
∥(Dw,x + D′
w,x)((W (0) + W ρ + W ′)x + b1)∥∞
≤∥(W (0) + W ρ + W ′)x + b1∥∞≤eO
and we have ∥ar(Dv,x + D′
v,x)V ′′∥2 ≤eO
. By Fact C.9, using the randomness of Σ, with
high probability
r (x; W (0) + W ρ + W ′, ηV ′′Σ)| = η eO
Second error term.
We write down the second error term
(x; ηΣW ′′, V (0) + V ρ + V ′) = ηar(Dv,x + D′
v,x)(V (0) + V ρ + V ′)(Dw,x + D′
w,x)ΣW ′′x
= ηar(Dv,x + D′
v,x)V ′(Dw,x + D′
w,x)ΣW ′′x
+ ηarDv,x(V (0) + V ρ)(Dw,x + D′
w,x)ΣW ′′x
v,x(V (0) + V ρ)(Dw,x + D′
w,x)ΣW ′′x
Obviously all the three terms on the right hand side have zero expectation with respect to Σ.
• For the ﬁrst term, since w.h.p. ∥ar(Dv,x+D′
v,x)V ′(Dw,x+D′
w,x)∥2 = eO(τvm1/2
) and ∥W ′′x∥∞≤
τw, by Fact C.9, using the randomness of Σ we know that w.h.p.
|ar(Dv,x + D′
v,x)V ′(Dw,x + D′
w,x)ΣW ′′x| ≤eO(τvm1/2
• For the second term, since ∥W ′′x∥2 ≤τwm1/4
and w.h.p.29
∥arDv,x(V (0) + V ρ)(Dw,x + D′
w,x)∥∞≤∥arDv,x(V (0) + V ρ)∥∞≤eO(1)
by Fact C.9, using the randomness of Σ we know that w.h.p.
|arDv,x(V (0) + V ρ)(Dw,x + D′
w,x)ΣW ′′x| = eO(τwm1/4
• For the third term, again by ∥W ′′x∥∞≤τw and Fact C.9, we have: w.h.p.
v,x(V (0) + V ρ)ΣW ′′x| ≤eO
v,x(V (0) + V ρ)∥2τw
The following are some tools used in the above proofs.
A variant of the following claim has appeared in .
Claim C.8. Suppose V ∈Rm2×m1 is a random matrix with entries drawn i.i.d. from N
For all unit vector h ∈Rm1, and for all g′ ∈Rm2 that can be written as
2 where ∥g′
1∥≤1 and ∥g′
Let D′ be the diagonal matrix where (D′)k,k = I(V h+g′)k≥0−I(V h)k≥0. Then, letting x = D′(V h+g′),
29We note that the derivation of ∥arDv,x(V (0) + V ρ)∥∞≤eO(1) may be non-trivial for some readers, because Dv,x
is dependent of the randomness of V (0) + V (ρ). In fact, for every ﬁxed basis vector ej, we have w.h.p. ∥Dv,x(V (0) +
V ρ)ej∥2 ≤eO(1), and thus by the randomness of ar it satisﬁes |arDv,x(V (0) + V ρ)ej| ≤eO(1). Taking a union bound
over j = 1, 2, . . . , m2 gives the bound. Such proof idea was repeatedly used in .
∥x∥0 ≤O(m2∥g′
1∥2/3 + m3/2
1∥4/3 + m3/2
Proof of Claim C.8. We ﬁrst observe g = V h follows from N
regardless of the choice of h.
Therefore, in the remainder of the proof, we just focus on the randomness of g.
We also observe that (D′)j,j is non-zero for some diagonal j ∈[m2] only if
2)j| > |(g)j| .
2√m2 be a parameter to be chosen later. We shall make sure that ∥g′
• We denote by S1 ⊆[m2] the index sets where j satisﬁes |(g)j| ≤ξ. Since we know (g)j ∼
N(0, 1/m2), we have Pr[|(g)j| ≤ξ] ≤O
for each j ∈[m2]. Using Chernoﬀbound for
all j ∈[m2], we have with high probability
|S1| = |{i ∈[m2]: |(g)j| ≤ξ}| ≤O(ξm3/2
Now, for each j ∈S1 such that xj ̸= 0, we must have |xj| = |(g + g′
2)j| ≤|(g′
1)j| + 2ξ so
we can calculate the ℓ2 norm of x on S1:
1)i| + 2ξ) ≤2ξ|S1| +
• We denote by S2 ⊆[m2] \ S1 the index set of all j ∈[m2] \ S1 where xj ̸= 0. Using (C.26), we
have for each j ∈S2:
1)j| ≥|(g)j| −|(g′
2)j| ≥ξ −∥g′
This means |S2| ≤4∥g′
. Now, for each j ∈S2 where xj ̸= 0, we know that the signs of
2)j and (g)j are opposite. Therefore, we must have
|xj| = |(g + g′
2)j| ≤|(g′
2)j| ≤|(g′
1)j| + ξ/2 ≤2|(g′
and therefore
From above, we have ∥x∥0 ≤|S1|+|S2| ≤O
. Choosing ξ = max
2∥∞, Θ(∥g′
we have the desired result on sparsity.
Combining the two cases, we have
Choosing ξ = max
2∥∞, Θ(∥g′
, we have the desired bound on Euclidean norm.
Fact C.9. If Σ is a diagonal matrix with diagonal entries randomly drawn from {−1, 1}. Then,
given vectors x, y, with high probability
|x⊤Σy| ≤eO(∥x∥2 · ∥y∥∞)
Corollary 6.6: Existence After Coupling
Corollary 6.6 is a corollary to Lemma 6.2 with g(0) replaced with g(b,b). Recall that g(b,b) is diﬀerent
from g(0) only by the diagonal signs, namely,
r (x; W ⋇, V ⋇) = arDv,xV ⋇Dw,xW ⋇x
(x; W ⋇, V ⋇) = ar(Dv,x + D′
v,x)V ⋇(Dw,x + D′
where Dv,x + D′
v,x and Dw,x + D′
w,x are the diagonal sign matrices determined at W (0) + W ′ + W ρ,
V (0) + V ′ + V ρ.
Corollary 6.6 (existence after coupling). In the same setting as Lemma 6.2, perturbation matrices
W ′, V ′ (that may depend on the randomness of the initialization and D) with
∥W ′∥2,4 ≤τw, ∥V ′∥F ≤τv .
Using parameter choices from Table 1, w.h.p. there exist W ⋇and V ⋇(independent of the randomness of W ρ, V ρ) satisfying
∥W ⋇∥2,∞= max
∥V ⋇∥2,∞= max
r (x) −g(b,b)
(x; W ⋇, V ⋇)
L(G(b,b)(x; W ⋇, V ⋇), y)
Proof of Corollary 6.6. The idea to prove Corollary 6.6 is simple. First construct W ⋇and V ⋇from
Lemma 6.2, and then show that g(0) and g(b,b) are close using Lemma 6.5. By Lemma 6.5 and our
parameter choices Table 1, we know
w,x∥0 ≤eO(τ 4/5
m2 + τ 2/3
≤eO((ε/C0)Θ(1)m2)
Now, recall that Lemma 6.2 says ∥W ⋇∥2,∞≤τw,∞and ∥V ⋇∥2,∞≤τv,∞for τw,∞τv,∞=
Therefore,
|arDv,xV ⋇D′
w,xW ⋇x| =
ar,j(Dv,x)j,j⟨v⋇
≤eO(m2τv,∞)∥D′
≤eO(m2τv,∞)
∥D′w,x∥0τw,∞≪ε
v,xV ⋇Dw,xW ⋇x| =
v,x)j,j⟨v⋇
j , Dw,xW ⋇x⟩
v,x∥0τv,∞)∥Dw,xW ⋇x∥2
v,x∥0τv,∞) · O(√m1τw,∞) ≪ε
In other words,
|g(b,b)(x; W ⋇, V ⋇) −g(0)(x; W ⋇, V ⋇)| ≤2ε .
Lemma 6.9: Smoothed Real vs Pseudo Networks
= fr(x; W + W ρ + ηΣW ′′, V + V ρ + ηV ′′Σ)
= arDv,x,ρ,η
(V + V ρ + ηV ′′Σ)Dw,x,ρ,η
 (W + W ρ + ηΣW ′′)x + b1
= gr(x; W + W ρ + ηΣW ′′, V + V ρ + ηV ′′Σ)
= arDv,x,ρ
(V + V ρ + ηV ′′Σ)Dw,x,ρ
 (W + W ρ + ηΣW ′′)x + b1
Lemma 6.9 (smoothed real vs pseudo). There exists η0 =
poly(m1,m2) such that, for every η ≤η0,
for every ﬁxed x with ∥x∥2 = 1, for every W ′, V ′, W ′′, V ′′ that may depend on the randomness of
the initialization and x, with
∥W ′∥2,4 ≤τw,
∥V ′∥2,2 ≤τv,
∥W ′′∥2,∞≤τw,∞,
∥V ′′∥2,∞≤τv,∞
we have with high probability:
|Pρ,η −P ′
where Op hides polynomial factor of m1, m2.
Proof of Lemma 6.9. Since Pρ,η and P ′
ρ,η only diﬀer in the sign pattern, we try to bound the
(expected) output diﬀerence Pρ,η −P ′
ρ,η by analyzing these sign changes. We use the same proof
structure as Lemma 6.5, that is to ﬁrst bound the sign changes in the ﬁrst layer, and then the
second layer. One can carefully verify
ρ,η = arDv,x,ρ(V + V ρ + ηV ′′Σ)
 Dw,x,ρ,η −Dw,x,ρ
  (W + W ρ + ηΣW ′′)x + b1
 Dv,x,ρ,η −Dv,x,ρ
(V + V ρ + ηV ′′Σ)Dw,x,ρ,η
 (W + W ρ + ηΣW ′′)x + b1
We call ♣the output diﬀerence caused by sign change of the ﬁrst layer; and ♠that caused by the
sign change of the second layer.
Sign Change of First Layer.
z = Dw,x,ρ
 (W + W ρ + ηΣW ′′)x + b1
z + z′ = Dw,x,ρ,η
 (W + W ρ + ηΣW ′′)x + b1
The ﬁrst observation here is that, since ∥ηΣW ′′x∥∞≤ητw,∞, when a coordinate i has sign change
(i.e. has z′
i ̸= 0), it has value at most |z′
i| ≤ητw,∞. In other words
∥z′∥∞≤ητw,∞.
Since ∥ηΣW ′′x∥∞≤ητw,∞, and since each coordinate of W ρx is i.i.d. from N(0, σ2
w), we know
i ̸= 0] ≤eO
One consequence of (C.27) is Pr[∥z′∥0 ≥2] ≤Op(η2). When ∥z′∥0 ≥2, the contribution to ♣is
Op(η). Multiplying them together, the total contribution to ♣in expectation is at most Op(η3).
Thus, we only need to consider the case ∥z∥0 = 1. Let i be this coordinate so that z′
i ̸= 0. This
happens with probability at most O(ητw,∞/σw) for each i ∈[m1]. The contribution of z′ to ♣is
♣= arDv,x,ρ(V + V ρ + ηV ′′)z′
and let us deal with the three terms separately:
• For the term arDv,x,ρηV ′′z′, it is of absolute value at most Op(η2). Since ∥z0∥0 = 1 happens
with probability Op(η), the total contribution to the expected value of ♣is only Op(η3).
• For the term arDv,x,ρ(V + V ρ)z′, we ﬁrst observe that with high probability ∥arDv,x,ρ(V +
V ρ)∥∞≤eO(∥ar∥2
√m2 ) ≤eO(1) (for a proof see Footnote 29). Therefore, given ∥z′∥0 = 1 and
∥z′∥∞≤ητw,∞, we have that ∥arDv,x,ρ(V + V ρ)z′∥≤eO(ητw,∞). Since this happens with
probability at most O(ητw,∞/σw) × m1—recall there are m1 many possible i ∈[m1]— the
total contribution to the expected value of ♣is eO
In sum, we have with high probability
EW ρ,V ρ[|♣|] ≤eO
Sign Change of Second Layer. Recall that the sign of the ReLU of the second layer is changed
from Dw,x,ρ to Dw,x,ρ,η. Let us compare the vector inputs of these two matrices before ReLU is
applied, that is
 (V + V ρ + ηV ′′)Dw,x,ρ,η
 (W + W ρ + ηW ′′)x + b1
−((V + V ρ)Dw,x,ρ ((W + W ρ)x + b1) + b2) .
This diﬀerence δ has the following four terms:
1. η(V + V ρ)Dw,x,ρΣW ′′x.
With ∥W ′′x∥∞≤τw,∞and ∥V ∥2 ≤eO(1), by Fact C.9 we know that w.h.p.
∥η(V + V ρ)Dw,x,ρΣW ′′x∥∞≤η∥(V + V ρ)Dw,x,ρ∥2 · ∥W ′′x∥∞· eO(1) ≤eO(ητw,∞) .
2. (V + V ρ + ηV ′′Σ)z′.
This is non-zero with probability Op(η), and when it is non-zero, its Euclidean norm is Op(η).
3. ηV ′′Σz.
Since ∥z∥∞≤eO(τw + σw + m−1/2
) = O(m−1/2
) owing to (C.25), by Fact C.9, we know that
∥ηV ′′Σz∥∞≤eO(η) · max
i ∥2 · ∥z∥∞≤eO(ητv,∞m−1/2
4. η2V ′′ΣDw,x,ρΣW ′′x is at most Op(η2) in Euclidean norm.
w.p. ≤Op(η);
√m1 + ητw,∞) + Op(η2),
otherwise.
Since originally each coordinate of V ρz follows from N(0, σv∥z∥2
2) with ∥z∥2 = eΩ(1), using a similar
argument as (C.27)30 we can bound the contribution to the expected value of ♠by:
EW ρ,V ρ[|♠|] ≤m2 × eO
τv,∞+ τw,∞
√m1 τv,∞+ τw,∞)
+ η2 m2τ 2
+ Op(η3) .
Lemma 6.11: Stronger Coupling
We will need the following coupling lemma when Σ is used (for Algorithm 2).
Lemma 6.11 (stronger coupling). Given a ﬁxed x, with high probability over the random initialization and over a random diagonal matrix Σ with diagonal entries i.i.d. generated from {−1, 1},
it satisﬁes that for every W ′, V ′ (that can depend on the initialization and x but not Σ) with
∥V ′∥2 ≤τv, ∥W ′∥2,4 ≤τw for τv ∈ and τw ∈
fr(x; W (0) + ΣW ′, V (0) + V ′Σ) = arDv,x(V (0)Dw,x(W (0)x + b1) + b2) + arDv,xV ′Dw,xW ′x
Under parameter choices Table 1, the last error term is at most ε/k.
Proof of Lemma 6.11. For notation simplicity, let us do the proof without the bias term b1 and b2.
The proof with them are analogous.
We use D(0)
w,x and D(0)
v,x to denote the sign matrices at random initialization W (0), V (0), and we
w,x and D(0)
v,x be the sign matrices at W (0) + ΣW ′, V (0) + V ′Σ. Deﬁne
w,x(W (0)x + ΣW ′x).
Since w.h.p. each coordinate of z has ∥z∥∞= eO(m−1/2
), using Fact C.9 (so using the randomness
of Σ), we know with high probability
2 = eO(τ 2
Thus we have: ∥V ′Σz∥2 ≤eO(τvm−1/2
On the other hand, applying Lemma 6.5 with W ′′ = 0 and V ′′ = 0, we have ∥z2∥0 ≤s =
30Namely, to ﬁrst show that each coordinate i ∈[m2] satisﬁes (Dv,x,ρ,η −Dv,x,ρ)i,i ̸= 0 with probability
√m1 τv,∞+τw,∞)
Then, since we can ignoring terms of magnitude Op(η3), it suﬃces to consider the case
of ∥Dv,x,ρ,η −Dv,x,ρ∥0 = 1, which occurs with probability at most m2 × eO
√m1 τv,∞+τw,∞)
by union bound.
Finally, each coordinate changes by at most eO
√m1 τv,∞+ τw,∞
by the argument above.
). Therefore, using the same derivation as (C.24), we can bound its Euclidean norm
∥z2∥2 = ∥D′
w,x(W (0) + ΣW ′)x∥2 ≤
i,(D′w,x)i,i̸=0
i,(D′w,x)i,i̸=0
Using the same derivation as (C.28), we also have w.h.p
∥V ′Σz2∥2 ≤eO
 ∥V ′∥F ∥z2∥∞
 ∥V ′∥F ∥z2∥2
Now, recall using Cauchy-Shwartz and the 1-Lipschitz continuity of the ReLU function, we have
for every (xi, yi)i∈[m2], with high probability (over a):
ai,r(σ(xi + yi) −σ(xi)) ≤eO(√m2)∥(σ(xi + yi) −σ(xi))i∈[m2]∥2 ≤eO(√m2)∥y∥2.
Therefore, we can bound that
fr(x; W (0) + ΣW ′, V (0) + V ′Σ)
i, z + z1 + z2
, z + z1 + z2⟩+ ⟨Σv′
τv + √m2τvτ 6/5
To bound x, we consider the diﬀerence between
x = ar(D(0)
V (0)(z + z1 + z2) + V ′D(0)
y = arD(0)
V (0)(z + z1 + z2) + V ′D(0)
v,x is the diagonal sign change matrix due to moving input from V (0)z to V (0)(z + z1 +
z2) + V ′D(0)
w,xW ′x. This diﬀerence has the following three terms.
• V (0)z1 = V (0)D(0)
w,xΣW ′x. Since ∥W ′x∥2 ≤τwm1/4
and maxi ∥V (0)
√m2 ), by Fact C.9
(thus using the randomness of Σ), we know that w.h.p. ∥V (0)z1∥∞≤eO
• V (0)z2. Using the sparsity of z2 we know w.h.p. ∥V (0)z2∥∞≤eO(∥z2∥2
• ∥V ′D(0)
w,xW ′x∥2 ≤∥V ′∥F · ∥W ′x∥2 ≤τwm1/4
Together, using ∥ar∥∞≤eO(1) and invoking Claim C.8, we can bound it by:
|x −y| ≤eO
τv)4/3m1/2
(When invoking Claim C.8, we need τw ≤m−9/16
and τwm1/4
Finally, from y to our desired goal
z = arD(0)
v,xV (0)D(0)
w,xW (0)x + arD(0)
v,xV ′D(0)
w,xW ′x = arD(0)
V (0)z + V ′D(0)
there are still two terms:
• Since w.h.p. ∥arD(0)
v,xV (0)∥∞= eO(1) and z2 is s sparse, we know that w.h.p.
v,xV (0)z2| ≤eO(∥z2∥2
• Since ∥W ′x∥2 ≤τwm1/4
and w.h.p.
v,xV (0)D(0)
eO(1) (see Footnote 29), by
v,xV (0)z1| = |arD(0)
v,xV (0)D(0)
w,xΣW ′x| ≤eO(∥arD(0)
v,xV (0)D(0)
w,x∥∞· ∥W ′x∥2) ≤eO
In other words
|y −z| ≤eO
Putting together (C.29), (C.30), (C.31), one can carefully verify that
fr(x; W (0) + ΣW ′, V (0) + V ′Σ) = x ± eO
τv + √m2τvτ 6/5
v,xV (0)D(0)
w,xW (0)x + arD(0)
v,xV ′D(0)
τv)4/3m1/2
τv + √m2τvτ 6/5
v,xV (0)D(0)
w,xW (0)x + arD(0)
v,xV ′D(0)
Above, we have used our parameter choices τv ∈ and τw ∈[
Optimization
Recall in the ﬁrst variant of SGD,
L′(λt, Wt, Vt) = EW ρ,V ρ,(x,y)∼Z
x; W (0) + W ρ + Wt, V (0) + V ρ + Vt
Lemma 6.7: Descent Direction
Lemma 6.7 (descent direction). For every ε0 ∈(0, 1) and ε =
2Cs(Φ,p2Cs(φ,1))Cs(φ,1)2 , for every
constant γ ∈(0, 1/4], consider the parameter choices in Table 1, and consider any λt, Wt, Vt (that
may depend on the randomness of W (0), b(0), V (0), b(1) and Z) with
 (ε/ log(m1m2))Θ(1), 1
L′(λt, Wt, Vt) ∈[(1 + γ)OPT + Ω(ε0/γ), eO(1)]
With high probability over the random initialization, there exists W ⋇, V ⋇with ∥W ⋇∥F , ∥V ⋇∥F ≤1
such that for every η ∈
poly(m1,m2)
L′  λt, Wt + √ηΣW ⋇, Vt + √ηV ⋇Σ
, L′ (1 −η)λt, Wt, Vt
≤(1 −ηγ/4)(L′(λt, Wt, Vt)) ,
where Σ ∈Rm1×m1 is a diagonal matrix with each diagonal entry i.i.d. uniformly drawn from {±1}.
Proof of Lemma 6.7. For each output r ∈[k],
• Deﬁne the “pseudo function” for every W ′, V ′ as
gr(x; W ′, V ′) = arDv,x,ρ,t[(V (0) + V ρ + V ′)Dw,x,ρ,t[(W (0) + W ρ + W ′)x + b1] + b2]
where Dv,x,ρ,t and Dw,x,ρ,t are the diagonal sign matrices at weights W (0) + W ρ + Wt, V (0) +
• Recall the real network as
fr(x; W ′, V ′) = arDv,x,ρ,V ′[(V (0) + V ρ + V ′)Dw,x,ρ,W ′[(W (0) + W ρ + W ′)x + b1] + b2]
where Dv,x,ρ,V ′ and Dw,x,ρ,W ′ are the diagonal sign matrices at weights W (0)+W ρ+W ′, V (0)+
V ρ + V ′.
• G(x; W ′, V ′) = (g1, . . . , gk) and F(x; W ′, V ′) = (f1, . . . , fk).
As a sanity check, we have G(x; Wt, Vt) = F(x; Wt, Vt). (Here we slightly override the notation and
use fr(x; W ′, V ′) to denote fr(x; W (0) + W ρ + W ′, V (0) + V ρ + V ′).)
By our regularizer parameters λw, λv in Table 1, as long as L′(λt, Wt, Vt) ≤eO(1), we know
λtVt) ≤eO(1) =⇒∥
λtWt∥2,4 ≤eO(τ ′
λtVt∥2,2 ≤eO(τ ′
=⇒∥Wt∥2,4 ≤τw and ∥Vt∥2,2 ≤τv
Applying Corollary 6.6 (but scaling up the target F ∗by
λt ), we know that there exists W ⋇, V ⋇
with (here we have scaled up W ⋇and scaled down V ⋇both by m0.005
λtW ⋇∥2,∞≤
λtV ⋇∥2,∞≤m1/2−0.005
F ∗(x) −G∗(x)∥2 ≤ε
r Dv,x,ρ,tV ⋇Dw,x,ρ,tW ⋇x
By our parameter choices in Table 1, this implies
∥V ⋇∥F ≪1 .
Let us study an update direction
W = Wt + √ηΣW ⋇, bV = Vt + √ηV ⋇Σ.
Change in Regularizer.
We ﬁrst consider the change of the regularizer. We know that
Vt + √ηV ⋇Σ
F + η∥V ⋇∥2
On the other hand,
Wt + √ηΣW ⋇
wt,i + √ηΣw⋇
For each term i ∈[m1], we can bound
∥wt,i + √ηΣw⋇
2 = ∥wt,i∥2
2 + 2√η⟨wt,i, w⋇
and therefore
wt,i + √ηΣw⋇
2 + 4η⟨wt,i, w⋇
i ⟩2 + η2∥w⋇
2 + 2η∥wt,i∥2
2 + 6η∥wt,i∥2
2 + Op(η2) .
(Recall we use Op(·) to hide polynomial factors in m1 and m2.) By Cauchy-Schwarz,
and therefore
Wt + √ηΣW ⋇
2 + 6η∥Wt∥2
2,4 + Op(η2)
By λv∥√λtV ⋇∥2
F ≤ε0, λw∥√λtW ⋇∥4
2,4 ≤ε0, and λw∥√λtWt∥4
2,4 ≤R(√λtWt, √λtVt), we know
λt bV )] ≤R(
λtVt) + 4ηε0 + 6η√ε0 ·
λtVt) + 10ηε0 + 1
Change in Objective.
We now consider the change in the objective value. Recall from (C.33)
the construction of good network W ⋇, V ⋇satisﬁes τv,∞≤
polynomially small η, by Lemma 6.9 (replacing its η with √η), we have:
W, bV ) −fr(x; c
≤O(εη) + Op(η1.5).
First we focus on G(x; c
W, bV ). Since ∥Wt∥2,4 ≤τw and ∥Vt∥2,2 ≤τv from (C.32), we can apply
Lemma 6.5 to get
W, bV ) = G(x; Wt, Vt) + √ηG′(x) + ηG∗(x)
= F(x; Wt, Vt) + √ηG′(x) + ηG∗(x)
where G′(x) is from Lemma 6.5 and satisﬁes EΣ[G′(x)] = 0 and w.h.p. ∥G′(x)∥2 ≤ε; and G∗(x) is
from (C.34).
Combining (C.36) and (C.37), we know that for every ﬁxed x, y in the support of distribution
EW ρ,V ρ,Σ[L(λtF(x; c
W, bV ), y)]
≤EW ρ,V ρ,Σ[L(λtG(x; c
W, bV ), y)] + O(ηε) + Op(η1.5).
L(λtEΣ[G(x; c
W, bV ), y])
+ EW ρ,V ρ,Σ
2 + O(ηε) + Op(η1.5).
≤EW ρ,V ρL (λtG(x; Wt, Vt) + λtηG∗(x), y) + O(ηε) + Op(η1.5).
≤EW ρ,V ρL (λtG(x; Wt, Vt) + ηF ∗(x), y) + O(ηε) + Op(η1.5).
= EW ρ,V ρL (λtF(x; Wt, Vt) + ηF ∗(x), y) + O(ηε) + Op(η1.5).
Above, x uses the 1-Lipschitz smoothness of L which implies
E[L(v)] ≤L(E[v]) + E[∥v −E[v]∥2]
and EΣ[G(c
W, bV , x)] = G(Wt, Vt, x)+ηG∗(x). Inequality y also uses EΣ[G(x; c
W, bV )] = G(x; Wt, Vt)+
ηG∗(x). Inequality z uses (C.34) and the 1-Lipschitz continuity of L.
Next, by convexity of the loss function, we have
L (λtF(x; Wt, Vt) + ηF ∗(x), y) = L
 (1 −η)(1 −η)−1λtF(x; Wt, Vt) + ηF ∗(x), y
 L((1 −η)−1λtF(x; Wt, Vt), y)
+ ηL(F ∗(x), y)
For suﬃciently small η, we know that
L((1 −η)−1λtF(x; Wt, Vt), y) + L((1 −η)λtF(x; Wt, Vt), y) ≤2L (λtF(x; Wt, Vt), y) + Op(η2)
Putting this into (C.39), we have
L (λtF(x; Wt, Vt) + ηF ∗(x), y)
≤(1 −η) (2L (λtF(x; Wt, Vt), y) −L((1 −η)λtF(x; Wt, Vt), y)) + ηL(F ∗(x), y) + Op(η2)
Putting All Together.
Let us denote
c1 = EW ρ,V ρ,Σ,(x,y)∼Z[L(λtF(x; c
W, bV ), y)]
c2 = EW ρ,V ρ,(x,y)∼Z[L((1 −η)λtF(x; Wt, Vt), y)]
c3 = EW ρ,V ρ,(x,y)∼Z[L(λtF(x; Wt, Vt), y)]
= c1 + EΣ[R(
2 = L′ ((1 −η)λt, Wt, Vt) = c2 + R(
(1 −η)λtWt,
(1 −η)λtVt)
3 = L′ (λt, Wt, Vt) = c3 + R(
The objective growth inequalities (C.38) and (C.40) together imply
c1 ≤(1 −η) (2c3 −c2) + η(OPT + O(ε)) + Op(η1.5)
The regularizer growth inequality (C.35) implies
1 −c1 ≤(1 + ηγ
3 −c3) + O(ηε0/γ)
and therefore
3 −c3) −(c′
3 −c3) −(1 −η)(c′
3 + O(ηε0/γ) + O(η2) .
Putting (C.41) and (C.42) together we have
3 + η(OPT + O(ε0/γ)) + Op(η1.5)
Multiplying
2(1−η) on both sides, we have:
2(1 −η)−1c′
2OPT + O(ηε0/γ) + Op(η1.5)
Therefore,
2(1 −η)−1 + 1
2OPT + O(ηε0/γ) + Op(η1.5)
and this implies that
2OPT + O(ηε0/γ) + Op(η1.5)
Therefore, as long as c′
3 ≥(1 + γ)OPT + Ω(ε0/γ) and γ ∈ , we have:
2} ≤(1 −ηγ/4)c′
This completes the proof.
Lemma 6.8: Convergence
Lemma 6.8 (convergence). In the setting of Theorem 3, with probability at least 99/100, Algorithm 3
(the ﬁrst SGD variant) converges in TTw = poly (m1, m2) iterations to a point
L′(λT , WT , VT ) ≤(1 + γ)OPT + ε0.
Proof of Lemma 6.8. For the ﬁrst variant of SGD, note that there are T = Θ(η−1 log log(m1m2)
rounds of weight decay, which implies that λt ≥(ε/ log(m1m2))O(1) is always satisﬁed (because γ
is a constant). By Lemma 6.7, we know that as long as L′ ∈[(1 + γ)OPT + Ω(ε0/γ), eO(1)], then
there exists ∥W ⋇∥F , ∥V ⋇∥F ≤1 such that either
L′  λt−1, Wt + √ηΣW ⋇, Vt + √ηV ⋇Σ
≤(1 −ηγ/4)(L′(λt−1, Wt, Vt))
L′((1 −η)λt−1, Wt, Vt) ≤(1 −ηγ/4)(L′(λt−1, Wt, Vt))
In the ﬁrst case, recall L′ is B = poly(m1, m2) second-order smooth,31 by Fact A.8, it satisﬁes
(λt−1 is ﬁxed and the Hessian is with respect to W and V ):
 ∇2L′(λt−1, Wt, Vt)
< −1/(m1m2)8 .
On the other hand, for every t ≥1, since Wt is the output of noisy SGD, by the escape saddle
point theorem of (stated in Lemma A.9), we know with probability at least 1 −p it satisﬁes
 ∇2L′(λt−1, Wt, Vt)
> −1/(m1m2)8 . Choosing p =
1000T , we know with probability at least
0.999, this holds for all rounds t = 1, 2, . . . , T. In other words, for all rounds t = 1, 2, . . . , T, the
ﬁrst case cannot happen and therefore as long as L′ ≥(1 + γ)OPT + Ω(ε0/γ),
L′((1 −η)λt−1, Wt, Vt) ≤(1 −ηγ/4)(L′(λt−1, Wt, Vt)).
On the other hand, for each round t = 0, 1, . . . , T −1, as long as L′ ≤eO(1), by Lemma A.9, it
holds that
L′(λt, Wt+1, Vt+1) ≤L′(λt, Wt, Vt) + (m1m2)−1 .
Since initially, L′(λ1, W0, V0) ≤eO(1) w.h.p., we have w.h.p. L′ ≤eO(1) throughout the process.
Since γ is a constant, after T = Θ(η−1 log log m
ε0 ) rounds of weight decay, we have L′ ≤(1+γ)OPT+
O(ε0/γ). Since γ is a constant, re-scaling ε0 down by a constant factor ﬁnishes the proof.
Remark C.10. For the second variant of the SGD, note that
Σ1Wt + √ηΣ1ΣW ⋇,
VtΣ1 + √ηV ⋇Σ1Σ
31When convoluted with Gaussian noise ∇(f ∗g) = f ∗∇g, every bounded function f becomes inﬁnite-order
diﬀerentiable with parameter B inversely-polynomially dependent on the noise level g.
satisﬁes that Σ1Σ is still a diagonal matrix with each diagonal entry i.i.d.
convergence results from Lemma 6.7 and Lemma 6.8 still apply, if we replace L′ with
L′′(λt, Wt, Vt) = EW ρ,V ρ,Σ,(x,y)∼Z
x; W (0) + W ρ + ΣWt, V (0) + V ρ + VtΣ
Generalization
Lemma 6.10: Generalization For LR = L1
We derive a very crude Rademacher complexity bound for our three-layer neural network. We have
not tried to tighten the polynomial dependency in m1 and m2.
Lemma 6.10 (generalization for LR = L1). For every τ ′
w ≥0, every σv ∈(0, 1/√m2], w.h.p.
for every r ∈[k] and every N ≥1, the empirical Rademacher complexity is bounded by
N Eξ∈{±1}N
∥V ′∥F ≤τ ′v,∥W ′∥2,4≤τ ′w
ξifr(xi; W (0) + W ρ + W ′, V (0) + V ρ + V ′)
wm1√m2 + τ ′
m1m2τ ′w(1/√m1 + τ ′w)
Proof. Let W = W (0) + W ρ and V = V (0) + V ρ for notation simplicity. Recall the input to the
j-th neuron on the second layer is
nj(x; W + W ′, V + V ′) =
(vj,i + v′
i, x⟩+ b(0)
For analysis purpose, let us truncate V ′ by zeroing out all of its large coordinates. Namely,
V ′′ ∈Rm2×m1 is deﬁned so that V ′′
i,j if |V ′
i,j| ≤δ and V ′′
i,j = 0 otherwise. At most (τ ′
coordinates will be zeroed out because ∥V ′∥F ≤τ ′
v. Since for each x in the training set, we have with
high probability |σ
i, x⟩+b(0)
w), and since ∥ar∥∞≤eO(1),
it satisﬁes
|fr(x; W + W ′, V + V ′) −fr(x; W + W ′, V + V ′′)| ≤eO(
We now bound the Rademacher complexity of fr(xi; W +W ′, V +V ′′) in the following simple steps.
• {x 7→⟨w′
i, x⟩| ∥w′
w} has Rademacher complexity O( τ ′
N ) by Proposition A.12a.
• {x 7→⟨wi + w′
i, x⟩+ bi | ∥w′
w} has Rademacher complexity O( τ ′
N ) because singleton
class has zero complexity and adding it does not aﬀect complexity by Proposition A.12c.
• {x 7→nj(x; W + W ′, V + V ′′) | ∥W ′∥2,∞≤τ ′
j ∥1 ≤√m1τ ′
j ∥∞≤δ} has Rademacher
complexity O( τ ′
N ) · eO( m1
√m2 + δm1) + eO( τ ′
N ). This is because w.h.p. ∥vj∥1 ≤eO( m1
√m2 ) so we
can apply Proposition A.12e, and because ∥v′′
j ∥1 ≤√m1τ ′
v and ∥v′′
j ∥∞≤δ so we can apply
Proposition A.12f by choosing f(0)
(x) = ⟨wi, x⟩+ bi which satisﬁes |f(0)
√m1 ) w.h.p.
• {x 7→fr(x; W + W ′, V + V ′′) | ∥W ′∥2,∞≤τ ′
w ∧∀j ∈[m2], ∥v′′
j ∥1 ≤√m1τ ′
v} has Rademacher
complexity
N ) · eO( m1
√m2 + δm1) + eO( τ ′
· eO(m2) because w.h.p. ∥ar∥1 ≤eO(m2) and
Proposition A.12e.
Finally, noticing that ∥W ′∥2,4 ≤τ ′
w implies ∥W ′∥2,∞≤τ ′
w and ∥V ′′∥F ≤τ ′
v implies ∥v′′
j ∥2 ≤√m1τ ′
v, we ﬁnish the proof that the Rademacher complexity of fr(xi; W +W ′, V +V ′′)
is at most
wm1√m2 + τ ′
Combining this with (C.43), and tuning the best choice of δ gives the desired result.
Lemma 6.12: Generalization For LR = L2
Lemma 6.12 (generalization for LR = L2). For every τ ′
v ∈ , τ ′
σw ∈[0, 1/√m1] and σv ∈[0, 1/√m2], w.h.p. for every r ∈[k] and every N ≥1, we have by our
choice of parameters in Lemma 6.7, the empirical Rademacher complexity is bounded by
N Eξ∈{±1}N
∥V ′∥F ≤τ ′v,∥W ′∥2,4≤τ ′w
ξiEΣ[fr(xi; W (0) + W ρ + ΣW ′, V (0) + V ρ + V ′Σ)]
w)8/5m9/10
w)16/5m9/5
Under parameter choices in Table 1, this is at most eO
Proof of Lemma 6.12. Let W = W (0) + W ρ and V = V (0) + V ρ for notation simplicity. Applying
Lemma 6.11 (with τw chosen as τ ′
w), we know that by our choice of parameters,
fr(x; W + ΣW ′, V + V ′Σ) = arDv,x (V Dw,x(Wx + b1) + b2) + arDv,xV ′Dw,xW ′x ± B
for B = eO
w)8/5m9/10
w)16/5m9/5
We bound the Rademacher complexity of the right hand side. It consists of three terms and
the Rademacher complexity is the summation of the three (recall Proposition A.12c).
The ﬁrst term does not depend on W ′ or V ′ so has Rademacher complexity zero.
The third term has Rademacher complexity at most B.
The second term corresponds to the function class
x 7→arDv,xV ′Dw,xW ′x | ∥V ′∥F ≤τ ′
v, ∥W ′∥2,4 ≤τ ′
We calculate its Rademacher complexity as follows.
∥W ′∥2,4≤τ ′w,∥V ′∥F ≤τ ′v
ξjarDv,xjV ′Dw,xjW ′xj
∥W ′∥2,4≤τ ′w,∥V ′∥F ≤τ ′v
ξjxjarDv,xjV ′Dw,xjW ′)
∥W ′∥2,4≤τ ′w,∥V ′∥F ≤τ ′v
ξjxjarDv,xjV ′Dw,xj
∥V ′∥F ≤τ ′v
ξjxjarDv,xjV ′Dw,xj
Let us bound the last term entry by entry. Let [Dw,xj]q denote the q-th column of Dw,xj, [V ′]q
the q-th column of V ′.
ξjxjarDv,xjV ′Dw,xj
p∈[d],q∈[m1]
ξjxj,parDv,xjV ′[Dw,xj]q
p∈[d],q∈[m1]
ξj[Dw,xj]q,qxj,parDv,xj[V ′]q
p∈[d],q∈[m1]
ξj[Dw,xj]q,qxj,parDv,xj
For random ξj, we know that w.h.p. over the randomness of ξj (notice that we can do so because
Dw,xj only depends on W (0) but not on W ′, so we can take randomness argument on ξ),
ξj[Dw,xj]q,qxj,parDv,xj
∥V ′∥F ≤τ ′v
p∈[d],q∈[m1]
ξj[Dw,xj]q,qxj,parDv,xj
∥V ′∥F ≤τ ′v
p∈[d],q∈[m1]
j,p∥[V ′]q∥2
∥V ′∥F ≤τ ′v
This implies bR(X; F) ≤eO
and ﬁnishes the proof.
Final Theorems
Theorem 3: First SGD Variant
Theorem 3. Consider Algorithm 3. For every constant γ ∈(0, 1/4], every ε0 ∈(0, 1/100], every
2Cs(Φ,p2Cs(φ,1))Cs(φ,1)2 , there exists
Cε(Φ, √p2Cε(φ, 1)), 1
such that for every m2 = m1 = m ≥M, and properly set λw, λv, σw, σv in Table 1, as long as
N ≥eΩ(Mm3/2)
there is a choice η = 1/poly(m1, m2) and T = poly(m1, m2) such that with probability ≥99/100,
E(x,y)∼DL(λT F(x; W (out)
), y) ≤(1 + γ)OPT + ε0.
Proof of Theorem 3. For notation simplicity let LF (z; λ, W, V )
= L(λF(x; W, V ), y) for z = (x, y).
For the ﬁrst SGD variant, recall from Lemma 6.8 that
Ez∼Z,W ρ,V ρLF (z; λT , W (0) + WT + W ρ, V (0) + VT + V ρ) + R(
λT VT ) ≤(1 + γ)OPT + ε0.
Since LF (z; λT , W (0) +WT +W ρ, V (0) +VT +V ρ) ∈[0, eO(1)] w.h.p., by randomly sampling eO(1/ε2
many W ρ,j, V ρ,j, we know w.h.p. there exists one j∗with
Ez∈ZLF (z; λT , W (0) + W ρ,j∗+ WT , V (0) + V ρ,j∗+ VT ) ≤(1 + γ)OPT + 2ε0
Now, recall that
λT WT ∥2,4 ≤τ ′
λT VT ∥F ≤τ ′
due to our regularizer (see (C.32)). By simple spectral norm bound, we also know w.h.p. for every
(x, y) ∼D and j,32
LF (z; λT , W (0) + W ρ,j + WT , V (0) + V ρ,j + VT )
Therefore, we can plug in the Rademacher complexity from Lemma 6.10 and b = eO(√km2) into
standard generalization statement Corollary A.11. Using our choices of τ ′
v from Table 1 as
well as m1 = m2, this bound implies as long as N ≥eO(M(m2)3/2), w.h.p. for every pair W ρ,j, V ρ,j,
Ez∈DLF (z; λT , W (0) + W ρ,j + WT , V (0) + V ρ,j + VT )
≤Ez∈ZLF (z; λT , W (0) + W ρ,j + WT , V (0) + V ρ,j + VT ) + ε0
Together, we have
Ez∈DLF (z; λT , W (0) + W ρ,j∗+ WT , V (0) + V ρ,j∗+ VT ) ≤(1 + γ)OPT + 3ε0
Theorem 2: Second SGD Variant
Theorem 2. Consider Algorithm 2. For every constant γ ∈(0, 1/4], every ε0 ∈(0, 1/100], every
2Cs(Φ,p2Cs(φ,1))Cs(φ,1)2 , there exists
Cε(Φ, √p2Cε(φ, 1)), 1
such that for every m2 = m1 = m ≥M, and properly set λw, λv, σw, σv in Table 1, as long as
Cε(Φ, √p2Cε(φ, 1)) · Cε(φ, 1) · √p2p1k2
there is a choice η = 1/poly(m1, m2) and T = poly(m1, m2) such that with probability ≥99/100,
E(x,y)∼DL(λT F(x; W (out)
), y) ≤(1 + γ)OPT + ε0.
32Indeed, with our parameter choices in Table 1, the spectral norms √λT ∥W (0) + W ρ,j + WT ∥2 ≤O(1) +
∥√λT WT ∥F ≤O(1 + m1/4
w) ≤O(1) and √λT ∥V (0) + V ρ,j + VT ∥2 ≤O(1) + ∥√λT VT ∥F ≤O(1 + τ ′
Therefore, the network output λT F(x; W (0) + W ρ,j + WT , V (0) + V ρ,j + VT ) must be bounded by eO(√km2) in Euclidean norm. By the assumption that L(0, y) ∈ and L(·, y) is 1-Lipschitz continuous in the ﬁrst variable, we
have that LF is bounded as stated.
Proof of Theorem 2. For notation simplicity let LF (z; λ, W, V )
= L(λF(x; W, V ), y) for z = (x, y).
Recall from Remark C.10 that Lemma 6.8 still works in this setting, so we have
EW ρ,V ρ,Σ,z∼Z
z; λT , W (0) + W ρ + ΣWT , V (0) + V ρ + VT Σ
λT VT ) ≤(1 + γ)OPT + ε0 .
For the same reason as the proof of Theorem 3, we know w.h.p. among eO(1/ε2
0) choices of j,
EΣ,z∈ZLF (z; λT , W (0) + W ρ,j + ΣWT , V (0) + V ρ,j + VT Σ)
≤(1 + γ)OPT + 2ε0
Without loss of generality, in the remainder of the proof we assume OPT ≤O(ε0). This can be
done because is ε0 is too small we can increase it to ε0 = Θ(OPT). By our regularizer parameters
λw, λv in Table 1, we know
λT VT ) ≤(1 + γ)OPT + ε0 ≤O(ε0)
λT WT ∥2,4 ≤O(τ ′
λT VT ∥2,2 ≤O(τ ′
By Lemma 6.11 (but viewing V (0) + V ρ as V (0) and viewing W (0) + W ρ as W (0)), we know for
every (x, y), w.h.p. over W (0), V (0), W ρ, V ρ, Σ
fr(x; W (0) + W ρ + ΣWT , V (0) + V ρ + VT Σ)
= arDv,x,ρ
(V (0) + V ρ)Dw,x,ρ
 (W (0) + V ρ)x + b1
+ arDv,x,ρVT Dw,x,ρWT x ± ε
= fr(x; W (0) + W ρ, V (0) + V ρ) + g(b,b)
(x; WT , VT ) ± ε/k
where Dv,x,ρ and Dw,x,ρ are the diagonal sign indicator matrices at weights W (0) + W ρ, V (0) + V ρ,
and we denote by g(b,b)
(x; W ′, V ′)
= arDv,x,ρV ′Dw,x,ρW ′x the output of the pseudo network. This
immediately implies for every (x, y) and every j, w.h.p. over W (0), V (0), W ρ,j, V ρ,j, Σ
fr(x; W (0) + W ρ,j + ΣWT , V (0) + V ρ,j + VT Σ)
= fr(x; W (0) + W ρ,j, V (0) + V ρ,j) + g(b,b)
(x; WT , VT ) ± ε/k
Using the 1-Lipschitz continuity of L together with (C.45) and (C.46), it is not hard to derive that
for every j, with high probability over (x, y) ∼D,W (0), V (0), W ρ,j, V ρ,j33
EΣLF (z; λT , W (0) + W ρ,j + ΣWT , V (0) + V ρ,j + VT Σ)
Therefore, we can plug in the Rademacher complexity from Lemma 6.12 with b = eO(C0) into
standard generalization statement Corollary A.11.34 Using our choices of τ ′
v from Table 1
as well as m1 = m2, the Rademacher complexity is dominated by
∥WT ∥2,4∥VT ∥2,2
In other words, as long as N ≥(kC0/ε0)2, the Rademacher complexity of a single output is at most
k , so the generalization error is at most ε0 by Corollary A.11. Or, in symbols, for every j, w.h.p.
|fr(x; W (0) + W ρ,j, V (0) + V ρ,j)|
eO(1) with high probability,
and λT |g(b,b)
(x; WT , VT )|
eO(√m2∥√λT VT ∥2∥√λT WT ∥2) ≤eO(√m2∥√λT VT ∥F ∥√λT WT ∥F ) ≤eO(ε3/4
v) ≤eO(C0) by spectral
norm bounds.
34Strictly speaking, Corollary A.11 requires an absolute value bound b as opposed to a high probability bound. It
is a simple exercise to deal with this issue, see for instance Remark B.6 in our two-layer proof.
over W (0), V (0), W ρ,j, V ρ,j,
EΣ,z∈DLF (z; λT , W (0) + W ρ,j + ΣWT , V (0) + V ρ,j + VT Σ)
≤EΣ,z∈ZLF (z; λT , W (0) + W ρ,j + ΣWT , V (0) + V ρ,j + VT Σ) + ε0
Putting this into (C.44), we have
EΣ,z∈DLF (z; λT , W (0) + W ρ,j + ΣWT , V (0) + V ρ,j + VT Σ)
≤(1 + γ)OPT + 3ε0
Next, let us take expectation over z ∼D for (C.46) (strictly speaking, this needs one to carefully
deal with the tail bound and apply the 1-Lipschitz continuity of L). We have for every j, w.h.p
over W (0), V (0), W ρ,j, V ρ,j, Σ
x; W (0) + W ρ,j + ΣWT , V (0) + V ρ,j + VT Σ
= E(x,y)∼D
x; W (0) + W ρ,j, V (0) + V ρ,j
+ λT G(b,b) (x; WT , VT ) , y
Since the right hand side (except the ±2ε term) does not depend on the randomness of Σ, we know
that the left hand side with respect to a random sample bΣ must stay close to its expectation with
respect to Σ. Or, in symbols, for every j, w.h.p over W (0), V (0), W ρ,j, V ρ,j
x; W (0) + W ρ,j + bΣWT , V (0) + V ρ,j + VT bΣ
= EΣ,(x,y)∼D
x; W (0) + W ρ,j + ΣWT , V (0) + V ρ,j + VT Σ
For similar reason, replacing D with Z, we have
x; W (0) + W ρ,j + bΣWT , V (0) + V ρ,j + VT bΣ
= EΣ,(x,y)∼Z
x; W (0) + W ρ,j + ΣWT , V (0) + V ρ,j + VT Σ
These imply two things.
• Putting (C.50) into (C.44), we have
Ez∈ZLF (z; λT , W (0) + W ρ,j∗+ bΣWT , V (0) + V ρ,j∗+ VT bΣ)
Ez∈ZLF (z; λT , W (0) + W ρ,j + bΣWT , V (0) + V ρ,j + VT bΣ)
≤(1 + γ)OPT + 3ε0 .
• Putting (C.50) and (C.49) into (C.47), we have
Ez∈DLF (z; λT , W (0) + W ρ,j + bΣWT , V (0) + V ρ,j + VT bΣ)
≤Ez∈ZLF (z; λT , W (0) + W ρ,j + bΣWT , V (0) + V ρ,j + VT bΣ) + 2ε0
Combining (C.51) and (C.52), we immediately have
x; W (0) + W ρ,j∗+ bΣWT , V (0) + V ρ,j∗+ VT bΣ
≤(1 + γ)OPT + 5ε0
as desired. Scaling down ε0 by constant ﬁnishes the proof.