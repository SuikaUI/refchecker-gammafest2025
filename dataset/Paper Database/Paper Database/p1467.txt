The Annals of Statistics
2004, Vol. 32, No. 2, 407–499
© Institute of Mathematical Statistics, 2004
LEAST ANGLE REGRESSION
BY BRADLEY EFRON,1 TREVOR HASTIE,2 IAIN JOHNSTONE3
AND ROBERT TIBSHIRANI4
Stanford University
The purpose of model selection algorithms such as All Subsets, Forward
Selection and Backward Elimination is to choose a linear model on the
basis of the same set of data to which the model will be applied. Typically
we have available a large collection of possible covariates from which we
hope to select a parsimonious set for the efﬁcient prediction of a response
variable. Least Angle Regression (LARS), a new model selection algorithm,
is a useful and less greedy version of traditional forward selection methods.
Three main properties are derived: (1) A simple modiﬁcation of the LARS
algorithm implements the Lasso, an attractive version of ordinary least
squares that constrains the sum of the absolute regression coefﬁcients;
the LARS modiﬁcation calculates all possible Lasso estimates for a given
problem, using an order of magnitude less computer time than previous
methods. (2) A different LARS modiﬁcation efﬁciently implements Forward
Stagewise linear regression, another promising new model selection method;
this connection explains the similar numerical results previously observed
for the Lasso and Stagewise, and helps us understand the properties of
both methods, which are seen as constrained versions of the simpler LARS
algorithm. (3) A simple approximation for the degrees of freedom of a LARS
estimate is available, from which we derive a Cp estimate of prediction error;
this allows a principled choice among the range of possible LARS estimates.
LARS and its variants are computationally efﬁcient: the paper describes
a publicly available algorithm that requires only the same order of magnitude
of computational effort as ordinary least squares applied to the full set of
covariates.
1. Introduction.
Automatic model-building algorithms are familiar, and
sometimes notorious, in the linear model literature: Forward Selection, Backward
Elimination, All Subsets regression and various combinations are used to automatically produce “good” linear models for predicting a response y on the basis
of some measured covariates x1,x2,...,xm. Goodness is often deﬁned in terms
of prediction accuracy, but parsimony is another important criterion: simpler models are preferred for the sake of scientiﬁc insight into the x −y relationship. Two
promising recent model-building algorithms, the Lasso and Forward Stagewise lin-
Received March 2002; revised January 2003.
1Supported in part by NSF Grant DMS-00-72360 and NIH Grant 8R01-EB002784.
2Supported in part by NSF Grant DMS-02-04162 and NIH Grant R01-EB0011988-08.
3Supported in part by NSF Grant DMS-00-72661 and NIH Grant R01-EB001988-08.
4Supported in part by NSF Grant DMS-99-71405 and NIH Grant 2R01-CA72028.
AMS 2000 subject classiﬁcation. 62J07.
Key words and phrases. Lasso, boosting, linear regression, coefﬁcient paths, variable selection.
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
ear regression, will be discussed here, and motivated in terms of a computationally
simpler method called Least Angle Regression.
Least Angle Regression (LARS) relates to the classic model-selection method
known as Forward Selection, or “forward stepwise regression,” described in
Weisberg [ , Section 8.5]: given a collection of possible predictors, we
select the one having largest absolute correlation with the response y, say xj1,
and perform simple linear regression of y on xj1. This leaves a residual vector
orthogonal to xj1, now considered to be the response. We project the other
predictors orthogonally to xj1 and repeat the selection process. After k steps this
results in a set of predictors xj1,xj2,...,xjk that are then used in the usual way
to construct a k-parameter linear model. Forward Selection is an aggressive ﬁtting
technique that can be overly greedy, perhaps eliminating at the second step useful
predictors that happen to be correlated with xj1.
Forward Stagewise, as described below, is a much more cautious version of
Forward Selection, which may take thousands of tiny steps as it moves toward
a ﬁnal model. It turns out, and this was the original motivation for the LARS
algorithm, that a simple formula allows Forward Stagewise to be implemented
using fairly large steps, though not as large as a classic Forward Selection, greatly
reducing the computational burden. The geometry of the algorithm, described in
Section 2, suggests the name “Least Angle Regression.” It then happens that this
same geometry applies to another, seemingly quite different, selection method
called the Lasso [Tibshirani ]. The LARS–Lasso–Stagewise connection is
conceptually as well as computationally useful. The Lasso is described next, in
terms of the main example used in this paper.
Table 1 shows a small part of the data for our main example.
Ten baseline variables, age, sex, body mass index, average blood pressure
and six blood serum measurements, were obtained for each of n = 442 diabetes
Diabetes study: 442 diabetes patients were measured on 10 baseline variables; a prediction model
was desired for the response variable, a measure of disease progression one year after baseline
Serum measurements
LEAST ANGLE REGRESSION
patients, as well as the response of interest, a quantitative measure of disease
progression one year after baseline. The statisticians were asked to construct
a model that predicted response y from covariates x1,x2,...,x10. Two hopes
were evident here, that the model would produce accurate baseline predictions
of response for future patients and that the form of the model would suggest which
covariates were important factors in disease progression.
The Lasso is a constrained version of ordinary least squares (OLS). Let x1,x2,
...,xm be n-vectors representing the covariates, m = 10 and n = 442 in the
diabetes study, and let y be the vector of responses for the n cases. By location
and scale transformations we can always assume that the covariates have been
standardized to have mean 0 and unit length, and that the response has mean 0,
for j = 1,2,...,m.
This is assumed to be the case in the theory which follows, except that numerical
results are expressed in the original units of the diabetes example.
A candidate vector of regression coefﬁcients β = (β1, β2,..., βm)′ gives
prediction vector µ,
xj βj = Xβ
[Xn×m = (x1,x2,...,xm)]
with total squared error
S(β) = ∥y −µ∥2 =
(yi −µi)2.
Let T (β) be the absolute norm of β,
The Lasso chooses β by minimizing S(β) subject to a bound t on T (β),
subject to
T (β) ≤t.
Quadratic programming techniques can be used to solve (1.5) though we will
present an easier method here, closely related to the “homotopy method” of
Osborne, Presnell and Turlach .
The left panel of Figure 1 shows all Lasso solutions β(t) for the diabetes study,
as t increases from 0, where β = 0, to t = 3460.00, where β equals the OLS
regression vector, the constraint in (1.5) no longer binding. We see that the Lasso
tends to shrink the OLS coefﬁcients toward 0, more so for small values of t.
Shrinkage often improves prediction accuracy, trading off decreased variance for
increased bias as discussed in Hastie, Tibshirani and Friedman .
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
Estimates of regression coefﬁcients βj , j = 1,2,...,10, for the diabetes study. (Left
panel) Lasso estimates, as a function of t = 
j |βj |. The covariates enter the regression equation
sequentially as t increases, in order j = 3,9,4,7,... ,1. (Right panel) The same plot for Forward
Stagewise Linear Regression. The two plots are nearly identical, but differ slightly for large t as
shown in the track of covariate 8.
The Lasso also has a parsimony property: for any given constraint value t, only
a subset of the covariates have nonzero values of βj. At t = 1000, for example,
only variables 3, 9, 4 and 7 enter the Lasso regression model (1.2). If this model
provides adequate predictions, a crucial question considered in Section 4, the
statisticians could report these four variables as the important ones.
Forward Stagewise Linear Regression, henceforth called Stagewise, is an
iterative technique that begins with µ = 0 and builds up the regression function
in successive small steps. If µ is the current Stagewise estimate, let c(µ) be the
vector of current correlations
c = c(µ) = X′(y −µ),
so that cj is proportional to the correlation between covariate xj and the current
residual vector. The next step of the Stagewise algorithm is taken in the direction
of the greatest current correlation,
j = argmax|cj|
µ →µ + ε · sign(c ˆj) · x ˆj,
with ε some small constant. “Small” is important here: the “big” choice ε = |c ˆj|
leads to the classic Forward Selection technique, which can be overly greedy,
impulsively eliminating covariates which are correlated with x ˆj. The Stagewise
procedure is related to boosting and also to Friedman’s MART algorithm
LEAST ANGLE REGRESSION
[Friedman ]; see Section 8, as well as Hastie, Tibshirani and Friedman
[ , Chapter 10 and Algorithm 10.4].
The right panel of Figure 1 shows the coefﬁcient plot for Stagewise applied to
the diabetes data. The estimates were built up in 6000 Stagewise steps [making
ε in (1.7) small enough to conceal the “Etch-a-Sketch” staircase seen in Figure 2,
Section 2]. The striking fact is the similarity between the Lasso and Stagewise
estimates. Although their deﬁnitions look completely different, the results are
nearly, but not exactly, identical.
The main point of this paper is that both Lasso and Stagewise are variants of
a basic procedure called Least Angle Regression, abbreviated LARS (the “S”
suggesting “Lasso” and “Stagewise”). Section 2 describes the LARS algorithm
while Section 3 discusses modiﬁcations that turn LARS into Lasso or Stagewise,
reducing the computational burden by at least an order of magnitude for either one.
Sections 5 and 6 verify the connections stated in Section 3.
Least Angle Regression is interesting in its own right, its simple structure
lending itself to inferential analysis. Section 4 analyzes the “degrees of freedom”
of a LARS regression estimate. This leads to a Cp type statistic that suggests which
estimate we should prefer among a collection of possibilities like those in Figure 1.
A particularly simple Cp approximation, requiring no additional computation
beyond that for the β vectors, is available for LARS.
Section 7 brieﬂy discusses computational questions. An efﬁcient S program for
all three methods, LARS, Lasso and Stagewise, is available. Section 8 elaborates
on the connections with boosting.
2. The LARS algorithm.
Least Angle Regression is a stylized version of the
Stagewise procedure that uses a simple mathematical formula to accelerate
the computations. Only m steps are required for the full set of solutions, where
m is the number of covariates: m = 10 in the diabetes example compared to the
6000 steps used in the right panel of Figure 1. This section describes the LARS
algorithm. Modiﬁcations of LARS that produce Lasso and Stagewise solutions are
discussed in Section 3, and veriﬁed in Sections 5 and 6. Section 4 uses the simple
structure of LARS to help analyze its estimation properties.
The LARS procedure works roughly as follows. As with classic Forward
Selection, we start with all coefﬁcients equal to zero, and ﬁnd the predictor
most correlated with the response, say xj1. We take the largest step possible in
the direction of this predictor until some other predictor, say xj2, has as much
correlation with the current residual. At this point LARS parts company with
Forward Selection. Instead of continuing along xj1, LARS proceeds in a direction
equiangular between the two predictors until a third variable xj3 earns its way
into the “most correlated” set. LARS then proceeds equiangularly between xj1,xj2
and xj3, that is, along the “least angle direction,” until a fourth variable enters, and
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
The remainder of this section describes the algebra necessary to execute the
equiangular strategy. As usual the algebraic details look more complicated than
the simple underlying geometry, but they lead to the highly efﬁcient computational
algorithm described in Section 7.
LARS builds up estimates µ = Xβ, (1.2), in successive steps, each step adding
one covariate to the model, so that after k steps just k of the βj’s are nonzero.
Figure 2 illustrates the algorithm in the situation with m = 2 covariates, X =
(x1,x2). In this case the current correlations (1.6) depend only on the projection ¯y2
of y into the linear space L(X) spanned by x1 and x2,
c(µ) = X′(y −µ) = X′(¯y2 −µ).
The algorithm begins at µ0 = 0 [remembering that the response has had its
mean subtracted off, as in (1.1)]. Figure 2 has ¯y2 −µ0 making a smaller angle
with x1 than x2, that is, c1(µ0) > c2(µ0). LARS then augments µ0 in the direction
µ1 = µ0 + γ1x1.
Stagewise would choose γ1 equal to some small value ε, and then repeat the
process many times. Classic Forward Selection would take γ1 large enough to
make µ1 equal ¯y1, the projection of y into L(x1). LARS uses an intermediate
value of γ1, the value that makes ¯y2 −µ, equally correlated with x1 and x2; that is,
¯y2 −µ1 bisects the angle between x1 and x2, so c1(µ1) = c2(µ1).
The LARS algorithm in the case of m = 2 covariates; ¯y2 is the projection of y into
L(x1,x2). Beginning at µ0 = 0, the residual vector ¯y2 −µ0 has greater correlation with x1 than x2;
the next LARS estimate is µ1 = µ0 + γ1x1, where γ1 is chosen such that ¯y2 −µ1 bisects the angle
between x1 and x2; then µ2 = µ1 + γ2u2, where u2 is the unit bisector; µ2 = ¯y2 in the case m = 2,
but not for the case m > 2; see Figure 4. The staircase indicates a typical Stagewise path. Here LARS
gives the Stagewise track as ε →0, but a modiﬁcation is necessary to guarantee agreement in higher
dimensions; see Section 3.2.
LEAST ANGLE REGRESSION
Let u2 be the unit vector lying along the bisector. The next LARS estimate is
µ2 = µ1 + γ2u2,
with γ2 chosen to make µ2 = ¯y2 in the case m = 2. With m > 2 covariates,
γ2 would be smaller, leading to another change of direction, as illustrated in
Figure 4. The “staircase” in Figure 2 indicates a typical Stagewise path. LARS
is motivated by the fact that it is easy to calculate the step sizes γ1, γ2,...
theoretically, short-circuiting the small Stagewise steps.
Subsequent LARS steps, beyond two covariates, are taken along equiangular
vectors, generalizing the bisector u2 in Figure 2. We assume that the covariate
vectors x1,x2,...,xm are linearly independent. For A a subset of the indices
{1,2,...,m}, deﬁne the matrix
XA = (··· sjxj ···)j∈A,
where the signs sj equal ±1. Let
A 1A)−1/2,
1A being a vector of 1’s of length equaling |A|, the size of A. The
equiangular vector
where wA = AAG−1
is the unit vector making equal angles, less than 90◦, with the columns of XA,
AuA = AA1A
∥uA∥2 = 1.
We can now fully describe the LARS algorithm. As with the Stagewise
procedure we begin at µ0 = 0 and build up µ by steps, larger steps in the LARS
case. Suppose that µA is the current LARS estimate and that
c = X′(y −µA)
is the vector of current correlations (1.6). The active set A is the set of indices
corresponding to covariates with the greatest absolute current correlations,
A = {j :|cj| = C}.
sj = sign{cj}
we compute XA,AA and uA as in (2.4)–(2.6), and also the inner product vector
Then the next step of the LARS algorithm updates µA, say to
µA+ = µA + γ uA,
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
“min+” indicates that the minimum is taken over only positive components within
each choice of j in (2.13).
Formulas (2.12) and (2.13) have the following interpretation: deﬁne
µ(γ ) = µA + γ uA,
for γ > 0, so that the current correlation
cj(γ ) = x′
 = cj −γ aj.
For j ∈A, (2.7)–(2.9) yield
|cj(γ )| = C −γ AA,
showing that all of the maximal absolute current correlations decline equally.
For j ∈Ac, equating (2.15) with (2.16) shows that cj(γ ) equals the maximal
value at γ = (C −cj)/(AA −aj). Likewise −cj(γ ), the current correlation for the
reversed covariate −xj, achieves maximality at (C +cj)/(AA + aj). Therefore γ
in (2.13) is the smallest positive value of γ such that some new index j joins
the active set; j is the minimizing index in (2.13), and the new active set A+ is
A ∪{j}; the new maximum absolute correlation is C+ = C −γ AA.
Figure 3 concerns the LARS analysis of the diabetes data. The complete
algorithm required only m = 10 steps of procedure (2.8)–(2.13), with the variables
LARS analysis of the diabetes study: (left) estimates of regression coefﬁcients βj ,
j = 1,2,... ,10; plotted versus |βj |; plot is slightly different than either Lasso or Stagewise,
Figure 1; (right) absolute current correlations as function of LARS step; variables enter active
set (2.9) in order 3,9,4,7,... ,1; heavy curve shows maximum current correlation Ck declining
LEAST ANGLE REGRESSION
joining the active set A in the same order as for the Lasso: 3,9,4,7,...,1. Tracks
of the regression coefﬁcients βj are nearly but not exactly the same as either the
Lasso or Stagewise tracks of Figure 1.
The right panel shows the absolute current correlations
|ckj| = |x′
j(y −µk−1)|
for variables j = 1,2,...,10, as a function of the LARS step k. The maximum
correlation
Ck = max{|ckj|} = Ck−1 −γk−1Ak−1
declines with k, as it must. At each step a new variable j joins the active set,
henceforth having |ckj| = Ck. The sign sj of each xj in (2.4) stays constant as the
active set increases.
Section 4 makes use of the relationship between Least Angle Regression and
Ordinary Least Squares illustrated in Figure 4. Suppose LARS has just completed
step k −1, giving µk−1, and is embarking upon step k. The active set Ak, (2.9),
will have k members, giving Xk,Gk,Ak and uk as in (2.4)–(2.6) (here replacing
subscript A with “k”). Let ¯yk indicate the projection of y into L(Xk), which, since
µk−1 ∈L(Xk−1), is
¯yk = µk−1 + XkG−1
k(y −µk−1) = µk−1 +
the last equality following from (2.6) and the fact that the signed current
correlations in Ak all equal Ck,
k(y −µk−1) = Ck1A.
Since uk is a unit vector, (2.19) says that ¯yk −µk−1 has length
Comparison with (2.12) shows that the LARS estimate µk lies on the line
At each stage the LARS estimate µk approaches, but does not reach, the corresponding
OLS estimate ¯yk.
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
from µk−1 to ¯yk,
µk −µk−1 =
(¯yk −µk−1).
It is easy to see that γk, (2.12), is always less than ¯γk, so that µk lies closer than ¯yk
to µk−1. Figure 4 shows the successive LARS estimates µk always approaching
but never reaching the OLS estimates ¯yk.
The exception is at the last stage: since Am contains all covariates, (2.13) is not
deﬁned. By convention the algorithm takes γm = ¯γm = Cm/Am, making µm = ¯ym
and βm equal the OLS estimate for the full set of m covariates.
The LARS algorithm is computationally thrifty. Organizing the calculations
correctly, the computational cost for the entire m steps is of the same order as
that required for the usual Least Squares solution for the full set of m covariates.
Section 7 describes an efﬁcient LARS program available from the authors.
With the modiﬁcations described in the next section, this program also provides
economical Lasso and Stagewise solutions.
3. Modiﬁed versions of Least Angle Regression.
Figures 1 and 3 show
Lasso, Stagewise and LARS yielding remarkably similar estimates for the diabetes
data. The similarity is no coincidence. This section describes simple modiﬁcations
of the LARS algorithm that produce Lasso or Stagewise estimates. Besides
improved computational efﬁciency, these relationships elucidate the methods’
rationale: all three algorithms can be viewed as moderately greedy forward
stepwise procedures whose forward progress is determined by compromise among
the currently most correlated covariates. LARS moves along the most obvious
compromise direction, the equiangular vector (2.6), while Lasso and Stagewise
put some restrictions on the equiangular strategy.
3.1. The LARS–Lasso relationship.
The full set of Lasso solutions, as shown
for the diabetes study in Figure 1, can be generated by a minor modiﬁcation of
the LARS algorithm (2.8)–(2.13). Our main result is described here and veriﬁed
in Section 5. It closely parallels the homotopy method in the papers by Osborne,
Presnell and Turlach , though the LARS approach is somewhat more
Let β be a Lasso solution (1.5), with µ = Xβ. Then it is easy to show that
the sign of any nonzero coordinate βj must agree with the sign sj of the current
correlation cj = x′
sign(βj) = sign(cj) = sj;
see Lemma 8 of Section 5. The LARS algorithm does not enforce restriction (3.1),
but it can easily be modiﬁed to do so.
LEAST ANGLE REGRESSION
Suppose we have just completed a LARS step, giving a new active set A as
in (2.9), and that the corresponding LARS estimate µA corresponds to a Lasso
solution µ = Xβ. Let
wA = AAG−1
a vector of length the size of A, and (somewhat abusing subscript notation)
deﬁne d to be the m-vector equaling sjwAj for j ∈A and zero elsewhere. Moving
in the positive γ direction along the LARS line (2.14), we see that
µ(γ ) = Xβ(γ ),
where βj(γ ) = βj + γ dj
for j ∈A. Therefore βj(γ ) will change sign at
γj = −βj/dj,
the ﬁrst such change occurring at
say for covariate x ˜j; γ equals inﬁnity by deﬁnition if there is no γj > 0.
If γ is less than γ , (2.13), then βj(γ ) cannot be a Lasso solution for γ > γ since
the sign restriction (3.1) must be violated: β ˜j(γ ) has changed sign while c ˜j(γ ) has
not. [The continuous function c ˜j(γ ) cannot change sign within a single LARS step
since |c ˜j(γ )| = C −γ AA > 0, (2.16).]
LASSO MODIFICATION.
If γ < γ , stop the ongoing LARS step at γ = γ and
remove j from the calculation of the next equiangular direction. That is,
µA+ = µA + γ uA
A+ = A −{j}
rather than (2.12).
THEOREM 1.
Under the Lasso modiﬁcation, and assuming the “one at a time”
condition discussed below, the LARS algorithm yields all Lasso solutions.
The active sets A grow monotonically larger as the original LARS algorithm
progresses, but the Lasso modiﬁcation allows A to decrease. “One at a time”
means that the increases and decreases never involve more than a single index j.
This is the usual case for quantitative data and can always be realized by adding a
little jitter to the y values. Section 5 discusses tied situations.
The Lasso diagram in Figure 1 was actually calculated using the modiﬁed LARS
algorithm. Modiﬁcation (3.6) came into play only once, at the arrowed point in
the left panel. There A contained all 10 indices while A+ = A −{7}. Variable 7
was restored to the active set one LARS step later, the next and last step then
taking β all the way to the full OLS solution. The brief absence of variable 7
had an effect on the tracks of the others, noticeably β8. The price of using Lasso
instead of unmodiﬁed LARS comes in the form of added steps, 12 instead of 10
in this example. For the more complicated “quadratic model” of Section 4, the
comparison was 103 Lasso steps versus 64 for LARS.
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
3.2. The LARS–Stagewise relationship.
The staircase in Figure 2 indicates
how the Stagewise algorithm might proceed forward from µ1, a point of equal
current correlations c1 = c2, (2.8). The ﬁrst small step has (randomly) selected
index j = 1, taking us to µ1 + εx1. Now variable 2 is more correlated,
2(y −µ1 −εx1) > x′
1(y −µ1 −εx1),
forcing j = 2 to be the next Stagewise choice and so on.
We will consider an idealized Stagewise procedure in which the step size ε goes
to zero. This collapses the staircase along the direction of the bisector u2 in
Figure 2, making the Stagewise and LARS estimates agree. They always agree
for m = 2 covariates, but another modiﬁcation is necessary for LARS to produce
Stagewise estimates in general. Section 6 veriﬁes the main result described next.
Suppose that the Stagewise procedure has taken N steps of inﬁnitesimal size
ε from some previous estimate µ, with
Nj ≡#{steps with selected index j},
j = 1,2,...,m.
It is easy to show, as in Lemma 11 of Section 6, that Nj = 0 for j not in the active
set A deﬁned by the current correlations x′
j(y −µ), (2.9). Letting
P ≡(N1,N2,...,Nm)/N,
with PA indicating the coordinates of P for j ∈A, the new estimate is
µ = µ + NεXAPA
(Notice that the Stagewise steps are taken along the directions sjxj.)
The LARS algorithm (2.14) progresses along
µA + γ XAwA,
where wA = AAG−1
[(2.6)–(3.2)].
Comparing (3.10) with (3.11) shows that LARS cannot agree with Stagewise if
wA has negative components, since PA is nonnegative. To put it another way, the
direction of Stagewise progress XAPA must lie in the convex cone generated by
the columns of XA,
sjxjPj, Pj ≥0
If uA ∈CA then there is no contradiction between (3.12) and (3.13). If not it
seems natural to replace uA with its projection into CA, that is, the nearest point
in the convex cone.
STAGEWISE MODIFICATION.
Proceed as in (2.8)–(2.13), except with uA
replaced by u ˆB, the unit vector lying along the projection of uA into CA. (See
Figure 9 in Section 6.)
LEAST ANGLE REGRESSION
THEOREM 2.
Under the Stagewise modiﬁcation, the LARS algorithm yields
all Stagewise solutions.
The vector u ˆB in the Stagewise modiﬁcation is the equiangular vector (2.6) for
the subset 
B ⊆A corresponding to the face of CA into which the projection falls.
Stagewise is a LARS type algorithm that allows the active set to decrease by one
or more indices. This happened at the arrowed point in the right panel of Figure 1:
there the set A = {3,9,4,7,2,10,5,8} was decreased to 
B = A−{3,7}. It took a
total of 13 modiﬁed LARS steps to reach the full OLS solution ¯βm = (X′X)−1X′y.
The three methods, LARS, Lasso and Stagewise, always reach OLS eventually,
but LARS does so in only m steps while Lasso and, especially, Stagewise can take
longer. For the m = 64 quadratic model of Section 4, Stagewise took 255 steps.
According to Theorem 2 the difference between successive Stagewise–modiﬁed
LARS estimates is
µA+ −µA = γ u ˆB = γ X ˆBw ˆB,
as in (3.13). Since u ˆB exists in the convex cone CA, w ˆB must have nonnegative
components. This says that the difference of successive coefﬁcient estimates for
coordinate j ∈
B satisﬁes
sign(β+j −βj) = sj,
where sj = sign{x′
j(y −µ)}.
We can now make a useful comparison of the three methods:
1. Stagewise—successive differences of βj agree in sign with the current
correlation cj = x′
2. Lasso—βj agrees in sign with cj;
3. LARS—no sign restrictions (but see Lemma 4 of Section 5).
From this point of view, Lasso is intermediate between the LARS and Stagewise
The successive difference property (3.14) makes the Stagewise βj estimates
move monotonically away from 0. Reversals are possible only if cj changes sign
while βj is “resting” between two periods of change. This happened to variable 7
in Figure 1 between the 8th and 10th Stagewise-modiﬁed LARS steps.
3.3. Simulation study.
A small simulation study was carried out comparing
the LARS, Lasso and Stagewise algorithms. The X matrix for the simulation was
based on the diabetes example of Table 1, but now using a “Quadratic Model”
having m = 64 predictors, including interactions and squares of the 10 original
covariates:
Quadratic Model
10 main effects,45 interactions,9 squares,
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
the last being the squares of each xj except the dichotomous variable x2. The true
mean vector µ for the simulation was µ = Xβ, where β was obtained by running
LARS for 10 steps on the original (X,y) diabetes data (agreeing in this case with
the 10-step Lasso or Stagewise analysis). Subtracting µ from a centered version of
the original y vector of Table 1 gave a vector ε = y −µ of n = 442 residuals. The
“true R2” for this model, ∥µ∥2/(∥µ∥2 + ∥ε∥2), equaled 0.416.
100 simulated response vectors y∗were generated from the model
y∗= µ + ε∗,
with ε∗= (ε∗
n) a random sample, with replacement, from the components of ε. The LARS algorithm with K = 40 steps was run for each simulated
data set (X,y∗), yielding a sequence of estimates µ(k)∗, k = 1,2,...,40, and likewise using the Lasso and Stagewise algorithms.
Figure 5 compares the LARS, Lasso and Stagewise estimates. For a given
estimate µ deﬁne the proportion explained pe(µ) to be
pe(µ) = 1 −∥µ −µ∥2/∥µ∥2,
so pe(0) = 0 and pe(µ) = 1. The solid curve graphs the average of pe(µ(k)∗)
over the 100 simulations, versus step number k for LARS, k = 1,2,...,40.
The corresponding curves are graphed for Lasso and Stagewise, except that the
horizontal axis is now the average number of nonzero β∗
j terms composing µ(k)∗.
For example, µ(40)∗averaged 33.23 nonzero terms with Stagewise, compared to
35.83 for Lasso and 40 for LARS.
Figure 5’s most striking message is that the three algorithms performed almost
identically, and rather well. The average proportion explained rises quickly,
Simulation study comparing LARS, Lasso and Stagewise algorithms; 100 replications of
model (3.15)–(3.16). Solid curve shows average proportion explained, (3.17), for LARS estimates
as function of number of steps k = 1,2,...,40; Lasso and Stagewise give nearly identical results;
small dots indicate plus or minus one standard deviation over the 100 simulations. Classic Forward
Selection (heavy dashed curve) rises and falls more abruptly.
LEAST ANGLE REGRESSION
reaching a maximum of 0.963 at k = 10, and then declines slowly as k grows
to 40. The light dots display the small standard deviation of pe(µ(k)∗) over the 100
simulations, roughly ±0.02. Stopping at any point between k = 5 and 25 typically
gave a µ(k)∗with true predictive R2 about 0.40, compared to the ideal value 0.416
The dashed curve in Figure 5 tracks the average proportion explained by classic
Forward Selection. It rises very quickly, to a maximum of 0.950 after k = 3 steps,
and then falls back more abruptly than the LARS–Lasso–Stagewise curves. This
behavior agrees with the characterization of Forward Selection as a dangerously
greedy algorithm.
3.4. Other LARS modiﬁcations.
Here are a few more examples of LARS type
model-building algorithms.
POSITIVE LASSO.
Constraint (1.5) can be strengthened to
subject to
T (β) ≤t and all βj ≥0.
This would be appropriate if the statisticians or scientists believed that the
variables xj must enter the prediction equation in their deﬁned directions.
Situation (3.18) is a more difﬁcult quadratic programming problem than (1.5),
but it can be solved by a further modiﬁcation of the Lasso-modiﬁed LARS
algorithm: change |cj| to cj at both places in (2.9), set sj = 1 instead of (2.10)
and change (2.13) to
The positive Lasso usually does not converge to the full OLS solution ¯βm, even for
very large choices of t.
The changes above amount to considering the xj as generating half-lines rather
than full one-dimensional spaces. A positive Stagewise version can be developed
in the same way, and has the property that the βj tracks are always monotone.
LARS–OLS hybrid.
After k steps the LARS algorithm has identiﬁed a set Ak
of covariates, for example, A4 = {3,9,4,7} in the diabetes study. Instead of βk we
might prefer ¯βk, the OLS coefﬁcients based on the linear model with covariates
in Ak—using LARS to ﬁnd the model but not to estimate the coefﬁcients. Besides
looking more familiar, this will always increase the usual empirical R2 measure of
ﬁt (though not necessarily the true ﬁtting accuracy),
R2( ¯βk) −R2(βk) =
ρk(2 −ρk)[R2(βk) −R2(βk−1)],
where ρk = γk/ ¯γk as in (2.22).
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
The increases in R2 were small in the diabetes example, on the order of 0.01
for k ≥4 compared with R2 ˙= 0.50, which is expected from (3.20) since we
would usually continue LARS until R2(βk) −R2(βk−1) was small. For the same
reason ¯βk and βk are likely to lie near each other as they did in the diabetes
Main effects ﬁrst.
It is straightforward to restrict the order in which variables
are allowed to enter the LARS algorithm. For example, having obtained A4 =
{3,9,4,7} for the diabetes study, we might then wish to check for interactions. To
do this we begin LARS again, replacing y with y −µ4 and x with the n × 6 matrix
whose columns represent the interactions x3:9,x3:4,...,x4:7.
Backward Lasso.
The Lasso–modiﬁed LARS algorithm can be run backward,
starting from the full OLS solution ¯βm. Assuming that all the coordinates of ¯βm
are nonzero, their signs must agree with the signs sj that the current correlations
had during the ﬁnal LARS step. This allows us to calculate the last equiangular
direction uA, (2.4)–(2.6). Moving backward from µm = X ¯βm along the line
µ(γ ) = µm −γ uA, we eliminate from the active set the index of the ﬁrst βj
that becomes zero. Continuing backward, we keep track of all coefﬁcients βj and
current correlations cj, following essentially the same rules for changing A as in
Section 3.1. As in (2.3), (3.5) the calculation of γ and γ is easy.
The crucial property of the Lasso that makes backward navigation possible
is (3.1), which permits calculation of the correct equiangular direction uA at each
step. In this sense Lasso can be just as well thought of as a backward-moving
algorithm. This is not the case for LARS or Stagewise, both of which are inherently
forward-moving algorithms.
4. Degrees of freedom and Cp estimates.
Figures 1 and 3 show all possible
Lasso, Stagewise or LARS estimates of the vector β for the diabetes data. The
scientists want just a single β of course, so we need some rule for selecting among
the possibilities. This section concerns a Cp-type selection criterion, especially as
it applies to the choice of LARS estimate.
Let µ = g(y) represent a formula for estimating µ from the data vector y.
Here, as usual in regression situations, we are considering the covariate vectors
x1,x2,...,xm ﬁxed at their observed values. We assume that given the x’s, y is
generated according to an homoskedastic model
y ∼(µ,σ 2I),
meaning that the components yi are uncorrelated, with mean µi and variance σ 2.
Taking expectations in the identity
(µi −µi)2 = (yi −µi)2 −(yi −µi)2 + 2(µi −µi)(yi −µi),
LEAST ANGLE REGRESSION
and summing over i, yields
cov(µi,yi)
The last term of (4.3) leads to a convenient deﬁnition of the degrees of freedom
for an estimator µ = g(y),
cov(µi,yi)/σ 2,
and a Cp-type risk estimation formula,
Cp(µ) = ∥y −µ∥2
−n + 2dfµ,σ 2.
If σ 2 and dfµ,σ 2 are known, Cp(µ) is an unbiased estimator of the true risk
E{∥µ −µ∥2/σ 2}. For linear estimators µ = My, model (4.1) makes dfµ,σ 2 =
trace(M), equaling the usual deﬁnition of degrees of freedom for OLS, and
coinciding with the proposal of Mallows . Section 6 of Efron and Tibshirani
 and Section 7 of Efron discuss formulas (4.4) and (4.5) and their
role in Cp, Akaike information criterion (AIC) and Stein’s unbiased risk estimated
(SURE) estimation theory, a more recent reference being Ye .
Practical use of Cp formula (4.5) requires preliminary estimates of µ,σ 2 and
dfµ,σ 2. In the numerical results below, the usual OLS estimates ¯µ and ¯σ 2 from
the full OLS model were used to calculate bootstrap estimates of dfµ,σ 2; bootstrap
samples y∗and replications µ∗were then generated according to
y∗∼N( ¯µ, ¯σ 2)
µ∗= g(y∗).
Independently repeating (4.6) say B times gives straightforward estimates for the
covariances in (4.4),
where y∗(·) =
covi/¯σ 2.
Normality is not crucial in (4.6). Nearly the same results were obtained using
y∗= ¯µ∗+ e∗, where the components of e∗were resampled from e = y −¯µ.
The left panel of Figure 6 shows
dfk for the diabetes data LARS estimates µk,k = 1,2,...,m = 10. It portrays a startlingly simple situation that we
will call the “simple approximation,”
df (µk) ˙= k.
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
Degrees of freedom for LARS estimates µk: (left) diabetes study, Table 1, k = 1,
2,...,m = 10; (right) quadratic model (3.15) for the diabetes data, m = 64. Solid line is simple
approximation dfk = k. Dashed lines are approximate 95% conﬁdence intervals for the bootstrap
estimates. Each panel based on B = 500 bootstrap replications.
The right panel also applies to the diabetes data, but this time with the quadratic
model (3.15), having m = 64 predictors. We see that the simple approximation (4.9) is again accurate within the limits of the bootstrap computation (4.8),
where B = 500 replications were divided into 10 groups of 50 each in order to
calculate Student-t conﬁdence intervals.
If (4.9) can be believed, and we will offer some evidence in its behalf, we can
estimate the risk of a k-step LARS estimator µk by
Cp(µk) ˙= ∥y −µk∥2/¯σ 2 −n + 2k.
The formula, which is the same as the Cp estimate of risk for an OLS estimator
based on a subset of k preselected predictor vectors, has the great advantage of not
requiring any further calculations beyond those for the original LARS estimates.
The formula applies only to LARS, and not to Lasso or Stagewise.
Figure 7 displays Cp(µk) as a function of k for the two situations of Figure 6.
Minimum Cp was achieved at steps k = 7 and k = 16, respectively. Both of the
minimum Cp models looked sensible, their ﬁrst several selections of “important”
covariates agreeing with an earlier model based on a detailed inspection of the data
assisted by medical expertise.
The simple approximation becomes a theorem in two cases.
THEOREM 3.
If the covariate vectors x1,x2,...,xm are mutually orthogonal,
then the k-step LARS estimate ˆµk has df ( ˆµk) = k.
To state the second more general setting we introduce the following condition.
LEAST ANGLE REGRESSION
Cp estimates of risk (4.10) for the two situations of Figure 6: (left) m = 10 model has
smallest Cp at k = 7; (right) m = 64 model has smallest Cp at k = 16.
POSITIVE CONE CONDITION.
For all possible subsets XA of the full design
where the inequality is taken element-wise.
The positive cone condition holds if X is orthogonal. It is strictly more general
than orthogonality, but counterexamples (such as the diabetes data) show that not
all design matrices X satisfy it.
It is also easy to show that LARS, Lasso and Stagewise all coincide under the
positive cone condition, so the degrees-of-freedom formula applies to them too in
this case.
THEOREM 4.
Under the positive cone condition, df ( ˆµk) = k.
The proof, which appears later in this section, is an application of Stein’s
unbiased risk estimate (SURE) [Stein ]. Suppose that g :Rn →Rn is almost
differentiable (see Remark A.1 in the Appendix) and set ∇· g = n
i=1 ∂gi/∂xi.
If y ∼Nn(µ,σ 2I), then Stein’s formula states that
cov(gi,yi)/σ 2 = E[∇· g(y)].
The left-hand side is df (g) for the general estimator g(y). Focusing speciﬁcally
on LARS, it will turn out that ∇· ˆµk(y) = k in all situations with probability 1,
but that the continuity assumptions underlying (4.12) and SURE can fail in certain
nonorthogonal cases where the positive cone condition does not hold.
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
A range of simulations suggested that the simple approximation is quite
accurate even when the xj’s are highly correlated and that it requires concerted
effort at pathology to make df (µk) much different than k.
Stein’s formula assumes normality, y ∼N(µ,σ 2I). A cruder “delta method”
rationale for the simple approximation requires only homoskedasticity, (4.1). The
geometry of Figure 4 implies
µk = ¯yk −cotk · ∥¯yk+1 −¯yk∥,
where cotk is the cotangent of the angle between uk and uk+1,
kuk+1)2]1/2 .
Let vk be the unit vector orthogonal to L(Xb), the linear space spanned by the ﬁrst
k covariates selected by LARS, and pointing into L(Xk+1) along the direction of
¯yk+1 −¯yk. For y∗near y we can reexpress (4.13) as a locally linear transformation,
k = µk + Mk(y∗−y)
with Mk = Pk −cotk · ukv′
Pk being the usual projection matrix from Rn into L(Xk); (4.15) holds within a
neighborhood of y such that the LARS choices L(Xk) and vk remain the same.
The matrix Mk has trace(Mk) = k. Since the trace equals the degrees of freedom
for linear estimators, the simple approximation (4.9) is seen to be a delta method
approximation to the bootstrap estimates (4.6) and (4.7).
It is clear that (4.9) df (µk) ˙= k cannot hold for the Lasso, since the degrees of
freedom is m for the full model but the total number of steps taken can exceed m.
However, we have found empirically that an intuitively plausible result holds: the
degrees of freedom is well approximated by the number of nonzero predictors in
the model. Speciﬁcally, starting at step 0, let ℓ(k) be the index of the last model
in the Lasso sequence containing k predictors. Then df (µℓ(k)) ˙= k. We do not yet
have any mathematical support for this claim.
4.1. Orthogonal designs.
In the orthogonal case, we assume that xj = ej
for j = 1,...,m. The LARS algorithm then has a particularly simple form,
reducing to soft thresholding at the order statistics of the data.
To be speciﬁc, deﬁne the soft thresholding operation on a scalar y1 at threshold t
if y1 > t,
if |y1| ≤t,
if y1 < −t.
The order statistics of the absolute values of the data are denoted by
|y|(1) ≥|y|(2) ≥··· ≥|y|(n) ≥|y|(n+1) := 0.
We note that ym+1,...,yn do not enter into the estimation procedure, and so we
may as well assume that m = n.
LEAST ANGLE REGRESSION
For an orthogonal design with xj = ej,j = 1,...,n, the kth LARS
estimate (0 ≤k ≤n) is given by
ˆµk,i(y) =
yi −|y|(k+1),
if yi > |y|(k+1),
if |yi| ≤|y|(k+1),
yi + |y|(k+1),
if yi < −|y|(k+1),
yi;|y|(k+1)
The proof is by induction, stepping through the LARS sequence. First
note that the LARS parameters take a simple form in the orthogonal setting:
AA = |A|−1/2,
uA = |A|−1/21A,
We assume for the moment that there are no ties in the order statistics (4.16), so
that the variables enter one at a time. Let j(l) be the index corresponding to the
lth order statistic, |y|(l) = slyj(l): we will see that Ak = {j(1),...,j(k)}.
We have x′
jy = yj, and so at the ﬁrst step LARS picks variable j(1) and sets
ˆC1 = |y|(1). It is easily seen that
|y|(1) −|yj|
 = |y|(1) −|y|(2)
|y|(1) −|y|(2)
which is precisely (4.17) for k = 1.
Suppose now that step k −1 has been completed, so that Ak = {j(1),...,j(k)}
and (4.17) holds for ˆµk−1. The current correlations ˆCk = |y|(k) and ˆck,j = yj
for j /∈Ak. Since Ak −ak,j = k−1/2, we have
j /∈Ak k1/2|y|(k) −|yj|
|y|(k) −|y|(k+1)
1{j ∈Ak}.
Adding this term to ˆµk−1 yields (4.17) for step k.
The argument clearly extends to the case in which there are ties in the order
statistics (4.16): if |y|(k+1) = ··· = |y|(k+r), then Ak(y) expands by r variables at
step k + 1 and ˆµk+ν(y),ν = 1,...,r, are all determined at the same time and are
equal to ˆµk+1(y).
PROOF OF THEOREM 4 (Orthogonal case).
The argument is particularly
simple in this setting, and so worth giving separately. First we note from (4.17)
that ˆµk is continuous and Lipschitz(1) and so certainly almost differentiable.
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
Hence (4.12) shows that we simply have to calculate ∇· ˆµk. Inspection of (4.17)
shows that
|yi| > |y|(k+1)
almost surely, that is, except for ties. This completes the proof.
4.2. The divergence formula.
While for the most general design matrices X, it
can happen that ˆµk fails to be almost differentiable, we will see that the divergence
∇· ˆµk(y) = k
does hold almost everywhere. Indeed, certain authors [e.g., Meyer and Woodroofe
 ] have argued that the divergence ∇· µ of an estimator provides itself a
useful measure of the effective dimension of a model.
Turning to LARS, we shall say that ˆµ(y) is locally linear at a data point y0
if there is some small open neighborhood of y0 on which ˆµ(y) = My is exactly
linear. Of course, the matrix M = M(y0) can depend on y0—in the case of LARS,
it will be seen to be constant on the interior of polygonal regions, with jumps
across the boundaries. We say that a set G has full measure if its complement has
Lebesgue measure zero.
There is an open set Gk of full measure such that, at all y ∈Gk,
ˆµk(y) is locally linear and ∇· ˆµk(y) = k.
We give here only the part of the proof that relates to actual
calculation of the divergence in (4.19). The arguments establishing continuity and
local linearity are delayed to the Appendix.
So, let us ﬁx a point y in the interior of Gk. From Lemma 13 in the Appendix,
this means that near y the active set Ak(y) is locally constant, that a single variable
enters at the next step, this variable being the same near y. In addition, ˆµk(y) is
locally linear, and hence in particular differentiable. Since Gk ⊂Gl for l < k, the
same story applies at all previous steps and we have
Differentiating the jth component of vector ˆµk(y) yields
LEAST ANGLE REGRESSION
In particular, for the divergence
∇· ˆµk(y) =
the brackets indicating inner product.
The active set is Ak = {1,2,...,k} and xk+1 is the variable to enter next.
For k ≥2, write δk = xl −xk for any choice l < k—as remarked in the Conventions
in the Appendix, the choice of l is immaterial (e.g., l = 1 for deﬁniteness).
Let bk+1 = ⟨δk+1,uk⟩, which is nonzero, as argued in the proof of Lemma 13.
As shown in (A.4) in the Appendix, (2.13) can be rewritten
γk(y) = b−1
k+1⟨δk+1,y −ˆµk−1⟩.
For k ≥2, deﬁne the linear space of vectors equiangular with the active set
Lk = Lk(y) =
u:⟨x1,u⟩= ··· = ⟨xk,u⟩for xl with l ∈Ak(y)
[We may drop the dependence on y since Ak(y) is locally ﬁxed.] Clearly dimLk =
n −k + 1 and
We shall now verify that, for each k ≥1,
⟨∇γk,uk⟩= 1
⟨∇γk,u⟩= 0
for u ∈Lk+1.
Formula (4.21) shows that this sufﬁces to prove Lemma 2.
First, for k = 1 we have γ1(y) = b−1
2 ⟨δ2,y⟩and ⟨∇γ1,u⟩= b−1
2 ⟨δ2,u⟩, and that
⟨δ2,u⟩= ⟨x1 −x2,u⟩=
if u = u1,
Now, for general k, combine (4.22) and (4.20):
bk+1γk(y) = ⟨δk+1,y⟩−
⟨δk+1,ul⟩γl(y),
bk+1⟨∇γk,u⟩= ⟨δk+1,u⟩−
⟨δk+1,ul⟩⟨∇γl,u⟩.
From the deﬁnitions of bk+1 and Lk+1 we have
⟨δk+1,u⟩= ⟨xl −xk+1⟩=
if u = uk,
if u ∈Lk+1.
Hence the truth of (4.24) for step k follows from its truth at step k −1 because of
the containment properties (4.23).
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
4.3. Proof of Theorem 4.
To complete the proof of Theorem 4, we state the
following regularity result, proved in the Appendix.
Under the positive cone condition, ˆµk(y) is continuous and almost
differentiable.
This guarantees that Stein’s formula (4.12) is valid for ˆµk under the positive
cone condition, so the divergence formula of Lemma 2 then immediately yields
Theorem 4.
5. LARS and Lasso properties.
The LARS and Lasso algorithms are
described more carefully in this section, with an eye toward fully understanding
their relationship. Theorem 1 of Section 3 will be veriﬁed. The latter material
overlaps results in Osborne, Presnell and Turlach , particularly in their
Section 4. Our point of view here allows the Lasso to be described as a quite
simple modiﬁcation of LARS, itself a variation of traditional Forward Selection
methodology, and in this sense should be more accessible to statistical audiences.
In any case we will stick to the language of regression and correlation rather
than convex optimization, though some of the techniques are familiar from the
optimization literature.
The results will be developed in a series of lemmas, eventually lending to a proof
of Theorem 1 and its generalizations. The ﬁrst three lemmas refer to attributes of
the LARS procedure that are not speciﬁc to its Lasso modiﬁcation.
Using notation as in (2.17)–(2.20), suppose LARS has completed step k −1,
giving estimate µk−1 and active set Ak for step k, with covariate xk the newest
addition to the active set.
If xk is the only addition to the active set at the end of
step k −1, then the coefﬁcient vector wk = AkG−1
k 1k for the equiangular vector
uk = Xkwk, (2.6), has its kth component wkk agreeing in sign with the current
correlation ckk = x′
k(y −µk−1). Moreover, the regression vector βk for µk = Xβk
has its kth component βkk agreeing in sign with ckk.
Lemma 4 says that new variables enter the LARS active set in the “correct”
direction, a weakened version of the Lasso requirement (3.1). This will turn out to
be a crucial connection for the LARS–Lasso relationship.
PROOF OF LEMMA 4.
The case k = 1 is apparent. Note that since
k(y −µk−1) = Ck1k,
(2.20), from (2.6) we have
wk = Ak C−1
k(y −µk−1)] := Ak C−1
LEAST ANGLE REGRESSION
The term in square braces is the least squares coefﬁcient vector in the regression
of the current residual on Xk, and the term preceding it is positive.
Note also that
k(y −¯yk−1) = (0,δ)′
with δ > 0,
k−1(y −¯yk−1) = 0 by deﬁnition (this 0 has k −1 elements), and ck(γ ) =
k(y −γ uk−1) decreases more slowly in γ than cj(γ ) for j ∈Ak−1:
for γ < γk−1,
= cj(γ ) = Ck,
for γ = γk−1,
for γk−1 < γ < ¯γk−1.
k(y −¯yk−1 + ¯yk−1 −µk−1)
k[( ¯γk−1 −γk−1)uk−1].
The kth element of w∗
k is positive, because it is in the ﬁrst term in (5.5) [(X′
positive deﬁnite], and in the second term it is 0 since uk−1 ∈L(Xk−1).
This proves the ﬁrst statement in Lemma 4. The second follows from
βkk = βk−1,k + γkwkk,
and βk−1,k = 0, xk not being active before step k.
Our second lemma interprets the quantity AA = (1′G−1
A 1)−1/2, (2.4) and (2.5).
Let SA indicate the extended simplex generated by the columns of XA,
“extended” meaning that the coefﬁcients Pj are allowed to be negative.
The point in SA nearest the origin is
vA = AAuA = AAXAwA
where wA = AAG−1
with length ∥vA∥= AA. If A ⊆B, then AA ≥AB, the largest possible value
being AA = 1 for A a singleton.
For any v ∈SA, the squared distance to the origin is ∥XAP ∥2 =
P ′GAP . Introducing a Lagrange multiplier to enforce the summation constraint,
we differentiate
P ′GAP −λ(1′
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
and ﬁnd that the minimizing PA = λG−1
A 1A. Summing, we get λ1′
A 1A = AAwA.
Hence vA = XAPA ∈SA and
∥vA∥2 = P ′
verifying (5.8). If A ⊆B, then SA ⊆SB, so the nearest distance AB must be
equal to or less than the nearest distance AA. AA obviously equals 1 if and only
if A has only one member.
The LARS algorithm and its various modiﬁcations proceed in piecewise linear
steps. For m-vectors β and d, let
β(γ ) = β + γ d
S(γ ) = ∥y −Xβ(γ )∥2.
Letting c = X′(y −Xβ) be the current correlation vector
at µ = Xβ,
S(γ ) −S(0) = −2c ′dγ + d′X′Xdγ 2.
S(γ ) is a quadratic function of γ , with ﬁrst two derivatives at γ = 0,
˙S(0) = −2c ′d
¨S(0) = 2d′X′Xd.
The remainder of this section concerns the LARS–Lasso relationship. Now
β = β(t) will indicate a Lasso solution (1.5), and likewise µ = µ(t) = Xβ(t).
Because S(β) and T (β) are both convex functions of β, with S strictly convex,
standard results show that β(t) and µ(t) are unique and continuous functions of t.
For a given value of t let
A = {j : βj(t) ̸= 0}.
We will show later that A is also the active set that determines the equiangular
direction uA, (2.6), for the LARS–Lasso computations.
We wish to characterize the track of the Lasso solutions β(t) or equivalently
of µ(t) as t increases from 0 to its maximum effective value. Let T be an open
interval of the t axis, with inﬁmum t0, within which the set A of nonzero Lasso
coefﬁcients βj(t) remains constant.
The Lasso estimates µ(t) satisfy
µ(t) = µ(t0) + AA(t −t0)uA
for t ∈T , where uA is the equiangular vector XAwA,wA = AAG−1
A 1A, (2.7).
LEAST ANGLE REGRESSION
The lemma says that, for t in T , µ(t) moves linearly along the
equiangular vector uA determined by A. We can also state this in terms of
the nonzero regression coefﬁcients βA(t),
βA(t) = βA(t0) + SAAA(t −t0)wA,
where SA is the diagonal matrix with diagonal elements sj, j ∈A. [SA is needed
in (5.17) because deﬁnitions (2.4), (2.10) require µ(t) = Xβ(t) = XASAβA(t).]
Since β(t) satisﬁes (1.5) and has nonzero set A, it also minimizes
S(βA) = ∥y −XASAβA∥2
subject to
sj βj = t
sign(βj) = sj
[The inequality in (1.5) can be replaced by T (β) = t as long as t is less than
| ¯βj| for the full m-variable OLS solution ¯βm.] Moreover, the fact that the
minimizing point βA(t) occurs strictly inside the simplex (5.19), combined with
the strict convexity of S(βA), implies we can drop the second condition in (5.19)
so that βA(t) solves
subject to
sj βj = t.
Introducing a Lagrange multiplier, (5.20) becomes
2∥y −XASAβA∥2 + λ
Differentiating we get
A(y −XASAβA) + λSA1A = 0.
Consider two values t1 and t2 in T with t0 < t1 < t2. Corresponding to each
of these are values for the Lagrange multiplier λ such that λ1 > λ2, and solutions
βA(t1) and βA(t2). Inserting these into (5.22), differencing and premultiplying
by SA we get
βA(t2) −βA(t1)
 = (λ1 −λ2)1A.
βA(t2) −βA(t1) = (λ1 −λ2)SAG−1
However, s′
A[(βA(t2) −βA(t1)] = t2 −t1 according to the Lasso deﬁnition, so
t2 −t1 = (λ1 −λ2)s′
A 1A = (λ1 −λ2)1′
A 1A = (λ1 −λ2)A−2
βA(t2) −βA(t1) = SAA2
A(t2 −t1)G−1
A 1A = SAAA(t −t1)wA.
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
Letting t2 = t and t1 →t0 gives (5.17) by the continuity of β(t), and
ﬁnally (5.16). Note that (5.16) implies that the maximum absolute correlation C(t)
equals C(t0) −A2
A(t −t0), so that C(t) is a piecewise linear decreasing function
of the Lasso parameter t.
The Lasso solution β(t) occurs on the surface of the diamond-shaped convex
D(t) increasing with t. Lemma 7 says that, for t ∈T , β(t) moves linearly along
edge A of the polytope, the edge having βj = 0 for j /∈A. Moreover the regression
estimates µ(t) move in the LARS equiangular direction uA, (2.6). It remains to
show that “A” changes according to the rules of Theorem 1, which is the purpose
of the next three lemmas.
A Lasso solution β has
cj = C · sign(βj)
where cj equals the current correlation x′
j(y −µ) = x′
j(y −Xβ). In particular,
this implies that
sign(βj) = sign(cj)
This follows immediately from (5.22) by noting that the jth element
of the left-hand side is cj, and the right-hand side is λ · sign(βj) for j ∈A.
Likewise λ = |cj| = C.
Within an interval T of constant nonzero set A, and also at t0 =
inf(T ), the Lasso current correlations cj(t) = x′
j(y −µ(t)) satisfy
|cj(t)| = C(t) ≡max{|cℓ(t)|}
|cj(t)| ≤C(t)
for j /∈A.
Equation (5.28) says that the |cj(t)| have identical values, say Ct,
for j ∈A. It remains to show that Ct has the extremum properties indicated
in (5.30). For an m-vector d we deﬁne β(γ ) = β(t) + γ d and S(γ ) as in (5.12),
likewise T (γ ) = |βj(γ )|, and
Rt(d) = −˙S(0)/ ˙T (0).
Again assuming βj > 0 for j ∈A, by redeﬁnition of xj if necessary, (5.14) and
(5.28) yield
LEAST ANGLE REGRESSION
If dj = 0 for j /∈A, and dj ̸= 0,
Rt(d) = 2Ct,
while if d has only component j nonzero we can make
Rt(d) = 2|cj(t)|.
According to Lemma 7 the Lasso solutions for t ∈T use dA proportional to wA
with dj = 0 for j /∈A, so
Rt ≡Rt(wA)
is the downward slope of the curve (T,S(T )) at T = t, and by the deﬁnition of
the Lasso must maximize Rt(d). This shows that Ct = C(t), and veriﬁes (5.30),
which also holds at t0 = inf(T ) by the continuity of the current correlations.
We note that Lemmas 7–9 follow relatively easily from the Karush–Kuhn–
Tucker conditions for optimality for the quadratic programming Lasso problem
[Osborne, Presnell and Turlach ]; we have chosen a more geometrical
argument here to demonstrate the nature of the Lasso path.
Figure 8 shows the (T,S) curve corresponding to the Lasso estimates in
Figure 1. The arrow indicates the tangent to the curve at t = 1000, which has
Plot of S versus T for Lasso applied to diabetes data; points indicate the 12 modiﬁed LARS
steps of Figure 1; triangle is (T,S) boundary point at t = 1000; dashed arrow is tangent at t = 1000,
negative slope Rt, (5.31). The (T,S) curve is a decreasing, convex, quadratic spline.
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
downward slope R1000. The argument above relies on the fact that Rt(d) cannot be
greater than Rt, or else there would be (T,S) values lying below the optimal curve.
Using Lemmas 3 and 4 it can be shown that the (T,S) curve is always convex, as
in Figure 8, being a quadratic spline with ˙S(T ) = −2C(T ) and ¨S(T ) = 2A2
We now consider in detail the choice of active set at a breakpoint of the
piecewise linear Lasso path. Let t = t0 indicate such a point, t0 = inf(T ) as in
Lemma 9, with Lasso regression vector β, prediction estimate µ = Xβ, current
correlations c = X′(y −µ), sj = sign(cj) and maximum absolute correlation C.
A1 = {j : βj ̸= 0},
A0 = {j : βj = 0 and |cj| = C},
A10 = A1 ∪A0 and A2 = Ac
10, and take β(γ ) = β + γ d for some m-vector d;
also S(γ ) = ∥y −Xβ(γ )∥2 and T (γ ) = |βj(γ )|.
The negative slope (5.31) at t0 is bounded by 2C,
R(d) = −˙S(0)/ ˙T (0) ≤2C,
with equality only if dj = 0 for j ∈A2. If so, the differences
S = S(γ ) −S(0)
T = T (γ ) −T (0) satisfy
T + L(d)2 · (
L(d) = ∥Xd/d+∥.
We can assume cj ≥0 for all j, by redeﬁnition if necessary, so βj ≥0
according to Lemma 8. Proceeding as in (5.32),
R(d) = 2C
(cj/C)dj
We need dj ≥0 for j ∈A0 ∪A2 in order to maximize (5.40), in which case
R(d) = 2C
(cj/C)dj
This is < 2C unless dj = 0 for j ∈A2, verifying (5.37), and also implying
T (γ ) = T (0) + γ
The ﬁrst term on the right-hand side of (5.13) is then −2C(
T ), while the second
term equals (d/d+)′X′X(d/d+)(
T )2 = L(d)2.
LEAST ANGLE REGRESSION
Lemma 10 has an important consequence. Suppose that A is the current active
set for the Lasso, as in (5.17), and that A ⊆A10. Then Lemma 5 says that L(d)
is ≥AA, and (5.38) gives
with equality if d is chosen to give the equiangular vector uA, dA = SAwA,
dAc = 0. The Lasso operates to minimize S(T ) so we want
S to be as negative
as possible. Lemma 10 says that if the support of d is not conﬁned to A10, then
˙S(0) exceeds the optimum value −2C; if it is conﬁned, then ˙S(0) = −2C but ¨S(0)
exceeds the minimum value 2AA unless dA is proportional to SAwA as in (5.17).
Suppose that β, a Lasso solution, exactly equals a β obtained from the Lassomodiﬁed LARS algorithm, henceforth called LARS–Lasso, as at t = 1000 in
Figures 1 and 3. We know from Lemma 7 that subsequent Lasso estimates will
follow a linear track determined by some subset A, µ(γ ) = µ + γ uA, and so will
the LARS–Lasso estimates, but to verify Theorem 1 we need to show that “A” is
the same set in both cases.
Lemmas 4–7 put four constraints on the Lasso choice of A. Deﬁne A1, A0 and
A10 as at (5.36).
CONSTRAINT 1.
A1 ⊆A. This follows from Lemma 7 since for sufﬁciently
small γ the subsequent Lasso coefﬁcients (5.17),
βA(γ ) = βA + γ SAwA,
will have βj(γ ) ̸= 0,j ∈A1.
CONSTRAINT 2.
A ⊆A10. Lemma 10, (5.37) shows that the Lasso choice d
in β(γ ) = β + γd must have its nonzero support in A10, or equivalently that
µ(γ ) = µ + γ uA must have uA ∈L(XA10). (It is possible that uA happens to
equal uB for some B ⊃A10, but that does not affect the argument below.)
CONSTRAINT 3.
wA = AAG−1
A 1A cannot have sign(wj) ̸= sign(cj) for any
coordinate j ∈A0. If it does, then sign(βj(γ )) ̸= sign(cj(γ )) for sufﬁciently
small γ , violating Lemma 8.
CONSTRAINT 4.
Subject to Constraints 1–3, A must minimize AA. This
follows from Lemma 10 as in (5.43), and the requirement that the Lasso
curve S(T ) declines at the fastest possible rate.
Theorem 1 follows by induction: beginning at β0 = 0, we follow the LARS–
Lasso algorithm and show that at every succeeding step it must continue to agree
with the Lasso deﬁnition (1.5). First of all, suppose that β, our hypothesized Lasso
and LARS–Lasso solution, has occurred strictly within a LARS–Lasso step. Then
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
A0 is empty so that Constraints 1 and 2 imply that A cannot change its current
value: the equivalence between Lasso and LARS–Lasso must continue at least to
the end of the step.
The one-at-a-time assumption of Theorem 1 says that at a LARS–Lasso
breakpoint, A0 has exactly one member, say j0, so A must equal A1 or A10.
There are two cases: if j0 has just been added to the set {|cj| = C}, then Lemma 4
says that sign(wj0) = sign(cj0), so that Constraint 3 is not violated; the other
three constraints and Lemma 5 imply that the Lasso choice A = A10 agrees with
the LARS–Lasso algorithm. The other case has j0 deleted from the active set as
in (3.6). Now the choice A = A10 is ruled out by Constraint 3: it would keep wA
the same as in the previous LARS–Lasso step, and we know that that was stopped
in (3.6) to prevent a sign contradiction at coordinate j0. In other words, A = A1,
in accordance with the Lasso modiﬁcation of LARS. This completes the proof
of Theorem 1.
A LARS–Lasso algorithm is available even if the one-at-a-time condition does
not hold, but at the expense of additional computation. Suppose, for example,
two new members j1 and j2 are added to the set {|cj| = C}, so A0 = {j1,j2}.
It is possible but not certain that A10 does not violate Constraint 3, in which
case A = A10. However, if it does violate Constraint 3, then both possibilities
A = A1 ∪{j1} and A = A1 ∪{j2} must be examined to see which one gives
the smaller value of AA. Since one-at-a-time computations, perhaps with some
added y jitter, apply to all practical situations, the LARS algorithm described in
Section 7 is not equipped to handle many-at-a-time problems.
6. Stagewise properties.
The main goal of this section is to verify Theorem 2.
Doing so also gives us a chance to make a more detailed comparison of the LARS
and Stagewise procedures. Assume that β is a Stagewise estimate of the regression
coefﬁcients, for example, as indicated at |βj| = 2000 in the right panel of
Figure 1, with prediction vector µ = Xβ, current correlations c = X′(y −µ),
C = max{|cj|} and maximal set A = {j :|cj| = C}. We must show that successive
Stagewise estimates of β develop according to the modiﬁed LARS algorithm of
Theorem 2, henceforth called LARS–Stagewise. For convenience we can assume,
by redeﬁnition of xj as −xj, if necessary, that the signs sj = sign(cj) are all nonnegative.
As in (3.8)–(3.10) we suppose that the Stagewise procedure (1.7) has taken
N additional ε-steps forward from µ = Xβ, giving new prediction vector µ(N).
For sufﬁciently small ε, only j ∈A can have Pj = Nj/N > 0.
Letting Nε ≡γ , ∥µ(N) −µ∥≤γ so that c(N) = X′(y −µ(N))
|cj(N) −cj| =
µ(N) −µ
 ≤∥xj∥· ∥µ(N) −µ∥≤γ.
LEAST ANGLE REGRESSION
2[C −maxAc{cj}], j in Ac cannot have maximal current correlation and
can never be involved in the N steps.
Lemma 11 says that we can write the developing Stagewise prediction vector as
µ(γ ) = µ + γ v,
where v = XAPA,
PA a vector of length |A|, with components Nj/N for j ∈A. The nature of the
Stagewise procedure puts three constraints on v, the most obvious of which is
the following.
CONSTRAINT I.
The vector v ∈S+
A, the nonnegative simplex
xjPj,Pj ≥0,
Equivalently, γ v ∈CA, the convex cone (3.12).
The Stagewise procedure, unlike LARS, is not required to use all of the maximal
set A as the active set, and can instead restrict the nonzero coordinates Pj to a
subset B ⊆A. Then v ∈L(XB), the linear space spanned by the columns of XB,
but not all such vectors v are allowable Stagewise forward directions.
CONSTRAINT II.
The vector v must be proportional to the equiangular vector
uB, (2.6), that is, v = vB, (5.8),
B 1B = ABuB.
Constraint II amounts to requiring that the current correlations in B decline at
an equal rate: since
cj(γ ) = x′
j(y −µ −γ v) = cj −γ x′
we need X′
Bv = λ1B for some λ > 0, implying v = λG−1
B 1B; choosing λ = A2
satisﬁes Constraint II. Violating Constraint II makes the current correlations cj(γ )
unequal so that the Stagewise algorithm as deﬁned at (1.7) could not proceed in
direction v.
Equation (6.4) gives X′
CONSTRAINT III.
The vector v = vB must satisfy
for j ∈A −B.
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
Constraint III follows from (6.5). It says that the current correlations for
members of A = {j :|cj| = C} not in B must decline at least as quickly as those
in B. If this were not true, then vB would not be an allowable direction for
Stagewise development since variables in A−B would immediately reenter (1.7).
To obtain strict inequality in (6.7), let B0 ⊂A −B be the set of indices for
B. It is easy to show that vB∪Bo = vB. In other words, if we
take B to be the largest set having a given vB proportional to its equiangular
vector, then x′
B for j ∈A −B.
Writing µ(γ ) = µ + γ v as in (6.2) presupposes that the Stagewise solutions
follow a piecewise linear track. However, the presupposition can be reduced
to one of piecewise differentiability by taking γ inﬁnitesimally small. We can
always express the family of Stagewise solutions as β(z), where the real-valued
parameter Z plays the role of T for the Lasso, increasing from 0 to some maximum
value as β(z) goes from 0 to the full OLS estimate. [The choice Z = T used in
Figure 1 may not necessarily yield a one-to-one mapping; Z = S(0) −S(β), the
reduction in residual squared error, always does.] We suppose that the Stagewise
estimate β(z) is everywhere right differentiable with respect to z. Then the right
derivative
v = dβ(z)/dz
must obey the three constraints.
The deﬁnition of the idealized Stagewise procedure in Section 3.2, in which
ε →0 in rule (1.7), is somewhat vague but the three constraints apply to any
reasonable interpretation. It turns out that the LARS–Stagewise algorithm satisﬁes
the constraints and is unique in doing so. This is the meaning of Theorem 2.
[Of course the LARS–Stagewise algorithm is also supported by direct numerical
comparisons with (1.7), as in Figure 1’s right panel.]
If uA ∈CA, then v = vA obviously satisﬁes the three constraints. The
interesting situation for Theorem 2 is uA /∈CA, which we now assume to be the
case. Any subset B ⊂A determines a face of the convex cone of dimension |B|,
the face having Pj > 0 in (3.12) for j ∈B and Pj = 0 for j ∈A −B. The
orthogonal projection of uA into the linear subspace L(XB), say ProjB(uA), is
proportional to B’s equiangular vector uB: using (2.7),
ProjB(uA) = XBG−1
BuA = XBG−1
B AA1B = (AA/AB) · uB,
or equivalently
ProjB(vA) = (AA/AB)2vB.
The nearest point to uA in CA, say uA, is of the form
Therefore uA exists strictly within face 
B, where 
B = {j : 
Pj > 0}, and must equal
Proj ˆB(uA). According to (6.9), uA is proportional to 
B’s equiangular vector u ˆB,
and also to v ˆB = ABuB. In other words v ˆB satisﬁes Constraint II, and it obviously
also satisﬁes Constraint I. Figure 9 schematically illustrates the geometry.
LEAST ANGLE REGRESSION
The geometry of the LARS–Stagewise modiﬁcation.
The vector v ˆB satisﬁes Constraints I–III, and conversely if v
satisﬁes the three constraints, then v = v ˆB.
Let Cos ≡AA/AB and Sin = [1 −Cos2]1/2, the latter being greater
than zero by Lemma 5. For any face B ⊂A, (6.9) implies
uA = Cos·uB + Sin·zB,
where zB is a unit vector orthogonal to L(XB), pointing away from CA. By
an n-dimensional coordinate rotation we can make L(XB) = L(c1,c2,...,cJ ),
J = |B|, the space of n-vectors with last n −J coordinates zero, and also
uB = (1,0,0,0),
uA = (Cos,0,Sin,0),
the ﬁrst 0 having length J −1, the second 0 length n −J −1. Then we can write
AB,xj2,0,0
the ﬁrst coordinate AB being required since x′
juB = AB, (2.7). Notice that
juA = Cos·AB = AA, as also required by (2.7).
For ℓ∈A −B denote xℓas
xℓ1,xℓ2,xℓ3,xℓ4
so (2.7) yields
ℓuA = Cos·xℓ1 + Sin·xℓ3.
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
Now assume B = 
B. In this case a separating hyperplane H orthogonal to
z ˆB in (6.11) passes between the convex cone CA and uA, through uA = Cos·u ˆB,
implying xℓ3 ≤0 [i.e., xℓand uA are on opposite sides of H, xℓ3 being
negative since the corresponding coordinate of uA, “Sin” in (6.12), is positive].
Equation (6.15) gives Cos·xℓ1 ≥AA = Cos·A ˆB or
ℓv ˆB = x′
ℓ(A ˆBu ˆB) = A ˆBxℓ1 ≥A2
verifying that Constraint III is satisﬁed.
Conversely suppose that v satisﬁes Constraints I–III so that v ∈S+
A and v = vB
for the nonzero coefﬁcient set B: vB =
BxjPj,Pj > 0. Let H be the hyperplane
passing through Cos · uB orthogonally to zB, (6.9), (6.11). If vB ̸= v ˆB, then at
least one of the vectors xℓ, ℓ∈A −B, must lie on the same side of H as uA, so
that xℓ3 > 0 (or else H would be a separating hyperplane between uA and CA,
and vB would be proportional to uA, the nearest point to uA in CA, implying
vB = v ˆB). Now (6.15) gives Cos · xℓ1 < AA = Cos · AB, or
ℓ(ABuB) = ABxℓ1 < A2
This violates Constraint III, showing that v must equal v ˆB.
Notice that the direction of advance v = v ˆB of the idealized Stagewise
procedure is a function only of the current maximal set
A = {j :|cj| = C},
say v = φ( 
A). In the language of (6.7),
The LARS–Stagewise algorithm of Theorem 2 produces an evolving family of
estimates β that everywhere satisﬁes (6.18). This is true at every LARS–Stagewise
breakpoint by the deﬁnition of the Stagewise modiﬁcation. It is also true between
breakpoints. Let 
A be the maximal set at the breakpoint, giving v = v ˆB = φ( 
In the succeeding LARS–Stagewise interval µ(γ ) = µ + γ v ˆB, the maximal set is
immediately reduced to 
B, according to properties (6.6), (6.7) of v ˆB, at which it
stays during the entire interval. However, φ( 
B ) = φ( 
A) = v ˆB since v ˆB ∈C ˆB,
so the LARS–Stagewise procedure, which continues in the direction v until a
new member is added to the active set, continues to obey the idealized Stagewise
equation (6.18).
All of this shows that the LARS–Stagewise algorithm produces a legitimate
version of the idealized Stagewise track. The converse of Lemma 12 says that
there are no other versions, verifying Theorem 2.
The Stagewise procedure has its potential generality as an advantage over LARS
and Lasso: it is easy to deﬁne forward Stagewise methods for a wide variety
of nonlinear ﬁtting problems, as in Hastie, Tibshirani and Friedman [ ,
Chapter 10, which begins with a Stagewise analysis of “boosting”]. Comparisons
LEAST ANGLE REGRESSION
with LARS and Lasso within the linear model framework, as at the end of
Section 3.2, help us better understand Stagewise methodology. This section’s
results permit further comparisons.
Consider proceeding forward from µ along unit vector u, µ(γ ) = µ + γ u, two
interesting choices being the LARS direction u ˆA and the Stagewise direction µ ˆB.
For u ∈L(X ˆA), the rate of change of S(γ ) = ∥y −µ(γ )∥2 is
= 2C · u′
(6.19) following quickly from (5.14). This shows that the LARS direction u ˆA
maximizes the instantaneous decrease in S. The ratio
∂SStage(γ )
∂SLARS(γ )
equaling the quantity “Cos” in (6.15).
The comparison goes the other way for the maximum absolute correlation C(γ ).
Proceeding as in (2.15),
The argument for Lemma 12, using Constraints II and III, shows that u ˆB maximizes (6.21) at A ˆB, and that
∂CLARS(γ )
∂CStage(γ )
The original motivation for the Stagewise procedure was to minimize residual
squared error within a framework of parsimonious forward search. However, (6.20)
shows that Stagewise is less greedy than LARS in this regard, it being more
accurate to describe Stagewise as striving to minimize the maximum absolute
residual correlation.
7. Computations.
The entire sequence of steps in the LARS algorithm
with m < n variables requires O(m3 + nm2) computations—the cost of a least
squares ﬁt on m variables.
In detail, at the kth of m steps, we compute m −k inner products cjk of the
nonactive xj with the current residuals to identify the next active variable, and then
invert the k × k matrix Gk = X′
kXk to ﬁnd the next LARS direction. We do this
by updating the Cholesky factorization Rk−1 of Gk−1 found at the previous step
[Golub and Van Loan ]. At the ﬁnal step m, we have computed the Cholesky
R = Rm for the full cross-product matrix, which is the dominant calculation for a
least squares ﬁt. Hence the LARS sequence can be seen as a Cholesky factorization
with a guided ordering of the variables.
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
The computations can be reduced further by recognizing that the inner products
above can be updated at each iteration using the cross-product matrix X′X and the
current directions. For m ≫n, this strategy is counterproductive and is not used.
For the lasso modiﬁcation, the computations are similar, except that occasionally one has to drop a variable, and hence downdate Rk [costing at most O(m2)
operations per downdate]. For the stagewise modiﬁcation of LARS, we need to
check at each iteration that the components of w are all positive. If not, one or
more variables are dropped [using the inner loop of the NNLS algorithm described
in Lawson and Hanson ], again requiring downdating of Rk. With many
correlated variables, the stagewise version can take many more steps than LARS
because of frequent dropping and adding of variables, increasing the computations
by a factor up to 5 or more in extreme cases.
The LARS algorithm (in any of the three states above) works gracefully for the
case where there are many more variables than observations: m ≫n. In this case
LARS terminates at the saturated least squares ﬁt after n−1 variables have entered
the active set [at a cost of O(n3) operations]. (This number is n −1 rather than n,
because the columns of X have been mean centered, and hence it has row-rank
n −1.) We make a few more remarks about the m ≫n case in the lasso state:
1. The LARS algorithm continues to provide Lasso solutions along the way, and
the ﬁnal solution highlights the fact that a Lasso ﬁt can have no more than n−1
(mean centered) variables with nonzero coefﬁcients.
2. Although the model involves no more than n −1 variables at any time, the
number of different variables ever to have entered the model during the entire
sequence can be—and typically is—greater than n −1.
3. The model sequence, particularly near the saturated end, tends to be quite
variable with respect to small changes in y.
4. The estimation of σ 2 may have to depend on an auxiliary method such as
nearest neighbors (since the ﬁnal model is saturated). We have not investigated
the accuracy of the simple approximation formula (4.12) for the case m > n.
Documented S-PLUS implementations of LARS and associated functions
are available from www-stat.stanford.edu/∼hastie/Papers/; the diabetes data also
appears there.
8. Boosting procedures.
One motivation for studying the Forward Stagewise
algorithm is its usefulness in adaptive ﬁtting for data mining. In particular, Forward
Stagewise ideas are used in “boosting,” an important class of ﬁtting methods for
data mining introduced by Freund and Schapire . These methods are one of
the hottest topics in the area of machine learning, and one of the most effective
prediction methods in current use. Boosting can use any adaptive ﬁtting procedure
as its “base learner” (model ﬁtter): trees are a popular choice, as implemented in
CART [Breiman, Friedman, Olshen and Stone ].
LEAST ANGLE REGRESSION
Friedman, Hastie and Tibshirani and Friedman studied boosting
and proposed a number of procedures, the most relevant to this discussion being
least squares boosting. This procedure works by successive ﬁtting of regression
trees to the current residuals. Speciﬁcally we start with the residual r = y and the
ﬁt ˆy = 0. We ﬁt a tree in x1,x2,...,xm to the response y giving a ﬁtted tree t1
(an n-vector of ﬁtted values). Then we update ˆy to ˆy + ε · t1, r to y −ˆy and
continue for many iterations. Here ε is a small positive constant. Empirical studies
show that small values of ε work better than ε = 1: in fact, for prediction accuracy
“the smaller the better.” The only drawback in taking very small values of ε is
computational slowness.
A major research question has been why boosting works so well, and
speciﬁcally why is ε-shrinkage so important? To understand boosted trees in
the present context, we think of our predictors not as our original variables
x1,x2,...,xm, but instead as the set of all trees tk that could be ﬁtted to our data.
There is a strong similarity between least squares boosting and Forward Stagewise
regression as deﬁned earlier. Fitting a tree to the current residual is a numerical way
of ﬁnding the “predictor” most correlated with the residual. Note, however, that the
greedy algorithms used in CART do not search among all possible trees, but only
a subset of them. In addition the set of all trees, including a parametrization for
the predicted values in the terminal nodes, is inﬁnite. Nevertheless one can deﬁne
idealized versions of least-squares boosting that look much like Forward Stagewise
regression.
Hastie, Tibshirani and Friedman noted the the striking similarity between
Forward Stagewise regression and the Lasso, and conjectured that this may help
explain the success of the Forward Stagewise process used in least squares
boosting. That is, in some sense least squares boosting may be carrying out a Lasso
ﬁt on the inﬁnite set of tree predictors. Note that direct computation of the Lasso
via the LARS procedure would not be feasible in this setting because the number
of trees is inﬁnite and one could not compute the optimal step length. However,
Forward Stagewise regression is feasible because it only need ﬁnd the the most
correlated predictor among the inﬁnite set, where it approximates by numerical
In this paper we have established the connection between the Lasso and Forward
Stagewise regression. We are now thinking about how these results can help to
understand and improve boosting procedures. One such idea is a modiﬁed form of
Forward Stagewise: we ﬁnd the best tree as usual, but rather than taking a small
step in only that tree, we take a small least squares step in all trees currently in our
model. One can show that for small step sizes this procedure approximates LARS;
its advantage is that it can be carried out on an inﬁnite set of predictors such as
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
A.1. Local linearity and Lemma 2.
CONVENTIONS.
We write xl with subscript l for members of the active
set Ak. Thus xl denotes the lth variable to enter, being an abuse of notation for
slxj(l) = sgn(ˆcj(l))xj(l). Expressions x′
l(y −ˆµk−1(y)) = ˆCk(y) and x′
clearly do not depend on which xl ∈Ak we choose.
By writing j /∈Ak, we intend that both xj and −xj are candidates for inclusion
at the next step. One could think of negative indices −j corresponding to “new”
variables x−j = −xj.
The active set Ak(y) depends on the data y. When Ak(y) is the same for all y in
a neighborhood of y0, we say that Ak(y) is locally ﬁxed [at Ak = Ak(y0)].
A function g(y) is locally Lipschitz at y if for all sufﬁciently small vectors
g∥= ∥g(y +
y) −g(y)∥≤L∥
If the constant L applies for all y, we say that g is uniformly locally Lipschitz (L),
and the word “locally” may be dropped.
For each k, 0 ≤k ≤m, there is an open set Gk of full measure
on which Ak(y) and Ak+1(y) are locally ﬁxed and differ by 1, and ˆµk(y) is locally
linear. The sets Gk are decreasing as k increases.
The argument is by induction. The induction hypothesis states that for
each y0 ∈Gk−1 there is a small ball B(y0) on which (a) the active sets Ak−1(y)
and Ak(y) are ﬁxed and equal to Ak−1 and Ak, respectively, (b) |Ak \ Ak−1| = 1
so that the same single variable enters locally at stage k −1 and (c) ˆµk−1(y) = My
is linear. We construct a set Gk with the same property.
Fix a point y0 and the corresponding ball B(y0) ⊂Gk−1, on which y −
ˆµk−1(y) = y −My = Ry, say. For indices j1,j2 /∈A, let N(j1,j2) be the set of y
for which there exists a γ such that
w′(Ry −γ uk) = x′
j1(Ry −γ uk) = x′
j2(Ry −γ uk).
Setting δ1 = xl −xj1, the ﬁrst equality may be written δ′
1Ry = γ δ′
1uk and so
1uk ̸= 0 determines
1uk = 0, there are no qualifying y, and N(j1,j2) is empty.] Now using
the second equality and setting δ2 = xl −xj2, we see that N(j1,j2) is contained
in the set of y for which
LEAST ANGLE REGRESSION
In other words, setting η2 = R′δ2 −(δ′
2uk)η1, we have
N(j1,j2) ⊂{y:η′
If we deﬁne
{N(j1,j2):j1,j2 /∈A,j1 ̸= j2},
it is evident that N(y0) is a ﬁnite union of hyperplanes and hence closed. For
y ∈B(y0) \ N(y0), a unique new variable joins the active set at step k. Near each
such y the “joining” variable is locally the same and γk(y)uk is locally linear.
We then deﬁne Gk ⊂Gk−1 as the union of such sets B(y)\N(y) over y ∈Gk−1.
Thus Gk is open and, on Gk, Ak+1(y) is locally constant and ˆµk(y) is locally
linear. Thus properties (a)–(c) hold for Gk.
The same argument works for the initial case k = 0: since ˆµ0 = 0, there is no
circularity.
Finally, since the intersection of Gk with any compact set is covered by a ﬁnite
number of B(yi) \ N(yi), it is clear that Gk has full measure.
Suppose that, for y near y0, ˆµk−1(y) is continuous (resp. linear)
and that Ak(y) = Ak. Suppose also that, at y0, Ak+1(y0) = A ∪{k + 1}.
Then for y near y0, Ak+1(y) = Ak ∪{k + 1} and ˆγk(y) and hence ˆµk(y) are
continuous (resp. linear) and uniformly Lipschitz.
Consider ﬁrst the situation at y0, with Ck and ckj deﬁned in
(2.18) and (2.17), respectively. Since k + 1 /∈Ak, we have | ˆCk(y0)| > ˆck,k+1(y0),
and ˆγk(y0) > 0 satisﬁes
ˆCk(y0) −ˆγk(y0)Ak
ˆck,j(y0) −ˆγk(y0)ak,j
j = k + 1
j > k + 1.
In particular, it must be that Ak ̸= ak,k+1, and hence
ˆCk(y0) −ˆck,k+1(y0)
Ak −ak,k+1
Call an index j admissible if j /∈Ak and ak,j ̸= Ak. For y near y0, this property
is independent of y. For admissible j, deﬁne
ˆCk(y) −ˆck,j(y)
which is continuous (resp. linear) near y0 from the assumption on ˆµk−1. By
deﬁnition,
j∈Pk(y)Rk,j(y),
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
Pk(y) = {j admissible and Rk,j(y) > 0}.
For admissible j, Rk,j(y0) ̸= 0, and near y0 the functions y →Rk,j(y) are
continuous and of ﬁxed sign. Thus, near y0 the set Pk(y) stays ﬁxed at Pk(y0)
and (A.3) implies that
Rk,k+1(y) < Rk,j(y),
j > k + 1,j ∈Pk(y).
Consequently, for y near y0, only variable k + 1 joins the active set, and so
Ak+1(y) = Ak ∪{k + 1}, and
ˆγk(y) = Rk,k+1(y) = (xl −xk+1)′(y −ˆµk−1(y))
(xl −xk+1)′uk
This representation shows that both ˆγk(y) and hence ˆµk(y) = ˆµk−1(y) + ˆγk(y)uk
are continuous (resp. linear) near y0.
To show that ˆγk is locally Lipschitz at y, we set δ = w −xk+1 and write, using
notation from (A.1),
As y varies, there is a ﬁnite list of vectors (xl,xk+1,uk) that can occur in
the denominator term δ′uk, and since all such terms are positive [as observed
below (A.3)], they have a uniform positive lower bound, amin say. Since ∥δ∥≤2
and ˆµk−1 is Lipschitz (Lk−1) by assumption, we conclude that
min(1 + Lk−1) =: Lk.
A.2. Consequences of the positive cone condition.
Suppose that |A+| = |A| + 1 and that XA+ = [XA x+] (where
x+ = sjxj for some j /∈A). Let PA = XAG−1
A denote projection on span(XA),
so that a = x′
+PAx+ < 1. The +-component of G−1
A+1A+)+ = (1 −a)−1
Consequently, under the positive cone condition (4.11),
Write GA+ as a partitioned matrix
LEAST ANGLE REGRESSION
Applying the formula for the inverse of a partitioned matrix [e.g., Rao , page
A+1A+)+ = −E−1F ′1 + E−1,
E = D −B′A−1B = 1 −x′
F = A−1B = G−1
from which (A.5) follows. The positive cone condition implies that G−1
A+1A+ > 0,
and so (A.6) is immediate.
A.3. Global continuity and Lemma 3.
We shall call y0 a multiple point at
step k if two or more variables enter at the same time. Lemma 14 shows that
such points form a set of measure zero, but they can and do cause discontinuities
in ˆµk+1 at y0 in general. We will see, however, that the positive cone condition
prevents such discontinuities.
We conﬁne our discussion to double points, hoping that these arguments will
be sufﬁcient to establish the same pattern of behavior at points of multiplicity 3
or higher. In addition, by renumbering, we shall suppose that indices k + 1 and
k + 2 are those that are added at double point y0. Similarly, for convenience only,
we assume that Ak(y) is constant near y0. Our task then is to show that, for y near
a double point y0, both ˆµk(y) and ˆµk+1(y) are continuous and uniformly locally
Lipschitz.
Suppose that Ak(y) = Ak is constant near y0 and that
Ak+(y0) = Ak ∪{k + 1,k + 2}. Then for y near y0, Ak+(y) \ Ak can only be
one of three possibilities, namely {k + 1},{k + 2} or {k + 1,k + 2}. In all cases
ˆµk(y) = ˆµk−1(y) + ˆγk(y)uk as usual, and both γk(y) and ˆµk(y) are continuous
and locally Lipschitz.
We use notation and tools from the proof of Lemma 14. Since y0 is a
double point and the positivity set Pk(y) = Pk near y0, we have
0 < Rk,k+1(y0) = Rk,k+2(y0) < Rk,j(y0)
for j ∈Pk \ {k + 1,k + 2}.
Continuity of Rk,j implies that near y0 we still have
0 < Rk,k+1(y),Rk,k+2(y) < min
Rk,j(y);j ∈Pk \ {k + 1,k + 2}
Hence Ak+ \ Ak must equal {k + 1} or {k + 2} or {k + 1,k + 2} according
as Rk,k+1(y) is less than, greater than or equal to Rk,k+2(y). The continuity of
ˆγk(y) = min{Rk,k+1(y),Rk,k+2(y)}
is immediate, and the local Lipschitz property follows from the arguments of
EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI
Assume the conditions of Lemma 16 and in addition that the
positive cone condition (4.11) holds. Then ˆµk+1(y) is continuous and locally
Lipschitz near y0.
Since y0 is a double point, property (A.3) holds, but now with equality
when j = k+1 or k+2 and strict inequality otherwise. In other words, there exists
δ0 > 0 for which
ˆCk+1(y0) −ˆck+1,j(y0)
if j = k + 2,
if j > k + 2.
Consider a neighborhood B(y0) of y0 and let N(y0) be the set of double points
in B(y0), that is, those for which Ak+1(y) \ Ak = {k + 1,k + 2}. We establish the
convention that at such double points ˆµk+1(y) = ˆµk(y); at other points y in B(y0),
ˆµk+1(y) is deﬁned by ˆµk(y) + ˆγk+1(y)uk+1 as usual.
Now consider those y near y0 for which Ak+1(y) \ Ak = {k + 1}, and so, from
the previous lemma, Ak+2(y) \ Ak+1 = {k + 2}. For such y, continuity and the
local Lipschitz property for ˆµk imply that
ˆCk+1(y) −ˆck+1,j(y)
= O(∥y −y0∥),
if j = k + 2,
if j > k + 2.
It is at this point that we use the positive cone condition (via Lemma 15) to
guarantee that Ak+1 > ak+1,k+2. Also, since Ak+1(y) \ Ak = {k + 1}, we have
ˆCk+1(y) > ˆck+1,k+2(y).
These two facts together show that k + 2 ∈Pk+1(y) and hence that
ˆγk+1(y) =
ˆCk+1(y) −ˆck+1,k+2(y)
Ak+1 −ak+1,k+2
= O(∥y −y0∥)
is continuous and locally Lipschitz. In particular, as y approaches N(y0), we have
ˆγk+1(y) →0.
REMARK A.1.
We say that a function g :Rn →R is almost differentiable if it
is absolutely continuous on almost all line segments parallel to the coordinate axes,
and its partial derivatives (which consequently exist a.e.) are locally integrable.
This deﬁnition of almost differentiability appears superﬁcially to be weaker than
that given by Stein, but it is in fact precisely the property used in his proof.
Furthermore, this deﬁnition is equivalent to the standard deﬁnition of weak
differentiability used in analysis.
PROOF OF LEMMA 3.
We have shown explicitly that ˆµk(y) is continuous
and uniformly locally Lipschitz near single and double points. Similar arguments
LEAST ANGLE REGRESSION
extend the property to points of multiplicity 3 and higher, and so all points y are
covered. Finally, absolute continuity of y →ˆµk(y) on line segments is a simple
consequence of the uniform Lipschitz property, and so ˆµk is almost differentiable.
Acknowledgments.
The authors thank Jerome Friedman, Bogdan Popescu,
Saharon Rosset and Ji Zhu for helpful discussions.
REFERENCES
BREIMAN, L., FRIEDMAN, J., OLSHEN, R. and STONE, C. . Classiﬁcation and Regression
Trees. Wadsworth, Belmont, CA.
EFRON, B. . How biased is the apparent error rate of a prediction rule? J. Amer. Statist. Assoc.
81 461–470.
EFRON, B. and TIBSHIRANI, R. . Improvements on cross-validation: The .632+ bootstrap
method. J. Amer. Statist. Assoc. 92 548–560.
FREUND, Y. and SCHAPIRE, R. . A decision-theoretic generalization of online learning and
an application to boosting. J. Comput. System Sci. 55 119–139.
FRIEDMAN, J. . Greedy function approximation: A gradient boosting machine. Ann. Statist.
29 1189–1232.
FRIEDMAN, J., HASTIE, T. and TIBSHIRANI, R. . Additive logistic regression: A statistical
view of boosting (with discussion). Ann. Statist. 28 337–407.
GOLUB, G. and VAN LOAN, C. . Matrix Computations. Johns Hopkins Univ. Press,
Baltimore, MD.
HASTIE, T., TIBSHIRANI, R. and FRIEDMAN, J. . The Elements of Statistical Learning: Data
Mining, Inference and Prediction. Springer, New York.
LAWSON, C. and HANSON, R. . Solving Least Squares Problems. Prentice-Hall, Englewood
Cliffs, NJ.
MALLOWS, C. . Some comments on Cp. Technometrics 15 661–675.
MEYER, M. and WOODROOFE, M. . On the degrees of freedom in shape-restricted regression.
Ann. Statist. 28 1083–1104.
OSBORNE, M., PRESNELL, B. and TURLACH, B. . A new approach to variable selection in
least squares problems. IMA J. Numer. Anal. 20 389–403.
OSBORNE, M. R., PRESNELL, B. and TURLACH, B. . On the LASSO and its dual.
J. Comput. Graph. Statist. 9 319–337.
RAO, C. R. . Linear Statistical Inference and Its Applications, 2nd ed. Wiley, New York.
STEIN, C. . Estimation of the mean of a multivariate normal distribution. Ann. Statist. 9 1135–
TIBSHIRANI, R. . Regression shrinkage and selection via the lasso. J. Roy. Statist. Soc. Ser. B.
58 267–288.
WEISBERG, S. . Applied Linear Regression. Wiley, New York.
YE, J. . On measuring and correcting the effects of data mining and model selection. J. Amer.
Statist. Assoc. 93 120–131.
DEPARTMENT OF STATISTICS
STANFORD UNIVERSITY
SEQUOIA HALL
STANFORD, CALIFORNIA 94305-4065
E-MAIL: 
DISCUSSION
DISCUSSION
BY HEMANT ISHWARAN
Cleveland Clinic Foundation
Being able to reliably, and automatically, select variables in linear regression
models is a notoriously difﬁcult problem. This research attacks this question
head on, introducing not only a computationally efﬁcient algorithm and method,
LARS (and its derivatives), but at the same time introducing comprehensive theory
explaining the intricate details of the procedure as well as theory to guide its
practical implementation. This is a fascinating paper and I commend the authors
for this important work.
Automatic variable selection, the main theme of this paper, has many goals.
So before embarking upon a discussion of the paper it is important to ﬁrst sit
down and clearly identify what the objectives are. The authors make it clear in
their introduction that, while often the goal in variable selection is to select a
“good” linear model, where goodness is measured in terms of prediction accuracy
performance, it is also important at the same time to choose models which
lean toward the parsimonious side. So here the goals are pretty clear: we want
good prediction error performance but also simpler models. These are certainly
reasonable objectives and quite justiﬁable in many scientiﬁc settings. At the same,
however, one should recognize the difﬁculty of the task, as the two goals, low
prediction error and smaller models, can be diametrically opposed. By this I mean
that certainly from an oracle point of view it is true that minimizing prediction error
will identify the true model, and thus, by going after prediction error (in a perfect
world), we will also get smaller models by default. However, in practice, what
happens is that small gains in prediction error often translate into larger models
and less dimension reduction. So as procedures get better at reducing prediction
error, they can also get worse at picking out variables accurately.
Unfortunately, I have some misgivings that LARS might be falling into this trap.
Mostly my concern is fueled by the fact that Mallows’ Cp is the criterion used for
determining the optimal LARS model. The use of Cp often leads to overﬁtting, and
this coupled with the fact that LARS is a forward optimization procedure, which
is often found to be greedy, raises some potential ﬂags. This, by the way, does not
necessarily mean that LARS per se is overﬁtting, but rather that I think Cp may
be an inappropriate model selection criterion for LARS. It is this point that will be
the focus of my discussion. I will offer some evidence that Cp can sometimes be
used effectively if model uncertainty is accounted for, thus pointing to ways for its
more appropriate use within LARS. Mostly I will make my arguments by way of
high-dimensional simulations. My focus on high dimensions is motivated in part
by the increasing interest in such problems, but also because it is in such problems
that performance breakdowns become magniﬁed and are more easily identiﬁed.
LEAST ANGLE REGRESSION
Note that throughout my discussion I will talk only about LARS, but, given the
connections outlined in the paper, the results should also naturally apply to the
Lasso and Stagewise derivatives.
1. Is Cp the correct stopping rule for LARS?
The Cp criterion was
introduced by Mallows to be used with the OLS as an unbiased estimator
for the model error. However, it is important to keep in mind that it was not
intended to be used when the model is selected by the data as this can lead
to selection bias and in some cases poor subset selection [Breiman ].
Thus, choosing the model with lowest Cp value is only a heuristic technique
with sometimes bad performance. Indeed, ultimately, this leads to an inconsistent
procedure for the OLS [Shao ]. Therefore, while I think it is reasonable to
assume that the Cp formula (4.10) is correct [i.e., that it is reasonable to expect that
df (µk) ≈k under a wide variety of settings], there is really no reason to expect
that minimizing the Cp value will lead to an optimal procedure for LARS.
In fact, using Cp in a Forward Stagewise procedure of any kind seems to
me to be a risky thing to do given that Cp often overﬁts and that Stagewise
procedures are typically greedy. Figure 5 of the paper is introduced (partly) to
dispel these types of concerns about LARS being greedy. The message there
is that pe(µ), a performance measurement related to prediction error, declines
slowly from its maximum value for LARS compared to the quick drop seen with
standard forward stepwise regression. Thus, LARS acts differently than wellknown greedy algorithms and so we should not be worried. However, I see the
message quite differently. If the maximum proportion explained for LARS is
roughly the same over a large range of steps, and hence models of different
dimension, then this implies that there is not much to distinguish between higherand lower-dimensional models. Combine this with the use of Cp which could
provide poor estimates for the prediction error due to selection bias and there is
real concern for estimating models that are too large.
To study this issue, let me start by reanalyzing the diabetes data (which was
the basis for generating Figure 5). In this analysis I will compare LARS to a
Bayesian method developed in Ishwaran and Rao , referred to as SVS
(short for Stochastic Variable Selection). The SVS procedure is a hybrid of the
spike-and-slab model approach pioneered by Mitchell and Beauchamp 
and later developed in George and McCulloch . Details for SVS can
be found in Ishwaran and Rao . My reason for using SVS as a
comparison procedure is that, like LARS, its coefﬁcient estimates are derived via
shrinkage. However, unlike LARS, these estimates are based on model averaging
in combination with shrinkage. The use of model averaging is a way of accounting
for model uncertainty, and my argument will be that models selected via Cp based
on SVS coefﬁcients will be more stable than those found using LARS thanks to
the extra beneﬁt of model averaging.
DISCUSSION
Cp values from main effects model for diabetes data: thick line is values from SVS; thin
dashed line is from LARS. Covariates listed at the top of the graph are ordered by importance as
measured by their absolute posterior mean.
Figures 1 and 2 present the Cp values for the main effects model and the
quadratic model from both procedures (the analysis for LARS was based on
S-PLUS code kindly provided by Trevor Hastie). The Cp values for SVS were
computed by (a) ﬁnding the posterior mean values for coefﬁcients, (b) ranking
covariates by the size of their absolute posterior mean coefﬁcient values (with
the top rank going to the largest absolute mean) and (c) computing the Cp value
Cp(µk) = ∥y−µk∥/σ 2 −n+2k, where µk is the OLS estimate based on the k top
ranked covariates. All covariates were standardized. This technique of using Cp
with SVS was discussed in Ishwaran and Rao .
We immediately see some differences in the ﬁgures. In Figure 1, the ﬁnal model
selected by SVS had k = 6 variables, while LARS had k = 7 variables. More
interesting, though, are the discrepancies for the quadratic model seen in Figure 2.
Here the optimal SVS model had k = 8 variables in contrast to the much higher
k = 15 variables found by LARS. The top eight variables from SVS (some of these
can be read off the top of the plot) are bmi, ltg, map, hdl, sex, age.sex, bmi.map
and glu.2. The last three variables are interaction effects and a squared main effects
term. The top eight variables from LARS are bmi, ltg, map, hdl, bmi.map, age.sex,
glu.2 and bmi.2. Although there is a reasonable overlap in variables, there is still
enough of a discrepancy to be concerned. The different model sizes are also cause
for concern. Another worrisome aspect for LARS seen in Figure 2 is that its Cp
values remain bounded away from zero. This should be compared to the Cp values
for SVS, which attain a near-zero mininum value, as we would hope for.
LEAST ANGLE REGRESSION
Cp values from quadratic model: best model from SVS is k = 8 (thick line) compared
with k = 15 from LARS (thin dashed line). Note how the minimum value for SVS is nearly zero.
2. High-dimensional simulations.
Of course, since we do not know the true
answer in the diabetes example, we cannot deﬁnitively assess if the LARS models
are too large. Instead, it will be helpful to look at some simulations for a more
systematic study. The simulations I used were designed following the recipe given
in Breiman . Data was simulated in all cases by using i.i.d. N(0,1) variables
for εi. Covariates xi, for i = 1,...,n, were generated independently from a
multivariate normal distribution with zero mean and with covariance satisfying
E(xi,jxi,k) = ρ|j−k|. I considered two settings for ρ: (i) ρ = 0 (uncorrelated);
(ii) ρ = 0.90 (correlated). In all simulations, n = 800 and m = 400. Nonzero
βj coefﬁcients were in 15 clusters of 7 adjacent variables centered at every
25th variable. For example, for the variables clustered around the 25th variable, the
coefﬁcient values were given by β25+j = |h −j|1.25 for |j| < h, where h = 4. The
other 14 clusters were deﬁned similarly. All other coefﬁcients were set to zero. This
gave a total of 105 nonzero values and 295 zero values. Coefﬁcient values were
adjusted by multiplying by a common constant to make the theoretical R2 value
equal to 0.75 [see Breiman for a discussion of this point]. Please note that,
while the various parameters chosen for the simulations might appear speciﬁc,
I also experimented with other simulations (not reported) by considering different
conﬁgurations for the dimension m, sample size n, correlation ρ and the number of
nonzero coefﬁcients. What I found was consistent with the results presented here.
For each ρ correlation setting, simulations were repeated 100 times independently. Results are recorded in Table 1. There I have recorded what I call TotalMiss,
DISCUSSION
Breiman simulation: m = 400, n = 800 and 105 nonzero βj
ρ = 0 (uncorrelated X)
ρ = 0.9 (correlated X)
pe(µ) TotalMiss
pe(µ) TotalMiss
0.547 0.055
0.347 0.135
0.323 0.072
0.153 0.164
svsBMA 400.00
0.737 0.000
0.737 0.000
0.367 0.075
0.552 0.208
FDR and FNR. TotalMiss is the total number of misclassiﬁed variables, that is, the
total number of falsely identiﬁed nonzero βj coefﬁcients and falsely identiﬁed zero
coefﬁcients; FDR and FNR are the false discovery and false nondiscovery rates de-
ﬁned as the false positive and false negative rates for those coefﬁcients identiﬁed as
nonzero and zero, respectively. The TotalMiss, FDR and FNR values reported are
the averaged values from the 100 simulations. Also recorded in the table is m, the
average number of variables selected by a procedure, as well as the performance
value pe(µ) [cf. (3.17)], again averaged over the 100 simulations.
Table 1 records the results from various procedures. The entry “svsCp”
refers to the Cp-based SVS method used earlier; “Step” is standard forward
stepwise regression using the Cp criterion; “svsBMA” is the Bayesian model
averaged estimator from SVS. My only reason for including svsBMA is to
gauge the prediction error performance of the other procedures. Its variable
selection performance is not of interest. Pure Bayesian model averaging leads to
improved prediction, but because it does no dimension reduction at all it cannot be
considered as a serious candidate for selecting variables.
The overall conclusions from Table 1 are summarized as follows:
1. The total number of misclassiﬁed coefﬁcients and FDR values is high in
the uncorrelated case for LARS and high in the correlated case for stepwise
regression. Their estimated models are just too large. In comparison, svsCp
does well in both cases. Overall it does the best in terms of selecting
variables by maintaining low FDR and TotalMiss values. It also maintains good
performance values.
2. LARS’s performance values are good, second only to svsBMA. However, low
prediction error does not necessarily imply good variable selection.
3. LARS Cp values in orthogonal models.
Figure 3 shows the Cp values
for LARS from the two sets of simulations. It is immediately apparent that the
Cp curve in the uncorrelated case is too ﬂat, leading to models which are too large.
These simulations were designed to reﬂect an orthogonal design setting (at least
asymptotically), so what is it about the orthogonal case that is adversely affecting
LEAST ANGLE REGRESSION
Cp values from simulations where ρ = 0 (left) and ρ = 0.9 (right): bottom curves are from
SVS; top curves are from LARS. The lines seen on each curve are the mean Cp values based on
100 simulations. Note how the minimum value for SVS is near zero in both cases. Also superimposed
on each curve are error bars representing mean values plus or minus one standard deviation.
We can use Lemma 1 of the paper to gain some insight into this. For this
argument I will assume that m is ﬁxed (the lemma is stated for m = n but
applies in general) and I will need to assume that Xn×m is a random orthogonal
matrix, chosen so that its rows are exchangeable. To produce such an X, choose
m values ei1,...,eim without replacement from {e1,...,en}, where ej is deﬁned
as in Section 4.1, and set X = [ei1,...,eim]. It is easy to see that this ensures
row-exchangeability. Hence, µ1,...,µn are exchangeable and, therefore, Yi =
µi + εi are exchangeable since εi are i.i.d. I will assume, as in (4.1), that εi are
independent N(0,σ 2) variables.
For simplicity take σ 2 = σ 2 = 1. Let Vj, for j = 0,...,m −1, denote the
(j + 1)st largest value from the set of values {|Yi1|,...,|Yim|}. Let k0 denote
the true dimension, that is, the number of nonzero coordinates of the true β, and
suppose that k is some dimension larger than k0 such that 1 ≤k0 < k ≤m ≤n.
Notice that Vk ≤Vk0, and thus, by Lemma 1 and (4.10),
Cp(µk) −Cp
 + 2(k −k0)
kBk + 2(k −k0),
k ≥0 and Bk = m
j=1 1{|Yij| > Vk0}. Observe that by
exchangeability Bk is a Binomial(m,k0/m) random variable. It is a little messy
to work out the distribution for
k explicitly. However, it is not hard to see that
k can be reasonably large with high probability. Now if k0 > k −k0 and k0 is
large, then Bk, which has a mean of k0, will become the dominant term in
DISCUSSION
kBk will become larger than 2(k −k0) with high probability. This suggests,
at least in this setting, that Cp will overﬁt if the dimension of the problem is high.
In this case there will be too much improvement in the residual sums of squares
when moving from k0 to k because of the nonvanishing difference between the
squared order statistics V 2
k0 and V 2
4. Summary.
The use of Cp seems to encourage large models in LARS,
especially in high-dimensional orthogonal problems, and can adversely affect
variable selection performance. It can also be unreliable when used with stepwise
regression. The use of Cp with SVS, however, seems better motivated due to
the beneﬁts of model averaging, which mitigates the selection bias effect. This
suggests that Cp can be used effectively if model uncertainty is accounted for.
This might be one remedy. Another remedy would be simply to use a different
model selection criteria when using LARS.
REFERENCES
BREIMAN, L. . The little bootstrap and other methods for dimensionality selection in
regression: X-ﬁxed prediction error. J. Amer. Statist. Assoc. 87 738–754.
GEORGE, E. I. and MCCULLOCH, R. E. . Variable selection via Gibbs sampling. J. Amer.
Statist. Assoc. 88 881–889.
ISHWARAN, H. and RAO, J. S. . Bayesian nonparametric MCMC for large variable selection
problems. Unpublished manuscript.
ISHWARAN, H. and RAO, J. S. . Detecting differentially expressed genes in microarrays using
Bayesian model selection. J. Amer. Statist. Assoc. 98 438–455.
MALLOWS, C. . Some comments on Cp. Technometrics 15 661–675.
MITCHELL, T. J. and BEAUCHAMP, J. J. . Bayesian variable selection in linear regression
(with discussion). J. Amer. Statist. Assoc. 83 1023–1036.
SHAO, J. . Linear model selection by cross-validation. J. Amer. Statist. Assoc. 88 486–494.
DEPARTMENT OF BIOSTATISTICS/WB4
CLEVELAND CLINIC FOUNDATION
9500 EUCLID AVENUE
CLEVELAND, OHIO 44195
E-MAIL: 
DISCUSSION
BY KEITH KNIGHT
University of Toronto
First, I congratulate the authors for a truly stimulating paper. The paper resolves
a number of important questions but, at the same time, raises many others. I would
LEAST ANGLE REGRESSION
like to focus my comments to two speciﬁc points.
The similarity of Stagewise and LARS ﬁtting to the Lasso suggests that
the estimates produced by Stagewise and LARS ﬁtting may minimize an objective
function that is similar to the appropriate Lasso objective function. It is not at
all (at least to me) obvious how this might work though. I note, though, that
the construction of such an objective function may be easier than it seems. For
example, in the case of bagging [Breiman ] or subagging [Bühlmann and
Yu ], an “implied” objective function can be constructed. Suppose that
θ1,...,θm are estimates (e.g., computed from subsamples or bootstrap samples)
that minimize, respectively, objective functions Z1,...,Zm and deﬁne
θ = g(θ1,...,θm);
then θ minimizes the objective function
Z(t) = inf{Z1(t1) + ··· + Zm(tm):g(t1,...,tm) = t}.
(Thanks to Gib Bassett for pointing this out to me.) A similar construction for
stagewise ﬁtting (or LARS in general) could facilitate the analysis of the statistical
properties of the estimators obtained via these algorithms.
When I ﬁrst started experimenting with the Lasso, I was impressed by
its robustness to small changes in its tuning parameter relative to more classical
stepwise subset selection methods such as Forward Selection and Backward
Elimination. (This is well illustrated by Figure 5; at its best, Forward Selection
is comparable to LARS, Stagewise and the Lasso but the performance of Forward
Selection is highly dependent on the model size.) Upon reﬂection, I realized that
there was a simple explanation for this robustness. Speciﬁcally, the strict convexity
in β for each t in the Lasso objective function (1.5) together with the continuity
(in the appropriate sense) in t of these objective functions implies that the Lasso
solutions β(t) are continuous in t; this continuity breaks down for nonconvex
objective functions. Of course, the same can be said of other penalized least
squares estimates whose penalty is convex. What seems to make the Lasso special
is (i) its ability to produce exact 0 estimates and (ii) the “fact” that its bias seems
to be more controllable than it is for other methods (e.g., ridge regression, which
naturally overshrinks large effects) in the sense that for a ﬁxed tuning parameter the
bias is bounded by a constant that depends on the design but not the true parameter
values. At the same time, though, it is perhaps unfair to compare stepwise methods
to the Lasso, LARS or Stagewise ﬁtting since the space of models considered by
the latter methods seems to be “nicer” than it is for the former and (perhaps more
important) since the underlying motivation for using Forward Selection is typically
not prediction. For example, bagged Forward Selection might perform as well as
the other methods in many situations.
DISCUSSION
REFERENCES
BREIMAN, L. . Bagging predictors. Machine Learning 24 123–140.
BÜHLMANN, P. and YU, B. . Analyzing bagging. Ann. Statist. 30 927–961.
DEPARTMENT OF STATISTICS
UNIVERSITY OF TORONTO
100 ST. GEORGE ST.
TORONTO, ONTARIO M5S 3G3
E-MAIL: 
DISCUSSION
BY JEAN-MICHEL LOUBES AND PASCAL MASSART
Université Paris-Sud
The issue of model selection has drawn the attention of both applied and
theoretical statisticians for a long time. Indeed, there has been an enormous
range of contribution in model selection proposals, including work by Akaike
 , Mallows , Foster and George , Birgé and Massart 
and Abramovich, Benjamini, Donoho and Johnstone . Over the last decade,
modern computer-driven methods have been developed such as All Subsets,
Forward Selection, Forward Stagewise or Lasso. Such methods are useful in the
setting of the standard linear model, where we observe noisy data and wish to
predict the response variable using only a few covariates, since they provide
automatically linear models that ﬁt the data. The procedure described in this paper
is, on the one hand, numerically very efﬁcient and, on the other hand, very general,
since, with slight modiﬁcations, it enables us to recover the estimates given by the
Lasso and Stagewise.
1. Estimation procedure.
The “LARS” method is based on a recursive
procedure selecting, at each step, the covariates having largest absolute correlation
with the response y. In the case of an orthogonal design, the estimates can then be
viewed as an l1-penalized estimator. Consider the linear regression model where
we observe y with some random noise ε, with orthogonal design assumptions:
y = Xβ + ε.
Using the soft-thresholding form of the estimator, we can write it, equivalently, as
the minimum of an ordinary least squares and an l1 penalty over the coefﬁcients
of the regression. As a matter of fact, at step k = 1,...,m, the estimators ˆβk =
X−1 ˆµk are given by
ˆµk = arg min
LEAST ANGLE REGRESSION
There is a trade-off between the two terms, balanced by the smoothing decreasing
sequence λ2
n(k). The more stress is laid on the penalty, the more parsimonious the
representation will be. The choice of the l1 penalty enables us to keep the largest
coefﬁcients, while the smallest ones shrink toward zero in a soft-thresholding
scheme. This point of view is investigated in the Lasso algorithm as well as in
studying the false discovery rate (FDR).
So, choosing these weights in an optimal way determines the form of the
estimator as well as its asymptotic behavior. In the case of the algorithmic
procedure, the suggested level is the (k + 1)-order statistic:
n(k) = |y|(k+1).
As a result, it seems possible to study the asymptotic behavior of the LARS
estimates under some conditions on the coefﬁcients of β. For instance, if there
exists a roughness parameter ρ ∈ , such that m
j=1 |βj|ρ ≤M, metric entropy
theory results lead to an upper bound for the mean square error ∥ˆβ −β∥2. Here we
refer to the results obtained in Loubes and van de Geer . Consistency should
be followed by the asymptotic distribution, as is done for the Lasso in Knight and
Fu .
The interest for such an investigation is double: ﬁrst, it gives some insight into
the properties of such estimators. Second, it suggests an approach for choosing the
threshold λ2
n which can justify the empirical cross-validation method, developed
later in the paper. Moreover, the asymptotic distributions of the estimators are
needed for inference.
Other choices of penalty and loss functions can also be considered. First, for
γ ∈ , in the regression framework.
Finally, a penalty over both the number of coefﬁcients and the smoothness of the
coefﬁcients can be used to study, from a theoretical point of view, the asymptotics
DISCUSSION
of the estimate. Such a penalty is analogous to complexity penalties studied in van
de Geer :
µ∈Rn,k∈[1,m]
n(k)∥µ∥1 + pen(k)
2. Mallows’ Cp.
We now discuss the crucial issue of selecting the number k
of inﬂuential variables. To make this discussion clear, let us ﬁrst assume the
variance σ 2 of the regression errors to be known. Interestingly the penalized
criterion which is proposed by the authors is exactly equivalent to Mallows’ Cp
when the design is orthogonal (this is indeed the meaning of their Theorem 3).
More precisely, using the same notation as in the paper, let us focus on the
following situation which illustrates what happens in the orthogonal case where
LARS is equivalent to the Lasso. One observes some random vector y in Rn,
with expectation µ and covariance matrix σ 2In. The variable selection problem
that we want to solve here is to determine which components of y are inﬂuential.
According to Lemma 1, given k, the kth LARS estimate µk of µ can be explicitly
computed as a soft-thresholding estimator. Indeed, considering the order statistics
of the absolute values of the data denoted by
|y|(1) ≥|y|(2) ≥··· ≥|y|(n)
and deﬁning the soft threshold function η(·,t) with level t ≥0 as
η(x,t) = x1|x|>t
yi,|y|(k+1)
To select k, the authors propose to minimize the Cp criterion
Cp(µk) = ∥y −µk∥2 −nσ 2 + 2kσ 2.
Our purpose is to analyze this proposal with the help of the results on penalized
model selection criteria proved in Birgé and Massart . In these papers
some oracle type inequalities are proved for selection procedures among some
arbitrary collection of projection estimators on linear models when the regression
errors are Gaussian. In particular one can apply them to the variable subset
selection problem above, assuming the random vector y to be Gaussian. If one
decides to penalize in the same way the subsets of variables with the same
cardinality, then the penalized criteria studied in Birgé and Massart 
take the form
p(µk) = ∥y −µk∥2 −nσ 2 + pen(k),
LEAST ANGLE REGRESSION
where pen(k) is some penalty to be chosen and µk denotes the hard threshold
estimator with components
µk,i = η′yi,|y|(k+1)
η′(x,t) = x1|x|>t.
The essence of the results proved by Birgé and Massart in this case is
the following. Their analysis covers penalties of the form
pen(k) = 2kσ 2C
[note that the FDR penalty proposed in Abramovich, Benjamini, Donoho and
Johnstone corresponds to the case C = 1]. It is proved in Birgé and Massart
 that if the penalty pen(k) is heavy enough (i.e., C > 1 and C′ is an
adequate absolute constant), then the model selection procedure works in the
sense that, up to a constant, the selected estimator µk performs as well as the
best estimator among the collection {µk,1 ≤k ≤n} in terms of quadratic risk.
On the contrary, it is proved in Birgé and Massart that if C < 1, then at
least asymptotically, whatever C′, the model selection does not work, in the sense
that, even if µ = 0, the procedure will systematically choose large values of k,
leading to a suboptimal order for the quadratic risk of the selected estimator µk.
So, to summarize, some 2kσ 2 log(n/k) term should be present in the penalty, in
order to make the model selection criterion (2) work. In particular, the choice
pen(k) = 2kσ 2 is not appropriate, which means that Mallows’ Cp does not work
in this context. At ﬁrst glance, these results seem to indicate that some problems
could occur with the use of the Mallows’ Cp criterion (1). Fortunately, however,
this is not at all the case because a very interesting phenomenon occurs, due to the
soft-thresholding effect. As a matter of fact, if we compare the residual sums of
squares of the soft threshold estimator µk and the hard threshold estimator µk, we
easily get
∥y −µk∥2 −∥y −µk∥2 =
(k+1)1|yi|>|y|(k+1) = k|y|2
so that the “soft” Cp criterion (1) can be interpreted as a “hard” criterion (2) with
random penalty
pen(k) = k|y|2
(k+1) + 2kσ 2.
Of course this kind of penalty escapes stricto sensu to the analysis of Birgé and
Massart as described above since the penalty is not deterministic.
However, it is quite easy to realize that, in this penalty, |y|2
(k+1) plays the role of the
apparently “missing” logarithmic factor 2σ 2 log(n/k). Indeed, let us consider the
DISCUSSION
pure noise situation where µ = 0 to keep the calculations as simple as possible.
Then, if we consider the order statistics of a sample U1,...,Un of the uniform
distribution on 
U(1) ≤U(2) ≤··· ≤U(n),
taking care of the fact that these statistics are taken according to the usual
increasing order while the order statistics on the data are taken according to the
reverse order, σ −2|y|2
(k+1) has the same distribution as
Q−1U(k+1)
where Q denotes the tail function of the chi-square distribution with 1 degree of
freedom. Now using the double approximations Q−1(u) ∼2log(|u|) as u goes
to 0 and U(k+1) ≈(k + 1)/n (which at least means that, given k, nU(k+1) tends
to k + 1 almost surely as n goes to inﬁnity but can also be expressed with much
more precise probability bounds) we derive that |y|2
(k+1) ≈2σ 2 log(n/(k+1)). The
conclusion is that it is possible to interpret the “soft” Cp criterion (1) as a randomly
penalized “hard” criterion (2). The random part of the penalty k|y|2
(k+1) cleverly
plays the role of the unavoidable logarithmic term 2σ 2k log(n/k), allowing the
hope that the usual 2kσ 2 term will be heavy enough to make the selection
procedure work as we believe it does. A very interesting feature of the penalty (3)
is that its random part depends neither on the scale parameter σ 2 nor on the tail
of the errors. This means that one could think to adapt the data-driven strategy
proposed in Birgé and Massart to choose the penalty without knowing the
scale parameter to this context, even if the errors are not Gaussian. This would
lead to the following heuristics. For large values of k, one can expect the quantity
−∥y −µk∥2 to behave as an afﬁne function of k with slope α(n)σ 2. If one is able
to compute α(n), either theoretically or numerically (our guess is that it varies
slowly with n and that it is close to 1.5), then one can just estimate the slope
(for instance by making a regression of −∥y −µk∥2 with respect to k for large
enough values of k) and plug the resulting estimate of σ 2 into (1). Of course, some
more efforts would be required to complete this analysis and provide rigorous
oracle inequalities in the spirit of those given in Birgé and Massart or
Abramovich, Benjamini, Donoho and Johnstone and also some simulations
to check whether our proposal to estimate σ 2 is valid or not.
Our purpose here was just to mention some possible explorations starting from
the present paper that we have found very stimulating. It seems to us that it
solves practical questions of crucial interest and raises very interesting theoretical
questions: consistency of LARS estimator; efﬁciency of Mallows’ Cp in this
context; use of random penalties in model selection for more general frameworks.
LEAST ANGLE REGRESSION
REFERENCES
ABRAMOVICH, F., BENJAMINI, Y., DONOHO, D. and JOHNSTONE, I. . Adapting to
unknown sparsity by controlling the false discovery rate. Technical Report 2000–19, Dept.
Statistics, Stanford Univ.
AKAIKE, H. . Maximum likelihood identiﬁcation of Gaussian autoregressive moving average
models. Biometrika 60 255–265.
BIRGÉ, L. and MASSART, P. . Gaussian model selection. J. Eur. Math. Soc. 3 203–268.
BIRGÉ, L. and MASSART, P. . A generalized Cp criterion for Gaussian model selection.
Technical Report 647, Univ. Paris 6 & 7.
FOSTER, D. and GEORGE, E. . The risk inﬂation criterion for multiple regression. Ann. Statist.
22 1947–1975.
KNIGHT, K. and FU, B. . Asymptotics for Lasso-type estimators. Ann. Statist. 28 1356–1378.
LOUBES, J.-M. and VAN DE GEER, S. . Adaptive estimation with soft thresholding penalties.
Statist. Neerlandica 56 453–478.
MALLOWS, C. . Some comments on Cp. Technometrics 15 661–675.
VAN DE GEER, S. . Least squares estimation with complexity penalties. Math. Methods Statist.
10 355–374.
CNRS AND LABORATOIRE DE MATHÉMATIQUES
EQUIPE DE PROBABILITÉS, STATISTIQUE
ET MODÉLISATION
UNIVERSITÉ PARIS-SUD, BÂT. 425
91405 ORDAY CEDEX
E-MAIL: 
LABORATOIRE DE MATHÉMATIQUES
EQUIPE DE PROBABILITÉS, STATISTIQUE
ET MODÉLISATION
UNIVERSITÉ PARIS-SUD, BÂT. 425
91405 ORDAY CEDEX
E-MAIL: 
DISCUSSION
BY DAVID MADIGAN AND GREG RIDGEWAY
Rutgers University and Avaya Labs Research, and RAND
Algorithms for simultaneous shrinkage and selection in regression and classiﬁcation provide attractive solutions to knotty old statistical challenges. Nevertheless,
as far as we can tell, Tibshirani’s Lasso algorithm has had little impact on statistical practice. Two particular reasons for this may be the relative inefﬁciency of the
original Lasso algorithm and the relative complexity of more recent Lasso algorithms [e.g., Osborne, Presnell and Turlach ]. Efron, Hastie, Johnstone and
Tibshirani have provided an efﬁcient, simple algorithm for the Lasso as well as
algorithms for stagewise regression and the new least angle regression. As such
this paper is an important contribution to statistical computing.
1. Predictive performance.
The authors say little about predictive performance issues. In our work, however, the relative out-of-sample predictive
performance of LARS, Lasso and Forward Stagewise (and variants thereof) takes
DISCUSSION
Stagewise, LARS and Lasso mean square error predictive performance, comparing
cross-validation with Cp
center stage. Interesting connections exist between boosting and stagewise algorithms so predictive comparisons with boosting are also of interest.
The authors present a simple Cp statistic for LARS. In practice, a crossvalidation (CV) type approach for selecting the degree of shrinkage, while
computationally more expensive, may lead to better predictions. We considered
this using the LARS software. Here we report results for the authors’ diabetes
data, the Boston housing data and the Servo data from the UCI Machine Learning
Repository. Speciﬁcally, we held out 10% of the data and chose the shrinkage level
using either Cp or nine-fold CV using 90% of the data. Then we estimated mean
square error (MSE) on the 10% hold-out sample. Table 1 shows the results for
main-effects models.
Table 1 exhibits two particular characteristics. First, as expected, Stagewise,
LARS and Lasso perform similarly. Second, Cp performs as well as crossvalidation; if this holds up more generally, larger-scale applications will want to
use Cp to select the degree of shrinkage.
Table 2 presents a reanalysis of the same three datasets but now considering
Predictive performance of competing methods: LM is a main-effects linear model with
least squares ﬁtting; LARS is least angle regression with main effects and CV shrinkage
selection; LARS two-way Cp is least angle regression with main effects and all two-way
interactions, shrinkage selection via Cp; GBM additive and GBM two-way use least
squares boosting, the former using main effects only, the latter using main effects
and all two-way interactions; MSE is mean square error on a 10% holdout
sample; MAD is mean absolute deviation
LARS two-way Cp
GBM additive
GBM two-way
LEAST ANGLE REGRESSION
ﬁve different models: least squares; LARS using cross-validation to select the
coefﬁcients; LARS using Cp to select the coefﬁcients and allowing for twoway interactions; least squares boosting ﬁtting only main effects; least squares
boosting allowing for two-way interactions. Again we used the authors’ LARS
software and, for the boosting results, the gbm package in R [Ridgeway ].
We evaluated all the models using the same cross-validation group assignments.
A plain linear model provides the best out-of-sample predictive performance for
the diabetes dataset. By contrast, the Boston housing and Servo data exhibit more
complex structure and models incorporating higher-order structure do a better job.
While no general conclusions can emerge from such a limited analysis, LARS
seems to be competitive with these particular alternatives. We note, however, that
for the Boston housing and Servo datasets Breiman reports substantially
better predictive performance using random forests.
2. Extensions to generalized linear models.
The minimal computational
complexity of LARS derives largely from the squared error loss function. Applying
LARS-type strategies to models with nonlinear loss functions will require some
form of approximation. Here we consider LARS-type algorithms for logistic
regression.
Consider the logistic log-likelihood for a regression function f (x) which will
be linear in x:
yif (xi) −log
1 + exp(f (xi))
We can initialize f (x) = log( ¯y/(1 −¯y)). For some α we wish to ﬁnd the
covariate xj that offers the greatest improvement in the logistic log-likelihood,
ℓ(f (x)+xt
jα). To ﬁnd this xj we can compute the directional derivative for each j
and choose the maximum,
j∗= argmax
f (x) + xt
1 + exp(−f (x))
Note that as with LARS this is the covariate that is most highly correlated with
the residuals. The selected covariate is the ﬁrst member of the active set, A. For α
small enough (3) implies
(sj∗xj∗−sjxj)t
1 + exp(−f (x) −xt
for all j ∈AC, where sj indicates the sign of the correlation as in the LARS
development. Choosing α to have the largest magnitude while maintaining the
constraint in (4) involves a nonlinear optimization. However, linearizing (4) yields
DISCUSSION
a fairly simple approximate solution. If x2 is the variable with the second largest
correlation with the residual, then
(sj∗xj∗−s2x2)t(y −p(x))
(sj∗xj∗−s2x2)t(p(x)(1 −p(x))xj∗).
The algorithm may need to iterate (5) to obtain the exact optimal ˆα. Similar logic
yields an algorithm for the full solution.
We simulated N = 1000 observations with 10 independent normal covariates
xi ∼N10(0,I) with outcomes Yi ∼Bern(1/(1 + exp(−xt
iβ))), where β ∼
N10(0,1). Figure 1 shows a comparison of the coefﬁcient estimates using Forward
Stagewise and the Least Angle method of estimating coefﬁcients, the ﬁnal
estimates arriving at the MLE. While the paper presents LARS for squared error
problems, the Least Angle approach seems applicable to a wider family of models.
However, an out-of-sample evaluation of predictive performance is essential to
assess the utility of such a modeling strategy.
Comparison of coefﬁcient estimation for Forward Stagewise and Least Angle Logistic
Regression.
LEAST ANGLE REGRESSION
Speciﬁcally for the Lasso, one alternative strategy for logistic regression is
to use a quadratic approximation for the log-likelihood. Consider the Bayesian
version of Lasso with hyperparameter γ (i.e., the penalized rather than constrained
version of Lasso):
logf (β|y1,...,yn)
yi(xiβ) + (1 −yi)
1 −(xiβ)
 + d log
ai(xiβ)2 + bi(xiβ) + ci
where  denotes the logistic link, d is the dimension of β and ai,bi and ci are
Taylor coefﬁcients. Fu’s elegant coordinatewise “Shooting algorithm” [Fu ],
can optimize this target starting from either the least squares solution or from zero.
In our experience the shooting algorithm converges rapidly.
REFERENCES
BREIMAN, L. . Random forests. Available at ftp://ftp.stat.berkeley.edu/pub/users/breiman/
randomforest2001.pdf.
FU, W. J. . Penalized regressions: The Bridge versus the Lasso. J. Comput. Graph. Statist. 7
OSBORNE, M. R., PRESNELL, B. and TURLACH, B. A. . A new approach to variable
selection in least squares problems. IMA J. Numer. Anal. 20 389–403.
RIDGEWAY, G. . GBM 0.7-2 package manual. Available at 
packages/gbm.pdf.
AVAYA LABS RESEARCH
DEPARTMENT OF STATISTICS
RUTGERS UNIVERSITY
PISCATAWAY, NEW JERSEY 08855
E-MAIL: 
RAND STATISTICS GROUP
SANTA MONICA, CALIFORNIA 90407-2138
E-MAIL: 
DISCUSSION
BY SAHARON ROSSET AND JI ZHU
IBM T. J. Watson Research Center and Stanford University
1. Introduction.
We congratulate the authors on their excellent work. The
paper combines elegant theory and useful practical results in an intriguing manner.
The LAR–Lasso–boosting relationship opens the door for new insights on existing
DISCUSSION
methods’ underlying statistical mechanisms and for the development of new and
promising methodology. Two issues in particular have captured our attention, as
their implications go beyond the squared error loss case presented in this paper,
into wider statistical domains: robust ﬁtting, classiﬁcation, machine learning and
more. We concentrate our discussion on these two results and their extensions.
2. Piecewise linear regularized solution paths.
The ﬁrst issue is the piecewise linear solution paths to regularized optimization problems. As the discussion
paper shows, the path of optimal solutions to the “Lasso” regularized optimization
ˆβ(λ) = argmin
β ∥y −Xβ∥2
is piecewise linear as a function of λ; that is, there exist ∞> λ0 > λ1 > ··· >
λm = 0 such that ∀λ ≥0, with λk ≥λ ≥λk+1, we have
ˆβ(λ) = ˆβ(λk) −(λ −λk)γk.
In the discussion paper’s terms, γk is the “LAR” direction for the kth step of the
LAR–Lasso algorithm.
This property allows the LAR–Lasso algorithm to generate the whole path of
Lasso solutions, ˆβ(λ), for “practically” the cost of one least squares calculation on
the data (this is exactly the case for LAR but not for LAR–Lasso, which may be
signiﬁcantly more computationally intensive on some data sets). The important
practical consequence is that it is not necessary to select the regularization
parameter λ in advance, and it is now computationally feasible to optimize it based
on cross-validation (or approximate Cp, as presented in the discussion paper).
The question we ask is: what makes the solution paths piecewise linear? Is it
the use of squared error loss? Or the Lasso penalty? The answer is that both play
an important role. However, the family of (loss, penalty) pairs which facilitates
piecewise linear solution paths turns out to contain many other interesting and
useful optimization problems.
We now brieﬂy review our results, presented in detail in Rosset and Zhu .
Consider the general regularized optimization problem
ˆβ(λ) = argmin
iβ) + λJ(β),
where we only assume that the loss L and the penalty J are both convex functions
of β for any sample {xt
i=1. For our discussion, the data sample is assumed
ﬁxed, and so we will use the notation L(β), where the dependence on the data is
Notice that piecewise linearity is equivalent to requiring that
LEAST ANGLE REGRESSION
is piecewise constant as a function of λ. If L, J are twice differentiable functions
of β, then it is easy to derive that
∇2L( ˆβ(λ)) + λ∇2J( ˆβ(λ))
With a little more work we extend this result to “almost twice differentiable”
loss and penalty functions (i.e., twice differentiable everywhere except at a ﬁnite
number of points), which leads us to the following sufﬁcient conditions for
piecewise linear ˆβ(λ):
1. ∇2L( ˆβ(λ)) is piecewise constant as a function of λ. This condition is met if
L is a piecewise-quadratic function of β. This class includes the squared error
loss of the Lasso, but also absolute loss and combinations of the two, such as
Huber’s loss.
2. ∇J( ˆβ(λ)) is piecewise constant as a function of λ. This condition is met if J is
a piecewise-linear function of β. This class includes the l1 penalty of the Lasso,
but also the l∞norm penalty.
2.1. Examples.
Our ﬁrst example is the “Huberized” Lasso; that is, we use the
(y −xtβ)2,
if |y −xtβ| ≤δ,
δ2 + 2δ(|y −xβ| −δ),
otherwise,
with the Lasso penalty. This loss is more robust than squared error loss against
outliers and long-tailed residual distributions, while still allowing us to calculate
the whole path of regularized solutions efﬁciently.
To illustrate the importance of robustness in addition to regularization, consider
the following simple simulated example: take n = 100 observations and p = 80
predictors, where all xij are i.i.d. N(0,1) and the true model is
yi = 10 · xi1 + εi,
∼0.9 · N(0,1) + 0.1 · N(0,100).
So the normality of residuals, implicitly assumed by using squared error loss, is
Figure 1 shows the optimal coefﬁcient paths ˆβ(λ) for the Lasso (right) and
“Huberized” Lasso, using δ = 1 (left). We observe that the Lasso fails in
identifying the correct model E(Y|x) = 10x1 while the robust loss identiﬁes it
almost exactly, if we choose the appropriate regularization parameter.
As a second example, consider a classiﬁcation scenario where the loss we use
depends on the margin yxtβ rather than on the residual. In particular, consider the
1-norm support vector machine regularized optimization problem, popular in the
DISCUSSION
Coefﬁcient paths for Huberized Lasso (left) and Lasso (right) for data example: ˆβ1(λ) is
the full line; the true model is E(Y|x) = 10x1.
machine learning community. It consists of minimizing the “hinge loss” with a
Lasso penalty:
L(y,xtβ) =
(1 −yxtβ),
if yxtβ ≤1,
otherwise.
This problem obeys our conditions for piecewise linearity, and so we can
generate all regularized solutions for this ﬁtting problem efﬁciently. This is
particularly advantageous in high-dimensional machine learning problems, where
regularization is critical, and it is usually not clear in advance what a good
regularization parameter would be. A detailed discussion of the computational
and methodological aspects of this example appears in Zhu, Rosset, Hastie, and
Tibshirani .
3. Relationship between “boosting” algorithms and l1-regularized ﬁtting.
The discussion paper establishes the close relationship between ε-stagewise linear
regression and the Lasso. Figure 1 in that paper illustrates the near-equivalence in
LEAST ANGLE REGRESSION
the solution paths generated by the two methods, and Theorem 2 formally states a
related result. It should be noted, however, that their theorem falls short of proving
the global relation between the methods, which the examples suggest.
In Rosset, Zhu and Hastie we demonstrate that this relation between the
path of l1-regularized optimal solutions [which we have denoted above by ˆβ(λ)]
and the path of “generalized” ε-stagewise (AKA boosting) solutions extends
beyond squared error loss and in fact applies to any convex differentiable loss.
More concretely, consider the following generic gradient-based “ε-boosting”
algorithm [we follow Friedman and Mason, Baxter, Bartlett and Frean
 in this view of boosting], which iteratively builds the solution path β(t):
ALGORITHM 1 (Generic gradient-based boosting algorithm).
1. Set β(0) = 0.
2. For t = 1 : T ,
(a) Let jt = argmaxj |∂
(b) Set β(t)
jt = β(t−1)
−ε sign(∂
) and β(t)
, k ̸= jt.
This is a coordinate descent algorithm, which reduces to forward stagewise, as
deﬁned in the discussion paper, if we take the loss to be squared error loss:
iβ(t−1)) = (yi −xt
iβ(t−1))2. If we take the loss to be the exponential loss,
iβ(t−1) = exp
we get a variant of AdaBoost [Freund and Schapire ]—the original and most
famous boosting algorithm.
Figure 2 illustrates the equivalence between Algorithm 1 and the optimal
solution path for a simple logistic regression example, using ﬁve variables from
the “spam” dataset. We can see that there is a perfect equivalence between the
regularized solution path (left) and the “boosting” solution path (right).
In Rosset, Zhu and Hastie we formally state this equivalence, with
the required conditions, as a conjecture. We also generalize the weaker result,
proven by the discussion paper for the case of squared error loss, to any convex
differentiable loss.
This result is interesting in the boosting context because it facilitates a view
of boosting as approximate and implicit regularized optimization. The situations
in which boosting is employed in practice are ones where explicitly solving
regularized optimization problems is not practical (usually very high-dimensional
predictor spaces). The approximate regularized optimization view which emerges
from our results allows us to better understand boosting and its great empirical
success [Breiman ]. It also allows us to derive approximate convergence
results for boosting.
DISCUSSION
Exact coefﬁcient paths (left) for l1-constrained logistic regression and boosting coefﬁcient
paths (right) with binomial log-likelihood loss on ﬁve variables from the “spam” dataset. The
boosting path was generated using ε = 0.003 and 7000 iterations.
4. Conclusion.
The computational and theoretical results of the discussion
paper shed new light on variable selection and regularization methods for linear
regression. However, we believe that variants of these results are useful and
applicable beyond that domain. We hope that the two extensions that we have
presented convey this message successfully.
Acknowledgment.
We thank Giles Hooker for useful comments.
REFERENCES
BREIMAN, L. . Prediction games and arcing algorithms. Neural Computation 11 1493–1517.
FREUND, Y. and SCHAPIRE, R. E. . A decision-theoretic generalization of on-line learning
and an application to boosting. J. Comput. System Sci. 55 119–139.
FRIEDMAN, J. H. . Greedy function approximation: A gradient boosting machine. Ann.
Statist. 29 1189–1232.
LEAST ANGLE REGRESSION
MASON, L., BAXTER, J., BARTLETT, P. and FREAN, M. . Boosting algorithms as gradient
descent. In Advances in Neural Information Processing Systems 12 512–518. MIT Press,
Cambridge, MA.
ROSSET, S. and ZHU, J. . Piecewise linear regularized solution paths. Advances in Neural
Information Processing Systems 16. To appear.
ROSSET, S., ZHU, J. and HASTIE, T. . Boosting as a regularized path to a maximum margin
classiﬁer. Technical report, Dept. Statistics, Stanford Univ.
ZHU, J., ROSSET, S., HASTIE, T. and TIBSHIRANI, R. . 1-norm support vector machines.
Neural Information Processing Systems 16. To appear.
IBM T. J. WATSON RESEARCH CENTER
P.O. BOX 218
YORKTOWN HEIGHTS, NEW YORK 10598
E-MAIL: 
DEPARTMENT OF STATISTICS
UNIVERSITY OF MICHIGAN
550 EAST UNIVERSITY
ANN ARBOR, MICHIGAN 48109-1092
E-MAIL: 
DISCUSSION
BY ROBERT A. STINE
University of Pennsylvania
I have enjoyed reading the work of each of these authors over the years, so
it is a real pleasure to have this opportunity to contribute to the discussion of
this collaboration. The geometry of LARS furnishes an elegant bridge between
the Lasso and Stagewise regression, methods that I would not have suspected to
be so related. Toward my own interests, LARS offers a rather different way to
construct a regression model by gradually blending predictors rather than using
a predictor all at once. I feel that the problem of “automatic feature generation”
(proposing predictors to consider in a model) is a current challenge in building
regression models that can compete with those from computer science, and LARS
suggests a new approach to this task. In the examples of Efron, Hastie, Johnstone
and Tibshirani (EHJT) (particularly that summarized in their Figure 5), LARS
produces models with smaller predictive error than the old workhorse, stepwise
regression. Furthermore, as an added bonus, the code supplied by the authors runs
faster for me than the step routine for stepwise regression supplied with R, the
generic version of S-PLUS that I use.
My discussion focuses on the use of Cp to choose the number of predictors.
The bootstrap simulations in EHJT show that LARS reaches higher levels of
“proportion explained” than stepwise regression. Furthermore, the goodness-of-
ﬁt obtained by LARS remains high over a wide range of models, in sharp contrast
to the narrow peak produced by stepwise selection. Because the cost of overﬁtting
with LARS appears less severe than with stepwise, LARS would seem to have a
clear advantage in this respect. Even if we do overﬁt, the ﬁt of LARS degrades
DISCUSSION
only slightly. The issue becomes learning how much LARS overﬁts, particularly
in situations with many potential predictors (m as large as or larger than n).
To investigate the model-selection aspects of LARS further, I compared LARS
to stepwise regression using a “reversed” ﬁve-fold cross-validation. The crossvalidation is reversed in the sense that I estimate the models on one fold
(88 observations) and then predict the other four. This division with more set aside
for validation than used in estimation offers a better comparison of models. For
example, Shao shows that one needs to let the proportion used for validation
grow large in order to get cross validation to ﬁnd the right model. This reversed
design also adds a further beneﬁt of making the variable selection harder. The
quadratic model ﬁtted to the diabetes data in EHJT selects from m = 64 predictors
using a sample of n = 442 cases, or about 7 cases per predictor. Reversed crossvalidation is closer to a typical data-mining problem. With only one fold of 88
observations to train the model, observation noise obscures subtle predictors.
Also, only a few degrees of freedom remain to estimate the error variance σ 2
that appears in Cp [equation (4.5)]. Because I also wanted to see what happens
when m > n, I repeated the cross-validation with 5 additional possible predictors
added to the 10 in the diabetes data. These 5 spurious predictors were simulated
i.i.d. Gaussian noise; one can think of them as extraneous predictors that one
might encounter when working with an energetic, creative colleague who suggests
many ideas to explore. With these 15 base predictors, the search must consider
 + 14 = 134 possible predictors.
Here are a few details of the cross-validation. To obtain the stepwise results,
I ran forward stepwise using the hard threshold 2logm, which is also known as
the risk inﬂation criterion (RIC) [Donoho and Johnstone and Foster and
George ]. One begins with the most signiﬁcant predictor. If the squared
t-statistic for this predictor, say t2
(1), is less than the threshold 2logm, then the
search stops, leaving us with the “null” model that consists of just an intercept.
If instead t2
(1) ≥2logm, the associated predictor, say X(1), joins the model and
the search moves on to the next predictor. The second predictor X(2) joins the
model if t2
(2) ≥2logm; otherwise the search stops with the one-predictor model.
The search continues until reaching a predictor whose t-statistic fails this test,
(q+1) < 2logm, leaving a model with q predictors. To obtain the results for LARS,
I picked the order of the ﬁt by minimizing Cp. Unlike the forward, sequential
stepwise search, LARS globally searches a collection of models up to some large
size, seeking the model which has the smallest Cp. I set the maximum model
size to 50 (for m = 64) or 64 (for m = 134). In either case, the model is chosen
from the collection of linear and quadratic effects in the 10 or 15 basic predictors.
Neither search enforces the principle of marginality; an interaction can join the
model without adding main effects.
I repeated the ﬁve-fold cross validation 20 times, each time randomly partitioning the 442 cases into 5 folds. This produces 100 stepwise and LARS ﬁts. For each
LEAST ANGLE REGRESSION
Five-fold cross-validation of the prediction error and size of stepwise regression and LARS
when ﬁtting models to a collection of 64 (left) or 134 predictors (right). LARS ﬁts chosen by Cp overﬁt
and have larger RMSE than stepwise; with Cp replaced by the alternative criterion Sp deﬁned in (3),
the LARS ﬁts become more parsimonious with smaller RMSE. The random splitting into estimation
and validation samples was repeated 20 times, for a total of 100 stepwise and LARS ﬁts.
of these, I computed the square root of the out-of-sample mean square error (MSE)
when the model ﬁt on one fold was used to predict the held-back 354 [= 4(88)+2]
observations. I also saved the size q for each ﬁt.
Figure 1 summarizes the results of the cross-validation. The comparison
boxplots on the left compare the square root of the MSE (top) and selected
model order (bottom) of stepwise to LARS when picking from m = 64 candidate
predictors; those on the right summarize what happens with m = 134. When
choosing from among 64 predictors, the median size of a LARS model identiﬁed
by Cp is 39. The median stepwise model has but 2 predictors. (I will describe the
Sp criterion further below.) The effects of overﬁtting on the prediction error of
LARS are clear: LARS has higher RMSE than stepwise. The median RMSE for
stepwise is near 62. For LARS, the median RMSE is larger, about 78. Although the
predictive accuracy of LARS declines more slowly than that of stepwise when it
DISCUSSION
overﬁts (imagine the ﬁt of stepwise with 39 predictors), LARS overﬁts by enough
in this case that it predicts worse than the far more parsimonious stepwise models.
With more predictors (m = 134), the boxplots on the right of Figure 1 show that
Cp tends to pick the largest possible model—here a model with 64 predictors.
Why does LARS overﬁt? As usual with variable selection in regression, it is
simpler to try to understand when thinking about the utopian orthogonal regression
with known σ 2. Assume, as in Lemma 1 of EHJT, that the predictors Xj are the
columns of an identity matrix, Xj = ej = (0,...,0,1j,0,...,0). Assume also
that we know σ 2 = 1 and use it in place of the troublesome σ 2 in Cp, so that for
this discussion
Cp = RSS(p) −n + 2p.
To deﬁne RSS(p) in this context, denote the ordered values of the response as
(2) > ··· > Y 2
The soft thresholding summarized in Lemma 1 of EHJT implies that the residual
sum-of-squares of LARS with q predictors is
RSS(q) = (q + 1)Y 2
Consequently, the drop in Cp when going from the model with q to the model with
q + 1 predictors is
Cq −Cq+1 = (q + 1)dq −2,
(q+1) −Y 2
Cp adds Xq+1 to the model if Cq −Cq+1 > 0.
This use of Cp works well for the orthogonal “null” model, but overﬁts when
the model includes much signal. Figure 2 shows a graph of the mean and standard
deviation of RSS(q) −RSS(0) + 2q for an orthogonal model with n = m = 100
∼N(0,1). I subtracted RSS(0) rather than n to reduce the variation in
the simulation. Figure 3 gives a rather different impression. The simulation is
identical except that the data have some signal. Now, EYi = 3 for i = 1,...,5.
The remaining 95 observations are N(0,1). The “true” model has only 5 nonzero
components, but the minimal expected Cp falls near 20.
This stylized example suggests an explanation for the overﬁtting—as well as
motivates a way to avoid some of it. Consider the change in RSS for a null model
when adding the sixth predictor. For this step, RSS(5) −RSS(6) = 6(Y 2
Even though we multiply the difference between the squares by 6, adjacent order
statistics become closer when removed from the extremes, and Cp tends to increase
LEAST ANGLE REGRESSION
A simulation of Cp for LARS applied to orthogonal, normal data with no signal correctly
identiﬁes the null model. These results are from a simulation with 1000 replications, each consisting
of a sample of 100 i.i.d. standard normal observations. Error bars indicate ±1 standard deviation.
as shown in Figure 2. The situation changes when signal is present. First, the ﬁve
observations with mean 3 are likely to be the ﬁrst ﬁve ordered observations. So,
their spacing is likely to be larger because their order is determined by a sample of
ﬁve normals; Cp adds these. When reaching the noise, the difference Y 2
now behaves like the difference between the ﬁrst two squared order statistics in an
A simulation of Cp for LARS applied to orthogonal, normal data with signal present
overﬁts. Results are from a simulation with 1000 replications, each consisting of 5 observations
with mean 3 combined with a sample of 95 i.i.d. standard normal observations. Error bars indicate
±1 standard deviation.
DISCUSSION
i.i.d. sample of 95 standard normals. Consequently, this comparison involves the
gap between the most extreme order statistics rather than those from within the
sample, and as a result Cp drops to indicate a larger model.
This explanation of the overﬁtting suggests a simple alternative to Cp that leads
to smaller LARS models. The idea is to compare the decreasing residual sum of
squares RSS(q) to what is expected under a model that has ﬁtted some signal and
some noise. Since overﬁtting seems to have relatively benign effects on LARS, one
does not want to take the hard-thresholding approach; my colleague Dean Foster
suggested that the criterion might do better by assuming that some of the predictors
already in the model are really noise. The criterion Sp suggested here adopts
this notion. The form of Sp relies on approximations for normal order statistics
commonly used in variable selection, particularly adaptive methods [Benjamini
and Hochberg and Foster and Stine ]. These approximate the size
of the jth normal order statistic in a sample of n with √2log(n/j). To motivate
the form of the Sp criterion, I return to the orthogonal situation and consider what
happens when deciding whether to increase the size of the model from q to q + 1
predictors. If I know that k of the already included q predictors represent signal
and the rest of the predictors are noise, then dq = Y 2
(q+1) −Y 2
(q+2) is about
q + 1 −k −2log
Since I do not know k, I will just set k = q/2 (i.e., assume that half of those already
in the model are noise) and approximate dq as
δ(q) = 2log q/2 + 2
[Deﬁne δ(0) = 2log2 and δ(1) = 2log3/2.] This approximation suggests choosing the model that minimizes
Sq = RSS(q) + ˆσ 2
where ˆσ 2 represents an “honest” estimate of σ 2 that avoids selection bias. The
Sp criterion, like Cp, penalizes the residual sum-of-squares, but uses a different
The results for LARS with this criterion deﬁne the third set of boxplots in
Figure 1. To avoid selection bias in the estimate of σ 2, I used a two-step procedure.
First, ﬁt a forward stepwise regression using hard thresholding. Second, use the
estimated error variance from this stepwise ﬁt as ˆσ 2 in Sp and proceed with
LARS. Because hard thresholding avoids overﬁtting in the stepwise regression,
the resulting estimator ˆσ 2 is probably conservative—but this is just what is needed
when modeling data with an excess of possible predictors. If the variance estimate
from the largest LARS model is used instead, the Sp criterion also overﬁts (though
LEAST ANGLE REGRESSION
not so much as Cp). Returning to Figure 1, the combination of LARS with Sp
obtains the smallest typical MSE with both m = 64 and 134 predictors. In either
case, LARS includes more predictors than the parsimonious stepwise ﬁts obtained
by hard thresholding.
These results lead to more questions. What are the risk properties of the LARS
predictor chosen by Cp or Sp? How is it that the number of possible predictors m
does not appear in either criterion? This deﬁnition of Sp simply supposes half
of the included predictors are noise; why half? What is a better way to set k
in (2)? Finally, that the combination of LARS with either Cp or Sp has less MSE
than stepwise when predicting diabetes is hardly convincing that such a pairing
would do well in other applications. Statistics would be well served by having
a repository of test problems comparable to those held at UC Irvine for judging
machine learning algorithms [Blake and Merz ].
REFERENCES
BENJAMINI, Y. and HOCHBERG, Y. . Controlling the false discovery rate: A practical and
powerful approach to multiple testing. J. Roy. Statist. Soc. Ser. B 57 289–300.
BLAKE, C. and MERZ, C. . UCI repository of machine learning databases. Technical
report, School Information and Computer Science, Univ. California, Irvine. Available at
www.ics.uci.edu/˜mlearn/MLRepository.html.
DONOHO, D. L. and JOHNSTONE, I. M. . Ideal spatial adaptation by wavelet shrinkage.
Biometrika 81 425–455.
FOSTER, D. P. and GEORGE, E. I. . The risk inﬂation criterion for multiple regression. Ann.
Statist. 22 1947–1975.
FOSTER, D. P. and STINE, R. A. . Variable selection via information theory. Technical Report
Discussion Paper 1180, Center for Mathematical Studies in Economics and Management
Science, Northwestern Univ.
SHAO, J. . Linear model selection by cross-validation. J. Amer. Statist. Assoc. 88 486–494.
DEPARTMENT OF STATISTICS
THE WHARTON SCHOOL
UNIVERSITY OF PENNSYLVANIA
PHILADELPHIA, PENNSYLVANIA 19104-6340
E-MAIL: 
DISCUSSION
BY BERWIN A. TURLACH
University of Western Australia
I would like to begin by congratulating the authors (referred to below as EHJT)
for their interesting paper in which they propose a new variable selection method
DISCUSSION
(LARS) for building linear models and show how their new method relates to other
methods that have been proposed recently. I found the paper to be very stimulating
and found the additional insight that it provides about the Lasso technique to be of
particular interest.
My comments center around the question of how we can select linear models
that conform with the marginality principle [Nelder and McCullagh
and Nelder ]; that is, the response surface is invariant under scaling and
translation of the explanatory variables in the model. Recently one of my interests
was to explore whether the Lasso technique or the nonnegative garrote [Breiman
 ] could be modiﬁed such that it incorporates the marginality principle.
However, it does not seem to be a trivial matter to change the criteria that these
techniques minimize in such a way that the marginality principle is incorporated
in a satisfactory manner.
On the other hand, it seems to be straightforward to modify the LARS technique
to incorporate this principle. In their paper, EHJT address this issue somewhat in
passing when they suggest toward the end of Section 3 that one ﬁrst ﬁt main effects
only and interactions in a second step to control the order in which variables
are allowed to enter the model. However, such a two-step procedure may have
a somewhat less than optimal behavior as the following, admittedly artiﬁcial,
example shows.
Assume we have a vector of explanatory variables X = (X1,X2,...,X10)
where the components are independent of each other and Xi, i = 1,...,10, follows
a uniform distribution on . Take as model
Y = (X1 −0.5)2 + X2 + X3 + X4 + X5 + ε,
where ε has mean zero, has standard deviation 0.05 and is independent of X.
It is not difﬁcult to verify that in this model X1 and Y are uncorrelated.
Moreover, since the Xi’s are independent, X1 is also uncorrelated with any residual
vector coming from a linear model formed only by explanatory variables selected
from {X2,...,X10}.
Thus, if one ﬁts a main effects only model, one would expect that the LARS
algorithm has problems identifying that X1 should be part of the model. That this
is indeed the case is shown in Figure 1. The picture presents the result of the
LARS analysis for a typical data set generated from model (1); the sample size
was n = 500. Note that, unlike Figure 3 in EHJT, Figure 1 (and similar ﬁgures
below) has been produced using the standardized explanatory variables and no
back-transformation to the original scale was done.
For this realization, the variables are selected in the sequence X2, X5, X4, X3,
X6, X10, X7, X8, X9 and, ﬁnally, X1. Thus, as expected, the LARS algorithm has
problems identifying X1 as part of the model. To further verify this, 1000 different
data sets, each of size n = 500, were simulated from model (1) and a LARS
analysis performed on each of them. For each of the 10 explanatory variables the
LEAST ANGLE REGRESSION
LARS analysis of simulated data with main terms only: (left) estimates of regression
coefﬁcients
ˆβj , j = 1,... ,10, plotted versus | ˆβj|; (right) absolute current correlations as
functions of LARS step.
step at which it was selected to enter the model was recorded. Figure 2 shows for
each of the variables the (percentage) histogram of these data.
It is clear that the LARS algorithm has no problems identifying that X2,...,X5
should be in the model. These variables are all selected in the ﬁrst four steps and,
not surprisingly given the model, with more or less equal probability at any of these
Percentage histogram of step at which each variable is selected based on 1000 replications:
results shown for LARS analysis using main terms only.
DISCUSSION
LARS analysis of simulated data with main terms and interaction terms: (left) estimates of
regression coefﬁcients ˆβj , j = 1,...,65, plotted versus | ˆβj|; (right) absolute current correlations
as functions of LARS step.
steps. X1 has a chance of approximately 25% of being selected as the ﬁfth variable,
otherwise it may enter the model at step 6, 7, 8, 9 or 10 (each with probability
roughly 15%). Finally, each of the variables X6 to X10 seems to be selected with
equal probability anywhere between step 5 and step 10.
This example shows that a main effects ﬁrst LARS analysis followed by a
check for interaction terms would not work in such cases. In most cases the LARS
analysis would miss X1 as ﬁfth variable and even in the cases where it was selected
at step 5 it would probably be deemed to be unimportant and excluded from further
How does LARS perform if one uses from the beginning all 10 main effects and
all 55 interaction terms? Figure 3 shows the LARS analysis for the same data used
to produce Figure 1 but this time the design matrix was augmented to contain all
main effects and all interactions. The order in which the variables enter the model
is X2: 5 = X2 × X5, X2: 4, X3: 4, X2: 3, X3: 5, X4: 5, X5: 5 = X2
5, X4, X3, X2, X5,
X4: 4, X1: 1, X1: 6, X1: 9, X1, . . . . In this example the last of the six terms that are
actually in model (1) was selected by the LARS algorithm in step 16.
Using the same 1000 samples of size n = 500 as above and performing a
LARS analysis on them using a design matrix with all main and interaction
terms shows a surprising result. Again, for each replication the step at which a
variable was selected into the model by LARS was recorded and Figure 4 shows
for each variable histograms for these data. To avoid cluttering, the histograms in
Figure 4 were truncated to ; the complete histograms are shown on the left
in Figure 7.
The most striking feature of these histograms is that the six interaction terms
Xi:j , i,j ∈{2,3,4,5}, i < j, were always selected ﬁrst. In no replication was any
LEAST ANGLE REGRESSION
Percentage histogram of step at which each variable is selected based on 1000 replications:
results shown for variables selected in the ﬁrst 20 steps of a LARS analysis using main and interaction
of these terms selected after step 6 and no other variable was ever selected in the
ﬁrst six steps. That one of these terms is selected as the ﬁrst term is not surprising
as these variables have the highest correlation with the response variable Y . It
can be shown that the covariance of these interaction terms with Y is by a factor
√12/7 ≈1.3 larger than the covariance between Xi and Y for i = 2,...,5. But
that these six interaction terms dominate the early variable selection steps in such
a manner came as a bit as a surprise.
After selecting these six interaction terms, the LARS algorithm then seems
to select mostly X2, X3, X4 and X5, followed soon by X1: 1 and X1. However,
especially the latter one seems to be selected rather late and other terms may be
selected earlier. Other remarkable features in Figure 4 are the peaks in histograms
of Xi : i for i = 2,3,4,5; each of these terms seems to have a fair chance of being
selected before the corresponding main term and before X1: 1 and X1.
One of the problems seems to be the large number of interaction terms that the
LARS algorithm selects without putting the corresponding main terms into the
model too. This behavior violates the marginality principle. Also, for this model,
one would expect that ensuring that for each higher-order term the corresponding
lower-order terms are in the model too would alleviate the problem that the six
interaction terms Xi : j, i,j ∈{2,3,4,5}, i < j, are always selected ﬁrst.
DISCUSSION
I give an alternative description of the LARS algorithm ﬁrst before I show how it
can be modiﬁed to incorporate the marginality principle. This description is based
on the discussion in EHJT and shown in Algorithm 1.
ALGORITHM 1 (An alternative description of the LARS algorithm).
1. Set ˆµ0 = 0 and k = 0.
Calculate ˆc = X′(y −ˆµk) and set ˆC = maxj{|ˆcj|}.
Let A = {j :|ˆcj| = ˆC}.
Set XA = (···xj ···)j∈A for calculating ¯yk+1 = (X′
Ay and a =
A(¯yk+1 −ˆµk).
ˆµk+1 = ˆµk + ˆγ (¯yk+1 −ˆµk),
where, if Ac ̸= ∅,
otherwise set ˆγ = 1.
8. until Ac = ∅.
We start with an estimated response ˆµ0 = 0 and then iterate until all variables
have been selected. In each iteration, we ﬁrst determine (up to a constant factor)
the correlation between all variables and the current residual vector. All variables
whose absolute correlation with the residual vector equals the maximal achievable
absolute correlation are chosen to be in the model and we calculate the least
squares regression response, say ¯yk+1, using these variables. Then we move from
our current estimated response ˆµk toward ¯yk+1 until a new variable would enter
the model.
Using this description of the LARS algorithm, it seems obvious how to modify
the algorithm such that it respects the marginality principle. Assume that for each
column i of the design matrix we set dij = 1 if column j should be in the model
whenever column i is in the model and zero otherwise; here j ̸= i takes values in
{1,...,m}, where m is the number of columns of the design matrix. For example,
abusing this notation slightly, for model (1) we might set d1: 1,1 = 1 and all other
d1: 1,j = 0; or d1: 2,1 = 1, d1: 2,2 = 1 and all other d1: 2,j equal to zero.
Having deﬁned such a dependency structure between the columns of the design
matrix, the obvious modiﬁcation of the LARS algorithm is that when adding,
say, column i to the selected columns one also adds all those columns for which
dij = 1. This modiﬁcation is described in Algorithm 2.
LEAST ANGLE REGRESSION
ALGORITHM 2 (The modiﬁed LARS algorithm).
1. Set ˆµ0 = 0 and k = 0.
Calculate ˆc = X′(y −ˆµk) and set ˆC = maxj{|ˆcj|}.
Let A0 = {j :|ˆcj| = ˆC}, A1 = {j :dij ̸= 0,i ∈A0} and A = A0 ∪A1.
Set XA = (···xj ···)j∈A for calculating ¯yk+1 = (X′
Ay and a =
A(¯yk+1 −ˆµk).
ˆµk+1 = ˆµk + ˆγ (¯yk+1 −ˆµk),
where, if Ac ̸= ∅,
otherwise set ˆγ = 1.
8. until Ac = ∅.
Note that compared with the original Algorithm 1 only the fourth line changes.
Furthermore, for all i ∈A it is obvious that for 0 ≤γ ≤1 we have
|ˆci(γ )| = (1 −γ )|ˆci|,
where ˆc(γ ) = X′(y −ˆµ(γ )) and ˆµ(γ ) = ˆµk + γ (¯yk+1 −ˆµk).
Note that, by deﬁnition, the value of |ˆcj| is the same for all j ∈A0. Hence, the
functions (2) for those variables are identical, namely (1−γ ) ˆC, and for all j ∈A1
the corresponding functions |ˆcj(γ )| will intersect (1−γ ) ˆC at γ = 1. This explains
why in line 6 we only have to check for the ﬁrst intersection between (1 −γ ) ˆC
and |ˆcj(γ )| for j ∈Ac.
It also follows from (2) that, for all j ∈A0, we have
j(¯yk+1 −ˆµk) = sign(ˆcj) ˆC.
Thus, for those variables that are in A0 we move in line 6 of the modiﬁed algorithm
in a direction that has a similar geometric interpretation as the direction along
which we move in the original LARS algorithm. Namely that for each j ∈A0 the
angle between the direction in which we move and sign(ˆcj)xj is the same and this
angle is less than 90◦.
Figure 5 shows the result of the modiﬁed LARS analysis for the same data
used above. Putting variables that enter the model simultaneously into brackets,
the order in which the variables enter the model is (X2: 5, X2, X5), (X3: 4, X3,
X4), X2: 5, X2: 3, (X1: 1,X1),.... That is, the modiﬁed LARS algorithm selects
in this case in ﬁve steps a model with 10 terms, 6 of which are the terms that are
indeed in model (1).
DISCUSSION
Modiﬁed LARS analysis of simulated data with main terms and interaction terms: (left)
estimates of regression coefﬁcients ˆβj , j = 1,...,65, plotted versus | ˆβj |; (right) absolute current
correlations as functions of k = #Ac.
Using the same 1000 samples of size n = 500 as above and performing
a modiﬁed LARS analysis on them using a design matrix with all main
and interaction terms also shows markedly improved results. Again, for each
replication the step at which a variable was selected into the model was recorded
and Figure 6 shows for each variable histograms for these data. To avoid cluttering,
the histograms in Figure 6 were truncated to ; the complete histograms are
shown on the right in Figure 7.
From Figure 6 it can be seen that now the variables X2, X3, X4 and X5
are all selected within the ﬁrst three iterations of the modiﬁed LARS algorithm.
Also X1: 1 and X1 are picked up consistently and early. Compared with Figure 4
there are marked differences in the distribution of when the variable is selected
for the interaction terms Xi:j , i,j ∈{2,3,4,5}, i ≤j, and the main terms Xi,
i = 6,...,10. The latter can be explained by the fact that the algorithm now
enforces the marginality principle. Thus, it seems that this modiﬁcation does
improve the performance of the LARS algorithm for model (1). Hopefully it would
do so also for other models.
In conclusion, I offer two further remarks and a question. First, note that the
modiﬁed LARS algorithm may also be used to incorporate factor variables with
more than two levels. In such a situation, I would suggest that indicator variables
for all levels are included in the initial design matrix; but this would be done mainly
to easily calculate all the correlations. The dependencies dij would be set up such
that if one indicator variable is selected, then all enter the model. However, to avoid
redundancies one would only put all but one of these columns into the matrix XA.
This would also avoid that XA would eventually become singular if more than one
explanatory variable is a factor variable.
LEAST ANGLE REGRESSION
Percentage histogram of step at which each variable is selected based on 1000 replications:
results shown for variables selected in the ﬁrst 20 steps of a modiﬁed LARS analysis using main and
interaction terms.
Second, given the insight between the LARS algorithm and the Lasso algorithm
described by EHJT, namely the sign constraint (3.1), it now seems also possible
to modify the Lasso algorithm to incorporate the marginality principle by
incorporating the sign constraint into Algorithm 2. However, whenever a variable
Percentage histogram of step at which each variable is selected based on 1000 replications:
(left) LARS analysis; (right) modiﬁed LARS analysis.
DISCUSSION
would be dropped from the set A0 due to violating the sign constraint there might
also be variables dropped from A1. For the latter variables these might introduce
discontinuities in the traces of the corresponding parameter estimates, a feature that
does not seem to be desirable. Perhaps a better modiﬁcation of the Lasso algorithm
that incorporates the marginality principle can still be found?
Finally, the behavior of the LARS algorithm for model (1) when all main terms
and interaction terms are used surprised me a bit. This behavior seems to raise a
fundamental question, namely, although we try to build a linear model and, as we
teach our students, correlation “measures the direction and strength of the linear
relationship between two quantitative variables” [Moore and McCabe ], one
has to wonder whether selecting variables using correlation as a criterion is a sound
principle? Or should we modify the algorithms to use another criterion?
REFERENCES
BREIMAN, L. . Better subset regression using the nonnegative garrote. Technometrics 37
MCCULLAGH, P. and NELDER, J. A. . Generalized Linear Models, 2nd ed. Chapman and
Hall, London.
MOORE, D. S. and MCCABE, G. P. . Introduction to the Practice of Statistics, 3rd ed.
Freeman, New York.
NELDER, J. A. . A reformulation of linear models (with discussion). J. Roy. Statist. Soc. Ser. A
140 48–76.
NELDER, J. A. . The statistics of linear models: Back to basics. Statist. Comput. 4 221–234.
SCHOOL OF MATHEMATICS AND STATISTICS
UNIVERSITY OF WESTERN AUSTRALIA
35 STIRLING HIGHWAY
CRAWLEY WA 6009
E-MAIL: 
DISCUSSION
BY SANFORD WEISBERG1
University of Minnesota
Most of this article concerns the uses of LARS and the two related methods
in the age-old, “somewhat notorious,” problem of “[a]utomatic model-building
algorithms...” for linear regression. In the following, I will conﬁne my comments
to this notorious problem and to the use of LARS and its relatives to solve it.
1Supported by NSF Grant DMS-01-03983.
LEAST ANGLE REGRESSION
1. The implicit assumption.
Suppose the response is y, and we collect
the m predictors into a vector x, the realized data into an n × m matrix X
and the response is the n-vector Y . If P is the projection onto the column
space of (1,X), then LARS, like ordinary least squares (OLS), assumes that,
for the purposes of model building, Y can be replaced by ˆY = P Y without loss
of information. In large samples, this is equivalent to the assumption that the
conditional distributions F(y|x) can be written as
F(y|x) = F(y|x′β)
for some unknown vector β. Efron, Hastie, Johnstone and Tibshirani use this
assumption in the deﬁnition of the LARS algorithm and in estimating residual
variance by ˆσ 2 = ∥(I −P )Y∥2/(n −m −1). For LARS to be reasonable, we
need to have some assurance that this particular assumption holds or that it is
relatively benign. If this assumption is not benign, then LARS like OLS is unlikely
to produce useful results.
A more general alternative to (1) is
F(y|x) = F(y|x′B),
where B is an m × d rank d matrix. The smallest value of d for which (2)
holds is called the structural dimension of the regression problem [Cook ].
An obvious precursor to ﬁtting linear regression is deciding on the structural
dimension, not proceeding as if d = 1. For the diabetes data used in the article, the
R package dr [Weisberg ] can be used to estimate d using any of several
methods, including sliced inverse regression [Li ]. For these data, ﬁtting
these methods suggests that (1) is appropriate.
Expanding x to include functionally related terms is another way to provide
a large enough model that (1) holds. Efron, Hastie, Johnstone and Tibshirani
illustrate this in the diabetes example in which they expand the 10 predictors to 65
including all quadratics and interactions. This alternative does not include (2) as a
special case, as it includes a few models of various dimensions, and this seems to
be much more complex than (2).
Another consequence of assumption (1) is the reliance of LARS, and of OLS,
on correlations. The correlation measures the degree of linear association between
two variables particularly for normally distributed or at least elliptically contoured
variables. This requires not only linearity in the conditional distributions of y
given subsets of the predictors, but also linearity in the conditional distributions
of a′x given b′x for all a and b [see, e.g., Cook and Weisberg ]. When the
variables are not linearly related, bizarre results can follow; see Cook and Weisberg
 for examples. Any method that replaces Y by P Y cannot be sensitive to
nonlinearity in the conditional distributions.
Methods based on P Y alone may be strongly inﬂuenced by outliers and high
leverage cases. As a simple example of this, consider the formula for Cp given by
DISCUSSION
Efron, Hastie, Johnstone and Tibshirani:
Cp( ˆµ) = ∥Y −ˆµ∥2
cov( ˆµi,yi)
Estimating σ 2 by ˆσ 2 = ∥(I −P )Y∥2/(n −m−1), and adapting Weisberg ,
(3) can be rewritten as a sum of n terms, the ith term given by
Cpi( ˆµ) = ( ˆyi −ˆµi)2
+ cov( ˆµi,yi)
hi −cov( ˆµi,yi)
where ˆyi is the ith element of P Y and hi is the ith leverage, a diagonal element
of P . From the simulation reported in the article, a reasonable approximation to the
covariance term is ˆσ 2ui, where ui is the ith diagonal of the projection matrix on
the columns of (1,X) with nonzero coefﬁcients at the current step of the algorithm.
We then get
Cpi( ˆµ) = ( ˆyi −ˆµi)2
+ ui −(hi −ui),
which is the same as the formula given in Weisberg for OLS except that
ˆµi is computed from LARS rather than from a projection. The point here is that
the value of Cpi( ˆµ) depends on the agreement between ˆµi and ˆyi, on the leverage
in the subset model and on the difference in the leverage between the full and
subset models. Neither of these latter two terms has much to do with the problem
of interest, which is the study of the conditional distribution of y given x, but they
are determined by the predictors only.
2. Selecting variables.
Suppose that we can write x = (xa,xu) for some
decomposition of x into two pieces, in which xa represents the “active” predictors
and xu the unimportant or inactive predictors. The variable selection problem is to
ﬁnd the smallest possible xa so that
F(y|x) = F(y|xa)
thereby identifying the active predictors. Standard subset selection methods attack
this problem by ﬁrst assuming that (1) holds, and then ﬁtting models with different
choices for xa, possibly all possible choices or a particular subset of them, and
then using some sort of inferential method or criterion to decide if (4) holds, or
more precisely if
F(y|x) = F(y|γ ′xa)
holds for some γ . Efron, Hastie, Johnstone and Tibshirani criticize the standard
methods as being too greedy: once we put a variable, say, x∗∈xa, then any
predictor that is highly correlated with x∗will never be included. LARS, on the
other hand, permits highly correlated predictors to be used.
LEAST ANGLE REGRESSION
LARS or any other methods based on correlations cannot be much better at
ﬁnding xa than are the standard methods. As a simple example of what can go
wrong, I modiﬁed the diabetes data in the article by adding nine new predictors,
created by multiplying each of the original predictors excluding the sex indicator
by 2.2, and then rounding to the nearest integer. These rounded predictors are
clearly less relevant than are the original predictors, since they are the original
predictors with noise added by the rounding. We would hope that none of these
would be among the active predictors.
Using the S-PLUS functions kindly provided by Efron, Hastie, Johnstone and
Tibshirani, the LARS procedure applied to the original data selects a sevenpredictor model, including, in order, BMI, S5, BP, S3, SEX, S6 and S1. LARS
applied to the data augmented with the nine inferior predictors selects an eightpredictor model, including, in order, BMI, S5, rBP, rS3, BP, SEX, S6 and S1,
where the preﬁx “r” indicates a rounded variable rather than the variable itself.
LARS not only selects two of the inferior rounded variables, but it selects both BP
and its rounded version rBP, effectively claiming that the rounding is informative
with respect to the response.
Inclusion and exclusion of elements in xa depends on the marginal distribution
of x as much as on the conditional distribution of y|x. For example, suppose
that the diabetes data were a random sample from a population. The variables
S3 and S4 have a large sample correlation, and LARS selects one of them, S3, as
an active variable. Suppose a therapy were available that could modify S4 without
changing the value of S3, so in the future S3 and S4 would be nearly uncorrelated.
Although this would arguably not change the distribution of y|x, it would certainly
change the marginal distribution of x, and this could easily change the set of active
predictors selected by LARS or any other method that starts with correlations.
A characteristic that LARS shares with the usual methodology for subset
selection is that the results are invariant under rescaling of any individual predictor,
but not invariant under reparameterization of functionally related predictors. In
the article, the authors create more predictors by ﬁrst rescaling predictors to have
zero mean and common standard deviation, and then adding all possible crossproducts and quadratics to the existing predictors. For this expanded deﬁnition
of the predictors, LARS selects a 15 variable model, including 6 main-effects,
6 two-factor interactions and 3 quadratics. If we add quadratics and interactions
ﬁrst and then rescale, LARS picks an 8 variable model with 2 main-effects, 6 twofactor interactions, and only 3 variables in common with the model selected by
scaling ﬁrst. If we deﬁne the quadratics and interactions to be orthogonal to the
main-effects, we again get a different result. The lack of invariance with regard to
deﬁnition of functionally related predictors can be partly solved by considering the
functionally related variables simultaneously rather than sequentially. This seems
to be self-defeating, at least for the purpose of subset selection.
3. Summary.
Long-standing problems often gain notoriety because solution
of them is of wide interest and at the same time illusive. Automatic model building
in linear regression is one such problem. My main point is that neither LARS
nor, as near as I can tell, any other automatic method has any hope of solving
this problem because automatic procedures by their very nature do not consider
the context of the problem at hand. I cannot see any solution to this problem that
is divorced from context. Most of the ideas in this discussion are not new, but
I think they bear repeating when trying to understand LARS methodology in the
context of linear regression. Similar comments can be found in Efron and
elsewhere.
REFERENCES
COOK, R. D. . Regression Graphics. Wiley, New York.
COOK, R. D. and WEISBERG, S. . Applied Regression Including Computing and Graphics.
Wiley, New York.
COOK, R. D. and WEISBERG, S. . Graphs in statistical analysis: Is the medium the message?
Amer. Statist. 53 29–37.
EFRON, B. . Discussion of “Statistical modeling: The two cultures,” by L. Breiman. Statist.
Sci. 16 218–219.
LI, K. C. . Sliced inverse regression for dimension reduction (with discussion). J. Amer.
Statist. Assoc. 86 316–342.
WEISBERG, S. . A statistic for allocating Cp to individual cases. Technometrics 23 27–31.
WEISBERG, S. . Dimension reduction regression in R. J. Statistical Software 7. (On-line
journal available at www.jstatsoft.org. The software is available from cran.r-project.org.)
SCHOOL OF STATISTICS
UNIVERSITY OF MINNESOTA
1994 BUFORD AVENUE
ST. PAUL, MINNESOTA 55108
E-MAIL: 
BY BRADLEY EFRON, TREVOR HASTIE, IAIN JOHNSTONE
AND ROBERT TIBSHIRANI
The original goal of this project was to explain the striking similarities
between models produced by the Lasso and Forward Stagewise algorithms, as
exempliﬁed by Figure 1. LARS, the Least Angle Regression algorithm, provided
the explanation and proved attractive in its own right, its simple structure
permitting theoretical insight into all three methods. In what follows “LAR”
will refer to the basic, unmodiﬁed form of Least Angle Regression developed in
Section 2, while “LARS” is the more general version giving LAR, Lasso, Forward
LEAST ANGLE REGRESSION
Stagewise and other variants as in Section 3.4. Here is a summary of the principal
properties developed in the paper:
1. LAR builds a regression model in piecewise linear forward steps, accruing
explanatory variables one at a time; each step is taken along the equiangular
direction between the current set of explanators. The step size is less greedy
than classical forward stepwise regression, smoothly blending in new variables
rather than adding them discontinuously.
2. Simple modiﬁcations of the LAR procedure produce all Lasso and Forward
Stagewise solutions, allowing their efﬁcient computation and showing that
these methods also follow piecewise linear equiangular paths. The Forward
Stagewise connection suggests that LARS-type methods may also be useful
in more general “boosting” applications.
3. The LARS algorithm is computationally efﬁcient; calculating the full set of
LARS models requires the same order of computation as ordinary least squares.
4. A k-step LAR ﬁt uses approximately k degrees of freedom, in the sense
of added prediction error (4.5). This approximation is exact in the case of
orthogonal predictors and is generally quite accurate. It permits Cp-type
stopping rules that do not require auxiliary bootstrap or cross-validation
computations.
5. For orthogonal designs, LARS models amount to a succession of soft
thresholding estimates, (4.17).
All of this is rather technical in nature, showing how one might efﬁciently carry
out a program of automatic model-building (“machine learning”). Such programs
seem increasingly necessary in a scientiﬁc world awash in huge data sets having
hundreds or even thousands of available explanatory variables.
What this paper, strikingly, does not do is justify any of the three algorithms
as providing good estimators in some decision-theoretic sense. A few hints
appear, as in the simulation study of Section 3.3, but mainly we are relying on
recent literature to say that LARS methods are at least reasonable algorithms
and that it is worthwhile understanding their properties. Model selection, the
great underdeveloped region of classical statistics, deserves careful theoretical
examination but that does not happen here. We are not as pessimistic as Sandy
Weisberg about the potential of automatic model selection, but agree that it
requires critical examination as well as (over) enthusiastic algorithm building.
The LARS algorithm in any of its forms produces a one-dimensional path of
prediction vectors going from the origin to the full least-squares solution. (Figures
1 and 3 display the paths for the diabetes data.) In the LAR case we can label the
predictors µ(k), where k is identiﬁed with both the number of steps and the degrees
of freedom. What the ﬁgures do not show is when to stop the model-building
process and report µ back to the investigator. The examples in our paper rather
casually used stopping rules based on minimization of the Cp error prediction
Robert Stine and Hemant Ishwaran raise some reasonable doubts about Cp
minimization as an effective stopping rule. For any one value of k, Cp is an
unbiased estimator of prediction error, so in a crude sense Cp minimization is
trying to be an unbiased estimator of the optimal stopping point kopt. As such it is
bound to overestimate kopt in a large percentage of the cases, perhaps near 100%
if kopt is near zero.
We can try to improve Cp by increasing the df multiplier “2” in (4.5). Suppose
we change 2 to some value mult. In standard normal-theory model building
situations, for instance choosing between linear, quadratic, cubic, ...regression
models, the mult rule will prefer model k + 1 to model k if the relevant t-statistic
mult in absolute value (here we are assuming σ 2 known); mult = 2
amounts to using a rejection rule with α = 16%. Stine’s interesting Sp method
chooses mult closer to 4, α = 5%.
This works ﬁne for Stine’s examples, where kopt is indeed close to zero. We tried
it on the simulation example of Section 3.3. Increasing mult from 2 to 4 decreased
the average selected step size from 31 to 15.5, but with a small increase in actual
squared estimation error. Perhaps this can be taken as support for Ishwaran’s point
that since LARS estimates have a broad plateau of good behavior, one can often
get by with much smaller models than suggested by Cp minimization. Of course
no one example is conclusive in an area as multifaceted as model selection, and
perhaps no 50 examples either. A more powerful theory of model selection is
sorely needed, but until it comes along we will have to make do with simulations,
examples and bits and pieces of theory of the type presented here.
Bayesian analysis of prediction problems tends to favor much bigger choices
of mult. In particular the Bayesian information criterion (BIC) uses mult =
log(sample size). This choice has favorable consistency properties, selecting the
correct model with probability 1 as the sample size goes to inﬁnity. However, it
can easily select too-small models in nonasymptotic situations.
Jean-Michel Loubes and Pascal Massart provide two interpretations using
penalized estimation criteria in the orthogonal regression setting. The ﬁrst uses
the link between soft thresholding and ℓ1 penalties to motivate entropy methods
for asymptotic analysis. The second is a striking perspective on the use of Cp
with LARS. Their analysis suggests that our usual intuition about Cp, derived
from selecting among projection estimates of different ranks, may be misleading in
studying a nonlinear method like LARS that combines thresholding and shrinkage.
They rewrite the LARS-Cp expression (4.5) in terms of a penalized criterion
for selecting among orthogonal projections. Viewed in this unusual way (for the
estimator to be used is not a projection!), they argue that mult in fact behaves
like log(n/k) rather than 2 (in the case of a k-dimensional projection). It is
indeed remarkable that this same model-dependent value of mult, which has
emerged in several recent studies [Foster and Stine , George and Foster
 , Abramovich, Benjamini, Donoho and Johnstone and Birgé and
Massart ], should also appear as relevant for the analysis of LARS. We look
LEAST ANGLE REGRESSION
forward to the further extension of the Birgé–Massart approach to handling these
nondeterministic penalties.
Cross-validation is a nearly unbiased estimator of prediction error and as such
will perform similarly to Cp (with mult = 2). The differences between the two
methods concern generality, efﬁciency and computational ease. Cross-validation,
and nonparametric bootstrap methods such as the 632+ rule, can be applied to
almost any prediction problem. Cp is more specialized, but when it does apply
it gives more efﬁcient estimates of prediction error [Efron ] at almost no
computational cost. It applies here to LAR, at least when m < n, as in David
Madigan and Greg Ridgeway’s example.
We agree with Madigan and Ridgeway that our new LARS algorithm may
provide a boost for the Lasso, making it more useful and attractive for data
analysts. Their suggested extension of LARS to generalized linear models is
interesting. In logistic regression, the L1-constrained solution is not piecewise
linear and hence the pathwise optimization is more difﬁcult. Madigan and
Ridgeway also compare LAR and Lasso to least squares boosting for prediction
accuracy on three real examples, with no one method prevailing.
Saharon Rosset and Ji Zhu characterize a class of problems for which
the coefﬁcient paths, like those in this paper, are piecewise linear. This is a
useful advance, as demonstrated with their robust version of the Lasso, and
the ℓ1-regularized Support Vector Machine. The former addresses some of the
robustness concerns of Weisberg. They also report on their work that strengthens
the connections between ε-boosting and ℓ1-regularized function ﬁtting.
Berwin Turlach’s example with uniform predictors surprised us as well. It turns
out that 10-fold cross-validation selects the model with |β1| ≈45 in his Figure 3
(left panel), and by then the correct variables are active and the interactions
have died down. However, the same problem with 10 times the noise variance
does not recover in a similar way. For this example, if the Xj are uniform
2] rather than , the problem goes away, strongly suggesting that
proper centering of predictors (in this case the interactions, since the original
variables are automatically centered by the algorithm) is important for LARS.
Turlach also suggests an interesting proposal for enforcing marginality, the
hierarchical relationship between the main effects and interactions. In his notation,
marginality says that βi : j can be nonzero only if βi and βj are nonzero. An
alternative approach, more in the “continuous spirit” of the Lasso, would be to
include constraints
|βi : j| ≤min{|βi|,|βj|}.
This implies marginality but is stronger. These constraints are linear and,
according to Rosset and Zhu above, a LARS-type algorithm should be available
for its estimation. Leblanc and Tibshirani used constraints like these for
shrinking classiﬁcation and regression trees.
As Turlach suggests, there are various ways to restate the LAR algorithm,
including the following nonalgebraic purely statistical statement in terms of
repeated ﬁtting of the residual vector r:
1. Start with r = y and βj = 0 ∀j.
2. Find the predictor xj most correlated with r.
3. Increase βj in the direction of the sign of corr(r,xj) until some other
competitor xk has as much correlation with the current residual as does xj.
4. Update r, and move (βj, βk) in the joint least squares direction for the
regression of r on (xj,xk) until some other competitor xℓhas as much
correlation with the current residual.
5. Continue in this way until all predictors have been entered. Stop when
corr(r,xj) = 0 ∀j, that is, the OLS solution.
Traditional forward stagewise would have completed the least-squares step at each
stage; here it would go only a fraction of the way, until the next competitor joins in.
Keith Knight asks whether Forward Stagewise and LAR have implicit criteria
that they are optimizing. In unpublished work with Trevor Hastie, Jonathan Taylor
and Guenther Walther, we have made progress on that question. It can be shown
that the Forward Stagewise procedure does a sequential minimization of the
residual sum of squares, subject to
This quantity is the total L1 arc-length of the coefﬁcient curve β(t). If each
component βj(t) is monotone nondecreasing or nonincreasing, then L1 arc-length
equals the L1-norm 
j |βj|. Otherwise, they are different and L1 arc-length
discourages sign changes in the derivative. That is why the Forward Stagewise
solutions tend to have long ﬂat plateaus. We are less sure of the criterion for LAR,
but currently believe that it uses a constraint of the form 
0 βj(s)ds| ≤A.
Sandy Weisberg, as a ranking expert on the careful analysis of regression problems, has legitimate grounds for distrusting automatic methods. Only foolhardy
statisticians dare to ignore a problem’s context. (For instance it helps to know
that diabetes progression behaves differently after menopause, implying strong
age–sex interactions.) Nevertheless even for a “small” problem like the diabetes
investigation there is a limit to how much context the investigator can provide. After that one is drawn to the use of automatic methods, even if the “automatic” part
is not encapsulated in a single computer package.
In actual practice, or at least in good actual practice, there is a cycle of activity
between the investigator, the statistician and the computer. For a multivariable
prediction problem like the diabetes example, LARS-type programs are a good ﬁrst
step toward a solution, but hopefully not the last step. The statistician examines the
output critically, as did several of our commentators, discussing the results with
LEAST ANGLE REGRESSION
the investigator, who may at this point suggest adding or removing explanatory
variables, and so on, and so on.
Fully automatic regression algorithms have one notable advantage: they permit
an honest evaluation of estimation error. For instance the Cp-selected LAR
quadratic model estimates that a patient one standard deviation above average
on BMI has an increased response expectation of 23.8 points. The bootstrap
analysis (3.16) provided a standard error of 3.48 for this estimate. Bootstrapping,
jackkniﬁng and cross-validation require us to repeat the original estimation
procedure for different data sets, which is easier to do if you know what the original
procedure actually was.
Our thanks go to the discussants for their thoughtful remarks, and to the Editors
for the formidable job of organizing this discussion.