Machine Learning, 28, 211–255 
c⃝1997 Kluwer Academic Publishers. Manufactured in The Netherlands.
Malicious Omissions and Errors in Answers to
Membership Queries
DANA ANGLUIN
 
M¯ARTIN¸ ˇS KRIK¸ IS
 
Department of Computer Science, Yale University, P.O. Box 208285, New Haven, CT 06520
ROBERT H. SLOAN
 
Dept. of Electrical Eng. and Computer Science, 851 S. Morgan St. Rm 1120, University of Illinois at Chicago,
Chicago, IL 60607
GY¨ORGY TUR´AN
 
Dept. of Math., Stat., and Comp. Sci., 851 S. Morgan St. Rm 322, University of Illinois at Chicago, Chicago, IL
60607, Automata Theory Research Group Hungarian Academy of Sciences, Szeged
Editor: David Haussler
Abstract. We consider two issues in polynomial-time exact learning of concepts using membership and equivalence queries: (1) errors or omissions in answers to membership queries, and (2) learning ﬁnite variants of concepts
drawn from a learnable class.
To study (1), we introduce two new kinds of membership queries: limited membership queries and malicious
membership queries. Each is allowed to give incorrect responses on a maliciously chosen set of strings in the
domain. Instead of answering correctly about a string, a limited membership query may give a special “I don’t
know” answer, while a malicious membership query may give the wrong answer. A new parameter L is used to
bound the length of an encoding of the set of strings that receive such incorrect answers. Equivalence queries
are answered correctly, and learning algorithms are allowed time polynomial in the usual parameters and L. Any
class of concepts learnable in polynomial time using equivalence and malicious membership queries is learnable
in polynomial time using equivalence and limited membership queries; the converse is an open problem. For
the classes of monotone monomials and monotone k-term DNF formulas, we present polynomial-time learning
algorithms using limited membership queries alone. We present polynomial-time learning algorithms for the
class of monotone DNF formulas using equivalence and limited membership queries, and using equivalence and
malicious membership queries.
To study (2), we consider classes of concepts that are polynomially closed under ﬁnite exceptions and a
natural operation to add exception tables to a class of concepts. Applying this operation, we obtain the class
of monotone DNF formulas with ﬁnite exceptions. We give a polynomial-time algorithm to learn the class
of monotone DNF formulas with ﬁnite exceptions using equivalence and membership queries. We also give a
general transformation showing that any class of concepts that is polynomially closed under ﬁnite exceptions and is
learnable in polynomial time using standard membership and equivalence queries is also polynomial-time learnable
using malicious membership and equivalence queries. Corollaries include the polynomial-time learnability of the
following classes using malicious membership and equivalence queries: deterministic ﬁnite acceptors, boolean
decision trees, and monotone DNF formulas with ﬁnite exceptions.
Keywords: Concept learning, queries, errors
Introduction
There is an impressive and growing number of polynomial-time algorithms, many of them
quite beautiful and ingenious, to learn various interesting classes of concepts using equiva-
D. ANGLUIN, ET AL.
lence and membership queries. To apply such algorithms in practice, researchers need to
overcome a number of problems.
Onesigniﬁcantissueistheproblemofomissionsanderrorsinanswerstoqueries. Previous
learning algorithms in the equivalence and membership query model are guaranteed to
perform well assuming that queries are answered correctly, but there is often no guarantee
that the performance of the algorithm will “degrade gracefully” if that assumption is not
exactly satisﬁed.
Lang and Baum report that this problem derailed their attempt to apply Baum’s
algorithm for learning neural nets from examples and membership queries to
the problem of recognizing hand-written digits. The attempt failed because the membership
questions posed by the algorithm were too difﬁcult for people to answer reliably. The
algorithm typically asked membership queries on, say, a random-looking blur midway
between a “5” and a “7,” and the humans being queried gave very inconsistent responses.
Studies in cognitive psychology indicate that this is the norm; people are typically quite
inconsistent in deciding where the precise boundary of a concept lies. (See, e.g., Anderson
Omissions and Limited Membership Queries
This motivated us to introduce the limited membership query. A limited membership query
may be answered either correctly, or with an omission, that is, a special value signifying “I
don’t know.” The answers are persistent; that is, repeated queries about the same example
are given the same answer. The choice of the set of strings on which to give answers of
“I don’t know” is assumed to be made by a malicious adversary. We introduce a new
parameter L to quantify the “amount” of omission—it is a bound on the table-size of the
set of strings on which the adversary answers “I don’t know.” (The table-size of a set of
strings is the number of strings in the set plus the sum of their lengths.) A polynomial-time
learning algorithm is permitted time polynomial in the usual parameters and L.
For this model, we deﬁne a hypothesis to be “nonstrictly” correct if it agrees with the
target concept on all examples except possibly ones for which a limited membership query
was answered “I don’t know.” Thus, domain elements answered with “I don’t know” are
allowed to be classiﬁed arbitrarily by the ﬁnal hypothesis of a learning algorithm. This
corresponds to the intuition that since a person could not be sure of the classiﬁcation of a
“blur between 5 and 7”, it does not matter how the ﬁnal hypothesis classiﬁes it.
We give a polynomial-time learning algorithm using just limited membership queries
for the class of monotone monomials and a lower bound on the query complexity of this
problem. We also give a polynomial-time learning algorithm in this model for the class of
k-term monotone DNF formulas.
We also consider combining limited membership queries with equivalence queries. We
assume that the answers to equivalence queries remain correct, that is, any counterexample
given is truly a counterexample to the hypothesis of the learning algorithm. In the nonstrict
model, the equivalence query is answered “yes” if the hypothesis is nonstrictly correct;
otherwise a counterexample must be returned from among examples not answered with “I
don’t know.” In the strict model, equivalence queries remain as usual, that is, an equivalence
query is answered with “yes” if the hypothesis is exactly equivalent to the target concept;
MALICIOUS OMISSIONS AND ERRORS
otherwiseanarbitrarycounterexampleisreturned(includingpossiblyanexamplepreviously
answered with “I don’t know.”)
We show that the same classes of concepts can be learned in polynomial time using limited
membership queries and equivalence queries in the nonstrict and strict models. We give
a polynomial-time algorithm to learn monotone DNF formulas using limited membership
queries and equivalence queries in the nonstrict model.
Malicious Membership Queries and Errors
In the case of limited membership queries, the answers that are not omissions are guaranteed
to be correct. We also consider the situation in which the answers to membership queries
may be wrong. In a malicious membership query, the answer given may be correct, or
it may be an error. As in the case of limited membership queries, we use the parameter
L to bound the table-size of the set of strings whose malicious membership queries are
answered erroneously, the choice of that set of strings is assumed to be made by a malicious
adversary, and the answers to queries are persistent. We assume that equivalence queries
remain correct, which corresponds to the strict model introduced above. That is, the ﬁnal
hypothesis of the learning algorithm must be exactly equivalent to the target concept.
We give a polynomial-time algorithm to learn monotone DNF formulas using malicious
membership queries and equivalence queries. It is not difﬁcult to see that any concept
class learnable in polynomial time using malicious membership queries and equivalence
queries is learnable in polynomial time using limited membership queries and equivalence
queries, but the converse direction is an interesting open question. We exhibit a class
of concepts for which the query complexity for equivalence and malicious membership
queries is not bounded by any polynomial in the query complexity for equivalence and
limited membership queries, but both complexities are exponential in the number of strings
that receive incorrect answers.
Finite Exceptions
A related issue is the assumption that the target concept is drawn from a particular class
of concepts, for example, monotone DNF formulas. Even if the target concept is “nearly”
a monotone DNF formula, there is typically no guarantee that the learning algorithm will
do anything reasonable. We approach this question by considering ﬁnite variants of the
concepts in a given class, using the table-size of the set of exceptions as a measure of “how
different” the target concept is from one in the speciﬁed class.
We deﬁne what it means for a concept class to be polynomially closed under ﬁnite
exceptions. Some concept classes, for example, DFA’s and decision trees, are polynomially
closed under ﬁnite exceptions, while others, like monotone DNF formulas, are not. For the
latter, we deﬁne a natural operation of adding exception tables to the concept class to make
it polynomially closed under exceptions. We give a polynomial-time learning algorithm
for the resulting class of monotone DNF formulas with ﬁnite exceptions, using equivalence
queries and standard membership queries.
We then give a general transformation that shows that any class of concepts that is polynomially closed under exceptions and polynomial-time learnable using equivalence que-
D. ANGLUIN, ET AL.
ries and standard membership queries is also polynomial-time learnable using equivalence
queries and malicious membership queries. Corollaries include polynomial-time learning
algorithms using equivalence queries and malicious membership queries for the concept
classes of DFA’s, decision trees, and monotone DNF formulas with ﬁnite exceptions.
The notion of a ﬁnite variant of a concept, that is, a concept with a ﬁnite set of exceptions, is a unifying theme between the models of learning with equivalence and malicious
membership queries and of learning a concept class with ﬁnite exceptions. Our model of
errors in membership queries can be viewed as combining an equivalence oracle for the
target concept and a membership oracle for a ﬁnite variant of the target concept. In the case
of learning a concept class with ﬁnite exceptions, the equivalence and membership oracles
present the same ﬁnite variant of a concept in the base class. In both cases, the goal is to
identify exactly the concept presented by the equivalence oracle.
Related Work
There is a considerable body of literature on errors in examples in the PAC model, starting
with the ﬁrst error-tolerant algorithm in the PAC model, given by Valiant . In this case
the goal is PAC-identiﬁcation of the target concept, despite the corruption of the examples
by one or another kind of error, for example, random or malicious misclassiﬁcation errors,
random or malicious attribute errors, or malicious errors (in which both attributes and
classiﬁcation may be arbitrarily changed).
There has been not as much work on omissions and errors in learning models in which
membership queries are available, and the issues are not as well understood. One relevant
distinctioniswhethertheomissionsoferrorsinanswerstomembershipqueriesarepersistent
or not. They are persistent if repeated queries to the same domain element always return the
same answer. In general, the case of persistent omissions or errors is more difﬁcult, since
non-persistent omissions or errors can yield extra information, and can always be made
persistent simply by caching and using the ﬁrst answer for each domain point queried.
Non-persistent Errors in Queries
Sakakibara deﬁnes one model of non-persistent errors, in which each answer to a query
may be wrong with some probability, and repeated queries constitute independent events
 . He gives a general technique of repeating each query sufﬁciently often
to establish the correct answer with high probability. This yields a uniform transformation of existing query algorithms. The method also works for both of Bultman’s models
 . This could be a reasonable model of a situation in which the answers to
queries were being transmitted through a medium subject to random independent errors;
then the technique of repeating the query is eminently sensible.
A related model is considered by Dean et al. for the case of a robot learning a
ﬁnite-state map of its environment using faulty sensors and reliable effectors. This model
assumes that observation errors are independent as long as there is a nonempty action
sequence separating the observations. This means that there is no simple way to “repeat
the same query”, since a nonempty action sequence may take the robot to another state,
and no reset operation is available. A polynomial-time learning algorithm is given for the
MALICIOUS OMISSIONS AND ERRORS
situation in which the environment has a known distinguishing sequence. It achieves exact
identiﬁcation with high probability.
Persistent Errors in Membership Queries
The method of “repeating the query” is insufﬁcient for the more difﬁcult case of persistent omissions or errors in membership queries. In this case, we must exploit the errorcorrecting properties of groups of “related” queries. In an explicit and very interesting
application of the ideas of error-correcting algorithms, Ron and Rubinfeld use the criterion
of PAC-identiﬁcation with respect to the uniform distribution, and give a polynomial-time
randomized algorithm using membership queries to learn DFA’s with high rates of random
persistent errors in the answers to the membership queries .
Algorithms that use membership queries to estimate probabilities ) are generally not too sensitive to small rates
of random persistent errors in the answers to queries. For example, Goldman, Kearns, and
Schapire give polynomial-time algorithms for exactly learning read-once majority formulas
and read-once positive NAND formulas of depth O(log n) with high probability using
membership queries with high rates of persistent random noise or modest rates of persistent
malicious noise . As another example, Kushilevitz
and Mansour’s algorithm that uses membership queries and exactly learns logarithmicdepth decision trees with high probability in polynomial time seems likely to be robust
under nontrivial rates of persistent random noise in the answers to queries .
However, learning algorithms for other classes of concepts using equivalence and membership queries may depend more strongly on the correctness of the answers to individual
queries; in these cases, there is no guarantee of a learning algorithm for the class that can
tolerate omissions or errors in the answers to membership queries.
One model that addressed these questions was introduced by Angluin and Slonim: equivalence queries are assumed to be answered correctly, while membership queries are either
answered correctly or with “I don’t know” and the answers are persistent. The “I don’t
know” answers are determined by independent coin ﬂips the ﬁrst time each query is made
 .
They give a polynomial-time algorithm to learn monotone
DNF formulas with high probability in this setting. They also show that a variant of this
algorithm can deal with one-sided errors, assuming that no negative point is classiﬁed as
positive. Goldman and Mathias also consider this model . Our
current models and results differ in that the omissions and errors are chosen by a malicious
adversary instead of a random process, and the rate of incorrect answers that can be tolerated
is consequently much lower.
Frazier et al. have introduced a model of omissions in answers to membership
queries, called learning from a consistently ignorant teacher. The basic idea is to require
that if the teacher gives answers to certain queries that would imply a particular answer to
another query, the teacher cannot answer the latter query with “I don’t know.” For example,
in the domain of monotone DNF formulas, if the teacher classiﬁes a particular point as
positive, then the teacher cannot answer “I don’t know” about any of the ancestors of the
point. The goal of the learner is to learn exactly the ternary classiﬁcation of points into
D. ANGLUIN, ET AL.
positive, negative, and “I don’t know” that is presented by the teacher. Such a ternary
classiﬁcation may be represented by the agreement of a set of binary-valued concepts; the
agreement classiﬁes a point as positive (respectively, negative) if all the concepts in the set
classify it as positive (respectively, negative), otherwise, the agreement classiﬁes the point
as “I don’t know.” Efﬁcient learning algorithms are given in this model for monomials
with at least one positive example, concepts represented as the agreement of a constant
number of monotone DNF formulas, k-term DNF formulas, DFA’s, or decision trees, and
concepts represented by an agreement of boxes with samplable intersection. Compared
to our model, this model has a different measure of the representational complexity of a
concept with omissions, which allows a much higher rate of omissions to be compactly
represented. It also differs in requiring the learner to reproduce exactly the “I don’t know”
labels of the teacher, whereas in our (nonstrict) model of omissions such examples can be
classiﬁed arbitrarily.
Preliminaries
Concepts and Concept Classes
Our deﬁnitions for concepts and concept classes are a bit non-standard. We have explicitly
introduced the domains of concepts in order to try to unify the treatment of ﬁxed-length
and variable-length domains. We take Σ and Γ to be two ﬁnite alphabets. Examples are
represented by ﬁnite strings over Σ and concepts are represented by ﬁnite strings over Γ.
A concept consists of a pair (X, f), where X ⊆Σ∗, and f maps X to {0, 1}. The set X
is the domain of the concept. The positive examples of (X, f) are those w ∈X such that
f(w) = 1, and the negative examples of (X, f) are those w ∈X such that f(w) = 0. Note
that strings not in the domain of the concept are neither positive nor negative examples of
A concept class is a triple (R, Dom, µ), where R is a subset of Γ∗, Dom is a map from R
to subsets of Σ∗, and for each r ∈R, µ(r) is a function from Dom(r) to {0, 1}. R is the
set of legal representations of concepts. For each r ∈R, the concept represented by r is
(Dom(r), µ(r)).
A concept (X, f) is represented by a concept class (R, Dom, µ) if and only if for some
r ∈R, (X, f) is the concept represented by r. The size of a concept (X, f) represented by
(R, Dom, µ) is deﬁned to be the length of the shortest string r ∈R such that r represents
(X, f). The size of (X, f) is denoted by |(X, f)|; note that it depends on the concept class
The concept classes we consider in this paper are boolean formulas and syntactically
restricted subclasses of them, boolean decision trees, and DFA’s. The representations are
more or less standard, except each concept representation speciﬁes the relevant domain. For
DFA’s, the domain of every concept is the set Σ∗itself. For boolean formulas and decision
trees, we assume that Σ = {0, 1}, and each concept representation speciﬁes a domain of
the form {0, 1}n.
For each ﬁnite set S of strings from Σ∗, we deﬁne its table-size, denoted ||S||, as the sum
of the lengths of the strings in S and the number of strings in S. Note that ||S|| = 0 if and
MALICIOUS OMISSIONS AND ERRORS
only if S =
  . The table-size of a set of strings is related in a straightforward way to an
encoding of a list of the strings; see Section 5.
For a learning problem we assume that there is an unknown target concept r drawn from
a known concept class (R, Dom, µ). Information about the target concept is available to
the learning algorithm as the answers to two types of queries: equivalence queries and
membership queries.
In an equivalence query, the algorithm gives as input a concept r′ ∈R with the same
domain as the target, and the answer depends on whether µ(r) = µ(r′). If so, the answer
is “yes”, and the learning algorithm has succeeded in its goal of exact identiﬁcation of the
target concept. Otherwise, the answer is a counterexample, any string w ∈Dom(r) on
which the functions µ(r) and µ(r′) differ. We denote an equivalence query on a hypothesis
h by EQ(h).
The label for a counterexample v = EQ(r′) is the value of µ(r) on v, giving its classi-
ﬁcation by the target concept. Since the hypothesized concept r′ and the target concept r
differ on the classiﬁcation of the counterexample v, the label of v is also the complement
of the value of µ(r′) on v. Positive counterexamples are those with label 1 and negative
counterexamples are those with label 0.
In a membership query, the learning algorithm gives as input a string w ∈Dom(r), and
the answer is either 0, 1, or ⊥. If the answer is equal to the value of µ(r) on w, then the
answer is correct. If the answer is equal to ⊥, we say that the answer is an omission or a
“Don’t know”. If the answer is 0 or 1 but not equal to the value of µ(r) on w, then the
answer is an error or a lie.
In a standard membership query, denoted MQ, all the answers are required to be correct.
In a limited membership query, denoted LMQ, each answer is required to be correct or an
omission. In a malicious membership query, denoted MMQ, each answer is required to be
correct or an error (no omissions). Note that an answer of 0 or 1 to a limited membership
query is always correct, but this is not true for answers to malicious membership queries.
The answers to malicious and limited membership queries are also restricted as follows.
They are persistent; that is, different membership queries with the same input string w
receive the same answer. Note that non-persistent queries may reveal some information;
in case two different queries to the same string receive different answers, the learning
algorithm knows that there has been an error on this string, though this will not in
general determine the correct classiﬁcation of the string. Every algorithm designed to
work with persistent queries can be made to work with non-persistent ones by caching
the queries and always using the ﬁrst answer for subsequent queries of the same string.
In addition, we bound the quantity of errors (or omissions) permitted in answers to
malicious (resp., limited) membership queries. One natural quantity to bound would be
the number of different strings whose membership queries can be answered incorrectly,
and this works well in ﬁxed-length domains. However, in variable-length domains, we
wish to account for the lengths of the strings as well as their number.
D. ANGLUIN, ET AL.
Therefore, in general the algorithm is given a bound L on the table-size, ||S||, of the
set S of strings whose malicious (resp., limited) membership queries are answered
erroneously (resp., with a ⊥) during a single run. In the case of a ﬁxed-length domain,
{0, 1}n, wemayinsteadgiveaboundℓonthenumberofdifferentstringswhoseMMQ’s
(resp., LMQ’s) are answered incorrectly (resp., with a ⊥). Note that L = ℓ(n + 1) is
a bound on the table-size in this case.
Note that when L = 0 or ℓ= 0 there can be no errors or omissions in the answers to
MMQ’s (or LMQ’s) and we have the usual model of standard membership queries as
a special case.
We assume that an on-line adversary controls the choice of counterexamples in answers to
equivalence queries and the choice of which elements of the domain will be answered with
errors (or ⊥’s) in malicious (or limited) membership queries. When the learning algorithms
we consider are deterministic, the adversary may be viewed as choosing in advance the set
of strings for which it will give incorrect answers to membership queries, as well as all the
counterexamples it will give to equivalence queries.
In this paper we consider models in which the learning algorithm has access to the
following combinations of queries:
membership and equivalence queries,
limited membership queries alone,
limited membership queries and equivalence queries, and
malicious membership queries and equivalence queries.
Model (1) is just the usual model of a minimally adequate teacher . In model
(2), the learning algorithm need only achieve nonstrict identiﬁcation. In other words, the
concept output by the learning algorithm must agree with the target concept on all points
not answered ⊥by the LMQ, but it may differ on points answered ⊥. This corresponds to
our view that ⊥points form the borderline of the concept and that the classiﬁcation of them
is irrelevant or meaningless.
For model (3) we consider both nonstrict and strict variants. In the nonstrict variant,
equivalence queries are modiﬁed so that if the queried concept and the target concept
differ only on points classiﬁed as ⊥by the LMQ, then the reply is “yes”. Otherwise, a
counterexample must be given from the set of points not classiﬁed as ⊥by the LMQ. In
this case, as in model (2), only nonstrict identiﬁcation is required. In the strict variant of
model (3), as well as in model (4), equivalence queries are not modiﬁed, and the learning
algorithm is required to achieve the usual kind of exact identiﬁcation, that is, the output
concept must agree with the target concept on every point in their common domain.
We extend the usual notion of polynomial-time learning to models (2-4) by allowing the
polynomial bound on the running time to depend on three parameters, that is, p(s, n, L).
Here s is the usual parameter bounding the length of the representation of the target concept,
n is the usual parameter bounding the length of the longest counterexample seen so far, and
L is a new parameter, bounding the table-size of the set of strings on which LMQ answers
⊥or MMQ gives incorrect answers.
MALICIOUS OMISSIONS AND ERRORS
The deﬁnitions are extended in the usual way to cover randomized learning algorithms and
their expected running times, and also extended equivalence queries, in which the inputs to
equivalence queries and the ﬁnal result of the algorithm are allowed to come from a concept
class different from (usually larger than) the concept class from which the target is drawn.
It is straightforward to transform any algorithm that uses malicious membership queries
into one that uses limited membership queries. Every ⊥answer can be replaced by a 0 or a 1
arbitrarily and given to the learner. Therefore learning with malicious membership queries
is at least as hard as learning with limited membership queries in the strict model. The
same applies to learning from equivalence and malicious membership queries and learning
from equivalence and limited membership queries in the strict model. Furthermore, in
Subsection 6.3 we show that the strict and the nonstrict models of learning from equivalence
and limited membership queries are in fact polynomial-time equivalent. Note that the most
general kind of membership queries is one in which both wrong and ⊥answers are possible,
but such queries are not harder than the malicious ones and therefore we consider only the
Monotone DNF Formulas
We assume a set of propositional variables V and denote its elements by x1, x2, . . . , xn,
where n is the cardinality of V . A monotone DNF formula over V is a DNF formula over
V where no literal is negated. The domain of such a formula is {0, 1}n. For example, for
x1x4 ∨x2x17x3 ∨x9x5x12x3 ∨x8
is a monotone DNF formula (with domain {0, 1}20). We assume that the target formula
h∗has been minimized, that is, it contains no redundant terms. (Incidentally, there is an
efﬁcient algorithm to minimize the number of terms of a monotone DNF formula.) For a
monotone DNF formula f, let #(f) denote the number of terms in f. In the above example,
We view the domain {0, 1}n of monotone DNF formulas (with or without exceptions)
as a lattice, with componentwise “or” and “and” as the lattice operations. The top element
is the vector of all 1’s, and the bottom element is the vector of all 0’s. The elements are
partially ordered by ≤, where v ≤w if and only if v[i] ≤w[i] for all 1 ≤i ≤n. Often we
refer to the examples as points of the hypercube {0, 1}n. For a point v, all points w such that
w ≤v are called the descendants of v. Those descendants that can be obtained by changing
exactly one coordinate of v from a 1 to a 0 are called the children of v. The ancestors and
the parents are deﬁned similarly. Note that v is both a descendant and ancestor of itself.
For convenience, we use a representation of monotone DNF formulas in which each term
is represented by the minimum vector, in the ordering ≤, that satisﬁes the term. Thus,
vector 10011 (where n = 5) denotes the term x1x4x5. In this representation, if h is a
monotone DNF formula and v is a vector in the domain, v satisﬁes h if and only if for some
term t of h, t ≤v. That is, a monotone DNF formula is satisﬁed only by the ancestors of
its terms. In the other direction, we say that term t covers point v if and only if v satisﬁes
t. For the sake of simplicity we often use in our algorithms something called “the empty
D. ANGLUIN, ET AL.
DNF formula”. This is the formula with no terms, which is not satisﬁed by any point, and
is therefore the identically false formula.
For any n-argument boolean function f, we call point x a local minimum point of f if
f(x) = 1 but for every child y of x in the lattice, f(y) = 0. The local minimum points of
a minimized DNF formula represent its terms in our representation.
For two n-argument boolean functions f1 and f2 we deﬁne the set Err(f1, f2) to be the
set of points where they differ. I.e., Err(f1, f2) = {x | f1(x) ̸= f2(x)}. The cardinality
of Err(f1, f2) is called the distance between f1 and f2 and is denoted by d(f1, f2).
Limited Membership Queries
In this section, we present results concerning learning with the help of limited membership
queries. We start with the description of a subroutine that is repeatedly used in all algorithms
for this model.
Delimiting A Term
Algorithm Delimit takes a positive point and ﬁnds a set of candidates for a term in
the target concept covering the point. This algorithm plays the role of the algorithms
called “Reduce” in other works on learning monotone DNF .
We choose a different name because those algorithms output a single monomial, whereas
Algorithm Delimit ﬁnds a set of points that must include a correct term.
Figure 1. Example run of Algorithm Delimit for target concept x1x2. Boldface +’s, −’s and ?s indicate
responses 1, 0, and ⊥, respectively, of the LMQ oracle.
Consider, for example, the situation in Figure 1 where the target concept is x1x2, and we
start with the known positive point p = 1111. With complete information, we would begin
by querying each child of p, updating p to be the ﬁrst positive child found. This process
would be repeated until eventually we had p = 1100. After determining by membership
queries that every child of p is negative, we could stop.
MALICIOUS OMISSIONS AND ERRORS
Delimit(p)
Figure 2. Algorithm Delimit. offbits is a bit array used in recursive subroutine Down to improve efﬁciency;
root is a special positive point (with everything beneath it being negative or belonging to DK); DK is a certain
set of points with LMQ ⊥, created by Down and further thinned by Up; P is a certain set of positive points
above DK, a useful byproduct of Up; T is the set of points among which the correct term of the target formula
Because Algorithm Delimit can make only limited membership queries, it may encounter the difﬁculty that some positive point p has children that are all “Don’t know,” or a mix
of “No” and “Don’t know.” For instance, in the example in Figure 1, all queries of children
of 1101 return 0 or ⊥. In this case, Algorithm Delimit continues by querying all the
children of all the “Don’t know” points. Should it ever get another 1 response to a limited
membership query, it replaces p by that point. What we have just described is the subroutine
Down of Delimit, which invokes itself recursively upon ﬁnding a new positive point.
Detailed code of algorithm Delimit and its subroutines is given in Figures 2, 3 and 5. In
our C-like pseudocode we often use a For loop over a changing set of points; for example,
“For (each b ∈A)”. By this we mean that in each iteration of the loop the current point
(i.e., b in this example) is marked, and that only unmarked points can be considered in the
next iterations. Furthermore, the loop condition is checked every time, that is, we check
for an existence of some unmarked point in the possibly changing set (A in this example).
If one exists, we mark it and do the body of the loop which may add new (unmarked)
points to the set or delete some existing points (either marked or unmarked). Points that
are deleted before marking are not processed. Furthermore, to ensure that the algorithms
are deterministic, we need that the current point is chosen according to some unambiguous
rule, i.e., there must not be any choice as to which unmarked point of the set will be marked
and processed next. Thus, we assume that there is some total order on all the elements in
the sample space and use this to unambiguously choose the current point. When the points
of the sample space correspond to assignments of 0 and 1 to n variables, the easiest total
order is created by treating each point as an n-bit number. In all our algorithms that learn
boolean formulas, we assume that this total order is used and always pick the point with the
lowest number.
Another thing worth explanation about our pseudocode is the use of ⟨·, ·, . . . , ·⟩on the
left side of assignments and as return values for subroutines. We do this to avoid confusing
global variables or things passed by reference. Thus, everything is passed by value (by
making a copy in the called subroutine) and everything that the calling routine needs is
returned to it explicitly. If many things need to be returned, then we put them in a tuple,
D. ANGLUIN, ET AL.
denoted by ⟨·, ·, . . . , ·⟩, and return the tuple. The tuple is basically just a convenient notation
for a structure.
Figure 3. Subroutine Down. p is the point it is called on; offbits is a bit array used to improve efﬁciency; DK is
a set of points with LMQ ⊥and with all descendants ⊥or negative; C is a set of points that have to be processed
in the main loop.
The subroutine Down uses a variable offbits to improve efﬁciency. If a query to a child
a of a known positive point p gives a 0 response, then we know that in any descendant of
p, switching off the bit position that distinguishes a from p will lead to a negative point,
because this point will be a descendant of a. Therefore, variable offbits keeps track of those
bit positions that must be 1, allowing subroutine Down to save some queries.
Eventually Down is called on some positive point p and ﬁnds a (possibly empty) set
DK of descendants of p such that the limited membership oracle responded ⊥for every
point in DK, and all other proper descendants of p are known to be negative. Then Down
returns and point p from then on is called the root. Since root is positive, and we know that
every descendant of root not in set DK is negative, a term of the monotone DNF must lie
somewhere in the set DK ∪{root}. This set is outlined in Figure 4.
The next major part of Delimit is the subroutine Up. Any point corresponding to a
term of the target concept must have only positive ancestors. Subroutine Up ensures that
MALICIOUS OMISSIONS AND ERRORS
Figure 4. The set of candidate terms obtained by subroutine Down.
Figure 5. Subroutine Up. DK is a set of points with LMQ ⊥that has to be thinned; P is a set of minimal
ancestors with LMQ 1 of points in DK; A is a set of certain ancestors of the current point a.
no point in DK has any ancestor with LMQ 0. Thus, Delimit gets a thinned set of
possible terms T. In the example of Figures 1 and 4, 0100 is deleted from DK because
LMQ(0101) = 0. If LMQ(1010) = 0, then 1000 will also be deleted.
As a useful byproduct of subroutine Up, we get a set P, which consists of the minimal
ancestors with LMQ 1 of the points in DK (i.e., the minimal points in the lattice ordering
of the set of those ancestors of elements of DK that have LMQ 1). If there are no points
in DK, P contains the root only.
D. ANGLUIN, ET AL.
We summarize our discussion in the following lemma. Recall that since Delimit is
deterministic, the adversary may be thought of as choosing in advance the answers to all
possible limited membership queries. We denote by LMQ(v) the value of this function on
the point v.
Lemma 1 Let T and P be the outputs of running Algorithm Delimit on a positive point
p when the target concept is a monotone DNF. Then
1. T contains a term of the target concept covering p.
2. For every t ∈T, every point covered by t has LMQ 1 or ⊥.
3. For any v such that LMQ(v) = 1, and for any t ∈T that covers v, t covers some point
in P that is a descendant of v.
Part 1 follows from the discussion above. Part 2 holds since every ancestor of the
root is positive (since root itself is), and since the code in Up ensures this condition for
every other point in T. To see Part 3, suppose v has LMQ(v) = 1 and t is some point of
T that covers v. If t is root, then root itself is a point in P that is a descendant of v. If
t is some other element of T, then Up has made an upward search from t to ﬁnd all the
minimal ancestors of t with LMQ 1 and placed them in P.
We are now ready to analyze the running time of Delimit.
Lemma 2 For any monotone DNF target concept, Algorithm Delimit makes at most
nℓ+ n limited membership queries, where ℓis the number of ⊥responses received. In
general, any sequence of s calls to Algorithm Delimit for the same target concept makes
at most n(ℓ+ s) limited membership queries.
Every point queried by Delimit(p) is either a child of p, or a child of a previously
queried point with LMQ 1, or else is within Hamming distance 1 of a ⊥point.
For the last case, each time we receive a ⊥response on a point v, we could in principle
need to query all the children of v in Down and then all the parents of v in Up, except
that there is a parent or a child of v that we must have queried before querying v. This
leads to at most n −1 queries for each ⊥received, plus 1 for the ⊥query itself. As long
as the algorithm remembers the answers to all queries, this holds for any number of calls
to Delimit on the same concept.
For the ﬁrst two cases, let us ﬁrst note that they only happen in subroutine Down. Let
us call the point that Down was called on the current point, which can be either p or some
previously queried point a with LMQ(a) = 1. When Down makes a query on a child
of the current point and receives a response of ⊥, the query is already accounted for. If it
receives a response of 0, then a bit is turned on in offbits, decreasing the Hamming distance
between the current point and offbits. If it receives a response of 1, then a recursive call to
Down is made, and the Hamming distance between the (new) current point and offbits is
also one less. Since the maximum Hamming distance between two points is n, there can
be at most n queries of this kind in every call to Delimit.
The running time of Delimit is polynomial in the number of queries it makes.
MALICIOUS OMISSIONS AND ERRORS
Learning Monotone Monomials from Limited Membership Queries Alone
We begin with a very simple application of Algorithm Delimit to learn monotone monomials from limited membership queries in the nonstrict model. This should elucidate the
basic ideas at the heart of the more complicated algorithm for learning arbitrary monotone
k-term DNF from limited membership queries that follows.
Theorem 1 Monotone monomials can be learned from no more than nℓ+ n + 1 limited
membership queries in the nonstrict model, where ℓis the number of ⊥responses received.
The method is to run Algorithm Delimit starting with the all 1’s vector, and
then output any term t ∈T that covers every point in P. (If either LMQ(11 · · · 1) = 0,
or LMQ(11 · · · 1) = ⊥and the “down” phase of Algorithm Delimit ﬁnds no positive
points, then we will output the empty DNF formula.)
First, observethatsuchatermtmustexistinT, sincethetruetargetmonomialcoversevery
point in P and lies in T by point 1 of Lemma 1. The output term t covers every point with
LMQ 1 by point 3 of Lemma 1, since it covers every point in P. And by point 2 of Lemma 1,
it cannot cover any points with LMQ 0, since it is in T. The bound on the number of queries
follows from Lemma 2, which counts all of them except for the ﬁrst query to 11 · · · 1.
The running time again is polynomial in the number of queries made.
Another computationally somewhat more efﬁcient way of learning monotone monomials
would be to run Delimit on the all 1’s vector, and then output the monomial m that is the
intersection of all the points in P. That is, the learner’s output would have a variable if and
only if that variable is in every term in P.
In this version, monomial m covers every point in P, so by point 3 of Lemma 1, it must
cover all points v such that LMQ(v) = 1. On the other hand, since m is the intersection
of positive points, it must either be the correct monotone monomial or an ancestor of the
correct monotone monomial, so m cannot cover any negative points.
Now we present a lower bound for this problem.
Theorem 2 For any integer c, if the limited membership oracle gives Pc
responses of ⊥, then any learner can be forced to use at least
−1 limited membership
queries not counting those answered ⊥to learn monotone monomials.
Let the target concept be deﬁned by a monomial of length n −(c + 1). It covers
no point with more than (c + 1) 0-bits, and exactly one point with (c + 1) 0-bits. Now let
us assume that all queries of a learning algorithm on points containing c or fewer 0-bits are
answeredby⊥, anditsﬁrst
−2queriesofpointswithatleast(c+1)0-bitsareanswered
by 0. After that there are at least two unqueried points with (c+1) 0-bits. The corresponding
twomonomialsarebothconsistentwiththeanswersgivensofarandtheydifferonpointsthat
did not receive a ⊥response. Hence the learning algorithm needs at least one more query.
D. ANGLUIN, ET AL.
Corollary 1 For any ﬁxed constant c, if the limited membership oracle gives O(nc)
responses of ⊥, then the learner can be forced to use Ω(nc+1) limited membership queries
to learn monotone monomials.
Learning Monotone k-term DNF from Limited Membership Queries Alone
We are now ready to present the algorithm OnlyLMQ, that learns monotone k-term DNF
in the nonstrict model from limited membership queries alone. The detailed code is given
in Figures 6 and 7.
FindPos(h)
Figure 6. Subroutine FindPos. S is the set of maximal points in the sample space that are not covered by the
current hypothesis h.
Algorithm OnlyLMQ initializes its hypothesis h to be the empty formula, and repeats
the following.
By searching down from each maximal point not covered by hypothesis h, a point q with
LMQ(q) = 1 that is not covered by h is found. This is done by the subroutine FindPos
of Algorithm OnlyLMQ. If no such point is found, then h is correct and the algorithm
outputs it and halts.
Now the point q is given to Algorithm Delimit, which returns the sets T and P. If there
is a single term in T that covers all the positive points in P not already covered by h, then
this term is added to h and we repeat the main For loop by looking for another point with
LMQ 1 that is not covered by the current hypothesis.
Otherwise, we have to add more than one term to h to cover all the points in P, and we
begin a search for a set of terms to add. This search may disclose more positive points,
so this process itself may have to be iterated. We begin by initializing Terms to be T and
Pos to be those points in P that are not covered by h. We record the fact that point q has
already been delimited by placing it in the set Delimited. We then call Algorithm Delimit
on every point in Pos and for each call gather together the points from P in NewPos as well
as add the terms from T to Terms. When all points in Pos have been delimited, we add the
points gathered in NewPos to Pos and try to cover the newly enlarged set Pos with any j = 2
terms from Terms. If we succeed, we put these terms in h; if we fail, we keep enlarging
Terms, gathering points in NewPos, then enlarging Pos, incrementing j, and trying again in
the same manner to cover all points in Pos with j terms from Terms.
MALICIOUS OMISSIONS AND ERRORS
Delimit(p)
Figure 7. Algorithm OnlyLMQ, which learns monotone k-term DNF from LMQ’s. Delimited is the set of
points that have been delimited in the current iteration of the main loop; Terms is the set of candidate terms; Pos
is a set of known positive instances that need to be covered by a certain number of points in Terms.
D. ANGLUIN, ET AL.
Theorem 3 Algorithm OnlyLMQ learns monotone k-term DNF from O(knk + n2ℓ)
limited membership queries in the nonstrict model, where ℓis the number of ⊥responses it
We prove the theorem in three stages. First we argue that the algorithm eventually
terminates with a DNF hypothesis h that correctly classiﬁes all non-⊥points. Next, in
Lemmas 3 through 6, we argue that h includes at most k terms. In particular, Lemma 6
says that at all times through the run of the algorithm, h is at least as efﬁcient—in terms of
positive instances covered per DNF term—as the target concept. Finally, we argue that the
query bound is correct.
* Algorithm Delimit terminates with a correct hypothesis. All the terms added to the
hypothesis are at some point in a set T output by Algorithm Delimit. Therefore, by point
2 of Lemma 1, no term that we ever add to our hypothesis can err by classifying a point
with LMQ 0 as positive.
Furthermore, since point 1 of Lemma 1 guarantees that each time we call the subroutine
Delimit from a point we get at least one term in T covering that point, the algorithm must
eventually succeed in covering the set Pos with some number of terms and break out of the
“For (j = 1;
; j++)” loop. Since there are only a ﬁnite number of positive points, this
means that the algorithm eventually terminates with a hypothesis that correctly classiﬁes
all non-⊥points.
* Final hypothesis contains at most k terms. The following lemmas provide the argument
that each hypothesis of Algorithm OnlyLMQ contains at most k terms.
Lemma 3 No term in the set Terms is ever implied by the current hypothesis h inside of
the “For (j = 1;
; j++)” loop.
The hypothesis h is constant during the execution of the loop. Every element of
Terms entered Terms from the set T generated by calling Algorithm Delimit with this
hypothesis h. The ﬁrst call to Delimit (the one made immediately before entering the
“For (j = 1;
; j++)” loop) is made on some point q such that h(q) = 0, and therefore
no element of T can be implied by h. All subsequent calls to Delimit are made on some
point r ∈Pos. The set Pos is constructed so that h(r) = 0, so again no points in the set
T returned by Delimit can be implied by h, because T contains only descendants of r.
Lemma 4 Whenever Algorithm OnlyLMQ attempts to cover the set of positive points
Pos with a disjunction of j terms from Terms (line “If (∃t1, t2, . . . , tj ∈Terms s.t. ∀p ∈
Pos ((t1 ∨. . . ∨tj)(p) == 1))” in the code), the set Terms actually contains at least j
distinct terms of the target concept.
The proof is by induction on j. For the base case, j = 1, the set Terms = T. Thus
the base case is provided by point 1 of Lemma 1, which says that there must must be a term
of the target concept in T at the end of the ﬁrst call to Algorithm Delimit.
For the inductive step, suppose that we are trying to cover Pos with j + 1 terms. Then
we know that we tried and failed to cover the previous version of Pos with j terms. By the
inductive hypothesis, this implies that the previous version of Terms contained at least j
MALICIOUS OMISSIONS AND ERRORS
distinct terms of the target concept. If the previous version of Terms in fact contained more
than j distinct terms of the target concept, then we are done.
Otherwise, the previous version of Terms contained exactly j distinct terms of the target
concept, but nevertheless failed to cover all points in Pos. That is, there was some point p
in the previous version of Pos not covered by any of those particular j terms. This point p
cannot have been delimited before the attempt to cover Pos with j terms, because if it had,
the previous version of Terms would have contained an element covering p. Therefore, after
attempting and failing to cover Pos with j terms p is delimited. Now by point 1 of Lemma 1,
a term of the target concept covering p is in the set T output by this call to Delimit, and
this point is added to Terms. Thus Terms now contains at least j + 1 distinct terms of the
target concept.
Lemma 5 Let h′ be the hypothesis that Algorithm OnlyLMQ obtains when it updates
its hypothesis h. Then h′ covers all points with LMQ 1 that are covered by any term in
Assume the contrary holds for h, Terms, and h′. Let t ∈Terms be a term and x
be a point with LMQ 1 such that h′(x) = 0 and t(x) = 1.
Term t was added to Terms by some call to subroutine Delimit. At that call, according
to point 3 of Lemma 1, the set P must have contained a point p such that t covered p and p
is a descendant of x. Since h′(x) = 0, we know h(x) = 0, so h(p) = 0. Therefore, after
the call to Algorithm Delimit when the outputs had t ∈T and p ∈P, point p was added
to Pos. Thus since h′ is satisﬁed by all points in Pos, it must be that h′(p) = 1 and thus
h′(x) = 1, a contradiction.
Lemma 6 Consider the hypothesis h of Algorithm OnlyLMQ at any point in the run of
it for a monotone DNF target concept c. There is a monotone DNF d consisting of at least
#(h) distinct terms from c such that {v : d(v) = LMQ(v) = 1} ⊆{v : h(v) = 1}.
The proof is by induction on the number of times that h is enlarged by adding new
terms. The base case with an empty h holds trivially. Let us now assume that the lemma
holds up to the k-th time we add new terms to h, and that it does not hold the k + 1-st time.
Let us denote h after these additions of terms by hk and hk+1, respectively. That is:
(1) there are #(hk) terms t1, t2, . . . , t#(hk) in c such that they cover a subset of those
points with LMQ 1 that are covered by hk,
(2) for any way we take #(hk+1) terms from c, there always is some point w with LMQ 1
that is covered by these terms but not by hk+1.
Furthermore, let us assume that exactly j terms were added to hk when making hk+1, that
is, #(hk+1) −#(hk) = j. At the moment these j terms were added, Lemma 4 guaranteed
the existence of j terms t′
2, . . . , t′
j from c in Terms. By Lemma 3 none of t′
2, . . . , t′
was implied by hk. Now suppose we take terms t1, t2, . . . , t#(hk), t′
2, . . . , t′
are all in c, for a total of #(hk+1) terms. By our assumption there is a point w, such that
LMQ(w) = 1, that is covered by these terms but not by hk+1. By the assumption, w
cannot be covered by any of the t1, t2, . . . , t#(hk), or it would be covered by hk and hence
D. ANGLUIN, ET AL.
hk+1. Therefore, it must be the case that w is covered by some of t′
2, . . . , t′
j. But then,
by Lemma 5, hk+1 must cover w, a contradiction.
Lemma 6 implies that the hypothesis h of Algorithm OnlyLMQ never contains more
than k terms, which is what we needed to show. What remains for the proof of Theorem 3
is to show that the number of limited membership queries is O(knk + n2ℓ).
* Number of queries made. A monotone k-term DNF formula can have at most nk maximal
negative points. This follows from the following observations. If v is a negative point and
t is a term, then there must be a variable xi in t such that v[i] = 0, or else v would satisfy
t. Thus, by setting to 0 at least one variable from each term, we can obtain exactly the
negative points. Maximal negative points are a subset of the points that can be obtained by
setting exactly one variable from each term to 0, which can be done in at most nk ways.
Thus, each time S is initialized in subroutine FindPos, it can have at most nk elements.
FindPos is called at most k + 1 times, since each time it is called (except the last), it
returns a new positive point q which causes at least one term to be added to h.
Each call to FindPos performs at most one LMQ for each of at most nk elements
initially in S. After that, each element queried is a child of a point with LMQ ⊥, so the
number of such queries over all calls to FindPos is at most nℓ. (We assume FindPos
caches answers.) Thus, the number of LMQ’s made by FindPos is at most (k+1)nk+nℓ.
The only other LMQ’s are made in calls to Delimit, and by Lemma 2 the total number
of queries is bounded by n(ℓ+ s), where s is the number of calls to Delimit. Delimit
is called at most k times on points q returned by FindPos, and is called at most once for
each element that is ever added to Pos. The elements added to Pos are all returned in P
by Up, which means that they are all parents of points with LMQ ⊥. Hence, at most nℓ
elements are ever added to Pos, and the total number s of calls to Delimit is bounded by
k + nℓ. This gives a bound of n(ℓ+ k + nℓ) for the number of LMQ’s made in the calls
to Delimit. By adding the bound for the number on LMQ’s made in calls to FindPos,
we can easily get the desired bound of O(knk + n2ℓ) on the total number of LMQ’s.
The running time of the algorithm OnlyLMQ is clearly polynomial in the number of
queries it makes.
Learning Monotone DNF from Equivalence and Limited Membership Queries
In this subsection, we give a very simple algorithm that learns monotone DNF with an
unbounded number of terms from equivalence and limited membership queries.
Theorem 4 Monotone DNF formulas can be learned from n(m+ℓ) limited membership
queries together with m + 1 equivalence queries in the nonstrict model, or together with
m + ℓ+ 1 equivalence queries in the strict model, where ℓis the number of ⊥responses
received, and m is the number of terms in the target monotone DNF.
We modify Angluin’s algorithm for learning monotone DNF from ordinary membership and equivalence queries , by using Algorithm Delimit. The com-
MALICIOUS OMISSIONS AND ERRORS
Figure 8. Algorithm EQToo for learning monotone DNF from EQ’s and LMQ’s. v is the current counterexample.
plete code of the algorithm is given in Figure 8. Note that the same algorithm works for
both the nonstrict and strict models.
The Up subroutine of Algorithm Delimit guarantees that all terms we put in h are either
correct, or err only by classifying negative points with LMQ ⊥as positive. Lemma 1
guarantees that each time we call Algorithm Delimit we get a new term of the target
monotone DNF.
In the nonstrict model, terms that cover negative points with LMQ ⊥do not matter, so
there is at most one call to Delimit and one equivalence query per term. In the strict
model we may add terms with LMQ ⊥that cover negative points, and such terms must be
subsequently deleted in response to equivalence queries. There is at most one equivalence
query for each such term, provided that we modify the algorithm to remember deleted terms
and never add them again to the hypothesis. This implies the bounds for the number of
equivalence queries. The bound for the number of LMQ’s then follows from Lemma 2.
The running time of the algorithm is polynomial in the number of queries it makes.
Note that polynomial learnability of monotone DNF from equivalence and limited membership queries is implied by the stronger result of Section 4. It is at least as difﬁcult to learn
from a malicious membership oracle as it is from a limited membership oracle, as pointed
out in Subsection 2.2, so that algorithm for monotone DNF could be used here. The direct
algorithm does, however, give better bounds on the number of queries required.
D. ANGLUIN, ET AL.
Malicious Membership Queries
In this section, we present and analyze an algorithm that uses equivalence and malicious
membership queries to learn monotone DNF formulas.
The key idea is to depend on
equivalence queries as much as possible, since they are correct.
The Algorithm
The algorithm keeps track of all the counterexamples and their labels received through
equivalence queries and consults them ﬁrst, before asking a membership query. The pairs
of counterexamples and their labels are kept in a set named CounterExamples. Obviously,
for a positive counterexample v, if x ≥v then it is not worth making a membership
query about x; it must be a positive point. Similarly, for a negative counterexample v, if
x ≤v then x has to be a negative point of the target formula. For this reason we deﬁne
a subroutine CheckedMQ and use it instead of a membership query. The subroutine is
given in Figure 9.
CounterExamples
CounterExamples
CounterExamples
Figure 9. Subroutine CheckedMQ.
As in and , our algorithm also uses a subroutine Reduce in order to move down in the lattice from a positive counterexample. All the
membership queries are done using the subroutine CheckedMQ, which possibly lets the
algorithm avoid some incorrect answers. The subroutine Reduce is given in Figure 10.
CounterExamples
(CheckedMQ
CounterExamples
CounterExamples
Figure 10. Subroutine Reduce.
The algorithm for exactly identifying monotone DNF formulas using equivalence queries
and malicious membership queries is given in Figure 11.
MALICIOUS OMISSIONS AND ERRORS
LearnMonDNF()
CounterExamples
CounterExamples
CounterExamples
Figure 11. The algorithm for learning monotone DNF from EQ’s and MMQ’s.
The algorithm is based on a few simple ideas. A positive counterexample is reduced to a
point that is added as a term to the existing hypothesis h, which is a monotone DNF. That
is, the new hypothesis classiﬁes the latest counterexample and possibly some other points
as positive.
Negative counterexamples are used to detect inconsistencies between membership and
equivalence queries. They show that there have been errors in membership queries that
have caused wrong terms to be added to the hypothesis. The algorithm reacts by removing
all the terms that are inconsistent with the latest counterexample. These are the terms that
have the negative counterexample above them. A term is removed only when there is a
negative counterexample above it.
Analysis of LearnMonDNF
Theorem 5 LearnMonDNF learns the class of monotone DNF formulas in polynomial time using equivalence and malicious membership queries.
We need a deﬁnition and a simple lemma before proving the theorem.
Let h∗be a monotone boolean function on {0, 1}n, and let h′ be an arbitrary boolean
function on {0, 1}n. Let C be any subset of {0, 1}n. The monotone correction of h′ with h∗
on C, denoted mc(h′, h∗, C), is the boolean function h′′ deﬁned for each string x ∈{0, 1}n
as follows.
D. ANGLUIN, ET AL.
if there exists y ∈C such that y ≤x and h∗(y) = 1,
if there exists y ∈C such that x ≤y and h∗(y) = 0,
h′(x) otherwise.
Note that since h∗is monotone, the ﬁrst two cases above cannot hold simultaneously.
It is also clear that if the value of h′′(x) is determined by one of the ﬁrst two cases,
h′′(x) = h∗(x). We prove a simple monotonicity property of the monotone correction
operation.
Lemma 7 Suppose h∗is a monotone boolean function and h′ is an arbitrary boolean
function on {0, 1}n. Let C1 ⊆C2 be two subsets of {0, 1}n. Let h1 = mc(h′, h∗, C1) and
h2 = mc(h′, h∗, C2). Then the set of points on which h2 and h∗differ is contained in the
set of points on which h1 and h∗differ. That is, Err(h2, h∗) ⊆Err(h1, h∗).
Let x be an arbitrary point on which h2(x) ̸= h∗(x).
Then it must be that
h2(x) = h′(x) and there does not exist any point y ∈C2 such that x ≤y and h∗(y) = 0
or y ≤x and h∗(y) = 1. Since C1 is contained in C2, there is no point y ∈C1 such
that x ≤y and h∗(y) = 0 or such that y ≤x and h∗(y) = 1. Thus, h1(x) = h′(x) and
h1(x) ̸= h∗(x). Consequently, Err(h2, h∗) ⊆Err(h1, h∗).
Now we start the proof of Theorem 5.
Let h∗denote the target concept, an arbitrary monotone DNF formula over {0, 1}n
with m terms. Let ℓbe a bound on the number of strings whose MMQ’s are answered
incorrectly. Because equivalence queries are answered correctly, if the algorithm ever halts,
the hypothesis output is correct, so we may focus on proving a polynomial bound on the
running time.
Since LearnMonDNF is deterministic and the target concept h∗is ﬁxed, we may
assume that the adversary chooses in advance how to answer all the queries, that is, chooses
a sequence y1, y2, . . . of counterexamples to equivalence queries and a set S of strings on
which to answer MMQ’s incorrectly. Note that |S| ≤ℓ.
In turn, these choices determine a particular computation of LearnMonDNF which we
now focus on. It sufﬁces to bound the length of this computation. In this computation the
answers to MMQ’s agree with the boolean function h0 deﬁned as follows. h0(x) = h∗(x)
for all strings x ̸∈S and h0(x) = 1 −h∗(x) for all strings x ∈S. Also, if CheckedMQ
is called with string x and set C = CounterExamples, the answer agrees with the boolean
function mc(h0, h∗, C).
The set CounterExamples only changes when a new counterexample is received. Therefore, the successive distinct sets of counterexamples in this computation can be denoted by
C0, C1, . . ., where C0 =
   and Ci = Ci−1 ∪{yi}, for i = 1, 2, . . .. If we also deﬁne
hi = mc(h0, h∗, Ci)
for i = 1, 2, . . ., then CheckedMQ answers according to h0 until the ﬁrst counterexample
is received, then according to h1 until the second counterexample is received, and so on.
Clearly, since h0 disagrees with h∗on at most ℓstrings, d(h0, h∗) ≤ℓ.
sets C0, C1, . . . are monotonically nondecreasing, Lemma 7 shows that Err(hi, h∗) ⊆
Err(hi−1, h∗) for i = 1, 2, . . ..
MALICIOUS OMISSIONS AND ERRORS
We say that a counterexample yi corrects a positive error at point x if hi−1(x) = 1 but
hi(x) = h∗(x) = 0. We say that a counterexample yi corrects a negative error at point x if
hi−1(x) = 0 but hi(x) = h∗(x) = 1. Note that from the construction of CheckedMQ it
follows that positive errors can be corrected only by negative counterexamples and negative
errors can be corrected only by positive counterexamples. Let there be ℓp positive and ℓn
negative errors corrected in the whole computation. Of course, ℓp + ℓn ≤ℓ.
Claim 1 If Reduce is called after counterexample yi and before counterexample yi+1,
it returns a local minimum point of hi.
After yi is added to CounterExamples, CheckedMQ answers according to hi.
The claim follows from the construction of Reduce.
Claim 2 The following condition is preserved. At the (i+1)th equivalence query EQ(h),
each term of h is a positive point of hi.
We prove the claim by induction.
Basis: The ﬁrst EQ is made on an empty formula. Thus, the claim is vacuously true.
Induction step: Suppose the claim is true up to the ith EQ. Let h′ be the hypothesis
h at the ith EQ and h′′ be the hypothesis h at the (i + 1)th EQ. There are two cases to
Case 1: yi is a positive counterexample. Then hi(x) = 1 if and only if hi−1(x) = 1 or
x ≥yi. LettbethetermreturnedbyReducewithparametersyi andCounterExamples.
Then h′′ = h′∨t. Let t′′ be a term in h′′. Then either t′′ is a term of h′ or t′′ = t. If t′′ is
a term of h′ then hi−1(t′′) = 1 by the inductive assumption and therefore hi(t′′) = 1.
If t′′ = t then hi(t′′) = 1 since t was returned by Reduce(yi, CounterExamples)
which used CheckedMQ, which answered according to hi.
Case 2: yi is a negative counterexample. Then hi(x) = 1 if and only if hi−1(x) = 1
and x ̸≤yi. Let t′′ be a term in h′′, which consists of all those terms t′ of h′ such that
t′ ̸≤yi. Therefore, t′′ ̸≤yi and by the inductive assumption hi−1(t′′) = 1. It follows
that hi(t′′) = 1.
Claim 3 Once a term x is deleted from hypothesis h, it can never reappear in it.
Since x was deleted, there must have been a negative counterexample yi such
that yi ≥x. But then (yi, 0) belongs to CounterExamples and the call CheckedMQ(x,
CounterExamples) can never return 1 again, which is necessary for x to be added to h.
We divide the run of the algorithm into non-overlapping stages. A new stage begins either
at the beginning of the run or with a new negative counterexample. Thus with each new
stage CounterExamples contains one more negative counterexample and some (possibly
none) new positive counterexamples. The following claim establishes that the distance
d(hi, h∗) decreases with every new stage.
D. ANGLUIN, ET AL.
Claim 4 Every negative counterexample corrects at least one error. More formally, if yi
is a negative counterexample, then there exists x ∈{0, 1}n such that hi−1(x) = 1 and
hi(x) = h∗(x) = 0.
Let yi be a negative counterexample returned by EQ(h). Hence h(yi) = 1, and
there is some term x ≤yi in h. By Claim 2, hi−1(x) = 1.
Since h∗(yi) = 0 and yi ≥x it follows that h∗(x) = 0. By the deﬁnition of hi it follows
that hi(x) = 0.
From Claim 4 it follows that there are at most ℓp negative counterexamples. Hence there
are at most ℓp + 1 stages in the run of the algorithm.
We divide each stage of the algorithm into non-overlapping substages. A substage begins
either at the beginning of a stage or with a new positive counterexample that corrects an
error. Obviously there can be no more than ℓn positive counterexamples that correct errors
and hence no more than ℓp + ℓn + 1 substages in the whole run of the algorithm. The
distance d(hi, h∗) decreases with every new substage. If, however, functions hi and hj
belong to the same substage, they are equivalent and their local minima are the same. This
allows us to bound the total number of positive counterexamples.
Claim 5 Every new positive counterexample is reduced to a local minimum point of
h0, h1, . . . that has not been found earlier.
Let v be a positive counterexample that Reduce is started with. Let t be the point
Reduce(v, CounterExamples)returns. Assume, bywayofcontradiction, thatthasalready
been found before. From Claim 3 it follows that t is a term in h. Since v ≥t, it follows that
h(v) = 1. This is a contradiction to the assumption that v is a positive counterexample.
We denote the set of local minimum points of a boolean function f by Lmp(f). We bound
the total number of different local minima of the functions h0, h1, . . ..
Lemma 8 Let f and f ′ be n-argument boolean functions such that Err(f, f ′) = {x}.
(a) If f ′(x) = 1 then |Lmp(f ′) −Lmp(f)| ≤1.
(b) If f ′(x) = 0 then |Lmp(f ′) −Lmp(f)| ≤n.
(a) The only point that can be a local minimum of f ′ and is not a local minimum of f, is
x itself. The claim follows immediately.
(b) Any point which is a local minimum of f ′ but not of f is a parent of x. Since x has at
most n parents, the claim follows.
MALICIOUS OMISSIONS AND ERRORS
Corollary 2 Let f and f ′ be n-argument boolean functions such that Err(f, f ′) contains dp positive points of f ′ and dn negative points of f ′. Then
|Lmp(f ′) −Lmp(f)| ≤ndn + dp.
Corollary 3 Let g0, g1, . . . , gr be the subsequence of h0, h1, . . ., such that each gi is
the ﬁrst of all the hj’s in its substage. Let Err(h∗, gi−1) −Err(h∗, gi) contain ℓp,i−1
positive and ℓn,i−1 negative points of h∗for all i = 1, 2, . . . , r. Let Err(h∗, gr) contain
ℓp,r positive and ℓn,r negative points of h∗. Then the total number of different local minima
of functions g0, g1, . . . , gr, h∗is bounded above by m + n Pr
i=0 ℓn,i + Pr
Note that g0, g1, . . . , gr are the different functions in h0, h1, . . ., and that subroutine
CheckedMQ ﬁrst answers according to g0, then according to g1 and so on. Obviously,
Err(h∗, gi) ⊆Err(h∗, gi−1) and Err(gi−1, gi) = Err(h∗, gi−1) −Err(h∗, gi) for all
i = 1, 2, . . . , r. Also note that for each i = 0, 1, . . . , r −1, one of ℓp,i and ℓn,i is 0, but
ℓp,r and ℓn,r may both be positive.
We want to ﬁnd
i=0 Lmp(gi) ∪Lmp(h∗)
¯¯, knowing that
¯¯ = m. Since
Lmp(gi) ∪Lmp(h∗)
⊆Lmp(h∗) ∪
Lmp(gr) −Lmp(h∗)
Lmp(gi) −Lmp(gi+1)
from Corollary 2 it follows that
Lmp(gi) ∪Lmp(h∗)
nℓn,r + ℓp,r
nℓn,i + ℓp,i
and the bound follows.
Since each error can be corrected at most once, it follows that Pr
i=0 ℓn,i ≤ℓn and
i=0 ℓp,i ≤ℓp. Hence the total number of the local minima and the total number of positive
counterexamples that can be found in a computation is bounded by m + nℓn + ℓp. The
number of negative counterexamples in a complete run is bounded by the number of positive
errors. The total number of counterexamples is therefore bounded by m+ℓnn+ℓp +ℓp ≤
m + ℓ(n + 1) = O(m + ℓn).
We now count the number of membership queries in a complete run of the algorithm.
Each positive counterexample v may cause at most n(n+1)/2 membership queries, before
Reduce(v, CounterExamples) returns. Therefore there can be at most O(mn2 + ℓn3)
membership queries in a complete run of the algorithm.
It is also clear that the running time of the algorithm is polynomial in m, n and ℓ. This
concludes the proof of Theorem 5.
Comparing LearnMonDNF with the algorithm EQToo of Theorem 4, we see that
LearnMonDNF is able to cope with MMQ’s instead of the more benign LMQ’s, but
D. ANGLUIN, ET AL.
at a cost of making more queries overall. In particular, it uses O(m + ℓn) equivalence
queries, versus m + ℓ+ 1 for EQToo, and O(mn2 + ℓn3) membership queries, versus
mn + ℓn for EQToo. It is open whether an algorithm to learn monotone DNF formulas
using EQ’s and MMQ’s can attain query complexity closer to that of EQToo.
Finite Exceptions
Exceptions
For a concept (X, f) and a ﬁnite set S ⊆X, we deﬁne the concept (X, f) with exceptions
S, denoted xcpt((X, f), S), as the concept (X, f ′) where f ′(w) = f(w) for strings in
X −S, and f ′(w) = 1 −f(w) for strings in S. (Thus f and f ′ have the same domain,
and are equal except on the set of strings S, which is a subset of their common domain.) It
is useful to note that S is partitioned by (X, f) into into the set of positive exceptions S+
that are classiﬁed as negative by f, and the set of negative exceptions S−that are classiﬁed
as positive by f. When the domain X of a function f is clearly understood and we do not
wish to mention it explicitly, we often just call this function itself a concept and we also
use a shorthand notation for xcpt((X, f), S), namely, we just write xcpt(f, S).
A concept class (R, Dom, µ) is closed under ﬁnite exceptions provided that for every
concept (X, f) represented by (R, Dom, µ) and every ﬁnite set S ⊆X, the concept
xcpt((X, f), S) is also represented by (R, Dom, µ). If, in addition, there is a ﬁxed polynomial of two arguments such that the concept xcpt((X, f), S) is of size bounded by this
polynomial in the size of (X, f) and ||S||, we say that (R, Dom, µ) is polynomially closed
under ﬁnite exceptions.
This deﬁnition differs from a similar earlier deﬁnition in that we do
not require the existence of a polynomial-time algorithm that produces the new concept
given the old concept and a list of exceptions. However, for the classes that we consider
there are such algorithms.
We deﬁne a natural operation of adding ﬁnite exception tables to a class of concepts to
produce another class of concepts that “embeds” the ﬁrst and is polynomially closed under
ﬁnite exceptions.
We assume Σ ⊆Γ and |Γ| ≥2. We deﬁne a simple encoding e that takes a string r from
Γ∗and a ﬁnite set of strings S ⊆Σ∗and produces a string r′ in Γ∗from which r and the
elements of S can easily be recovered, and is such that |r′| = 2(1+|r|+||S||). The details
of the encoding are as follows.
Assume that 0 and 1 are distinct symbols in Γ. We deﬁne
eb(b1b2 . . . bj)
= bbbb1bb2 . . . bbj,
for b ∈{0, 1} and b1, b2, . . . , bj ∈Γ. Note that |eb(w)| = 2(1 + |w|) for every string
w ∈Γ∗. We then deﬁne the encoding of r and S as
r′ = e(r, S)
= e0(r)e1(s1)e0(s2) . . . ek mod 2(sk),
where s1, s2, . . . , sk are the strings in S.
MALICIOUS OMISSIONS AND ERRORS
Given a concept class (R, Dom, µ), we deﬁne the class obtained from it by adding exception tables to be (R′, Dom′, µ′), where R′ is the set of all strings of the form e(r, S)
such that r ∈R and S is a ﬁnite subset of Dom(r), and for each r′ ∈R′, the concept
represented by r′ = e(r, S) is the concept represented by r with exceptions S, that is,
(Dom′(r′), µ′(r′)) = xcpt((Dom(r), µ(r)), S).
For example, adding exception tables to the monotone DNF formulas produces a concept
class which we term monotone DNF formulas with ﬁnite exceptions. More detailed discussion of classes obtained by adding exception tables and of polynomial closure under ﬁnite
exceptions can be found in Subsection 5.2.
Examples and Lemmas
Example: The class of regular languages represented by DFA’s is polynomially closed
under ﬁnite exceptions. Board and Pitt give an algorithm that takes as input a DFA M and
an exception set S, and produces a new DFA for xcpt(M, S) . The
DFA’s size is polynomial in the size of M and S.
Example: Another example of a class that is polynomially closed under ﬁnite exceptions
is the class of boolean decision trees. This result is taken from but
since the construction is not given there, we sketch it here.
Lemma 9 The class of boolean decision trees is polynomially closed under ﬁnite exceptions.
Let T be a decision tree on n variables. Let S be the exception set for T. We
construct the decision tree for xcpt(T, S) as follows. We treat each exception point x ∈S
individually. First we walk down from the root of the original tree T to see where x
is located in it. If this leads us to a leaf with depth n (i.e., if all variables are tested
on this path), then we just reverse the value of the leaf, because this path is for x only.
However, if we ﬁnd ourselves at a leaf with depth less than n, we have to add new internal
nodes to the tree. Denote the value of this leaf by b. We then continue the path that led
us to this leaf with a path in which all the remaining variables are tested. We end the
path by a leaf with value 1 −b. For each new internal node on the path, we make the
other child (the one not on the path) a leaf, and give it the original value b. Thus, each
counterexample adds at most n new internal nodes to the tree. The size of the new tree,
measured as the number of internal nodes, is bounded by |T| + n × |S| = |T| + ||S||.
Example: One more interesting example is the class of DNF formulas.
Lemma 10 The class of DNF formulas is polynomially closed under ﬁnite exceptions.
Let f be an m-term DNF formula over n variables and S be an exception set
for it. Let S be partitioned into the sets of positive and negative exceptions (S+ and S−,
respectively), as described in Section 5.1. We construct a DNF formula for xcpt(f, S) from
the formula (f ∧f−) ∨f+, where f−is a DNF formula which is true on all the points in
D. ANGLUIN, ET AL.
its domain except the ones in S−, and f+ is a DNF formula which is true exactly on the
points in S+. The domain for all these formulas is {0, 1}n.
Obtaining f+ is easy—straightforward disjunction of all the terms in S+, where we make
terms from points by substituting the respective variable for a 1 value of a coordinate and its
negation for a 0 value. Obtaining f−is harder. First we make a decision tree corresponding
to f−. We put each point from S−individually in the tree as a 0-valued leaf at the end of
a path of length n. All the remaining leaves get value 1. Then for each leaf with value
1 we make a term that will go into f−by following the path from this leaf to the root.
Obviously f−has at most n × |S−| terms. Thus, after “multiplying” the terms out, the
formula (f ∧f−) ∨f+ will have at most mn × |S−| + |S+| ≤(mn + 1) × |S| terms.
Example: By duality it follows that the class of CNF formulas is polynomially closed
under ﬁnite exceptions.
Note that stronger bounds on the size of the new formula can be obtained by using the
result in . We, however, chose to present a simpler argument.
Also note that the size bound is insufﬁcient for strong polynomial closure under exception
lists as deﬁned in .
Example: As our ﬁnal example we show that any class that is obtained by adding exception tables to another class is polynomially closed under ﬁnite exceptions.
Lemma 11 Let (R, Dom, µ) be any class of concepts. Then the concept class obtained
from it by adding exception tables is polynomially closed under ﬁnite exceptions.
Let (R′, Dom′, µ′) be the class obtained from (R, Dom, µ) by adding exception
tables, as deﬁned in Section 5.1. Let (X′, f ′) be any concept from (R′, Dom′, µ′) and
let r′ ∈R′ be a shortest representation of (X′, f ′). Then there exists a concept r ∈R
and a ﬁnite set S ⊆Dom(r), such that (Dom′(r′), µ′(r′)) = xcpt((Dom(r), µ(r)), S)
and |r′| = 2(1 + |r| + ||S||). Let S′ ⊆Dom′(r′) = Dom(r) be any ﬁnite set. Let
concept h′′ be deﬁned as h′′ def
= xcpt((Dom′(r′), µ′(r′)), S′). It is easy to see that h′′ =
xcpt((Dom(r), µ(r)), S △S′) and thus h′′ is represented by some r′′ ∈R′ with size
2(1 + |r| + ||S △S′||) ≤2(1 + |r| + ||S|| + ||S′||) = |r′| + 2||S′||.
Corollary 4 TheclassofmonotoneDNFformulaswithﬁniteexceptionsispolynomially
closed under ﬁnite exceptions.
Learning Monotone DNF Formulas With Finite Exceptions
In this section, we present an algorithm that learns the class of monotone DNF formulas
with ﬁnite exceptions. The target concept is a boolean function on n variables h∗def
M, S∗), where h∗
M is some monotone DNF formula and S∗is a set of exceptions for
it. The domain of the target concept is {0, 1}n.
We assume that we have an upper bound on the cardinality of S∗and denote it by l (i.e.,
|S∗| ≤l). If this bound is not known, we can start out by assuming it to be any positive
MALICIOUS OMISSIONS AND ERRORS
integer and doubling it whenever convergence is not achieved within the proper time bound,
which will be given later. We assume that h∗
M is minimized and has m terms.
Like LearnMonDNF, our current algorithm also has a set CounterExamples that stores
all labeled counterexamples received from equivalence queries. The purpose of it is slightly
different: it lets the algorithm conclude that some points cannot be classiﬁed by h∗
and, therefore, have to be included in the exception set.
The algorithm tries to ﬁnd a suitable monotone DNF formula, which, coupled with a
proper exception set, would give the target concept. The equivalence queries are made on a
pair ⟨h, S⟩of a monotone DNF formula h and a set of exceptions S. The algorithm focuses
only on building h, and sets S to be those elements of the set CounterExamples that are
currently misclassiﬁed by h. It uses a simple subroutine GetExceptions for building S.
The subroutine is given in Figure 12.
ceptions(h,
CounterExamples
CounterExamples
Figure 12.
Subroutine GetExceptions.
h is the monotone DNF part of the current hypothesis;
CounterExamples is the set of pairs of counterexamples and their labels seen so far; S is the set of those counterexamples that are misclassiﬁed by h.
In order to classify the counterexamples received, the algorithm needs to evaluate the current function xcpt(h, S). This is done by another very simple subroutine TheFunction,
given in Figure 13.
TheFunction , , and Section 4, our algorithm also uses
a subroutine Reduce to move down in the lattice from a positive counterexample. Its
goal is to reduce the positive counterexample to some point that can be added as a term to
the formula h. Then the new hypothesis would classify the counterexample and possibly
some other points as positive. However, this may not always be possible. There can be
D. ANGLUIN, ET AL.
overwhelming evidence that the candidate point is just a positive exception and thus should
not be added to h. More precisely, if there are more than l negative counterexamples above
a term of h, then they all have to be in the exception set, which is then too big. Therefore
the current subroutine Reduce is somewhat more complex and checks whether a point
has enough evidence to be an undoubted exception point or not. The subroutine is given in
Figure 14.
CounterExamples
CounterExamples
CounterExamples
Figure 14. Subroutine Reduce. CounterExamples is the set of counterexamples and their labels seen so far; l is
the bound on the number of exception points, a globally known constant.
The algorithm for learning monotone DNF formulas with at most l exceptions using
equivalence queries and membership queries is given in Figure 15.
The algorithm is based on the following ideas. Each positive counterexample is reduced
if possible to a new term to be added to the formula, as was explained above. In case this
is not possible, the algorithm beneﬁts anyway by storing it in the set CounterExamples.
Negative counterexamples imply that there are not as many positive points in the target
concept as we thought. Sometimes more exception points are necessary for the hypothesis
to be correct. Other times some terms have to be removed from the formula. Deleting a
term happens only when there is enough evidence that a term is wrong, namely, when there
are more than l negative counterexamples above it.
Correctness and Complexity of the Algorithm
Theorem 6 LearnMonDNFwithFX learns the class of monotone DNF formulas
with exceptions in polynomial time using equivalence and standard membership queries.
We begin the analysis with this simple claim.
Claim 6 Once a term t is deleted from hypothesis h, it can never reappear in it.
A term t can be deleted only if there are more than l negative counterexamples
above it. To reappear, t must be returned by Reduce. But every point returned by Reduce
must have at most l negative counterexamples above it at the time it is returned, so Reduce
cannot return t again.
The following lemma shows what points Reduce can return.
Lemma 12 Reduce always returns either a local minimum of h∗or a parent of a positive
exception in S∗.
MALICIOUS OMISSIONS AND ERRORS
LearnMonDNFwithFX()
CounterExamples
 TheFunction(h;
CounterExamples
(TheFunction
CounterExamples
CounterExamples
CounterExamples
CounterExamples
CounterExamples)
Figure 15. The algorithm for learning monotone DNF formulas with ﬁnite exceptions. CounterExamples is the
set of counterexamples and their labels seen so far; l is the bound on the number of exception points, a globally
known constant; h is the monotone DNF part of the current hypothesis; S is the set of points in CounterExamples
misclassiﬁed by h.
First note that Reduce can only be called on points x such that h∗(x) = 1 and
can only return points w such that h∗(w) = 1. Let w be a point returned by Reduce.
Assume w is not a local minimum point of h∗. Then there is some child y of w such that
h∗(y) = 1, and the number of negative counterexamples above y must exceed l (or else
Reduce would have been called recursively on y). Hence, y cannot be above any term t
M, since each term t can have at most l negative counterexamples above it. Therefore,
y is a positive exception in S∗.
Now we are ready to bound the number of different points that can be returned by the
subroutine Reduce.
Claim 7 The number of different points that Reduce can return is at most m+(n+1)l.
By Lemma 12, the number of different points that can be returned by Reduce is
at most the number of points that are local minima of h∗or parents of positive exceptions
in S∗. Let S∗contain lp positive exceptions and ln negative exceptions, where lp + ln ≤l.
The formula h∗
M has m terms and therefore m local minima. By Lemma 8, the number of
local minima of h∗is at most m + lp + nln. Each positive exception has at most n parents,
so the number of parents of positive exceptions is bounded by nlp. Thus, the number of
D. ANGLUIN, ET AL.
different points Reduce can return, and the number of calls to Reduce, is bounded by
m + (n + 1)lp + nln ≤m + (n + 1)l.
All equivalence queries are asked about the current hypothesis xcpt(h, S). Since S is
computed right before each equivalence query, the argument of an equivalence query is
always consistent with all the counterexamples seen to that point. Let hi and hj denote the
function xcpt(h, S) at the time when ith and jth equivalence query is asked, respectively,
and let i < j. Let vi be the counterexample returned by the ith equivalence query. Clearly,
the values of hi(vi) and hj(vi) must be different. Thus, the function xcpt(h, S) is different
for each equivalence query. This allows us to bound the total number of equivalence queries.
Claim 8 The number of equivalence queries before success is bounded by O(m2n2l3).
We examine howxcpt(h, S) changes. Either h itself changes, orh remains the same
and S changes; namely, it contains exactly one more point, the most recent counterexample.
By Claim 6, each term of h can appear in h or disappear from it only once. Thus each
possible term can induce at most two changes in formula h—ﬁrst by appearing in it and
then by disappearing. Thus, h can only change twice as many times as the number of terms
that Reduce can return. Therefore, by Claim 7, there can be at most 2(m + (n + 1)l) + 1
different functions h in a complete run of the algorithm.
We now count the number of times S can change while h remains the same. Set S grows
larger by one with each new counterexample. It contains some (possibly none) points x
such that h(x) = 1 and some (possibly none) points x such that h(x) = 0. We bound the
number of each of these separately.
Each point x ∈S such that h(x) = 1 is above some term of h. No term can have more
than l negative counterexamples above it. Therefore, the number of points x ∈S such that
h(x) = 1 can be bounded by l times the bound m + (n + 1)l on the number of different
terms of h, that is, by ml + (n + 1)l2.
Each point x ∈S such that h(x) = 0 is a positive counterexample, and thus is not
above any term in h. Such an x must have more than l negative counterexamples above it.
Otherwise, the algorithm would have called Reduce on x and added a new term t ≤x to
h. If x has more than l negative counterexamples above it, then it cannot be above a term
M and thus has to be a positive exception in S∗. Hence we have a bound of lp on the
number of points x ∈S such that h(x) = 0.
Altogether, we can bound the cardinality of S by |S| ≤ml + (n + 1)l2 + lp ≤(m +
1)l + (n + 1)l2. While h stays the same, the number of possible different sets S is at most
(m + 1)l + (n + 1)l2 + 1.
Hence, the total number of equivalence queries in a complete run of the algorithm is
bounded by (2(m + (n + 1)l) + 1) × ((m + 1)l + (n + 1)l2 + 1) = O(m2n2l3).
We now count the total number of membership queries. Membership queries are made
only in Reduce, at most n(n + 1)/2 per call to Reduce. Claim 7 bounds the number of
different points that Reduce can return by m + (n + 1)l. By Claim 6, the number of calls
to Reduce is bounded by the number of different points that it can return. Therefore, the
total number of membership queries is bounded by O(mn2 + n3l).
MALICIOUS OMISSIONS AND ERRORS
It is not difﬁcult to see that the total running time of the algorithm is polynomial in n, m
and l. This concludes the proof of Theorem 6.
Comparison of the Models
In this section, we compare the models of learning discussed earlier, and give a relation between learning concepts with exceptions and learning with malicious membership queries.
Exceptions and Lies
In this subsection, we give a generic algorithm transformation. This transformation shows
that any class of concepts that is polynomially closed under ﬁnite exceptions and learnable
in polynomial time with equivalence and standard membership queries is also learnable in
polynomial time using equivalence and malicious membership queries.
Theorem 7 Let H be a class of concepts that is polynomially closed under ﬁnite exceptions and learnable in polynomial time with equivalence and standard membership queries.
Then H is learnable in polynomial time with equivalence and malicious membership queries.
LetH = (R, Dom, µ)beatargetclassofconceptsthatispolynomiallyclosedunder
ﬁnite exceptions. We assume that Learn is an algorithm to learn H using equivalence (EQ)
and standard membership queries (MQ) in time pA(s, n), for some polynomial pA. Without
loss of generality, pA is non-decreasing in both arguments. We transform this algorithm
into algorithm LearnAnyway, which learns any concept h∗∈H using equivalence and
malicious membership queries in time polynomial in |h∗|, n and the table-size L of the set
of strings on which MMQ may lie.
As in Sections 4 and 5.3 the main idea is to keep track of all the counterexamples seen
and to use them to avoid unnecessary membership queries. For this purpose we use a set
CounterExamples again. As before it stores pairs of counterexamples and their labels. Now,
before asking a membership query about string x, we scan CounterExamples to see whether
it already contains x and a label for it. If x and the label are found, the algorithm knows
the answer and does not make the query. (For some concept classes, such as monotone
DNF formulas, it might be possible to infer the classiﬁcation of x according to the target
concept h∗even though x and its label are not contained in CounterExamples. However,
this simple checking sufﬁces for our algorithm and, what is more important, works in the
general case.)
Another idea is to keep track of the answers received from membership queries, and to
use them to conclude that MMQ has lied. For this purpose LearnAnyway has a set
MembershipAnswers. This set stores pairs ⟨x, b⟩for which MMQ was called on string
x and returned answer b. After receiving a new counterexample from EQ, the algorithm
stores it in CounterExamples and checks whether this counterexample is already contained
in MembershipAnswers. If it is present in MembershipAnswers with the wrong label, the
algorithm discards everything except the set CounterExamples and starts from scratch. If
this is not the case, the algorithm continues the simulation of Learn, which we now
describe in detail.
D. ANGLUIN, ET AL.
CounterExamples
CounterExamples
Figure 16. Subroutine NewMQ. CounterExamples is the set of counterexamples and their labels seen so far;
MembershipAnswers is the set of points queried using MMQ and the corresponding answers.
The new algorithm simulates Learn on the target concept, but modiﬁes Learn’s queries
as follows:
Each membership query MQ(x) of algorithm Learn is replaced by a subroutine
call NewMQ(x, CounterExamples, MembershipAnswers). The subroutine is given in
Figure 16.
Each equivalence query of Learn, x = EQ(h), as well as the output statement,
Output h, is replaced by the block of code given in Figure 17.
CounterExamples
CounterExamples
Figure 17. The block of code replacing “x = EQ(h)” or “Output h”. h is the current hypothesis; x is the current
counterexample; CounterExamples is the set of counterexamples and their labels seen so far; MembershipAnswers
is the set of points queried using MMQ and the corresponding answers.
Note that when the simulation is restarted, only the set CounterExamples reﬂects any work
done so far. We now show that LearnAnyway is correct and runs in time polynomial in
|h∗|, n, and L. We partition the run of the algorithm into stages, where a stage begins with
a new simulation of Learn. First we show that a stage cannot last forever.
MALICIOUS OMISSIONS AND ERRORS
Claim 9 Every stage ends in time polynomial in |h∗|, n, and L.
Note that H is polynomially closed under ﬁnite exceptions, which means that there
is a polynomial p(·, ·) such that for every concept h ∈H and every ﬁnite set S ⊆Dom(h)
there exists a concept h′ ∈H equal to xcpt(h, S) such that size |h′| ≤p(|h|, ||S||). Without
loss of generality we can assume that p is non-decreasing in both arguments. We now prove
that each stage ends in time bounded by pA(p(|h∗|, L), n), where we count only the time
spentonLearnoperations(i.e., wedonotcountthesimulationandbookkeepingoverhead).
We prove this by contradiction. Assume that stage i goes over the limit. Let us look at
the situation right after the number of simulated steps of Learn exceeds our stated time
bound. Let Si denote the set of strings the MMQ has lied about during this stage, up to
the time bound. Let n denote the length of the longest counterexample received during this
stage, up to the time bound.
None of the strings in Si can belong to CounterExamples. Assume by way of contradiction
otherwise. Let x ∈Si be a string contained in CounterExamples with some label. Set Si
contains exactly the strings that theMMQ lied on in this stage and time bound, so there was a
query MMQ(x). It must have happened before x was added to CounterExamples. But then
at the moment x was added to CounterExamples it already belonged to MembershipAnswers
and an inconsistency had to be found. The stage had to end.
Therefore, considering Si as an exception set, all the information received by Learn in
this stage and within the given time bound is consistent with the concepth′ = xcpt(h∗, Si) ∈
H. Learn either has to output h′ in time bounded by
p(|h∗|, ||Si||), n
p(|h∗|, L), n
or it has to receive a counterexample x ∈Si.
In the former case, LearnAnyway
makes an equivalence query EQ(h′) and receives a counterexample x ∈Si, since only
counterexamples from Si are possible at that point. In either case, an element of Si is
added to CounterExamples by the above time bound, which we showed above was impossible. This is a contradiction to the assumption that stage i goes over this bound.
What remains is to show that there can be only a small number of stages. That is, we do
not restart the simulation too many times.
Claim 10 There are at most L + 1 stages in the run of the algorithm LearnAnyway.
At the beginning of each stage (except the ﬁrst one) the algorithm discovers a new
string where the MMQ lies and from then on MMQ can never lie on this string again,
because it is added to CounterExamples. To be more precise, MMQ does not get a chance
to lie on this string because it is never asked about it again. Let S be the set of the strings
that MMQ lies on. Since |S| ≤||S|| ≤L, in stage L + 1 the MMQ can lie on no strings
(i.e., it is not asked queries about any of the strings where it may lie). Therefore Learn
has to converge to the target concept h∗.
The time spent on simulation and bookkeeping is clearly polynomial in |h∗|, n, and L.
Thus, LearnAnyway is a polynomial-time algorithm that uses equivalence and malicious
D. ANGLUIN, ET AL.
membership queries to learn the class of concepts H = (R, Dom, µ). This concludes the
proof of Theorem 7.
As corollaries of Theorem 7 we have the following.
Corollary 5 The class of regular languages, represented by DFA’s, is learnable in
polynomial time with equivalence and malicious membership queries.
In it was shown that this class of concepts is polynomially
closed under ﬁnite exceptions. In it was shown that it is learnable in
polynomial time using membership and equivalence queries.
Corollary 6 The class of boolean decision trees is learnable in polynomial time with
extended equivalence and malicious membership queries.
Lemma 9 shows that the class of boolean decision trees is polynomially closed
under ﬁnite exceptions. In it was shown that it is learnable in polynomial
time using membership and extended equivalence queries.
Corollary 7 The class of monotone DNF formulas with ﬁnite exceptions is learnable
in polynomial time with equivalence and malicious membership queries.
Corollary 4 shows that the class of monotone DNF formulas with exceptions is
polynomially closed under ﬁnite exceptions. In Section 5.3 we gave an algorithm that learns
this class in polynomial time with membership and equivalence queries.
Note that we can also learn the class of monotone DNF formulas without any exceptions
with this generic algorithm, using extended equivalence and malicious membership queries,
since it is just a subclass of the class that allows exceptions. However, the algorithm is much
less efﬁcient than the one described in Section 4.
Learning with and without “Don’t knows”
In this subsection, we digress from exceptions and malicious membership queries, and
focus again on limited membership queries and standard membership queries. We present
a lower bound result, the proof of which has ideas useful in further subsections.
We start by brieﬂy describing a method for converting any algorithm for exact identiﬁcation from membership and equivalence queries to one that works for limited membership
queries. We can also show that in some cases an exponential blowup in the number of
queries is necessary.
Theorem 8 Every concept class that is learnable with m equivalence and membership
queries is learnable with 2ℓ(m−ℓ+1)+ℓ−1 equivalence and limited membership queries,
where ℓis the number of ⊥responses received.
Let Algorithm A exactly identify concept class C from at most m equivalence and
membership queries. We construct a learning algorithm A′ for equivalence and limited
MALICIOUS OMISSIONS AND ERRORS
membership queries as follows: For each instance x such that LMQ(x) = ⊥, start running
two copies of A in parallel, one assuming x is positive and the other assuming that x is
negative. Furthermore, store all the queries and their answers in a global table, and do not
repeat a query that has already been made (possibly by another copy of A). For each copy
of A, if the answers that it has seen (including the guesses for the ⊥answers) are consistent
with some concept from C, then it must output a ﬁnal hypothesis after at most m queries
(including the ⊥ones). Those copies that have answers inconsistent with any concept from
C may be stopped; this will take at most m queries. For those copies that do obtain the
ﬁnal hypothesis after at most m queries (except for one, possibly), we may have to ask the
ﬁnal equivalence query to see which one of them has the correct answer. But, obviously,
some copy of A will make the correct guesses for the ⊥answers and therefore it will have
a correct ﬁnal hypothesis after at most m queries.
The exact bound can be proven by induction on the number ℓof ⊥answers out of the
total m of EQ’s and LMQ’s. The proof is easier if we think about the computation of A′
as a tree. Every query that A′ makes is a node in the tree. Each node is labeled by the
query made. Every ⊥answer is a branching node (i.e., such nodes have two children, one
that assumes the answer is 0 and the other that assumes it is 1). There is no branching on
equivalence queries or LMQ’s that return a 0 or a 1. All paths from the root to the leaves
have at most m + 1 nodes on them (for the sake of simplicity, we will assume that a ﬁnal
EQ is made, and allow for this in the formula). Of course, on each path there are at most ℓ
branching nodes. Furthermore, on every root-to-leaf path, the labels of the branching nodes
(i.e., queries made) are all distinct.
We need to bound the total number of nodes in the tree, but for the branching nodes we
need count only how many different labels they have (since no query is repeated). That is,
we basically have to count the non-branching nodes and add ℓ.
For convenience let us name these trees. If such a tree as described above has ℓbranching
nodes and at most m + 1 nodes on each path from the root to the leaves, we call it an
ℓ-m-branching-tree. (Of course, ℓ≤m for every valid ℓ-m-branching-tree.) We call
ℓplus the number of the tree’s non-branching nodes the labeled-node count, since if all
the non-branching nodes had different labels (which they may, if the tree corresponds to a
computation of A′ and the labels are given with respect to the LMQ’s or EQ’s being done in
A′), then this would really be just the count of the labels. We now begin the inductive proof
that the labeled-node count of any ℓ-m-branching-tree does not exceed 2ℓ(m −ℓ+ 1) + ℓ.
Base case, ℓ= 0, m ≥0: If there are no branching nodes then there is only one path
from the root to the leaf in the tree, and since its length is bounded by m + 1 the bound
Inductive assumption: Assume that for all ℓ′ ≤ℓand for all m ≥ℓ′ we have proved
the bound. That is, the labeled-node count of every ℓ′-m-branching-tree (where ℓ′ ≤ℓ)
is bounded by 2ℓ′(m −ℓ′ + 1) + ℓ′.
Induction step: Now we prove the bound for ℓ+ 1 and every m ≥ℓ+ 1. That is, we
take an arbitrary (ℓ+1)-m-branching-tree. We start from the root of the tree and follow
down the only path until we reach the ﬁrst branching node b. Let the number of nodes
from the root to b, inclusively, be m∗. The left subtree of b is an ℓ0-(m−m∗)-branchingtree, for some ℓ0 ≤ℓ. The right subtree of b is an ℓ1-(m −m∗)-branching-tree, for
D. ANGLUIN, ET AL.
some ℓ1 ≤ℓ. We need to further elaborate on the labels of the branching nodes in these
subtrees. None of these labels are the same as the label for b, since on every path all
labels have to be different. Let ℓ∗
0 be the number of branching nodes that have labels
not occurring in the right subtree and let ℓ∗
1 be the number of branching nodes that have
labels not occurring in the left subtree. Let ℓ∗be the labels that exist in both subtrees
of b. Obviously, we know that ℓ0 = ℓ∗
0 + ℓ∗and that ℓ1 = ℓ∗
1 + ℓ∗. We also know that
1 + ℓ∗= ℓ.
The labeled-node count of the original (ℓ+ 1)-m-branching-tree can be expressed as
the labeled-node count of the left subtree of b, plus the labeled-node count of the right
subtree of b, plus m∗, and minus ℓ∗, the number of branching nodes that have been
counted in the labeled-node count of both subtrees.
If we use the inductive assumption for the labeled-node counts of the left and right
subtrees, we just have to verify that
(m −m∗) −(ℓ∗
0 + ℓ∗) + 1
(m −m∗) −(ℓ∗
1 + ℓ∗) + 1
1 + ℓ∗) + m∗−ℓ∗
m −(ℓ+ 1) + 1
Some simpliﬁcations on the left side of this inequality, lead us to
0(m −(ℓ−ℓ∗
0) + 1) + ℓ
and we have to verify that it does not exceed 2ℓ+1(m −ℓ) + ℓ+ 1. We can increase the
left side by taking m∗as small as possible, namely 1. Therefore, we now only need to
≤2ℓ+1(m −ℓ).
It is easy to prove that 2ℓ−k¡
≤2ℓ(m −ℓ), if k ≤ℓ≤m. Since
1 ≤ℓ≤m and ℓ∗
0 ≤ℓ≤m, we can apply the above formula to both terms of the left
side of the inequality that we are trying to prove. This completes the inductive proof.
The only difference between the number of queries A′ makes and the labeled-node count
of a ℓ-m-branching-tree that corresponds to its computation is that we can save the last
equivalence query for one of the copies of A. This concludes the proof.
The next theorem shows that in some cases such an exponential blowup in the number of
queries is in fact necessary.
Theorem 9 There is a concept class learnable with m equivalence and membership
queries that requires 2ℓ(m −ℓ+ 1) −1 equivalence and limited membership queries,
where ℓis the number of ⊥responses received.
We construct a concept class C that is a variant of ADDRESSING (Maass & Tur´an,
Let the instance space be X = S2ℓ
i=0 Xi, where the Xi’s are disjoint, X0 =
MALICIOUS OMISSIONS AND ERRORS
{1, . . . , ℓ}, and |Xi| = m −ℓ+ 1 for 1 ≤i ≤2ℓ. Since |X0| = ℓ, each of its subsets can
be viewed as an ℓ-bit number. A set c ⊆X is in C if and only if it has the following form.
It contains exactly one element x that is not in X0, and if i denotes the number represented
by c ∩X0, then that x is in Xi.
Concept class C can be learned by ℓmembership queries for the elements in X0 followed
by m −ℓmembership queries for the elements of Xi, where i is the number represented
by the responses obtained in the ﬁrst phase.
On the other hand, the following adversary strategy shows that at least 2ℓ(m −ℓ+
1) −1 equivalence and limited membership queries are required to learn C. The limited
membership oracle responds ⊥to all instances in X0.
Membership queries for other
elements are answered by “No.” Equivalence queries are answered by providing a negative
counterexample outside X0.
Taking ℓ= m in the proof of Theorem 9 gives us the original concept class ADDRESS-
ING, and an example where m ⊥responses increase the number of queries required from
m for ordinary membership queries to 2m −1 for limited membership and equivalence
Note that ADDRESSING also causes the incomplete membership query model to have an expected exponential blowup over ordinary membership queries when the probability of a ⊥response is a constant. For constant probability p of ⊥,
the expected number of instances in X0 answered ⊥is pm. This will increase the number of queries required from m for ordinary membership queries to 2pm for incomplete
membership and equivalence queries.
If, instead of allowing equivalence queries only from the concept class, one allows extended equivalence queries with any set, then such a blowup cannot occur. This follows
from a result of Auer and Long showing that in this model membership queries can
speed up learning by only a constant factor.
Strict versus Nonstrict
Recall that in the nonstrict model the ﬁnal hypothesis need only agree with the target concept
on points x such that LMQ(x) ̸= ⊥, while in the strict model, they must be exactly equal.
Every learning algorithm that works in the strict model can be run in the nonstrict model
without increasing its complexity. A relationship in the other direction can be established
by a method similar to the one used in the proof of Theorem 7 in subsection 6.1.
Every learning algorithm that works in the nonstrict model can be adapted to work in
the strict model as follows. First note that the problem that may occur when running
a nonstrict algorithm in the strict model is that it may receive as a counterexample to an
equivalence query a point that was previously classiﬁed as a “Don’t know” in a membership
query. In this case, the execution of the algorithm is interrupted. The algorithm is then
restarted, remembering the point and its classiﬁcation for possible later use in answering
a membership query. Since each interruption corresponds to a new “Don’t know,” this
simulation essentially adds a multiplicative factor of ℓto the complexity of the learning
algorithm.
D. ANGLUIN, ET AL.
Hence, from the point of view of polynomial learnability, the strict and nonstrict models
using EQ’s and LMQ’s are equivalent.
Lies versus Omissions
As noted in Subsections 2.2 and 3.4, learning with MMQ’s and EQ’s is at least as difﬁcult
as with LMQ’s and EQ’s. To show that learning with MMQ’s and EQ’s is in fact more
difﬁcult, we construct yet another variant of ADDRESSING parameterized by m and ℓas
follows. The universe consists of a set X0 of m elements, and a disjoint set X1 of
elements. We choose some ﬁxed one-to-one correspondence between the elements in X1
and subsets of X0 of cardinality ℓ. The desired class contains concepts Y consisting of a
subset of X0 of cardinality ℓtogether with the corresponding element of X1. This concept
class can be learned using m LMQ’s followed by at most 2ℓEQ’s. On the other hand,
consider an adversary that answers MMQ’s with 0, and EQ’s with the element of the
queried concept from X1 as the counterexample. Then MMQ’s convey no information,
and EQ’s eliminate concepts one at a time, so at least
−1 MMQ’s and EQ’s are
required to learn this concept class.
This example, with ℓ= log m, shows that the number of MMQ’s and EQ’s necessary
to learn a concept class cannot be bounded by any polynomial in the number of LMQ’s
and EQ’s. On the other hand, since in this example the number of LMQ’s and EQ’s is
exponential in ℓ, it does not answer the question “Are there any concept classes polynomially
learnable from EQ’s and LMQ’s that are not polynomially learnable from EQ’s and
Most of the results proven in this paper are summarized in Table 1. The remaining ones are
given below.
Strict and nonstrict models of learning from equivalence and limited membership queries are polynomial-time equivalent. (Subsection 6.3.)
Polynomial-time learnability from equivalence and malicious membership queries implies polynomial-time learnability from equivalence and limited membership queries.
(Subsection 2.2.)
Learning monotone monomials in the nonstrict model from limited membership queries
alone may require Ω(nc+1) queries when O(nc) omissions are given. (Corollary 1.)
Any class of concepts that is polynomially closed under ﬁnite exceptions and is learnable
in polynomial time from equivalence and standard membership queries is also learnable
in polynomial time from equivalence and malicious membership queries. (Theorem 7.)
Every concept class that is learnable from m equivalence and standard membership
queries is learnable in the strict model from 2ℓ(m −ℓ+ 1) + ℓ−1 equivalence and
limited membership queries. (Theorem 8.)
MALICIOUS OMISSIONS AND ERRORS
Summary of the results for various boolean formulas: n denotes the number of variables; m denotes
the number of terms in a formula; s denotes the size of a concept; ℓdenotes the number of lies or omissions; l
denotes the number of exceptions.
Class of Concepts
Type of MQ’s
Number of EQ’s
Number of MQ’s
Monotone Monomials (Theorem 1)
Monotone k-term DNF (Theorem 3)
O(knk + n2ℓ)
Monotone DNF, Nonstrict (Theorem 4)
Monotone DNF, Strict (Theorem 4)
Monotone DNF (Theorem 5)
O(mn2 + ℓn3)
Monotone DNF with Finite Exceptions
(Theorem 6)
O(mn2 + ln3)
DFA’s (Corollary 5)
poly(s, n, ℓ)
poly(s, n, ℓ)
Decision Trees (with Extended EQ’s,
Corollary 6)
poly(n, ℓ)
poly(n, ℓ)
Monotone DNF with Finite Exceptions
(Corollary 7)
poly(n, m, l, ℓ)
poly(n, m, l, ℓ)
There exists a concept class learnable with m equivalence and standard membership
queries that requires 2ℓ(m −ℓ+ 1) −1 equivalence and limited membership queries
to be learned in the strict model. (Theorem 9.)
There exists a concept class learnable in the strict model with m + 2ℓequivalence and
limited membership queries that requires
−1 equivalence and malicious membership queries to be learned. (Subsection 6.4.)
Discussion and Open Problems
As noted in the introduction, there are many classes of concepts that are efﬁciently learnable with membership and equivalence queries. For some of them we now have learning
algorithms that use equivalence and limited or malicious membership queries. Many other
problems still remain unexplored. For example, there is not yet any algorithm for learning
read-once formulas from equivalence and limited or malicious membership queries, even
though there is an algorithm for learning read-once formulas from equivalence and standard membership queries. A start in this direction is made in , which gives
a randomized polynomial-time algorithm to learn µ-DNF formulas with equivalence and
malicious membership queries. In the model of PAC learning with membership queries,
it would be interesting to see whether Baum’s algorithm can be modiﬁed to
tolerate “I don’t know” answers.
Another type of open problem is ﬁnding lower bounds for any of the classes of concepts for
which we give learning algorithms using equivalence and limited or malicious membership
So far, we have lower bounds for only a specially constructed class and for
monotone monomials in the model that uses only limited membership queries. For other
D. ANGLUIN, ET AL.
classes, the following question is still relevant: can we prove something stronger than the
trivial bound that there must be more membership queries than lies or omissions?
Moving on to the comparison of models, we have two very intriguing questions. The ﬁrst
one is, are there any classes of concepts that are polynomial-time learnable from equivalence and limited membership queries, but not polynomial-time learnable from equivalence
and malicious membership queries? The second question is, are learning with exceptions
and learning with lies equally difﬁcult for classes that are polynomially closed under ﬁnite
exceptions, or is learning with exceptions more difﬁcult for these classes? That is, is there a
class that is polynomially closed under ﬁnite exceptions, is learnable with malicious membership queries in polynomial time, and is not polynomial-time learnable with exceptions?
We also do not know how the difﬁculty of learning with exceptions classes that are not
polynomially closed under ﬁnite exceptions relates to learning such classes with malicious
membership queries.
A less important extension of this work would be to improve the time bound for the
algorithm that learns monotone DNF formulas with exceptions and possibly for the one
that learns monotone DNF from equivalence and malicious membership queries.
Acknowledgments
This research was funded in part by the National Science Foundation, under grants CCR-
9213881, CCR-9108753, CCR-9314258 and CCR-9208170, and by Esprit BRA ILP, Project 6020, OTKA grant T-014228, and OTKA grant T-016349.
We thank the referees for their careful reading and helpful suggestions. This work appeared as two separate papers in the Proceedings of the 7th Annual ACM Conference on
Computational Learning Theory , . Part
of it is also available as a technical report .