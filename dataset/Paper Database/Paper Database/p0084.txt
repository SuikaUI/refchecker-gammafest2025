IEEE TRANSACTIONS ON CYBERNETICS
Locally Weighted Ensemble Clustering
Dong Huang, Member, IEEE, Chang-Dong Wang, Member, IEEE, and Jian-Huang Lai, Senior Member, IEEE
Abstractâ€”Due to its ability to combine multiple base clusterings into a probably better and more robust clustering, the
ensemble clustering technique has been attracting increasing
attention in recent years. Despite the signiï¬cant success, one
limitation to most of the existing ensemble clustering methods is
that they generally treat all base clusterings equally regardless of
their reliability, which makes them vulnerable to low-quality base
clusterings. Although some efforts have been made to (globally)
evaluate and weight the base clusterings, yet these methods tend
to view each base clustering as an individual and neglect the
local diversity of clusters inside the same base clustering. It
remains an open problem how to evaluate the reliability of
clusters and exploit the local diversity in the ensemble to enhance
the consensus performance, especially in the case when there
is no access to data features or speciï¬c assumptions on data
distribution. To address this, in this paper, we propose a novel
ensemble clustering approach based on ensemble-driven cluster
uncertainty estimation and local weighting strategy. In particular,
the uncertainty of each cluster is estimated by considering the
cluster labels in the entire ensemble via an entropic criterion.
A novel ensemble-driven cluster validity measure is introduced,
and a locally weighted co-association matrix is presented to serve
as a summary for the ensemble of diverse clusters. With the local
diversity in ensembles exploited, two novel consensus functions
are further proposed. Extensive experiments on a variety of
real-world datasets demonstrate the superiority of the proposed
approach over the state-of-the-art.
Termsâ€”Ensemble
clustering,
clustering,
Cluster uncertainty estimation, Local weighting.
I. INTRODUCTION
ATA clustering is a fundamental yet still very challenging
problem in the ï¬eld of data mining and machine learning
 . The purpose of it is to discover the inherent structures of
a given dataset and partition the dataset into a certain number
of homogeneous groups, i.e., clusters. During the past few
decades, a large number of clustering algorithms have been
developed by exploiting various techniques , , , ,
 , , , , , , , , , , . Each
clustering algorithm has its advantages as well as its drawbacks, and may perform well for some speciï¬c applications.
There is no single clustering algorithm that is capable of
The authors would like to thank the anonymous reviewers for their valuable
comments and suggestions. This project was supported by National Key
Research and Development Program of China (2016YFB1001003), NSFC
(61602189, 61502543 & 61573387), the PhD Start-up Fund of Natural
Science Foundation of Guangdong Province, China (2016A030310457 &
2014A030310180), and Guangdong Natural Science Funds for Distinguished
Young Scholar (2016A030306014).
Mathematics
Informatics,
Agricultural
University,
Guangzhou,
 .
Chang-Dong Wang and Jian-Huang Lai are with the School of Data
and Computer Science, Sun Yat-sen University, Guangzhou, China, and
also with Guangdong Key Laboratory of Information Security Technology, Guangzhou, China, and also with Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China. E-mail:
 , .
dealing with all types of data structures and cluster shapes.
Given a data set, different clustering algorithms, or even the
same algorithm with different initializations or parameters,
may lead to different clustering results. However, without prior
knowledge, it is extremely difï¬cult to decide which algorithm
would be the appropriate one for a given clustering task. Even
with the clustering algorithm given, it may still be difï¬cult to
ï¬nd proper parameters for it.
Different clusterings produced by different algorithms (or
the same algorithm with different initializations and parameters) may reï¬‚ect different perspectives of the data. To exploit
the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as
a powerful tool for data clustering and has been attracting
increasing attention in recent years , , , , ,
 , , , , , , , , , , ,
 , , , . Ensemble clustering aims to combine
multiple clusterings to obtain a probably better and more
robust clustering result, which has shown advantages in ï¬nding
bizarre clusters, dealing with noise, and integrating clustering
solutions from multiple distributed sources . In ensemble
clustering, each input clustering is referred to as a base
clustering, while the ï¬nal clustering result is referred to as
the consensus clustering.
In ensemble clustering, the quality of the base clusterings
plays a crucial role in the consensus process. The consensus
results may be badly affected by low-quality (or even ill)
base clusterings. To deal with low-quality base clusterings,
some efforts have been made to evaluate and weight the base
clusterings to enhance the consensus performance , ,
 . However, these approaches , , are developed
based on an implicit assumption that all of the clusters in the
same base clustering have the same reliability. They typically
treat each base clustering as an individual and assign a global
weight to each base clustering regardless of the diversity of
the clusters inside it , , . However, due to the noise
and inherent complexity of real-world datasets, the different
clusters in the same clustering may have different reliability.
There is a need to respect the local diversity of ensembles and
deal with the different reliability of clusters. More recently,
Zhong et al. proposed to evaluate the reliability of
clusters by considering the Euclidean distances between data
objects in clusters. The method in requires access to
the original data features, and its efï¬cacy heavily relies on
the data distribution of the dataset. However, in the general
formulation of ensemble clustering (see Section III-B), there is
no access to the original data features. Without needing access
to the data features or relying on speciï¬c assumptions about
data distribution, the key problem here is how to evaluate the
reliability of clusters and weight them accordingly to enhance
the accuracy and robustness of the consensus clusterings.
 
IEEE TRANSACTIONS ON CYBERNETICS
Input: the clustering ensemble Î 
Cluster uncertainty estimation using entropy criterion
Ensemble-driven cluster validity
Refining co-association matrix by local weighting
Consensus functions
Output: the consensus clustering ğœ‹âˆ—
Fig. 1. Flow diagram of the proposed approach.
Aiming to address the aforementioned problem, in this
paper, we propose a novel ensemble clustering approach based
on ensemble-driven cluster uncertainty estimation and local
weighting strategy. The overall process of our approach is
illustrated in Fig. 1. We take advantage of the ensemble diversity at the cluster-level and integrate the cluster uncertainty
and validity into a locally weighted scheme to enhance the
consensus performance. A cluster can be viewed as a local
region in the corresponding base clustering. Without needing
access to the data features, in our work, the uncertainty of
each cluster is estimated with regard to the cluster labels
in the entire ensemble based on an entropic criterion. In
particular, given a cluster, we investigate its uncertainty by
considering how the objects inside this cluster are grouped
in the multiple base clusterings. Based on cluster uncertainty
estimation, an ensemble-driven cluster index (ECI) is then
presented to measure the reliability of clusters. In this paper,
we argue that the crowd of diverse clusters in the ensemble
can provide an effective indication for evaluating each individual cluster. By evaluating and weighting the clusters in
the ensemble via the ECI measure, we further present the
concept of locally weighted co-association (LWCA) matrix,
which incorporates local adaptivity into the conventional coassociation (CA) matrix and serves as a summary for the
ensemble of diverse clusters. Finally, to achieve the ï¬nal
clustering result, we propose two novel consensus functions,
termed locally weighted evidence accumulation (LWEA) and
locally weighted graph partitioning (LWGP), respectively, with
the diversity of clusters exploited and the local weighting
strategy incorporated.
For clarity, we summarize the main contributions of this
paper as follows:
â€¢ We propose to estimate the uncertainty of clusters by
considering the distribution of all cluster labels in the
ensemble using an entropic criterion, which requires
no access to the original data features and makes no
assumptions on the data distribution.
â€¢ We present an ensemble-driven cluster validity index to
evaluate and weight the clusters in the ensemble, which
provides an indication of reliability at the cluster-level
and plays a crucial role in the local weighting scheme.
â€¢ We propose two novel consensus functions to construct
the ï¬nal clusterings based on ensemble-driven cluster
uncertainty estimation and local weighting strategy.
â€¢ Extensive experiments have been conducted on a variety
of real-world datasets, which demonstrate the superiority
of the proposed ensemble clustering approach in terms of
both clustering quality and efï¬ciency.
The rest of the paper is organized as follows. The related
work is reviewed in Section II. The background knowledge
about entropy and ensemble clustering is introduced in Section III. The proposed ensemble clustering approach based on
cluster uncertainty estimation and local weighting strategy is
described in Section IV. The experimental results are reported
in Section V. Finally, we conclude the paper in Section VI.
II. RELATED WORK
Ensemble learning is an important technique in machine
learning, which aims to combine multiple base learners to
obtain a probably better learner . Typically, there are
two major directions in ensemble learning, that is, ensemble
classiï¬ers , , and ensemble clustering . The
ensemble classiï¬ers technique is generally involved in supervised scenarios, while the ensemble clustering technique is
generally involved in unsupervised scenarios. In this paper, our
research focuses on the ensemble clustering technique, whose
purpose is to combine multiple base clusterings to obtain a
probably better and more robust consensus clustering. Due to
its inherent unsupervised nature, ensemble clustering is still a
very challenging direction in ensemble learning.
In the past decade, many ensemble clustering approaches
have been developed, which can be mainly classiï¬ed into three
categories, i.e., the pair-wise co-occurrence based approaches
 , , , , , , the graph partitioning based
approaches , , , , and the median partition
based approaches , , , .
The pair-wise co-occurrence based approaches , ,
 , , , typically construct a co-association (CA)
matrix by considering how many times two objects occur
in the same cluster among the multiple base clusterings.
By exploiting the CA matrix as the similarity matrix, the
conventional clustering techniques, such as the agglomerative
clustering methods , can be exploited to build the ï¬nal
clustering result. Fred and Jain for the ï¬rst time presented the concept of CA matrix and proposed the evidence
accumulation clustering (EAC) method. Wang et al. 
extended the EAC method by taking the sizes of clusters
into consideration, and proposed the probability accumulation
method. Iam-On et al. reï¬ned the CA matrix by considering the shared neighbors between clusters to improve the
consensus results. Wang introduced a dendrogram-like
hierarchical data structure termed CA-tree to facilitate the coassociation based ensemble clustering process. LourencÂ¸o et
al. proposed a new ensemble clustering approach which
is based on the EAC paradigm and is able to determine the
IEEE TRANSACTIONS ON CYBERNETICS
probabilistic assignments of data objects to clusters. Liu et
al. employed spectral clustering on the CA matrix and
developed an efï¬cient ensemble clustering approach termed
spectral ensemble clustering (SEC).
The graph partitioning based approaches , , ,
 address the ensemble clustering problem by constructing
a graph model to reï¬‚ect the ensemble information. The consensus clustering is then obtained by partitioning the graph
into a certain number of segments. Strehl and Ghosh 
proposed three graph partitioning based ensemble clustering
algorithms, i.e., cluster-based similarity partitioning algorithm
(CSPA), hypergraph partitioning algorithm (HGPA), and metaclustering algorithm (MCLA). Fern and Brodley constructed a bipartite graph for the clustering ensemble by
treating both clusters and objects as graph nodes, and obtain
the consensus clustering by partitioning the bipartite graph.
Yu et al. designed a double afï¬nity propagation based
ensemble clustering framework, which is able to handle the
noisy attributes and obtain the ï¬nal consensus clustering by
the normalized cut algorithm.
The median partition based approaches , , , 
formulate the ensemble clustering problem into an optimization problem, which aims to ï¬nd a median partition (or clustering) by maximizing the similarity between this clustering and
the multiple base clusterings. The median partition problem
is NP-hard . Finding the globally optimal solution in the
huge space of all possible clusterings is computationally infeasible for large datasets. Cristofor and Simovici proposed
to obtain an approximate solution using the genetic algorithm,
where clusterings are treated as chromosomes. Topchy et
al. cast the median partition problem into a maximum
likelihood problem and approximately solve it by the EM
algorithm. Franek and Jiang cast the median partition
problem into an Euclidean median problem by clustering
embedding in vector spaces. Huang et al. formulated the
median partition problem into a binary linear programming
problem and obtained an approximate solution by means of
the factor graph theory.
These algorithms attempt to solve the ensemble clustering
problem in various ways , , , , , , ,
 , , , , , , , , . However,
one common limitation to most of the existing methods is
that they generally treat all clusters and all base clusterings
in the ensemble equally and may suffer from low-quality
clusters or low-quality base clusterings. To partially address
this limitation, recently some weighted ensemble clustering
approaches have been presented , , . Li and
Ding cast the ensemble clustering problem into a nonnegative matrix factorization problem and proposed a weighted
consensus clustering approach, where each base clustering is
assigned a weight in order to improve the consensus result. Yu
et al. exploited the feature selection techniques to weight
and select the base clusterings. In fact, clustering selection
 can be viewed as a 0-1 weighting scheme, where 1
indicates selecting a clustering and 0 indicates removing a
clustering. Huang et al. proposed to evaluate and weight
the base clusterings based on the concept of normalized crowd
agreement index (NCAI), and devised two weighted consensus
functions to obtain the ï¬nal clustering result.
Although the above-mentioned weighted ensemble clustering approaches , , are able to estimate the
reliability of base clusterings and weight them accordingly,
yet they generally treat a base clustering as a whole and
neglect the local diversity of clusters inside the same base
clustering. To explore the reliability of clusters, Alizadeh et
al. proposed to evaluate clusters in the ensemble by
averaging normalized mutual information (NMI) between
clusterings, which results in a very expensive computational
cost and is not feasible for large datasets. Zhong et al. 
exploited the Euclidean distances between objects to estimate
the cluster reliability, which needs access to the original data
features and is only applicable to numerical data. However, in
the more general formulation of ensemble clustering , ,
 , , , , , , , , the original data
features are not available in the consensus process. Moreover,
by measuring the within-cluster similarity based on Euclidean
distances, the efï¬cacy of the method in heavily relies
on some implicit assumptions about data distribution, which
places an unstable factor in the consensus process. Different
from , in this paper, our ensemble clustering approach
requires no access to the original data features. We propose to
estimate the uncertainty of clusters by considering the cluster
labels in the entire ensemble based on an entropic criterion,
and then present an ensemble-driven cluster index (ECI) to
evaluate cluster reliability without making any assumptions
on the data distribution. Further, to obtain the consensus
clustering results, two novel consensus functions are developed
based on cluster uncertainty estimation and local weighting
strategy. Extensive experiments on a variety of real-world
datasets have shown that our approach exhibits signiï¬cant
advantages in clustering accuracy and efï¬ciency over the stateof-the-art approaches.
III. PRELIMINARIES
A. Entropy
In this section, we brieï¬‚y review the concept of entropy.
In information theory , the entropy is a measure of the
uncertainty associated with a random variable. The formal
deï¬nition of entropy is provided in Deï¬nition 1.
Deï¬nition 1. For a discrete random variable X, the entropy
H(X) is deï¬ned as
p(x) log2 p(x),
where X is the set of values that X can take, and p(x) is the
probability mass function of X.
The joint entropy is a measure of the uncertainty associated
with a set of random variables. The formal deï¬nition of joint
entropy is provided in Deï¬nition 2.
Deï¬nition 2. For a pair of discrete random variables (X, Y ),
the joint entropy H(X, Y ) is deï¬ned as
H(X, Y ) = âˆ’
p(x, y) log2 p(x, y),
IEEE TRANSACTIONS ON CYBERNETICS
where p(x, y) is the joint probability of (X, Y ).
If and only if the two random variables X and Y are independent of each other, it holds that H(X, Y ) = H(X)+H(Y ).
Hence, given n independent random variables X1, Â· Â· Â· , Xn, we
H(X1, Â· Â· Â· , Xn) = H(X1) + Â· Â· Â· + H(Xn).
B. Formulation of the Ensemble Clustering Problem
In this section, we introduce the general formulation of the
ensemble clustering problem. Let O = {o1, Â· Â· Â· , oN} be a
dataset, where oi is the i-th data object and N is the number
of objects in O. Consider M partitions (or clusterings) for the
dataset O, each of which is treated as a base clustering and
consists of a certain number of clusters. Formally, we denote
the ensemble of M base clusterings as follows:
Î  = {Ï€1, Â· Â· Â· , Ï€M},
1 , Â· Â· Â· , Cm
denotes the m-th base clustering in Î , Cm
denotes the i-th
cluster in Ï€m, and nm denotes the number of clusters in Ï€m.
Each cluster is a set of data objects. Obviously, the union
of all clusters in the same base clustering covers the entire
dataset, i.e., âˆ€Ï€m âˆˆÎ , Snm
= O. Different clusters in
the same base clustering do not intersect with each other, i.e.,
âˆˆÏ€m s.t. i Ì¸= j, Cm
= âˆ…. Let Clsm(oi)
denote the cluster in Ï€m âˆˆÎ  that object oi belongs to. That
is, if oi belongs to the k-th cluster in Ï€m, i.e., oi âˆˆCm
we have Clsm(oi) = Cm
For convenience, we represent the set of all clusters in the
ensemble Î  as
C = {C1, Â· Â· Â· , Cnc},
where Ci denotes the i-th cluster and nc denotes the total
number of clusters in Î . It is obvious that nc = n1+Â· Â· Â·+nM.
The purpose of ensemble clustering is to combine the
multiple base clusterings in the ensemble Î  to obtain a
probably better and more robust clustering. With regard to
the difference in the input information, there are two different
formulations of the ensemble clustering problem. In the ï¬rst
formulation, the ensemble clustering system only takes the
multiple base clusterings as input and has no access to the
original data features , , , , , , ,
 , , , . In the other formulation, the ensemble
clustering system takes both the multiple base clusterings and
the original data features as inputs , . In this paper, we
comply with the ï¬rst formulation of the ensemble clustering
problem, which is also the common practice for most of the
existing ensemble clustering approaches . Hence, in our
formulation, the input is the clustering ensemble Î , and the
output is the consensus clustering Ï€âˆ—.
IV. LOCALLY WEIGHTED ENSEMBLE CLUSTERING
In this paper, we propose a novel ensemble clustering
approach based on ensemble-driven cluster uncertainty estimation and local weighting strategy. Without needing access
to the original data features or making some assumptions
about data distribution, we exploit the ensemble information to
estimate the uncertainty (or unreliability) of clusters based on
an entropic criterion. With the cluster uncertainty obtained, an
ensemble-driven cluster validity index termed ECI is presented
to evaluate the reliability of each cluster with the help of the
cluster labels in the clustering ensemble. In this paper, we
argue that the diverse clusters in the ensemble can provide
an effective indication for evaluating the reliability of each
individual cluster. Then, we reï¬ne the conventional CA matrix
using a local weighting strategy based on the ECI measure,
and introduce the concept of locally weighted co-association
(LWCA) matrix , which serves as a summary for the ensemble
with diverse clusters. To obtain the ï¬nal clustering results, in
this paper, two novel consensus functions are further presented,
that is, LWEA and LWGP. In the following of this section, we
will describe each step of our approach in detail.
A. Measuring Cluster Uncertainty in Ensembles
In the general formulation of ensemble clustering, there
is no access to the original data features. To evaluate the
reliability of each cluster, we appeal to the concept of entropy
with the help of the cluster labels in the entire ensemble.
As introduced in Section III-A, entropy is a measure of
uncertainty associated with a random variable. Each cluster
is a set of data objects. Given a cluster Ci âˆˆC and a base
clustering Ï€m âˆˆÎ , if cluster Ci does not belong to Ï€m, then
it is possible that the objects in Ci belong to more than one
cluster in Ï€m. In fact, the objects in Ci may belong to at most
nm different clusters in Ï€m, where nm is the total number of
clusters in Ï€m. The uncertainty (or entropy) of Ci w.r.t. Ï€m
can be computed by considering how the objects in Ci are
clustered in Ï€m.
Deï¬nition 3. Given the ensemble Î , the uncertainty of cluster
Ci w.r.t. the base clustering Ï€m âˆˆÎ  is computed as
Hm(Ci) = âˆ’
j ) log2 p(Ci, Cm
where nm is the number of clusters in Ï€m, Cm
is the j-th
cluster in Ï€m, T computes the intersection of two sets (or
clusters), and |Ci| outputs the number of objects in Ci.
The formal deï¬nition of the cluster uncertainty w.r.t. a
base clustering is given in Deï¬nition 3. Because it holds that
j ) âˆˆ for any i, j, m, so we have Hm(Ci) âˆˆ
[0, +âˆ). When all the objects in Ci belong to the same cluster
in Ï€m, the uncertainty of Ci w.r.t. Ï€m reaches its minimum,
i.e., zero. When the objects in Ci belong to more different
clusters in Ï€m, the uncertainty of Ci w.r.t. Ï€m typically gets
greater, which indicates that the objects in Ci are less likely
to be in the same cluster with regard to Ï€m.
Without loss of generality, based on the assumption that
the base clusterings in the ensemble are independent , the
IEEE TRANSACTIONS ON CYBERNETICS
Fig. 2. Illustration of an ensemble of three base clusterings, namely, Ï€1, Ï€2,
COMPUTATION OF CLUSTER UNCERTAINTY AND ECI (WITH Î¸ = 0.5) FOR
THE CLUSTERS IN THE ENSEMBLE SHOWN IN FIG. 2.
Clustering
Cluster Uncertainty
w.r.t. the Ensemble
uncertainty (or entropy) of Ci w.r.t. the entire ensemble Î 
can be computed by summing up the uncertainty of Ci w.r.t.
the M base clusterings in Î  according to Eq. (3). Its formal
deï¬nition is given in Deï¬nition 4.
Deï¬nition 4. Given the ensemble Î , the uncertainty of cluster
Ci w.r.t. the entire ensemble Î  is computed as
where M is the number of base clusterings in Î .
Intuitively, the uncertainty of Ci w.r.t. Î  reï¬‚ects how the
objects in Ci are clustered in the ensemble of multiple base
clusterings. If the objects in Ci belong to the same cluster in
each of the base clusterings, which can be viewed as that all
base clusterings agree that the objects in Ci should be assigned
to the same cluster, then the uncertainty of Ci w.r.t. Î  reaches
its minimum, i.e., zero. When the uncertainty of Ci w.r.t. Î 
gets larger, it is indicated that the objects in Ci are less likely
to be in the same cluster with consideration to the ensemble
of multiple base clusterings.
We provide an example1 in Fig. 2 and Table I to show the
computation of cluster uncertainty w.r.t. an ensemble of three
base clusterings. For the dataset O = {o1, Â· Â· Â· , o16} with 16
data objects, three base clusterings (Ï€1, Ï€2, and Ï€3) are generated, each of which consists of three clusters (as illustrated in
Fig. 2). Of the three clusters in Ï€1, C1
1 contains eight objects,
2 contains three objects, and C1
3 contains ï¬ve objects. Then,
we proceed to compute the uncertainty of the three clusters in
Ï€1 w.r.t. the ensemble. The eight objects in cluster C1
to three different clusters in Ï€2. According to Deï¬nition 3,
8, and p(C1
1We have ï¬xed an error in the computation of this example which happened
in the previous version. We would like to thank Mr. Recep DoË‡ga Siyli for
pointing out this issue.
Cluster Uncertainty w.r.t. Î 
Correlation between ECI and cluster uncertainty with different
parameters Î¸.
the uncertainty of C1
1 w.r.t. base clustering Ï€2 is computed
Similarly, we can obtain H3(C1
1) = 1. It is obvious that the
uncertainty of cluster C1
1 w.r.t. the base clustering that contains
it equals zero, i.e, H1(C1
1) = 0. Therefore, the uncertainty
of cluster C1
1 w.r.t. the entire ensemble Î  can be computed
1) = 0 + 1.56 + 1 = 2.56. In a similar way, the
uncertainty of the other clusters in Î  can be obtained (see
Table I). It is noteworthy that the three objects in C1
to the same cluster in each of the three base clusterings in
Î , i.e., all base clusterings in Î  agree that the objects in C1
should be in the same cluster. Thereby the uncertainty of C1
w.r.t. Î  reaches the minimum value, that is, HÎ (C1
As shown in Table I, of the nine clusters in Î , C1
cluster with the greatest uncertainty, while C1
two most stable clusters. For clarity, in the following, when
we refer to cluster uncertainty without mentioning whether
it is with respect to a base clustering or with respect to the
ensemble, we mean cluster uncertainty w.r.t. the ensemble.
B. Ensemble-Driven Cluster Validity
Having obtained the uncertainty (or entropy) of each cluster
in the clustering ensemble, we further propose an ensembledriven cluster index (ECI), which measures the reliability of
clusters by considering their uncertainty w.r.t. the ensemble.
Deï¬nition 5. Given an ensemble Î  with M base clusterings,
the ensemble-driven cluster index (ECI) for a cluster Ci is
ECI(Ci) = eâˆ’HÎ (Ci)
where Î¸ > 0 is a parameter to adjust the inï¬‚uence of the
cluster uncertainty over the index.
The formal deï¬nition of ECI is given in Deï¬nition 5.
According to the deï¬nition, because HÎ (Ci) âˆˆ[0, +âˆ), it
holds that ECI(Ci) âˆˆ(0, 1] for any Ci âˆˆC. Obviously,
smaller uncertainty of a cluster leads to a greater ECI value.
As an example, Table I shows the ECI values for the clusters
in the ensemble illustrated in Fig. 2.
When the uncertainty of a cluster Ci reaches its minimum,
i.e., HÎ (Ci) = 0, its ECI will thereby reaches its maximum,
i.e., ECI(Ci) = 1. The ECI of a cluster approaches zero
when its cluster uncertainty approaches inï¬nity. A parameter
Î¸ is adopted in the computation of ECI to adjust the inï¬‚uence
IEEE TRANSACTIONS ON CYBERNETICS
of the cluster uncertainty over the index (see Eq. (10)).
As shown in Fig. 3, when setting Î¸ to small values, e.g.,
setting Î¸ < 0.1, the ECI decreases dramatically as the cluster
uncertainty increases. When setting Î¸ to large values, the
difference between the ECI values of high-uncertainty clusters
and low-uncertainty ones will be narrowed down. Empirically,
it is suggested that the parameter Î¸ be set in the interval
of [0.2, 1]. The consensus performance of our approach with
different parameters Î¸ is evaluated by extensive experiments.
Please see Section V-B for more details.
C. Reï¬ning Co-association Matrix by Local Weighting
The co-association (CA) matrix is ï¬rst proposed by Fred
and Jain , which reï¬‚ects how many times two data objects
are grouped into the same cluster among the multiple base
clusterings in the ensemble.
Deï¬nition 6. Given an ensemble Î , the co-association (CA)
matrix is computed as
A = {aij}NÃ—N
if Clsm(oi) = Clsm(oj),
otherwise,
where Clsm(oi) denotes the cluster in Ï€m âˆˆÎ  that object oi
belongs to.
The CA matrix is a classical and widely used tool for
dealing with the ensemble clustering problem , , ,
 . Despite the signiï¬cant success, one limitation of the CA
matrix is that it treats all clusters and all base clusterings
in the ensemble equally and lack the ability to evaluate and
weight the ensemble members w.r.t. their reliability. Huang
et al. exploited the NCAI index to weight the base
clusterings and thereby construct a weighted co-association
(WCA) matrix, which, however, only considers the reliability
of base clusterings, but still neglects the cluster-wise diversity
inside the same base clustering.
Different from the (globally) weighting strategy that
treats each base clustering as a whole, in this section, we reï¬ne
the CA matrix by a local weighting strategy based on the
ensemble-driven cluster validity and propose the concept of
locally weighted co-association (LWCA) matrix.
Deï¬nition 7. Given an ensemble Î , the locally weighted coassociation (LWCA) matrix is computed as
ËœA = {Ëœaij}NÃ—N
i =ECI (Clsm(oi)) ,
if Clsm(oi) = Clsm(oj),
otherwise,
where Clsm(oi) denotes the cluster in Ï€m âˆˆÎ  that object oi
belongs to.
A cluster can be viewed as a local region in a base
clustering. To take into consideration the different reliability
of clusters in the ensemble, the weighting term wm
is incorporated to assign weights to clusters via the ECI measure (see
Deï¬nition 7). The intuition is that the objects that co-occur in
more reliable clusters (with higher ECI values) are more likely
to belong to the same cluster in the true clustering. With the
local weighting strategy, the LWCA matrix not just considers
how many times two objects occur in the same cluster among
the multiple base clusterings, but also reï¬‚ects how reliable the
clusters in the ensemble are.
D. Consensus Functions
In this paper, based on ensemble-driven cluster uncertainty
estimation and local weighting strategy, we further propose
two novel consensus functions, i.e., locally weighted evidence
accumulation (LWEA) and locally weighted graph partitioning
(LWGP), which will be described in Section IV-D1 and
Section IV-D2, respectively.
1) Locally Weighted Evidence Accumulation (LWEA): In
this section, we introduce the consensus function termed
LWEA, which is based on hierarchical agglomerative clustering.
Hierarchical agglomerative clustering is a widely used clustering technique , which typically takes a similarity matrix
as input and performs region merging iteratively to achieve a
dendrogram, i.e., a hierarchical representation of clusterings.
Here, we exploit the LWCA matrix (see Deï¬nition 7) as the
initial similarity matrix, denoted as
S(0) = {S(0)
ij = Ëœaij,
where Ëœaij is the (i, j)-th entry in the LWCA matrix. The
N original data objects are treated as the N initial regions.
Formally, we denote the set of initial regions as follows:
R(0) = {R(0)
1 , Â· Â· Â· , R(0)
for i = 1, Â· Â· Â· , N.
denotes the i-th region in R(0). Note that each initial region
contains exactly one data object.
With the initial similarity matrix and the initial regions
constructed, the region merging process is then performed
iteratively. In each step of region merging, the two regions
with the highest similarity will be merged into a new and
larger region and thereby the set of regions will be updated.
The set of the obtained regions in the t-th step is denoted as
R(t) = {R(t)
1 , Â· Â· Â· , R(t)
where R(t)
denotes the i-th region and |R(t)| denotes the
number of regions in R(t). After each step of region merging,
IEEE TRANSACTIONS ON CYBERNETICS
to get prepared for the next iteration, the similarity matrix will
be updated according to the new set of regions. Typically, we
adopt the average-link (AL), which is a classical agglomerative
clustering method , to update the similarity matrix for the
t-step. That is
S(t) = {S(t)
ij }|R(t)|Ã—|R(t)|
i | Â· |R(t)
where |R(t)
i | denotes the number of objects in the region R(t)
By iterative region merging, a dendrogram is constructed.
The root of the dendrogram is the entire dataset, while the
leaves of it are the original data objects. Each level of the
dendrogram represents a clustering with a certain number of
clusters. Therefore, the ï¬nal clustering result can be obtained
by specifying a number of clusters for the dendrogram.
For clarity, the overall algorithm of LWEA is summarized
in Algorithm 1.
Algorithm 1 (Locally Weighted Evidence Accumulation)
Input: Î , k.
1: Compute the uncertainty of the clusters in Î  w.r.t. Deï¬nition 4.
2: Compute the ECI measures of the clusters in Î  w.r.t. Deï¬nition 5.
3: Construct the LWCA matrix w.r.t. Deï¬nition 7.
4: Initialize the set of regions R(0) and the similarity matrix S(0).
5: Construct the dendrogram iteratively:
for t = 1, 2, Â· Â· Â· , Ëœ
Merge the two most similar regions in R(tâˆ’1) w.r.t. S(tâˆ’1).
Obtain the new set of regions R(t).
Obtain the new similarity matrix S(t).
6: Obtain the clustering with k clusters in the dendrogram.
Output: the consensus clustering Ï€âˆ—.
2) Locally Weighted Graph Partitioning (LWGP): In this
section, we introduce the consensus function termed LWGP,
which is based on bipartite graph formulating and partitioning.
To construct the bipartite graph, we treat both clusters and
objects as graph nodes. A link between two nodes exists if
and only if one node is a data object and the other node is the
cluster that contains it (see Fig. 4). Given an object oi âˆˆO and
a cluster Cj âˆˆC such that oi âˆˆCj, the link weight between
them is decided by the ECI value of Cj, i.e., the weight of a
link is correlated to the reliability of the cluster that it connects
to. Hence, with the ECI measure incorporated, the bipartite
graph not only considers the belong-to relationship between
objects and clusters, but also reï¬‚ects the local reliability, i.e.,
the reliability of clusters, in the ensemble. Formally, the locally
weighted bipartite graph (LWBG) is deï¬ned in Deï¬nition 8.
Deï¬nition 8. The locally weighted bipartite graph (LWBP) is
G = (V, L),
where V = O S C is the node set and L is the link set. The
link weight between two nodes vi and vj is deï¬ned as
if vi âˆˆO, vj âˆˆC, and vi âˆˆvj,
if vj âˆˆO, vi âˆˆC, and vj âˆˆvi,
otherwise.
Data Objects
Fig. 4. Illustration of the locally weighted bipartite graph (LWBG).
Having constructed the LWBG according to Deï¬nition 8,
we proceed to partition the graph using the Tcut algorithm
 , which is able to take advantage of the bipartite graph
structure to greatly facilitate the computation of the graph
partitioning process. The graph is partitioned into a certain
number of disjoint node sets. The object nodes in the same
segment are treated as a cluster, and hence the ï¬nal clustering
result can be obtained.
For clarity, we summarize the LWGP algorithm in Algorithm 2.
Algorithm 2 (Locally Weighted Graph Partitioning)
Input: Î , k.
1: Compute the uncertainty of the clusters in Î  w.r.t. Deï¬nition 4.
2: Compute the ECI measures of the clusters in Î  w.r.t. Deï¬nition 5.
3: Build the LWBG graph w.r.t. Deï¬nition 8.
4: Partition the LWBG into a certain number of segments using the
Tcut algorithm .
5: Treat objects in the same segment as a cluster and form clusters
for the entire dataset.
6: Obtain the consensus clustering by the obtained clusters .
Output: the consensus clustering Ï€âˆ—.
V. EXPERIMENTS
In this section, we conduct experiments on a variety of realworld datasets to compare the proposed methods against the
state-of-the-art ensemble clustering methods. The MATLAB
source code and experimental data of this work are available
at: 
A. Datasets and Evaluation Methods
In our experiments, ï¬fteen real-world datasets are used,
namely, Caltech20, Forest Covertype (FCT), Image Segmentation (IS), ISOLET, Letter Recognition (LR), Landsat Satellite
(LS), Multiple Features (MF), MNIST, Optical Digit Recognition (ODR), Pen Digit (PD), Semeion, Steel Plates Faults
(SPF), Texture, Vehicle Silhouettes (VS), and USPS. Following
the practice of , we select 20 representative categories out
of the 101 categories in the Caltech dataset2 to form the
Caltech20 dataset. The MNIST and USPS datasets are from
Dr. Sam Roweisâ€™s website3, where a subset of 5, 000 objects
is used here for the MNIST dataset. The other twelve datasets
are from the UCI machine learning repository4. The details of
the ï¬fteen datasets are given in Table II.
2 
3 
4 
IEEE TRANSACTIONS ON CYBERNETICS
DESCRIPTION OF THE BENCHMARK DATASETS.
#Attribute
Two widely used evaluation measures, i.e., normalized
mutual information (NMI) and adjusted rand index (ARI)
 , are used to evaluate the quality of clusterings. Note that
larger values of NMI and ARI indicate better clustering results.
The NMI measure provides a sound indication of the
shared information between two clusterings. Let Ï€â€² be the test
clustering and Ï€G the ground-truth clustering. The NMI score
of Ï€â€² w.r.t. Ï€G is deï¬ned as follows :
NMI(Ï€â€², Ï€G) =
j=1 nij log nijn
where nâ€² is the number of clusters in Ï€â€², nG is the number of
clusters in Ï€G, nâ€²
i is the number of objects in the i-th cluster
j is the number of objects in the j-th cluster of Ï€G,
and nij is the number of common objects shared by cluster i
in Ï€â€² and cluster j in Ï€G.
The ARI is a generalization of the rand index (RI) ,
which is computed by considering the number of pairs of objects on which two clusterings agree or disagree. Speciï¬cally,
the ARI score of Ï€â€² w.r.t. Ï€G is computed as follows :
ARI(Ï€â€², Ï€G) =
2(N00N11 âˆ’N01N10)
(N00 + N01)(N01 + N11) + (N00 + N10)(N10 + N11),
where N11 is the number of object pairs that appear in the
same cluster in both Ï€â€² and Ï€G, N00 is the number of object
pairs that appear in different clusters in Ï€â€² and Ï€G, N10 is
the number of object pairs that appear in the same cluster in
Ï€â€² but in different clusters in Ï€G, and N01 is the number of
object pairs that appear in different clusters in Ï€â€² but in the
same cluster in Ï€G.
To evaluate the consensus performances of different algorithms over various ensembles, we construct a pool of a large
number of candidate base clusterings. Each of the candidate
clusterings is produced by the k-means algorithm with the
number of clusters k randomly selected in the interval of
N], where N is the number of objects in the dataset. In
THE PERFORMANCE OF LWEA WITH VARYING PARAMETERS Î¸ (IN TERMS
THE PERFORMANCE OF LWGP WITH VARYING PARAMETERS Î¸ (IN TERMS
Caltech20 FCT
Base Clusterings
Fig. 5. Average performances in terms of NMI of our methods and the base
clusterings over 100 runs.
this work, a pool of 100 candidate clusterings are randomly
generated for each benchmark dataset.
With the base clustering pool generated, to rule out the factor of getting lucky occasionally and provide a fair comparison,
the proposed methods and the baseline methods are evaluated
by their average performances over a large number of runs,
where the clustering ensemble for each run is constructed
IEEE TRANSACTIONS ON CYBERNETICS
AVERAGE PERFORMANCES (W.R.T. NMI) OVER 100 RUNS BY DIFFERENT ENSEMBLE CLUSTERING METHODS (THE BEST TWO SCORES IN EACH COLUMN
ARE HIGHLIGHTED IN BOLD).
0.478Â±0.011 0.452Â±0.011
0.247Â±0.014 0.231Â±0.024
0.647Â±0.026 0.621Â±0.026
0.756Â±0.008 0.745Â±0.011
0.446Â±0.011 0.416Â±0.017
0.463Â±0.013 0.430Â±0.014
0.244Â±0.009 0.200Â±0.031
0.664Â±0.024 0.629Â±0.029
0.749Â±0.007 0.743Â±0.010
0.448Â±0.008 0.411Â±0.013
0.401Â±0.014 0.377Â±0.015
0.218Â±0.011 0.148Â±0.038
0.591Â±0.027 0.437Â±0.092
0.699Â±0.014 0.651Â±0.034
0.408Â±0.010 0.299Â±0.021
0.405Â±0.011 0.379Â±0.013
0.216Â±0.011 0.157Â±0.034
0.594Â±0.028 0.506Â±0.066
0.695Â±0.011 0.669Â±0.019
0.407Â±0.006 0.327Â±0.013
0.399Â±0.015 0.382Â±0.013
0.228Â±0.016 0.199Â±0.032
0.574Â±0.034 0.476Â±0.055
0.712Â±0.013 0.691Â±0.015
0.427Â±0.012 0.353Â±0.017
0.454Â±0.015 0.415Â±0.011
0.237Â±0.009 0.190Â±0.018
0.636Â±0.022 0.619Â±0.019
0.747Â±0.006 0.740Â±0.008
0.440Â±0.005 0.392Â±0.011
0.461Â±0.014 0.435Â±0.012
0.232Â±0.014 0.206Â±0.021
0.619Â±0.020 0.600Â±0.021
0.749Â±0.008 0.734Â±0.018
0.435Â±0.008 0.384Â±0.018
0.462Â±0.012 0.447Â±0.014
0.237Â±0.013 0.211Â±0.021
0.630Â±0.019 0.603Â±0.019
0.755Â±0.008 0.719Â±0.029
0.434Â±0.010 0.384Â±0.021
0.456Â±0.015 0.434Â±0.013
0.229Â±0.014 0.203Â±0.022
0.620Â±0.021 0.599Â±0.022
0.749Â±0.008 0.730Â±0.021
0.431Â±0.009 0.365Â±0.021
0.453Â±0.013 0.416Â±0.010
0.233Â±0.008 0.188Â±0.021
0.627Â±0.023 0.609Â±0.025
0.748Â±0.006 0.742Â±0.008
0.440Â±0.004 0.385Â±0.013
0.413Â±0.012 0.339Â±0.070
0.232Â±0.013 0.218Â±0.025
0.632Â±0.030 0.621Â±0.035
0.719Â±0.021 0.665Â±0.015
0.404Â±0.018 0.351Â±0.017
0.363Â±0.021 0.316Â±0.028
0.172Â±0.011 0.118Â±0.035
0.501Â±0.033 0.446Â±0.071
0.637Â±0.023 0.486Â±0.042
0.359Â±0.009 0.174Â±0.019
0.381Â±0.013 0.349Â±0.010
0.220Â±0.017 0.207Â±0.019
0.611Â±0.025 0.610Â±0.028
0.670Â±0.014 0.629Â±0.014
0.347Â±0.080 0.280Â±0.065
0.632Â±0.018 0.616Â±0.027
0.679Â±0.019 0.659Â±0.021
0.655Â±0.020 0.646Â±0.022
0.838Â±0.014 0.829Â±0.018
0.793Â±0.015 0.769Â±0.022
0.648Â±0.014 0.644Â±0.019
0.695Â±0.019 0.682Â±0.026
0.646Â±0.015 0.635Â±0.017
0.831Â±0.011 0.816Â±0.015
0.792Â±0.015 0.774Â±0.021
0.478Â±0.034 0.380Â±0.074
0.478Â±0.034 0.380Â±0.074
0.506Â±0.022 0.423Â±0.049
0.697Â±0.027 0.604Â±0.065
0.653Â±0.025 0.552Â±0.065
0.494Â±0.033 0.442Â±0.061
0.494Â±0.033 0.442Â±0.061
0.523Â±0.018 0.480Â±0.037
0.719Â±0.022 0.667Â±0.039
0.664Â±0.023 0.598Â±0.053
0.521Â±0.031 0.510Â±0.038
0.701Â±0.025 0.687Â±0.029
0.584Â±0.025 0.553Â±0.034
0.814Â±0.020 0.794Â±0.030
0.801Â±0.020 0.789Â±0.027
0.629Â±0.014 0.619Â±0.020
0.661Â±0.028 0.638Â±0.031
0.628Â±0.027 0.616Â±0.026
0.825Â±0.018 0.813Â±0.020
0.767Â±0.021 0.735Â±0.031
0.613Â±0.031 0.601Â±0.048
0.638Â±0.029 0.609Â±0.038
0.623Â±0.026 0.615Â±0.027
0.820Â±0.019 0.801Â±0.019
0.757Â±0.021 0.716Â±0.030
0.622Â±0.026 0.602Â±0.055
0.650Â±0.028 0.614Â±0.043
0.634Â±0.024 0.613Â±0.034
0.822Â±0.017 0.798Â±0.027
0.766Â±0.017 0.706Â±0.038
0.597Â±0.044 0.559Â±0.086
0.632Â±0.028 0.597Â±0.038
0.611Â±0.026 0.592Â±0.037
0.807Â±0.023 0.781Â±0.035
0.751Â±0.020 0.697Â±0.036
0.630Â±0.021 0.618Â±0.035
0.658Â±0.026 0.636Â±0.031
0.618Â±0.029 0.607Â±0.030
0.819Â±0.019 0.810Â±0.018
0.760Â±0.019 0.730Â±0.024
0.547Â±0.025 0.518Â±0.036
0.653Â±0.033 0.627Â±0.065
0.574Â±0.030 0.554Â±0.040
0.792Â±0.030 0.775Â±0.038
0.694Â±0.026 0.678Â±0.035
0.386Â±0.031 0.312Â±0.066
0.538Â±0.040 0.479Â±0.078
0.426Â±0.031 0.296Â±0.077
0.621Â±0.042 0.409Â±0.089
0.560Â±0.040 0.308Â±0.060
0.522Â±0.037 0.485Â±0.040
0.625Â±0.027 0.617Â±0.030
0.527Â±0.040 0.521Â±0.042
0.741Â±0.049 0.738Â±0.052
0.661Â±0.032 0.659Â±0.035
0.663Â±0.015 0.655Â±0.017
0.167Â±0.017 0.151Â±0.029
0.784Â±0.023 0.778Â±0.028
0.163Â±0.014 0.133Â±0.010
0.661Â±0.027 0.633Â±0.032
0.658Â±0.017 0.642Â±0.024
0.169Â±0.014 0.152Â±0.023
0.757Â±0.021 0.743Â±0.024
0.170Â±0.011 0.132Â±0.012
0.650Â±0.019 0.614Â±0.020
0.544Â±0.025 0.466Â±0.046
0.132Â±0.009 0.073Â±0.025
0.642Â±0.020 0.533Â±0.053
0.148Â±0.011 0.116Â±0.029
0.477Â±0.021 0.372Â±0.049
0.551Â±0.019 0.507Â±0.033
0.130Â±0.008 0.079Â±0.028
0.648Â±0.018 0.569Â±0.042
0.146Â±0.012 0.126Â±0.025
0.503Â±0.015 0.450Â±0.041
0.603Â±0.026 0.575Â±0.034
0.166Â±0.011 0.153Â±0.015
0.740Â±0.026 0.646Â±0.051
0.144Â±0.012 0.104Â±0.038
0.601Â±0.028 0.573Â±0.035
0.640Â±0.022 0.623Â±0.026
0.156Â±0.009 0.137Â±0.016
0.725Â±0.024 0.717Â±0.025
0.163Â±0.011 0.127Â±0.011
0.609Â±0.030 0.597Â±0.033
0.642Â±0.021 0.628Â±0.026
0.154Â±0.013 0.124Â±0.024
0.730Â±0.027 0.713Â±0.030
0.163Â±0.012 0.130Â±0.015
0.602Â±0.036 0.583Â±0.038
0.653Â±0.020 0.628Â±0.034
0.161Â±0.012 0.131Â±0.022
0.742Â±0.027 0.722Â±0.032
0.164Â±0.013 0.126Â±0.012
0.604Â±0.029 0.579Â±0.036
0.637Â±0.023 0.616Â±0.033
0.153Â±0.013 0.114Â±0.024
0.717Â±0.027 0.695Â±0.032
0.160Â±0.012 0.129Â±0.014
0.582Â±0.034 0.555Â±0.047
0.637Â±0.023 0.621Â±0.026
0.157Â±0.008 0.129Â±0.018
0.719Â±0.023 0.706Â±0.027
0.162Â±0.011 0.127Â±0.012
0.597Â±0.030 0.581Â±0.031
0.595Â±0.022 0.572Â±0.038
0.139Â±0.012 0.095Â±0.029
0.701Â±0.019 0.687Â±0.027
0.150Â±0.012 0.130Â±0.026
0.553Â±0.023 0.531Â±0.036
0.503Â±0.026 0.452Â±0.053
0.121Â±0.012 0.093Â±0.028
0.495Â±0.029 0.346Â±0.061
0.127Â±0.016 0.096Â±0.032
0.373Â±0.027 0.120Â±0.046
0.549Â±0.041 0.537Â±0.050
0.115Â±0.007 0.072Â±0.014
0.658Â±0.022 0.655Â±0.022
0.137Â±0.017 0.131Â±0.022
0.513Â±0.046 0.501Â±0.054
THE NUMBER OF TIMES THAT OUR METHOD IS (SIGNIFICANTLY BETTER THAN / COMPARABLE TO / SIGNIFICANTLY WORSE THAN) A BASELINE
METHOD BY STATISTICAL TEST (T-TEST WITH p < 0.05) ON THE RESULTS IN TABLE V.
by randomly choosing M base clusterings from the pool.
Typically, the ensemble size M = 10 is used. The consensus
performances of different methods with varying ensemble
sizes are also evaluated in the following of this paper (see
Section V-E).
B. Choices of Parameter Î¸
The parameter Î¸ controls the inï¬‚uence of the cluster uncertainty over the consensus process of LWEA and LWGP.
A smaller Î¸ leads to a stronger inï¬‚uence of cluster uncertain
over the consensus process via the ECI measure (see Fig. 3).
We evaluate the clustering performances of LWEA and
LWGP with varying parameters Î¸. For each value of parameter
Î¸, we run the proposed LWEA and LWGP methods 20
times, respectively, with the ensemble of base clusterings
randomly drawn from the base clustering pool at each time,
and report their average NMI scores with varying parameters
Î¸ in Table III and Table IV. As can be seen in Table III
and Table IV, the proposed LWEA and LWGP methods yield
IEEE TRANSACTIONS ON CYBERNETICS
AVERAGE PERFORMANCES (W.R.T. ARI) OVER 100 RUNS BY DIFFERENT ENSEMBLE CLUSTERING METHODS (THE BEST TWO SCORES IN EACH COLUMN
ARE HIGHLIGHTED IN BOLD).
0.448Â±0.037 0.352Â±0.036
0.161Â±0.022 0.129Â±0.019
0.586Â±0.027 0.522Â±0.031
0.572Â±0.017 0.555Â±0.021
0.223Â±0.013 0.200Â±0.016
0.399Â±0.035 0.267Â±0.032
0.152Â±0.012 0.117Â±0.026
0.573Â±0.031 0.529Â±0.039
0.536Â±0.017 0.518Â±0.024
0.188Â±0.007 0.162Â±0.013
0.331Â±0.056 0.221Â±0.043
0.119Â±0.019 0.078Â±0.038
0.497Â±0.041 0.300Â±0.109
0.469Â±0.024 0.390Â±0.066
0.157Â±0.010 0.097Â±0.020
0.343Â±0.047 0.225Â±0.037
0.123Â±0.017 0.084Â±0.032
0.508Â±0.041 0.395Â±0.079
0.467Â±0.022 0.420Â±0.038
0.160Â±0.007 0.122Â±0.012
0.270Â±0.052 0.169Â±0.022
0.127Â±0.023 0.110Â±0.030
0.386Â±0.046 0.266Â±0.081
0.439Â±0.029 0.417Â±0.032
0.138Â±0.011 0.116Â±0.014
0.376Â±0.043 0.238Â±0.028
0.151Â±0.009 0.099Â±0.016
0.552Â±0.025 0.521Â±0.029
0.531Â±0.017 0.507Â±0.021
0.175Â±0.005 0.140Â±0.012
0.395Â±0.036 0.302Â±0.032
0.147Â±0.020 0.120Â±0.020
0.552Â±0.027 0.497Â±0.031
0.539Â±0.018 0.518Â±0.028
0.177Â±0.008 0.148Â±0.014
0.392Â±0.023 0.334Â±0.036
0.151Â±0.020 0.127Â±0.024
0.560Â±0.025 0.505Â±0.031
0.546Â±0.018 0.511Â±0.037
0.175Â±0.009 0.138Â±0.012
0.390Â±0.037 0.308Â±0.032
0.144Â±0.020 0.122Â±0.020
0.550Â±0.029 0.491Â±0.037
0.536Â±0.019 0.516Â±0.030
0.171Â±0.007 0.134Â±0.017
0.360Â±0.025 0.235Â±0.023
0.147Â±0.011 0.097Â±0.018
0.550Â±0.029 0.509Â±0.030
0.529Â±0.020 0.511Â±0.020
0.170Â±0.004 0.138Â±0.010
0.347Â±0.031 0.161Â±0.077
0.145Â±0.012 0.119Â±0.015
0.517Â±0.042 0.480Â±0.043
0.518Â±0.026 0.478Â±0.107
0.186Â±0.013 0.161Â±0.027
0.242Â±0.036 0.148Â±0.018
0.081Â±0.010 0.062Â±0.017
0.362Â±0.034 0.315Â±0.061
0.379Â±0.033 0.365Â±0.037
0.130Â±0.007 0.116Â±0.010
0.319Â±0.029 0.170Â±0.006
0.151Â±0.008 0.118Â±0.009
0.468Â±0.036 0.458Â±0.042
0.486Â±0.037 0.482Â±0.042
0.107Â±0.064 0.097Â±0.063
0.614Â±0.037 0.568Â±0.054
0.572Â±0.026 0.525Â±0.030
0.572Â±0.032 0.550Â±0.037
0.836Â±0.017 0.782Â±0.032
0.747Â±0.017 0.675Â±0.029
0.598Â±0.013 0.580Â±0.032
0.591Â±0.021 0.562Â±0.035
0.540Â±0.022 0.512Â±0.026
0.823Â±0.019 0.763Â±0.026
0.739Â±0.019 0.675Â±0.040
0.370Â±0.054 0.235Â±0.093
0.465Â±0.027 0.361Â±0.072
0.369Â±0.037 0.263Â±0.070
0.602Â±0.047 0.427Â±0.100
0.532Â±0.060 0.373Â±0.093
0.399Â±0.051 0.304Â±0.078
0.474Â±0.025 0.402Â±0.042
0.400Â±0.031 0.333Â±0.056
0.642Â±0.042 0.525Â±0.069
0.551Â±0.046 0.438Â±0.082
0.423Â±0.056 0.362Â±0.053
0.571Â±0.035 0.549Â±0.046
0.403Â±0.037 0.385Â±0.045
0.738Â±0.034 0.701Â±0.055
0.737Â±0.031 0.686Â±0.047
0.600Â±0.033 0.538Â±0.038
0.558Â±0.023 0.513Â±0.032
0.558Â±0.023 0.511Â±0.032
0.808Â±0.027 0.760Â±0.033
0.700Â±0.025 0.628Â±0.049
0.590Â±0.063 0.538Â±0.084
0.531Â±0.023 0.467Â±0.037
0.531Â±0.023 0.467Â±0.037
0.797Â±0.036 0.731Â±0.031
0.695Â±0.034 0.593Â±0.045
0.606Â±0.050 0.549Â±0.077
0.539Â±0.024 0.475Â±0.034
0.539Â±0.024 0.475Â±0.034
0.816Â±0.029 0.729Â±0.041
0.721Â±0.018 0.579Â±0.052
0.571Â±0.073 0.486Â±0.115
0.526Â±0.022 0.455Â±0.037
0.526Â±0.022 0.455Â±0.037
0.779Â±0.041 0.698Â±0.058
0.686Â±0.031 0.566Â±0.052
0.586Â±0.046 0.540Â±0.062
0.554Â±0.028 0.505Â±0.036
0.498Â±0.032 0.479Â±0.038
0.795Â±0.027 0.751Â±0.033
0.690Â±0.026 0.621Â±0.039
0.496Â±0.051 0.443Â±0.053
0.543Â±0.045 0.508Â±0.084
0.451Â±0.042 0.428Â±0.055
0.727Â±0.054 0.706Â±0.062
0.594Â±0.035 0.551Â±0.057
0.275Â±0.040 0.228Â±0.055
0.378Â±0.043 0.315Â±0.074
0.260Â±0.032 0.185Â±0.052
0.476Â±0.066 0.281Â±0.068
0.423Â±0.055 0.197Â±0.050
0.460Â±0.062 0.402Â±0.048
0.513Â±0.039 0.507Â±0.045
0.420Â±0.048 0.411Â±0.053
0.676Â±0.076 0.675Â±0.076
0.559Â±0.042 0.551Â±0.051
0.548Â±0.022 0.539Â±0.024
0.097Â±0.020 0.084Â±0.026
0.712Â±0.028 0.689Â±0.046
0.126Â±0.012 0.116Â±0.014
0.559Â±0.046 0.512Â±0.049
0.541Â±0.022 0.520Â±0.035
0.098Â±0.013 0.083Â±0.020
0.656Â±0.032 0.620Â±0.043
0.121Â±0.011 0.097Â±0.021
0.534Â±0.028 0.461Â±0.028
0.406Â±0.034 0.297Â±0.063
0.070Â±0.014 0.033Â±0.020
0.510Â±0.034 0.343Â±0.088
0.112Â±0.017 0.088Â±0.032
0.308Â±0.035 0.191Â±0.062
0.423Â±0.030 0.355Â±0.044
0.066Â±0.009 0.041Â±0.019
0.526Â±0.033 0.401Â±0.072
0.113Â±0.017 0.100Â±0.025
0.348Â±0.021 0.278Â±0.051
0.436Â±0.042 0.407Â±0.053
0.099Â±0.018 0.087Â±0.021
0.574Â±0.052 0.438Â±0.083
0.099Â±0.020 0.077Â±0.034
0.443Â±0.043 0.421Â±0.046
0.513Â±0.027 0.488Â±0.038
0.086Â±0.010 0.069Â±0.013
0.609Â±0.035 0.585Â±0.040
0.118Â±0.012 0.097Â±0.021
0.493Â±0.043 0.449Â±0.044
0.512Â±0.027 0.495Â±0.036
0.081Â±0.013 0.062Â±0.016
0.641Â±0.034 0.590Â±0.042
0.120Â±0.015 0.103Â±0.025
0.472Â±0.053 0.433Â±0.055
0.519Â±0.026 0.499Â±0.038
0.087Â±0.013 0.065Â±0.019
0.655Â±0.031 0.601Â±0.047
0.119Â±0.013 0.098Â±0.023
0.470Â±0.048 0.434Â±0.046
0.501Â±0.030 0.478Â±0.044
0.078Â±0.013 0.058Â±0.016
0.628Â±0.034 0.567Â±0.044
0.118Â±0.014 0.099Â±0.024
0.437Â±0.055 0.396Â±0.064
0.511Â±0.028 0.491Â±0.039
0.083Â±0.009 0.064Â±0.013
0.601Â±0.034 0.572Â±0.041
0.118Â±0.010 0.099Â±0.021
0.474Â±0.042 0.425Â±0.044
0.473Â±0.036 0.442Â±0.058
0.086Â±0.015 0.061Â±0.024
0.597Â±0.028 0.577Â±0.039
0.120Â±0.018 0.102Â±0.023
0.412Â±0.031 0.378Â±0.047
0.350Â±0.033 0.313Â±0.049
0.074Â±0.019 0.064Â±0.024
0.316Â±0.034 0.206Â±0.046
0.097Â±0.020 0.076Â±0.026
0.200Â±0.031 0.058Â±0.026
0.428Â±0.055 0.416Â±0.064
0.062Â±0.013 0.042Â±0.012
0.563Â±0.024 0.561Â±0.023
0.113Â±0.021 0.103Â±0.021
0.383Â±0.058 0.359Â±0.065
TABLE VIII
THE NUMBER OF TIMES THAT OUR METHOD IS (SIGNIFICANTLY BETTER THAN / COMPARABLE TO / SIGNIFICANTLY WORSE THAN) A BASELINE
METHOD BY STATISTICAL TEST (T-TEST WITH p < 0.05) ON THE RESULTS IN TABLE VII.
consistent clustering performances with different values of Î¸
on the benchmark datasets. Empirically, it is suggested that
the parameter Î¸ be set to moderate values, e.g., in the interval
of [0.2, 1]. In the following of this paper, for both LWEA
and LWGP, we will use Î¸ = 0.4 in all experiments on the
benchmark datasets.
C. Comparison against Base Clusterings
The purpose of ensemble clustering is to combine multiple base clusterings to obtain a probably better and more
robust consensus clustering. In this section, we compare the
consensus clusterings of the proposed LWEA and LWGP
methods against the base clusterings. For each benchmark
dataset, we run the proposed LWEA and LWGP methods
100 times, respectively, with the ensemble of base clusterings
randomly drawn from the pool at each time. The average NMI
scores and variances of LWEA, LWGP, as well as the base
clusterings are illustrated in Fig. 5. The proposed methods
exhibit signiï¬cant improvements over the base clusterings on
all the ï¬fteen benchmark datasets (see Fig. 5). Especially, for
IEEE TRANSACTIONS ON CYBERNETICS
Ensemble size
(a) Caltech20
Ensemble size
Ensemble size
Ensemble size
(d) ISOLET
Ensemble size
Ensemble size
Ensemble size
Ensemble size
Ensemble size
Ensemble size
Ensemble size
(k) Semeion
Ensemble size
Ensemble size
(m) Texture
Ensemble size
Ensemble size
Fig. 6. The average performances (w.r.t. NMI) over 20 runs by different methods with varying ensemble sizes M.
the IS, LS, MF, MNIST, ODR, PD, Semeion, texture, and USPS
datasets, the advantage of the proposed methods over the base
clusterings is even greater.
D. Comparison against Other Ensemble Clustering Methods
In this section, we compare the proposed LWEA and LWGP
methods against eleven ensemble clustering methods, namely,
CSPA , HGPA , MCLA , hybrid bipartite graph
formulation (HBGF) , EAC , weighted connected
triple based method (WCT) , weighted evidence accumulation clustering (WEAC) , graph partitioning with
multi-granularity link analysis (GP-MGLA) , Two-levelreï¬ned cO-association Matrix Ensemble (TOME) , kmeans based consensus clustering (KCC) , and spectral
ensemble clustering (SEC) . For each of the proposed
methods and the baseline methods, we use two criteria to
specify the number of clusters for the consensus clustering,
that is, best-k and true-k. For best-k, the number of clusters
that leads to the best performance is adopted for each test
method. For true-k, the actual number of classes in the dataset
is adopted for each method.
To achieve a fair comparison, we run each of the proposed
methods and the baseline methods 100 times with the ensembles randomly constructed from the base clustering pool
(see Section V-A). The average performances and standard
deviations of different methods over 100 runs are reported in
Table V (w.r.t. NMI) and Table VII (w.r.t. ARI).
As shown in Table V, the proposed LWEA and LWGP
methods achieve the best NMI scores on the IS, LR, MNIST,
ODR, Semeion, Texture, and USPS datasets in terms of both
best-k and true-k, and nearly the best scores on the ISOLET,
LS, SPF, and VS datasets. As shown in Table VII, the proposed
LWEA and LWGP methods achieve the best ARI scores on
the IS, LR, ODR, Semeion, Texture, and USPS datasets in
both best-k and true-k, and nearly the best ARI scores on the
FCT, ISOLET, LS, MF, MNIST, and PD datasets. Although
the TOME method outperforms the proposed methods on
the MF and PD datasets w.r.t. NMI, yet on all of the other
thirteen datasets it shows a lower or signiï¬cantly lower NMI
scores than our methods (see Table V). That is probably
due to the fact that the TOME method exploits Euclidian
distances between objects to improve the consensus process
and its efï¬cacy heavily relies on some implicit assumptions
on the data distribution, which places an unstable factor for
the consensus performance of TOME. To summarize, as shown
in Tables V and VII, in comparison with the eleven baseline
methods, the proposed LWEA and LWGP methods yield
overall the best performance on the benchmark datasets.
To further analyze the experimental results in Tables V and
IEEE TRANSACTIONS ON CYBERNETICS
Ensemble size
(a) Caltech20
Ensemble size
Ensemble size
Ensemble size
(d) ISOLET
Ensemble size
Ensemble size
Ensemble size
Ensemble size
Ensemble size
Ensemble size
Ensemble size
(k) Semeion
Ensemble size
Ensemble size
(m) Texture
Ensemble size
Ensemble size
Fig. 7. The average performances (w.r.t. ARI) over 20 runs by different methods with varying ensemble sizes M.
Execution time (in seconds)
Fig. 8. Execution time of different methods with varying data sizes.
VII, we use the t-test (with p < 0.05) to evaluate the
statistical signiï¬cance of the differences between our methods
and the baseline methods. Because ï¬fteen benchmark datasets
are used in our experiment and for each dataset we conduct
two comparisons (in terms of best-k and true-k respectively),
so there are totally 30 comparisons in Table V and in Table VII. It is noteworthy that in each comparison every test
method is performed 100 times and their average performances
and standard deviations are reported. In a comparison, if our
method achieves a higher (or lower) score than a baseline
method and the difference is statistically signiï¬cant according
to t-test with p < 0.05, then we say our method is signiï¬cantly
better (or signiï¬cantly worse) than the baseline method for
one time. If the difference between our method and a baseline
method is not statistically signiï¬cant in a comparison, then we
say these two methods are comparable to each other for one
time. Table VI and Table VIII report the number of times
that the proposed methods are signiï¬cantly better than or
comparable to or signiï¬cantly worse than a baseline method
w.r.t. NMI and ARI, respectively. Speciï¬cally, as shown in
Table VI, in terms of NMI, the proposed LWEA and LWGP
methods exhibit statistically signiï¬cant improvements over the
SEC, KCC, and HGPA methods in all of the 30 comparisons,
and statistically signiï¬cantly outperform each of the other
eight baseline methods at least 23 times out of the totally
30 comparisons. Similar advantages can also be observed in
Table VIII, which shows that LWEA and LWGP signiï¬cantly
outperform each baseline method (w.r.t. ARI) at least 23 times
out of the totally 30 comparisons according to t-test.
E. Robustness to Ensemble Sizes M
Furthermore, we evaluate the performances of our methods
and the baseline methods with varying ensemble sizes M. For
each ensemble size M, we run the proposed methods and the
baseline methods 20 times on each benchmark dataset, with
the ensemble of M base clusterings randomly selected at each
IEEE TRANSACTIONS ON CYBERNETICS
time. Then we illustrate the average performances, w.r.t. NMI
and ARI, of different methods with varying ensemble sizes in
Fig. 6 and Fig. 7, respectively. In terms of NMI, the TOME
method yields better performance than the proposed methods
in the MF and PD datasets, but in all of the other thirteen
datasets the proposed methods signiï¬cantly outperform the
TOME method. As shown in Fig. 6, compared with the baseline methods, the proposed methods achieve overall the most
consistent and robust performances (w.r.t. NMI) with varying
ensemble sizes on the benchmark datasets. When it comes to
the ARI measure, the proposed LWEA and LWGP methods
still achieve the best or nearly the best ARI scores on each
benchmark dataset and exhibit overall the best performances
with varying ensemble sizes (as shown in Fig. 7).
F. Execution Time
In this section, we compare the execution time of different
ensemble clustering methods with varying data sizes. The
experiments are performed on different subsets of the LR
dataset. The LR dataset consists of totally 20, 000 data objects.
When testing the data size of N â€², we randomly select a subset
of N â€² objects from the LR dataset and run different methods
on this subset to evaluate their execution time. As illustrated
in Fig. 8, the proposed LWEA method requires 75.20 seconds
to process the entire LR dataset, which is comparable to GP-
MGLA but much faster than CSPA, WCT, SRS, and TOME.
Out of the totally thirteen test methods, the MCLA method
is the fastest method, while the proposed LWGP method
is the second fastest method. The MCLA method and the
proposed LWGP method consume 5.31 seconds and 8.74
seconds respectively to process the entire LR dataset. Note that,
although the proposed LWGP method is slightly slower than
MCLA (but faster than all of the other eleven test methods),
yet it signiï¬cantly outperforms MCLA in clustering accuracy
and robustness on the benchmark datasets (see Tables V, VI,
VII, and VIII and Figs. 6 and 7).
To summarize, as shown in the experimental results on
various datasets (see Tables V, VI, VII, and VIII and Figs. 6,
7, and 8), the proposed LWEA and LWGP methods yield more
consistent and better consensus performances than the baseline
methods while exhibiting competitive efï¬ciency.
All experiments are conducted in MATLAB R2014a 64-bit
on a workstation .
VI. CONCLUSION
In this paper, we have proposed a novel ensemble clustering
approach based on ensemble-driven cluster uncertainty estimation and local weighting strategy. We propose to estimate
the uncertainty of clusters by considering the cluster labels
in the entire ensemble based on an entropic criterion, and
devise a new ensemble-driven cluster validity index termed
ECI. The ECI measure requires no access to the original data
features and makes no assumptions on the data distribution.
Then, a local weighting scheme is presented to extend the
conventional CA matrix into the LWCA matrix via the ECI
measure. With the reliability of clusters investigated and the
local diversity in ensembles exploited, we further propose
two novel consensus functions, termed LWEA and LWGP,
respectively. We have conducted extensive experiments on a
variety of real-world datasets. The experimental results have
shown the superiority of the proposed approach in terms of
both clustering quality and efï¬ciency when compared to the
state-of-the-art approaches.