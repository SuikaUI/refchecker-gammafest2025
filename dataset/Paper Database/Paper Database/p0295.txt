Sparsity and Incoherence in Compressive Sampling
Emmanuel Cand`es† and Justin Romberg♯
† Applied and Computational Mathematics, Caltech, Pasadena, CA 91125
♯Electrical and Computer Engineering, Georgia Tech, Atlanta, GA 90332
November 2006
We consider the problem of reconstructing a sparse signal x0 ∈Rn from a limited number of
linear measurements. Given m randomly selected samples of Ux0, where U is an orthonormal
matrix, we show that ℓ1 minimization recovers x0 exactly when the number of measurements
m ≥Const · µ2(U) · S · log n,
where S is the number of nonzero components in x0, and µ is the largest entry in U properly
normalized: µ(U) = √n · maxk,j |Uk,j|. The smaller µ, the fewer samples needed.
The result holds for “most” sparse signals x0 supported on a ﬁxed (but arbitrary) set T .
Given T , if the sign of x0 for each nonzero entry on T and the observed values of Ux0 are drawn
at random, the signal is recovered with overwhelming probability. Moreover, there is a sense
in which this is nearly optimal since any method succeeding with the same probability would
require just about this many samples.
Acknowledgments. E. C. is partially supported by National Science Foundation grants ITR ACI-0204932
and CCF515362, by the 2006 Waterman Award (NSF), and by a grant from DARPA. J. R. is supported
by National Science Foundation grants CCF515362 and ITR ACI-0204932. E. C. would like to thank Joel
Tropp and Houman Owhadi for fruitful conversations related to this project.
Keywords. ℓ1-minimization, basis pursuit, restricted orthonormality, sparsity, singular values of random
matrices, wavelets, discrete Fourier transform.
Introduction
Sparse recovery from partial measurements
This paper addresses the problem of signal acquisition in a broad setting. We are interested in
“sampling” a vector x0 ∈Rn. Instead of observing x0 directly, we sample a small number m of
transform coeﬃcients of x0. For an orthogonal matrix1 U with
these transform coeﬃcients are given by y0 = Ux0. Of course, if all n of the coeﬃcients y0 are
observed, recovering x0 is trivial: we simply apply 1
n U∗to the vector of observations y0. Instead,
we are concerned with the highly underdetermined case in which only a small fraction of the
components of y0 are actually sampled or observed. Given a subset Ω⊂{1, . . . , n} of size |Ω| = m,
the challenge is to infer the “long” n-dimensional vector x0 from the “short” m-dimensional vector
of observations y = UΩx0, where UΩis the m × n matrix consisting of the rows of U indexed by Ω.
In plain English, we wish to solve a system of linear equations in which there are fewer equations
than unknowns.
A special instance of this problem was investigated in a recent paper , where U is taken as the
usual discrete Fourier transform. The main result of this work is that if x0 is S-sparse (at most
S of the n components of x0 are nonzero), then it can be recovered perfectly from on the order
of S log n Fourier-domain samples. The recovery algorithm is concrete and tractable: given the
discrete Fourier coeﬃcients
x0(t)e−i2π(t−1)k/n,
or y = FΩx0 for short, we solve the convex optimization program
subject to
For a ﬁxed x0, the recovery is exact for the overwhelming majority of sample sets Ωof size
|Ω| ≥C · S · log n,
where C is a known (small) constant.
Since , a theory of “compressed sensing” has developed around several papers demonstrating the eﬀectiveness of ℓ1 minimization for recovering sparse signals from a limited number
of measurements. To date, most of this eﬀort has been focused on systems which take completely
unstructured, noise-like measurements, i.e. the observation vector y is created from a series of inner
products against random test vectors {φk}:
yk = ⟨φk, x0⟩,
k = 1, . . . , m.
The collection {φk} is sometimes referred to as a measurement ensemble; we can write (1.4) compactly as y = Φx0, where the rows of Φ are the φk. Published results take φk to be a realization
of Gaussian white noise, or a sequence of Bernoulli random variables taking values ±1 with equal
probability. This work has shown that taking random measurements is in some sense an optimal
strategy for acquiring sparse signals; it requires a near-minimal number of measurements 
1On a ﬁrst reading, our choice of normalization of U may seem a bit strange. The advantages of taking the row
vectors of U to have Euclidean norm √n are that 1) the notation in the sequel will be cleaner, and 2) it will be easier
to see how this result generalizes the special case of incomplete sampling in the Fourier domain presented in .
— m measurements can recover signals with sparsity S ≲m/ log(n/m), and all of the constants
appearing in the analysis are small . Similar bounds have also appeared using greedy and
complexity-based recovery algorithms in place of ℓ1 minimization.
Although theoretically powerful, the practical relevance of results for completely random measurements is limited in two ways. The ﬁrst is that we are not always at liberty to choose the types
of measurements we use to acquire a signal. For example, in magnetic resonance imaging (MRI),
subtle physical properties of nuclei are exploited to collect samples in the Fourier domain of a
two- or three-dimensional object of interest. While we have control over which Fourier coeﬃcients
are sampled, the measurements are inherently frequency based. A similar statement can be made
about tomographic imaging; the machinery in place measures Radon slices, and these are what we
must use to reconstruct an image.
The second drawback to completely unstructured measurement systems is computational. Random
(i.e. unstructured) measurement ensembles are unwieldy numerically; for large values of m and n,
simply storing or applying Φ (tasks which are necessary to solve the ℓ1 minimization program) are
nearly impossible. If, for example, we want to reconstruct a megapixel image (n = 1, 000, 000) from
m = 25, 000 measurements (see the numerical experiment in Section 2), we would need more than
3 gigabytes of memory just to store the measurement matrix, and on the order of gigaﬂops to apply
it. The goal from this point of view, then, is to have similar recovery bounds for measurement
matrices Φ which can be applied quickly (in O(n) or O(n log n) time) and implicitly (allowing us
to use a “matrix free” recovery algorithm).
Our main theorem, stated precisely in Section 1.2 and proven in Section 3, states that bounds
analogous to (1.3) hold for sampling with general orthogonal systems. We will show that for a ﬁxed
signal support T of size |T| = S, the program
subject to
UΩx = UΩx0
recovers the overwhelming majority of x0 supported on T and observation subsets Ωof size
|Ω| ≥C · µ2(U) · S · log n,
where µ(U) is simply the largest magnitude among the entries in U:
µ(U) = max
It is important to understand the relevance of the parameter µ(U) in (1.6). µ(U) can be interpreted
as a rough measure of how concentrated the rows of U are.
Since each row (or column) of U
necessarily has an ℓ2-norm equal to √n, µ will take a value between 1 and √n. When the rows
of U are perfectly ﬂat — |Uk,j| = 1 for each k, j, as in the case when U is the discrete Fourier
transform, we will have µ(U) = 1, and (1.6) is essentially as good as (1.3).
If a row of U is
maximally concentrated — all the row entries but one vanish — then µ2(U) = n, and (1.6) oﬀers
us no guarantees for recovery from a limited number of samples.
This result is very intuitive.
Suppose indeed that Uk0,j0 = √n and x0 is 1-sparse with a nonzero entry in the j0th location. To
reconstruct x0, we need to observe the k0th entry of Ux0 as otherwise, the data vector y will vanish.
In other words, to reconstruct x0 with probability greater than 1 −1/n, we will need to see all the
components of Ux0, which is just about the content of (1.6). This shows informally that (1.6) is
fairly tight on both ends of the range of the parameter µ.
For a particular application, U can be decomposed as a product of a sparsity basis Ψ, and an
orthogonal measurement system Φ. Suppose for instance that we wish to recover a signal f ∈Rn
from m measurements of the form y = Φf. The signal may not be sparse in the time domain but
its expansion in the basis Ψ may be
(the columns of Ψ are the discrete waveforms ψj). Our program searches for the coeﬃcient sequence
in the Ψ-domain with minimum ℓ1 norm that explains the samples in the measurement domain Φ.
In short, it solves (1.6) with
The result (1.6) then tells us how the relationship between the sensing modality (Φ) and signal model
(Ψ) aﬀects the number of measurements required to reconstruct a sparse signal. The parameter µ
can be rewritten as
µ(ΦΨ) = max
k,j |⟨φk, ψj⟩|,
and serves as a rough characterization of the degree of similarity between the sparsity and measurement systems. For µ to be close to its minimum value of 1, each of the measurement vectors
(rows of Φ) must be “spread out” in the Ψ domain. To emphasize this relationship, µ(U) is often
referred to as the mutual coherence . The bound (1.6) tells us that an S-sparse signal can
be reconstructed from ∼S log n samples in any domain in which the test vectors are “ﬂat”, i.e. the
coherence parameter is O(1).
Main result
The ability of the ℓ1-minimization program (1.5) to recover a given signal x0 depends only on 1)
the support set T of x0, and 2) the sign sequence z0 of x0 on T.2 For a ﬁxed support T, our main
theorem shows that perfect recovery is achieved for the overwhelming majority of the combinations
of sign sequences on T, and sample locations (in the U domain) of size m obeying (1.6).
The language “overwhelming majority” is made precise by introducing a probability model on the
set Ωand the sign sequence z. The model is simple: select Ωuniformly at random from the set of
all subsets of the given size m; choose each z(t), t ∈T to be ±1 with probability 1/2. Our main
result is:
Theorem 1.1 Let U be an n × n orthogonal matrix (U∗U = nI) with |Uk,j| ≤µ(U). Fix a subset
T of the signal domain. Choose a subset Ωof the measurement domain of size |Ω| = m, and a sign
sequence z on T uniformly at random. Suppose that
m ≥C0 · |T| · µ2(U) · log(n/δ)
2In other words, the recoverability of x0 is determined by the facet of the ℓ1 ball of radius ∥x0∥ℓ1 on which x0
and also m ≥C′
0 · log2(n/δ) for some ﬁxed numerical constants C0 and C′
0. Then with probability
exceeding 1−δ, every signal x0 supported on T with signs matching z can be recovered from y = UΩx0
by solving (1.5).
The hinge of Theorem 1.1 is a new weak uncertainty principle for general orthobases. Given T and
Ωas above, it is impossible to ﬁnd a signal which is concentrated on T and on Ωin the U domain.
In the example above where U = ΦΨ, this says that one cannot be concentrated on small sets in
the Ψ and Φ domains simultaneously. As noted in previous publications , this is a statement
about the eigenvalues of minors of the matrix U. Let UT be the n × |T| matrix corresponding to
the columns of U indexed by T, and let UΩT be the m × |T| matrix corresponding to the rows of
UT indexed by Ω. In Section 3, we will prove the following:
Theorem 1.2 Let U, T,and Ωbe as in Theorem 1.1. Suppose that the number of measurements m
m ≥|T| · µ2(U) · max(C1 log |T|, C2 log(3/δ)),
for some positive constants C1, C2. Then
ΩT UΩT −I∥≥1/2
where ∥· ∥is the standard operator ℓ2 norm—here, the largest eigenvalue (in absolute value).
For small values of δ, the eigenvalues of U∗
ΩT UΩT are all close to m with high probability. To see
that this is an uncertainty principle, let x ∈Rn be a sequence supported on T, and suppose that
ΩT UΩT −I∥≤1/2. It follows that
ℓ2 ≤∥UΩx∥2
which asserts that only a small portion of the energy of x will be concentrated on the set Ωin
the U-domain (the total energy obeys ∥Ux∥2
ℓ2 = n∥x∥2
ℓ2). Moreover, this portion is essentially
proportional to the size of Ω.
Contributions and relationship to prior work
The relationship of the mutual incoherence parameter µ to the performance of ℓ1 minimization
programs with equality constraints ﬁrst appeared in the context of Basis Pursuit for sparse approximation, see and also .
As mentioned in the previous section, demonstrated the eﬀectiveness of ℓ1 recovery from Fourierdomain samples in slightly more general situations than in Theorem 1.1 (randomization of the signs
on T is not required). Obviously, the results presented in this paper considerably extend this Fourier
sampling theorem.
We also note that since , several papers have appeared on using ℓ1 minimization to recover sparse
signals from a limited number of measurements . In particular and provide bounds
for reconstruction from a random subset of measurements selected from an orthogonal basis; these
papers ask that all sparse signals to be simultaneously recoverable from the same set of samples
(which is stronger than our goal here), and their bounds have log factors of (log n)6 and (log n)5
respectively. These results are based on uniform uncertainty principles, which require (1.10) to
hold for all sets T of a certain size simultaneously once Ωis chosen. Whether or not this log power
can be reduced in this context remains an open question.
A contribution of this paper is to show that if one is only interested in the recovery of nearly all
signals on a ﬁxed set T, these extra log factors can indeed be removed. We show that to guarantee
exact recovery, we only require UΩT to be well behaved for this ﬁxed T as opposed to all T’s of the
same size, which is a signiﬁcantly weaker requirement. By examining the singular values of UΩT ,
one can check whether or not (1.11) holds.
Our method of proof, as the reader will see in Section 3, relies on a variation of the powerful results
presented in about the expected spectral norm of certain random matrices. We also introduce
a novel large-deviation inequality, similar in spirit to those reviewed in but carefully tailored
for our purposes, to turn this statement about expectation into one about high probability.
Finally, we would like to contrast this work with , which also draws on the results from .
First, there is a diﬀerence in how the problem is framed. In , the m × n measurement system is
ﬁxed, and bounds for perfect recovery are derived when the support and sign sequence are chosen
at random, i.e. a ﬁxed measurement system works for most signal supports of a certain size. In
this paper, we ﬁx an arbitrary signal support, and show that we will be able to recover from
most sets of measurements of a certain size in a ﬁxed domain. Second, although slightly more
general class of measurement systems is considered in , the ﬁnal bounds for sparse recovery in
the context of (1.5) do not fundamentally improve on the uniform bounds cited above; draws
weaker conclusions since the results are not shown to be universal in the sense that all sparse signals
are recovered as in and .
Applications
In the 1990s, image compression algorithms were revolutionized by the introduction of the wavelet
transform. The reasons for this can be summarized with two major points: the wavelet transform is a much sparser representation for photograph-like images than traditional Fourier-based
representations, and it can be applied and inverted in O(n) computations.
To exploit this wavelet-domain sparsity in acquisition, we must have a measurement system which
is incoherent with the wavelet representation (so that µ in (1.6) is small) and that can be applied
quickly and implicitly (so that large-scale recovery is computationally feasible). In this section, we
present numerical experiments for two such measurement strategies.
Fourier sampling of sparse wavelet subbands
Our ﬁrst measurement strategy takes advantage of the fact that at ﬁne scales, wavelets are very
much spread out in frequency. We will illustrate this in 1D; the ideas are readily applied to 2D
Figure 1: Wavelets in the frequency domain. The curves shown above are the magnitude of the discrete
Fourier transform (2.1) of Daubechies-8 wavelets for n = 1024 and j = 1, 2, 3. The magnitude of ˆψj,· over
the subband (2.3) is shown in bold.
Labeling the scales of the wavelet transform by j = 1, 2, . . . , J, where j = 1 is the ﬁnest scale and
j = J the coarsest, the wavelets3 ψj,k at scale j are almost ﬂat in the Fourier domain over a band
of size nj = n2−j. The magnitude of the Fourier transform
ˆψj,k(ω) =
ψ(t)e−i2π(t−1)ω/n,
ω = −n/2 + 1, . . . , n/2,
is the same for each wavelet at scale j, since
ˆψj,k(ω) = e−i2π(k−1)ω/nj ˆψj,1(ω).
These spectrum magnitudes are shown for the Daubechies-8 wavelet in Figure 1. We see that over
frequencies in the jth subband
ω ∈Bj := {nj/2 + 1, . . . , nj} ∪{−nj + 1, . . . , −nj/2},
maxω∈Bj | ˆψj,k(ω)|
minω∈Bj | ˆψj,k(ω)|
Suppose now that a signal x0 is a superposition of S wavelets at scale j, that is, we can write
where w0 ∈Rnj is S-sparse, and Ψj is the n × nj matrix whose columns are the ψj,k(t) for
k = 1, . . . , nj. We will measure x0 by selecting Fourier coeﬃcients from the band Bj at random. To
see how this scheme ﬁts into the domain of the results in the introduction, let ω index the subband
Bj, let Fj be the nj × n matrix whose rows are the Fourier vectors for frequencies in Bj, let Dj be
a diagonal matrix with
(Dj)ω,ω = ˆψj,1(ω),
3Wavelets are naturally parameterized by a scale j and a shift k with k = 1, 2, . . . , n2−j — see . The wavelets
at a set scale are just circular shifts of one another: ψj,k(t) = ψj,1(t −2jk), where the substraction is modulo n.
Table 1: Number of measurements required to reconstruct a sparse subband. Here, n = 1024, S is the
sparsity of the subband, and M(S, j) is the smallest number of measurements so that the S-sparse subband
at wavelet level j was recovered perfectly in 1000/1000 trials.
and consider the nj × nj system
The columns of FjΨj are just the Fourier transforms of the wavelets given in (2.2),
(FjΨj)ω,k = e−i2π(k−1)ω/nj ˆψj,1(ω)
Uω,k = e−i2π(k−1)ω/nj,
and so U is just a nj × nj Fourier system. In fact, one can easily check that U∗U = U∗U = nj I.
We choose a set of Fouier coeﬃcients Ωof size m in the band Bj, and measure
y = FΩx0 = FΩΨjw0,
which can easily be turned into a set of samples in the U domain y′ = UΩw0 just by re-weighting
y. Since the mutual incoherence of D−1FjΨj is µ = 1, we can recover w0 from ∼S log n samples.
Table 1 summarizes the results of the following experiment: Fix the scale j, sparsity S, and a
number of measurements m. Perform a trial for (S, j, m) by ﬁrst generating a signal support T of
size S, a sign sequence on that support, and a measurement set Ωj of size m uniformly at random,
and then measuring y = FΩjΨjx0 (x0 is just the sign sequence on T and zero elsewhere), solving
(1.5), and declaring success if the solution matches x0. A thousand trials were performed for each
(S, j, m). The value M(S, j) recorded in the table is the smallest value of m such that the recovery
was successful in all 1000 trials. As with the partial Fourier ensemble (see the numerical results
in ), we can recover from m ≈2S to 3S measurements.
To use the above results in an imaging system, we would ﬁrst separate the signal/image into wavelet
subband, measure Fourier coeﬃcients in each subband as above, then reconstruct each subband
independently. In other words, if PWj is the projection operator onto the space spanned by the
columns of Ψj, we measure
yj = FΩjPWjx0
for j = 1, . . . , J, then set wj to be the solution to
subject to
FΩjΨjw = yj.
If all of the wavelet subbands of the object we are imaging are appropriately sparse, we will be able
to recover the image perfectly.
Finally, we would like to note that this projection onto Wj in the measurement process can be
avoided by constructing the wavelet and sampling systems a little more carefully. In , a “bisinusoidal” measurement system is introduced which complements the orthonormal Meyer wavelet
transform. These bi-sinusoids are an alternative orthobasis to the Wj spanned by Meyer wavelets
at a given scale (with perfect mutual incoherence), so sampling in the bi-sinusoidal basis isolates a
given wavelet subband automatically.
In the next section, we examine an orthogonal measurement system which allows us to forgo this
subband separation all together.
Noiselet measurements
In , a complex “noiselet” system is constructed that is perfectly incoherent with the Haar wavelet
representation. If Ψ is an orthonormal system of Haar wavelets, and Φ is the orthogonal noiselet
system (renormalized so that Φ∗Φ = nI), then U = ΦΨ has entries of constant magnitude:
|Uk,j| = 1,
which implies
Just as the canonical basis is maximally incoherent with the Fourier basis, so is the noiselet system
with Haar wavelets. Thus if an n-pixel image is S-sparse in the Haar wavelet domain, it can be
recovered (with high probability) from ∼S log n randomly selected noiselet coeﬃcients.
In addition to perfect incoherence with the Haar transform, noiselets have two additional properties
that make them ideal for coded image acquisition:
1. The noiselet matrix Φ can be decomposed as a multiscale ﬁlterbank. As a result, it can be
applied O(n log n) time.
2. The real and imaginary parts of each noiselet function are binary valued. A noiselet measurement of an image is just an inner product with a sign pattern, which make their implementation in an actual acquisition system easier. (It would be straightforward to use them
in the imaging architecture proposed in , for example.)
A large-scale numerical example is shown in Figure 2. The n = 10242 pixel synthetic image in
panel (a) is an exact superposition of S = 25, 000 Haar wavelets4. The observation vector y was
created from m = 70, 000 randomly chosen noiselet coeﬃcients (each noiselet coeﬃcient has a real
and imaginary part, so there are really 140, 000 real numbers recorded). From y, we are able to
recover the image exactly by solving (1.5).
This result is a nice demonstration of the compressed sensing paradigm. A traditional acquisition
process would measure all n ∼106 pixels, transform into the wavelet domain, and record the S that
are important. Many measurements are made, but comparably very few numbers are recorded. Here
we take only a fraction of the number of measurements, and are able to ﬁnd the S active wavelets
coeﬃcients without any prior knowledge of their locations.
The measurement process can be adjusted slightly in a practical setting. We know that almost all
of the coarse-scale wavelet coeﬃcients will be important (see Figure 2(b)), so we can potentially
4The image was created in the obvious way: the well-known test image was transformed into the Haar domain,
all but the 25, 000 largest Haar coeﬃcients were set to zero, and the result inverse transformed back into the spatial
Figure 2: Sparse image recovery from noiselet measurements. (a) Synthetic n = 10242-pixel image
with S = 25, 000 non-zero Haar wavelet coeﬃcients. (b) Locations (in the wavelet quadtree) of signiﬁcant wavelet coeﬃcients. (c) Image recovered from m = 70, 000 complex noiselet measurements.
The recovery matches (a) exactly.
reduce the number of measurements needed for perfect recovery by measuring these directly. In
fact, if we measure the 128 × 128 block of coarse wavelet coeﬃcients for the image in Figure 2
directly (equivalent to measuring averages over 8 × 8 blocks of pixels, 16, 384 measurement total),
we are able to recover the image perfectly from an additional 41, 808 complex noiselet measurements
(the total number of real numbers recorded is 100, 000).
General strategy
The proof of Theorem 1.1 follows the program set forth in . As detailed in these references,
the signal x0 is the unique solution to (1.5) if and only if there exists a dual vector π ∈Rn with
the following properties:
• π is in the row space of UΩ,
• π(t) = sgn x0(t) for t ∈T, and
• |π(t)| < 1 for t ∈T c.
We consider the candidate
ΩT UΩT )−1z0,
where z0 is a |T|-dimensional vector whose entries are the signs of x0 on T, and show that under
the conditions in the theorem 1) π is well deﬁned (i.e. U∗
ΩT UΩT is invertible), and given this 2)
|π(t)| < 1 on T c (we automatically have that π is in the row space of UΩand π(t) = sgn x(t) on T).
We want to show that with the support ﬁxed, a dual vector exists with high probability when
selecting Ωuniformly at random. Following , it is enough to show that the desired properties
when Ωis sampled using a Bernoulli model. Suppose Ω1 of size m is sampled uniformly at random,
and Ω2 is sampled by setting
Ω2 := {k : δk = 1},
where here and below δ1, δ2, . . . , δn is a sequence of independent identically distributed 0/1 Bernoulli
random variables with
P(δk = 1) = m/n.
P(Failure(Ω1)) ≤2 P(Failure(Ω2))
(see for details). With this established, we will establish the existence of a dual vector for x0
with high probability for Ωsampled using the Bernoulli model.
The matrix U∗
ΩT UΩT is now a random variable, which can be written as
where the uk are the row vectors of UT ; uk = (Ut,k)t∈T .
Proof of Theorem 1.2
Our ﬁrst result, which is an analog to a theorem of Rudelson [24, Th. 1], states that if m is large
enough, then on average the matrix m−1U∗
ΩT UΩT deviates little from the identity.
Theorem 3.1 Let U be an orthogonal matrix obeying (1.1). Consider a ﬁxed set T and let Ωbe a
random set sampled using the Bernoulli model. Then
ΩT UΩT −I∥≤CR ·
1≤k≤n ∥uk∥
for some positive constant CR, provided the right-hand side is less than 1. Since the coherence µ(U)
1≤k≤n ∥uk∥≤µ(U)
this implies
ΩT UΩT −I∥≤CR · µ(U)
|T| log |T|
The probabilistic model is diﬀerent here than in . The argument, however, is similar.
Proof We are interested in E ∥Y ∥where Y is the random sum
δk uk ⊗uk −I.
Note that since U∗U = nI,
n uk ⊗uk −I = 1
uk ⊗uk −I = 0.
We now use a symmetrization technique to bound the expected value of the norm of Y . We let Y ′
be an independent copy of Y , i.e.
k uk ⊗uk −I,
1, . . . , δ′
n are independent copies of δ1, . . . , δn, and write
E ∥Y ∥≤E ∥Y −Y ′∥,
which follows from Jensen’s inequality and the law of iterated expectation (also known as Fubini’s
theorem). Now let ǫ1, . . . , ǫn be a sequence of Bernoulli variables taking values ±1 with probability
1/2 (and independent of the sequences δ and δ′). We have
E ∥Y ∥≤Eδ,δ′ ∥1
k) uk ⊗uk∥
= Eǫ Eδ,δ′ ∥1
k) uk ⊗uk∥
≤2 Eǫ Eδ ∥1
ǫkδk uk ⊗uk∥;
the ﬁrst equality follows from the symmetry of the random variable (δk −δ′
k) uk ⊗uk while the last
inequality follows from the triangle inequality.
We may know apply Rudelson’s powerful lemma which states that
ǫkδk uk ⊗uk∥≤CR/4 ·
log |T| · max
k:δk=1 ∥uk∥·
for some universal constant CR > 0 (the notation should make it clear that the left-hand side is
only averaged over ǫ). Taking expectation over δ then gives
E ∥Y ∥≤CR/2 ·
1≤k≤n ∥uk∥· E
1≤k≤n ∥uk∥·
δkuk ⊗uk∥,
where the second inequality uses the fact that for a nonnegative random variable Z, E
Observe now that
δkuk ⊗uk∥= E ∥mY + mI∥≤m(E ∥Y ∥+ 1)
and, therefore, (3.7) gives
E ∥Y ∥≤a ·
E ∥Y ∥+ 1,
a = CR/2 ·
1≤k≤n ∥uk∥.
It then follows that if a ≤1,
E ∥Y ∥≤2a,
which concludes the proof of the theorem.
With Theorem 3.1 established, we have a bound on the expected value of ∥m−1U∗
ΩT UΩT −I∥.
Theorem 1.2 shows that m−1U∗
ΩT UΩT is close to the identity with high probability, turning the
statement about expectation into a corresponding large deviation result.
The proof of Theorem 1.2 uses remarkable estimates about the large deviations of suprema of sums
of independent random variables. Let Y1, . . . , Yn be a sequence of independent random variables
taking values in a Banach space and let Z be the supremum deﬁned as
where F is a countable family of real-valued functions. In a striking paper, Talagrand proved
a concentration inequality about Z which is stated below, see also [Corollary 7.8].
Theorem 3.2 Assume that |f| ≤B for every f in F, and E f(Yi) = 0 for every f in F and
i = 1, . . . , n. Then for all t ≥0,
P(|Z −E Z| > t) ≤3 exp
σ2 + B E ¯Z
where σ2 = supf∈F
i=1 E f 2(Yi), ¯Z = supf∈F | Pn
i=1 f(Yi)|, and K is a numerical constant.
We note that very precise values of the numerical constant K are known and are small, see 
and .
Proof of Theorem 1.2.
Set Y to be the matrix 1
ΩT UΩT −I and recall that 1
k=1 uk⊗uk = I,
which allows to express Y as
Note that E Yk = 0. We are interested in the spectral norm ∥Y ∥. By deﬁnition,
⟨f1, Y f2⟩= sup
⟨f1, Ykf2⟩,
where the supremum is over a countable collection of unit vectors. For a ﬁxed pair of unit vectors
(f1, f2), let f(Yk) denote the mapping ⟨f1, Ykf2⟩. Since E f(Yk) = 0, we can apply Theorem 3.2
with B obeying
|f(Yk)| ≤|⟨f1, uk⟩⟨uk, f2⟩|
for all k.
As such, we can take B = max1≤k≤n ∥uk∥2/m. We now compute
E f 2(Yk) = m
 |⟨f1, uk⟩⟨uk, f2⟩|2
|⟨uk, f2⟩|2.
1≤k≤n |⟨uk, f2⟩|2 = n, we proved that
E f 2(Yk) ≤
1≤k≤n ∥uk∥2 ≤B.
In conclusion, with Z = ∥Y ∥= ¯Z, Theorem 3.2 shows that
P(| ∥Y ∥−E ∥Y ∥| > t) ≤3 exp
1 + E ∥Y ∥
Take m large enough so that E ∥Y ∥≤1/4 in (3.4), and pick t = 1/4. Since B ≤µ2(U)|T|/m,
(3.10) gives
P(∥Y ∥> 1/2) ≤3e
CT µ2(U)|T | ,
for CT = 4K/ log(6/5). Taking C1 = 16CR and C2 = CT ﬁnishes the proof.
Proof of Theorem 1.1
With Theorem 1.2 established, we know that with high probability the eigenvalues of U∗
ΩT UΩT will
be tightly controlled — they are all between m/2 and 3m/2. Under these conditions, the inverse
ΩT UΩT ) not only exists, but we can guarantee that ∥(U∗
ΩT UΩT )−1∥≤2/m, a fact which we
will use to show |π(t)| < 1 for t ∈T c.
For a particular t0 ∈T c, we can rewrite π(t0) as
π(t0) = ⟨v0, (U∗
ΩT UΩT )−1z⟩= ⟨w0, z⟩,
where v0 is the row vector of U∗
ΩUΩT with row index t0, and w0 = (U∗
ΩT UΩT )−1v0. The following
three lemmas give estimates for the sizes of these vectors. From now on and for simplicity, we drop
the dependence on U in µ(U).
Lemma 3.1 The second moment of Z0 := ∥v0∥obeys
0 ≤µ2 m |T|.
Proof Set λ0
k = uk,t0. The vector v0 is given by
(δk −E δk) λ0
where the second equality holds due to the orthogonality of the rows of U: P
1≤k≤n uk,t0 uk,t = 0. We thus can view v0 as a sum of independent random variables:
Yk = (δk −m/n)λ0
where we note that E Yk = 0. It follows that
E⟨Yk, Yk⟩+
E⟨Yk, Yk′⟩=
E⟨Yk, Yk⟩.
E ∥Yk∥2 = m
k|2∥uk∥2 ≤m
k|2 µ2|T|.
k|2 = n, we proved that
This establishes the claim.
The next result shows that the tail of Z0 exhibits a Gaussian behavior.
Lemma 3.2 Fix t0 ∈T c and let Z0 = ∥v0∥. Deﬁne σ as
σ2 = µ2 m · max(1, µ|T|/√m).
Fix a > 0 obeying a ≤(m/µ2)1/4 if µ|T|/√m > 1 and a ≤(m/µ2|T|)1/2 otherwise. Then
m|T| + aσ) ≤e−γa2,
for some positive constant γ > 0.
The proof of this lemma uses the powerful concentration inequality (3.9).
Proof By deﬁnition, Z0 is given by
⟨v0, f⟩= sup
(and observe Z0 = ¯Z0). For a ﬁxed unit vector f, let f(Yk) denote the mapping ⟨Yk, f⟩. Since
E f(Yk) = 0, we can apply Theorem 3.2 with B obeying
|f(Yk)| ≤|λ0
k| |⟨f, uk⟩| ≤|λ0
k| ∥uk∥≤µ2 |T|1/2 := B.
Before we do this, we also need bounds on σ2 and E Z0. For the latter, we simply use
For the former
E f 2(Yk) = m
k|2 |⟨uk, f⟩|2 ≤m
µ2 |⟨uk, f⟩|2.
1≤k≤n |⟨uk, f⟩|2 = n, we proved that
E f 2(Yk) ≤mµ2 
In conclusion, Theorem 3.2 shows that
P(|Z0 −E Z0| > t) ≤3 exp
Suppose now σ2 = Bµ
m|T| ≥µ2m, and ﬁx t = aσ. Then it follows from (3.15) that
P(|Z0 −E Z0| > t) ≤3e−γa2,
provided that Bt ≤σ2. The same is true if σ2 = µ2m ≥Bµ
m|T| and Bt ≤µ2m. We omit the
details. The lemma follows from (3.14).
Lemma 3.3 Let w0 = (U∗
ΩT UΩT )−1v0. With the same notations and hypotheses as in Lemma 3.2,
t0∈T c ∥w0∥≥2µ
|T|/m + 2aσ/m
≤n e−γ a2 + P(∥U∗
ΩT UΩT ∥≤m/2).
Let A and B be the events {∥U∗
ΩT UΩT ∥≥m/2} and {supt0∈T c ∥v0∥≤µ
m ∥T| + a σ}
respectively, and observe that Lemma 3.2 gives P(Bc) ≤ne−γ a2. On the event A ∩B
t0∈T c ∥w0∥≤2
m |T| + a σ)
The claim follows.
Lemma 3.4 Assume that z(t), t ∈T is an i.i.d. sequence of symmetric Bernoulli random variables.
For each λ > 0, we have
t∈T c |π(t)| > 1
≤2ne−1/2λ2 + P
t0∈T c ∥w0∥> λ
Proof The proof is essentially an application of Hoeﬀding’s inequality . Conditioned on the w0,
this inequality states that
 |⟨w0, z⟩| > 1 | w0
Recall that π(t0) = ⟨w0, z⟩. It then follows that
t0∈T c |π(t0)| > 1 | sup
t0∈T c ∥w0∥≤λ
which proves the result.
The pieces are in place to prove Theorem 1.1. Set λ = 2µ
|T|/m+2aσ/m. Combining Lemmas 3.4
and 3.3, we have for each a > 0 obeying the hypothesis of Lemma 3.2
t∈T c |π(t)| > 1
≤2ne−1/2λ2 + ne−γ a2 + P (∥(U∗
ΩT UΩT )∥≤m/2) .
For the second term to be less than δ, we choose a such that
a2 = γ−1 log(n/δ),
and assume this value from now on. The ﬁrst term is less than δ if
λ2 ≥2 log(2n/δ).
Suppose µ |T| ≥√m. The condition in Lemma 3.2 is a ≤(m/µ2)1/4 or equivalently
m ≥µ2 γ−2 [log(n/δ)]2,
where γ is a numerical constant. In this case, aσ ≤µ
m |T| which gives
Suppose now that µ |T| ≤√m. Then if |T| ≥a2, aσ ≤µ
m |T| which gives again (3.20). On the
other hand if |T| ≤a, λ ≤4aσ/m and
To verify (3.19), it suﬃces to take m obeying
≥2 log(2n/δ).
This analysis shows that the second term is less than δ if
m ≥K1 µ2 max(|T|, log(n/δ)) log(n/δ)
for some constant K1. Finally, by Theorem 1.2, the last term will be bounded by δ if
m ≥K2 µ2 |T| log(n/δ)
for some constant K2. In conclusion, we proved that there exists a constant K3 such that the
reconstruction is exact with probability at least 1 −δ provided that the number of measurements
m ≥K3 µ2 max(|T|, log(n/δ)) log(n/δ).
The theorem is proved.
Discussion
It is possible that a version of Theorem 1.1 exists that holds for all sign sequences on a set T
simultaneously, i.e. we can remove the condition that the signs are chosen uniformly at random.
Proving such a theorem with the methods above would require showing that the random vector
ΩT UΩT )−1v0, where v0 is as in (3.12), will not be aligned with the ﬁxed sign sequence
z. We conjecture that this is indeed true, but proving such a statement seems considerably more
The new large-deviation inequality of Theorem 1.2 can also be used to sharpen results presented
in about using ℓ1 minimization to ﬁnd the sparsest decomposition of a signal in a union of
bases. Consider a signal f ∈Rn that can be written as a sparse superposition of the columns of a
dictionary D = (Ψ1 Ψ2) where each Ψi is an orthonormal basis. In other words f = Dx0, where
x0 ∈R2n has small support. Given such an f, we attempt to recover x0 by solving
subject to
Combining Theorem 1.2 with the methods used in , we can establish that if
| supp x| ≤Const ·
1Ψ2) · log n,
then the following will be true with high probability (where the support and signs of x0 are drawn
at random):
1. There is no x ̸= x0 with | supp x| ≤| supp x0| with f = Dx. That is, x0 is the sparsest
possible decomposition of f.
2. We can recover x0 from f by solving (4.1).
This is a signiﬁcant improvement over the bounds presented in , which have logarithmic factors
of (log n)6.