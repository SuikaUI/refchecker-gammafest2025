Neural Network Ensembles: Evaluation of
Aggregation Algorithms
P.M. Granitto, P.F. Verdes and H.A. Ceccatto
Instituto de F´ısica Rosario, CONICET/UNR, Boulevard 27 de Febrero 210 Bis,
2000 Rosario, Rep´ublica Argentina
Ensembles of artiﬁcial neural networks show improved generalization capabilities
that outperform those of single networks. However, for aggregation to be eﬀective,
the individual networks must be as accurate and diverse as possible. An important
problem is, then, how to tune the aggregate members in order to have an optimal
compromise between these two conﬂicting conditions. We present here an extensive
evaluation of several algorithms for ensemble construction, including new proposals
and comparing them with standard methods in the literature. We also discuss a potential problem with sequential aggregation algorithms: the non-frequent but damaging selection through their heuristics of particularly bad ensemble members. We
introduce modiﬁed algorithms that cope with this problem by allowing individual
weighting of aggregate members. Our algorithms and their weighted modiﬁcations
are favorably tested against other methods in the literature, producing a sensible
improvement in performance on most of the standard statistical databases used as
benchmarks.
Key words: Machine Learning, Ensemble Methods, Neural Networks, Regression
Introduction
For most regression and classiﬁcation problems, combining the outputs of several predictors improves on the performance of a single generic one . Formal
support to this property is provided by the so-called bias/variance dilemma
 , based on a suitable decomposition of the prediction error. According to
these ideas, good ensemble members must be both accurate and diverse, which
poses the problem of generating a set of predictors with reasonably good individual performances and independently distributed predictions for the test
 
25 September 2018
Diverse individual predictors can be obtained in several ways. These include:
i) using diﬀerent algorithms to learn from the data (classiﬁcation and regression trees, artiﬁcial neural networks, support vector machines, etc.), ii)
changing the internal structure of a given algorithm (for instance, number of
nodes/depth in trees or architecture in neural networks), and iii) learning from
diﬀerent adequately-chosen subsets of the data set.
The probability of success in strategy iii), the most frequently used, is directly tied to the instability of the learning algorithm . That is, the method
must be very sensitive to small changes in the structure of the data and/or
in the parameters deﬁning the learning process. Again, classical examples in
this sense are classiﬁcation and regression trees and artiﬁcial neural networks
(ANNs). In particular, in the case of ANNs the instability comes naturally
from the inherent data and training process randomness, and also from the
intrinsic non-identiﬁability of the model.
The combination of strong instability of the learning algorithm with the tradeoﬀpredictors’ diversity vs. good individual generalization capabilities requires
an adequate selection of the ensemble members. Attempts to achieve a good
compromise between the above mentioned properties include elaborations of
two general techniques: bagging and boosting . These standard methods
for ensemble construction follow two diﬀerent strategies: Bagging (short for
’bootstrap aggregation’), and variants thereof, train independent predictors on
bootstrap re-samples Ln (n = 1, M) of the available data D, usually employing
the unused examples Vn = D−Ln for validation purposes. These predictors are
then aggregated according to diﬀerent rules (for instance, simple or weighted
average). Boosting and its variants are stagewise procedures that, starting
from a predictor trained on D, sequentially train new aggregate members
on bootstrap re-samples drawn with modiﬁed probabilities. According to the
general approach, each example in D is given a diﬀerent chance to appear in a
new training set by prioritizing patterns poorly learnt on previous stages. In
the end, the predictions of the diﬀerent members so generated are weighted
with a decreasing function of the error each predictor makes on its training
For regression problems, on which we will focus here, boosting is still a construction area, where no algorithm has emerged yet as ’the’ proper way of
implementing this technique . Consequently, bagging is the
most common method for ANN aggregation. On the other hand, intermediate alternatives between bagging and boosting, which optimize directly the
ensemble generalization performance instead of seeking for the best individual members, have not been much explored . In this work we compare
diﬀerent strategies for ensemble construction, restricting ourselves to work in
the regression setting and using ANNs as learning method. These restrictions
are not essential; in principle, our analysis can be extended to classiﬁcation
problems and to other regression/classiﬁcation methods. Furthermore, we will
discuss stepwise algorithms to build the best aggregate after network training,
thus incorporating the condition of optimal ensemble performance. Our main
purpose is to establish rules as general as possible to build accurate regression
aggregates. For this, we will
• discuss, in a unifying picture, several alternatives already proposed in the
literature for the aggregation of ANNs,
• present a new algorithm that is optimal within this uniﬁed point of view,
• propose a simple weighting scheme of ensemble members that improves the
aggregates’ generalization performances, and
• perform an extensive comparison of all these methods among themselves
and with boosting techniques on several synthetic and real-world data sets.
The organization of this work is the following: In Section 2 we re-discuss
several bagging-like methods proposed in the literature, considering them as
diﬀerent strategies for selecting the termination point of training processes for
ensemble members. In this section we also present a new algorithm that is
optimal from this point of view. In Section 3 we introduce the synthetic and
real-world databases considered in this study, and describe the experimental
settings used to learn from them. In Section 4 we obtain empirical evidence
on the relative eﬃcacy of all the methods discussed in Section 2 by applying
them to these databases. Then, in Section 5 we present a modiﬁed, weighted
version of the best algorithms and test their performances by comparison with
the results in Section 4 and also against boosting and other techniques. Finally,
in Section 6 we summarize the work done and the main results obtained, and
draw some conclusions.
Ensemble Construction Algorithms
The simplest way of generating a regressor aggregate is bagging . According
to this method, from the data set D containing N examples (t, x) one generates
bootstrap re-samples Ln (n = 1, M) by drawing with replacement N training
patterns. Thus, each training set Ln will contain, on average, 0.63N diﬀerent
examples, some of them repeated one or more times . The remaining 0.37N
examples in Vn = D −Ln are generally used for validation purposes in the
regressor learning phase (backpropagation training of the ANN in our case).
In this way one generates M diﬀerent members fn of the ensemble, whose outputs on a test point x are ﬁnally averaged to produce the aggregate prediction
Φ(x) = w1f1(x) + ... + wMfM(x). The weights wn are usually taken equal to
1/M (simple averaging). Other options will be discussed in the next section.
Notice that, according to this method, all the regressors are trained independently and their performances individually optimized using the “out-of-bag”
data in Vn. Then, although there is no ﬁne-tuning of the ensemble members’
diversity, the method frequently improves largely on the average performance
of the single regressors fn.
Bagging can be viewed as a ﬁrst stage in a sequence of increasingly more sophisticated algorithms for building a composite ANN regressor. To understand
this, let’s consider ﬁrst the situation in which a common validation subset V of
the dataset D is kept unseen by all the networks during their training phases.
Let’s also consider training to convergence M ANNs on bootstrap re-samples
Ln obtained now from L = D−V, saving the intermediate states fn(τ) at each
training epoch τ (i.e., fn(τ) is the ANN model whose weights and biases take
the values obtained at epoch τ of the training process). Building an ensemble
is then translated to the task of selecting a combination of one state fn(τ opt
from each of the M runs to create an optimal ensemble, that is, an ensemble with the smallest error on V. In this light, bagging solves the problem by
choosing the state using only information on the given run (τ opt
is the number of training epochs for which the validation error on V is minimum). In
more advanced algorithms, the regressors are not optimized individually but
as part of the aggregate. For ANNs, the simplest way of doing this is choosing
a (common) optimal number of training epochs τ opt
= τ opt for all networks
by optimizing the ensemble performance on V :
τ opt = argminτ
[t −Φ(x, w(τ))]2.
Here w(τ) are the ANN internal parameters (weights and biases) at epoch τ.
Thus, instead of validating the ensemble members one by one to maximize
their individual performances as in bagging, the algorithm selects a common
optimal stopping point τ opt for all the networks in the ensemble. In practice,
one ﬁnds that τ opt is in general larger than the individual stopping points
found in bagging, i.e., some controlled degree of single network overﬁtting
improves the aggregate’s performance. In the following we will refer to this
algorithm as “Epoch”.
The above described strategy can be further pushed on by selecting not a
single optimal τ opt for all networks but independent τ opt
for each network in
the ensemble. This requires minimizing
[t −Φ(x, w(⃗τ))]2
as a function of the set of training epochs ⃗τ = {τn; n = 1, M} for all networks. This can be accomplished, for instance, by using simulated annealing
in ⃗τ-space. That is, starting from networks trained ⃗τ0 epochs, we randomly
change τ0n and check whether the ensemble generalization error (2) increases
or decreases when network n is trained up to τ0n + ∆τ. As usual, we accept
the move with probability 1 when E(⃗τ) decreases, and with probability
exp{−β[E(⃗τ) −E(⃗τ0)]}
1 + exp{−β[E(⃗τ) −E(⃗τ0)]}
when E(⃗τ) increases. This is repeated many times considering diﬀerent networks n (chosen either at random or sequentially), while the annealing parameter β is conveniently increased at each step; the algorithm runs until E(⃗τ)
settles in a deep local minimum. In practice we have taken ∆τ = rτ max/20,
where τ max is the maximum number of training epochs and r is a random number in the interval [−1, 1]. The annealing temperature was decreased according to β−1 = 0.995qE(⃗τ0)/2, where q is the annealing step. We point out that
the minimization problem is simple enough not to depend critically on these
choices. As far as we know, this algorithm —which we will call “SimAnn”—
has not been previously discussed in the literature and constitutes one of the
main contributions of this work. Notice that for its implementation, as well
as for the simplest implementation of Epoch, one is forced to store all the
intermediate networks fn[w(τ)]. However, given the large storage capacity in
computers nowadays, in most applications this requirement is not severe.
In the common situation of scarcity of data, the need to keep an independent
validation set V is a serious drawback that limits the eﬃcacy of the methods
discussed above. An alternative approach is to resort to the out-of-bag patterns
(tp, xp) ∈Vn unseen by network fn, and optimize with respect to the number
of training epochs the error
[tp −Φp(xp, w(⃗τ))]2.
Here Φp(xp, w(⃗τ)) = P
n=1,M wpnfn[xp, w(⃗τ)] is the aggregate regressor built
with those networks that have not seen pattern (tp, xp) in their training phase,
where γpn = 1 if (tp, xp) ∈Vn and 0 otherwise. Notice that the validation
procedure generated by Eq. (4) amounts to eﬀectively optimizing the performances of several subsets of the M trained ANNs, each subset including on
average 0.37M networks. The advantage is that, like in the description of bagging at the beginning of this section, no sub-utilization of data for validation
purposes is necessary.
The above described strategy can be slightly simpliﬁed by selecting independent τ opt
for each network in the ensemble. This is the proposal of the so-called
NeuralBAG algorithm , which chooses
(tp,xp)∈Vn
[tp −Φp(xp, w(τ))]2
This is a rather ad hoc criterion: notice that in (6) the networks fm with m ̸= n
are trained up to τ opt
n , but they are eﬀectively trained τ opt
epochs in the ﬁnal
ensemble. Nevertheless, judging from the reported results , it seems to be
eﬀective in practice.
All the strategies for ANN aggregation discussed so far minimize some particular error function in a global way. A diﬀerent approach is to adapt the
typical hill-climbing search method to this problem. In a previous work 
we proposed a simple way of generating a ANN ensemble through the sequential aggregation of individual predictors, where the learning process of a new
ensemble member is validated by the previous-stage aggregate prediction performance. That is, the early-stopping method is applied by monitoring the
generalization capability on Vn+1 of the n-stage aggregate predictor plus the
n + 1 network being currently trained. In this way we retain the simplicity of
independent network training and only the validation process becomes slightly
more involved, leading again to a controlled overtraining (“late-stopping”) of
the individual networks. Notice that, despite the stepwise characteristic of this
algorithm (here called SECA, for Stepwise Ensemble Construction Algorithm),
it can be implemented after the parallel training of networks if desirable. Alternatively, if implemented sequentially it avoids completely the burden of storing
networks at intermediate training times like in the algorithms described above.
For the sake of completeness, we summarize the implementation of SECA as
Step 1: Generate a training set L1 by a bootstrap re-sample from dataset D,
and a validation set V1 = D −L1 by collecting all instances in D that are
not included in L1. Produce a model f1 by training a network on L1 until a
minimum ef(V1) of the generalization error on V1 is reached.
Step 2: Generate new training and validation sets L2 and V2 respectively,
using the procedure described in Step 1. Produce a model f2 training a network
until the generalization error on V2 of the aggregate predictor Φ2 = (f1+f2)/2
reaches a minimum eΦ(V2). In this step the parameters of model f1 are kept
constant and the model f2 is trained with the usual (quadratic) cost function
Step 3: Iterate the process until a number M of models is produced. A suitable
M can be estimated from the behavior of eΦ(Vn) as a function of n, since
this error will stabilize when adding more networks to the aggregate becomes
In this algorithm the individual networks are directly trained with a latestopping method based on the current ensemble generalization performance.
The method seems to reduce the aggregate generalization error without paying much attention to whether this improvement is related to enhancing the
members’ diversity or not. However, one can see that it actually ﬁnds
diverse models to reduce the ensemble error by looking, at every stage, for a
new model anticorrelated with the current ensemble. Notice that SECA can
be also implemented using an external validation set V, in which case all the
bootstrap complements Vn are replaced by this ﬁxed set.
All the above described methods constitute a chain of increasingly optimized
algorithms for ensemble building, starting from the simplest Bagging idea
of optimizing networks independently to SimAnn, which should produce the
“optimal” ensemble (i.e., the ensemble with the minimum validation error 4).
Let’s consider a simple analysis of the computational cost involved in the implementation of these algorithms. Once the M ANNs have been independently
trained and T networks saved along each training evolution, which is common
to all the algorithms, Bagging requires a computational time t ∼M × T to
select the best combination (essentially, the evaluation of the T ANN’s validation errors for each of the M networks to ﬁnd the corresponding minima).
Epoch requires exactly the same computational eﬀort to ﬁnd the (common)
optimal stopping point for all networks. NeuralBag uses, instead, t ∼M2 × T
evaluations to ﬁnd the best aggregate. Finally, SECA and SimAnn require
× M × T and p × M × T network evaluations, respectively. Here we
have written the number of simulated annealing steps Nsa = pT, with p an
arbitrary integer, to facilitate the comparison. In the following we will take
p ∼M to have a fair comparison between NeuralBag, SECA and SimAnn.
Notice, however, that the major demand from a computational point of view
is the ANN training and not the network selection to build the ensemble.
In practice, in the algorithms’ evaluations in Section 4 and 5 we have taken
M = 20, T = 200 and p = 15, with all the networks trained a maximum of
10T to 100T epochs, depending on the database.
As mentioned in the Introduction, a completely diﬀerent strategy for building composite regression/classiﬁcation machines is boosting. For classiﬁcation
problems, its main diﬀerence with bagging is the use of modiﬁed probabilities
to re-sample the training sets Ln. At stage n, the weights associated to examples in D are larger for those examples poorly learnt in previous stages, so that
they eventually appear several times in Ln. In this way, the new predictor fn
trained on Ln specializes on these hard examples. Finally, the inclusion of fn
in the ensemble with a suitably-chosen weight allows the exponential decrease
with boosting rounds n of the ensemble’s training error on the whole dataset D.
Notice that, in addition to the above mentioned modiﬁcation of re-sampling
probabilities, other diﬀerences with bagging are: i) boosting is essentially a
stage-wise approach, which requires a sequential training of the aggregate
members fn, and ii) in the ﬁnal ensemble these members are weighted according to their performances on the respective training sets Ln (using a decreasing
function of the training error). A further consideration of this last characteristic will be done in Section 4, where we discuss a weighting scheme for bagged
regressors alternative to the simple average considered in this section.
While boosting is, as explained above, a well deﬁned procedure in the classiﬁcation setting, for regression problems there are several ways of implementing its
basic ideas. Unfortunately, none of them has yet emerged as “the” proper way
of boosting regressors. Without the intention of exhausting all the proposed
implementations, we can distinguish two boosting strategies for solving regression problems: i) by forward stage-wise additive modelling, which modiﬁes the
target values to eﬀectively ﬁt residual errors , and ii) by reducing the
regression problem to classiﬁcation and essentially changing example weights
to emphasize those which were poorly learnt on previous stages of the ﬁtting
process . In order to compare with the bagging-like algorithms described above, in this work we will implement the boosting techniques from
 and as examples of these two diﬀerent strategies.
In Sections 4 and 5 we will show how all the heuristic algorithms described in
this section work on real and synthetic data. This will provide a fairly extensive
comparison of the already known methods and will test the new SimAnn
algorithm against all the other methods. In the next section we brieﬂy describe
the databases and experimental settings considered for this comparison.
Benchmark Databases and Experimental Settings
We have evaluated the algorithms described in the previous section by applying them to several benchmark databases: the synthetic Friedman #1, 2, 3
data sets and chaotic Ikeda map, and the real-world Abalone, Boston Housing, Ozone and Servo data sets. In the cases of the Friedman data sets we can
control the (additive) noise level, which allows us to investigate its inﬂuence
on the diﬀerent algorithm’s performances. We present the results for the Ikeda
map together with those of real-world sets because the level of noise in this
problem is ﬁxed by its intrinsic dynamics. In addition, at the end of next section we will present results on the Mackey-Glass equation, which allows a more
general comparison with other regression methods in the literature previously
applied to this problem .
In the following we give brief descriptions of the databases and the ANN architectures used. In all cases, the number of hidden units h have been selected
by trial and error, using a validation set and looking for the minimum generalization error on this set as a function of h. Once the network architecture
was chosen, it was kept the same during all the calculations. Notice that this
is not a particularly important point, since we want to compare the eﬃcacy of
diﬀerent aggregation methods and all of them use the same trained networks.
• Friedman #1
The Friedman #1 synthetic data set corresponds to training vectors with
10 input and one output variables generated according to
t = 10 sin(x1x2) + 20(x3 −0.5)2 + 10x4 + 5x5 + ε,
where ε is Gaussian noise and x1, . . . x10 are uniformly distributed over the
interval . Notice that x6, . . . x10 do not enter in the deﬁnition of t and
are only included to check the prediction method’s ability to ignore these
inputs. In order to explore the algorithm’s performances in diﬀerent situations we considered diﬀerent noise levels and training set lengths. The
Gaussian noise component was alternatively set to: ε = 0 (No noise, labeled
“free”), ε with normal distribution N(µ = 0, σ = 1) (low noise), and ε with
normal distribution N(µ = 0, σ = 2)) (high noise). We generated 1200 sample vectors for each noise level and these data sets were randomly split in
training and test sets. The training sets D had alternatively 50, 100 and 200
patterns, while the test set contained always 1000 examples. We considered
ANNs with 10:h:1 architectures, with the number of hidden units h = 6, 10
and 15 for increasing number of patterns in the training set.
• Friedman #2
Friedman #2 has four independent variables and the target data are generated according to
x2x3 −(x2x4)−2 + ε
where the zero-mean, normal noise is adjusted to give noise-to-signal power
ratios of 0 (no noise), 1:9 (low noise) and 1:3 (high noise). The variables xi
are uniformly distributed in the ranges
0 < x1 < 100,
0 < x3 < 1,
1 < x4 < 11
The training sets contained 20, 50 and 100 patterns, and the test set had
always 1000 patterns. We considered 4:h:1 ANNs, with h = 4, 6 and 8 according to the training set length.
• Friedman #3
Friedman #3 has also four independent variables distributed as above
but the target data are generated as
"x2x3 −(x2x4)−2
The noise-to-signal ratios were chosen as before, but in this case the training sets contained 100, 200 and 400 patterns. Accordingly, we considered
h = 6, 8 and 12. As in the previous cases, the test sets had always 1000
The age of abalone is determined by cutting the shell through the cone,
staining it, and counting the number of rings through a microscope. To avoid
this boring task, other measurements easier to obtain are used to predict
the age. Here we considered the data set that can be downloaded from
the UCI Machine Learning Repository (ftp to ics.uci.edu/pub/machinelearning-databases), containing 8 attributes and 4177 examples without
missing values. Of these, 1045 patterns were used for testing and 3132 for
training (for all real-world problems considered, the data set splitting in
learning and test sets was chosen following ). The ANNs used to learn
from this set had a 8:5:1 architecture.
• Boston Housing
This data set consists of 506 training vectors, with 11 input variables
and one target output. The inputs are mainly socioeconomic information
from census tracts on the greater Boston area and the output is the median
housing price in the tract. These data can also be downloaded from the UCI
Machine Learning Repository.
Here we considered 450 training examples and 56 data points for the test
set. The ANNs used had a 11:5:1 architecture.
The Ozone data correspond to meteorological information (humidity, temperature, etc.) related to the maximum daily ozone (regression target) at a
location in Los Angeles area. Removing missing values one is left with 330
training vectors, containing 8 inputs and one target output in each one. The
data set can be downloaded by ftp (to ftp.stat.berkeley.edu/pub/users/breiman)
from the Department of Statistics, University of California at Berkeley.
We considered ANNs with 8:5:1 architectures and performed a (random)
splitting of the data in training and test sets containing, respectively, 295
and 35 patterns.
The servo data cover an extremely non-linear phenomenon –predicting the
rise time of a servomechanism in terms of two (continuous) gain settings and
two (discrete) choices of mechanical linkages. The set contains 167 instances
and can be downloaded from the UCI Machine Learning Repository.
We considered 4:15:1 ANNs, using 150 examples for training and 17 examples for testing purposes.
The Ikeda laser map , which describes instabilities in the transmitted
light by a ring cavity system, is given by the real part of the complex iterates
zn+1 = 1 + 0.9zn exp
(1 + |zn|2)
Here we have generated 1100 iterates, using 100 in the training set and 1000
for testing purposes.
After some preliminary investigations, we chose an embedding dimension
5 for this map and considered ANNs with a 5:10:1 architecture.
For each one of these databases we trained M = 20 independent networks,
storing T = 200 intermediate weights and biases w(⃗τ) on long training experiments until convergence (10T to 100T epochs, depending on the database).
We considered this number of networks after checking on preliminary evaluations that there were no sensible performance improvements with bigger
ensembles. With these 20 ANNs we implemented the diﬀerent bagging-like
ensemble construction algorithms, changing the training stopping points of
individual networks according to the criteria discussed in the previous section. We did this for the following two diﬀerent validation scenarios:
• Keeping an external validation set V, randomly selected from the data set
D, and training the 20 ANNs on diﬀerent bootstrap re-samples of L = D−V.
Here we considered two partitions of D: 20/80% and 37/63% (following the
bootstrap proportion), where in each case the ﬁrst number indicates the
fraction of data points in V. For this case only the bagging-like algorithms
discussed in the previous section were considered.
• Validating the training process directly with the out-of-bag data, as explained in the previous section. This procedure makes full use of the available data, and in general should produce better results than the previous
situation. In this case we tested bagging-like techniques and also boosted
ANNs according to the Friedman and Drucker algorithms, considering a maximum of 20 boosting rounds for comparison.
The results given in the following section correspond to an average over 50
independent runs of the above-described procedures, without discarding any
anomalous case (for Boston, Ozone and Servo databases we averaged over 100
experiments because the smaller test sets allow larger sample ﬂuctuations).
We will not indicate the variance of average errors, since these deviations
only characterize the dispersion in performances due to diﬀerent realizations
of training and test sets. They have no direct relevance in comparing the
average performances of diﬀerent methods (in each run all the algorithms
use the same 20 networks). This procedure guarantees that diﬀerences in the
ﬁnal ensemble performances are only due to the aggregation methods and/or
validation settings.
Finally, at the end of Section 5 we compare the best performing algorithms here
considered with several other methods in the literature. For this comparison
we use the chaotic Mackey-Glass time series:
• Mackey-Glass
The Mackey-Glass time-delay diﬀerential equation is a model for blood
cell regulation. It is deﬁned by
0.2x(t −τ)
1 + x10(t −τ) −0.1x(t)
When x(0) = 1.2 and τ = 17, we have a non-periodic and non-convergent
time series that is very sensitive to initial conditions (we assume x(t) = 0
when t < 0).
In order to compare with the results in and , we have downloaded
the database used by these authors and considered, like in these works,
an embedding dimension d = 6 and 1194 patterns for training and 1000
patterns for testing purposes. For this problem we took h = 40.
Evaluation Results
The results quoted below are given in terms of the normalized mean-squared
test error:
NMSET = MSET
deﬁned as the mean-squared error on the test set T divided by the variance of
the total data set D. According to this deﬁnition, NMSE ≃1 for a constant
predictor equal to the data average and 0 for a perfect one. Then, its value allows to appraise both the predictor’s performance and the relative complexity
of the diﬀerent regression tasks. Notice that, as indicated in the table captions,
the results are given in units of 10−2, so that all the errors are much smaller
than 1 and, consequently, the predictions much better than the trivial data
In Tables 1a and 1b we present results for synthetic and real databases respectively, in the situation in which an external validation set containing 20% of
the data is used. Tables 2a and 2b correspond to the same case but with 37% of
validation data. As mentioned in the previous section, here only bagging-like
algorithms are compared. Tables 3a and 3b present the corresponding results
for all bagging-like algorithms and out-of-bag validation (no hold out data).
First, for external validation, the experiments indicate that Epoch performs
better than Bagging in only 2 of the 27 cases corresponding to synthetic
databases (Friedman #1,2 and 3, Tables 1a and 2a), and in 2 or none out of 5
cases for the real-world databases (Tables 1b and 2b), depending on the validation set size. This poor performance becomes slightly better for out-of-bag
validation (9/27, Table 3a, and 3/5, Table 3b, respectively). Consequently,
we do not ﬁnd any advantage in using this algorithm instead of Bagging.
Something similar happens with NeuralBag, which, in spite of the good results presented in , in our experiments only improves on Bagging in roughly
half the cases. On the contrary, both SECA and SimAnn are clearly better
than Bagging: on average, both methods outperform Bagging approximately
in 21 of the 27 Friedman problems and in all but one case for real databases,
independently of the validation used.
Considering all the methods together, Table 1a shows that, for the 27 learning problems associated to the Friedman synthetic data, in 22 cases the best
method is either SECA or the network selection via simulated annealing
(SimAnn). This pattern is conﬁrmed by the results in Table 2a, where SECA
and SimAnn are again the best performers in 22 of the 27 cases. For the
real-world databases these two methods outperform the other bagging-like algorithms in all cases (see Tables 1b and 2b), with a particularly good performance of SimAnn. For out-of-bag validation, Table 3a shows that, consistently
with the previous results, in 19 of the 27 experimental situations SECA and
SimmAnn are the best performers. We stress, however, the good performance
of Epoch on Friedman #2 data set, particularly for noise-free data. For the
real-world databases, Table 3b shows that SECA and SimAnn produced the
best results in all but one (Servo) of the regression problems investigated. All
these results obey the expected behaviors with noise level and data set length.
Furthermore, for the synthetic Friedman problems in general the test error
is larger when more data are held out for validation, although this not the
case for the real-world Abalone, Boston and Ozone datasets. Moreover, for
bagged regressors, independently of the method used to ensemble them, in all
cases the out-of-bag validation is more eﬃcient than keeping an external set,
in agreement with other works in the literature . Notice, however, that this
last observation is not valid in the noisy Friedman #2 and Servo problems for
a single ANN.
In the case of out-of-bag validation, we have performed a paired t-test to check
whether SECA and SimAnn signiﬁcantly outperform Bagging. Following the
procedure in , we considered a binary variable that assumes a value of 1
when SECA/SimAnn is better than Bagging and 0 otherwise. If the average
of this variable diﬀers from 0.5 and this diﬀerence is statistically signiﬁcant,
we can aﬃrm that one of the methods is better than the other. The results of
the t-test are given in Tables 4a and 4b. For Friedman #1, SECA and SimAnn
are better than Bagging in all cases; for Friedman #2 there are no clear differences between the methods, and for Friedman #3 SECA and SimAnn are
signiﬁcantly better than Bagging except for high noise and few patterns in
the training set. For the real-world databases, SECA and SimAnn are always
better than Bagging, with more than 95% of statistical signiﬁcance in several
cases. These results are in complete agreement with the NMSE comparison in
Tables 1-3.
Accuracy vs. Diversity
In order to gain some insight into SECA and SimAnn’s behaviors that might
explain the good performances shown in Tables 1-4, we have investigated the
standard bias-variance decomposition of the generalization error .
Consider general regression problems where vectors x of predictor variables are
obtained from some distribution P(x) and regression targets t are generated
according to t = f(x) + ε. Here f is the true regression function and ε is
random noise with zero mean. If we estimate f learning from a data set L and
obtain a model fL, the (quadratic) generalization error on a test point (t, x)
averaged over all possible realizations of L (with respect to P and noise ε) can
be decomposed as:
E[(t −fL(x))2|L] = E[ε2|ε]
+ (E[fL(x)|L] −f(x))2 + E[(fL(x) −E[fL(x)|L])2|L]
The ﬁrst term on the right-hand side is simply the noise variance σ2
second and third terms are, respectively, the squared bias and variance of the
estimation method.
From the point of view of a single estimator fL, we can interpret this equation
by saying that a good method should be not biased and have as little variance
as possible between diﬀerent realizations. There are learning methods (for
instance, ANNs) for which the ﬁrst condition is reasonably well met but the
second one is not satisﬁed since, for small changes or even with no changes at
all in L, diﬀerent learning experiments lead to distinct predictors fL (unstable
learning methods). A way to take advantage of this apparent weakness of these
methods is to make an aggregate of them.
If we rewrite the error decomposition in the form:
E[(t −E[fL(x)|L])2|L] ≡Bias2 + σ2
ε = MeanError −Variance,
we can reinterpret this equation in the following way: using the ensemble
average Φ ≡E[fL|L] as estimator, the generalization error can be reduced if
we produce fairly accurate models fL (small MeanError) that output diverse
predictions for each test point (large Variance). Of course, there is a trade-oﬀ
between these two conditions, but ﬁnding a good compromise between the
regressors’ mean accuracy (≡1/MeanError) and diversity (≡Variance) seems
particularly feasible for largely unstable methods like ANNs.
For the results shown in Table 2a we have estimated separately the accuracy
and diversity components of the error according to 15. In Table 5 we present
the results obtained; for easier comparison, we give them normalized by the
mean accuracy and diversity of the bagging ensemble members. As expected,
bagging produces the most accurate but less diverse predictors; instead, the
other aggregation methods resign some accuracy to gain diversity. More interestingly, we see that, despite the similar performance of SECA and SimAnn in
Table 2a, these aggregation methods select very diﬀerent ensemble members.
In particular, SimAnn seeks mainly for diverse predictors although they are
not very accurate while SECA introduces diversity in a more balanced way
with accuracy. Epoch strategy is in general intermediate between these two
methods but not eﬀective enough to outperform them.
Weighting Ensemble Members
In the previous section we evaluated several ensemble construction algorithms
that essentially diﬀer in the way they select the particular stopping points for
independently-trained ANNs. The ﬁnal aggregate prediction on a test point is
simply the mean of the individual predictions, without weighting the outputs
of the ensemble members (wn = 1/M, n = 1, M). This is not particularly wise
for SECA, since some of these members may have poor generalization capabilities. SECA is a stepwise optimization technique, and a known problem with
these heuristics is that during the optimization process they cannot review
the choices made in the past. Figure 1 shows a typical example of the problem
one can ﬁnd for a given realization of the Friedman #1 data set. Open circles
represent the evolution of training and test errors during the construction of
the ensemble using SECA. In this example, the fourth added network clearly
Train Error
Number of Networks
Test Error
Fig. 1. Evolution of training and test errors during the ensemble construction, in
arbitrary units. Open circles correspond to SECA; dots indicate the same evolution
when ensemble members are weighted according to W-SECA.
deteriorates the ensemble performance, and this eﬀect cannot be compensated
by the addition of more networks. Obviously, it also inﬂuences the selection
of the following ensemble members.
In a previous work we explored a possible way to cope with this problem,
using a slightly diﬀerent SECA algorithm that only accepts networks that
improve the ensemble performance. Unfortunately, new results showed that
this algorithm also produces some overﬁtting, being unable to clearly outperform bagging on small and noisy data sets. A possible intermediate solution
is weighting the ensemble members, instead of rejecting them if they do not
improve the overall ensemble performance. This allows us to reduce the in-
ﬂuence of bad choices made in the past by simply giving smaller weights to
troublesome networks. Then, following general ideas from boosting, we propose to modify the algorithm so that the output of the ensemble at the m-th
stage becomes
where wn is a decreasing function of en, the MSE of the n-th member over D;
i.e., we weight each ensemble member according to its individual performance
on the whole dataset. This is the way in which boosting reduces the importance
of overﬁtted members in the ﬁnal ensemble. In practice we have explored two
Coefficient
Fig. 2. Normalized mean-squared test error NMSE as a function of the weighting coeﬃcient α for Friedman #1 data sets (From top to bottom: high noise, low
noise and noise-free data). Open circles (dots) correspond to exponential (potential)
weighting.
diﬀerent weighting functions:
j exp(−αej).
Figure 2 shows the results obtained with SECA on Friedman #1 for both
weighting schemes. As we can see, for small to intermediate values of α weighting produces better results than simply averaging the individual predictions.
For large values of α some overﬁtting is observed, since only a few particular
networks eﬀectively contribute to the ensemble. As expected, this is more acute
for exponential weighting, but there are no other major diﬀerences between
both laws. On the other hand, the smaller the noise the larger one can take
α before overﬁtting is observed. We have also considered Friedman #2 and
3 data sets, and the behavior in Figure 2 is representative of the general trend.
In Figure 1 we have included the results of weighting SECA using the power
law with α = 2 (this algorithm will be called W-SECA) for the case discussed above. The problematic fourth network is given a small weight, and is
practically ignored by the ensemble.
We have performed the t-test described before to establish whether the performance obtained with W-SECA is signiﬁcantly better than that of SECA. To
have a fair evaluation of the algorithm just described we used the same ANNs
considered in the previous section. Tables 6a and 6b show the corresponding results, which indicate that W-SECA outperforms SECA with statistical
signiﬁcance in practically all situations studied.
We have also applied the weighting scheme to Bagging and SimAnn to investigate if the eﬀective elimination of some bad ensemble members (by giving
them small weights) has also impact on these algorithms. One question to
answer here is: Will this improvement wash out the diﬀerences observed in
Tables 1-4 between the diﬀerent algorithms? In order to have a fair evaluation
of the algorithms we used the same ANNs considered in the previous section.
The results obtained are collected in Tables 7a and 7b.
These tables show that for the 27 regression problems corresponding to Friedman #1, 2 and 3 data sets, W-SECA and W-SimAnn outperform Bagging in
21 and 22 cases respectively. Moreover, for the real-world databases and Ikeda
map, W-SimAnn is always better than W-Bagging, and W-SECA looses only
on the Servo database against W-Bagging. That is, although weighting is in
general beneﬁcial for all algorithms, the member selection strategy is still important to obtain good performances. This is also supported by the results
of paired t-tests between W-SECA and W-SimAnn against W-Bagging (see
Tables 8a and 8b).
It is also of interest to mention that the weighted algorithms outperform nonweighted ones in 26 out of the 32 cases investigated (compare best results
in Tables 3 and 7). From the remaining 6 cases, 4 correspond to high noisescarce data situations. Notice also that in these cases the best performers are
SECA and SimAnn. Finally, we stress that from the 32 problems considered,
W-Bagging performs better than Bagging in 27 cases, W-SimAnn performs
better than SimAnn in 29 cases, and W-SECA performs better than SECA
in 31 cases. We remark the important improvements for SECA, which were
expected according to the above discussion in connection with Figure 1.
In Tables 7a and 7b we also present results obtained with the boosting algorithms proposed in (“D-Boosting”) and (“F-Boosting”) for comparison.
For these algorithms we used a maximum of 20 boosting rounds, which should
produce a fair test considering the 20 ANNs ensembled in the bagging-like
methods. Notice that for the 27 Friedman datasets the boosting algorithms
perform better than W-SECA and W-SimAnn only in three cases, and in these
few cases the “D-Boosting” implementation is always the best performer. For
the real-world databases and Ikeda map this implementation and W-SimAnn
are the top performers.
As a ﬁnal investigation on W-SECA and W-SimAnn, we have considered the
Mackey-Glass problem. This allows us to make a comparison with seven other
regression methods based on Support Vector Machines (SVM) and regularized
boosting using Radial Basis Function (RBF) networks, as described in and
 . Following these works, we introduced three levels of uniform noise to the
training set, with signal-to-noise ratios of 6.2%, 12.4% and 18.6% respectively,
and Gaussian noise with signal-to-noise ratios of 22.15% and 44.30% respectively. The test set is kept noiseless to measure the true prediction error. As
mentioned in Section 4, to have a fair comparison all the experimental settings
(training and test set lengths, embedding dimension, etc.) are the same as in
 and . Table 9 presents the corresponding results, which show that W-
SECA and W-SimAnn are among the top performers in most cases. We stress
that they perform worse than SVM methods only for the largest Gaussian
noise case (we are disregarding the CG-k result for the largest uniform noise
since it seems to be abnormally small).
Summary and Conclusions
We have performed a thorough evaluation of simple methods for the construction of neural network ensembles. In particular, we considered algorithms that
can be implemented with an independent (parallel) training of the ensemble
members, and introduced a framework that suggests naturally the SimAnn
algorithm as the optimal one. Taking as the ensemble prediction the simple
average of the ANN outputs, we have shown that SECA and SimAnn are the
best performers in the large majority of cases. These include synthetic data
with diﬀerent noise levels and training set sizes, and also real-world databases.
We have also shown that these methods resolve very diﬀerently the compromise between accuracy and diversity through their particular search strategies.
The greedy method that we termed SECA seeks at every stage for a new member that is at least partially anticorrelated with the previous-stage ensemble
estimator. This is achieved by applying a late-stopping method in the learning
phase of individual networks, leading to a controlled level of overtraining of
the ensemble members. In principle this algorithm retains the simplicity of
independent network training, although, if necessary, it can avoid the computational burden of saving intermediate networks in this phase since it can
be implemented in a sequential way. In this implementation the method is
a stepwise construction of the ensemble, where each network is selected at a
time and only its parameters have to be saved. We showed, by comparison
with several other algorithms in the literature, that this strategy is eﬀective,
as exempliﬁed by the results in Tables 1 to 4.
The SimAnn algorithm, ﬁrst proposed in this work, uses simulated annealing
to minimize the error on unseen data with respect to the number of training epochs for each individual ensemble member. This method is also very
eﬀective, being competitive with SECA on most databases. Furthermore, the
implementation of the minimization step at the end of the ANNs training
process is, in practice, not very time consuming from a computational point
of view, being only a fraction of the time required to train the networks.
We also discussed a known problem with stepwise selection procedures like
SECA, and proposed a modiﬁcation of this algorithm to overcome it. The
modiﬁed algorithm, which we called W-SECA, weights the predictions of ensemble members depending on their individual performances. We showed that
it improves the results obtained with SECA in practically all cases. Moreover,
since weighting is in general beneﬁcial for all the methods considered, we investigated whether this procedure overrides the diﬀerences between ensemble
construction algorithms. We found that the weighted versions of SECA and
SimAnn (W-SECA and W-SimAnn) are again the best performers, indicating
the intrinsic eﬃciency of these construction methods.
Finally, we have also performed a comparison of W-SECA and W-SimAnn
with several other regression methods, including methods based on SVMs
and regularized boosting. For this we used published results in the literature
corresponding to the Mackey-Glass equation. Again in this case we found
that the algorithms here proposed are among the top performers in almost
all situations considered (Tables 7a,b and 9). Given this competitive behavior
of weighted bagging-like algorithms, one is tempted to speculate that, for
regression, the success of boosting ideas might not be mainly related to the
modiﬁcation of resampling probabilities but to the ﬁnal error weighting of
ensemble members.
We want to comment on the performance improvement obtained with the
aggregation algorithms discussed in this work. We found that in general SECA
and SimAnn, either in their weighted or non-weighted versions, produce better
results than other algorithms in the literature (Bagging, NeuralBAG, Epoch).
Although this holds true in several cases with more than 95% of statistical
signiﬁcance, the performance improvement obtained depends largely on the
problem considered. For instance, with respect to Bagging, the most common
algorithm, one ﬁnds the following (compare Tables 3 and 7): For the Friedman
databases the improvement can be very low with high noise (1% or less), to
very large (up to 200%) in some noise-free cases. For databases with ﬁxed noise
level (real-world data and Ikeda map), the improvement ranges from less than
1% (Abalone) to nearly 12% (Ikeda). The answer to the question as to whether
these performances justify the use of the algorithms here proposed instead of
Bagging would depend, then, on the concrete application, particularly on how
critical it is. However, even for non-critical ones there is always a chance that
using W-SECA or W-SimAnn one might obtain fairly large improvements. In
any case, the best justiﬁcation is perhaps the fact that not much additional
computational time is required to implement these algorithms.
Before closing, we want to comment on a recent work partially related to
the present one. In Ref. , the authors use genetic algorithms (GA) to select
a suitable subset of all the trained nets to build the ensemble. For this, they
train a number of ANNs to the optimal validation point like in Bagging, and
then assign to these networks an importance weight through a GA strategy.
Finally, only those networks that have weights larger than a given threshold
are kept in the ensemble. In the algorithm –that they termed GASEN– the
predictions of the retained ANNs are combined by simple average, which leads
to good generalization capabilities when compared to Bagging and Boosting.
This strategy can be readily implemented within our SimAnn algorithm by
simply allowing an appropriate random change in the number of aggregated
ANNs, in addition to the stochastic search of optimal training epochs. Notice
that this procedure would extend the GASEN optimization to ANNs trained
an arbitrary number of epochs (instead of searching only among those at the
optimal validation point), which might be important since some degree of
single-network overtraining is known to improve the ensemble performance.
This combined approach, which would presumably bring the best of SimAnn
and GASEN into a single algorithm, is, however, beyond the scope of this
In addition to the above proposal, as future work we are also considering
extending the methods here proposed to classiﬁcation problems and comparing
their performances with those of boosting strategies.
Acknowledgements
We acknowledge support for this project from the National Agency for the
Promotion of Science and Technology (ANPCyT) of Argentina (grant PICT
11-11150).