xDeepFM: Combining Explicit and Implicit Feature Interactions
for Recommender Systems
Jianxun Lian
University of Science and Technology
 
Xiaohuan Zhou
Beijing University of Posts and
Telecommunications
 
Fuzheng Zhang
Microsoft Research
 
Zhongxia Chen
University of Science and Technology
 
Microsoft Research
 
Guangzhong Sun
University of Science and Technology
 
Combinatorial features are essential for the success of many commercial models. Manually crafting these features usually comes
with high cost due to the variety, volume and velocity of raw data
in web-scale systems. Factorization based models, which measure
interactions in terms of vector product, can learn patterns of combinatorial features automatically and generalize to unseen features
as well. With the great success of deep neural networks (DNNs)
in various fields, recently researchers have proposed several DNNbased factorization model to learn both low- and high-order feature
interactions. Despite the powerful ability of learning an arbitrary
function from data, plain DNNs generate feature interactions implicitly and at the bit-wise level. In this paper, we propose a novel
Compressed Interaction Network (CIN), which aims to generate
feature interactions in an explicit fashion and at the vector-wise
level. We show that the CIN share some functionalities with convolutional neural networks (CNNs) and recurrent neural networks
(RNNs). We further combine a CIN and a classical DNN into one
unified model, and named this new model eXtreme Deep Factorization Machine (xDeepFM). On one hand, the xDeepFM is able
to learn certain bounded-degree feature interactions explicitly; on
the other hand, it can learn arbitrary low- and high-order feature
interactions implicitly. We conduct comprehensive experiments on
three real-world datasets. Our results demonstrate that xDeepFM
outperforms state-of-the-art models. We have released the source
code of xDeepFM at 
CCS CONCEPTS
• Information systems →Personalization; • Computing methodologies →Neural networks; Factorization methods;
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from .
KDD ’18, August 19–23, 2018, London, United Kingdom
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5552-0/18/08...$15.00
 
Factorization machines, neural network, recommender systems,
deep learning, feature interactions
ACM Reference Format:
Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie,
and Guangzhong Sun. 2018. xDeepFM: Combining Explicit and Implicit
Feature Interactions for Recommender Systems. In KDD ’18: The 24th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining,
August 19–23, 2018, London, United Kingdom. ACM, New York, NY, USA,
10 pages. 
INTRODUCTION
Features play a central role in the success of many predictive systems. Because using raw features can rarely lead to optimal results,
data scientists usually spend a lot of work on the transformation of
raw features in order to generate best predictive systems 
or to win data mining games . One major type of feature
transformation is the cross-product transformation over categorical
features . These features are called cross features or multi-way
features, they measure the interactions of multiple raw features. For
instance, a 3-way feature AND(user_organization=msra,
item_category=deeplearning, time=monday) has value
1 if the user works at Microsoft Research Asia and is shown a technical article about deep learning on a Monday.
There are three major downsides for traditional cross feature engineering. First, obtaining high-quality features comes with a high
cost. Because right features are usually task-specific, data scientists need spend a lot of time exploring the potential patterns from
the product data before they become domain experts and extract
meaningful cross features. Second, in large-scale predictive systems
such as web-scale recommender systems, the huge number of raw
features makes it infeasible to extract all cross features manually.
Third, hand-crafted cross features do not generalize to unseen interactions in the training data. Therefore, learning to interact features
without manual engineering is a meaningful task.
Factorization Machines (FM) embed each feature i to a
latent factor vector vi = [vi1,vi2, ...,viD], and pairwise feature
interactions are modeled as the inner product of latent vectors:
f (2)(i, j) = ⟨vi, vj⟩xixj. In this paper we use the term bit to denote
a element (such as vi1) in latent vectors. The classical FM can be
extended to arbitrary higher-order feature interactions , but one
 
KDD ’18, August 19–23, 2018, London, United Kingdom
J. Lian, X. Zhou, F. Zhang, Z. Chen, X. Xie, and G. Sun
major downside is that, proposes to model all feature interactions, including both useful and useless combinations. As revealed
in , the interactions with useless features may introduce noises
and degrade the performance. In recent years, deep neural networks
(DNNs) have become successful in computer vision, speech recognition, and natural language processing with their great power of
feature representation learning. It is promising to exploit DNNs to
learn sophisticated and selective feature interactions. proposes
a Factorisation-machine supported Neural Network (FNN) to learn
high-order feature interactions. It uses the pre-trained factorization
machines for field embedding before applying DNN. further
proposes a Product-based Neural Network (PNN), which introduces
a product layer between embedding layer and DNN layer, and does
not rely on pre-trained FM. The major downside of FNN and PNN is
that they focus more on high-order feature interactions while capture little low-order interactions. The Wide&Deep and DeepFM
 models overcome this problem by introducing hybrid architectures, which contain a shallow component and a deep component
with the purpose of learning both memorization and generalization.
Therefore they can jointly learn low-order and high-order feature
interactions.
All the abovementioned models leverage DNNs for learning
high-order feature interactions. However, DNNs model high-order
feature interactions in an implicit fashion. The final function learned
by DNNs can be arbitrary, and there is no theoretical conclusion
on what the maximum degree of feature interactions is. In addition,
DNNs model feature interactions at the bit-wise level, which is
different from the traditional FM framework which models feature
interactions at the vector-wise level. Thus, in the field of recommender systems, whether DNNs are indeed the most effective model
in representing high-order feature interactions remains an open
question. In this paper, we propose a neural network-based model
to learn feature interactions in an explicit, vector-wise fashion.
Our approach is based on the Deep & Cross Network (DCN) ,
which aims to efficiently capture feature interactions of bounded
degrees. However, we will argue in Section 2.3 that DCN will lead
to a special format of interactions. We thus design a novel compressed interaction network (CIN) to replace the cross network in
the DCN. CIN learns feature interactions explicitly, and the degree
of interactions grows with the depth of the network. Following
the spirit of the Wide&Deep and DeepFM models, we combine
the explicit high-order interaction module with implicit interaction module and traditional FM module, and name the joint model
eXtreme Deep Factorization Machine (xDeepFM). The new model
requires no manual feature engineering and release data scientists
from tedious feature searching work. To summarize, we make the
following contributions:
• We propose a novel model, named eXtreme Deep Factorization Machine (xDeepFM), that jointly learns explicit and
implicit high-order feature interactions effectively and requires no manual feature engineering.
• We design a compressed interaction network (CIN) in xDeepFM
that learns high-order feature interactions explicitly. We
show that the degree of feature interactions increases at each
layer, and features interact at the vector-wise level rather
than the bit-wise level.
• We conduct extensive experiments on three real-world dataset,
and the results demonstrate that our xDeepFM outperforms
several state-of-the-art models significantly.
The rest of this paper is organized as follows. Section 2 provides
some preliminary knowledge which is necessary for understanding
deep learning-based recommender systems. Section 3 introduces
our proposed CIN and xDeepFM model in detail. We will present
experimental explorations on multiple datasets in Section 4. Related
works are discussed in Section 5. Section 6 concludes this paper.
PRELIMINARIES
Embedding Layer
In computer vision or natural language understanding, the input
data are usually images or textual signals, which are known to be
spatially and/or temporally correlated, so DNNs can be applied
directly on the raw feature with dense structures. However, in
web-scale recommender systems, the input features are sparse, of
huge dimension, and present no clear spatial or temporal correlation. Therefore, multi-field categorical form is widely used by
related works . For example, one input instance
[user_id=s02,gender=male,
organization=msra,interests=comedy&rock] is normally transformed into a high-dimensional sparse features via field-aware
one-hot encoding:
[0, 1, 0, 0, ..., 0
] [0, 1, 0, 0, ..., 0
orдanization
] [0, 1, 0, 1, ..., 0
An embedding layer is applied upon the raw feature input to compress it to a low dimensional, dense real-value vector. If the field
is univalent, the feature embedding is used as the field embedding. Take the above instance as an example, the embedding of
feature male is taken as the embedding of field gender. If the field
is multivalent, the sum of feature embedding is used as the field
embedding. The embedding layer is illustrated in Figure 1. The
result of embedding layer is a wide concatenated vector:
e = [e1, e2, ..., em]
where m denotes the number of fields, and ei ∈RD denotes the
embedding of one field. Although the feature lengths of instances
can be various, their embeddings are of the same length m × D,
where D is the dimension of field embedding.
Figure 1: The field embedding layer. The dimension of embedding in this example is 4.
Combining Explicit and Implicit Feature Interactions for Recommender Systems KDD ’18, August 19–23, 2018, London, United Kingdom
Implicit High-order Interactions
FNN , Deep Crossing , and the deep part in Wide&Deep 
exploit a feed-forward neural network on the field embedding vector e to learn high-order feature interactions. The forward process
x1 = σ(W(1)e + b1)
xk = σ(W(k)x(k−1) + bk)
where k is the layer depth, σ is an activation function, and xk is
the output of the k-th layer. The visual structure is very similar
to what is shown in Figure 2, except that they do not include the
FM or Product layer. This architecture models the interaction in a
bit-wise fashion. That is to say, even the elements within the same
field embedding vector will influence each other.
PNN and DeepFM modify the above architecture slightly.
Besides applying DNNs on the embedding vector e, they add a twoway interaction layer in the architecture. Therefore, both bit-wise
and vector-wise interaction is included in their model. The major
difference between PNN and DeepFM, is that PNN connects the
outputs of product layer to the DNNs, whereas DeepFM connects
the FM layer directly to the output unit (refer to Figure 2).
Figure 2: The architecture of DeepFM (with linear part omitted) and PNN. We re-use the symbols in , where red edges
represent weight-1 connections (no parameters) and gray
edges represent normal connections (network parameters).
Explicit High-order Interactions
 proposes the Cross Network (CrossNet) whose architecture is
shown in Figure 3. It aims to explicitly model the high-order feature interactions. Unlike the classical fully-connected feed-forward
network, the hidden layers are calculated by the following cross
operation:
k−1wk + bk + xk−1
where wk, bk, xk ∈RmD are weights, bias and output of the k-th
layer, respectively. We argue that the CrossNet learns a special type
of high-order feature interactions, where each hidden layer in the
CrossNet is a scalar multiple of x0.
Theorem 2.1. Consider a k-layer cross network with the (i+1)-th
layer defined as xi+1 = x0xT
i wi+1 + xi. Then, the output of the cross
network xk is a scalar multiple of x0.
Figure 3: The architecture of the Cross Network.
Proof. When k=1, according to the associative law and distributive law for matrix multiplication, we have:
x1 = x0(xT
0 w1) + x0
where the scalar α1 = xT
0 w1 + 1 is actually a linear regression of
x0. Thus, x1 is a scalar multiple of x0. Suppose the scalar multiple
statement holds for k=i. For k=i + 1, we have :
xi+1 = x0xT
i wi+1 + xi
= x0((αix0)T wi+1) + αix0
where, αi+1 = αi(xT
0 wi+1 + 1) is a scalar. Thus xi+1 is still a scalar
multiple of x0. By induction hypothesis, the output of cross network
xk is a scalar multiple of x0.
Note that the scalar multiple does not mean xk is linear with x0.
The coefficient αi+1 is sensitive with x0. The CrossNet can learn
feature interactions very efficiently (the complexity is negligible
compared with a DNN model), however the downsides are: (1) the
output of CrossNet is limited in a special form, with each hidden
layer is a scalar multiple of x0; (2) interactions come in a bit-wise
OUR PROPOSED MODEL
Compressed Interaction Network
We design a new cross network, named Compressed Interaction
Network (CIN), with the following considerations: (1) interactions
are applied at vector-wise level, not at bit-wise level; (2) high-order
feature interactions is measured explicitly; (3) the complexity of
network will not grow exponentially with the degree of interactions.
Since an embedding vector is regarded as a unit for vector-wise
interactions, hereafter we formulate the output of field embedding
as a matrix X0 ∈Rm×D, where the i-th row in X0 is the embedding
vector of the i-th field: X0
i,∗= ei, and D is the dimension of the field
embedding. The output of the k-th layer in CIN is also a matrix
Xk ∈RHk ×D, where Hk denotes the number of (embedding) feature
vectors in the k-th layer and we let H0 = m. For each layer, Xk are
KDD ’18, August 19–23, 2018, London, United Kingdom
J. Lian, X. Zhou, F. Zhang, Z. Chen, X. Xie, and G. Sun
(a) Outer products along each dimension for
feature interactions. The tensor Zk+1 is an intermediate result for further learning.
(b) The k-th layer of CIN. It compresses the
intermediate tensor Zk+1 to Hk+1 embedding
vectors (aslo known as feature maps).
(c) An overview of the CIN architecture.
Figure 4: Components and architecture of the Compressed Interaction Network (CIN).
calculated via:
where 1 ≤h ≤Hk, Wk,h ∈RHk−1×m is the parameter matrix for
the h-th feature vector, and ◦denotes the Hadamard product, for
example, ⟨a1,a2,a3⟩◦⟨b1,b2,b3⟩= ⟨a1b1,a2b2,a3b3⟩. Note that Xk
is derived via the interactions between Xk−1 and X0, thus feature
interactions are measured explicitly and the degree of interactions
increases with the layer depth. The structure of CIN is very similar
to the Recurrent Neural Network (RNN), where the outputs of the
next hidden layer are dependent on the last hidden layer and an
additional input. We hold the structure of embedding vectors at all
layers, thus the interactions are applied at the vector-wise level.
It is interesting to point out that Equation 6 has strong connections with the well-known Convolutional Neural Networks (CNNs)
in computer vision. As shown in Figure 4a, we introduce an intermediate tensor Zk+1, which is the outer products (along each
embedding dimension) of hidden layer Xk and original feature matrix X0. Then Zk+1 can be regarded as a special type of image and
Wk,h is a filter. We slide the filter across Zk+1 along the embedding
dimension (D) as shown in Figure 4b, and get an hidden vector
i,∗, which is usually called a feature map in computer vision.
Therefore, Xk is a collection of Hk different feature maps. The term
“compressed" in the name of CIN indicates that the k-th hidden layer
compress the potential space of Hk−1 × m vectors down to Hk vectors.
Figure 4c provides an overview of the architecture of CIN. Let T
denotes the depth of the network. Every hidden layer Xk,k ∈[1,T]
has a connection with output units. We first apply sum pooling on
each feature map of the hidden layer:
fori ∈[1,Hk]. Thus, we have a pooling vector pk = [pk
2 , ...,pk
with length Hk for the k-th hidden layer. All pooling vectors from
hidden layers are concatenated before connected to output units:
p+ = [p1, p2, ..., pT ] ∈R
i=1 Hi . If we use CIN directly for binary
classification, the output unit is a sigmoid node on p+:
1 + exp(p+T wo)
where wo are the regression parameters.
CIN Analysis
We analyze the proposed CIN to study the model complexity and
the potential effectiveness.
Space Complexity. The h-th feature map at the k-th layer
contains Hk−1 × m parameters, which is exactly the size of Wk,h.
Thus, there are Hk × Hk−1 × m parameters at the k-th layer. Considering the last regression layer for the output unit, which has
k=1 Hk parameters, the total number of parameters for CIN is
k=1 Hk × (1 + Hk−1 × m). Note that CIN is independent of the
embedding dimension D. In contrast, a plainT-layers DNN contains
m × D × H1 + HT + ÍT
k=2 Hk × Hk−1 parameters, and the number
of parameters will increase with the embedding dimension D.
Usually m and Hk will not be very large, so the scale of Wk,h is
acceptable. When necessary, we can exploit a L-order decomposition and replace Wk,h with two smaller matrices Uk,h ∈RHk−1×L
and Vk,h ∈Rm×L:
Wk,h = Uk,h(Vk,h)T
where L ≪H and L ≪m. Hereafter we assume that each hidden
layer has the same number (which is H) of feature maps for simplicity. Through the L-order decomposition, the space complexity
of CIN is reduced from O(mTH2) to O(mTHL +TH2L). In contrast,
the space complexity of the plain DNN is O(mDH +TH2), which
is sensitive to the dimension (D) of field embedding.
Time Complexity. The cost of computing tensor Zk+1 (as
shown in Figure 4a) is O(mHD) time. Because we have H feature maps in one hidden layer, computing a T-layers CIN takes
O(mH2DT) time. AT-layers plain DNN, by contrast, takesO time. Therefore, the major downside of CIN lies in the time
complexity.
Polynomial Approximation. Next we examine the highorder interaction properties of CIN. For simplicity, we assume that
numbers of feature maps at hidden layers are all equal to the number
of fields m. Let [m] denote the set of positive integers that are less
than or equal to m. The h-th feature map at the first layer, denoted
h ∈RD, is calculated via:
Therefore, each feature map at the first layer models pair-wise
interactions with O(m2) coefficients. Similarly, the h-th feature
map at the second layer is:
Note that all calculations related to the subscript l and k is already
finished at the previous hidden layer. We expand the factors in
Equation 11 just for clarity. We can observe that each feature map
at the second layer models 3-way interactions with O(m2) new
parameters.
A classical k-order polynomial has O(mk) coefficients. We show
that CIN approximate this class of polynomial with only O(km3)
parameters in terms of a chain of feature maps. By induction hypothesis, we can prove that the h-th feature map at the k-th layer
i,j ...W1,r
j ◦... ◦x0
For better illustration, here we borrow the notations from . Let
α = [α1, ...,αm] ∈Nd denote a multi-index, and |α | = Ím
i=1 αi. We
omit the original superscript from x0
i , and use xi to denote it since
we only we the feature maps from the 0-th layer (which is exactly
the field embeddings) for the final expanded expression (refer to Eq.
12). Now a superscript is used to denote the vector operation, such
i = xi ◦xi ◦xi. Let VPk(X) denote a multi-vector polynomial
of degree k:
2 ◦... ◦xαm
2 ⩽|α | ⩽k
Each vector polylnomial in this class has O(mk) coefficients. Then,
our CIN approaches the coefficient wα with:
where, B = [B1, B2, ..., B|α |] is a multi-index, and Pα is the set of
all the permutations of the indices ( 1, ...1
, ..., m, ...,m
Combination with Implicit Networks
As discussed in Section 2.2, plain DNNs learn implicit high-order
feature interactions. Since CIN and plain DNNs can complement
each other, an intuitive way to make the model stronger is to combine these two structures. The resulting model is very similar to
the Wide&Deep or DeepFM model. The architecture is shown in
Figure 5. We name the new model eXtreme Deep Factorization
Machine (xDeepFM), considering that on one hand, it includes both
low-order and high-order feature interactions; on the other hand,
it includes both implicit feature interactions and explicit feature
interactions. Its resulting output unit becomes:
linear a + wT
cinp+ + b)
where σ is the sigmoid function, a is the raw features. xk
dnn, p+ are
the outputs of the plain DNN and CIN, respectively. w∗and b are
learnable parameters. For binary classifications, the loss function is
the log loss:
yiloд ˆyi + (1 −yi)loд(1 −ˆyi)
where N is the total number of training instances. The optimization
process is to minimize the following objective function:
J = L + λ∗||Θ||
where λ∗denotes the regularization term and Θ denotes the set of
parameters, including these in linear part, CIN part, and DNN part.
Figure 5: The architecture of xDeepFM.
Relationship with FM and DeepFM. Suppose all fields are
univalent. It’s not hard to observe from Figure 5 that, when the
depth and feature maps of the CIN part are both set to 1, xDeepFM
is a generalization of DeepFM by learning the linear regression
weights for the FM layer (note that in DeepFM, units of FM layer
are directly linked to the output unit without any coefficients).
When we further remove the DNN part, and at the same time use a
constant sum filter (which simply takes the sum of inputs without
any parameter learning) for the feature map, then xDeepFM is
downgraded to the traditional FM model.
KDD ’18, August 19–23, 2018, London, United Kingdom
J. Lian, X. Zhou, F. Zhang, Z. Chen, X. Xie, and G. Sun
EXPERIMENTS
In this section, we conduct extensive experiments to answer the
following questions:
• (Q1) How does our proposed CIN perform in high-order
feature interactions learning?
• (Q2) Is it necessary to combine explicit and implicit highorder feature interactions for recommender systems?
• (Q3) How does the settings of networks influence the performance of xDeepFM?
We will answer these questions after presenting some fundamental
experimental settings.
Experiment Setup
Datasets. We evaluate our proposed models on the following three datasets:
1. Criteo Dataset. It is a famous industry benchmarking dataset
for developing models predicting ad click-through rate, and is publicly accessible1. Given a user and the page he is visiting, the goal
is to predict the probability that he will clik on a given ad.
2. Dianping Dataset. Dianping.com is the largest consumer review site in China. It provides diverse functions such as reviews,
check-ins, and shops’ meta information (including geographical
messages and shop attributes). We collect 6 months’ users checkin activities for restaurant recommendation experiments. Given
a user’s profile, a restaurant’s attributes and the user’s last three
visited POIs (point of interest), we want to predict the probability
that he will visit the restaurant. For each restaurant in a user’s
check-in instance, we sample four restaurants which are within 3
kilometers as negative instances by POI popularity.
3. Bing News Dataset. Bing News2 is part of Microsoft’s Bing
search engine. In order to evaluate the performance of our model
in a real commercial dataset, we collect five consecutive days’ impression logs on news reading service. We use the first three days’
data for training and validation, and the next two days for testing.
For the Criteo dataset and the Dianping dataset, we randomly
split instances by 8:1:1 for training , validation and test. The characteristics of the three datasets are summarized in Table 1.
Table 1: Statistics of the evaluation datasets. M indicates million and K indicates thousand.
#instances
#features (sparse)
Evaluation Metrics. We use two metrics for model evaluation: AUC (Area Under the ROC curve) and Logloss (cross entropy).
These two metrics evaluate the performance from two different
angels: AUC measures the probability that a positive instance will
be ranked higher than a randomly chosen negative one. It only
takes into account the order of predicted instances and is insensitive to class imbalance problem. Logloss, in contrast, measures the
1 
2 
distance between the predicted score and the true label for each
instance. Sometimes we rely more on Logloss because we need to
use the predicted probability to estimate the benefit of a ranking
strategy (which is usually adjusted as CTR × bid).
Baselines. We compare our xDeepFM with LR(logistic regression), FM, DNN (plain deep neural network), PNN (choose the
better one from iPNN and oPNN) , Wide & Deep , DCN (Deep
& Cross Network) and DeepFM . As introduced and discussed in Section 2, these models are highly related to our xDeepFM
and some of them are state-of-the-art models for recommender systems. Note that the focus of this paper is to learn feature interactions
automatically, so we do not include any hand-crafted cross features.
Reproducibility. We implement our method using Tensorflow3. Hyper-parameters of each model are tuned by grid-searching
on the validation set, and the best settings for each model will be
shown in corresponding sections. Learning rate is set to 0.001. For
optimization method, we use the Adam with a mini-batch size
of 4096. We use a L2 regularization with λ = 0.0001 for DNN, DCN,
Wide&Deep, DeepFM and xDeepFM, and use dropout 0.5 for PNN.
The default setting for number of neurons per layer is: (1) 400 for
DNN layers; (2) 200 for CIN layers on Criteo dataset, and 100 for
CIN layers on Dianping and Bing News datasets. Since we focus on
neural networks structures in this paper, we make the dimension
of field embedding for all models be a fixed value of 10. We conduct
experiments of different settings in parallel with 5 Tesla K80 GPUs.
The source code is available at 
Table 2: Performance of individual models on the Criteo, Dianping, and Bing News datasets. Column Depth indicates the
best network depth for each model.
Model name
3 
Combining Explicit and Implicit Feature Interactions for Recommender Systems KDD ’18, August 19–23, 2018, London, United Kingdom
Table 3: Overall performance of different models on Criteo, Dianping and Bing News datasets. The column Depth presents the
best setting for network depth with a format of (cross layers, DNN layers).
Model name
Performance Comparison among
Individual Neural Components (Q1)
We want to know how CIN performs individually. Note that FM
measures 2-order feature interactions explicitly, DNN model highorder feature interactions implicitly, CrossNet tries to model highorder feature interactions with a small number of parameters (which
is proven not effective in Section 2.3), and CIN models high-order
feature interactions explicitly. There is no theoretic guarantee of
the superiority of one individual model over the others, due to that
it really depends on the dataset. For example, if the practical dataset
does not require high-order feature interactions, FM may be the
best individual model. Thus we do not have any expectation for
which model will perform the best in this experiment.
Table 2 shows the results of individual models on the three practical datasets. Surprisingly, our CIN outperform the other models
consistently. On one hand, the results indicate that for practical
datasets, higher-order interactions over sparse features are necessary, and this can be verified through the fact that DNN, CrossNet
and CIN outperform FM significantly on all the three datasets. On
the other hand, CIN is the best individual model, which demonstrates the effectiveness of CIN on modeling explicit high-order
feature interactions. Note that a k-layer CIN can model k-degree
feature interactions. It is also interesting to see that it take 5 layers
for CIN to yield the best result ON the Bing News dataset.
Performance of Integrated Models (Q2)
xDeepFM integrates CIN and DNN into an end-to-end model. While
CIN and DNN covers two distinct properties in learning feature
interactions, we are interested to know whether it is indeed necessary and effective to combine them together for jointly explicit and
implicit learning. Here we compare several strong baselines which
are not limited to individual models, and the results are shown in
Table 3. We observe that LR is far worse than all the rest models,
which demonstrates that factorization-based models are essential
for measuring sparse features. Wide&Deep, DCN, DeepFM and
xDeepFM are significantly better than DNN, which directly reflects
that, despite their simplicity, incorporating hybrid components are
important for boosting the accuracy of predictive systems. Our
proposed xDeepFM achieves the best performance on all datasets,
which demonstrates that combining explicit and implicit high-order
feature interaction is necessary, and xDeepFM is effective in learning this class of combination. Another interesting observation is
that, all the neural-based models do not require a very deep network structure for the best performance. Typical settings for the
depth hyper-parameter are 2 and 3, and the best depth setting for
xDeepFM is 3, which indicates that the interactions we learned are
at most 4-order.
Hyper-Parameter Study (Q3)
We study the impact of hyper-parameters on xDeepFM in this section, including (1) the number of hidden layers; (2) the number of
neurons per layer; and (3) activation functions. We conduct experiments via holding the best settings for the DNN part while varying
the settings for the CIN part.
Depth of Network. Figure 6a and 7a demonstrate the impact
of number of hidden layers. We can observe that the performance
of xDeepFM increases with the depth of network at the beginning.
However, model performance degrades when the depth of network
is set greater than 3. It is caused by overfitting evidenced by that
we notice that the loss of training data still keeps decreasing when
we add more hidden layers.
Number of Neurons per Layer. Adding the number of neurons per layer indicates increasing the number of feature maps in
CIN. As shown in Figure 6b and 7b, model performance on Bing
News dataset increases steadily when we increase the number of
neurons from 20 to 200, while on Dianping dataset, 100 is a more
suitable setting for the number of neurons per layer. In this experiment we fix the depth of network at 3.
Activation Function. Note that we exploit the identity as activation function on neurons of CIN, as shown in Eq. 6. A common
practice in deep learning literature is to employ non-linear activation functions on hidden neurons. We thus compare the results
of different activation functions on CIN (for neurons in DNN, we
keep the activation function with relu). As shown in Figure 6c and
7c, identify function is indeed the most suitable one for neurons in
KDD ’18, August 19–23, 2018, London, United Kingdom
J. Lian, X. Zhou, F. Zhang, Z. Chen, X. Xie, and G. Sun
number of layers
AUC on Dianping
AUC on Bing News
(a) Number of layers.
number of neurons per layer
AUC on Dianping
AUC on Bing News
(b) Number of neurons per layer.
activation functions
AUC on Dianping
AUC on Bing News
(c) Activation functions
Figure 6: Impact of network hyper-parameters on AUC performance.
number of layers
Logloss on Dianping
Logloss on Bing News
(a) Number of layers.
number of neurons per layer
Logloss on Dianping
Logloss on Bing News
(b) Number of neurons per layer.
activation functions
Logloss on Dianping
Logloss on Bing News
(c) Activation functions
Figure 7: Impact of network hyper-parameters on Logloss performance.
RELATED WORK
Classical Recommender Systems
Non-factorization Models. For web-scale recommender
systems (RSs), the input features are usually sparse, categoricalcontinuous-mixed, and high-dimensional. Linear models, such as
logistic regression with FTRL , are widely adopted as they are
easy to manage, maintain, and deploy. Because linear models lack
the ability of learning feature interactions, data scientists have to
spend a lot of work on engineering cross features in order to achieve
better performance . Considering that some hidden features
are hard to design manually, some researchers exploit boosting
decision trees to help build feature transformations .
Factorization Models. A major downside of the aforementioned models is that they can not generalize to unseen feature
interactions in the training set. Factorization Machines overcome this problem via embedding each feature into a low dimension
latent vector. Matrix factorization (MF) , which only considers
IDs as features, can be regarded as a special kind of FM. Recommendations are made via the product of two latent vectors, thus it
does not require the co-occurrence of user and item in the training set. MF is the most popular model-based collaborative filtering
method in the RS literature . extend MF to
leveraging side information, in which both a linear model and a
MF model are included. On the other hand, for many recommender
systems, only implicit feedback datasets such as users’ watching
history and browsing activities are available. Thus researchers extend the factorization models to a Bayesian Personalized Ranking
(BPR) framework for implicit feedback.
Recommender Systems with Deep Learning
Deep learning techniques have achieved great success in computer
vision , speech recognition and natural language understanding . As a result, an increasing number of researchers
are interested in employing DNNs for recommender systems.
Deep Learning for High-Order Interactions. To avoid manually building up high-order cross features, researchers apply DNNs
on field embedding, thus patterns from categorical feature interactions can be learned automatically. Representative models include FNN , PNN , DeepCross , NFM , DCN ,
Wide&Deep , and DeepFM . These models are highly related
to our proposed xDeepFM. Since we have reviewed them in Section 1 and Section 2, we do not further discuss them in detail in
this section. We have demonstrated that our proposed xDeepFM
has two special properties in comparison with these models: (1)
xDeepFM learns high-order feature interactions in both explicit
and implicit fashions; (2) xDeepFM learns feature interactions at
the vector-wise level rather than at the bit-wise level.
Deep Learning for Elaborate Representation Learning. We
include some other deep learning-based RSs in this section due to
that they are less focused on learning feature interactions. Some
early work employs deep learning mainly to model auxiliary information, such as visual data and audio data . Recently,
deep neural networks are used to model the collaborative filtering
(CF) in RSs. proposes a Neural Collaborative Filtering (NCF)
so that the inner product in MF can be replaced with an arbitrary
function via a neural architecture. model CF base on the
autoencoder paradigm, and they have empirically demonstrated
that autoencoder-based CF outperforms several classical MF models. Autoencoders can be further employed for jointly modeling
Combining Explicit and Implicit Feature Interactions for Recommender Systems KDD ’18, August 19–23, 2018, London, United Kingdom
CF and side information with the purpose of generating better latent factors . employ neural networks to jointly
train multiple domains’ latent factors. proposes the Attentive
Collaborative Filtering (ACF) to learn more elaborate preference
at both item-level and component-level. shows tha traditional
RSs can not capture interest diversity and local activation effectively,
so they introduce a Deep Interest Network (DIN) to represent users’
diverse interests with an attentive activation mechanism.
CONCLUSIONS
In this paper, we propose a novel network named Compressed Interaction Network (CIN), which aims to learn high-order feature
interactions explicitly. CIN has two special virtues: (1) it can learn
certain bounded-degree feature interactions effectively; (2) it learns
feature interactions at a vector-wise level. Following the spirit of
several popular models, we incorporate a CIN and a DNN in an
end-to-end framework, and named the resulting model eXtreme
Deep Factorization Machine (xDeepFM). Thus xDeepFM can automatically learn high-order feature interactions in both explicit and
implicit fashions, which is of great significance to reducing manual
feature engineering work. We conduct comprehensive experiments
and the results demonstrate that our xDeepFM outperforms stateof-the-art models consistently on three real-world datasets.
There are two directions for future work. First, currently we
simply employ a sum pooling for embedding multivalent fields.
We can explore the usage of the DIN mechanism to capture
the related activation according to the candidate item. Second, as
discussed in section 3.2.2, the time complexity of the CIN module
is high. We are interested in developing a distributed version of
xDeepFM which can be trained efficiently on a GPU cluster.
ACKNOWLEDGEMENTS
The authors would like to thank the anonymous reviewers for
their insightful reviews, which are very helpful on the revision
of this paper. This work is supported in part by Youth Innovation
Promotion Association of CAS.