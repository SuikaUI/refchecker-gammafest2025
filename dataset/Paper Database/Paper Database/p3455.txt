Vector Quantized Diffusion Model for Text-to-Image Synthesis
Shuyang Gu1
Dong Chen2
Jianmin Bao2
Dongdong Chen3
Baining Guo2
1University of Science and Technology of China
2Microsoft Research
3Microsoft Cloud+AI
 
{doch,jianbao,fangwen,zhanbo,dochen,luyuan,bainguo}@microsoft.com
We present the vector quantized diffusion (VQ-Diffusion)
model for text-to-image generation. This method is based
on a vector quantized variational autoencoder (VQ-VAE)
whose latent space is modeled by a conditional variant
of the recently developed Denoising Diffusion Probabilistic Model (DDPM). We ﬁnd that this latent-space method
is well-suited for text-to-image generation tasks because
it not only eliminates the unidirectional bias with existing methods but also allows us to incorporate a mask-andreplace diffusion strategy to avoid the accumulation of errors, which is a serious problem with existing methods. Our
experiments show that the VQ-Diffusion produces significantly better text-to-image generation results when compared with conventional autoregressive (AR) models with
similar numbers of parameters. Compared with previous
GAN-based text-to-image methods, our VQ-Diffusion can
handle more complex scenes and improve the synthesized
image quality by a large margin. Finally, we show that the
image generation computation in our method can be made
highly efﬁcient by reparameterization. With traditional AR
methods, the text-to-image generation time increases linearly with the output image resolution and hence is quite
time consuming even for normal size images.
Diffusion allows us to achieve a better trade-off between
quality and speed. Our experiments indicate that the VQ-
Diffusion model with the reparameterization is ﬁfteen times
faster than traditional AR methods while achieving a better image quality. The code and models are available at
 
1. Introduction
Recent success of Transformer in neural language processing (NLP) has raised tremendous interest
in using successful language models for computer vision
tasks. Autoregressive (AR) model is one of the
most natural and popular approach to transfer from text-totext generation (i.e., machine translation) to text-to-image
generation. Based on the AR model, recent work DALL-
E has achieved impressive results for text-to-image
generation.
Despite their success, existing text-to-image generation
methods still have weaknesses that need to be improved.
One issue is the unidirectional bias. Existing methods predict pixels or tokens in the reading order, from top-left
to bottom-right, based on the attention to all preﬁx pixels/tokens and the text description. This ﬁxed order introduces unnatural bias in the synthesized images because important contextual information may come from any part of
the image, not just from left or above. Another issue is
the accumulated prediction errors. Each step of the inference stage is performed based on previously sampled tokens – this is different from that of the training stage, which
relies on the so-called “teacher-forcing” practice and
provides the ground truth for each step. This difference is
important and its consequence merits careful examination.
In particular, a token in the inference stage, once predicted,
cannot be corrected and its errors will propagate to the subsequent tokens.
Diffusion) model for text-to-image generation, a model
that eliminates the unidirectional bias and avoids accumulated prediction errors.
We start with a vector quantized variational autoencoder (VQ-VAE) and model its latent space by learning a parametric model using a conditional variant of the Denoising Diffusion Probabilistic
Model (DDPM) , which has been applied to image synthesis with compelling results . We show that
the latent-space model is well-suited for the task of textto-image generation. Roughly speaking, the VQ-Diffusion
model samples the data distribution by reversing a forward
diffusion process that gradually corrupts the input via a
ﬁxed Markov chain. The forward process yields a sequence
of increasingly noisy latent variables of the same dimensionality as the input, producing pure noise after a ﬁxed
number of timesteps. Starting from this noise result, the
reverse process gradually denoises the latent variables towards the desired data distribution by learning the condiarXiv:2111.14822v3 [cs.CV] 3 Mar 2022
tional transit distribution.
The VQ-Diffusion model eliminates the unidirectional
bias. It consists of an independent text encoder and a diffusion image decoder, which performs denoising diffusion
on discrete image tokens. At the beginning of the inference stage, all image tokens are either masked or random.
Here the masked token serves the same function as those in
mask-based generative models . The denoising diffusion process gradually estimates the probability density of
image tokens step-by-step based on the input text. In each
step, the diffusion image decoder leverages the contextual
information of all tokens of the entire image predicted in
the previous step to estimate a new probability density distribution and use this distribution to predict the tokens in
the current step. This bidirectional attention provides global
context for each token prediction and eliminates the unidirectional bias.
The VQ-Diffusion model, with its mask-and-replace diffusion strategy, also avoids the accumulation of errors. In
the training stage, we do not use the “teacher-forcing” strategy.
Instead, we deliberately introduce both masked tokens and random tokens and let the network learn to predict the masked token and modify incorrect tokens. In the
inference stage, we update the density distribution of all tokens in each step and resample all tokens according to the
new distribution. Thus we can modify the wrong tokens
and prevent error accumulation. Comparing to the conventional replace-only diffusion strategy for unconditional image generation , the masked tokens effectively direct the
network’s attention to the masked areas and thus greatly reduce the number of token combinations to be examined by
the network. This mask-and-replace diffusion strategy signiﬁcantly accelerates the convergence of the network.
To assess the performance of the VQ-Diffusion method,
we conduct text-to-image generation experiments with a
wide variety of datasets, including CUB-200 , Oxford-
102 , and MSCOCO . Compared with AR model
with similar numbers of model parameters, our method
achieves signiﬁcantly better results, as measured by both
image quality metrics and visual examination, and is much
faster. Compared with previous GAN-based text-to-image
methods , our method can handle more complex scenes and the synthesized image quality is improved
by a large margin. Compared with extremely large models
(models with ten times more parameters than ours), including DALL-E and CogView , our model achieves
comparable or better results for speciﬁc types of images,
i.e., the types of images that our model has seen during
the training stage. Furthermore, our method is general and
produces strong results in our experiments on both unconditional and conditional image generation with FFHQ 
and ImageNet datasets.
The VQ-Diffusion model also provides important bene-
ﬁts for the inference speed. With traditional AR methods,
the inference time increases linearly with the output image
resolution and the image generation is quite time consuming
even for normal-size images (e.g., images larger than small
thumbnail images of 64 × 64 pixels). The VQ-Diffusion
provides the global context for each token prediction and
makes it independent of the image resolution. This allows
us to provide an effective way to achieve a better tradeoff
between the inference speed and the image quality by a
simple reparameterization of the diffusion image decoder.
Speciﬁcally, in each step, we ask the decoder to predict
the original noise-free image instead of the noise-reduced
image in the next denoising diffusion step. Through experiments we have found that the VQ-Diffusion method
with reparameterization can be ﬁfteen times faster than AR
methods while achieving a better image quality.
2. Related Work
GAN-based Text-to-image generation. In the past few
years, Generative Adversarial Networks (GANs) have
shown promising results on many tasks ,, especially
text-to-image generation . GAN-INT-CLS was
the ﬁrst to use a conditional GAN formulation for text-toimage generation.
Based on this formulation, some approaches were proposed to further improve the generation quality. These models generate high
ﬁdelity images on single domain datasets, e.g., birds 
and ﬂowers . However, due to the inductive bias on the
locality of convolutional neural networks, they struggle on
complex scenes with multiple objects, such as those in the
MS-COCO dataset .
Other works adopt a two-step process which ﬁrst
infer the semantic layout then generate different objects, but
this kind of method requires ﬁne-grained object labels, e.g.,
object bounding boxes or segmentation maps.
Autoregressive Models. AR models have shown
powerful capability of density estimation and have been
applied for image generation recently. PixelRNN , Image Transformer and ImageGPT factorized the probability density on an image
over raw pixels. Thus, they only generate low-resolution
images, like 64 × 64, due to the unaffordable amount of
computation for large images.
VQ-VAE , VQGAN and ImageBART 
train an encoder to compress the image into a lowdimensional discrete latent space and ﬁt the density of the
hidden variables. It greatly improves the performance of
image generation.
DALL-E , CogView and M6 propose ARbased text-to-image frameworks. They model the joint distribution of text and image tokens.
With powerful large
transformer structure and massive text-image pairs, they
greatly advance the quality of text-to-image generation, but
still have weaknesses of unidirectional bias and accumulated prediction errors due to the limitation of AR models.
Denoising Diffusion Probabilistic Models. Diffusion generative models were ﬁrst proposed in and achieved
strong results on image generation and image
super super-resolution recently. However, most previous works only considered continuous diffusion models on
the raw image pixels. Discrete diffusion models were also
ﬁrst described in , and then applied to text generation
in Argmax Flow . D3PMs applies discrete diffusion
to image generation. However, it also estimates the density
of raw image pixels and can only generate low-resolution
(e.g.,32 × 32) images.
3. Background:
Learning Discrete Latent
Space of Images Via VQ-VAE
Transformer architectures have shown great promise in
image synthesis due to their outstanding expressivity . In this work, we aim to leverage the transformer to
learn the mapping from text to image. Since the computation cost is quadratic to the sequence length, it is computationally prohibitive to directly model raw pixels using
transformers. To address this issue, recent works 
propose to represent an image by discrete image tokens with
reduced sequence length. Hereafter a transformer can be effectively trained upon this reduced context length and learn
the translation from the text to image tokens.
Formally, a vector quantized variational autoencoder
(VQ-VAE) is employed. The model consists of an encoder E, a decoder G and a codebook Z = {zk}K
RK×d containing a ﬁnite number of embedding vectors ,
where K is the size of the codebook and d is the dimension of codes. Given an image x ∈RH×W ×3, we obtain
a spatial collection of image tokens zq with the encoder
z = E(x) ∈Rh×w×d and a subsequent spatial-wise quantizer Q(·) which maps each spatial feature zij into its closest codebook entry zk:
zq = Q(z) =
∥zij −zk∥2
Where h × w represents the encoded sequence length and
is usually much smaller than H × W. Then the image can
be faithfully reconstructed via the decoder, i.e., ˜x = G(zq).
Hence, image synthesis is equivalent to sampling image tokens from the latent distribution. Note that the image tokens are quantized latent variables in the sense that they
take discrete values. The encoder E, the decoder G and
the codebook Z can be trained end-to-end via the following
loss function:
LVQVAE = ∥x −˜x∥1+∥sg[E(x)] −zq∥2
+β∥sg[zq] −E(x)∥2
Where, sg[·] stands for the stop-gradient operation. In practice, we replace the second term of Equation 2 with exponential moving averages (EMA) to update the codebook entries which is proven to work better than directly
using the loss function.
4. Vector Quantized Diffusion Model
Given the text-image pairs, we obtain the discrete image
tokens x ∈ZN with a pretrained VQ-VAE, where N = hw
represents the sequence length of tokens. Suppose the size
of the VQ-VAE codebook is K, the image token xi at location i takes the index that speciﬁes the entries in the codebook, i.e., xi ∈{1, 2, ..., K}. On the other hand, the text tokens y ∈ZM can be obtained through BPE-encoding .
The overall text-to-image framework can be viewed as maximizing the conditional transition distribution q(x|y).
Previous autoregressive models, e.g., DALL-E and
CogView , sequentially predict each image token depends on the text tokens as well as the previously predicted
image tokens, i.e., q(x|y) = QN
i=1 q(xi|x1, · · · , xi−1, y).
While achieving remarkable quality in text-to-image synthesis, there exist several limitations of autoregressive modeling. First, image tokens are predicted in a unidirectional
ordering, e.g., raster scan, which neglects the structure of
2D data and restricts the expressivity for image modeling
since the prediction of a speciﬁc location should not merely
attend to the context on the left or the above. Second, there
is a train-test discrepancy as the training employs ground
truth whereas the inference relies on the prediction as previous tokens. The so-called “teacher-forcing” practice 
or exposure bias leads to error accumulation due to the
mistakes in the earlier sampling. Moreover, it requires a
forward pass of the network to predict each token, which
consumes an inordinate amount of time even for the sampling in the latent space of low resolution (i.e., 32 × 32),
making the AR model impractical for real usage.
We aim to model the VQ-VAE latent space in a
non-autoregressive manner.
The proposed VQ-Diffusion
method maximizes the probability q(x|y) with the diffusion model , an emerging approach that produces
compelling quality on image synthesis . While the majority of recent works focus on continuous diffusion models, using them for categorical distribution is much less researched . In this work, we propose to use its conditional variant discrete diffusion process for text-to-image
generation. We will subsequently introduce the discrete diffusion process inspired by the masked language modeling
(MLM) , and then discuss how to train a neural network
to reverse this process.
4.1. Discrete diffusion process
On a high level, the forward diffusion process gradually corrupts the image data x0 via a ﬁxed Markov chain
𝑞𝑞𝑥𝑥𝑡𝑡𝑥𝑥𝑡𝑡−1
Step1: VQ-VAE
Step2: VQ-Diffusion
𝑝𝑝𝜃𝜃𝑥𝑥𝑡𝑡−1|𝑥𝑥𝑡𝑡, 𝑦𝑦
A single giraffe
grassing leave
Transformer
Diffusion Image Decoder
Transformer
Figure 1. Overall framework of our method. It starts with the VQ-
VAE. Then, the VQ-Diffusion models the discrete latent space by
reversing a forward diffusion process that gradually corrupts the
input via a ﬁxed Markov chain.
q(xt|xt−1), e.g., random replace some tokens of xt−1.
After a ﬁxed number of T timesteps, the forward process yields a sequence of increasingly noisy latent variables
z1, ..., zT of the same dimensionality as z0, and zT becomes pure noise tokens. Starting from the noise zT , the
reverse process gradually denoises the latent variables and
restore the real data x0 by sampling from the reverse distribution q(xt−1|xt, x0) sequentially. However, since x0
is unknown in the inference stage, we train a transformer
network to approximate the conditional transit distribution
pθ(xt−1|xt, y) depends on the entire data distribution.
To be more speciﬁc, consider a single image token xi
of x0 at location i, which takes the index that speciﬁes the
entries in the codebook, i.e., xi
0 ∈{1, 2, ..., K}. Without introducing confusion, we omit superscripts i in the following
description. We deﬁne the probabilities that xt−1 transits to
xt using the matrices [Qt]mn = q(xt = m|xt−1 = n) ∈
RK×K. Then the forward Markov diffusion process for the
whole token sequence can be written as,
q(xt|xt−1) = v⊤(xt)Qtv(xt−1)
where v(x) is a one-hot column vector which length is K
and only the entry x is 1. The categorical distribution over
xt is given by the vector Qtv(xt−1).
Importantly, due to the property of Markov chain, one
can marginalize out the intermediate steps and derive the
probability of xt at arbitrary timestep directly from x0 as,
q(xt|x0) = v⊤(xt)Qtv(x0), with Qt = Qt · · · Q1. (4)
Besides, another notable characteristic is that by conditioning on z0, the posterior of this diffusion process is
tractable, i.e.,
q(xt−1|xt, x0) = q(xt|xt−1, x0)q(xt−1|x0)
 v⊤(xt)Qtv(xt−1)
  v⊤(xt−1)Qt−1v(x0)
v⊤(xt)Qtv(x0)
The transition matrix Qt is crucial to the discrete diffusion model and should be carefully designed such that it is
not too difﬁcult for the reverse network to recover the signal
from noises.
Previous works propose to introduce a small
amount of uniform noises to the categorical distribution and
the transition matrix can be formulated as,
with αt ∈ and βt = (1 −αt)/K. Each token has a
probability of (αt + βt) to remain the previous value at the
current step while with a probability of Kβt to be resampled
uniformly over all the K categories.
Nonetheless, the data corruption using uniform diffusion
is a somewhat aggressive process that may pose challenge
for the reverse estimation. First, as opposed to the Gaussian
diffusion process for ordinal data, an image token may be
replaced to an utterly uncorrelated category, which leads to
an abrupt semantic change for that token. Second, the network has to take extra efforts to ﬁgure out the tokens that
have been replaced prior to ﬁxing them. In fact, due to the
semantic conﬂict within the local context, the reverse estimation for different image tokens may form a competition
and run into the dilemma of identifying the reliable tokens.
Mask-and-replace diffusion strategy. To solve the above
issues of uniform diffusion, we draw inspiration from mask
language modeling and propose to corrupt the tokens
by stochastically masking some of them so that the corrupted locations can be explicitly known by the reverse network. Speciﬁcally, we introduce an additional special token, [MASK] token, so each token now has (K + 1) discrete
states. We deﬁne the mask diffusion as follows: each ordinary token has a probability of γt to be replaced by the
[MASK] token and has a chance of Kβt to be uniformly diffused, leaving the probability of αt = 1 −Kβt −γt to
be unchanged, whereas the [MASK] token always keeps its
own state. Hence, we can formulate the transition matrix
Qt ∈R(K+1)×(K+1) as,
Algorithm 1 Training of the VQ-Diffusion, given transition
matrix {Qt}, initial network parameters θ, loss weight λ,
learning rate η.
(I, s) ←sample training image-text pair
x0 ←VQVAE-Encoder(I), y ←BPE(s)
t ∼Uniform({1, · · · , T})
xt ←sample from q(xt|x0)
▷Eqn. 4 and 8
Lt−1 + λLx0,
otherwise ▷Eqn. 9 and 12
θ ←θ −η∇θL
▷Update network parameters
8: until converged
The beneﬁt of this mask-and-replace transition is that:
1) the corrupted tokens are distinguishable to the network,
which eases the reverse process. 2) Comparing to the mask
only approach in , we theoretically prove that it is necessary to include a small amount of uniform noises besides
the token masking, otherwise we get a trivial posterior when
xt ̸= x0. 3) The random token replacement forces the network to understand the context rather than only focusing on
the [MASK] tokens. 4) The cumulative transition matrix Qt
and the probability q(xt|x0) in Equation 4 can be computed
in closed form with:
Qtv(x0) = αtv(x0) + (γt −βt)v(K + 1) + βt
Where αt = Qt
i=1 αi, γt = 1 −Qt
i=1(1 −γi), and
βt = (1 −αt −γt)/K can be calculated and stored in
advance. Thus, the computation cost of q(xt|x0) is reduced
from O(tK2) to O(K). The proof is given in the supplemental material.
4.2. Learning the reverse process
To reverse the diffusion process, we train a denoising
network pθ(xt−1|xt, y) to estimate the posterior transition
distribution q(xt−1|xt, x0). The network is trained to minimize the variational lower bound (VLB) :
Lvlb = L0 + L1 + · · · + LT −1 + LT ,
L0 = −log pθ(x0|x1, y),
Lt−1 = DKL(q(xt−1|xt, x0) || pθ(xt−1|xt, y)),
LT = DKL(q(xT |x0) || p(xT )).
Where p(xT ) is the prior distribution of timestep T. For the
proposed mask-and-replace diffusion, the prior is:
βT , βT , · · · , βT , γT
Note that since the transition matrix Qt is ﬁxed in the training, the LT is a constant number which measures the gap
between the training and inference and can be ignored in
the training.
Algorithm 2 Inference of the VQ-Diffusion, given fast inference time stride ∆t, input text s.
1: t ←T, y ←BPE(s)
2: xt ←sample from p(xT )
3: while t > 0 do
xt ←sample from pθ(xt−∆t|xt, y)
6: end while
7: return VQVAE-Decoder(xt)
Reparameterization trick on discrete stage. The network
parameterization affects the synthesis quality signiﬁcantly.
Instead of directly predicting the posterior q(xt−1|xt, x0),
recent works ﬁnd that approximating some surrogate variables, e.g., the noiseless target data q(x0) gives
better quality. In the discrete setting, we let the network
predict the noiseless token distribution pθ(˜x0|xt, y) at each
reverse step. We can thus compute the reverse transition
distribution according to:
pθ(xt−1|xt, y) =
q(xt−1|xt, ˜x0)pθ(˜x0|xt, y). (11)
Based on the reparameterization trick, we can introduce an
auxiliary denoising objective, which encourages the network to predict noiseless token x0:
Lx0 = −log pθ(x0|xt, y)
We ﬁnd that combining this loss with Lvlb improves the image quality.
Model architecture.
We propose an encoder-decoder
transformer to estimate the distribution pθ(˜x0|xt, y). As
shown in Figure 1, the framework contains two parts: a
text encoder and a diffusion image decoder. Our text encoder takes the text tokens y and yields a conditional feature sequence. The diffusion image decoder takes the image token xt and timestep t and outputs the noiseless token distribution pθ(˜x0|xt, y). The decoder contains several
transformer blocks and a softmax layer. Each transformer
block contains a full attention, a cross attention to combine text information and a feed forward network block.
The current timestep t is injected into the network with
Adaptive Layer Normalization (AdaLN) operator, i.e.,
AdaLN(h, t) = atLayerNorm(h) + bt, where h is the intermediate activations, at and bt are obtained from a linear
projection of the timestep embedding.
Fast inference strategy In the inference stage, by leveraging the reparameterization trick, we can skip some steps in
diffusion model to achieve a faster inference.
Speciﬁcally, assuming the time stride is ∆t, instead
of sampling images in the chain of xT , xT −1, xT −2...x0,
we sample images in the chain of xT , xT −∆t, xT −2∆t...x0
A small gray bird
with white and
dark gray wingbars
and white breast
A small sized blue
bird that has a
short pointed bill
This beautiful little
bird has a white
breast and very
intriguing red eyes
The long wings
spreaded showing
the breast and the
belly of the large bird
An airplane that is
parked at airport
A giraffe is
standing in
a green field
Some children are
playing soccer
on the field
A white and blue
bus driving down a
road next to trees
Figure 2. Comparison with GAN-based method on CUB-200 and MSCOCO datasets.
with the reverse transition distribution:
pθ(xt−∆t|xt, y) =
q(xt−∆t|xt, ˜x0)pθ(˜x0|xt, y).
We found it makes the sampling more efﬁcient which
only causes little harm to quality. The whole training and
inference algorithm is shown in Algorithm 1 and 2.
5. Experiments
In this section, we ﬁrst introduce the overall experiment
setups and then present extensive results to demonstrate the
superiority of our approach in text-to-image synthesis. Finally, we point out that our method is a general image synthesis framework that achieves great performance on other
generation tasks, including unconditional and class conditional image synthesis.
To demonstrate the capability of our proposed method for text-to-image synthesis, we conduct
experiments on CUB-200 , Oxford-102 , and
MSCOCO datasets. The CUB-200 dataset contains
8855 training images and 2933 test images belonging to
200 bird species. Oxford-102 dataset contains 8189 images of ﬂowers of 102 categories. Each image in CUB-
200 and Oxford-102 dataset contains 10 text descriptions.
MSCOCO dataset contains 82k images for training and 40k
images for testing. Each image in this dataset has ﬁve text
descriptions.
To further demonstrate the scalability of our method,
we also train our model on large scale datasets, including
Conceptual Captions and LAION-400M . The
Conceptual Caption dataset, including both CC3M and
CC12M datasets, contains 15M images. To balance the
text and image distribution, we ﬁlter a 7M subset according
to the word frequency. The LAION-400M dataset contains
400M image-text pairs. We train our model on three subsets
from LAION, i.e., cartoon, icon, and human, each of them
contains 0.9M, 1.3M, 42M images, respectively. For each
subset, we ﬁlter the data according to the text.
Traning Details.
Our VQ-VAE’s encoder and decoder
follow the setting of VQGAN which leverages the
GAN loss to get a more realistic image. We directly adopt
the publicly available VQGAN model trained on Open-
Images dataset for all text-to-image synthesis experiments. It converts 256×256 images into 32×32 tokens. The
codebook size K = 2886 after removing useless codes. We
adopt a publicly available tokenizer of the CLIP model 
as text encoder, yielding a conditional sequence of length
77. We ﬁx both image and text encoders in our training.
For fair comparison with previous text-to-image methods under similar parameters, we build two different diffusion image decoder settings: 1) VQ-Diffusion-S (Small),
it contains 18 transformer blocks with dimension of 192.
The model contains 34M parameters. 2) VQ-Diffusion-B
(Base), it contains 19 transformer blocks with dimension of
1024. The model contains 370M parameters.
In order to show the scalability of our method, we also
train our base model on a larger database Conceptual Captions, and then ﬁne-tune it on each database. This model is
denoted as VQ-Diffusion-F.
For the default setting, we set timesteps T = 100 and
loss weight λ = 0.0005. For the transition matrix, we linearly increase γt and βt from 0 to 0.9 and 0.1, respectively.
We optimize our network using AdamW with β1 = 0.9
and β2 = 0.96. The learning rate is set to 0.00045 after
5000 iterations of warmup. More training details are provided in the appendix.
5.1. Comparison with state-of-the-art methods
We qualitatively compare the proposed method with several state-of-the-art methods, including some GAN-based
methods , DALL-E and
CogView , on MSCOCO, CUB-200 and Oxford-102
datasets. We calculate the FID between 30k generated images and 30k real images, and show the results in
We can see that our small model, VQ-Diffusion-S, which
has the similar parameter number with previous GAN-based
models, has strong performance on CUB-200 and Oxford-
102 datasets. Our base model, VQ-Diffusion-B, further improves the performance. And our VQ-Diffusion-F model
achieves the best results and surpasses all previous methods by a large margin, even surpassing DALL-E and
CogView , which have ten times more parameters than
ours, on MSCOCO dataset.
Some visualized comparison results with DM-GAN 
and DF-GAN are shown in Figure 2. Obviously, our
synthesized images have better realistic ﬁne-grained details
and are more consistent with the input text.
5.2. In the wild text-to-image synthesis
To demonstrate the capability of generating in-the-wild
images, we train our model on three subsets from LAION-
400M dataset, e.g., cartoon, icon and human. We provide
our results here in Figure 3. Though our base model is much
smaller than previous works like DALL-E and CogView, we
also achieved a strong performance.
Compared with the AR method which generates images
from top-left to down-right, our method generates images
in a global manner. It makes our method can be applied to
many vision tasks, e.g., irregular mask inpainting. For this
task, we do not need to re-train a new model. We simply
set the tokens in the irregular region as [MASK] token, and
send them to our model. This strategy supports both unconditional mask inpainting and text conditional mask inpainting. We show these results in the appendix.
5.3. Ablations
Number of timesteps.
We investigate the timesteps in
training and inference. As shown in Table 2, we perform
the experiment on the CUB-200 dataset. We ﬁnd when the
training steps increase from 10 to 100, the result improves,
when it further increase to 200, it seems saturated. So we
set the default timesteps number to 100 in our experiments.
To demonstrate the fast inference strategy, we evaluate the
generated images from 10, 25, 50, 100 inference steps on
ﬁve models with different training steps. We ﬁnd it still
maintains a good performance when dropping 3/4 inference
steps, which may save about 3/4 inference times.
Mask-and-replace diffusion strategy. We explore how the
mask-and-replace strategy beneﬁts our performance on the
Oxford-102
StackGAN 
StackGAN++ 
EFF-T2I 
SEGAN 
AttnGAN 
DM-GAN 
DF-GAN 
DAE-GAN 
DALLE 
Cogview 
VQ-Diffusion-S
VQ-Diffusion-B
VQ-Diffusion-F
FID comparison of different text-to-image synthesis
method on MSCOCO, CUB-200, and Oxford-102 datasets.
A handsome man
with thick eyebrows
and moustache
The man with
sunglasses has
a big beard
The sunset on the
beach is wonderful
Snow mountain
and tree reflection
in the lake
Black and white
icon of man
Two smiling
beautiful ladies are
standing together
A cartoon boy
is smiling
A movie poster of
mountain and sea
is driving
on the road
A picture of a
very tall stop sign
A bare kitchen has
wood cabinets and
white appliances
A green heart
with shadow
Figure 3. In the wild text-to-image synthesis results.
Oxford-102 dataset. We set different ﬁnal mask rate (γT )
to investigate the effect. Both mask only strategy (γT = 1)
and replace only strategy (γT = 0) are special cases of our
mask-and-replace strategy. From Figure 4, we ﬁnd it get
the best performance when M = 0.9. When M > 0.9,
it may suffer from the error accumulation problem, when
M < 0.9, the network may be difﬁcult to ﬁnd which region
needs to pay more attention.
Truncation. We also demonstrate that the truncation sampling strategy is extremely important for our discrete diffusion based method. It may avoid the network sampling
from low probability tokens. Speciﬁcally, we only keep top
r tokens of pθ(˜x0|xt, y) in the inference stage. We evaluate the results with different truncation rates r on CUB-200
inference steps
training steps
Table 2. Ablation study on training steps and inference steps. Each
column shares the same training steps while each row shares the
same inference steps.
Figure 4. Ablation study on the mask rate and the truncation rate.
throughput(imgs/s)
VQ-Diffusion-S
VQ-Diffusion-S
VQ-Diffusion-S
VQ-Diffusion-B
VQ-Diffusion-B
VQ-Diffusion-B
Table 3. Comparison between VQ-Diffusion and VQ-AR models. By changing the inference steps, the VQ-Diffusion model is
15 times faster than the VQ-AR model while maintaining better
performance.
dataset. As shown in Figure 4, we ﬁnd that it achieves the
best performance when the truncation rate equals 0.86.
VQ-Diffusion vs VQ-AR. For a fair comparison, we replace our diffusion image decoder with an autoregressive
decoder with the same network structure and keep other settings the same, including both image and text encoders. The
autoregressive model is denoted as VQ-AR-S and VQ-AR-
B, corresponding to VQ-Diffusion-S and VQ-Diffusion-B.
The experiment is performed on the CUB-200 dataset. As
shown in Table 3 , on both -S and -B settings the VQ-
Diffusion model surpasses the VQ-AR model by a large
margin. Meanwhile, we evaluate the throughput of both
methods on a V100 GPU with a batch size of 32. The VQ-
Diffusion with the fast inference strategy is 15 times faster
than the VQ-AR model with a better FID score.
5.4. Uniﬁed generation model
Our method is general, which can also be applied to
other image synthesis tasks, e.g., unconditional image syn-
StyleGAN2 
BigGAN 
BigGAN-deep 
IDDPM 
ADM-G 
VQGAN 
ImageBART 
ADM-G (1.0guid) 
VQGAN (acc0.05) 
ImageBART (acc0.05) 
Ours (acc0.05)
Table 4. FID score comparison for class-conditional synthesis on
ImageNet, and unconditional synthesis on FFHQ dataset. ’guid’
denotes using classiﬁer guidance , ’acc’ denotes adopting acceptance rate .
thesis and image synthesis conditioned on labels. To generate images from a given class label, we ﬁrst remove the
text encoder network and cross attention part in transformer
blocks, and inject the class label through the AdaLN operator. Our network contains 24 transformer blocks with dimension 512. We train our model on the ImageNet dataset.
For VQ-VAE, we adopt the publicly available model from
VQ-GAN trained on ImageNet dataset, which downsamples images from 256 × 256 to 16 × 16.
For unconditional image synthesis, we trained our model on the
FFHQ256 dataset, which contains 70k high quality face
images. The image encoder also downsamples images to
16 × 16 tokens.
We assess the performance of our model in terms of FID
and compare with a variety of previously established models . For a fair comparison, we calculate
FID between 50k generated images and all real images. Following we can further increase the quality by only accepting images with a top 5% classiﬁcation score, denoted
as acc0.05. We show the quantitative results in Table 4.
While some task-specialized GAN models report better FID
scores, our approach provides a uniﬁed model that works
well across a wide range of tasks.
6. Conclusion
In this paper, we present a novel text-to-image architecture named VQ-Diffusion. The core design is to model the
VQ-VAE latent space in a non-autoregressive manner. The
proposed mask-and-replace diffusion strategy avoids the accumulation of errors of the AR model. Our model has the
capacity to generate more complex scenes, which surpasses
previous GAN-based text-to-image methods. Our method
is also general and produces strong results on unconditional
and conditional image generation.
Acknowledgement
We thank Qiankun Liu from University of Science and
Technology of China for his help, he provided the initial
code and datasets.