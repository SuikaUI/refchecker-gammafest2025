Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785–794
Copenhagen, Denmark, September 7–11, 2017. c⃝2017 Association for Computational Linguistics
RACE: Large-scale ReAding Comprehension Dataset From Examinations
Guokun Lai⇤and Qizhe Xie⇤and Hanxiao Liu and Yiming Yang and Eduard Hovy
{guokun, qzxie, hanxiaol, yiming, hovy}@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
We present RACE, a new dataset for
benchmark evaluation of methods in the
reading comprehension task.
from the English exams for middle and
high school Chinese students in the age
range between 12 to 18, RACE consists of near 28,000 passages and near
100,000 questions generated by human
experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students’
ability in understanding and reasoning.
In particular, the proportion of questions
that requires reasoning is much larger
in RACE than that in other benchmark
datasets for reading comprehension, and
there is a signiﬁcant gap between the
performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset
can serve as a valuable resource for research and evaluation in machine comprehension.
The dataset is freely available at 
˜glai1/data/race/ and the code is
available at 
qizhex/RACE_AR_baselines
Introduction
Constructing an intelligence agent capable of understanding text as people is the major challenge
of NLP research. With recent advances in deep
learning techniques, it seems possible to achieve
human-level performance in certain language understanding tasks, and a surge of effort has been
devoted to the machine comprehension task where
people aim to construct a system with the ability to
⇤* indicates equal contribution
answer questions related to a document that it has
to comprehend .
Towards this goal, several large-scale datasets
 have been proposed, which allow researchers to train deep learning systems and obtain results comparable to the human performance.
While having a suitable dataset is crucial for evaluating the system’s true ability in reading comprehension, the existing datasets suffer several critical
limitations. Firstly, in all datasets, the candidate
options are directly extracted from the context (as
a single entity or a text span), which leads to the
fact that lots of questions can be solved trivially
via word-based search and context-matching without deeper reasoning; this constrains the types of
questions as well. Secondly, answers and questions of most datasets are either crowd-sourced
or automatically-generated, bringing a signiﬁcant
amount of noises in the datasets and limits the ceiling performance by domain experts, such as 82%
for Childrens Book Test and 84% for Who-did-
What. Yet another issue in existing datasets is that
the topic coverages are often biased due to the speciﬁc ways that the data were initially collected,
making it hard to evaluate the ability of systems in
text comprehension over a broader range of topics.
To address the aforementioned limitations, we
constructed a new dataset by collecting a large
set of questions, answers and associated passages in the English exams for middle-school and
high-school Chinese students within the 12–18
age range.
Those exams were designed by domain experts (instructors) for evaluating the reading comprehension ability of students, with ensured quality and broad topic coverage.
Furthermore, the answers by machines or by humans can be objectively graded for evaluation
and comparison using the same evaluation metrics. Although efforts have been made with a similar motivation, including the MCTest dataset created by and several others
 , the usefulness
of those datasets is signiﬁcantly restricted due to
their small sizes, especially not suitable for training powerful deep neural networks whose success
relies on the availability of relatively large training
Our new dataset, namely RACE, consists of
27,933 passages and 97,687 questions. After reading each passage, each student is asked to answer
several questions where each question is provided
with four candidate answers – only one of them is
correct . Unlike existing datasets, both the questions and candidate answers in RACE are not restricted to be the text spans in the original passage;
instead, they can be described in any words. A
sample from our dataset is presented in Table 1.
Our latter analysis shows that correctly answering a large portion of questions in RACE requires
the ability of reasoning, the most important feature as a machine comprehension dataset . RACE also offers two important subdivisions of the reasoning types in its questions,
namely passage summarization and attitude analysis, which have not been introduced by the any of
the existing large-scale datasets to our knowledge.
In addition, compared to other existing datasets
where passages are either domain-speciﬁc or of a
single ﬁxed style (namely news stories for CNN/-
Daily Mail, NEWSQA and Who-did-What, ﬁction
stories for Children’s Book Test and Book Test,
and Wikipedia articles for SQUAD), passages in
RACE almost cover all types of human articles,
such as news, stories, ads, biography, philosophy,
etc., in a variety of styles. This comprehensiveness
of topic/style coverage makes RACE a desirable
resource for evaluating the reading comprehension
ability of machine learning systems in general.
The advantages of our proposed dataset over existing large datasets in machine reading comprehension can be summarized as follows:
• All questions and candidate options are generated by human experts, which are intentionally designed to test human agent’s ability in
reading comprehension. This makes RACE a
relatively accurate indicator for reﬂecting the
text comprehension ability of machine learning systems under human judge.
• The questions are substantially more difﬁcult
than those in existing datasets, in terms of the
large portion of questions involving reasoning. At the meantime, it is also sufﬁciently
large to support the training of deep learning
• Unlike existing large-scale datasets, candidate options in RACE are human generated
sentences which may not appear in the original passage. This makes the task more challenging and allows a rich type of questions
such as passage summarization and attitude
• Broad coverage in various domains and writing styles: a desirable property for evaluating
generic (in contrast to domain/style-speciﬁc)
comprehension ability of learning models.
Related Work
In this section, we brieﬂy outline existing datasets
for the machine reading comprehension task, including their strengths and weaknesses.
MCTest is a popular
dataset for question answering in the same format as RACE, where each question is associated
with four candidate answers with a single correct answer. Although questions in MCTest are
of high-quality ensured by careful examinations
through crowdsourcing, it contains only 500 stores
and 2000 questions, which substantially restricts
its usage in training advanced machine comprehension models. Moreover, while MCTest is designed for 7 years old children, RACE is constructed for middle and high school students at
12–18 years old hence is more complicated and
requires stronger reasoning skills. In other words,
RACE can be viewed as a larger and more difﬁcult
version of the MCTest dataset.
Cloze-style datasets
The past few years have witnessed several largescale cloze-style datasets , whose questions are formulated by obliterating a word or an entity in a sentence.
In a small village in England about 150 years ago, a mail coach was standing on the street. It didn’t come to that village often.
People had to pay a lot to get a letter. The person who sent the letter didn’t have to pay the postage, while the receiver had to.
“Here’s a letter for Miss Alice Brown,” said the mailman.
“ I’m Alice Brown,” a girl of about 18 said in a low voice.
Alice looked at the envelope for a minute, and then handed it back to the mailman.
“I’m sorry I can’t take it, I don’t have enough money to pay it”, she said.
A gentleman standing around were very sorry for her. Then he came up and paid the postage for her.
When the gentleman gave the letter to her, she said with a smile, “ Thank you very much, This letter is from Tom. I’m going
to marry him. He went to London to look for work. I’ve waited a long time for this letter, but now I don’t need it, there is
nothing in it.”
“Really? How do you know that?” the gentleman said in surprise.
“He told me that he would put some signs on the envelope. Look, sir, this cross in the corner means that he is well and this
circle means he has found work. That’s good news.”
The gentleman was Sir Rowland Hill. He didn’t forgot Alice and her letter.
“The postage to be paid by the receiver has to be changed,” he said to himself and had a good plan.
“The postage has to be much lower, what about a penny? And the person who sends the letter pays the postage. He has to buy
a stamp and put it on the envelope.” he said . The government accepted his plan. Then the ﬁrst stamp was put out in 1840. It
was called the “Penny Black”. It had a picture of the Queen on it.
Questions:
1): The ﬁrst postage stamp was made .
A. in England B. in America C. by Alice D. in 1910
2): The girl handed the letter back to the mailman because
A. she didn’t know whose letter it was
B. she had no money to pay the postage
C. she received the letter but she didn’t want to open it
D. she had already known what was written in the letter
3): We can know from Alice’s words that
A. Tom had told her what the signs meant before leaving
B. Alice was clever and could guess the meaning of the signs
C. Alice had put the signs on the envelope herself
D. Tom had put the signs as Alice had told him to
4): The idea of using stamps was thought of by
A. the government
B. Sir Rowland Hill
C. Alice Brown
5): From the passage we know the high postage made
A. people never send each other letters
B. lovers almost lose every touch with each other
C. people try their best to avoid paying it
D. receivers refuse to pay the coming letters
Answer: ADABC
Table 1: Sample reading comprehension problems from our dataset.
CNN/Daily Mail are
the largest machine comprehension datasets with
1.4M questions.
However, both require limited
reasoning ability . In fact, the
best machine performance obtained by researchers
 is close to
human’s performance on CNN/Daily Mail.
Childrens Book Test (CBT) 
and Book Test (BT) are constructed in a similar manner. Each passage in CBT
consist of 20 contiguous sentences extracted from
children’s books and the next (21st) sentence is
used to make the question. The main difference
between the two datasets is the size of BT being
60 times larger. Machine comprehension models
have also matched human performance on CBT
 .
Who Did What (WDW) 
is yet another cloze-style dataset constructed from
the LDC English Gigaword newswire corpus. The
authors generate passages and questions by picking two news articles describing the same event,
using one as the passage and the other as the question.
High noise is inevitable in cloze-style datasets
due to their automatic generation process, which
is reﬂected in the human performance on these
datasets: 82% for CBT and 84% for WDW.
Datasets with Span-based Answers
In datasets such as SQUAD , NEWSQA and MS
MARCO , the answer to each
question is in the form of a text span in the article.
Articles of SQUAD, NEWSQA and MS MARCO
come from Wikipedia, CNN news and the Bing
search engine respectively. The answer to a certain question may not be unique and could be multiple spans. Instead of evaluating the accuracy, researchers need to use F1 score, BLEU or ROUGE 
as metrics, which measure the overlap between
the prediction and ground truth answers since the
questions come without candidate spans.
Datasets with span-based answers are challenging as the space of possible spans is usually large.
However, restricting answers to be text spans in
the context passage may be unrealistic and more
importantly, may not be intuitive even for humans,
indicated by the suffered human performance of
80.3% on SQUAD ) and 46.5% on NEWSQA. In other
words, the format of span-based answers may not
necessarily be a good examination of reading comprehension of machines whose aim is to approach
the comprehension ability of humans.
Datasets from Examinations
There have been several datasets extracted from
examinations, aiming at evaluating systems under the same conditions as how humans are evaluated in schools. E.g., the AI2 Elementary School
Science Questions dataset 
contains 1080 questions for students in elementary
schools; NTCIR QA Lab 
evaluates systems by the task of solving real-world
university entrance exam questions; The Entrance
Exams task at CLEF QA Track evaluates the system’s reading comprehension ability.
However, data provided in these existing tasks are far from sufﬁcient
for the training of advanced data-driven machine
reading models, partially due to the expensive data
generation process by human experts.
To the best of our knowledge, RACE is the ﬁrst
large-scale dataset of this type, where questions
are created based on exams designed to evaluate
human performance in reading comprehension.
Data Analysis
In this section, we study the nature of questions
covered in RACE at a detailed level. Speciﬁcally,
we present the dataset statistics in Section 3.1, and
then analyze different reasoning/question types in
RACE in the remaining subsections.
Dataset Statistics
As mentioned in section 1, RACE is collected
from English examinations designed for 12–15
year-old middle school students, and 15–18 yearold high school students in China.
To distinguish the two subgroups with drastic difﬁculty
gap, RACE-M denotes the middle school examinations and RACE-H denotes high school examinations. We split 5% data as the development set
and 5% as the test set for RACE-M and RACE-H
respectively. The number of samples in each set is
shown in Table 2. The statistics for RACE-M and
RACE-H is summarized in Table 3. We can ﬁnd
that the length of the passages and the vocabulary
size in the RACE-H are much larger than that of
the RACE-M, an evidence of the higher difﬁculty
of high school examinations.
However, notice that since the articles and questions are selected and designed to test Chinese
students learning English as a foreign language,
the vocabulary size and the complexity of the language constructs are simpler than news articles
and Wikipedia articles in other QA datasets.
Reasoning Types of the Questions
To get a comprehensive picture about the reasoning difﬁculty requirement of RACE, we conduct
human annotations of questions types. Following
Chen et al. ; Trischler et al. , we stratify the questions into ﬁve classes as follows with
ascending order of difﬁculty:
matches a span in the article. The answer is
self-evident.
• Paraphrasing:
The question is entailed or
paraphrased by exactly one sentence in the
passage. The answer can be extracted within
the sentence.
• Single-sentence reasoning: The answer could
be inferred from a single sentence of the article by recognizing incomplete information or
conceptual overlap.
• Multi-sentence reasoning: The answer must
be inferred from synthesizing information
distributed across multiple sentences.
• Insufﬁcient/Ambiguous: The question has no
answer or the answer is not unique based on
the given passage.
We refer readers to for examples of each category.
To obtain the proportion of different question
types, we sample 100 passages from RACE (50
from RACE-M and 50 from RACE-H), all of
which have 5 questions hence there are 500 questions in total. We put the passages on Amazon Mechanical Turk1, and a Hit is generated by a passage
1 
# passages
# questions
Table 2: The separation of the training, development and test sets of RACE-M,RACE-H and RACE
Passage Len
Question Len
Option Len
Vocab size
Table 3: Statistics of RACE where Len denotes
length and Vocab denotes Vocabulary.
with 5 questions. Each question is labeled by two
crowdworkers. We require the turkers to both answer the questions and label the reasoning type.
We pay $0.70 and $1.00 per passage in RACE-M
and RACE-H respectively, and restrict the access
to master turkers only. Finally, we get 1000 labels
for the 500 questions.
The statistics about the reasoning type is summarized in Table 4.
The higher difﬁculty level
of RACE is justiﬁed by its higher ratio of reasoning questions in comparison to CNN, SQUAD
and NEWSQA. Speciﬁcally, 59.2% questions of
RACE are either in the category of single-sentence
reasoning or in the category of multi-sentence
reasoning, while the ratio is 21%, 20.5% and
33.9% for CNN, SQUAD and NEWSQA respectively. Also notice that the ratio of word matching questions on RACE is only 15.8%, the lowest
among several categories. In addition, questions
in RACE-H are more complex than questions in
RACE-M since RACE-M has more word matching questions and fewer reasoning questions.
Subdividing Reasoning Types
To better understand our dataset and facilitate future research, we list the subdivisions of questions under the reasoning category. We ﬁnd the
most frequent reasoning subdivisions include: detail reasoning, whole-picture understanding, passage summarization, attitude analysis and world
knowledge. One question may fall into multiple
divisions.
Deﬁnition of these subdivisions and
their associated examples are as follows:
1. Detail reasoning: to answer the question, the
agent should be clear about the details of the passage. The answer appears in the passage but it cannot be found by simply matching the question with
the passage. For example, Question 1 in the sample passage falls into this category.
2. Whole-picture reasoning: the agent needs to
understand the whole picture of the story to obtain the correct answer. For example, to answer
the Question 2 in the sample passage, the agent is
required to comprehend the entire story.
Passage summarization: The question requires the agent to select the best summarization
of the passage among four candidate summarizations. A typical question of this type is “The main
idea of this passage is
.”. An example question
can be found in Appendix A.1.
4. Attitude analysis: The question asks about
the opinions/attitudes of the author or a character
in the story towards somebody or something, e.g.,
• Evidence:
“. . . Many
optimistically
thought industry awards for better equipment
production
appliances. It was even suggested that noise from
building sites could be alleviated . . . ”
• Question: What was the author’s attitude towards
the industry awards for quieter?
• Options:
A.suspicious
B.positive
C.enthusiastic D.indifferent
5. World knowledge: Certain external knowledge is needed. Most frequent questions under this
category involve simple arithmetic.
• Evidence: “The park is open from 8 am to 5 pm.”
• Question: The park is open for
hours a day.
• Options: A.eight
To the best of our knowledge, questions like
passage summarization and attitude analysis have
not been introduced by any of the existing largescale machine comprehension datasets. Both are
crucial components in evaluating humans’ reading
comprehension abilities.
Word Matching
Paraphrasing
Single-Sentence Reasoning
Multi-Sentence Reasoning
Ambiguous/Insufﬁcient
Table 4: Statistic information about Reasoning type in different datasets. * denotes the numbers coming
from based on 1000 samples per dataset, and numbers with † come from .
Collection Methodology
We collected the raw data from three large free
public websites234 in China5, where the reading
comprehension problems are extracted from English examinations designed by teachers in China.
The data before cleaning contains 137,918 passages and 519,878 questions in total, where there
are 38,159 passages with 156,782 questions in the
middle school group, and 99,759 passages with
363,096 questions in the high school group.
The following ﬁltering steps are conducted to
clean the raw data. Firstly, we remove all problems and questions that do not have the same format as our problem setting, e.g., a question would
be removed if the number of its options is not four.
Secondly, we ﬁlter all articles and questions that
are not self-contained based on the text information, i.e. we remove the articles and questions containing images or tables. We also remove all questions containing keywords “underlined” or “paragraph”, since it is difﬁcult to reproduce the effect
of underlines and the paragraph segment information. Thirdly, we remove all duplicated articles.
On one of the websites (xkw.com), the answers
are stored as images. We used two standard OCR
programs tesseract 6 and ABBYY FineReader 7 to
process the images. We remove all the answers
that two software disagree. The OCR task is easy
since we only need to recognize printed alphabet
A, B, C, D with a standard font. Finally, we get
the cleaned dataset RACE, with 27,933 passages
and 97,687 questions.
2 
3 
4 
5We checked that our dataset does not include example questions of exams with copyright, such as SSAT, SAT,
TOEFL and GRE.
6 
7 
Experiments
In this section, we compare the performance
of several state-of-the-art reading comprehension
models with human performance. We use accuracy as the metric to evaluate different models.
Methods for Comparison
Sliding Window Algorithm
Firstly, we build
the rule-based baseline introduced by Richardson
et al. . It chooses the answer having the
highest matching score. Speciﬁcally, it ﬁrst concatenates the question and the answer and then calculates the TF-IDF style matching score between
the concatenated sentence with every window (a
span of text) of the article. The window size is
decided by the model performance in the training
and dev sets.
Stanford Attentive Reader
Stanford Attentive
Reader (Stanford AR) is a
strong model that achieves state-of-the-art results
on CNN/Daily Mail. Moreover, the authors claim
that their model has nearly reached the ceiling performance on these two datasets.
Suppose that the triple of passage, question and
options is denoted by (p, q, o1,··· ,4). We ﬁrst employ bidirectional GRUs to encode p and q respectively into hp
2, . . . , hp
n and hq. Then we summarize the most relevant part of the passage into
sp with an attention model. Following Chen et al.
 , we adopt a bilinear attention form. Specifically,
↵i = Softmaxi((hp
i )T W1hq)
Similarly, we use bidirectional GRUs to encode
option oi into a vector hoi.
Finally, we compute the matching score between the i-th option
(i = 1, · · · , 4) and the summarized passage using
RACE-M RACE-H RACE MCTest CNN DM CBT-N CBT-C WDW
Sliding Window
Stanford AR
73.6† 76.6†
77.9† 80.9† 70.1†
Ceiling Performance
Table 5: Accuracy of models and human on the each dataset, where † denotes the results coming from
previous publications. DM denotes Daily Mail and WDW denotes Who-Did-What .
Word-Match
Paraphrasing
Single-Reason
Multi-Reason
Sliding Window
Stanford AR
(a) RACE-M
Word-Match
Paraphrasing
Single-Reason
Multi-Reason
Sliding Window
Stanford AR
(b) RACE-H
Figure 1: Test accuracy of different baselines on each reasoning type category introduced in Section 3.2,
where Word-Match, Single-Reason, Multi-Reason and Ambiguous are the abbreviations for Word matching, Single-sentence Reasoning, Multi-sentence Reasoning and Insufﬁcient/Ambiguous respectively.
a bilinear attention. We pass the scores through
softmax to get a probability distribution. Specifically, the probability of option i being the right
answer is calculated as
pi = Softmaxi(hoiW2sd)
Gated-Attention Reader
Gated AR is the state-of-the-art model on multiple datasets. To build query-speciﬁc representations of tokens in the document, it employs an
attention mechanism to model multiplicative interactions between the query embedding and the
document representation.
With a multi-hop architecture, GA also enables a model to scan the
document and the question iteratively for multiple passes. In other words, the multi-hop structure makes it possible for the reader to reﬁne token
representations iteratively and the attention mechanism ﬁnd the most relevant part of the document.
We refer readers to for more
After obtaining a query speciﬁc document representation sd, we use the same method as bilinear
operation listed in Equation 2 to get the output.
Note that our implementation slightly differs
from the original GA reader. Speciﬁcally, the Attention Sum layer is not applied at the ﬁnal layer
and no character-level embeddings are used.
Implementation Details
We follow Chen et al.
 in our experiment settings. The vocabulary
size is set to 50k. We choose word embedding
size d = 100 and use the 100-dimensional Glove
word embedding as embedding initialization. GRU weights are initialized from Gaussian distribution N(0, 0.1). Other
parameters are initialized from a uniform distribution on (−0.01, 0.01). The hidden dimensionality is set to 128 and the number of layers is
set to one for both Stanford AR and GA. We use
vanilla stochastic gradient descent (SGD) to train
our models. We apply dropout on word embeddings and the gradient is clipped when the norm
of the gradient is larger than 10. We use a grid
search on validation set to choose the learning
rate within {0.05, 0.1, 0.3, 0.5} and dropout rate
within {0.2, 0.5, 0.7}.
The highest accuracy on
validation set is obtained by setting learning rate to
0.1 for Stanford AR and 0.3 for GA and dropout
rate to 0.5. The data of RACE-M and RACE-H
is used together to train our model and testing is
performed separately.
Human Evaluation
As described in section 3.2, a randomly sampled subset of test set has been labeled by Amazon Turkers, which contains 500 questions with
half from RACE-H and with the other half from
RACE-M. The turkers’ performance is 85% for
RACE-M and 70% for RACE-H. However, it is
hard to guarantee that every turker performs the
survey carefully, given the difﬁcult and long passages of high school problems. Therefore, to obtain the ceiling human performance on RACE,
we manually labeled the proportion of valid questions. A question is valid if it is unambiguous and
has a correct answer. We found that 94.5% of the
data is valid, which sets the ceiling human performance. Similarly, the ceiling performance on
RACE-M and RACE-H is 95.4% and 94.2% respectively.
Main Results
We compare models’ and human ceiling performance on datasets which have the same evaluation metric with RACE. The compared datasets
include RACE, MCTest, CNN/Daily Mail (CNN
and DM), CBT and WDW. On CBT, we report performance on two subsets where the missing token
is either a common noun (CBT-C) or name entity
(CBT-N) since the language models have already
reached human-level performance on other types
 . The comparison is shown in
Performance of Sliding Window
We ﬁrst compare MCTest with RACE using Sliding Window,
where it is unable to train Stanford AR and Gated
AR on MCTest’s limited training data.
Sliding Window achieves an accuracy of 51.5% on
MCTest while only 37.3% on RACE, meaning that
to answer the questions of RACE requires more
reasoning than MCTest.
The performance of sliding window on RACE
is not directly comparable with CBT and WDW
since CBT has ten candidate answers for each
question and WDW has an average of three. Instead, we evaluate the performance improvement
of sliding window on the random baseline. Larger
improvement indicates more questions solvable by
simple matching. On RACE, Sliding Window is
28.6% better than the random baseline, while the
improvement is 58.5%, 92.2% and 50% for CBT-
N, CBT-C and WDW.
The accuracy on RACE-M (37.3%) and RACE-
H (30.4%) indicates that the middle school questions are simpler based on the matching algorithm.
Performance of Neural Models
We further
compare the difﬁculty of different datasets by
state-of-the-art neural models’ performance.
lower performance means that more problems are
unsolvable by machines. The Stanford AR and
Gated AR achieve an accuracy of only 43.3% and
44.1% on RACE while their accuracy is much
CNN/Daily Mail,
Childrens Book
Test and Who-Did-What. It justiﬁes the fact that,
among current large-scale machine comprehension datasets, RACE is the most challenging one.
Human Ceiling Performance
The human performance is 94.5% which shows our data is quite
clean compared to other large-scale machine comprehension datasets. Since we cannot enforce every turker do the test cautiously, the result shows
a gap between turkers’ performance and human
performance. Reasonably, problems in the high
school group with longer passages and more complex questions lead to more signiﬁcant divergence.
Nevertheless, the start-of-the-art models still have
a large room to be improved to reach turkers’ performance. The performance gap is 41% for the
middle school problems and 25% for the high
school problems. What’s more, The performance
of Stanford AR and GA is only less than a half
of the ceiling human performance, which indicates
that to match the humans’ reading comprehension
ability, we still have a long way to go.
Reason Types Analysis
We evaluate human and models on different types
of questions, shown in Figure 1. Turkers do the
best on word matching problems while doing the
worst on reasoning problems.
Sliding window
performs better on word matching than problems
needing reasoning or paraphrasing. Surprisingly,
Stanford AR does not have a stronger performance
on the word matching category than reasoning categories. A possible reason is that the proportion
of data in reasoning categories is larger than that
Also, the candidate answers of simple
matching questions may share similar word embeddings. For example, if the question is about
color, it is difﬁcult to distinguish candidate answers, “green”, “red”, “blue” and “yellow”, in the
embedding vector space. The similar performance
on different categories also explains the reason
that the performance of the neural models is close
in the middle and high school groups in Table 5.
Conclusion
We introduce a large, high-quality dataset for reading comprehension that is carefully designed to
examine human ability on this task. Some desirable properties of RACE include the broad coverage of domains/styles and the richness in the question format. Most importantly, it requires substantially more reasoning to do well on RACE than
on other datasets, as there is a signiﬁcant gap between the performance of state-of-the-art machine
comprehension models and that of the human. We
hope this dataset will stimulate the development of
more advanced machine comprehension models.
Acknowledgement
We would like to thank Graham Neubig for suggestions on the draft and Diyi Yang’s help on obtaining the crowdsourced labels.
This research was supported in part by DARPA
grant FA8750-12-2-0342 funded under the DEFT