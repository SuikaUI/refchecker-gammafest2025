SUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021
SpectralFormer: Rethinking Hyperspectral Image
Classiﬁcation with Transformers
Danfeng Hong, Senior Member, IEEE, Zhu Han, Student Member, IEEE, Jing Yao, Lianru Gao, Senior
Member, IEEE, Bing Zhang, Fellow, IEEE, Antonio Plaza, Fellow, IEEE, and Jocelyn Chanussot, Fellow, IEEE
Abstract—Hyperspectral (HS) images are characterized by
approximately contiguous spectral information, enabling the ﬁne
identiﬁcation of materials by capturing subtle spectral discrepancies. Owing to their excellent locally contextual modeling ability,
convolutional neural networks (CNNs) have been proven to be a
powerful feature extractor in HS image classiﬁcation. However,
CNNs fail to mine and represent the sequence attributes of
spectral signatures well due to the limitations of their inherent
network backbone. To solve this issue, we rethink HS image
classiﬁcation from a sequential perspective with transformers,
and propose a novel backbone network called SpectralFormer.
Beyond band-wise representations in classic transformers, SpectralFormer is capable of learning spectrally local sequence
information from neighboring bands of HS images, yielding
group-wise spectral embeddings. More signiﬁcantly, to reduce
the possibility of losing valuable information in the layer-wise
propagation process, we devise a cross-layer skip connection to
convey memory-like components from shallow to deep layers
by adaptively learning to fuse “soft” residuals across layers. It
is worth noting that the proposed SpectralFormer is a highly
ﬂexible backbone network, which can be applicable to both
pixel- and patch-wise inputs. We evaluate the classiﬁcation performance of the proposed SpectralFormer on three HS datasets by
conducting extensive experiments, showing the superiority over
classic transformers and achieving a signiﬁcant improvement in
comparison with state-of-the-art backbone networks. The codes
of this work will be available at 
IEEE TGRS SpectralFormer for the sake of reproducibility.
Index Terms—Hyperspectral image classiﬁcation, convolutional neural networks, deep learning, local contextual information, remote sensing, sequence data, skip fusion, transformer.
I. INTRODUCTION
N hyperspectral (HS) imaging, hundreds of (narrow) wavelength bands are collected at each pixel across the complete
This work was supported in part by the National Natural Science Foundation of China under Grant 42030111 and Grant 41722108, and by the
MIAI@Grenoble Alpes (ANR-19-P3IA-0003) and the AXA Research Fund.
(Corresponding author: Lianru Gao)
D. Hong, J. Yao and L. Gao are with the Key Laboratory of Digital
Earth Science, Aerospace Information Research Institute, Chinese Academy
of Sciences, 100094 Beijing, China. (e-mail: ; ; )
Z. Han and B. Zhang are with the Key Laboratory of Digital Earth
Science, Aerospace Information Research Institute, Chinese Academy of
Sciences, Beijing 100094, China, and also with the College of Resources and
Environment, University of Chinese Academy of Sciences, Beijing 100049,
China. (e-mail: ; )
A. Plaza is with the Hyperspectral Computing Laboratory, Department
of Technology of Computers and Communications, Escuela Polit´ecnica,
University of Extremadura, 10003 C´aceres, Spain. (e-mail: ).
J. Chanussot is with the Univ. Grenoble Alpes, INRIA, CNRS, Grenoble
INP, LJK, F-38000 Grenoble, France, and also with the Aerospace Information
Research Institute, Chinese Academy of Sciences, Beijing 100094, China. (email: )
electromagnetic spectrum, which enables the identiﬁcation or
detection of materials at a ﬁne-grained level, particularly for
those that have extremely similar spectral signatures in visual
cues (e.g., RGB) . This provides a great potential in a
variety of high-level Earth observation (EO) missions, such
as accurate land cover mapping, precision agriculture, target
/ object detection, urban planning, tree species classiﬁcation,
mineral exploration, and so on.
A general sequential process in a HS image classiﬁcation system consists of image restoration (e.g., denoising,
missing data recovery) – , dimensionality reduction ,
 , spectral unmixing – , and feature extraction –
 . Among them, feature extraction is a crucial step in HS
image classiﬁcation, which has received increasing attention
by researchers. Over the past decade, a large number of
advanced hand-crafted and subspace learning-based feature
extraction approaches have been proposed for HS image
classiﬁcation . These methods are capable of performing
well in small-sample classiﬁcation problems. However, they
tend to meet the performance bottleneck, when the training
size gradually increases and the training set becomes more
complex. The possible reason is due to the limited data ﬁtting
and representation ability of these traditional methods. Inspired
by the great success of deep learning (DL), that is capable
of ﬁnding out connotative, intrinsic, and potentially valuable
knowledge from the vast amounts of pluralistic data ,
enormous efforts have been made in designing and adding
advanced modules in networks to extract more diagnostic
features from remote sensing data. For example, Zhao et al.
 developed a joint classiﬁcation framework using HS and
light detection and ranging (LiDAR) data, which has been
shown to be excellent at extracting features from multisource
RS data. Zhang et al. designed an extraordinary patch-topatch convolutional neural network (CNN), which has yielded
signiﬁcantly better results than other techniques.
In recent years, many well-recognized backbone networks
have been widely and successfully applied in the HS image
classiﬁcation task , such as autoencoders (AEs), CNNs,
recurrent neural networks (RNNs), generative adversarial networks (GANs), capsule networks (CapsNet), graph convolutional networks (GCNs). Chen et al. stacked multiple
autoencoder networks to extract deep feature representations
from dimension-reduced HS images – generated by principal
component analysis (PCA) – with the application to HS
image classiﬁcation. Chen et al. employed CNNs instead
of stacked AEs to semantically extract spatial-spectral features
by considering local contextual information of HS images,
 
SUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021
Reconstructed Pixel
Convolution
Batch Normalization
Activation Function
Autoencoders
Graph Data
Graph Embedding
Real Spectral Signatures
Generator G
Fake Spectral Signatures
Input Noise
Discriminator D
Primary Capsule
Digit Capsule
Multi-Head
Positional
Add & Norm
Feed Forward
Add & Norm
Positional
Multi-Head
Add & Norm
Feed Forward
Multi-Head
Add & Norm
Add & Norm
Probabilities
Transformers
Fig. 1. An overview of currently well-recognized backbone networks for the HS image classiﬁcation task, such as autoencoders , CNNs , RNNs ,
GANs , CapsNet , GCNs , and Transformers .
achieving higher classiﬁcation accuracies. Hang et al. 
designed a cascaded RNN for HS image classiﬁcation by
taking advantage of RNNs that can model the sequentiality
to represent the relations of neighboring spectral bands effectively. In , GANs were improved, making them applicable
to the HS image classiﬁcation task, with the input of three PCA
components and random noises. Paoletti et al. extended a
CNN-based model by deﬁning a novel spatial-spectral capsule
unit, yielding a high-performance classiﬁcation framework
of HS images and meanwhile reducing the complexity of
the network design. Hong et al. made a comprehensive
comparison between CNNs and GCNs on the classiﬁcation of
HS images both qualitatively and quantitatively, and proposed
a mini-batch GCN (miniGCNs), providing a feasible solution
for addressing the issue of large graphs in GCNs, for state-ofthe-art HS image classiﬁcation.
Although these backbone networks and their variants have
been able to obtain promising classiﬁcation results, their ability
in characterizing spectral series information (particularly in
capturing subtle spectral discrepancies along spectral dimension) remains inadequate. Fig. 1 gives an overview illustration
of these state-of-the-art backbone networks in the HS image
classiﬁcation task. The speciﬁc limitations can be roughly
summarized as follows.
• As a mainstream backbone architecture, CNNs have
shown their powerful ability in extracting spatially structural information and locally contextual information from
HS images. Nevertheless, on the one hand, CNNs can
hardly be capable of capturing the sequence attributes
well, particularly middle- and long-term dependencies.
This unavoidably meets the performance bottleneck in the
HS image classiﬁcation task, especially when the to-beclassiﬁed categories are of a great variety and extremely
similar in spectral signatures. On the other hand, CNNs
are overly concerned with spatial content information,
which spectrally distorts sequential information in the
learned features. This, to great extent, enables more
difﬁculties of mining diagnostic spectral attributes.
• Unlike CNNs, RNNs are designed for sequence data,
which accumulatively learns spectral features band-byband from HS images in an orderly fashion. Such a
mode extremely depends on the order of spectral bands
and tends to generate gradient vanishing, thereby being
hard to learn long-term dependencies . This might
further lead to the difﬁculty of capturing spectrally salient
changes in time series. More importantly, there are usually tons of HS samples (or pixels) available in real
HS image scenes, yet RNNs fail to train the models
in parallel, limiting the classiﬁcation performance in
practical applications.
• For other backbone networks, i.e., GANs, CapsNet,
GCNs, despite their respective advantages in learning
spectral representations (e.g., robustness, equivalence,
long-range relevance between samples), one thing in
common is that almost all of them could be inherently
incompetent for modeling sequential information effectively. That is, the utilization of spectral information is
insufﬁcient (being a critical bottleneck in ﬁne land cover
classiﬁcation or mapping using HS data).
Being aimed at the aforementioned limitations, we rethink
the HS image classiﬁcation process from a sequence data
perspective with current state-of-the-art transformers .
Totally different from CNNs and RNNs, transformers are (at
present) one of the cutting-edge backbone networks, owing to
the use of self-attention techniques, which are well-designed
for the sake of processing and analyzing sequential (or time
series) data more effectively. This will provide a good ﬁt for
HS data processing and analysis, e.g., HS image classiﬁcation.
It is well-known that the self-attention block in transformers
enables to capture globally sequential information by the
means of positional encoding . However, there also exist
some drawbacks in transformers that hinder its performance
to be further improved. For example,
1) although transformers perform outstandingly in solving
the problem of long-term dependencies of spectral signatures, they lose the power of capturing locally contextual
or semantic characteristics (cf. CNNs or RNNs);
2) as mentioned in , the skip connection plays a crucial
role in transformers. This might be explained well by
either using “residuals” to make the gradients better
propagated or enhancing “memories” to reduce the forgetting or loss of key information. But unfortunately, the
simple additive skip connection operation only occurs
within each transformer block, weakening the connectivity across different layers or blocks.
For these reasons, we aim to develop a novel transformers-
SUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021
based network architecture, SpectralFormer for short, enabling
the high-performance HS image classiﬁcation task. SpectralFormer provides point-to-point solutions corresponding to
the above two drawbacks. More speciﬁcally, SpectralFormer
is capable of learning locally spectral representations from
multiple neighboring bands rather than single band (in original transformers) in each encoded position, e.g., group-wise
versus band-wise embeddings. Furthermore, a cross-layer skip
connection is designed in SpectralFormer to progressively
convey memory-like components from shallow to deep layers
by adaptively learning to fuse their “soft” residuals. The main
contributions of this paper can be summarized as follows.
• We revisit the HS image classiﬁcation problem from a
sequential perspective and propose a new transformersbased backbone network, called SpectralFormer, in order
to substitute for CNNs or RNNs-based architectures. To
the best of our knowledge, this is the ﬁrst time that
transformers (without any pre-processing operation, e.g.,
feature extraction using convolution and recurrent units
or other transformation techniques) are purely applied to
the HS image classiﬁcation task.
• We devise two simple but effective modules in SpectralFormer, i.e., group-wise spectral embedding (GSE)
and cross-layer adaptive fusion (CAF), to learn locally
detailed spectral representations and convey memory-like
components from shallow to deep layers, respectively.
• We qualitatively and quantitatively evaluate the classiﬁcation performance of the proposed SpectralFormer on
three representative HS datasets, i.e., Indian Pines, Pavia
University, and University of Houston, with extensive
ablation studies. The experimental results demonstrate
a signiﬁcant superiority over classic transformers (with
an increase of approximately 10% OA) and other stateof-the-art backbone networks (at least 2% OA improvement).
The remaining of the paper is organized as follows. Section
II ﬁrst reviews classic transformers-related literature and then
details the proposed SpectralFormer with two well-designed
modules for HS image classiﬁcation. Extensive experiments
are conducted with ablation studies and discussions in Section
III. Section IV draws comprehensive conclusions and a brief
outlook on future possible research directions.
II. SPECTRALFORMER
In this section, we start to review some preliminaries of
classic transformers. On this basis, we propose the SpectralFormer with two well-designed modules, i.e., GSE and
CAF, making it more applicable to the HS image classiﬁcation
task. Finally, we also investigate the ability that the proposed
SpectralFormer models the spatially contextual information
with the input of image patches.
A. A Brief Review of Transformers
As is known to all, transformers have cut a conspicuous ﬁgure in processing sequence-to-sequence problems in
natural language processing (NLP), e.g., machine translation.
Since they abandon the sequence dependence characteristic in
Mask (opt.)
(a) Self-attention
Scaled Dot-Product A!ention
(b) Multi-head attention
Fig. 2. An illustration of the attention mechanism in transformers. (a) Selfattention module; (b) Multi-head attention.
RNNs and alternatively introduce a brand-new self-attention
mechanism. This enables the global information (long-term
dependencies) capture for the units at any position, which
greatly promotes the development of time series data processing models. Even not only limiting in NLP, image processing
and computer vision ﬁelds have also begun exploring the transformer architecture. Very recently, the vision transformer (ViT)
 seems to have achieved or approached CNNs-based stateof-the-art effects on various vision domain tasks, providing
new insight, inspiration, and creative space on vision-related
The success of transformers, to a great extent, depends on
the use of multi-head attention, where multiple self-attention
(SA) layers are stacked and integrated. As the name
suggests, SA mechanism is better at capturing the internal correlation of data or features, thereby reducing the dependence
on external information. Fig. 2(a) illustrates the process of the
SA module in transformers. More speciﬁcally, the SA layer
can be performed according to the following six steps:
Step 1. Input the sequence data x with the length of m,
where xi, i = 1, ..., m denotes either a scalar or a vector.
Step 2. The feature embedding, denoted as ai, is obtained
on each scalar or vector xi by a shared matrix W .
Step 3. Each embedding multiplies by three different transformation matrices Wq, Wk, Wv, respectively, yielding three
vectors, i.e., Query (Q = [q1, ...qm]), Key (K = [k1, ...km]),
Value (V = [v1, ..., vm]).
Step 4. Compute the attention score s between each Q
vector and each K vector in the form of inner product, e.g.,
qi · kj, and to stabilize the gradients, the scaled score is
obtained by normalization, i.e., si,j = qi · kj/
d, where d
is the dimension of qi or kj.
Step 5. The Softmax activation function is performed on
s; we then have in the position-1 for instance: ˆs1,i
Step 6. Generate attention representations z = [z1, ..., zm],
e.g., z1 = P
i ˆs1,ivi.
To sum up, the SA layer can be integrally formulated as
z = Attention(Q, K, V ) = softmax 
Spectral Signatures (Pixel or Patch)
Spectral Bands: e.g., !om 1 to n
Transformer Encoders with Cross-layer Adaptive Fusion (CAF)
Embedded Spectrum
Multi-Head
Transformer Encoder
. . . . . .
SpectralFromer for Hyperspectral Image Classiﬁcation
Fig. 3. An overview illustration of the proposed SpectralFormer network for the HS image classiﬁcation task. SpectralFormer consists of two well-designed
modules, i.e., GSE and CAF, making it (the transformers-based backbone) better applicable to HS images.
With Eq. (1), multiple different SA layers can be assembled
to be a multi-head attention, as shown in Fig. 2(b). In detail,
we ﬁrst obtain multiple attention representations (e.g., h = 8),
denoted as zh, h = 1, ..., 8, and concatenate them to be a larger
feature matrix. A linear transformation matrix (e.g., Wo) is
ﬁnally used to make the feature dimension identical to the
input data.
It should be noted, however, that there is no position
information in the SA layer, which fails to make use of
sequence information. For this reason, the position information
is encoded into the feature embedding. The embedding with
time signal can be thus formulated as ai+ei, where ei denotes
a unique positional vector given manually.
B. Overview of SpectralFormer
We aim at developing a novel and generic ViT-based
baseline network (i.e., SpectralFormer) with a focus on the
spectrometric characteristics, making it well-applicable to the
highly accurate and ﬁne classiﬁcation of HS images. To
this end, we devise two key modules, i.e., GSE and CAF,
and integrate them into the transformer framework, in order
to improve the detail-capturing capacity of subtle spectral
discrepancies and enhance the information transitivity (or
connectivity) between layers (i.e., reduce the information loss
with the gradual deepening of layers), respectively. Moreover,
the proposed SpectralFormer is not only applied to the pixelwise HS image classiﬁcation, but also extensible to the spatialspectral classiﬁcation with the batch-wise input, yielding the
spatial-spectral SpectralFormer version. Fig. 3 illustrates an
overview of the proposed SpectralFormer in the HS image
classiﬁcation task, while Table I details the deﬁnition of
notations used in the proposed SpectralFormer.
C. Group-wise Spectral Embedding (GSE)
Unlike the discrete sequentiality in classic transformers or
ViT (e.g., image patches), hundreds or thousands of spectral
channels in HS images are densely sampled from the electromagnetic spectrum at a subtle interval (e.g., 10nm), yielding
approximately continuous spectral signatures. The spectral
information in different position reﬂects different absorption
characteristics corresponding to different wavelength. This to
a great extent shows the physical properties of the current
material. Capturing locally detailed absorption (or changes) of
such spectral signatures is a crucial factor to accurately and
ﬁnely classify the materials lying in the HS scene. For this
purpose, we propose to learn group-wise spectral embeddings,
i.e., GSE, instead of band-wise input and representations.
Given a spectral signature (a pixel in the HS image) x =
[x1, x2, . . . , xm] ∈R1×m, the feature embeddings a obtained
by classic transformers are formulated by
where w ∈Rd×1 denotes the linear transformation that is
equivalently used for all bands in spectral signatures and A ∈
Rd×m collects the output features. Whereas the proposed GSE
learns the feature embeddings from locally spectral proﬁles (or
neighboring bands). Thus, the GSE can be modeled as
˙A = W X = W g(x),
∈Rd×n and X ∈Rn×m correspond to the
grouped representations with respect to the variables w and
x, respectively, n represents the number of neighboring bands.
The variable W can be simply seen as one layer of network,
which can be optimized by updating the whole network. The
function g(·) denotes the overlapping grouping operation in
regard to the variable x, i.e.,
X = g(x) = [x1, ..., xq, ..., xm],
SUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021
DEFINITION OF NOTATIONS USED IN THE PROPOSED SPECTRALFORMER
the number of spectral bands
spectral signature of a HS pixel
reﬂectance or radiance value in the i-th band location
the dimension of the feature embeddings
the linear transformation w.r.t a pixel (sample)
the feature embeddings
the number of considered HS pixels
the linear transformations w.r.t. n pixels (samples)
spectral signatures of n pixels
the overlapping grouping operation w.r.t. the variable (·)
the feature embeddings of GSE
the round operation
the feature representations in the (l)-th layer
the learnable network parameter for adaptive fusion
the fused representations in the (l)-th layer with the CAF
2 ⌋, ..., xq, ..., xq+⌊n
2 ⌋]⊤∈Rn×1, ⌊•⌋
denotes the round operation. Fig. 4 illustrates the differences
between band-wise and group-wise spectral embeddings in
transformers-based backbone networks, i.e., BSE versus GSE.
D. Cross-layer Adaptive Fusion (CAF)
The skip connection (SC) mechanism has been proven to
be an effective strategy in deep networks, which can enhance
information exchange between layers and reduce information
loss in the network learning process. The use of SC has
recently gained tremendous success in image recognition and
segmentation, e.g., short SC for ResNet and long SC for
U-Net . It should be noted, however, that the information
“memory” ability of the short SC remains limited, while the
long SC tends to yield insufﬁcient fusion due to a big gap between high- and low-level features. This is also a key problem
existed in transformers, which will pose a new challenge to the
architecture design of transformers. To this end, we devise a
middle-range SC in SpectralFormer to adaptively learn crosslayer feature fusion (i.e., CAF, see Fig. 5).
Let z(l−2) ∈R1×dz and z(l) ∈R1×dz be the outputs (or
representations) in the (l−2)-th and (l)-th layers, respectively,
CAF can be then expressed by
where ˆz(l) denotes the fused representations in the (l)-th layer
with the proposed CAF, and ¨
w ∈R1×2 is the learnable
network parameter for adaptive fusion. It should be noted
that our CAF only skips one encoder, e.g., from z(l−2) (the
output of Encoder 1) to z(l) (the output of Encoder 3) in
Fig. 5. The reasons for this setting are two-fold. On the one
hand, there is a big semantic gap between low- and deeplevel features obtained from the shallow and deep layers,
respectively. If the relatively long SC, e.g., two, three, and even
more encoders, is used, this then might lead to an insufﬁcient
fusion and potential information loss. On the other hand, the
HS image classiﬁcation can be often regarded as a smallsample problem, due to the limited available training (need
. . . . . .
Pixel- or Patch-wise Spectral Bands: e.g., !om 1 to n
Linear Projections of Spectral Bands
(a) Band-wise Spectral Embedding (BSE)
. . . . . .
. . . . . .
. . . . . .
Linear Projections of Locally Neighboring Spectral Bands
Pixel- or Patch-wise Spectral Bands: e.g., !om 1 to n
Local Grouping
(b) Group-wise Spectral Embedding (GSE)
An illustrative comparison of band-wise and group-wise spectral
embeddings (BSE versus GSE) in transformers.
to be labeled manually) samples. A “small” or “shallow”
backbone network, e.g., 4 or 5 layers, may already be a good
ﬁt for the HS image classiﬁcation task. As a result, this, to
some extent, can explain why we propose to skip only one
encoder in the CAF module (since a 4 or 5-layered shallow
network is small, which is not capable of adding multiple CAF
E. Spatial-Spectral SpectralFormer
Beyond the pixel-wise HS image classiﬁcation, we similarly
investigate the patch-wise input (inspired by CNNs), yielding
the spatial-spectral SpectralFormer version, i.e., patch-wise
SpectralFormer. Different from CNNs, that directly input a
3-D patch cube, we unfold the 2-D patch of each band to the
corresponding 1-D vector representations. Given a 3-D cube
X ∈Rm×w×h (w and h are the width and length of the patch),
SUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021
. . . . . .
Output of encoder
A diagram of cross-layer adaptive fusion (CAF) module in the
proposed SpectralFormer.
which can be unfolded along with the spatial direction, we then
X = [⃗x1, ..., ⃗xi, ..., ⃗xm],
where ⃗xi ∈Rwh×1 denotes the unfolded patch for the ith band. Such an input way can to a great extent preserve
the spectrally sequential information in network learning and
meanwhile consider spatially contextual information.
III. EXPERIMENTS
In this section, three well-known HS datasets are ﬁrstly described, the implementation details and compared state-of-theart methods are then introduced. Finally, extensive experiments
are conducted with ablation analysis to assess the HS image
classiﬁcation performance of the proposed SpectralFormer
both quantitatively and qualitatively.
A. Data Description
1) Indian Pines Data: The ﬁrst HS data was collected in
1992 by using the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) sensor over North-Western Indiana, USA.
The HS image consists of 145 × 145 pixels at a ground
sampling distance (GSD) of 20m, and 220 spectral bands
covering the wavelength range of 400nm to 2500nm with a
10m spectral resolution. After removing 20 noisy and water
absorption bands, 200 spectral bands are retained, i.e., 1-103,
109-149, 164-219. There are 16 mainly-investigated categories
in this studied scene. The class name and the number of
samples used for training and testing in the classiﬁcation task
are listed in Table II, while the spatial distribution of training
and testing sets are also given in Fig. 7 to reproduce the
classiﬁcation results.
2) Pavia University Data: The second HS scene was acquired by the Reﬂective Optics System Imaging Spectrometer
(ROSIS) sensor over Pavia University and its surroundings,
Pavia, Italy. The sensor can capture 103 spectral bands ranging
from 430nm to 860nm, and the image consists of 610 × 340
pixels at a GSD of 1.3m. This scene includes 9 land cover
classes, where the ﬁxed number of training and testing samples
are detailed in Table III and spatially visualized in Fig. 8.
LAND-COVER CLASSES OF THE INDIAN PINES DATASET, WITH THE
STANDARD TRAINING AND TESTING SETS FOR EACH CLASS.
Class Name
Corn Notill
Corn Mintill
Grass Pasture
Grass Trees
Hay Windrowed
Soybean Notill
Soybean Mintill
Soybean Clean
Buildings Grass Trees Drives
Stone Steel Towers
Grass Pasture Mowed
LAND-COVER CLASSES OF THE PAVIA UNIVERSITY DATASET, WITH THE
STANDARD TRAINING AND TESTING SETS FOR EACH CLASS.
Class Name
Metal Sheets
3) Houston2013 Data: The third dataset was acquired
by the ITRES CASI-1500 sensor over the campus of the
University of Houston and its neighboring rural areas, Texas,
USA, which, as a benchmark dataset, has been widely used
for evaluating the performance of land cover classiﬁcation
 . The HS cube comprises of 349 × 1905 pixels with 144
wavelength bands in the range of 364nm to 1046nm at 10nm
intervals. It is worth noting, however, that the Houston2013
dataset we used is a cloud-free version, which is processed
to recover the missing data or remove occlusions by generating illumination-related threshold maps1. Table IV lists 15
challenging land-cover and land-use categories as well as the
corresponding sample number of training and testing sets.
Similarly, the visualization results with respect to the falsecolor HS image and the released training and testing samples
provided by the 2013 IEEE GRSS data fusion contest2 are
given in Fig. 10.
1The data were provided by Prof. N. Yokoya from the University of Tokyo
and RIKEN AIP.
2 
SUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021
LAND-COVER CLASSES OF THE HOUSTON2013 DATASET, WITH THE
STANDARD TRAINING AND TESTING SETS FOR EACH CLASS.
Class Name
Healthy Grass
Stressed Grass
Synthetic Grass
Residential
Commercial
Parking Lot1
Parking Lot2
Tennis Court
Running Track
B. Experimental Setup
1) Evaluation Metrics: We evaluate the classiﬁcation performance of each model quantitatively in terms of three
commonly-used indices, i.e., Overall Accuracy (OA), Average
Accuracy (AA), Kappa Coefﬁcient (κ). Moreover, the classi-
ﬁcation maps obtained by different models are visualized to
make a qualitative comparison.
2) Comparison with State-of-the-art Backbone Networks:
Several representative baselines and backbone networks are
selected for the following comparison experiments. They are
K-nearest neighbor (KNN), support vector machine (SVM),
random forest (RF), 1-D CNN , 2-D CNN , RNN
 , miniGCN , transformers , and the proposed SpectralFormer. The parameter conﬁgurations of these compared
methods are detailed below:
• For the KNN, the number of nearest neighbors (K) is an
important factor, which greatly impacts the classiﬁcation
performance. We set it to 10.
• For the RF, 200 decision trees are used in our experiments.
• For the SVM, the libsvm toolbox3 is selected for the
implementation of the HS image classiﬁcation task. The
SVM is performed by using the radial basis function
(RBF) kernel. In RBF, two hyperparameters σ and λ can
be optimally determined by ﬁve-fold cross validation on
the training set in the range of σ = [2−3, 2−2, . . . , 24]
and λ = [10−2, 10−1, . . . , 104], respectively.
• For the 1-D CNN, one convolutional block is deﬁned as
the basic network unit, including a set of 1-D convolutional ﬁlters with the output size of 128, a batch normalization (BN) layer, and a ReLU activation function. A
softmax function is ﬁnally added on the top layer of the
• The 2-D CNN architecture has three 2-D convolutional
blocks and a softmax layer. Similar to the 1-D CNN,
each convolutional block of 2-D CNN consists of a 2-D
conventional layer, a BN layer, a max-pooling layer, and
3 
a ReLU activation function. Moreover, the spatially and
spectrally receptive ﬁelds in each 2-D convolutional layer
are 3 × 3 × 32, 3 × 3 × 64, and 1 × 1 × 128, respectively.
• For the RNN, there are two recurrent layers with the gated
recurrent unit (GRU). Each of them has 128 neuron units.
The used codes are openly available from 
com/danfenghong/HyFTech.
• For the miniGCN, the network block successively contains a BN layer, a graph convolutional layer with 128
neuron units, and a ReLU layer. Note that the adjacency
matrix in GCN can be generated using a KNN-based
graph (the number of K is the same with the KNN classiﬁer, i.e., K = 10). The miniGCN and 1-D CNN share
the same network architecture (for a fair comparison).
As the name suggests, the miniGCN can be trained in a
mini-batch fashion. We refer to for more details and
the codes4 for the sake of reproducibility.
• For the transformers5, we follow the ViT network architecture , i.e., only including transformer encoders.
In detail, ﬁve encoder blocks are used in the ViT-based
network for HS image classiﬁcation.
• For the proposed SpectralFormer, we adopt the same
backbone architecture as the above transformers for a fair
comparison. More speciﬁcally, the embedded spectrum
with 64 units are fed into 5 cascaded transformer encoder blocks for HS image classiﬁcation. Each encoder
block consists of a four-head SA layer, a MLP with 8
hidden dimensions, and a GELU nonlinear activation
layer. The dropout layer is employed after encoding
positional embeddings and in MLPs for inhibiting 10%
neurons. Considering the fact that the parameter size is
evidently increased from the pixel-wise SpectralFormer
to the patch-wise one (the patch size is empirically set
as 7 × 7), we additionally employ an ℓ2 weight decay
regularization parameterized by 5e −3 to prevent a
potential overﬁtting risk for the latter.
3) Implementation Details: Our proposed SpectralFormer
was implemented on the PyTorch platform using a workstation
with i7-6850K CPU, 128GB RAM, and an NVIDIA GTX
1080Ti 11GB GPU. We adopt the Adam optimizer with
a mini-batch size of 64. The learning rate is initialized with
5e −4 and decayed by multiplying a factor of 0.9 after each
one-tenth of total epochs. We roughly set the epochs on the
three datasets to 10006. It is worth noting, however, that we
found in practice our SpectralFormer with the CAF module
is capable of embracing an evident efﬁciency improvement
by convergence using much less epochs, i.e., 300 epochs for
the Indian Pines dataset, and 600 epochs for the other two
4) Computational Complexity Analysis: For a given HS
image with spectral length m, the per-layer computational
complexity of the proposed SpectralFormer is mainly dominated by self-attention and multi-head operations that require
4 TGRS GCN
5The codes of transformers will be provided by authors, making it applicable to the HS image classiﬁcation task.
6The epochs might be slightly adjusted for different algorithms and different
datasets, which will be speciﬁcally provided in our codes.
SUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021
ABLATION ANALYSIS OF THE PROPOSED SPECTRALFORMER WITH A
COMBINATION OF DIFFERENT MODULES ON THE INDIAN PINES DATASET.
Transformers (ViT)
pixel-wise
SpectralFormer
pixel-wise
SpectralFormer
pixel-wise
SpectralFormer
pixel-wise
SpectralFormer
patch-wise
SENSITIVITY ANALYSIS OF THE NUMBER OF NEIGHBORING BANDS IN
SPECTRALFORMER (ONLY GSE AND GSE + CAF) IN TERMS OF OA, AA,
AND κ ON THE INDIAN PINES DATASET.
SpectralFormer
The number of neighboring bands
CLASSIFICATION PERFORMANCE ANALYSIS BETWEEN DIFFERENT SCS IN
TRANSFORMERS ON THE INDIAN PINES DATASET.
Transformers (ViT)
pixel-wise
short-range
Transformers (ViT)
long-range
Transformers (ViT)
an overall O(m2d + md2), where d is the size of the hidden
features that are also used in deep competitors for a fair
theoretical comparison. The RNN yields the complexity of
O(md2) after m times sequential operations, while a CNN
with the kernel width k increases it considerably to O(kmd2).
The GCN (i.e., miniGCN) requires O(bmd + b2m) owing to
its batch-wise graph sampling, where b denotes the size of
mini-batches.
C. Model Analysis
1) Ablation Study: We investigate the performance gain
of the proposed SpectralFormer in terms of classiﬁcation
accuracies by adding (in stepwise fashion) different modules
(i.e., GSE and CAF) in networks. For that, we conduct
extensive ablation experiments on the Indian Pines dataset to
verify the effectiveness of these components (or modules) in
SpectralFormer for HS image classiﬁcation applications, as
listed in Table V.
In detail, the classic transformers (ViT) without GSE and
CAF modules yield the lowest classiﬁcation accuracies, which
to some extent indicates that the ViT architecture might not be
a good ﬁt for the HS image classiﬁcation. By plugging either
GSE or CAF into ViT, the classiﬁcation results of pixel-wise
SpectralFormer are better than that of ViT (beyond around
4% and 3% OAs, respectively). What is better still, the joint
exploitation of GSE and CAF can further bring a dramatic
performance improvement (more than 4% OA). More remarkably, our SpectralFormer is also capable of capturing the
locally spatial semantics of HS images by simply unfolding the
patch-wise input. As a result, the patch-wise SpectralFormer
performs observably better than pixel-wise ones at an increase
at least 3% OA (compared to the second record, i.e., 78.55%).
Interestingly, there is a noteworthy trend in Table V. That
is, the joint use of GSE and CAF tends to obtain the best
performance when only using a smaller number of neighboring
bands in GSE, compared to that of only using GSE. This
might be well explained by that after adding CAF, the spectral
information is capable of being learned more efﬁciently and
easier. In other words, the less overlap between spectral bands
(i.e., the number of neighboring bands is smaller in GSE)
could be enough to obtain a better classiﬁcation performance,
after the CAF module is activated.
2) Parameter Sensitivity Analysis: Apart from the learnable
parameters in networks and hyper-parameters required in the
training process, the number of neighboring bands in GES
plays a vital role in the ﬁnal classiﬁcation performance. It
is, therefore, indispensable to explore the proper parameter
range. Similarly, we investigate the parameter sensitivity on the
Indian pines dataset in an ablation manner, i.e., only with GSE
and the joint use of GSE and CAF. Table VI lists the changing
trend of the classiﬁcation accuracies with the gradual increase
of the number of grouped bands in terms of OA, AA, and κ.
A common conclusion is that GSE can better excavate subtle
spectral discrepancies by effectively capturing locally spectral
embeddings from neighboring bands. Within a certain range,
the parameter is not sensitive to the classiﬁcation performance.
This provides great potentials of the proposed model in the
practical applications. In other words, the parameter can be
simply and directly used for other datasets.
We also make a quantitative comparison among short-range
SC (e.g., in ResNet), long-range SC (e.g., in U-Net), and
middle-range (i.e., our CAF module) SC in transformers,
in order to verify the effectiveness of the proposed SpectralFormer in processing spectral data. Table VII quantiﬁes
the classiﬁcation performance comparison of using short-,
middle-, and long-range SCs, respectively, on the Indian Pines
dataset. In general, the ViT with long-range SC yields a
poor performance, possibly since such an SC strategy might
fail to sufﬁciently fuse and convey long-range cross-layer
features, tending to lose partially “important” information.
This demonstrates the superiority of the CAF module that can
exchange information across different layers more effectively
(cf. short-range SC) and reduce the information loss (cf. longrange SC).
Furthermore, we randomly selected varying number of
training samples from the given training set on the Indian
Pines dataset out of 10 runs in the proportion of [10%, 20%,
... , 100%] at intervals of 10%. The average results with
standard deviation values in terms of the OA obtained by
the proposed SpectralFormer (including pixel-wise and patchwise versions) are reported in Fig. 6. There is a basically
reasonable trend in OA’s results (see Fig. 6). That is, with
the increase of the percentage of used training samples, the
classiﬁcation performance gradually improves. Note that the
OAs are tending towards stability when more training samples
(e.g., 80%, 90%, and 100%) are involved, showing the stability
of the proposed SpectralFormer to a great extent. Moreover,
SUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021
TABLE VIII
QUANTITATIVE PERFORMANCE OF DIFFERENT CLASSIFICATION METHODS IN TERMS OF OA, AA, AND κ AS WELL AS THE ACCURACIES FOR EACH
CLASS ON THE INDIAN PINES DATASET. THE BEST ONE IS SHOWN IN BOLD.
Conventional Classiﬁers
Classic Backbone Networks
Transformers (ViT)
SpectralFormer
pixel-wise
patch-wise
QUANTITATIVE PERFORMANCE OF DIFFERENT CLASSIFICATION METHODS IN TERMS OF OA, AA, AND κ, AS WELL AS THE ACCURACIES FOR EACH
CLASS ON THE PAVIA UNIVERSITY DATASET. THE BEST ONE IS SHOWN IN BOLD.
Conventional Classiﬁers
Classic Backbone Networks
Transformers (ViT)
SpectralFormer
pixel-wise
patch-wise
the patch-wise SpectralFormer observably outperforms the
pixel-wise one, as expected.
D. Quantitative Results and Analysis
Quantitative classiﬁcation results in terms of three overall
indices, i.e., OA, AA, and κ, and the accuracies for each class
are reported in Tables VIII, IX, and X for Indian Pines, Pavia
University, and Houston2013 HS datasets, respectively.
Overall, the conventional classiﬁers, e.g., KNN, RF, SVM,
achieve similar classiﬁcation performance on all three data
sets, except the accuracies in terms of OA, AA, and κ with
KNN on the Indian Pines dataset (which are far inferior to
those using RF and SVM). Owing to the powerful learning
ability of DL techniques, classic backbone networks, e.g., 1-D
CNN, 2-D CNN, RNN, miniGCN, observably perform better
than aforementioned conventional classiﬁers, i.e., KNN, RF,
SVM. The results, to a great extent, demonstrate the value
and practicality of DL-based approaches in HS image classiﬁcation. Without any convolutional and recurrent operations,
transformers extract ﬁner spectral representations from the
sequence perspectives, yielding a comparable performance to
CNNs-, RNNs-, or GCNs-based models.
Although transformers are capable of capturing globally
sequential information, the ability in modeling some key
factors – locally spectral discrepancies – remains limited,
leads to a performance bottleneck. To overcome this problem, the proposed SpectralFormer fully extracts local spectral
information from neighboring bands, dramatically improving the classiﬁcation performance. In particular, the pixelwise SpectralFormer unexpectedly outperforms others, even
SUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021
QUANTITATIVE PERFORMANCE OF DIFFERENT CLASSIFICATION METHODS IN TERMS OF OA, AA, AND κ AS WELL AS THE ACCURACIES FOR EACH
CLASS ON THE HOUSTON2013 DATASET. THE BEST ONE IS SHOWN IN BOLD.
Conventional Classiﬁers
Classic Backbone Networks
Transformers (ViT)
SpectralFormer
pixel-wise
patch-wise
10% 20% 30% 40% 50% 60% 70% 80% 90%
The percentage of used training samples
SpectralFormer (pixel-wise)
SpectralFormer (patch-wise)
62.55% 62.02%
77.08% 78.76%
80.52% 81.76%
Fig. 6. Classiﬁcation results (OA) obtained by our proposed SpectralFormer
(pixel-wise and patch-wise) with the varying number of used training samples
on the Indian Pines dataset.
though comparing to CNNs-based methods considering spatial
contents. Undoubtedly, the patch-wise SpectralFormer obtains
higher classiﬁcation accuracies which are far superior to those
obtained by other competitors, since the spatial-contextual
information is jointly considered in the sequential feature
extraction process.
In addition, for those challenging classes that have limited
training samples (e.g., Grass Pasture Mowed, Oats) and imbalanced (or noisy) samples (e.g., Corn Mintill, Grass Pasture,
Hay Windrowed) on the Indian Pines data, SpectralFormer,
either pixel-wise or patch-wise input, tends to obtain better
classiﬁcation performance by focusing on particular absorption
positions of spectral proﬁles. By contrary, despite the excellent
representation ability for spectral sequence data, transformers
(ViT) fail to accurately capture the detailed spectral absorption
or change due to the weak modeling ability in locally spectral
discrepancies.
E. Visual Evaluation
We make a qualitative evaluation by visualizing the classi-
ﬁcation maps obtained by different methods. Figs. 7, 8, and
10 provide the obtained results for the Indian Pines, Pavia
University, and Houston2013 datasets, respectively. Roughly,
conventional classiﬁcation models (e.g., KNN, RF, SVM) tend
to generate salt and pepper noises in classiﬁcation maps of
three considered datasets. This indirectly indicates that these
classiﬁers fail to accurately identifying the materials of objects.
Not surprisingly, DL-based models, e.g., CNNs, RNNs, GCNs,
obtain relatively smooth classiﬁcation maps, owing to their
powerful nonlinear data ﬁtting ability. As an emerging network
architecture, transformers (ViT used in our case) can extract
highly sequential representations from HS images, leading
to visualized classiﬁcation maps that are comparable to the
above classic backbone networks. By enhancing spectrally
neighboring information and conveying “memory” information
across layers more effectively, our proposed SpectralFormer
obtains highly desirable classiﬁcation maps, especially in
terms of texture and edge details. Furthermore, we selected
a region of interest (ROI) (from Figs. 7, 8, and 10) zoomedin 2 times to highlight the differences in classiﬁcation maps
between different models, further evaluating their classiﬁcation
performance more intuitively. As can be seen from these ROIs,
a remarkable phenomenon is that our methods, i.e., pixel-wise
and patch-wise SpectralFormers, show more realistic and ﬁner
details. In particular, the results of our methods have less noisy
points compared to those pixel-wise methods, e.g., KNN, RF,
SVM, 1-D CNN, RNN, miniGCN, and ViT, but also avoid
over-smoothness in edges or some small semantic objects
SUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021
Transformers (ViT)
SpectralFormer (pixel)
CornNotill
CornMintill
GrassPature
GrassTrees
HayWindrow
SoybeanNotill
SoybeanMintill
SoybeanClean
BuilGraTrDri
StoSteTower
GrassPastMow
SpectralFormer (patch)
Fig. 7. Spatial distribution of training and testing sets, and the classiﬁcation maps obtained by different models on the Indian Pines dataset, where a ROI
zoomed in 2 times is highlighted for more detailed observation.
Metal Sheets
Transformers (ViT)
SpectralFormer (pixel)
SpectralFormer (patch)
Fig. 8. Spatial distribution of training and testing sets, and the classiﬁcation maps obtained by different models on the Pavia University dataset, where a ROI
zoomed in 2 times is highlighted for more detailed observation.
Fig. 9. Visualization of selected encoder output features obtained with only CAF and without CAF (i.e., the original ViT).
SUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021
Transformers (ViT)
SpectralFormer (pixel)
SpectralFormer (patch)
Healthy GrassStressed GrassSynthetic Grass
Residential
Commercial
Parkinig Lot1 Parkinig Lot2 Tennis Court Running Track
Fig. 10. Spatial distribution of training and testing sets, and the classiﬁcation maps obtained by different models on the Houston2013 dataset, where a ROI
zoomed in 2 times is highlighted for more detailed observation.
(cf. 2-D CNN), which yields more accurate classiﬁcation
performance.
Feature Visualization. Fig. 9 visualizes selected encoder
output features using the proposed SpectralFormer framework
with only CAF and without CAF (i.e., ViT). We selectively
pick out some representative feature maps for visual comparison, where the visualization results of using the CAF module
have ﬁner appearance (e.g., the edge or outline of objects,
textural structure, etc.) than those without CAF. This also
demonstrates the effectiveness and superiority of the designed
CAF module from the visual perspective.
IV. CONCLUSION
HS images are typically collected (or represented) as a data
cube with spatial-spectral information, which can be generally
regarded as a sequence of data along the spectral dimension.
Unlike CNNs, that focus mainly on contextual information
modeling, transformers have been proven to be a powerful
architecture in characterizing the sequential properties globally. However, the classic transformers-based vision networks,
e.g., ViT, inevitably suffer from performance degradation when
processing HS-like data. This might be explained well by
the fact that ViT fails to model locally detailed spectral
discrepancies and convey “memory”-like components (from
shallow to deep layers) effectively. To this end, in this paper we
propose a new transformers-based backbone network, called
SpectralFormer, which is more focused on extracting spectral
information. Without using any convolution or recurrent units,
the proposed SpectralFormer can achieve state-of-the-art classiﬁcation results for HS images.
In the future, we will investigate strategies to further improve the transformers-based architecture by utilizing more
advanced techniques, e.g., attention, self-supervised learning,
making it more applicable to the HS image classiﬁcation task,
and also attempt to establish a lightweight transformers-based
network to reduce the network complexity while maintaining
SUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021
its performance. Moreover, we would also like to embed more
physical characteristics of spectral bands and prior knowledge
of HS images into the proposed framework, yielding more
interpretable deep models. Furthermore, the number of skipped
and connected encoders in the CAF module is an important
factor that might be capable of improving the classiﬁcation
performance of the proposed SpectralFormer, which should
be paid more attention to in future work.