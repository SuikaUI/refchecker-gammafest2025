Knowledge Graph Embedding by Translating on Hyperplanes
Zhen Wang1∗, Jianwen Zhang2, Jianlin Feng1, Zheng Chen2
1Department of Information Science and Technology, Sun Yat-sen University, Guangzhou, China
2Microsoft Research, Beijing, China
1{wangzh56@mail2, fengjlin@mail}.sysu.edu.cn
2{jiazhan, zhengc}@microsoft.com
We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space.
TransE is a promising method proposed recently, which is
very efﬁcient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations
which should be considered in embedding, such as reﬂexive, one-to-many, many-to-one, and many-to-many. We note
that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these
mapping properties but sacriﬁce efﬁciency in the process. To
make a good trade-off between model capacity and efﬁciency,
in this paper we propose TransH which models a relation as a
hyperplane together with a translation operation on it. In this
way, we can well preserve the above mapping properties of
relations with almost the same model complexity of TransE.
Additionally, as a practical knowledge graph is often far from
completed, how to construct negative examples to reduce
false negative labels in training is very important. Utilizing
the one-to-many/many-to-one mapping property of a relation,
we propose a simple trick to reduce the possibility of false
negative labeling. We conduct extensive experiments on link
prediction, triplet classiﬁcation and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show
TransH delivers signiﬁcant improvements over TransE on
predictive accuracy with comparable capability to scale up.
Introduction
Knowledge graphs such as Freebase ,
WordNet and GeneOntology have become very important resources to support
many AI related applications, such as web/mobile search,
Q&A, etc. A knowledge graph is a multi-relational graph
composed of entities as nodes and relations as different types
of edges. An instance of edge is a triplet of fact (head entity,
relation, tail entity) (denoted as (h, r, t)). In the past decade,
there have been great achievements in building large scale
knowledge graphs, however, the general paradigm to support
computing is still not clear. Two major difﬁculties are: (1) A
knowledge graph is a symbolic and logical system while
∗This work was done during Zhen Wang’s internship in Microsoft Research.
Copyright c⃝2014, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
applications often involve numerical computing in continuous spaces; (2) It is difﬁcult to aggregate global knowledge
over a graph. The traditional method of reasoning by formal
logic is neither tractable nor robust when dealing with
long range reasoning over a real large scale knowledge
graph. Recently a new approach has been proposed to deal
with the problem, which attempts to embed a knowledge
graph into a continuous vector space while preserving
certain properties of the original graph . For example,
each entity h (or t) is represented as a point h (or t) in
the vector space while each relation r is modeled as an
operation in the space which is characterized by an a vector
r, such as translation, projection, etc. The representations
of entities and relations are obtained by minimizing a
global loss function involving all entities and relations.
As a result, even the embedding representation of a single
entity/relation encodes global information from the whole
knowledge graph. Then the embedding representations can
be used to serve all kinds of applications. A straightforward
one is to complete missing edges in a knowledge graph. For
any candidate triplet (h, r, t), we can conﬁrm the correctness
simply by checking the compatibility of the representations
h and t under the operation characterized by r.
Generally, knowledge graph embedding represents an
entity as a k-dimensional vector h (or t) and deﬁnes a
scoring function fr(h, t) to measure the plausibility of the
triplet (h, r, t) in the embedding space. The score function
implies a transformation r on the pair of entities which
characterizes the relation r. For example, in translation
based method (TransE) , fr(h, t) ≜
∥h+r−t∥ℓ1/2, i.e., relation r is characterized by the translating (vector) r. With different scoring functions, the implied
transformations vary between simple difference , translation , afﬁne , general linear ,
bilinear , and nonlinear transformations . Accordingly the model complexities (in terms
of number of parameters) vary signiﬁcantly. (Please refer to
Table 1 and Section “Related Works” for details.)
Among previous methods, TransE is
a promising one as it is simple and efﬁcient while achieving
Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence
state-of-the-art predictive performance. However, we ﬁnd
that there are ﬂaws in TransE when dealing with relations
with mapping properties of reﬂexive/one-to-many/manyto-one/many-to-many. Few previous work discuss the role
of these mapping properties in embedding. Some advanced
models with more free parameters are capable of preserving
these mapping properties, however, the model complexity
and running time is signiﬁcantly increased accordingly.
Moreover, the overall predictive performances of the
advanced models are even worse than TransE . This motivates us to propose a method which makes
a good trad-off between model complexity and efﬁciency
so that it can overcome the ﬂaws of TransE while inheriting
the efﬁciency.
In this paper, we start by analyzing the problems of
reﬂexive/one-to-many/many-to-one/many-tomany relations. Accordingly we propose a method named
translation on hyperplanes (TransH) which interprets a
relation as a translating operation on a hyperplane. In
TransH, each relation is characterized by two vectors, the
norm vector (wr) of the hyperplane, and the translation
vector (dr) on the hyperplane. For a golden triplet (h, r, t),
that it is correct in terms of worldly facts, the projections
of h and t on the hyperplane are expected to be connected
by the translation vector dr with low error. This simple
method overcomes the ﬂaws of TransE in dealing with
reﬂexive/one-to-many/many-to-one/many-to-many
relations while keeping the model complexity almost the same
as that of TransE. Regarding model training, we point out
that carefully constructing negative labels is important in
knowledge embedding. By utilizing the mapping properties
of relations in turn, we propose a simple trick to reduce
the chance of false negative labeling. We conduct extensive
experiments on the tasks of link prediction, triplet classiﬁcation and fact extraction on benchmark datasets like
WordNet and Freebase, showing impressive improvements
on different metrics of predictive accuracy. We also show
that the running time of TransH is comparable to TransE.
Related Work
The most related work is brieﬂy summarized in Table 1. All
these methods embed entities into a vector space and enforce the embedding compatible under a scoring function.
Different models differ in the deﬁnition of scoring functions
fr(h, r) which imply some transformations on h and t.
TransE represents a relation by
a translation vector r so that the pair of embedded entities in a triplet (h, r, t) can be connected by r with low
error. TransE is very efﬁcient while achieving state-of-theart predictive performance. However, it has ﬂaws in dealing
with reﬂexive/one-to-many/many-to-one/many-to-many relations.
Unstructured is a simpliﬁed case of TransE, which considers the graph as mono-relational and sets all translations
r = 0, i.e., the scoring function is ∥h −t∥. It is used as a
naive baseline in . Obviously it
cannot distinguish different relations.
Distant Model introduces two independent projections to the entities in a relation. It represents
a relation by a left matrix Wrh and a right matrix Wrt. Dissimilarity is measured by L1 distance between Wrhh and
Wrtt. As pointed out by , this model is
weak in capturing correlations between entities and relations
as it uses two separate matrices.
Bilinear Model models second-order correlations between entity embeddings by a quadratic form:
h⊤Wrt. Thus, each component of an entity interacts with
each component of the other entity.
Single Layer Model introduces
nonlinear transformations by neural networks. It concatenates h and t as an input layer to a non-linear hidden
layer then the linear output layer gives the resulting score:
r f(Wrhh + Wrtt + br). A similar structure is proposed
in .
NTN is the most expressive model
so far. It extends the Single Layer Model by considering
the second-order correlations into nonlinear transformation
(neural networks). The score function is u⊤
r f(h⊤Wrt +
Wrhh + Wrtt + br). As analyzed by the authors, even
when the tensor Wr degenerates to a matrix, it covers all
the above models. However, the model complexity is much
higher, making it difﬁcult to handle large scale graphs.
Beyond these works directly targeting the same problem
of embedding knowledge graphs, there are extensive related
works in the wider area of multi-relational data modeling,
matrix factorization, and recommendations. Please refer to
the Introduction part of .
Embedding by Translating on Hyperplanes
We ﬁrst describe common notations. h denotes a head entity, r denotes a relation and t denotes a tail entity. The bold
letters h, r, t denote the corresponding embedding representations. ∆denotes the set of golden triplets, and ∆′ denotes
the set of incorrect triplets. Hence we use (h, r, t) ∈∆to
state “(h, r, t) is correct”. E is the set of entities. R is the set
of relations.
Relations’ Mapping Properties in Embedding
As introduced in Introduction & Related Work (Table 1),
TransE models a relation r as a translation vector r ∈Rk
and assumes the error ∥h + r −t∥ℓ1/ℓ2 is low if (h, r, t) is
a golden triplet. It applies well to irreﬂexive and one-to-one
relations but has problems when dealing with reﬂexive or
many-to-one/one-to-many/many-to-many relations.
Considering the ideal case of no-error embedding where
h + r −t = 0 if (h, r, t) ∈∆, we can get the following
consequences directly from TransE model.
• If (h, r, t) ∈∆and (t, r, h) ∈∆, i.e., r is a reﬂexive map,
then r = 0 and h = t.
• If ∀i ∈{0, . . . , m}, (hi, r, t) ∈∆, i.e., r is a many-to-one
map, then h0 = . . . = hm. Similarly, if ∀i, (h, r, ti) ∈∆,
i.e., r is a one-to-many map, then t0 = . . . = tm.
The reason leading to the above consequences is, in
TransE, the representation of an entity is the same when involved in any relations, ignoring distributed representations
Table 1: Different embedding models: the scoring functions fr(h, t) and the model complexity (the number of parameters). ne
and nr are the number of unique entities and relations, respectively. It is the often case that nr ≪ne. k is the dimension of
embedding space. s is the number of hidden nodes of a neural network or the number of slices of a tensor.
Score function fr(h, t)
# Parameters
TransE 
∥h + r −t∥ℓ1/2, r ∈Rk
O(nek + nrk)
Unstructured 
Distant 
∥Wrhh −Wrtt∥1, Wrh, Wrt ∈Rk×k
O(nek + 2nrk2)
Bilinear 
h⊤Wrt, Wr ∈Rk×k
O(nek + nrk2)
Single Layer
r f(Wrhh + Wrtt + br)
O(nek + nr(sk + s))
ur, br ∈Rs, Wrh, Wrt ∈Rs×k
NTN 
r f(h⊤Wrt + Wrhh + Wrtt + br)
O(nek + nr(sk2 + 2sk + 2s))
ur, br ∈Rs, Wr ∈Rk×k×s, Wrh, Wrt ∈Rs×k
TransH (this paper)
r hwr) + dr −(t −w⊤
O(nek + 2nrk)
wr, dr ∈Rk
(a) TransE
(b) TransH
Figure 1: Simple illustration of TransE and TransH.
of entities when involved in different relations. Although
TransE does not enforce h + r −t = 0 for golden triplets,
it uses a ranking loss to encourage lower error for golden
triplets and higher error for incorrect triplets , the tendency in the above propositions still exists.
Translating on Hyperplanes (TransH)
reﬂexive/one-to-many/many-to-one/many-to-many
relations, we propose a model which enables an entity to
have distributed representations when involved in different
relations. As illustrated in Figure 1, for a relation r, we
position the relation-speciﬁc translation vector dr in the
relation-speciﬁc hyperplane wr (the normal vector) rather
than in the same space of entity embeddings. Speciﬁcally,
for a triplet (h, r, t), the embedding h and t are ﬁrst
projected to the hyperplane wr. The projections are denoted
as h⊥and t⊥, respectively. We expect h⊥and t⊥can be
connected by a translation vector dr on the hyperplane
with low error if (h, r, t) is a golden triplet. Thus we
deﬁne a scoring function ∥h⊥+ dr −t⊥∥2
2 to measure
the plausibility that the triplet is incorrect. By restricting
∥wr∥2 = 1, it is easy to get
Then the score function is
fr(h, t) = ∥(h −w⊤
r hwr) + dr −(t −w⊤
The score is expected to be lower for a golden triplet and
higher for an incorrect triplet. We name this model TransH.
The model parameters are, all the entities’ embeddings,
i=1, all the relations’ hyperplanes and translation vectors, {(wr, dr)}|R|
In TransH, by introducing the mechanism of projecting to
the relation-speciﬁc hyperplane, it enables different roles of
an entity in different relations/triplets.
To encourage discrimination between golden triplets and incorrect triplets, we use the following margin-based ranking
(h′,r′,t′)∈∆′
[fr(h, t) + γ −fr′(h′, t′)]+,
where [x]+ ≜max(0, x), ∆is the set of positive (golden)
triplets, ∆′
(h,r,t) denotes the set of negative triplets constructed by corrupting (h, r, t), γ is the margin separating
positive and negative triplets. The next subsection will introduce the details of constructing ∆′
The following constraints are considered when we minimize the loss L:
∀e ∈E, ∥e∥2 ≤1, //scale
∀r ∈R, |w⊤
r dr|/∥dr∥2 ≤ϵ, //orthogonal
∀r ∈R, ∥wr∥2 = 1, //unit normal vector
where the constraint (2) guarantees the translation vector dr
is in the hyperplane. Instead of directly optimizing the loss
function with constraints, we convert it to the following unconstrained loss by means of soft constraints:
(h′,r′,t′)∈∆′
fr(h, t) + γ −fr′(h′, t′)
where C is a hyper-parameter weighting the importance of
soft constraints.
We adopt stochastic gradient descent (SGD) to minimize
the above loss function. The set of golden triplets (the
triplets from the knowledge graph) are randomly traversed
multiple times. When a golden triplet is visited, a negative
triplet is randomly constructed (according to the next section). After a mini-batch, the gradient is computed and the
model parameters are updated. Notice that the constraint (3)
is missed in Eq. (4). Instead, to satisfy constraint (3),
we project each wr to unit ℓ2-ball before visiting each
mini-batch.
Reducing False Negative Labels
As described in the previous section, training involves constructing negative triplets for a golden triplet. Previous methods simply get negative triplets by randomly corrupting the
golden triplet. For example, in TransE, for a golden triplet
(h, r, t), a negative triplet (h′, r, t′) is obtained by randomly
sampling a pair of entities (h′, t′) from E. However, as a
real knowledge graph is often far from completed, this way
of randomly sampling may introduce many false negative
labels into training.
We adopt a different approach for TransH. Basically, we
set different probabilities for replacing the head or tail entity
when corrupting the triplet, which depends on the mapping
property of the relation, i.e., one-to-many, many-to-one or
many-to-many. We tend to give more chance to replacing
the head entity if the relation is one-to-many and give more
chance to replacing the tail entity if the relation is manyto-one. In this way, the chance of generating false negative
labels is reduced. Speciﬁcally, among all the triplets of a
relation r, we ﬁrst get the following two statistics: (1) the
average number of tail entities per head entity, denoted as
tph; (2) the average number of head entities per tail entity, denoted as hpt. Then we deﬁne a Bernoulli distribution
with parameter
tph+hpt for sampling: given a golden triplet
(h, r, t) of the relation r, with probability
tph+hpt we corrupt the triplet by replacing the head, and with probability
tph+hpt we corrupt the triplet by replacing the tail.
Experiments
We empirically study and evaluate related methods on three
tasks: link prediction , triplets classiﬁcation , and relational fact extraction . All three tasks evaluate the accuracy of predicting unseen triplets, from different viewpoints
and application context.
Table 2: Data sets used in the experiments.
#Trip. (Train / Valid / Test)
19,193,556
Link Prediction
Used in , this task is to complete
a triplet (h, r, t) with h or t missing, i.e., predict t given
(h, r) or predict h given (r, t). Rather than requiring one
best answer, this task emphasizes more on ranking a set of
candidate entities from the knowledge graph.
We use the same two data sets which are used in
 : WN18, a subset of
Wordnet; FB15k, a relatively dense subgraph of Freebase
where all entities are present in Wikilinks database 1. Both
are released in . Please see Table 2 for
more details.
Evaluation protocol. We follow the same protocol in
TransE : For each testing triplet
(h, r, t), we replace the tail t by every entity e in the knowledge graph and calculate a dissimilarity score (according to
the scoring function fr) on the corrupted triplet (h, r, e) .
Ranking the scores in ascending order, we then get the rank
of the original correct triplet. Similarly, we can get another
rank for (h, r, t) by corrupting the head h. Aggregated over
all the testing triplets, two metrics are reported: the averaged rank (denoted as Mean), and the proportion of ranks
not larger than 10 (denoted as Hits@10). This is called the
“raw” setting. Notice that if a corrupted triplet exists in the
knowledge graph, as it is also correct, ranking it before the
original triplet is not wrong. To eliminate this factor, we remove those corrupted triplets which exist in either training,
valid, or testing set before getting the rank of each testing
triplet. This setting is called “ﬁlt”. In both settings, a lower
Mean is better while a higher Hits@10 is better.
Implementation. As the data sets are the same, we
directly copy experimental results of several baselines
from . In training TransH, we use
learning rate α for SGD among {0.001, 0.005, 0.01},
the margin γ among {0.25, 0.5, 1, 2}, the embedding dimension k among {50, 75, 100}, the weight C among
{0.015625, 0.0625, 0.25, 1.0}, and batch size B among
{20, 75, 300, 1200, 4800}. The optimal parameters are determined by the validation set. Regarding the strategy of
constructing negative labels, we use “unif” to denote the traditional way of replacing head or tail with equal probability,
and use “bern.” to denote reducing false negative labels by
replacing head or tail with different probabilities. Under the
“unif” setting, the optimal conﬁgurations are: α = 0.01,
γ = 1, k = 50, C = 0.25, and B = 75 on WN18;
α = 0.005, γ = 0.5, k = 50, C = 0.015625, and B = 1200
on FB15k. Under “bern” setting, the optimal conﬁgurations
1 
are: α = 0.01, γ = 1, k = 50, C = 0.25, and B = 1200
on WN18; α = 0.005, γ = 0.25, k = 100, C = 1.0, and
B = 4800 on FB15k. For both datasets, we traverse all the
training triplets for 500 rounds.
Results. The results are reported in Table 3. The simple models TransE, TransH, and even the naive baseline
Unstructured (i.e., TransE without translation) outperform
other approaches on WN18 in terms of the Mean metric.
This may be because the number of relations in WN18 is
quite small so that it is acceptable to ignore the different
types of relations. On FB15k, TransH consistently outperforms the counterparts. We hypothesize that the improvements are due to the relaxed geometric assumption compared with TransE so that the reﬂexive/one-to-many/manyto-one/many-to-many relations can be better handled. To
conﬁrm the point, we dig into the detailed results of different mapping categories of relations, as reported in Table 4. Within the 1,345 relations, 24% are one-to-one, 23%
are one-to-many, 29% are many-to-one, and 24% are manyto-many2. Overall, TransE is the runner up on FB15k. However, its relative superiorities on one-to-many and many-toone relations are not as good as those on one-to-one relations. TransH brings promising improvements to TransE
on one-to-many, many-to-one, and many-to-many relations.
Outstripping our expectations, the performance on one-toone is also signiﬁcantly improved (> 60%). This may be
due to the “graph” property: entities are connected with
relations so that better embeddings of some parts lead to
better results on the whole. Table 5 reports the results of
Hits@10 on some typical one-to-many/many-to-one/manyto-many/reﬂexive relations. The imrovement of TransH over
TransE on these relations are very promising.
Triplets Classiﬁcation
This task is to conﬁrm whether a given triplet (h, r, t) is correct or not, i.e., binary classiﬁcation on a triplet. It is used
in to evaluate NTN model.
Three data sets are used in this task. Two of them are
the same as in NTN : WN11, a subset of WordNet; FB13, a subset of Freebase. As WN11 and
FB13 contain very small number of relations, we also use
the FB15k data set which contains much more relations. See
Table 2 for details.
Evaluation protocol. We follow the same protocol in
NTN . Evaluation of classiﬁcation
needs negative labels. The released sets of WN11 and
FB13 already contain negative triplets which are constructed
by , where each golden triplet is corrupted to get one negative triplet. For FB15k, we construct
the negative triplets following the same procedure used for
FB13 in .
The decision rule for classiﬁcation is simple: for a triplet
(h, r, t), if the dissimilarity score (by the score function
2For each relation r, we compute averaged number of tails per
head (tphr), averaged number of head per tail (hptr). If tphr <
1.5 and hptr < 1.5, r is treated as one-to-one. If tphr ≥1.5
and hptr ≥1.5, r is treated as a many-to-many. If hptr < 1.5
and tphr ≥1.5, r is treated as one-to-many. If hptr ≥1.5 and
tphr < 1.5, r is treated as many-to-one.
Table 5: Hits@10 of TransE and TransH on some examples of one-to-many∗, many-to-one†, many-to-many‡, and
reﬂexive§ relations.
Hits@10 (TransE / TransH)
football position/players∗
16.7 / 22.2
production company/ﬁlms∗
65.6 / 85.6
9.3 / 16.0
director/ﬁlm∗
75.8 / 89.6
50.5 / 80.2
disease/treatments†
33.3 / 66.6
person/place of birth†
30.0 / 37.5
72.1 / 87.6
ﬁlm/production companies†
11.3 / 21.0
77.6 / 87.8
ﬁeld of study/students majoring‡24.5 / 66.0
28.3 / 62.3
award winner/awards won‡
40.2 / 87.5
42.8 / 86.6
sports position/players‡
28.6 / 100
64.3 / 86.2
person/sibling s§
21.1 / 63.2
21.1 / 36.8
person/spouse s§
18.5 / 35.2
18.5 / 42.6
fr) is below a relation-speciﬁc threshold σr, then predict
positive. Otherwise predict negative. The relation-speciﬁc
threshold σr is determined according to (maximizing) the
classiﬁcation accuracy on the validation set.
Implementation. For WN11 and FB13, as we use the
same data sets, directly copying the results of different methods from . For FB15k not used in , we implement TransE and TransH by ourselves,
and use the released code for NTN.
{0.001, 0.005, 0.01, 0.1}, margin γ
in {1.0, 2.0}, embedding dimension k in {20, 50, 100}, and batch size B in
{30, 120, 480, 1920}. We also apply the trick of reducing
false negative labels to TransE. The optimal conﬁgurations
of TransE (bern.) are: α = 0.01, k = 20, γ = 2.0,
B = 120, and L1 as dissimilarity on WN11; α = 0.001,
k = 100, γ = 2.0, B = 30, and L1 as dissimilarity on
FB13; α = 0.005, k = 100, γ = 2.0, B = 480, and L1
as dissimilarity on FB15k. For TransH, the search space of
hyperparameters is identical to link prediction. The optimal
hyperparameters of TransH (bern.) are: α = 0.01, k = 100,
γ = 2.0, C = 0.25, and B = 4800 on WN11; α = 0.001,
k = 100, γ = 0.25, C = 0.0625, and B = 4800 on
FB13; α = 0.01, k = 100, γ = 0.25, C = 0.0625, and
B = 4800 on FB15k. We didn’t change the conﬁguration of
NTN code on FB113 where dimension k = 100, number of
slices equals 3. Since FB15k is relatively large, we limit the
number of epochs to 500.
Results. Accuracies are reported in Table 6. On WN11,
TransH outperforms all the other methods. On FB13, the
powerful model NTN is indeed the best one. However, on
the larger set FB15k, TransE and TransH are much better
than NTN. Notice that the number (1,345) of relations of
FB15k is much larger than that (13) of FB13 while the
number of entities are close (see Table 2). This means
FB13 is a very dense subgraph where strong correlations
exist between entities. In this case, modeling the complex
correlations between entities by tensor and nonlinear
Table 3: Link prediction results
Unstructured 
RESCAL 
SE 
SME (Linear) 
SME (Bilinear) 
LFM 
TransE 
TransH (unif.)
TransH (bern.)
Table 4: Results on FB15k by relation category
Predicting left (HITS@10)
Predicting right (HITS@10)
Relation Category
Unstructured 
SE 
SME (Linear) 
SME (Bilinear) 
TransE 
TransH (unif.)
TransH (bern.)
transformation helps with embedding. However, on the
much sparser subgraph of FB15k, it seems the simple
assumption of translation or translation on hyperplanes is
enough while the complex model of NTN is not necessary.
Concerning running time, the cost of NTN is much higher
than TransE/TransH. In addition, on all the three data sets,
the trick of reducing false negative labeling (the results with
“bern.”) helps both TransE and TransH.
In NTN , the results of combining
it with word embedding are also
reported. However, how best to combine word embedding
is model dependent and also an open problem that goes
beyond the scope of this paper. For a clear and fair comparison, all the results in Table 6 are without combination with
word embedding.
Relational Fact Extraction from Text
Extracting
relational
important channel for enriching a knowledge graph. Most
extracting
Riedel, Yao, and McCallum 2010; Hoffmann et al. 2011;
Surdeanu et al. 2012) distantly collect evidences from
an external text corpus for a candidate fact, ignoring the
capability of the knowledge graph itself to reason the new
fact. Actually, knowledge graph embedding is able to score
a candidate fact, without observing any evidence from external text corpus. Recently combined
the score from TransE (evidence from knowledge graphs)
with the score from a text side extraction model (evidence
Table 6: Triplet classiﬁcation: accuracies (%). “40h”, “5m”
and “30m” in the brackets are the running (wall clock) time.
Distant Model
Hadamard Model
Single Layer Model
Bilinear Model
66.5 (≈40h)
TransE (unif.)
79.7 (≈5m)
TransE (bern.)
87.3 (≈5m)
TransH (unif.)
80.2 (≈30m)
TransH (bern.)
87.7 (≈30m)
from text corpus) and observed promising improvement.
In this experiment, we compare the contribution of TransH
and TransE to improve relational fact extraction.
This experiment involves two major parts: text side extraction model and knowledge graph embedding.
For text side, we use the same data set in —NYT+FB 3 released by . They aligned Freebase relations with the New
York Times corpus by tagging entities in text using Stanford NER and linking
them to Freebase IDs through string matching on names. We
only consider the most popular 50 predicates in the data set
3 
TransH (unif.)
TransH (bern.)
TransH (unif.)
TransH (bern.)
Figure 2: Precision-recall curves of TransE/TransH for fact extraction. (a) Combining the score from TransE/TransH and the
score from Sm2r using the same rule in . (b) On the candidate facts accepted by Sm2r, we only use the
score from TransE/TransH for prediction.
including the negative class—“NA”. Then the data set is split
into two parts: one for training, another for testing. As to the
text side extraction method, both TransE and TransH can be
used to provide prior scores for any text side methods. For a
clear and fair comparison with TransE reported in , we implement the same text side method Wsabie M2R in , which is denoted as Sm2r
in this paper.
For knowledge graph embedding, 
used a subset of Freebase consisting of the most popular 4M
entities and all the 23k Freebase relations. As they have not
released the subset used in their experiment, we follow a
similar procedure to produce a subset FB5M (Table 2) from
Freebase. What is important is, we remove all the entity
pairs that appear in the testing set from FB5M so that the
generalization testing is not fake. We choose parameters for
TransE/TransH without a comprehensive search due to the
scale of FB5M. For simplicity, in both TransE and TransH,
we set the embedding dimension k to be 50, the learning rate
for SGD α to 0.01, the margin γ to 1.0, and dissimilarity of
TransE to L2.
Following the same rule of combining the score from
knowledge graph embedding with the score from the text
side model, we can obtain the precision-recall curves for
TransE and TransH, as shown in Figure 2 (a). From the
ﬁgure we can see TransH consistently outperforms TransE
as a “prior” model on improving the text side extraction
method Sm2r.
The results in Figure 2 (a) depend on the speciﬁc rule of
combining the score from knowledge graph embedding with
the score from text side model. Actually the combining rule
in is quite ad-hoc, which may not be
the best way. Thus Figure 2 (a) does not clearly demonstrate
the separate capability of TransE/TransH as a stand-alone
model for relational fact prediction. To clearly demonstrate
the stand-alone capability of TransE/TransH, we ﬁrst use the
text side model Sm2r to assign each entity pair to the relation with the highest conﬁdence score, then keep those facts
where the assigned relation is not “NA”. On these accepted
candidate facts, we only use the score of TransE/TransH
to predict. The results are illustrated in Figure 2 (b). Both
TransE and TransH perform better than the text side model
Sm2r on this subset of candidates. TransH performs much
better than TransE when recall is higher than 0.6.
Conclusion
In this paper, we have introduced TransH, a new model
to embed a knowledge graph in a continuous vector
space. TransH overcomes the ﬂaws of TransE concerning
the reﬂexive/one-to-many/many-to-one/many-to-many relations while inheriting its efﬁciency. Extensive experiments
on the tasks of link prediction, triplet classiﬁcation, and relational fact extraction show that TransH brings promising improvements to TransE. The trick of reducing false negative
labels proposed in this paper is also proven to be effective.