Interpretable Convolutional Neural Networks
Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu
University of California, Los Angeles
This paper proposes a method to modify traditional
convolutional neural networks (CNNs) into interpretable
CNNs, in order to clarify knowledge representations in high
conv-layers of CNNs. In an interpretable CNN, each ﬁlter in a high conv-layer represents a speciﬁc object part.
Our interpretable CNNs use the same training data as ordinary CNNs without a need for additional annotations of
object parts or textures for supervision. The interpretable
CNN automatically assigns each ﬁlter in a high conv-layer
with an object part during the learning process. We can
apply our method to different types of CNNs with various
structures. The explicit knowledge representation in an interpretable CNN can help people understand logic inside
a CNN, i.e. what patterns are memorized by the CNN for
prediction. Experiments have shown that ﬁlters in an interpretable CNN are more semantically meaningful than those
in traditional CNNs.1.
1. Introduction
Convolutional neural networks (CNNs) have
achieved superior performance in many visual tasks, such
as object classiﬁcation and detection.
As discussed in
Bau et al. , besides the discrimination power, model interpretability is another crucial issue for neural networks.
However, the interpretability is always an Achilles’ heel
of CNNs, and has presented considerable challenges for
In this paper, we focus on a new problem, i.e. without
any additional human supervision, can we modify a CNN to
obtain interpretable knowledge representations in its convlayers? We expect the CNN has a certain introspection of
its representations during the end-to-end learning process,
so that the CNN can regularize its representations to ensure
high interpretability. Our learning for high interpretability
is different from conventional off-line visualization and diagnosis of pre-trained
CNN representations.
1The code is available at 
interpretableCNN
Conv-layer 1
Conv-layer L-1
Conv-layer L
Several fullyconnected layers
Feature maps of a certain filter in a high
conv-layer computed using different images
Feature maps of an interpretable filter
Feature maps of an ordinary filter
Figure 1. Comparison of a ﬁlter’s feature maps in an interpretable
CNN and those in a traditional CNN.
Bau et al. deﬁned six kinds of semantics in CNNs,
i.e. objects, parts, scenes, textures, materials, and colors.
In fact, we can roughly consider the ﬁrst two semantics as
object-part patterns with speciﬁc shapes, and summarize the
last four semantics as texture patterns without clear contours. Moreover, ﬁlters in low conv-layers usually describe
simple textures, whereas ﬁlters in high conv-layers are more
likely to represent object parts.
Therefore, in this study, we aim to train each ﬁlter in a
high conv-layer to represent an object part. Fig. 1 shows the
difference between a traditional CNN and our interpretable
CNN. In a traditional CNN, a high-layer ﬁlter may describe
a mixture of patterns, i.e. the ﬁlter may be activated by both
the head part and the leg part of a cat. Such complex representations in high conv-layers signiﬁcantly decrease the
network interpretability. In contrast, the ﬁlter in our interpretable CNN is activated by a certain part. In this way, we
can explicitly identify which object parts are memorized in
the CNN for classiﬁcation without ambiguity. The goal of
this study can be summarized as follows.
• We propose to slightly revise a CNN to improve its
interpretability, which can be broadly applied to CNNs
with different structures.
• We do not need any annotations of object parts or textures for supervision. Instead, our method automatically pushes the representation of each ﬁlter towards
an object part.
• The interpretable CNN does not change the loss function on the top layer and uses the same training sam-
 
ples as the original CNN.
• As an exploratory research, the design for interpretability may decrease the discrimination power a
bit, but we hope to limit such a decrease within a small
Methods: Given a high conv-layer in a CNN, we propose a simple yet effective loss for each ﬁlter in the convlayer to push the ﬁlter towards the representation of an object part. As shown in Fig. 2, we add a loss for the output
feature map of each ﬁlter. The loss encourages a low entropy of inter-category activations and a low entropy of spatial distributions of neural activations. I.e. each ﬁlter must
encode a distinct object part that is exclusively contained by
a single object category, and the ﬁlter must be activated by
a single part of the object, rather than repetitively appear on
different object regions. For example, the left eye and the
right eye may be represented using two different part ﬁlters,
because contexts of the two eyes are symmetric, but not the
same. Here, we assume that repetitive shapes on various
regions are more prone to describe low-level textures (e.g.
colors and edges), instead of high-level parts.
The value of network interpretability: The clear semantics in high conv-layers is of great importance when we
need human beings to trust a network’s prediction. In spite
of the high accuracy of neural networks, human beings usually cannot fully trust a network, unless it can explain its
logic for decisions, i.e. what patterns are memorized for
prediction. Given an image, current studies for network diagnosis localize image regions that contribute
most to network predictions at the pixel level. In this study,
we expect the CNN to explain its logic at the object-part
level. Given an interpretable CNN, we can explicitly show
the distribution of object parts that are memorized by the
CNN for object classiﬁcation.
Contributions: In this paper, we focus on a new task,
i.e. end-to-end learning a CNN whose representations in
high conv-layers are interpretable. We propose a simple yet
effective method to modify different types of CNNs into
interpretable CNNs without any additional annotations of
object parts or textures for supervision. Experiments show
that our approach has signiﬁcantly improved the object-part
interpretability of CNNs.
2. Related work
The interpretability and the discrimination power are two
important properties of a model . In recent years, different methods are developed to explore the semantics hidden
inside a CNN. Many statistical methods have
been proposed to analyze CNN features.
Network visualization:
Visualization of ﬁlters in a
CNN is the most direct way of exploring the pattern hidden
inside a neural unit. showed the appearance
that maximized the score of a given unit. up-convolutional
nets were used to invert CNN feature maps to images.
Pattern retrieval:
Some studies go beyond passive visualization and actively retrieve certain units from CNNs for
different applications. Like the extraction of mid-level features from images, pattern retrieval mainly learns midlevel representations from conv-layers. Zhou et al. 
selected units from feature maps to describe “scenes”. Simon et al. discovered objects from feature maps of unlabeled images , and selected a certain ﬁlter to describe
each semantic part in a supervised fashion . extracted certain neural units from a ﬁlter’s feature map to describe an object part in a weakly-supervised manner. 
used a gradient-based method to interpret visual questionanswering models. Studies of selected neural units with speciﬁc meanings from CNNs for various applications.
Model diagnosis:
Many methods have been developed to diagnose representations of a black-box model.
The LIME method proposed by Ribeiro et al. , inﬂuence functions and gradient-based visualization methods and extracted image regions that were responsible for each network output, in order to interpret
network representations. These methods require people to
manually check image regions accountable for the label prediction for each testing image. extracted relationships
between representations of various categories from a CNN.
Lakkaraju et al. and Zhang et al. explored unknown knowledge of CNNs via active annotations and active question-answering. In contrast, given an interpretable
CNN, people can directly identify object parts (ﬁlters) that
are used for decisions during the inference procedure.
Learning a better representation: Unlike the diagnosis and/or visualization of pre-trained CNNs, some approaches are developed to learn more meaningful representations. required people to label dimensions of the input that were related to each output, in order to learn a better model. Hu et al. designed some logic rules for network outputs, and used these rules to regularize the learning process. Stone et al. learned CNN representations
with better object compositionality, but they did not obtain explicit part-level or texture-level semantics. Sabour et
al. proposed a capsule model, which used a dynamic
routing mechanism to parse the entire object into a parsing
tree of capsules, and each capsule may encode a speciﬁc
meaning. In this study, we invent a generic loss to regularize the representation of a ﬁlter to improve its interpretability. We can analyze the interpretable CNN from the perspective of information bottleneck as follows. 1) Our
interpretable ﬁlters selectively model the most distinct parts
of each category to minimize the conditional entropy of the
ﬁnal classiﬁcation given feature maps of a conv-layer. 2)
Each ﬁlter represents a single part of an object, which max-
Loss for filter 1
Loss for filter 2
Loss for filter 3
Traditional
Conv-layer
Interpretable
Conv-layer
Figure 2. Structures of an ordinary conv-layer and an interpretable
conv-layer. Green and red lines indicate the forward and backward
propagations, respectively.
imizes the mutual information between the input image and
middle-layer feature maps (i.e. “forgetting” as much irrelevant information as possible).
3. Algorithm
Given a target conv-layer of a CNN, we expect each
ﬁlter in the conv-layer to be activated by a certain object
part of a certain category, and keep inactivated on images
of other categories. Let I denote a set of training images,
where Ic ⊂I represents the subset that belongs to category c, (c = 1, 2, . . . , C). Theoretically, we can use different
types of losses to learn CNNs for multi-class classiﬁcation,
single-class classiﬁcation (i.e. c = 1 for images of a category and c = 2 for random images), and other tasks.
Fig. 2 shows the structure of our interpretable conv-layer.
In the following paragraphs, we focus on the learning of a
single ﬁlter f in the target conv-layer. We add a loss to
the feature map x of the ﬁlter f after the ReLu operation.
The feature map x is an n × n matrix, xij ≥0. Because
f’s corresponding object part may appear at different locations in different images, we design n2 templates for f
Tµ1, Tµ2, . . . , Tµn2 }. As shown in Fig. 3, each template Tµi
is also an n × n matrix, and it describes the ideal distribution of activations for the feature map x when the target part
mainly triggers the i-th unit in x.
During the forward propagation, given each input image I, the CNN selects a speciﬁc template Tˆµ from the
n2 template candidates as a mask to ﬁlter out noisy activations from x. I.e. we compute ˆµ = argmax[i,j]xij and
xmasked = max{x ◦Tˆµ, 0}, where ◦denotes the Hadamard
(element-wise) product. µ = [i, j], 1 ≤i, j ≤n denotes the
unit (or location) in x potentially corresponding to the part.
backpropagation for end-to-end learning. Note that the CNN
may select different templates for different input images.
Fig. 4 visualizes the masks Tˆµ chosen for different images,
as well as the original and masked feature maps.
During the back-propagation process, our loss pushes
Figure 3. Templates of Tµi. In fact, the algorithm also supports a
round template based on the L-2 norm distance. Here, we use the
L-1 norm distance instead to speed up the computation.
ﬁlter f to represent a speciﬁc object part of the category c
and keep silent on images of other categories. Please see
Section 3.1 for the determination of the category c for ﬁlter
f. Let X = {x|x = f(I), I ∈I} denote feature maps of f
after an ReLU operation, which are computed on different
training images. Given an input image I, if I ∈Ic, we
expect the feature map x = f(I) to exclusively activated at
the target part’s location; otherwise, the feature map keeps
inactivated. In other words, if I ∈Ic, the feature map x is
expected to the assigned template Tˆµ; if I ̸∈Ic, we design a
negative template T −and hope the feature map x matches to
T −. Note that during the forward propagation, our method
omits the negative template, and all feature maps, including
those of other categories, select positive templates as masks.
Thus, each feature map is supposed to be well ﬁt
to one of all the n2 + 1 template candidates T
{T −, Tµ1, Tµ2, . . . , Tµn2 }. We formulate the loss for f as
the mutual information between X and T.
Lossf = −MI(X; T)
for ﬁlter f
p(x|T) log p(x|T)
The prior probability of a template is given as p(Tµ) =
n2 , p(T −) = 1 −α, where α is a constant prior likelihood.
The ﬁtness between a feature map x and a template T is
measured as the conditional likelihood p(x|T).
where ZT = P
x∈X exp(tr(x · T)). x · T indicates the multiplication between x and T; tr(·) indicates the trace of a
matrix, and tr(x · T) = P
ij xijtij. p(x) = P
T p(T)p(x|T).
Part templates: As shown in Fig. 3, a negative template
is given as T −= (t−
ij = −τ < 0, where τ is a positive
constant. A positive template corresponding to µ is given
as Tµ =(t+
ij =τ · max(1 −β ∥[i,j]−µ∥1
, −1), where ∥· ∥1
denotes the L-1 norm distance; β is a constant parameter.
Map after mask
Receptive field
Figure 4. Given an input image I, from the left to the right, we
consequently show the feature map of a ﬁlter after the ReLU layer
x, the assigned mask Tˆµ, the masked feature map xmasked, and the
image-resolution RF of activations in xmasked computed by .
3.1. Learning
We train the interpretable CNN via an end-to-end manner. During the forward-propagation process, each ﬁlter in
the CNN passes its information in a bottom-up manner, just
like traditional CNNs. During the back-propagation process, each ﬁlter in an interpretable conv-layer receives gradients w.r.t. its feature map x from both the ﬁnal task loss
k) and the local ﬁlter loss Lossf, as follows:
∂L(ˆyk, y∗
where λ is a weight.
We compute gradients of Lossf w.r.t. each element xij
of feature map x as follows2.
p(T)tijetr(x·T )n
tr(x · T)−log
≈p( ˆT)ˆtij
tr(x · ˆT) −log Z ˆ
T −log p(x)
where ˆT is the target template for feature map x. If the
given image I belongs to the target category of ﬁlter f, then
ˆT = Tˆµ, where ˆµ = argmax[i,j]xij. If image I belongs to
other categories, then ˆT = T −. Considering ∀T ∈T \ { ˆT},
T ) ≫etr(x·T ) after initial learning episodes, we make
the above approximation to simplify the computation. Because ZT is computed using numerous feature maps, we can
roughly treat ZT as a constant to compute gradients computation in the above equation. We gradually update the value
of ZT during the training process3. Similarly, we can also
approximate p(x) without huge computation3.
2Please see the proof in the Appendix.
3We can use a subset of feature maps to approximate the value of
ZT , and continue to update ZT when we receive more feature maps
during the training process. Similarly, we can approximate p(x) using
a subset of feature maps.
We compute p(x) = P
T p(T)p(x|T) =
T p(T) exp[tr(x·T )]
T p(T)meanx
exp[tr(x·T )]
Determining the target category for each ﬁlter: We
need to assign each ﬁlter f with a target category ˆc to approximate gradients in Eqn. (4). We simply assign the ﬁlter
f with the category ˆc whose images activate f most, i.e.
ˆc = argmaxcmeanx=f(I):I∈Ic
4. Understanding of the loss
In fact, the loss in Eqn. (1) can be re-written as2
Lossf = −H(T) + H(T′ = {T −, T+}|X)
p(T+, x)H(T+|X = x)
T ∈T p(T) log p(T)
the prior entropy of part templates.
Low inter-category entropy: The second term H(T′ =
{T −,T+}|X) is computed as
H(T′ ={T −,T+}|X) = −
T ∈{T −,T+}
p(T|x) log p(T|x)
{Tµ1, Tµ2, . . . , Tµn2 }
T, p(T+|x)
µ p(Tµ|x). This term encourages a low conditional entropy of inter-category activations, i.e. a well-learned ﬁlter
f needs to be exclusively activated by a certain category c
and keep silent on other categories. We can use a feature
map x of f to identify whether the input image belongs to
category c or not, i.e. x ﬁtting to either Tˆµ or T −, without great uncertainty. Here, we deﬁne the set of all positive
templates T+ as a single label to represent category c. We
use the negative template T −to denote other categories.
Low spatial entropy: The third term in Eqn. (5) is given as
H(T+|X =x) =
˜p(Tµ|x) log ˜p(Tµ|x)
where ˜p(Tµ|x) =
p(T+|x). This term encourages a low conditional entropy of spatial distribution of x’s activations. I.e.
given an image I ∈Ic, a well-learned ﬁlter should only be
activated by a single region ˆµ of the feature map x, instead
of repetitively appearing at different locations.
5. Experiments
In experiments, to demonstrate the broad applicability,
we applied our method to CNNs with four types of structures. We used object images in three different benchmark
datasets to learn interpretable CNNs for single-category
classiﬁcation and multi-category classiﬁcation. We visualized feature maps of ﬁlters in interpretable conv-layers to
illustrate semantic meanings of these ﬁlters. We used two
types of metrics, i.e. the object-part interpretability and the
location stability, to evaluate the clarity of the part semantics of a convolutional ﬁlter. Experiments showed that ﬁlters
in our interpretable CNNs were much more semantically
meaningful than those in ordinary CNNs.
Three benchmark datasets:
Because we needed
ground-truth annotations of object landmarks4 (parts) to
evaluate the semantic clarity of each ﬁlter, we chose three
benchmark datasets with landmark4/part annotations for
training and testing, including the ILSVRC 2013 DET
Animal-Part dataset , the CUB200-2011 dataset ,
and the Pascal VOC Part dataset . As discussed in ,
non-rigid parts of animal categories usually present great
challenges for part localization. Thus, we followed 
to select the 37 animal categories in the three datasets for
evaluation.
All the three datasets provide ground-truth bounding
boxes of entire objects.
For landmark annotations, the
ILSVRC 2013 DET Animal-Part dataset contains
ground-truth bounding boxes of heads and legs of 30 animal categories. The CUB200-2011 dataset contains a
total of 11.8K bird images of 200 species, and the dataset
provides center positions of 15 bird landmarks. The Pascal
VOC Part dataset contain ground-truth part segmentations of 107 object landmarks in six animal categories.
Four types of CNNs: To demonstrate the broad applicability of our method, we modiﬁed four typical CNNs, i.e.
the AlexNet , the VGG-M , the VGG-S , the
VGG-16 , into interpretable CNNs. Considering that
skip connections in residual networks usually make a
single feature map encode patterns of different ﬁlters, in
this study, we did not test the performance on residual networks to simplify the story. Given a certain CNN structure,
we modiﬁed all ﬁlters in the top conv-layer of the original network into interpretable ones. Then, we inserted a
new conv-layer with M ﬁlters above the original top convlayer, where M is the channel number of the input of the
new conv-layer. We also set ﬁlters in the new conv-layer as
interpretable ones. Each ﬁlter was a 3 × 3 × M tensor with
a bias term. We added zero padding to input feature maps
to ensure that output feature maps were of the same size as
the input.
Implementation details: We set parameters as τ = 0.5
1+n2 , and β = 4. We updated weights of ﬁlter losses
w.r.t. magnitudes of neural activations in an online manner,
λ = 5 × 10−6meanx∈X maxi,j xij. We initialized parameters
of fully-connected (FC) layers and the new conv-layer, and
loaded parameters of other conv-layers from a traditional
CNN that was pre-trained using 1.2M ImageNet images in
 . We then ﬁne-tuned the interpretable CNN using
training images in the dataset. To enable a fair comparison,
traditional CNNs were also ﬁne-tuned by initializing FClayer parameters and loading conv-layer parameters.
4To avoid ambiguity, a landmark is referred to as the central position
of a semantic part (a part with an explicit name, e.g. a head, a tail). In
contrast, the part corresponding to a ﬁlter does not have an explicit name.
5.1. Experiments
Single-category classiﬁcation: We learned four types of
interpretable CNNs based on the AlexNet, VGG-M, VGG-
S, and VGG-16 structures to classify each category in the
ILSVRC 2013 DET Animal-Part dataset , the CUB200-
2011 dataset , and the Pascal VOC Part dataset . Besides, we also learned ordinary AlexNet, VGG-M, VGG-
S, and VGG-16 networks using the same training data
for comparison. We used the logistic log loss for singlecategory classiﬁcation. Following experimental settings in
 , we cropped objects of the target category based
on their bounding boxes as positive samples with groundtruth labels y∗=+1. We regarded images of other categories
as negative samples with ground-truth labels y∗=−1.
Multi-category classiﬁcation: We used the six animal categories in the Pascal VOC Part dataset and the
thirty categories in the ILSVRC 2013 DET Animal-Part
dataset respectively, to learn CNNs for multi-category
classiﬁcation. We learned interpretable CNNs based on the
VGG-M, VGG-S, and VGG-16 structures. We tried two
types of losses, i.e. the softmax log loss and the logistic log
loss5 for multi-class classiﬁcation.
5.2. Quantitative evaluation of part interpretability
As discussed in , ﬁlters in low conv-layers usually
represent simple patterns or object details (e.g. edges, simple textures, and colors), whereas ﬁlters in high conv-layers
are more likely to represent complex, large-scale parts.
Therefore, in experiments, we evaluated the clarity of part
semantics for the top conv-layer of a CNN. We used the
following two metrics for evaluation.
Evaluation metric: part interpretability
We followed the metric proposed by Bau et al. to measure the object-part interpretability of ﬁlters. We brieﬂy
introduce this evaluation metric as follows. For each ﬁlter f, we computed its feature maps X after ReLu/mask
operations on different input images. Then, the distribution of activation scores in all positions of all feature maps
was computed. set an activation threshold Tf such that
p(xij > Tf) = 0.005, so as to select top activations from all
spatial locations [i, j] of all feature maps x ∈X as valid map
regions corresponding to f’s semantics. Then, scaled up
low-resolution valid map regions to the image resolution,
thereby obtaining the receptive ﬁeld (RF)6 of valid activa-
5We considered the output yc for each category c independent to outputs for other categories, thereby a CNN making multiple independent
single-class classiﬁcations for each image. Table 7 reported the average
accuracy of the multiple classiﬁcation outputs of an image.
6Note that accurately computes the RF when the ﬁlter represents
an object part, and we used RFs computed by for ﬁlter visualization in
Fig. 5. However, when a ﬁlter in an ordinary CNN does not have consistent
contours, it is difﬁcult for to align different images to compute an
0.332 0.363 0.340 0.374 0.308 0.373 0.348
AlexNet, interpretable
0.770 0.565 0.618 0.571 0.729 0.669 0.654
0.519 0.458 0.479 0.534 0.440 0.542 0.495
VGG-16, interpretable
0.818 0.653 0.683 0.900 0.795 0.772 0.770
0.357 0.365 0.347 0.368 0.331 0.373 0.357
VGG-M, interpretable
0.821 0.632 0.634 0.669 0.736 0.756 0.708
0.251 0.269 0.235 0.275 0.223 0.287 0.257
VGG-S, interpretable
0.526 0.366 0.291 0.432 0.478 0.251 0.390
Table 1. Average part interpretability of ﬁlters in CNNs for singlecategory classiﬁcation using the Pascal VOC Part dataset .
tions on each image. The RF on image I, denoted by SI
described the part region of f.
The compatibility between each ﬁlter f and the k-th part
on image I was reported as an intersection-over-union score
k∥, where SI
k denotes the ground-truth mask
of the k-th part on image I. Given an image I, we associated ﬁlter f with the k-th part if IoU I
f,k > 0.2.
that the criterion of IoU I
f,k > 0.2 for part association is
much stricter than IoU I
f,k > 0.04 that was used in . It
is because compared to other CNN semantics discussed
in (such as colors and textures), object-part semantics
requires a stricter criterion.
We computed the probability of the k-th part being associating with the ﬁlter f as
Pf,k = meanI:with k-th part1(IoU I
f,k > 0.2). Note that one ﬁlter might be associated with multiple object parts in an image. Among all parts, we reported the highest probability of part association as the interpretability of ﬁlter f, i.e.
Pf = maxk Pf,k.
For single-category classiﬁcation, we used testing images of the target category for evaluation. In the Pascal
VOC Part dataset , we used four parts for the bird category. We merged ground-truth regions of the head, beak,
and l/r-eyes as the head part, merged regions of the torso,
neck, and l/r-wings as the torso part, merged regions of l/rlegs/feet as the leg part, and used tail regions as the fourth
part. We used ﬁve parts for the cat category. We merged
regions of the head, l/r-eyes, l/r-ears, and nose as the head
part, merged regions of the torso and neck as the torso part,
merged regions of frontal l/r-legs/paws as the frontal legs,
merged regions of back l/r-legs/paws as the back legs, and
used the tail as the ﬁfth part. We used four parts for the cow
category, which were deﬁned in a similar way to the cat category. We added l/r-horns to the head part and omitted the
tail part. We applied ﬁve parts of the dog category in the
same way as the cat category. We applied four parts of both
the horse and sheep categories in the same way as the cow
category. We computed the average part interpretability Pf
average RF. Thus, for ordinary CNNs, we simply used a round RF for
each valid activation. We overlapped all activated RFs in a feature map
to compute the ﬁnal RF as mentioned in . For a fair comparison, in
Section , we uniformly applied these RFs to both interpretable CNNs and
ordinary CNNs.
Logistic log loss5
Softmax log loss
VGG-16, interpretable
VGG-M, interpretable
VGG-S, interpretable
Table 2. Average part interpretability of ﬁlters in CNNs that are
trained for multi-category classiﬁcation.
Filters in our interpretable CNNs exhibited signiﬁcantly better part interpretability
than other CNNs in all comparisons.
over all ﬁlters for evaluation.
For multi-category classiﬁcation, we ﬁrst assigned
each ﬁlter f with a target category ˆc, i.e.
the category
that activated the ﬁlter most ˆc=argmaxcmeanx:I∈Ic
Then, we computed the object-part interpretability using
images of category ˆc, as introduced above.
Evaluation metric: location stability
The second metric measures the stability of part locations,
which was proposed in . Given a feature map x of ﬁlter f, we regarded the unit ˆµ with the highest activation
as the location inference of f. We assumed that if f consistently represented the same object part through different
objects, then distances between the inferred part location ˆµ
and some object landmarks4 should not change a lot among
different objects. For example, if f represented the shoulder, then the distance between the shoulder and the head
should keep stable through different objects.
Therefore, computed the deviation of the distance
between the inferred position ˆµ and a speciﬁc ground-truth
landmark among different images, and used the average deviation w.r.t. various landmark to evaluate the location stability of f. A smaller deviation indicates a higher location
stability. Let dI(pk, ˆµ) = ∥pk−p(ˆµ)∥
w2+h2 denote the normalized
distance between the inferred part and the k-th landmark
pk on image I, where p(ˆµ) denotes the center of the unit
ˆµ’s RF when we backward propagated the RF to the image
w2 + h2 denotes the diagonal length of the input
image. We computed Df,k =
varI[dI(pk, ˆµ)] as the relative location deviation of ﬁlter f w.r.t. the k-th landmark,
where varI[dI(pk, ˆµ)] is referred to as the variation of the
distance dI(pk, ˆµ). Because each landmark could not appear
in all testing images, for each ﬁlter f, we only used inference results with the top-100 highest activation scores xˆµ
on images containing the k-th landmark to compute Df,k.
Thus, we used the average of relative location deviations
of all the ﬁlters in a conv-layer w.r.t. all landmarks, i.e.
meanfmeanK
k=1Df,k, to measure the location instability of f,
where K denotes the number of landmarks.
More speciﬁcally, object landmarks for each category
were selected as follows.
For the ILSVRC 2013 DET
0.161 0.167
0.153 0.175 0.128 0.123
0.143 0.148
AlexNet, interpretable
0.084 0.095
0.107 0.097 0.079 0.077
0.087 0.095
0.153 0.156
0.150 0.170 0.127 0.126
0.137 0.148
VGG-16, interpretable
0.076 0.099
0.115 0.113 0.070 0.084
0.069 0.086
0.161 0.166
0.153 0.176 0.128 0.125
0.145 0.150
VGG-M, interpretable
0.088 0.088
0.108 0.099 0.080 0.074
0.082 0.103
0.158 0.166
0.151 0.173 0.127 0.124
0.142 0.148
VGG-S, interpretable
0.087 0.101
0.107 0.096 0.084 0.078
0.082 0.101
0.152 0.154
0.141 0.144 0.155 0.147
0.159 0.160
AlexNet, interpretable
0.098 0.084
0.089 0.097 0.101 0.085
0.104 0.095
0.150 0.153
0.140 0.140 0.150 0.144
0.154 0.163
VGG-16, interpretable
0.106 0.077
0.083 0.102 0.097 0.091
0.093 0.100
0.151 0.158
0.140 0.143 0.155 0.146
0.160 0.161
VGG-M, interpretable
0.095 0.080
0.084 0.092 0.094 0.077
0.102 0.093
0.149 0.155
0.140 0.141 0.155 0.143
0.158 0.157
VGG-S, interpretable
0.096 0.080
0.088 0.094 0.101 0.077
0.105 0.094
Table 3. Location instability of ﬁlters (Ef,k[Df,k]) in CNNs that are trained for single-category classiﬁcation using the ILSVRC 2013 DET
Animal-Part dataset . Filters in our interpretable CNNs exhibited signiﬁcantly lower localization instability than ordinary CNNs in all
comparisons. Please see supplementary materials for performance of other structural modiﬁcations of CNNs.
0.153 0.131 0.141 0.128 0.145 0.140 0.140
AlexNet, interpretable
0.090 0.089 0.090 0.088 0.087 0.088 0.088
0.145 0.133 0.146 0.127 0.143 0.143 0.139
VGG-16, interpretable
0.101 0.098 0.105 0.074 0.097 0.100 0.096
0.152 0.132 0.143 0.130 0.145 0.141 0.141
VGG-M, interpretable
0.086 0.094 0.090 0.087 0.084 0.084 0.088
0.152 0.131 0.141 0.128 0.144 0.141 0.139
VGG-S, interpretable
0.089 0.092 0.092 0.087 0.086 0.088 0.089
Table 4. Location instability of ﬁlters (Ef,k[Df,k]) in CNNs that
are trained for single-category classiﬁcation using the Pascal VOC
Part dataset . Filters in our interpretable CNNs exhibited signiﬁcantly lower localization instability than ordinary CNNs in all
comparisons. Please see supplementary materials for performance
of other structural modiﬁcations of CNNs.
Animal-Part dataset , we used the head and frontal legs
of each category as landmarks for evaluation. For the Pascal
VOC Part dataset , we selected the head, neck, and torso
of each category as the landmarks. For the CUB200-2011
dataset , we used ground-truth positions of the head,
back, tail of birds as landmarks. It was because these landmarks appeared on testing images most frequently.
For multi-category classiﬁcation, we needed to determine two terms for each ﬁlter f, i.e. 1) the category that
f mainly represented and 2) the relative location deviation
Df,k w.r.t. landmarks in f’s target category. Because ﬁlters
in ordinary CNNs did not exclusively represent a single category, we simply assigned ﬁlter f with the category whose
landmarks can achieve the lowest location deviation to simplify the computation. I.e. we used the average location
deviation meanf minc meank∈P artcDf,k to evaluate the location stability, where Partc denotes the set of part indexes
Avg. location instability
AlexNet, interpretable
VGG-16, interpretable
VGG-M, interpretable
VGG-S, interpretable
Table 5. Location instability of ﬁlters (Ef,k[Df,k]) in CNNs
for single-category classiﬁcation based on the CUB200-2011
dataset . Please see supplementary materials for performance
of other structural modiﬁcations on ordinary CNNs.
ILSVRC Part 
Pascal VOC Part 
Logistic log loss5
Logistic log loss5
Softmax log loss
interpretable
interpretable
interpretable
Table 6. Location instability of ﬁlters (Ef,k[Df,k]) in CNNs that
are trained for multi-category classiﬁcation. Filters in our interpretable CNNs exhibited signiﬁcantly lower localization instability than ordinary CNNs in all comparisons.
belonging to category c.
Experimental results and analysis
Tables 1 and 2 compare part interpretability of CNNs for
single-category classiﬁcation and that of CNNs for multicategory classiﬁcation, respectively. Tables 3, 4, and 5 list
Ordinary CNNs
Interpretable CNNs
Figure 5. Visualization of ﬁlters in top conv-layers. We used to estimate the image-resolution receptive ﬁeld of activations in a feature
map to visualize a ﬁlter’s semantics. The top four rows visualize ﬁlters in interpretable CNNs, and the bottom two rows correspond to ﬁlters
in ordinary CNNs. We found that interpretable CNNs usually encoded head patterns of animals in its top conv-layer for classiﬁcation.
Figure 6. Heat maps for distributions of object parts that are encoded in interpretable ﬁlters. We use all ﬁlters in the top convlayer to compute the heat map.
average relative location deviations of CNNs for singlecategory classiﬁcation. Table 6 compares average relative
location deviations of CNNs for multi-category classiﬁcation. Our interpretable CNNs exhibited much higher interpretability and much better location stability than ordinary CNNs in almost all comparisons. Table 7 compares
classiﬁcation accuracy of different CNNs. Ordinary CNNs
performed better in single-category classiﬁcation. Whereas,
for multi-category classiﬁcation, interpretable CNNs exhibited superior performance to ordinary CNNs. The good performance in multi-category classiﬁcation may be because
that the clariﬁcation of ﬁlter semantics in early epochs reduced difﬁculties of ﬁlter learning in later epochs.
multi-category
single-category
ILSVRC Part
ILSVRC PartVOC Part CUB200
logistic5 logistic5 softmax
interpretable
interpretable
interpretable
interpretable
Table 7. Classiﬁcation accuracy based on different datasets. In
single-category classiﬁcation, ordinary CNNs performed better,
while in multi-category classiﬁcation, interpretable CNNs exhibited superior performance.
5.3. Visualization of ﬁlters
We followed the method proposed by Zhou et al. 
to compute the RF of neural activations of an interpretable
ﬁlter, which was scaled up to the image resolution. Fig. 5
shows RFs6 of ﬁlters in top conv-layers of CNNs, which
were trained for single-category classiﬁcation. Filters in interpretable CNNs were mainly activated by a certain object
part, whereas ﬁlters in ordinary CNNs usually did not have
explicit semantic meanings. Fig. 6 shows heat maps for distributions of object parts that were encoded in interpretable
ﬁlters. Interpretable ﬁlters usually selectively modeled distinct object parts of a category and ignored other parts.
6. Conclusion and discussions
In this paper, we have proposed a general method to
modify traditional CNNs to enhance their interpretability.
As discussed in , besides the discrimination power, the
interpretability is another crucial property of a network. We
design a loss to push a ﬁlter in high conv-layers toward the
representation of an object part without additional annotations for supervision. Experiments have shown that our interpretable CNNs encoded more semantically meaningful
knowledge in high conv-layers than traditional CNNs.
In future work, we will design new ﬁlters to describe
discriminative textures of a category and new ﬁlters for object parts that are shared by multiple categories, in order to
achieve a higher model ﬂexibility.