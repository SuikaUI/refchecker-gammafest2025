Human Activity Recognition
using Recurrent Neural Networks
Deepika Singh1, Erinc Merdivan1, Ismini Psychoula2, Johannes Kropf1, Sten
Hanke1, Matthieu Geist3, and Andreas Holzinger4
1 AIT Austrian Institute of Technology, Austria
 , 
2 School of Computer Science and Informatics, De Montfort University, UK
3 CentraleSupelec, France
4 Holzinger Group, HCI-KDD, Institute for Medical Informatics/Statistics, Medical
University Graz, Austria
Abstract. Human activity recognition using smart home sensors is one
of the bases of ubiquitous computing in smart environments and a topic
undergoing intense research in the ﬁeld of ambient assisted living. The
increasingly large amount of data sets calls for machine learning methods.
In this paper, we introduce a deep learning model that learns to classify
human activities without using any prior knowledge. For this purpose,
a Long Short Term Memory (LSTM) Recurrent Neural Network was
applied to three real world smart home datasets. The results of these
experiments show that the proposed approach outperforms the existing
ones in terms of accuracy and performance.
Keywords: machine learning, deep learning, human activity recognition, sensors, ambient assisted living, LSTM
Introduction
Human Activity recognition has been an active research area in the last decades
due to its applicability in diﬀerent domains and the increasing need for home
automation and convenience services for the elderly . Among them, activity
recognition in Smart Homes with the use of simple and ubiquitous sensors, has
gained a lot of attention in the ﬁeld of ambient intelligence and assisted living
technologies for enhancing the quality of life of the residents within the home
environment .
The goal of activity recognition is to identify and detect simple and complex
activities in real world settings using sensor data. It is a challenging task, as the
data generated from the sensors are sometimes ambiguous with respect to the
activity taking place. This causes ambiguity in the interpretation of activities.
Sometimes the data obtained can be noisy as well. Noise in the data can be
caused by humans or due to error in the network system which fails to give
correct sensor readings. Such real-world settings are full of uncertainties and
calls for methods to learn from data, to extract knowledge and helps in making
 
Deepika Singh et al.
decisions. Moreover, the inverse probability allows to infer unknowns and to
make predictions .
Consequently, many diﬀerent probabilistic, but also non-probabilistic models,
have been proposed for human activity recognition. Patterns corresponding to
the activities are detected using sensors such as accelerometers, gyroscopes or
passive infrared sensors, etc., either using feature extraction on sliding window
followed by classiﬁcation or with Hidden Markov Modeling (HMM) .
In recent years, there has been a growing interest in deep learning techniques.
Deep learning is a general term for neural network methods which are based on
learning representations from raw data and contain more than one hidden layer.
The network learns many layers of non-linear information processing for feature
extraction and transformation. Each successive layer uses the output from the
previous layer as input. Deep learning techniques have already outperformed
other machine learning algorithms in applications such as computer vision ,
audio and speech recognition .
In this paper, we introduce a recurrent neural network model for human
activity recognition. The classiﬁcation of the human activities such as cooking,
bathing, and sleeping is performed using the Long Short-Term Memory classiﬁer
(LSTM) on publicly available Benchmark datasets . An evaluation of the
results has been performed by comparing with the standardized machine learning
algorithms such as Naive Bayes, HMM, Hidden Semi-Markov Model (HSMM)
and Conditional Random Fields (CRF).
The paper is organized as follows. Section 2 presents an overview of activity
recognition models and related work in machine learning techniques. Section 3
introduces Long Short-Term Memory (LSTM) recurrent neural networks. Section 4 describes the datasets that were used and explains the results in comparison to diﬀerent well-known algorithms. Finally, Section 5 discusses the outcomes
of the experiments and suggestions for future work.
Related work
In previous research, activity recognition models have been classiﬁed into datadriven and knowledge-driven approaches. The data-driven approaches are capable of handling uncertainties and temporal information but require large
datasets for training and learning. Unfortunately, the availability of large real
world datasets is a major challenge in the ﬁeld of ambient assisted living. The
knowledge-driven techniques are used in predictions and follow a descriptionbased approach to model the relationships between sensor data and activities.
These approaches are easy to understand and use but they cannot handle uncertainty and temporal information .
Various approaches have been explored for activity recognition, among them
the majority of the techniques focuses on classiﬁcation algorithms such as Naive
Bayes (NB) , Decision Trees , HMM , CRF , Nearest Neighbor
(NN) , Support Vector Machines (SVM) and diﬀerent boosting techniques.
Human Activity Recognition using Recurrent Neural Networks
A simple probabilistic classiﬁer in machine learning is the Naive Bayes classiﬁer which yields good accuracy with large amounts of sample data but does
not model any temporal information. The HMM, HSMM, and CRF are the most
popular approaches for including such temporal information. However, these approaches sometimes discard pattern sequences that convey information through
the length of intervals between events. This motivates the study of recurrent neural networks (RNN) which promises the recognition of patterns that are deﬁned
by temporal distance .
LSTM is a recurrent neural network architecture that is designed to model
temporal sequences and learn long-term dependency problems. The network is
well suited for language modeling tasks; it has been shown that the network in
combination with clustering techniques increases the training and testing time
of the model and outperforms the large scale acoustic model in speech recognition systems .
LSTM Model
LSTM is a recurrent neural network architecture that was proposed in .
Another version without a forget gate was later proposed in and extended
in . LSTM has been developed in order to deal with gradient decay or gradient
blow-up problems and can be seen as a deep neural network architecture when
unrolled in time. The LSTM layer’s main component is a unit called memory
block. An LSTM block has three gates which are input, output and forget gates.
These gates can be seen as write, read and reset operations for the cells. An
LSTM cell state is the key component which carries the information between
each LSTM block. Modiﬁcations to the cell state are controlled with the three
gates described above. An LSTM single cell, as well as how each gate is connected
to each other and the cell state itself, can be seen in Figure 1.
Fig. 1: LSTM single cell image .
Deepika Singh et al.
Each gate and cell state are governed by multiplicative equations that are
it = σ(Wxixt + Whiht−1 + Wcict−1 + bi),
ft = σ(Wxfxt + Whfht−1 + Wcfct−1 + bf),
ot = σ(Wxoxt + Whoht−1 + Wcoct + bo),
ct = ftct−1 + it tanh(Wxcxt + Whcht−1 + bc),
ht = ot tanh ct,
with W being the weight matrix and x is the input, σ being the sigmoid and
tanh is the hyperbolic tangent activation function. The terms i, f and o are
named after their corresponding gates and c represents the memory cell .
Fig. 2: Illustrations of an LSTM network with x being the binary vector for
sensor input and y being the activity label prediction of the LSTM network.
By unrolling LSTM single cells in time we construct an LSTM layer where
ht is the hidden state and yt is the output at time t as shown in Figure 2.
Experiments
Publicly available and annotated sensor datasets have been used to evaluate
the performance of the proposed approach . In this dataset, there are three
houses with diﬀerent settings to collect sensory data. The three diﬀerent houses
were all occupied by a single user named A, B, and C respectively. Each user
recorded and annotated their daily activities. Diﬀerent number of binary sensors
were deployed in each house such as passive infrared (PIR) motion detectors to
detect motion in a speciﬁc area, pressure sensors on couches and beds to identify
the user’s presence, reed switches on cupboards and doors to measure open or
Human Activity Recognition using Recurrent Neural Networks
close status, and ﬂoat sensors in the bathroom to measure toilet being ﬂushed
or not. The data were annotated using two approaches: (1) keeping a diary in
which the activities were logged by hand and (2) with the use of a blue tooth
headset along with a speech recognition software. A total of three datasets were
collected from the three diﬀerent houses. Details about the datasets are shown in
Table 1 where each column shows the details of the house with the information
of the user living in it, the sensors placed in the house and the number of activity
labels that were used.
Table 1: Details of the datasets.
Apartment Apartment
Activities
Annotation Bluetooth
The data used in the experiments have diﬀerent representation forms. The
ﬁrst form is raw sensor data, which are the data received directly from the sensor.
The second form is last-ﬁred sensor data which are the data received from the
sensor that was ﬁred last. The last ﬁring sensor gives continuously 1 and changes
to 0 when another sensor changes its state. For each house, we left one day out
of the data to be used later for the testing phase and used the rest of the data
for training. We repeated this for every day and for each house. Separate models
are trained for each house since the number of sensors varies, and a diﬀerent
user resides in each house. Sensors are recorded at one-minute intervals for 24
hours, which totals in 1440 length input for each day.
The results presented in Table 2 show the performance of the LSTM model
on raw sensor data in comparison with the results of NB, HMM, HSMM and
CRF . Table 3 shows the results of the LSTM model on last-ﬁred sensor data
again in comparison with the results of NB, HMM, HSMM and CRF. For the
LSTM model, a time slice of (70) with hidden state size (300) are used. For the
optimization of the network, Adam is used with a learning rate of 0.0004 
and Tensorﬂow was used to implement the LSTM network. The training took
place on a Titan X GPU and the time required to train one day for one house is
approximately 30 minutes, but training times diﬀer amongst the houses. Since
diﬀerent houses have diﬀerent days we calculated the average accuracy amongst
Deepika Singh et al.
all days. The training is performed using a single GPU but the trained models
can be used for inference without losing performance when there is no GPU.
Table 2: Results of raw sensor data
Naive Bayes
77.1 ± 20.8
80.4 ± 18.0
46.5 ± 22.6
59.1 ± 28.7
63.2 ± 24.7
26.5 ± 22.7
59.5 ± 29.0
63.8 ± 24.2
31.2 ± 24.6
89.8 ± 8.5
78.0 ± 25.9
46.3 ± 25.5
LSTM(Ours)
89.8 ± 8.2
85.7 ± 14.3
64.22 ± 21.9
Table 2 shows the results of diﬀerent models on raw data from three diﬀerent
houses. The LSTM model has the best performance for all three data sets. In
House B and House C, LSTM improves the best result signiﬁcantly especially
on House C where the improvement is approximately 40%.
Table 3: Results of last-ﬁred sensor data
Naive Bayes
95.3 ± 2.8
86.2 ± 13.8
87.0 ± 12.2
89.5 ± 8.4
48.4 ± 26.0
83.9 ± 13.9
91.0 ± 7.2
67.1 ± 24.8
84.5 ± 13.2
96.4 ± 2.4
89.2 ± 13.9
89.7 ± 8.4
95.3 ± 2.0
88.5 ± 12.6
85.9 ± 10.6
Table 3 shows the results on last ﬁred data from three diﬀerent houses using
the same models as in Table 2. The LSTM model did not improve the results in
this section but it matched the best performance for two data sets with a slight
drop in House C.
Discussion
The results presented in this paper show that the deep learning based approaches
for activity recognition from raw sensory inputs can lead to signiﬁcant improvement in performance, increased accuracy, and better results. As shown in Section
4.2 our LSTM based activity predictor matched or outperformed existing probabilistic models such as Naive Bayes, HMM, HSMM and CRF on raw input
and in one case improved the best result by 40%. Predicting on raw input also
reduces the human eﬀorts required on data preprocessing and handcrafting features which can be very time consuming when it comes to an AAL (Ambient
Assisted Living) environment.
Human Activity Recognition using Recurrent Neural Networks
Future Work
Our future work will focus on reducing the variance on our predictions and early
stopping criteria while training on diﬀerent days. The LSTM model has diﬀerent
hyperparameters which aﬀect the performance of the model signiﬁcantly. Diﬀerent optimization and hyperparameter search techniques could be investigated
in the future. Since the LSTM model has proven to be superior on raw data it
would be interesting to also apply other deep learning models. One problem is
that deep learning badly captures model uncertainty. Bayesian models oﬀer a
framework to reason about model uncertainty. Recently, Yarin & Ghahramani
 developed a theoretical framework casting dropout training in deep
neural networks as approximate Bayesian inference in deep Gaussian processes.
This mitigates the problem of representing uncertainty in deep learning without
sacriﬁcing either computational complexity or test accuracy.
Acknowledgement
This work has been funded by the European Union Horizon2020 MSCA ITN
ACROSSING project (GA no. 616757). The authors would like to thank the
members of the project’s consortium for their valuable inputs.