Distribution Category:
Mathematics and Computers
ANL--81-83
DE82 005656
ARGONNE NATIONAL LABORATORY
9700 South Cass Avenue
Argonne, Illinois 60439
COMPUTING A TRUST REGION STEP
Jorge J. Mord and D. C. Sorensen
Applied Mathematics Division
DISCLAIMER
December 1981
Computing a Trust Region Step
Jorge J. Mord and D. C. Sorensen
Applied Mathematics Division
Argonne National Laboratory
Argonne Illinois 60439
1. Introduction.
In an important class of minimization algorithms called "trust region methods"
(see, for example, Sorensen ), the calculation of the step between iterates
requires the solution of a problem of the form
minij(w): 1|w |isAl
where A is a positive parameter, I- II is the Euclidean norm in R", and
#(w) = g w + Mw7Bw,
with g E R", and B E R
a symmetric matrix. The quadratic function ' generally
represents a local model to the objective function defined by interpolatory data at an
iterate and thus it is important to be able to solve (1.1) for any symmetric matrix B; in
particular, for a matrix B with negative eigenvalues.
In trust region methods it is sometimes helpful to include a scaling matrix for the
variables. In this case, problem (1.1) is replaced by
mint%(v):IIDu Is A
where D E R""" is a nonsingular matrix. The change of variables Du = w shows that
problem (1.3) is equivalent to
minif(w):1|w| Is Al
where f(w) _%f(D-'w), and that the solutions of problems (1.3) and (1.4) are related
by Dv = w. Because of this equivalence, we only consider problem (1.1). Also note
that if, as is usually the case, D is a diagonal matrix, then it is easy to explicitly carry
out the change of variables and solve problem (1.4).
The use of a trust region method in a nonlinear optimization problem requires the
solution of many problems of type (1.1). These problems do not usually require accurate solutions, but in all cases we must be able to find an approximate solution with a
reasonable amount of computational effort, and the approximate solution found mus'
guarantee that the trust region method has the right sort of convergence properties.
In this paper we are concerned with these two issues; namely, robust and stable algorithms for the solution of (1.1) and the impact of these algorithms on the convergence
Work supported in part by the Applied Mathematical Sciences Research Program (KC-04-OB)
of the Office of Energy Research of the U.S. Department of Energy under Contract W-31-109-
-2properties of trust region methods.
Goldfeld, Quandt, and Trotter , Hebden , Fletcher , Gay ,
and Sorensen , have discussed the solution of (1.1) in connection with trust
region methods. Their algorithms are based on the fact that if (1.1) has a solution on
the boundary of
:1w IIs Aj then, in most cases, a solution of (1.1) can be found by
det rainingg X ~ 0 such that B + XJ is positive definite and
II(B + XI)~1g II = A
In one case - the hard case - equation (1.5) has no solution with B + u positive definite,
and this leads to numerical difficulties.
Hebden proposed an algorithm for the
solution of (1.1) which is basically sound except for its treatment of the hard case. Gay
 improved Hebden's scheme and showed that under certain conditions the
approximate solution determined by his algorithm is nearly optimal. His algorithm,
however, may require a large number of iterations in the hard case.
In this paper we propose an algorithm for the solution of (1.1) which is guaranteed
to produce a nearly optimal solution in a finite number of steps. Specifically, given
parameters al and a2 in (0,1), the approximate solution s satisfies
*(s) - *' s
a1(2-ai)maxl
||s|11 !(1
is the optimal value of (1.1).
We also consider the use of our algorithm in a
trust region Newton's method. In particular, we prove that under reasonable assumptions the sequence
generated by Newton's method has a limit point z' which
satisfies the first and second order necessary conditions for a minimizer of the objective function f .
Numerical results for GQTPAR, which is a Fortran implementation of
our algorithm, show that GQTPAR is quite successful in a trust region method. In our
tests a call to GQTPAR only required 1.6 iterations on the average.
The outline of the paper is as follows. The theoretical basis of an algorithm for the
solution of (1.1) is laid out in Section 2, while in Section 3 we present the algorithm and
show that the solution generated by the algorithm is nearly optimal. In Section 4 we
consider the use of this algorithm in a trust region Newton's method and prove that the
combined algorithm has very strong convergence properties. Numerical resuts are
presented in Section 5.
2. Structure of the Problem.
Problem (1.1) has a tremendous amount of structure and it is important to understand this structure in order to construct a suitable algorithm. The following results
expose this structure and provide a theoretical basis for the numerical algorithm. Note
that these results provide necessary and sufficient conditions for a point p E R" to be a
solution to (1.1) and that there is no "gap" between the necessary and sufficient conditions.
Lemma (2.1). If p is a solution to (1.1) then p is a solution to an equation of the form
(B + AI)p = -y,
with B + Al positive semideflnite, A z 0, and A(A -IIp II) = 0.
Lmma (2.3). Let A E R,p ER" satisfy (2.2) with B + XI positive semideflnite.
(t) If X= 0and i|p II <Athenp solves (1.1).
(ii) p solves gy(p) = minf(w ) : 11w II = Ip Ill.
(iii) If A & 0 and |Ip || = A then p solves (1.1).
If B + Al is positive definite then p is the only solution to (1.1).
Simple proofs of these lemmas are given by Sorensen . Lemma (2.3) is
important from a computational standpoint since it provides a perturbation result that
is useful in setting termination rules for the iterative method used to solve (1.1).
The solution of (1.1) is straightforward if (1.1) has no solutions on the boundary of
w:||w II |
Al. In fact, (1.1) has no solution p with I|p|II|= A if and only if B is positive
definite and IIB-
II < A. To prove this claim, first note that if B is positive definite and
IB-1g II < A then Lemma (2.3) immediately shows that p = -B-'g is the solution to
(1.1). On the other hand, if (1.1) has a solution p with I|p II < A then Lemma (2.1) shows
that A = 0 and that B is positive semidefinite. If B were singular then Bz = 0 for some
z with|Ilp + z II= A and then Lemma (2.3) implies that p + z is a solution to (1.1) on the
boundary. This contradiction establishes our claim.
Now assume that (1.1) has a solution on the boundary of jw :||w I|
If g is not
perpendicular to the eigenspace
S1 =r. : Bz = A 1z, z of0;
where Al is the smallest eigenvalue of B, then the nonlinear equation lip. II = A where
p. = -(B + al)~1 g
has a solution A z 0 in (-Aa). Moreover, Lemma (2.3) implies that pa is the solution of
problem (1.1). Reinsch and Hebden observed independently that to
solve ip II = A great advantage could be taken of the fact that the function Ilpa112 is a
rational function in a with second order poles on a subset of the negatives of the eigenvalues of the symmetric matrix B. To see this consider the decomposition
B = QAQT'with A = diag(A, A, -... ,A
) and QTQ = 1,
and observe that
Ip.Ii'= IIQ(A + al)-'Qrg I||=7
-4where 7y
is the i-th component of QTg. In the next section we elaborate on the importance of this observation.
If g is perpendicular to S1 then the equation Ipa|II= A may still have a solution
X z 0 in (-XI,oo), and in this case the solution to (1.1) can be obtained as above. If, however, Ilpa II = A has no solutions in (-X 1,o) then this leads to numerical difficulties. We
call this situation the "hard case".
A characteristic difficulty of the hard case is that jjpII< A whenever B + a! is
positive definite. For example, if
1then Al = -1 ,and if B + al is positive definite then 11p 1 1
In the hard case, a solution to (1.1) can be obtained by solving
1 1)p = -g
for p with I|p ||:
A and by determining an eigenvector z E S1 . Then
(B -X 1I)(p + rz) =-g
and Ip + rz II = A for some r. Lemma (2.3) now shows that p + 'rz solves (1.1).
3. The Algorithm.
Consider tne solution of |Ip II = A.
The rational structure of Ipa|I|2 may be
exploited by applying Newton's method to the zero finding problem
Newton's method is very efficient when applied to (3.1) since Sp is almost linear on
(-X 1*o). Moreover, the computation of the Cholesky factorization of B + a! makes it
possible to compute the necessary derivative whenever a E (-Xx). There is no need to
compute the eigensystem of B as suggested by Goldfeld, Quandt, and Trotter .
The following algorithm updates A by Newton's method applied to (3.1).
Algorithm (3.2). Let A z 0 with B + Al positive definite and A > 0 be given.
1) Factor B + Al = RTR
2) Solve RTRp = -g;
3) Solve Rigq
4) Let A:= A+
In this algorithm RTR is the Cholesky factorization of B + XI with R E R"" upper triangular
Although not represented in this simplified version, it is necessary to safeguard A in order to obtain a positive definite B + AI and guarantee convergence. These
safeguards, and the convergence criteria for this algorithm are discussed later on in
this section.
If properly safeguarded, the iteration produced by Algorithm (3.2) is sufficiently
rapid to solve most problems of type (1.1) expected in practice. However, in the hard
case this scheme may require a large number of iterations to converge; in particular, if
g = 0 then Algorithm (3.2) breaks down. In the hard case it is necessary to supply a
vector z that is an approximate eigenvector of B corresponding to A1. Indeed, as
pointed out at the end of Section 2, in the hard case a solution to (1.1) is
p = -(B - XI)tg + rz
where z E SI and r is chosen so that IIp 1I= A. Note that actual computation of the two
orthogonal components of p indicated in (3.3) may require more computational effort
than is reasonable in the context of an optimization algorithm. Moreover, recognizing
tha.t a solution of the form (3.3) is required may also be time consuming. Fortunately,
there is a completely acceptable alternative. The following result provides a foundation
for this technique.
Lemma (3.4). Let 0 <co < 1 be given and suppose that
B +XI = RTR , (B + XI)p = -g X 0.
Let z ER" satisfy
||p + z |=
A , |Rz ||2 s;a(||Rp ||a + A2).
--gyp +z) a}(1
||2 + AA2)
taherefis the optimal value of (1.1)
Proof : First note that for any z E R",
'(p + z) = -WIIRp ||2 + X||p + z 1I2) + I||lRz 11.
Then for any z which satisfies (3.5),
+za) z }(1
-a)(||Rp 1|2 + M2E).
Moreover, if #' = '(p + z') where IIp + z * 1 s A, then (3.5) and (3.6) imply that
- (p+z')isjI|RpII| + XA0].
The last two inequalities yield Lemma (3.4).
A consequence of Lemma (3.4) is that |%#(p + s) -f# ' I
This shows that if
(3.5) holds then p + z is a nearly optimal solution to problem (1.1).
A consequence of the proof of Lemma (3.4) is equation (3.6).
This expression is
quite useful and will be used throughout this section.
Gay has a result similar to Lemma (3.4) but the proof is quite involved and
the assumptions are stronger than those in Lemma (3.4).
Instead of (3.5), Gay's
assumptions imply that maxl||p|,||zI|I < A and that
= A, ll(B + XI)zl|
(-(l|Rp||2 + IIp112).
Since |ipII < A and IIz |1 < A, it is not difficult to show that (3.7) implies (3.5).
A weakness of (3.7) is that in the hard case it may be a severe restriction on A. This claim can
be made precise by first noting that (3.7) implies that
I A + A I ||z |1 (! )(l|R 11 + A)||p |2
Now, when I| II|cA for some e in (0,1) then |iz I|
IA+X , I s(1
(||R |2+A)e2
Since e can be quite small (specially if g is small) in the hard case, this estimate shows
that an algorithm based on (3.7) may require a large number of iterations in the hard
case. Note that this weakness is not shared by (3.5).
The main use of Lemma (3.4) is in the hard case. In this situation we have A
with B + Al positive definite and the solution p of (2.2) satisfies lp |I< A. We can then
attempt to satisfy (3.5) with z = ri by letting r satisfy lp +Tr I = A and by choosing z
with || z|1 = 1 such that |RII || is as small as possible.
with 11| 11I= 1 and p withi|p|| < A there are usually two choices of r which
satisfy ip + ri || = A, and equation (3.6) implies that the choice wi ! the smaller magnitude minimizes '(p + 'in). This choice is
e 2-lp 1II
+ sgn(p)(pp )2 + (A2 -
|lI = 1 such that 1| Rz |1 is as small as possible can be obtained with
the UNPACK technique (Cline, Moler, Stewart, and Wilkinson ) for estimating the
smallest singular value of a triangular matrix R. In this technique a vector e with cornponents
1 is selected so that the solution w to the system RTw = e is large. Essentially the idea is to select the sign of the components of e to cause maximum local
growth of w as the forward substitution proceeds. Then the system Ru = w is solved
for v . The vector v has the property that
IIRII = ri 4
is close to the smallest singular value of R. In particular, if R is nearly singular then
I|Rs his close to zero and this ensures that (3.5) is eventually satisfied by z = TI . This
algorithm is attractive because it is computationally inexpensive
(roughly n2
-7arithmetic operations) and quite reliable, but there are other possibilities, for example, the algorithms of Cline, Conn, and Van Loan .
An important ingredient of the iteration is the safeguarding required to ensure
that a solution is found. The safeguarding depends on the fact that (p is convex and
strictly decreasing on (-XIoo). This fact was discovered by Reinsch and follows
from (2.4). It implies that NewtoL's method started from A E (-,,oo) with p(A) > 0 produces a monotonically increasing sequence converging to the solution of P(a) = 0. In
addition, if A E (-Aj,) and 9P(A) < 0 then the next Newton iterate X+ is such that either
X+ g -XI, or p(X+) & 0.
The safeguarding scheme uses parameters AL, A,, and As such that [ AL , A V] is an
interval of uncertainty which contains the desired A, and As is a lower bound on -A1
Safeguard A:
max(A, AL);
min(A,XA);
3) If A ! AS then A:= max(0.001Av, (ALA ri)M);
Safeguarding schemes of this type have been used by More for the case in which
B is positive semidefinite and by Gay for the general case. The first two steps of
the safeguarding scheme ensure that A E [ AL , A v] while the third step guarantees that
the length of the interval of uncertainty is reduced. The third step is critical; if the
length of the interval of uncertainty remains bounded away from zero then the third
step can only be executed a finite number of times. This last pcint will become clear
once we set down the rules for updating the safeguarding parameters.
Given A3 and a trial A, the rules for revising As are as follows.
If A E (-AI, o) and
p(A) < 0 then compute r and i as described above and set
As : =max(As, A-|II|RI|1 2).
If A s -Al then during the Cholesky decomposition of B + Al it is possible to find 6 a 0
such that the leading submatrix of order L s n of
is singular. In addition, it is possible to determine u E R" such that
with u = 1and u4 = 0 fori >L1. It follows that
AS:max(As , A
Is a lower bound on -A1.
Gay proposed updating As via (3.9) but (3.8) is new. Since ||R 112 is usually
close to A + A1, updating As via (3.8) tends to avoid trial A for which B + A! is not positive definite and thus reduces the number of iterations required for convergence.
The rules for updating AL and At are fairly simple; they are presented in the following summary of the updating rules for the safeguarding parameters.
Update AL, ArV, A5 :
1) If A E (-A,1 oo) and Sp(A) < 0 then
AXV.:=min(AvA)
AL =max(AL,A);
2) Update As using (3.8) and (3.9);
3) Let AL := max(AL , As);
Initial values for the safeguarding parameters are
As = maxlwhere Pg is the i-th diagonal element of B, and
-J|IB|111),
Other choices of initial values are possible, but these are simple and are particularly
effective for large values of IIg II/
The final ingredient of the iteration is the convergence criteria. The idea is to terminate the iteration with a nearly optimal solution of problem (1.1). Given al and a in
(0.1), and a trial A : 0 such that B + Al is positive definite, a vector p is computed as in
Algorithm (3.2). If
|p!II|sa, or ||p |
then the algorithm terminates with s = p as an approximate solution. The hard case is
taken into account by computing p and z whenever |ip II < A, and if
a1(2-a,)maxi
er,||IRp ||2+\A2;g
thee the algorithm terminates with s = p +4 as the approximate solution.
An additional subtlety of the convergence tests is that if both (3.10) and (3.11) are
satisfied then we choose the approximate solution s for which hi(s) is least. This is easy
to do because (3.6) shows that ,0(p +4d) s d(p) if and only if
IIR(r)112 g X(A2 - iI 112).
This subtlety is not theoretically necessary but is nice to have from a computational
point of view. Also note that the factor of a1 (2-ca1) in (3.11) is needed so that in each
case ,i(s) satisfies a bound of the same form. This is made clear in the discussion that
We now show that (3.10) and (3.11) guarantee that if the algorithm terminates then
the approximate solution s satisfies
p(s) - ,' s a(2-cT)maxUIa j,
| II s(1+ai)A
and thus s
is a nearly optimal solution
First consider
II Rp 112+ XA 2 > a2 then the assumptions of Lemma (3.4) are satisfied when a is replaced
by a1(2 - a1) and hence (3.12) holds for s = p +'rz. Now suppose that II|Rp 112 + )M2 s a2
To establish that (3.12) holds in this case, first note that if 0'
(p + z') where
l|p + z 11|9 A then (3.6) implies that
We now use this result and (3.6) to obtain that
,/(p + 'i)
= -}(II|Rp|112 + XA 2) + %I R (T) )1 }|| ' +Xa(2 - al)a2.
Hence, (3.12) also holds in this case.
The next result shows that if the algorithm terminates when (3.10) is satisfied then
(3.12) holds with s = p
Lemma (3.13). Let 0 < a < 1 be given and suppose that
B +AI =RTR, (B +AI)p = -g ,XA0.
'is the optimal value of(1.1) and if IIP II I (1-a)A then
-#(p) z(1 -a)2(||Rp|112
Proof : Just as in the proof of Lemma (3.4), note that (3.6) holds for any z ER" and
-1 sI|iRp1|2 + XA2].
Moreover, (3.8) with z = 0 also implies that
-#(p) zj(1 -a)2(IIRp 11 + A).
The last two inequalities yield Lemma (3.13).
We have now discussed all the ingredients of the iterative scheme for solving problem (1.1). The following algorithm summarizes these ingredients and defines a typical
iteration.
Algorithm (3.14):
1) Saf eguard A;
2) If B + A is positive definite then factor B + Al = R TR ;
otherwise go to 5;
3) Solve RTRp = -g;
4) If ipII < A, compute r and z ;
5) Update AL, AV, As;
6) Check the convergence criteria;
7) If B + A is positive definite and g # 0 then update A via Algr rithm (3.2) ;
otherwise update A via A : = As ;
The last step of Algorithm (3.14) deserves some explanation.
If B + Al is positive
definite and g 0 0 then the Newton iterate of Algorithm (3.2) tends to be a lower bound
on -A 1 for 1|g II sufficiently small and thus updating A via AS is a reasonable choice when
g = 0. Also note that setting A to AS forces a safeguarded choice of A in the next iteration, and that this is a desirable strategy whenever the Newton iteration cannot b
We now claim that after a finite number of iterations AJgorithm (3.14) produces c.
AE (-A 1 ,o)
with p(A) ? 0 or an arbitrarily small interval o uncertainty.
If we assume
that the length of the interval of uncertainty remains bounded away from zero then thoi
third step of the safeguarding scheme guarantees that A s As only happens a finite
number of times. Now, if A !
-A 1 then A S AS holds on the next iteration. Finally, if
AE (-A 1,o) and
(A) < 0 then recall that the next Newton iterate A is such that either
-A 1, or sp(A+) z 0. The above argument shows that A !
-A1 can only happen a
finite number of times, so eventually A+ E (- .,o)
and sp(A) -
0. This establishes our
The importance of the above claim should be evident; given A E (-A 1,ao) with
gp(A) z 0 then Algorithm (3.14) eventually satisfies (3.10), while if the interval of uncertainty is small then R is nearly singular and it is then possible to satisfy (3.11). Thus
Algorithm (3.14) terminates -n a finite number of iterations with an approximate solution s which satisfies (3.12).
A frequent application of Algorithm (3.14) is to the solution of a sequence of problems of the form (1.1) in which only A is changing. In particular, in trust rv gion
methods we need to solve a sequence of problems for decreasing values of A and then it
is possible to improve the initial choice of AL. Assume that A and AL are the final values
of these parameters for a specific value of A. Given a new value 6+ < A then AL is still a
-11kiwc" bound for the new problem. Moreover, the convexity and monotonicity of (P shows
that an update of A based on a Newton step for
is alto a lower bouAd for the rnew problem. This improvement on the initial choice of AL
follows a suggestion of Ron Dembo.
One of the differences between Gay's algorithm and Algorithm (3.14) is that
in Gay's algorithm A = 0 is E.lways tried first. If is not at all clear that this is a desirable
strategy, and it seems prF erable to try A = 0 first only if the safeguarded A is zero.
Note that if B is positive csfinite and |IB-1 g |I A then Algorithm (3.14) terminates in at
most two iterations. In fact, if initially A > 0 then the convexity and monotonicity of rp
and the positive definiteness of B guarantee that the next trial A is zero.
We have already mentioned that another difference is the updating of As via (3.8).
A final difference occurs when g = 0; Gay's algorithm does not apply in this situation,
but Algorithm (3.14) handles all cases.
4. Thst Region Methods in Unconstrained Minimization.
We now consider the use of Algorithm (3.14) in the context of trust region methods
for unconstrained minimization and show how Algorithm (3.14) can be used to produce
an efficient and reliable version of Newton's method.
Let f : R"-R be a twice continuously differentiable function with gradient Vf and
Hessian V2fJ. In Newton's method with a trust region strategy, each iterate zk has a
bound 4 such that
(zk) + f(w),
(W ) = Vf (zk)Tw +y TV2Jf (zk)w
In other words, Pk is a model of the reduction in f within a neighborhood of the iterate
zb. This suggests that it may be desirable to compute a step sk which approximately
solves the problem
minIk(w) : IIw|IIs t[.
If the step is satisfactory in the sense that zk + sk produces a sufficient reduction in f
then 4 can be increased; if the step is unsatisfactory then 4 should be decreased.
Algorithm (4.2). Let 0 <M<7 <1 and 0<y1<72< 1 <7s be specified constants.
1)LetzoER" andA > 0begiven.
2) For k = 0,1,2, - - - until "convergence"
-12a) Compute Vf (Xk) and V2f (zk).
b) Determine an approximate solution s to problem (4.1).
f (xk+ sk) - f(xk)
c) Compute pk =
d) If pk -cyAthen
A := AE[71Ak,72]and go to b).
e) zk+1=Xzk+sk.
f) If pk s r, then Ak+1 E [72A4,Ak] else Ak+1 E [Aky3]-
This is a basic form of Newton's method which does not include a scaling matrix for the
variables. To include a scaling matrix, subproblem (4.1) is replaced by
mini lpk(w)
: ||Da wIlls
where Dk is a nonsingular matrix. We shall not discuss this generalization here; however, it is important to note that our results hold if JDk has uniformly bounded condition numbers.
In this section we are mainly interested in conditions on the approximate solution
of problem (4.1) which guarantee that the sequence i xk
generated by Algorithm
(4.2) is convergent to a point x
with Vf (z') = 0 and V2f (x') positive semidefinite.
minimal requirement on sk is that there is a
E E (0,1) such that
-*(sk) L #max -
= aVf (zk) ,11 w
Under this assumption, Powell proved that if y = 0 then some subsequence of
iVf (zb)l converges to zero, while Thomas showed that if ju> 0 then the whole
sequence JVf (zk)j converges to zero. These results indicate that we can expect zk j to
converge to a point x' with Vf (x') = 0. Sorensen proved that we can also
expect to have Vf (z') positive semidefinite provided there is a constant a E (0,1) such
#,t(s) =min jik (w):|wI1|!<_v
with 1IskII1|9 6k|E[(1-a)4,(1+a)Ak]
Unfortunately the termination criterion (3.11) is not necessarily consistent with these
conditions and thus this result does not allow the choice of sk provided by Algorithm
(3.14). An appropriate generalization of Sorensen's results car be obtained by assuming that there are constants Pi > 0 and #2 > 0 such that
-*k (s) PlIfihI with ||skII s PeAk
An immediate consequence of (3.12) is that if fk s 0 then the approximate solution sk
provided by Algorithm (3.14) with a2 = 0 satisfies (4.3).
Of course, if #i
Vf (zb) = 0 and ef1 (Zk) is positive semidefinite and thus Algorithm (4.2) terminates at
zb. Lemma (3.13) shows that Sorensen's assumptions imply that (4.3) holds.
Assumption (4.3) can be expressed in an alternate form which is more convenient
for proofs of convergence.
If p E R" is a solution to problem (4.1) then Lemma (2.1)
implies that there is a parameter X such that
V2f(zh) + XI = RRk ,(V2f(zk) +XI)pt = -VfI(zk) . Xk >0
and with X(Ak
IIPk II) = 0 .
A calculation now shows that
(I|| R pk I|I2 + k
This expression for #x shows that if (4.3) holds then
-%#b(sk) L
and thus the iterates fxk I generated by Algorithm (4.2) satisfy
f (zb) - f(zk+) ! %s,(IIRkp 112 + kAb).
These two inequalities are essential to the proof of our next result.
Theorem (4.7). Let f : R" - R be twice continuously differentiable on the level set
n = Jz : f (z) ! f(z0) I and consider the sequence Jxb produced by Algorithm (4.2)
where sb satisfies (4.3). If 0 is a compact set then either the algorithm terminates ct
zjE 0 because Vf (xi) = 0 and V2f (z) is positive semidefinite , or jxk I has a limit point
z* E 0 with Vf (zx) = 0 and V2f (z') positive semidefinite.
Proof : If Vf (z5) = 0 and V2f (z1) is positive semidefinite for some iterate xL E 0 then the
algorithm terminates; otherwise (4.3) implies that k (sak) < 0 for k z 0 and thus fXk I is
well defined and lies in 0.
Let us now prove the result under the assumption that Xk I is not bounded away
from zero. If some subsequence of
converges to zero then since 0 is compact we
can assume, without loss of generality, that the same subsequence of xb I converges to
some z in the level set 0. Since V2f (xk) +,XI is positive semidefinite, V2f (z') is also
positive semidefinite , and Vf (z') = 0 follows by noting that
11V2 1f(zk)||1+ Ak
and that (4.6) implies that I||Rhph |I converges to zero.
We can show that JA *
is not bounded away from zero by contradiction.
a e > 0 then (4.3) and (4.5) yield that
d lIz.|12.
Now, a standard estimate is that
|If (zh +sh) -f (zh) -#k(sb) I s)M1 I seIIImax||I f (zh+(sb) - V'f1(z)II,
and thus the last two inequalities show that
[ 7 )maxIIV
2f (zk+ s) -V 2f (zk)|II
Inequality (4.6) implies that A
converges to zero and hence f||s Ii also converges to
zero. Thus the uniform continuity of V2f on 0 together with (4.9) implies that p > 7
for all k sufficiently large and then the updating rules for 4 yield that J4 J is bounded
away from zero. This is in contradiction of the fact that J4
J converges to zero.
The result we have just established is only a sample of the available results for
Algorithm (4.7) under assumption (4.3) for sk. All of the results of Sorensen 
hold, and in particular, it can be shown that if f has a finite number of critical points
in the level set 0 then every limit point of the sequence Jxk
satisfies the second order
necessary conditions. We now prove a stronger version of this result.
Leama (4.10). Let z' be an isolated limit point of a sequence Jxk J in RR. If frk
not converge then there is a subsequence Jxif; which converges to z' and an e> C such
that IX 1 +1i -zX,\| ?E.
Proof: Choose s >0 so that if IIx -x' |
Is and x is a limit point of zui then:x = x'. If
Iz, -z' II|
then define ly by
maxil:I j-x'fI
,i = kj, . . . , L.
In this manner, a subsequence fx1,3 is defined such that
| , -z'||s
E, IIi,+-z'II>
It follows that z1
converges to x' and thus |Izi, -- z'||1 !
for all lj sufficiently large.
IIz+ 1i-zLjII||
||zi,+-z'|II -IzIZ-z'|II|z
as desired.
Theorem (4.11). Let f
: R" -+ R be twice continuously differentiable on the level set
0 = Jx :f (x) ! f (xo) J and consider the sequence fzu| produced by Algorithm (4.2)
where sk satisfies (4.3). If x' is an isolated limit point of Jxk 3 then V2f (x') is positive
semidefinite.
Proof: If fzk I converges to x' then Theorem (4.7) shows ( the compactness of 0 is only
used to guarantee that jzr j is bounded ) that V2f (z) is positive semidefinite.
does not converge then Lemma (4.10) applies. Thus, if (zJ is the subsequence
guaranteed by Lemma (4.10) then Al,
Pu2e, and since (4.6) shows that k A,
must then converge to zero. We can now use the positive semidefiniteness
of V'f (zs) + AI to conclude that V'f (z') is positive semidefinite. a
We have already noted that Thomas proved that JVf (zk); converges to zero.
Hence, if V2f (z') is nonsingular at a limit point z' of ink) then z' is an isolated limit
point, and Theorem (4.11) shows that V2f (z') is positive definite. Since 1i (s) S 0 we
IIV2f (zk)~1|IIVf (zk)|1
whenever V2f (zk) is positive definite, and thus Lemma (4.10) shows that jZb converges
to z'. This establishes the following result.
Theorem (4.13). Let f
: R" - R be twice continuously differentiable on the level set
0 = z : f (z) !gf (zo) ; and consider the sequence jzk I produced by Agorithm (4.2)
where sk satisfies (4.3). If z' is a limit point of z 3 and V 2 f(z') is nonsingular then
Izk I converges to z' and V2jf (z*) is positive definite..
An additional result which is helpful in establishing rate of convergence results is
that under the assumptions of Theorem (4.13) the sequence ik I is bounded away from
zero. To prove this first note that if so> 0 is a lower bound on the eigenvalues of
then (4.4) shows that
I jf I Iz ominiIIse'II2
sF_-V2f (zh)-Vf (Zk).
Now, (4.12) implies that
|IsII| ,cIslII where s
is an upper bound on the condition
number of V2f (z=), and hence assumption (4.3) shows that there is a constant r
- (Sk) Z M 1|s 1
This estimate and (4.8) then yield that
- Vff (zk) I|,
and thus pb > i7 for all k sufficiently large. It follows that fe j is bounded away from
zero as desired.
Rate of convergence results can be obtained with the additional - but mild
assumption that there is a constant f .> 0 such that if Ish |II PA then V2f (zb) is positive definite and sb = s.
For example, Algorithm (3.14) satisfies this assumption
because if ||sb || < (1-i1)4 then A& = 0.
Under the above assumption, Theorem (4.13) can be extended to show that jzb
converges to z' with a Q-superlinear rate of convergence and that if V'f is Lipschitz
continuous then the rate of convergence is quadratic. The proof is not difficult; since
y||s' || J|IIsl|| where K is an upper bound on the condition number of the Hessian
matrix at zb, eventually ||s, ||1
s4, and then Izb 3 becomes an unmodified Newton's
-16method. The rate of convergence results are then standard.
5. Computational Results.
Algorithm (3.14) has been implemented in a Fortran subroutine GQTPAR, and in
this section we present the results of GQTPAR on two sets of test problems. Since our
main concern here is the performance of GQTPAR in a trust region method, we used
al = 0.1 and a2 = 0 in the convergence criteria (3.10) and (3.11). The reason for setting
a2 = 0 is that a2 is only required to deal with the case where g = 0 and B is positive
semidefinite and singular, and in this situation, a trust region method terminates. The
initial choice of A depends on the test and is described below.
All tests were performed
in double precision on a VAX 11 /780. This provides an accuracy of about 17 significant
decimal digits.
The first set of tests is concerned with the performance of GQTPAR in the context
of a trust region Newton's method. The test problems used are the 18 unconstrained
minimization problems described in Mord, Garbow, and Hillstrom [1981a].
problem it is possible to specify a set of starting points, and in 8 of the problems it is
also possible to specify the dimension. A particular set of test cases is defined by the
data provided to the test drivers. We used the sample data provided in Mord, Garbow,
and Hillstrom [1981b] which defines 52 test cases.
The trust region Newton's method used follows Algorithm (4.2) and proved to be
quite successful on these problems. Details of the Newton method will appear elsewhere. For the purposes of this paper it suffices to remark that on the first call to
GQTPAR the initial A is zero, but on succeeding calls the initial A is the same as the final
A from the previous call of GQTPAR. At the end of Section 3 we pointed out that it is
possible to obtain a more educated guess for the initial A, but this choice provides a
stringent test of GQTPAR.
The performance of GQTPAR on these problems was very satisfactory. There were
2580 calls to GQTPAR and the average number of iterations per call was 1.63; the largest number of iterations was 10. In about 20% of the calls convergence criteria (3.11)
was satisfied.
The second set of tests is designed to exercise the various features of GQTPAR as
an individual algorithm on problems of type (1.1). For these problems we decided to
as the initial A. Unless other information is available, this is a reasonable automatic
In these problems we generated sequences of uniformly distributed random
numbers with the RAND function of Schrage . Given an integer seed, RAND generates a random number in (0,1) and changes the seed. Thus a sequence of random
numbers can be generated by repeated calls to RAND.
A convenient way to define a problem of type (1.1) is to set B = QDQT for some
orthogonal matrix Q and diagonal matrix D, and to then let g = Q6 for some vector g9.
This makes it possible to generate a (potentially) hard case by setting to zero the component of 9 corresponding to the smallest element of D. The structure of B is scrambled by choosing the orthogonal matrix Q of the form QQ2QS where
and the components of w3 are random numbers uniformly distributed in (-1,1). A problem of type (1.1) can be generated be specifying A, 9, and D; different choices lead to
problems with various characteristics.
We consider four different ways of specifying 9 and D. In all four cases, the elements of g and D are initially chosen as uniformly distributed random numbers in (-
1,1). This choice leads to the general case; as mentioned above, a hard case can then
be obtained by setting to zero the component of 9 corresponding to the smallest element of D. A positive definite case is obtained by replacing D by I|D 1, and in the saddle
point case all the components of g are set to zero.
The choice of A is critical; if A is chosen from (0,1) then the tests are easy because
(5.1) is almost always an excellent choice. A harder test is obtained if A is chosen as
uniformly distributed from (0,100), and this choice is made in our tests. We have
observed that a wider distribution in the choice of A does not affect the results
significantly, and that the range (0,100) appears to be the hardest choice for these
We now present the results of tests in each of the above four cases and for dimensions 10, 20, 40, 60, 80, and 100. For each case and each dimension we generated 5
problems and recorded both the average and the maximum number of iterations
required for convergence. The results are presented in the tables below.
Table 1. The General Case.
Number of iterations
Au interesting aspect of the results for the general case is that Algorithm (3.14)
terminated on condition (3.11) in 26 out of the 30 cases. This shows that (3.11) is
powerful enough to terminate the algorithm even on non-hard cases.
For smaller
values of a1, however, it is more difficult to satisfy (3.11) and this gives GQTPAR a
chance to produce an iterate X > -A1 with p(A) > 0. Once this occurs, the Newton
iteration converges quadratically and (3.10) is eventually satisfied. As noted above, the
results improve for smaller choices of A, and for example, if A is chosen from (0,1) then
the maximum number of iterations is 2.
Table 2. The Hard Case.
Number of iterations
The results of Table 2 show that the hard case, once recognized and treated properly, can be handled with the same computational effort as the general case. In contrast to the general case, the results for the hard case are sensitive to the choice of a1
since in this case it is necessary to determine X1 and Algorithm (3.14) determines X1
with a bisection-type process. Another interesting point is that for these problems
Algorithm (3.14) does not always terminate on condition (3.11) since the hard case only
occurs if A is greater than f(-X1).
This situation is avoided in the saddle point case by
choosing g = 0
Table 3. The Saddle Point Case.
Number of iterations
The saddle point case is unusual because the algorithm and the results are
independent of the choice of A, and termination always occurs on condition (3.11).
Although sitting g = 0 is an extreme choice, the numerical results are insensitive to
the choice of g provided the components of g are sufficiently small. For example, if the
components of g are chosen from (-10~ ,10- ) then the number of iterations increases
by 1 in two of the problems, but otherwise the results are unchanged.
In the positive definite case, the choice of 0 as a uniformly distributed random
number from (0,100) resulted in exits with A = 0 in about half the problems, and this
explains why the average number of iterations is close to 2. On the other hand, if A is
chosen from (0,1) then (5.1) leads to termination on the first iteration.
These results show that GQ'I'PAR performs adequately in all cases.
In particular,
the dimension of the problem has no apparent correlation to the number of iterations
required for convergence.
As expected, a smaller value of al requires more iterations,
but the increase is surprisingly small in most cases. The choice a = 0.1 is very satisfactory in many cases since it does not require a large number of iterations and produces a nearly optimal approximate solution as predicted by the theory.
6. Concluding Remarks.
We have presented an algorithm for the constrained quadratic minimization problem (1.1) and reported the computational results of the implementation GQTPAR. This
implementation uses the Cholesky factorization to solve systems of the form
(B + AI)u =v,
but it is possible to use other factorizations. For example, the decomposition
where Q is orthogonal and T is tridiagonal leads to systems of the form
(T + AI)w = QTv, u = Qw,
and since Algorithm (3.14) is invariant with respect to orthogonal transformations, it is
possible to produce an implementation which only requires on the order of n arithmetic operations per iteration. We have not used this factorization because we expect
GQTPAR to be used in a trust region method , and in this case our numerical results
show that a call to GQTPAR requires less than two Cholesky factorizations on the average.
Another argument against the use of factorization (6.1) is that it usually ignores
the structure of B. In particular, for sparse systems the Cholesky factorization offers
many advantages. Good software based on the Cholesky factorization currently exists
for the solution of positive definite linear systems, and this together with an estimator
of the smallest singular value of a sparse upper triangular matrix is all that is required
to provide a trust region Newton's method for optimization problems with a sparse Hessian matrix.
It would be of interest to develop a method for large scale problems of type (1.1)
which does not require the solution of linear systems. Iterative approaches along the
lines of conjugate directions or Lanczos type methods have been considered, but a
complete solution is not known to us.
Acknowledgment.
We would like to thank David Gay for his comments on a preliminary version of this
manuscript.
References.
Cline, A. K., Conn, A. R., and Van Loan, C. . Generalizing the LINPACK condition
estimator, Cornell University, Department of Computer Science, Report 51-462, Ithaca,
Cline, A. K., Moler, C. B., Stewart, G. W., and Wilkinson, J. H. . An estimate for the
condition number of a matrix, SIAM J. Numer. Anal. 16, 368-375.
Fletcher, R. . Practical Methods of Optimization, Volume 1: Unconstrained
Optimization, John Wiley & Sons.
Gay, D. M. . Computing optimal locally constrained steps, SIAM J. Sci. Stat. Conput. 2, 186-197.
Goldfeld, S., Quandt, R. and Trotter, H. . Maximization by quadratic hill climbing,
Econometrica 34, 541-551.
Hebden, M. D. . An algorithm for minimization using exact second derivatives,
Atomic Energy Research Establishment report T.P. 515, Harwell, England.
Mord, J. J. . The Levenberg-Marquardt algorithm: Implementation and theory,
Froceedings of the Dundee Conference on Numerical Analysis, G. A. Watson, ed.,
Springer-Verlag.
Mord, J. J., Garbow, B. S., and Hillstrom, K. E. [191a]. Testing unconstrained optimization software, ACM Trans. Math. Software 7, 17-41.
Mord, J. J., Garbow, B. S., and Hillstrom, K. E. [1981b]. Fortran subroutines for testing
unconstrained optimization software, ACM Trans. Math. Software 7, 136-140.
Powell, M. J. D. . Convergence properties of a class of minimization algorithms,
Nonlinear Programming 2, 0. L. Mangasarian, R. R. Meyer, and S. M. Robinson, eds.,
Academic Press.
Reinsch, C. H. . Smoothing by spline functions, Numer. Math. 10, 177-183.
Reinsch, C. H. . Smoothing by spline functions II, Numer. Math. 16, 451-454.
Schrage, L. . A more portable Fortran random number generator, ACM Trans.
Math. Software 5, 132-138.
Sorensen, D. C. . Newton's method with a model trust-region modification,
Argonne National Laboratory, Report ANL-80-106, Argonne, Illinois.
SIAM J. Numer.
Anal., to appear.
Sorensen, D. C. . Trust region methods for unconstrained minimization, Proceedings of the Advanced Research Institute on Nonlinear Optimization, M. J. D. Powell, ed.,
Academic Press.
Thomas, S. W. . Sequential estimation techniques for quasi-Newton algorithms,
Ph. D. dissertation, Cornell University, Ithaca, New York.