Doing Things Twice (Or Diﬀerently): Strategies
to Identify Studies for Targeted Validation
Gopal P. Sarma
1. School of Medicine, Emory University, Atlanta, GA USA
The “reproducibility crisis” has been a highly visible source of scientiﬁc controversy and dispute. Here,
I propose and review several avenues for identifying and prioritizing research studies for the purpose of
targeted validation. Of the various proposals discussed, I identify scientiﬁc literature text mining as being a
strategy that merits greater attention among those interested in reproducibility. I argue that the tremendous
potential of scientiﬁc literature text mining for uncovering high-value research studies is a signiﬁcant and
rarely discussed beneﬁt of the transition to a fully open-access publishing model.
Introduction
In recent years, signiﬁcant attention has been
given to problems with reproducibility in many
areas of science. Some of these analyses have been
theoretical in nature while others have been
focused eﬀorts aimed at replicating large numbers
of studies in a speciﬁc ﬁeld .
The issue has become suﬃciently high proﬁle
that it has been dubbed the “reproducibility
crisis,” and is now a major topic of and debate
in both the scientiﬁc and popular press .
One thing is clear—we simply do not know what
the “reproducibility distribution” looks like for the
entirety of science. Taking this position as a starting point, how then do we identify and prioritize
published results to investigate in greater detail?
Should reproducibility initiatives be strictly local
and originate from individual scientists themselves,
or should there be more global, distributed eﬀorts
as well? In this brief note, I examine this and other
questions and propose several strategies for identifying key results to be the focus of validation eﬀorts.
∗Email: 
Uncovering “Linchipin” Results
It goes without saying that all scientiﬁc results, even
the true ones, are not created equally. In order to
use resources eﬃciently, we would ideally identify “linchipin” results, that is, those studies which
would carry the highest impact if we had greater
certainty in their outcome. For instance, in cases
where poorly conducted or fraudulent studies form
the basis for guidelines or procedures in medicine,
lengthy retractions can have signiﬁcantly deleterious consequences for the public . To use an ecological metaphor, these studies might be described
as “keystone species” of the scientiﬁc ecosystem.
How can we uncover such results?
Reproduction Versus Validation
Although the phrase “reproducibility crisis” has
taken root in contemporary discussions, simply
re-doing an experiment may not always be the
most appropriate course of action. For instance,
there might be linchipin theoretical results which
simply need greater scrutiny or investigation with
alternative methods.
The same may be true of
certain experimental results, where the highest
value would be gained from re-thinking a given
experimental design using alternative techniques.
Therefore, to broaden the scope of the discussion
Doing Things Twice (Or Diﬀerently)
to include all scientiﬁc results, not just experimental ones, as well as approaches other than simply
repeating the original study under question, I will
use the phrase “validation eﬀort” rather than “reproducibility eﬀort.”
Have Individual Scientists Initiate Validation
A purely “local” approach would be for validation
eﬀorts to be initiated by investigators themselves.
For instance, Schooler and several colleagues at
UCLA arrived at an agreement whereby each
researcher’s lab would attempt to replicate the
results of the others prior to publication .
However, not all scientists are in a position to
create such arrangements. Therefore, formal mechanisms for arranging validation eﬀorts should be
encouraged. As an example, this is the principle
behind ScienceExchange’s Reproducibility Project1,
a marketplace for scientists to identify researchers
from a network of laboratories to validate their research.
Polling Scientists or Crowdsourcing
This approach would be more globally oriented
and could be initiated by funding agencies, individual laboratories, or by “open science” projects. The
strategy would be to distribute polls to scientists in
diﬀerent disciplines asking them what they believe
to be high-value results. Like any poll, a number
of practical issues will arise in arriving at reliable
data. Questions will likely have to be written by
scientists with suﬃcient experience in a given
ﬁeld to elicit reliable answers and to follow-up on
ambiguities in responses. The questions will have
to be framed appropriately. The administrators of
such polls will have to be mindful of the fact that
individuals could use a call for reproducibility as a
political tactic for attacking a competitor’s research.
Nonetheless, polling scientists is likely a very
straightforward strategy to uncover high-value results. The results of such polls could be used by
funding agencies, independent foundations, or in-
1 
dividual scientists themselves in deciding how to
allocate resources for validation eﬀorts.
Let Larger Research Agendas be the Focal
This strategy would be to focus validation eﬀorts
around those results that form the foundation of
major research agendas. For example, while soliciting proposals for new programs, funding agencies could ask scientists to submit a bibliography
containing results upon which their proposed research relies. Or more directly, researchers could
be asked to submit a separate document alongside
their grant proposals suggesting experiments that
merit additional investigation and which would
advance their own research. Scientists could also
publish such documents independent of grant applications, perhaps along the lines of a review article.
Scientiﬁc Literature Text Mining
Scientiﬁc literature text mining refers to the use
of data analytic techniques to treat the scientiﬁc
corpus itself as a massive data set for analysis .
Although the growth of data science has largely
been driven by commercial applications in social
media and business intelligence, we are now
beginning to see the applications of data science to
the scientiﬁc literature as well.
At the most basic level, article recommendation
by Google Scholar or the many reference managers
used by researchers is an example of data science
applied to scientiﬁc papers. Other examples of
contemporary research in scientiﬁc literature text
mining include fraud detection (applying natural
language processing to uncover linguistic signatures of fraudulent research) , characterizing
the emergence of global scientiﬁc trends (using
n-grams and patent citation networks to model the
ﬂow of ideas and technological development) ,
and resource allocation in the biomedical sciences
(developing metrics which incorporate disease
burden, research literature coverage, and clinical
trial coverage to uncover underfunded areas of
research) .
Doing Things Twice (Or Diﬀerently)
One can easily imagine using techniques from
machine learning and natural language processing to identify linchpin results, perhaps by examining citations networks, or using entity extraction
to model the emergence of new terminology and
concepts. The process would not need to be fully
automated. We could employ a hybrid approach
whereby data analytic techniques allow us to narrow down a corpus of tens of thousands of research
papers to a few dozen or a hundred. Subsequently,
with the guidance of experts and manual curation,
we would arrive at a list of candidate results or studies to be the focus of targeted validation eﬀorts.
Conclusion
Modern science is witnessing many growing pains,
one of which is an increasing concern about the
quality of research, as measured by reproducibility,
in a number of distinct areas of inquiry. Although
this concern has quickly grown to the point of
being labeled a “crisis,” the reality is that we
simply do not know what the distribution of
reproducibility rates looks like for the entirety of
Nonetheless, this very uncertainty should be
suﬃcient motivation to put into place procedures
and incentives to increase the reliability of published results. There are many approaches to take
in addressing this problem, a few of which have
been outlined above.
Most of these ideas have been discussed or attempted in some form or another in recent years.
The proposal that has received the least attention in
the context of the reproducibility crisis is scientiﬁc
literature text mining. One of the primary roadblocks to open-ended, exploratory data analysis
with a large corpus of scientiﬁc papers is restrictions on their availability—in other words, closed
access publishing models. Therefore, researchers
should consider the applications of scientiﬁc literature text mining to uncovering linchpin results to be
a key motivating factor in encouraging a complete
transition to an open access publishing model.
Acknowledgments
I would like to thank Adam Safron, P. Ravi Sarma,
and Daniel Weissman for insightful discussions and
feedback on the manuscript. A special thanks to
A.S. for suggesting the phrase “keystone species.”
Gopal P. Sarma
0000-0002-9413-6202