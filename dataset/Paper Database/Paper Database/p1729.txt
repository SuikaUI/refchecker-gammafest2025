CenterNet: Keypoint Triplets for Object Detection
Kaiwen Duan1∗
Lingxi Xie3
Honggang Qi1
Qingming Huang1
1University of Chinese Academy of Sciences
2University of Oxford
3Huawei Noah’s Ark Lab
 
 
 
 
 
 
In object detection, keypoint-based approaches often suffer a large number of incorrect object bounding boxes, arguably due to the lack of an additional look into the cropped
regions. This paper presents an efﬁcient solution which explores the visual patterns within each cropped region with
minimal costs.
We build our framework upon a representative one-stage keypoint-based detector named Corner-
Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which
improves both precision and recall. Accordingly, we design two customized modules named cascade corner pooling and center pooling, which play the roles of enriching
information collected by both top-left and bottom-right corners and providing more recognizable information at the
central regions, respectively. On the MS-COCO dataset,
CenterNet achieves an AP of 47.0%, which outperforms all
existing one-stage detectors by at least 4.9%. Meanwhile,
with a faster inference speed, CenterNet demonstrates quite
comparable performance to the top-ranked two-stage detectors. Code is available at 
Duankaiwen/CenterNet.
1. Introduction
Object detection has been signiﬁcantly improved and advanced with the help of deep learning, especially convolutional neural networks (CNNs). In the current era, one
of the most popular ﬂowcharts is anchor-based , which placed a set of rectangles with pre-deﬁned
sizes, and regressed them to the desired place with the help
of ground-truth objects.
These approaches often need a
large number of anchors to ensure a sufﬁciently high IoU
(intersection over union) rate with the ground-truth objects,
and the size and aspect ratio of each anchor box need to
be manually designed. In addition, anchors are usually not
∗This work was done when the ﬁrst author was interning at Huawei
Noah’s Ark Lab.
potted plant
potted plant
potted plant
potted plant
potted plant
potted plant
potted plant
potted plant
potted plant
potted plant
potted plant
potted plant
potted plant
potted plant
potted plant
potted plant
potted plant
potted plant
potted plant
potted plant
potted plant
Figure 1: In the ﬁrst row, we visualize the top 100 bounding boxes (according to the MS-COCO dataset standard) of
CornerNet. Ground-truth and predicted objects are marked
in blue and red, respectively. In the second row, we show
that correct predictions can be determined by checking the
central parts.
aligned with the ground-truth boxes, which is not conducive
to the bounding box classiﬁcation task.
To overcome the drawbacks of anchor-based approaches,
a keypoint-based object detection pipeline named Corner-
Net was proposed.
It represented each object by a
pair of corner keypoints, which bypassed the need of anchor
boxes and achieved the state-of-the-art one-stage object detection accuracy. Nevertheless, the performance of Corner-
Net is still restricted by its relatively weak ability of referring to the global information of an object. That is to say,
since each object is constructed by a pair of corners, the algorithm is sensitive to detect the boundary of objects, meanwhile not being aware of which pairs of keypoints should be
grouped into objects. Consequently, as shown in Figure 1,
it often generates some incorrect bounding boxes, most of
which could be easily ﬁltered out with complementary information, e.g., the aspect ratio.
 
To address this issue, we equip CornerNet with an ability
of perceiving the visual patterns within each proposed region, so that it can identify the correctness of each bounding
box by itself. In this paper, we present a low-cost yet effective solution named CenterNet, which explores the central
part of a proposal, i.e., the region that is close to the geometric center, with one extra keypoint. Our intuition is that,
if a predicted bounding box has a high IoU with the groundtruth box, then the probability that the center keypoint in its
central region is predicted as the same class is high, and vice
versa. Thus, during inference, after a proposal is generated
as a pair of corner keypoints, we determine if the proposal
is indeed an object by checking if there is a center keypoint
of the same class falling within its central region. The idea,
as shown in Figure 1, is to use a triplet, instead of a pair, of
keypoints to represent each object.
Accordingly, for better detecting center keypoints and
corners, we propose two strategies to enrich center and corner information, respectively. The ﬁrst strategy is named
center pooling, which is used in the branch for predicting center keypoints. Center pooling helps the center keypoints obtain more recognizable visual patterns within objects, which makes it easier to perceive the central part of a
proposal. We achieve this by getting out the max summed
response in both horizontal and vertical directions of the
center keypoint on a feature map for predicting center keypoints. The second strategy is named cascade corner pooling, which equips the original corner pooling module 
with the ability of perceiving internal information.
achieve this by getting out the max summed response in
both boundary and internal directions of objects on a feature map for predicting corners. Empirically, we verify that
such a two-directional pooling method is more stable, i.e.,
being more robust to feature-level noises, which contributes
to the improvement of both precision and recall.
We evaluate the proposed CenterNet on the MS-COCO
dataset , one of the most popular benchmarks for largescale object detection. CenterNet, with both center pooling and cascade corner pooling incorporated, reports an AP
of 47.0% on the test-dev set, which outperforms all existing one-stage detectors by a large margin. With an average
inference time of 270ms using a 52-layer hourglass backbone and 340ms using a 104-layer hourglass backbone per image, CenterNet is quite efﬁcient yet closely
matches the state-of-the-art performance of the other twostage detectors.
The remainder of this paper is organized as follows. Section 2 brieﬂy reviews related work, and Section 3 details the
proposed CenterNet. Experimental results are given in Section 4, followed by the conclusion in Section 5.
2. Related Work
Object detection involves locating and classifying the
objects. In the deep learning era, powered by deep convolutional neural networks, object detection approaches can
be roughly categorized into two main types of pipelines,
namely, two-stage approaches and one-stage approaches.
Two-stage approaches divide the object detection task into
two stages: extract RoIs, then classify and regress the RoIs.
R-CNN uses a selective search method to locate RoIs in the input images and uses a DCN-based regionwise classiﬁer to classify the RoIs independently. SPP-
Net and Fast-RCNN improve R-CNN by extracting the RoIs from the feature maps. Faster-RCNN is
allowed to be trained end to end by introducing RPN (region proposal network). RPN can generate RoIs by regressing the anchor boxes. Later, the anchor boxes are widely
used in the object detection task. Mask-RCNN adds
a mask prediction branch on the Faster-RCNN, which can
detect objects and predict their masks at the same time.
R-FCN replaces the fully connected layers with the
position-sensitive score maps for better detecting objects.
Cascade R-CNN addresses the problem of overﬁtting at
training and quality mismatch at inference by training a sequence of detectors with increasing IoU thresholds. The
keypoint-based object detection approaches are
proposed to avoid the disadvantages of using anchor boxes
and bounding boxes regression. Other meaningful works
are proposed for different problems in object detection,
e.g., focus on the architecture design, 
focus on the contextual relationship, focus on the
multi-scale uniﬁcation.
One-stage approaches remove the RoI extraction process
and directly classify and regress the candidate anchor boxes.
YOLO uses fewer anchor boxes (divide the input
image into an S × S grid) to do regression and classiﬁcation.
YOLOv2 improves the performance by using more anchor boxes and a new bounding box regression
method. SSD places anchor boxes densely over an input image and use features from different convolutional layers to regress and classify the anchor boxes. DSSD introduces a deconvolution module into SSD to combine lowlevel and high-level features. While R-SSD uses pooling and deconvolution operations in different feature layers to combine low-level and high-level features. RON 
proposes a reverse connection and an objectness prior to extract multiscale features effectively. ReﬁneDet reﬁnes
the locations and sizes of the anchor boxes for two times,
which inherits the merits of both one-stage and two-stage
approaches. CornerNet is another keypoint-based approach, which directly detects an object using a pair of corners. Although CornerNet achieves high performance, it
still has more room to improve.
Embeddings
and Offsets
Figure 2: Architecture of CenterNet. A convolutional backbone network applies cascade corner pooling and center pooling to
output two corner heatmaps and a center keypoint heatmap, respectively. Similar to CornerNet, a pair of detected corners and
the similar embeddings are used to detect a potential bounding box. Then the detected center keypoints are used to determine
the ﬁnal bounding boxes.
Table 1: False discovery rates (%) of CornerNet. The false
discovery rate reﬂects the distribution of incorrect bounding
boxes. The results suggest the incorrect bounding boxes
account for a large proportion.
3. Our Approach
3.1. Baseline and Motivation
This paper uses CornerNet as the baseline.
detecting corners, CornerNet produces two heatmaps: a
heatmap of top-left corners and a heatmap of bottom-right
corners. The heatmaps represent the locations of keypoints
of different categories and assigns a conﬁdence score for
each keypoint. Besides, it also predicts an embedding and a
group of offsets for each corner. The embeddings are used
to identify if two corners are from the same object. The
offsets learn to remap the corners from the heatmaps to the
input image. For generating object bounding boxes, top-k
left-top corners and bottom-right corners are selected from
the heatmaps according to their scores, respectively. Then,
the distance of the embedding vectors of a pair of corners is
calculated to determine if the paired corners belong to the
same object. An object bounding box is generated if the distance is less than a threshold. The bounding box is assigned
a conﬁdence score, which equals to the average scores of
the corner pair.
In Table 1, we provide a deeper analysis of CornerNet.
We count the FD1 (false discovery) rate of CornerNet on
the MS-COCO validation dataset, deﬁned as the proportion of the incorrect bounding boxes. The quantitative re-
1 −AP, where AP denotes the average precision at
IoU = [0.05 : 0.05 : 0.5] on the MS-COCO dataset.
1 −APi, where APi denotes the average precision at IoU = i/100,
FDscale = 1 −APscale, where scale = {small, medium, large}, denotes the scale of object.
sults demonstrate the incorrect bounding boxes account for
a large proportion even at low IoU thresholds, e.g., Corner-
Net obtains 32.7% FD rate at IoU = 0.05. This means in
average, 32.7 out of every 100 object bounding boxes have
IoU lower than 0.05 with the ground-truth. The small incorrect bounding boxes are even more, which achieves 60.3%
FD rate. One of the possible reasons lies in that CornerNet
cannot look into the regions inside the bounding boxes. To
make CornerNet perceive the visual patterns in bounding boxes, one potential solution is to adapt CornerNet into
a two-stage detector, which uses the RoI pooling to
look into the visual patterns in bounding boxes. However,
it is known that such a paradigm is computationally expensive.
In this paper, we propose a highly efﬁcient alternative
called CenterNet to explore the visual patterns within each
bounding box. For detecting an object, our approach uses
a triplet, rather than a pair, of keypoints. By doing so, our
approach is still a one-stage detector, but partially inherits
the functionality of RoI pooling. Our approach only pays
attention to the center information, the cost of our approach
is minimal. Meanwhile, we further introduce the visual patterns within objects into the keypoint detection process by
using center pooling and cascade corner pooling.
3.2. Object Detection as Keypoint Triplets
The overall network architecture is shown in Figure 2.
We represent each object by a center keypoint and a pair of
corners. Speciﬁcally, we embed a heatmap for the center
keypoints on the basis of CornerNet and predict the offsets
of the center keypoints. Then, we use the method proposed
in CornerNet to generate top-k bounding boxes. However, to effectively ﬁlter out the incorrect bounding boxes,
we leverage the detected center keypoints and resort to the
following procedure: (1) select top-k center keypoints according to their scores; (2) use the corresponding offsets
to remap these center keypoints to the input image; (3) de-
(tlx, tly)
(brx, bry)
(cbrx, cbry)
(ctlx, ctly)
(tlx, tly)
(brx, bry)
(cbrx, cbry)
(ctlx, ctly)
(tlx, tly)
(brx, bry)
(cbrx, cbry)
(ctlx, ctly)
Figure 3: (a) The central region when n = 3. (b) The central region when n = 5. The solid rectangles denote the
predicted bounding boxes and the shaded regions denote the
central regions.
ﬁne a central region for each bounding box and check if the
central region contains center keypoints. Note that the class
labels of the checked center keypoints should be same as
that of the bounding box; (4) if a center keypoint is detected
in the central region, we will preserve the bounding box.
The score of the bounding box will be replaced by the average scores of the three points, i.e., the top-left corner, the
bottom-right corner and the center keypoint. If there are no
center keypoints detected in its central region, the bounding
box will be removed.
The size of the central region in the bounding box affects the detection results. For example, smaller central regions lead to a low recall rate for small bounding boxes,
while larger central regions lead to a low precision for large
bounding boxes. Therefore, we propose a scale-aware central region to adaptively ﬁt the size of bounding boxes.
The scale-aware central region tends to generate a relatively
large central region for a small bounding box, while a relatively small central region for a large bounding box. Suppose we want to determine if a bounding box i needs to be
preserved. Let tlx and tly denote the coordinates of the topleft corner of i and brx and bry denote the coordinates of
the bottom-right corner of i. Deﬁne a central region j. Let
ctlx and ctly denote the coordinates of the top-left corner of
j and cbrx and cbry denote the coordinates of the bottomright corner of j. Then tlx, tly, brx, bry, ctlx, ctly, cbrx
and cbry should satisfy the following relationship:
ctlx = (n + 1)tlx + (n −1)brx
ctly = (n + 1)tly + (n −1)bry
cbrx = (n −1)tlx + (n + 1)brx
cbry = (n −1)tly + (n + 1)bry
where n is odd that determines the scale of the central remax
Figure 4: (a) Center pooling takes the maximum values in
both horizontal and vertical directions. (b) Corner pooling
only takes the maximum values in boundary directions. (c)
Cascade corner pooling takes the maximum values in both
boundary directions and internal directions of objects.
gion j. In this paper, n is set to be 3 and 5 for the scales
of bounding boxes less and greater than 150, respectively.
Figure 3 shows two central regions when n = 3 and n = 5,
respectively. According to Equation (1), we can determine
a scale-aware central region, then we check if the central
region contains center keypoints.
3.3. Enriching Center and Corner Information
Center pooling. The geometric centers of objects do not
necessarily convey very recognizable visual patterns (e.g.,
the human head contains strong visual patterns, but the center keypoint is often in the middle of the human body). To
address this issue, we propose center pooling to capture
richer and more recognizable visual patterns. Figure 4(a)
shows the principle of center pooling. The detailed process of center pooling is as follows: the backbone outputs a
feature map, and to determine if a pixel in the feature map
is a center keypoint, we need to ﬁnd the maximum value
in its both horizontal and vertical directions and add them
together. By doing this, center pooling helps the better detection of center keypoints.
Cascade corner pooling. Corners are often outside the
objects, which lacks local appearance features.
Net uses corner pooling to address this issue.
principle of corner pooling is shown in Figure 4(b). Corner
pooling aims to ﬁnd the maximum values on the boundary
directions so as to determine corners. However, it makes
corners sensitive to the edges.
To address this problem,
we need to let corners “see” the visual patterns of objects.
The principle of cascade corner pooling is presented in Figure 4(c). It ﬁrst looks along a boundary to ﬁnd a boundary maximum value, then looks inside along the location of
the boundary maximum value2 to ﬁnd an internal maximum
value, and ﬁnally, add the two maximum values together.
By doing this, the corners obtain both the the boundary information and the visual patterns of objects.
Both the center pooling and the cascade corner pooling
can be easily achieved by combining the corner pooling 
2For the topmost, leftmost, bottommost and rightmost boundary, look
vertically towards the bottom, horizontally towards the right, vertically towards the top and horizontally towards the left, respectively.
Center Pooling Module
3x3 Conv-BN-ReLU
Top Pooling
Bottom Pooling
Left Pooling
Right Pooling
3x3 Conv-BN
Cascade Top Corner Pooling Module
Figure 5: The structures of the center pooling module (a)
and the cascade top corner pooling module (b). We achieve
center pooling and the cascade corner pooling by combining
the corner pooling at different directions.
at different directions. Figure 5(a) shows the structure of the
center pooling module. To take a maximum value in a direction, e.g., the horizontal direction, we only need to connect
the left pooling and the right pooling in series. Figure 5(b)
shows the structure of a cascade top corner pooling module.
Compared with the top corner pooling in CornerNet ,
we add a left corner pooling before the top corner pooling.
3.4. Training and Inference
Our method is implemented in Pytorch 
and the network is trained from scratch. The resolution of
the input image is 511 × 511, leading to heatmaps of size
128×128. We use the data augmentation strategy presented
in to train a robust model. Adam is used to optimize the training loss:
det + αLco
pull + βLco
push + γ (Lco
det and Lce
det denote the focal losses, which are used
to train the network to detect corners and center keypoints,
respectively. Lco
pull is a “pull” loss for corners, which is used
to minimize the distance of the embedding vectors that belongs to the same objects. Lco
push is a “push” loss for corners,
which is used to maximize the distance of the embedding
vectors that belongs to different objects. Lco
ℓ1-losses , which are used to train the network to predict
the offsets of corners and center keypoints, respectively. α,
β and γ denote the weights for corresponding losses, which
are set to 0.1, 0.1 and 1, respectively. Ldet, Lpull, Lpush
and Loﬀare all deﬁned in the CornerNet, we suggest to refer to for details. We train the CenterNet on 8 Tesla
V100 (32GB) GPUs and use a batch size of 48. The maximum number of iterations is 480K. We use a learning rate
of 2.5×10−4 for the ﬁrst 450K iterations and then continue
training 30K iterations with a rate of 2.5 × 10−5.
Inference. Following , for the single-scale testing, we
input both the original and horizontally ﬂipped images with
the original resolutions into the network.
While for the
multi-scale testing, we input both the original and horizontally ﬂipped images with the resolutions of 0.6, 1, 1.2, 1.5
and 1.8. We select top 70 center keypoints, top 70 top-left
corners and top 70 bottom-right corners from the heatmaps
to detect the bounding boxes. We ﬂip the bounding boxes
detected in the horizontally ﬂipped images and mix them
into the original bounding boxes. Soft-nms is used to remove the redundant bounding boxes. We ﬁnally select top
100 bounding boxes according to their scores as the ﬁnal
detection results.
4. Experiments
4.1. Dataset, Metrics and Baseline
We evaluate our method on the MS-COCO dataset .
It contains 80 categories and more than 1.5 million object
instances. The large number of small objects makes it a very
challenging dataset. We use the ‘trainval35k’ set (i.e.,
80K training images and 35K validation images) for training and test the results on the test-dev set. We use another
5K images in the validation set to perform ablation studies
and visualization experiments.
MS-COCO dataset uses AP and AR metrics to characterize the performance of a detector. AP represents the
average precision rate, which is computed over ten different
IoU thresholds (i.e., 0.5 : 0.05 : 0.95) and all categories. It
is considered the single most important metric on the MS-
COCO dataset. AR represents the maximum recall rate,
which is computed over a ﬁxed number of detections (i.e., 1,
10 and 100 ) per image and averaged over all categories and
the ten different IoU thresholds. Additionally, AP and AR
can be used to evaluate the performance under different object scales, including small objects (area < 322), medium
objects (322 < area < 962) and large objects (area > 962).
Our direct baseline is CornerNet . Following it, we
use the stacked hourglass network with 52 and 104 layers as the backbone – the latter has two hourglass modules
while the former has only one. All modiﬁcations on the
hourglass architecture, made by , are preserved.
4.2. Comparisons with State-of-the-art Detectors
Table 2 shows the comparison with the state-of-the-art
detectors on the MS-COCO test-dev set.
Compared with the baseline CornerNet , the proposed CenterNet achieves a remarkable improvement. For
example, CenterNet511-52 (means that the resolution of input images is 511 × 511 and the backbone is Hourglass-52)
reports a single-scale testing AP of 41.6%, an improvement
of 3.8% over 37.8%, and a multi-scale testing AP of 43.5%,
an improvement of 4.1% over 39.4%, achieved by Corner-
Net under the same setting. When using the deeper backbone (i.e., Hourglass-104), the AP improvement over CornerNet are 4.4% (from 40.5% to 44.9%) and 4.9% (from
42.1% to 47.0%) under the single-scale and multi-scale testing, respectively. These results ﬁrmly demonstrate the effectiveness of CenterNet.
Train input
Test input
Two-stage:
DeNet 
ResNet-101 
CoupleNet 
ResNet-101
Faster R-CNN by G-RMI 
Inception-ResNet-v2 
Faster R-CNN +++ 
ResNet-101
Faster R-CNN w/ FPN 
ResNet-101
Faster R-CNN w/ TDM 
Inception-ResNet-v2
Aligned-Inception-ResNet
Regionlets 
ResNet-101
Mask R-CNN 
ResNeXt-101
Soft-NMS 
Aligned-Inception-ResNet
Fitness R-CNN 
ResNet-101
Cascade R-CNN 
ResNet-101
Grid R-CNN w/ FPN 
ResNeXt-101
D-RFCN + SNIP (multi-scale) 
DPN-98 
∼2000×1200
∼2000×1200
PANet (multi-scale) 
ResNeXt-101
One-stage:
YOLOv2 
DarkNet-19
DSOD300 
DS/64-192-48-1
GRP-DSOD320 
DS/64-192-48-1
SSD513 
ResNet-101
DSSD513 
ResNet-101
ReﬁneDet512 (single-scale) 
ResNet-101
CornerNet511 (single-scale) 
Hourglass-52
RetinaNet800 
ResNet-101
CornerNet511 (multi-scale) 
Hourglass-52
CornerNet511 (single-scale) 
Hourglass-104
ReﬁneDet512 (multi-scale) 
ResNet-101
CornerNet511 (multi-scale) 
Hourglass-104
CenterNet511 (single-scale)
Hourglass-52
CenterNet511 (single-scale)
Hourglass-104
CenterNet511 (multi-scale)
Hourglass-52
CenterNet511 (multi-scale)
Hourglass-104
Table 2: Performance comparison (%) with the state-of-the-art methods on the MS-COCO test-dev dataset. CenterNet
outperforms all existing one-stage detectors by a large margin and ranks among the top of state-of-the-art two-stage detectors.
Meanwhile, it can be seen that the most contribution
comes from the small objects. For instance, CenterNet511-
52 improves the AP for small objects by 5.5% (singlescale) and by 6.4% (multi-scale).
As for the backbone
Hourglass-104, the improvements are 6.2% (single-scale)
and by 8.1% (multi-scale), respectively. The beneﬁt stems
from the center information modeled by the center keypoints: the smaller the scale of an incorrect bounding box is,
the lower probability a center keypoint can be detected in its
central region. Figure 6(a) and Figure 6(b) show some qualitative comparisons, which demonstrate the effectiveness of
CenterNet in reducing small incorrect bounding boxes.
CenterNet also leads to a large improvement for reducing medium and large incorrect bounding boxes. As Table 2 shows, CenterNet511-104 improves the single-scale
testing AP by 4.7% (from 42.7% to 47.4%) and 3.5% (from
53.9% to 57.4%), respectively. Figure 6(c) and Figure 6(d)
show some qualitative comparisons for reducing medium
and large incorrect bounding boxes. It is worth noting that
the AR is also signiﬁcantly improved, with the best performance achieved with multi-scale testing. This is because
our approach removes lots of incorrect bounding boxes,
which is equivalent to improving the conﬁdence of those
bounding boxes with accurate locations but lower scores.
approaches,
CenterNet511-52 reports 41.6% single-scale testing AP.
This achievement is already better than those using deeper
models (e.g., RetinaNet800 and ReﬁneDet ). The
best performance of CenterNet is AP 47.0%, dramatically
surpassing all the published one-stage approaches to our
best knowledge.
At last, one can observe that the performance of
CenterNet is also competitive with the two-stage approaches, e.g., the single-scale testing AP of CenterNet511-
52 is comparable to the two-stage approach Fitness R-
CNN (41.6% vs. 41.8%) and that of CenterNet511-104
is comparable to D-RFCN + SNIP (44.9% vs. 45.7%),
respectively. Nevertheless, it should be mentioned that twostage approaches usually use larger resolution input images
(e.g., ∼1000 × 600), which signiﬁcantly improves the detection accuracy especially for small objects. The multiscale testing AP 47.0% achieved by CenterNet511-104
closely matches the state-of-the-art AP 47.4%, achieved by
the two-stage detector PANet . We present some qualitative detection results in Figure 7.
4.3. Incorrect Bounding Box Reduction
The AP metric reﬂects how many high quality object bounding boxes (usually IoU ⩾0.5) a network can predict, but cannot directly reﬂect how many incorrect object
bounding boxes (usually IoU ≪0.5) a network generates.
The FD rate is a suitable metric, which reﬂects the proportion of the incorrect bounding boxes. Table 3 shows the FD
rates for CornerNet and CenterNet. CornerNet generates
many incorrect bounding boxes even at IoU = 0.05 threshold, i.e., CornerNet511-52 and CornerNet511-104 obtain
35.2% and 32.7% FD rate, respectively.
On the other
hand, CornerNet generates more small incorrect bounding
sports ball
motorcycle
motorcycle
dining table
cell phone
cell phone
traffic light
traffic light
traffic light
traffic light
traffic light
potted plant
traffic light
traffic light
traffic light
potted plant
Figure 6: (a) and (b) show the small incorrect bounding boxes are signiﬁcantly reduced by modeling center information.
(c) and (d) show that the center information works for reducing medium and large incorrect bounding boxes. (e) shows the
results of detecting the center keypoints without/with the center pooling. (f) shows the results of detecting the corners with
corner pooling and cascade corner pooling, respectively. The blue boxes above denote the ground-truth. The red boxes and
dots denote the predicted bounding boxes and keypoints, respectively.
sports ball
traffic light
traffic light
traffic light
traffic light
traffic light
traffic light
fire hydrant
traffic light
traffic light
traffic light
traffic light
motorcycle
motorcycle
motorcycle
wine glass
dining table
refrigerator
sports ball
baseball bat
baseball glove
Figure 7: Some qualitative detection results on the MS-COCO validation dataset. Only detections with scores higher than
0.5 are shown.
CornerNet511-52
CenterNet511-52
CornerNet511-104
CenterNet511-104
Table 3: Comparison of false discovery rates (%) of CornerNet and CenterNet on the MS-COCO validation dataset.
The results suggest CenterNet avoids a large number of
incorrect bounding boxes, especially for small incorrect
bounding boxes.
boxes than medium and large incorrect bounding boxes,
which reports 62.5% for CornerNet511-52 and 60.3% for
CornerNet511-104, respectively. Our CenterNet decreases
the FD rates at all criteria via exploring central regions.
For instance, CenterNet511-52 and CenterNet511-104 decrease FD5 by both 4.5%. In addition, the FD rates for
small bounding boxes decrease the most, which are 9.5%
by CenterNet511-52 and 9.6% by CenterNet511-104, respectively. This is also the reason why the AP improvement
for small objects is more prominent.
4.4. Inference Speed
The proposed CenterNet explores the visual patterns
within each proposed region with minimal costs. To ensure
a fair comparison, we test the inference speed of both CornerNet and CenterNet on a NVIDIA Tesla P100 GPU.
We obtain that the average inference time of CornerNet511-
104 is 300ms per image and that of CenterNet511-104 is
340ms. Meanwhile, using the Hourglass-52 backbone can
speed up the inference speed. Our CenterNet511-52 takes
an average of 270ms to process per image, which is faster
and more accurate than CornerNet511-104.
4.5. Ablation Study
Our work has contributed three components, including
central region exploration, center pooling and cascade corner pooling. To analyze the contribution of each individual
component, an ablation study is given here. The baseline is
CornerNet511-52 . We add the three components to the
baseline one by one and follow the default parameter setting
detailed in Section 4.1. The results are given in Table 4.
Central region exploration. To understand the importance
of the central region exploration (see CRE in the table), we
add a center heatmap branch to the baseline and use a triplet
of keypoints to detect bounding boxes. For the center keypoint detection, we only use conventional convolutions. As
presented in the third row in Table 4, we improve the AP
by 2.3% (from 37.6% to 39.9%). However, we ﬁnd that the
improvement for the small objects (that is 4.6%) is more
signiﬁcant than that for other object scales. The improvement for large objects is almost negligible (from 52.2% to
52.3%). This is not surprising because, from a probabilistic
point of view, the center keypoint for a small object is easier
Table 4: Ablation study on the major components of CenterNet511-52 on the MS-COCO validation dataset. The CRE denotes
central region exploration, the CTP denotes center pooling, and the CCP denotes cascade corner pooling.
CenterNet511-52 w/o GT
CenterNet511-52 w/ GT
CenterNet511-104 w/o GT
CenterNet511-104 w/ GT
Error analysis of center keypoints via using
ground-truth.
we replace the predicted center keypoints
with the ground-truth values, the results suggest there is still
room for improvement in detecting center keypoints.
to be located than that of a large object.
Center pooling. To demonstrate the effectiveness of proposed center pooling, we then add the center pooling module to the network (see CTP in the table). The fourth row in
Table 4 shows that center pooling improves the AP by 0.9%
(from 39.9% to 40.8%). Notably, with the help of center
pooling, we improve the AP for large objects by 1.4% (from
52.2% to 53.6%), which is much higher than the improvement using conventional convolutions (i.e., 1.4% vs. 0.1%).
It demonstrates that our center pooling is effective in detecting center keypoints of objects, especially for large objects.
Our explanation is that center pooling can extract richer internal visual patterns, and larger objects contain more accessible internal visual patterns. Figure 6(e) shows the results
of detecting center keypoints without/with center pooling.
We can see the conventional convolution fails to locate the
center keypoint for the cow, but with center pooling, it successfully locates the center keypoint.
Cascade corner pooling. We replace corner pooling 
with cascade corner pooling to detect corners (see CCP in
the table). The second row in Table 4 shows the results that
we test on the basis of CornerNet511-52. We ﬁnd that cascade corner pooling improves the AP by 0.7% (from 37.6%
to 38.3%). The last row shows the results that we test on
the basis of CenterNet511-52, which improves the AP by
0.5% (from 40.8% to 41.3%). The results of the second
row show there is almost no change in the AP for large objects (i.e., 52.2% vs. 52.2%), but the AR is improved by
1.8% (from 74.0% to 75.8%). This suggests that cascade
corner pooling can “see” more objects due to the rich internal visual patterns in large objects, but too rich visual
patterns may interfere with its perception for the boundary
information, leading to many inaccurate bounding boxes.
After equipping with our CenterNet, the inaccurate bounding boxes are effectively suppressed, which improves the
AP for large objects by 2.2% (from 53.6% to 55.8%). Figure 6(f) shows the result of detecting corners with corner
pooling or cascade corner pooling. We can see that cascade
corner pooling can successfully locate a pair of corners for
the cat on the left while corner pooling cannot.
4.6. Error Analysis
The exploration of visual patterns within each bounding
box depends on the center keypoints. In other words, once
a center keypoint is missed, the proposed CenterNet would
miss the visual patterns within the bounding box. To understand the importance of center keypoints, we replace the
predicted center keypoints with the ground-truth values and
evaluate performance on the MS-COCO validation dataset.
Table 5 shows that using the ground-truth center keypoints
improves the AP from 41.3% to 56.5% for CenterNet511-
52 and from 44.8% to 58.1% for CenterNet511-104, respectively. APs for small, medium and large objects are improved by 15.5%, 16.5%, and 14.5% for CenterNet511-52
and 14.5%, 14.1%, and 13.3% for CenterNet511-104, respectively. This demonstrates that the detection of center
keypoints is far from the bottleneck.
5. Conclusions
In this paper, we propose CenterNet, which detects objects using a triplet, including one center keypoint and two
corners. Our approach addresses the problem that Corner-
Net lacks an additional look into the cropped regions by
exploring the visual patterns within each proposed region
with minimal costs. In fact, this is a common defect for all
one-stage approaches. As one-stage approaches remove the
RoI extraction process, they cannot pay attention to internal
information within cropped regions.
An intuitive explanation of our contribution lies in
that we equip a one-stage detector with the ability of
two-stage approaches, with an efﬁcient discriminator
being added. We believe that our idea of adding an extra
branch for the center keypoint can be potentially generalized to other existing one-stage approaches (e.g., SSD ).
Meanwhile, some advanced training strategies can be
used for better performance. We leave as our future work.