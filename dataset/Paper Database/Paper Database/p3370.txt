MULTI-DISTRIBUTION DEEP BELIEF NETWORK FOR SPEECH SYNTHESIS
Shiyin Kang, Xiaojun Qian and Helen Meng
Human Computer Communications Laboratory,
Department of Systems Engineering and Engineering Management,
The Chinese University of Hong Kong, Hong Kong SAR, China
Deep belief network (DBN) has been shown to be a good
generative model in tasks such as hand-written digit image
generation. Previous work on DBN in the speech community
mainly focuses on using the generatively pre-trained DBN to
initialize a discriminative model for better acoustic modeling
in speech recognition (SR). To fully utilize its generative
nature, we propose to model the speech parameters including
spectrum and F0 simultaneously and generate these parameters from DBN for speech synthesis. Compared with the predominant HMM-based approach, objective evaluation shows
that the spectrum generated from DBN has less distortion.
Subjective results also conﬁrm the advantage of the spectrum
from DBN, and the overall quality is comparable to that of
context-independent HMM.
Index Terms— Speech synthesis, Deep belief network
1. INTRODUCTION
The past decade has witnessed the success of HMM-based
text-to-speech (TTS) synthesis . The core underpinning
techniques include: (1) the adoption of multi-space distribution HMMs in estimating the statistical behavior of speech
parameters ; (2) the parameter generation algorithm which
uses dynamic features to smooth the originally piece-wise
constant speech parameters drawn from the HMM states .
This work is our ﬁrst attempt in using deep belief network
(DBN) to synthesize speech. DBN is a probabilistic generative model which is composed of multiple layers of stochastic
hidden variables . Previously, DBN has been shown to
learn very good generative and discriminative models on
high-dimensional data such as handwritten digits ,facial
pictures , human motion and large vocabulary speech
recognition . This motivates us to use DBN to model the
wide-band spectrogram and F0 contour for speech synthesis.
In this paper, the basic linguistic unit is the Mandarin
tonal syllable. We keep the speech vocoding framework as in
conventional statistical parametric speech synthesis . DBN
is used to model the joint probability of syllables and speech
parameters.
The major difference between our approach and the prevalent HMM-based approach is: Instead of using a variable
sequence of states to represent the time dynamics of each
syllable, we evenly sample a ﬁxed number of frames within
the delimited syllable boundary with high resolution.
Besides, in contrast to previous application of DBN in acoustic
modeling for SR, we model the continuous spectrum, discrete
voice/unvoiced decision and the multi-space F0 pattern simultaneously rather than only the continuous spectrum .
The rest of the paper is organized as follows: We will
introduce DBN in the context of speech synthesis in Section 2.
The procedures to synthesize speech using a DBN will be
described in Section 3. The experiments and results will be
shown in Section 4. Finally we present the conclusions and
the future directions.
2. MULTI-DISTRIBUTION DEEP BELIEF
NETWORK (MD-DBN)
Speech production theory describes how the linguistic message undergoes a series of neuromuscular and articulatory
processes before acoustic realization. To mimic this sequential process, we attempt to model speech production with a
directed belief network. However, it is difﬁcult to learn a
multi-layered belief network layer by layer, which involves
inferring the posterior of hidden units immediately above.
The insight of states that this inference can be greatly
simpliﬁed, i.e., just deriving the posterior from an up-pass,
if we assume the factorial prior coming from the upper layers
is deﬁned by a restricted Boltzmann machine (RBM) which
shares the same connection weights with the current layer.
This insight also enables a layer-wise greedy construction of
a DBN from bottom-up using RBMs as the building blocks.
To build an MD-DBN for speech synthesis, we use
three types of RBMs: (1) mixed Gaussian-Bernoulli RBMs,
for spectrum, log-F0 and voiced-unvoiced representation
with assumed Gaussian or Bernoulli distributions;
Categorical-Bernoulli
correspondence between syllable identities and the binary
data derived from the speech representation; and (3) Bernoulli
RBMs, which are used to encode binary data.
2.1. Bernoulli RBM (B-RBM)
A B-RBM is an undirected graphical model with one layer of
stochastic visible binary units v and one layer of stochastic
hidden binary units h. There is no interaction between units
in the same layer and is thus “restricted”.
It deﬁnes the
“energy” of a visible-hidden conﬁguration (v, h) as follows:
E(v, h; Θ) = −hTW v −bTv −aTh,
where Θ = {W , a, b} is the set of parameters of a RBM
and Θ will be omitted for clarity hereafter. wij is the weight
of the symmetric connection between the hidden unit i and
the visible unit j, while ai and bj are their bias terms.
The distribution of the (v, h) conﬁguration is: Pr(v, h) =
exp(−E(v, h))/Z (Z is the normalization term), i.e., the
higher the energy, the lower the probability.
The nice property of this setting is that the two conditionals Pr(hi = 1|v) and Pr(vj = 1|h) can be obtained easily
using the fact that hi and vj can only be either 0 or 1:
Pr(hi = 1|v) = σ(
wijvj + ai),
Pr(vj = 1|h) = σ(
wijhi + bj),
where σ(x) = (1 + e−x)−1.
To optimize the log-likelihood of v in a ﬁrst-order approach, we need the gradient of log Pr(v) with respect to any
Given the instantiated observation v, the expectation of
derivatives in the ﬁrst term in Eqn. (4) can be easily computed. Unfortunately, the second term in Eqn. (4) involves a
summation over all possible v and is intractable. A widely
applied method that approximates this summation is the
Gibbs sampler which (optionally starts from v) proceeds in
a Markov chain as follows:
h(0) ∼Pr(h|v(0));
v(1) ∼Pr(v|h(0)),
h(1) ∼Pr(h|v(1));
Contrastive divergence (CD) training makes two
further approximations: (1) that the chain starts from the
clamped v and is run for only k steps (CD-k); (2) the
summation is replaced by a single sample. In particular, CD-1
measures the discrepancy between v and its reconstruction to
present a direction for optimization. Starting from a training
frame v(0), we only sample h(0) in Eqn. (5a) and use the
expectations for Eqn. (2) and Eqn. (3) to replace the random
samples v(1) and h(1) in Eqn. (5b) for stability.
2.2. Mixed Gaussian-Bernoulli RBM (GB-RBM)
Speech parameters for synthesis include the spectrum and the
log-F0 with assumed Gaussian distribution, and the voicedunvoiced switches which are essentially binary. To model
these parameters simultaneously, we design the following
energy function for GB-RBM:
E(vg, vb, h) = −hTW gvg + 1
2(vg −µ)T(vg −µ)
−hTW bvb −bTvb −aTh,
where vg and vb are the Gaussian units and the Bernoulli
units in the visible layer, W g and W b are the respective
weight matrices, and µ is the mean of vg. The conditional
Pr(h|vg, vb) can be similarly derived as:
Pr(hi = 1|vg, vb) = σ(
j + ai), (7)
1|h) follows Eqn. (3).
The conditional
j |h) involves an integral over the continuous vg. We
can show that:
j |h) = N(vg
ijhi + µ, 1).
Here we have assumed that the data is normalized to have unit
variance. Given these deﬁned conditional probabilities, CD-1
training is the same as described in Section 2.1.
2.3. Mixed Categorical-Bernoulli RBM (CB-RBM)
As mentioned previously that the posterior of hidden units can
be inferred directly from an up-pass, to associate the inferred
posteriors vb with their corresponding indexed syllable label
lc, we need to deﬁne the energy of the CB-RBM as follows:
E(lc, vb, h) = −hTW llc −bcTlc
−hTW bvb −bbTvb −aTh.
To make lc ∈{0, 1}K follow a categorical distribution, i.e.
representing the syllable identity using the 1-out-of-K code
(K is the number of tonal syllables in the TTS system), we
restrict lc to have only K 1-out-of-K codes. It can be shown
that Pr(lc
j = 1|h) is deﬁned by the soft-max:
j = 1|h) =
The conditionals Pr(vb
j = 1|h) and Pr(hi = 1|lc, vb) take
the same form as Eqn. (3) & (7) respectively in the CD-k
training procedure.
3. DBN-BASED SPEECH SYNTHESIS
3.1. Training Stage
Given a syllable’s start and end times, we extract 50
uniformly-spaced
Mel-Generalized
Cepstrum coefﬁcients (MGCs) plus log-energy, 200
uniformly-spaced
voiced/unvoiced
decisions and the corresponding log-F0 values within the
syllable’s boundary.
The voiced and unvoiced frames are
assigned 1s and 0s in their V/UV units, respectively. The
log-F0 values for the unvoiced frames are set to be all 0 – a
dummy value for log-F0. Both MGC and log-F0 have been
normalized to have zero mean and unit variance.
The MGCs, log-F0 and V/UV units are concatenated
to form a 1650-dimensional super-vector for each syllable,
which is used as the visible layer v for GB-RBM. The
posteriors of the hidden layer Pr(h(1)
i |vg, vb) for all i yielded
from a simple up-pass can be used as the visible data for
training the immediate upper-layer B-RBM. Likewise, we
stack up as many layers of B-RBMs as we want in a similar
fashion. The joint distribution of the top hidden layer’s posteriors (recursively propagated from below) and the associated
syllable label is modeled by a CB-RBM.
3.2. Synthesis Stage
For an arbitrary text prompt, we look up the characters in
a dictionary to ﬁnd out their tonal syllable pronunciations.
Starting from a clamped 1-out-of-K coded syllable label lc
and an initial all-zero h(N−1) in the top-layer CB-RBM, we
calculate the conditionals Pr(h(N)
= 1|lc, h(N−1)) for all
i in layer h(N), Pr(h(N−1)
= 1|h(N)) for all j in layer
h(N−1), in alternative fashion. This procedure continues until
convergence or a maximum number of iterations is reached.
, lc) is then recursively passed down in the DBN to
give out Pr(vg
j |h(1)) and Pr(vb
j|h(1)) for the Gaussian units
and the Bernoulli units respectively in the GB-RBM at the
bottom. The diagram of parameter generation from a DBN is
shown in Fig. (1).
For each V/UV frame, we make a voicing decision
depending on whether Pr(vb
j|h(1)) > 0.5.
The log-F0
(if the corresponding V/UV decision is voiced) and MGC
parameters are recovered via scaling by the standard deviation
and offsetting by the mean. The duration of each syllable is
the average estimated from the training data.
We apply a 25-point median ﬁlter on log-F0 trajectory
to reduce noise.
Then both MGCs and log-F0 parameter
sequences are interpolated using cubic spline on the utterance
level and decimated to yield parameter sequences with a
constant 5-ms frame shift. The resulting speech parameter sequences are then fed to the Mel Log Spectral Approximation
(MLSA) ﬁlter for the ﬁnal output signal.
Tonal Syllable ID
MGCs & log-F0
Fig. 1. Architecture of the MD-DBN for speech synthesis.
4. EXPERIMENTS
4.1. Experiment Setup
A manually transcribed Mandarin corpus recorded from a
female speaker is used for the experiments. The training set
contains 1,000 utterances with a total length of 80.9 minutes,
including 23,727 syllable samples.
All these samples are
partitioned into 1,364 classes of tonal syllables.
test set with 100 utterances is used for model architecture
determination and the objective evaluation.
The RBMs are trained using stochastic gradient descent
with a mini-batch size of 200 training samples. For GB-RBM,
400 epochs are executed with a learning rate of 0.01 while
for B-RBMs and CB-RBM 200 epochs are executed with a
learning rate of 0.1. During the weight updates, we apply a
0.9 momentum and a 0.001 weight decay.
The training procedure is accelerated by an NVIDIA Tesla
M2090 GPU system. For an MD-DBN with 4 hidden layers
and 2000 units per layer, the training takes about 1.1 hours.
Each epoch of training GB-RBM, B-RBM and CB-RBM
takes 3.5s, 3.7s and 5.7s respectively. A single GPU system
runs at about 8 times faster than an 8-core 2.4 GHz Intel Xeon
E5-2609 CPU system.
4.2. HMM-based Synthesis Baseline
We build a Mandarin HMM-based speech synthesis system
on the same training set using a standard recipe . MGCs
and log-F0 together with their ∆and ∆2 are modeled by
multi-stream HMMs. Each syllable HMM has a left-to-right
topology with 10 states. Initially, 416 mono-syllable HMMs
are estimated as the seed for 1,364 tonal syllable HMMs. In
the synthesis stage, speech parameters including MGCs and
log-F0 are obtained by the maximum likelihood parameter
generation algorithm , and are later used to produce the
speech waveform through the MLSA ﬁlter.
The syllable
durations are the same as those in the DBN approach. For a
fair comparison, no contextual information or post-processing
voice enhancement techniques are incorporated.
4.3. Optimizing MD-DBN Architecture
Finding an adequate architecture for MD-DBN is important
in DBN-based speech synthesis. However, tweaking based on
subjective evaluation is not practical due to the vast number
of combinations of MD-DBN depth and layer width.
One commonly-used objective method in voice conversion as well as speech synthesis is to employ spectral distortion between generated speech and target speech.
Here we use Mel-Generalized Cepstral Distortion (MGCD)
to determine the MD-DBN architecture.
MGCD is the
Euclidean distance between the MGCs of synthesized speech
and that of original speech recording. All the test set prompts
are synthesized to compute the average MGCDs for HMM
baseline and MD-DBN with different number of hidden
layers and different number of units in each hidden layer. As
the syllable duration of the speech from the test set and that
of HMM and MD-DBN can be different, we align the MGCs
according to the syllable boundary.
MGC Distortion
Width of the hidden layers
3 hidden layers
4 hidden layers
5 hidden layers
6 hidden layers
Fig. 2. MGCD as a function of the width of the hidden layers.
3H-7H: No. of hidden layers.
The MGCD of the HMM baseline is 0.223, while the
DBN approach archives better result with a minimal MGCD
of 0.194. As shown in Fig. (2), the MGC distortions are high
when the layer width is 1,000, and too many units (4,000) in
the hidden layer causes large variance of MGCD. The MD-
DBN with 4 hidden layers and hidden layer width of 2,000
units consistently gives the best MGCD. Hence we will be
using these parameters for the remaining of the evaluation.
4.4. Subjective Evaluation
A Mean Opinion Scoring (MOS) test is conducted to compare
the subjective perception between the DBN approach and
the HMM baseline, together with a hybrid approach (MIX)
using MGCs from DBN and log-F0 from HMM. In the MOS
test, each of the 10 experienced listeners is asked to rate 10
utterances synthesized by the DBN and the HMM using a 5point scale (5:excellent, 4:good, 3:fair, 2:poor, 1:bad). The
MOS result is shown in Table (1).
MIX: DBN MGCs + HMM Log-F0
Table 1. MOS test result.
Although the overall MOS score shows a draw between
the DBN and the HMM, the two sets of synthesized speech
sounds different1.
Without post-processing, HMM’s voice
sounds quite mufﬂed, but the prosody remains smooth and
stable. The DBN voice sounds much clearer than the HMM
baseline, and the prosody is lively. However, it seems that
tonal and U/VU decision errors have a relatively higher
chance to occur in the DBN approach, which can be the main
reason that lowers its MOS score. The MIX approach gets
highest score, which probably suggests that the spectrum can
be captured by DBN appropriately but HMM does a better job
in F0 modeling.
The cross-comparison reveals more details.
same log-F0 curve (HMM vs. MIX), MGCs generated by
DBN leads to higher MOS scores. This result agrees with the
objective MGCD measure (MGCDHMM>MGCDDBN). With
the same MGCs (DBN vs. MIX), log-F0 curve generated by
HMM results in higher MOS scores.
It can be seen that
listeners prefer smoother F0 patterns when there is no higherlevel prosody control of the synthesized speech.
5. CONCLUSIONS AND FUTURE WORK
We have described a DBN model with a multi-distribution
visible layer for statistical parametric speech synthesis, in
which the spectrum and F0 are modeled simultaneously in
a uniﬁed framework. It is shown that DBN models spectrum
better than HMM and achieves an overall performance that is
comparable to context-independent HMM synthesis. Future
directions for improvement include: (1) introducing context
information to improve the prosody; and (2) better modeling
of the F0 contour with higher resolution of frame sampling.
6. ACKNOWLEDGMENT
The authors would like to thank Dr. Li Deng from MSR and
Prof. Fei Sha from USC for fruitful exchanges.
1The synthesized speech samples can be downloaded from http://
www.se.cuhk.edu.hk/˜sykang/icassp2013/
7. REFERENCES
 T. Yoshimura, K. Tokuda, T. Masuko, T. Kobayashi,
and T. Kitamura, “Simultaneous modeling of spectrum,
pitch and duration in HMM-based speech synthesis,” in
Eurospeech, 1999, pp. 2347–2350.
 K. Tokuda, T. Mausko, N. Miyazaki, and T. Kobayashi,
“Multi-space probability distribution HMM,”
Trans. Inf. & Syst., vol. E85-D, pp. 455–464, 2002.
 K. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi, and
T. Kitamura, “Speech parameter generation algorithms
for HMM-based speech synthesis,” in ICASSP, 2000,
pp. 1315–1318.
 G. E. Hinton, S. Osindero, and Y. W. Teh,
fast learning algorithm for deep belief nets,”
Computation, vol. 18, no. 7, pp. 1527–1554, 2006.
 Joshua M Susskind, Geoffrey E Hinton, Javier R
Movellan, and Adam K Anderson, “Generating facial
expressions with deep belief nets,” Affective Computing,
Emotion Modelling, Synthesis and Recognition, pp.
421–440, 2008.
 G. W. Taylor, G. E. Hinton, and S. T. Roweis,
“Two distributed-state models for generating highdimensional time series,” Journal of Machine Learning
Research, vol. 12, pp. 1025–1068, 2011.
 George E Dahl, Dong Yu, Li Deng, and Alex Acero,
“Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition,”
Speech, and Language Processing, IEEE Transactions
on, vol. 20, no. 1, pp. 30–42, 2012.
 A.W. Black, H. Zen, and K. Tokuda,
“Statistical
parametric speech synthesis,”
in ICASSP, 2007, pp.
1229–1232.
 A. Mohamed, G. E. Dahl, and G. E. Hinton, “Acoustic
modeling using deep belief networks,” IEEE Transactions on Audio, Speech & Language Processing, vol. 20,
no. 1, pp. 14–22, 2012.
 K. Tokuda, T. Kobayashi, T. Masuko, and S. Imai, “Melgeneralized cepstral analysis - a uniﬁed approach to
speech spectral estimation,” in ICSLP, 1994.
 T. Fukada, K. Tokuda, T. Kobayashi, and S. Imai, “An
adaptive algorithm for mel-cepstral analysis of speech,”
in ICASSP, 1992, pp. 137–140.
 Z. Shuang, S. Kang, Q. Shi, Y. Qin, and L. Cai,
“Syllable HMM based mandarin TTS and comparison
with concatenative TTS,” in INTERSPEECH, 2009, pp.
1767–1770.
 S. Desai, E. V. Raghavendra, B. Yegnanarayana, A. W.
Black, and K. Prahallad,
“Voice conversion using
artiﬁcial neural networks,” in ICASSP, 2009, pp. 3893–