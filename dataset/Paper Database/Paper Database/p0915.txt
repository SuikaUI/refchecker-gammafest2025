Vis Comput
DOI 10.1007/s00371-013-0867-4
ORIGINAL ARTICLE
SalientShape: group saliency in image collections
Ming-Ming Cheng · Niloy J. Mitra · Xiaolei Huang ·
Shi-Min Hu
© Springer-Verlag Berlin Heidelberg 2013
Abstract Efﬁciently identifying salient objects in large image collections is essential for many applications including image retrieval, surveillance, image annotation, and object recognition. We propose a simple, fast, and effective
algorithm for locating and segmenting salient objects by
analysing image collections. As a key novelty, we introduce
group saliency to achieve superior unsupervised salient object segmentation by extracting salient objects (in collections of pre-ﬁltered images) that maximize between-image
similarities and within-image distinctness. To evaluate our
method, we construct a large benchmark dataset consisting of 15 K images across multiple categories with 6000+
pixel-accurate ground truth annotations for salient object regions where applicable. In all our tests, group saliency consistently outperforms state-of-the-art single-image saliency
algorithms, resulting in both higher precision and better recall. Our algorithm successfully handles image collections,
of an order larger than any existing benchmark datasets, consisting of diverse and heterogeneous images from various
internet sources.
Keywords Saliency detection · Group saliency · Object of
interest segmentation · Image retrieval
M.-M. Cheng (B) · S.-M. Hu
TNList, Tsinghua University, Beijing, China
e-mail: 
N.J. Mitra
University College London, London, UK
Lehigh University, Bethlehem, USA
1 Introduction
The ubiquity of acquisition devices, e.g., cameras and smartphones, and the growing popularity of social media have resulted in an explosion of digital images accessible in the
form of personal and internet photo-collections. Typically,
such image collections are huge in size, have heterogeneous
content, and are noisy due to diverse background and illumination conditions. Although such images form a wellestablished communication medium for sharing experiences
or blogging events, we still lack efﬁcient and effective methods to analyze and organize such images.
Determining characteristic or salient regions of images
allows transitioning from low-level pixels to more meaningful high-level regions, and thus form an essential step for
many computer graphics and computer vision applications,
including interactive image editing , image retrieval , and internet visual media processing . Recently, signiﬁcant success has
been reported in saliency-based image segmentation producing near ground-truth performance on simple images ( and references therein). The next challenge is to reliably segment salient object regions in large heterogeneous
image collections such as internet images, e.g., Flickr, Picasa. Since such collections contain rich information about
our surroundings, their effective analysis will naturally provide improved understanding of image contents.
We introduce SalientShape, a group-saliency based framework for salient object detection and segmentation in image collections. We demonstrate that even when the shared
content across image collections is small, e.g., 30 %, our
framework produces superior results as compared to individually processing the images. Our proposed method is simple, scales well with increasing size of collections, has low
memory-footprint, and can be effectively integrated with existing image handling systems.
M.-M. Cheng et al.
Fig. 1 System pipeline. Our system explicitly extracts salient object
regions from a group of related images with heterogeneous quality of-
ﬂine (a–d, f) to enable efﬁcient online (e) shape based query. To enable
effective salient object segmentation for a large collection of images
with heterogenous contents, our system only requires a simple input
text keyword and a coarse sketch (a), for initial query of internet image
candidates (b) and shape ranking (e). For a new query shape related
to a processed keyword, the segmentation results (d) can be re-used
for efﬁcient query (only shape matching is required in this typical use
Our algorithm (see Fig. 1) runs in the following key
stages: First, for a query object class, we retrieve candidate images by pre-ﬁltering using keywords. Such retrieved
images are usually noisy and contain outliers due to limitations in keyword-based image search, ambiguity of keywords, and heterogeneous tags. Next, we detect and segment salient object regions of each candidate image using
SaliencyCut and automatically remove candidates with
fragmented scenes or with unreliable segmentation quality by jointly analyzing the salient regions. Then, in a key
stage, we re-rank the remaining candidates based on the consistency between their saliency cuts and the user provided
sketches. In this step, access to image collections proves
critical since even in noisy ensembles we observe that segments corresponding to the inlier objects have consistent
appearance and shape properties. To exploit this, we build
global (group) foreground and background appearance models from the top-ranked candidate images for the query object class. Finally, we use the extracted appearance model
for group saliency region detection and segmentation. We
iterate the process to alternately improve saliency estimates
and appearance models.
We compare the resultant segmentation results with stateof-the-art single image salient region segmentation methods (see Fig. 7(b)), and to retrieval performance
with SHoG on 30 categories (see Table 1). Further,
we introduce a benchmark dataset consisting of 15,000 images collected from Flickr along with 6000+ pixel-accurate
ground truth salient object masks where applicable (to be
made publicly available for academic use). To the best of
our knowledge, our benchmark dataset with pixel accurate
salient object region ground truth labeling is the largest of
its kind (15× larger than ), while the images are more
difﬁcult and closer to real-world scenarios. In our extensive
tests, group saliency consistently outperforms existing stateof-the-art alternatives, especially on images with cluttered
backgrounds.
The improved performance is primarily due to the joint
saliency estimation and (single-image and group/global) appearance models learning. Our system also beneﬁts from
meta-data,1 visual saliency, and shape similarity to explicitly detect salient object regions and enable shape retrieval
without inﬂuence from background clutter. In summary, we
(i) introduce group saliency to extract object of interest regions from a group of correlated but heterogeneous images,
and (ii) present a large benchmark dataset to objectively
compare the superiority of group saliency over traditional
single image saliency detection. Since our focus is on consistent segmentation, we show retrieval only as a potential
application rather than being the focus of this work.
2 Related work
Salient region extraction
Various methods have been proposed for extracting salient regions from single images: Ko
and Nam select salient regions using a support vector
machine trained on image segment features, and then cluster these regions to extract salient objects. Han et al. 
model color, texture, and edge features using a Markov random ﬁeld framework and grow salient object regions from
seed values in the saliency maps. Achanta et al. average saliency values within image segments produced by
mean-shift segmentation, and ﬁnd salient objects by identifying image segments with average saliency above a threshold. Cheng et al. propose a saliency estimation
method to automatically initialize an iterative version of
GrabCut to segment salient object regions. These methods aim at salient region extraction for individual images,
while ignoring useful global information available from
correlated image collections. Recently, co-saliency methods have been proposed to ﬁnd common salient object(s)
between pair of images or among multiple images . Such methods, however, require salient areas to
contain parts of the foreground objects across most images.
Further, the algorithms are difﬁcult to scale to large number
1Meta-data is the current industry standard for image retrieval as popularized by search engines like Google image, Flickr, etc.
SalientShape: group saliency in image collections
of images (largest demonstrated collection has 30 images).
In contrast, we focus on detecting and segmenting correlated
salient object regions from large (thousands or more) image
collections with heterogeneous contents (e.g., internet images).
Internet image re-ranking
Fergus et al. use the top results returned from a web-based image search engine to train
a classiﬁer, and then use the classiﬁer to ﬁlter the search results. Ben-Haim et al. automatically segment images into
regions and build color histogram features for each region,
which are then clustered to obtain principal modes. The remaining images are then re-sorted based on the distance of
their regions to the mean feature values of the principal clusters. Cui et al. categorize a query image into one of
several predeﬁned categories, and employ a speciﬁc similarity measure in each category to combine image features
for re-ranking based on the query image. Popescu et al. 
re-rank images based on the visual coherence of queries using a diversiﬁcation function to avoid near-duplicate images
and ensure that different aspects of a query are presented to
the user. None of these algorithms use visual attention and
shape information of the desired object. In contrast, we use
such information to capture potential appearances that a desired object class may have, enabling superior salient region
extraction.
Sketch based image retrieval (SBIR)
Early works by Hirata and Kato perform retrieval by comparing shape
similarity between user sketches and edge images in a
database, expecting precise sketches from the users. Alberto
and Pala further employ elastic matching to a usersketched template for robust retrieval, with the cost of expensive computation. Recently, Cao et al. developed a novel
indexing technique to support efﬁcient contour-based
matching for a retrieval system that handles millions of
images. However, the method does not provide translation,
scale, or rotation invariance, and more importantly expects
the desired object to appear at roughly similar positions,
scale, and rotation as in the user-drawn query sketch.
In an important recent system, Eitz et al. use local descriptors to achieve state-of-the-art retrieval performance. Their success is mainly attributed to translation invariance of local descriptors as well as using large local features (20 %–25 % image’s diagonal length) to retain largescale image characteristics. All such methods compare user
sketches with image edges (or boundaries), suffering from
inﬂuence of background edges when ﬁnding a desired object. Salient object region extraction and multiresolution region representation have been used to handle background clutter. We also use explicit region information to support SBIR. However, instead of feature designing,
matching, or indexing, we use visual attention and (learned)
global appearance information to improve salient region extraction, which naturally supports shape retrieval with scale,
rotation, and translation variations.
Segmentation transfer
Our work is also related to recent
advances in segmentation transfer. Kuettel and Ferrari 
transfers segmentation mask from training windows that are
visually similar to the target image windows. In an impressive concurrent effect, Kuettel et al. successfully generate pixelwise segmentations for ImageNet , which contains 577 classes over 500 K images, by leveraging existing manual annotations in form of class label, boundingboxes, and external pixel-wise ground truth segmentations
in the PASCAL VOC10 dataset . These methods also
use class-wise appearance models, captured by the Gaussian Mixture Model, and model the segmentation problem
in an extended MRF framework. However, in absence of
appropriate methods to choose good segmentations before
segmentation propagation, the accuracy degrades gracefully
over the stages. Instead, we carefully choose good segmentations by measuring scene complexity, imprecise cut, region incompleteness, and shape consistence. This allows us
to select reliable candidates leading to high quality global
appearance, which accords with human understanding about
the classes (see also Fig. 4 and supplemental materials).2
Thus, instead of external pixel-accurate ground truth labeling, our method only requires a few (typically one is enough)
sketches for each class to help learn useful global appearance information, thus signiﬁcantly lowering required annotation efforts.
3 Unsupervised segmentation of individual candidate
For any given keyword (e.g., dogs, jumping dogs, etc.), we
ﬁrst retrieve a set of candidate images using Flickr, typically
around 3,000 (see Fig. 1). For each such image, we perform
unsupervised segmentation to estimate a salient object, as
described next. The key stage comes later (Sect. 4) when we
exploit correlation in salient objects’ appearance and shape
among related images for a query object class, toward group
3.1 Saliency guided image segmentation
We brieﬂy describe our previous SaliencyCut work,
which is used here for single image saliency estimation. Segmenting a color image I := {Ii} consisting of pixels Ii in
the RGB color space amounts to solving for corresponding
opacity values α := {αi} with αi ∈{0,1} at each pixel. We
2 
M.-M. Cheng et al.
enable unsupervised segmentation by building a Gaussian
Mixture Model (GMM) for foreground/background color
distribution G, which we then use to directly extract a binary segmentation mask to avoid manual thresholding.
We use GrabCut formulation to model single image saliency-based segmentation problem and use their suggested parameters. The segmentation problem can be solved
by optimizing a Gibbs energy function E as
α E(α,G,I) = min
U(α,G,I) + V (α,I)
where U(α,G,I) evaluates the ﬁtness of the opacity distribution α with respect to the data I under a given color model
G and V (α,I) measures the smoothness of α. The ﬁtness
term U(α,G,I) of a pixel Ii is deﬁned as the negative log
probability of this pixel belonging to its corresponding color
distribution G (α is a binary value. α = 1 indicates a foreground GMM, while α = 0 denotes a background GMM).
The smoothness term V (α,I) is deﬁned as the sum of neighboring pixel color similarity when they take different α values (see for details about the measurement and its parameters estimation). In SaliencyCut , the continuous
saliency values are thresholded to automatically initialize
foreground and background color models in GrabCut .
To improve robustness to noisy automatic initialization, the
segmentation process is iterated, with morphological operations to improve performance (see for more details).
Implementation details
Unlike , we use a new initialization procedure that avoids the un-intuitive threshold
choosing process. Foreground and background are modeled
with color GMMs. Instead of assigning pixel colors to a
model using a threshold, we treat every pixel color as a
weighted sample that contributes to both foreground and
background color GMMs, e.g., for a pixel with saliency
value 0.7, we use weight factors 0.7 and 0.3 when building the foreground and background color GMMs, respectively. Although this soft assignment incurs a small computational overhead (<10 %) in each SaliencyCut iteration, it
introduces more accurate initialization, which reduces the
number of iterations required. In the experiments, the overall segmentation quality and computational time is similar to
 with a manually chosen threshold. During GMM estimation, we use the color quantization bin (see ) as a unit
of samples instead of each pixel color for computational ef-
3.2 Measuring the reliability of SaliencyCut
Keyword based image retrieval often produces a large percentage of irrelevant images (see also Sect. 5.3 and supplementary materials). Luckily, in our retrieval application,
users are interested in the precision rate of the top ranked
images (e.g., top 50 images) rather than the recall rate of the
Fig. 2 Complex and cluttered scenes usually lead to segmentations
composed of many small regions, or fragmented saliency maps
Fig. 3 Different SaliencyCuts are marked in red in (a), (c), and (d).
While (a) is imprecise and (c) produces an incomplete region-of-interest, (d) yields a good cut. Undesirable cuts are detected based on relevant foreground probability maps (b, e). Segmentation quality ranking
scores (imprecise RP and incomplete RB) are overlaid on top
entire searched results (typically a few 1000 s). Hence, we
aggressively prune away likely outlier images, as described
Scene complexity
Saliency maps are often poor for complex/cluttered scenes. We use the number of regions produced by segmentation as an indicator of scene complexity (see Fig. 2). Intuitively, images with a small number of segments are simpler. We sort the images based on
increasing number of regions and retain only the top TR images for subsequent stages. We use TR = 70 % in our tests
leading to around a thousand images being discarded.
Note that in our problem setting huge sets of candidates
are available, e.g., internet images, and the users simply
want to easily ﬁnd some high quality desired targets rather
than explore the whole collection. Hence, as a design choice,
we decided to favor higher precision over higher recall. We
empirically threshold values and use them for all our tests.
This resulted in reliable global statistics for group segmentation.
Segmentation quality
Even for relatively simple scenes,
SaliencyCut can have imprecise or incomplete boundaries,
which we detect as follows: (i) For each image, we use its
SaliencyCut region and its remaining parts to train GMMs
for foreground and background regions, respectively. We
then estimate the foreground probability of relevant image
pixels according to these two GMMs. Speciﬁcally, we take
the sum of foreground probabilities for pixels inside a narrow band (of 30 pixel width) surrounding the SaliencyCut
as a measure for imprecise cut (e.g., Fig. 3a): the higher this
sum, the lower the predicted quality of the cut. (ii) We take
the total number of SaliencyCut region pixels within a narrow band (of 20 pixel width) along the image border as a
SalientShape: group saliency in image collections
measure for incompleteness of the object-of-interest region
(see Fig. 3c): the higher this number, the more likely the
cut object region is incomplete. We sort the images according to increasing order of the above two measures and retain
the top TP and TB of images for subsequent stages. We use
TP = 80 % and TB = 80 % in our experiments. The retained
images are next analyzed for image collection consistency.
4 Group saliency
Retrieved images in a collection (e.g., Flickr) are largely
correlated, but can have differences due to pose, appearance, etc. (see Fig. 8 and also supplementary). We use rough
sketches as an indicator of the poses that the user is interested in (e.g., when user searches for a speciﬁc style of ‘dog
jump’), while we use consistency across the retrieved images to extract what are plausible appearance models for the
salient object (e.g., color of the ‘dog’). Such saliency, which
we refer to as group saliency, favors both similarities between images and distinctness within each image. Speciﬁcally, we ﬁrst use an efﬁcient cascade model to rank single
image unsupervised segmentation results according to their
shape consistency with user input sketch. Top ranked results
are then used to train a global appearance statistical model
for reﬁning group saliency and segmentation results.
4.1 Cascade ﬁltering for sketch based retrieval
Having reliable SaliencyCut boundaries from initial images,
we beneﬁt from existing shape matching algorithms 
to retrieve desired images based on consistency with userinput sketches. We use the following simple measures,
which are easy to calculate on regions with clean background:
– circularity: Perimeter2/Area ,
– solidity: RegionArea/ConvexHullArea ,
– Fourier Descriptor , and
– Shape Context .
We proceed in a cascaded fashion. For each measure, we
sort the shapes in decreasing order based on their similarity to user sketch, and retain a top percentage of the candidates. The measures are arranged in increasing complexity, allowing efﬁcient and early rejection of candidates with
large dissimilarity. In our experiments, we keep TC = 80 %,
TS = 80 %, and TF = 70 % images according to circularity, solidity, and Fourier descriptor, respectively; the corresponding dimensions for these descriptors are 1, 1, and 15.
We compare these descriptors using simple Euclidean distance with corresponding features of the user sketch. We ﬁnally use the Shape Context (with default parameters and
matching methods as suggested in the original paper ),
Fig. 4 Appearance histogram of sample colors in object of interest
region in ‘dog jump’ images according to the learned global prior.
Left shows typical foreground colors, while right shows typical background colors. Probability values {p} are ordered and histogram height
is |p −0.5|. We ignore color samples with probability around 0.5 as
chance. Inset shows a typical input image example
which is complex but effective to properly order the remaining candidates. While one can employ more complex shape
descriptors (e.g., ), we ﬁnd the above selection sufﬁciently diverse to prune out most shape outliers (see also
supplementary material). Note that at this stage we are left
with only TR · TP · TB · TC · TS · TF ≈20.0 % of the image shapes, which are used for appearance consistency, as
described next.
4.2 Statistical global appearance models
After cascade ﬁltering, the top ranked images typically have
high precision. We use the top 50 images as a high quality training set to learn a global appearance prior to guide
subsequent group saliency detection and segmentation. We
choose GMM models to capture such a prior ¯G for two reasons: (i) GMM models generalize better on small amounts
of training data than histogram models ; (ii) GMM priors can be easily integrated in our unsupervised image segmentation framework (see Sect. 3). For example, Fig. 4
shows foreground and background GMM models for the
‘dog jump’ example indicating that dogs are typically yellow or dark in color and are like to play on green grass/ﬁelds;
for the ‘giraffe’ we ﬁnd typical background colors consist of
blue/green indicating sky/trees, as typically associated with
context information for giraffes. Although other attributes
like texture and visual vocabulary can be considered, we
currently use only color. We empirically chose 8 Gaussians
each to model major appearance of foreground/background
per category and found that this number is not sensitive.
Note that since shape features are typically orthogonal
to appearance attributes, the samples we retrieved based on
shape largely preserve their appearance diversity and can be
used to learn representative appearance models. Typically
only a fraction of such images (15 %–57 % in our tests)
contain desired objects. These objects may have different
M.-M. Cheng et al.
colors, textures, and even a single object may comprise of
several regions with very different appearance (e.g., butter-
ﬂy). We found that considering the largest appearance cluster or top-ranked internet images as an initial set to
be unsatisfactory. In an interesting effort, Chang et al. 
use repeatedness among images as a global prior of multiple
images and assume that most images contain at least parts of
the foreground object, an assumption that is often violated in
our setting. Further, since each image is compared with all
others, the method cannot be used for large collections (e.g.,
they considered image sets of maximum size 30, while we
handle a few 1000 s).
4.3 Estimating group saliency
Finally, we use the learned global appearance statistics to
improve the saliency detection and SaliencyCut of each image. Since the estimated global color prior ¯G is encoded
as GMMs, we simply add a global prior constraint to our
single image unsupervised segmentation energy function of
Eq. (1). The new energy function takes the form:
E(α,G, ¯G,I) = λU(α, ¯G,I) + (1 −λ)U(α,G,I)
where, the additional term U(α, ¯G,I) evaluates the ﬁtness
of the opacity distribution α to the global color prior ¯G,
while weight λ (0.3 in our test) balances between global
color prior and per image color distribution. Here, the global
color prior ¯G reﬂects similarity between the targets, while
the per-image color distribution G is trained according to
the individual image saliency map and captures distinctness
within an image.
Similar to Eq. (1), we optimize Eq. (2) to get group
saliency segmentation results. We then encode group
saliency maps as probability maps of pixels belonging to
the object-of-interest region obtained by group saliency
segmentation. Note that although the change compared to
Eq. (1) is small, the improvement in estimated saliency is
signiﬁcant with only marginal computational overhead.
Figure 5 demonstrates typical improvements in saliency
cut and segmentation using global color priors. In the ‘dog
jump’ image, the green parts of the image are estimated to
be more likely to be background rather than foreground according to the learned global color prior. Similarly, in the
plane example, missing object regions are correctly recovered with the help of global statistics (see supplementary
material for more examples).
5 Experiments
We implemented our framework in C++ and evaluated it using a Quad Core i7 920 CPUs with 6 G RAM. We use the
Fig. 5 Examples of using statistics to reﬁne unsupervised segmentation: (a) source image, (b) single image saliency map, (c) Saliency-
Cut (using Eq. (1)), (d) global color prior, (e) group saliency map, and
(f) group saliency segmentation (using Eq. (2)). Note the improvement
from (c) to (f) (see supplementary material)
Fig. 6 Example images from the benchmark dataset that correspond
to the keyword ‘dog jump’, with pixel-accurate ground-truth labeling
for the corresponding object of interest regions (if such a region exists;
4 out of the 7 here)
group saliency based retrieval results to re-train new appearance statistics and iteratively improve saliency segmentation
(see Fig. 1). Experimentally, we found two rounds of iterations to be sufﬁcient.
We evaluated the proposed method for three different applications using a benchmark dataset: (i) ﬁxed thresholding
of group saliency maps, (ii) object of interest segmentation,
and (iii) sketch based image retrieval. For the ﬁrst two applications, we consider the average segmentation performance
only over those images that do contain the target object (according to annotated ground truth).
5.1 Benchmark dataset for saliency segmentation
We collected a labeled dataset of categorized images initially extracted by querying with keywords from Flickr. We
downloaded about 3,000 images for each of the 5 keywords:
‘butterﬂy’, ‘coffee mug’, ‘dog jump’, ‘giraffe’, and ‘plane’,
and manually annotated saliency maps to mark the object
regions (see Fig. 6 and supplementary material for examples; full dataset to be publicly made available). To normalize these images, we uniformly scale them so that their
maximal dimension is 400 pixels long. Some images in the
dataset do not contain any salient object matching the keyword; we leave such images unlabeled. Further, since partially occluded objects are less reliable for shape matching,
we only label object regions that are mostly un-occluded. In
the end, we got 6000+ images with pixel accurate ground
SalientShape: group saliency in image collections
Fig. 7 Evaluation results on our benchmark dataset. (a) Precisionrecall curves for naive thresholding of saliency maps. S, G1, G2 represent single image saliency, group saliency after the 1st and 2nd iterations, respectively. Subscripts B, C, D, G, P represent groups of
‘butterﬂy’, ‘coffee mug’, ‘dog jump’, ‘giraffe’, ‘plane’, respectively.
(b) Comparison of Fβ for image groups using single image saliency
segmentation methods (FT , SEG , RC ) vs. group saliency
(GS) segmentation. The RC and GS in (b) corresponds to results of
Eq. (1) and Eq. (2) respectively
truth segmentation (see Fig. 6 for sample images and supplemental material for more statistics). Note that Eitz et
al. introduce a benchmark dataset for evaluating SBIR
systems by annotating how well a given sketch and image
pair match. Our benchmark contains pixel-accurate segmentation of the targets, when present, thus allowing evaluation
of corresponding segmentation algorithms. Our dataset is
15× larger than previously largest public available benchmark with pixel accuracy salient region annotation. In
contrast to the benchmark in , where salient regions are
unambiguous, clearly separated from the background, and
often positioned near the image centers, images in our proposed dataset are more challenging and represent typical
cluttered real-world scenes.
5.2 Fixed thresholding of group saliency maps
We threshold the saliency map with T ∈ and compare the segmentation results with ground truth labeling (see
also ). The precision and recall curves in Fig. 7(a) show
that our group saliency algorithm stabilizes after 2 iterations
and signiﬁcantly outperforms state-of-the-art single image
saliency detection method .
5.3 Object of interest segmentation
We also evaluate how accurately our algorithm extracts target objects from heterogeneous internet images. For images
containing a target, we compare their pixel-level labeling
with our group-saliency segmentation according to precision, recall, and F-Measure, which is deﬁned as
Fβ = (1 + β2)Precision × Recall
β2 × Precision + Recall .
Note that for salient region segmentation, precision is more
important than recall , since recall can trivially be
100 % by taking all image regions as targets. For internet
retrieval, precision is more important as a false detection
is undesirable over missing some good candidates among
thousands of possibilities. Hence, we use β2 = 0.3 to weight
precision more than recall for fair comparison with state-ofthe-art methods . Figure 7(b) illustrates the improvement due to group saliency segmentation.
Note that most of the nature-scene images contain multiple objects and be associated with multiple text tags in by the
search engine. We use the group saliency based segmentation to extract saliency object regions in images of the same
group. This allows us to extract objects in an image even
when the text tags differ.
5.4 Sketch based image retrieval
As an application, we compare our retrieval algorithm with
state-of-the-art SBIR proposed by Eitz et al. (using author implementations). Our method explicitly extracts object of interest regions from images, thus enabling us to exploit the power of existing shape matching techniques. For
heterogeneous internet images, the combination of groupsaliency segmentation and shape matching effectively selects good images containing target objects, leading to im-
M.-M. Cheng et al.
Table 1 True positive ratios (TPR) among top 50 and 100 retrievals
using Flickr, our method, and SHoG , for 30 different categories
Among top 50
Among top 100
Coffee mug
Strawberry
proved results (see Table 1, Fig. 8, and supplementary material). We leave exploring beneﬁts of hybrid systems using
additional attributes including appearance , local features , or additional lines to future research.
We pre-process each image in the database by performing single image unsupervised segmentation (about 100 images per minute). Further, we pre-process each category using a representative sketch to initialize a good appearance
learning and unsupervised segmentation of the object. In
Fig. 8(e), we show retrieval results for ‘plane’ with two
different input sketches, for which results for the second
sketches just have to compare salient shapes generated with
the help of the ﬁrst sketch. Our results contain explicit region information allowing input sketches to retrieve results
with more relevant pose.
Note that currently our system expect users to supply
both keywords and sketches. Existing shape matching techniques, which are able to effectively select high quality
matchings from shapes with clean background, even with
very rough sketches (e.g., state-of-the-art method ),
could achieve 93.3 % accuracy in the very challenging
MPEG7 shape dataset. Once the user inputs a rough sketch
to help distinguish between desired object and irrelevant region shapes, it would help us to get useful global appearance information. The low correlation between shape feature and appearance statistics allows us to reuse the input
sketch, learned appearance, and segmented regions. Recent
advances in human object sketch classiﬁcations can potentially be used in conjunction with our system toward a
keyword-free retrieval interface, which can be attractive for
gesture-based devices. At runtime, we only compare a new
user sketch with object shapes using the cascade ﬁltering
process (see Sect. 4) taking less than 1 second to handle an
initial retrieved set of 3,000 images. For larger databases,
efﬁcient retrieval algorithms using shape context may
be useful. Our method only segments the most salient object region from each image and perform retrieval based on
that object region. Since there are a huge number of internet
images, we are mainly focused on quality of the top ranked
results rather than the recall of every image.
6 Conclusion
We introduced a method to exploit correlations across internet images within same categories to achieve superior
salient object segmentation and image retrieval. Starting
from a simple user sketch, we estimate high quality image labeling to build appearance models for target image regions and their backgrounds. These appearance models are
in turn used to improve saliency detection and image segmentation. We introduced a benchmark consisting of 6000+
pixel-accurate labeled dataset initially obtained by querying
keywords from Flickr and use it to demonstrate that our proposed method produces high quality saliency maps and segmentation, with potential application to SBIR. Our approach
makes use of the powerful user sketch information to select
good segmentation candidates for getting global appearance
information (see Sect. 4.2). This selection process avoids error accumulative problems which typically exist in iterative
segmentation transfer methods , resulting in the consistent result improvements observed in our experiments.
In the future, we plan to learn additional texture and
shape statistics to further improve the segmentation.
We also plan to investigate efﬁcient shape indexing algorithms and GPU speed up for increased efﬁciency.
SalientShape: group saliency in image collections
Fig. 8 SBIR comparison. In each group from left to right, ﬁrst column shows images downloaded from Flickr using the corresponding keyword; second column shows our retrieval results obtained by
comparing user-input sketch with group saliency segmentation results;
third column shows corresponding sketch based retrieval results using
SHoG . Two input sketches with their retrieval results are shown
M.-M. Cheng et al.
Acknowledgements
We would like to thank the anonymous reviewers for their constructive comments. This research was supported by the 973 Program (2011CB302205), the 863 Program
(2009AA01Z327), the Key Project of S&T (2011ZX01042-001-002),
and NSFC (U0735001). Ming-Ming Cheng was funded by Google
Ph.D. fellowship, IBM Ph.D. fellowship, and New Ph.D. Researcher
Award (Ministry of Edu., CN).