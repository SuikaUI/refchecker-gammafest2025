IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 1, NO. 1, APRIL 1997
No Free Lunch Theorems for Optimization
David H. Wolpert and William G. Macready
Abstract—A framework is developed to explore the connection
between effective optimization algorithms and the problems they
are solving. A number of “no free lunch” (NFL) theorems are
presented which establish that for any algorithm, any elevated
performance over one class of problems is offset by performance over another class. These theorems result in a geometric
interpretation of what it means for an algorithm to be well
suited to an optimization problem. Applications of the NFL
theorems to information-theoretic aspects of optimization and
benchmark measures of performance are also presented. Other
issues addressed include time-varying optimization problems and
a priori “head-to-head” minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems’
enforcing of a type of uniformity over all algorithms.
Index Terms— Evolutionary algorithms, information theory,
optimization.
I. INTRODUCTION
HE past few decades have seen an increased interest
in general-purpose “black-box” optimization algorithms
that exploit limited knowledge concerning the optimization
problem on which they are run. In large part these algorithms
have drawn inspiration from optimization processes that occur
in nature. In particular, the two most popular black-box
optimization strategies, evolutionary algorithms – and
simulated annealing , mimic processes in natural selection
and statistical mechanics, respectively.
In light of this interest in general-purpose optimization
algorithms, it has become important to understand the relationship between how well an algorithm
performs and the
optimization problem
on which it is run. In this paper
we present a formal analysis that contributes toward such
an understanding by addressing questions like the following:
given the abundance of black-box optimization algorithms and
of optimization problems, how can we best match algorithms
to problems (i.e., how best can we relax the black-box nature
of the algorithms and have them exploit some knowledge
concerning the optimization problem)? In particular, while
serious optimization practitioners almost always perform such
matching, it is usually on a heuristic basis; can such matching
be formally analyzed? More generally, what is the underlying
mathematical “skeleton” of optimization theory before the
“ﬂesh” of the probability distributions of a particular context
and set of optimization problems are imposed? What can
Manuscript received August 15, 1996; revised December 30, 1996. This
work was supported by the Santa Fe Institute and TXN Inc.
D. H. Wolpert is with IBM Almaden Research Center, San Jose, CA 95120-
W. G. Macready was with Santa Fe Institute, Santa Fe, NM 87501 USA.
He is now with IBM Almaden Research Center, San Jose, CA 95120-6099
Publisher Item Identiﬁer S 1089-778X(97)03422-X.
information theory and Bayesian analysis contribute to an
understanding of these issues? How a priori generalizable are
the performance results of a certain algorithm on a certain
class of problems to its performance on other classes of
problems? How should we even measure such generalization?
How should we assess the performance of algorithms on
problems so that we may programmatically compare those
algorithms?
Broadly speaking, we take two approaches to these questions. First, we investigate what a priori restrictions there are
on the performance of one or more algorithms as one runs
over the set of all optimization problems. Our second approach
is to instead focus on a particular problem and consider the
effects of running over all algorithms. In the current paper
we present results from both types of analyses but concentrate
largely on the ﬁrst approach. The reader is referred to the
companion paper for more types of analysis involving the
second approach.
We begin in Section II by introducing the necessary notation. Also discussed in this section is the model of computation
we adopt, its limitations, and the reasons we chose it.
One might expect that there are pairs of search algorithms
performs better than
on average, even if
sometimes outperforms
. As an example, one might expect
that hill climbing usually outperforms hill descending if one’s
goal is to ﬁnd a maximum of the cost function. One might also
expect it would outperform a random search in such a context.
One of the main results of this paper is that such expectations are incorrect. We prove two “no free lunch” (NFL) theorems in Section III that demonstrate this and more generally
illuminate the connection between algorithms and problems.
Roughly speaking, we show that for both static and timedependent optimization problems, the average performance
of any pair of algorithms across all possible problems is
identical. This means in particular that if some algorithm
performance is superior to that of another algorithm
some set of optimization problems, then the reverse must be
true over the set of all other optimization problems. (The reader
is urged to read this section carefully for a precise statement
of these theorems.) This is true even if one of the algorithms
is random; any algorithm
performs worse than randomly
just as readily (over the set of all optimization problems) as
it performs better than randomly. Possible objections to these
results are addressed in Sections III-A and III-B.
In Section IV we present a geometric interpretation of the
NFL theorems. In particular, we show that an algorithm’s
average performance is determined by how “aligned” it is
with the underlying probability distribution over optimization
problems on which it is run. This section is critical for an
1089–778X/97$10.00 1997 IEEE
IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 1, NO. 1, APRIL 1997
understanding of how the NFL results are consistent with the
well-accepted fact that many search algorithms that do not take
into account knowledge concerning the cost function work
well in practice.
Section V-A demonstrates that the NFL theorems allow
one to answer a number of what would otherwise seem to
be intractable questions. The implications of these answers
for measures of algorithm performance and of how best to
compare optimization algorithms are explored in Section V-B.
In Section VI we discuss some of the ways in which,
despite the NFL theorems, algorithms can have a priori
distinctions that hold even if nothing is speciﬁed concerning
the optimization problems. In particular, we show that there
can be “head-to-head” minimax distinctions between a pair of
algorithms, i.e., that when considering one function at a time,
a pair of algorithms may be distinguishable, even if they are
not when one looks over all functions.
In Section VII we present an introduction to the alternative
approach to the formal analysis of optimization in which
problems are held ﬁxed and one looks at properties across
the space of algorithms. Since these results hold in general,
they hold for any and all optimization problems and thus
are independent of the types of problems one is more or
less likely to encounter in the real world. In particular,
these results show that there is no a priori justiﬁcation for
using a search algorithm’s observed behavior to date on a
particular cost function to predict its future behavior on that
function. In fact when choosing between algorithms based on
their observed performance it does not sufﬁce to make an
assumption about the cost function; some (currently poorly
understood) assumptions are also being made about how the
algorithms in question are related to each other and to the
cost function. In addition to presenting results not found in
 , this section serves as an introduction to the perspective
adopted in .
We conclude in Section VIII with a brief discussion, a
summary of results, and a short list of open problems.
We have conﬁned all proofs to appendixes to facilitate the
ﬂow of the paper. A more detailed, and substantially longer,
version of this paper, a version that also analyzes some issues
not addressed in this paper, can be found in .
II. PRELIMINARIES
We restrict attention to combinatorial optimization in which
the search space
, though perhaps quite large, is ﬁnite.
We further assume that the space of possible “cost” values
is also ﬁnite. These restrictions are automatically met
for optimization algorithms run on digital computers where
is some 32 or 64 bit representation of the real
The size of the spaces
are indicated by
respectively. An optimization problem
(sometimes called
a “cost function” or an “objective function” or an “energy
function”) is represented as a mapping
indicates the space of all possible problems.
is of size
—a large but ﬁnite number. In addition to
, we are also interested in optimization problems that
depend explicitly on time. The extra notation required for such
time-dependent problems will be introduced as needed.
It is common in the optimization community to adopt
an oracle-based view of computation. In this view, when
assessing the performance of algorithms, results are stated
in terms of the number of function evaluations required to
ﬁnd a given solution. Practically though, many optimization
algorithms are wasteful of function evaluations. In particular,
many algorithms do not remember where they have already
searched and therefore often revisit the same points. Although
any algorithm that is wasteful in this fashion can be made
more efﬁcient simply by remembering where it has been (cf.
tabu search , ), many real-world algorithms elect not to
employ this stratagem. From the point of view of the oraclebased performance measures, these revisits are “artifacts”
distorting the apparent relationship between many such realworld algorithms.
This difﬁculty is exacerbated by the fact that the amount
of revisiting that occurs is a complicated function of both
the algorithm and the optimization problem and therefore
cannot be simply “ﬁltered out” of a mathematical analysis.
Accordingly, we have elected to circumvent the problem
entirely by comparing algorithms based on the number of
distinct function evaluations they have performed. Note that
this does not mean that we cannot compare algorithms that
are wasteful of evaluations—it simply means that we compare
algorithms by counting only their number of distinct calls to
the oracle.
We call a time-ordered set of
distinct visited points
a “sample” of size
. Samples are denoted by
. The points in a
sample are ordered according to the time at which they
were generated. Thus
indicates the
value of the
th successive element in a sample of size
its associated cost or
will be used to indicate the ordered set of cost values. The
space of all samples of size
) and the set of all possible samples of arbitrary
As an important clariﬁcation of this deﬁnition, consider a
hill-descending algorithm. This is the algorithm that examines
a set of neighboring points in
and moves to the one having
the lowest cost. The process is then iterated from the newly
chosen point. (Often, implementations of hill descending stop
when they reach a local minimum, but they can easily be
extended to run longer by randomly jumping to a new unvisited point once the neighborhood of a local minimum has been
exhausted.) The point to note is that because a sample contains
all the previous points at which the oracle was consulted, it
includes the
values of all the neighbors of the current
point, and not only the lowest cost one that the algorithm
moves to. This must be taken into account when counting the
An optimization algorithm
is represented as a mapping
from previously visited sets of points to a single new (i.e.,
previously unvisited) point in
. Formally,
. Given our decision to only measure distinct
function evaluations even if an algorithm revisits previously
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZATION
searched points, our deﬁnition of an algorithm includes all
common black-box optimization techniques like simulated annealing and evolutionary algorithms. (Techniques like branch
and bound are not included since they rely explicitly on
the cost structure of partial solutions.)
As deﬁned above, a search algorithm is deterministic; every
sample maps to a unique new point. Of course, essentially, all
algorithms implemented on computers are deterministic,1 and
in this our deﬁnition is not restrictive. Nonetheless, it is worth
noting that all of our results are extensible to nondeterministic
algorithms, where the new point is chosen stochastically from
the set of unvisited points. This point is returned to later.
Under the oracle-based model of computation any measure
of the performance of an algorithm after
iterations is a
function of the sample
. Such performance measures will
be indicated by
. As an example, if we are trying
to ﬁnd a minimum of
, then a reasonable measure of the
performance of
might be the value of the lowest
. Note that measures
of performance based on factors other than
(e.g., wall clock
time) are outside the scope of our results.
We shall cast all of our results in terms of probability
theory. We do so for three reasons. First, it allows simple
generalization of our results to stochastic algorithms. Second,
even when the setting is deterministic, probability theory
provides a simple consistent framework in which to carry out
proofs. The third reason for using probability theory is perhaps
the most interesting. A crucial factor in the probabilistic
framework is the distribution
This distribution, deﬁned over
, gives the probability that
is the actual optimization problem at hand.
An approach based on this distribution has the immediate
advantage that often knowledge of a problem is statistical in
nature and this information may be easily encodable in
For example, Markov or Gibbs random ﬁeld descriptions 
of families of optimization problems express
Exploiting
, however, also has advantages even when
we are presented with a single uniquely speciﬁed cost function.
One such advantage is the fact that although it may be fully
speciﬁed, many aspects of the cost function are effectively
unknown (e.g., we certainly do not know the extrema of the
function). It is in many ways most appropriate to have this
effective ignorance reﬂected in the analysis as a probability
distribution. More generally, optimization practitioners usually
act as though the cost function is partially unknown, in that the
same algorithm is used for all cost functions in a class of such
functions (e.g., in the class of all traveling salesman problems
having certain characteristics). In so doing, the practitioner
implicitly acknowledges that distinctions between the cost
functions in that class are irrelevant or at least unexploitable.
In this sense, even though we are presented with a single
particular problem from that class, we act as though we are
presented with a probability distribution over cost functions,
a distribution that is nonzero only for members of that class
of cost functions.
is thus a prior speciﬁcation of the
class of the optimization problem at hand, with different
1In particular, note that pseudorandom number generators are deterministic
given a seed.
classes of problems corresponding to different choices of
what algorithms we will use, and giving rise to different
distributions
Given our choice to use probability theory, the performance of an algorithm
times on a cost function
is measured with
. This is the conditional
probability of obtaining a particular sample
stated conditions. From
performance measures
can be found easily.
In the next section we analyze
and in particular how it varies with the algorithm
. Before proceeding
with that analysis, however, it is worth brieﬂy noting that there
are other formal approaches to the issues investigated in this
paper. Perhaps the most prominent of these is the ﬁeld of computational complexity. Unlike the approach taken in this paper,
computational complexity largely ignores the statistical nature
of search and concentrates instead on computational issues.
Much, though by no means all, of computational complexity is
concerned with physically unrealizable computational devices
(e.g., Turing machines) and the worst-case resource usage
required to ﬁnd optimal solutions. In contrast, the analysis
in this paper does not concern itself with the computational
engine used by the search algorithm, but rather concentrates
exclusively on the underlying statistical nature of the search
problem. The current probabilistic approach is complimentary
to computational complexity. Future work involves combining
our analysis of the statistical nature of search with practical
concerns for computational resources.
III. THE NFL THEOREMS
In this section we analyze the connection between algorithms and cost functions. We have dubbed the associated
results NFL theorems because they demonstrate that if an
algorithm performs well on a certain class of problems then
it necessarily pays for that with degraded performance on the
set of all remaining problems. Additionally, the name emphasizes a parallel with similar results in supervised learning
 , .
The precise question addressed in this section is: “How does
the set of problems
for which algorithm
better than algorithm
compare to the set
the reverse is true?” To address this question we compare the
sum over all
to the sum over all
. This comparison constitutes a major result of
this paper:
is independent of
when averaged
over all cost functions.
Theorem 1: For any pair of algorithms
A proof of this result is found in Appendix A. An immediate
corollary of this result is that for any performance measure
, the average over all
is independent of
. The precise way that the sample is mapped to
a performance measure is unimportant.
This theorem explicitly demonstrates that what an algorithm
gains in performance on one class of problems is necessarily
IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 1, NO. 1, APRIL 1997
offset by its performance on the remaining problems; that is
the only way that all algorithms can have the same
performance.
A result analogous to Theorem 1 holds for a class of timedependent cost functions. The time-dependent functions we
consider begin with an initial cost function
that is present
at the sampling of the ﬁrst
value. Before the beginning of
each subsequent iteration of the optimization algorithm, the
cost function is deformed to a new function, as speciﬁed by a
.2 We indicate this mapping with the
. So the function present during the th iteration is
is assumed to be a (potentially -dependent)
bijection between
. We impose bijectivity because if
it did not hold, the evolution of cost functions could narrow
in on a region of
’s for which some algorithms may perform
better than others. This would constitute an a priori bias in
favor of those algorithms, a bias whose analysis we wish to
defer to future work.
How best to assess the quality of an algorithm’s performance on time-dependent cost functions is not clear. Here we
consider two schemes based on manipulations of the deﬁnition
of the sample. In scheme 1 the particular
corresponding to a particular
is given by the
cost function that was present when
was sampled. In
contrast, for scheme 2 we imagine a sample
given by the
values from the present cost function for each of the
. Formally if
, then in scheme 1
in scheme 2 we have
is the ﬁnal cost function.
In some situations it may be that the members of the
sample “live” for a long time, compared to the time scale
of the dynamics of the cost function. In such situations it may
be appropriate to judge the quality of the search algorithm
; all those previous elements of the sample that are
still “alive” at time
, and therefore their current cost is of
interest. On the other hand, if members of the sample live
for only a short time on the time scale of the dynamics of
the cost function, one may instead be concerned with things
like how well the “living” member(s) of the sample track
the changing cost function. In such situations, it may make
more sense to judge the quality of the algorithm with the
Results similar to Theorem 1 can be derived for both
schemes. By analogy with that theorem, we average over all
possible ways a cost function may be time dependent, i.e., we
average over all
(rather than over all
). Thus we consider
is the initial cost function.
only takes effect for
, and since
there are a priori distinctions between algorithms as far as
the ﬁrst member of the sample is concerned. After redeﬁning
samples, however, to only contain those elements added after
the ﬁrst iteration of the algorithm, we arrive at the following
result, proven in Appendix B.
2An obvious restriction would be to require that T does not vary with time,
so that it is a mapping simply from F to F. An analysis for T’s limited in
this way is beyond the scope of this paper.
Theorem 2: For all
, algorithms
, and initial cost functions
So, in particular, if one algorithm outperforms another for
certain kinds of cost function dynamics, then the reverse must
be true on the set of all other cost function dynamics.
Although this particular result is similar to the NFL result
for the static case, in general the time-dependent situation
is more subtle. In particular, with time dependence there
are situations in which there can be a priori distinctions
between algorithms even for those members of the sample
arising after the ﬁrst. For example, in general there will be
distinctions between algorithms when considering the quantity
. To see this, consider the case where
is a set of contiguous integers and for all iterations
shift operator, replacing
]. For such a case we can construct
algorithms which behave differently a priori. For example,
to be the algorithm that ﬁrst samples
, and so on, regardless of the values in the sample.
Then for any
is always made up of identical
Accordingly,
is nonzero only for
which all values
are identical. Other search algorithms,
even for the same shift
, do not have this restriction on
values. This constitutes an
a priori distinction between
algorithms.
A. Implications of the NFL Theorems
As emphasized above, the NFL theorems mean that if an
algorithm does particularly well on average for one class of
problems then it must do worse on average over the remaining
problems. In particular, if an algorithm performs better than
random search on some class of problems then in must
perform worse than random search on the remaining problems.
Thus comparisons reporting the performance of a particular
algorithm with a particular parameter setting on a few sample
problems are of limited utility. While such results do indicate
behavior on the narrow range of problems considered, one
should be very wary of trying to generalize those results to
other problems.
Note, however, that the NFL theorems need not be viewed
as a way of comparing function classes
classes of evolution operators
, as the case might
be). They can be viewed instead as a statement concerning
any algorithm’s performance when
is not ﬁxed, under the
uniform prior over cost functions,
. If we wish
instead to analyze performance where
is not ﬁxed, as in this
alternative interpretation of the NFL theorems, but in contrast
with the NFL case
is now chosen from a nonuniform prior,
then we must analyze explicitly the sum
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZATION
Since it is certainly true that any class of problems faced by
a practitioner will not have a ﬂat prior, what are the practical
implications of the NFL theorems when viewed as a statement
concerning an algorithm’s performance for nonﬁxed
question is taken up in greater detail in Section IV but we
offer a few comments here.
First, if the practitioner has knowledge of problem characteristics but does not incorporate them into the optimization
algorithm, then
is effectively uniform. (Recall that
can be viewed as a statement concerning the practitioner’s
choice of optimization algorithms.) In such a case, the NFL
theorems establish that there are no formal assurances that the
algorithm chosen will be at all effective.
Second, while most classes of problems will certainly have
some structure which, if known, might be exploitable, the
simple existence of that structure does not justify choice
of a particular algorithm; that structure must be known and
reﬂected directly in the choice of algorithm to serve as such a
justiﬁcation. In other words, the simple existence of structure
per se, absent a speciﬁcation of that structure, cannot provide a
basis for preferring one algorithm over another. Formally, this
is established by the existence of NFL-type theorems in which
rather than average over speciﬁc cost functions
, one averages
over speciﬁc “kinds of structure,” i.e., theorems in which one
over distributions
. That such
theorems hold when one averages over all
means that
the indistinguishability of algorithms associated with uniform
is not some pathological, outlier case. Rather, uniform
is a “typical” distribution as far as indistinguishability
of algorithms is concerned. The simple fact that the
hand is nonuniform cannot serve to determine one’s choice of
optimization algorithm.
Finally, it is important to emphasize that even if one is
considering the case where
is not ﬁxed, performing the
associated average according to a uniform
is not essential
for NFL to hold. NFL can also be demonstrated for a range
of nonuniform priors. For example, any prior of the form
is the distribution of
values) will also give NFL theorems. The
-average can also
enforce correlations between costs at different
values and
NFL-like results will still be obtained. For example, if costs
are rank ordered (with ties broken in some arbitrary way) and
we sum only over all cost functions given by permutations of
those orderings, then NFL remains valid.
The choice of uniform
was motivated more from
theoretical rather than pragmatic concerns, as a way of analyzing the theoretical structure of optimization. Nevertheless,
the cautionary observations presented above make clear that
an analysis of the uniform
case has a number of
ramiﬁcations for practitioners.
B. Stochastic Optimization Algorithms
Thus far we have considered the case in which algorithms
are deterministic. What is the situation for stochastic algorithms? As it turns out, NFL results hold even for these
algorithms.
The proof is straightforward. Let
be a stochastic “nonpotentially revisiting” algorithm. Formally, this means that
a mapping taking any sample
-dependent distribution
that equals zero for all
. In this sense
what in statistics community is known as a “hyper-parameter,”
specifying the function
. One can now reproduce the derivation of the NFL
result for deterministic algorithms, only with
replaced by
throughout. In so doing, all steps in the proof remain valid.
This establishes that NFL results apply to stochastic algorithms
as well as deterministic ones.
IV. A GEOMETRIC PERSPECTIVE ON THE NFL THEOREMS
Intuitively, the NFL theorem illustrates that if knowledge
, perhaps speciﬁed through
, is not incorporated into
, then there are no formal assurances that
will be effective.
Rather, in this case effective optimization relies on a fortuitous
matching between
. This point is formally established
by viewing the NFL theorem from a geometric perspective.
Consider the space
of all possible cost functions. As previously discussed in regard to (1), the probability of obtaining
is the prior probability that the optimization
problem at hand has cost function
. This sum over functions
can be viewed as an inner product in
. Deﬁning the
components
, respectively
This equation provides a geometric interpretation of the optimization process.
can be viewed as ﬁxed to the sample
that is desired, usually one with a low cost value, and
is a measure of the computational resources that can be
afforded. Any knowledge of the properties of the cost function
goes into the prior over cost functions
. Then (2) says the
performance of an algorithm is determined by the magnitude
of its projection onto
, i.e., by how aligned
the problems . Alternatively, by averaging over
, it is easy
to see that
is an inner product between
. The expectation of any performance measure
can be written similarly.
In any of these cases,
must “match” or be aligned
to get the desired behavior. This need for matching
provides a new perspective on how certain algorithms can
perform well in practice on speciﬁc kinds of problems. For
example, it means that the years of research into the traveling
salesman problem (TSP) have resulted in algorithms aligned
with the (implicit)
describing traveling salesman problems
of interest to TSP researchers.
is independent of
has the interpretation
that for any particular
, all algorithms
same projection onto the uniform
, represented by the
diagonal vector
. Formally,
is some constant depending only upon
deterministic algorithms, the components of
 are all either zero or one,
so NFL also implies that
Geometrically, this means that the length of
independent of
. Different algorithms thus generate different
all having the same length and lying on a
cone with constant projection onto
. A schematic of this
situation is shown in Fig. 1 for the case where
dimensional. Because the components of
are binary,
we might equivalently view
as lying on the subset
of Boolean hypercube vertices having the same hamming
distance from
Now restrict attention to algorithms having the same probability of some particular
. The algorithms in this set lie in
the intersection of two cones—one about the diagonal, set by
the NFL theorem, and one set by having the same probability
. This is in general an
dimensional manifold.
Continuing, as we impose yet more
-based restrictions on
a set of algorithms, we will continue to reduce the dimensionality of the manifold by focusing on intersections of more and
more cones.
This geometric view of optimization also suggests measures
for determining how “similar” two optimization algorithms
are. Consider again (2). In that the algorithm only gives
, perhaps the most straightforward way to compare
two algorithms
would be by measuring how similar
the vectors
are, perhaps by evaluating
the dot product of those vectors. Those vectors, however, occur
on the right-hand side of (2), whereas the performance of the
algorithms—which is after all our ultimate concern—occurs on
the left-hand side. This suggests measuring the similarity of
two algorithms not directly in terms of their vectors
but rather in terms of the dot products of those vectors with .
For example, it may be the case that algorithms behave very
similarly for certain
but are quite different for other
. In many respects, knowing this about two algorithms
is of more interest than knowing how their vectors
As another example of a similarity measure suggested by the
geometric perspective, we could measure similarity between
algorithms based on similarities between
’s. For example,
for two different algorithms, one can imagine solving for the
that optimizes
for those algorithms, in
some nontrivial sense.3 We could then use some measure of
distance between those two
distributions as a gauge of
how similar the associated algorithms are.
Unfortunately, exploiting the inner product formula in practice, by going from a
to an algorithm optimal for
, appears to often be quite difﬁcult. Indeed, even
determining a plausible
for the situation at hand is
often difﬁcult. Consider, for example, TSP problems with
cities. To the degree that any practitioner attacks all
TSP cost functions with the same algorithm, he/she implicitly
ignores distinctions between such cost functions. In this, that
practitioner has implicitly agreed that the problem is one of
how their ﬁxed algorithm does across the set of all
TSP cost functions. But the detailed nature of the
is uniform over this class of problems appears to be difﬁcult
to elucidate.
On the other hand, there is a growing body of work that does
rely explicitly on enumeration of
. For example, applications of Markov random ﬁelds , to cost landscapes
directly as a Gibbs distribution.
V. CALCULATIONAL APPLICATIONS OF THE NFL THEOREMS
In this section, we explore some of the applications of
the NFL theorems for performing calculations concerning
optimization. We will consider both calculations of practical
and theoretical interest and begin with calculations of theoretical interest, in which information-theoretic quantities arise
naturally.
A. Information-Theoretic Aspects of Optimization
For expository purposes, we simplify the discussion slightly
by considering only the histogram of number of instances of
each possible cost value produced by a run of an algorithm, and
not the temporal order in which those cost values were generated. Many real-world performance measures are independent
of such temporal information. We indicate that histogram with
the symbol ;
components
is the number of times cost value
occurs in the sample
Now consider any question like the following: “What fraction of cost functions give a particular histogram
values after
distinct cost evaluations produced by using a
particular instantiation of an evolutionary algorithm?”
At ﬁrst glance this seems to be an intractable question,
but the NFL theorem provides a way to answer it. This is
because—according to the NFL theorem—the answer must be
independent of the algorithm used to generate . Consequently,
3In particular, one may want to impose restrictions on P (f). For instance,
one may wish to only consider P (f) that are invariant under at least partial
relabeling of the elements in X, to preclude there being an algorithm that will
assuredly “luck out” and land on minx2X f(x) on its very ﬁrst query.
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZATION
we can choose an algorithm for which the calculation is
tractable.
Theorem 3: For any algorithm, the fraction of cost functions that result in a particular histogram
For large enough
, this can be approximated as
is the entropy of the distribution
is a constant that does not depend on
This theorem is derived in Appendix C. If some of the
zero, the approximation still holds, only with
redeﬁned to
exclude the ’s corresponding to the zero-valued
. However,
is deﬁned and the normalization constant of (3) can be found
by summing over all
lying on the unit simplex .
A related question is the following: “For a given cost
function, what is the fraction
of all algorithms that give
rise to a particular
?” It turns out that the only feature of
relevant for this question is the histogram of its cost values
formed by looking across all
. Specify the fractional form
of this histogram by
so that there are
has the th
In Appendix D it is shown that to leading order,
depends on yet another information-theoretic quantity, the
Kullback–Liebler distance between
Theorem 4: For a given
with histogram
fraction of algorithms that give rise to a histogram
is given by
For large enough
, this can be written as
is the Kullback–Liebler distance between
the distributions
As before,
can be calculated by summing
over the unit
B. Measures of Performance
We now show how to apply the NFL framework to calculate
certain benchmark performance measures. These allow both
the programmatic assessment of the efﬁcacy of any individual
optimization algorithm and principled comparisons between
algorithms.
Without loss of generality, assume that the goal of the search
process is ﬁnding a minimum. So we are interested in the dependence of
, by which we mean
the probability that the minimum cost an algorithm
on problem
distinct evaluations is larger than
least three quantities related to this conditional probability can
be used to gauge an algorithm’s performance in a particular
optimization run:
i) the uniform average of
cost functions;
ii) the form
takes for the random
algorithm, which uses no information from the sample
iii) the fraction of algorithms which, for a particular
, result in a
whose minimum exceeds .
These measures give benchmarks which any algorithm run
on a particular cost function should surpass if that algorithm is
to be considered as having worked well for that cost function.
Without loss of generality assume that the th cost value
) equals . So cost values range from minimum of one
to a maximum of
, in integer increments. The following
results are derived in Appendix E.
Theorem 5:
is the fraction of cost lying above .
In the limit of
, this distribution obeys the following
relationship:
Unless one’s algorithm has its best-cost-so-far drop faster
than the drop associated with these results, one would be hard
pressed indeed to claim that the algorithm is well suited to the
cost function at hand. After all, for such a performance the
algorithm is doing no better than one would expect it to when
run on a randomly chosen cost function.
Unlike the preceding measure, the measures analyzed below
take into account the actual cost function at hand. This is
manifested in the dependence of the values of those measures
on the vector
given by the cost function’s
histogram.
Theorem 6: For the random algorithm
is the fraction of points in
. To ﬁrst order in
This result allows the calculation of other quantities of
interest for measuring performance, for example the quantity
Note that for many cost functions of both practical and
theoretical interest, cost values are approximately distributed
IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 1, NO. 1, APRIL 1997
Gaussianly. For such cases, we can use the Gaussian nature
of the distribution to facilitate our calculations. In particular,
if the mean and variance of the Gaussian are
respectively, then we have
where erfc is the complimentary error function.
To calculate the third performance measure, note that for
, for any (deterministic) algorithm
is either one or zero. Therefore the fraction of
algorithms that result in a
whose minimum exceeds
Expanding in terms of
, we can rewrite the numerator of
this ratio as
ratio of this quantity to
, however, is exactly what was
calculated when we evaluated measure ii) [see the beginning
of the argument deriving (4)]. This establishes the following
Theorem 7: For ﬁxed
, the fraction of algorithms
which result in a
whose minimum exceeds
is given by the
quantity on the right-hand sides of (4) and (5).
As a particular example of applying this result, consider
measuring the value of
produced in a particular run
of an algorithm. Then imagine that when it is evaluated for
equal to this value, the quantity given in (5) is less than 1/2.
In such a situation the algorithm in question has performed
worse than over half of all search algorithms, for the
at hand, hardly a stirring endorsement.
None of the above discussion explicitly concerns the dynamics of an algorithm’s performance as
increases. Many
aspects of such dynamics may be of interest. As an example,
let us consider whether, as
grows, there is any change in
how well the algorithm’s performance compares to that of the
random algorithm.
To this end, let the sample generated by the algorithm
, and deﬁne
the number of additional steps it takes the algorithm to ﬁnd
. Now we can estimate the number
of steps it would have taken the random search algorithm to
and ﬁnd a point whose
was less than
expected value of this number of steps is
is the fraction of
. Therefore
is how much worse
did than the random
algorithm, on average.
Next, imagine letting
run for many steps over some ﬁtness
and plotting how well
did in comparison to the
random algorithm on that run, as
increased. Consider the
step where
th new value of
. For that step,
there is an associated
[the number of steps until the next
. Accordingly, indicate that step on our
plot as the point
. Put down as many points
on our plot as there are successive values of
If throughout the run
is always a better match to
is the random search algorithm, then all the points in the plot
will have their ordinate values lie below zero. If the random
algorithm won for any of the comparisons however, that would
give a point lying above zero. In general, even if the points
all lie to one side of zero, one would expect that as the
search progresses there would be a corresponding (perhaps
systematic) variation in how far away from zero the points lie.
That variation indicates when the algorithm is entering harder
or easier parts of the search.
Note that even for a ﬁxed
, by using different starting
points for the algorithm one could generate many of these
plots and then superimpose them. This allows a plot of the
mean value of
as a function of
along with
an associated error bar. Similarly, the single number
characterizing the random algorithm could be replaced with
a full distribution over the number of required steps to ﬁnd a
new minimum. In these and similar ways, one can generate
a more nuanced picture of an algorithm’s performance than
is provided by any of the single numbers given by the
performance measure discussed above.
VI. MINIMAX DISTINCTIONS BETWEEN ALGORITHMS
The NFL theorems do not directly address minimax properties of search. For example, say we are considering two
deterministic algorithms
. It may very well be that
there exist cost functions
’s histogram is much
better (according to some appropriate performance measure)
’s, but no cost functions for which the reverse is true.
For the NFL theorem to be obeyed in such a scenario, it would
have to be true that there are many more
histogram is better than
’s than vice-versa, but it is only
slightly better for all those
. For such a scenario, in a certain
has better “head-to-head” minimax behavior than
badly, but none for which
does substantially worse than
Formally, we say that there exists head-to-head minimax
distinctions between two algorithms
iff there exists
such that for at least one cost function
, the difference
, but there is no other
. A similar
deﬁnition can be used if one is instead interested in
, rather than
It appears that analyzing head-to-head minimax properties
of algorithms is substantially more difﬁcult than analyzing
average behavior as in the NFL theorem. Presently, very
little is known about minimax behavior involving stochastic
algorithms. In particular, it is not known if there are any senses
in which a stochastic version of a deterministic algorithm
has better/worse minimax behavior than that deterministic
algorithm. In fact, even if we stick completely to deterministic
algorithms, only an extremely preliminary understanding of
minimax issues has been reached.
What is known is the following. Consider the quantity
for deterministic algorithms
the distribution of a random variable
evaluated at
For deterministic algorithms, this quantity is just the number
such that it is both true that
produces a sample
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZATION
components
produces a sample with
components
In Appendix F, it is proven by example that this quantity
need not be symmetric under interchange of
Theorem 8: In general
This means that under certain circumstances, even knowing
components of the samples produced by two algorithms run on the same unknown
, we can infer something
concerning which algorithm produced each population.
Now consider the quantity
again for deterministic algorithms
. This quantity is
just the number of
such that it is both true that
a histogram
produces a histogram
need not be symmetric under interchange of
Appendix F). This is a stronger statement than the asymmetry
’s statement, since any particular histogram corresponds
to multiple samples.
It would seem that neither of these two results directly
implies that there are algorithms
such that for
’s histogram is much better than
’s, but for
’s is the reverse is true. To investigate this problem
involves looking over all pairs of histograms (one pair for
) such that there is the same relationship between (the
performances of the algorithms, as reﬂected in) the histograms.
Simply having an inequality between the sums presented above
does not seem to directly imply that the relative performances
between the associated pair of histograms is asymmetric. (To
formally establish this would involve creating scenarios in
which there is an inequality between the sums, but no headto-head minimax distinctions. Such an analysis is beyond the
scope of this paper.)
On the other hand, having the sums be equal does carry obvious implications for whether there are head-to-head minimax
distinctions. For example, if both algorithms are deterministic,
then for any particular
equals one for one
pair and zero for all others. In such
is just the number
that result in the pair
that there are no head-to-head minimax distinctions between
. The converse, however, does not appear to hold.4
4Consider the grid of all (z; z0) pairs. Assign to each grid point the number
of f that result in that grid point’s (z; z0) pair. Then our constraints are i)
by the hypothesis that there are no head-to-head minimax distinctions, if grid
point (z1; z2) is assigned a nonzero number, then so is (z2; z1) and ii) by
the no-free-lunch theorem, the sum of all numbers in row z equals the sum
of all numbers in column z. These two constraints do not appear to imply
that the distribution of numbers is symmetric under interchange of rows and
columns. Although again, like before, to formally establish this point would
involve explicitly creating search scenarios in which it holds.
As a preliminary analysis of whether there can be headto-head minimax distinctions, we can exploit the result in
Appendix F, which concerns the case where
First, deﬁne the following performance measures of twoelement samples,
of any other argument
In Appendix F we show that for this scenario there exist
pairs of algorithms
such that for one
generates the histogram
generates the histogram
, but there is no
for which the reverse occurs (i.e.,
there is no
generates the histogram
So in this scenario, with our deﬁned performance measure,
there are minimax distinctions between
the performance measures of algorithms
respectively, zero and two. The difference in the
for the two algorithms is two for that
. There are no other
, however, for which the difference is
2. For this
is minimax superior to algorithm
It is not currently known what restrictions on
needed for there to be minimax distinctions between the
algorithms. As an example, it may well be that for
there are no minimax distinctions between algorithms.
More generally, at present nothing is known about “how
big a problem” these kinds of asymmetries are. All of the
examples of asymmetry considered here arise when the set
has visited overlaps with those that
visited. Given such overlap, and certain properties of how the
algorithms generated the overlap, asymmetry arises. A precise
speciﬁcation of those “certain properties” is not yet in hand.
Nor is it known how generic they are, i.e., for what percentage
of pairs of algorithms they arise. Although such issues are
easy to state (see Appendix F), it is not at all clear how best
to answer them.
Consider, however, the case where we are assured that,
steps, the samples of two particular algorithms have
not overlapped. Such assurances hold, for example, if we are
comparing two hill-climbing algorithms that start far apart cost function at hand.
be two different choosing procedures. Let
5a can know to avoid the elements it has seen before. However a priori, a
has no way to avoid the elements observed by a0 has (and vice-versa). Rather
than have the deﬁnition of a somehow depend on the elements in d0   d
(and similarly for a0), we deal with this problem by deﬁning c>m to be set
only by those elements in d>m that lie outside of d[. (This is similar to the
convention we exploited above to deal with potentially retracing algorithms.)
Formally, this means that the random variable c>m is a function of d[ as
well as of d>m. It also means there may be fewer elements in the histogram
c>m than there are in the sample d>m.
the number of elements in
Implicit in this result is the assumption that the sum excludes
those algorithms
that do not result in
respectively when run on
In the precise form it is presented above, the result may
appear misleading, since it treats all samples equally, when
for any given
some samples will be more likely than others.
Even if one weights samples according to their probability
of occurrence, however, it is still true that, on average, the
choosing procedure one uses has no effect on likely
is established by the following result, proven in Appendix H.
Theorem 11: Under the conditions given in the preceding
These results show that no assumption for
justiﬁes using some choosing procedure as far as subsequent
search is concerned. To have an intelligent choosing procedure,
one must take into account not only
but also the search
algorithms one is choosing among. This conclusion may be
surprising. In particular, note that it means that there is no
intrinsic advantage to using a rational choosing procedure,
which continues with the better of
, rather than using
an irrational choosing procedure which does the opposite.
These results also have interesting implications for degenerate choosing procedures
always use algorithm
always use algorithm
. As applied to this case, they
mean that for ﬁxed
does better (on average)
with the algorithms in some set
does better (on
average) with the algorithms in the set of all other algorithms.
In particular, if for some favorite algorithms a certain “wellbehaved”
results in better performance than does the random
, then that well-behaved
gives worse than random behavior
on the set all remaining algorithms. In this sense, just as there
are no universally efﬁcacious search algorithms, there are no
universally benign
which can be assured of resulting in better
than random performance regardless of one’s algorithm.
In fact, things may very well be worse than this. In supervised learning, there is a related result . Translated into
the current context, that result suggests that if one restricts
sums to only be over those algorithms that are a good match
, then it is often the case that “stupid” choosing
procedures—like the irrational procedure of choosing the
algorithm with the less desirable
—outperform “intelligent”
ones. What the set of algorithms summed over must be in
order for a rational choosing procedure to be superior to an
irrational procedure is not currently known.
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZATION
VIII. CONCLUSIONS
A framework has been presented in which to compare
general-purpose optimization algorithms. A number of NFL
theorems were derived that demonstrate the danger of comparing algorithms by their performance on a small sample of
problems. These same results also indicate the importance of
incorporating problem-speciﬁc knowledge into the behavior of
the algorithm. A geometric interpretation was given showing
what it means for an algorithm to be well suited to solving
a certain class of problems. The geometric perspective also
suggests a number of measures to compare the similarity of
various optimization algorithms.
More direct calculational applications of the NFL theorem were demonstrated by investigating certain informationtheoretic aspects of search, as well as by developing a number
of benchmark measures of algorithm performance. These
benchmark measures should prove useful in practice.
We provided an analysis of the ways that algorithms can
differ a priori despite the NFL theorems. We have also
provided an introduction to a variant of the framework that
focuses on the behavior of a range of algorithms on speciﬁc problems (rather than speciﬁc algorithms over a range
of problems). This variant leads directly to reconsideration
of many issues addressed by computational complexity, as
detailed in .
Much future work clearly remains. Most important is the
development of practical applications of these ideas. Can the
geometric viewpoint be used to construct new optimization
techniques in practice? We believe the answer to be yes. At
a minimum, as Markov random ﬁeld models of landscapes
become more wide spread, the approach embodied in this
paper should ﬁnd wider applicability.
APPENDIX A
NFL PROOF FOR STATIC COST FUNCTIONS
We show that
has no dependence on
. Conceptually, the proof is quite simple but necessary
bookkeeping complicates things, lengthening the proof considerably. The intuition behind the proof is straightforward:
by summing over all
we ensure that the past performance of an algorithm has no bearing on its future performance. Accordingly, under such a sum, all algorithms
perform equally.
The proof is by induction. The induction is based on
, and the inductive step is based on breaking
two independent parts, one for
and one for
These are evaluated separately, giving the desired result.
, we write the ﬁrst sample as
. The only possible value for
so we have
is the Kronecker delta function.
Summing over all possible cost functions,
one only for those functions which have cost
Therefore that sum equals
, independent of
which is independent of
. This bases the induction.
The inductive step requires that if
independent of
, then so also is
. Establishing this step completes the proof.
We begin by writing
, will depend on the new
, and nothing else. So we expand over these possible
values, obtaining
Next note that since
, it does not depend
directly on
. Consequently we expand in
to remove the
dependence in
where use was made of the fact that
and the fact that
The sum over cost functions
is done ﬁrst. The cost
function is deﬁned both over those points restricted to
those points outside of
will depend on the
values deﬁned over points inside
IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 1, NO. 1, APRIL 1997
depends only on the
values deﬁned over
points outside
. (Recall that
.) So we have
contributes a constant,
equal to the number of functions deﬁned over points not in
passing through
By hypothesis, the right-hand side of this equation is independent of
, so the left-hand side must also be. This completes
the proof.
APPENDIX B
NFL PROOF FOR TIME-DEPENDENT COST FUNCTIONS
In analogy with the proof of the static NFL theorem, the
proof for the time-dependent case proceeds by establishing the
-independence of the sum
, where here
To begin, replace each
in this sum with a set of cost
functions,
, one for each iteration of the algorithm. To do
this, we start with the following:
where the sequence of cost functions,
, has been indicated
by the vector
. In the next step, the sum over
all possible
is decomposed into a series of sums. Each sum
in the series is over the values
can take for one particular
iteration of the algorithm. More formally, using
is independent of the values
, so those values can be absorbed into an overall
-independent proportionality constant.
Consider the innermost sum over
, for ﬁxed values of
the outer sum indexes
. For ﬁxed values of the
outer indexes,
is just a particular
ﬁxed cost function. Accordingly, the innermost sum over
is simply the number of bijections of
that map that
ﬁxed cost function to
. This is the constant,
Consequently, evaluating the
sum yields
The sum over
can be accomplished in the same manner
is summed over. In fact, all the sums over all
be done, leaving
In this last step, the statistical independence of
been used.
Further progress depends on whether
represents
. We begin with analysis of the
case. For this case
only reﬂects cost
values from the last cost function,
. Using this result gives
The ﬁnal sum over
is a constant equal to the number of
ways of generating the sample
from cost values drawn
. The important point is that it is independent of
the particular
. Because of this the sum over
evaluated eliminating the
dependence
This completes the proof of Theorem 2 for the case of
The proof of Theorem 2 is completed by turning to the
case. This is considerably more difﬁcult since
cannot be simpliﬁed so that the sums over
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZATION
decoupled. Nevertheless, the NFL result still holds. This is
proven by expanding (7) over possible
The innermost sum over
only has an effect on the
term so it contributes
. This is a constant, equal to
The sum over
is now simple
The above equation is of the same form as (8), only with a
remaining sample of size
rather than
. Consequently, in
an analogous manner to the scheme used to evaluate the sums
that existed in (8), the sums over
can be evaluated. Doing so simply generates
-independent proportionality constants. Continuing in
this manner, all sums over the
can be evaluated to ﬁnd
There is algorithm dependence in this result, but it is the
trivial dependence discussed previously. It arises from how
the algorithm selects the ﬁrst
point in its sample,
Restricting interest to those points in the sample that are generated subsequent to the ﬁrst, this result shows that there are no
distinctions between algorithms. Alternatively, summing over
the initial cost function
, all points in the sample could be
considered while still retaining an NFL result.
APPENDIX C
As noted in the discussion leading up to Theorem 3, the
fraction of functions giving a speciﬁed histogram
is independent of the algorithm. Consequently, a simple algorithm is used to prove the theorem. The algorithm visits points
in some canonical order, say
that the histogram
is speciﬁed by giving the frequencies
of occurrence, across the
, for each of the
possible cost values. The number of
’s giving the desired
histogram under this algorithm is just the multinomial giving
the number of ways of distributing the cost values in . At the
the cost can assume any of
values giving the ﬁrst result of Theorem 3.
The expression of
in terms of the entropy of
follows from an application of Stirling’s approximation to
, which is valid when all of the
are large.
In this case the multinomial is written
from which the theorem follows by exponentiating this result.
APPENDIX D
In this section the proportion of all algorithms that give a
particular
for a particular
is calculated. The calculation
proceeds in several steps
is ﬁnite there are a ﬁnite number of different
samples. Therefore any (deterministic)
is a huge, but ﬁnite,
list indexed by all possible
’s. Each entry in the list is the
in question outputs for that
Consider any particular unordered set of
where no two of the pairs share the same
value. Such a
set is called an unordered path
. Without loss of generality,
from now on we implicitly restrict the discussion to unordered
paths of length
. A particular
is in or from a particular
if there is a unordered set of
pairs identical to
The numerator on the right-hand side of (3) is the number of
unordered paths in the given
that give the desired .
The number of unordered paths in
that give the desired
, the numerator on the right-hand side of (3), is proportional
to the number of
’s that give the desired
proof of this claim constitutes a proof of (3). Furthermore, the
proportionality constant is independent of
Proof: The proof is established by constructing a mapping
taking in an
that gives the desired
, and producing a
that is in
and gives the desired
Showing that for any
the number of algorithms
IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 1, NO. 1, APRIL 1997
is a constant, independent of
. and that
is single valued will complete the proof.
Recalling that every
value in an unordered path is distinct,
any unordered path
gives a set of
different ordered paths.
Each such ordered path
in turn provides a set of
successive
’s (if the empty
is included) and a following
. Indicate by
this set of the ﬁrst
’s provided by
From any ordered path
a “partial algorithm” can be
constructed. This consists of the list of an
, but with only the
entries in the list ﬁlled in, the remaining entries are
blank. Since there are
distinct partial
’s for each
for each ordered path corresponding to
), there are
partially ﬁlled-in lists for each
. A partial algorithm may or
may not be consistent with a particular full algorithm. This
allows the deﬁnition of the inverse of
that is in
(the set of all
that are consistent
with at least one partial algorithm generated from
when run on
To complete the ﬁrst part of the proof, it must be shown that
that are in
contains the same
number of elements, regardless of
, or . To that end, ﬁrst
generate all ordered paths induced by
and then associate each
such ordered path with a distinct
-element partial algorithm.
Now how many full algorithm lists are consistent with at least
one of these partial algorithm partial lists? How this question is
answered is the core of this appendix. To answer this question,
reorder the entries in each of the partial algorithm lists by
permuting the indexes
of all the lists. Obviously such a
reordering will not change the answer to our question.
Reordering is accomplished by interchanging pairs of
indexes. First, interchange any
index of the form
entry is ﬁlled in any of our partial algorithm lists with
arbitrary constant
refers to the
th element
. Next, create some arbitrary but ﬁxed ordering of
. Then interchange any
of the form
whose entry
is ﬁlled in any of our (new) partial algorithm lists with
. Recall that all the
be distinct. By construction, the resultant partial algorithm lists
are independent of
, as is the number of such lists
). Therefore the number of algorithms consistent with
at least one partial algorithm list in
is independent of
. This completes the ﬁrst part of the proof.
For the second part, ﬁrst choose any two unordered paths
that differ from one another,
. There is no ordered
constructed from
that equals an ordered path
constructed from
. So choose any such
and any such
. If they disagree for the null
, then we know that there
is no (deterministic)
that agrees with both of them. If they
agree for the null , then since they are sampled from the same
, they have the same single-element
. If they disagree for
, then there is no
that agrees with both of them. If they
agree for that
, then they have the same double-element
Continue in this manner all the up to the
Since the two ordered paths differ, they must have disagreed
at some point by now, and therefore there is no
that agrees
with both of them. Since this is true for any
, we see that there is no
. This completes the proof.
To show the relation to the Kullback–Liebler distance the
product of binomials is expanded with the aid of Stirling’s
approximation when both
It has been assumed that
, which is reasonable when
. Expanding
, to second
order gives
then in terms of
is the Kullback–Liebler
distance between the distributions
. Exponentiating
this expression yields the second result in Theorem 4.
APPENDIX E
BENCHMARK MEASURES OF PERFORMANCE
The result for each benchmark measure is established in
The ﬁrst measure is
. Consider
for which the summand equals zero or one for all
deterministic
. It is one only if
and so on. These restrictions will ﬁx the value of
points while
remains free at all other points. Therefore
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZATION
Using this result in (9) we ﬁnd
which is the result quoted in Theorem 5.
In the limit as
gets large write
and substitute in for
. Replacing
turns the sum into
. Next, write
and multiply and divide the summand by
. To take the limit of
, apply L’hopital’s rule
to the ratio in the summand. Next use the fact that
to zero to cancel terms in the summand. Carrying through the
algebra and dividing by
, we get a Riemann sum of the
. Evaluating the integral gives
the second result in Theorem 5.
The second benchmark concerns the behavior of the random
algorithm. Summing over the
values of different histograms
, the performance of
is the probability of obtaining histogram
random draws from the histogram
of the function
. This can be viewed as the deﬁnition of
. This probability
has been calculated previously as
which is (4) of Theorem 6.
APPENDIX F
PROOF RELATED TO MINIMAX
DISTINCTIONS BETWEEN ALGORITHMS
This proof is by example. Consider three points in
, and three points in
1) Let the ﬁrst point
and the ﬁrst point
2) If at its ﬁrst point
, it jumps to
Otherwise it jumps to
3) If at its ﬁrst point
, it jumps to
. If it sees
, it jumps to
Consider the cost function that has as the
values for the
, respectively.
will produce a sample
function, and
will produce
The proof is completed if we show that there is no cost
function so that
produces a sample containing
and such that
produces a sample containing
There are four possible pairs of samples to consider:
Since if its ﬁrst point is a
which is where
starts, when
’s ﬁrst point is a
its second point must
’s ﬁrst point. This rules out possibilities i) and ii).
For possibilities iii) and iv), by
’s sample we know that
must be of the form
, for some variable
case iii),
would need to equal
, due to the ﬁrst point in
’s sample. For that case, however, the second point
would be the value at
, which is
, contrary to hypothesis.
For case iv), we know that the
would have to equal
to the ﬁrst point in
’s sample. That would mean, however,
for its second point and would therefore
, contrary to hypothesis.
Accordingly, none of the four cases is possible. This is
a case both where there is no symmetry under exchange of
’s between
, and no symmetry under exchange of
histograms.
APPENDIX G
FIXED COST FUNCTIONS AND CHOOSING PROCEDURES
Since any deterministic search algorithm is a mapping from
, any search algorithm is a vector in the
. The components of such a vector are indexed by
the possible samples, and the value for each component is the
that the algorithm produces given the associated sample.
Consider now a particular sample
we can say whether or not any other sample of size greater
has the (ordered) elements of
as its ﬁrst
elements. The set of those samples that do start with
way deﬁnes a set of components of any algorithm vector
Those components will be indicated by
The remaining components of
are of two types. The ﬁrst is
given by those samples that are equivalent to the ﬁrst
elements in
. The values of those components
for the vector algorithm
will be indicated by
second type consists of those components corresponding to
all remaining samples. Intuitively, these are samples that are
not compatible with
. Some examples of such samples are
those that contain as one of their ﬁrst
elements an element
not found in
, and samples that re-order the elements found
. The values of
for components of this second type will
be indicated by
IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 1, NO. 1, APRIL 1997
represent either
. We are interested in
The summand is independent of the values of
for either of our two
’s. In addition, the number of such
values is a constant. (It is given by the product, over all
samples not consistent with
, of the number of possible
each such sample could be mapped to.) Therefore, up to an
overall constant independent of
By deﬁnition, we are implicitly restricting the sum to those
so that our summand is deﬁned. This means that
we actually only allow one value for each component in
(namely, the value that gives the next
element in
similarly for
. Therefore the sum reduces to
Note that no component of
. The same is true
. So the sum over
is over the same components
as the sum over
. Now for ﬁxed
’s choice of
is ﬁxed. Accordingly, without loss of
generality, the sum can be rewritten as
with the implicit assumption that
. This sum
is independent of
APPENDIX H
PROOF OF THEOREM 11
refer to a choosing procedure. We are interested in
The sum over
can be moved outside the sum
. Consider any term in that sum (i.e.,
any particular pair of values of
). For that
that result in
, respectively, when
, and zero otherwise. (Recall the assumption
are deterministic.) This means that the
factor simply restricts our sum
considered in our theorem.
Accordingly, our theorem tell us that the summand of the sum
is the same for choosing procedures
Therefore the full sum is the same for both procedures.