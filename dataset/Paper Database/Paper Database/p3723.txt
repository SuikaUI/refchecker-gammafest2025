Machine learning time series regressions with an application
to nowcasting
Online Appendix
Andrii Babii∗
Eric Ghysels†
Jonas Striaukas‡
February 24, 2021
∗Department of Economics, University of North Carolina–Chapel Hill - Gardner Hall, CB 3305 Chapel Hill, NC
27599-3305. Email: 
†Department of Economics and Kenan-Flagler Business School, University of North Carolina–Chapel Hill. Email:
 .
‡LIDAM UC Louvain and FRS–FNRS Research Fellow. Email: .
Dictionaries
In this section, we review the choice of dictionaries for the MIDAS weight function. It is possible
to construct dictionaries using arbitrary sets of functions, including a mix of algebraic polynomials,
trigonometric polynomials, B-splines, Haar basis, or wavelets. In this paper, we mostly focus on
dictionaries generated by orthogonalized algebraic polynomials, though it might be interesting to
tailor the dictionary for each particular application. The attractiveness of algebraic polynomials
comes from their ability to generate a variety of shapes with a relatively low number of parameters,
which is especially desirable in low signal-to-noise environments. The general family of appropriate
orthogonal algebraic polynomials is given by Jacobi polynomials that nest Legendre, Gegenbauer,
and Chebychev’s polynomials as a special case.
Example A.1.1 (Jacobi polynomials). Applying the Gram-Schmidt orthogonalization to the power
polynomials {1, x, x2, x3, . . . } with respect to the measure
dµ(x) = (1 −x)α(1 + x)βdx,
α, β > −1,
on [−1, 1], we obtain Jacobi polynomials. In practice Jacobi polynomials can be computed through
the well-known tree-term recurrence relation for n ≥0
n+1 (x) = axP (α,β)
(x) + bP (α,β)
(x) −cP (α,β)
with a = (2n+α+β +1)(2n+α+β +2)/2(n+1)(n+α+β +1), b = (2n+α+β +1)(α2−β2)/2(n+
1)(n+α+β+1)(2n+α+β), and c = (α+n)(β+n)(2n+α+β+2)/(n+1)(n+α+β+1)(2n+α+β).
To obtain the orthogonal basis on , we shift Jacobi polynomials with aﬃne bijection x 7→2x−1.
For α = β, we obtain Gegenbauer polynomials, for α = β = 0, we obtain Legendre polynomials,
while for α = β = −1/2 or α = β = 1/2, we obtain Chebychev’s polynomials of two kinds.
In the mixed frequency setting, non-orthogonalized polynomials, {1, x, x2, x3, . . . }, are also
called Almon polynomials. It is preferable to use orthogonal polynomials in practice due to reduced multicollinearity and better numerical properties. At the same time, orthogonal polynomials
are available in Matlab, R, Python, and Julia packages. Legendre polynomials is our default recommendation, while other choices of α and β are preferable if we want to accommodate MIDAS
weights with other integrability/tail properties.
Online Appendix - 1
We noted in the main body of the paper that the speciﬁcation in equation (2) deviates from
the standard MIDAS polynomial speciﬁcation as it results in a linear regression model - a subtle
but key innovation as it maps MIDAS regressions in the standard regression framework. Moreover,
casting the MIDAS regressions in a linear regression framework renders the optimization problem
convex, something only achieved by Siliverstovs using the U-MIDAS of Foroni, Marcellino,
and Schumacher which does not recognize the mixed frequency data structure, unlike our
Proofs of main results
Lemma A.2.1. Consider ∥.∥= α|.|1 + (1 −α)|.|2, where |.|q is ℓq norm on Rp. Then the dual
norm of ∥.∥, denoted ∥.∥∗, satisﬁes
∥z∥∗≤α|z|∗
1 + (1 −α)|z|∗
where |.|∗
1 is the dual norm of |.|1 and |.|∗
2 is the dual norm of |.|2.
Proof. Clearly, ∥.∥is a norm. By the convexity of x 7→x−1 on (0, ∞)
+ (1 −α)|⟨z, b⟩|
+ (1 −α) sup
1 + (1 −α)|z|∗
Proof of Theorem 3.1. By H¨older’s inequality for every ς > 0
j∈[p] ∥u0x0,j∥ς ≤∥u0∥ςq1 max
j∈[p] ∥x0,j∥ςq2
= 1 and q1, q2 ≥1. Therefore, under Assumption 3.1 (i), maxj∈[p] ∥u0x0,j∥ς = O(1)
with ς = qr/(q + r). Recall also that E[utxt,j] = 0, ∀j ∈[p], see equation (3), which in conjunction
with Assumption 3.1 (ii) veriﬁes conditions of Theorem A.1 and shows that there exists C > 0
such that for every δ ∈(0, 1)
Online Appendix - 2
Let G∗= maxG∈G |G| be the size of the largest group in G. Note that the sg-LASSO penalty Ωis
a norm. By Lemma A.2.1, its dual norm satisﬁes
Ω∗(X⊤u/T) ≤α|X⊤u/T|∞+ (1 −α) max
G∈G |(X⊤u)G/T|2
≤(α + (1 −α)
G∗)|X⊤u/T|∞
≤(α + (1 −α)
where the ﬁrst inequality follows since |z|∗
1 = |z|∞and
∗= maxG∈G |zG|2, the second
by elementary computations, the third by equation (A.1) with probability at least 1−δ for every δ ∈
(0, 1), and the last from the deﬁnition of λ in Assumption 3.3, where c > 1 is as in Assumption 3.2.
By Fermat’s rule, the sg-LASSO satisﬁes
X⊤(Xˆβ −y)/T + λz∗= 0
for some z∗∈∂Ω(ˆβ), where ∂Ω(ˆβ) is the subdiﬀerential of b 7→Ω(b) at ˆβ. Taking the inner product
with β −ˆβ
⟨X⊤(y −Xˆβ), β −ˆβ⟩T = λ⟨z∗, β −ˆβ⟩≤λ
Ω(β) −Ω(ˆβ)
where the inequality follows from the deﬁnition of the subdiﬀerential.
Using y = m + u and
rearranging this inequality
∥X(ˆβ −β)∥2
Ω(β) −Ω(ˆβ)
≤⟨X⊤u, ˆβ −β⟩T + ⟨X⊤(m −Xβ), ˆβ −β⟩T
Ω(ˆβ −β) + ∥X(ˆβ −β)∥T∥m −Xβ∥T
≤c−1λΩ(ˆβ −β) + ∥X(ˆβ −β)∥T∥m −Xβ∥T.
where the second line follows by the dual norm inequality and the last by Ω∗(X⊤u/T) ≤λ/c as
shown in equation (A.2). Therefore,
T ≤c−1λΩ(∆) + ∥X∆∥T∥m −Xβ∥T + λ
Ω(β) −Ω(ˆβ)
≤(c−1 + 1)λΩ(∆) + ∥X∆∥T∥m −Xβ∥T
with ∆= ˆβ −β. Note that the sg-LASSO penalty can be decomposed as a sum of two seminorms
Ω(b) = Ω0(b) + Ω1(b), ∀b ∈Rp with
Ω0(b) = α|bS0|1 + (1 −α)
Ω1(b) = α|bSc
0|1 + (1 −α)
Online Appendix - 3
Note also that Ω1(β) = 0 and Ω1(ˆβ) = Ω1(∆). Then by the triangle inequality
Ω(β) −Ω(ˆβ) ≤Ω0(∆) −Ω1(∆).
If ∥m−Xβ∥T ≤2−1∥X∆∥T, then it follows from the ﬁrst inequality in equation (A.3) and equation
(A.4) that
T ≤2c−1λΩ(∆) + 2λ {Ω0(∆) −Ω1(∆)} .
Since the left side of this equation is positive, this shows that Ω1(∆) ≤c0Ω0(∆) with c0 = (c +
1)/(c −1), and whence ∆∈C(c0), cf., Assumption 3.2. Then
Ω(∆) ≤(1 + c0)Ω0(∆)
|S0||∆S0|2 + (1 −α)
≤(1 + c0)√sα
where we use the Jensen’s inequality, Assumption 3.2, and the deﬁnition of √sα. Next, note that
∆⊤Σ∆= ∥X∆∥2
T + ∆⊤(Σ −ˆΣ)∆
≤2(c−1 + 1)λΩ(∆) + Ω(∆)Ω∗
≤2(c−1 + 1)λΩ(∆) + Ω2(∆)G∗|vech(ˆΣ −Σ)|∞,
where the ﬁrst inequality follows from equation (A.3) and the dual norm inequality and the second
by Lemma A.2.1 and elementary computations
≤α|(ˆΣ −Σ)∆|∞+ (1 −α) max
[(ˆΣ −Σ)∆]G
≤α|∆|1|vech(ˆΣ −Σ)|∞+ (1 −α)
G∗|vech(ˆΣ −Σ)|∞|∆|1
≤G∗|vech(ˆΣ −Σ)|∞Ω(∆).
Combining the inequalities obtained in equations (A.5 and A.6)
Ω(∆) ≤(1 + c0)2γ−1sα
2(c−1 + 1)λ + G∗|vech(ˆΣ −Σ)|∞Ω(∆)
≤2(1 + c0)2γ−1sα(c−1 + 1)λ + (1 −A−1)Ω(∆),
Online Appendix - 4
where the second line holds on the event E ≜{|vech(ˆΣ−Σ)|∞≤γ/2G∗sα(1+2c0)2} with 1−A−1 =
(1 + c0)2/2(1 + 2c0)2 < 1. Therefore, inequalities in equation (A.3 and A.7) yield
γ (1 + c0)2(c−1 + 1)sαλ
γ (1 + c0)2(c−1 + 1)2sαλ2.
On the other hand, if ∥m −Xβ∥T > 2−1∥X∆∥T, then
T ≤4∥m −Xβ∥2
Therefore, on the event E we always have
T ≤C1sαλ2 + 4∥m −Xβ∥2
with C1 = 4Aγ−1(1 + c0)2(c−1 + 1)2. This proves the ﬁrst claim of Theorem 3.1 if we show that
Pr(Ec) ≤2p(p + 1)(c1T 1−µsµ
α + exp(−c2T/s2
α). To that end, by the Cauchy-Schwartz inequality
under Assumptions 3.1 (i)
1≤j≤k≤p ∥x0,jx0,k∥r/2 ≤max
j∈[p] ∥x0,j∥2
This in conjunction with Assumption 3.1 (ii) veriﬁes assumptions of Babii, Ghysels, and Striaukas
 , Theorem 3.1 and shows that
Pr(Ec) = Pr
2G∗sα(1 + 2c0)2
≤c1T 1−µsµ
αp(p + 1) + 2p(p + 1) exp
for some c1, c2 > 0 and B2
T = maxj,k∈[p]
l=1 |Cov(xt,jxt,k, xl,jxl,k)|. Lastly, under Assumption 3.1, by Babii, Ghysels, and Striaukas , Lemma A.1.2 B2
To prove the second claim of Theorem 3.1, suppose ﬁrst that ∆∈C(2c0). Then on the event E
Ω2(∆) = (Ω0(∆) + Ω1(∆))2
≤(1 + 2c0)2Ω2
≤(1 + 2c0)2∆⊤Σ∆sα/γ
= (1 + 2c0)2 n
T + ∆⊤(Σ −ˆΣ)∆
≤(1 + 2c0)2 n
C1sαλ2 + 4∥m −Xβ∥2
T + Ω2(∆)G∗|vech(ˆΣ −Σ)|∞
≤(1 + 2c0)2 
C1sαλ2 + 4∥m −Xβ∥2
Online Appendix - 5
where we use the inequality in equations (A.5, A.6, and A.8). Therefore,
Ω2(∆) ≤2(1 + 2c0)2 
C1sαλ2 + 4∥m −Xβ∥2
On the other hand, if ∆̸∈C(2c0), then ∆̸∈C(c0), which as we have already shown implies
∥m −Xβ∥T > 2−1∥X∆∥T. In conjunction with equations (A.3 and A.4), this shows that
0 ≤λc−1Ω(∆) + 2∥m −Xβ∥2
T + λ {Ω0(∆) −Ω1(∆)} ,
and whence
Ω1(∆) ≤c0Ω0(∆) +
λ(c −1)∥m −Xβ∥2
λ(c −1)∥m −Xβ∥2
This shows that
Ω(∆) ≤(1 + (2c0)−1)Ω1(∆)
≤(1 + (2c0)−1)
λ(c −1)∥m −Xβ∥2
Combining this with the inequality in equation (A.9), we obtain the second claim of Theorem 3.1.
The following result is proven in Babii, Ghysels, and Striaukas , see their Theorem 3.1.
Theorem A.1. Let (ξt)t∈Z be a centered stationary stochastic process in Rp such that (i) for some
ς > 2, maxj∈[p] ∥ξ0,j∥ς = O(1); (ii) for every j ∈[p], τ-mixing coeﬃcients of ξt,j satisfy τ (j)
for some constants c > 0 and a > (ς −1)/(ς −2). Then there exists C > 0 such that for every
with κ = ((a + 1)ς −1)/(a + ς −1).
ARDL-MIDAS: moments and τ-mixing coeﬃcients
The ARDL-MIDAS process (yt)t∈Z is deﬁned as
φ(L)yt = ξt,
where φ(L) = I −ρ1L −ρ2L2 −· · · −ρJLJ is a lag polynomial and ξt = Pp
j=0 xt,jγj + ut. The
process (yt)t∈Z is τ-mixing and has ﬁnite moments of order q > 1 as illustrated below.
Online Appendix - 6
Assumption A.3.1. Suppose that (ξt)t∈Z is a stationary process such that (i) ∥ξt∥q < ∞for some
q > 1; (ii) the β-mixing coeﬃcients satisfy βk ≤Cak for some a ∈(0, 1) and C > 0; and (iii)
φ(z) ̸= 0 for all z ∈C such that |z| ≤1.
Note that by Davydov , (ii) holds if (ξt)t∈Z is a geometrically ergodic Markov process
and that (iii) rules out the unit root process.
Proposition A.3.1. Under Assumption A.3.1, the ARDL-MIDAS process has moments of order
q > 1 and τ-mixing coeﬃcients τk ≤C(abk + ck) for some c ∈(0, 1), C > 0, and b = 1 −1/q.
Proof. Under (iii) we can invert the autoregressive lag polynomial and obtain
for some (ψj)∞
j=0 ∈ℓ1. Note that (yt)t∈Z has dependent innovations. Clearly, (yt)t∈Z is stationary
provided that (ξt)t∈Z is stationary, which is the case by the virtue of Assumption A.3.1. Next, since
and ∥ξ0∥q < ∞under (i), we verify that ∥yt∥q < ∞. Let (ξ′
t)t∈Z be a stationary process distributed
as (ξt)t∈Z and independent of (ξt)t≤0. Then by Dedecker and Prieur , Example 1, the τ-mixing
coeﬃcients of (yt)t∈Z satisfy
τk ≤∥ξ0 −ξ′
|ψj| + 2∥ξ0∥q
|ψj|β1−1/q
where (βk)k≥1 are β-mixing coeﬃcients of (ξt)t∈Z and the second line follows by H¨older’s inequality.
Brockwell and Davis , p.85 shows that there exist c ∈(0, 1) and K > 0 such that |ψj| ≤Kcj.
Therefore,
|ψj| = O(ck)
and under (ii)
|ψj|β1−1/q
cja(k−j)(q−1)/q ≤
CK ak(q−1)/q−ck
1−ca(1−q)/q
if c ̸= a(q−1)/q,
CKkak(q−1)/q
otherwise.
This proves the second statement.
Online Appendix - 7
Monte Carlo Simulations
FLOW STOCK MIDDLE LASSO-U LASSO-M SGL-M
FLOW STOCK MIDDLE LASSO-U LASSO-M SGL-M
Baseline scenario
εh ∼i.i.d. student-t(5)
High-frequency process: VAR(1)
Legendre degree L = 5
Legendre degree L = 10
Low frequency noise level σ2
Half high-frequency lags
Number of covariates p = 50
Baseline scenario, ρ = 0.7
Number of covariates p = 50, ρ = 0.7
Table A.1: Forecasting accuracy results. – See Table A.2
Online Appendix - 8
FLOW STOCK MIDDLE LASSO-U LASSO-M SGL-M
FLOW STOCK MIDDLE LASSO-U LASSO-M SGL-M
Baseline scenario
εh ∼i.i.d. student-t(5)
High-frequency process: VAR(1)
Legendre degree L = 5
Legendre degree L = 10
Low frequency noise level σ2
Half high-frequency lags
Number of covariates p = 50
Baseline scenario, ρ = 0.7
Number of covariates p = 50, ρ = 0.7
Table A.2: Nowcasting accuracy results.
The table reports simulation results for nowcasting accuracy. The baseline DGP (upper-left block) is with the low-frequency noise level
u = 1, the degree of Legendre polynomial L = 3, and Gaussian high-frequency noise. All remaining blocks report results for deviations
from the baseline DGP. In the upper-right block, the noise term of high-frequency covariates is student-t(5). Each block reports results
for LASSO-U-MIDAS (LASSO-U), LASSO-MIDAS (LASSO-M), and sg-LASSO-MIDAS (SGL-M) (the last three columns). We also
report results for aggregated predictive regressions with ﬂow aggregation (FLOW), stock aggregation (STOCK), and taking the middle
value (MIDDLE). We vary the sample size T from 50 to 200. Each entry in the odd row is the average mean squared forecast error,
while each even row is the simulation standard error.
Online Appendix - 9
LASSO-U LASSO-M SGL-M LASSO-U LASSO-M SGL-M LASSO-U LASSO-M SGL-M
Baseline scenario
Beta(1, 3)
Beta(2, 3)
Beta(2, 2)
εh ∼i.i.d. student-t(5)
Beta(1, 3)
Beta(2, 3)
Beta(2, 2)
high-frequency process: VAR(1)
Beta(1, 3)
Beta(2, 3)
Beta(2, 2)
Legendre degree L = 5
Beta(1, 3)
Beta(2, 3)
Beta(2, 2)
Baseline scenario, ρ = 0.7
Beta(1, 3)
Beta(2, 3)
Beta(2, 2)
Table A.3: Shape of weights estimation accuracy I.
The table reports results for shape of weights estimation accuracy for the ﬁrst four DGPs of Tables A.1-A.2 using LASSO-U, LASSO-M
and SGL-M estimators for the weight functions Beta(1, 3), Beta(2, 3), and Beta(2, 2) with sample size T = 50, 100 and 200. Entries in
odd rows are the average mean integrated squared error and in even rows the simulation standard error.
Online Appendix - 10
LASSO-U LASSO-M SGL-M LASSO-U LASSO-M SGL-M LASSO-U LASSO-M SGL-M
Legendre degree L = 10
Beta(1, 3)
Beta(2, 3)
Beta(2, 2)
low frequency noise level σ2
Beta(1, 3)
Beta(2, 3)
Beta(2, 2)
Half high-frequency lags
Beta(1, 3)
Beta(2, 3)
Beta(2, 2)
Number of covariates p = 50
Beta(1, 3)
Beta(2, 3)
Beta(2, 2)
Number of covariates p = 50, ρ = 0.7
Beta(1, 3)
Beta(2, 3)
Beta(2, 2)
Table A.4: Shape of weights estimation accuracy II. – See Table A.3
Online Appendix - 11
(a) LASSO-U-MIDAS
(b) LASSO-MIDAS
(c) sg-LASSO-MIDAS
(d) LASSO-U-MIDAS
(e) LASSO-MIDAS
(f) sg-LASSO-MIDAS
Figure A.1: The ﬁgure shows the ﬁtted Beta(1,3) weights. We plot the estimated weights for the LASSO-U-
MIDAS, LASSO-MIDAS, and sg-LASSO-MIDAS estimators for the baseline DGP scenario. The ﬁrst row plots
weights for the sample size T = 50, the second row plots weights for the sample size T = 200. The black solid line is
the median estimate of the weights function, the black dashed line is the population weight function, and the gray
area is the 90% conﬁdence interval.
(a) LASSO-U-MIDAS
(b) LASSO-MIDAS
(c) sg-LASSO-MIDAS
(d) LASSO-U-MIDAS
(e) LASSO-MIDAS
(f) sg-LASSO-MIDAS
Figure A.2: The ﬁgure shows the ﬁtted Beta(2,3) weights. We plot the estimated weights for the LASSO-U-
MIDAS, LASSO-MIDAS, and sg-LASSO-MIDAS estimators for the baseline DGP scenario. The ﬁrst row plots
weights for the sample size T = 50, the second row plots weights for the sample size T = 200. The black solid line is
the median estimate of the weights function, the black dashed line is the population weight function, and the gray
area is the 90% conﬁdence interval.
Online Appendix - 12
(a) LASSO-U-MIDAS
(b) LASSO-MIDAS
(c) sg-LASSO-MIDAS
(d) LASSO-U-MIDAS
(e) LASSO-MIDAS
(f) sg-LASSO-MIDAS
Figure A.3: The ﬁgure shows the ﬁtted Beta(2,2) weights. We plot the estimated weights for the LASSO-U-
MIDAS, LASSO-MIDAS, and sg-LASSO-MIDAS estimators for the baseline DGP scenario. The ﬁrst row plots
weights for the sample size T = 50, the second row plots weights for the sample size T = 200. The black solid line is
the median estimate of the weights function, the black dashed line is the population weight function, and the gray
area is the 90% conﬁdence interval.
Online Appendix - 13
Detailed description of data and models
The standard macro variables are collected from Haver Analytics and ALFRED databases. AL-
FRED is a public data source for real-time data made available by the Federal Serve Bank of St.
Louis; see the full list of the series with further details in Table A.5. For series that are collected
from the Haver Analytics database, we use as reported data, that is the ﬁrst release is used for
each data point. For the data that we collect from ALFRED, full data vintages are used. All
the data is real-time, hence publication delays for each series are taken into consideration and we
align each series accordingly. We use twelve monthly and four quarterly lags for each monthly and
quarterly series respectively and apply Legendre aggregation with polynomial degree set to three.
The groups are deﬁned as lags of each series.
On top of macro data, we add eight ﬁnancial series which are collected from FRED database;
the full list of the series appears in Table A.6.
These series are available in real time, hence
no publication delays are needed in this case. We use three monthly lags and apply Legendre
aggregation with polynomial degree set to two. As for macro, we group all lags of each series.
Lastly, we add textual analysis covariates which are available at 
The data is real time, i.e., topic models are estimated for each day and the monthly series are
obtained by aggregating daily data; see Bybee, Kelly, Manela, and Xiu for further details
on the data construction. We use categories of series are potentially closely tied with economic
activity, which are Banks, Economic Growth, Financial Markets, Government, Industry, International Aﬀairs, Labor/income, and Oil & Mining. In total, we add 76 news attention series; the
full list is available in Table A.7. Three lags are used and Legendre aggregation of degree two is
applied to each series. In this case, we group variables based on categories.
To make the comparison with the NY Fed nowcasts as close as possible, we use 15 years (60
quarters) of the data and use rolling window estimation. The ﬁrst nowcast is for the 2002 Q1
(ﬁrst quarter that NY Fed publishes its historic nowcasts) and the eﬀective sample size starts at
1988 Q1 (taking 15 years of data accounting for lags). We calculate predictions until the sample is
exhausted, which is 2017 Q2, the last date for which news attention data is available. Real GDP
growth rate data vintages are taken from ALFRED database. Some macro series start later than
1988 Q1, in which case we impute zero values. Lastly, we use four lags of real GDP growth rate in
all models.
Online Appendix - 14
Alternative estimators
We implemented the following alternative machine learning nowcasting
methods. The ﬁrst method is the PCA factor-augmented autoregression, where we estimate the
ﬁrst principal component of the data panel and use it together with four autoregressive lags. We
denote this model PCA-OLS. We then consider three alternative penalty functions for the same
linear model: ridge, LASSO, and Elastic Net. For these methods, we leave high-frequency lags
unrestricted, and thus we call these methods the unrestricted MIDAS (U-MIDAS). As for the
sg-LASSO-MIDAS model, we tune one- and two-dimensional regularization parameters via 5-fold
cross-validation.
ADP nonfarm private payroll employment
Level change (thousands)
Building permits
Level change (thousands)
Capacity utilization
Ppt. change
Civilian unemployment rate
Ppt. change
CPI-U: all items
MoM % change
CPI-U: all items less food and energy
MoM % change
Empire State Mfg. survey: general business conditions
Exports: goods and services
MoM % change
Export price index
MoM % change
Housing starts
MoM % change
Imports: goods and services
MoM % change
Import price index
MoM % change
Industrial production index
MoM % change
Inventories: Total business
MoM % change
ISM mfg.: PMI composite index
ISM mfg.: Prices index
ISM mfg.: Employment index
ISM nonmanufacturing: NMI composite index
JOLTS: Job openings: total
Level change (thousands)
Manufacturers new orders: durable goods
MoM % change
Manufacturing payrolls
Level change (thousands)
Manufacturers shipments: durable goods
MoM % change
Manufacturers inventories: durable goods
MoM % change
Manufacturers’ unﬁlled orders: total manufacturing
MoM % change
New single family houses sold
MoM % change
Nonfarm business sector: unit labor cost
QoQ % change (annual rate)
PCE less food and energy: chain price index
MoM % change
PCE: chain price index
MoM % change
Philly Fed Mfg. business outlook: current activity
Retail sales and food services
MoM % change
Real personal consumption expenditures
MoM % change
Real gross domestic income
QoQ % change (annual rate)
Real disposable personal income
MoM % change
Value of construction put in place
MoM % change
Table A.5: Data description table (macro data)– The Series column gives a time-series name, which is given in the second column
Source. The column Units denotes the data transformation applied to a time-series.
Online Appendix - 15
BAA less AAA corporate bond spread
BAA less 10-year bond spread
Log-returns %
TED spread
10-year less 3-month bond spread
Economic policy uncertainty index (EPUI)
Equity market-related economic uncertainty index (EMEUI)
Table A.6: Data description table (ﬁnancial and uncertainty series) – The Series column gives a time-series name, which is given
in the second column Source. The column Units denotes the data transformation applied to a time-series.
Online Appendix - 16
Bank loans
Credit ratings
Financial crisis
Nonperforming loans
Savings & loans
Economic Growth
Economic growth
Economic Growth
European sovereign debt
Economic Growth
Federal Reserve
Economic Growth
Macroeconomic data
Economic Growth
Economic Growth
Product prices
Economic Growth
Economic Growth
Record high
Financial Markets
Bear/bull market
Financial Markets
Bond yields
Financial Markets
Commodities
Financial Markets
Currencies/metals
Financial Markets
Exchanges/composites
Financial Markets
International exchanges
Financial Markets
Financial Markets
Options/VIX
Financial Markets
Share payouts
Financial Markets
Short sales
Financial Markets
Small caps
Financial Markets
Trading activity
Financial Markets
Treasury bonds
Government
Environment
Government
National security
Government
Political contributions
Government
Private/public sector
Government
Regulation
Government
Safety administrations
Government
State politics
Government
Government
Chemicals/paper
Competition
Credit cards
Foods/consumer goods
Luxury/beverages
Revenue growth
Small business
Soft drinks
Subsidiaries
Venture capital
International Aﬀairs
Canada/South Africa
International Aﬀairs
International Aﬀairs
France/Italy
International Aﬀairs
International Aﬀairs
International Aﬀairs
Latin America
International Aﬀairs
International Aﬀairs
Southeast Asia
International Aﬀairs
Trade agreements
International Aﬀairs
Online Appendix - 17
Labor/income
Executive pay
Labor/income
Labor/income
Government budgets
Labor/income
Health insurance
Labor/income
Labor/income
Labor/income
Labor/income
Oil & Mining
Agriculture
Oil & Mining
Oil & Mining
Oil & Mining
Oil drilling
Oil & Mining
Oil market
Oil & Mining
Table A.7: Data description table (textual data) – The Group column is a group name of individual textual analysis series which
appear in the column Series. Data is taken in levels.
Online Appendix - 18
Additional results
2-month horizon
sg-LASSO-MIDAS
1-month horizon
sg-LASSO-MIDAS
End-of-quarter
sg-LASSO-MIDAS
Table A.8:
Nowcast comparisons for models with macro data including series with short samples – Nowcast
horizons are 2- and 1-month ahead, as well as the end of the quarter. Column Rel-RMSE reports root mean squared
forecasts error relative to the AR(1) model. Column DM-stat-1 reports Diebold and Mariano test statistic of
all models relative to NY Fed nowcasts, while column DM-stat-2 reports the Diebold Mariano test statistic relative
to sg-LASSO-MIDAS model. Out-of-sample period: 2002 Q1 to 2017 Q2.
Online Appendix - 19
2-month horizon
sg-LASSO-MIDAS
1-month horizon
sg-LASSO-MIDAS
End-of-quarter
sg-LASSO-MIDAS
Table A.9:
Nowcast comparison table (excluding ﬁnancial data in Table A.6) – Nowcast horizons are 2- and
1-month ahead, as well as the end of the quarter. Column Rel-RMSE reports root mean squared forecasts error
relative to the AR(1) model. Column DM-stat-1 reports Diebold and Mariano test statistic of all models
relative to the NY FED nowcast. Out-of-sample period: 2002 Q1 to 2017 Q2.
Online Appendix - 20