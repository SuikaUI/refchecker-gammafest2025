SANDIA REPORT
SAND91--1144
Fast Parallel
Algorithms
for Short-
Steve Plimpton
Laboratories
Albuquerque,
and Livermore,
California
for the United
Oepartment
undor Contract
DE.ACO4-76DPOO789
D!81-RI_/=iJUTIONOF THIS DOCUMENT
i6 UNLIMITED
Laboratories,
Department
Corporation.
was prepared
as an account
of the United
Government.
the United
Government
employees,
contractors,
subcontractors,
employees,
or implied,
or assumes
or responsibility
the accuracy,
completeness,
usefulness
information,
apparatus,
disclosed,
or represents
to any specific
commercial
trademark,
manufacturer,
or otherwise,
necessarily
constitute
its endorsement,
recommendation,
or favoring
Government,
contractors
or subcontractors.
necessarily
or reflect
Government,
contractors.
of America.
reproduced
contractors
of Scientific
Information
to t'.m public
Information
US Department
of Commerce
Springfield,
Microfiche
Distribution
SAND91-1144
Algorithms
Short-Range
Department
Labort.tories
Albuquerque,
 
algorithms
for classical
presented.
of inter-atomic
to compute;
algorithms
are suitable
be difficult
to parallelize
efficiently
short-range
can be implemented
distributed-memory
message-passing
independently
processors.
algorithms
on a standard
Lennard-Jones
for system
500 to 10,000,000
supercomputers,
vectorized
generation
of parallel
is competitive
conventional
supercomputers
efficiencies
of 90% and
processor.
Trade--otis
algorithms
guidelines
for adapting
simulations
are also discussed.
work was partially
by the Applied
Mathematical
Sciences program,
U.S. Department
of Energy, Office of
and was performed
at Sandia National
Laboratori_,
for the DOE
under cotltract
No. DE-AC04-
76DP00789.
is a pre-print
of a paper
subnfitted
to a journal;
please contact
the author
if you would like the latest
up-to-date
version of the manuscript.
For exaxnple, when they become available,
Intel Paragon
mad Cray MPP
timings will likely be added
to the tables in Section 7.
BISTRtl_t.JTtON
16 UNLIMITEE)
Introduction
rnolecular
is a commonly
computational
tool for simulating
the properties
of liquids,
and molecules
Each of the
or molecules
in the simulation
is treated
are integrated
to compute
tile motion
of at.ores a variety
microscopic
macroscopic
inforrnation
be extracted
coefficients,
structural
or conformational
properties.
model is contained
in a potential
functional
for the system
from which
individual
force equations
are derived.
MD simulations
are typically
not memory
since only
information
are stored.
Computationally,
lhc simulations
.......the
timesteps.
for atomic
coordinates
is Angstroms;
dimensions
or millions of atoms
be sinmlated
to approach
even tile microscopic
In liquids
and solids
size is constrained
vibrational
be accurately
This limits
to the femtosecond
scale and so tens
or hundreds
of thousands
of timesteps
are necessary
to sirnulate
even picoseconds
con_putational
considerable
by researchers
to optimize
calculations
for vector
SUl_ercomputers
 and even to build
special---purpose
for performing
silnulations
 . The
state-of--the-art
simulating
tento hundred--thousand
for picoseconds
hours of CPU
time on machines
as the Cray
MD computations
are inherently
extensively
literature
 . There
considerable
effort, in the
last few years
by researchers
to exploit
this parallelism
on various
of the work
has inchlded
implementations
of proposed
algorithms
has been for single---instruction/multiple-data
 , or for
multiple-instruction/nmltiple-data
with a few dozens
of processors
 .
More recently
been efforts
mt creating
algorithms
well on hundred-.to
thousand-processor
 . We are convinced
programnfing
(or the single-program/multiple-data
as it is sometimes
is the only one that
flexibility
to implement
all the data
and computational
enhancements
are conmmnly
in MD codes
it, is only
generation
of massively
to thousands
of processors
computational
to be competitive
with the fastest, vector
for Ml) calculations.
In this paper we present
algorithnls
which are appropriate
for a general
class of M D problems
has two salient
characteristics.
first characteristic
forces are limited
are geometrically
and liquids
this way due to electronic
to avoid the computational
cost of including
long-range
For short-range
MD tlm computational
the number
algorithms
take full advantage
of the local
of the forces.
The second
characteristic
displacements
over the duration
simulation.
to diffusion
in a solid or liquid
or conformational
in a biological
from a computational
standpoint
the simulation
progresses.
While the algorithms
we discuss
could also be used for fixed-neighbor
simulations
on lattice
in a solid),
it, is a harder
to continually
the neighbors
and maintain
for the overall
computation
on a parallel
Our first goal in this effort, was to develop
algorithms
would be competitive
with the fastest
supercomputers
algorithms
to work well
on problems
for large problems
parallelism
is often easier to
is because
of MD simulations
are performed
on systems
to several
where N is chosen
to be as small as possible
while still accurate
the desired
etl\_cts . The
computational
goal in these
calculations
is to perform
as quickly
as possible.
is particularly
in non--equilibrium
macroscopic
in tlle system
n-lay take significant
time to evolve, requiring
of timesteps
to be able to perform
simulation
of a 1000 atom system
fast rather
1000 timestel)s
of a 100,000
the computational
To this end, we consider
sizes a.ssmall
as a few hundred
in this paper.
For very large
MD problelns,
our second
goal in this work was to develop
aigorithnls
be scalable
to large.r and
Inacllines.
the timings
we present;
for large MI) models (10 s
to 10r atoms)
generation
ot" parallel
supercomputers
to thousands
of processors)
fast compared
t,o vector
supercomputers,
are still too slow to allow long-timescale
simulations
to be done
routinely,
large-system
algorithnl
to N and P (the
of processors)
a.s parallel
in the next few years,
algorithms
to it will enable
to be studied.
Our earlier
area produced
algorit, hnls which
were fast for systems
up to tens of thousands
optitnally
for larger
a scalable
large-system
 we have
an idea of Tamayo
and Giles 
t,hat has improved
the algoritllm's
performance
on medium-sized
by reducing
the inter-processor
communication
requirements.
We have also recently
a new parallel
which we present
here in the context
of Ml) simulations
for the first time.
lt offers the advantages
of both simplicity
to medium-sized
we present
culmination
algorithms
we have found,
implementing
on different
to be the fastest
short-range
a wide range
of problem
By implementing
the algorithms
with hundreds
to thousands
of processors,
to understand
ill practical
algorithmic
and tailor
tile algorithms
accordingly
to opt,imize their
performance
a function
Due to their
scalability,
also predict,
algorithms
will perform
the faster,
larger parallel
of tile filture.
In tile next section,
the computational
of MD are highlighted
and efforts
the calculations
on vector and parallel machines
are reviewed.
In Sections
3, 4, and 5 we describe
algorithms
in detail.
A standard
Lennard-Jones
calculation
is outlined
in Section
In Section
7, implementation
and tinting
for the parallel
algorithms
MIM D machines
are given and comparisons
to the best
and C90 limings
for the benchmark
calculation.
Discussion
of the scaling
properties
of the algorithms
is also included.
Next,, in Section
using the parallel
algorithms
in difiZrent
of MD simulations
are discussed.
in Section
draw conclusions
and give several
guidelines
for deciding
is likely to be fastest
a particular
short-range
MD simulation.
Computational
The computational
task in a Ml) simulation
is to integrate
the set, of coupled
differential
equations)
_-_ F2(7_, _'j ) + _-_ _-_ Fa( ,'i, ,'j , ,'k ) -t- . . .
where mi is the mass of atom i, 7i and Fi are its position
and velocity
F2 is a force function
describing
interactions
Fa describes
three-body
interactions,
and many-body
interactions
force terms
are derivatives
expressions
the energy
i is typically
as a function
of only the positions
of itself and other
In practice,
only one or a few terms
(1) are kept
F2, b)_, etc. are constructed
so as to include
and quantum
the extent
the at)proximations
are accurate
give a filll descril)tion
of the tinm-evolution
the system.
Thus, the great
computational
of classical
M D, as compared
to ab initio
electronic
calculations,
the dynamic
of the atomic
is described
empirically
to solve Schrodinger's
in equation
(1) are typically
non-linear
long-range
or short-range
in nature.
For long-range
a.s Coulombic
interactions
in an ionic solid
or biological
with ali others.
these forces scales as N "2and is too costly
for large N.
approximate
this difficulty.
particle-mesh
algorithms
 which scale as f(M)N
is the number
hierarchical
 which scale as N log(N),
and fast-,nultipole
 which scale as N.
implementations
algorithms
 have improved
of applicability
for manybody
simulations,
long-range
force models
in classical
M D simulations.
By contrast,
short-range
extensively
in M D and
we are concerned
are chosen
electronic
effectively
of influence
interatomic
to truncate
long-range
interactions
and lessen
computational
Ira either
summations
in equation
are restricted
region surrounding
is typically
implemented
rc, outside
ali interactions
are ignored.
The work to coml,ut.e
forces now scales
even with this
the vast majority
of computation
time spent
in a short-range
force MD simulation
is in evaluating
force terms
in equation
time integration
only 2-3% of tile total
To evaluate
efficiently
are within
the cutoff
rC at every
key' is to minimize
of neighboring
be checked
for possible
interactions
calculations
ol_ neighbors
at, a distance
rc. are wasted
coml)utation.
are two basic
techniques
to accomplish
and vector
we discuss
here since
algorithms
incorporate
first, idea,
of neighbor
was originally
maintained
Typically,
tile list is formed,
ali neighboring
an extended
r, = rc + 6 are stored.
list is tlsed for a few timesteps
to calculate
ali force interactions.
it is rebuilt,
have moved
from a distance
r > r., to r < rC. Though
6 is always
to be small
to rC, an optimal
value depends
on the parameters
temperature,
diffusivity,
of the particular
simulation.
of the neighbor
list is that
once it is built,
it for possible
interactions
is much faster
in tile system.
The second
used for speeding
up MD calculations
as tile link-cell
ali the atoms
are binned
cells of side length
d = rC or slightly
This reduces
the t.ask of finding
of a given atom
to checking
in 27 bins -- the bin the atom
is in and the
26 surrounding
only requires
associated
with it is acceptable
for the savings
of only having
a local region
for neighbors.
The fastest
MD algorithms
and vector
use a combination
of neighbor
lists and link-cell
In the combined
are only binned
once every few timesteps
for the purpose
of forming
In this c_se atoms
are binned
into cells of size d > r,.
At intermediate
the neighbor
list.s alone are used irl tire usual
way to find neighbors
a distance
rC of each atom.
This is a significant
over a conventional
are far fewer atoms
to check in a sphere
in a cube of volume
27rc a. Additional
can be gained
due to Newton's
3td law by only
a force oP,ce for each pair of atones (rather
than once for each
in the pair).
In the combined
this is done by only searching
half the surrounding
bins of each atom
to form its neighbor
has the effect of storing
atom j in atom
i's list, but
i iii atorn
list,, thus
tile number
force computations
that. must
are simply
described,
performance
oil a vector
to data structures
and loop constructs
to insure complete
vectorization.
The fastest
implementation
in the literature
 . They
use the combined
list/link-cell
long lists
of neighboring
keep only those
the cutoff
rc. Finally,
the lists into packets
in which no
atom appears
 . The force computation
for each packet
be completely
vectorized,
in performance
on the benchmark
in Section
is from 2 to 10 times
vectorized
algorithms
 over a wide range
of simulation
there has been considerable
in devising
MD algorithms.
Tile natural
parallelism
in MD is that
the force calculations
and velocity/position
can be done simultaneously
for ali atoms.
to achieve
parallelism.
is to divide
force computations
in equation
(1) evenly
the processors
so as to extract,
pa.rallelism.
our knowledge,
all algorithms
have been proposed
or implemented
(including
have been variations
two methods.
References
 include
good overviews
of various
techniques.
In the first
class of methods
a pre-determined
set of force
computations
is assigned
processor.
The assignment
fixed for the duration
of the simulation.
The simplest
way of doing
this is to give a
to each processor.
We call this method
an atom-decomposition
of the workload,
forces on its atoms
they move in the siinulation
More generally,
of tile force loops
in equation
(1) can be assigned
to each processor.
this a forcedecomposition
arid describe
a new algorithm
of this type later
in the paper.
decompositions
to Lagrangian
in a fluids simulations
the grid cells (computational
with the fluid (atoms
By contrast,
in the second
class of methods,
which we call a spatialdecornpos*tion of the workload,
each processor
is assigned
of the physical
sinmlation
the forces on at,ores
irt its sub-domain.
As the sinlulation
progresses
processors
move from one sub-domain
to another.
is analogous
to an Eulerian
for a fluids simulation
the grid remains
as fluid moves
the two classes
of methods
for parallelization
of MD, a variety
of algorithms
have been proposed
and implemented
by various
researchers.
The details
of the algorithms
vary widely from one parallel
to another
since there
are numerous
problem-dependent
and machine-dependent
trade-offs
to consider,
as the relative
of computation
and communication.
A brief review
of some notable
Atom-decomposition
also called
replicated-data
 because
information
are replicated
ali processors,
used in MD simulations
of molecular
the duplication
of information
makes for straight-forward
computation
of additional
threeand fourbody
force terms.
implementations
of state-of-the-art
biological
MD programs
as CItARMM
and GROMOS
are discussed
in . Force--decomposition
which systolically cycle
or through
of processors
 . Other
force-decomposition
use tile force-matrix
in Sections
3 and .1 have been
in and
 . Boyer and
 block--decompose
the force matrix
in a manner
in Section
4, while the method
of Brunet,
et. al. 
partitions
the matrix
by element.
of tile methods
are designed
for long-range
force systems
calculations
(no neighbor
lists) oil SIMD
tile scaling
algorithms
is different
is presented
in Section
4 as is the way they
distribute
among processors
and perform
inter-processor
communication.
Spatial-decomposition
also called
 , are more common
in the literature
are well-suited
large MD simulations.
implementations
 , CM--5 , and
 have some features
spatial-decomposition
we present
in Section
algorithms
SIMD machines
also employ
spatial-decomposition
techniques
 : However,
the SIMD programming
processors
to operate
simultaneously
on a global
structure,
introduces
inefficiencies
in short-range
MD algorithms,
particularly
when coding
the construction
and access
of variable-length
lists via indirect
addressing.
the timings
in for the benchmark
in Section
6 on a 32K--processor
single-processor
in Section
7. By contrast,
the timings
for the MIMD
algorithms
in this paper and
references
 are considerably
indicating
the advantage
capability
offers for exploiting
parallelism
in short-range
MD simulations.
Atom-Decomposition
In our first
each of the
P processors
is assigned
at the beginning
of the simulation.
in a group
have any special
relationship
of exposition,
N is a multiple
it is simple
this constraint.
A processor
forces on only its N/P
and will update
and velocities
for the duration
simulation
move in the physical
As discussed
in the previous
atom-decompositiou
of the computational
A useful construct
for representing
the computational
work involved
in the algorithm
is the N x N force
of F represents
the force on atom
i due to atom
F is sparse
to short-range
skew-symmetric,
due to Newton's
We also define x
f as vectors
the position
force on each
For a 3-D simulation,
would store
coordinates
of at.ore i.
definitions,
atoln-decomposition
a sub-block
of F which consists
This is shown
1 where we let
the z subscript
the processor
from 0 to P-
P_ computes
in the Fz block of rows.
lt also is assigned
the corresponding
sub-vectors
1: The division of the force matrix
processors
ill tile atom-decomposition
algorithm.
z is assigned
a group of N/P
rows of the matrix
and corresponding
of the position
and force vectors,
tile computation
Fij requires
relax this assumption
in Section
To compute
ali the elements
in F_, processor
P_ will need the positions
processors.
atom-decomposition
algorithm,
is accomplished
each processor
its updated
to ali the other
processors
an operation
called all-to-ali
communication.
algorithms
have been developed
for performing
efficiently
on different
architectures
We use an idea due
et. al. that
is simple,
and works well on a variety
of machines.
We describe
it here because
is the chief communication
the atom-decomposition
algorithms
force-decomposition
algorithms
in the next
nomenclature,
all-to-ali
communication
operation.
Each processor
N to store
the entire
At the beginning
of the expand,
P, has z,,
an updated
piece of z of length
to acquire
ali the other
processor's
in its copy
2 illustrates
accomplish
this for an 8 processor
processors
are mapped
consecutively
to the sub-pieces
the vector.
In the first communication
with an adjacent
in the vector.
2 exchanges
Now, every
has a contiguous
piece of x that
is of length
Iii the second
this piece with
a processor
two positions
away (2 exchanges
a 4N/P.-length
piece of x.
In the last
an N/2-1ength
piece of z with
a processor
away (2 exchanges
with 6); the entire
now resides on each
processor.
1 liiiii ill
314151617 [
415 1617 [
[!i_i!i[!ii!i!i!!
fill_!I }_ili]4,1 51
2: An ezpand
8 processors.
2 exchanges
successively
longer sub-vectors
processors
3, 0, and 6.
A pseudo-code
is given in Figure
For simplicity
we again assume
power-of-two
of processors;
assumption
is straightforward.
At each step
P_ performs
with a partner
The new processor
P_ is obt.ained
by flipping
one bit in z, which itself is a string
of log2(P ) bits.
The sub-vector
to pc and the received
sub-vector
w is concatenated
with Y (the
"l" operation)
in the proper
at every step;
at the end of the expand
V has become
the filll N-length
for a communication
are typically
quantified
by the number
of messages
and the total
is optimal;
and receives
and exchanges
is the reason
the expand
works well
A drawback
it requires
processor.
Alternative
for performing
all-to-ali
communication
less storage
at the cost of more
sends and receives.
for MD simulations
as we shall see,
large problems
with an atom-decomposition
in the many Mbytes
of local memory
on current-generation
processors.
0,...,log2(P
k th bit of z flipped
y to processor
w from processor
IF bit k of z is 0 THEN
for processor
A communication
is essentially
atomand force-decomposition
algorithms.
each processor
has stored
values throughout
its copy of the force vector
to know the
values in fz, where
each of the values is
across ali P processors.
A procedure
for doing this is known
as a ft ld operation
 and is outlined
in log2(P ) steps.
y represents
force vector
two pieces,
One of the pieces
to a partner
The received
sub-vector
w is summed
by element
the retained
This summed
sub-vector
y is halving
the fold is
y has become
with values summed
ali P processors.
Like the expand,
the fold operation
log2(P ) sends
and receives
to be exchanged
processor.
Additionally
operations
to do the summations,
defined the expand
and fold operations,
we now present
two versions
of the atom-decomposition
algorithm.
is simpler
of Newton's
We call this
it is outlined
dominating
computation
or communication
_t_ep listed
of the timestep
of ali N atoms,
i.e. each has a copy of the entire
(1) of the algorithm
lists for ali the pairwise
interactions
be computed
will only be dorle once every few timesteps.
If the ratio
of the physical
D to the extended
force cutoff length
rs is relatively
it is quicker
for Pz to construct
the lists by checking
in its F, block.
the simulation
is large enough
bins can be created
in each dimension,
it is quicker
to bin ali N atoms,
27 surrounding
bins of each of its N/P
to form the lists.
This checking
has a large coefficient,
so the overall
list construction
is recorded
k = log_(P)
u := top half of y vector
v := bottom
half of y vector
:= Ps with k th bit of z flipped
bit k of z is 0 THEN
v to processor
u to processor
fold operation
for processor
(2) of the algorithm,
the neighbor
lists are used
to compute
the non-zero
As each pairwise
force interaction
is computed,
the force components
are summed
into fz, so that
Fz is never
as a matrix.
At. the completion
of the step,
each processor
fs on each
of its N/P
and velocities
(3) will be added
algorithms
in this and the following sections.)
(5) the updated
are shared
ali P processors
in preparation
for the next
via the expand
3. As discussed
this operation
a.s N, the volume
in the position
lists of non-zero
il;l.eractions
(D < 4r8) Ali pairs
(D >_ 4r,)
ali processors,
5" Single timestep
of atom-decomposition
A1 for processor
As mentioned
If different
processors
j as is usually
processors
tile (i j)
interaction
tile resulting
be avoided
at the cost
communication
a modified
references
each pairwise
interaction
only once.
are several
ways to do this by striping
the force matrix
 ; we choose instead
to form G as follows.
Let Gi.i = Fij,
Gij = 0 when
even, and likewise
Gij = 0 when
i < j and i + j is odd.
Conceptually,
G is colored
like a checkerboard
red squares
the diagonal
set to zero and black squares
below the diagonal
also set to zero.
A modified
atom-decomposition
uses G to take advantage
of Newton's
law is outlined
lists of non-zero
interactions
into local copy of f
ali processors,
ali processors,
6: Single tirnestep
of atom-decomposition
A2 for processor
which takes
Step (1) is the same
as in algorithm
only half as many
list entries
Gs has only half the non-zero
This is reflected
in the factors-of-two
in the scaling
For neighbor
lists formed
by binning,
bin ali N atoms,
only need check half the surrounding
bins of each of its NIP
(2) the neighbor
lists are used
to compute
For an interaction
i and j, the resulting
forces on atoms
are summed
the i and j locations
of force vector
each processor
of the entire
force vector,
as opposed
f_ as in algorithm
ali the matrix
ali P processors
ends up with fz,
forces on its atoms.
(4) and (5) then
implementing
3rd law essentially
the computational
cost in steps
(1) :Lhd (2),
at the expense
of doubling
the communication
are now two communication
(3) and (5),
each of which scale as N.
will only be a net gain
if the communication
is less than
of the overall run time.
As we shall see,
this will usually
case on large numbers
of processors,
in practice
always choose
A1 instead
for an atom-decomposition
algorithm.
P or expensive
force models,
A2 can be faster.
we discuss
the issue of load-balance.
Each processor
will has an equal
of work if each
Fz or Gz block has roughly
of non-zero
This will be the case if the atom density
is uniform
the simulation
non-uniform
can arise if, for example,
free surfaces
so that. some atoms
on vacuum,
are occurring
a liquid or solid.
This is only a problem
for load-balancing
of the atom-decomposition
computation
across processors
are ordered
in a geometric
as is typically
near a surface,
for example,
in the interior.
by randomly
the at.ore ordering
at the beginning
of the simulation,
which is equivalent
to permuting
of F or G. This
will have roughly
the same number
of non.-zeros
is non-uniform.
permutation
also has the advantage
the load-balance
will likely persist
as atoms move about
the simulation.
this permutation
need only be done
as a pre-processing
the dynamics.
In summary,
the atom-decomposition
algorithms
the MD force computation
and integration
the processors
list construction
which is usually
significant).
algorithms
communication,
information
held by ali the other
processors.
communication
scales as N, independent
of P, so it limits
of processors
effectively.
chief advantage
of the algorithms
simplicity.
(1), (2), and (4) can be implemented
the loops and data structures
code to treat
and fold communication
operations
be treated
as black-box
and inserted
at the proper
(3) and (5).
are typically
to parallelize
an existing
Force-Decomposition
Our next parallel
MD algorithm
on a block-decomposition
of the force matrix
a rowwise decomposition
as used in the previous
this a force-decomposition
of the workload.
we shall see, this improves
of the communication
cost to O(N/x/-P).
Block--decompositions
of matrices
are common
algorithms
 for parallel
which sparked
our interest
the idea, but
to our knowledge
we are the first to apply
this idea to short-range
MD simulations
 .
assignment
of sub-blocks
of F to processors
is depicted
7. We assume
for ease of exposition
P is an even
of 2 and that
is a multiple
it is straightforward
constraints.
The block owned by each
is thus of size (N/x/r'fi)
x (N/v/ft).
We use the Greek
subscripts
the row and column
of F running
1. A sub-block
as FoZ, and the processor
it is P_Z.
We note that
a and _ also index
sub-vectors
f of length
2'o compute
the matrix
know the xa and x_ pieces
are computed
will be stored
in local copies of the force sub-vectors,
Figure 7: The division of the force matrix
proce3sors
in the force-decomposition
algorithm.
Pac is assigned a sub-block
Foe of size N/v/-fi
by N/_,/'fi.
the corresponding
pieces of the position
and force vectors.
In addition
to computing
the matrix
will be responsible
for updating
the p_itions
and velocities
atom-decomposkion
algorithm.
sub-vector
v/-fi processors
in row c_ divide
is responsible
a contiguous
piece of length
the column
processor,
each processor's
piece with
a superscript,
zZ_. Similarly,
force acting
N/P-length
sub-vector
atom-decomposition
an element
is the sum
of ali the
the corresponding
Our first force-decomposition
F1 is outlined
8. As before,
each processor
has updated
copies of the needed
atom positions
at the beginning
of the timestep.
In this case it is the current
sub-vectors
azo and xe.
(1) neighbor
are constructed.
be checking
For large problems,
in x e are binned,
27 surrounding
bins of each
is checked.
of interactions
processor 's lists is still O(N/P).
The scaling
of the binned
list construction
(2) the neighbor
lists are used
to compute
the nlatrix
the elements
into a local copy
are computed,
so FoB need
(3) a fold operation
is performed
each row of processors
Pa_ obtains
on its N/P
the fold algorithnl
as in tile preceding
is a key difference.
case the vector
is only of length
and only the v_
processors
in one row are participating
in tile fold. Thus
this operation
of N as in
the atom-decomposition
communication
(4), f_ is used by P_e to update
atom positions
these updated
with ali the processors
will need them
for the next
are the processors
a row or column
First,, in (5a),
the processors
in row a perform
x_ subvectors
each acquires
the entire
As with the fold, this operation
as the N/x/_
of as N as it, did in algorithms
A1 and A2.
its updated
with processor
which owns the block of F in the
of the matrix.
of this operation
as the N/P
of the data
being exchanged.
the processors
of the received
sub-vector
As a result, they
ali acquire
xa and are
lists of non-zero
interactions
< 4rs) Ali pairs
of Fc,_ storing
row a, result
in z_ using
(5a) Expand
row a, result
(5b) Exchange
with transpose
x_a to PZ,_
x/_ from PZt,
(5c) Expand
8: Single timestep
of force-decomposition
for processor
As with algorithm
take advantage
of Newton's
3rd law; each
interaction
is computed
this duplicated
effort by using the same
checkerboarded
was defined
in the preceding
now the total
force on atom
i is the sum of
ali matrix
in row i minus
the sum of ali elements
force-decomposition
is outlined
(1) is the same
as in F1, except
half as many interactions
are stored
in the neighbor
(2) requires
only half as many matrix
be computed.
the computed
components
are now summed
Tire force on atom
i is summed
into f,_ in _e
corresponding
to row i. The same force on atom
j is also summed
into f_ in tile location
corresponding
accumulate
P,_ ends up with the total
force on its N/P
processors
fl fold their
local copies of f_.
Each element
of this N/P-length
sub-vector
the sum of an entire
(3t)) this sub--vector
is exchanged
transpose-position
in the sub-vector
column contribution
to the forces on its N/P
the row contributions
to the forces
are summed
by performing
a fold of the
row oc. The
which is the sum across
a row of G. Finally,
(3d) the column
and row contributions
are subtracted
elemev.t by element
to yield tile total
oil the atoms
by processor
now update
the positions
and velocities
of its atoms;
4 and 5 are identical
lists of non-zero
interactions
(D < 4r_) Ali pairs
(D >_4v_) Binning
in local copies of f_ and f_
/3, result
(3b) Exchange
forces with
row (_, result
(3d) Subtract
from folded
(5a) Expand
row _, result
(5b) Exchange
(5c) Expand
/3, result
9: Single timestep
of force-decomposition
for processor
In the force-decomposition
algorithms,
exploiting
3rd law again halves tile computation
in steps (1) and (2). However,
the communication
cost in steps
(3) and (5) does not double.
and folds required
in F2 versus
are also two transpose
operations
in practice,
it is usually
to use algorithm
with its reduced
computational
and slightly
communication
ali the expand
and fold operations
F1 and F2 scale as N/x/-fi
as N as was the case in algorithms
A.1 and A2.
As we shall see, when
run o:l large
of processors
this significantly
the force-decomposition
algorithms
on communication
as compared
to the atona-decomposition
algorithms.
of load-balance
force-decomposition
algorithms.
Processors
to do only if ali the
F_,t_ or G,_
are uniformly
are ordered
geometrically
this will not be the case even for problems
with uniform
is because
an ordering
of non-zero
atom-decomposition
permutation
of the atom
the desired
now the permutation
be done as a pre-processing
step for ali problems,
even those
with uniform
densities.
In summary,
algorithms
MD computations
processors
t_s did the
atom-decomposition
algorithms.
block-decomposition
of the force
only needs
O(N/x/'-fi)
information
to perform
its computations.
the communication
and memory
are reduced
by a factor
of v/-fi versus
algorithnLs
A1 and A2.
The force-decomposition
simplicity
of the atom-deconlposition
technique;
be implemented
"blackbox"
communication
force-decomposition
algorithms
no geometric
information
t.o perform
optimally.
for load-balancing
the algoritllms
intentionally
ignore such
illformation
by using a random
at.ore ordering.
Spatial-Decomposition
In our final parallel
the physical
simulation
is subdivided
into small
processor.
We call this a spatial-decomposition
and updates
the positions
velocities
of ali atoms
its box at each timestep.
are reassigned
to new processors
move through
the physical
to compute
forces on its atoms,
need only know
communication
in the spatialdecomposition
is thus local in nature
as compared
in the atom-and
force-decoml)osition
will depend
the physical
to be a 3-D
rectangular
parallelepiped.
constraints
of processors
so as to make
processor's
is; to mininfize
communication
communication
spatial-decomposition
proportional
in contrast
to the link-cell
for conventional
MD described
in Section
2, the box lengths
may now be smaller
the force cutoff
rc and rs.
in our spatial-decomposition
structures,
one for the NIP
ill its box and one for atoms
III tile first data structure,
each processor
information
--positions,
velocities,
in a linked
list to allow insertions
move to new boxes.
In the second
only atonl
are stored.
Interprocessor
communication
this information
The communication
we use to acquire
this izlformation
from processors
the nearby
10. The first
(a) is for each
t,o pair up with
an adjacent
dimension,
1 for exaznple.
2 fills a message
are within
7"_of processor
r_ instead
will be made
clear below.)
If d < 7"_,where d is the box length
in the east/west
direction,
will be ali of
2's atoms;
it. will be those
to box 1. Now pro_essors
2 alld 1 exchange
|nessages.
information
ii, receives
into its second
structure.
the processors
in the opposite
east--west
direction,
3 in this case,
operation.
If d > r_, ali
atom positions
in the east,-west
have now been
processor.
If d < v,, this
is repeated
more needed
atom positions
to its adjacent
processors.
from box 3 (which
2 now has in its se.cond
sl.ructure).
be repeated
each processor
kllows ali atom
a distance
its box, as indicated
by the dotted
boxes in the figure.
is now repeated
in the. north/south
dilnension;
(b) of the figure.
The only difference
to the adjacent
not only atoms
l]le processor
owns (in its lirsl
structure),
are needed
by the adjacel_t, processor.
7_this has the effect _,fsending
boxes worth of atom
iii one message
iii (I)). Finally,
iii sl.el) (c) the l)rocess
is repeated
the up/down
dimension.
Now alolll
frolll ali elltire
pla.zle of boxes
(9 in the tigtire)
are effectively
being exchanged
in cacti message.
are se.veral key advantages
to this schmne,
ali of which reduce
the overall cost of conmlu|lication
our algorilhln.
for d >_ v,,
l_osil.ions from ali 26 surrounding
boxes are. obtained
exchanges.
Moreow_r,a.s
will be discussed
in the results
macllineis
a. hypercube,
processors
a way tllat
ali 6 of these
processors
will be directly
to the center
processor.
will be fast and contentionfree. l']ven if d < 7:,so
atom informalion
from more. distant
this occurs
with only a few extra
exchanges,
all of which are still with the 6imnlediateneighborprocessors.
the. amount
of dataconlinunicated
mininlized.
Each processor
only the al,ot_! positions
are within
,_ distance
r., of its box.
ali of the received
a.s contiguous
into the processor's
structure.
rearranging
the buffered
to be sent.
as will be discussed
in more. detail
this message
w'_ry quickly.
scan of the two data
structures
is only done once every Dw timesteps,
when the neighbor
lists are created,
which a_,om positions
a list of atoins
east/west exchanges
north/south
up/down exchanges
10: Method
by which a processor
in the spatial-decomposition
algorithm.
ali at,on l positions
in adjacent
in the (a) east/west,
(b) north/south,
(c) up/down
directions
can be communicated.
ali the other
timesteps,
the lists can be used,
in lieu of scanning
the full atom list,
to directly
index the referenced
and buffer up the messages
is the equivalent
of a gather
operation.
We now outline
our spatial
decomposition
$1 in Figure
11. Box z is assigned
to processor
1 ms before.
of its NIP
forces on those
are the neighbor
list, construction,
once every
few timesteps.
is somewhat
in the other
algorithms
as discussed
it illcludes
will be communicated
tinlestep.
the positions,
velocities,
and any other
identifying
information
are no longer inside
from xz and stored
in a lnessage
atones are exchanged
the 6 adjacent
processors
via the communication
10. As the information
each dimension,
for new atoms
are now inside
its box boundaries,
a distance
r_ of box z are acquired
by the communication
As the different,
are buffered
by scanning
the two data
structures,
lists of included
lists will be used in step
The scaling
A for steps
will be explained
are complete,
of the processor's
structures
are current.
to new boxes
lists of ali atoms
will need to be exchanged
lists of interaction
(d _>2r_) Binning
(2) Compute
forces on atoms
iii I)OXz, douMv, storing
(4) Update
in box z using f_
(5) Exehange
box boundaries
with neighboring
processors
-J.,-(1+ 2r_/d) a
(d < r_) Send N/P
to many neiglll)ors
r.,) Send NIP
to nearest
neigilbors
Send l)ositions
box stlrface
to nearest
neighi)ors
11" Single timestel)
of spatial-decolnposition
algorithzn
for i)rocessor
lists for its N/P
can now be constructed
in step (lc).
i and j are both
in box z (an inner- box
interaction),
pair is only stored
once in the height)or
If i and j arc in dilferent
boxes (a two--box
interaction),
processors
the interaction
respective
If l.his were not
processors
forces on atoms
do not own and coz_ununication
back to tlm
processors
the atolns
be required.
A modified
algoritlllll
which l)erforlils
this conmlunication
to avoid tile duplicated
force computation
of two-box
interactions
is discussed
d, the length
of box z, is less than
two cutoff distances,
it is quicker
to lind neighbor
illteractions
by checking
box z against
ali the atol_ls in both
of the. processor's
structures.
as the square
If d > 2rs,
tile shell of atoms
box z, there
arc 4 or lllore bins
in each (liJ_iension.
In this case,
as with the other
algorithms,
it is quicker
to perform
the Imighl)or
list construction
by bitching.
Ali the atorns
structures
are llashed
into bins of size r,.
The surrounding
bins of each
box z are then
for possible
neighbors.
t_ call now compute
on its atoms
neigl_l)or
interaction
is between
two atolns
box z, the
in f_, once for atom
i mid once
For two--I)ox
interactions,
force on the
proeessor's
is stored.
l)ositions
are ul)dated
comnmnicated
to the surrounding
processors
in preparation
for the next
in st(q) (5)
the previously
lists to create
and the communication
in this operation
is a fianction
of the relative
of the force
and box length
is discussed
paragral)h.
Also, we note
ti|nestel)s
lists are constructed,
not have to I)e performed
since step
h_s the santa
communication
operations
in algorithm
communication
in tile latter
is identical.
Tile cost of these
sl,eps scales as tile volume
exchanged.
(5), if we assunw
this is proportional
to the physical
of the shell of thickness
box z, namely
(d + 2rs) 3da. Note there
are roughly
in a volume
of d 3, since d3 is
the size of box
are 3 cases
to consider.
if d < 7".,data
neighboring
be exchanged
t.he operation
as 8r., . Second,
if d _ v,,, the data
in ali 26 surrounding
and the operation
if d is much larger
v_, only atom
the 6 faces of [)ox z will be exchanged.
'Fhe conamunication
as the surface
area of box z, namely
are explicitly
in the scaling
l_lsewhere
11, we use
A to represent
of the three
is al)plicable
for a given
7'_. We note
less comnmnication
not ali the atoms
a cutofr distance
of a box face will move out
of the box.
this operation
sl.iii scales
as the surface
area of box z, so we list, its scaling
computat.ional
of algoritlltl,
is in st.eps
Ali of these
addil.ional
are neighboring
in the second
structure.
is l)roportional
to A so it is included
in the scaling
'File leading
in the scalillg
(2) is listed
a.s in algorithins
inlwr--box
interactions
are only stored
in algorithm
a.s d grows large relative
to 7', as it will for very large simulat.iolls,
the A contril)ution
ow.'rall colnputation
and tll_. overall
of algorithm
api)roaches
the ol_t,imal
In essence,
each processor
ali its ti,lle working
in its own box and only exchanges
a relatively
of information
wilh neighboring
processors
t,o update
its boundary
conditions.
An important,
of algorithln
and st,ruclure
of the data
are only changed
few timesteps
wll_'n neighbor
constructod.
In particular,
if an atom
moves outside
z's bolirl(laries
it, is not
reassigned
lo a new l)rocessor
_lnl.il step
(la.) is executed
 . Processor
I{ can still
forces for t.he atoln
so long as two criteria
two neighbor
list collst, ructions
is t.llat ali nearl)y
wit.!lin a distance
of 7'e, lllllst, be updated
tinlestep.
alternative
is to move
new processors
at ew__ry timestep
 . This
only atoms
a distance
7":.of box :
be exchanged
ali timesteps
constrtlcted.
the volulne
of communication
now the neigllbor
a reassigned
also lte sent.
The illfornlation
in tile neighbor
list is atom
referencillg
tile neighbor
at,onls are sl.ored.
are continuously
to new processors,
t.llese local i_dices
t_ecome meaningless.
To overco_ne
l.his, our i_nplementation
in ['_4]assigned
to each at,ore which n_ow,d with l.l_e at.o_
to processor.
to local _wmory
ill a wwtor
of size N by each
or the global
and searched
to find tile correct, ato_ns wl_en they
are referenced
in a ne,ighbor
'I'l_e forn_er solution
the size of problems
the latter
a considerable
for the sort, and
operations.
implementing
the Tamayo
idea in our algorithm
tile resulting
code less complex
the computational
cornnaunication
This _:iid not
the timings
for simulations
with large N, but improved
the algorithm's
performance
for medium-sized
A modified
filll advantage
of Newton's
If processor
P_. acquires
its west,, south,
and down directions
only in the east., north,
and tip directions),
interaction
need only be computed
once, even when
the two atoms
in different
force results
in the opposite
directions
to the processors
who own the atoms,
(3) in the algorithm.
communication
half as much
information
is communicated
a.s often,
duplicated
computations
for two-box
interactioiis.
An algorit.hm
is detailed
in with excellent
for the Fujitsu
the overall
of $2 over
particularly
t,he A term
(2) is saved.
as we mention
in our conclusions,
real speed
to be gained
in spatial-decomposition
algorithms
is by improving
single--processor
performance
of force computation
in step (2).
As floating
processors
in parallel
sophisticated
to data structures
and loop orderings
in the force and neighborlist construction
to achieve
high single-processor
flop rates.
Implementing
$2 requires
special-case
and corners
t.o insure
ali interactions
are counted
this optimization
the issue of load-balance
is an important
in any spatial-decomposition
algorithm.
$1 will be load-balanced
only if ali boxes have a roughly
surrounding
be the case
if the physical
is non-uniform.
Additionally,
if the physical
a rectangular
parallelepiped,
be difficult
into P equal-sized
Sophisticated
load-balancing
algorithms
 to partition
non-uniformly
but in general
sub-dornains
which are irregular
or are connected
in an irregular
neighboring
sub-donaains.
case, the task of assigning
to sub-domains
and communicating
with neighbors
and complex.
If the physical
over time during
MD simulat.ion,
the load-balance
is compounded.
load-balancing
additional
computational
In sun,mary,
spatial-decomposition
algorithm,
like the atomand
force-decomposition
algorithms,
the MD computations
ali the processors.
Its chief benefit
full advantage
of the local nature
of the interatomic
by performing
only local communication.
it achieves
and is clearly
the fastest
algorithm,
this is only if good
load-balance
is also achievable.
Since its performance
is sensitive
restrictive
perfornaance
is geometry-'i,-_dependent.
is its complexity;
it, is more difl'icult to iml_lenwnt
efficiently
the simpler
ai,ota-- and force decomposition
algoritllms.
In particular
the communication
and bookkeeping
and access
neighboring
In practice,
integrating
algorithna
into an existing
a sul._stantial
structures
case used to benchmark
algorithlns
is a MI) probleln
has been used extensively
by various
researchers
 . lt. models atom interactions
with a I,ennard-dones
of at.ores separal.ed
by a distance
_ and cr are constant.s.
'l'lle derivative
of this energy
expression
to r is the
(1); Fa and higher-order
are ignored.
are sinmlated
in a 3-1) lmrallelepiped
conditions
at, the I,ennard
by the reduced
and reduced
teznperature
is a liquid
Lennard--.lones
simulation
tlm atonls
rando,nized
w_locities
ft'ore a Boltzmann
distribution.
solM quickly
to its nat.llral
liquid state.
fox' the duration
sthrelation.
The simulation
is run at COllStmit N, volume
I/, aEld energy
E, a statistical
lnicrocanonical
enseml_le.
Force col_q:,utatiolls
using the l_ot,ential
in equation
(2) are t.ruxlcated
at. a distance
r_ = 2.50". The integration
is 0.0046?
in reduced
tlnit,s. For simplicity
we use a leapfrog
i_t.egratc
(1) as in . ()tlwr
in_l_lenwntatio_s
of the bench_nark
 have used predictor--correct.or
this ollly slows tlleir l)ert'ortnance
For tiilfing
purtmses , the critical
of the belJchxnark
for a given
are p" and
deterxlfine
force interactions
lllllS[, ge
Tlm. numl_er
in a sphere
r" = r/0" is given by 4a'p'(r')a/3.
For this benchmark,
r:. = 2.5o', each atom
on average
55 neighbors.
If m'ighl_or
lists are. used,
l.he benchnmrk
detixies an extended
rs = 2.8o" (encompassing
for forming
the neighbor
lists and specifies
the lists be created
or updated
20 timestel_s.
'Filnixlgs for the benchmark
are usually
seconds/timestep.
If neighl)or
cost of creating
l.henl every
is amortized
over the per
lt is worth
tidal, without
a standard
be difticult
to accurately
the performmlce
algorithm.
In particular,
it can be misleading
performance
of a code to the original
vectorized
code because,
as we have learned
as well as other's
code performance
may well be far from optinlal.
specifications
are reported,
it can be difficult
to compare
two algorithm's
performance
two different
This is because
of tile wide variability
in the cost, of calculating
equations,
of neighbors
distances,
of neighbor
as a function
of temperature,
atom density,
distances,
The parallel
algorithms
of Sections
3, 4, and 5 were tested
MIM D supercomputers,
2, an Intel
two machines
is at. Cal Tech.
2 is a 1024-processor
hypercube.
Each processor
is capable
2 Mflops and has 4 Gbytes
of memory.
has 64 i860 processors
in a hypercube
Its processors
have 8 Mbytes
of melilory
are capable
60 Mflops,
in practice
5--10 Mflops is the
performance.
Delta has 512 processors
configured
The il_dividual
processors
have 16 Mbytes
and are identical
to those in the iPSC/8¢i0,
the communication
is somewhat
the algorithms
implemented
in standard
calls to vendor-supplied
message-passing
subroutines,
only minor
were required
to implement
The algorithms
as described
do not specify
of processors
to the computational
sub-blocks,
potentially
for a particular
architecture
to minimize
contention
communication
and the distance
have to travel
of processors
are not directly
by a communication
are simple
for hyl)ercubes.
portability
we used the same
mesh-architecture
For the atom--decomposition
we siml)ly
the processors
in ascending
to the rowblocks
of the force
folds then
as in Figure
the force--decomposition
we use a natural
of the processors
to the force matrix
7. On a hypercube
this means
row and column
of the matrix
is a sub-cube
of processors
folds within
rows :_lld columns
optimally.
the transpose
operations
in algorithms
now require
communication
ot" processors
are architecturally
will be some message
contention
transposes
ms multiple
destinations
simultaneously.
operations
even with some slow-down
due to message
congestion,
the overall
of the communication
of the force-decomposition
algorithms
is not affected.
not implement
it for this work,
of processors
to the force
contention-free
transposes
for a hypercube
is possible
and is described
in . We have also recently
a modified
force-decomposition
not. require
operations
mesh-architecture
spatial-decomposition
Mgorit.hna,
we use a processor
essentially
configures
a hypercube
a Gray-coded
 of the
processors.
processor's
box ill Figure
10 has 6 spatial
in the east,
down directions)
are assigned
to processors
are also nearest
topology. Communication
is thus contention-free
as fast as possible.
Gray-coding
for periodic
conditions
MD simulation
processors
the edge of
are topological
on the opposite
only restriction
Gray-coding
of processors
dinaension
power-of-two.
is no obvious
way to map
to its 2-D
processors.
We use the same
3-D Gray-coding
assignment
for code portability.
for the benchmark
on the different
III for the
spatial-decomposition
algorithms.
of problem
considered
from N = 500 atoms
to N = 107 atoms.
size tbr each
is also specified;
are 4 atoms
cell for the fcc lattices.
with a dashed
line are for problems
would not fit
in available
last entries
are roughly
the largest
sizes for this benchmark
can be run with each
due to memory
restrictions
For comparison,
we also implemented
the vectorized
ct. al. on single
processors
of Sandia's
and a Cray C90 at Cray
Our version
is slightly
from the original
code, using a simpler
integrator
and allowing
for non-cubic
The timings
in reference
for a Cray
We believe
for the faster
C90 architectures
for this benchmark
on a single
supercomputer.
a C90 processor
to be about
algorithm.
The st,:rred
ix_the tables
are estimates
for problems
t.t_fit in memory
the machines
accessible
to us. They
are extrapo!ations
of the N = 10s system
on the observed
of the Cray
algorithm,
lt is also worth
nor,ing that
in the parallel
algorithms
of i_he previous
could be used to create
efficient parallel
Cray codes for multiple
processors
For example,
a speed-up
of 6.8 on a 8-processor
with the Grest,
ct. al. algorithm
The parallel
in the three
are ali for single-precision
implementations
of the benchmark.
Cray tinamgs
are, of course,
f'gr 64-bit
arithmetic
is the only option.
MD simulations
approximation
the potential
and the integrator.
is particularly
true of Lennard-Jones
the e and _r
coefficients
are only specified
to a few digits of accuracy
as an approximate
of the interatomic
in a real material.
this said, double
can be easily estimated
for the parallel
algorithms.
The processors
in ali three
of the parallel
about 20-30% slower
in double-precision
lntel iPSC/860
seconds/timestep
atom-decomposition
oil several
the benchmark
simulation.
and C90 timings
using a fully vectorized
are also given for comparison.
metic than
so the time spent
would be increased
Conununication
each of the algorithn-Ls would essentially
since the volume of information
being exchanged
iii messages
would increase
by a factor
on the fraction
of time being spent
in communication
a particular
N and P (see the scaling
discussion
the overall
double-precision
The tables
show the parallel
to be competitive
with the Cray
and C90 machines
of problem
for ali three
algorithlrLs.
force-decomposition
for the smallest
sizes; spatial-decomposition
is fastest
of the three
up to 30 times
smes using
the spatial-decomposition
a C90 processor.
simulations
.498 seconds/timestep
respectively.
A surprising
are competitive
of the Cray
even for the smallest
being enough
parallelism
to exploit
are only a few atoms
processor.
The hardware
Problem Size
Intel iPSC/860
Intel Delta
P=-512 I P=-1024
P=-256 I P=512
Table II: CPU
seconds/timestep
for the force-decomposition
F2 on several
performance
on the Cray
also provides
the useful metric
the vectorized
260 Mflops for large
N on a C90 processor.
tile floating
is not identical
the 512-processor
the spatial-decomposition
for the largest
sizes, or about
6 Mflops/processor
which is typical
of i860 chip performance
on compiled
subroutines
run at 20-30
the i860 (out
of a peak speed
of 60 Mflops),
this indicates
much higher
performance
could still be achieved
MD algorithms
by writing
code optimized
for a particular
processor.
for this benchmark
are also discussed
in , ali for spatialdecomposition
algorithms.
et. al. implemented
algorithms
on a 32K-processor
(1024 floating
processors).
ran at 0.57 sec/timestep
for a N = 18000
of two slower than
the single
in the tables
and Giles achieved
a time of 0.4 sec/timestep
simulation
256 processors
of a CM--5.
is roughly
of two faster
programmed
with explicit
the timings
dramatically
the vector
work on the CM-5,
and Lomdahl
 report
time of 1.00 sec/timestep
for a N = 106 atom simulation
on 1024 processors,
also without
was for a shorter
with only an average
of 21 neighbors/atom
of 55 as in this
benchmark.
et. al. detail
an algorithm
in Section
5. For a N = 729000 atom
(at a slightly
of p* = 0.8) run on 512 processors
Problem Size
Intel iPSC/860
lntel Delta
!OOxlOOx125
10,000,000
lOOx125x200
seconds/timestep
for the spatial--decomposition
a time of 0.927 see/timestep.
The timings
I show that
communication
o "ts have begun
to dorninate
in the atom-deconlposition
by the time hundreds
of processors
is little speed
by doubling
the number
of processors
By contrast
II show the force-decomposition
is speeding
by roughly
30% when the number
of processors
is doubled.
The timings
for the largest
sizes in Table
III evidence
properties.
the run times
for a given
Similarly,
the run times
as the surface-to-volume
processor's
is reduced.
this scaling
on uniform
as the rectangular
parallelepiped
of the benchmark
A comparison
of the different
algorithm's
performance
data from ali 3 tables
can be better
in graphical
performance
simulation
processors
as a function
of problem
of ali the algorithms
N is large is evident.
force-decomposition
atomdecomposition
ali problem
sizes due to its reduced
communication
On this many processors,
spatial-decomposition
has significant
This is because
has to communicate
with a large
of neighboring
processors
to acquire
ali its needed
information.
As N increases,
this overhead
is reduced
to the computation
the processor's
and the algoritllm's
performance
asymptotically
approaches
its optimal
performance.
is a cross-over
size N at which the spatial-decomposition
force-decomposition.
to this point
in the conclusions
Cray Y-MP/1
Atom-Decomposition
Force-Decomposition
Spatial-Decomposition
(seconds/timestep)
algorithms
1024 processors
2 for different
Single-processor
are also given for comparison.
13 we plot
performance
"- 10976 atom
as a function
of processors.
The single-processor
i860 and Y--MP
are also shown;
13.3 times
a i860 processor
on this problem.
line is the maximum
achievable
of the Delta
if any of the
algorithms
100% efficient.
efficiency
is defined
as the run time
1 processor
by the quantity
× run time on P processors).
512-processor
1-processor
the algorithm
is 50% efficient.
of processors
communication
is not a significant
and ali the algorithms
similarly.
But as P increases,
algorithms
less efficient.
atoI,-decomposition
falls off most
due to the O(N)
of its communication.
mesh the all-to-ali
communication
this algorithm
is particularly
inefficient
of message
contention),
a slow-down
512 processors.
Force-decomposition
due to its O(N/v/"fi)
communication
competitive
with tire spatial-deconlposition
a wide range of numbers
of processors.
or thousands;
of processors
even the spatial-decomposition
efficient, since
now tile box size is small
to the force cutoff
Ii, is worth
in the plots
of Figures
13 are the same
and prol:flem sizes
in this study.
the absolute
are functions
of N, P, and the benchmark
attributes,
the relative
trade--offs
algorithms
are consistently
Using one-node
on the nCUBI'3 and Intel
ms reference
efficiencies
for ali the algorithms.
2 one-processor
tin ring is 9.15 × 10-4 seconds/tinaestep/atom.
give a one-processor
of 2.03 × 10-4 seconds/timestep/atom.
the algorithms
scale so linearly,
values can
be used to predict
for problems
will fit on a single
processor.
million-atom
simulation,
spatial-decomposition
has a parallel
efficiency
of 76% on 1024 processors
of the nCUBE
and 80% on 512 processors
late! Delta.
simulations
a 90% parallel
efficiency.
in context,
on the nCUBE,
the million.-atoln
simulation
has about,
1000 atoms
in itu box.
But, the range
of the cutoff
in the benchmark
2600 atoms
surrounding
to compute
the spatial-decomposition
is 76% efficient
even though
two-and--a-h',df
are communicated
as are updated
processor.
we discuss
the scalability
of the different
algorithms
in the large N limit.
the overall
of the computation
and communication
of the 5 algorithn_s.
is constructed
the scaling
for the various
algorithms
in Figures
5, 6, 8, 9, and
is an opt.ion.
coefficients
are included
contra.,_ts
algorithms.
per processor
and force vectors
in the table.
Computation
atom-decomposition
the second
for binned
list construction.
The coefficient
on this term
so it is usually
not a significant
communication
positions.
By contrast,
atom-decomposition
A2 implements
3rd law so its leading
computational
Now the communication
is doubled
and the entire
force vector
Force-decomposition
algorithms
computational
complexity
respectively
for neighbor
list construction
now scales
as N/v/ft,
significant
are 3 expands/folds
and one transpose
for a total
communication
cost of 3N/v/-fi
F2 requires
4 expands/folds
and 2 transposes,
hnplementing
sub-vectors
and one force sub-vector,
ali of length
Atom-Decomposition
Force-Decomposition
--" Spatial-Decomposition
Cray Y-MP/1
P (processors)
(seconds/timestep)
for the three
algorithms
on the Intel
for different
of processors
on a benchmark
simulation
N = 10976 atoms.
Single-processor
i860 and Cray
for comparison.
force sub-vector.
Computation
in the spatial-decomposition
it implements
3rd law for interactions
atom pairs
a processor's
For large N problems
is an extra
for computations
a distance
rs of the box
is proportional
to the surface
of the box
2/3) times
rs for each
communication
in algorithm
requirements
for storing
the nearby
Additionally,
be allocated
for storing
in a processor's
As mentioned
the memory
IV are for the
algorithm.
in ali of the algorithms
processors
additional
information
as neighbor
velocities
In practice,
for the forceand spatialdecomposition
algorithms,
of neighbor
is actually
the dominant
in limiting
the size of
Computation
Communication
2 P + 6rs (-rf)
-ff + 6rs (-rf)
IV: Scaling
properties
of ali 5 parallel
algorithms
as a function
of problem
processors
Run time scaling
for the communication
and computation
of tile algorithms
per-processor
requirements
are listed.
can be run.
For example,
the 4-6 Gbytes
of user memory
on tile nCUBE
2 and lntel
in this study
simulation
to be run wit,la tile rc = 2.5cr cutoff
1024-processor
a 70-million
3-D simulation
 . These
are indicative
the size of problems
be run on current-generation
Application
Algorithms
in Sections
7 is relatively
algorithms
in this paper
can be used
in a variety
MD simulations
modification.
We discuss
the parallel
implications
of some common
in the next
paragraphs.
computationally
Lennard-Jones
potentials
used in MD
simulations
of various
materials.
forces, even if they are very expensive,
be pre-computed
form or as a set of interpolating
coefficients.
to be little
more expensive
to compute
potentials.
have ample
large tables
of force values
coefficients
in duplicate
processor.
are functions
velocities,
quantities
positions,
are sometimes
An example
is the embedded
potentials
 commonly
alloys where an atom's
is a function
of electron
contributions
neighboring
as well as conventional
pair--potential
interactions.
simulation
in fluid dynamics
"particles"
vorticities.
the parallel
algorithms
be augmented
(5) to communicate
additional
atom-based
quantities
 without
sophisticated
force models
used in MD simulations
of covalently
materials.
(three-body)
for silicon
(four-body)
or proteins.
in parallel
if a single
the positions
of ali the atoms
in a particular
to implement
in the atomdecomposition
spatialdecomposition
be modified
t.o insure
information
surrounding
boxes to compute
ali the many--body
are a party
short-range
in nature,
the force-decomposition
care in this respect.
is because
a processor
only knows the positions
have no special
relationship
OHe solution
is to perform
a pre-processing
to reorder
in such a way that
more processors
will know the positions
of all the atoms
in each bond group.
We have developed
this in organic
MI) simulations
the connectivity
of the bond groups
 . tlowever,
we know of no simple
way to use the
force--decomposition
of dynamically
connectivities,
as for silicon
three--body
potentials.
(D) Though
force calculation
is the key computational
kernel in MD simulations,
the quantities
of interest
parameters
like pressure,
and diffusion
coefficients.
thermodynamic
properties
calculated
once every
50 or 100 timesteps
to tile overall
computational
of a serial
for the parallel
In short-range
can compute
its partial
contribution
to one of these
quantities
from the atom
information
it already
the local values can
be accumulated
as a global
sum across
ali the processors.
list construction
is triggered
For example,
lists will
only be recreated
when some atom has moved
half the distance
this can easily be implemented
in the parallel
algorithms
if any of its
criterion,
exchanging
flag to decide
if the neighbor
list routines
be called.
If the list of interacting
in a particular
MD simulation
(e.g. atoms
on a lattice),
(1) in ali of the parallel
algorithms
unnecessary.
The remaining
steps of the algorithms
are still a fast way to p'arallelize
computation
and communication
for this special
(F) The benchmark
implements
a constant
N, volume V, and energy
E microcanonical
is to hold N, pressure
P, and temperature
T constant,
from the canonical
the simulation
dimensions
and velocities
at each timestep
few timesteps)
to hold the pressure
and temperature
In parallel
additional
communication,
or exchange
of the rescaling
parameters,
to the effort
integrator
implementation
integrators
as Runge-Kutta
or predictor--corrector
of any of the parallel
algorithms
are perfectly
information
processor.
old timestep
or work vectors.
(H) Multiple-timescale
MD methods
have been proposed
 , where
work is done at staggered
to allow longer
to be taken
on average.
very short--range
information
to compute
in the smallest
timesteps.
are an effort
to include
longer-range
while avoiding
long-range
force computation.
are typically
implemented
by a hierarchy
which store
information
for the different
inherently
shortrange force models,
can be implemented
the general
of any of the parallel
algorithms
we have presented.
In the limit
computation
long-range
in nature,
forces are usually
the computational
as discussed
in Section
2. However,
if long-range
can still be computed
in the force-matrix
of the atom--and
force-decomposition
algorithms
By contrast
the spatie,]-decomposition
now require
long-range
communication
and become
an inefficient
Conclusion
We have detailed
the construction
implementation
of parallel
algorithms
for MD simulations
short-range
Each of them
has advantages
and disadvantages.
atom-decomposition
is simplest
to implement
and load-balances
automatically,
it performs
all-to-ali
communication,
its communication
to dominate
time on large
of processors.
force-decomposition
is also relatively
some pre-processing
to assure load-balance.
It also works well independent
of the physical
Its O(N/v/-fi)
atom-decomposition
algorithm,
simulations.
spatial-decomposition
does exhibit
for large problems.
it suffers
from load-imbalance
and is more difficult
to implement
efficiently.
In practical
how does one choose
for a particular
MD simulation?
the simulation
guidelines
(A) Choose
an atom-decomposition
only if the communication
cost is expected
to be negligible.
In this case simplicity
tile inefficient
communications.
this will only be true for small
(say P <_ 16 processors)
or very expensive
where computation
communication
A force-decomposition
will be faster
atom-decomposition
in ali other
the atomand
force-decomposition
algorithms
N for fixed
for a given
the parallel
efficiency
is independent
ms P doubles,
efficiency
of the communication
of the atom-decomposition
by a factor
of 2, while the
force-decomposition
algorithm's
efficiency
by a factor
of only v/2.
P is large enough
force-decomposition
is significantly
faster than
atom-decomposition,
ii, will remain
faster ms P increases,
independent
For the benchmark
this was the
case for P :> 16 processors.
(C) For a given
the scaling
of the spatial-decomposition
is not linear
N communication
are significant
the efficiency
efficiency
is asymptotically
to a force-decomposition
some cross-over
as N increa.ses for a given P where a spatial-decomposition
In the benchmark
the cross-over
size was several
on hundreds
of processors,
as in Figure
12. In general,
the cross-over
size is a function
of the complexity
of the force model,
force cutoff
distances,
computational
communication
capabilities
of a particular
lt will also be a
For example,
if [:,lie force
is reduced
21160, as is common
repuisiw. • forces),
a spatial-decomposition
will require
less exchange
of information
processors
to neighboring
the cross-over
size will be reduced
of a spatial--deconlposition
algorithm's
efficiency
for a given
P can be made
processor's
box has voluu_e da -
it computes
and communicates
information
of (d + 2r_)3.
the extended
to the box volume
gives a rough
of the extra
(inefficient)
tlw algorithln
is perfornfing.
the computation
in the spatial--decomposition
is perfectly
load-balanced.
Load-iznbalance
on the efficiency
a spatial-decomposition
can achieve.
For example,
biological
simulations
of proteins
by water often
are performed
in a vacuum
in the simulation
fill a roughly
is treated
the sphere
fills only
a _'/6 fraction
of the cube
50% parallel
inefficiency
net effect
of load-imbalance
is to increase
cross-over
a spatialdecomposition
a force-decomposition
In practice,
we have found
decomposition
or at lea.st quite
competitive
spatial-decomposition
algorithms
for simulations
of up to many tens
of thousands
In Section
7 we discussed
the performance
of the parallel
algorithms
computers,
current-generation
competitive
multi-processor
Cray-class
supercomputers
for short-range
MD simulations.
generally,
algorithms
can be implemented
on any parallel
allows its processors
to execute
independently
processors
by standard
message-passing
techniques.
This is the definition
of a multiple
instruction/multiple
architecture.
currentand next-generation
supercomputers
of programming,
of tile algorithms
the flexibility
of the MIMD
the code to build
and access
variable-length
via indirect
addressing,
to select/pack/unpack
data for messages,
and to efficiently
variable-length
structures
sub-groups
of processors
as in Figures
we are confident
algorithms
or versions
on similar
will continue
to be good
MD simulations
on parallel
Optimizing
performance
nextgeneration
will require
single-processor
computational
performance.
As the individual
processors
used in parallel
high computational
only be achieved
by writing
or vectorized
of the data
reorganization
optimization
techniques
for MD on vector
 will become
for parallel
implementations
Acknowledgments
I am indebted
tlendrickson
discussions
MD algorithms,
particularly
force-decomposition
techniques
algorithms
was particularly
me in this effort.
I also thank
for sending
me a copy of his vectorized
and have benefited
from discussions
with Pablo
at Los Alamos
concerning
MD techniques.
in Section
Additionally,
ali of these
individuals
for suggesting
improvements
manuscript.
was supported
by the Concurrent
Supercomputing
Consortium
at Cal Tech; I thank
of the CSC staff
Ibr timely
assistance
in this regard.