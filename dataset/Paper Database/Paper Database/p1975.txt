Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinﬂection, pages 90–99,
Vancouver, Canada, August 3–4, 2017. c⃝2017 Association for Computational Linguistics
Data Augmentation for Morphological Reinﬂection
Miikka Silfverberg, Adam Wiemerslage, Ling Liu and Lingshuang Jack Mao
Department of Linguistics
University of Colorado Boulder
 
This paper presents the submission of the
Linguistics Department of the University
of Colorado at Boulder for the 2017
CoNLL-SIGMORPHON Shared Task on
Morphological
Reinﬂection.
The system is implemented as an RNN
Encoder-Decoder. It is speciﬁcally geared
toward a low-resource setting. To this end,
it employs data augmentation for counteracting overﬁtting and a copy symbol
for processing characters unseen in the
training data. The system is an ensemble
of ten models combined using a weighted
voting scheme.
It delivers substantial
improvement in accuracy compared to a
non-neural baseline system in presence of
varying amounts of training data.
Introduction
Natural language processing (NLP) for English
is typically word based, that is, words such as
dogs, cat’s and they’ve are treated as atomic units.
In the case of English, this is a viable approach
because lexemes correspond to a handful of in-
ﬂected forms. However, for languages with more
extensive inﬂectional morphology, the approach
fails because one lexeme can be realized by thousands of distinct word forms in the worst case.
Therefore, NLP systems for languages with extensive inﬂectional morphology often need to be
able to generate new inﬂected word forms based
on known word forms. This is the task of morphological reinﬂection.
The traditional approach to word form generation is rule-based. For example, ﬁnite-state technology has been successfully applied in constructing morphological analyzers and generators for a
large variety of languages . Unfortunately, the rule-based approach is
labor-intensive and therefore costly. Additionally,
coverage can become a problem because systems
need to be continually updated with new lexemes.
For these reasons, machine learning approaches
have recently gained ground.
Results from the 2016 SIGMORPHON Shared
Task on Morphological Reinﬂection indicate that models based on recurrent neural networks can deliver high accuracies
for reinﬂection.
The winning system by Kann
and Sch¨utze achieved an average accuracy
in excess of 95% when tested on 10 languages.1
Based on these results, morphological reinﬂection
could be considered a solved problem. However,
the 2016 shared task employed training sets of
more than 10,000 word forms for most languages.
In a setting with less training data, the reinﬂection task becomes much more challenging. In an
extreme low-resource setting of 100 training examples, a standard RNN Encoder-Decoder system
like the one used by Kann and Sch¨utze will
typically perform quite poorly.2
This paper documents the submission of the
CU Boulder Linguistics Department for the 2017
CoNLL-SIGMORPHON Shared Task on Universal Morphological Reinﬂection.
The task covers 52 languages from different language families
with a wide geographical distribution. The task
evaluates systems trained on varying amounts of
data ranging from 100 to more than 10,000 training examples.
Our system is an RNN Encoder-Decoder speciﬁcally geared toward a lowresource setting. The system closely resembles the
1For inﬂecting lemmas according to a given morphological feature set.
2According to experiments performed by the authors, the
system employed by Kann and Sch¨utze delivered accuracies between 0% and 1% for most languages in the shared
tasks when using 100 training examples.
system introduced by Kann and Sch¨utze .
However, the novelty of our approach lies in the
training procedure. We augment the training data
with generated training examples. This is a commonly used technique in image processing but it
has been employed to a lesser degree in NLP. Data
augmentation counteracts overﬁtting and allows us
to learn reinﬂection systems using small training
We employ an ensemble of 10 models under a
weighted voting scheme.
We also implement a
mechanism, the copy symbol, which allows the
system to copy unseen characters from an input
lemma to the resulting word form. This improves
accuracy for small training sets.
Unfortunately,
due to time constraints, we were only able to use
the copy symbol in Task 2 of the shared task.
For Task 1 of the shared task, we achieve substantial improvements over a non-neural baseline
 , even in the low resource
The paper is organized as follows: Section 2
presents related work on morphological reinﬂection and data augmentation for natural language
processing. In Section 3, we describe the shared
task and associated data sets. We provide a detailed description of our system in Section 4 and
present experiments and results in Section 5. Finally, we provide a discussion of results and conclusions in Section 6.
Related Work
Several existing approaches to morphological reinﬂection are based on traditional structured prediction models. For example, Liu and Mao 
and King use Conditional Random Fields
(CRF) and Alegria and Etxeberria and
Nicolai et al. employ different phoneme-tographeme translation systems. Other approaches
include learning a morphological analyzer from
training data and applying it to reinﬂect test examples and extracting morphological paradigms from the training data which are
then applied on test words . The results of the 2016 SIGMOR-
PHON Shared Task on Morphological Reinﬂection indicate that none of these approaches can
compete with deep learning models.
learning systems outperformed all other systems
by a wide margin.
The three best performing teams in the 2016 SIGMORPHON shared task
employed deep learning approaches based on the
RNN Encoder-Decoder framework proposed by
Cho et al. and later used for machine translation by Bahdanau et al. .
This family
of models is intuitively appealing for morphological reinﬂection because of the obvious parallels between the reinﬂection and translation tasks.
The success of the winning system by Kann and
Sch¨utze highlights the importance of an additional attention mechanism introduced by Bahdanau et al. .
Although the RNN Encoder-Decoder framework has proven to be highly successful in morphological reinﬂection, an out-of-the-box RNN
Encoder-Decoder system performs poorly in presence of small training sets due to overﬁtting. To
alleviate this problem, we employ data augmentation, that is, augmentation of the training set with
artiﬁcial, generated, training examples. The technique is well known in the ﬁeld of image processing . Even though the technique is used less frequently in NLP, a number of notable approaches
do exist. Sennrich et al. use monolingual
target language data to improve the performance
of an Encoder-Decoder translation system. They
ﬁrst train a translation system from the target language to the source language, which is used to
back-translate target language sentences to source
language sentences. The sentence pairs consisting
of a translated source sentence and a genuine target sentence are then added to the training data.
Other approaches to data augmentation in NLP include substitution of words by synonyms and paraphrasing.
Task Description and Data
The shared task consists of two subtasks:
generation of word-forms based on a lemma and
a set of morphological features (for example,
dog+N+Pl →dogs), and (2) completion of morphological paradigms given a small number of
known forms (see Figure 1).
Systems are evaluated on 52 languages.3
3Albanian, Arabic, Armenian, Basque, Bengali, Bokmal,
Bulgarian, Catalan, Czech, Danish, Dutch, English, Estonian, Faroese, Finnish, French, Georgian, German, Haida,
Hebrew, Hindi, Hungarian, Icelandic, Irish, Italian, Khaling,
Kurmanji, Latin, Latvian, Lithuanian, Lower Sorbian, Mace-
For both subtasks and all languages, there are
three data settings, which differ with respect to the
size of available training data: low, medium, and
high. In Task 1, these span 100, 1,000 and 10,000
examples respectively. However, there is no training set for the high setting for Gaelic. In Task
2, there are 10 example paradigms in the low setting. Most languages have 50 example paradigms
in the medium setting (Basque has 16, Haida 21
and Gaelic 23).
In the high settings, most languages have 200 example paradigms (Bengali has
86, Urdu 123 and Welsh 133). There is no training
set for the high setting in Task 2 for Basque, Haida
and Gaelic. All settings use the same development
and test sets. Further details concerning the shared
task and languages can be found in Cotterell et al.
V V.PTCP PRS
V 3 SG PRS
V V.PTCP PST
Figure 1: Illustration of Task 2 – the paradigm completion
The system will ﬁll in missing forms based on the
lemma, morphological features and the known word forms.
System Description
Our system is an RNN Encoder-Decoder network
heavily inﬂuenced by Kann and Sch¨utze .
The key difference is that our system is trained
using augmented data, which substantially improves accuracy given small training sets.
train several models and employ a weighted voting scheme, which improves results upon a baseline majority voting system. Additionally, we use
copy symbols which allow the system to process
lemmas that contain characters that were missing
in the training data.
RNN Encoder-Decoder with Attention
We use an RNN Encoder-Decoder model with attention proposed by Bahdanau et al. for
machine translation, which was later applied to
morphological reinﬂection by Kann and Sch¨utze
 . The architecture of our model differs from
donian, Navajo, Northern Sami, Nynorsk, Persian, Polish,
Portuguese, Quechua, Romanian, Russian, Gaelic, Serbo-
Croatian, Slovak, Slovene, Sorani, Spanish, Swedish, Turkish, Ukrainian, Urdu, and Welsh.
Figure 2: The RNN Encoder-Decoder for morphological reinﬂection. The system takes a lemma and associated tags as
input and produces an output form.
the model proposed by Kann and Sch¨utze 
only with regard to minor details.
The high-level intuition of the system is conveyed by Figure 2. The system takes a sequence of
lemma characters and morphological features as
input (for examples d, o, g, N, PL) and produces a
sequence of word form characters as output (d, o,
g, s). It incorporates two encoder LSTMs, which
operate on embeddings of input characters and
morphological features. One of the encoders consumes the input lemma and features from left to
right and the other one consumes them from right
to left. This results in two sequences of state vectors, which are translated into a sequence of output
characters by a decoder LSTM with an attention
mechanism.
More speciﬁcally, our system ﬁrst computes
character embeddings e(·) for input characters and
features. These embeddings are then encoded into
forward state vectors fi and backward state vectors bi by a bidirectional LSTM (a combination
of a forward and backward LSTM). Each forward
and backward state pair ei = (fi, bi) is used as
the bidirectional LSTM state at position i. Subsequently, a decoder LSTM generates a sequence of
embeddings which is then transformed into output characters by a softmax layer. At each state
during decoding, the current state vector of the decoder is computed based on (1) the previous decoder state, (2) the previous output embedding,
and (3) all encoder states (fi, bi). The simultaneous use of all encoder states is realized by an
attention mechanism A which computes a weight
wi,j−1 for each encoder state ei given the previous
decoder state fj−1. These weights are then normalized into weighting factors ϵi,j−1 using softmax,
that is ϵi,j−1 = exp(wi,j−1)/ Pn
i=1 exp(wi,j−1).
The next decoder state fj is then determined based
on the previous decoder state fj−1, the previous
output embedding and a weighted average of all
encoder states A(fj−1, e1, ..., en) given in Equation 1.
A(fj−1, e1, ..., en) =
The attention mechanism A is implemented as
a feed-forward neural network with one hidden
layer and hyperbolic tangent non-linearity (tanh).
For the encoders and the decoder, we use 2layer LSTMs 
with peephole connections and coupled input and forget gates
 . We train our system using
Stochastic Gradient Descent. Our system is implemented using the Dynet toolkit 4 and our code is freely available.5
There are three hyper-parameters in our system:
the character embedding dimension, the size of the
hidden layer of the LSTM models and the size of
the hidden layer of the attention network. We set
these to 32 for most languages but use 100 for a
number of languages, as explained in Section 5.
Data Augmentation
In order to counteract overﬁtting caused by data
sparsity in the low and medium data settings of
the shared task, we use data augmentation. That is,
we generate new training examples from existing
training examples.
Our data augmentation technique is based on
the observation that in most cases word forms
can be split into three parts: an inﬂectional pre-
ﬁx, a word stem and an inﬂectional sufﬁx. For
example, the English word ﬁzzling can be split
into 0+ﬁzzl+ing. In many cases, as in the case
4 
latest/index.html
5 
of the lemma ﬁzzle and word form ﬁzzling, the
stem is shared between the lemma and word
By replacing it, in both the lemma and
word form, with another string, we can produce a new training example from an existing
one. For instance, we can produce a new example (sfkekgivlofe+V+PRS+PCP, sfkekgivloﬁng)
from (ﬁzzle+V+PRS+PCP, ﬁzzling) by replacing
ﬁzzl with sfkekgivlof.
Data augmentation requires that we can identify word stems. We approximate this by identifying the longest common continuous substring of
the word form and lemma. This strategy can be
expected to work well for languages with largely
concatenative morphology. In languages with extensive stem changes or stem allomorphy, it can,
however, fail.
We experimented with two different techniques
for generating new stems:
• Draw each character from a uniform distribution over the set of characters occurring in the
training ﬁle.
• First, train a language model on the training data.
Then, use a sampling-based
method to identify a likely character sequence c1, ..., cm based on the probability given by the language model to the
string p1...plc1...cms1...sn, where p1...pl and
s1...sl are the inﬂectional preﬁx and sufﬁx respectively.
We experimented with two different language
models—a simple trigram based model with additive smoothing and a 5-gram model with Witten-
Bell smoothing .
The augmented training data generated using
the language models seems to be phonotactically
superior to the data generated by the uniform distribution over all characters.
However, surprisingly, it fails to produce comparable accuracy.
Therefore, we only report results for the strings
drawn from the uniform distribution.
For each language and setting, we train an ensemble of ten models. The most straightforward way
of utilizing such an ensemble is majority voting
which is employed by Kann and Sch¨utze .
In majority voting, the output candidate which was
generated by the greatest number of models is the
ﬁnal output of the ensemble. In contrast to Kann
and Sch¨utze , we apply a weighted voting
scheme to the model ensemble.
In weighted voting, each model receives a
weight wi ∈ . It then uses this weight to vote
for the output candidate that it generated. Let Sj
be the set of models that generated output candidate cj. Then the total weight Wj of candidate cj
is given by Equation 2. The candidate with the
highest total weight is the output of the ensemble. It is easy to see that setting all model weights
wi = 1/10 gives regular majority voting.
We tune model weights using Gibbs sampling in
order to attain improved accuracy. Gibbs sampling
is implemented as a function which iteratively adjusts the weight distribution {w1...w10} in order
to ﬁnd weights that result in improved accuracy
on the development set. Each adjustment is made
by moving some probably mass of size α from
a randomly selected weight wi onto another randomly selected weight wj as illustrated in Figure
3.6 The new weight distribution is then accepted
or rejected based on the resulting development set
accuracy. We initialize the weights using an even
distribution, where wi = 1/10.
The development set accuracy a2 of the adjusted weight distribution is checked against the
development set accuracy a1 of the previous distribution, and the adjusted distribution is accepted
with a probability proportional to a2/a1.
draws upon the intuition of Gibbs Sampling that
an inferior conﬁguration is sometimes accepted in
order to account for the non-convex nature of the
objective function.
After Gibbs sampling completes, the weight
distribution attaining maximal development set accuracy wmax = {w1...w10} is returned.
The Copy Symbol
The decoder of an RNN Encoder-Decoder system
can only emit characters that were observed in the
training data. This is typically a minor problem
when using large training sets because these are
likely to contain all frequent orthographic symbols. However, it can become a severe problem
when the training set is very small. The problem
6We test α values in the set {.001, .01, .05, .1, .2} and run
Gibbs sampling for 10,000 iterations.
Figure 3: The probability mass is moved from model 4 to
model 7 in order to test a new weight distribution
ﬁzzle+V+Prs+Pcp
@i@@le+V+Prs+Pcp
substitute
Figure 4: Substitution of unknown characters with copy symbols (@), inﬂection, and subsequent reversion. In this example, the characters f and z are missing from the training data.
can have a surprisingly large effect on overall accuracy because reinﬂection will often fail when
even one of the characters in the lemma is unknown to the system.
In order to solve the problem of missing characters, we use a special copy symbol. During test
time, unknown symbols are substituted by copy
symbols and reinﬂection is performed. After reinﬂection, each copy symbol is reverted back to
the original unknown symbol as shown in Figure
4. Reversion is performed by substituting the ith
copy symbol in the output string with the ith unknown symbol in the lemma. If extra copy symbols remain after reversion, they are replaced with
the empty string.
Generated stems with copy symbols are added
to the training data during data augmentation. This
allows the system to learn to copy the symbols
from the input lemma to the output word form.
Experiments and Results
For Task 1, we train ten models for each language
and setting. We then apply weighted voting as explained in Section 4. For most languages, a hidden
layer size, embeddings size, and attention layer
size of 32 gave reasonable results.
For 11 languages, Faroese, French, German, Haida, Hungarian, Icelandic, Latin, Lithuanian, Navajo,
Bokmal, and Nynorsk, we found 32 insufﬁcient,
and set hidden layer size, embedding size and attention layer size to 100 instead. Setting the layer
size to 100 might improve results for other languages as well. Unfortunately, we did not have
enough time to test this.
Data augmentation is used in order to improve
accuracy in the low and medium training data settings for Task 1. In the low setting, we add 4900
augmented training examples to the training set,
and in the medium data setting, we add 9900 augmented training examples. Given that the original
low training data spans 100 and the medium training data spans 1000 examples, this means that the
original training data accounts for 2% of the augmented low training set and 10% of the augmented
medium training set.
For Task 2, we also use augmented data. In the
low setting, we add augmented data until the total
size of the training set is 20,000 examples. In the
medium and high settings, we add augmented examples until the size of the training set is 25,000.
Time constraints prohibited us from using more
generated data.
Because of the large variance of the sizes of
training sets in Task 2 (for example the low Basque
training data spans 4,750 examples, whereas the
low English training data spans 50 examples),
some languages use substantially more augmented
data than other languages.
In the high setting,
some languages, in fact, draw upon no augmented
data at all due to the large size of the training set.
For Task 2, we use the copy symbol as explained in Section 4. This would probably have
resulted in improved accuracy for Task 1 as well.
Unfortunately, we were unable to run experiments
using the copy symbol for Task 1 because of time
constraints.
The test results for Task 1 and Task 2 are shown
in Table 1. For Task 1, the RNN system achieves
average accuracy 45.74% for the low settings,
77.60% for the medium setting and 92.97% for
the high setting. All of these ﬁgures are substantially greater than the baseline accuracies which
are 37.90%, 64.70% and 77.81% for the different
settings, respectively.
The RNN system fails to achieve the baseline accuracy for eight languages in the low settings:
Dutch (51.90% versus 53.60%), Haida
(24.00% versus 32.00%), Hungarian (16.00% versus 21.00%), Kurmanji (79.50% versus 82.80%),
Latvian (62.60% versus 64.20%),
Lithuanian
(19.80% versus 23.30%), Navajo (11.70% versus
19.00%) and Romanian (43.10% versus 44.80%).
Additionally, there is one language in the medium
setting where the RNN does not achieve the baseline, namely Danish (76.70% versus 78.10%) and
another one in the high setting, namely Quechua
(90.30 versus 95.40).
For Task2, the RNN system fails to achieve the
baseline accuracy for most languages and settings.
Discussion and Conclusions
The experiments clearly demonstrate that the system presented in this paper delivers substantial improvements in accuracy over a non-neural baseline
for most of the 52 languages in the shared task and
in all data settings in Task 1. Due to data augmentation, it improves upon the baseline even in the
extreme low resource setting of a mere 100 training examples. In this setting, a conventional RNN
system will overﬁt the training data and, consequently, generalize poorly. Indeed, we found it impossible to train models for the low training data
setting without using data augmentation (all models delivered accuracies in the range 0-1%). In
Task 1, we did not apply copy symbols due to time
constraints. We estimate that this reduces accuracy
for the low setting by about 2%.
Even though our system achieves substantial
improvements over baseline in Task 1, there are
several languages which do not reach the performance of the baseline system in Task 2.
possible cause for this is overﬁtting due to insufﬁcient variation in the training set. A single
lemma occurs multiple times in the Task 2 training
data sets because training examples form complete
paradigms, which contain dozens (or even hundreds) of word forms. Additionally, the number
of unique lemmas in Task 2 training sets is substantially lower than the number of unique lemmas in Task 1 training sets of the same setting.
For example, the low setting Task 1 training data
for Finnish contains 100 unique lemmas, whereas
the Task 2 data set only contains 10 unique lemmas. Finally, time constraints prevented us from
training a model ensemble for Task 2. This would
probably have improved accuracy for several lan-
Lithuanian
Lower Sorbian
Macedonian
Northern Sami
Norwegian Bokmal
Norwegian Nynorsk
Portuguese
Scottish Gaelic
Serbo-Croatian
Table 1: Results from Task 1 and Task 2. RNN refers to the RNN Encoder-Decoder with data augmentation and weighted
voting presented in Section 4. Baseline refers to the non-neural baseline system presented in Cotterell et al. . RNN
accuracies which are greater than the baseline accuracy are shown in boldface.
The overall performance in Task 1 varies greatly
between languages especially in the low and
medium data settings. For example, the accuracy
for Basque in the low setting is 4.00%, whereas
the accuracy for Danish is 68.90%. One explaining factor may be the number of distinct morphological feature sets in the test data.
We found that there is a link between low accuracy and the number of distinct morphological
feature sets occurring in the test data in the low
training data setting, as is shown in Figure 5. A
larger number of distinct feature sets correlates
with lower accuracy. No such trend exists for the
high or medium setting. This can partly be explained by the number of unseen morphological
feature sets.
In languages with many different morphological feature sets, the test data may contain a large
amount of morphological feature sets which were
unseen in the low training data spanning 100 examples.
This seems to adversely impact accuracy even though the Encoder LSTM does not treat
morphological feature sets as atomic units (for example ”V;PRS;PCP”) but instead splits them into
separate symbols (”V”, ”PRS”, ”PCP”).
conclusion is supported by the results for Basque:
for the low setting, the system achieves accuracy
4%, whereas it achieves accuracy 100% for the
high training data setting. A mere 8% of the morphological feature sets in the Basque test data occur in the low training data of 100 examples. However, 99% of them occur in the high training data
containing 10,000 examples.
The present work employs a very na¨ıve form of
data augmentation. A new training example is created from an existing one by replacing the longest
common substring of the stem and word form with
a sequence of random characters from the training data. We also tried to use more sophisticated
language models for generating the examples. Interestingly, this failed to bring improvements. In
fact, it resulted in reduced performance. This may
be due to overﬁtting because the generated strings
too closely resemble existing training examples.
For eight languages (Dutch, Haida, Hungarian,
Kurmanji, Latvian, Lithuanian, Navajo and Romanian), the RNN system failed to reach the baseline
in the low training data setting. Except for Haida
and Navajo, the difference between the baseline
and the RNN system is, quite small (≤5%). The
Figure 5: Low accuracy on the dev data (for the low setting
in Task 1) trends downwards as the number of unique MSD
combinations in a language’s dev data increases. The red regression line shows the slope of this trend, with a 95% conﬁdence interval represented as the translucent shadow around
Haida test set is very small (100 examples). Therefore, random ﬂuctuations play a big role in the accuracy. For Navajo, the difference of 7.3%-points
is substantial. We conjecture that this happens because data augmentation is not effective in the case
of Navajo due to the short average length of the
longest common substrings (LCS) of Navajo lemmas and word forms. For example, the average
word lengths in the low training data for Navajo
and Danish are nearly the same: 9.9 and 9.6 characters, respectively. However, the average length
of the LCS of lemmas and word forms is a mere
2.9 characters for Navajo but it is 6.7 characters for
Danish. Therefore, generated examples for Navajo
will contain long substrings that occur in the original training data which may lead to overﬁtting.
In conclusion, we have demonstrated that an
RNN Encoder-Decoder system can be applied to
morphological reinﬂection even in a low resource
setting. We achieve substantial improvements over
a non-neural baseline in Task 1. However, the system performs poorly in Task 2 due to overﬁtting.
Improving performance for Task 2 remains future
work at the present time.
Acknowledgments
The third author has been partly sponsored by
DARPA I20 in the program Low Resource Languages for Emergent Incidents (LORELEI) issued
by DARPA/I20 under Contract No. HR0011-15-