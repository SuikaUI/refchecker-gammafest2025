Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1907–1917
Melbourne, Australia, July 15 - 20, 2018. c⃝2018 Association for Computational Linguistics
Harvesting Paragraph-Level Question-Answer Pairs
from Wikipedia
Xinya Du and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY, 14853, USA
{xdu, cardie}@cs.cornell.edu
We study the task of generating from
Wikipedia articles question-answer pairs
that cover content beyond a single sentence.
We propose a neural network
approach that incorporates coreference
knowledge via a novel gating mechanism.
Compared to models that only take into
account sentence-level information , we ﬁnd that the linguistic knowledge introduced by the coreference representation aids question generation signiﬁcantly, producing models
that outperform the current state-of-theart. We apply our system (composed of
an answer span extraction system and the
passage-level QG system) to the 10,000
top-ranking Wikipedia articles and create
a corpus of over one million questionanswer pairs. We also provide a qualitative analysis for this large-scale generated
corpus from Wikipedia.
Introduction
Recently, there has been a resurgence of work in
NLP on reading comprehension 
with the goal of developing systems that can answer questions about the content of a given passage or document. Large-scale QA datasets are indispensable for training expressive statistical models for this task and play a critical role in advancing the ﬁeld. And there have been a number of efforts in this direction. Miller et al. ,
for example, develop a dataset for open-domain
question answering; Rajpurkar et al. and
Joshi et al. do so for reading comprehension (RC); and Hill et al. and Hermann
Paragraph:
(1)Tesla was renowned for his achievements
and showmanship, eventually earning him a
reputation in popular culture as an archetypal
"mad scientist".
(2)His patents earned him a
considerable amount of money, much of which
was used to ﬁnance his own projects with varying degrees of success. (3)He lived most of his
life in a series of New York hotels, through his
retirement. (4)Tesla died on 7 January 1943. ...
Questions:
– What was Tesla’s reputation in popular culture?
mad scientist
– How did Tesla ﬁnance his work?
– Where did Tesla live for much of his life?
New York hotels
Figure 1: Example input from the fourth paragraph of a Wikipedia article on
Nikola Tesla,
along with the natural questions and their answers
from the SQuAD dataset.
We show in italics the set of mentions that refer to
Nikola Tesla — Tesla, him, his, he, etc.
et al. , for the related task of answering
cloze questions synthetic approaches are used.
The (semi-)synthetic datasets ) are large in size and cheap to obtain;
however, they do not share the same characteristics as explicit QA/RC questions . In comparison, high-quality crowdsourced
datasets are much smaller in size, and the annotation process is quite expensive because the labeled examples require expertise and careful design .
Thus, there is a need for methods that can automatically generate high-quality question-answer
pairs. Serban et al. propose the use of recurrent neural networks to generate QA pairs from
structured knowledge resources such as Freebase.
Their work relies on the existence of automatically
acquired KBs, which are known to have errors and
suffer from incompleteness. They are also nontrivial to obtain. In addition, the questions in the
resulting dataset are limited to queries regarding a
single fact (i.e., tuple) in the KB.
Motivated by the need for large scale QA
pairs and the limitations of recent work, we investigate methods that can automatically “harvest” (generate) question-answer pairs from raw
text/unstructured documents, such as Wikipediatype articles.
Recent work along these lines (see Section 2) has proposed
the use of attention-based recurrent neural models
trained on the crowdsourced SQuAD dataset for question generation.
While successful, the resulting QA pairs are based
on information from a single sentence.
As described in Du et al. , however, nearly 30%
of the questions in the human-generated questions
of SQuAD rely on information beyond a single
sentence. For example, in Figure 1, the second
and third questions require coreference information (i.e., recognizing that “His” in sentence 2 and
“He” in sentence 3 both corefer with “Tesla” in
sentence 1) to answer them.
Thus, our research studies methods for incorporating coreference information into the training of a question generation system. In particular, we propose gated Coreference knowledge for
Neural Question Generation (CorefNQG), a neural sequence model with a novel gating mechanism that leverages continuous representations of
coreference clusters — the set of mentions used
to refer to each entity — to better encode linguistic knowledge introduced by coreference, for
paragraph-level question generation.
In an evaluation using the SQuAD dataset, we
ﬁnd that CorefNQG enables better question generation. It outperforms signiﬁcantly the baseline
neural sequence models that encode information
from a single sentence, and a model that encodes
all preceding context and the input sentence itself.
When evaluated on only the portion of SQuAD
that requires coreference resolution, the gap between our system and the baseline systems is even
By applying our approach to the 10,000 topranking Wikipedia articles, we obtain a question answering/reading comprehension dataset
with over one million QA pairs; we provide a
qualitative analysis in Section 6.
The dataset
and the source code for the system are available at 
HarvestingQA.
Related Work
Question Generation
Since the work by Rus et al. , question generation (QG) has attracted interest from both the
NLP and NLG communities. Most early work in
QG employed rule-based approaches to transform
input text into questions, usually requiring the application of a sequence of well-designed general
rules or templates . Heilman and Smith introduced an overgenerate-and-rank approach: their
system generates a set of questions and then ranks
them to select the top candidates.
Apart from
generating questions from raw text, there has also
been research on question generation from symbolic representations .
With the recent development of deep representation learning and large QA datasets, there
has been research on recurrent neural network
based approaches for question generation. Serban et al. used the encoder-decoder framework to generate QA pairs from knowledge base
triples; Reddy et al. generated questions
from a knowledge graph; Du et al. studied
how to generate questions from sentences using
an attention-based sequence-to-sequence model
and investigated the effect of exploiting sentencevs. paragraph-level information. Du and Cardie
 proposed a hierarchical neural sentencelevel sequence tagging model for identifying
question-worthy sentences in a text passage. Finally, Duan et al. investigated how to use
question generation to help improve question answering systems on the sentence selection subtask.
In comparison to the related methods from
above that generate questions from raw text, our
method is different in its ability to take into account contextual information beyond the sentencelevel by introducing coreference knowledge.
Question Answering Datasets and
Recently there has been an increasing interest
in question answering with the creation of many
datasets. Most are built using crowdsourcing; they
are generally comprised of fewer than 100,000
QA pairs and are time-consuming to create. WebQuestions , for example, contains 5,810 questions crawled via the Google Suggest API and is designed for knowledge base QA
with answers restricted to Freebase entities. To
tackle the size issues associated with WebQuestions, Bordes et al. introduce SimpleQuestions, a dataset of 108,442 questions authored by
English speakers. SQuAD 
is a dataset for machine comprehension; it is created by showing a Wikipedia paragraph to human annotators and asking them to write questions
based on the paragraph. TriviaQA includes 95k question-answer authored by
trivia enthusiasts and corresponding evidence documents.
(Semi-)synthetic generated datasets are easier to
build to large-scale .
They usually come in the form
of cloze-style questions. For example, Hermann
et al. created over a million examples by
pairing CNN and Daily Mail news articles with
their summarized bullet points. Chen et al. 
showed that this dataset is quite noisy due to the
method of data creation and concluded that performance of QA systems on the dataset is almost
saturated.
Closest to our work is that of Serban et al.
They train a neural triple-to-sequence
model on SimpleQuestions, and apply their system to Freebase to produce a large collection of
human-like question-answer pairs.
Task Deﬁnition
Our goal is to harvest high quality questionanswer pairs from the paragraphs of an article
of interest.
In our task formulation, this consists of two steps:
candidate answer extraction and answer-speciﬁc question generation.
Given an input paragraph, we ﬁrst identify a
set of question-worthy candidate answers ans =
(ans1, ans2, ..., ansl), each a span of text as denoted in color in Figure 1. For each candidate answer ansi, we then aim to generate a question Q
— a sequence of tokens y1, ..., yN — based on the
sentence S that contains candidate ansi such that:
• Q asks about an aspect of ansi that is of potential interest to a human;
• Q might rely on information from sentences
that precede S in the paragraph.
Mathematically then,
Q = arg max
P (Q|S, C)
where P(Q|S, C)
n=1 P (yn|y<n, S, C)
where C is the set of sentences that precede S in
the paragraph.
Methodology
In this section, we introduce our framework for
harvesting the question-answer pairs.
As described above, it consists of the question generator
CorefNQG (Figure 2) and a candidate answer extraction module. During test/generation time, we
(1) run the answer extraction module on the input
text to obtain answers, and then (2) run the question generation module to obtain the corresponding questions.
Question Generation
As shown in Figure 2, our generator prepares
the feature-rich input embedding — a concatenation of (a) a reﬁned coreference position feature embedding, (b) an answer feature embedding,
and (c) a word embedding, each of which is described below. It then encodes the textual input
using an LSTM unit .
Finally, an attention-copy equipped
decoder is used to decode the question.
More speciﬁcally, given the input sentence S
(containing an answer span) and the preceding
context C, we ﬁrst run a coreference resolution
system to get the coref-clusters for S and C
and use them to create a coreference transformed
input sentence:
for each pronoun, we append
its most representative non-pronominal coreferent
mention. Speciﬁcally, we apply the simple feedforward network based mention-ranking model of
Clark and Manning to the concatenation
of C and S to get the coref-clusters for all entities in C and S.
The C&M model produces
a score/representation s for each mention pair
s(m1, m2) = Wmhm(m1, m2) + bm
Decoder LSTMs
Context Vector
Natural Question
What team did the Panthers defeat … ?
coref. gate vector
Panthers defeated
Arizona Cardinals …
answer feature
coref. position feature
mention-pair score
refined coref. position feature
coreference transformed sentence S’
Figure 2: The gated Coreference knowledge for Neural Question Generation (CorefNQG) Model.
ans. feature
coref. feature
Table 1: Example input sentence with coreference and answer position features. The corresponding gold
question is “What team did the Panthers defeat in the NFC championship game ?”
where Wm is a 1 × d weight matrix and b is the
bias. hm(m1, m2) is representation of the last hidden layer of the three layer feedforward neural network.
For each pronoun in S, we then heuristically
identify the most “representative” antecedent from
its coref-cluster. (Proper nouns are preferred.) We
append the new mention after the pronoun. For example, in Table 1, “the panthers” is the most representative mention in the coref-cluster for “they”.
The new sentence with the appended coreferent
mention is our coreference transformed input sentence S
′ (see Figure 2).
Coreference Position Feature Embedding
each token in S
′, we also maintain one position
feature fc = (c1, ..., cn), to denote pronouns (e.g.,
“they”) and antecedents (e.g., “the panthers”). We
use the BIO tagging scheme to label the associated spans in S
′. “B_ANT” denotes the start of an
antecedent span, tag “I_ANT” continues the antecedent span and tag “O” marks tokens that do
not form part of a mention span. Similarly, tags
“B_PRO” and “I_PRO” denote the pronoun span.
(See Table 1, “coref. feature”.)
Reﬁned Coref. Position Feature Embedding
mechanisms for controlling information ﬂow in neural networks , we propose to use a gating network here to obtain a reﬁned representation of the coreference position feature vectors
(c1, ..., cn).
The main idea is to utilize the mention-pair score (see Equation 2) to
help the neural network learn the importance of
the coreferent phrases. We compute the reﬁned
(gated) coreference position feature vector fd =
(d1, ..., dn) as follows,
gi = ReLU(Waci + Wbscorei + b)
di = gi ⊙ci
where ⊙denotes an element-wise product between two vectors and ReLU is the rectiﬁed linear
activation function. scorei denotes the mentionpair score for each antecedent token (e.g., “the”
and “panthers”) with the pronoun (e.g., “they”);
scorei is obtained from the trained model (Equation 2) of the C&M. If token i is not added later
as an antecedent token, scorei is set to zero. Wa,
Wb are weight matrices and b is the bias vector.
Answer Feature Embedding
We also include
an answer position feature embedding to generate answer-speciﬁc questions; we denote the answer span with the usual BIO tagging scheme (see,
e.g., “the arizona cardinals” in Table 1). During
training and testing, the answer span feature (i.e.,
“B_ANS”, “I_ANS” or “O”) is mapped to its feature embedding space: fa = (a1, ..., an).
Word Embedding
To obtain the word embedding for the tokens themselves, we just map
the tokens to the word embedding space: x =
(x1, ..., xn).
Final Encoder Input
As noted above, the ﬁnal input to the LSTM-based encoder is a concatenation of (1) the reﬁned coreference position feature embedding (light blue units in Figure 2), (2)
the answer position feature embedding (red units),
and (3) the word embedding for the token (green
ei = concat(di, ai, xi)
As for the encoder itself, we use bidirectional LSTMs to read the input e = (e1, ..., en)
in both the forward and backward directions. After encoding, we obtain two sequences of hidden vectors, namely, −→
h1, ..., −→
hn) and ←−
h1, ..., ←−
hn). The ﬁnal output state of the encoder
is the concatenation of −→
hi = concat as the decoder for generating the question y1, ..., ym one
token at a time. To deal with rare/unknown words,
the decoder also allows directly copying words
from the source sentence via pointing .
At each time step t, the decoder LSTM reads
the previous word embedding wt−1 and previous
hidden state st−1 to compute the new hidden state,
st = LSTM(wt−1, st−1)
Then we calculate the attention distribution αt as
in Bahdanau et al. ,
αt = softmax(et)
where Wc is a weight matrix and attention distribution αt is a probability distribution over the
source sentence words. With αt, we can obtain
the context vector h∗
Then, using the context vector h∗
t and hidden
state st, the probability distribution over the target
(question) side vocabulary is calculated as,
Pvocab = softmax(Wdconcat(h∗
training/generating with the ﬁxed target side vocabulary, we also consider copying from the source
sentence. The copy probability is based on the
context vector h∗
t and hidden state st,
and the probability distribution over the source
sentence words is the sum of the attention scores
of the corresponding words,
Pcopy(w) =
t ∗1{w == wi}
Finally, we obtain the probability distribution over
the dynamic vocabulary (i.e., union of original target side and source sentence vocabulary) by summing over Pcopy and Pvocab,
P(w) = λcopy
Pcopy(w) + (1 −λcopy
)Pvocab(w)
where σ is the sigmoid function, and Wd, We,
Wf are weight matrices.
Answer Span Identiﬁcation
We frame the problem of identifying candidate answer spans from a paragraph as a sequence labeling task and base our model on the BiLSTM-CRF
approach for named entity recognition . Given a paragraph of n tokens, instead of directly feeding the sequence of word
vectors x = (x1, ..., xn) to the LSTM units, we
ﬁrst construct the feature-rich embedding x
each token, which is the concatenation of the word
embedding, an NER feature embedding, and a
character-level representation of the word . We use the concatenated vector
as the “ﬁnal” embedding x
′ for the token,
i = concat(xi, CharRepi, NERi)
where CharRepi is the concatenation of the last
hidden states of a character-based biLSTM. The
intuition behind the use of NER features is that
SQuAD answer spans contain a large number of
named entities, numeric phrases, etc.
Then a multi-layer Bi-directional LSTM is applied to (x
n) and we obtain the output state
zt for time step t by concatenation of the hidden states (forward and backward) at time step
t from the last layer of the BiLSTM. We apply
the softmax to (z1, ..., zn) to get the normalized
score representation for each token, which is of
size n × k, where k is the number of tags.
Instead of using a softmax training objective
that minimizes the cross-entropy loss for each
individual word, the model is trained with a
CRF objective, which minimizes the negative log-likelihood for the entire
correct sequence: −log(py),
y′∈Y′ exp(q(x
′, y) = Pn
t=1 Pt,yt + Pn−1
t=0 Ayt,yt+1,
Pt,yt is the score of assigning tag yt to the tth token, and Ayt,yt+1 is the transition score from tag
yt to yt+1, the scoring matrix A is to be learned.
′ represents all the possible tagging sequences.
Experiments
We use the SQuAD dataset 
to train our models. It is one of the largest general purpose QA datasets derived from Wikipedia
with over 100k questions posed by crowdworkers on a set of Wikipedia articles. The answer to
each question is a segment of text from the corresponding Wiki passage. The crowdworkers were
users of Amazon’s Mechanical Turk located in the
US or Canada. To obtain high-quality articles, the
authors sampled 500 articles from the top 10,000
articles obtained by Nayuki’s Wikipedia’s internal PageRanks. The question-answer pairs were
generated by annotators from a paragraph; and
although the dataset is typically used to evaluate
reading comprehension, it has also been used in an
open domain QA setting . For training/testing answer extraction systems, we pair each paragraph in the dataset
with the gold answer spans that it contains. For the
question generation system, we pair each sentence
that contains an answer span with the corresponding gold question as in Du et al. .
To quantify the effect of using predicted (rather
than gold standard) answer spans on question generation (e.g., predicted answer span boundaries
can be inaccurate), we also train the models on
an augmented “Training set w/ noisy examples”
(see Table 2). This training set contains all of the
original training examples plus new examples for
predicted answer spans (from the top-performing
answer extraction model, bottom row of Table 3)
that overlap with a gold answer span. We pair the
new training sentence (w/ predicted answer span)
with the gold question. The added examples comprise 42.21% of the noisy example training set.
For generation of our one million QA pair corpus, we apply our systems to the 10,000 topranking articles of Wikipedia.
Evaluation Metrics
generation
evaluation,
TEOR .1
measures average n-gram precision vs. a set of
reference questions and penalizes for overly short
sentences. METEOR is a recall-oriented metric
that takes into account synonyms, stemming, and
paraphrases.
For answer candidate extraction evaluation, we
use precision, recall and F-measure vs. the gold
standard SQuAD answers. Since answer boundaries are sometimes ambiguous, we compute Binary Overlap and Proportional Overlap metrics in
addition to Exact Match. Binary Overlap counts
every predicted answer that overlaps with a gold
answer span as correct, and Proportional Overlap
give partial credit proportional to the amount of
overlap .
Baselines and Ablation Tests
For question generation, we compare to the stateof-the-art baselines and conduct ablation tests
as follows: Du et al. ’s model is an
attention-based RNN sequence-to-sequence neural network (without using the answer location information feature). Seq2seq + copyw/ answer is the
attention-based sequence-to-sequence model augmented with a copy mechanism, with answer features concatenated with the word embeddings during encoding. Seq2seq + copyw/ full context + answer
is the same model as the previous one, but we allow access to the full context (i.e., all the preceding sentences and the input sentence itself). We
denote it as ContextNQG henceforth for simplicity. CorefNQG is the coreference-based model
proposed in this paper. CorefNQG–gating is an
1We use the evaluation scripts of Du et al. .
Training set
Training set w/ noisy examples
Baseline (w/o answer)
Seq2seq + copy (w/ answer)
ContextNQG: Seq2seq + copy
(w/ full context + answer)
- mention-pair score
Table 2: Evaluation results for question generation.
BiLSTM w/ NER
BiLSTM-CRF w/ char
BiLSTM-CRF w/ char w/ NER
Table 3: Evaluation results of answer extraction systems.
ablation test, the gating network is removed and
the coreference position embedding is not reﬁned.
CorefNQG–mention-pair score is also an ablation test where all mention-pair scorei are set to
For answer span extraction, we conduct experiments to compare the performance of an off-theshelf NER system and BiLSTM based systems.
For training and implementation details,
please see the Supplementary Material.
Results and Analysis
Automatic Evaluation
Table 2 shows the BLEU-{3, 4} and METEOR
scores of different models. Our CorefNQG outperforms the seq2seq baseline of Du et al. 
by a large margin.
This shows that the copy
mechanism, answer features and coreference resolution all aid question generation.
In addition, CorefNQG outperforms both Seq2seq+Copy
models signiﬁcantly, whether or not they have access to the full context. This demonstrates that
the coreference knowledge encoded with the gating network explicitly helps with the training and
generation: it is more difﬁcult for the neural sequence model to learn the coreference knowledge
in a latent way. (See input 1 in Figure 3 for an example.) Building end-to-end models that take into
account coreference knowledge in a latent way is
an interesting direction to explore. In the ablation
tests, the performance drop of CorefNQG–gating
Seq2seq + copy
ContextNQG
Table 4: Evaluation results for question generation
on the portion that requires coreference knowledge
(36.42% examples of the original test set).
shows that the gating network is playing an important role for getting reﬁned coreference position
feature embedding, which helps the model learn
the importance of an antecedent. The performance
drop of CorefNQG–mention-pair score shows the
mention-pair score introduced from the external
system helps the neural network better encode coreference knowledge.
To better understand the effect of coreference
resolution, we also evaluate our model and the
baseline models on just that portion of the test set
that requires pronoun resolution (36.42% of the
examples) and show the results in Table 4. The
gaps of performance between our model and the
baseline models are still signiﬁcant. Besides, we
see that all three systems’ performance drop on
this partial test set, which demonstrates the hardness of generating questions for the cases that require pronoun resolution (passage context).
We also show in Table 2 the results of the
QG models trained on the training set augmented
with noisy examples with predicted answer spans.
Input 1: The elizabethan navigator, sir francis drake
was born in the nearby town of tavistock and was the
mayor of plymouth. ... . ::
died::of::::::::
dysentery::
Human: In what year did Sir Francis Drake die ?
ContextNQG: When did he die ?
CorefNQG: When did sir francis drake die ?
Input 2: american idol is an american singing competition ... .
began:::::
on:june 11 , 2002,
addition::to:::
idols::::::
format :::::
since:::::::
become :::
one::of:::
successful:::::
shows::in:::
history::of:::::::
american::::::::
television.
Human: When did american idol ﬁrst air on tv ?
ContextNQG: When did fox begin airing ?
CorefNQG: When did american idol begin airing ?
Input 3: ... the a38 dual-carriageway runs from east
to west across the north of the city .
within :::
:it::is::::::::
designated::as::‘ the parkway :’:::
and::::::::
represents:::
boundary ::::::
between:::
generally ::::
recent :::::::
suburban ::::
Human: What is the a38 called inside the city ?
ContextNQG: What is another name for the city ?
CorefNQG: What is the city designated as ?
Figure 3: Example questions (with answers highlighted) generated by human annotators (ground
truth questions), by our system CorefNQG, and by
the Seq2seq+Copy model trained with full context
(i.e., ContextNQG).
There is a consistent but acceptable drop for each
model on this new training set, given the inaccuracy of predicted answer spans.
We see that
CorefNQG still outperforms the baseline models
across all metrics.
Figure 3 provides sample output for input sentences that require contextual coreference knowledge. We see that ContextNQG fails in all cases;
our model misses only the third example due to an
error introduced by coreference resolution — the
“city” and “it” are considered coreferent. We can
also see that human-generated questions are more
natural and varied in form with better paraphrasing.
In Table 3, we show the evaluation results for
different answer extraction models. First we see
that all variants of BiLSTM models outperform
the off-the-shelf NER system (that proposes all
NEs as answer spans), though the NER system has
a higher recall. The BiLSTM-CRF that encodes
the character-level and NER features for each token performs best in terms of F-measure.
Human Study
We hired four native speakers of English to rate
the systems’ outputs. Detailed guidelines for the
raters are listed in the supplementary materials.
Grammaticality
Making Sense
Answerability
ContextNQG
Table 5: Human evaluation results for question
generation. “Grammaticality”, “Making Sense” and “Answerability” are rated on a 1–5 scale (5 for the best, see the
supplementary materials for a detailed rating scheme), “Average rank” is rated on a 1–3 scale (1 for the most preferred,
ties are allowed.) Two-tailed t-test results are shown for our
method compared to ContextNQG (stat. signiﬁcance is indicated with ∗(p < 0.05), ∗∗(p < 0.01).)
The evaluation can also be seen as a measure of the
quality of the generated dataset (Section 6.3). We
randomly sampled 11 passages/paragraphs from
the test set; there are in total around 70 questionanswer pairs for evaluation.
We consider three metrics — “grammaticality”,
“making sense” and “answerability”. The evaluators are asked to ﬁrst rate the grammatical correctness of the generated question (before being
shown the associated input sentence or any other
textual context). Next, we ask them to rate the degree to which the question “makes sense” given
the input sentence (i.e., without considering the
correctness of the answer span). Finally, evaluators rate the “answerability” of the question given
the full context.
Table 5 shows the results of the human evaluation. Bold indicates top scores. We see that the
original human questions are preferred over the
two NQG systems’ outputs, which is understandable given the examples in Figure 3. The humangenerated questions make more sense and correspond better with the provided answers, particularly when they require information in the preceding context. How exactly to capture the preceding
context so as to ask better and more diverse questions is an interesting future direction for research.
In terms of grammaticality, however, the neural
models do quite well, achieving very close to human performance.
In addition, we see that our
method (CorefNQG) performs statistically significantly better across all metrics in comparison to
the baseline model (ContextNQG), which has access to the entire preceding context in the passage.
The Generated Corpus
Our system generates in total 1,259,691 questionanswer pairs, nearly 126 questions per article. Figure 5 shows the distribution of different types of
Exact Match
DocReader 
Table 6: Performance of the neural machine reading comprehension model (no initialization with
pretrained embeddings) on our generated corpus.
The United States of America (USA), commonly referred to as the United States (U.S.) or America, is a
federal republic composed of states, a federal district,
ﬁve major self-governing territories, and various possessions. ... . The territories are scattered about the
Paciﬁc Ocean and the Caribbean Sea. Nine time zones
are covered. The geography, climate and wildlife of the
country are extremely diverse.
Q1: What is another name for the united states of america ?
Q2: How many major territories are in the united states?
Q3: What are the territories scattered about ?
Figure 4: Example question-answer pairs from our
generated corpus.
questions in our dataset vs. the SQuAD training
set. We see that the distribution for “In what”,
“When”, “How long”, “Who”, “Where”, “What
does” and “What do” questions in the two datasets
is similar.
Our system generates more “What
is”, “What was” and “What percentage” questions,
while the proportions of “What did”, “Why” and
“Which” questions in SQuAD are larger than ours.
One possible reason is that the “Why”, “What did”
questions are more complicated to ask (sometimes
involving world knowledge) and the answer spans
are longer phrases of various types that are harder
to identify. “What is” and “What was” questions,
on the other hand, are often safer for the neural
networks systems to ask.
In Figure 4, we show some examples of the generated question-answer pairs. The answer extractor identiﬁes the answer span boundary well and
all three questions correspond to their answers. Q2
is valid but not entirely accurate. For more examples, please refer to our supplementary materials.
Table 6 shows the performance of a topperforming system for the SQuAD dataset ) when applied
to the development and test set portions of our
generated dataset. The system was trained on the
training set portion of our dataset.
We use the
SQuAD evaluation scripts, which calculate exact
match (EM) and F-1 scores.2 Performance of the
2F-1 measures the average overlap between the predicted
answer span and ground truth answer .
what percentage
Our corpus
Figure 5: Distribution of question types of our corpus and SQuAD training set. The categories are
the ones used in Wang et al. , we add one
more category: “what percentage”.
neural machine reading model is reasonable. We
also train the DocReader on our training set and
test the models’ performance on the original dev
set of SQuAD; for this, the performance is around
45.2% on EM and 56.7% on F-1 metric. DocReader trained on the original SQuAD training set
achieves 69.5% EM, 78.8% F-1 indicating that our
dataset is more difﬁcult and/or less natural than the
crowd-sourced QA pairs of SQuAD.
Conclusion
We propose a new neural network model for better
encoding coreference knowledge for paragraphlevel question generation. Evaluations with different metrics on the SQuAD machine reading
dataset show that our model outperforms state-ofthe-art baselines. The ablation study shows the effectiveness of different components in our model.
Finally, we apply our question generation framework to produce a corpus of 1.26 million questionanswer pairs, which we hope will beneﬁt the QA
research community. It would also be interesting
to apply our approach to incorporating coreference
knowledge to other text generation tasks.
Acknowledgments
We thank the anonymous reviewers and members
of Cornell NLP group for helpful comments.