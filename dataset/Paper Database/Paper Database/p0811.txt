Flamingo: a Visual Language Model
for Few-Shot Learning
Jean-Baptiste Alayrac*,‚Ä°
Jeff Donahue*
Pauline Luc*
Antoine Miech*
Iain Barr‚Ä†
Yana Hasson‚Ä†
Karel Lenc‚Ä†
Arthur Mensch‚Ä†
Katie Millican‚Ä†
Malcolm Reynolds‚Ä†
Roman Ring‚Ä†
Eliza Rutherford‚Ä†
Serkan Cabi
Tengda Han
Zhitao Gong
Sina Samangooei
Marianne Monteiro
Jacob Menick
Sebastian Borgeaud
Andrew Brock
Aida Nematzadeh
Sahand Sharifzadeh
Mikolaj Binkowski
Ricardo Barreira
Oriol Vinyals
Andrew Zisserman
Karen Simonyan*,‚Ä°
* Equal contributions, ordered alphabetically, ‚Ä† Equal contributions, ordered alphabetically,
‚Ä° Equal senior contributions
Building models that can be rapidly adapted to novel tasks using only a handful of
annotated examples is an open challenge for multimodal machine learning research.
We introduce Flamingo, a family of Visual Language Models (VLM) with this
ability. We propose key architectural innovations to: (i) bridge powerful pretrained
vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as
inputs. Thanks to their Ô¨Çexibility, Flamingo models can be trained on large-scale
multimodal web corpora containing arbitrarily interleaved text and images, which
is key to endow them with in-context few-shot learning capabilities. We perform a
thorough evaluation of our models, exploring and measuring their ability to rapidly
adapt to a variety of image and video tasks. These include open-ended tasks such
as visual question-answering, where the model is prompted with a question which
it has to answer; captioning tasks, which evaluate the ability to describe a scene or
an event; and close-ended tasks such as multiple-choice visual question-answering.
For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a
new state of the art with few-shot learning, simply by prompting the model with
task-speciÔ¨Åc examples. On numerous benchmarks, Flamingo outperforms models
Ô¨Åne-tuned on thousands of times more task-speciÔ¨Åc data.
Corresponding authors: {jalayrac|jeffdonahue|paulineluc|miech}@deepmind.com
36th Conference on Neural Information Processing Systems .
 
Input Prompt
Completion
chinchilla. They
are mainly found
This is a shiba.
They are very
popular in Japan.
a Ô¨Çamingo.
They are found
Caribbean and
South America.
What is the title
of this painting?
Answer: The
Hallucinogenic
Where is this
displayed?
Answer: Louvres
Museum, Paris.
What is the name
of the city where
this was painted?
"Underground"
"Congress"
"Soulomes"
propaganda
poster depicting a
cat dressed as
French emperor
holding a piece
of cheese.
Output: A pink
room with a
Ô¨Çamingo pool
A portrait of
Salvador Dali
with a robot
Les sanglots
longs des violons
de l‚Äôautomne
blessent mon
coeur d‚Äôune
Pour qui sont ces
serpents qui
sifÔ¨Çent sur vos
Je suis un c≈ìur
qui bat pour
giraffes: 4
I like reading
, my favourite
play is Hamlet. I
, my favorite
Dreams from
my Father.
What happens to
the man after
hitting the ball?
he falls down.
This is a picture of two teddy
bears on the moon.
What are they doing?
conversation.
What object are they using?
It looks like a computer.
Is this surprising?
Yes, it is surprising.
Why is this picture surprising
I think it is surprising because
teddy bears are not usually
found on the moon.
What is the common thing
about these three images?
They are all Ô¨Çamingos.
What is the difference between these three images?
The Ô¨Årst one is a cartoon, the
second one is a real Ô¨Çamingo,
and the third one is a 3D
model of a Ô¨Çamingo.
This is an apple with a sticker
What does the sticker say?
The sticker says "iPod".
Where is the photo taken?
It looks like it‚Äôs taken in a
Do you think it is printed or
handwritten?
It looks like it‚Äôs handwritten.
What color is the sticker?
It‚Äôs white.
This is a cityscape. It looks
like Chicago.
What makes you think this is
I think it‚Äôs Chicago because
of the Shedd Aquarium in the
background.
What about this one? Which
city is this and what famous
landmark helped you recognise the city?
This is Tokyo.
I think it‚Äôs
Tokyo because of the Tokyo
Figure 1: Selected examples of inputs and outputs obtained from Flamingo-80B. Flamingo can
rapidly adapt to various image/video understanding tasks with few-shot prompting (top). Out of
the box, Flamingo is also capable of multi-image visual dialogue (bottom). More examples in
Appendix C.
Performance relative to FT SOTA
Flamingo (80B)
zero/few-shot SotA
Number of shots
Aggregated performance
Flamingo-80B
Flamingo-9B
Flamingo 3B
Figure 2: Flamingo results overview. Left: Our largest model, dubbed Flamingo, outperforms
state-of-the-art Ô¨Åne-tuned models on 6 of the 16 tasks we consider with no Ô¨Åne-tuning. For the 9
tasks with published few-shot results, Flamingo sets the new few-shot state of the art. Note: We omit
RareAct, our 16th benchmark, as it is a zero-shot benchmark with no available Ô¨Åne-tuned results to
compare to. Right: Flamingo performance improves with model size and number of shots.
Introduction
One key aspect of intelligence is the ability to quickly learn to perform a new task given a short
instruction . While initial progress has been made towards a similar capability in computer
vision, the most widely used paradigm still consists of Ô¨Årst pretraining on a large amount of supervised
data, before Ô¨Åne-tuning the model on the task of interest . However, successful Ô¨Ånetuning often requires many thousands of annotated data points. In addition, it often requires careful
per-task hyperparameter tuning and is also resource intensive. Recently, multimodal vision-language
models trained with a contrastive objective have enabled zero-shot adaptation to novel tasks,
without the need for Ô¨Åne-tuning. However, because these models simply provide a similarity score
between a text and an image, they can only address limited use cases such as classiÔ¨Åcation, where a
Ô¨Ånite set of outcomes is provided beforehand. They crucially lack the ability to generate language,
which makes them less suitable to more open-ended tasks such as captioning or visual questionanswering. Others have explored visually-conditioned language generation 
but have not yet shown good performance in low-data regimes.
We introduce Flamingo, a Visual Language Model (VLM) that sets a new state of the art in few-shot
learning on a wide range of open-ended vision and language tasks, simply by being prompted with a
few input/output examples, as illustrated in Figure 1. Of the 16 tasks we consider, Flamingo also
surpasses the Ô¨Åne-tuned state of the art on 6 tasks, despite using orders of magnitude less task-speciÔ¨Åc
training data (see Figure 2). To achieve this, Flamingo takes inspiration from recent work on large
language models (LMs) which are good few-shot learners . A single large LM can
achieve strong performance on many tasks using only its text interface: a few examples of a task are
provided to the model as a prompt, along with a query input, and the model generates a continuation
to produce a predicted output for that query. We show that the same can be done for image and
video understanding tasks such as classiÔ¨Åcation, captioning, or question-answering: these can be
cast as text prediction problems with visual input conditioning. The difference from a LM is that
the model must be able to ingest a multimodal prompt containing images and/or videos interleaved
with text. Flamingo models have this capability‚Äîthey are visually-conditioned autoregressive text
generation models able to ingest a sequence of text tokens interleaved with images and/or videos,
and produce text as output. Flamingo models leverage two complementary pre-trained and frozen
models: a vision model which can ‚Äúperceive‚Äù visual scenes and a large LM which performs a basic
form of reasoning. Novel architecture components are added in between these models to connect
them in a way that preserves the knowledge they have accumulated during computationally intensive
pre-training. Flamingo models are also able to ingest high-resolution images or videos thanks to
a Perceiver-based architecture that can produce a small Ô¨Åxed number of visual tokens per
image/video, given a large and variable number of visual input features.
A crucial aspect for the performance of large LMs is that they are trained on a large amount of
text data. This training provides general-purpose generation capabilities that allows these LMs to
perform well when prompted with task examples. Similarly, we demonstrate that the way we train
the Flamingo models is crucial for their Ô¨Ånal performance. They are trained on a carefully chosen
1st LM block
a very serious cat.
Pretrained and frozen
1st GATED XATTN-DENSE
Interleaved visual/text data
This is a very cute dog. This is
Trained from scratch
n-th GATED XATTN-DENSE
n-th LM block
Output: text
<image> This is a very cute dog.<image> This is
Processed text
Figure 3: Flamingo architecture overview. Flamingo is a family of visual language models (VLMs)
that take as input visual data interleaved with text and produce free-form text as output.
mixture of complementary large-scale multimodal data coming only from the web, without using any
data annotated for machine learning purposes. After this training, a Flamingo model can be directly
adapted to vision tasks via simple few-shot learning without any task-speciÔ¨Åc tuning.
Contributions. In summary, our contributions are the following: (i) We introduce the Flamingo
family of VLMs which can perform various multimodal tasks (such as captioning, visual dialogue,
or visual question-answering) from only a few input/output examples. Thanks to architectural
innovations, the Flamingo models can efÔ¨Åciently accept arbitrarily interleaved visual data and text
as input and generate text in an open-ended manner. (ii) We quantitatively evaluate how Flamingo
models can be adapted to various tasks via few-shot learning. We notably reserve a large set of heldout benchmarks which have not been used for validation of any design decisions or hyperparameters
of the approach. We use these to estimate unbiased few-shot performance. (iii) Flamingo sets a new
state of the art in few-shot learning on a wide array of 16 multimodal language and image/video
understanding tasks. On 6 of these 16 tasks, Flamingo also outperforms the Ô¨Åne-tuned state of the
art despite using only 32 task-speciÔ¨Åc examples, around 1000 times less task-speciÔ¨Åc training data
than the current state of the art. With a larger annotation budget, Flamingo can also be effectively
Ô¨Åne-tuned to set a new state of the art on Ô¨Åve additional challenging benchmarks: VQAv2, VATEX,
VizWiz, MSRVTTQA, and HatefulMemes.
This section describes Flamingo: a visual language model that accepts text interleaved with images/videos as input and outputs free-form text. The key architectural components shown in Figure 3
are chosen to leverage pretrained vision and language models and bridge them effectively. First,
the Perceiver Resampler (Section 2.1) receives spatio-temporal features from the Vision Encoder
(obtained from either an image or a video) and outputs a Ô¨Åxed number of visual tokens. Second,
these visual tokens are used to condition the frozen LM using freshly initialised cross-attention
layers (Section 2.2) that are interleaved between the pretrained LM layers. These new layers offer
an expressive way for the LM to incorporate visual information for the next-token prediction task.
Flamingo models the likelihood of text ùë¶conditioned on interleaved images and videos ùë•as follows:
ùëù(ùë¶‚Ñì|ùë¶<‚Ñì, ùë•‚â§‚Ñì),
where ùë¶‚Ñìis the ‚Ñì-th language token of the input text, ùë¶<‚Ñìis the set of preceding tokens, ùë•‚â§‚Ñìis the set
of images/videos preceding token ùë¶‚Ñìin the interleaved sequence and ùëùis parametrized by a Flamingo
model. The ability to handle interleaved text and visual sequences (Section 2.3) makes it natural
to use Flamingo models for in-context few-shot learning, analogously to GPT-3 with few-shot text
prompting. The model is trained on a diverse mixture of datasets as described in Section 2.4.
self attention
tanh gating
tanh gating
GATED XATTN-DENSE
cross attention
def gated_xattn_dense(
y, # input language features
x, # input visual features
alpha_xattn, # xattn gating parameter ‚Äì init at 0.
alpha_dense, # ffw gating parameter ‚Äì init at 0.
"""Applies a GATED XATTN-DENSE layer."""
# 1. Gated Cross Attention
y = y + tanh(alpha_xattn) * attention(q=y, kv=x)
# 2. Gated Feed Forward (dense) Layer
y = y + tanh(alpha_dense) * ffw(y)
# Regular self-attention + FFW on language
y = y + frozen_attention(q=y, kv=y)
y = y + frozen_ffw(y)
return y # output visually informed language features
GATED XATTN-DENSE layers. To condition the LM on visual inputs, we insert new
cross-attention layers between existing pretrained and frozen LM layers. The keys and values in these
layers are obtained from the vision features while the queries are derived from the language inputs.
They are followed by dense feed-forward layers. These layers are gated so that the LM is kept intact
at initialization for improved stability and performance.
Visual processing and the Perceiver Resampler
Vision Encoder: from pixels to features. Our vision encoder is a pretrained and frozen Normalizer-
Free ResNet (NFNet) ‚Äì we use the F6 model. We pretrain the vision encoder using a contrastive
objective on our datasets of image and text pairs, using the two-term contrastive loss from Radford
et al. . We use the output of the Ô¨Ånal stage, a 2D spatial grid of features that is Ô¨Çattened to a 1D
sequence. For video inputs, frames are sampled at 1 FPS and encoded independently to obtain a 3D
spatio-temporal grid of features to which learned temporal embeddings are added. Features are then
Ô¨Çattened to 1D before being fed to the Perceiver Resampler. More details on the contrastive model
training and performance are given in Appendix B.1.3 and Appendix B.3.2, respectively.
Perceiver Resampler: from varying-size large feature maps to few visual tokens. This module
connects the vision encoder to the frozen language model as shown in Figure 3. It takes as input a
variable number of image or video features from the vision encoder and produces a Ô¨Åxed number of
visual outputs (64), reducing the computational complexity of the vision-text cross-attention. Similar
to Perceiver and DETR , we learn a predeÔ¨Åned number of latent input queries which are fed
to a Transformer and cross-attend to the visual features. We show in our ablation studies (Section 3.3)
that using such a vision-language resampler module outperforms a plain Transformer and an MLP.
We provide an illustration, more architectural details, and pseudo-code in Appendix A.1.1.
Conditioning frozen language models on visual representations
Text generation is performed by a Transformer decoder, conditioned on the visual representations
produced by the Perceiver Resampler. We interleave pretrained and frozen text-only LM blocks with
blocks trained from scratch that cross-attend to the visual output from the Perceiver Resampler.
Interleaving new GATED XATTN-DENSE layers within a frozen pretrained LM. We freeze the
pretrained LM blocks, and insert gated cross-attention dense blocks (Figure 4) between the original
layers, trained from scratch. To ensure that at initialization, the conditioned model yields the same
results as the original language model, we use a tanh-gating mechanism . This multiplies the
output of a newly added layer by tanh(ùõº) before adding it to the input representation from the residual
connection, where ùõºis a layer-speciÔ¨Åc learnable scalar initialized to 0 . Thus, at initialization, the
model output matches that of the pretrained LM, improving training stability and Ô¨Ånal performance.
In our ablation studies (Section 3.3), we compare the proposed GATED XATTN-DENSE layers against
recent alternatives and explore the effect of how frequently these additional layers are inserted
to trade off between efÔ¨Åciency and expressivity. See Appendix A.1.2 for more details.
Varying model sizes. We perform experiments across three models sizes, building on the 1.4B, 7B,
and 70B parameter Chinchilla models ; calling them respectively Flamingo-3B, Flamingo-9B and
Flamingo-80B. For brevity, we refer to the last as Flamingo throughout the paper. While increasing
the parameter count of the frozen LM and the trainable vision-text GATED XATTN-DENSE modules,
we maintain a Ô¨Åxed-size frozen vision encoder and trainable Perceiver Resampler across the different
models (small relative to the full model size). See Appendix B.1.1 for further details.
Multi-visual input support: per-image/video attention masking
The image-causal modelling introduced in Equation (1) is obtained by masking the full text-to-image
cross-attention matrix, limiting which visual tokens the model sees at each text token. At a given text
token, the model attends to the visual tokens of the image that appeared just before it in the interleaved
sequence, rather than to all previous images (formalized and illustrated in Appendix A.1.3). Though
the model only directly attends to a single image at a time, the dependency on all previous images
remains via self-attention in the LM. This single-image cross-attention scheme importantly allows
the model to seamlessly generalise to any number of visual inputs, regardless of how many are
used during training. In particular, we use only up to 5 images per sequence when training on our
interleaved datasets, yet our model is able to beneÔ¨Åt from sequences of up to 32 pairs (or ‚Äúshots‚Äù) of
images/videos and corresponding texts during evaluation. We show in Section 3.3 that this scheme is
more effective than allowing the model to cross-attend to all previous images directly.
Training on a mixture of vision and language datasets
We train the Flamingo models on a mixture of three kinds of datasets, all scraped from the web: an
interleaved image and text dataset derived from webpages, image-text pairs, and video-text pairs.
M3W: Interleaved image and text dataset. The few-shot capabilities of Flamingo models rely on
training on interleaved text and image data. For this purpose, we collect the MultiModal MassiveWeb
(M3W) dataset. We extract both text and images from the HTML of approximately 43 million
webpages, determining the positions of images relative to the text based on the relative positions of
the text and image elements in the Document Object Model (DOM). An example is then constructed
by inserting <image> tags in plain text at the locations of the images on the page, and inserting a
special <EOC> (end of chunk) token (added to the vocabulary and learnt) prior to any image and at the
end of the document. From each document, we sample a random subsequence of ùêø= 256 tokens and
take up to the Ô¨Årst ùëÅ= 5 images included in the sampled sequence. Further images are discarded in
order to save compute. More details are provided in Appendix A.3.
Pairs of image/video and text. For our image and text pairs we Ô¨Årst leverage the ALIGN 
dataset, composed of 1.8 billion images paired with alt-text. To complement this dataset, we collect
our own dataset of image and text pairs targeting better quality and longer descriptions: LTIP (Long
Text & Image Pairs) which consists of 312 million image and text pairs. We also collect a similar
dataset but with videos instead of still images: VTP (Video & Text Pairs) consists of 27 million short
videos (approximately 22 seconds on average) paired with sentence descriptions. We align the syntax
of paired datasets with the syntax of M3W by prepending <image> and appending <EOC> to each
training caption (see Appendix A.3.3 for details).
Multi-objective training and optimisation strategy. We train our models by minimizing a weighted
sum of per-dataset expected negative log-likelihoods of text, given the visual inputs:
ùúÜùëö¬∑ E(ùë•,ùë¶)‚àºùíüùëö
log ùëù(ùë¶‚Ñì|ùë¶<‚Ñì, ùë•‚â§‚Ñì)
where ùíüùëöand ùúÜùëöare the ùëö-th dataset and its weighting, respectively. Tuning the per-dataset weights
ùúÜùëöis key to performance. We accumulate gradients over all datasets, which we found outperforms a
‚Äúround-robin‚Äù approach . We provide further training details and ablations in Appendix B.1.2.
Task adaptation with few-shot in-context learning
Once Flamingo is trained, we use it to tackle a visual task by conditioning it on a multimodal
interleaved prompt. We evaluate the ability of our models to rapidly adapt to new tasks using incontext learning, analogously to GPT-3 , by interleaving support example pairs in the form of
(ùëñùëöùëéùëîùëí, ùë°ùëíùë•ùë°) or (ùë£ùëñùëëùëíùëú, ùë°ùëíùë•ùë°), followed by the query visual input, to build a prompt (details in Appendix A.2). We perform open-ended evaluations using beam search for decoding, and close-ended
MSVDQA (V)
VizWiz (I)
Flick30K (I)
MSRVTTQA (V)
YouCook2 (V)
VisDial (I)
TextVQA (I)
NextQA (I)
HatefulMemes (I)
RareAct (V)
Flamingo-3B
Flamingo-9B
Pretrained
Table 1: Comparison to the state of the art. A single Flamingo model reaches the state of the art
on a wide array of image (I) and video (V) understanding tasks with few-shot learning, signiÔ¨Åcantly
outperforming previous best zero- and few-shot methods with as few as four examples. More
importantly, using only 32 examples and without adapting any model weights, Flamingo outperforms
the current best methods ‚Äì Ô¨Åne-tuned on thousands of annotated examples ‚Äì on seven tasks. Best
few-shot numbers are in bold, best numbers overall are underlined.
evaluations using our model‚Äôs log-likelihood to score each possible answer. We explore zero-shot
generalization by prompting the model with two text-only examples from the task, with no corresponding images. Evaluation hyperparameters and additional details are given in Appendix B.1.5.
Experiments
Our goal is to develop models that can rapidly adapt to diverse and challenging tasks. For this, we
consider a wide array of 16 popular multimodal image/video and language benchmarks. In order to
validate model design decisions during the course of the project, 5 of these benchmarks were used as
part of our development (DEV) set: COCO, OKVQA, VQAv2, MSVDQA and VATEX. Performance
estimates on the DEV benchmarks may be biased, as a result of model selection. We note that this is
also the case for prior work which makes use of similar benchmarks to validate and ablate design
decisions. To account for this, we report performance on an additional set of 11 benchmarks, spanning
captioning, video question-answering, as well as some less commonly explored capabilities such as
visual dialogue and multi-choice question-answering tasks. The evaluation benchmarks are described
in Appendix B.1.4. We keep all evaluation hyperparameters Ô¨Åxed across all benchmarks. Depending
on the task, we use four few-shot prompt templates we describe in more detail in Appendix B.1.5.
We emphasize that we do not validate any design decisions on these 11 benchmarks and use them
solely to estimate unbiased few-shot learning performance of our models.
Concretely, estimating few-shot learning performance of a model involves prompting it with a set of
support samples and evaluating it on a set of query samples. For the DEV benchmarks that are used
both to validate design decisions and hyperparameters, as well as to report Ô¨Ånal performance, we
therefore use four subsets: validation support, validation query, test support and test query. For other
benchmarks, we need only the latter two. We report in Appendix B.1.4 how we form these subsets.
We report the results of the Flamingo models on few-shot learning in Section 3.1. Section 3.2
gives Flamingo Ô¨Åne-tuned results. An ablation study is given in Section 3.3. Appendix B.2 provides
more results including Flamingo‚Äôs performance on the ImageNet and Kinetics700 classiÔ¨Åcation tasks,
and on our contrastive model‚Äôs performance. Appendix C includes additional qualitative results.
Few-shot learning on vision-language tasks
Few-shot results. Results are given in Table 1. Flamingo outperforms by a large margin all previous
zero-shot or few-shot methods on the 16 benchmarks considered. This is achieved with as few as four
examples per task, demonstrating practical and efÔ¨Åcient adaptation of vision models to new tasks.
More importantly, Flamingo is often competitive with state-of-the-art methods additionally Ô¨Åne-tuned
on up to hundreds of thousands of annotated examples. On six tasks, Flamingo even outperforms
the Ô¨Åne-tuned SotA despite using a single set of model weights and only 32 task-speciÔ¨Åc examples.
HatefulMemes
Fine-tuned
Table 2: Comparison to SotA when Ô¨Åne-tuning Flamingo. We Ô¨Åne-tune Flamingo on all nine
tasks where Flamingo does not achieve SotA with few-shot learning. Flamingo sets a new SotA on
Ô¨Åve of them, outperfoming methods (marked with ‚Ä†) that use tricks such as model ensembling or
domain-speciÔ¨Åc metric optimisation (e.g., CIDEr optimisation).
Flamingo-3B
original value
Flamingo-3B model
Training data
w/o Video-Text pairs
w/o Image-Text pairs
Image-Text pairs‚ÜíLAION
Optimisation
Accumulation
Round Robin
Tanh gating
Cross-attention
XATTN-DENSE
VANILLA XATTN
architecture
Cross-attention
Single in middle
Transformer
Vision encoder
CLIP ViT-L/14
Freezing LM
 (random init)
 (pretrained)
Table 3: Ablation studies. Each row should be compared to the baseline Flamingo run (top row).
Step time measures the time spent to perform gradient updates on all training datasets.
Finally, despite having only used the DEV benchmarks for design decisions, our results generalize
well to the other benchmarks, conÔ¨Årming the generality of our approach.
Scaling with respect to parameters and shots. As shown in Figure 2, the larger the model, the
better the few-shot performance, similar to GPT-3 . The performance also improves with the
number of shots. We further Ô¨Ånd that the largest model better exploits larger numbers of shots.
Interestingly, even though our Flamingo models were trained with sequences limited to only 5
images on M3W, they are still able to beneÔ¨Åt from up to 32 images or videos during inference. This
demonstrates the Ô¨Çexibility of the Flamingo architecture for processing a variable number of videos
or images.
Fine-tuning Flamingo as a pretrained vision-language model
While not the main focus of our work, we verify that when given more data, Flamingo models can be
adapted to a task by Ô¨Åne-tuning their weights. In Table 2, we explore Ô¨Åne-tuning our largest model,
Flamingo, for a given task with no limit on the annotation budget. In short, we do so by Ô¨Åne-tuning the
model on a short schedule with a small learning rate by additionally unfreezing the vision backbone
to accommodate a higher input resolution (details in Appendix B.2.2). We Ô¨Ånd that we can improve
results over our previously presented in-context few-shot learning results, setting a new state of the
art on Ô¨Åve additional tasks: VQAv2, VATEX, VizWiz, MSRVTTQA, and HatefulMemes.
Ablation studies
In Table 3, we report our ablation results using Flamingo-3B on the validation subsets of the Ô¨Åve
DEV benchmarks with 4 shots. Note that we use smaller batch sizes and a shorter training schedule
compared to the Ô¨Ånal models. The Overall score is obtained by dividing each benchmark score by its
state-of-the-art (SotA) performance from Table 1 and averaging the results. More details and results
are given in Appendix B.3 and Table 10.
Importance of the training data mixture. As shown in row (i), getting the right training data plays
a crucial role. In fact, removing the interleaved image-text dataset M3W leads to a decrease of more
than 17% in performance while removing the conventional paired image-text pairs also decreases
performance (by 9.8%), demonstrating the need for different types of datasets. Moreover, removing
our paired video-text dataset negatively affects performance on all video tasks. We ablate replacing
our image-text pairs (ITP) by the publicly available LAION-400M dataset , which leads to a
slight degradation in performance. We show in row (ii) the importance of our gradient accumulation
strategy compared to using round-robin updates .
Visual conditioning of the frozen LM. We ablate the use of the 0-initialized tanh gating when
merging the cross-attention output to the frozen LM output in row (iii). Without it, we see a drop of
4.2% in our overall score. Moreover, we have noticed that disabling the 0-initialized tanh gating leads
to training instabilities. Next, we ablate different conditioning architectures in row (iv). VANILLA
XATTN, refers to the vanilla cross-attention from the original Transformer decoder . In the
GRAFTING approach from , the frozen LM is used as is with no additional layers inserted, and a
stack of interleaved self-attention and cross-attention layers that take the frozen LM output are learnt
from scratch. Overall, we show that our GATED XATTN-DENSE conditioning approach works best.
Compute/Memory vs. performance trade-offs. In row (v), we ablate the frequency at which we
add new GATED XATTN-DENSE blocks. Although adding them at every layer is better, it signiÔ¨Åcantly
increases the number of trainable parameters and time complexity of the model. Notably, inserting
them every fourth block accelerates training by 66% while only decreasing the overall score by 1.9%.
In light of this trade-off, we maximize the number of added layers under hardware constraints and add
a GATED XATTN-DENSE every fourth layer for Flamingo-9B and every seventh for Flamingo-80B.
We further compare in row (vi) the Perceiver Resampler to a MLP and a vanilla Transformer given a
parameter budget. Both underperform the Perceiver Resampler while also being slower.
Vision encoder. In row (vii), we compare our NFNet-F6 vision encoder pretrained with contrastive
learning (details in Appendix B.1.3) to the publicly available CLIP ViT-L/14 model trained at
224 resolution. Our NFNet-F6 has a +5.8% advantage over the CLIP ViT-L/14 and +8.0% over a
smaller NFNet-F0 encoder, which highlights the importance of using a strong vision backbone.
Freezing LM components prevents catastrophic forgetting. We verify the importance of freezing
the LM layers at training in row (viii). If trained from scratch, we observe a large performance decrease of ‚àí12.9%. Interestingly, Ô¨Åne-tuning our pretrained LM also leads to a drop in performance of
‚àí8.0%. This indicates an instance of ‚Äúcatastrophic forgetting‚Äù , in which the model progressively
forgets its pretraining while training on a new objective. In our setting, freezing the language model
is a better alternative to training with the pre-training dataset (MassiveText) in the mixture.
Related work
Language modelling and few-shot adaptation. Language modelling has recently made substantial
progress following the introduction of Transformers . The paradigm of Ô¨Årst pretraining on a
vast amount of data followed by an adaptation on a downstream task has become standard . In this work, we build on the 70B Chinchilla language model as the
base LM for Flamingo. Numerous works have explored techniques to adapt language models to
novel tasks using a few examples. These include adding small adapter modules , Ô¨Åne-tuning
a small part of the LM , showing in-context examples in the prompt , or optimizing the
prompt through gradient descent. In this paper, we take inspiration from the in-context 
few-shot learning technique instead of more involved few-shot learning approaches based on metric
learning or meta-learning .
When language meets vision. These LM breakthroughs have been inÔ¨Çuential for vision-language
modelling. In particular, BERT inspired a large body of vision-language work . We differ from these approaches as
Flamingo models do not require Ô¨Åne-tuning on new tasks. Another family of vision-language models
is based on contrastive learning . Flamingo differs from
contrastive models as it can generate text, although we build and rely upon them for our vision encoder.
Similar to our work are VLMs able to generate text in an autoregressive manner .
Concurrent works also propose to formulate numerous vision tasks as text
generation problems. Building on top of powerful pretrained language models has been explored
in several recent works. One recent line of work proposes to freeze the
pretrained LM weights to prevent catastrophic forgetting . We follow this idea by freezing the
Chinchilla LM layers and adding learnable layers within the frozen LM. We differ from prior
work by introducing the Ô¨Årst LM that can ingest arbitrarily interleaved images, videos, and text.
Web-scale vision and language training datasets. Manually annotated vision and language datasets
are costly to obtain and thus relatively small (10k-100k) in scale . To alleviate
this lack of data, numerous works automatically scrape readily available paired
vision-text data. In addition to such paired data, we show the importance of also training on entire
multimodal webpages containing interleaved images and text as a single sequence. Concurrent work
CM3 proposes to generate HTML markup from pages, while we simplify the text prediction
task by only generating plain text. We emphasize few-shot learning and vision tasks while CM3 
primarily evaluates on language-only benchmarks in a zero-shot or Ô¨Åne-tuned setup.
Discussion
Limitations. First, our models build on pretrained LMs, and as a side effect, directly inherit their
weaknesses. For example, LM priors are generally helpful, but may play a role in occasional
hallucinations and ungrounded guesses. Furthermore, LMs generalise poorly to sequences longer
than the training ones. They also suffer from poor sample efÔ¨Åciency during training. Addressing
these issues can accelerate progress in the Ô¨Åeld and enhance the abilities of VLMs like Flamingo.
Second, the classiÔ¨Åcation performance of Flamingo lags behind that of state-of-the-art contrastive
models . These models directly optimize for text-image retrieval, of which classiÔ¨Åcation is
a special case. In contrast, our models handle a wider range of tasks, such as open-ended ones. A
uniÔ¨Åed approach to achieve the best of both worlds is an important research direction.
Third, in-context learning has signiÔ¨Åcant advantages over gradient-based few-shot learning methods,
but also suffers from drawbacks depending on the characteristics of the application at hand. We
demonstrate the effectiveness of in-context learning when access is limited to only a few dozen
examples. In-context learning also enables simple deployment, requiring only inference, generally
with no hyperparameter tuning needed. However, in-context learning is known to be highly sensitive
to various aspects of the demonstrations , and its inference compute cost and absolute
performance scale poorly with the number of shots beyond this low-data regime. There may be
opportunities to combine few-shot learning methods to leverage their complementary beneÔ¨Åts. We
discuss the limitations of our work in more depth in Appendix D.1.
Societal impacts. In terms of societal impacts, Flamingo offers a number of beneÔ¨Åts while carrying
some risks. Its ability to rapidly adapt to a broad range of tasks have the potential to enable non-expert
users to obtain good performance in data-starved regimes, lowering the barriers to both beneÔ¨Åcial
and malicious applications. Flamingo is exposed to the same risks as large language models, such
as outputting offensive language, propagating social biases and stereotypes, as well as leaking
private information . Its ability to additionally handle visual inputs poses speciÔ¨Åc risks
such as gender and racial biases relating to the contents of the input images, similar to a number
of visual recognition systems . We refer the reader to Appendix D.2 for a
more extensive discussion of the societal impacts of our work, both positive and negative; as well
as mitigation strategies and early investigations of risks relating to racial or gender bias and toxic
outputs. Finally we note that, following prior work focusing on language models , the
few-shot capabilities of Flamingo could be useful for mitigating such risks.
Conclusion. We proposed Flamingo, a general-purpose family of models that can be applied to image
and video tasks with minimal task-speciÔ¨Åc training data. We also qualitatively explored interactive
abilities of Flamingo such as ‚Äúchatting‚Äù with the model, demonstrating Ô¨Çexibility beyond traditional
vision benchmarks. Our results suggest that connecting pre-trained large language models with
powerful visual models is an important step towards general-purpose visual understanding.
Acknowledgments and Disclosure of Funding.
This research was funded by DeepMind. We
would like to thank many colleagues for useful discussions, suggestions, feedback, and advice, including: Samuel Albanie, Relja Arandjelovi¬¥c, Kareem Ayoub, Lorrayne Bennett, Adria Recasens Continente, Tom Eccles, Nando de Freitas, Sander Dieleman, Conor Durkan, Aleksa Gordi¬¥c, Raia Hadsell,
Will Hawkins, Lisa Anne Hendricks, Felix Hill, Jordan Hoffmann, Geoffrey Irving, Drew Jaegle,
Koray Kavukcuoglu, Agustin Dal Lago, Mateusz Malinowski, SoÀána Mokr√°, Gaby Pearl, Toby Pohlen,
Jack Rae, Laurent Sifre, Francis Song, Maria Tsimpoukelli, Gregory Wayne, and Boxi Wu.