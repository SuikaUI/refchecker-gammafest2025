Journal of Artiﬁcial Intelligence Research 61 863-905
Submitted 06/17; published 04/18
SMOTE for Learning from Imbalanced Data: Progress and
Challenges, Marking the 15-year Anniversary
Alberto Fern´andez
 
Salvador Garc´ıa
 
Francisco Herrera
 
Department of Computer Science and Artiﬁcial Intelligence
University of Granada, Spain
Nitesh V. Chawla
 
Department of Computer Science and Engineering
and Interdisciplinary Center
for Network Science & Applications
University of Notre Dame, IN, USA
The Synthetic Minority Oversampling Technique (SMOTE) preprocessing algorithm is
considered “de facto” standard in the framework of learning from imbalanced data. This
is due to its simplicity in the design of the procedure, as well as its robustness when applied to diﬀerent type of problems.
Since its publication in 2002, SMOTE has proven
successful in a variety of applications from several diﬀerent domains. SMOTE has also inspired several approaches to counter the issue of class imbalance, and has also signiﬁcantly
contributed to new supervised learning paradigms, including multilabel classiﬁcation, incremental learning, semi-supervised learning, multi-instance learning, among others. It is
standard benchmark for learning from imbalanced data. It is also featured in a number of
diﬀerent software packages — from open source to commercial. In this paper, marking the
ﬁfteen year anniversary of SMOTE, we reﬂect on the SMOTE journey, discuss the current
state of aﬀairs with SMOTE, its applications, and also identify the next set of challenges
to extend SMOTE for Big Data problems.
1. Introduction
In the 1990s as more data and applications of machine learning and data mining started
to become prevalent, an important challenge emerged: how to achieve desired classiﬁcation
accuracy when dealing with data that had signiﬁcantly skewed class distributions . Authors from
several disciplines observed an unexpected behavior for standard classiﬁcation algorithms
over datasets with uneven class distributions . In many cases, the speciﬁcity
or local accuracy on the majority class examples overwhelmed the one achieved on the
minority ones. This led to the beginning of an active area of research in machine learning,
now termed as “learning from imbalanced data”. It was in the beginning of the 2000’s when
the foundations of the topic were established during the ﬁrst workshop on class imbalanced
c⃝2018 AI Access Foundation. All rights reserved.
Fern´andez, Garc´ıa, Herrera, & Chawla
learning during the American Association for Artiﬁcial Intelligence Conference . The second milestone was set in 2003 during the ICML-KDD Workshop
on learning from imbalanced datasets, leading to a special issue on the topic .
The signiﬁcance of this area of research continues to grow largely driven by the challenging problem statements from diﬀerent application areas (such as face recognition, software
engineering, social media, social networks, and medical diagnosis), providing a novel and
contemporaneous set of challenges to the machine learning and data science researchers
 . The overarching question that researchers have been trying to solve is: how
to push the boundaries of prediction on the underrepresented or minority classes while
managing the trade-oﬀwith with false positives? The solution space has ranged from sampling approaches to new learning algorithms designed speciﬁcally for imbalanced data. The
sampling approaches are broadly divided into two broad categories — undersampling or
oversampling.
Undersampling techniques are known to provide a compact balanced training set that
also reduces the cost of the learning stage. However, it also leads to some derived problems. First, it increases the variance of the classiﬁer and ii) it produces warped posterior
probabilities . It may also might discard some
useful examples for the modeling of the classiﬁer. Particularly when the ratio of imbalance
is high, then more examples need to be removed leading to the problem of lack of data
 . This may aﬀect the generalization ability of the classiﬁer. As
a result, researchers developed oversampling methods that may not lead to reduction of
majority class examples, and tackle the class imbalance issue by replicating the minority
class examples.However, applying random oversampling only implies a higher weight or cost
for the minority instances. Therefore, the correct modeling of those clusters of minority
data by the classiﬁcation algorithm might still be hard in the case of overlapping or small disjuncts proposed a novel approach as an
alternative to the standard random oversampling. The idea was to overcome the overﬁtting
rendered by simply oversampling by replication, and assist the classiﬁer to improve its
generalization on the testing data. Instead of “weighting” data points, the basis of this new
data preprocessing technique was to create new minority instances. This technique was
titled Synthetic Minority Oversampling Technique, now widely known as SMOTE . The basis of the SMOTE procedure was to carry out an interpolation among
neighboring minority class instances. As such, it is able to increase the number of minority
class instances by introducing new minority class examples in the neighborhood, thereby
assisting the classiﬁers to improve its generalization capacity.
SMOTE preprocessing technique became a pioneer for the research community in imbalanced classiﬁcation. Since its release, many extensions and alternatives have been proposed
to improve its performance under diﬀerent scenarios. Due to its popularity and inﬂuence,
SMOTE is considered as one of the most inﬂuential data preprocessing/sampling algorithms
in machine learning and data mining . Some approaches
SMOTE for Learning from Imbalanced Data: 15-year Anniversary
combine SMOTE with data cleaning techniques . Other
authors focus on the inner procedure by modifying some of its components, such as the
selection of the instances for new data generation , or the type
of interpolation , among others.
In this paper, we present a summary of SMOTE and its impact in the last 15 years, celebrate its contributions to machine learning and data mining, and present the next state of
challenges to keep pushing the frontier on learning from imbalanced data. While we don’t include a discussion on the over 5,370 citations of SMOTE , we speciﬁcally
focus this paper on enumerating various SMOTE extensions as well as discussing the road
ahead. For example, we discuss the extensions of SMOTE to other learning paradigms,
such as streaming data , incremental learning , concept drift , or multi-label/multi-instance classiﬁcation tasks , among others. We also present an analysis about
potential scenarios within imbalanced data that require a deeper dive into application of
SMOTE, such as the data intrinsic characteristics , including small
disjuncts, overlapping classes, and so on. Finally, we posit challenges of imbalanced classi-
ﬁcation in Big Data problems . Our hope is
that this paper provides a summative overview of SMOTE, its extensions, and challenges
that remain to be addressed in the community.
This paper is organized as follows. Section 2 introduces the SMOTE algorithm. Then,
Section 3 enumerates those extensions to the standard SMOTE that have been proposed
along these years. Section 4 presents the use of SMOTE under diﬀerent learning paradigms.
The challenges and topics for future work on SMOTE based preprocessing algorithms are
given in Section 5. Finally, Section 6 summarizes and concludes the paper.
2. Synthetic Minority Oversampling Technique
In this section, we will ﬁrst point out the origins of the SMOTE algorithm, setting the
context under which it was designed (Section 2.1). Then, we will describe its properties
in detail in order to present the working procedure of this preprocessing approach . So he actually did
worse than a majority class guess classiﬁer. Moreover, the decision tree classiﬁer performed
poorly in the important task of predicting calciﬁcations correctly. This, thus presented the
challenge of: how to improve the performance of the classiﬁer on minority class instances?
Fern´andez, Garc´ıa, Herrera, & Chawla
An accompanying challenge was a low tolerance of false positives, i.e.
examples of the
majority class identiﬁed as minority ones.
That is, one had to achieve an appropriate
trade-oﬀbetween the true positives and false positives, and not just be overly aggressive in
predicting minority class (cancerous pixels) to compensate for the 2.32% distribution. This
was because there were costs associated with errors — every false negative bore the burden
of misclassifying a cancer as non-cancer, and every false positive bore the cost of additional
tests by misclassifying a non-cancer as a cancer. The errors clearly were not of equal types.
Chawla tried the standard tools in the research arsenal at that time — oversampling by
replication and undersampling. Both of the approaches, while improving the performance,
did not provide satisfactorily results. On further investigation, he noticed the challenge
arising from overﬁtting the minority class instances because of oversampling. This observation led to the question of: how to improve the generalization capacity of the underling
classiﬁer? And thus SMOTE was created to synthetically generate new instances to provide
new information to the learning algorithm to improve its predictability about the minority
class instances. SMOTE provided statistically signiﬁcantly superior performance on the
mammography data, as well as several others, thus laying the foundation for learning from
imbalanced datasets. Of course, SMOTE like other sampling approaches, faces the challenge of the sampling amount, which Chawla and his colleagues also tried to mitigate by
developing a wrapper framework, akin to feature selection . We will discuss these
issues hereinafter in Section 5 A simple example of SMOTE is illustrated in Figure 1. An
xi minority class instance is selected as basis to create new synthetic data points. Based on
a distance metric, several nearest neighbors of the same class (points xi1 to xi4) are chosen
from the training set. Finally, a randomized interpolation is carried out in order to obtain
new instances r1 to r4.
The formal procedure works as follows. First, the total amount of oversampling N (an
integer value) is set up, which can either be set-up to obtain an approximate 1:1 class
distribution or discovered via a wrapper process . Then, an iterative
process is carried out, composed of several steps. First, a minority class instance is selected
at random from the training set. Next, its K nearest neighbors (5 by default) are obtained.
Finally, N of these K instances are randomly chosen to compute the new instances by
SMOTE for Learning from Imbalanced Data: 15-year Anniversary
Figure 1: An illustration of how to create the synthetic data points in the SMOTE algorithm
interpolation. To do so, the diﬀerence between the feature vector (sample) under consideration and each of the selected neighbors is taken. This diﬀerence is multiplied by a random
number drawn between 0 and 1, and then it is added to the previous feature vector. This
causes the selection of a random point along the “line segment” between the features. In
case of nominal attributes, one of the two values is selected at random. The whole process
is summarized in Algorithm 1.
Algorithm 1 SMOTE algorithm
1: function SMOTE(T, N, k)
Input: T; N; k
▷#minority class examples, Amount of oversampling, #nearest
Output: (N/100) * T synthetic minority class samples
Variables: Sample[][]: array for original minority class samples;
newindex: keeps a count of number of synthetic samples generated, initialized to 0;
Synthetic[][]: array for synthetic samples
if N < 100 then
Randomize the T minority class samples
T = (N/100)*T
N = (int)N/100
▷The amount of SMOTE is assumed to be in integral multiples
for i = 1 to T do
Compute k nearest neighbors for i, and save the indices in the nnarray
POPULATE(N, i, nnarray)
12: end function
Figure 2 shows a simple example of the SMOTE application in order to understand how
synthetic instances are computed.
To conclude this section, we aim at introducing some of the ﬁrst real applications that
made a successful use of the SMOTE preprocessing algorithm, both of which are based on
the area Bioinformatics. Speciﬁcally, we stress a multi-class problem of molecular functions
Fern´andez, Garc´ıa, Herrera, & Chawla
Algorithm 2 Function to generate synthetic samples
1: function Populate(N, i, nnarray)
Input: N; i; nnarray
▷#instances to create, original sample index, array of nearest
Output: N new synthetic samples in Synthetic array
while N ̸= 0 do
nn = random(1,k)
for attr = 1 to numattrs do
▷numattrs = Number of attributes
Compute: dif = Sample[nnarray[nn]][attr] −Sample[i][attr]
Compute: gap = random(0, 1)
Synthetic[newindex][attr] = Sample[i][attr] + gap · dif
newindex + +
12: end function
Consider a sample (6,4) and let (4,3) be its nearest neighbor.
(6,4) is the sample for which k-nearest neighbors are being
identified (4,3) is one of its k-nearest neighbors.
Let: f1_1 = 6 f2_1 = 4,
f2_1 - f1_1 = -2
f2_2 - f1_2 = -1
The new samples will be generated as
f1’,f2’ = (6,4) + rand(0-1) * (-2,-1)
rand(0-1) generates a vector of two random numbers between 0 and 1.
Figure 2: Example of the SMOTE application.
of yeast proteins . The original problem was
divided into imbalanced binary subsets, so that new synthetic instances were needed prior
to the learning stage of a modular neural network to avoid the bias towards the majority
3. Extensions to SMOTE
In the following, we present the most signiﬁcant SMOTE-based approaches proposed in
the last 15 years and a set of common properties shared by them. We consider SMOTE
as a foundation for over-sampling with artiﬁcial generation of minority class instances.
For this reason, we understand that any preprocessing method in the area of imbalanced
classiﬁcation that is based on the synthetic creation of examples by any type of interpolation
or other process has some degree of relationship with the original SMOTE algorithm. First,
in Section 3.1, the essential characteristics will be outlined. Next, in Section 3.2, we will
enumerate all the extensions based on SMOTE proposed in the scientiﬁc literature until
SMOTE for Learning from Imbalanced Data: 15-year Anniversary
now. Then, each method will be categorized according to the studied properties to provide
a comprehensive taxonomy. Next, in Section 3.3 we will present a list of SMOTE-based
multiclassiﬁers proposed together with their categorization. Finally, Section 3.4 will outline
the most inﬂuential experimental studies presented in the literature involving SMOTE as
key point.
3.1 Properties for Categorizing the SMOTE-Based Extensions
This section provides a framework for the organization of the SMOTE-based extensions
that will be presented in Sections 3.2 and 3.3. The aspects discussed here consist of (1)
initial selection of instances to be oversampled, (2) integration with Undersampling as step
in the technique, (3) type of interpolation, (4) operation with dimensionality changes, (5)
adaptive generation of synthetic examples, (6) possibility of relabeling and (7) ﬁltering of
noisy generated instances.
These mentioned facets are involved in the deﬁnition of the
categorization, because they determine the way of operation of each technique. Next, we
describe in detail each property.
• Initial selection of instances to be oversampled: It is usual to determine the
best candidates to be oversampled in the data before the process of synthetic example
generation starts. This strategy is intended to reduce the overlapping and noise in
the ﬁnal dataset. Many techniques opt to choose the instances near to the boundary
classes or to not generate a synthetic example depending on the
number of minority class examples belonging to the neighborhood . Although many alternatives of initial selection
have been proposed in the literature, almost all follow any of the two mentioned
strategies. Two exceptions are the generation of synthetic examples after a LVQ optimization process and the selection of
initial points from the support vectors obtained by a SVM .
• Integration with Undersampling: The examples belonging to the majority class
are also removed by either using a random or an informed technique of undersampling.
The undersampling step can be either done at the beginning of the oversampling or as
an internal operation together with the generation of synthetic examples. Generally,
oversampling follows undersampling.
• Type of interpolation: This property oﬀers varied mechanisms about generation of
artiﬁcial or synthetic examples and is frequently associated with the main originality
of the novel development. It deﬁnes the way new artiﬁcial examples are created and
many alternatives can be found. The interpolation mechanisms can be range restricted
 ,
for example by looking not only for nearest neighbours from the minority class but
also from majority class; creating new examples closer to the selected instance than its
neighbor or by using feature weighting ;
multiple interpolations involving more than two examples or following topologies based on geometric shapes, such
as ellipses and voronoi diagrams , and graphs ; clustering-based interpolation , in which the new examples can be either the
centroids of the cluster or can be created involving examples that belong to the same
cluster; interpolations that use diﬀerent random distributions, such as the gaussian
 , estimations of the probability distribution function of the
data , probability smoothing , preservation of covariances 
among data and more complex interpolations, such as Markov chains or Q-unions . It is even possible to have
no interpolation, such as when the new data is generated using only a single point,
through jittering , gaussians disturbances , just simple copies with changes of label or even by combining oversampling with pushing the majority samples
out of a sphere .
• Operation with dimensionality changes: This occurs when the technique incorporates either a reduction or augmentation of dimensionality before or during the
generation of artiﬁcial or synthetic examples. The most common approach is to change
the dimensionality of the data at the beginning and then to work in the new dimensional space; either by reducing it through Principal Component Analysis (PCA)
 or related techniques , feature selection , Bagging , manifold techniques and
auto-encoders , and by using kernel functions . Also, an estimation of the principal components of
the data may be used to lead the interpolation .
• Adaptive generation of synthetic examples: The hypothesis of adaptive generation, ADASYN , was to use a weighted distribution
depending on each minority class example according to their degree of diﬃculty when
learning. This way, more synthetic data will be generated for some minority class instances that are more complicated to learn compared to other. Inspired by ADASYN,
lots of techniques incorporate similar mechanisms to control the quantity of new artiﬁcial examples to be generated associated with each minority example or subgroups
of minority examples .
• Relabeling: The technique oﬀers the choice to relabel the examples belonging to
the majority class during the synthetic generation of examples or replacing the interpolation mechanism .
• Filtering of noisy generated instances: The ﬁrst extensions of SMOTE motivated
by its well known drawback of generating overlapped and noisy examples was the addition of a noise ﬁltering step just after SMOTE process ends. Two typical techniques
are SMOTE-TomekLinks and SMOTE+ENN . Filtering of artiﬁcial examples is a frequent operation that supports the success of SMOTE on real data.
SMOTE for Learning from Imbalanced Data: 15-year Anniversary
Many kind of ﬁlters have been proposed for enhancing SMOTE, such as greedy ﬁltering strategies , rough-sets based ﬁltering , ensembles-based ﬁltering and bioinspired optimization procedures .
3.2 SMOTE-Based Extensions for Oversampling
Till date, more than 85 extensions of SMOTE have been proposed in the specialized literature. This section is devoted to enumerate and categorize them according to the properties
studied before. Table 1 presents an enumeration of the methods reviewed in this paper.
In this ﬁeld, it is usual that the authors provide a name for their proposal, with a few
exceptions.
As we can see in Table 1, the most frequent properties exploited by the techniques are the
initial selection and adaptive generation of synthetic examples. Filtering is becoming more
common in recent years, as well as the use of kernel functions. Regarding the interpolation
procedure, it is also usual to replace the original method with other more complex ones, such
as clustering-based or derived from a probabilistic function. It is worth mentioning that
there is no technique that applies the four mechanisms pertinent to the calibration of the
generation of artiﬁcial examples, selection and removal of harmful examples either synthetic
or belonging to the majority class; namely initial selection, integration with undersampling,
adaptive generation and ﬁltering all-together. Due to space limitations, it is not possible
to describe all the reviewed techniques. Nevertheless, we will provide brief explanations for
the most well-known techniques from Table 1:
• Borderline-SMOTE : This algorithm draws from the premise
of that the examples far from the borderline may contribute little to the classiﬁcation success.
Thus, the technique indentiﬁes those examples which belong to the
borderline by using the ratio between the majority and minority examples within the
neighborhood of each instance to be oversampled. Noisy examples, those that have all
the neighbours from the majority class, are not considered. The so-called dangerous
examples, with a suitable ratio, are oversampled.
• AHC : It was the ﬁrst attempt to use clustering to generate
new synthetic examples to balance the data. The K-means algorithm was used to
undersample the majority examples and agglomerative hierarchical clustering was
used to oversample the minority examples. Here, the clusters are gathered from all
levels of the resulting dendograms and their centroids are interpolated with the original
minority class examples.
• ADASYN : Its main idea proceeds from the assumption of utilizing
a weighted distribution depending on the type of minority examples according to their
complexity for learning. The quantity of synthetic data for each one is associated with
the level of diﬃculty of each minority example. This diﬃculty estimation is based on
Fern´andez, Garc´ıa, Herrera, & Chawla
Table 1: Enumeration and categorization of SMOTE algorithm extensions
Integration
Dimensionality
Relabeling
Interpolation
generation
 
SMOTE+TomekLinks
 
 
Borderline-SMOTE
Range restricted
 
Clustering
 
 
Distance-SMOTE
 
Without-Gaussian
 
Polynom-Fit-OS
Topologies
 
 
< no name >
Without-Copy
 
 
Safe-Level-SMOTE
Range restricted
 
Isomap-Hybrid
 
 
DE-Oversampling
DE operators
 
 
Edge-Det-SMOTE
 
Clustering
 
 
Gaussian+Cov.
 
FS with GA
 
Random-SMOTE
 
 
 
Range restricted
 
Distribution-SMOTE
 
NDO-Sampling
Without-Gaussian
 
Graph based
 
SVM-Balance
 
TRIM-SMOTE
 
SMOTE-RSB*
 
 
Clustering
 
SL-Graph-SMOTE
Range restricted
 
NRSBoundary-SMOTE
 
 
 
Range restricted
 
Clustering+Jittering
 
Range restricted
 
Graph based
 
Assembled-SMOTE
 
Without-Smoothing
 
Clustering
 
PDF+Gaussian
 
Range restricted
 
SMOTE-Cosine
 
Selected-SMOTE
 
 
 
 
Gaussian+Q-union
 
Gaussian+Non-linear
 
 
RWO-Sampling
Without-Gaussian
 
< no name >
 
 
 
Auto-Encoder
 
 
Without-Markov
 
< no name >
Topologies
 
Without-Copy
 
SMOTE-PSO/BAT
 
MinorityDegree-SMOTE
 
 
Without-Gaussian
 
 
KernelADASYN
 
Clustering
 
 
 
 
PCA+Auto-Encoder
 
 
Clustering
 
 
Clustering
 
 
 
SMOTE-FRST-2T
 
 
Range restricted
 
 
 
CURE-SMOTE
Clustering
 
 
FS + Kernels
 
Clustering
(Li, Fong, Wong, & Chu, 201)
the ratio of examples belonging to the majority class in the neighborhood. Then a
density distribution is computed using all the ratios of the minority instances, which
SMOTE for Learning from Imbalanced Data: 15-year Anniversary
will be used to compute the number of synthetic examples required to be generated
for each minority example.
• Safe-Level-SMOTE : It assigns each minority example a safe level before generating synthetic instances. Each synthetic instance will be
positiones closer to the largest safe level, thus generating all synthetic instances only
in safe regions. The safe level is the ratio between the number of minority examples
within the neighborhood and the safe level ratio depends on the safe level of each
instance and that of the examples in its neighborhood. The interpolation is controlled
by a gap which depends on the safe level ratio of each minority instance.
• DBSMOTE : This algorithm relies on a density-based
approach of clustering called DBSCAN and performs oversampling by generating synthetic samples along a shortest path from each minority instance to a pseudocentroid
of a minority-class cluster. DBSMOTE was inspired by Borderline-SMOTE in the
sense it operates in an overlapping region, but unlike Borderline-SMOTE, it also tries
to maintain both the minority and majority class accuracies.
• ROSE : ROSE is an oversampling technique proposed
within a complete framework to obtain classiﬁcation rules in imbalanced data. It is
established from the generation of new artiﬁcial data from the classes, according to a
smoothed bootstrap form and the idea behind it is supported by the theoretical wellknown properties of the kernel methods. The algorithm samples a new instance using
the probability distribution centered at a randomly selected example and depending
on a smoothing matrix of scale parameters.
• MWMOTE : Based on the assumption of that existing oversampling methods may generate wrong synthetic minority samples, MWMOTE analyzes
the most diﬃcult minority examples and assigns each them a weight according to
their distance from the nearest majority examples. The synthetic examples are then
generated from the weighted informative minority class instance using a clustering
approach, ensuring that they must lie inside a minority class cluster.
• MDO : It is one of the recent multi-class approaches inspired by Mahalanobis distance. MDO builds synthetic examples having the same
Mahalanobis distance from each examined class mean as the other minority examples. Thus, the region of minority instances can be better learned by preserving the
covariance during the generation of synthetic examples along the probability contours.
Also, the risk of overlapping between diﬀerent class regions is reduced.
3.3 SMOTE-Based Extensions for Ensembles
Ensembles of classiﬁers has emerged as a popular learning framework to address imbalanced
classiﬁcation problems. SMOTE has also involved in and / or extended to many ensemble
based methods. Table 2 shows a list of ensemble based techniques that incorporate SMOTE
itself or a derivative of SMOTE as a major step to achieve the diversity of the set of classiﬁers
learned to form the ensemble. Note that this table only contains the methods concerned with
Fern´andez, Garc´ıa, Herrera, & Chawla
Table 2: Enumeration and categorization of SMOTE-based ensemble methods
Integration
Relabeling
Multi-Class
multi-classiﬁer
Interpolation
generation
 
SMOTEBoost
 
DataBoost-IM
 
inputSmearing
Without-Gaussian
 
JOUS-Boost
 
SMOTEBagging
 
 
AdaOUBoost
 
Feature-weighted
 
IIvotes+SPIDER
Without-Copy
 
 
< no name >
Boostrap-Resampling
 
I-SMOTEBagging
 
MDO 
 
 
Boosting+OVA
 
Range restricted
 
Without-Smoothing
oversampling and generation of synthetic examples; the reader can consult the specialized
literature to review other ensembles proposed for imbalanced learning in which SMOTE does
not take part in .
Speciﬁcally, it is
important to point out that we may ﬁnd several studies that show a good behavior for the
undersampling-based approaches in synergy with ensemble learning .
The structure of the Table 2 is very similar to the previous one, Table 1. The dimensionality change and ﬁltering are two properties not used in ensembles. Furthermore, we
add a new column designating the type of ensemble method, namely if the method is a
boosting, bagging or One-Versus-All (OVA) approach. The rest of properties are explained
in Section 3.1.
3.4 Exhaustive Empirical Studies Involving SMOTE
SMOTE is established as the “de facto” standard or benchmark in learning from imbalanced dataset.
Although it would be impossible to survey all the analytic studies that
involve SMOTE in any step, in this brief section, we review some of the most inﬂuential empirical studies that studied SMOTE in depth. The ﬁrst type of experimental studies
emerged to check whether oversampling is more eﬀective than undersampling and what rate
of oversampling or undersampling rate should be used .
Several studies tackled this issue from a more general point of view and
speciﬁcally focused on SMOTE to ask about how to discover the proper amount and type
of sampling . In the work of Batista et al. , some common resampling approaches are compared and the hybridizations of SMOTE with undersampling
was shown to outperform the rest of resampling techniques. Later, Prati, Batista, and Silva
 designed a renewed experimental setup to answer some open-ended questions on the
relationship and performance between learning paradigms, imbalance degrees and proposed
solutions. More complex analytic studies can be found to analyze data intrinsic characteristics , data diﬃculty factors such as rare sub-concepts of minority
instances, overlapping of classes and diﬀerent types of minority class examples .
SMOTE for Learning from Imbalanced Data: 15-year Anniversary
Table 3: List of SMOTE-based approaches for other learning paradigms
Algorithm Name
Learning Paradigm
 
Learn++.SMOTE
Data Streams
 
Time Series
 
Low Quality Data
 
< no name >
Image Retrieval
 
< no name >
High Dimensional Data
 
Time Series
 
Learn++.NSE-SMOTE
Data Streams
 
Active Learning
 
Text Classiﬁcation
 
Semi-Supervised Learning
 
Instance-SMOTE, Bag-SMOTE
Multi-Instance Learning
 
< no name >
Multi-Instance Learning
 
Semi-Supervised Learning
 
< no name >
Semi-Supervised Learning
 
Data Streams
 
Multi-Label Learning
 
Informative-Bag-SMOTE
Multi-Instance Learning
 
OGO-NI, OGO-ISP, OGO-SP
Ordinal Regression
 
Regression
 
Semi-Supervised Learning
 
Semi-Supervised Learning
 
SM B, SM T, SM TPhi
Time Series
Another issue studied particularly in SMOTE is the relationship between data preprocessing and cost-sensitive learning.
In the review by Lopez, Fernandez, Moreno-Torres,
and Herrera , an exhaustive empirical study was performed to this goal, concluding
that both preprocessing and cost-sensitive learning are good and equivalent approaches to
address the imbalance problem.
Regarding diﬀerent typologies of algorithms, SMOTE has been deeply analyzed in combination with cost-sensitive neural networks , SVMs , linguistic fuzzy rule based classiﬁcation systems and genetics-based machine learning for rule induction
 .
4. Variations of SMOTE to Other Learning Paradigms
In this section, we will introduce the SMOTE-based approaches that address other learning
paradigms. In particular, the section will be divided into ﬁve subsections, each one providing
an overview of each paradigm and the techniques devised to tackle it. Extensions of SMOTE
have been applied other learning paradigms: (1) streaming data (see Section 4.1); (2)
Semi-supervised and active learning (in Section 4.2); (3) Multi-instance and multi-label
classiﬁcation (Section 4.3); (4) Regression (in Section 4.4) and (5) Other and more complex
prediction problems and such as text classiﬁcation, low quality data classiﬁcation, and so
on (see Section 4.5).
Table 3 presents a summary of the SMOTE extensions by chronological order, indicating
their references, algorithm names and learning paradigms they tackle. In the following,
we will give a brief description of each learning paradigm and the associated developed
techniques.
4.1 Streaming Data
Many applications for learning algorithms need to tackle dynamic environments where data
arrive in a streaming fashion. The online nature of data creates some additional compu-
Fern´andez, Garc´ıa, Herrera, & Chawla
tational requirements for a classiﬁer . In addition, the prediction
models are usually required to adapt to the concept drifts, which are phenomena derived
from the non-stationary characteristics of data streams. In the oﬄine version of imbalance
classiﬁcation, the classiﬁer can estimate the relationship between the minority class and
majority class before learning begins. Nevertheless, in online learning, it is not possible to
do this due to the fact that classes can change their distribution over time, thus they have
to cope with the dynamic of the data.
Two preprocessing techniques based on SMOTE have
been proposed to deal with imbalanced data streams. The ﬁrst is Learn++.NSE-SMOTE
 , which is an extension of Learn++.SMOTE .
First, the authors incorporated SMOTE within the algorithm Learn++.NSE and after they
decided to replace SMOTE with a subensemble that makes strategic use of minority class
data. The second technique is GOS-IL . It works by updating a base
learner incrementally using standard Oversampling.
When a data stream is received over time and we have disposal of time information, we
refer to time series classiﬁcation. A time series data sample is an ordered set of real-valued
variables coming from a continuous signal, which can be either in time or spatial domain.
The variables close to each other are often highly correlated in time series. The methods
SPO and INOS propose an integration of SMOTE in
time series classiﬁcation. INOS can be viewed as an extension of SPO and addresses the
imbalanced learning issue by oversampling the minority class in the signal space. An hybrid
technique was used to generate synthetic examples by means of estimating and maintaining
the main covariance structure in the reliable eigen subspace and ﬁxing the unreliable eigen
A third family of techniques called SM B, SM T and SM TPhi were
also devised for time series, but for regression. Details for them will be given in Section 4.4.
4.2 Semi-supervised and Active Learning
An important limitation of supervised learning is the great eﬀort to obtain enough labeled
data to train predictive models. In a perfect situation, we want to train classiﬁers using
diverse labeled data with a good representation of all classes. However, in many real applications, there is a huge amount of unlabeled data and the obtaining of a representative subset
is a complex process. Active learning produces training data incrementally by identifying
the most informative data to label. When external supervision is involved (humans or other
system), we are referring to real active learning in which the new examples are selected and
then labeled by the expert. If this is not the case, we refer to semi-supervised classiﬁcation, which utilizes unlabeled data to improve the predictive performance, modifying the
learned hypothesis obtained from labeled examples. Diﬀerent perspectives are employed to
tackle semi-supervised classiﬁcation, such as self-training, graph-based approaches, generative models, and so on .
Several methods based on SMOTE have been developed for this learning paradigm:
• VIRTUAL is designed for active learning problems and SVMs and it
adaptively creates instances from the real positive support vectors selected in each
active learning step.
SMOTE for Learning from Imbalanced Data: 15-year Anniversary
• INNO is a technique for graph-based semi-supervised learning and
performs an iterative search to generate a few unlabeled samples around known labeled
• GS4 , SEG-SSC and OCHS-SSC
 generate synthetic examples to diminish the drawbacks produced
by the absence of labeled examples. Several learning techniques were checked and
some properties such as the common hidden space between labeled samples and the
synthetic sample were exploited.
• The technique proposed by Park et al. is a semi-supervised active learning
method in which labels are incrementally obtained and applied using a clustering
algorithm.
4.3 Multi-class, Multi-instance and Multi-label Classiﬁcation
Although the original SMOTE technique can be applied to multi-class problems by identifying the minority class against the remaining ones (One-versus-all approach), there are
some extensions speciﬁcally employed for tackling multi-class imbalanced classiﬁcation problems : the work from Fern´andez-Navarro et al. , Alejo et al. 
and Abdi and Hashemi .
In multi-instance learning, the structure of the data is more complex than in singleinstance learning . Here,
a learning sample is called a bag.
The main feature in this paradigm is that a bag is
associated with multiple instances or descriptions. Each instance is described by a feature
vector, like in single-instance learning, but associated output is unknown. An instance,
apart from its feature values, only knows its membership relationship to a bag.
Several ideas based on SMOTE have been proposed to tackle multi-instance learning.
The ﬁrst ones were Instance-SMOTE and Bag-SMOTE . The Instance-
SMOTE algorithm creates synthetic minority instances in each bag, without creating new
bags. Besides, Bag-SMOTE creates new synthetic minority bags with new instances. In
the work of Mera, Orozco-Alzate, and Branch ﬁrst, and next in Mera, Arrieta,
Orozco-Alzate, and Branch , the Informative-Bag-SMOTE technique was presented
and improved. It uses a model of the negative population to ﬁnd the best instances in
the minority class to be oversampled. The new synthetic bags created support the target
concept in the minority class.
In multilabel classiﬁcation each instance of the data has associated
a vector of outputs, instead of only one value. This vector has a ﬁxed size according to
the number of diﬀerent labels in the dataset. The vector is composed by binary values
based elements which indicate whether or not the corresponding label is compatible to the
instance. Of course, several labels can be active at once, showing diﬀerent combinations of
labels, which is known as labelset.
MLSMOTE is the most popular extension of SMOTE designed for
multilabel classiﬁcation. Its objective is to produce synthetic instances related to minority
labels. The subset of minority labels within the labelset is identiﬁed by two proposed measures. Input features of the synthetic examples are obtained using SMOTE, but the labelsets
Fern´andez, Garc´ıa, Herrera, & Chawla
of these new instances are also gathered from the nearest neighbors, taking advantage of
label correlation information in the neighborhood.
4.4 Regression
Regression tasks consider the output variable as continuous and hence, the values are represented by real numbers. Unlike standard classiﬁcation, they are ordered. The imbalance
learning correspondence for regression tasks is the correct prediction of rare extreme values of a continuous target variable. In the work of Torgo et al. , several techniques
for resampling were successfully applied for regression.
Among them, SMOTER is the
SMOTE-based contribution of Oversampling regression. SMOTER employs a user-deﬁned
threshold to deﬁne the rare cases as extreme high and low values, dealing both types as
separate cases. Another major diﬀerence is the way the target value for the new cases is
generated, in which a weighted average between two seed cases is used. SMOTER has been
extended to tackle time series forecasting in the study of Moniz et al. . Here, three
methods are derived from SMOTER: SM B, SM T, SM TPhi. They take into account the
characteristics of the bins of the time series and manage the temporal and relevance bias.
The ordinal regression (or classiﬁcation) problem is half way between the standard
classiﬁcation and regression. There exists a predeﬁned order among the categories of the
output variable, but the distance between two consecutive categories is unknown. Thus, the
penalization of misclassiﬁcation errors can be greater or lower depending on the diﬀerence
between the real category and the predicted category. An imbalance scenario of classes
may be usual in this kind of domains when addressing real applications. In the research
conducted by P´erez-Ortiz et al. , an approach of Oversampling from a graph-based
perspective is used to balance the ordinal information. Three schemes of generation were
proposed, namely OGO-NI, OGO-ISP, OGO-SP; depending on the use of intra-class edges,
shortest paths and interior shortest paths of the graph constructed.
4.5 Other and More Complex Prediction Problems
Other problems in which a variant of SMOTE has been applied are the following:
• Imbalanced classiﬁcation with imprecise datasets. This problem refers to the presence of vagueness in the data, preventing the values of the classes to be precisely
known. SMOTE-LQD is a generalized version of SMOTE for
this environment. It delivers the selection of minority instances assuming that the
imbalance ratio is not precisely known and the computation of the nearest neighbors
and generation of synthetic instances is carried out with fuzzy arithmetic operators.
• Image retrieval and semantic search of images is a challenging problem nowadays. In
the work of Piras and Giacinto , the authors proposed a technique that address the imbalance problem in image retrieval tasks by generating synthetic patterns
according to nearest neighbor information.
• In bioinformatics problems, it is usual to have high-dimensional classiﬁcation problems. In the work of Blagus and Lusa , SMOTE was tested in such scenarios
in both theoretical and empirical perspectives. Among the conclusions achieved, the
SMOTE for Learning from Imbalanced Data: 15-year Anniversary
most important was that SMOTE has hardly any eﬀect on most classiﬁers trained on
high-dimensional data. Other techniques such as Undersampling may be preferable
on high-simensional settings.
• A variation of SMOTE based on document content to manage the class imbalance
problem in text classiﬁcation was proposed by Iglesias et al. . The method
called COS-HMM incorporates an Hidden Markov Model that is trained with a corpus
in order to create new samples according to current documents.
5. Challenges in SMOTE-Based Algorithms
When working in the scenario of imbalanced classiﬁcation, we must be aware that the skewed
class distribution is not the only drawback for the performance degradation. Instead, its
conjunction with several data intrinsic characteristic is the cause for the achievement of
sub-optimal models . For example, if the two classes, despite having
severely imbalanced data distribution are easily separable in two clusters or segments, then
it becomes easy for any classiﬁer to learn to discriminate between them. It is when the
classes are interspersed when the challenges become profound, as is often the case with the
real-world applications.
Throughout this section, we will discuss in detail several of these issues and their relationship with SMOTE. Particularly, we will ﬁrst study the problems related to those areas
where minority class are represented as small disjuncts , and their relationship with the
lack of data and noisy instances (Section 5.1). Next, we will consider an issue that hinders the performance
in imbalanced classiﬁcation, i.e. overlapping or class separability (Section 5.2). In addition, since SMOTE applies an interpolation procedure to generate new
synthetic data on the feature space, we will analyze the curse of dimensionality as well as diﬀerent aspects for the interpolation process (Section 5.4). We
must also take into account that a diﬀerent data distribution between the training and test
partitions, i.e. the dataset shift , can also alter the
validation of the results in these cases (Section 5.3).
Finally, we will consider two signiﬁcant novel scenarios for addressing imbalanced classiﬁcation. On the one hand, we focus on real time processing, and more speciﬁcally data
streams imbalanced classiﬁcation (Section 5.5).
Next, we analyze the topic of Big Data and the constraints associated with the
skewed class distribution (Section 5.6).
5.1 Small Disjuncts, Noise and Lack of Data
We refer to a dataset containing small disjuncts when some concepts (disregard their class)
are represented within small clusters . In
the case of imbalanced classes, this problem occurs very often as underrepresented concepts
are usually located in small areas of the dataset. This situation is represented in Figure
3, where we show two cases.
First, Figure 3a depicts an artiﬁcially generated dataset
Fern´andez, Garc´ıa, Herrera, & Chawla
with small disjuncts for the minority class. Then, Figure 3b shows the “Subclus” problem
created in the work of Napierala, Stefanowski, and Wilk , where we can ﬁnd small
disjuncts for both classes: the majority class samples are underrepresented with respect to
the minority class samples in the central region of minority class rectangular areas, whereas
the minority samples only cover a small part of the whole dataset and are placed inside the
negative class.
(a) Artiﬁcial dataset: small disjuncts for
the minority classt
(b) Subclus dataset: small disjuncts for
both classes
Figure 3: Example of small disjuncts on imbalanced data
This situation increases the complexity in the search for quality solutions. This is due to
the common working procedure of standard learning models which aim at achieving a good
generalization ability. As such, most classiﬁcation algorithms may consider these examples
to be in the category of class-noise , just
because they are located in the “safe-area” of the contrary class. Taking into account that
classiﬁcation algorithms are more sensitive to noise than imbalance ,
diﬀerent overﬁtting management techniques are often used to cope with this problem, i.e.
pruning for decision trees. However, and as stated previously, this may cause to ignore
correct clusters of minority class examples.
The problem of small disjuncts aﬀects to a higher degree, those learning algorithms
whose procedure is based on a divide-and-conquer strategy.
Since the original problem
is divided into diﬀerent subsets, in several iterations this can lead to data fragmentation
 . Some clear examples of this behavior are decision trees ,
and the well-known MapReduce programming model that is used for Big Data applications
 .
The small sample size (lack of data) and small disjuncts are two
closely related topics. This synergy is straightforward as information is barely represented in
those small disjuncts. Therefore, learning classiﬁers cannot carry out a good generalization
when there is not enough data to represent the boundaries of the problem . This way, small disjuncts, noisy data and lack of data
are three inter-related problems that comprise a challenge to the research community in
imbalanced classiﬁcation.
Simpler oversampling approaches based on instance replication do not cope well with
such data intrinsic problems. On the contrary, SMOTE based algorithms implicitly consider
SMOTE for Learning from Imbalanced Data: 15-year Anniversary
a mechanism to counteract both the class imbalance and the small disjuncts. By means of
creating new instances in between close examples, it allows to reinforce the representation
within the clusters. The premise for the good behavior of SMOTE is related to the fact that
the nearest examples should be selected within that very area. Of course than depends on
the number of elements composing the small disjuncts and the value of K selected for the
oversampling. In addition, if the cluster with the small disjunct also contains any example
from the contrary class, i.e. overlapping, SMOTE will not be able to correct this issue of
the within-class imbalance. This is the main reason for using SMOTE hybridizations with
cleaning techniques.
Fortunately, and as introduced in Section 3.2, there are several SMOTE extensions that
try to analyze these clusters of data. This way, cluster-based approaches based on local
densities in conjunction with SMOTE are of high interest for a two-fold reason. On the
one hand, they focus on those areas that truly need the instance generation, i.e. those
with lack of representation. On the other hand, they avoid the overgeneralization problem increasing the density of examples on the cores of the minority class, and making
them sparse far from the centroid. Finally, recent works suggest that changing the representation of the problem, i.e.
taking into account the pairwise diﬀerences among the
data may somehow overcome the issue of small disjuncts . However, we must point out
that the problem of ﬁnding such class areas is still far from being properly addressed, as
most of the clustering techniques previously described make several simpliﬁed assumptions
to address real complex distribution problems.
Another approach is to apply a synergy of preprocessing models, i.e. ﬁltering and/or
instance generation to remove those instances that are actually noisy prior to the SMOTE
application . Some studies
shown that simple undersampling techniques such as random undersampling and cleaning
techniques are known to be robust for diﬀerent levels of noise and imbalance . This way, many hybrid approaches between ﬁltering techniques and SMOTE have
been developed so far, since this allow to improve the quality of the data either a priori (from
the original data), a posteriori (from the preprocessed data) or iteratively while creating
new synthetic instances.
The cooperation between boosting algorithms and SMOTE can successfully address the
problem of the small disjuncts.
These learning algorithms are iterative and they apply
diﬀerent weights to the data instances dynamically as the procedure evolves , and those involving a derivative of SMOTE were mentioned in
Section 3.3. However, we must take into account that in case that several data intrinsic
characteristics (overlapping, small disjuncts, noise, among others) converge in the same
problem, even ensemble learning algorithm will ﬁnd quite diﬃcult to carry out a proper
class discrimination.
Fern´andez, Garc´ıa, Herrera, & Chawla
5.2 Overlapping or Class Separability
Among all data intrinsic characteristics, the overlapping between classes is possibly the
most harmful issue . It is deﬁned as those regions of the data space
in which the representation of the classes is similar. This situation leads to develop an
inference with almost the same a priori probabilities in this overlapping area, which makes
very hard or even impossible the distinction between the two classes. Indeed, any “linearly
separable” problem can be solved by a na¨ıve classiﬁer, regardless of the class distribution
 .
The common occurrence of overlapping and class imbalance implies a harder restriction
for the learning models. This issue was pointed out in the research conducted by Luengo
et al. , in which authors depicted the performance of several datasets ordered with respect to diﬀerent data complexity measures in order to search for some regions of interesting
good or bad behavior. The ﬁndings in this work show that the metrics which measure the
overlap between the classes can better characterize the degree of ﬁnal precision obtained,
in contrast to the imbalance ratio.
The widest use metric to compute the degree of overlap for a given dataset is known
as maximum Fisher’s discriminant ratio, or simply F1 (it must not be
confused with the F1-score performance metric). It is obtained for every individual feature
(one dimension) as:
f = (µ1 −µ2)2
being µ1, µ2, σ2
2 the means and variances of the two classes respectively. Finally, F1 is
obtained as the maximum value for all features.
Datasets with a small value for the F1 metric will have a high degree of overlapping.
Figures 4 to 7 show an illustrative example of this behavior, which have been built with
synthetic data, using two variables within the range [0.0; 1.0] and two classes.
The overlapping areas are directly related to the concept of “borderline examples”
 . As its name suggests, these are deﬁned as those instances that are
located in the area surrounding class boundaries, where the minority and majority classes
overlap. The main issue is again trying to determine whether these examples are simply
noise or they represent useful information. Thus, it is of special importance being able
to identify among diﬀerent types of instances for a given problem, i.e. linearly separable,
borderline, and overlapping data .
This way, we will be able to discard “misleading” instances and to focus on those areas
that are hard to discriminate, carrying out an informed oversampling process. Therefore,
a similar procedure to that used in small disjuncts can be followed in this case, i.e. combining ﬁltering techniques, clustering, and analyzing the neighborhood of each instance to
determine their actual contribution to the problem.
Additionally, feature selection or feature weighting can be combined with SMOTE preprocessing . In this sense, SMOTE preprocessing will
deal with class distribution and small disjuncts (“IR part”) and feature preprocessing somehow reduces the degree of overlapping (“F1 part”). A recent approach proposed a synergy
between SMOTE and both feature and instance selection . The basis of this novel methodology is similar to the previous ones, but
SMOTE for Learning from Imbalanced Data: 15-year Anniversary
Figure 4: F1 = 12.5683
Figure 5: F1 = 5.7263
Figure 6: F1 = 3.3443
Figure 7: F1 = 0.6094
instead of learning a single solution, it provides a Multi-Objective Evolutionary Algorithm
 to achieve a diverse set of classiﬁers
under diﬀerent training sets, i.e. considering diﬀerent features and instances. The key is to
specialize several classiﬁers in diﬀerent areas of the problem, leading to a robust ensemble
5.3 Dataset Shift
The problem of dataset shift is deﬁned as the case where training and test data follow diﬀerent distributions.
There are three potential types of dataset shift:
1. Prior Probability Shift: when the class distribution is diﬀerent between the training
and test sets .
This case can be directly addressed by applying a
stratiﬁed cross validation scheme so that the same number of instances per class are
represented in both sets.
2. Covariate Shift: when the input attribute values that have diﬀerent distributions
between the training and test sets . The incidence of this issue
mainly depends on the partitioning of the data for validation purposes. The widest
used procedure for this task, the stratiﬁed k-fold cross validation may lead to this type
Fern´andez, Garc´ıa, Herrera, & Chawla
of induced dataset shift, as the instances are randomly shuﬄed among the diﬀerent
3. Concept Shift: when the relationship between the input and class variables changes
 . This represents the
hardest challenge among the diﬀerent types of dataset shift. In the specialized literature it is usually referred to as “Concept Drift” .
As described above, dataset shift comprises a general and common problem that can
aﬀect all kind of classiﬁcation problems. In other words, it is not a condition intrinsically
related to data streams or real time processing. Particularly, in imbalanced domains this
issue can be especially sensitive due to the low number of examples for the minority class
 . In the most extreme cases, a single misclassiﬁed example
of the minority class can create a signiﬁcant drop in performance.
In the case of covariate shift, it is necessary to combine the SMOTE oversampling
technique with a suitable validation technique. In particular, we may ﬁnd in the work of
Moreno-Torres et al. a novel approach that is not biased to this problem. Named as
DOB-SCV, this partitioning strategy aims at assigning close-by examples to diﬀerent folds,
so that each fold will end up with enough representatives of every region. Lopez, Fernandez,
and Herrera considered the use of the former procedure in the scenario of imbalanced
classiﬁcation and they found to be an stable performance estimator. Avoiding diﬀerent data
distribution inside each fold will allow researchers on imbalanced data to concentrate their
eﬀorts on designing new learning models based only on the skewed data, rather than seeking
for complex solutions when trying to overcome the gaps between training and test results.
Finally, regarding concept shift more sophisticated solutions must be applied. As we
mentioned in Section 4.1, Ditzler and Polikar integrated the SMOTE preprocessing
within a novel ensemble boosting approach that applies distribution weights among the
instances depending on their distribution at each time step.
5.4 Curse of Dimensionality and Interpolation Mechanisms
Classiﬁcation problems with a large number of attributes imply a signiﬁcant handicap for
the correct development of the ﬁnal models. First, because most of the learning approaches
take into account the whole feature space to build the system, it is harder to ﬁnd a real
optimal solution. Second, because of the overlap between classes for some of these attributes,
which can cause overﬁtting, as pointed out previously.
In addition to the former, we must take into account that the dimensionality problem
also gives rise to the phenomenon of hubness ,
deﬁned as a small number of points that become most of the observed nearest neighbors.
In the case of the SMOTE procedure, this aﬀects the quality of the new synthetic examples
for two inter-related reasons . On the one hand, the computation of
the neighborhood becomes skewed to the actual one. On the other hand, the variance for
the new created instances becomes higher.
One way to overcome this problem can be to predict and rectify the detrimental hub
point occurrences, for example using methods based on naive bayes to avoid borderline
SMOTE for Learning from Imbalanced Data: 15-year Anniversary
examples and outliers . Another simpler solution is to beneﬁt from the use of a feature selection approach prior to the application of the SMOTE
oversampling, as suggested in several works . Some
studies also show that k-NN classiﬁers obtain a higher beneﬁt from this synergy . However, we may ﬁnd other works in which authors follow the contrary procedure, i.e. they ﬁrst rebalance the data and then apply the feature selection scheme , also achieving very good results.
The use of diﬀerent interpolation mechanisms can provide some interesting insight to
this problem. Additionally, there is a need to add more variability to the new synthetic
instances, and this could be achieved by means of a partial extrapolation. Therefore, the
generalization will be positively biased, leading to a better coverage of the “possibly-sparse”
minority examples.
Another interesting perspective to obtain more relevant synthetic instances is to analyze
diﬀerent distance measures to obtain the nearest neighbors. One example is the Malanahobis distance that creates an elliptic area of inﬂuence that could be better suited in case of
overlapping . The Hellinger distance metric, being based on probability distributions and strongly skew insensitive, have been also applied in the context of
imbalanced learning, although rather focused on feature selection . Finally, we must consider the case of mixed attributes in which metrics
such as HOEM or HVDM are mandatory in order to ﬁnd neighbor instances .
Finally, feature extraction to transform the problem into a lower dimensional space is
another way to address this issue. When this process is carried out before the application
of SMOTE, the new clusters of this transformed dataset may allow a better generation of
instances . It also can be applied after the dataset is rebalanced . In this latter case, the feature extraction is suggested for
a better learning process of the classiﬁer.
5.5 Real-Time Processing
As it has been reported in this manuscript, the problem of imbalanced classiﬁcation has been
commonly focused on stationary datasets. However, there is large number of applications
in which data arrive continuously and where queries must be answered in real time. We are
referring to the topic of online learning of classiﬁers from data streams . In this
scenario, the uneven distribution of examples occurs in many case studies, such as video
surveillance , or fault detection . The hitch related to this issue is that it demands a mechanism to
intensify the underrepresented class concepts to provide a high overall performance .
In addition to the former, the dynamical structure of the problem itself also implies
the management of unstable class concepts, i.e.
concept drifts . To this end, several methods have been proposed to deal
with both obstacles from the point of view of preprocessing , particularly using SMOTE , and/or cost-sensitive learning via ensembles of classiﬁers .
The adaptation of SMOTE to this framework is not straightforward. The windowing
process implies that only a subset of the total data is feed to the preprocessing algorithm,
limiting the quality of the generated data.
But if we could even store a history of the
data, the issue of concept drift, both from the point of view of data and class distribution,
diminishes the optimal performance that could be achieved.
Therefore, the correlation
between the generated synthetic instances along time, and the new incoming minority class
instances should be computed. In case of ﬁnding a high variance, an update process must
be carried out.
5.6 Imbalanced Classiﬁcation in Big Data Problems
The signiﬁcance of the topic of Big Data is related to the large advantage from knowledge
extraction for these types of problems with huge Volume, high Velocity, and large in Variety
 .
This implies the need for a novel framework that allows the scalability of the traditional
learning approaches. This framework is MapReduce and its
open source implementation (Hadoop-MapReduce). This new execution paradigm carries
out a “divide-and-conquer” distributed procedure in a fault-tolerant way to adapt for commodity hardware. To allow computational algorithms to be embedded into this framework,
programmers must implement two simple functions, namely Map and Reduce. In general
terms, Map tasks are devoted to work with a subset of the original data and to produce
partial results. Reduce tasks take as input the output from the Maps (all of which must
share the same “key” information) and carry out a fusion or aggregation process.
At present, few research has been developed on the topic of imbalanced classiﬁcation
for Big Data problems . Among all research studies, we must ﬁrst
emphasize the one carried out by R´ıo et al. in which the ﬁrst SMOTE adaptation
to Big Data was adapted to the MapReduce work-ﬂow. Particularly, each Map task was
responsible for the data generation for its chunk of data, whereas a unique Reduce stage
joined the outputs from the former to provide a single balanced dataset.
We may also
ﬁnd a couple of SMOTE extensions to MapReduce, the ﬁrst one based on Neighborhood
Rough Set Theory , and the latter on ensemble
learning and data resampling . However, none of these works
are actual Big Data solutions as their scalability is limited. Finally, a recent approach based
on the use of Graphics Processing Units (GPUs) for the parallel computation of SMOTE
has been proposed by Gutierrez, Lastra, Benitez, and Herrera . The preprocessing
technique is adapted to commodity hardware by means of a smart use of the main memory,
i.e. by including only the minority class instances, and the neighborhood computation via
a fast GPU implementation of the kNN algorithm .
One of the reasons of such few works on the topic is probably due to the technical diﬃculties associated to the adaptation of standard solutions to the MapReduce programming
style. Regarding this issue, the main point is to focus on the development and adoption of
global and exact parallel techniques in MapReduce . Focusing on SMOTE, the problem is mainly related to the use
of a fast and exact kNN approach, considering that all minority class instances should be
considered for the task.
In addition, the use of streaming processors with GPUs is not a straightforward solution. The technical capabilities of the programmer, in conjunction with the restrictions
of memory and data structures of the GPU implementation, imply a signiﬁcant challenge.
Finally, we must also take into account the availability of proper hardware equipment for
an experimental study with such Big Data.
We must also point out that the data repartition applied to overcome the scalability
problem implies additional sources of complexity. We must keep in mind the lack of data
and the small disjuncts , which may
become more severe in this scenario. As we have already pointed out, these problems have
a strong inﬂuence for the behavior of the SMOTE algorithm. This has been stressed as one
possible issue for the low performance of SMOTE in comparison with simpler techniques
such as random oversampling and random undersampling in Big Data problems . This fact implies the necessity of carrying out a thorough design of the data
generation procedure to improve the quality of the new synthetic instances. Additionally,
it is recommended to study diﬀerent possibilities related to the fusion of models or the
management of an ensemble system with respect to the ﬁnal Reduce task.
6. Conclusion
This paper presented a state-of-the-art of SMOTE algorithm in its 15th year anniversary,
celebrating the abundant research and developments. It provided a summative analysis of
the variations of SMOTE devised with respect to both the improvements on diﬀerent drawbacks detected on the original idea and its potential application to more complex prediction
problems such as streaming data, semi-supervised learning, multi-instance and multi-label
learning and regression. In the context of current challenges outlined, we highlighted the
need for enhancing the treatment of small disjuncts, noise, lack of data, overlapping, dataset
shift and the curse of dimensionality. To do so, the theoretical properties of SMOTE regarding these data characteristics, and its relationship with the new synthetic instances,
must be further analyzed in depth. Finally, we also posited that it is important to focus on
data sampling and pre-processing approaches (such as SMOTE and its extension) within
the framework of Big Data and real-time processing.
Developments and applications to new ﬁelds of more reﬁned data preprocessing approaches which follow a SMOTE-inspired similar oversampling strategy based on the artiﬁcial generation of data is still a demanding issue for the next 15 years. To inspire this
purpose, we wanted provide a valuable overview to this respect for both, beginners and
researchers working in any perspective of data mining, and especially in imbalance learning
scenarios.
Acknowledgments
This work have been partially supported by the Spanish Ministry of Science and Technology under projects TIN2014-57251-P, TIN2015-68454-R and TIN2017-89517-P; the Project
Fern´andez, Garc´ıa, Herrera, & Chawla
BigDaP-TOOLS - Ayudas Fundaci´on BBVA a Equipos de Investigaci´on Cient´ıﬁca 2016;
and the National Science Foundation (NSF) Grant IIS-1447795.