Sliding Shapes for 3D Object Detection in Depth Images
Shuran Song
Jianxiong Xiao
Princeton University
 
Abstract. The depth information of RGB-D sensors has greatly simpliﬁed some
common challenges in computer vision and enabled breakthroughs for several
tasks. In this paper, we propose to use depth maps for object detection and design a 3D detector to overcome the major difﬁculties for recognition, namely
the variations of texture, illumination, shape, viewpoint, clutter, occlusion, selfocclusion and sensor noises. We take a collection of 3D CAD models and render
each CAD model from hundreds of viewpoints to obtain synthetic depth maps.
For each depth rendering, we extract features from the 3D point cloud and train
an Exemplar-SVM classiﬁer. During testing and hard-negative mining, we slide a
3D detection window in 3D space. Experiment results show that our 3D detector
signiﬁcantly outperforms the state-of-the-art algorithms for both RGB and RGB-
D images, and achieves about ×1.7 improvement on average precision compared
to DPM and R-CNN. All source code and data are available online.
Introduction
Template matching with the image pattern is inadequate for three-dimensional
scene analysis for many reasons, such as occlusion, changes in viewing angle,
and articulation of parts. The patterns at image level are not invariant.
– Nevatia and Binford, 1977 .
Despite rapid progress on image patch classiﬁcation , object detection remains
an open research challenge. Meanwhile, the availability of inexpensive RGB-D sensors, such as Microsoft Kinect, Apple PrimeSense, Intel RealSense, and Google Project
Tango, has greatly simpliﬁed some common challenges in vision and enabled breakthroughs for several tasks, such as body pose estimation , intrinsic image , segmentation and 3D modeling . In this paper, we propose an algorithm to
use depth images for generic object detection and we achieve signiﬁcantly performance
improvement compared to the state-of-the-art results on RGB images .
The main idea is to exploit the depth information in a data-driven fashion to overcome the major difﬁculties in object detection, namely the variations of texture, illumination, shape, viewpoint, self occlusion, clutter and occlusion. For a given object
category (e.g. chair), we use Computer Graphics (CG) CAD models from the Internet.
We render each CG model from hundreds of viewpoints to obtain synthetic depth maps,
as if they are viewed by a typical RGB-D sensor. As shown in Fig. 1, for each rendering, a feature vector is extracted from the 3D point cloud corresponding to the rendered
depth map to train an exemplar Support Vector Machine (SVM) , using negative data
Shuran Song and Jianxiong Xiao
(a) Training each 3D exemplar detector independently.
(b) Testing with exemplars.
Fig. 1. Sliding Shapes: We extract 3D features of point cloud from depth rendering of CG model
to train a 3D classiﬁer. And during testing time, we slide a window in 3D to evaluate the score
for each window using an ensemble of Exemplar-SVMs.
from a RGB-D dataset . During testing and hard-negative mining, we slide a 3D detection window in the 3D space to match the exemplar shape and each window. Finally,
we use depth map segmentation to further improve the performance.
The success of our design is based on several key insights: To handle texture and
illumination variance, we use depth maps instead of RGB images. To handle shape
variance, we use a data-driven approach to leverage a collection of CG models that
cover the space of shape variance in real world. We also add a small variance on the
size of CG model to improve the robustness. Furthermore, in contrast to direct mesh
alignment , learning the SVM using both positive and negative data also increases
the generality of the detector. To handle viewpoint variance, we can densely render
different viewpoints of an object to cover all typical viewing angles. To handle depthsensor error and noise, we use CG models to obtain perfect rendering and use it as
positive training data (experiments in Table. 2 shows that it helps a lot). To bridge the
domain gap between CG training data and RGB-D testing data, we render the depth
map (but not color) as if the CG model is viewed from a typical RGB-D sensor. To
handle clutter (e.g. a chair with its seat under a table), we use 3D sliding window with
a mask to indicate which parts should be considered during classiﬁcation. To handle
inter-object occlusion, we make use of the depth map to reason about the source of
occlusion and regard the occluded area as missing data. To make use of self-occlusion,
we render the CG model and compute the Truncated Signed Distance Function (TSDF)
 as a feature.
Since our generic object detector does not reply on any assumption about the background or requires a dominant supporting plane , and its single-view nature
doesn’t require a (semi-)complete scan of an object , it can be used as a basic building block for general scene understanding tasks. To improve the testing speed during
3D convolution, we generalize integral image to 3D to skip empty windows.
In the following section, we will describe our algorithm in greater details. In Section
3, we will talk about the evaluation metric and experiments to evaluate the algorithm.
In Section 4, we will discuss the relation of our proposed method with existing ones.
Sliding Shapes for 3D Object Detection in Depth Images
Fig. 2. Training procedure: We use a collection of CG models to train a 3D detector. For each
CG model, we render it from hundreds of view angles to generate a pool of positive training
data. For each rendering, we train an Exemplar-SVM model. And we ensemble all SVMs from
renderings of CG chair models to build a 3D chair detector.
The Sliding Shapes detector
During training (Sec. 2.1), we learn an ensemble of linear Exemplar-SVM classiﬁers,
each of which is trained with a rendered depth map from CG model as the single positive
and many negatives from labeled depth maps. During testing (Sec. 2.2), we take a depth
image with the gravity direction as input. The learned SVMs are used to classify a
sliding window in 3D and output 3D bounding boxes with detection scores. We design
four types of 3D features (Sec. 2.3) and propose several methods to handle clutter,
occlusion and missing depth (Sec. 2.4).
Fig. 2 shows the the training process of our Sliding Shapes detector, where we treat
each viewpoint rendering as an exemplar and train a separate classiﬁer for it.
Rendering depth maps For each object category, a set of CG models with typical
shapes is collected from the Internet to cover the intra-category shape variance. Because
most objects in real environment have some support surfaces (e.g. chairs are typically
on the ﬂoor), we also synthesize a support surface when rendering the graphic model to
emulate such condition. For each CG model, we render it from different view angles and
locations in the 3D space. Speciﬁcally, we render the CG models varying the following
parameters: orientation, scale, 3D location, and camera tilt angle. Some assumptions are
made based on dataset statistics and observation to reduce the sample space. We assume
most objects are aligned on gravity direction so there is only rotation around gravity
axis. We also obtained the statistics of object sizes and 3D locations of each category
from the training set, and sample viewpoint parameters based on this prior. Apart from
sampling the above parameters, we also slightly scaling the meshes to improve the
robustness. Finally, we render the depth map as if the CG model is viewed from a
typical RGB-D sensor, by using the same camera intrinsic parameters, and resolution
to virtual camera.
Training Exemplar-SVMs As shown in shown in Fig. 1, after converting each depth
rendering of CG model to a 3D point cloud, we extract a feature vector and use it as
Shuran Song and Jianxiong Xiao
positive to train a linear Exemplar-SVM . The initial negatives are randomly picked
point cloud from annotated Kinect images (RMRC dataset ) that do not overlap with ground truth positives. We perform hard negative mining by searching hard
negatives over the entire training set.
No calibration Although we train each Exemplar-SVM separately, we do not calibrate
our detectors as , mainly because of the limited size of RMRC dataset . Calibration requires the training (or validation) set to have a similar positive distribution
as the testing set. Especially, most of the exemplar should ﬁre at least once in order to
adjust their scores accordingly. In our case, we have an Exemplar-SVM for each viewpoint in each CG model. The total number of exemplar models largely exceeds the total
number of positive object instances in RMRC dataset, and some detectors will never
ﬁre in the training set, which makes calibration not possible.
During testing, we exhaustively classify each possible bounding box in the 3D space
using all Exemplar-SVMs, each of which evaluates whether the corresponding shape
exists inside the bounding box, and output a detection score. Then we perform nonmaximum suppression on all detection boxes in 3D.
3D local search Given an Exemplar-SVM trained on a CG model rendered at a speciﬁc 3D location relative to the virtual camera, we perform 3D convolution only at the
nearby region. Such restriction on search space improves the speed as well as detection accuracy, because objects far away from the training location are of different point
density, and presents different self-occlusion condition due to their difference in view
angles. The SVM and 3D feature may not be robust enough to model this difference.
Therefore, we take a more conservative search with restriction to only nearby locations.
Jumping window In 2D sliding window scheme, it is not a trivial task to efﬁciently
ﬁlter out unnecessary window positions (e.g. ). However in 3D, there is a lot of empty
space which can be safely skipped. To identify the empty boxes and skip them during
convolution, a 3D integral image is computed for each testing image, where each cell
stores the sum of point count of all cells that on the front-left-up side of the cell. During
convolution, given a model’s window size and its current cell location, the total number
of points inside this window can be quickly calculated from the 3D integral image in
constant time. If the total number of points inside this window is smaller than 50, our
detector skips this window without performing the dot product.
Bounding box adjustment The initial resulting bounding boxes form convolution are
aligned with the deﬁned feature axes, which is not optimal for most objects. Therefore,
after we obtain the axis-aligned bounding box, we replace it with a tighter bounding
boxes aligned with objects’ principle axes, which are imported from the CG models.
View-dependent 3D features
To support sliding a window in 3D, the 3D space is divided into cubic cells of size 0.1
meter, and several features are extracted from each cell. To capture properties of 3D
objects such as their geometrical shape, orientation and distance to camera, we design
the following features and combine all of them, forming a discriminative descriptor.
Sliding Shapes for 3D Object Detection in Depth Images
Point Density
All Combined
Fig. 3. Visualization of “inverse” features using nearest neighbor (refer to the footnote). We
reduce the feature dimension into three and map them into RGB color space. Therefore, similar
colors between two cells indicate that they are similar in the high-dimensional feature space.
Point density feature To describe point density distribution inside a cell, we divide
each cell into 6 × 6 × 6 voxels, and build a histogram of the number of points in each
voxel. A 3D Gaussian kernel is used to weight each point, canceling the bias of the voxel
discretization. After obtaining the histogram inside the cell, which is a 216 dimensional
vector, we randomly pick 1000 pairs of entries and compute the difference within each
pair (inspired by the stick feature in ). The stick feature is then concatenated with the
original count histogram. Such descriptor captures both the ﬁrst order (point count) and
second order (count difference) statistics of the point cloud.
3D shape feature Apart from point distribution across voxels, their distribution within
voxels are also important clue, which we use local 3D shape feature to encode. We
divide each cell into 3×3×3 voxels, and represent the internal point cloud distribution
of a voxel by their scatter-ness (λ1), linear-ness (λ1 −λ2) and surface-ness (λ2 −λ3),
obtained from the principal components of the point cloud (assume the eigenvalues of
the covariance matrix of the points are λ1 > λ2 > λ3).
3D normal feature Surface normal is critical to describe the orientation of an object.
To compute 3D normals, we pick 25 nearest neighbor for each point, and estimate
the surface normal at that point as the direction of the ﬁrst principal component. We
divided the orientation half-sphere into 24 bins uniformly, and for each cell, we build a
histogram of the normal orientation across these bins as normal feature.
TSDF feature Self-occlusion is a useful cue for view-based shape matching. We adopt
Truncated Signed Distance Function (TSDF) as one of the features. Different from
other features that only describe local information within a single cell, TSDF feature is
a volumetric measure of the global shape. For each cell divided into 6 × 6 × 6 voxels,
TSDF value of each voxel is deﬁned as the signed distance between the voxel center
and the nearest object point on the line of sight from the camera. The distance is clipped
to be between -1 and 1 and the sign here indicates whether cell in front of or behind the
surface. After computing the TSDF value for each voxel, we use the same random-stick
as in point density feature to calculate difference within pairs and concatenate it with
the original TSDF vector.
Shuran Song and Jianxiong Xiao
'()*+,$&-+.,'-&
+.*+,$&-+.,'-&
/001(*+'.&
Out of sight
Free space
Surface with points
Inter-object occlusion
Self-occlusion
Sliding window
(a) Occlusion reasoning using the occluder’s location.
Don’t Care
(b) Occupation mask to slide a shape.
Fig. 4. Beyond sliding windows. Depth and 3D mesh are used to handle occlusion and clutter.
Feature coding and combining We perform dictionary coding on top each of them
 . Speciﬁcally, we use k-means to obtain 50 cluster centers for each type of the features as our codebook. Every feature vector x is then coded as a 50-dimensional vector
f containing its distance to each of the 50 centers: f(i) = exp(−(∥x −ci∥2))/σ2
i is the standard deviation of i-th cluster. After feature coding we concatenate
all coded feature vectors to from the ﬁnal combined feature vector.
Feature visualization In order to visualize our features, we use a nearest neighbor approach similar as to “inverse” our feature1. Fig. 3 shows an example to illustrate
what property each type of feature captures. The inverse TSDF feature has very distinctive color for cells in front of and behind surfaces, indicating its ability to describe
the relation between cells and surfaces. The 3D normal feature captures surface orientation but not the shape inside the cell. The point destiny feature doesn’t capture the
right shape on the leg of the chair, although the point cloud used for feature reconstruction has similar point density with the original cell. For the shape feature, because the
covariance matrix of point coordinates cannot be calculate for cells has points smaller
than 3, therefore it is unable to distinguish empty cell with cells have 1 to 3 points. The
combined feature achieves the best reconstruction result, suggesting that it has a better
ability to describes the model than each single feature alone.
Beyond sliding window
Different from standard sliding window approach, we improve robustness of our model
by adjusting features according to occlusion, missing value and clutter.
Occlusion modeling A difﬁcult problem in 2D object detection is occlusion. Occluders usually lower the detection scores considerably, since they replace part of the target
objects with themselves in the 2D image, which usually violates learned rules describing the target objects. Ideally, one would exclude the occluded region and mark them
as “don’t care”, but robustly recognizing such region is just as challenging. However,
because we have depth as input, such occlusion can be easily identiﬁed. In the TSDF
feature described above, voxels with value -1 (behind surface) indicates this voxel being occluded. We want to exclude only the real inter-object occlusion region and use
1 Firstly, a large pool of feature vector (for one single cell) and their corresponding point cloud are collected. Then given
a new feature vector, we reconstruct the point cloud by searching for its nearest neighbor among all collected feature
vectors, and replace the original cell with point cloud from the nearest neighbor found.
Sliding Shapes for 3D Object Detection in Depth Images
#view(#CG) #Kinect # view(#CG) #Kinect #view(#CG) #Kinect #view(#CG) #Kinect #view(#CG) #Kinect
Sliding Shapes
Kinect align
Table 1. Number of positive training samples.
the self-occlusion ones as an useful cue. We identify the occlusion type by checking
depth value of the occlusion source, and compare it with the depth of current sliding
window (Fig. 4(a)). If the occlusion source is outside the sliding window, it is an interobject occlusion. For voxels under inter-object occlusion, we set their feature vectors
(after feature coding) to zeros, so that they make no contribution to the ﬁnal detection
score in the linear SVM. After setting their feature to zeros, we also append an extra
bit ﬂagging to the end of the feature vector, giving SVM a way to weight the special
condition. To avoid false positives on those heavily occluded location, we count the
total number of occluded cells, and if it is above threshold, we will keep the feature
unchanged, which naturally penalizes the detection scores.
Missing depth and boundary Similarly, objects with vast missing depth or partially
outside the ﬁeld of view are likely to get missed if not handled explicitly, since some
part of the objects would have the same feature as empty cells and thus considerably
lower the detection score. Therefore, we identify those cells and set their features to
zeros. Similar as occlusion, for the cells with missing depth or out of sight, we also
append an extra bit ﬂagging to the end of the feature vector.
Clutter In general, a 3D bounding box is not a tight representation for objects, leaving a large portion of empty space that produces redundant and sometimes misleading
feature. Especially, our training positives are clean CG models which implicitly assume
empty cell inside the bounding box apart from the object of interest. But during testing,
the object are often surrounded by clutter. Therefore, we construct an occupation mask
for each training CG model to select the cells inside or close to its mesh surface (Fig.
4(b)), and only use the features inside the occupation mask to train classiﬁers.
Post-processing using segmentation We observe that the top false positives of our
algorithm detect an object as a part of a big object (e.g. the ﬁrst row of Fig. 10), because
of the local nature of sliding windows. To prune this kind of false positives, we use
plane ﬁtting on 3D point cloud to obtain a segmentation. For each detection result B,
we pick the largest segment Si inside its bounding box and compute the overlap ratio
area(Si∩B)
area(Si) . If R is larger than a certain threshold (learned from training set), it
means that the current hypothesis is a part of a larger object, and we reduce its score by
1. This post-processing step is more helpful for toilet and sofa, while less helpful to bed
and table (see Table 2).
Evaluation
The 3D CG models that we used for training are collected from Trimble 3D Warehouse.
The total number of CG models and the rendering view point are shown in Table 1.
Shuran Song and Jianxiong Xiao
Fig. 5. Detection results. Here we show multiple detections in one image.
We evaluate our Sliding Shapes detector on RMRC dataset (a subset of NYU Depth
v2 with 3D box annotation derived from ). We choose ﬁve common indoor
objects: chair, toilet, bed, sofa, and table, and manually go through the annotation to
make sure that they are correctly labelled. We split RMRC dataset into 500 depth images
for training and 574 depth images for testing. We split the dataset in a way that the
images from same video are grouped together and appear only in training or testing set,
and try to balance the instance number in training and testing set for each category.
Our algorithm takes a depth image from RGB-D sensor with the gravity direction
as input. Aligning the point cloud and CG model with the gravity direction enables the
axis-aligned sliding window for detection. Note that the gravity direction can be obtained via several ways. For example, if a RGB-D camera is mounted on a robot, we
know the robot’s conﬁguration and its camera tilt angle. For the cameras on the mobile
devices, we can use the accelerometer to obtain the gravity direction and camera’s relative tilt angle. For this paper, the gravity direction for the RMRC dataset is provided
as ground truth. For datasets without ground truth gravity direction, it is also easy to
compute by ﬁtting planes to the ﬂoor and walls .
Without local search and jumping window, our time complexity is exactly the same
with Exemplar-SVMs . On average, there are 25,058 3D detection windows per image. Local search reduces it to 19%. Jumping window reduces it to 44%. Using both,
it reduces to 8%. For testing, it takes about 2 second per detector to test on a depth
image in Matlab. The computation is naturally parallelizable except the non-maximal
suppression at the end of detection. For training, it takes 4 to 8 hours to train a single
detector with single thread in Matlab, which is also naturally parallelizable.
Evaluation metric
We adopt the standard 2D object detection evaluation scheme as in PASCAL VOC ,
with the following modiﬁcations. PASCAL VOC evaluation criteria uses 2D bounding
box overlapping ratio (intersection over union), assuming they are aligned with images
axis. For 3D, we calculate the 3D bounding box overlapping ratio, we assume the boxes
Sliding Shapes for 3D Object Detection in Depth Images
RGB-D or 3D
Sliding Shapes 0.316 0.765 0.331 0.749 0.643 0.736 0.644 0.736 0.381 0.741 0.412 0.751 0.315 0.403 0.339 0.418 0.289 0.474 0.314 0.478
without seg
0.312 0.752 0.326 0.741 0.588 0.681 0.588 0.681 0.381 0.740 0.411 0.750 0.303 0.384 0.324 0.390 0.289 0.474 0.313 0.478
0.144 0.211 0.180 0.268 0.183 0.269 0.147 0.243 0.187 0.254 0.050 0.040 0.074 0.049 0.012 0.008 0.035 0.015
Kinect align
0.251 0.607 0.251 0.566 0.440 0.528 0.444 0.531 0.369 0.693 0.403 0.696 0.20 0.235 0.190 0.202 0.265 0.405 0.278 0.372
0.129 0.286 0.185 0.327 0.449 0.538 0.456 0.544 0.164 0.280 0.208 0.296 0.052 0.043 0.075 0.053 0.012 0.008 0.035 0.015
DPM-VOC 
0.176 0.446
0.163 0.213
0.127 0.175
DPM-SUN 
0.131 0.345
0.309 0.532
0.279 0.503
0.109 0.132
0.120 0.157
DPM-RMRC 
0.115 0.269
0.344 0.419
0.318 0.427
0.099 0.137
0.045 0.048
RCNN-VOC 
0.182 0.342
0.200 0.203
0.213 0.237
Table 2. Comparision. The numbers are the average precisions for various algorithms, categories
and evaluation metrics. 3D and 2D are evaluation using normal ground truth boxes; 2D+ and 3D+
are evaluation using all ground truth boxes including difﬁcult cases. The best preforming 2D and
3D algorithms are in bold. * indicates the result is evaluated using different training-testing splits.
are aligned with gravity direction, but make no assumption on the other two axes. To
compare with 2D detection, the evaluation on 2D is done by projecting both ground
truth and detection boxes into 2D and compute their 2D overlapping ratio. For 2D, a
predicted box is considered to be correct if the overlapping ratio is more than 0.5. To let
the same result produce similar precision-recall curve for 2D and 3D evaluation empirically, we set the threshold to be 0.25 for 3D. Similar as PASCAL VOC, we also add a
difﬁcult ﬂag to indicate whether the ground truth is difﬁcult to detect. The difﬁcult cases
include heavy occlusion, missing depth and out of sight. We evaluate on normal ground
truth boxes (denoted as 3D and 2D), as well as on all ground truth boxes including
difﬁcult cases (denoted as 3D+ and 2D+) respectively.
Experiments
Fig. 5 shows example results of our Sliding Shapes detector, and Fig. 10 and 11 show
some failure cases. Our detector not only recognizes the object, but also identiﬁes its orientation and type of 3D style, which is imported from the corresponding model proposing the detection. Fig. 8 demonstrates the power of our design. Row 1 and 2 are cases
where our detector successfully handles occlusion. Row 3 shows that the occupation
masks can ﬁlter out the clutter, where a dining table is partially inside the proposed
box, yet it does not affect the chair detector since it is not in the occupation mask. Row
4 shows the case with severe missing depth. Even with the whole back of the chair
missing, our detector is able to detect it, although the corresponding CG model is not
identical to the object.
Comparison We compare our Sliding Shapes detector quantitatively with 2D and 3D
detector. In the 2D case, we compare with standard DPM and the state-of-the-art
deep learning algorithm RCNN . We show the result of DPM trained on PASCAL
VOC 2010 , SUN2012 , and RMRC dataset. In RGBD/3D case we compare with
 which use 2D HOG on RGBD image with 2D sliding window, and gDPM 
which trains a geometry driven deformable part model. Table 2 shows the average precision. Our approach achieves about ×1.7 improvement on average precision compared
to the best of all RGB algorithms, and also outperforms other RGB-D or 3D detectors.
Shuran Song and Jianxiong Xiao
score(rank)
Detection depth
-0.50 (10)
Fig. 6. True positives. Besides labels, our detector also predicts object orientation and 3D model.
Sliding Shapes for 3D Object Detection in Depth Images
Sliding Shape
without Seg
KinectAlign
(b) toilet
Fig. 7. Precision-recall curve. The top rows shows the evaluation on 2D (DPM) and 3D (all
others) ground truth, The bottom rows shows the evaluation on 2D+ and 3D+ ground truth. The
best preforming 2D and 3D algorithms are in bold.
Kinect vs. CG model To justify our choice of CG models as training data, we evaluate
the performance using Kinect point cloud as positive training data. The point clouds
are picked from training set ground truth which are labeled as non difﬁcult (to avoid
heavy occlusion or vast missing depth). Then we use exactly the same feature, negative
data and training procedure to train an Exemplar-SVM for each positive Kinect point
cloud. In our proposed approach, a large number of rendered CG models are used as
training data. To achieve a fair comparison, we limit the size of the CG training set to
be no larger than the Kinect point cloud: for each kinect point cloud positive example,
we pick the most similar rendered CG model to it and add to the CG training set. This
is done by testing all CG models on training set and picked top one that has highest
conﬁdence for each positive ground truth. Thus the total number of picked CG models
can only be smaller than the total number of Kinect positives, because multiple Kinect
positives may correspond to one CG model.
In Table 2 [Kinect align], we shows that even with less positive training data, detectors trained on CG models still peform siginﬁcanly better. We believe that the real
Kinect depth data is inferior as positive examples due to its high variation in sensor
noise, missing depth, occlusion and background clutter. For instance, in order for a
point cloud to match well to an exemplar with certain parts occluded, the point cloud
must have similar occlusion condition otherwise parts available for matching will be insufﬁcient, whereas if the positive data is complete, candidate point clouds can be more
ﬂexible as long as there are enough portion of visible parts. Besides, it is very rare for
two object instances to have similar sensor noise if they are not captured under exact
same condition. Usually, classiﬁers trained on objects isolated from background or another dataset have inferior performance. However, our algorithm is able to bridge the
domain gap between CG training data and RGB-D testing data, and achieve a signiﬁcant improvement. We also tested the combination using both CG models and Kinetic
point clouds as positive to train the detector. Table 2 [Kinect+CG] shows that it is does
Shuran Song and Jianxiong Xiao
score(rank)
Detection depth
Fig. 8. Challenging cases. The difﬁculties are mainly come from occlusion, missing value and
clutter. Green in RGB image highlights the points in box. In CG + points the point cloud in
detection box is replaced by the exemplar CG model. Black in depth indicates missing value.
3D+ ground truth
number of rendering
3D ground truth
number of rendering
(a) AP vs. number of viewpoints.
3D+ ground truth
number of model
3D ground truth
number of model
(b) AP vs. number of CG models.
Fig. 9. Average Precision (AP) vs. number of positive data.
not yield a better performance than just using CG models alone, which suggests that the
information is redudant and the point cloud quality of Kinect model are bad.
Number of exemplars We experiment on how the size of positive training data (number of GC models and number of viewpoint rendering) affect the performance. Given
number of training view points / model, we randomly pick 5 possible cases to evaluate the average precision. Fig. 9 shows how the average precision changes, when the
number of rendering viewpoints and number of CG models changes.
Related works and discussions
Our work has been inspired by research in object recognition of images, range scans,
depth maps, RGB-D and CAD models, but we only refer to the most relevant ones here.
Image-based detection: Popular detectors typically train a classiﬁer on image area
within a window, and test using the classiﬁer via a sliding window or on selected areas . Typical ways to account for object variation are deformable parts
Sliding Shapes for 3D Object Detection in Depth Images
score(rank)
Detection depth
-0.87 (160)
-0.87 (160)
-0.76 (41)
–0.81 (42)
–0.76 (34)
Fig. 10. False positives. Without using color or context information, our detector sometime get
confused between objects with similar shape.
with pictorial structure and ensemble of exemplars . showed that the
latter option is much simpler and has the generalizability to all object categories. Our
model can be understood as a novel way to extend this framework to 3D. There are also
works on using CAD models for training , but they are not for depth images.
Semantic segmentation: A popular way of formulating object recognition in 3D is
to predict the semantic label for each region of a depth map or 3D mesh . Because of the bottom-up nature, these algorithms can only see a part of
object but not the whole object. One advantage of a sliding window based approach is
to enable the classiﬁer to use the information for the whole object to make a decision.
Voting: There are many works focus on how to integrate local information via voting
 , such as Hough voting or Implicit Shape Model. This type of models can
consider multiple local regions at the same time for object recognition, but it is difﬁcult
to formulate them in a data-driven machine-learning framework to weight the relative
importance and correlation among objects parts (especially negative correlation).
Keypoint matching: Just as SIFT keypoints for image matching , a popular type
of algorithms is to detect keypoints on a 3D point cloud or a mesh, generate descriptors for the keypoints (e.g. spin image and 3D shape context), and use the
matching to align with models in the training data. Same as voting-based approach, the
non-learning nature of this type of algorithms make it very difﬁcult to discriminatively
learn from a data to weight the importance of different keypoints.
Shuran Song and Jianxiong Xiao
Fig. 11. Misses. Note that in many cases there are considerable missing value in depth data, object
out of sight, or ground truth being poorly localized. We also miss object instances very different
from training data, such as folded chairs.
Model ﬁtting: Similar to keypoint matching, model ﬁtting algorithms align an input
with the training models , but without using descriptors. There are robust algorithms
that ﬁt 3D shapes to the scene . But again, because of the non-data-driven nature,
these approachs have the same problem that it cannot learn from data.
3D classiﬁcation: Classiﬁcation-based approaches typically consider the whole object at the same time by extracting a holistic feature for the whole
object and classifying the feature vector via a classiﬁer. But the typical setting is to
have the segmented object as the input (or even a solo 3D model with complete mesh),
and classify an object into one of the ﬁxed categories, which is a much easier task than
object detection that needs to localize the object and tell a non-object window apart.
2.5D detector: There are several seminal works that try to extend standard 2D imagebased object detector to use depth maps . The main difference is that
our algorithm operates fully in 3D, using 3D sliding windows and 3D features, which
can handle occlusion and other problems naturally.
RGB-D scene understanding: Besides the RGB-D segmentation and detection works
mentioned above, proposed to estimate the room layout, support surfaces,
and scene understanding for the whole room including objects. Our 3D detector can be
used as a basic building block of object detection for all these higher level tasks.
Conclusion
We propose an algorithm for generic 3D object detection for RGB-D images. Our detector can exploit the depth information in a data-driven fashion to overcome the major
limitations in object detection, namely the variations of texture, illumination, shape,
viewpoint, self occlusion, clutter, occlusion and sensor noises. One of the major limitation now is the lack of a good RGB-D testing set for evaluation that contains more
images, more instances, and more reliable annotation. Currently, we are capturing a
large-scale RGB-D dataset using the new Microsoft Kinect V2 time-of-ﬂight sensor.
As future work, we plan to investigate how to combine with RGB-based detection, and
learn the 3D features automatically from data , as well as exploring context information in 3D .
Sliding Shapes for 3D Object Detection in Depth Images