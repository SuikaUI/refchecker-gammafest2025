Conditional Probability Models for Deep Image Compression
Fabian Mentzer∗Eirikur Agustsson∗
Michael Tschannen
Radu Timofte
Luc Van Gool
 
 
 
 
 
ETH Z¨urich, Switzerland
Deep Neural Networks trained as image auto-encoders
have recently emerged as a promising direction for advancing the state-of-the-art in image compression. The key challenge in learning such networks is twofold: To deal with
quantization, and to control the trade-off between reconstruction error (distortion) and entropy (rate) of the latent
image representation. In this paper, we focus on the latter
challenge and propose a new technique to navigate the ratedistortion trade-off for an image compression auto-encoder.
The main idea is to directly model the entropy of the latent
representation by using a context model: A 3D-CNN which
learns a conditional probability model of the latent distribution of the auto-encoder. During training, the auto-encoder
makes use of the context model to estimate the entropy of its
representation, and the context model is concurrently updated to learn the dependencies between the symbols in the
latent representation. Our experiments show that this approach, when measured in MS-SSIM, yields a state-of-theart image compression system based on a simple convolutional auto-encoder.
1. Introduction
Image compression refers to the task of representing images using as little storage (i.e., bits) as possible. While in
lossless image compression the compression rate is limited
by the requirement that the original image should be perfectly reconstructible, in lossy image compression, a greater
reduction in storage is enabled by allowing for some distortion in the reconstructed image. This results in a so-called
rate-distortion trade-off, where a balance is found between
the bitrate R and the distortion d by minimizing d + βR,
where β > 0 balances the two competing objectives. Recently, deep neural networks (DNNs) trained as image autoencoders for this task led to promising results, achieving
better performance than many traditional techniques for image compression . Another advantage of
∗The ﬁrst two authors contributed equally.
MS-SSIM on Kodak
Rippel & Bourdev
Johnston et al.
Ball´e et al.
Theis et al.
Figure 1: State-of-the-art performance achieved by our simple compression system composed of a standard convolutional auto-encoder and a 3D-CNN-based context model.
DNN-based learned compression systems is their adaptability to speciﬁc target domains such as areal images or stereo
images, enabling even higher compression rates on these
domains. A key challenge in training such systems is to optimize the bitrate R of the latent image representation in the
auto-encoder. To encode the latent representation using a
ﬁnite number of bits, it needs to be discretized into symbols
(i.e., mapped to a stream of elements from some ﬁnite set
of values). Since discretization is non-differentiable, this
presents challenges for gradient-based optimization methods and many techniques have been proposed to address
them. After discretization, information theory tells us that
the correct measure for bitrate R is the entropy H of the
resulting symbols. Thus the challenge, and the focus of this
paper, is how to model H such that we can navigate the
trade-off d + βH during optimization of the auto-encoder.
Our proposed method is based on leveraging context
models, which were previously used as techniques to im-
 
prove coding rates for already-trained models ,
directly as an entropy term in the optimization. We concurrently train the auto-encoder and the context model with respect to each other, where the context model learns a convolutional probabilistic model of the image representation in
the auto-encoder, while the auto-encoder uses it for entropy
estimation to navigate the rate-distortion trade-off. Furthermore, we generalize our formulation to spatially-aware networks, which use an importance map to spatially attend the
bitrate representation to the most important regions in the
compressed representation. The proposed techniques lead
to a simple image compression system1, which achieves
state-of-the-art performance when measured with the popular multi-scale structural similarity index (MS-SSIM) distortion metric , while being straightforward to implement with standard deep-learning toolboxes.
2. Related work
Full-resolution image compression using DNNs has attracted considerable attention recently. DNN architectures
commonly used for image compression are auto-encoders
 and recurrent neural networks (RNNs) .
The networks are typically trained to minimize the meansquared error (MSE) between original and decompressed
image , or using perceptual metrics such as
MS-SSIM . Other notable techniques involve progressive encoding/decoding strategies , adversarial
training , multi-scale image decompositions , and
generalized divisive normalization (GDN) layers .
Context models and entropy estimation—the focus of
the present paper—have a long history in the context of
engineered compression methods, both lossless and lossy
 . Most of the recent DNN-based lossy
image compression approaches have also employed such
techniques in some form. uses a binary context model
for adaptive binary arithmetic coding . The works of
 use learned context models for improved coding performance on their trained models when using adaptive arithmetic coding. use non-adaptive arithmetic
coding but estimate the entropy term with an independence
assumption on the symbols.
Also related is the work of van den Oord et al. ,
who proposed PixelRNN and PixelCNN, powerful RNNand CNN-based context models for modeling the distribution of natural images in a lossless setting, which can be
used for (learned) lossless image compression as well as
image generation.
3. Proposed method
Given a set of training images X, we wish to learn a
compression system which consists of an encoder, a quan-
1 
tizer, and a decoder. The encoder E : Rd →Rm maps an
image x to a latent representation z = E(x). The quantizer
Q : R →C discretizes the coordinates of z to L = |C| centers, obtaining ˆz with ˆzi := Q(zi) ∈C, which can be losslessly encoded into a bitstream. The decoder D then forms
the reconstructed image ˆx = D(ˆz) from the quantized latent representation ˆz, which is in turn (losslessy) decoded
from the bitstream. We want the encoded representation ˆz
to be compact when measured in bits, while at the same time
we want the distortion d(x, ˆx) to be small, where d is some
measure of reconstruction error, such as MSE or MS-SSIM.
This results in the so-called rate-distortion trade-off
d(x, ˆx) + βH(ˆz),
where H denotes the cost of encoding ˆz to bits, i.e., the entropy of ˆz. Our system is realized by modeling E and D as
convolutional neural networks (CNNs) (more speciﬁcally,
as the encoder and decoder, respectively, of a convolutional
auto-encoder) and minimizing (1) over the training set X,
where a large/small β draws the system towards low/high
average entropy H. In the next sections, we will discuss
how we quantize z and estimate the entropy H(ˆz). We note
that as E, D are CNNs, ˆz will be a 3D feature map, but for
simplicity of exposition we will denote it as a vector with
equally many elements. Thus, ˆzi refers to the i-th element
of the feature map, in raster scan order (row by column by
3.1. Quantization
We adopt the scalar variant of the quantization approach
proposed in to quantize z, but simplify it using ideas
from . Speciﬁcally, given centers C = {c1, · · · , cL} ⊂
R, we use nearest neighbor assignments to compute
ˆzi = Q(zi) := arg minj∥zi −cj∥,
but rely on (differentiable) soft quantization
exp(−σ∥zi −cj∥)
l=1 exp(−σ∥zi −cl∥)
to compute gradients during the backward pass. This combines the beneﬁt of where the quantization is restricted
to a ﬁnite set of learned centers C (instead of the ﬁxed (nonlearned) integer grid as in ) and the simplicity of ,
where a differentiable approximation of quantization is only
used in the backward pass, avoiding the need to choose
an annealing strategy (i.e., a schedule for σ) as in to
drive the soft quantization (3) to hard assignments (2) during training. In TensorFlow, this is implemented as
¯zi = tf.stopgradient(ˆzi −˜zi) + ˜zi.
We note that for forward pass computations, ¯zi = ˆzi, and
thus we will continue writing ˆzi for the latent representation.
3.2. Entropy estimation
To model the entropy H(ˆz) we build on the approach
of PixelRNN and factorize the distribution p(ˆz) as a
product of conditional distributions
p(ˆzi|ˆzi−1, . . . , ˆz1),
where the 3D feature volume ˆz is indexed in raster
scan order.
We then use a neural network P(ˆz), which
we refer to as a context model, to estimate each term
p(ˆzi|ˆzi−1, . . . , ˆz1):
Pi,l(ˆz) ≈p(ˆzi = cl|ˆzi−1, . . . , ˆz1),
where Pi,l speciﬁes for every 3D location i in ˆz the probabilites of each symbol in C with l = 1, . . . , L. We refer to the resulting approximate distribution as q(ˆz) :=
i=1 Pi,I(ˆzi)(ˆz), where I(ˆzi) denotes the index of ˆzi in C.
Since the conditional distributions p(ˆzi|ˆzi−1, . . . , ˆz1)
only depend on previous values ˆzi−1, . . . , ˆz1, this imposes a
causality constraint on the network P: While P may compute Pi,l in parallel for i = 1, . . . , m, l = 1, . . . , L, it needs
to make sure that each such term only depends on previous
values ˆzi−1, . . . , ˆz1.
The authors of PixelCNN study the use of 2D-
CNNs as causal conditional models over 2D images in a
lossless setting, i.e., treating the RGB pixels as symbols.
They show that the causality constraint can be efﬁciently enforced using masked ﬁlters in the convolution. Intuitively,
the idea is as follows: If for each layer the causality condition is satisﬁed with respect to the spatial coordinates of
the layer before, then by induction the causality condition
will hold between the output layer and the input. Satisfying
the causality condition for each layer can be achieved with
proper masking of its weight tensor, and thus the entire network can be made causal only through the masking of its
weights. Thus, the entire set of probabilities Pi,l for all (2D)
spatial locations i and symbol values l can be computed in
parallel with a fully convolutional network, as opposed to
modeling each term p(ˆzi|ˆzi−1, · · · , ˆz1) separately.
In our case, ˆz is a 3D symbol volume, with as much as
K = 64 channels. We therefore generalize the approach
of PixelCNN to 3D convolutions, using the same idea of
masking the ﬁlters properly in every layer of the network.
This enables us to model P efﬁciently, with a light-weight2
3D-CNN which slides over ˆz, while properly respecting the
causality constraint. We refer to the supplementary material
for more details.
As in , we learn P by training it for maximum likelihood, or equivalently (see ) by training Pi,: to classify
2We use a 4-layer network, compared to 15 layers in .
the index I(ˆzi) of ˆzi in C with a cross entropy loss:
CE := Eˆz∼p(ˆz)[
−log Pi,I(ˆzi)].
Using the well-known property of cross entropy as the coding cost when using the wrong distribution q(ˆz) instead of
the true distribution p(ˆz), we can also view the CE loss as
an estimate of H(ˆz) since we learn P such that P = q ≈p.
That is, we can compute
H(ˆz) = Eˆz∼p(ˆz)[−log(p(ˆz))]
= Eˆz∼p(ˆz)[
−log p(ˆzi|ˆzi−1, · · · , ˆz1)]
≈Eˆz∼p(ˆz)[
−log q(ˆzi|ˆzi−1, · · · , ˆz1)]
= Eˆz∼p(ˆz)[
−log Pi,I(ˆzi)]
Therefore, when training the auto-encoder we can indirectly
minimize H(ˆz) through the cross entropy CE. We refer to
argument in the expectation of (7),
−log Pi,I(ˆzi),
as the coding cost of the latent image representation, since
this reﬂects the coding cost incurred when using P as a context model with an adaptive arithmetic encoder . From
the application perspective, minimizing the coding cost is
actually more important than the (unknown) true entropy,
since it reﬂects the bitrate obtained in practice.
To backpropagate through P(ˆz) we use the same approach as for the encoder (see (4)). Thus, like the decoder
D, P only sees the (discrete) ˆz in the forward pass, whereas
the gradient of the soft quantization ˜z is used for the backward pass.
3.3. Concurrent optimization
Given an auto-encoder (E, D), we can train P to model
the dependencies of the entries of ˆz as described in the previous section by minimizing (7). On the other hand, using the model P, we can obtain an estimate of H(ˆz) as
in (12) and use this estimate to adjust (E, D) such that
d(x, D(Q(E(x))))+βH(ˆz) is reduced, thereby navigating
the rate distortion trade-off. Therefore, it is natural to concurrently learn P (with respect to its own loss), and (E, D)
(with respect to the rate distortion trade-off) during training, such that all models which the losses depend on are
continuously updated.
3.4. Importance map for spatial bit-allocation
Recall that since E and D are CNNs, ˆz is a 3D featuremap. For example, if E has three stride-2 convolution layers
and the bottleneck has K channels, the dimensions of ˆz will
8 × K. A consequence of this formulation is that
we are using equally many symbols in ˆz for each spatial
location of the input image x. It is known, however, that in
practice there is great variability in the information content
across spatial locations (e.g., the uniform area of blue sky
vs. the ﬁne-grained structure of the leaves of a tree).
This can in principle be accounted for automatically in
the trade-off between the entropy and the distortion, where
the network would learn to output more predictable (i.e.,
low entropy) symbols for the low information regions, while
making room for the use of high entropy symbols for the
more complex regions. More precisely, the formulation in
(7) already allows for variable bit allocation for different
spatial regions through the context model P.
However, this arguably requires a quite sophisticated
(and hence computationally expensive) context model, and
we ﬁnd it beneﬁcial to follow Li et al. instead by using
an importance map to help the CNN attend to different regions of the image with different amounts of bits. While
 uses a separate network for this purpose, we consider a
simpliﬁed setting. We take the last layer of the encoder E,
and add a second single-channel output y ∈R
expand this single channel y into a mask m ∈R
of the same dimensionality as z as follows:
if k < yi,j
if k ≤yi,j ≤k + 1
if k + 1 > yi,j
where yi,j denotes the value of y at spatial location (i, j).
The transition value for k ≤yi,j ≤k + 1 is such that the
mask smoothly transitions from 0 to 1 for non-integer values of y.
We then mask z by pointwise multiplication with the binarization of m, i.e., z ←z ⊙⌈m⌉. Since the ceiling operator ⌈·⌉is not differentiable, as done by , we use
identity for the backward pass.
With this modiﬁcation, we have simply changed the architecture of E slightly such that it can easily “zero out”
portions of columns zi,j,: of z (the rest of the network stays
the same, so that (2) still holds for example). As suggested
by , the so-obtained structure in z presents an alternative coding strategy: Instead of losslessly encoding the entire symbol volume ˆz, we could ﬁrst (separately) encode the
mask ⌈m⌉, and then for each column ˆzi,j,: only encode the
ﬁrst ⌈mi,j⌉+ 1 symbols, since the remaining ones are the
constant Q(0), which we refer to as the zero symbol.
Work uses binary symbols (i.e., C = {0, 1}) and assumes independence between the symbols and a uniform
prior during training, i.e., costing each 1 bit to encode. The
importance map is thus their principal tool for controlling
the bitrate, since they thereby avoid encoding all the bits
in the representation. In contrast, we stick to the formulation in (5) where the dependencies between the symbols are
modeled during training. We then use the importance map
as an architectural constraint and use their suggested coding strategy to obtain an alternative estimate for the entropy
H(ˆz), as follows.
We observe that we can recover ⌈m⌉from ˆz by counting the number of consecutive zero symbols at the end of
each column ˆzi,j,:.3
⌈m⌉is therefore a function of the
masked ˆz, i.e., ⌈m⌉= g(ˆz) for g recovering ⌈m⌉as described, which means that we have for the conditional entropy H(⌈m⌉|ˆz) = 0. Now, we have
H(ˆz) = H(⌈m⌉|ˆz) + H(ˆz)
= H(ˆz, ⌈m⌉)
= H(ˆz|⌈m⌉) + H(⌈m⌉).
If we treat the entropy of the mask, H(⌈m⌉), as constant
during optimization of the auto-encoder, we can then indirectly minimize H(ˆz) through H(ˆz|m).
To estimate H(ˆz|m), we use the same factorization of p
as in (5), but since the mask ⌈m⌉is known we have p(ˆzi =
c0) = 1 deterministic for the 3D locations i in ˆz where
the mask is zero. The logs of the corresponding terms in (9)
then evaluate to 0. The remaining terms, we can model with
the same context model Pi,l(ˆz), which results in
H(ˆz|⌈m⌉) ≈Eˆz∼p(ˆz)[
−⌈mi⌉log Pi,I(ˆzi)],
where mi denotes the i-th element of m (in the same raster
scan order as ˆz).
Similar to the coding cost (13), we refer to the argument
in the expectation in (18),
−⌈mi⌉log Pi,I(ˆzi)
as the masked coding cost of ˆz.
While the entropy estimate (18) is almost estimating the
same quantity as (7) (only differing by H(⌈m⌉)), it has the
beneﬁt of being weighted by mi. Therefore, the encoder E
has an obvious path to control the entropy of ˆz, by simply
increasing/decreasing the value of y for some spatial location of x and thus obtaining fewer/more zero entries in m.
When the context model P(ˆz) is trained, however, we
still train it with respect to the formulation in (8), so it does
3If z contained zeros before it was masked, we might overestimate the
number of 0 entries in ⌈m⌉. However, we can redeﬁne those entries of m
as 0 and this will give the same result after masking.
not have direct access to the mask m and needs to learn the
dependencies on the entire masked symbol volume ˆz. This
means that when encoding an image, we can stick to standard adaptive arithmetic coding over the entire bottleneck,
without needing to resort to a two-step coding process as in
 , where the mask is ﬁrst encoded and then the remaining
symbols. We emphasize that this approach hinges critically
on the context model P and the encoder E being trained
concurrently as this allows the encoder to learn a meaningful (in terms of coding cost) mask with respect to P (see the
next section).
In our experiments we observe that during training, the
two entropy losses (7) and (18) converge to almost the same
value, with the latter being around ≈3.5% smaller due to
H(⌈m⌉) being ignored.
While the importance map is not crucial for optimal ratedistortion performance, if the channel depth K is adjusted
carefully, we found that we could more easily control the
entropy of ˆz through β when using a ﬁxed K, since the network can easily learn to ignore some of the channels via the
importance map. Furthermore, in the supplementary material we show that by using multiple importance maps for a
single network, one can obtain a single model that supports
multiple compression rates.
3.5. Putting the pieces together
We made an effort to carefully describe our formulation
and its motivation in detail. While the description is lengthy,
when putting the resulting pieces together we get a quite
straightforward pipeline for learned image compression, as
Given the set of training images X, we initialize (fully
convolutional) CNNs E, D, and P, as well as the centers C
of the quantizer Q. Then, we train over minibatches XB =
{x(1), · · · , x(B)} of crops from X. At each iteration, we
take one gradient step for the auto-encoder (E, D) and the
quantizer Q, with respect to the rate-distortion trade-off
LE,D,Q = 1
d(x(j), ˆx(j)) + βMC(ˆz(j)),
which is obtained by combining (1) with the estimate (18)
& (19) and taking the batch sample average. Furthermore,
we take a gradient step for the context model P with respect
to its objective (see (7) & (13))
d(x(j), ˆx(j)) + βC(ˆz(j)).
To compute these two batch losses, we need to perform
the following computation for each x ∈XB:
1. Obtain compressed (latent) representation z and importance map y from the encoder: (z, y) = E(x)
2. Expand importance map y to mask m via (14)
3. Mask z, i.e., z ←z ⊙⌈m⌉
4. Quantize ˆz = Q(z)
5. Compute the context P(ˆz)
6. Decode ˆx = D(ˆz),
which can be computed in parallel over the minibatch on a
GPU since all the models are fully convolutional.
3.6. Relationship to previous methods
We are not the ﬁrst to use context models for adaptive
arithmetic coding to improve the performance in learned
deep image compression. Work uses a PixelRNN-like
architecture to train a recurrent network as a context
model for an RNN-based compression auto-encoder. Li et
al. extract cuboid patches around each symbol in a binary feature map, and feed them to a convolutional context
model. Both these methods, however, only learn the context
model after training their system, as a post-processing step
to boost coding performance.
In contrast, our method directly incorporates the context
model as the entropy term for the rate-distortion term (1)
of the auto-encoder, and trains the two concurrently. This
is done at little overhead during training, since we adopt a
3D-CNN for the context model, using PixelCNN-inspired
 masking of the weights of each layer to ensure causality in the context model. Adopting the same approach to the
context models deployed by or would be non-trivial
since they are not designed for fast feed-forward computation. In particular, while the context model of is also
convolutional, its causality is enforced through masking the
inputs to the network, as opposed to our masking of the
weights of the networks. This means their context model
needs to be run separately with a proper input cuboid for
each symbol in the volume (i.e., not fully convolutionally).
4. Experiments
Architecture
Our auto-encoder has a similar architecture
as but with more layers, and is described in Fig. 2. We
adapt the number of channels K in the latent representation
for different models. For the context model P, we use a
simple 4-layer 3D-CNN as described in Fig. 3.
Distortion measure
Following , we use the multiscale structural similarity index (MS-SSIM) as measure of distortion d(x, ˆx) = 100 · (1 −MS-SSIM(x, ˆx))
for our models. MS-SSIM reportedly correlates better with
human perception of distortion than mean squared error
(MSE). We train and test all our models using MS-SSIM.
15 Residual Blocks,
skip connection between every 3
k5 n(K+1)-2
quantization
15 Residual Blocks
skip connection between every 3
Denormalize
importance
K+1 × H × W
Figure 2: The architecture of our auto-encoder. Dark gray
blocks represent residual units. The upper part represents
the encoder E, the lower part the decoder D. For the encoder, “k5 n64-2” represents a convolution layer with kernel size 5, 64 output channels and a stride of 2. For the decoder it represents the equivalent deconvolution layer. All
convolution layers are normalized using batch norm , and
use SAME padding. Masked quantization is the quantization described in Section 3.4. Normalize normalizes the input to using a mean and variance obtained from a subset of the training set. Denormalize is the inverse operation.
The architecture of our context
“3D k3 n24” refers to a 3D masked convolution with ﬁlter size 3 and 24 output channels. The last layer outputs L
values for each voxel in ˆz.
We use the Adam optimizer with a minibatch size of 30 to train seven models. Each model is trained
to maximize MS-SSIM directly. As a baseline, we used a
learning rate (LR) of 4 · 10−3 for each model, but found it
beneﬁcial to vary it slightly for different models. We set
σ = 1 in the smooth approximation (3) used for gradient
backpropagation through Q. To make the model more predictably land at a certain bitrate t when optimizing (1), we
found it helpful to clip the rate term (i.e., replace the entropy
term βH with max(t, βH)), such that the entropy term is
“switched off” when it is below t. We found this did not
hurt performance. We decay the learning rate by a factor 10
every two epochs. To obtain models for different bitrates,
we adapt the target bitrate t and the number of channels K,
while using a moderately large β = 10. We use a small
regularization on the weights and note that we achieve very
stable training. We trained our models for 6 epochs, which
took around 24h per model on a single GPU. For P, we use
a LR of 10−4 and the same decay schedule.
We train on the the ImageNet dataset from
the Large Scale Visual Recognition Challenge 2012
(ILSVRC2012) . As a preprocessing step, we take random 160×160 crops, and randomly ﬂip them. We set aside
100 images from ImageNet as a testing set, ImageNetTest.
Furthermore, we test our method on the widely used Kodak dataset. To asses performance on high-quality fullresolution images, we also test on the datasets B100 
and Urban100 , commonly used in super-resolution.
Other codecs
We compare to JPEG, using libjpeg4, and
JPEG2000, using the Kakadu implementation5. We also
compare to the lesser known BPG6, which is based on
HEVC, the state-of-the-art in video compression, and which
outperforms JPEG and JPEG2000. We use BPG in the nondefault 4:4:4 chroma format, following .
Comparison
Like , we proceed as follows to compare to other methods. For each dataset, we compress each
image using all our models. This yields a set of (bpp, MS-
SSIM) points for each image, which we interpolate to get
a curve for each image. We ﬁx a grid of bpp values, and
average the curves for each image at each bpp grid value
(ignoring those images whose bpp range does not include
the grid value, i.e., we do not extrapolate). We do this for
our method, BPG, JPEG, and JPEG2000. Due to code being
unavailable for the related works in general, we digitize the
Kodak curve from Rippel & Bourdev , who have carefully collected the curves from the respective works. With
this, we also show the results of Rippel & Bourdev ,
Johnston et al. , Ball´e et al. , and Theis et al. .
To validate that our estimated MS-SSIM is correctly implemented, we independently generated the BPG curves for
Kodak and veriﬁed that they matched the one from .
Fig. 1 shows a comparison of the aforementioned
methods for Kodak. Our method outperforms BPG, JPEG,
and JPEG2000, as well as the neural network based approaches of Johnston et al. , Ball´e et al. , and Theis
et al. . Furthermore, we achieve performance comparable to that of Rippel & Bourdev . This holds for all bpps
we tested, from 0.3 bpp to 0.9 bpp. We note that while Rippel & Bourdev and Johnston et al. also train to maximize
(MS-)SSIM, the other methods minimize MSE.
4 
5 
6 
MS-SSIM on ImageNetVal
MS-SSIM on B100
MS-SSIM on Urban100
Figure 4: Performance of our approach on ImageNetTest, B100, Urban100, where we outperform BPG, JPEG and JPEG2000 in MS-SSIM.
Ours 0.124bpp
0.147 bpp BPG
JPEG2000 0.134bpp
0.150bpp JPEG
Figure 5: Example image (kodim21) from the Kodak testing set, compressed with different methods.
In each of the other testing sets, we also outperform
BPG, JPEG, and JPEG2000 over the reported bitrates, as
shown in Fig. 4.
In Fig. 5, we compare our approach to BPG, JPEG,
and JPEG2000 visually, using very strong compression on
kodim21 from Kodak. It can be seen that the output of our
network is pleasant to look at. Soft structures like the clouds
are very well preserved. BPG appears to handle high frequencies better (see, e.g., the fence) but loses structure in
the clouds and in the sea. Like JPEG2000, it produces block
artifacts. JPEG breaks down at this rate. We refer to the
supplementary material for further visual examples.
Ablation study: Context model
In order to show the effectiveness of the context model, we performed the following ablation study. We trained the auto-encoder without entropy loss, i.e., β = 0 in (20), using L = 6 centers and
K = 16 channels. On Kodak, this model yields an average MS-SSIM of 0.982, at an average rate of 0.646 bpp
(calculated assuming that we need log2(L) = 2.59 bits per
symbol). We then trained three different context models for
this auto-encoder, while keeping the auto-encoder ﬁxed: A
zeroth order context model which uses a histogram to estimate the probability of each of the L symbols; a ﬁrst order
(one-step prediction) context model, which uses a conditional histogram to estimate the probability of each of the
L symbols given the previous symbol (scanning ˆz in raster
order); and P, i.e., our proposed context model. The resulting average rates are shown in Table 1. Our context model
reduces the rate by 10 %, even though the auto-encoder was
optimized using a uniform prior (see supplementary material for a detailed comparison of Table 1 and Fig. 1).
Baseline (Uniform)
Zeroth order
First order
Our context model P 0.579 bpp
Table 1: Rates for different context models, for the same
architecture (E, D).
Importance map
As described in detail in Section 3.4,
we use an importance map to dynamically alter the number
of channels used at different spatial locations to encode an
image. To visualize how this helps, we trained two autoencoders M and M ′, where M uses an importance map
and at most K = 32 channels to compress an image, and
M ′ compresses without importance map and with K = 16
channels (this yields a rate for M ′ similar to that of M). In
Fig. 6, we show an image from ImageNetTest along with
the same image compressed to 0.463 bpp by M and compressed to 0.504 bpp by M ′. Furthermore, Fig. 6 shows the
importance map produced by M, as well as ordered visualizations of all channels of the latent representation for both
M and M ′. Note how for M, channels with larger index
are sparser, showing how the model can spatially adapt the
number of channels. M ′ uses all channels similarly.
Importance map of M
Output of M
Latent representation of M
Output of M ′
Latent representation of M ′
Figure 6: Visualization of the latent representation of the
auto-encoder for a high-bpp operating point, with (M) and
without (M ′) incorporating an importance map.
5. Discussion
Our experiments showed that combining a convolutional
auto-encoder with a lightweight 3D-CNN as context model
and training the two networks concurrently leads to a highly
effective image compression system.
Not only were we
able to clearly outperform state-of-the-art engineered compression methods including BPG and JPEG2000 in terms
of MS-SSIM, but we also obtained performance competitive with the current state-of-the-art learned compression
method from . In particular, our method outperforms
BPG and JPEG2000 in MS-SSIM across four different
testing sets (ImageNetTest, Kodak, B100, Urban100), and
does so signiﬁcantly, i.e., the proposed method generalizes
We emphasize that our method relies on elementary techniques both in terms of the architecture (standard
convolutional auto-encoder with importance map, convolutional context model) and training procedure (minimize
the rate-distortion trade-off and the negative log-likelihood
for the context model), while uses highly specialized
techniques such as a pyramidal decomposition architecture,
adaptive codelength regularization, and multiscale adversarial training.
The ablation study for the context model showed that our
3D-CNN-based context model is signiﬁcantly more powerful than the ﬁrst order (histogram) and second order (onestep prediction) baseline context models. Further, our experiments suggest that the importance map learns to condensate the image information in a reduced number of channels of the latent representation without relying on explicit
supervision. Notably, the importance map is learned as a
part of the image compression auto-encoder concurrently
with the auto-encoder and the context model, without introducing any optimization difﬁculties. In contrast, in 
the importance map is computed using a separate network,
learned together with the auto-encoder, while the context
model is learned separately.
6. Conclusions
In this paper, we proposed the ﬁrst method for learning
a lossy image compression auto-encoder concurrently with
a lightweight context model by incorporating it into an entropy loss for the optimization of the auto-encoder, leading
to performance competitive with the current state-of-the-art
in deep image compression .
Future works could explore heavier and more powerful context models, as those employed in . This
could further improve compression performance and allow
for sampling of natural images in a “lossy” manner, by sampling ˆz according to the context model and then decoding.
Acknowledgements
This work was supported by ETH
Z¨urich and by NVIDIA through a GPU grant.