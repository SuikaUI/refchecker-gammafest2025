Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163–4174
November 16 - 20, 2020. c⃝2020 Association for Computational Linguistics
TinyBERT: Distilling BERT for Natural Language Understanding
Xiaoqi Jiao1∗†, Yichun Yin2∗‡, Lifeng Shang2‡, Xin Jiang2
Xiao Chen2, Linlin Li3, Fang Wang1‡ and Qun Liu2
1Key Laboratory of Information Storage System, Huazhong University of
Science and Technology, Wuhan National Laboratory for Optoelectronics
2Huawei Noah’s Ark Lab
3Huawei Technologies Co., Ltd.
{jiaoxiaoqi,wangfang}@hust.edu.cn
{yinyichun,shang.lifeng,jiang.xin}@huawei.com
{chen.xiao2,lynn.lilinlin,qun.liu}@huawei.com
Language model pre-training, such as BERT,
has signiﬁcantly improved the performances
of many natural language processing tasks.
However, pre-trained language models are usually computationally expensive, so it is difﬁcult to efﬁciently execute them on resourcerestricted devices.
To accelerate inference
and reduce model size while maintaining
accuracy, we ﬁrst propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the
Transformer-based models. By leveraging this
new KD method, the plenty of knowledge encoded in a large “teacher” BERT can be effectively transferred to a small “student” Tiny-
BERT. Then, we introduce a new two-stage
learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-speciﬁc learning stages. This
framework ensures that TinyBERT can capture
the general-domain as well as the task-speciﬁc
knowledge in BERT.
TinyBERT41 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERTBASE on GLUE
benchmark, while being 7.5x smaller and 9.4x
faster on inference. TinyBERT4 is also signiﬁcantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ∼28%
parameters and ∼31% inference time of them.
Moreover, TinyBERT6 with 6 layers performs
on-par with its teacher BERTBASE.
Introduction
Pre-training language models then ﬁne-tuning on
downstream tasks has become a new paradigm for
∗Authors contribute equally.
†This work is done when Xiaoqi Jiao is an intern at
Huawei Noah’s Ark Lab.
‡Corresponding authors.
1The code and models are publicly available at https:
//github.com/huawei-noah/Pretrained-Language-Model/tree/
master/TinyBERT
natural language processing (NLP). Pre-trained language models (PLMs), such as BERT , XLNet , RoBERTa , ALBERT , T5 and ELECTRA ,
have achieved great success in many NLP tasks
 
and the challenging multi-hop reasoning task ).
However, PLMs usually have a
large number of parameters and take long inference time, which are difﬁcult to be deployed on
edge devices such as mobile phones. Recent studies demonstrate that there is redundancy
in PLMs. Therefore, it is crucial and feasible to
reduce the computational overhead and model storage of PLMs while retaining their performances.
There have been many model compression techniques proposed to accelerate
deep model inference and reduce model size while
maintaining accuracy. The most commonly used
techniques include quantization ,
weights pruning , and knowledge distillation (KD) . In
this paper, we focus on knowledge distillation, an
idea originated from Hinton et al. , in a
teacher-student framework. KD aims to transfer
the knowledge embedded in a large teacher network to a small student network where the student
network is trained to reproduce the behaviors of the
teacher network. Based on the framework, we propose a novel distillation method speciﬁcally for the
Transformer-based models ,
and use BERT as an example to investigate the
method for large-scale PLMs.
KD has been extensively studied in NLP as well as for
pre-trained language models . The
pre-training-then-ﬁne-tuning paradigm ﬁrstly pre-
Large-scale
Text Corpus
Fine-tuned
Distillation
Task Dataset
Task Dataset
Data Augmentation
Task-specific
Distillation
Figure 1: The illustration of TinyBERT learning.
trains BERT on a large-scale unsupervised text
corpus, then ﬁne-tunes it on task-speciﬁc dataset,
which greatly increases the difﬁculty of BERT distillation. Therefore, it is required to design an effective KD strategy for both training stages.
To build a competitive TinyBERT, we ﬁrstly
propose a new Transformer distillation method to
distill the knowledge embedded in teacher BERT.
Speciﬁcally, we design three types of loss functions to ﬁt different representations from BERT
layers: 1) the output of the embedding layer; 2) the
hidden states and attention matrices derived from
the Transformer layer; 3) the logits output by the
prediction layer. The attention based ﬁtting is inspired by the recent ﬁndings 
that the attention weights learned by BERT can capture substantial linguistic knowledge, and it thus
encourages the linguistic knowledge can be well
transferred from teacher BERT to student Tiny-
BERT. Then, we propose a novel two-stage learning framework including the general distillation
and the task-speciﬁc distillation, as illustrated in
Figure 1. At general distillation stage, the original
BERT without ﬁne-tuning acts as the teacher model.
The student TinyBERT mimics the teacher’s behavior through the proposed Transformer distillation
on general-domain corpus. After that, we obtain
a general TinyBERT that is used as the initialization of student model for the further distillation.
At the task-speciﬁc distillation stage, we ﬁrst do
the data augmentation, then perform the distillation on the augmented dataset using the ﬁne-tuned
BERT as the teacher model. It should be pointed
out that both the two stages are essential to improve
the performance and generalization capability of
The main contributions of this work are as follows: 1) We propose a new Transformer distillation method to encourage that the linguistic knowledge encoded in teacher BERT can be adequately
transferred to TinyBERT; 2) We propose a novel
two-stage learning framework with performing the
proposed Transformer distillation at both the pretraining and ﬁne-tuning stages, which ensures that
TinyBERT can absorb both the general-domain and
task-speciﬁc knowledge of the teacher BERT. 3)
We show in the experiments that our TinyBERT4
can achieve more than 96.8% the performance of
teacher BERTBASE on GLUE tasks, while having
much fewer parameters (∼13.3%) and less inference time (∼10.6%), and signiﬁcantly outperforms
other state-of-the-art baselines with 4 layers on
BERT distillation; 4) We also show that a 6-layer
TinyBERT6 can perform on-par with the teacher
BERTBASE on GLUE.
Preliminaries
In this section, we describe the formulation of
Transformer and Knowledge
Distillation . Our proposed
Transformer distillation is a specially designed KD
method for Transformer-based models.
Transformer Layer
Most of the recent pre-trained language models (e.g., BERT, XLNet and RoBERTa) are built
with Transformer layers, which can capture longterm dependencies between input tokens by selfattention mechanism.
Speciﬁcally, a standard
Transformer layer includes two main sub-layers:
multi-head attention (MHA) and fully connected
feed-forward network (FFN).
Multi-Head Attention (MHA). The calculation of
attention function depends on the three components
of queries, keys and values, denoted as matrices Q,
K and V respectively. The attention function can
be formulated as follows:
Attention(Q, K, V )=softmax(A)V , (2)
where dk is the dimension of keys and acts as a
scaling factor, A is the attention matrix calculated
from the compatibility of Q and K by dot-product
operation. The ﬁnal function output is calculated
as a weighted sum of values V , and the weight is
computed by applying softmax() operation on
the each column of matrix A. According to Clark
et al. , the attention matrices in BERT can
capture substantial linguistic knowledge, and thus
play an essential role in our proposed distillation
Multi-head attention is deﬁned by concatenating
the attention heads from different representation
subspaces as follows:
MHA(Q, K, V )=Concat(h1, . . . , hk)W , (3)
where k is the number of attention heads, and hi
denotes the i-th attention head, which is calculated
by the Attention() function with inputs from
different representation subspaces. The matrix W
acts as a linear transformation.
Position-wise Feed-Forward Network (FFN).
Transformer layer also contains a fully connected
feed-forward network, which is formulated as follows:
FFN(x) = max(0, xW1 + b1)W2 + b2.
We can see that the FFN contains two linear transformations and one ReLU activation.
Knowledge Distillation
KD aims to transfer the knowledge of a large
teacher network T to a small student network S.
The student network is trained to mimic the behaviors of teacher networks. Let fT and fS represent
the behavior functions of teacher and student networks, respectively. The behavior function targets
at transforming network inputs to some informative representations, and it can be deﬁned as the
output of any layer in the network. In the context of
Transformer distillation, the output of MHA layer
or FFN layer, or some intermediate representations
(such as the attention matrix A) can be used as
behavior function. Formally, KD can be modeled
as minimizing the following objective function:
 fS(x), fT (x)
where L(·) is a loss function that evaluates the difference between teacher and student networks, x
is the text input and X denotes the training dataset.
Thus the key research problem becomes how to de-
ﬁne effective behavior functions and loss functions.
Different from previous KD methods, we also need
to consider how to perform KD at the pre-training
stage of BERT in addition to the task-speciﬁc training stage.
In this section, we propose a novel distillation
method for Transformer-based models, and present
a two-stage learning framework for our model distilled from BERT, which is called TinyBERT.
(ℝℎ𝑒𝑎𝑑∗𝑙∗𝑙)
(ℝℎ𝑒𝑎𝑑∗𝑙∗𝑙)
ADD & Norm
ADD & Norm
ADD & Norm
ADD & Norm
Attention Matrices
Attention Matrices
Hidden States
Hidden States
Teacher Layer
Student Layer
Figure 2: The details of Transformer-layer distillation
consisting of Attnloss(attention based distillation) and
Hidnloss(hidden states based distillation).
Transformer Distillation
The proposed Transformer distillation is a specially
designed KD method for Transformer networks. In
this work, both the student and teacher networks are
built with Transformer layers. For a clear illustration, we formulate the problem before introducing
our method.
Problem Formulation. Assuming that the student model has M Transformer layers and teacher
model has N Transformer layers, we start with
choosing M out of N layers from the teacher
model for the Transformer-layer distillation. Then
a function n = g(m) is deﬁned as the mapping
function between indices from student layers to
teacher layers, which means that the m-th layer
of student model learns the information from the
g(m)-th layer of teacher model. To be precise, we
set 0 to be the index of embedding layer and M +1
to be the index of prediction layer, and the corresponding layer mappings are deﬁned as 0 = g(0)
and N + 1 = g(M + 1) respectively. The effect
of the choice of different mapping functions on the
performances is studied in the experiment section.
Formally, the student can acquire knowledge from
the teacher by minimizing the following objective:
λmLlayer(fS
g(m)(x)), (6)
where Llayer refers to the loss function of a given
model layer (e.g., Transformer layer or embedding layer), fm(x) denotes the behavior function
induced from the m-th layers and λm is the hyper-
parameter that represents the importance of the
m-th layer’s distillation.
Transformer-layer Distillation.
The proposed
Transformer-layer distillation includes the attention based distillation and hidden states based distillation, which is shown in Figure 2. The attention based distillation is motivated by the recent
ﬁndings that attention weights learned by BERT
can capture rich linguistic knowledge . This kind of linguistic knowledge includes
the syntax and coreference information, which is
essential for natural language understanding. Thus
we propose the attention based distillation to encourage that the linguistic knowledge can be transferred from teacher (BERT) to student (TinyBERT).
Speciﬁcally, the student learns to ﬁt the matrices
of multi-head attention in the teacher network, and
the objective is deﬁned as:
i=1 MSE(AS
where h is the number of attention heads, Ai ∈
Rl×l refers to the attention matrix corresponding
to the i-th head of teacher or student, l is the input
text length, and MSE() means the mean squared
error loss function. In this work, the (unnormalized) attention matrix Ai is used as the ﬁtting target
instead of its softmax output softmax(Ai), since
our experiments show that the former setting has a
faster convergence rate and better performances.
In addition to the attention based distillation,
we also distill the knowledge from the output of
Transformer layer, and the objective is as follows:
Lhidn = MSE(HSWh, HT ),
where the matrices HS ∈Rl×d′ and HT ∈Rl×d
refer to the hidden states of student and teacher networks respectively, which are calculated by Equation 4. The scalar values d and d′ denote the hidden
sizes of teacher and student models, and d′ is often
smaller than d to obtain a smaller student network.
The matrix Wh ∈Rd′×d is a learnable linear transformation, which transforms the hidden states of
student network into the same space as the teacher
network’s states.
Embedding-layer Distillation.
Similar to the
hidden states based distillation, we also perform
embedding-layer distillation and the objective is:
Lembd = MSE(ESWe, ET ),
where the matrices ES and HT refer to the embeddings of student and teacher networks, respectively. In this paper, they have the same shape as
the hidden state matrices. The matrix We is a linear
transformation playing a similar role as Wh.
Prediction-layer Distillation. In addition to imitating the behaviors of intermediate layers, we
also use the knowledge distillation to ﬁt the predictions of teacher model as in Hinton et al. .
Speciﬁcally, we penalize the soft cross-entropy loss
between the student network’s logits against the
teacher’s logits:
Lpred = CE(zT /t, zS/t),
where zS and zT are the logits vectors predicted
by the student and teacher respectively, CE means
the cross entropy loss, and t means the temperature value. In our experiment, we ﬁnd that t = 1
performs well.
Using the above distillation objectives (i.e. Equations 7, 8, 9 and 10), we can unify the distillation loss of the corresponding layers between the
teacher and the student network:
Lhidn+Lattn, M ≥m>0
TinyBERT Learning
The application of BERT usually consists of two
learning stages: the pre-training and ﬁne-tuning.
The plenty of knowledge learned by BERT in the
pre-training stage is of great importance and should
be transferred to the compressed model. Therefore, we propose a novel two-stage learning framework including the general distillation and the
task-speciﬁc distillation, as illustrated in Figure 1.
General distillation helps TinyBERT learn the rich
knowledge embedded in pre-trained BERT, which
plays an important role in improving the generalization capability of TinyBERT. The task-speciﬁc
distillation further teaches TinyBERT the knowledge from the ﬁne-tuned BERT. With the two-step
distillation, we can substantially reduce the gap
between teacher and student models.
General Distillation. We use the original BERT
without ﬁne-tuning as the teacher and a large-scale
text corpus as the training data. By performing the
Transformer distillation 2 on the text from general
2In the general distillation, we do not perform predictionlayer distillation as Equation 10. Our motivation is to make
Algorithm 1 Data Augmentation Procedure for
Task-speciﬁc Distillation
Input: x is a sequence of words
Params: pt: the threshold probability
Na: the number of samples augmented per example
K: the size of candidate set
Output: D′: the augmented data
1: n ←0 ; D′ ←[ ]
2: while n < Na do
for i ←1 to len(x) do
if x[i] is a single-piece word then
Replace xm[i] with [MASK]
C ←K most probable words of BERT(xm)[i]
C ←K most similar words of x[i] from GloVe
Sample p ∼Uniform(0, 1)
if p ≤pt then
Replace xm[i] with a word in C randomly
Append xm to D′
18: end while
19: return D′
domain, we obtain a general TinyBERT that can be
ﬁne-tuned for downstream tasks. However, due to
the signiﬁcant reductions of the hidden/embedding
size and the layer number, general TinyBERT performs generally worse than BERT.
Task-speciﬁc Distillation.
Previous studies
show that the complex models, such as ﬁnetuned BERTs, suffer from over-parametrization for
domain-speciﬁc tasks . Thus,
it is possible for smaller models to achieve comparable performances to the BERTs. To this end,
we propose to produce competitive ﬁne-tuned Tiny-
BERTs through the task-speciﬁc distillation. In
the task-speciﬁc distillation, we re-perform the proposed Transformer distillation on an augmented
task-speciﬁc dataset. Speciﬁcally, the ﬁne-tuned
BERT is used as the teacher and a data augmentation method is proposed to expand the task-speciﬁc
training set. Training with more task-related examples, the generalization ability of the student model
can be further improved.
Data Augmentation. We combine a pre-trained
language model BERT and GloVe word embeddings to do word-level
the TinyBERT primarily learn the intermediate structures of
BERT at pre-training stage. From our preliminary experiments, we also found that conducting prediction-layer distillation at pre-training stage does not bring extra improvements
on downstream tasks, when the Transformer-layer distillation
(Attn and Hidn distillation) and Embedding-layer distillation
have already been performed.
replacement for data augmentation. Speciﬁcally,
we use the language model to predict word replacements for single-piece words , and
use the word embeddings to retrieve the most similar words as word replacements for multiple-pieces
words3. Some hyper-parameters are deﬁned to control the replacement ratio of a sentence and the
amount of augmented dataset. More details of the
data augmentation procedure are shown in Algorithm 1. We set pt = 0.4, Na = 20, K = 15 for all
our experiments.
The above two learning stages are complementary to each other: the general distillation provides
a good initialization for the task-speciﬁc distillation, while the task-speciﬁc distillation on the augmented data further improves TinyBERT by focusing on learning the task-speciﬁc knowledge. Although there is a signiﬁcant reduction of model size,
with the data augmentation and by performing the
proposed Transformer distillation method at both
the pre-training and ﬁne-tuning stages, TinyBERT
can achieve competitive performances in various
NLP tasks.
Experiments
In this section, we evaluate the effectiveness and
efﬁciency of TinyBERT on a variety of tasks with
different model settings.
We evaluate TinyBERT on the General Language
Understanding Evaluation (GLUE) benchmark, which consists of 2 singlesentence tasks: CoLA , SST-
2 , 3 sentence similarity tasks:
MRPC , STS-B , QQP , and 4 natural
language inference tasks: MNLI , QNLI , RTE and WNLI . The metrics for these tasks can be found in
the GLUE paper .
TinyBERT Settings
We instantiate a tiny student model (the number
of layers M=4, the hidden size d′=312, the feedforward/ﬁlter size d′
i=1200 and the head number
h=12) that has a total of 14.5M parameters. This
model is referred to as TinyBERT4. The original
3A word is tokenized into multiple word-pieces by the
tokenizer of BERT.
MNLI-(m/mm)
BERTBASE (Teacher)
DistilBERT4
MobileBERTTINY†
TinyBERT4 (ours)
DistilBERT6
TinyBERT6 (ours)
Table 1: Results are evaluated on the test set of GLUE ofﬁcial benchmark. The best results for each group of student
models are in-bold. The architecture of TinyBERT4 and BERTTINY is (M=4, d=312, di=1200), BERTSMALL
is , BERT4-PKD and DistilBERT4 is (M=4, d=768, di=3072) and the architecture of
BERT6-PKD, DistilBERT6 and TinyBERT6 is (M=6, d=768, di=3072). All models are learned in a single-task
manner. The inference speedup is evaluated on a single NVIDIA K80 GPU. † denotes that the comparison between
MobileBERTTINY and TinyBERT4 may not be fair since the former has 24 layers and is task-agnosticly distilled
from IB-BERTLARGE while the later is a 4-layers model task-speciﬁcally distilled from BERTBASE.
BERTBASE (N=12, d=768, di=3072 and h=12)
is used as the teacher model that contains 109M
parameters. We use g(m) = 3 × m as the layer
mapping function, so TinyBERT4 learns from every 3 layers of BERTBASE. The learning weight
λ of each layer is set to 1. Besides, for a direct
comparisons with baselines, we also instantiate a
TinyBERT6 (M=6, d′=768, d′
i=3072 and h=12)
with the same architecture as BERT6-PKD and DistilBERT6 .
TinyBERT learning includes the general distillation and the task-speciﬁc distillation. For the
general distillation, we set the maximum sequence
length to 128 and use English Wikipedia (2,500M
words) as the text corpus and perform the intermediate layer distillation for 3 epochs with the supervision from a pre-trained BERTBASE and keep
other hyper-parameters the same as BERT pretraining . For the task-speciﬁc
distillation, under the supervision of a ﬁne-tuned
BERT, we ﬁrstly perform intermediate layer distillation on the augmented data for 20 epochs4 with
batch size 32 and learning rate 5e-5, and then perform prediction layer distillation on the augmented
data 5 for 3 epochs with choosing the batch size
from {16, 32} and learning rate from {1e-5, 2e-5,
3e-5} on dev set. At task-speciﬁc distillation, the
maximum sequence length is set to 64 for singlesentence tasks, and 128 for sequence pair tasks.
4For large datasets MNLI, QQP, and QNLI, we only perform 10 epochs of the intermediate layer distillation, and for
the challenging task CoLA, we perform 50 epochs at this step.
5For regression task STS-B, the original train set is better.
BERTSMALL6 and several
state-of-the-art KD baselines including BERT-
PKD , PD ,
DistilBERT and Mobile-
BERT .
BERTTINY means
directly pretraining a small BERT, which has the
same model architecture as TinyBERT4. When
training BERTTINY, we follow the same learning
strategy as described in the original BERT . To make a fair comparison, we use
the released code to train a 4-layer BERT4-PKD7
and a 4-layer DistilBERT48 and ﬁne-tuning these
4-layer baselines with suggested hyper-paramters.
For 6-layer baselines, we use the reported numbers
or evaluate the results on the test set of GLUE with
released models.
Experimental Results on GLUE
We submitted our model predictions to the ofﬁcial
GLUE evaluation server to obtain results on the
test set9, as summarized in Table 1.
The experiment results from the 4-layer student
models demonstrate that: 1) There is a large performance gap between BERTTINY (or BERTSMALL)
and BERTBASE due to the dramatic reduction in
model size.
2) TinyBERT4 is consistently better than BERTTINY on all the GLUE tasks and
6 
7 
PKD-for-BERT-Model-Compression
8 
master/examples/distillation
9 
obtains a large improvement of 6.8% on average. This indicates that the proposed KD learning framework can effectively improve the performances of small models on a variety of downstream tasks. 3) TinyBERT4 signiﬁcantly outperforms the 4-layer state-of-the-art KD baselines (i.e.,
BERT4-PKD and DistilBERT4) by a margin of at
least 4.4%, with ∼28% parameters and 3.1x inference speedup. 4) Compared with the teacher
BERTBASE, TinyBERT4 is 7.5x smaller and 9.4x
faster in the model efﬁciency, while maintaining
competitive performances. 5) For the challenging CoLA dataset (the task of predicting linguistic acceptability judgments), all the 4-layer distilled models have big performance gaps compared
to the teacher model, while TinyBERT4 achieves
a signiﬁcant improvement over the 4-layer baselines. 6) We also compare TinyBERT with the 24layer MobileBERTTINY, which is distilled from
24-layer IB-BERTLARGE. The results show that
TinyBERT4 achieves the same average score as
the 24-layer model with only 38.7% FLOPs. 7)
When we increase the capacity of our model to
TinyBERT6, its performance can be further elevated and outperforms the baselines of the same
architecture by a margin of 2.6% on average and
achieves comparable results with the teacher. 8)
Compared with the other two-stage baseline PD,
which ﬁrst pre-trains a small BERT, then performs
distillation on a speciﬁc task with this small model,
TinyBERT initialize the student in task-speciﬁc
stage via general distillation. We analyze these two
initialization methods in Appendix C.
In addition, BERT-PKD and DistilBERT initialize their student models with some layers of a pretrained BERT, which makes the student models
have to keep the same size settings of Transformer
layer (or embedding layer) as their teacher. In our
two-stage distillation framework, TinyBERT is initialized through general distillation, making it more
ﬂexible in choosing model conﬁguration.
More Comparisons. We demonstrate the effectiveness of TinyBERT by including more baselines such as Poor Man’s BERT , BERT-of-Theseus and
MiniLM , some of which only
report results on the GLUE dev set. In addition,
we evaluate TinyBERT on SQuAD v1.1 and v2.0.
Due to the space limit, we present our results in the
Appendix A and B.
Table 2: Ablation studies of different procedures (i.e.,
TD, GD, and DA) of the two-stage learning framework.
The variants are validated on the dev set.
Table 3: Ablation studies of different distillation objectives in the TinyBERT learning. The variants are validated on the dev set.
Ablation Studies
In this section, we conduct ablation studies to investigate the contributions of : a) different procedures of the proposed two-stage TinyBERT learning framework in Figure 1, and b) different distillation objectives in Equation 11.
Effects of Learning Procedure
The proposed two-stage TinyBERT learning framework consists of three key procedures: GD (General Distillation), TD (Task-speciﬁc Distillation)
and DA (Data Augmentation). The performances
of removing each individual learning procedure are
analyzed and presented in Table 2. The results indicate that all of the three procedures are crucial for
the proposed method. The TD and DA has comparable effects in all the four tasks. We note that
the task-speciﬁc procedures (TD and DA) are more
helpful than the pre-training procedure (GD) on all
of the tasks. Another interesting observation is that
GD contribute more on CoLA than on MNLI and
MRPC. We conjecture that the ability of linguistic
generalization learned by
GD plays an important role in the task of linguistic
acceptability judgments.
Effects of Distillation Objective
We investigate the effects of distillation objectives on the TinyBERT learning. Several baselines are proposed including the learning without the Transformer-layer distillation (w/o Trm),
the embedding-layer distillation (w/o Emb) or the
prediction-layer distillation (w/o Pred)10 respectively. The results are illustrated in Table 3 and
show that all the proposed distillation objectives
are useful.
The performance w/o Trm11 drops
signiﬁcantly from 75.6 to 56.3. The reason for
the signiﬁcant drop lies in the initialization of student model. At the pre-training stage, obtaining
a good initialization is crucial for the distillation
of transformer-based models, while there is no supervision signal from upper layers to update the
parameters of transformer layers at this stage under the w/o Trm setting. Furthermore, we study
the contributions of attention (Attn) and hidden
states (Hidn) in the Transformer-layer distillation.
We can ﬁnd the attention based distillation has a
greater impact than hidden states based distillation.
Meanwhile, these two kinds of knowledge distillation are complementary to each other, which makes
them the most important distillation techniques for
Transformer-based model in our experiments.
Effects of Mapping Function
We also investigate the effects of different mapping functions n = g(m) on the TinyBERT learning. Our original TinyBERT as described in section 4.2 uses the uniform strategy, and we compare
with two typical baselines including top-strategy
(g(m) = m + N −M; 0 < m ≤M) and bottomstrategy (g(m) = m; 0 < m ≤M).
The comparison results are presented in Table 4.
We ﬁnd that the top-strategy performs better than
the bottom-strategy on MNLI, while being worse
on MRPC and CoLA, which conﬁrms the observations that different tasks depend on the knowledge
from different BERT layers. The uniform strategy
covers the knowledge from bottom to top layers
of BERTBASE, and it achieves better performances
than the other two baselines in all the tasks. Adaptively choosing layers for a speciﬁc task is a challenging problem and we leave it as future work.
Related Work
Pre-trained Language Models Compression
Generally, pre-trained language models (PLMs)
can be compressed by low-rank approximation (Ma
10The prediction-layer distillation performs soft crossentropy as Equation 10 on the augmented data. “w/o Pred”
means performing standard cross-entropy against the groundtruth of original train set.
11Under “w/o Trm” setting, we actually 1) conduct
embedding-layer distillation at the pre-training stage; 2) perform embedding-layer and prediction-layer distillation at ﬁnetuning stage.
Table 4: Results (dev) of different mapping strategies
for TinyBERT4.
et al., 2019; Lan et al., 2020), weight sharing , knowledge
distillation , pruning or quantization . In this
paper, our focus is on knowledge distillation.
Knowledge Distillation for PLMs There have
been some works trying to distill pre-trained
language models (PLMs) into smaller models.
BiLSTMSOFT distills taskspeciﬁc knowledge from BERT into a singlelayer BiLSTM. BERT-PKD extracts knowledges not only from the last layer
of the teacher, but also from intermediate layers at ﬁne-tuning stage. DistilBERT performs distillation at pre-training stage
on large-scale corpus. Concurrent works, Mobile-
BERT distills a BERTLARGE
augmented with bottleneck structures into a 24layer slimmed version by progressive knowledge
transfer at pre-training stage. MiniLM conducts deep self-attention distillation also
at pre-training stage. By contrast, we propose a new
two-stage learning framework to distill knowledge
from BERT at both pre-training and ﬁne-tuning
stages by a novel transformer distillation method.
Pretraining Lite PLMs Other related works aim
at directly pretraining lite PLMs. Turc et al. 
pre-trained 24 miniature BERT models and show
that pre-training remains important in the context of smaller architectures, and ﬁne-tuning pretrained compact models can be competitive. AL-
BERT incorporates embedding
factorization and cross-layer parameter sharing to
reduce model parameters. Since ALBERT does not
reduce hidden size or layers of transformer block,
it still has large amount of computations. Another
concurrent work, ELECTRA 
proposes a sample-efﬁcient task called replaced token detection to accelerate pre-training, and it also
presents a 12-layer ELECTRAsmall that has comparable performance with TinyBERT4. Different
from these small PLMs, TinyBERT4 is a 4-layer
model which can achieve more speedup.
Conclusion and Future Work
In this paper, we introduced a new method for
Transformer-based distillation, and further proposed a two-stage framework for TinyBERT. Extensive experiments show that TinyBERT achieves
competitive performances meanwhile signiﬁcantly
reducing the model size and inference time of
BERTBASE, which provides an effective way to
deploy BERT-based NLP models on edge devices.
In future work, we would study how to effectively
transfer the knowledge from wider and deeper
teachers (e.g., BERTLARGE) to student TinyBERT.
Combining distillation with quantization/pruning
would be another promising direction to further
compress the pre-trained language models.
Acknowledgements
This work is supported in part by NSFC
NO.61832020, No.61821003, 61772216, National Science and Technology Major Project
No.2017ZX01032-101, Fundamental Research
Funds for the Central Universities.