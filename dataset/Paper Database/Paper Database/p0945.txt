Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163‚Äì4174
November 16 - 20, 2020. c‚Éù2020 Association for Computational Linguistics
TinyBERT: Distilling BERT for Natural Language Understanding
Xiaoqi Jiao1‚àó‚Ä†, Yichun Yin2‚àó‚Ä°, Lifeng Shang2‚Ä°, Xin Jiang2
Xiao Chen2, Linlin Li3, Fang Wang1‚Ä° and Qun Liu2
1Key Laboratory of Information Storage System, Huazhong University of
Science and Technology, Wuhan National Laboratory for Optoelectronics
2Huawei Noah‚Äôs Ark Lab
3Huawei Technologies Co., Ltd.
{jiaoxiaoqi,wangfang}@hust.edu.cn
{yinyichun,shang.lifeng,jiang.xin}@huawei.com
{chen.xiao2,lynn.lilinlin,qun.liu}@huawei.com
Language model pre-training, such as BERT,
has signiÔ¨Åcantly improved the performances
of many natural language processing tasks.
However, pre-trained language models are usually computationally expensive, so it is difÔ¨Åcult to efÔ¨Åciently execute them on resourcerestricted devices.
To accelerate inference
and reduce model size while maintaining
accuracy, we Ô¨Årst propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the
Transformer-based models. By leveraging this
new KD method, the plenty of knowledge encoded in a large ‚Äúteacher‚Äù BERT can be effectively transferred to a small ‚Äústudent‚Äù Tiny-
BERT. Then, we introduce a new two-stage
learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-speciÔ¨Åc learning stages. This
framework ensures that TinyBERT can capture
the general-domain as well as the task-speciÔ¨Åc
knowledge in BERT.
TinyBERT41 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERTBASE on GLUE
benchmark, while being 7.5x smaller and 9.4x
faster on inference. TinyBERT4 is also signiÔ¨Åcantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ‚àº28%
parameters and ‚àº31% inference time of them.
Moreover, TinyBERT6 with 6 layers performs
on-par with its teacher BERTBASE.
Introduction
Pre-training language models then Ô¨Åne-tuning on
downstream tasks has become a new paradigm for
‚àóAuthors contribute equally.
‚Ä†This work is done when Xiaoqi Jiao is an intern at
Huawei Noah‚Äôs Ark Lab.
‚Ä°Corresponding authors.
1The code and models are publicly available at https:
//github.com/huawei-noah/Pretrained-Language-Model/tree/
master/TinyBERT
natural language processing (NLP). Pre-trained language models (PLMs), such as BERT , XLNet , RoBERTa , ALBERT , T5 and ELECTRA ,
have achieved great success in many NLP tasks
 
and the challenging multi-hop reasoning task ).
However, PLMs usually have a
large number of parameters and take long inference time, which are difÔ¨Åcult to be deployed on
edge devices such as mobile phones. Recent studies demonstrate that there is redundancy
in PLMs. Therefore, it is crucial and feasible to
reduce the computational overhead and model storage of PLMs while retaining their performances.
There have been many model compression techniques proposed to accelerate
deep model inference and reduce model size while
maintaining accuracy. The most commonly used
techniques include quantization ,
weights pruning , and knowledge distillation (KD) . In
this paper, we focus on knowledge distillation, an
idea originated from Hinton et al. , in a
teacher-student framework. KD aims to transfer
the knowledge embedded in a large teacher network to a small student network where the student
network is trained to reproduce the behaviors of the
teacher network. Based on the framework, we propose a novel distillation method speciÔ¨Åcally for the
Transformer-based models ,
and use BERT as an example to investigate the
method for large-scale PLMs.
KD has been extensively studied in NLP as well as for
pre-trained language models . The
pre-training-then-Ô¨Åne-tuning paradigm Ô¨Årstly pre-
Large-scale
Text Corpus
Fine-tuned
Distillation
Task Dataset
Task Dataset
Data Augmentation
Task-specific
Distillation
Figure 1: The illustration of TinyBERT learning.
trains BERT on a large-scale unsupervised text
corpus, then Ô¨Åne-tunes it on task-speciÔ¨Åc dataset,
which greatly increases the difÔ¨Åculty of BERT distillation. Therefore, it is required to design an effective KD strategy for both training stages.
To build a competitive TinyBERT, we Ô¨Årstly
propose a new Transformer distillation method to
distill the knowledge embedded in teacher BERT.
SpeciÔ¨Åcally, we design three types of loss functions to Ô¨Åt different representations from BERT
layers: 1) the output of the embedding layer; 2) the
hidden states and attention matrices derived from
the Transformer layer; 3) the logits output by the
prediction layer. The attention based Ô¨Åtting is inspired by the recent Ô¨Åndings 
that the attention weights learned by BERT can capture substantial linguistic knowledge, and it thus
encourages the linguistic knowledge can be well
transferred from teacher BERT to student Tiny-
BERT. Then, we propose a novel two-stage learning framework including the general distillation
and the task-speciÔ¨Åc distillation, as illustrated in
Figure 1. At general distillation stage, the original
BERT without Ô¨Åne-tuning acts as the teacher model.
The student TinyBERT mimics the teacher‚Äôs behavior through the proposed Transformer distillation
on general-domain corpus. After that, we obtain
a general TinyBERT that is used as the initialization of student model for the further distillation.
At the task-speciÔ¨Åc distillation stage, we Ô¨Årst do
the data augmentation, then perform the distillation on the augmented dataset using the Ô¨Åne-tuned
BERT as the teacher model. It should be pointed
out that both the two stages are essential to improve
the performance and generalization capability of
The main contributions of this work are as follows: 1) We propose a new Transformer distillation method to encourage that the linguistic knowledge encoded in teacher BERT can be adequately
transferred to TinyBERT; 2) We propose a novel
two-stage learning framework with performing the
proposed Transformer distillation at both the pretraining and Ô¨Åne-tuning stages, which ensures that
TinyBERT can absorb both the general-domain and
task-speciÔ¨Åc knowledge of the teacher BERT. 3)
We show in the experiments that our TinyBERT4
can achieve more than 96.8% the performance of
teacher BERTBASE on GLUE tasks, while having
much fewer parameters (‚àº13.3%) and less inference time (‚àº10.6%), and signiÔ¨Åcantly outperforms
other state-of-the-art baselines with 4 layers on
BERT distillation; 4) We also show that a 6-layer
TinyBERT6 can perform on-par with the teacher
BERTBASE on GLUE.
Preliminaries
In this section, we describe the formulation of
Transformer and Knowledge
Distillation . Our proposed
Transformer distillation is a specially designed KD
method for Transformer-based models.
Transformer Layer
Most of the recent pre-trained language models (e.g., BERT, XLNet and RoBERTa) are built
with Transformer layers, which can capture longterm dependencies between input tokens by selfattention mechanism.
SpeciÔ¨Åcally, a standard
Transformer layer includes two main sub-layers:
multi-head attention (MHA) and fully connected
feed-forward network (FFN).
Multi-Head Attention (MHA). The calculation of
attention function depends on the three components
of queries, keys and values, denoted as matrices Q,
K and V respectively. The attention function can
be formulated as follows:
Attention(Q, K, V )=softmax(A)V , (2)
where dk is the dimension of keys and acts as a
scaling factor, A is the attention matrix calculated
from the compatibility of Q and K by dot-product
operation. The Ô¨Ånal function output is calculated
as a weighted sum of values V , and the weight is
computed by applying softmax() operation on
the each column of matrix A. According to Clark
et al. , the attention matrices in BERT can
capture substantial linguistic knowledge, and thus
play an essential role in our proposed distillation
Multi-head attention is deÔ¨Åned by concatenating
the attention heads from different representation
subspaces as follows:
MHA(Q, K, V )=Concat(h1, . . . , hk)W , (3)
where k is the number of attention heads, and hi
denotes the i-th attention head, which is calculated
by the Attention() function with inputs from
different representation subspaces. The matrix W
acts as a linear transformation.
Position-wise Feed-Forward Network (FFN).
Transformer layer also contains a fully connected
feed-forward network, which is formulated as follows:
FFN(x) = max(0, xW1 + b1)W2 + b2.
We can see that the FFN contains two linear transformations and one ReLU activation.
Knowledge Distillation
KD aims to transfer the knowledge of a large
teacher network T to a small student network S.
The student network is trained to mimic the behaviors of teacher networks. Let fT and fS represent
the behavior functions of teacher and student networks, respectively. The behavior function targets
at transforming network inputs to some informative representations, and it can be deÔ¨Åned as the
output of any layer in the network. In the context of
Transformer distillation, the output of MHA layer
or FFN layer, or some intermediate representations
(such as the attention matrix A) can be used as
behavior function. Formally, KD can be modeled
as minimizing the following objective function:
 fS(x), fT (x)
where L(¬∑) is a loss function that evaluates the difference between teacher and student networks, x
is the text input and X denotes the training dataset.
Thus the key research problem becomes how to de-
Ô¨Åne effective behavior functions and loss functions.
Different from previous KD methods, we also need
to consider how to perform KD at the pre-training
stage of BERT in addition to the task-speciÔ¨Åc training stage.
In this section, we propose a novel distillation
method for Transformer-based models, and present
a two-stage learning framework for our model distilled from BERT, which is called TinyBERT.
(‚Ñù‚Ñéùëíùëéùëë‚àóùëô‚àóùëô)
(‚Ñù‚Ñéùëíùëéùëë‚àóùëô‚àóùëô)
ADD & Norm
ADD & Norm
ADD & Norm
ADD & Norm
Attention Matrices
Attention Matrices
Hidden States
Hidden States
Teacher Layer
Student Layer
Figure 2: The details of Transformer-layer distillation
consisting of Attnloss(attention based distillation) and
Hidnloss(hidden states based distillation).
Transformer Distillation
The proposed Transformer distillation is a specially
designed KD method for Transformer networks. In
this work, both the student and teacher networks are
built with Transformer layers. For a clear illustration, we formulate the problem before introducing
our method.
Problem Formulation. Assuming that the student model has M Transformer layers and teacher
model has N Transformer layers, we start with
choosing M out of N layers from the teacher
model for the Transformer-layer distillation. Then
a function n = g(m) is deÔ¨Åned as the mapping
function between indices from student layers to
teacher layers, which means that the m-th layer
of student model learns the information from the
g(m)-th layer of teacher model. To be precise, we
set 0 to be the index of embedding layer and M +1
to be the index of prediction layer, and the corresponding layer mappings are deÔ¨Åned as 0 = g(0)
and N + 1 = g(M + 1) respectively. The effect
of the choice of different mapping functions on the
performances is studied in the experiment section.
Formally, the student can acquire knowledge from
the teacher by minimizing the following objective:
ŒªmLlayer(fS
g(m)(x)), (6)
where Llayer refers to the loss function of a given
model layer (e.g., Transformer layer or embedding layer), fm(x) denotes the behavior function
induced from the m-th layers and Œªm is the hyper-
parameter that represents the importance of the
m-th layer‚Äôs distillation.
Transformer-layer Distillation.
The proposed
Transformer-layer distillation includes the attention based distillation and hidden states based distillation, which is shown in Figure 2. The attention based distillation is motivated by the recent
Ô¨Åndings that attention weights learned by BERT
can capture rich linguistic knowledge . This kind of linguistic knowledge includes
the syntax and coreference information, which is
essential for natural language understanding. Thus
we propose the attention based distillation to encourage that the linguistic knowledge can be transferred from teacher (BERT) to student (TinyBERT).
SpeciÔ¨Åcally, the student learns to Ô¨Åt the matrices
of multi-head attention in the teacher network, and
the objective is deÔ¨Åned as:
i=1 MSE(AS
where h is the number of attention heads, Ai ‚àà
Rl√ól refers to the attention matrix corresponding
to the i-th head of teacher or student, l is the input
text length, and MSE() means the mean squared
error loss function. In this work, the (unnormalized) attention matrix Ai is used as the Ô¨Åtting target
instead of its softmax output softmax(Ai), since
our experiments show that the former setting has a
faster convergence rate and better performances.
In addition to the attention based distillation,
we also distill the knowledge from the output of
Transformer layer, and the objective is as follows:
Lhidn = MSE(HSWh, HT ),
where the matrices HS ‚ààRl√ód‚Ä≤ and HT ‚ààRl√ód
refer to the hidden states of student and teacher networks respectively, which are calculated by Equation 4. The scalar values d and d‚Ä≤ denote the hidden
sizes of teacher and student models, and d‚Ä≤ is often
smaller than d to obtain a smaller student network.
The matrix Wh ‚ààRd‚Ä≤√ód is a learnable linear transformation, which transforms the hidden states of
student network into the same space as the teacher
network‚Äôs states.
Embedding-layer Distillation.
Similar to the
hidden states based distillation, we also perform
embedding-layer distillation and the objective is:
Lembd = MSE(ESWe, ET ),
where the matrices ES and HT refer to the embeddings of student and teacher networks, respectively. In this paper, they have the same shape as
the hidden state matrices. The matrix We is a linear
transformation playing a similar role as Wh.
Prediction-layer Distillation. In addition to imitating the behaviors of intermediate layers, we
also use the knowledge distillation to Ô¨Åt the predictions of teacher model as in Hinton et al. .
SpeciÔ¨Åcally, we penalize the soft cross-entropy loss
between the student network‚Äôs logits against the
teacher‚Äôs logits:
Lpred = CE(zT /t, zS/t),
where zS and zT are the logits vectors predicted
by the student and teacher respectively, CE means
the cross entropy loss, and t means the temperature value. In our experiment, we Ô¨Ånd that t = 1
performs well.
Using the above distillation objectives (i.e. Equations 7, 8, 9 and 10), we can unify the distillation loss of the corresponding layers between the
teacher and the student network:
Lhidn+Lattn, M ‚â•m>0
TinyBERT Learning
The application of BERT usually consists of two
learning stages: the pre-training and Ô¨Åne-tuning.
The plenty of knowledge learned by BERT in the
pre-training stage is of great importance and should
be transferred to the compressed model. Therefore, we propose a novel two-stage learning framework including the general distillation and the
task-speciÔ¨Åc distillation, as illustrated in Figure 1.
General distillation helps TinyBERT learn the rich
knowledge embedded in pre-trained BERT, which
plays an important role in improving the generalization capability of TinyBERT. The task-speciÔ¨Åc
distillation further teaches TinyBERT the knowledge from the Ô¨Åne-tuned BERT. With the two-step
distillation, we can substantially reduce the gap
between teacher and student models.
General Distillation. We use the original BERT
without Ô¨Åne-tuning as the teacher and a large-scale
text corpus as the training data. By performing the
Transformer distillation 2 on the text from general
2In the general distillation, we do not perform predictionlayer distillation as Equation 10. Our motivation is to make
Algorithm 1 Data Augmentation Procedure for
Task-speciÔ¨Åc Distillation
Input: x is a sequence of words
Params: pt: the threshold probability
Na: the number of samples augmented per example
K: the size of candidate set
Output: D‚Ä≤: the augmented data
1: n ‚Üê0 ; D‚Ä≤ ‚Üê[ ]
2: while n < Na do
for i ‚Üê1 to len(x) do
if x[i] is a single-piece word then
Replace xm[i] with [MASK]
C ‚ÜêK most probable words of BERT(xm)[i]
C ‚ÜêK most similar words of x[i] from GloVe
Sample p ‚àºUniform(0, 1)
if p ‚â§pt then
Replace xm[i] with a word in C randomly
Append xm to D‚Ä≤
18: end while
19: return D‚Ä≤
domain, we obtain a general TinyBERT that can be
Ô¨Åne-tuned for downstream tasks. However, due to
the signiÔ¨Åcant reductions of the hidden/embedding
size and the layer number, general TinyBERT performs generally worse than BERT.
Task-speciÔ¨Åc Distillation.
Previous studies
show that the complex models, such as Ô¨Ånetuned BERTs, suffer from over-parametrization for
domain-speciÔ¨Åc tasks . Thus,
it is possible for smaller models to achieve comparable performances to the BERTs. To this end,
we propose to produce competitive Ô¨Åne-tuned Tiny-
BERTs through the task-speciÔ¨Åc distillation. In
the task-speciÔ¨Åc distillation, we re-perform the proposed Transformer distillation on an augmented
task-speciÔ¨Åc dataset. SpeciÔ¨Åcally, the Ô¨Åne-tuned
BERT is used as the teacher and a data augmentation method is proposed to expand the task-speciÔ¨Åc
training set. Training with more task-related examples, the generalization ability of the student model
can be further improved.
Data Augmentation. We combine a pre-trained
language model BERT and GloVe word embeddings to do word-level
the TinyBERT primarily learn the intermediate structures of
BERT at pre-training stage. From our preliminary experiments, we also found that conducting prediction-layer distillation at pre-training stage does not bring extra improvements
on downstream tasks, when the Transformer-layer distillation
(Attn and Hidn distillation) and Embedding-layer distillation
have already been performed.
replacement for data augmentation. SpeciÔ¨Åcally,
we use the language model to predict word replacements for single-piece words , and
use the word embeddings to retrieve the most similar words as word replacements for multiple-pieces
words3. Some hyper-parameters are deÔ¨Åned to control the replacement ratio of a sentence and the
amount of augmented dataset. More details of the
data augmentation procedure are shown in Algorithm 1. We set pt = 0.4, Na = 20, K = 15 for all
our experiments.
The above two learning stages are complementary to each other: the general distillation provides
a good initialization for the task-speciÔ¨Åc distillation, while the task-speciÔ¨Åc distillation on the augmented data further improves TinyBERT by focusing on learning the task-speciÔ¨Åc knowledge. Although there is a signiÔ¨Åcant reduction of model size,
with the data augmentation and by performing the
proposed Transformer distillation method at both
the pre-training and Ô¨Åne-tuning stages, TinyBERT
can achieve competitive performances in various
NLP tasks.
Experiments
In this section, we evaluate the effectiveness and
efÔ¨Åciency of TinyBERT on a variety of tasks with
different model settings.
We evaluate TinyBERT on the General Language
Understanding Evaluation (GLUE) benchmark, which consists of 2 singlesentence tasks: CoLA , SST-
2 , 3 sentence similarity tasks:
MRPC , STS-B , QQP , and 4 natural
language inference tasks: MNLI , QNLI , RTE and WNLI . The metrics for these tasks can be found in
the GLUE paper .
TinyBERT Settings
We instantiate a tiny student model (the number
of layers M=4, the hidden size d‚Ä≤=312, the feedforward/Ô¨Ålter size d‚Ä≤
i=1200 and the head number
h=12) that has a total of 14.5M parameters. This
model is referred to as TinyBERT4. The original
3A word is tokenized into multiple word-pieces by the
tokenizer of BERT.
MNLI-(m/mm)
BERTBASE (Teacher)
DistilBERT4
MobileBERTTINY‚Ä†
TinyBERT4 (ours)
DistilBERT6
TinyBERT6 (ours)
Table 1: Results are evaluated on the test set of GLUE ofÔ¨Åcial benchmark. The best results for each group of student
models are in-bold. The architecture of TinyBERT4 and BERTTINY is (M=4, d=312, di=1200), BERTSMALL
is , BERT4-PKD and DistilBERT4 is (M=4, d=768, di=3072) and the architecture of
BERT6-PKD, DistilBERT6 and TinyBERT6 is (M=6, d=768, di=3072). All models are learned in a single-task
manner. The inference speedup is evaluated on a single NVIDIA K80 GPU. ‚Ä† denotes that the comparison between
MobileBERTTINY and TinyBERT4 may not be fair since the former has 24 layers and is task-agnosticly distilled
from IB-BERTLARGE while the later is a 4-layers model task-speciÔ¨Åcally distilled from BERTBASE.
BERTBASE (N=12, d=768, di=3072 and h=12)
is used as the teacher model that contains 109M
parameters. We use g(m) = 3 √ó m as the layer
mapping function, so TinyBERT4 learns from every 3 layers of BERTBASE. The learning weight
Œª of each layer is set to 1. Besides, for a direct
comparisons with baselines, we also instantiate a
TinyBERT6 (M=6, d‚Ä≤=768, d‚Ä≤
i=3072 and h=12)
with the same architecture as BERT6-PKD and DistilBERT6 .
TinyBERT learning includes the general distillation and the task-speciÔ¨Åc distillation. For the
general distillation, we set the maximum sequence
length to 128 and use English Wikipedia (2,500M
words) as the text corpus and perform the intermediate layer distillation for 3 epochs with the supervision from a pre-trained BERTBASE and keep
other hyper-parameters the same as BERT pretraining . For the task-speciÔ¨Åc
distillation, under the supervision of a Ô¨Åne-tuned
BERT, we Ô¨Årstly perform intermediate layer distillation on the augmented data for 20 epochs4 with
batch size 32 and learning rate 5e-5, and then perform prediction layer distillation on the augmented
data 5 for 3 epochs with choosing the batch size
from {16, 32} and learning rate from {1e-5, 2e-5,
3e-5} on dev set. At task-speciÔ¨Åc distillation, the
maximum sequence length is set to 64 for singlesentence tasks, and 128 for sequence pair tasks.
4For large datasets MNLI, QQP, and QNLI, we only perform 10 epochs of the intermediate layer distillation, and for
the challenging task CoLA, we perform 50 epochs at this step.
5For regression task STS-B, the original train set is better.
BERTSMALL6 and several
state-of-the-art KD baselines including BERT-
PKD , PD ,
DistilBERT and Mobile-
BERT .
BERTTINY means
directly pretraining a small BERT, which has the
same model architecture as TinyBERT4. When
training BERTTINY, we follow the same learning
strategy as described in the original BERT . To make a fair comparison, we use
the released code to train a 4-layer BERT4-PKD7
and a 4-layer DistilBERT48 and Ô¨Åne-tuning these
4-layer baselines with suggested hyper-paramters.
For 6-layer baselines, we use the reported numbers
or evaluate the results on the test set of GLUE with
released models.
Experimental Results on GLUE
We submitted our model predictions to the ofÔ¨Åcial
GLUE evaluation server to obtain results on the
test set9, as summarized in Table 1.
The experiment results from the 4-layer student
models demonstrate that: 1) There is a large performance gap between BERTTINY (or BERTSMALL)
and BERTBASE due to the dramatic reduction in
model size.
2) TinyBERT4 is consistently better than BERTTINY on all the GLUE tasks and
6 
7 
PKD-for-BERT-Model-Compression
8 
master/examples/distillation
9 
obtains a large improvement of 6.8% on average. This indicates that the proposed KD learning framework can effectively improve the performances of small models on a variety of downstream tasks. 3) TinyBERT4 signiÔ¨Åcantly outperforms the 4-layer state-of-the-art KD baselines (i.e.,
BERT4-PKD and DistilBERT4) by a margin of at
least 4.4%, with ‚àº28% parameters and 3.1x inference speedup. 4) Compared with the teacher
BERTBASE, TinyBERT4 is 7.5x smaller and 9.4x
faster in the model efÔ¨Åciency, while maintaining
competitive performances. 5) For the challenging CoLA dataset (the task of predicting linguistic acceptability judgments), all the 4-layer distilled models have big performance gaps compared
to the teacher model, while TinyBERT4 achieves
a signiÔ¨Åcant improvement over the 4-layer baselines. 6) We also compare TinyBERT with the 24layer MobileBERTTINY, which is distilled from
24-layer IB-BERTLARGE. The results show that
TinyBERT4 achieves the same average score as
the 24-layer model with only 38.7% FLOPs. 7)
When we increase the capacity of our model to
TinyBERT6, its performance can be further elevated and outperforms the baselines of the same
architecture by a margin of 2.6% on average and
achieves comparable results with the teacher. 8)
Compared with the other two-stage baseline PD,
which Ô¨Årst pre-trains a small BERT, then performs
distillation on a speciÔ¨Åc task with this small model,
TinyBERT initialize the student in task-speciÔ¨Åc
stage via general distillation. We analyze these two
initialization methods in Appendix C.
In addition, BERT-PKD and DistilBERT initialize their student models with some layers of a pretrained BERT, which makes the student models
have to keep the same size settings of Transformer
layer (or embedding layer) as their teacher. In our
two-stage distillation framework, TinyBERT is initialized through general distillation, making it more
Ô¨Çexible in choosing model conÔ¨Åguration.
More Comparisons. We demonstrate the effectiveness of TinyBERT by including more baselines such as Poor Man‚Äôs BERT , BERT-of-Theseus and
MiniLM , some of which only
report results on the GLUE dev set. In addition,
we evaluate TinyBERT on SQuAD v1.1 and v2.0.
Due to the space limit, we present our results in the
Appendix A and B.
Table 2: Ablation studies of different procedures (i.e.,
TD, GD, and DA) of the two-stage learning framework.
The variants are validated on the dev set.
Table 3: Ablation studies of different distillation objectives in the TinyBERT learning. The variants are validated on the dev set.
Ablation Studies
In this section, we conduct ablation studies to investigate the contributions of : a) different procedures of the proposed two-stage TinyBERT learning framework in Figure 1, and b) different distillation objectives in Equation 11.
Effects of Learning Procedure
The proposed two-stage TinyBERT learning framework consists of three key procedures: GD (General Distillation), TD (Task-speciÔ¨Åc Distillation)
and DA (Data Augmentation). The performances
of removing each individual learning procedure are
analyzed and presented in Table 2. The results indicate that all of the three procedures are crucial for
the proposed method. The TD and DA has comparable effects in all the four tasks. We note that
the task-speciÔ¨Åc procedures (TD and DA) are more
helpful than the pre-training procedure (GD) on all
of the tasks. Another interesting observation is that
GD contribute more on CoLA than on MNLI and
MRPC. We conjecture that the ability of linguistic
generalization learned by
GD plays an important role in the task of linguistic
acceptability judgments.
Effects of Distillation Objective
We investigate the effects of distillation objectives on the TinyBERT learning. Several baselines are proposed including the learning without the Transformer-layer distillation (w/o Trm),
the embedding-layer distillation (w/o Emb) or the
prediction-layer distillation (w/o Pred)10 respectively. The results are illustrated in Table 3 and
show that all the proposed distillation objectives
are useful.
The performance w/o Trm11 drops
signiÔ¨Åcantly from 75.6 to 56.3. The reason for
the signiÔ¨Åcant drop lies in the initialization of student model. At the pre-training stage, obtaining
a good initialization is crucial for the distillation
of transformer-based models, while there is no supervision signal from upper layers to update the
parameters of transformer layers at this stage under the w/o Trm setting. Furthermore, we study
the contributions of attention (Attn) and hidden
states (Hidn) in the Transformer-layer distillation.
We can Ô¨Ånd the attention based distillation has a
greater impact than hidden states based distillation.
Meanwhile, these two kinds of knowledge distillation are complementary to each other, which makes
them the most important distillation techniques for
Transformer-based model in our experiments.
Effects of Mapping Function
We also investigate the effects of different mapping functions n = g(m) on the TinyBERT learning. Our original TinyBERT as described in section 4.2 uses the uniform strategy, and we compare
with two typical baselines including top-strategy
(g(m) = m + N ‚àíM; 0 < m ‚â§M) and bottomstrategy (g(m) = m; 0 < m ‚â§M).
The comparison results are presented in Table 4.
We Ô¨Ånd that the top-strategy performs better than
the bottom-strategy on MNLI, while being worse
on MRPC and CoLA, which conÔ¨Årms the observations that different tasks depend on the knowledge
from different BERT layers. The uniform strategy
covers the knowledge from bottom to top layers
of BERTBASE, and it achieves better performances
than the other two baselines in all the tasks. Adaptively choosing layers for a speciÔ¨Åc task is a challenging problem and we leave it as future work.
Related Work
Pre-trained Language Models Compression
Generally, pre-trained language models (PLMs)
can be compressed by low-rank approximation (Ma
10The prediction-layer distillation performs soft crossentropy as Equation 10 on the augmented data. ‚Äúw/o Pred‚Äù
means performing standard cross-entropy against the groundtruth of original train set.
11Under ‚Äúw/o Trm‚Äù setting, we actually 1) conduct
embedding-layer distillation at the pre-training stage; 2) perform embedding-layer and prediction-layer distillation at Ô¨Ånetuning stage.
Table 4: Results (dev) of different mapping strategies
for TinyBERT4.
et al., 2019; Lan et al., 2020), weight sharing , knowledge
distillation , pruning or quantization . In this
paper, our focus is on knowledge distillation.
Knowledge Distillation for PLMs There have
been some works trying to distill pre-trained
language models (PLMs) into smaller models.
BiLSTMSOFT distills taskspeciÔ¨Åc knowledge from BERT into a singlelayer BiLSTM. BERT-PKD extracts knowledges not only from the last layer
of the teacher, but also from intermediate layers at Ô¨Åne-tuning stage. DistilBERT performs distillation at pre-training stage
on large-scale corpus. Concurrent works, Mobile-
BERT distills a BERTLARGE
augmented with bottleneck structures into a 24layer slimmed version by progressive knowledge
transfer at pre-training stage. MiniLM conducts deep self-attention distillation also
at pre-training stage. By contrast, we propose a new
two-stage learning framework to distill knowledge
from BERT at both pre-training and Ô¨Åne-tuning
stages by a novel transformer distillation method.
Pretraining Lite PLMs Other related works aim
at directly pretraining lite PLMs. Turc et al. 
pre-trained 24 miniature BERT models and show
that pre-training remains important in the context of smaller architectures, and Ô¨Åne-tuning pretrained compact models can be competitive. AL-
BERT incorporates embedding
factorization and cross-layer parameter sharing to
reduce model parameters. Since ALBERT does not
reduce hidden size or layers of transformer block,
it still has large amount of computations. Another
concurrent work, ELECTRA 
proposes a sample-efÔ¨Åcient task called replaced token detection to accelerate pre-training, and it also
presents a 12-layer ELECTRAsmall that has comparable performance with TinyBERT4. Different
from these small PLMs, TinyBERT4 is a 4-layer
model which can achieve more speedup.
Conclusion and Future Work
In this paper, we introduced a new method for
Transformer-based distillation, and further proposed a two-stage framework for TinyBERT. Extensive experiments show that TinyBERT achieves
competitive performances meanwhile signiÔ¨Åcantly
reducing the model size and inference time of
BERTBASE, which provides an effective way to
deploy BERT-based NLP models on edge devices.
In future work, we would study how to effectively
transfer the knowledge from wider and deeper
teachers (e.g., BERTLARGE) to student TinyBERT.
Combining distillation with quantization/pruning
would be another promising direction to further
compress the pre-trained language models.
Acknowledgements
This work is supported in part by NSFC
NO.61832020, No.61821003, 61772216, National Science and Technology Major Project
No.2017ZX01032-101, Fundamental Research
Funds for the Central Universities.