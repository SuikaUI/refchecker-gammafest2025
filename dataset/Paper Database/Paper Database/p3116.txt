HAL Id: hal-00778520
 
 
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Cellular Tree Classifiers
Gérard Biau, Luc Devroye
To cite this version:
Gérard Biau, Luc Devroye. Cellular Tree Classifiers. 2013. ￿hal-00778520v2￿
Cellular Tree Classiﬁers
G´erard Biau
Universit´e Pierre et Marie Curie1 & Ecole Normale Sup´erieure2, France
 
Luc Devroye
McGill University, Canada3
 
The cellular tree classiﬁer model addresses a fundamental problem in
the design of classiﬁers for a parallel or distributed computing world:
Given a data set, is it suﬃcient to apply a majority rule for classiﬁcation, or shall one split the data into two or more parts and send each
part to a potentially diﬀerent computer (or cell) for further processing? At ﬁrst sight, it seems impossible to deﬁne with this paradigm
a consistent classiﬁer as no cell knows the “original data size”, n.
However, we show that this is not so by exhibiting two diﬀerent consistent classiﬁers. The consistency is universal but is only shown for
distributions with nonatomic marginals.
Index Terms — Classiﬁcation, pattern recognition, tree classiﬁers, cellular computation, Bayes risk consistency, asymptotic analysis, nonparametric estimation.
2010 Mathematics Subject Classiﬁcation: 62G05, 62G20.
Introduction
The problem
We explore in this paper a new way of dealing with the supervised classiﬁcation problem. In the model we have in mind, a basic computational unit
in classiﬁcation, a cell, takes as input training data, and makes a decision
whether a majority rule should be applied to all data, or whether the data
should be split, and each part of the partition should be given to another cell.
1Research partially supported by the French National Research Agency (grant ANR-
09-BLAN-0051-02 “CLARA”) and by the Institut universitaire de France.
2Research carried out within the INRIA project “CLASSIC” hosted by Ecole Normale
Sup´erieure and CNRS.
3Research sponsored by NSERC Grant A3456 and FQRNT Grant 90-ER-0291.
All cells must be the same—their function is not altered by external inputs.
In other words, the decision to split depends only upon the data presented to
the cell. Classiﬁers designed according to this autonomous principle will be
called cellular tree classiﬁers, or simply cellular classiﬁers. This manner of
tackling the classiﬁcation problem is novel, but has a wide reach in a world in
which parallel and distributed computation are important. In the short term,
parallelism will take hold in massive data sets and complex systems and, as
such, is one of the exciting questions that will be asked to the statistics and
machine learning ﬁelds.
The purpose of the present document is to formalize the setting and to provide a foundational discussion of various properties, good and bad, of tree
classiﬁers that are formulated following these principles. Our constructions
lead to classiﬁers that always converge. They are the ﬁrst consistent cellular
classiﬁers that we are aware of. This article is also motivated by the challenges involved in “big data” issues , in which recursive approaches such as divide-and-conquer algorithms play a central role. Such procedures are naturally adapted for execution in multi-processor machines, especially shared-memory systems where
the communication of data between processors does not need to be planned
in advance.
In the design of classiﬁers, we have an unknown distribution of a random
prototype pair (X, Y ), where X takes values in Rd and Y takes only ﬁnitely
many values, say 0 or 1 for simplicity. Classical pattern recognition deals
with predicting the unknown nature Y of the observation X via a measurable
classiﬁer g : Rd →{0, 1}. Since it is not assumed that X fully determines the
label, it is certainly possible to misspecify its associated class. Thus, we err
if g(X) diﬀers from Y , and the probability of error for a particular decision
rule g is L(g) = P{g(X) ̸= Y }. The Bayes classiﬁer
if P{Y = 1|X = x} > P{Y = 0|X = x}
has the smallest probability of error, that is
L⋆= L(g⋆) =
g:Rd→{0,1} P{g(X) ̸= Y }
 .
However, most
of the time, the distribution of (X, Y ) is unknown, so that g⋆is unknown
too. Fortunately, it is often possible to collect a sample (the data) Dn =
((X1, Y1), . . . , (Xn, Yn)) of independent and identically distributed (i.i.d.) copies of (X, Y ). We assume that Dn and (X, Y ) are independent. In this
context, a classiﬁer gn(x; Dn) is a measurable function of x and Dn, and it
attempts to estimate Y from X and Dn. For simplicity, we suppress Dn in
the notation and write gn(x) instead of gn(x; Dn).
The probability of error of a given classiﬁer gn is the random variable
L(gn) = P{gn(X) ̸= Y |Dn},
and the rule is consistent if
n→∞EL(gn) = L⋆.
It is universally consistent if it is consistent for all possible distributions of
(X, Y ). Many popular classiﬁers are universally consistent. These include
several brands of histogram rules, k-nearest neighbor rules, kernel rules, neural networks, and tree classiﬁers. There are too many references to be cited
here, but the monographs by Devroye et al. and Gy¨orﬁet al. 
will provide the reader with a comprehensive introduction to the domain and
a literature review. Among these rules, tree methods loom large for several
reasons. All procedures that partition space, such as histogram rules, can
be viewed as special cases of partitions generated by trees. Simple neural
networks that use voting methods can also be regarded as trees, and similarly, kernel methods with kernels that are indicator functions of sets are but
special cases of tree methods. Tree classiﬁers are conceptually simple, and
explain the data very well. However, their design can be cumbersome, as
optimizations performed over all possible tree classiﬁers that follow certain
restrictions could face a huge combinatorial and computational hurdle. The
cellular paradigm addresses these concerns.
Partitions of Rd based upon trees have been studied in the computational
geometry literature and the computer graphics
literature . Most popular among these are the k-d trees
and quadtrees. Our version of space partitioning corresponds to Bentley’s
k-d trees . The basic notions of trees as related to pattern recognition
can be found in Chapter 20 of Devroye et al. . However, trees have
been suggested as tools for classiﬁcation more than twenty years before that.
We mention in particular the early work of Fu . Other references from the 1970s include Meisel and Michalopoulos
 ; Bartolucci et al. ; Payne and Meisel ; Sethi and Chatterjee ; Swain and Hauska ; Gordon and Olshen ; Friedman
 . Most inﬂuential in the classiﬁcation tree literature was the CART
Figure 1: A binary tree (left) and the corresponding partition (right).
proposal by Breiman et al. .
While CART proposes partitions by
hyperrectangles, linear hyperplanes in general position have also gained in
popularity—the early work on that topic is by Loh and Vanichsetakul ,
and Park and Sklansky . Additional references on tree classiﬁcation
include Gustafson et al. ; Argentiero et al. ; Hartmann et al.
 ; Kurzynski ; Wang and Suen ; Suen and Wang ;
Shlien ; Chou ; Gelfand and Delp ; Gelfand et al. ;
Simon ; Guo and Gelfand .
The cellular computation spirit
In general, classiﬁcation trees partition Rd into regions, often hyperrectangles
parallel to the axes (an example is depicted in Figure 1). In t-ary trees,
each node has exactly t or 0 children.
If a node u represents the set A
and its children u1, . . . , ut represent A1, . . . , At, then it is required that A =
A1 ∪· · · ∪At and Ai ∩Aj = ∅for i ̸= j. The root of the tree represents Rd,
and the terminal nodes (or leaves), taken together, form a partition of Rd. If
a leaf represents region A, then the tree classiﬁer takes the simple form
i=1 1[Xi∈A,Yi=1] > Pn
i=1 1[Xi∈A,Yi=0],
otherwise.
That is, in every leaf region, a majority vote is taken over all (Xi, Yi)’s with
Xi’s in the same region. Ties are broken, by convention, in favor of class 0.
The tree structure is usually data-dependent, as well, and indeed, it is in
the construction itself where diﬀerent trees diﬀer. Thus, there are virtually
inﬁnitely many possible strategies to build classiﬁcation trees. Nevertheless,
despite this great diversity, all tree species end up with two fundamental
questions at each node:
x Should the node be split?
y In the aﬃrmative, what are its children?
These two questions are typically answered using global information regarding the tree, such as, for example, a function of the data Dn, the level of the
node within the tree, the size of the data set and, more generally, any parameter connected with the structure of the tree. This parameter could be,
for example, the total number k of cells in a k-partition tree or the penalty
term in the pruning of the CART algorithm .
Cellular trees proceed from a diﬀerent philosophy. In short, a cellular tree
should, at each node, be able to answer questions x and y using local
information only, without any help from the other nodes. In other words,
each cell can perform as many operations it wishes, provided it uses only
the data that are transmitted to it, regardless of the general structure of the
tree. Just imagine that the calculations to be carried out at the nodes are
sent to diﬀerent computers, eventually asynchronously, and that the system
architecture is so complex that computers do not communicate.
situation may arise, for example, in the context of massive data sets, that
is, when both n and d are astronomical, and no single human and no single
computer can handle this alone. Thus, once a computer receives its data,
it has to make its own decisions x and y based on this data subset only,
independently of the others and without knowing anything of the overall
ediﬁce. Once a data set is split, it can be given to another computer for
further splitting, since the remaining data points have no inﬂuence. This
greedy mechanism is schematized in Figure 2.
But there is a more compelling reason for making local decisions. A neurologist seeing twenty patients must make decisions without knowing anything
about the other patients in the hospital that were sent to other specialists.
Neither does he need to know how many other patients there are. The neurologist’s decision, in other words, should only be based on the data—the
patients—in his care.
Decision tree learning is a method commonly used in data mining . Its goal is to create a model that partitions the
Majority vote
Cellular decision
Figure 2: Schematization of the cell, the computational unit.
space recursively, as in a tree, in which leaf nodes (terminal nodes) correspond
to ﬁnal decisions. This process of top-down induction of decision trees—a
phrase introduced by Quinlan in 1968—is called greedy in the data mining
and computer science literature. It is by far the most common strategy for
learning decision trees from data.
The literature on this topic is largely
concerned with the manner in which splits are made, and with the stopping
For example, in CART , splits are made perpendicular
to the axes based on the notion of Gini impurity. Splits are performed until
all data are isolated.
In a second phase, nodes are recombined from the
bottom-up in a process called pruning. It is this second process that makes
the CART trees non-cellular, as global information is shared to manage the
recombination process. Quinlan’s C4.5 also prunes. Others split until
all nodes or cells are homogeneous (i.e., have the same class)—the prime
example is Quinlan’s ID3 . This strategy, while compliant with the
cellular framework, leads to non-consistent rules, as we point out in the
present paper. In fact, the choice of a good stopping rule for decision trees
is very hard—we were not able to ﬁnd any in the literature that guarantee
convergence to the Bayes error.
We note here that decision networks have received renewed attention in wireless sensor networks . Phys-
ical and energy considerations impose a natural restriction on the classiﬁers—
decisions must be taken locally. This corresponds, in spirit, to the cellular
framework we are proposing. However, most sensor network decision trees
use global criteria such as pruning that are based on a global method of deciding where to prune. The consistency question has not been addressed in
these applications.
Cellular tree classiﬁers
A mathematical model
The objective of this subsection is to discuss a tentative mathematical model
for cellular tree classiﬁers. Without loss of generality, we consider binary tree
classiﬁers based on a class C of possible Borel subsets of Rd that can be used
for splits. A typical example of such a class is the family of all hyperplanes,
or the class of all hyperplanes that are perpendicular to one of the axes.
Higher order polynomial splitting surfaces can be imagined as well.
The class is parametrized by a vector σ ∈Rp. There is a splitting function
f(x, σ), x ∈Rd, σ ∈Rp, such that Rd is partitioned into A = {x ∈Rd :
f(x, σ) ≥0} and B = {x ∈Rd : f(x, σ) < 0}. Formally, a cellular split can
be viewed as a family of measurable mappings σ from (Rd × {0, 1})n to Rp
(for all n ≥1). That is, for each possible input size n, we have a map. In
addition, there is a family of measurable mappings θ from (Rd × {0, 1})n to
{0, 1} that indicate decisions: θ = 1 indicates that a split should be applied,
while θ = 0 corresponds to a decision not to split. In that case, the cell acts
as a leaf node in the tree. Note that θ and σ correspond to the decisions
given in x and y.
A cellular binary classiﬁcation tree is a machine that partitions the space
recursively in the following manner. With each node we associate a subset of
Rd, starting with Rd for the root node. Let the data set be Dn. If θ(Dn) = 0,
the root cell is ﬁnal, and the space is not split. Otherwise, Rd is split into
x ∈Rd : f (x, σ(Dn)) ≥0
x ∈Rd : f (x, σ(Dn)) < 0
The data Dn are partitioned into two groups—the ﬁrst group contains all
(Xi, Yi), i = 1, . . . , n, for which Xi ∈A, and the second group all others.
The groups are sent to child cells, and the process is repeated.
A priori, there is no reason why this tree should be ﬁnite. We will impose
conditions later on that ensure that with probability 1, the tree is ﬁnite for
all n and for all possible values of the data.
For example, this could be
achieved by hyperplane splits perpendicular to the axes that are forced to
visit (contain) one of the Xi’s.
By insisting that the data point selected
on the boundary be “eaten”, i.e., not sent down to the child nodes, one
reduces the data set by one at each split, thereby ensuring the ﬁniteness of
the decision tree. We will employ such a (crude) method.
When x ∈Rd needs to be classiﬁed, we ﬁrst determine the unique leaf set
A(x) to which x belongs, and then take votes among the {Yi : Xi ∈A(x), i =
1, . . . , n}. Classiﬁcation proceeds by a majority vote, with the majority deciding the estimate gn(x). In case of a tie, we set gn(x) = 0.
A cellular binary tree classiﬁer is said to be randomized if each node in the
tree has an independent copy of a uniform random variable associated
with it, and θ and σ are mappings that have one extra real-valued component
in the input. For example, we could ﬂip an unbiased coin at each node to
decide whether θ = 0 or θ = 1.
Remark 2.1 It is tempting to say that any classiﬁer gn is a cellular tree
classiﬁer with the following mechanism: Set θ = 1 if we are at the root, and
θ = 0 elsewhere. The root node is split by the classiﬁer into a set
A = {x ∈Rd : gn(x) = 1}
and its complement, and both child nodes are leaves. However, the decision
to cut can only be a function of the input data, and not the node’s position
in the tree, and thus, this is not allowed.
Are there consistent cellular tree classiﬁers?
At ﬁrst sight, it appears that there are no universally consistent cellular tree
classiﬁers.
Consider for example complete binary trees with k full levels,
i.e., there are 2k leaf regions. We can have consistency when k is allowed to
depend upon n. An example is the median tree . When d = 1, split by ﬁnding the median element among the Xi’s, so
that the child sets have cardinality given by ⌊(n−1)/2⌋and ⌈(n−1)/2⌉, where
⌊.⌋and ⌈.⌉are the ﬂoor and ceiling functions. The median itself does stay
behind and is not sent down to the subtrees, with an appropriate convention
for breaking cell boundaries as well as empty cells. Keep doing this for k
rounds—in d dimensions, one can either rotate through the coordinates for
median splitting, or randomize by selecting uniformly at random a coordinate
to split orthogonally.
This rule is known to be consistent as soon as the marginal distributions of
X are nonatomic, provided k →∞and k2k/n →0. However, this is not
a cellular tree classiﬁer. While we can indeed specify σ, it is impossible to
deﬁne θ because θ cannot be a function of the global value of n. In other
words, if we were to apply median splitting and decide to split for a ﬁxed
k, then the leaf nodes would all correspond to a ﬁx proportion of the data
points. It is clear that the decisions in the leaves are oﬀwith a fair probability
if we have, for example, Y independent of X and P{Y = 1} = 1/2. Thus,
we cannot create a cellular tree classiﬁer in this manner.
In view of the preceding discussion, it seems paradoxical that there indeed exist universally consistent cellular tree classiﬁers. (We note here that we abuse
the word “universal”—we will assume throughout, to keep the discussion at
a manageable level, that the marginal distributions of X are nonatomic. But
no other conditions on the joint distribution of (X, Y ) are imposed.) Our
ﬁrst construction, which is presented in Section 3, follows the median tree
principle and uses randomization. In a second construction (Section 4) we
derandomize, and exploit the idea that each cell is allowed to explore its own
subtrees, thereby anticipating the decisions of its children. For the sake of
clarity, proofs of the most technical results are gathered in Section 5 and
Section 6.
A randomized cellular tree classiﬁer
From now on, to keep things simple, it is assumed that the marginal distributions of X are nonatomic. The cellular splitting method σ described
in this section mimics the median tree classiﬁer discussed above. We ﬁrst
choose a dimension to cut, uniformly at random from the d dimensions, as
rotating through the dimensions by level number would violate the cellular
condition. The selected dimension is then split at the data median, just as
in the classical median tree. Repeating this for k levels of nodes leads to 2k
leaf regions. On any path of length k to one of the 2k leaves, we have a deterministic sequence of cardinalities n0 = n(root), n1, n2, . . . , nk. We always
have ni/2 −1 ≤ni+1 ≤ni/2. Thus, by induction, one easily shows that, for
2i −2 ≤ni ≤n
In particular, each leaf has at least max(n/2k−2, 0) points and at most n/2k.
Remark 3.1 The problem of atoms in the coordinates can be dealt with separately, but still within the cellular framework. The particularity is that the
threshold for splitting may now be at a position at which one or more data
values occur. This leaves two sets that may diﬀer in size by more than one.
The atoms in the distribution of X can never be separated, but that is as it
should be. We leave it to the reader to adapt the subsequent arguments to the
case of atomic distributions.
The novelty is in the choice of the decision function. This function ignores
the data altogether and uses a randomized decision that is based on the size
of the input. More precisely, consider a nonincreasing function ϕ : N →(0, 1]
with ϕ(0) = ϕ(1) = 1. Cells correspond in a natural way to sets of Rd. So,
we can and will speak of a cell A, where A ⊂Rd. The number of data points
in A is denoted by N(A):
Then, if U is the uniform random variable associated with the cell A
and the input to the cell is N(A), the stopping rule x takes the form:
x Put θ = 0 if
U ≤ϕ (N(A)) .
In this manner, we obtain a possibly inﬁnite randomized binary tree classiﬁer.
Splitting occurs with probability 1 −ϕ(n) on inputs of size n. Note that no
attempt is made to split empty sets or singleton sets. For consistency, we
need to look at the random leaf region to which X belongs. This is roughly
equivalent to studying the distance from that cell to the root of the tree.
In the sequel, the notation un = o(vn) (respectively, un = ω(vn) and un =
O(vn)) means that un/vn →0 (respectively, vn/un →0 and un ≤Cvn for
some constant C) as n →∞. Many choices ϕ(n) = o(1), but not all, will do
for us. The next lemma makes things more precise.
Lemma 3.1 Let β ∈(0, 1). Deﬁne
Let K(X) denote the random path distance between the cell of X and the root
of the tree. Then
n→∞P {K(X) ≥kn} =
if kn = ω(logβ n)
if kn = o(logβ n).
Proof of Lemma 3.1
Let us recall that, at level k, each cell of the underlying median tree contains at least max(n/2k −2, 0) points and at most
n/2k. Since the function ϕ(.) is nonincreasing, the ﬁrst result follows from
P {K(X) ≥kn} ≤
≤exp (−knϕ(n)) .
The second statement follows from
P {K(X) < kn} ≤
 ⌈n/2i −2⌉
valid for all n large enough since n/2kn →∞as n →∞.
Lemma 3.1, combined with the median tree consistency result of Devroye
et al. , suﬃces to establish consistency of the randomized cellular tree
classiﬁer.
Theorem 3.1 Let β be a real number in (0, 1). Deﬁne
Let gn be the associated randomized cellular binary tree classiﬁer. Assume
that the marginal distributions of X are nonatomic. Then the classiﬁcation
rule gn is consistent:
n→∞EL(gn) = L⋆
Proof of Theorem 3.1
By diam(A) we mean the diameter of the cell
A, i.e., the maximal distance between two points of A. We recall a general
consistency theorem for partitioning classiﬁers whose cell design depends on
the Xi’s only . According to this theorem,
such a classiﬁer is consistent if both
1. diam(A(X)) →0
in probability as n →∞, and
2. N(A(X)) →∞
in probability as n →∞,
where A(X) is the cell of the random partition containing X.
Condition 2. is proved in Lemma 3.1. Notice that
N (A(X)) ≥
≥1[K(X)<log(β+1)/2 n]
2log(β+1)/2 n −2
= ω(1)1[K(X)<log(β+1)/2 n].
Therefore, by Lemma 3.1, N (A(X)) →∞in probability as n →∞.
To show that diam(A(X)) →0 in probability, observe that on a path of
length K(X), the number of times the ﬁrst dimension is cut is binomial
(K(X), 1/d). This tends to inﬁnity in probability. Following the proof of
Theorem 20.2 in Devroye et al. , the diameter of the cell of X tends
to 0 in probability with n. Details are left to the reader.
Let us ﬁnally take care of the randomization. Can one do without randomization? The hint to the solution of that enigma is in the hypothesis that the
data elements in Dn are i.i.d. The median classiﬁer does not use the ordering
in the data. Thus, one can use the randomness present in the permutation of
the observations, e.g., the ℓ-th components of the Xi’s can form n! permutations if ties do not occur. This corresponds to (1 + o(1))n log2 n independent
fair coin ﬂips, which are at our disposal. Each decision to split requires on
average at most 2 independent bits. The selection of a random direction to
cut requires no more than 1 + log2 d independent bits. Since the total tree
size is, with probability tending to 1, O(2logβ+ε n) for any ε > 0, a fact that
follows with a bit of work from summing the expected number of nodes at
each level, the total number of bits required to carry out all computations is
(3 + log2 d)2logβ+ε n
which is orders of magnitude smaller than n provided that β + ε < 1. Thus,
there is suﬃcient randomness at hand to do the job. How it is actually implemented is another matter, as there is some inevitable dependence between
the data sets that correspond to cells and the data sets that correspond to
their children. We will not worry about the ﬁner details of this in the present
Remark 3.2 For more on random tree models and their analyses, see the
texts of Drmota , and Flajolet and Sedgewick . Additional material on information-theory and bit complexity can be found in the monograph
by Cover and Thomas .
Remark 3.3 In the spirit of Breiman’s random forests , one could
envisage to use a collection of randomized cellular tree classiﬁers and make
ﬁnal predictions by aggregating over the ensemble. Since each individual rule
is consistent (by Theorem 3.1), then the same property is also true for the
ensemble . Improvements are
expected at the level of predictive accuracy and stability.
A non-randomized cellular tree classiﬁer
The cellular tree classiﬁer that we consider in this section is more sophisticated and autonomous, in the sense that it does not rely on any randomization scheme. It partitions the data recursively as follows. With each node we
associate a set of Rd, starting with Rd for the root node. We ﬁrst consider
a full 2d-ary tree (see Figure 3 for an illustration in dimension 2), with the
cuts decided in the following manner. The dimensions are ordered once and
for all from 1 to d. At the root, we ﬁnd the median of (the projection of)
the n data points in direction 1, then on each of the two subsets, we ﬁnd the
median in direction 2, then on each of the four subsets, we ﬁnd the median
in direction 3, and so forth. A split, contrary to our discussion thus far, is
into 2d parts, not two parts. This corresponds to Bentley’s k-d tree .
Repeating this splitting for k levels of nodes leads to 2dk leaf regions, each
having at least max(n/2dk −2, 0) points and at most n/2dk.
Figure 3: A full 2d-ary tree in dimension d = 2.
This procedure is equivalent to dk consecutive binary splits at the median,
where we rotate through the dimensions. However, in our cellular set-up,
such rotations through the dimensions are impossible, and this forces us to
employ this equivalent strategy. Note, therefore, that the split parameter σ is
an extension of the binary classiﬁer split σ—one could consider it as a vector
of dimension 2d −1, as we need to specify 2d −1 coordinate positions to fully
specify a partition into 2d regions. It remains to specify a stopping rule θ
which respects the cellular constraint. To this aim, we need some additional
Remark 4.1 By the very construction of the tree, at each node, the median
itself does stay behind and is not sent down to the subtrees. From a topological
point of view, this means that, in the partition building, each cell A and its
2d child cells A1, . . . , A2d are considered as open hyperrectangles. Thus, for
classiﬁcation, assuming nonatomic marginals, we would thus strictly speaking
not be able to classify any data that fall “on the border” between A1, . . . , A2d.
This is a non-important detail for the calculations since the marginal distributions of X are nonatomic. In practice, this issue can be solved with an
appropriate convention to break the boundary ties.
If A is any cell of the full 2d-ary tree deﬁned above, we let N(A) be the
number of Xi’s falling in A, and estimate the quality of the majority vote
classiﬁer at this node by
1[Xi∈A,Yi=1],
1[Xi∈A,Yi=0]
(Throughout, we adopt the convention 0/0 = 0.)
Remark 4.2 Each cut at the median eliminates 1 data point. Thus, given a
cell A, the construction of its oﬀspring k generations later rules out at most
1+· · ·+2dk−1 = 2dk−1 observations. In particular, if A has cardinality N(A),
then, k generations later, its oﬀspring A1, . . . , A2dk have a total combined
cardinality at least N(A) −(2dk + 1).
Fix a positive real parameter α and deﬁne the nonnegative integer k+ by
k+ = ⌊α log2(N(A) + 1)⌋,
where, for simplicity, we drop the dependency of k+ upon A and α. Finally,
letting Pk+(A) be the 2dk+ leaf regions (terminal nodes) of the full 2d-ary
tree rooted at A of height k+, we set
ˆLn(A, k+) =
ˆLn(Aj)N(Aj)
The quantity ˆLn(A, k+) is interpreted as the total (normalized) error of a
majority vote over the oﬀspring of A living k+ generations later. It should
be stressed that both ˆLn(A) and ˆLn(A, k+) may be evaluated on the basis
of the data points falling in A only (no matter what the rest of the tree looks
like), thereby respecting the cellular constraint.
Now, let β be a positive real parameter. With this notation, the stopping
rule x takes the following simple form:
x Put θ = 0 if
ˆLn(A) −ˆLn(A, k+)
In other words, at each cell, the algorithm compares the actual classiﬁcation error with the total error of the cell oﬀspring k+ generations later.
This bounded lookahead principle suggested by us is quite well-developed
in the artiﬁcial intelligence literature—see, for example, Pearl’s book 
on probabilistic reasoning. If the diﬀerence is below some well-chosen threshold, then the cellular classiﬁcation procedure stops and the node returns a
terminal signal. Otherwise, the node outputs 2d sets of data, and the process continues recursively. The protocol stops once all nodes have returned
a terminal signal, and ﬁnal decisions are taken by majority vote. Thus, for
x falling in a terminal node A, the rule is as usual
i=1 1[Xi∈A,Yi=1] > Pn
i=1 1[Xi∈A,Yi=0]
otherwise.
In the next section, we prove the following theorem:
Theorem 4.1 Let gn be the cellular tree classiﬁer deﬁned above, with 1 −
dα −2β > 0. Assume that the marginal distributions of X are nonatomic.
Then the classiﬁcation rule gn is consistent:
n→∞EL(gn) = L⋆
From a technical point of view, this theorem poses a challenge, as there are
no conditions on the distribution, and the rectangular cells do in general
not shrink to zero. In fact, it is easy to ﬁnd distributions of X for which
the maximal cell diameter does not tend to zero in probability, even if all
is restricted to the unit cube. For distributions with inﬁnite support, there
are always cells of inﬁnite diameter. This observation implies that classical
consistency proofs, that often use diﬀerentiation of measure arguments or rely
on asymptotic justiﬁcations related to Lebesgue’s density theorem, cannot
be applied. The proof uses global arguments instead.
For partitions that do not depend upon the Y -values in the data, consistency can be shown by relatively simple means, following for example the
arguments given in Devroye et al. . However, our partition and tree
depend upon the Y -values in the data. Within the constraints imposed by
the cellular model, we believe that this is the ﬁrst (and only) proof of universal consistency of a Y -dependent cellular tree classiﬁer. On the other hand,
we have proposed a model that is a priori too simple to be competitive. There
are choices of parameters to be made, and there is absolutely no minimax
theory of lower bounds for the rate with which cellular tree classiﬁers can
approach the Bayes error. On the practical side, besides the question of how
to eﬃciently implement the model, it is also clear that the performance of
the cellular estimate will be conditional on a good tuning of both parameters
α and β. As a ﬁrst step, a good route to follow is to attack the rate of convergence problem—we expect dependence on the smoothness of (X, Y )—and
deduce from this analysis the best parameter choices. In any case, the work
ahead is enormous and the road arduous.
Proof of Theorem 4.1
Notation and preliminary results
We start with some notation (see Figure 4). For each level k ≥0, we denote
by Pk the partition represented by the leaves of the underlying full 2d-ary
median-type tree. This partition has 2dk cells and its construction depends
on the Xi’s only. The labels Yi’s do not play a role in the building of Pk,
though they are involved in making the decision whether to cut a cell or not.
For each Aj ∈Pk, we let N(Aj) be the number of Xi’s falling in Aj and note
j=1 N(Aj) ≤n, with a strict inequality as soon as k > 0 (see Remark
4.2). For each level k, Ak(X) denotes the cell of the partition Pk into which
X falls, and N(Ak(X)) the number of data points falling in this set.
We let µ be the distribution of X and η the regression function of Y on X.
More precisely, for any Borel-measurable set A ⊂Rd,
µ(A) = P{X ∈A}
and, for any x ∈Rd,
η(x) = P{Y = 1|X = x} = E[Y |X = x].
Figure 4: Some key notation.
It is known that the Bayes error is
Rd min (η(z), 1 −η(z)) µ(dz).
Let us recall that, for any cell A,
1[Xi∈A,Yi=1],
1[Xi∈A,Yi=0]
Also, for every k ≥0,
ˆLn(A, k) =
ˆLn(Aj)N(Aj)
where Pk(A) is the full 2d-ary median-type tree rooted at A of height k. At
the population level, we set
η(z)µ(dz),
(1 −η(z)) µ(dz)
L⋆(A, k) =
L⋆(Aj)µ(Aj)
For all k ≥0, we shall also need the quantity
k = E [L⋆(Ak(X))] .
Note that whenever A = A(X1, . . . , Xn) is a random cell, we take the liberty
to abbreviate
A dµ by µ(A) throughout the manuscript, since this should
cause no confusion. We write for instance
E [L⋆(Ak(X)) | X1, . . . , Xn]
instead of
Our proof starts with some easy but important facts.
(i) For all levels k′ ≥k ≥0,
(ii) For each cell A and each level k ≥0,
ˆLn(A, k) ≤ˆLn(A) +
N(A)1[N(A)>0].
(iii) For each cell A and all levels k′ ≥k ≥0,
ˆLn(A, k′) ≤ˆLn(A, k) + 2dk′
N(A)1[N(A)>0].
(iv) For each cell A and all levels k, k′ ≥0,
E [L⋆(Ak(X), k′)] = L⋆
In particular, for k′′ ≥k′ ≥0,
L⋆≤E [L⋆(Ak(X), k′′)] ≤E [L⋆(Ak(X), k′)] .
Proof of statement (i) is based on the nesting of the partitions. To
establish (ii), observe that, by deﬁnition,
ˆLn(A) = 1
1[Xi∈A,Yi=1]
1[Xi∈Aj,Yi=1]
1[Xi∈Aj,Yi=1]
But, by the triangle inequality and Remark 4.2,
1[Xi∈A,Yi=1]
1[Xi∈Aj,Yi=1]
This proves (ii). Proof of (iii) is similar. To show (iv), just note that
E [L⋆(Ak(X), k′)] = E
L⋆(Aj)µ(Aj)
= E [L⋆(Ak+k′(X))]
The next two propositions will be decisive in our analysis. Proposition 5.1
asserts that the diameter of Ak(X) tends to 0 in probability, provided k (as a
function of n) tends suﬃciently slowly to inﬁnity. Proposition 5.2 introduces
a particular level k⋆
n which will play a central role in the proof of Theorem
Proposition 5.1 Assume that the marginal distributions of X are nonatomic. Then, if
diam (Ak(X)) →0
in probability as n →∞.
Proof of Proposition 5.1
Median-split trees are analyzed in some detail
in Section 20.3 of the monograph by Devroye et al. . Starting on page
323, it is shown that the diameter of a randomly selected cell tends to 0
in probability. The adaptation to our 2d-ary median-type trees is straightforward. However, a few remarks are in order. Section 20.3 of that book
assumes that all marginals are uniform. This can also be the set-up for us,
because our rule is invariant under monotone transformations of the axes.
Note however that it is crucial that splits are made exactly at data points for
this property to be true. Also, the proofs in Section 20.3 of Devroye et al.
 assume d = 2, but are clearly true for general d. The only condition
for the diameter result is that of Theorem 20.2, page 323:
The second condition is only necessary to make sure that the data medians
do not run too far away from the true distributional medians.
Proposition 5.2 Let ψ(n, k) be the function deﬁned for all n ≥1 and k ≥0
ψ(n, k) = L⋆
(i) Let {kn}n≥1 be a sequence of nonnegative integers such that kn →∞
and kn2dkn/n →0. Then
ψ(n, kn) →0
(ii) Assume that α ∈(0, 1/d) and, for ﬁxed n, set
ℓ≥0 : ψ(n, ℓ) <
Proof of Proposition 5.2
At ﬁrst we note, according to Fact 5.1(ii),
that for all n ≥1 and k ≥0, ψ(n, k) ≥0. For x ∈Rd, introduce
µ (Akn(x))
η(z)µ(dz).
With this notation,
ψ(n, k) = E [L⋆(Akn(X))] −L⋆
≤E |η(X) −¯ηn(X)| + E |(1 −η(X)) −(1 −¯ηn(X))| .
Let us prove that the ﬁrst of the two terms above tends to 0 as n tends to
inﬁnity—the second term is handled similarly. To this aim, ﬁx ε > 0 and
ﬁnd a uniformly continuous function ηε on a bounded set C and vanishing oﬀ
C so that E|η(X) −ηε(X)| < ε. Clearly, by the triangle inequality,
E |η(X) −¯ηn(X)| ≤E |η(X) −ηε(X)|
+ E |ηε(X) −¯ηn,ε(X)|
+ E |¯ηn,ε(X) −¯ηn(X)|
= I + II + III,
¯ηn,ε(x) =
µ (Akn(x))
ηε(z)µ(dz).
By choice of ηε, one has I < ε. Next, note that
|ηε(X) −ηε(z)| µ(dz)
µ (Akn(X))
As ηε is uniformly continuous, there exists a number δ = δ(ε) > 0 such that
if diam(A) ≤δ, then |ηε(x)−ηε(z)| < ε for every x, z ∈A. In addition, there
is a positive constant M such that |ηε(x)| ≤M for every x ∈Rd. Thus,
II < ε + 2M P {diam (Akn(X)) > δ} .
Therefore, II < 2ε for all n large enough by Proposition 5.1.
III ≤I < ε. Taken together, these steps prove the ﬁrst statement of the
proposition.
Next, suppose assertion (ii) is false and set, to simplify notation, δ = 1−dα >
0. Then we can ﬁnd a subsequence {k⋆
ni}i≥1 of {k⋆
n}n≥1 and a positive constant
C such that, for all i,
Since ni →∞, it can be assumed, without loss of generality, that ni ≥2 and
log2(Cni) ≥2d for all i. This implies in particular
ni −1 ≥log2(Cni)
≥log2(Cni)
ni ≥2 as well.
On the one hand, by the very deﬁnition of k⋆
2d(k⋆ni−1)
On the other hand, by (5.1) and the monotonicity of ψ(ni, .) (Fact 5.1(ii)),
we may write
ni, log2(Cni)
But, setting
tni = log2(Cni)
= log2(Cni)
This quantity goes to 0 as ni →∞. Moreover, tni →∞and thus, according
to the ﬁrst statement of the proposition,
This contradicts (5.2).
Proof of the theorem
n}n≥1 be deﬁned as in Proposition 5.2.
We denote by Gn the leaf
regions of the cellular tree, and by G−
k⋆n (respectively, G+
k⋆n) the collection of
leaves at level at most (respectively, strictly at least) k⋆
n. Finally, for any cell
Ln(A) = P{gn(X) ̸= Y, X ∈A | Dn}.
With this notation, we have
L⋆≤EL(gn) = E
Then, clearly,
1[|ˆLn(A)−ˆLn(A,k+)|>ϕ(A)]µ(A)
−ˆLn(Ak⋆n(X), k+)
In the second inequality, we used the deﬁnition of the stopping rule of the
cellular tree. Therefore, according to technical Lemma 6.5,
Since 1−dα−2β > 0, this term tends to 0 as n →∞by the second statement
of Proposition 5.2. Next, introduce the notation
1[Xi∈A,Yi=0]
1[Xi∈A,Yi=1],
and observe that
1[N0(A)≥N1(A)]
+ 1[N0(A)<N1(A)]
(1 −η(z)) µ(dz)
For x falling in the region covered by G−
k⋆n, denote by A−
k⋆n(x) the cell of G−
containing x, and set
k⋆n(x),Yi=1],
we may write
ˆLn(A)µ(A)
1[N0(A)≥N1(A)]
η(z)µ(dz) −
ˆηn(z)µ(dz)
1[N0(A)<N1(A)]
(1 −η(z)) µ(dz) −
(1 −ˆηn(z)) µ(dz)
It follows, evoking Lemma 6.6, that
ˆLn(A)µ(A)
The rightmost term tends to 0 according to the second statement of Proposition 5.2.
Thus, to complete the proof, it remains to establish that
ˆLn(A)µ(A)
To this aim, observe that by the very deﬁnition of G−
k⋆n, we have
ˆLn(A)µ(A)
ˆLn(A, k+) + ϕ(A)
ˆLn(A, k+)µ(A)
For every cell A of G−
k⋆n, one has
2dk⋆n −1, 1
≤N(A) + 1 ≤
2dk⋆n + 1.
Therefore, taking n so large that n/2dk⋆
n > 2 (this is possible by Proposition
5.2(ii)), we obtain
Applying Proposition 5.2(ii) again, we conclude that II →0 as n →∞.
Next, deﬁne
Inequality (5.3) implies that for every A ∈G−
k⋆n and all n large enough,
kn ≤k+ ≤k′
Thus, by Fact 5.1(iii),
ˆLn (A, kn) µ(A)
ˆLn (A, kn) µ(A)
On the other hand,
ˆLn (A, kn) µ(A)
L⋆(A, kn)µ(A)
ˆLn(A, kn) −L⋆(A, kn)
L⋆(A, kn)µ(A)
(by Lemma 6.4).
Consequently,
L⋆(A, kn)µ(A)
and the rightmost term tends to 0 as n →∞by Proposition 5.2(ii). Thus,
the proof will be ﬁnalized if we show that
L⋆(A, kn)µ(A)
L⋆(A, kn)µ(A)
L⋆(Aj)µ(Aj)
where, in the inequality, we use the fact that the cells in the double sum are
at level at least kn. But, clearly,
and consequently, since dα < 1,
Thus, by Proposition 5.2(i), the term L⋆
kn tends to L⋆. This concludes the
Some technical results
Throughout this section, we adopt the general notation of the document. In
particular, we let α and β be two positive real numbers such that 1−dα−2β >
0. The sequence {k⋆
n}n≥1 is deﬁned as in Proposition 5.2 and we set
k+ = ⌊α log2(N(A) + 1)⌋.
We will repeatedly use the fact that, by Proposition 5.2(ii), 2dk⋆
n →∞. For any k ≥0, Tk stands for the full 2d-ary median-type tree with
k levels of nodes, whose leaves represent Pk.
Recall that X has probability measure µ on Rd and that its marginals are
assumed to be nonatomic. The ﬁrst important result that is needed here is
the following one:
Proposition 6.1 Let {kn}n≥1 be a sequence of nonnegative integers such
that 2dkn/n →0. Then
Proof of Proposition 6.1
In the sequel, we let n be large enough to
ensure that n/2dkn > 2, so that we do not have to worry about empty cells.
To prove the lemma, recall the construction of Tkn.
At the root, which
represents Rd, we order the points by the ﬁrst component. We deﬁne the pivot
as the r-th smallest point, where r = ⌊(n+1)/2⌋, and cut perpendicularly to
the ﬁrst component at the pivot. Let the pivot’s ﬁrst component have value
x ∈Rd : x = (x1, · · · , xd), x1 < x⋆
x ∈Rd : x = (x1, · · · , xd), x1 > x⋆
The sample points that fall in A, conditionally on the pivot, are distributed
according to µ restricted to A, and similarly for B. Also, importantly,
L= Beta(r, n −r + 1)
L= Beta(n −r + 1, r),
from the theory of order statistics .
We need to see how large µ(A), µ(B), N(A) and N(B) are. To this aim, we
distinguish between the cases where n is odd and n is even.
Now r = (n + 1)/2, N(A) = r −1 = (n −1)/2, N(B) =
n −r = (n −1)/2, and
2. n even.
In this case we have r = n/2, N(A) = (n−2)/2, N(B) = n/2,
As N(A) + N(B) = n −1, the pivot is not sent down to the subtrees. Let us
have a canonical way of deciding who goes left and right, e.g., A is left and
B is right. Next, still at the root, we rotate the coordinate and repeat the
median splitting process for the sample points in A and B (both open sets)
in direction 2, then in direction 3, and so forth until direction d. We create
this way the 2d children of the root and, repeating this scheme for kn levels
of nodes, we construct the 2d-ary tree up to distance kn from the root. It has
exactly 2dkn leaves.
On any path of length kn to one of the 2dkn leaves, we have a deterministic
sequence of cardinalities
n0 = n(root), n1, n2, . . . , nkn.
We have already seen that, for all i = 0, . . . , kn,
2di −2 ≤ni ≤n
Now, consider a ﬁxed path to a ﬁxed leaf, (n0, n1, . . . , nkn). Then, conditionally on the pivots, the set of Rd that corresponds to that leaf, i.e., a
hyperrectangle of Rd, has µ-measure distributed as
Beta(n1 + 1, n0 −n1) × · · · × Beta(nkn + 1, nkn−1 −nkn) def
= Z1 × · · · × Zkn
Observe that
ni−1 + 1 = nkn + 1
(ni + 1)(ni + 2)
(ni−1 + 1)(ni−1 + 2) = (nkn + 1)(nkn + 2)
(n + 1)(n + 2)
The objective is to bound
E |Z −EZ|2 +
n −nkn + 1
where the symbol V stands for the variance. Note
n −nkn + 1
 nkn + 2
n + 2 −nkn + 1
(n + 2)(n + 1)
(n + 1)(n + 2).
(n + 1)(n + 2) +
Sum over all 2dkn sets in the partition Pkn, and call the set cardinalities
nkn(1), . . . , nkn(2dkn). Then, denoting by Zi the “Z” for the i-th set in the
partition, we obtain
nkn(i) + 2
(nkn(i) + 2)
(by the Cauchy-Schwarz inequality).
Therefore,
n + 2dkn+1
Since 2dkn/n →0 as n →∞, this last term is O(
Corollary 6.1 Let {kn}n≥1 be a sequence of nonnegative integers such that
2dkn/n →0, and let P−
kn be the partition of Rd corresponding to the leaves of
any subtree of Tkn rooted at Rd. Then
Proof of Corollary 6.1
The proof is similar to the proof of Proposition
6.1—just note that P−
kn has at most 2dkn cells.
Proposition 6.2 Let {kn}n≥1 be a sequence of nonnegative integers such
that 2dkn/n →0. Then
N (Akn(X))
1[Xi∈Akn(X),Yi=1] −
µ (Akn(X))
and, similarly,
N (Akn(X))
1[Xi∈Akn(X),Yi=0] −
µ (Akn(X))
(1 −η(z)) µ(dz)
Proof of Proposition 6.2
We only prove the ﬁrst statement.
n/2dkn →∞as n →∞, we can always choose n large enough so that no
cell of Pkn is empty. A quick check of Tkn reveals that given the pivots (see
Proposition 6.1), the points inside each cell are distributed in an i.i.d. manner
according to the restriction of µ to the cell. Moreover, conditionally on X
and the pivots, N(Akn(X)) has a deterministic, ﬁxed value. Thus, setting
µ (Akn(x))
η(z)µ(dz),
we obtain, conditionally on X and the pivots,
N (Akn(X))
1[Xi∈Akn(X),Yi=1] −
µ (Akn(X))
¯ηn(X) (1 −¯ηn(X))
N (Akn(X))
N (Akn(X))
The result follows from the condition 2dkn/n →0.
Corollary 6.2 Let {kn}n≥1 be a sequence of nonnegative integers such that
2dkn/n →0 , and let P−
kn be the partition of Rd corresponding to the leaves
of any subtree of Tkn rooted at Rd. For each x ∈Rd, denote by A−
cell of P−
kn containing x. Then
kn(X),Yi=1] −
and, similarly,
kn(X),Yi=0] −
(1 −η(z)) µ(dz)
Proof of Corollary 6.2
The proof is similar to that of Proposition 6.2—
just note that
for all n large enough.
Lemma 6.1 Let {kn}n≥1 be a sequence of nonnegative integers such that
2dkn/n →0. Then
ˆLn (Akn(X)) −L⋆(Akn(X))
Proof of Lemma 6.1
Using the deﬁnition of ˆLn(Akn(X)) and L⋆(Akn(X)),
we may write
ˆLn (Akn(X)) −L⋆(Akn(X))
N (Akn(X))
1[Xi∈Akn(X),Yi=1] −
µ (Akn(X))
N (Akn(X))
1[Xi∈Akn(X),Yi=0]
µ (Akn(X))
(1 −η(z)) µ(dz)
Each term of the sum goes to 0 by Proposition 6.2.
Lemma 6.2 Let {kn}n≥1 be a sequence of nonnegative integers such that
2dkn/n →0, and let P−
kn be the partition of Rd corresponding to the leaves of
any subtree of Tkn rooted at Rd. For each x ∈Rd, denote by A−
kn(x) the cell
kn containing x. Then
Proof of Lemma 6.2
The proof is similar to that of Lemma 6.1. It uses
Corollary 6.2 instead of Proposition 6.2.
Lemma 6.3 Let
ˆLn(Ak⋆n(X), kn) −L⋆(Ak⋆n(X), kn)
Proof of Lemma 6.3
ˆLn(Ak⋆n(X), kn) −L⋆(Ak⋆n(X), kn)
ˆLn(Aj)N(Aj)
N(A) −L⋆(Aj)µ(Aj)
ˆLn(Aj)N(Aj)
N(A) −ˆLn(Aj)µ(Aj)
ˆLn(Aj) −L⋆(Aj)
 Ak⋆n+kn(X)
−L⋆ Ak⋆n+kn(X)
whence, according to Lemma 6.1,
On the other hand, since ˆLn(Aj) ≤1,
N(A) −µ(Aj)
N(A) µ(A) −N(Aj)
The inequality
N(Aj) ≤N(A)
µ(A) −N(A)
µ(A) −N(A)
Thus, by Proposition 6.1,
Collecting bounds, we obtain
I + II = O
Lemma 6.4 Let G−
k⋆n be the collection of cells of Gn at level at most k⋆
ˆLn(A, kn) −L⋆(A, kn)
Proof of Lemma 6.4
Denote by ¯G−
k⋆n the cells of Pk⋆n such that the path
from the root to the cell does not cross G−
k⋆n. By construction, the subset
collection
is a partition of Rd represented by a subtree of Tk⋆n rooted at Rd. Moreover,
ˆLn(A, kn) −L⋆(A, kn)
ˆLn(A, kn) −L⋆(A, kn)
Thus, denoting by A−
k⋆n(x) the cell of P−
k⋆n containing x, we are led to
ˆLn(A, kn) −L⋆(A, kn)
k⋆n(X), kn) −L⋆(A−
k⋆n(X), kn)
The end of the proof is similar to the proof of Lemma 6.3. Replace Pk⋆n by
k⋆n and invoke Corollary 6.1 (instead of Proposition 6.1) and Lemma 6.2
(instead of Lemma 6.1).
Proposition 6.3 Let k+ be deﬁned as in (6.1). Then
−ˆLn(Ak⋆n(X), k+)
ψ(n, k) = L⋆
Proof of Proposition 6.3
For every cell A of P−
k⋆n, one has
2dk⋆n −1, 1
≤N(A) + 1 ≤
2dk⋆n + 1.
and note that, by inequalities (6.2), for all n large enough,
n ≤k+ ≤kn.
Thus, by the triangle inequality and Fact 5.1(ii), we may write
−ˆLn(Ak⋆n(X), k+)
−ˆLn(Ak⋆n(X), k+) +
N (A(X))1[N(A(X))>0]
N (A(X))1[N(A(X))>0]
−ˆLn(Ak⋆n(X), k+)
Consequently, by Fact 5.1(iii),
−ˆLn(Ak⋆n(X), k+)
−ˆLn(Ak⋆n(X), kn)
With respect to the ﬁrst term on the right-hand side, we have
−ˆLn(Ak⋆n(X), kn)
−L⋆ Ak⋆n(X)
L⋆(Ak⋆n(X), kn)
L⋆(Ak⋆n(X), kn) −ˆLn(Ak⋆n(X), kn)
According to Lemma 6.1, the ﬁrst of the four terms above is O(
whereas the third one is nonpositive by Fact 5.1(iv). Consequently,
 Ak⋆n(X), kn
L⋆(Ak⋆n(X), kn) −ˆLn(Ak⋆n(X), kn)
Evoking ﬁnally Lemma 6.3, we see that
−ˆLn(Ak⋆n(X), kn)
Combining this result with (6.3) leads to the desired statement.
Lemma 6.5 Let k+ be deﬁned as in (6.1). Then
−ˆLn(Ak⋆n(X), k+)
Proof of Lemma 6.5
Since N(Ak⋆n(x)) ≤n/2dk⋆
n, one has
−ˆLn(Ak⋆n(X), k+)
−ˆLn(Ak⋆n(X), k+)
n/2dk⋆n + 1
Therefore, by Markov’s inequality,
−ˆLn(Ak⋆n(X), k+)
n + 1)β × E
−ˆLn(Ak⋆n(X), k+)
Thus, by Proposition 6.3,
−ˆLn(Ak⋆n(X), k+)
But, by deﬁnition of k⋆
It follows, since n/2dk⋆
n →∞, that
−ˆLn(Ak⋆n(X), k+)
Lemma 6.6 Let G−
k⋆n be the collection of cells of Gn at level at most k⋆
x ∈Rd, denote by A−
k⋆n(x) the cell of G−
k⋆n containing x, and set N(A−
i=1 1[Xi∈A−
k⋆n(x)]. Deﬁne
k⋆n(x),Yi=1].
ˆηn(z)µ(dz) −
and, similarly,
(1 −ˆηn(z)) µ(dz) −
(1 −η(z)) µ(dz)
Proof of Lemma 6.6
We only have to prove the ﬁrst statement. To this
aim, observe that
ˆηn(z)µ(dz) −
1[Xi∈A,Yi=1] −
Denote by ¯G−
k⋆n the cells of Pk⋆n such that the path from the root to the cell
does not cross G−
k⋆n. By construction, the subset collection
is a partition of Rd represented by a subtree of Tk⋆n rooted at Rd. Now,
ˆηn(z)µ(dz) −
1[Xi∈A,Yi=1] −
But, since P−
k⋆n is a partition of Rd, one has
1[Xi∈A,Yi=1] −
k⋆n(X),Yi=1] −
This term goes to 0 by Corollary 6.2.
Acknowledgments
We thank the Editor and an anonymous referee for
valuable comments and insightful suggestions.