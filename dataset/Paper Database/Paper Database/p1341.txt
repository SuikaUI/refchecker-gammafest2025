A Continuation Method for Semi-Supervised SVMs
Olivier Chapelle
 
Mingmin Chi
 
Alexander Zien
 
Max Planck Institute for Biological Cybernetics, T¨ubingen, Germany
Semi-Supervised Support Vector Machines
(S3VMs) are an appealing method for using unlabeled data in classiﬁcation: their objective function favors decision boundaries
which do not cut clusters.
However their
main problem is that the optimization problem is non-convex and has many local minima, which often results in suboptimal performances. In this paper we propose to use a
global optimization technique known as continuation to alleviate this problem.
Compared to other algorithms minimizing the
same objective function, our continuation
method often leads to lower test errors.
1. Introduction
While classical Support Vector Machines (SVMs) are
usually a powerful tool in supervised classiﬁcation
tasks, in many problems labeled data are rather scarce,
and it is desirable to additionally take advantage of
training examples without labels.
It has early been
proposed to extend the margin maximizing property of SVMs to unlabeled data
This favors classiﬁcation boundaries in lowdensity areas, which makes a lot of sense if one believes
in the “cluster assumption” : that usually each cluster of data points
belongs to a single class.
Algorithms implementing this principle have been introduced in under the
name Transductive Support Vector Machine (TSVM),
and in under the name Semi-Supervised Support
Vector Machine (S3VM). In this paper we use the lat-
Appearing in Proceedings of the 23 rd International Conference on Machine Learning, Pittsburgh, PA, 2006. Copyright 2006 by the author(s)/owner(s).
ter name since, according to our understanding, this
algorithm is actually inductive.
Further implementations include ,
[...] more research in algorithms for training
TSVMs is needed. How well does the algorithm presented here approximate the global
solution? Will the results get even better, if
we invest more time into search ?
Section 2 starts with a formal exposition of the S3VMs.
Then Section 3 presents the continuation approach and
its instantiation for S3VMs. We give experimental support in Section 4. While the focus of this paper is the
improvement of S3VM accuracy, we brieﬂy compare it
to graph-based semi-supervised learning and consider
computational complexity (Section 5).
2. Semi-Supervised SVMs
We ﬁrst introduce the cost function of S3VM in the
linear case, and then extend it to the non-linear case.
2.1. The Linear Case
We consider here the problem of binary classiﬁcation. The training set consists of n labeled examples
{(xi, yi)}n
i=1, yi = ±1, and of m the unlabeled examples {xi}n+m
A Continuation Method for Semi-Supervised SVMs
Standard linear supervised SVMs output a decision function of the form f(x) =
sign(w⊤x + b), by minimizing the following objective
yi(w⊤xi + b)
where ℓ(t) = max(0, 1−t)p is a loss function penalizing
the training errors either linearly (p = 1) or quadratically (p = 2), and C is a trade-oﬀconstant. In the rest
of the paper, we consider p = 1. The corresponding
loss function is depicted in Figure 1(a).
This objective function is usually optimized in the
dual, but can also be optimized directly in the primal,
e.g. by gradient descent .
In the semi-supervised case, an additional term is
added to the objective function that drives the outputs w⊤xi + b of the unlabeled points xi away from 0
(thereby implementing the cluster assumption):
yi(w⊤xi + b)
|w⊤xi + b|
The main problem is that this additional term in the
objective function (2) is non-convex and gives rise to
local minima (see Figure 1(b)).
2.2. Balancing Constraint
In addition to the local minima, the objective function (2) has another problem: it tends to give unbalanced solutions which classify all the unlabeled points
in the same class. In it was proposed
to add the additional constraint that the class ratio
on the unlabeled set should be the same as on the labeled set. However, if the true class ratio is not well
estimated from the labeled set, this constraint can be
harmful. We thus adopt the following more ﬂexible
constraint from 
w⊤xi + b = 1
This is in analogy to the treatment of the min-cut
problem in spectral clustering, which is usually replaced by the normalized cut to enforce balanced solutions .
One easy way to enforce the constraint (3) is to translate all the points such that the mean of the unlabeled
points is the origin, Pn+m
i=n+1 xi = 0. Then, by ﬁxing
i=1 yi, we can have an unconstrained optimization on w.
2.3. The Non-Linear Case
Support Vector Machines are often used to ﬁnd nonlinear decision boundaries by applying the “kernel
trick”: the data are ﬁrst mapped in a high dimensional
Hilbert space H through a mapping Φ : X 7→H, and
then a linear decision boundary is constructed in that
space. The mapping Φ is never explicitly computed,
but only implicitly through the use of a kernel function
k such that k(x, y) = Φ(x)⊤Φ(y). We will use K to
denote the kernel matrix, which is the n + m square
matrix of elements Kij = k(xi, xj).
Instead of expressing the optimization of (2) in terms
of dot products, we will perform it directly in feature
space. The idea why this is feasible is that even in the
case of a high dimensional feature space H, a training set (x1, . . . , xq) of size q (for S3VM, q = n + m),
when mapped to this feature space, spans a vectorial
subspace E ⊂H whose dimension is at most q. By
choosing a basis for E and expressing the coordinates
of all the points in that basis, we can then directly work
in E. The representer theorem also ensures that the optimal solution is in E.
Let (v1, . . . , vq) ∈Eq be an orthonormal basis of E,1
where vp is expressed as
AipΦ(xi) .
The orthonormality of the basis thus translates into
the requirement A⊤KA = I, which is equivalent to
K = (AA⊤)−1. Because of rotations, there is an in-
ﬁnite number of matrices A satisfying this condition.
One of them can be found by inverting the Cholesky
decomposition of K. Another possibility is to perform
the eigendecomposition of K as K = UΛU ⊤and set
A = UΛ−1/2. This letter case corresponds to the kernel PCA map introduced in .
Now we can use the following mapping ψ : X 7→Rq:
ψp(x) = Φ(x)⊤vp =
Aipk(x, xi),
It can be easily checked that ψ(xi)⊤ψ(xj) = k(xi, xj).
Thus applying the linear approach presented in the
1We suppose that dim(E) = q, i.e. that the points are
linearly independent in feature space or that K has full
rank. The same analysis can be followed when dim(E) < q.
A Continuation Method for Semi-Supervised SVMs
Signed output
Signed output
Signed output
Figure 1. Losses in the S3VM objective function (2): (a) hinge loss for the labeled points, i.e., ℓ(t) = max(0, 1 −t),
(b) symmetric hinge loss for the unlabeled points, i.e., ℓ(|t|) = max(0, 1 −|t|) and (c) diﬀerentiable surrogate for (b),
˜ℓ(t) = exp(−3t2).
previous section to this representation is equivalent to
solving the non-linear problem.
3. The Continuation Approach
The continuation method belongs to the ﬁeld of global
optimization techniques . The idea is similar to deterministic annealing: a smoothed version of
the objective function is ﬁrst minimized. With enough
smoothing, the optimization will be convex, and the
global minimum can be found. Then the smoothing
decreases and the new minimum is computed, where
the solution found in the previous step serves as a
starting point. The algorithm iterates until there is
no smoothing (cf Algorithm 1 and Figure 2).
Algorithm 1 Continuation method
Require: Function f : Rd 7→R, initial point x0 ∈Rd
Require: Sequence γ0 > γ1 > . . . γp−1 > γp = 0.
Let fγ(x) = (πγ)−d/2 R
f(x −t) exp(−∥t∥2/γ)dt.
for i = 0 to p do
Starting from xi, ﬁnd local minimizer xi+1 of fγi.
Note that in general, other smoothing functions than
the Gaussian can be used.
3.1. Smoothing of the S3VM Objective
convolution
(πγ)−d/2 exp(−t2/γ) of variance γ/2.
The convolution is done only on w and not on b, since the latter
is ﬁxed (see Section 2.2). Finally, since it is easy to
convolve two Gaussians together, we decided to replace the loss of an unlabeled point, max(0, 1−|t|), by
Figure 2. Illustration of the continuation method: the original objective function in blue has two local minima. By
smoothing it, we ﬁnd one global minimum (the red star
on the green curve). And by reducing the smoothing, the
minimum moves toward the global minimum of the original
function (cf the blue-green curve).
exp(−st2) with s = 3 as in ,
cf Figure 1(c).
The calculations are lengthy but straightforward, and
we just present the ﬁnal results. The convolution of
the loss associated with one training point xi turns out
to be a one-dimensional convolution, since the function
w⊤xi is constant on the subspace orthogonal to xi.
The convolved loss is
L(w −t) exp(−∥t∥2/γ)dt (5)
−eierfc(ei)
µ−s(w⊤xi + b)2
ai = 1 + 2γs∥xi∥2
ei = yi(w⊤xi+b)−1
A Continuation Method for Semi-Supervised SVMs
where erfc(·) denotes the complementary error function, erfc(x) =
x e−t2 dt.
The gradient of Lγ(·) at w is
∇Lγ(w) = w⊤−C
erfc(ei)yix⊤
2s(w⊤xi + b)
µ−s(w⊤xi + b)2
3.2. Choice of the Smoothing Sequence
We decide to choose γ0 large enough such that Lγ0 is
convex and thus easy to optimize, but not too large
to avoid wasting time in this non-interesting regime.
More precisely, we compute a lower bound on the
smallest eigenvalue of the Hessian and set γ0 such that
this lower bound is 0.
The Hessian of (7), the term of Lγ corresponding to
the labeled loss, is positive deﬁnite (since the convolution of a convex function with a positive function is
convex). To control the term (8), we consider any unlabeled point xi, n + 1 ≤i ≤n + m, and lower bound
the associated term of the Hessian,
ai (w⊤xi−b)
using √ai > √2γs∥xi∥. We want the sum of all lower
bound terms plus the Hessian for the regularizer (6)
(which is just the identity matrix) to be positive definite.
This is the case for γ ≥
(C∗λmax)2/3
where λmax is the maximum eigenvalue of the matrix
The ﬁnal value of γ is chosen such that Lγ and L0
(the original objective function) are close. Again, we
look at the unlabeled part of the objective function (8).
When γ →0, ai →1 and, as expected, (8) converges
to the original loss. So we choose the ﬁnal value of γ
such that ai ≈1, or 2γs∥xi∥2 ≪1. In practice, we ﬁx
2s maxi ∥xi∥2 =: γend.
Finally, we decide to take eleven steps on a geometric
scale between γ0 and γend, yielding the sequence
0 ≤i ≤10 .
3.3. Link with Kernel Feature Extraction
By construction of γ0, for a value of γ = ˜γ0 which
is smaller than γ0, the smallest eigenvalue of the Hessian of (6) will be zero. The corresponding eigenvector
is related to the leading eigenvector v∗of the matrix
∥xi∥3 (it would be the same if (9) were
an equality). Thus for γ = ˜γ0, (6)+(8) do not penalize
w in the direction of v∗, which means that the algorithms encourages the use of this direction in the task
of minimizing (7).
Ignoring the denominator in M and recalling that the
points are in feature space, v∗is the principal kernel component .
This renders the behavior observed above very satisfying, as it has been argued that the ﬁrst principal kernel
components capture the cluster structure in the data
 . So the term in the objective function
corresponding to the unlabeled points can be seen as
a data dependent regularization term, encouraging solutions that are smooth with respect to the cluster
structure contained in the unlabeled data.
4. Experiments
To validate the performance of our method, we conduct numerical experiments. First we investigate the
relationship between objective function and test error.
Then we compare the described continuation method,
which we call cS3VM, to the ∇S3VM described in
 and the S3VM implementation in SVMlight , here denoted by
S3VMlight. For simplicity we start with the hyperparameters being ﬁxed to reasonable values, and afterwards verify the robustness of the ﬁndings with respect
to changes of the hyperparameters.
4.1. Experimental Setup
We conduct experiments on three datasets.
classical semi-supervised learning task, we use the
two-class text dataset Text from where the goal is to classify the newsgroups mac
and mswindows from the Newsgroup20 dataset. However, S3VMs are known to perform well on text data
 . Analyzing
the experimental results from ,
it appears that ∇S3VM and S3VMlight do not perform
well for the recognition of handwritten digits and objects in images. As more challenging datasets, we thus
use Uspst and Coil from .
Uspst contains 2007 images of hand-written digits (the
test set of the original USPS data set), hence is consists
A Continuation Method for Semi-Supervised SVMs
of ten classes. Coil contains 1440 images of twenty
diﬀerent objects, each of which corresponds to a class.
For each of these datasets, we use the same ten splits
into labeled and unlabeled points as . For the multiclass datasets, we focus on a given
pair of classes by removing the other classes from both
the labeled set and the unlabeled set. For Uspst, this
results in 450 experiments: 45 one-vs-one tasks with
ten repetitions (splits) for each. For Coil, we end up
with 1900 experiments.
We note that all three datasets seem to comply to
some extent with the cluster assumption: S3VMs can
reliably outperform SVMs that use just the labeled
The behavior of S3VMs on a dataset (g10n)
that violates this assumption has been investigated by
 ; there, using the unlabeled
data does not yield an accuracy improvement.
4.2. Objective versus Test Error
For this ﬁrst investigation we pick Uspst, the most
challenging of our datasets. We choose the Gaussian
RBF kernel, which is a good general purpose kernel,
and known to work well for USPS. We ﬁx the kernel
width (standard deviation) σ to 8. Then we ﬁx the
regularization parameter C to 100: this a high value,
well suited for the low noise present in that dataset.
Finally, we set C∗in (2) to C.
To verify that lower values of the objective function indeed correspond to lower test errors, we compare the
local minima found by cS3VM and ∇S3VM. As Figure 3 shows, there is indeed a clear correlation: when
the objective function is higher (stuck in a worse local
minimum), the test error is frequently higher as well.
This justiﬁes putting eﬀort into the optimization.
∆ Objective value
∆ Test error
Figure 3. Diﬀerence of test error versus diﬀerence in objective function value.
Each dot corresponds to the two
local minima found with with cS3VM and ∇S3VM for one
of the 450 learning tasks of Uspst.
4.3. Comparing S3VM Optimization Methods
Next, we compare the prediction errors on the 450 experiments described above by scatter plots shown in
Figure 4. cS3VM is better than each of the two competitors in the majority of cases. It is also apparent
that the average test error of cS3VM is lower than
those of the other two methods. These averages are
provided in Table 1 for for all three data sets. As a
reference point, we include the results of a plain (supervised) SVM.
Figure 4. Comparison of test errors achieved in the 450
Uspst experiments.
Top, cS3VM vs ∇S3VM; bottom,
cS3VM vs S3VMlight.
Test errors on diﬀerent datasets averaged over
splits (and, for Uspst and Coil, over pairs of classes) with
ﬁxed hyperparameters. Uspst, Gaussian RBF kernel with
σ = 8; Coil, Gaussian RBF kernel with σ = 1000; Text,
linear kernel. In all cases, C∗= C = 100 (which is close to
hard margin).
For Coil the diﬀerence between cS3VM and ∇S3VM is
A Continuation Method for Semi-Supervised SVMs
signiﬁcant at 95% conﬁdence, but not between cS3VM
and S3VMlight. This dataset is much easier than the
Uspst, and all the S3VM implementations achieve a
good test error, making it more diﬃcult to compare
them. Note that for Coil S3VMlight has a slight advantage because of its hard balancing constraint: here
the fraction of positive points in the unlabeled set exactly equals that in the labeled data (50%).
Finally, it should be veriﬁed that the improvement over
∇S3VM is not due to a too coarse annealing sequence.
We repeat the Uspst experiment with a long and slow
annealing sequence for ∇S3VM: start C∗at very small
value (10−6 its ﬁnal value) and took 100 steps. This
did not result in an improvement (test error of 4.9%).
As for S3VMlight, it has a very conservative annealing
schedule built in.
4.4. Inﬂuence of the Hyperparameters
Since all three compared algorithms minimize (almost)
the same objective function,2 the ﬁxed hyperparameters should be (almost) equally good or bad for all of
them. However, there is still a small possibility that
some regime of hyperparameter settings is more suitable for some algorithm than for another.
To be sure that the hyperparameters do not unduly in-
ﬂuence the conclusions drawn above, we try diﬀerent
values of the hyperparameters. All the results reported
in this section are again averages over splits and (except for Text) pairs of classes.
First we vary the kernel width σ on an exponential
scale while keeping C (and C∗). Figure 5 shows that
cS3VM is consistently better than the other methods
(exempliﬁed for Uspst).
We also try a smaller values of C, namely C = 10 for
Uspst and C = 1 for Text. Again, cS3VM turns out
best in all cases (cf Table 2).
Table 2. Test errors with varying C (with C∗= C). Top,
Uspst with σ = 8; bottom, Text.
Finally, we vary C∗(not for S3VMlight, since this is
2The objective function of cS3VM and ∇S3VM is the
same. S3VMlight uses the loss (b) instead of (c) in Figure
1 and has a diﬀerent balancing constraint.
Test error
Figure 5. Test errors on Uspst of the diﬀerent implementations of S3VM with various values of σ (for C = C∗= 100).
not oﬀered by the actual implementation); see Table
3. All results indicate that the superior accuracy of
cS3VM is stable with respect to the hyperparameter
Table 3. Test errors on Uspst with σ = 8 and C = 100.
5. Further Considerations
5.1. Relation with Graph-based Approaches
As the last section demonstrates, cS3VM is relatively
good and novel method to train S3VMs.
good is the S3VM itself when compared to other semisupervised approaches? To answer this question, we
compare S3VMs to a modern graph-based approach,
namely LapSVM . In this approach, a supervised SVM is trained on the labeled
points, but in addition to the “normal” kernel, the
graph Laplacian derived from both labeled and unlabeled points is used for regularization. In their setting,
the two resulting regularizers are added to the data ﬁtting term after being weighted with γA for the normal
kernel, and γI for the Laplacian kernel. The standard
supervised SVM is recovered as a special case with
γI = 0; then γA corresponds to 1/C in the SVM.
For Text, being two-class, we can immediately compare our results to those in :
there, LapSVM achieved 10.4% – signiﬁcantly worse
than any S3VM method (Table 2, bottom). However,
it is known that graph-based methods work well on
data with manifold structure, but less so on text.
A Continuation Method for Semi-Supervised SVMs
As a more challenging test for the S3VM, we compare
the performance on Uspst. We set up the graph Laplacian kernel with the same hyperparameters (σ, degree
of the Laplacian, and number of nearest neighbors) as
chosen in . We only change
γA to 1/C = 0.01 );
however, both correspond to an almost hard margin.
After computing both Gaussian RBF and the graph
Laplacian kernel,3 we vary γI and train both an SVM
and the cS3VM on top of them. The results are shown
in Figure 6.
Test error
Figure 6. SVM and cS3VM trained with additional graphbased regularization (of strength γI).
When comparing cS3VM (with just the normal kernel)
and LapSVM (with best γI), we have 2.9% vs 2.8% (cf
also Table 1). This is very encouraging, since graph
based methods were known to be much better than
S3VM on manifold datasets. Second, hybrid methods
— combining best of both worlds — can further improve the results (here, to 1.8%).
5.2. Computational Considerations
The runtime of the cS3VM algorithm as described
above is cubic in the number of total data points,
. First, the eigendecomposition takes
cubic time; then, the continuation method requires
a constant number (eleven) of gradient descent minimizations in the kernel PCA space, each of which
is again of complexity O
Given that
semi-supervised learning is designed for problems with
plenty of unlabeled data, cubic time complexity seems
impractical.
However, if desired, the computation can be sped up
considerably. First, in many cases the data lie close to
a lower-dimensional subspace in H. In this case, the
leading p eigenvectors of K can provide a suﬃcient
basis for representing the data.
These eigenvectors
can be approximated by applying the kernel PCA to a
3using the code from 
~vikass/research.html
subset of q > p points. The total computational cost
of the algorithm is then reduced to O(q3 +pq(n+m)).
To demonstrate this, we classify the digits “5” versus “8” from the MNIST dataset. We randomly draw
10 labeled points of each class and 5000 unlabeled
points. The hyperparameters are set to σ = 5 ∗256
and C = C∗= 100.
Then we vary p = q over
5000 ∗{1/64, 1/32, . . . , 1/2, 1}. Running time and results are shown in Figure 7. The training time can be
2 orders of magnitude faster without loosing too much
Time (sec)
Test error
Figure 7. Time vs test error for various values of p = q.
The bars represent the standard error of the mean. The
red cross is an SVM trained on the 20 labeled points.
5.3. One-vs-the Rest
The results reported above for Uspst and Coil are
in a 1-vs-1 setting. For 1-vs-rest, the performance of
cS3VM is disappointing (cf Table 4).
Table 4. Performances of diﬀerent S3VM implementations
on the Uspst dataset in a 1-vs-rest setting or 5-vs-5 setting
(digits 0 to 4 against 5 to 9). The hyperparameters are the
same as in Section 4.2.
One possible explanation why cS3VM does not perform well in 1-vs-rest and 5-vs-5 is the following: unlike 1-vs-1, each class consists several clusters.
continuation method might have diﬃculties with this
situation, which presents several possibilities for undesirable low density separations. The analysis in section
3.3 suggests that cS3VM favors the split corresponding
to the largest principal component.
Also, note that none of the S3VMs methods performs
well: they hardly improve over a standard SVM, and
a graph-based approach can achieve much better results. Indeed, a 12.7% error on Uspst in a 1-vs-rest
A Continuation Method for Semi-Supervised SVMs
setting has been reported in .4
This shows that S3VMs are not yet a state-of-the-art
method for multiclass problems.
6. Conclusion and Discussion
In it was conjectured that
the objective function of S3VM is well designed, since
it implements the cluster assumption.
Our results
(e.g. Figure 3) conﬁrm that a better optimization of
this function indeed tends to result in a higher predictive accuracy. Consequently care should be taken not
to get trapped in bad local minima.
We use a global optimization technique for non-convex
problems to handle this diﬃculty. Evaluated in a onevs-one setting, the results compare favorably against
two competing approaches suggested in the literature.
However, 1-vs-1 is not suﬃcient for real-world multiclass semi-supervised learning, since the unlabeled
data cannot be restricted to the two classes under consideration. Future work should therefore extend to the
one-vs-rest scheme or to a true multiclass method.