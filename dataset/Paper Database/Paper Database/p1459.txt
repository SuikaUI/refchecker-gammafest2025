Path Aggregation Network for Instance Segmentation
Haifang Qin§
Jianping Shi‡
Jiaya Jia†,♭
†The Chinese University of Hong Kong
§Peking University
‡SenseTime Research
♭YouTu Lab, Tencent
{sliu, luqi, leojia}@cse.cuhk.edu.hk
 
 
The way that information propagates in neural networks
is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information
ﬂow in proposal-based instance segmentation framework.
Speciﬁcally, we enhance the entire feature hierarchy with
accurate localization signals in lower layers by bottom-up
path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each feature level
propagate directly to following proposal subnetworks. A
complementary branch capturing different views for each
proposal is created to further improve mask prediction.
These improvements are simple to implement, with subtle extra computational overhead. Our PANet reaches the
1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. It is also state-of-the-art on MVD
and Cityscapes. Code is available at 
com/ShuLiu1993/PANet.
1. Introduction
Instance segmentation is one of the most important and
challenging tasks. It aims to predict class label and pixelwise instance mask to localize varying numbers of instances
presented in images. This task widely beneﬁts autonomous
vehicles, robotics, video surveillance, to name a few.
With the help of deep convolutional neural networks,
several frameworks for instance segmentation, e.g., , were proposed where performance grows rapidly
 . Mask R-CNN is a simple and effective system
for instance segmentation. Based on Fast/Faster R-CNN
 , a fully convolutional network (FCN) is used for
mask prediction, along with box regression and classiﬁcation. To achieve high performance, feature pyramid network (FPN) is utilized to extract in-network feature
hierarchy, where a top-down path with lateral connections
is augmented to propagate semantically strong features.
Several newly released datasets make large
room for algorithm improvement. COCO consists of
200k images. Lots of instances with complex spatial layout are captured in each image. Differently, Cityscapes 
and MVD provide street scenes with a large number
of trafﬁc participants in each image. Blur, heavy occlusion
and extremely small instances appear in these datasets.
There have been several principles proposed for designing networks in image classiﬁcation that are also effective for object recognition. For example, shortening information path and easing information propagation by clean
residual connection and dense connection are
useful. Increasing the ﬂexibility and diversity of information paths by creating parallel paths following the splittransform-merge strategy is also beneﬁcial.
Our Findings
Our research indicates that information
propagation in state-of-the-art Mask R-CNN can be further
improved. Speciﬁcally, features in low levels are helpful for
large instance identiﬁcation. But there is a long path from
low-level structure to topmost features, increasing difﬁculty
to access accurate localization information. Further, each
proposal is predicted based on feature grids pooled from one
feature level, which is assigned heuristically. This process
can be updated since information discarded in other levels
may be helpful for ﬁnal prediction. Finally, mask prediction
is made on a single view, losing the chance to gather more
diverse information.
Our Contributions
Inspired by these principles and observations, we propose PANet, illustrated in Figure 1, for
instance segmentation.
First, to shorten information path and enhance feature pyramid with accurate localization signals existing in
low-levels, bottom-up path augmentation is created.
fact, features in low-layers were utilized in the systems of
 . But propagating low-level
features to enhance entire feature hierarchy for instance
recognition was not explored.
 
Figure 1. Illustration of our framework. (a) FPN backbone. (b) Bottom-up path augmentation. (c) Adaptive feature pooling. (d) Box
branch. (e) Fully-connected fusion. Note that we omit channel dimension of feature maps in (a) and (b) for brevity.
Second, to recover broken information path between
each proposal and all feature levels, we develop adaptive
feature pooling. It is a simple component to aggregate features from all feature levels for each proposal, avoiding arbitrarily assigned results. With this operation, cleaner paths
are created compared with those of .
Finally, to capture different views of each proposal, we
augment mask prediction with tiny fully-connected (fc) layers, which possess complementary properties to FCN originally used by Mask R-CNN. By fusing predictions from
these two views, information diversity increases and masks
with better quality are produced.
The ﬁrst two components are shared by both object detection and instance segmentation, leading to much enhanced performance of both tasks.
Experimental Results
With PANet, we achieve state-ofthe-art performance on several datasets. With ResNet-50
 as the initial network, our PANet tested with a single
scale already outperforms champion of COCO 2016 Challenge in both object detection and instance segmentation tasks. Note that these previous results are achieved
by larger models together with multi-scale and horizontal ﬂip testing.
We achieve the 1st place in COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. We also benchmark our system on Cityscapes and MVD, which similarly
yields top-ranking results, manifesting that our PANet is a
very practical and top-performing framework.
and models are available at 
ShuLiu1993/PANet.
2. Related Work
Instance Segmentation
There are mainly two streams of
methods in instance segmentation. The most popular one is
proposal-based. Methods in this stream have a strong connection to object detection. In R-CNN , object proposals from were fed into the network to extract features for classiﬁcation. While Fast/Faster R-CNN 
and SPPNet sped up the process by pooling features
from global feature maps. Earlier work took mask
proposals from MCG as input to extract features while
CFM , MNC and Hayder et al. merged feature
pooling to network for faster speed. Newer design was to
generate instance masks in networks as proposal 
or ﬁnal result . Mask R-CNN is an effective
framework falling in this stream. Our work is built on Mask
R-CNN and improves it from different aspects.
Methods in the other stream are mainly segmentationbased.
They learned specially designed transformation
 or instance boundaries . Then instance
masks were decoded from predicted transformation.
Instance segmentation by other pipelines also exists. DIN 
fused predictions from object detection and semantic segmentation systems. A graphical model was used in 
to infer the order of instances. RNN was utilized in 
to propose one instance in each time step.
Multi-level Features
Features from different layers were
used in image recognition. SharpMask , Peng et al. 
and LRR fused feature maps for segmentation with
ﬁner details. FCN , U-Net and Noh et al. fused
information from lower layers through skip-connections.
Both TDM and FPN augmented a top-down path
with lateral connections for object detection. Different from
TDM, which took the fused feature map with the highest
resolution to pool features, SSD , DSSD , MS-CNN
 and FPN assigned proposals to appropriate feature
levels for inference. We take FPN as a baseline and much
enhance it.
ION , Zagoruyko et al. , Hypernet and Hypercolumn concatenated feature grids from different
layers for better prediction. But a sequence of operations,
i.e., normalization, concatenation and dimension reduction
are needed to get feasible new features. In comparison, our
Figure 2. Illustration of our building block of bottom-up path augmentation.
design is much simpler.
Fusing feature grids from different sources for each proposal was also utilized in . But this method extracted
feature maps on input with different scales and then conducted feature fusion (with the max operation) to improve
feature selection from the input image pyramid. In contrast,
our method aims at utilizing information from all feature
levels in the in-network feature hierarchy with single-scale
input. End-to-end training is enabled.
Larger Context Region
Methods of pooled
features for each proposal with a foveal structure to exploit context information from regions with different resolutions. Features pooled from a larger region provide surrounding context. Global pooling was used in PSPNet 
and ParseNet to greatly improve quality of semantic
segmentation. Similar trend was observed by Peng et al.
 where global convolutionals were utilized. Our mask
prediction branch also supports accessing global information. But the technique is completely different.
3. Framework
Our framework is illustrated in Figure 1. Path augmentation and aggregation is conducted for improving performance. A bottom-up path is augmented to make low-layer
information easier to propagate. We design adaptive feature
pooling to allow each proposal to access information from
all levels for prediction. A complementary path is added
to the mask-prediction branch. This new structure leads to
decent performance. Similar to FPN, the improvement is
independent of the CNN structure, e.g., .
3.1. Bottom-up Path Augmentation
Motivation
The insightful point that neurons in high
layers strongly respond to entire objects while other neurons
are more likely to be activated by local texture and patterns
manifests the necessity of augmenting a top-down path to
propagate semantically strong features and enhance all features with reasonable classiﬁcation capability in FPN.
Our framework further enhances the localization capability of the entire feature hierarchy by propagating strong
responses of low-level patterns based on the fact that high
response to edges or instance parts is a strong indicator to
accurately localize instances. To this end, we build a path
with clean lateral connections from the low level to top
ones. Therefore, there is a “shortcut” (dashed green line
in Figure 1), which consists of less than 10 layers, across
these levels. In comparison, the CNN trunk in FPN gives a
long path (dashed red line in Figure 1) passing through even
100+ layers from low layers to the topmost one.
Augmented Bottom-up Structure
Our framework ﬁrst
accomplishes bottom-up path augmentation.
FPN to deﬁne that layers producing feature maps with the
same spatial size are in the same network stage. Each feature level corresponds to one stage. We also take ResNet
 as the basic structure and use {P2, P3, P4, P5} to denote feature levels generated by FPN. Our augmented path
starts from the lowest level P2 and gradually approaches
P5 as shown in Figure 1(b).
From P2 to P5, the spatial size is gradually down-sampled with factor 2. We use
{N2, N3, N4, N5} to denote newly generated feature maps
corresponding to {P2, P3, P4, P5}. Note that N2 is simply
P2, without any processing.
As shown in Figure 2, each building block takes a higher
resolution feature map Ni and a coarser map Pi+1 through
lateral connection and generates the new feature map Ni+1.
Each feature map Ni ﬁrst goes through a 3 × 3 convolutional layer with stride 2 to reduce the spatial size. Then
each element of feature map Pi+1 and the down-sampled
map are added through lateral connection. The fused feature map is then processed by another 3 × 3 convolutional
layer to generate Ni+1 for following sub-networks. This
is an iterative process and terminates after approaching P5.
In these building blocks, we consistently use channel 256
of feature maps. All convolutional layers are followed by
a ReLU . The feature grid for each proposal is then
pooled from new feature maps, i.e., {N2, N3, N4, N5}.
3.2. Adaptive Feature Pooling
Motivation
In FPN , proposals are assigned to different feature levels according to the size of proposals. It
makes small proposals assigned to low feature levels and
large proposals to higher ones. Albeit simple and effective,
it still could generate non-optimal results. For example, two
proposals with 10-pixel difference can be assigned to different levels. In fact, these two proposals are rather similar.
Further, importance of features may not be strongly correlated to the levels they belong to. High-level features are
generated with large receptive ﬁelds and capture richer context information. Allowing small proposals to access these
features better exploits useful context information for prediction. Similarly, low-level features are with many ﬁne details and high localization accuracy. Making large proposals
access them is obviously beneﬁcial. With these thoughts,
we propose pooling features from all levels for each pro-
FEATURE DISTRIBUTION
Figure 3. Ratio of features pooled from different feature levels
with adaptive feature pooling. Each line represents a set of proposals that should be assigned to the same feature level in FPN,
i.e., proposals with similar scales. The horizontal axis denotes the
source of pooled features. It shows that proposals with different
sizes all exploit features from several different levels.
posal and fusing them for following prediction. We call this
process adaptive feature pooling.
We now analyze the ratio of features pooled from different levels with adaptive feature pooling. We use max
operation to fuse features from different levels, which lets
network select element-wise useful information. We cluster
proposals into four classes based on the levels they were assigned to originally in FPN. For each set of proposals, we
calculate the ratio of features selected from different levels.
In notation, levels 1 −4 represent low-to-high levels. As
shown in Figure 3, the blue line represents small proposals that were assigned to level 1 originally in FPN. Surprisingly, nearly 70% of features are from other higher levels.
We also use the yellow line to represent large proposals that
were assigned to level 4 in FPN. Again, 50%+ of the features are pooled from other lower levels. This observation
clearly indicates that features in multiple levels together are
helpful for accurate prediction. It is also a strong support of
designing bottom-up path augmentation.
Adaptive Feature Pooling Structure
Adaptive feature
pooling is actually simple in implementation and is demonstrated in Figure 1(c). First, for each proposal, we map them
to different feature levels, as denoted by dark grey regions
in Figure 1(b). Following Mask R-CNN , ROIAlign is
used to pool feature grids from each level. Then a fusion
operation (element-wise max or sum) is utilized to fuse feature grids from different levels.
In following sub-networks, pooled feature grids go
through one parameter layer independently, which is followed by the fusion operation, to enable network to adapt
features. For example, there are two fc layers in the box
branch in FPN. We apply the fusion operation after the ﬁrst
layer. Since four consecutive convolutional layers are used
in mask prediction branch in Mask R-CNN, we place fusion
operation between the ﬁrst and second convolutional layers.
Ablation study is given in Section 4.2. The fused feature
ROI conv1 conv2 conv3 conv4
conv4_fc conv5_fc
Figure 4. Mask prediction branch with fully-connected fusion.
grid is used as the feature grid of each proposal for further
prediction, i.e., classiﬁcation, box regression and mask prediction. A detailed illustration of adaptive feature pooling
on box branch is shown by Figure 6 in Appendix.
Our design focuses on fusing information from innetwork feature hierarchy instead of those from different
feature maps of input image pyramid . It is simpler
compared with the process of , where L-2 normalization, concatenation and dimension reduction are needed.
3.3. Fully-connected Fusion
Motivation
Fully-connected layers, or MLP, were widely
used in mask prediction in instance segmentation and mask proposal generation .
Results of
 show that FCN is also competent in predicting pixelwise masks for instances. Recently, Mask R-CNN applied a tiny FCN on the pooled feature grid to predict corresponding masks avoiding competition between classes.
We note fc layers yield different properties compared
with FCN where the latter gives prediction at each pixel
based on a local receptive ﬁeld and parameters are shared
at different spatial locations. Contrarily, fc layers are location sensitive since predictions at different spatial locations
are achieved by varying sets of parameters. So they have
the ability to adapt to different spatial locations. Also prediction at each spatial location is made with global information of the entire proposal. It is helpful to differentiate
instances and recognize separate parts belonging to the
same object. Given properties of fc and convolutional layers different from each other, we fuse predictions from these
two types of layers for better mask prediction.
Mask Prediction Structure
Our component of mask prediction is light-weighted and easy to implement. The mask
branch operates on pooled feature grid for each proposal.
As shown in Figure 4, the main path is a small FCN, which
consists of 4 consecutive convolutional layers and 1 deconvolutional layer. Each convolutional layer consists of 256
3 × 3 ﬁlters and the deconvolutional layer up-samples feature with factor 2.
It predicts a binary pixel-wise mask
for each class independently to decouple segmentation and
classiﬁcation, similar to that of Mask R-CNN. We further
Champion 2016 
6×ResNet-101
Mask R-CNN +FPN 
ResNet-101
Mask R-CNN +FPN 
ResNeXt-101
PANet / PANet [ms-train]
36.6 / 38.2
58.0 / 60.2
39.3 / 41.4
16.3 / 19.1
38.1 / 41.1
53.1 / 52.6
PANet / PANet [ms-train]
40.0 / 42.0
62.8 / 65.1
43.1 / 45.7
18.8 / 22.4
42.3 / 44.7
57.2 / 58.1
ResNeXt-101
Table 1. Comparison among PANet, winner of COCO 2016 instance segmentation challenge, and Mask R-CNN on COCO test-dev subset
in terms of Mask AP, where the latter two are baselines.
create a short path from layer conv3 to a fc layer. There
are two 3×3 convolutional layers where the second shrinks
channels to half to reduce computational overhead.
A fc layer is used to predict a class-agnostic foreground/background mask. It not only is efﬁcient, but also
allows parameters in the fc layer trained with more samples, leading to better generality. The mask size we use is
28 × 28 so that the fc layer produces a 784 × 1 × 1 vector.
This vector is reshaped to the same spatial size as the mask
predicted by FCN. To obtain the ﬁnal mask prediction, mask
of each class from FCN and foreground/background prediction from fc are added. Using only one fc layer, instead of
multiple of them, for ﬁnal prediction prevents the issue of
collapsing the hidden spatial feature map into a short feature
vector, which loses spatial information.
4. Experiments
We compare our method with state-of-the-arts on challenging COCO , Cityscapes and MVD datasets.
Our results are top ranked in all of them. Comprehensive
ablation study is conducted on the COCO dataset. We also
present our results on COCO 2017 Instance Segmentation
and Object Detection Challenges.
4.1. Implementation Details
We re-implement Mask R-CNN and FPN based on Caffe
 . All pre-trained models we use in experiments are publicly available. We adopt image centric training . For
each image, we sample 512 region-of-interests (ROIs) with
positive-to-negative ratio 1 : 3. Weight decay is 0.0001 and
momentum is set to 0.9. Other hyper-parameters slightly
vary according to datasets and we detail them in respective
experiments. Following Mask R-CNN, proposals are from
an independently trained RPN for convenient ablation and fair comparison, i.e., the backbone is not shared
with object detection/instance segmentation.
4.2. Experiments on COCO
Dataset and Metrics
COCO dataset is among the
most challenging ones for instance segmentation and object
detection due to the data complexity. It consists of 115k
images for training and 5k images for validation . 20k images are used in test-dev and 20k images
are used as test-challenge. Ground-truth labels of both testchallenge and test-dev are not publicly available. There are
80 classes with pixel-wise instance mask annotation. We
train our models on train-2017 subset and report results on
val-2017 subset for ablation study. We also report results on
test-dev for comparison.
We follow the standard evaluation metrics, i.e., AP,
AP50, AP75, APS, APM and APL. The last three measure
performance with respect to objects with different scales.
Since our framework is general to both instance segmentation and object detection, we also train independent object
detectors. We report mask AP, box ap APbb of an independently trained object detector, and box ap APbbM of the
object detection branch trained in the multi-task fashion.
Hyper-parameters
We take 16 images in one image
batch for training. The shorter and longer edges of the images are 800 and 1000, if not specially noted. For instance
segmentation, we train our model with learning rate 0.02 for
120k iterations and 0.002 for another 40k iterations. For object detection, we train one object detector without the mask
prediction branch. Object detector is trained for 60k iterations with learning rate 0.02 and another 20k iterations with
learning rate 0.002.
These parameters are adopted from
Mask R-CNN and FPN without any ﬁne-tuning.
Instance Segmentation Results
We report performance
of our PANet on test-dev for comparison, with and without multi-scale training. As shown in Table 1, our PANet
with ResNet-50 trained on multi-scale images and tested on
single-scale images already outperforms Mask R-CNN and
Champion in 2016, where the latter used larger model ensembles and testing tricks . Trained
and tested with image scale 800, which is same as that of
Mask R-CNN, our method outperforms the single-model
state-of-the-art Mask R-CNN with nearly 3 points under the
same initial models.
Object Detection Results
Similar to the way adopted
in Mask R-CNN, we also report bounding box results inferred from the box branch. Table 2 shows that our method
with ResNet-50, trained and tested on single-scale images,
outperforms, by a large margin, all other single-model
ones even using much larger ResNeXt-101 as initial
model. With multi-scale training and single-scale testing,
our PANet with ResNet-50 outperforms Champion in 2016,
which used larger model ensemble and testing tricks.
Champion 2016 
2×ResNet-101 + 3×Inception-ResNet-v2
RentinaNet 
ResNet-101
Mask R-CNN +FPN 
ResNet-101
Mask R-CNN +FPN 
ResNeXt-101
PANet / PANet [ms-train]
41.2 / 42.5 60.4 / 62.3 44.4 / 46.4 22.7 / 26.3 44.0 / 47.0 54.6 / 52.3
PANet / PANet [ms-train]
45.0 / 47.4 65.0 / 67.2 48.6 / 51.8 25.4 / 30.1 48.6 / 51.7 59.1 / 60.0
ResNeXt-101
Table 2. Comparison among PANet, winner of COCO 2016 object detection challenge, RentinaNet and Mask R-CNN on COCO test-dev
subset in terms of box AP, where the latter three are baselines.
MRB RBL MST MBN BPA AFP FF HHD AP/APbb/APbbM AP50 AP75 APS/APbb
33.6 / 33.9 /
33.4 / 35.0 / 36.4
14.1 / 18.7 / 20.0
35.7 / 38.9 / 39.7
50.8 / 47.0 / 48.8
35.3 / 35.0 / 38.2
17.6 / 20.8 / 24.3
38.6 / 39.9 / 42.3
50.6 / 44.1 / 48.8
35.7 / 37.1 / 38.9
18.6 / 24.2 / 25.3
39.4 / 42.5 / 43.6
51.7 / 47.1 / 49.9
36.4 / 38.0 / 39.9
19.3 / 23.3 / 26.2
39.7 / 42.9 / 44.3
52.6 / 49.4 / 51.3
36.3 / 37.9 / 39.6
19.0 / 25.4 / 26.4
40.1 / 43.1 / 44.9
52.4 / 48.6 / 50.5
36.9 / 39.0 / 40.6
19.6 / 25.7 / 27.0
40.7 / 44.2 / 45.7
53.2 / 49.5 / 52.1
37.8 / 39.2 / 42.1
19.2 / 25.8 / 27.0
41.5 / 44.3 / 47.3
54.3 / 50.6 / 54.1
+4.4 / +4.2 / +5.7
+5.1 / +7.1 / +7.0
+5.8 / +5.4 / +7.6
+3.5 / +3.6 / +5.3
Table 3. Performance in terms of mask AP, box ap APbb of an independently trained object detector, and box ap APbbM of the box
branch trained with multi-task fashion on val-2017. Based on our re-implemented baseline (RBL), we gradually add multi-scale training
(MST), multi-GPU synchronized batch normalization (MBN), bottom-up path augmentation (BPA), adaptive feature pooling (AFP), fullyconnected fusion (FF) and heavier head (HHD) for ablation studies. MRB is short for Mask R-CNN result reported in the original paper.
The last line shows total improvement compared with baseline RBL.
Component Ablation Studies
First, we analyze importance of each proposed component. Besides bottom-up path
augmentation, adaptive feature pooling and fully-connected
fusion, we also analyze multi-scale training, multi-GPU
synchronized batch normalization and heavier
head. For multi-scale training, we set longer edge to 1, 400
and the other to range from 400 to 1, 400. We calculate
mean and variance based on all samples in one batch across
all GPUs, do not ﬁx any parameters during training, and
make all new layers followed by a batch normalization
layer, when using multi-GPU synchronized batch normalization. The heavier head uses 4 consecutive 3 × 3 convolutional layers shared by box classiﬁcation and box regression, instead of two fc layers. It is similar to the head used in
 but the convolutional layers for box classiﬁcation and
box regression branches are not shared in their case.
Our ablation study from the baseline gradually to all
components incorporated is conducted on val-2017 subset
and is shown in Table 3. ResNet-50 is our initial model.
We report performance in terms of mask AP, box ap APbb of
an independently trained object detector and box ap APbbM
of box branch trained in the multi-task fashion.
1) Re-implemented Baseline.
Our re-implemented Mask
R-CNN performs comparable with the one described in
original paper and our object detector performs better.
2) Multi-scale Training & Multi-GPU Sync. BN. These
two techniques help the network to converge better and increase the generalization ability.
3) Bottom-up Path Augmentation. With or without adaptive feature pooling, bottom-up path augmentation consistently improves mask AP and box ap APbb by more than 0.6
and 0.9 respectively. The improvement on instances with
large scale is most signiﬁcant. This veriﬁes usefulness of
information from lower feature levels.
4) Adaptive Feature Pooling.
With or without bottomup path augmentation, adaptive feature pooling consistently
improves performance. The performance in all scales generally improves, in accordance with our observation that
features in other layers are also useful in ﬁnal prediction.
5) Fully-connected Fusion.
Fully-connected fusion aims
at predicting masks with better quality. It yields 0.7 improvement in terms of mask AP. It is general for instances
at all scales.
6) Heavier Head.
Heavier head is quite effective for box
ap APbbM of bounding boxes trained with multi-task fashion. While for mask AP and independently trained object
detector, the improvement is minor.
With all these components in PANet, improvement on
mask AP is 4.4 over baselines. Box ap APbb of independently trained object detector increases 4.2. They are significant. Small- and medium-size instances contribute most.
Half improvement is from multi-scale training and multi-
GPU sync. BN, which are effective strategies to help train
better models.
Ablation Studies on Adaptive Feature Pooling
We conduct ablation studies on adaptive feature pooling to ﬁnd
where to place the fusion operation and the most appropri-
Table 4. Ablation study on adaptive feature pooling on val-2017 in
terms of mask AP and box ap APbb of the independently trained
object detector.
Table 5. Ablation study on fully-connected fusion on val-2017 in
terms of mask AP.
ate fusion operation. We place it either between ROIAlign
and fc1, represented by “fu.fc1fc2” or between fc1 and fc2,
represented by “fc1fu.fc2” in Table 4. Similar settings are
also applied to mask prediction branch. For feature fusing,
max and sum operations are tested.
As shown in Table 4, adaptive feature pooling is not sensitive to the fusion operation. Allowing a parameter layer
to adapt feature grids from different levels, however, is of
great importance. We use max as fusion operation and use
it behind the ﬁrst parameter layer in our framework.
Ablation Studies on Fully-connected Fusion
We investigate performance with different ways to instantiate the
augmented fc branch. We consider two aspects, i.e., the
layer to start the new branch and the way to fuse predictions
from the new branch and FCN. We experiment with creating new paths from conv2, conv3 and conv4, respectively.
“max”, “sum” and “product” operations are used for fusion.
We take our reimplemented Mask R-CNN with bottom-up
path augmentation and adaptive feature pooling as the baseline. Corresponding results are shown in Table 5. They
clearly show that staring from conv3 and taking sum for
fusion produces the best results.
COCO 2017 Challenge
With PANet, we participated in
the COCO 2017 Instance Segmentation and Object Detection Challenges. Our framework reaches the 1st place in Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. As shown in Tables
6 and 7, compared with last year champions, we achieve
9.1% absolute and 24% relative improvement on instance
segmentation. While for object detection, 9.4% absolute
and 23% relative improvement is yielded.
The top performance comes with a few more details in
PANet. First, we use deformable convolutions where DCN
 is adopted. The common testing tricks [23, 33, 10,
Champion 2015 
Champion 2016 
Our Team 2017
PANet baseline
+testing tricks
+larger model
Table 6. Mask AP of COCO Instance Segmentation Challenge in
different years on test-dev.
Champion 2015 
Champion 2016 
Our Team 2017
Table 7. Box AP of COCO Object Detection Challenge in different
years on test-dev.
15, 39, 62], such as multi-scale testing, horizontal ﬂip testing, mask voting and box voting, are adopted. For multiscale testing, we set the longer edge to 1, 400 and the other
ranges from 600 to 1, 200 with step 200. Only 4 scales are
used. Second, we use larger initial models from publicly
available ones. We use 3 ResNeXt-101 (64 × 4d) , 2
SE-ResNeXt-101 (32 × 4d) , 1 ResNet-269 and 1
SENet as ensemble for bounding box and mask generation. Performance with different larger initial models are
similar. One ResNeXt-101 (64 × 4d) is used as the base
model to generate proposals. We train these models with
different random seeds, with and without balanced sampling to enhance diversity between models. Detection
results we submitted are acquired by tightening instance
masks. We show a few visual results in Figure 5 – most
of our predictions are with high quality.
4.3. Experiments on Cityscapes
Dataset and Metrics
Cityscapes contains street
scenes captured by car-mounted cameras. There are 2, 975
training images, 500 validation images and 1, 525 testing
images with ﬁne annotations. Another 20k images are with
coarse annotations, excluded for training. We report our
results on val and secret test subsets. 8 semantic classes
are annotated with instance masks. Each image is with size
1024 × 2048. We evaluate results based on AP and AP50.
Hyper-parameters
We use the same set of hyperparameters as in Mask R-CNN for fair comparison.
Speciﬁcally, we use images with shorter edge randomly
sampled from {800, 1024} for training and use images with
shorter edge 1024 for inference. No testing tricks or DCN
is used. We train our model for 18k iterations with learning rate 0.01 and another 6k iterations with learning rate
0.001. 8 images (1 image per GPU) are in one image batch.
ResNet-50 is taken as the initial model on this dataset.
Figure 5. Images in each row are visual results of our model on COCO test-dev, Cityscapes test and MVD test, respectively.
motorcycle
Mask R-CNN [ﬁne-only] 
Mask R-CNN [COCO] 
PANet [ﬁne-only]
PANet [COCO]
Table 8. Results on Cityscapes val subset, denoted as AP [val], and on Cityscapes test subset, denoted as AP.
our re-implement
our re-implement + MBN
Table 9. Ablation study results on Cityscapes val subset. Only ﬁne
annotations are used for training. MBN is short for multi-GPU
synchronized batch normalization.
AP [test] AP50 [test] AP [val] AP50 [val]
UCenter-Single 
UCenter-Ensemble 
PANet [test tricks]
Table 10. Results on MVD val subset and test subset.
Results and Ablation Study
We compare with state-ofthe-arts on test subset in Table 8. Trained on “ﬁne-only”
data, our method outperforms Mask R-CNN with “ﬁneonly” data by 5.6 points. It is even comparable with Mask
R-CNN pre-trained on COCO. By pre-training on COCO,
we outperform Mask R-CNN with the same setting by 4.4
points. Visual results are shown in Figure 5.
Our ablation study to analyze the improvement on val
subset is given in Table 9. Based on our re-implemented
baseline, we add multi-GPU synchronized batch normalization to help network converge better. It improves the accuracy by 1.5 points. With our full PANet, the performance is
further boosted by 1.9 points.
4.4. Experiments on MVD
MVD is a relatively new and large-scale dataset for
instance segmentation. It provides 25, 000 images on street
scenes with ﬁne instance-level annotations for 37 semantic
classes. They are captured from several countries using different devices. The content and resolution vary greatly. We
train our model on train subset with ResNet-50 as initial
model and report performance on val and secret test subsets
in terms of AP and AP50.
We present our results in Table 10.
Compared with
UCenter – winner on this dataset in LSUN 2017 instance segmentation challenge, our PANet with one ResNet-
50 tested on single-scale images already performs comparably with the ensemble result with pre-training on COCO.
With multi-scale and horizontal ﬂip testing, which are also
adopted by UCenter, our method performs even better.
Qualitative results are illustrated in Figure 5.
5. Conclusion
We have presented our PANet for instance segmentation. We designed several simple and yet effective components to enhance information propagation in representative pipelines. We pool features from all feature levels and
shorten the distance among lower and topmost feature levels for reliable information passing. Complementary path is
augmented to enrich feature for each proposal. Impressive
results are produced. Our future work will be to extend our
method to video and RGBD data.
Acknowledgements
We would like to thank Yuanhao Zhu, Congliang Xu and
Qingping Fu in SenseTime for technical support.
A. Training Details and Strategy of Generating
Anchors on Cityscapes and MVD.
Cityscapes
hyper-parameters
adopted from Mask R-CNN and described in Section
4.3. RPN anchors span 5 scales and 3 aspect ratios following . While on MVD , we adopt training
hyper-parameters from the winning entry . We train our
model with learning rate 0.02 for 60k iterations and 0.002
for another 20k iterations. We take 16 images in one image
batch for training. We set the longer edge of the input image
to 2400 pixels and the other ranges from 600 to 2000 pixels for multi-scale training. Scales {1600, 1800, 2000} are
adopted for multi-scale testing. The RPN anchors span 7
scales, i.e., {82, 162, 322, 642, 1282, 2562, 5122}, and 5 aspect ratios, i.e., {0.2, 0.5, 1, 2, 5}. RPN is trained with the
same scales as those of object detection/instance segmentation network training.
B. Details on Implementing Multi-GPU Synchronized Batch Normalization.
We implement multi-GPU batch normalization on Caffe
 and OpenMPI. Given n GPUs and training samples in
batch B, we ﬁrst split training samples evenly into n subbatches, each is denoted as bi, assigned to one GPU. On
each GPU, we calculate means µi based on samples in bi.
AllReduce operation is then applied to gather all µi across
all GPUs to get the mean µB of entire batch B. µB is broadcast to all GPUs. We then calculate temporary statistics on
each GPU independently and apply the AllReduce operation to produce the variance σ2
B of entire batch B. σ2
also broadcast to all GPUs. As a result, each GPU has the
statistics calculated on all training samples in B. We then
perform normalization ym = γ xm−µB
B+ϵ + β as in for
each training sample. In backward operations, AllReduce
operation is similarly applied to gather information from all
GPUs for gradient calculation.