Knowledge-Based Systems 263 110273
Contents lists available at ScienceDirect
Knowledge-Based Systems
journal homepage: www.elsevier.com/locate/knosys
Explainable AI (XAI): A systematic meta-survey of current challenges
and future opportunities
Waddah Saeed a,b,∗, Christian Omlin a
a Center for Artificial Intelligence (CAIR), University of Agder, Jon Lilletuns vei 9, Grimstad, 4879, Agder, Norway
b School of Computer Science and Informatics, De Montfort University, Leicester, LE1 9BH, UK
a r t i c l e
Article history:
Received 16 May 2022
Received in revised form 2 January 2023
Accepted 3 January 2023
Available online 11 January 2023
Explainable AI (XAI)
Interpretable AI
Machine learning
Deep learning
Meta-survey
Responsible AI
a b s t r a c t
The past decade has seen significant progress in artificial intelligence (AI), which has resulted in
algorithms being adopted for resolving a variety of problems. However, this success has been met by
increasing model complexity and employing black-box AI models that lack transparency. In response
to this need, Explainable AI (XAI) has been proposed to make AI more transparent and thus advance
the adoption of AI in critical domains. Although there are several reviews of XAI topics in the literature
that have identified challenges and potential research directions of XAI, these challenges and research
directions are scattered. This study, hence, presents a systematic meta-survey of challenges and future
research directions in XAI organized in two themes: (1) general challenges and research directions of
XAI and (2) challenges and research directions of XAI based on machine learning life cycle’s phases:
design, development, and deployment. We believe that our meta-survey contributes to XAI literature
by providing a guide for future exploration in the XAI area.
© 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license
( 
1. Introduction
Artificial intelligence (AI) has undergone significant and continuous progress in the past decade, resulting in the increased
adoption of its algorithms (e.g., machine learning (ML) algorithms) for solving many problems, even those that were difficult
to resolve in the past. However, these outstanding achievements
are accompanied by increasing model complexity and utilizing
black-box AI models that lack transparency. Therefore, it becomes
necessary to come up with solutions that can contribute to addressing such a challenge, which could help expand utilizing AI
systems in critical and sensitive domains (e.g., healthcare and
security domains) where other criteria must be met besides the
high accuracy.
Explainable artificial intelligence (XAI) has been proposed as a
solution that can help to move towards more transparent AI and
thus avoid limiting the adoption of AI in critical domains .
Generally speaking, according to , XAI focuses on developing
explainable techniques that empower end-users in comprehending, trusting, and efficiently managing the new age of AI systems.
Historically, the need for explanations dates back to the early
works in explaining expert systems and Bayesian networks .
Corresponding author at: School of Computer Science and Informatics, De
Montfort University, Leicester, LE1 9BH, UK.
E-mail addresses:
 (W. Saeed),
 (C. Omlin).
Deep learning (DL), however, has made XAI a thriving research
Every year, a large number of studies dealing with XAI are
published. At the same time, various review studies are published
covering a range of general or specific aspects of XAI. With many
of these review studies, several challenges and research directions
are discussed. While this has led to identifying challenges and
potential research directions, however, they are scattered.
To the best of our knowledge, this is the first meta-survey that
explicitly organizes and reports on the challenges and potential
research directions of XAI. This meta-survey aims to provide a reference point for researchers interested in working on challenges
and potential research directions in XAI.
The organization of the paper is as follows (also shown in
Fig. 1). In Section 2, we discuss the need for XAI from various
perspectives. Following that, Section 3 tries to contribute to a
better distinction between explainability and interpretability. The
protocol used in planning and executing this systematic metasurvey is presented in Section 4. Afterward, Section 5 discusses
the challenges and research directions of XAI. Section 6 shows
how some of the discussed challenges and research directions
can be considered in medicine (which can also be tailored to any
other domains). Lastly, final remarks are highlighted in Section 7
2. Why explainable AI is needed?
Nowadays, we are surrounded by black-box AI systems utilized to make decisions for us, as in autonomous vehicles, social
 
0950-7051/© 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license ( 
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
Fig. 1. The organization of this meta-survey paper.
networks, and medical systems. Most of these decisions are taken
without knowing the reasons behind these decisions.
According to , not all black-box AI systems need to explain
why they take each decision because this could result in many
consequences such as reducing systems efficiency and increasing development costs. Generally, explainability/interpretability
is not needed in two situations : (1) results that are unacceptable are not accompanied by severe consequences, (2) the
problem has been studied in-depth and well-tested in practice,
so the decision made by the black-box system is trustworthy,
e.g., advertisement system and postal code sorting. Therefore, we
should think about why and when explanations/interpretations
can be helpful .
Based on the retrieved surveys in this work, the need for XAI
can be discussed from various perspectives as shown in Fig. 2. The
perspective groups below are to some extent based on the work
• Regulatory perspective: Black-box AI systems are being utilized in many areas of our daily lives, which could be resulting in unacceptable decisions, especially those that may
lead to legal effects. Thus, it poses a new challenge for the
legislation. The European Union’s General Data Protection
Regulation (GDPR)1 is an example of why XAI is needed
from a regulatory perspective. These regulations create what
is called the ‘‘right to explanation,’’ by which a user is
entitled to request an explanation about the decision made
by the algorithm that considerably influences them . For
example, if an AI system rejects one’s application for a loan,
the applicant is entitled to request justifications behind that
decision to guarantee it is in agreement with other laws
and regulations . However, the implementation of such
regulations is not straightforward, challenging, and without an enabling technology that can provide explanations,
the ‘‘right to explanation’’ is nothing more than a ‘‘dead
letter’’ .
1 
• Scientific perspective: When building black-box AI models,
we aim to develop an approximate function to address the
given problem. Therefore, after creating the black-box AI
model, the created model represents the basis of knowledge, rather than the data . Based on that, XAI can be
helpful to reveal the scientific knowledge extracted by the
black-box AI models, which could lead to discovering novel
concepts in various branches of science.
• Industrial perspective: Regulations and user distrust in
black-box AI systems represent challenges to the industry in applying complex and accurate black-box AI systems . Less accurate models that are more interpretable
may be preferred in the industry because of regulation
reasons . A major advantage of XAI is that it can help
in mitigating the common trade-off between model interpretability and performance , thus meeting these common challenges. However, it can increase development and
deployment costs.
• Model’s developmental perspective: Several reasons could
contribute to inappropriate results for black-box AI systems, such as limited training data, biased training data,
outliers, adversarial data, and model overfitting. Therefore,
what black-box AI systems have learned and why they make
decisions need to be understood, primarily when they affect humans’ lives. For that, the aim will be to use XAI to
understand, debug, and improve the black-box AI system to
enhance its robustness, increase safety and user trust, and
minimize or prevent faulty behavior, bias, unfairness, and
discrimination . Furthermore, when comparing models
with similar performance, XAI can help in the selection
by revealing the features that the models used to produce
their decisions . In addition, XAI can serve as a proxy
function for the ultimate goal because the algorithm may be
optimized for an incomplete objective . For instance, optimizing an AI system for cholesterol control with ignoring
the likelihood of adherence .
• End-user and social perspectives: In the literature of deep
learning , it has been shown that altering an image
such that humans cannot observe the change can lead the
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
Fig. 2. The five main perspectives for the need for XAI.
model in producing a wrong class label. On the contrary,
completely unrecognizable images to humans can be recognizable with high confidence using DL models . Such
findings could raise doubts about trusting such black-box
AI models . The possibility to produce unfair decisions
is another concern about black-box AI systems. This could
happen in case black-box AI systems are developed using
data that may exhibit human biases and prejudices .
Therefore, producing explanations and enhancing the interpretability of the black-box AI systems will help in increasing trust because it will be possible to understand the
rationale behind the model’s decisions, and we can know if
the system serves what it is designed for instead of what
it was trained for . Furthermore, the demand for the
fairness of black-box AI systems’ decisions, which cannot
be ensured by error measures, often leads to the need for
interpretable models .
The above list is far from complete, and there may be an
overlap between these perspectives. However, we believe that
these perspectives highlight the most critical reasons why XAI is
3. From explainability to interpretability
In the literature, there seems to be no agreement on what
‘‘explainability’’ or ‘‘interpretability’’ mean. While both terms are
often used interchangeably in the literature, some examples from
the selected papers distinguish them . To show that
there is no agreement upon definitions, three different definitions
from are provided. In , the authors stated that ‘‘...
we consider interpretability a property related to an explanation and
explainability a broader concept referring to all actions to explain.’’.
In another work , interpretability is defined as ‘‘the ability
to explain or to provide the meaning in understandable terms to
a human.’’, while ‘‘explainability is associated with the notion of
explanation as an interface between humans and a decision-maker
that is, at the same time, both an accurate proxy of the decisionmaker and comprehensible to humans...’’. Another distinction is
drawn in , in which authors stated that ‘‘... In the case of
interpretation, abstract concepts are translated into insights useful
for domain knowledge (for example, identifying correlations between
layers in a neural network for language analysis and linguistic
knowledge). An explanation provides information that gives insights
to users as to how a model came to a decision or interpretation.’’.
It can be noticed from these distinctions that the authors have
different definitions for these two terms. In addition, there is still
considerable ambiguity in some of the given distinctions.
To contribute to a distinction between explainability and interpretability, this paper attempts to present a distinction between these terms as follows:
Explainability provides insights to a targeted audience to fulfill a need, whereas interpretability is the degree to which the
provided insights can make sense for the targeted audience’s
domain knowledge.
There are three components in the definition of explainability,
as shown in the above distinction: insights, targeted audience,
and need. Insights are the output from explainability techniques
used (e.g., text explanation, feature relevance, local explanation).
These insights are provided to a targeted audience such as domain experts (e.g., medical doctors), end-users (e.g., users affected by the model decision), and modeling experts (e.g., data
scientists). The need for the provided insights may be to handle
any issues discussed in Section 2 such as justifying decisions,
discovering new knowledge, improving the black-box AI model,
and ensuring fair decisions. That means explainability aims to
help the targeted audience to fulfill a need based on the provided
insights from the explainability techniques used.
As for interpretability, are the provided explanations consistent with the targeted audience’s knowledge? Do the explanations make sense to the targeted audience? Is the targeted
audience able to reason/infer to support decision-making? Are
the provided explanations reasonable for the model’s decision?
Although the distinction is not ideal, we believe that it represents an initial step towards understanding the difference between explainability and interpretability. Because the interpretability definition cannot be generalized , it is crucial to take
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
The selected databases and a search engine.
Database/search engine
 
Web of Science
 
Science Direct
 
IEEEXplore
 
Springer Link
 
Association for Computing Machinery Digital Library (ACM)
 
Google Scholar (Search engine)
 
 
into account the problem domain and user type when
measuring interpretability properties. Establishing formalized rigorous evaluation metrics is one of the challenges in XAI, which
has been discussed in Towards more formalism section. This
paper will use this proposed distinction when discussing the
challenges and research directions in XAI.
4. Systematic review planning and execution
This work is mainly based on a systematic literature review
(SLR) introduced by Kitchenham and Charters . SLR is used to
identify all relevant papers that can help in answering a specified
research question in an unbiased manner. SLR has been used in
many published works, for example, the works in . We
started our SLR by specifying the following research question:
What are the challenges and research directions in XAI reported in
the existing survey studies? The answer to this question will help
researchers and practitioners to know the various dimensions
that one can consider when working in the XAI research area.
Having the research question established, the search terms
based on the research question are:
• XAI keywords: explainable, XAI, interpretable.
• Review keywords: survey, review, overview, literature, bibliometric, challenge, prospect, agenda, trend, insight, opportunity, lesson, research direction
With these selected search terms, Boolean ANDs were used between groups and ORs within groups to construct search strings
as follows: (explainable OR XAI OR interpretable) AND (survey OR
review OR overview OR literature OR bibliometric OR challenge
OR prospect OR agenda OR trend OR insight OR opportunity OR
lesson OR ‘‘research direction’’).
Relevant and important electronic databases and a search
engine were used for searching the primary studies based on the
search terms. These databases and the search engine are shown in
Table 1. The search using these databases and the search engine
was based on different search schemes as shown in Table 2. That
was adapted depending on the needs of these databases and the
search engine. We retrieved papers published before 1 September
Inclusion and exclusion criteria were used to select or discard
the retrieved studies. The inclusion criteria are the following:
• The study presents a survey of explainable AI.
• The study presents challenges and/or research directions for
On the other hand, the exclusion criteria are the following:
• The study is not written in English.
• The study presents a survey of XAI without discussing any
challenges or research directions.
After obtaining search results, all studies were analyzed individually by the first author to assess their relevance in the context
of this SLR considering the inclusion and exclusion criteria. These
Search schemes for the databases and search engine used along with the number
of retrieved papers.
Database/search engine
Search scheme
Web of Science
Science Direct
IEEEXplore
Springer Link
Google Scholar
aIt is not possible to use all keywords in one search string. Therefore, the count
of retrieved papers is not correct as some papers appeared more than one time.
bThe first 20 pages.
Distribution of selected papers per year.
Number of papers
aPublished before 1 September 2021.
studies were first analyzed by their titles and abstracts to decide
if the paper matched the first inclusion criterion. If matched, the
paper was analyzed in detail in the second step. In the second
step, the exclusion criteria and the second inclusion criterion
were checked. Throughout the work on this SLR, the authors met
regularly to discuss what has been done and what needs to be
done next.
We reviewed the list of references of the selected studies
to include other papers that may not be retrieved from the
selected electronic databases, which resulted in retrieving eight
non-survey papers that reported challenges and/or research directions in XAI . It is good to note that each
arXiv paper was only included if it did not have a peer-reviewed
version, otherwise, the peer-reviewed version was included.
Overall, the total number of selected papers is 73, as shown
in Table 3. As shown in Fig. 3, the primary outlet for the selected
papers is journal articles followed by conference papers and arXiv
papers. The distribution of the selected papers per publisher is
shown in Fig. 4.
5. Discussion
To our best knowledge, there are two meta-survey papers
on XAI that primarily used survey papers as a basis for their
discussions. The first meta-survey focused its discussion on the
visual interpretation of ML models using 15 survey papers and 3
non-survey papers published between 2014–2018 . The second meta-survey paper included over 70 survey papers published up to the
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
Fig. 3. Distribution of selected papers per publication type.
Fig. 4. Distribution of selected papers per publisher.
beginning of 2021 with a focus on XAI and XAI methods. In our
survey, we included 73 papers that covered a range of general and
specific aspects of XAI with a focus on challenges and research
directions in XAI reported in these included papers.
There are various taxonomies proposed for XAI which mainly
depend on the aspects discussed in the papers. For example, the
authors of proposed a taxonomy that has three main components: explainability problem definition, explanator properties,
and metrics. In , the taxonomy is based on the explainability/interpretability methods (i.e., ad-hoc and post-hoc methods).
A three dimensions taxonomy based on interpretability intervention (passive vs. active) methods, type of explanations, and input
space (global vs. local) was used in , and a taxonomy based on
explanation forms (visual, text, graph, and symbol explanations)
was used in .
In order to place the challenges and research directions in
XAI in a meaningful context, the discussion in our meta-survey
is based on a taxonomy based on two themes, as shown in Fig. 5.
The first theme focuses on the general challenges and research
directions in XAI and the second theme is about the challenges
and research directions of XAI based on the ML life cycle’s phases.
Before stating the reason for using ML life cycle’s phases in our
taxonomy, it is important to highlight that ML models are datadriven models so they learn through experience (i.e., data). If
training data contain biases and/or specific design choices can
result in biased behavior in ML algorithms, ML models can produce biased outcomes and these biased outcomes can affect users’
decisions and these decisions can result in more biased data that
could be used to train other ML algorithms . That means bias
can occur along all phases of the ML pipeline. Since XAI is meant
to detect and prevent or at least mitigate bias, we categorized the
challenges and research directions in XAI based on ML life cycle’s
For simplicity, we divided the life cycle into three main phases:
design, development, and deployment phases. It is good to note
that the reported challenges and research directions were sorted
from the most commonly reported in the selected papers to the
least. The following subsections shed light on these challenges
and research directions.
5.1. General challenges and research directions in XAI
In this section, we reported XAI’s general challenges and
research directions. These general challenges and research directions in XAI are shown in Fig. 6. The selected papers that
discussed these general challenges and research directions are
mentioned in Table 4. As shown in Fig. 7, the majority of these
challenges and research directions have received much attention
recently .
5.1.1. Towards more formalism
It is one of the most raised challenges in the literature of
XAI . It was suggested that more
formalism should be considered in terms of systematic definitions, abstraction, and formalizing and quantifying .
Starting from the need for systematic definitions, until now,
there is no agreement on what an explanation is . Furthermore, it has been found that similar or identical concepts are
called by different names and different concepts are called by the
same names . In addition, without a satisfying definition of
interpretability, how it is possible to determine if a new approach
better explains ML models ? Therefore, to facilitate easier
sharing of results and information, definitions must be agreed
upon .
With regards to abstraction, many works have been proposed
in an isolated way; thus, there is a need to be consolidated to
build generic explainable frameworks that would guide the development of end-to-end explainable approaches . Additionally,
taking advantage of the abstraction explanations in identifying
properties and generating hypotheses about data-generating processes (e.g., causal relationships) could be essential for future
artificial general intelligence (AGI) systems .
Regarding the formalization and quantification of explanations, it was highlighted in that some current works focus
on a detailed problem formulation which becomes irrelevant as
the method of interpretation or the explanation differs. Therefore, regardless of components that may differ, the expansibility
problem must be generalized and formulated rigorously, and this
will improve the state-of-the-art for identifying, classifying, and
evaluating sub-issues of explainability . The work in also
stressed the need for a thorough formalization and theoretical understanding of XAI to answer important and unanswered
theoretical questions such as the weighing of model and data
distribution into the generated explanation.
Establishing formalized rigorous evaluation metrics needs to
be considered as well . However, due to the absence of an
agreement on the definitions of interpretability/explainability,
no established approach exists to evaluate XAI results . The
lack of ground truth explanations in most cases is the biggest
challenge for rigorous evaluations , and because of that
and other factors such as the cost of predictability or run-time
efficiency, the question of what is the optimal explanation remains an open question . So far, different evaluation metrics have been proposed, such as reliability, trustworthiness,
usefulness, soundness, completeness, compactness, comprehensibility, human-friendly or human-centered, correctness or fidelity, complexity, and generalizability . However, it seems
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
Fig. 5. The proposed organization to discuss the challenges and research directions in XAI. For simplicity, the arrows that show the flow in the life cycle are removed.
Fig. 6. General Challenges and Research Directions in XAI.
A summary of the selected papers for the general challenges and research directions in XAI.
Challenges and research directions
Towards more formalism
 , a
Explanations and the nature of user experience and expertise
 , a
XAI for trustworthiness AI
 , a
Multidisciplinary research collaborations
 , a
Interpretability vs. performance trade-off
 
XAI for non-image, non-text, and heterogeneous data
 , a
Explainability methods composition
 , a
Causal explanations
 
Challenges in the existing XAI models/methods
 a
Contrastive and counterfactual explanations
 , a
Communicating uncertainties
Time constraints
 , a
Natural language generation
Analyzing assumption-free black-box models, not assumption-based data models
Reproducibility
The economics of explanations
aA non-peer-reviewed paper from arXiv.
that there are two main evaluation metrics groups: objective
and human-centered evaluations . The former is quantifiable
mathematical metrics, and the latter relies on user studies .
Further progress is needed towards evaluating XAI techniques’
performance and establishing objective metrics for evaluating XAI
approaches in different contexts, models, and applications .
Recently, the work in suggested that further research should
be undertaken in objective and human-centered evaluations.
For human-centered evaluations, looking into effective designs
for human experiments and subjective explanation evaluation
measures can help establish agreed criteria on human-centered
evaluations thus making comparisons between explanations easier. As for objective evaluations, it seems that previous work has
mainly focused on attribution-based explanations (e.g., feature
importance). Therefore, considering other types of explanations is
needed (e.g., example-based explanations). Additionally, there is
a need to come up with more objective evaluations that measure
explainability properties of clarity and broadness of interpretability as the current focus is on the evaluation of the soundness
of fidelity of explanations. Finally, it has been suggested by the
authors the need to integrate both human-centered and objective
evaluations for a comprehensive evaluation as well as understand the contribution each metric makes in this comprehensive
evaluation. Following the evaluation of the explanations, the recommended explanation is shown based on the task and the type
of user, which is ultimately best to build using a model-agnostic
framework .
5.1.2. Explanations and the nature of user experience and expertise
Based on the nature of the application, users who use ML
models can vary (e.g., data scientists, domain experts, decisionmakers, and non-experts). The nature of user experience and
expertise matters in terms of what kind of cognitive chunks they
possess and the complexity they expect in their explanations .
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
Fig. 7. Number of research papers per year for each general challenge and research direction. Method: Publication year was used for peer-reviewed papers published
before 2022, otherwise, arXiv’s publication year was considered. As can be seen from this figure, the majority of these challenges and research directions have
received much attention recently .
In general, users have varying backgrounds, knowledge, and communication styles . However, it seems that the current focus
of explanation methods is tailored to users who can interpret the
explanations based on their knowledge of the ML process .
The works in have highlighted what is
needed to be considered with regards to explanations and the
nature of user experience and expertise. In , user-friendly
explanations have been suggested so users can interpret the explanations with less technical knowledge. Therefore, figuring out
what to explain should follow the identification of the end-user.
In , it has been highlighted that previous works in explainable
AI systems (e.g., expert systems) generally neglected to take
into account the knowledge, goals, skills, and abilities of users.
Additionally, the goals of users, systems, and explanations were
not clearly defined. Therefore, clearly stating goals and purposes
are needed to foster explanation testing within the appropriate
context. In , the authors have discussed that identifying the
users’ goals and keeping up with their dynamic nature means
collecting more data from them. It is also essential to develop
change detection approaches to goals and needs for the purpose
of adapting these changes to end-users. For a deeper understanding of these dynamics, user studies (e.g., diary studies, interviews,
and observation) can help develop guidelines for developing longterm explainable systems and determining which user data to
gather to improve personalization .
In , it has been suggested that abstraction can be used
to simplify the explanations. Understanding how abstractions are
discovered and shared in learning and explanation is an essential
part of the current XAI research. The work in has mentioned that the inclusion of end-users in the design of black-box
AI models is essential, especially for specific domains, e.g., the
medical domain. That would help to understand better how the
end-users will use the outputs and interpret explanations. It is
a way to educate them about the predictions and explanations
produced by the system. In , the authors have discussed
that utilizing users’ previous knowledge is a significant challenge
for visualization tools today. Customizing visualization tools for
different user types can be useful at several stages of the ML
model pipeline . However, to use prior users’ knowledge
in predictive models, it is important to establish processes to
digitally capture and quantify their prior knowledge .
In , it has been mentioned that DL models often use
concepts that are unintelligible to predict outcomes. Therefore,
using systems that use such models requires human-centric explanations that can accurately explain a decision and make sense
to the users (e.g., medical domain expert) . An approach to
come up with human-centric explanations is examining the role
of human-understandable concepts acquired by DL models .
It is also essential to analyze the features used by the DL models
in predicting correct decisions based on incorrect reasoning .
Having an understanding of the model’s concepts would help
reduce reliability concerns and develop trust when deploying the
system, especially in critical applications . The authors also
highlighted the importance of addressing the domain-specific
needs of specific applications and their users when developing
XAI methods. Finally, the work in has discussed that XAI
can facilitate the process of explaining to non-experts how a
model reached a given decision, which can substantially increase
information exchange among heterogeneous people regarding
the knowledge learned by models, especially when working in
projects with a multi-disciplinary team.
To sum up, it is crucial to tailor explanations based on user
experience and expertise. Explanations should be provided differently to different users in different contexts . In addition, it is
also essential to clearly define the goals of users, systems, and explanations. Stakeholder engagement and system design are both
required to understand which explanation type is needed .
5.1.3. XAI for trustworthiness AI
Increasing the use of AI in everyday life applications will
increase the need for AI trustworthiness, especially in situations
where undesirable decisions may have severe consequences .
The High-Level Expert Group in European Commission put seven
essentials for achieving trustworthy AI2: (1)human agency and
oversight; (2) robustness and safety; (3) privacy and data governance; (4) transparency; (5) diversity, non-discrimination, and
fairness; (6) societal and environmental well-being; and (7) accountability. The discussion about privacy, security, and safety is
given in XAI and Privacy Section, XAI and Security Section, and
XAI and Safety Section, respectively. The discussion in this section
is about what is reported in the selected papers regarding fairness
and accountability.
2 
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
With regard to fairness, ML algorithms must not be biased
or discriminatory in the decisions they provide. However, with
the increased usage of ML techniques, new ethical, policy, and
legal challenges have also emerged, for example, the risk of
unintentionally encoding bias into ML decisions . Meanwhile,
the opaque nature of data mining processes and the complexity
of ML models make it more challenging to justify consequential
decisions of following what the models say . The work in 
argues that data, algorithmic, and social biases need to be remedied in order to promote fairness. Further, it is imperative to be
able to analyze AI systems to have trust in the model and its
predictions, especially for some critical applications. Researchers
started trying to form a definition of fairness and the meaning of fairness in an algorithm as discussed in . According
to , it would also be necessary to devise new techniques for
discrimination-aware data mining. It is also worth noting that
when converting fairness into a computational problem, we need
to keep the fairness measures fair . The work in states
that it is possible to visualize learned features using XAI methods
and assess bias using methods other than explanation methods.
On the other hand, regulations and laws are necessary for the
suspicion about unfair outcomes .
Turning now to accountability, having accountability means
having someone responsible for the results of AI decisions if harm
occurs. In , it has been mentioned that investigating and
appealing decisions with major consequences for people is an
important aspect of systems of accountability, and some current
regulations also aim to achieve this. XAI can be an important
factor in systems of accountability by providing users with the
means to appeal a decision or modify their behavior in the future
to achieve a better result. However, more work should be done to
establish an environment that promotes individual autonomy and
establish a system of accountability. It has also been discussed
in that developing procedures for testing AI algorithms for
policy compliance is necessary so that we can establish whether
or not a given algorithm adheres to a specific policy without
revealing its proprietary information. It is also desirable for a
model to specify its purposes and provide external verification
of whether these goals are met and, if not, describe the causes of
the predicted outcomes.
The use of XAI can enhance understanding, increase trust, and
uncover potential risks . Therefore, when designing XAI techniques, it is imperative to maintain fairness, accountability, and
transparency . On the other hand, it is necessary to highlight
that not only black-box AI models are vulnerable to adversarial
attacks, but also XAI approaches . There is also a risk that
to promote trust in black-box AI models predictions; explainers
may be more persuasive but misleading than informative, so
users may become deceived, thinking the system to be trustworthy . It is possible to increase trust through explanations,
but explanations do not always produce systems that produce
trustworthy outputs or ensure that system implementers make
trustworthy claims about their abilities .
The work in discusses measures to create trustworthy
AI. It has been highlighted that before employing AI systems
in practice, it is essential to have quantitative proxy metrics
to assess explanation quality objectively, compare explanation
methods, and complement them with human evaluation methods
(e.g., data quality reporting, extensive testing, and regulation).
Finally, it is good to note that a further explore the idea of
Responsible AI with a discussion about principles of AI, fairness,
privacy, and data fusion can be found in .
5.1.4. Multidisciplinary research collaborations
One area of research that can offer new insights for explainable methods is working closely with researchers from other
disciplines such as psychology, behavioral and social sciences,
human–computer interaction, physics, and neuroscience. Multidisciplinary research is therefore imperative to promote humancentric AI and expand utilizing XAI in critical applications .
For example, in healthcare , military , common law ,
and transportation .
Several studies, for instance , have suggested some potential multidisciplinary research works. In , it
has been highlighted that approaching the psychology discipline
can help to get insights into both the structure and the attributes
of explanations and the way they can influence humans. They
also have suggested that defining the context of explanations is
an important research direction. Here, it is essential to consider
the domain of the application, the users, the type of explanations (e.g., textual, visual, combinations of solutions), and how
to provide the explanations to the users. This research direction
can form a connection with behavioral and social sciences. The
paper in also has shown that XAI can benefit from the work
in philosophy, cognitive psychology/science, and social psychology. The paper summarizes some findings and suggests ways to
incorporate these findings into work on XAI.
Approaching Human–Computer Interaction (HCI) studies are
essential to XAI. However, few user experiments have been conducted in the area of explainability . Therefore, more should
be conducted to study the topic adequately . Humans must
be included in the process of creating and utilizing XAI models,
as well as enhancing their interpretability/explainability .
In , it has been highlighted that interactive tools may help
users understand, test, and engage with AI algorithms, thereby
developing new approaches that can improve algorithms’ explainability. Furthermore, interactive techniques can help users to
interpret predictions and hypothesis-test users’ intuitions rather
than relying solely upon algorithms to explain things to them.
In , it has been suggested to draw from the HCI research
on interaction design and software learnability to improve the
usability of intelligible or explainable interfaces. Additionally, HCI
researchers can take advantage of the theoretical work on the
cognitive psychology of explanations to make understandable
explanations. They can also empirically evaluate the effectiveness
of new explanation interfaces.
The advances in neuroscience should be of great benefit to
the development and interpretation of DL techniques (e.g., cost
function, optimization algorithm, and bio-plausible architectural
design) owing to the close relationship between biological and
neural networks . It is imperative to learn from biological neural networks so that better and explainable neural network architectures can be designed . Finally, connecting with
physics and other disciplines that have a history of explainable visual methods might provide new insights for explainable
methods .
5.1.5. Interpretability vs. performance trade-off
The belief that complicated models provide more accurate
outcomes is not necessarily correct . However, this can be
incorrect in cases when the given data is structured and with
meaningful features . In a situation where the function being
approximated is complex, that the given data is widely distributed among suitable values for each variable and the given
data is sufficient to generate a complex model, the statement
‘‘models that are more complex are more accurate’’ can be true .
In such a situation, the trade-off between interpretability and
performance becomes apparent .
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
When the performance is coupled with model complexity,
model interpretability is in question . Explainability techniques, however, could help in minimizing the trade-off . However, according to , what determines this trade-off? and who
determines it? The authors have highlighted the importance of
discussing with end-users this trade-off so that they can be aware
of the potential risks of misclassification or opacity. Another point
that should be considered is the approximation dilemma: models
need to be explained in enough detail and in a way that matches
the audience for whom they are intended while keeping in mind
that explanations reflect the model and do not oversimplify its
essential features . Even though studying the trade-off is essential, it is impossible to proceed without standardized metrics
for assessing the quality of explanations .
Another possible solution for the trade-off is suggested in 
which is developing fully transparent models throughout the
entire process of creation, exploitation, and exploration and can
provide local and global explanations. In turn, this leads to using
methods that embed learning capabilities to develop accurate
models and representations . The methods should also be able
to describe these representations in effective natural language
consistent with human understanding and reasoning .
5.1.6. Explainability methods composition
For specific applications in healthcare (e.g., predicting disease
progression), several types of explanations at different levels
are needed (e.g., local and global explanations) in order to
provide the most complete and diverse explanations we can .
This is derived from the way clinicians communicate decisions
utilizing visualizations and temporal coherence as well as textual
descriptions .
Some overlap exists between explainability methods, but for
the most part, each seems to address a different question .
According to , combining various methods to obtain more
powerful explanations is rarely considered. In addition, rather
than using disparate methods separately, we should investigate
how we can use them as basic components that can be linked and
synergized to develop innovative technologies . Furthermore,
it could help to provide answers in a simple human interpretable
language . First efforts, as cited in , have been made as
in where the authors proposed a model that can provide
visual relevance and textual explanations. Recently, the work
in proposed a new framework with multi-modal explanations component derived based on a wide range of images and
vocabulary. In this model, the generated textual explanations are
paired with their corresponding visual regions in the image. As
a result of this explanation generation method and other components in the framework, better reasoning and interpretability
were achieved than in some state-of-the-art models. These findings suggest opportunities for future research with the aim to
enhance both interpretability and accuracy .
5.1.7. Causal explanations
Developing causal explanations for AI algorithms (i.e., why
they made those predictions instead of how they arrived at those
predictions) can help increase human understanding . In addition, causal explanations strengthen models’ resistance to adversarial attacks, and they gain more value when they become
part of decision-making . However, there can be conflicts
between predicting performance and causality . For example,
when the confounder, which is a variable that influences both the
dependent variable and independent variable, is missing from the
model .
Causal explanations are anticipated to be the next frontier of
ML research and to become an essential part of the XAI literature . There is a need for further research to determine
when causal explanations can be made from an ML model .
In addition, according to a recent survey on causal interpretability
for ML , it has been highlighted the absence of ground truth
data for causal explanations and verification of causal relationships makes evaluating causal interpretability more challenging.
Therefore, more research is needed to guide on how to evaluate
causal interpretability models .
5.1.8. XAI for non-image, non-text, and heterogeneous data
The focus of XAI works is mainly on image and text data
(e.g., Natural language generation and Interpretability for natural
language processing sections for text explanations and Developing visual analytics approaches for advanced DL architectures for
visual explanations). Other data types exist, however, but they
have received less attention , such as time-series ,
graphs , and spatio-temporal data .
Using visualization to transform non-image data into images
creates opportunities to discover explanations through salient
pixels and features . However, this should not be the only way
for explainability for non-image or non-text data for different reasons. For example, existing explanation approaches for image or
text data need to be adjusted to be used with graph data , and
the outcomes which are clearly interpretable from explanation
approaches for images (e.g., saliency maps) might need expert
knowledge to be understood for time series data .
Additionally, there is a need to develop new approaches for
explaining the information that exists with non-image or nontext data, e.g., structural information for graph data and
multivariate time series data of variable length .
Finally, with the advent of AI systems that use various types
of data, explainability approaches that can handle such heterogeneity of information are more promising . For example,
such systems can simulate clinicians’ diagnostic processes in the
medical domain where both images and physical parameters are
utilized to make decisions . Thus, they can enhance the diagnostic effectiveness of the systems as well as explain phenomena
more thoroughly .
5.1.9. Challenges in the existing XAI models/methods
There are some challenges in the existing XAI models/methods
that have been discussed in the literature. Starting with scalability, which is a challenge that exists in explainable models as
discussed in . For example, each case requiring an explanation
entails creating a local model using LIME explainable model .
Scalability can be an issue when there is a huge number of
cases for which prediction and explanation are needed. Likewise,
when computing Shapley values , all combinations of variables must be considered when computing variable contributions.
Therefore, such computations can be costly for problems that
have lots of variables.
Feature dependence presents problems in attribution and extrapolation . If features are correlated, attribution of importance and features effects becomes challenging. For sensitivity
analyses that permute features, when the permuted feature has
some dependence on another feature, the association breaks, resulting in data points outside the distribution, which could cause
misleading explanations. In , the authors discussed some limitations with heatmaps explanations. Heatmaps explanations visualize what features are relevant for making predictions. There is,
however, a lack of clarity regarding their relationship (e.g., their
importance for the predictions either individually or in combination). Low abstraction levels of explanations are another limitation. Heatmaps highlight that specific pixels are significant
without indicating how the relevance values relate to abstract
concepts in the image, such as objects or scenes. The model’s
behavior can be explained in more abstract, more easily understood ways by meta-explanations that combine evidence from
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
low-level heatmaps. Therefore, further research is needed on
meta-explanations.
Model-based (i.e., ante-hoc models) and post-hoc explainability models have some challenges, as have been discussed in .
When model-based methods cannot predict with reasonable accuracy, practitioners start the search for more accurate models.
Therefore, one way to increase the usage of model-based methods
is to develop new modeling methods that maintain the model’s
interpretability and render more accurate predictions . The
availability of such methods is needed and useful especially when
implemented in clinical applications to benefit from the predictive power of advanced DL models and interpretability .
More details about this direction are provided in . Further,
according to , for model-based methods, there is a need to
develop more tools for feature engineering. Simpler but accurate
model-based methods can be built when the input features used
with these methods are more informative and meaningful. Two
categories of works can help achieve that: improve tools for
exploratory data analysis and improve unsupervised techniques.
The former helps to understand the data, and domain knowledge
could help to identify helpful features. The latter is needed because unsupervised techniques are often used to identify relevant
structures automatically, so advances in unsupervised techniques
may result in better features.
The authors in have also discussed some challenges for
post-hoc explainability models. According to the authors, it is
challenging to determine what format or combination of formats
will adequately describe the model’s behavior. Furthermore, there
is uncertainty over whether the current explanation methods
are adequate to capture a model’s behavior or if novel methods
are still needed. Another challenge is if post-hoc explanations
methods identify learned relationships by the model that practitioners know to be incorrect, is it possible that practitioners fix
these relationships learned and increase the predictive accuracy?
Further research in post-hoc explanations can help exploit prior
knowledge to improve the predictive accuracy of the models.
Finally, some research directions have been suggested in 
to deal with some challenges associated with perturbation-based
methods. For example, the need to find an optimal scope of
perturbations of the inputs as sampling all perturbations is not
possible (i.e., combinatorial complexity explosion problem). The
development of cross-domain applications of perturbations methods would also benefit from empirical studies comparing perturbations to different data types .
5.1.10. Contrastive and counterfactual explanations
Contrastive explanations describe why one event occurred but
not another, while counterfactual explanations describe what is
needed to produce a contrastive output with minimal changes
in the input . Questions in the contrastive form ‘‘Why x and
not y?’’ and questions of the counterfactual form ‘‘What if?’’ and
‘‘What would happen if?’’ .
In a recent survey of contrastive and counterfactual explanations , it has been found that contrastive and counterfactual
explanations help improve the interaction between humans and
machines and personalize the explanation of algorithms. A further important point as observed by that one of the significant barriers towards a fair assessment of new frameworks is
the lack of standardization of evaluation methods. The theoretical
frameworks are also found inadequate for applying to XAI as a
result of the disconnect between the philosophical accounts of
counterfactual explanation to scientific modeling as well as MLrelated concepts. Furthermore, it has been found that different
domains of science define counterfactual explanations differently,
as do the approaches used to solve specific tasks.
In the light of possible research directions on this point, it
has been suggested in the importance of including endusers in the evaluation of generated explanations since these
explanations are designed to be user-oriented. In addition, since
contrastive and counterfactual explanations address causal and
non-causal relationships, new horizons open to the XAI community by unifying causal and non-causal explanatory engines
within a contractually-driven framework. Furthermore, bringing together researchers from the humanities and the computational sciences could contribute to the further development
of contrastive and counterfactual explanations generation. The
work in also highlighted that some existing contrastive explanation models cannot be applied to visual reasoning tasks.
Visual reasoning involves solving problems about visual information . Therefore, researchers in this area may contribute
to finding answers for the reason(s) behind the why-not question and how to generate text or visual explanations for visual
reasoning tasks .
5.1.11. Communicating uncertainties
Communicating uncertainty is an important research direction
because it can help to inform the users about the underlying
uncertainties in the model and explanations. According to ,
there are already inherent uncertainties in ML models; and model
refinement efforts by developers may introduce new uncertainties (e.g., overfitting). Furthermore, some explanation methods
such as permutation feature importance and Shapley value give
explanations without measuring the uncertainty implied by the
explanations .
Quantifying uncertainty is an open research topic . However, some works exist towards quantifying uncertainty in areas
such as feature importance, layer-wise relevance propagation,
and Shapley values . For example, the work in aimed at
quantifying the importance of a variable averaging across the entire population of interest by introducing a method for estimating
the Shapley population variable importance measure. The obtained results in a simulation study showed that the method has
good finite sample performance. Additionally, the results from
an in-hospital mortality prediction task showed that the method
yielded similar estimates of variable importance using different
machine learning algorithms. Recently, the SHApley eFfects via
random Forests (SHAFF) was proposed to estimate Shapley effects
for measuring variable importance based on the random forests
algorithm . Unlike Monte-Carlo sampling and training one
model for each selected subset of variable in , SHAFF improved Monte-Carlo sampling by utilizing importance sampling
as a means to focus on the most relevant subsets of variables
identified by the forest. Additionally, SHAFF allows fast and accurate estimates of the conditional expectations for any variable
subset because of the utilized projected random forest algorithm.
Through several experiments, it was found that SHAFF offers
practical performance improvements over many existing Shapley
algorithms.
The uncertainty surrounding ML models can take many forms
and occur throughout the ML life phases . Therefore, in order to make progress, it is needed to become more rigorous in
studying and reliably quantifying uncertainties at the model’s
various phases and with the explanation methods and communicate them to the users, then users can respond accordingly [24,
5.1.12. Time constraints
Time is an essential factor in producing explanations and in
interpretations. Some explanations must be produced promptly
to let the user react to the decision . Producing explanations
efficiently can save computing resources, thereby making it useful
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
for industrial use or in environments with limited computing capability . In some situations (e.g., plant operation application),
the provided explanations need to be understood quickly to help
the end-user to make a decision . On the other hand, in some
situations (e.g., scientific applications), users would likely be willing to devote considerable time to understanding the provided
explanation . Therefore, time is an essential factor considering
the situation, available resources, and end-users.
5.1.13. Natural language generation
Explaining in natural language needs to be accurate, useful,
and easy to understand . Furthermore, in order to produce
good quality explanations, the generated explanations need to
be tailored to a specific purpose and audience, be narrative and
structured, and communicate uncertainty and data quality that
could affect the system’s output .
Four challenges that are crucial in generating good quality
explanations have been discussed in :
• Evaluation challenge: The need for developing inexpensive
but reliable ways of evaluating the quality of the generated
explanation based on a range of rigor levels (e.g., scrutability, trust, etc.). A recent taxonomy for existing automated
evaluation metrics for natural language generation can be
found in .
• Vague Language challenge: Using vague terms in explanations is much easier to understand for humans because they
think in qualitative terms . However, how can vague
language be used in explanations, such that the user does
not interpret it in a way that will lead to a misunderstanding of the situation? In addition, setting the priority of
messages based on features and concepts that the user is
aware of would be helpful. Furthermore, the phrasing and
terminology used should be intuitive to users.
• Narrative challenge: Explaining symbolic reasoning narratively is more straightforward to comprehend than numbers
and probabilities . Therefore, we need to develop algorithms for creating narrative explanations to present the
reasoning.
• Communicating data quality challenge: Techniques should
be developed to keep users informed when data problems
affect results. We have discussed this issue in detail in
Communicating Data Quality Section.
Another challenge has been discussed in . In some medical
domains, it could be necessary for AI systems to generate long
textual coherent reports to mimic the behavior of doctors. The
challenge here is that after generating a few coherent sentences,
language generation models usually start producing seemingly
random words that have no connection to previously generated
words. One of the solutions to this problem would be to use
transformer networks as language model decoders, which
can capture word relationships in a longer sentence. In order to
evaluate the generated reports, it is essential to compare them
with human-generated reports. However, since human-generated
reports are usually free-text reports (i.e., not following any specific template), it is important to first eliminate unnecessary
information for the final diagnosis from human-generated reports
and then conduct the comparison.
5.1.14. Analyzing
assumption-free
assumption-based data models
The author in has discussed that knowledge can be extracted from the data using ML models that can be interpreted.
With proper training, the interpretable ML model can help in
identifying features’ importance and relationships, and approximate reality to a reasonable degree .
The author added that we should focus more on analyzing
assumption-free black-box AI models than analyzing assumptionbased data models. That is because making assumptions about
the data (i.e., distribution assumptions) is questionable. Further,
assumption-based data models in many domains are typically less
predictive than black-box AI models (i.e., generalization) when
having lots of quality data, which is available due to digitization.
Therefore, the author has argued that there should be a development of all the tools that statistics offer for answering questions
(e.g., hypothesis tests, correlation measures, and interaction measures) and rewriting them for black-box AI models. Some relevant
works to these already exist . For example, in a linear model,
the coefficients quantify the effects of an individual feature on
the result. The partial dependent plot represents this idea in
a more generalized form .
5.1.15. Reproducibility
In a recent review of XAI models based on electronic health
records, it has been found that research reproducibility was not
stressed well in the reviewed literature, though it is paramount
 . In order to facilitate comparisons between new ideas and
existing works, researchers should use open data, describe the
methodology and infrastructure they used, and share their code
 . In addition, it has been suggested that publication venues
should establish reproducibility standards that authors must follow as part of their publication process .
5.1.16. The economics of explanations
Research into the economic perspective of XAI is sparse, but it
is essential . With the pressures of social and ethical concerns
about trusting black-box AI models, XAI has the potential to drive
a real business value . XAI, however, comes at a cost .
Recently, the work in identified costs of explanations
in seven main categories (1) costs of explanations design, (2)
costs of creating and storing audit logs, (3) costs of trade secrets
violation (e.g., the forced disclosure of source code), (4) costs of
slowing down innovation (e.g., increasing time-to-market), (5)
costs of reducing decisional flexibility if the future situation does
not justify the previous explanation, (6) cost of conflict with
security and privacy matters, and (7) costs of using less efficient
models for their interpretability. Therefore, costs associated with
algorithmic explanations should be incurred when the benefits of
the explanations outweigh the costs .
Cost estimation is one of the issues that should be addressed
by encouraging economic interpretations. Other issues include
algorithms proprietary, revealing trade secrets and predicting XAI
market evolution .
5.2. Challenges and research directions of XAI based on the ML life
cycle’s phases
In this section, we reported the challenges and research directions in XAI based on three ML life cycle phases. The selected
papers that discussed these challenges and research directions are
mentioned in Table 5. As shown in Fig. 8, the majority of these
challenges and research directions have received much attention
recently .
5.2.1. Challenges and research directions of XAI in the design phase
This phase is focused on the processes needed before starting
training machine learning models on the given data (e.g., data collection and understanding). Challenges and Research Directions of
XAI discussed in this section are shown in Fig. 9
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
A summary of the selected papers, categorized by phases as well as challenges and research directions.
Challenges and research directions
Communicating data quality
Sparsity of analysis
Development
Knowledge infusion
 , a
Rules extraction
 
Developing approaches supporting explaining the training process
 
Developing visual analytics approaches for advanced DL architectures
Developing model debugging techniques
Model innovation
Using interpretability/explainability for models/architectures comparison
Bayesian approach to interpretability
Explaining competencies
Interpretability for natural language processing
Deployment
Human-machine teaming
 , a
XAI and security
 
XAI and reinforcement learning
 , a
XAI and safety
 
Machine-to-machine explanation
 
XAI and privacy
Explainable AI planning (XAIP)
Explainable recommendation
Explainable agency and explainable embodied agents
XAI as a service
Improving explanations with ontologies
aA non-peer-reviewed paper from arXiv.
Fig. 8. Number of research papers per year for each challenge and research direction categorized by phases. Method: Publication year was used for peer-reviewed
papers published before 2022, otherwise, arXiv’s publication year was considered. As can be seen from this figure, the majority of these challenges and research
directions have received much attention recently .
5.2.1.1. Communicating data quality. The provided explanations
for the AI system or its outcomes depend on the data used to
build the system. Data bias, data incompleteness, and data incorrectness are issues that affect the quality of the data. Training
AI systems using low-quality data will be reflected in their outcomes . For example, an AI system developed for lung cancer
risk prediction using data from Americans may not accurately
estimate risks for a resident of Delhi due to the differences in
polluted environments in which they are living at . So, what
can be of high quality for a particular purpose can be of low
quality for another . Reducing system accuracy is not the only
consequence of building an AI system using low-quality data;
producing unfair decisions and degrading the explainability of the
AI system are other possible consequences.
With this in mind, it has been suggested to be aware of
how data was collected and any limitations associated with the
collected data . Further, it has been highlighted the importance of clarifying any data issues that can reduce accuracy when
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
Fig. 9. Challenges and Research Directions of XAI in the Design Phase.
producing explanations . However, how can we communicate
data quality to users to let them know how the results are
influenced by the data used.
In , the authors discussed several issues that arise when
producing explanations for AI models that use imputation of
missing data. They recommended disclaimers accompanied by
the derived explanations and educated end-users about the risks
involved in incorrect explanations. Even though it is good to come
up with appropriate disclaimers, we believe that future studies
should be undertaken to develop a practical and measurable way
to communicate data quality to users. Proposing dimensions of
data quality could be the basis for that. We recommend starting
with the following questions which are inspired by the work
• Which essential dimensions of data quality are wanted?
• What are the definitions of those dimensions? and how to
measure them?
• How to deal with them to improve the AI model and hence
its explanations?
• How to communicate them (and highlight any possible
According to , there is a variety of data quality dimensions
such as completeness, accuracy, and consistency. For an extensive list of dimensions of data quality that occur in information
systems, the reader may refer to the research paper in .
The fairness dimension can also be included, which may include
demographic parity differences. It is essential to highlight that the
way that can be used to communicate data quality can vary based
on the type of users.
5.2.1.2. Sparsity of analysis. Interpreting and validating the reasoning behind an ML model may require examining many visualizations, which is a challenging task for the user, especially if
there are a vast number of samples for such examination .
Therefore, the number of visualizations that a user has to analyze
should be as small as possible to reduce the sparsity of the
analysis . A way to achieve that can be by developing novel
methods to identify a meaningful subset of the entire dataset
to interpret; then, by using this meaningful subset, it is needed
to come up with an interpretation of the relationship between
various samples and various subsets .
5.2.2. Challenges and research directions of XAI in the development
There are three main types of learning in ML: supervised,
unsupervised, and reinforcement learning. In supervised learning,
a learning algorithm is used to train an ML model to capture
patterns in the training data that map inputs to outputs. With
unsupervised learning, which is used when only the input data
is available, an ML model is trained to describe or extract relationships in the training data. For reinforcement learning, an ML
model is trained to make decisions in a dynamic environment to
perform a task to maximize a reward function. In the following
subsections, we discuss the challenges and research directions
during the development of ML models (e.g., model training and
model understating) (see Fig. 10).
5.2.2.1. Knowledge infusion. A promising research direction is incorporating human domain knowledge into the learning process
(e.g., to capture desired patterns in the data). According to ,
understanding how experts analyze images and which regions of
the image are essential to reaching a decision could be helpful to
come up with novel model architectures that mimic that process.
Furthermore, our explanations can be better interpretable and
more informative if we use more domain/task-specific terms .
Recently, the work in highlights various ways of incorporating approaches for medical domain knowledge with DL models
such as transfer learning, curriculum learning, decision level fusion, and feature level fusion. According to that survey, it was
seen that with appropriate integrating methods, different kinds of
domain knowledge could be utilized to improve the effectiveness
of DL models. A review focused on knowledge-aware methods for
XAI is given by . Based on the knowledge source, two categories are identified: knowledge methods and knowledge-based
methods. Unstructured data is used as a knowledge source in
knowledge methods, while knowledge-based methods use structured knowledge to build explanations. According to , when
we use external domain knowledge, we are able to produce explanations that identify important features and why they matter.
As concluded in that survey, many questions remain unanswered
regarding utilizing external knowledge effectively. For instance,
in a vast knowledge space, how can relevant knowledge be obtained or retrieved? To demonstrate this point, let us take the
Human-in-the-loop approach as an example . Typically, a user
has a wide range of knowledge in multiple domains; thus, the XAI
system must ensure that the knowledge provided to the user is
desirable.
Recent works in the knowledge that can be incorporated during training ML are given in . In , a one-shot
learning technique was presented for incorporating knowledge
about object categories, which may be obtained from previously
learned models, to predict new objects when very few examples are available from a given class. Another work in 
has shown how a knowledge graph is integrated into DL using
knowledge-infused learning and presented examples of how to
utilize knowledge-infused learning towards interpretability and
explainability in education and healthcare. The work in 
has mentioned that the middle-to-end learning of neural networks with weak supervision via human–computer interaction is
believed to be a fundamental research direction in the future.
Based on all that, it can be seen that using XAI to explain
the outcomes of the models (e.g., pointing out which regions
of the image were used to reach the decision) can help to understand better what was learned from the incorporated human
knowledge. Thus, it would help to adjust the way used in incorporating the knowledge or come up with innovations in model
architectures. Furthermore, it could be used to confirm whether
a model follows the injected knowledge and rules, especially
with critical applications, e.g., autonomous driving model .
Therefore, more research is needed to investigate how experts
can interact with ML models to understand them and improve
their abilities, which would be a promising direction in which XAI
can contribute.
5.2.2.2. Rules extraction. Historically, the need for explanations
dates back to the early works in explaining expert systems and
Bayesian networks . Rule extraction from ML models has
been studied for a long time . However, there is still
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
Fig. 10. Challenges and Research Directions of XAI in the Development Phase.
increasing interest in utilizing rule extraction for explainability/interpretability . Therefore, to discover methods that
may work for explainability/interpretability, we should revisit the
past research works .
According to , there are three main approaches for
rule extraction: (1) Decomposition approach based on the principle that the rules are extracted at the neuron level, such as
visualizing a network’s structure, (2) Pedagogical approach that
extracts rules that map inputs directly to outputs regardless of
their underlying structure, such as computing gradient, (3) Eclectics approach, which is the combination of both decompositional
and pedagogical approaches.
Further research for rules extraction is needed, which has
been discussed in . First, visualize neural networks’ internal structure. Through visualizing each activated weight connection/neuron/filter from input to output, one can understand
how the network works internally and produce the output from
the input. Second, transform a complex neural network into an
interpretable structure by pruning unimportant or aggregating
connections with similar functions. By doing this, the overfitting
issue can be reduced, and the model’s structure becomes easier
to interpret. Third, explore the correspondence between inputs
and outputs, for example, by modifying the inputs and observing
their effects on the output. Fourth, calculate the gradient of the
output to the inputs to know their contributions.
In visual reasoning tasks, logic rule extraction and representation are challenging tasks . For rule extraction, predicates and
arguments may show multi-granularity concepts, which causes
difficulty in the extraction task. For rule representation, there is a
need to go beyond implementing discriminative rule representations at the syntactic level, which is the current research focus, to
the semantic level. Therefore, further work needs to be done to
address the challenges associated with logic rule extraction and
representation for interpretable visual reasoning. Finally, it has
been suggested to combine the best of DL and fuzzy logic towards
an enhanced interpretability .
5.2.2.3. Developing approaches supporting explaining the training
process. Training ML models, especially DL, is a lengthy process
that usually takes hours to days to finish, mainly because of
the large datasets used to train the models . Therefore,
researchers and practitioners have contributed to developing systems that could help steer the training process and develop better
Examples of progressive visual analytics systems are cited
in . For example, DeepEyes is an example of a progressive visual analytics system that enables advanced analysis
of DNN models during training. The system can identify stable
layers, identify degenerated filters that are worthless, identify
inputs that are not processed by any filter in the network, reasons
the size of a layer, and helps to decide whether more layers
are needed or eliminate unnecessary layers. DGMTracker is another example which is developed for better understanding
and diagnosing the training process of deep generative models
(DGMs). In addition, big tech companies such as Google and
Amazon have developed toolkits to debug and improve the performance of ML models such as Tensor-Board3 and SageMaker
Debugger.4
Future studies to deal with this challenge are therefore recommended in order to develop XAI approaches supporting the online
training monitoring to get insights that could help to steer the
training process by the experts, which could help in developing
better models and minimizing time and resources .
5.2.2.4. Developing visual analytics approaches for advanced DL architectures. While visual analytic approaches have been developed for basic DL architectures (e.g., CNNs and RNNs), advanced
DL architectures pose several challenges for visual analytic and
information visualization communities due to their large number
of layers, the complexity of network design for each layer, and
3 
4 
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
the highly connected structure between layers . Therefore,
developing efficient visual analytics approaches for such architectures in order to increase their interpretability as well as
the explainability of their results is needed . The work
in provided two recent works on using variations of class
activation maps (CAMs) to explain the generated results from an
ensemble of advanced DL architectures (e.g., ResNet) .
Therefore, it is expected that several visual analytics approaches
will be developed for advanced DL architectures due to the wide
applications of the advanced DL architectures.
5.2.2.5. Developing model debugging techniques. The model is already trained at this stage, and we want to discover any problems
that can limit its predictions. The debugging of ML models is
paramount for promoting trust in the processes and predictions,
which could result in creating new applications , e.g., visual applications for CNN. A variety of debugging techniques exists, including model assertion, security audit, variants of residual
analysis and residual explanation, and unit tests . According
to , understanding what causes errors in the model can form
the foundation for developing interpretable explanations. The
next step is developing more model debugging techniques and
combining them with explanatory techniques to provide insight
into the model’s behavior, enhance its performance, and promote
trust .
5.2.2.6. Model innovation. By explaining DL models, we can gain
a deeper understanding of their internal structure which can
lead to the emergence of new models (e.g., ZFNet ) .
Therefore, in the future, the development of explanation methods
for DL and new DL models are expected to complement each
other .
Another research area is developing new hybrid models where
the expressiveness of opaque models is combined with the apparent semantics of transparent models (e.g., combining a neural
network with a linear regression) . This research area can
be helpful for bridging the gap between opaque and transparent
models and could help in developing highly efficient interpretable
models .
5.2.2.7. Using interpretability/explainability for models/architectures
comparison. It is widely known that the performance of ML models/architectures varies from one dataset/task to another .
Usually, error performance metrics are used for the comparison to
choose the suitable model/architecture for the given dataset/task
and to decide how to combine models/architectures for better performance . However, even if the models may have
the same performance, they can use different features to reach
the decisions . Therefore, the interpretability/explainability of
models can be helpful for models/architectures comparison .
It could even be said that the better we understand models’
behavior and why they fail in some situations, the more we
can use those insights to enhance them . In the future, it is
expected that explanations will be an essential part of a more
extensive optimization process to achieve some goals such as
improving a model’s performance or reducing its complexity .
Further, XAI can be utilized in models/architectures comparison.
For example, the works in show some recent works
using visual explanations for model comparison.
5.2.2.8. Bayesian approach to interpretability. The work in has
discussed that there exist elements in DL and Bayesian reasoning
that complement each other. Comparing Bayesian reasoning with
DL, Bayesian reasoning offers a unified framework for model
development, inference, prediction, and decision-making. Furthermore, uncertainty and variability of outcomes are explicitly
accounted for. In addition, the framework has an ‘‘Occam’s Razor’’
effect that penalizes overcomplicated models, which makes it
robust to model overfitting. However, to ensure computational
tractability, Bayesian reasoning is typically limited to conjugate
and linear models.
In a recent survey on Bayesian DL (BDL) this complement
observation has been exploited, and a general framework for BDL
within a uniform probabilistic framework has been proposed.
Further research is needed to be done to exploit this complement
observation because it could improve model transparency and
functionality .
5.2.2.9. Explaining competencies. There is a need for users to gain
a deeper understanding of the competencies of the AI system,
which includes knowing what competencies it possesses, how
its competencies can be measured, as well as whether or not
it has blind spots (i.e., classes of solutions it never finds) .
Through knowledge and competency research, XAI could play
a significant role in society. Besides explaining to individuals,
there are other roles including leveraging existing knowledge for
further knowledge discovery and applications and teaching both
agents and humans .
5.2.2.10. Interpretability for natural language processing. There are
many ways to categorize XAI methods. One standard category is
categorizing XAI methods as local or global methods. The local
methods explain a single decision from the model, while the
global methods explain the entire model . The authors in 
suggested adding class explanation methods to this category.
Such methods are focused on explaining the entire output-class,
hence the name . An example of a class explanation method is
summarizing a model focusing on one class only . There is still
no attention given to these types of methods by researchers (not
only for NLP), therefore, these methods should be investigated
further . It has been also suggested in to pay attention to particularly developing more explanation methods for
sequence-to-sequence models which have numerous real-world
applications such as machine translation, question answering,
and text summarization.
5.2.3. Challenges and research directions of XAI in the deployment
The following subsections are dedicated to challenges and
research directions during the deployment of AI systems. The
deployment phase starts with deploying ML solutions until we
stop using the solutions (or maybe after that).
Challenges and
Research Directions of XAI discussed for this phase are shown in
5.2.3.1. Human-machine teaming. Most provided explanations for
AI systems are typically static and carry one message per explanation . Explanations alone do not translate to understanding . Therefore, for a better understanding of the system, users
should be able to explore the system via interactive explanations, which is a promising research direction to advance the XAI
field 
as the majority of current XAI libraries lack user
interactivity and personalization of the explanations .
Even though there are already some works in this research
direction as has been reported in , much work is still needed
to tailor interfaces to different audiences, exploit interactivity,
and choose appropriate interactions for better visualization designs . Various works have also been suggested to go
beyond static explanations and enhance human-machine teaming. In , open-ended visual question answering (VQA) has
been suggested to be used rather than providing a report with
too many details. Here, an user queries (or make follow-up questions), and the system answers. Achieving that would provide
better interaction between the system and the expert user. In another work , it has been mentioned that generative models
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
Fig. 11. Challenges and Research Directions of XAI in the Deployment Phase.
can allow for interactive DL steering because they allow for multiple answers. They highlighted that developing new DL models
capable of adapting to various user inputs and generating outputs
accordingly as well as developing visualization-based interfaces
that enable effective interaction with DL systems are promising
research areas in the future.
In , rather than providing static explanations, the authors have suggested building on existing intelligibility work for
context-aware systems (e.g., design space explorations, conceptual models for implicit interaction, and intelligible interfaces
for various scenarios and using a variety of modalities). Additionally, they have highlighted a research area that is effectively interacting with AI augmentation tools. In , an XAI
system was proposed which involves a group of robots that
attempt to infer human values (i.e., the importance of various
goals) learned using a cooperative communication model while
providing explanations of their decision-making process to the
users. Through experiments, it was found that it is possible to
achieve real-time mutual understanding between humans and
robots in complex cooperative tasks through a learning model
based on bidirectional communication. The work in stressed
the importance and benefits of multimodal interactive explanations with the users for better understanding and traceability
of model working and its decisions, better user satisfaction and
trust, improving transparency in the decision-making process,
and acting as an enabler to make advances in human–computer
interaction field. In , it has been emphasized the importance
of bridging HCI empirical studies with human sciences theories to
make explainability models more human-centered models. In this
way, adaptive explainable models would emerge by providing
context-aware explanations that could be adapted to any changes
in the parameters of their environment, such as user profile
(e.g., expertise level, domain knowledge, cultural background,
interests, and preferences) and the explanation request setting
(e.g., justification).
The authors in have mentioned that extracting, visualizing, and keeping track of the history of interaction data between
users and systems can allow users to undo certain actions and
examine them interactively would help to address some common
challenges (e.g., hyperparameter exploration). Finally, the authors
in have highlighted that user-friendliness and intelligent
interface modalities need to take into account the type of explanations that meet users’ goals and needs. For example, the
system can ask for feedback from the users to know how good
was the provided explanations (e.g., ‘‘explain more’’, ‘‘redundant
explanation’’, or ‘‘different explanation’’). Such interaction can
help to improve future explanations.
Taken together, it seems that different ways are needed to
enhance human-machine teaming. Approaching HCI and other
related studies can contribute to making explainability models
more human-centered. In addition, humans can provide feedback
on the provided explanations, which can help in improving future
explanations.
5.2.3.2. XAI and security. Two main concerns have been discussed
for XAI and security: confidentiality and adversarial attacks . For the confidentiality concern, several aspects of
a model may possess the property of confidentiality . As an
example given by , think of a company invested in a multi-year
research project to develop an AI model. The model’s synthesized
knowledge may be regarded as confidential, and hence if only
inputs and outputs are made available, one may compromise this
knowledge . The work in presented the first results on
how to protect private content from automatic recognition models. Further research is recommended to develop XAI tools that
explain ML models while maintaining models’ confidentiality .
Turning now to the adversarial attacks concern, the information revealed by XAI can be utilized in generating efficient
adversarial attacks to cause security violations, confusing the
model and causing it to produce a specific output, and manipulation of explanations . In adversarial ML, three types of
security violations can be caused by attackers using adversarial examples : integrity attacks (i.e., the system identifies
intrusion points as normal), availability attacks (i.e., the system
makes multiple classification errors, making it practically useless), and privacy violation 110273
users). Attackers can do such security violations because an AI
model can be built based on training data influenced by them,
or they might send carefully crafted inputs to the model and see
its results . According to , existing solutions to handle
perturbations still suffer from some issues, including instabilities
and lack of variability. Therefore, it is necessary to develop new
methods to handle perturbations more robustly . Additionally, a challenge ahead for XAI is to come up with provable
guarantees that the provided explanations for the predictions are
robust to many external distortion types .
The information uncovered by XAI can also be utilized in
developing techniques for protecting private data, e.g., utilizing
generative models to explain data-driven decisions . Two recent research directions have been highlighted in this context :
using generative models as an attribution method to show a
direct relationship between a particular output and its input
variables . The second is creating counterfactuals through
generative models . It is expected that generative models
will play an essential role in scenarios requiring understandable
machine decisions .
5.2.3.3. XAI and reinforcement learning. The use of DL by reinforcement learning (RL) has been applied successfully to many
areas . Through the explicit modeling of the interaction
between models and environments, RL can directly address some
of the interpretability objectives . Despite that, unexplained
or non-understandable behavior makes it difficult to users to trust
RL agents in a real environment, especially when it comes to
human safety or failure costs . Additionally, we lack a clear
understanding of why an RL agent decides to perform an action
and what it learns during training . RL’s interpretability can
help in exploring various approaches to solving problems .
For instance, understanding why the RL AlphaFold system is
capable of making accurate predictions can assist bioinformatics
scientists in understanding and improving the existing techniques
in protein structures to speed produce better treatment before
new outbreaks happen .
Recently, the work in highlighted several issues that
need to be addressed and potential research directions in the area
of XAI for RL. The authors find that the selected studies used ‘‘toy’’
examples or case studies that were intentionally limited in scope
mainly to prevent the combinatorial explosion problem in the
number of combinations of states and actions. Therefore, more
focus on real-world applications has been suggested. It has also
been mentioned that there is a lack of new algorithms in the area.
Therefore, the design of RL algorithms with an emphasis on explainability is essential. Symbolic representations can be utilized
so RL agents can inherently be explained and verified. Another
issue is highlighted, which is the lack of user testing with the
existing approaches, which is in line with what was mentioned
in . As for the complexity of the provided explanations, it has
been found that the current focus is presenting explanations for
users with a background in AI. Therefore, it has been suggested
to conduct further research to present the explanations for those
who might interact with the agents, which may have no background in AI. For example, providing more visceral explanations,
e.g., annotations in a virtual environment. Additionally, enriching
visualization techniques by considering the temporal dimensions
of RL and multi-modal forms of visualization, e.g., virtual or
augmented reality. Lastly, it has been emphasized the importance
of open-source code sharing for the academic community.
Another interesting point for consideration has been highlighted in , which is learning from explanations. The work
in provides a starting point, which presents an agent who
trained to simulate the Mario Bros. game using explanations
instead of prior play logs.
Finally, various RL techniques such as hierarchical, multi-goal,
multi-objective, and intrinsically motivated learning have been
suggested to be used in goal-driven explanation and emotionaware XAI . Further, event-based and expectation-based explanations can be investigated to increase the usage of RL in
human-agent mixed application domains .
5.2.3.4. XAI and safety. Trust and acceptance are benefits of explainability/interpretability . However, focusing on benefits
without considering the potential risks may have severe consequences (e.g., relying too much or too little on the advice provided
by the prescription recommendation system) . Several studies have been conducted to evaluate the safety of processes that
depend on model outputs because erroneous outputs can lead to
harmful consequences in some domains . Therefore, possible
risks must be at the top priority when designing the presented
explanations .
Many techniques have been proposed to minimize the risk
and uncertainty of adverse effects of decisions made using model
outputs . As an example, the model’s output confidence technique can examine the extent of uncertainty resulting from a
lack of knowledge regarding the inputs and the corresponding
output confidence of the model to notify the user and cause
them to reject the output produced by the model . In order
to achieve this, explaining what region of the inputs was used by
the model to arrive at the outcome can be used for separating
out such uncertainty that may exist within the input domain .
Additionally, as has been suggested in , it is important to
develop explanations that evolve with time, keeping in mind past
explanations for long-term interactions with end-users and identifying ways to minimize risks. Developing evaluation metrics and
questionnaires would be essential to integrate the user-centric
aspects of explanations as well as evaluating error-proneness and
any possible risks . Finally, in , some major challenges
have been discussed, including developing distance metrics that
more closely reflect human perception, improvement to robustness by designing a set of measurable metrics for comparing the
robustness of black-box AI models across various architectures,
verification completeness using various verification techniques,
scalable verification with tighter bounds, and unifying formulation of interpretability.
It is good to note that the utilization
of formal verification methods has been suggested as a potential step to move forward towards establishing a truly safe and
trustworthy model .
5.2.3.5. Machine-to-machine explanation. A promising area of research is enabling machine-to-machine communication and understanding . Furthermore, it is an important research area
because of the increasing adoption of the Internet of Things (IoT)
in different industries. A growing body of research has begun
exploring how multiple agents can efficiently cooperate and exploring the difference between explanations intended for humans
and those intended for machines .
According to , future explainable approaches are likely to
provide both human and machine explanations, especially adaptive explainable approaches . For machine explanations, complex structures that are beyond the comprehension of humans
may be developed . However, how is it possible to measure
the success of ‘‘transfer of understanding’’ between agents? The
work in has suggested a metric for that, which is measuring
the improvement of agent B’s performance on a particular task, or
set of tasks, as a result of the information obtained from agent A
- though it will be crucial to determine some key details, such as
the bandwidth constraints and already existing knowledge with
Based on what has been mentioned above, it is expected that
much work is going to be done on how to construct machine
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
explanations, how to communicate these explanations, and which
metrics we need to measure as a success of the transfer of understanding between agents and how to measure them. With more
research into how machines communicate/explain themselves,
we will be able to understand intelligence better and create
intelligent machines .
5.2.3.6. XAI and privacy. When individuals are affected by automated decision-making systems, two rights conflict: the right
to privacy and the right to an explanation . At this stage,
it could be a demand to disclose the raw training data and
thus violate the privacy rights of the individuals from whom the
raw training data came . Another legal challenge has been
discussed in , which is the right to be forgotten . By
this right, individuals can claim to delete specific data so that
they cannot be traced by a third party . Data preservation
is another related issue because to use XAI to justify a decision
reached by automated decision-making, the raw data used for
training must be kept, at least until we stop using the AI solution.
One of the key challenges is establishing trust in the handling
of personal data, particularly in cases where the algorithms used
are challenging to understand . This can pose a significant
risk for acceptance to end-users and experts alike . For
example, end-users need to trust that their personal information
is secured and protected as well as that only their consented data
is used, while experts need to trust that their input is not altered
later .
Anonymization of data can be used to obscure the identity of people. However, privacy cannot always be protected by
anonymization . According to , the more information
in a data set, the greater the risk of de-anonymization, even if the
information is not immediately visible. Asserting that anonymization helps conceal who supplied the data to train the automated
decision-making system might be comforting for the individuals
whom the training data came from, but this does not the case
with individuals who are entitled to an explanation of the results
produced by the system .
In order to address some issues with anonymization techniques, it is recommended that further research should be undertaken in privacy-aware ML, which is the intersection between
ML and security areas . XAI can play an essential role in this
matter because to develop new techniques to ensure privacy and
security, it will be essential to learn more about the inner workings of the system they are meant to protect . In addition, in
the future, to promote the acceptance of AI and increase privacy
protection, XAI needs to provide information on how the personal
data of a particular individual was utilized in a data analysis
workflow . However, according to , what if it is needed
to review the data of many individuals and they may not have
consented to review their data in litigation. In such cases, a path
to review data for which individuals have not consented would
be demanded, but it would be difficult to find such a path .
Data sharing is another related issue because AI is used as
a data-driven method and therefore any requested explanations
depend on data used to build AI systems. Data sharing in this
context means making raw data available to be used by other
partners . According to , the implementation of watermarking or fingerprinting are a typical reactive technique used
to deal with this issue. Federated learning can be a possible
solution to avoid raw data sharing. Federated learning allows
building ML models using raw data distributed across multiple devices or servers . Even though the data never
leaves the user’s device, increasing the number of clients involved in a collaborative model makes it more susceptible to
inference attacks intended to infer sensitive information from
training data . Possible research directions to deal with
privacy challenges of federated learning have been discussed
in such as privacy-preserving security assurance, defining optimal bounds of noise ratio, and proposing granular and
adaptive privacy solutions.
5.2.3.7. Explainable AI planning (XAIP). Existing literature focuses
mainly on explainability in ML, though similar challenges apply
to other areas in AI as well . AI planning is an example of such
an area that is important in applications where learning is not an
option . Recent years have seen increased interest in research
on explainable AI planning (XAIP) . XAIP includes a variety
of topics from epistemic logic to ML, and techniques including
domain analysis, plan generation, and goal recognition .
There are, however, some major trends that have emerged, such
as plan explanations, contrastive explanations, human factors,
and model reconciliation .
Recently, the work in has explored the explainability
opportunities that arise in AI planning. They have provided some
of the questions requiring explanation. They also have described
initial results and a roadmap towards achieving the goal of generating effective explanations. Additionally, they have suggested
several future directions in both plan explanations and executions. Temporal planning, for instance, can open up interesting
choices regarding the order of achieving (sub)goals. It is also
interesting to consider whether giving the planner extra time to
plan would improve the performance. In addition, one of the challenges in plan execution is explaining what has been observed at
the execution time that prompts the planner to make a specific
choice. As with XAI, it is crucial to have a good metric for XAIP
that defines what constitutes a good explanation. Finally, it is
imperative that the existing works on XAIP be reconsidered and
leveraged so that XAIP will be more effective and efficient when
used in critical domains.
5.2.3.8. Explainable recommendation. Explainable recommendation aims to build models that produce high quality recommendations as well as provide intuitive explanations that can help
to enhance the transparency, persuasiveness, effectiveness, trustworthiness, and satisfaction of recommendation systems .
The work in conducted a comprehensive survey of
explainable recommendations, and they discussed potential future directions to promote explainable recommendations. With
regards to the methodology perspective, it has been suggested
that (1) further research is needed to make deep models explainable for recommendations because we still do not fully
understand what makes something recommended versus other
options, (2) develop knowledge-enhanced explainable recommendation which allows the system to make recommendations
based on domain knowledge, e.g., combine graph embedding
learning with recommendation models, (3) use heterogeneous
information for explainability such as multi-modal explanations,
transfer learning over heterogeneous information sources, information retrieval and recommendation cross-domain explanations, and the impact that specific information modalities have
on user receptiveness on the explanations, (4) develop contextaware explainable recommendations, (5) aggregate different explanations, (6) integrate symbolic reasoning and ML to make
recommendations and explainability better by advancing collaborative filtering to collaborative reasoning, (7) further research
is needed to help machines explain themselves using natural
language, and (8) with the evolution of conversational recommendations powered by smart agent devices, users may ask
‘‘why’’ questions to get explanations when a recommendation
does not make sense. Therefore, it is essential to answer the
‘‘why’’ in conversations which could help to improve system
efficiency, transparency, and trustworthiness.
For the evaluation perspective, the authors in have suggested the importance of developing reliable and easily implemented evaluation metrics for different evaluation perspectives
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
(i.e., user perspective and algorithm perspective). Additionally,
evaluating explainable recommendation systems using user behavior perspectives may be beneficial as well. Lastly, it has been
highlighted that explanations should have broader effects than
just persuasion. For example, investigate how explanations can
make the system more trustworthy, efficient, diverse, satisfying,
and scrutable.
In , the authors have presented several research challenges in delivery methods and modalities in user experience.
As mentioned in that paper, for the delivery method, the current
focus in the literature is on providing an explanation to the users
while they are working on a task or looking for recommendations.
However, more focus should be done on the long-term retrieval
of such explanations, for example, through a digital archive, and
their implications for accountability, traceability, and users’ trust
and adoption. That could increase the adoption of intelligent
human-agent systems in critical domains. Another challenge is
designing autonomous delivery capable of considering the context and situation in which users may need explanations and
suitable explanations for them. It is worth mentioning that privacy matters should be taken into account when deriving the
recommendations.
It has also been highlighted in that users’ goals and
needs would have to be met by user-friendly and intelligent interface modalities that provide appropriate explanations. Further,
interaction with the system is needed and could help to improve
future generated explanations. Finally, focusing on the benefits of
explainability without considering the potential risks may have
severe consequences. Therefore, when designing explanations,
possible risks should be the first priority.
5.2.3.9. Explainable agency and explainable embodied agents. Explainable agency refers to a general capability in which autonomous agents must provide explanations for their decisions
and the reasons leading to these decisions . Based on the
three explanation phases proposed in , the authors in 
presents a research roadmap for the explainable agency.
The first phase is explanation generation which is intended
to explain why an action/result was taken/achieved . This
phase of research focuses on the following key research directions: (1) there is a need to connect the internal AI mechanism of the agent/robot with the explanation generation module,
(2) to produce dynamic explanations, new mechanisms are required for identifying relevant explanation elements, identifying
its rationales, and combining these elements to form a coherent
explanation.
The second phase is the explanation communication phase.
Here, the focus is on what content end users will receive and
how to present that content . According to , explainable
agents/robots may be deployed in a variety of environments.
Therefore, in some cases, multimodal explanation presentations
(e.g., visual, audio, and expressive) could be a useful explanation communication approach for enabling efficient explainable
agency communication.
For the last phase, explanation reception, the focus is on
the human’s understanding of explanations. Some considerations should be taken into account to ensure an accurate reception . It is important to develop metrics to measure the
explanations’ effectiveness and the users’ reaction to the provided explanations. In addition, the agent/robot should maintain
a model of user knowledge and keep updating it based on the
evolution of user expertise and the user’s perception of the State
of Mind (SoM) of the agent/robot, i.e., an internal representation
of how the agent/robot treats the outer world.
The work in reviewed the works related to explainable
embodied agents. Embodied agents can interact with humans
using both verbal and non-verbal communicative behaviors .
Although these behaviors, the actions taken by agents are not
necessarily understandable . Therefore, there is an increasing interest in how to make embodied agents explainable .
According to , there are still unanswered questions in the
literature on explainable embodied agents that need further investigation like what are the suitable models that can help to predict/track human expectations/beliefs about the goals and actions
of an embodied agent? What is the efficient way to include the
Human-in-the-loop approach when designing embodied agents
with explainability? What are the impact of the environment and
social cues embodiment in the selection of social cues used for
explainability (e.g., speech, text, or movement)? How can we best
objectively measure trust? and why there is a mixed impact of
explainability on the efficiency of the human-agent interaction?
5.2.3.10. XAI as a service. There is an increasing trend in developing automated ML (AutoML) tools . AutoML tool is an endto-end pipeline starting with raw data and going all the way to
a deployable ML model. Model-agnostic explanation methods are
applicable to any ML model resulting from automated ML .
Similarly, we can automate the explanation step: calculate the
importance of each feature, plot the partial dependence, construct
a surrogate model, etc . Further, at a more advanced level,
Auto XAI can be further designed to extract collective variables
and explain their terms, for example, extracting mathematical
formulas used in the formation of the collective variables and
then using these formulas to explain the generated predictions
by ML .
Some existing AutoML tools provide automatically generated
explanations, e.g., AutoML H2O and MLJAR AutoML .
We expect that more Auto XAI tools will be available in the future,
either incorporated with AutoML tools or as services. Since these
would be services, so one can expect that these services would
be developed to be of great help to a wide range of end-users
(e.g., non-technical experts).
5.2.3.11. Improving explanations with ontologies. An ontology is
defined as ‘‘an explicit specification of a conceptualization’’ .
The use of ontologies for representing knowledge of the relationships between data is helpful for understanding complex
data structures . Therefore, the use of ontologies can help to
produce better explanations as found in .
The work in has discussed some recent works of the
literature on this topic such as . In , Doctor XAI
was introduced as a model-agnostic explainer that focused on
explaining the diagnosis prediction task of Doctor AI , which
is a black-box AI model that predicts the patient’s next visit time.
It was shown that taking advantage of the temporal dimension
in the data and incorporating the domain knowledge into the
ontology helped improve the explanations’ quality. Another work
in showed that ontologies can enhance human comprehension of global post-hoc explanations, expressed in decision
It should be noted that ontologies are thought of as contributing a lot to explaining AI systems because they provide a
user’s conceptualization of the domain, which could be used as
a basis for explanations or debugging . Towards that goal,
new design patterns, new methodologies for creating ontologies that can support explainable systems, and new methods for
defining the interplay between ontologies and AI techniques are
needed . Furthermore, it is essential to conduct several user
studies to determine the benefits of combining ontologies with
explanations .
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
6. What do we think?
XAI is a hot research direction with many existing original
and survey papers published covering various aspects of XAI. We
believe this will continue due to the need for XAI from regulatory, scientific, industrial, model developmental, and end-user
and social perspectives (as discussed in Section 2).
In the literature, there seems to be no agreement on what
‘‘explainability’’ or ‘‘interpretability’’ mean. There are some examples in the selected papers that distinguish between the two
terms, despite the fact that they are often used interchangeably
in the literature. We think a distinction is needed towards more
formalism for XAI. Therefore, we proposed a distinction between
the two terms (as discussed in Section 3), in which explainability
aims to satisfy a need by providing insights through explainability
techniques for the targeted audience, whereas interpretability is
more about how the provided insights can make sense for the
targeted audience’s domain knowledge to be able to reason/infer
to support decision-making.
The reported challenges and research directions of XAI in the
literature are scattered so placing them in a meaningful context
was a challenge. Since XAI is meant to detect and prevent or
at least mitigate bias that can occur along all phases of the ML
pipeline, our taxonomy was developed based on ML life cycle’s
phases so it can help the readers to better understand the type of
challenges and research directions of XAI in general and in each
ML life cycle’s phases.
Even though the reported challenges and research directions
are presented individually in 39 points (as discussed in Section 5),
they can overlap and combine based on researchers’ backgrounds
and interests, resulting in new research opportunities where XAI
can play an important role. Researchers also can use these points
(or some of them) as abstract ideas and think about how these
challenges can be further explored in their domains. There are
many domains in which XAI can promote ML algorithms’ adoption, but medicine is one of the most essential. Below are some
points, which are derived based on the discussed challenges and
research directions in Section 5), that need further exploration:
• How to determine if any newly proposed approach is better
at explaining ML models compared to other existing ML
models considering that there is no agreement on what
‘‘explainability’’ or ‘‘interpretability’’ mean and the lack of
formalized rigorous evaluation metrics?
• Since it is crucial to tailor explanations based on user experience and expertise, how one can define the meaning of a
quality explanation, and how we can measure the degree of
explanation quality?
• Are the explanations provided by the existing XAI methods
tailored to different users (e.g., radiologists, medical image analysis scientists) or only designed for users with ML
backgrounds?
• How different types of data and explanations can be composited in a framework to help generate several types of
explanations so that they are comprehensive and diverse to
support clinicians to care for patients?
• How to better communicate uncertainty to inform the clinicians about the underlying uncertainties in the model and
explanations?
• What data quality dimensions can be used to communicate
data quality and how these dimensions can be measured?
• How can XAI help medical experts to interact with ML
models to understand what was learned from the incorporated human domain knowledge and improve their abilities? Can this help to come up with innovations in model
architectures?
• How can XAI establish trust in handling personal data in
medicine considering the right to privacy and the right to
an explanation?
These are some questions on how some of the discussed
challenges and research directions in our meta-survey can be
considered in medicine for further research works (which can
also be tailored to other domains).
7. Conclusions
In this systematic meta-survey paper, we presented two main
contributions to the literature on XAI. First, we proposed an
attempt to present a distinction between explainability and interpretability terms. Second, we shed light on the significant
challenges and future research directions of XAI resulting from
the selected 73 papers, which guide future exploration in the XAI
The discussion is divided into two main themes. On the first
theme, we discussed general challenges and research directions
in XAI. As the second theme, we have discussed the challenges
and research directions of XAI based on ML life cycle phases.
For the first theme, we have highlighted the following points:
(1) The importance of working towards more formalism (e.g., establishing systematic definitions and the formalization and quantification of explanations and performance metrics), (2) The
importance of tailoring explanations based on user experience
and expertise, (3) The role that XAI can play in fostering trustworthy AI, (4) The value of multidisciplinary research collaborations
in offering new insights for explainable methods, (5) The interpretability vs. performance trade-off, (6) The value of explainability methods composition for more powerful explanations, (7)
The value of causal, contrastive, and counterfactual explanations
for better human understanding, (8) The importance to put much
efforts to explain other data types (e.g., sequences, graphs, and
spatio-temporal data), (9) The challenges in the existing XAI models/methods, (10) The value of communicating uncertainty to the
users to know about the underlying uncertainties in the model
and explanations, (11) The value of time as an essential factor in
producing explanations and in interpretation. Time matters when
considering the situation, available resources, and end users,
(12) The challenges of generating good quality explanations from
a natural language generation perspective, (13) the advantages
of analyzing models rather than data, (14) The imperative of
establishing reproducibility standards for XAI models to facilitate
comparisons between new ideas and existing works, and (15) The
importance to know when it is reasonable to incur additional
costs for explanations.
For the second theme, during the design phase, it is important
to communicate data quality to users, which can vary based on
the type of users. The quality of data used to train AI systems
can reduce their performance as well as cause unfair decisions
and deteriorate the explainability of the AI system. Therefore,
developing a practical and measurable way to communicate data
quality to users is essential. In addition, there is a need to reduce
the challenge of the sparsity of the analysis that a user has to
analyze if there are a huge number of samples.
For the development phase, we have highlighted that XAI can
help explain how the included human knowledge has contributed
to the outcomes of the models. Thus, it would help with changing
the way utilized in integrating the knowledge or come with
innovations in model architectures. Other research directions are
utilizing rule extraction for explainability/interpretability, developing XAI approaches for explaining the training process, developing visual analytic approaches for advanced DL architectures,
developing model debugging techniques and combining them
W. Saeed and C. Omlin
Knowledge-Based Systems 263 110273
with explanatory techniques, using XAI approaches to gain a
deeper understanding of the internal structure of the models and
then develop new models, using interpretability/explainability for
models/architectures comparison, utilizing Bayesian approach to
interpretability, explaining the competencies of the AI systems,
and interpretability for natural language processing.
With regards to the deployment phase, we have discussed
human-machine teaming, XAI and security, some issues and research directions in the area of XAI for reinforcement learning,
XAI and safety, machine-to-machine explanation, the two rights
conflict (i.e., privacy and explanation). In addition, we pointed out
the need to focus on explainability in other AI areas (e.g., explainable AI planning, explainable recommendations, and explainable
agency and explainable embodied agents). Finally, pointing out
to the prominence of Auto XAI as a service, and the potential of
improving explanations with ontologies.
Finally, this meta-survey has three limitations. First, because
we cannot ensure that the selected keywords are complete, we
could miss some very recent papers. Second, to avoid listing the
challenges and future research directions per each paper, we
come up with the reported 39 points, which are the results of
combining what was reported in the selected papers based on the
authors’ points of view. Third, we believe that more challenges
and future research directions can be added where XAI can play
an important role in some domains, such as IoT, 5G, and digital
forensics. However, related surveys did not exist at the time of
writing this meta-survey.
CRediT authorship contribution statement
Waddah Saeed: Conceptualization, Methodology, Investigation, Writing – original draft, Writing – review & editing.
Conceptualization,
editing, Supervision.
Declaration of competing interest
The authors declare that they have no known competing
financial interests or personal relationships that could have
appeared to influence the work reported in this paper.
Data availability
No data was used for the research described in the article.
Acknowledgment
Open access funding provided by University of Agder.