Elliptical Head Tracking Using Intensity Gradients and Color Histograms
Stan BirchÔ¨Åeld
Computer Science Department
Stanford University
Stanford, CA 94305
 
An algorithm for tracking a person‚Äôs head is presented.
The head‚Äôs projection onto the image plane is modeled
as an ellipse whose position and size are continually updated by a local search combining the output of a module
concentrating on the intensity gradient around the ellipse‚Äôs
perimeter withthat of another module focusingon the color
histogram of the ellipse‚Äôs interior. Since these two modules have roughly orthogonal failure modes, they serve to
complement one another. The result is a robust, real-time
system that is able to track a person‚Äôs head with enough
accuracy to automatically control the camera‚Äôs pan, tilt,
and zoom in order to keep the person centered in the Ô¨Åeld
of view at a desired size. Extensive experimentation shows
the algorithm‚Äôs robustness with respect to full 360-degree
out-of-plane rotation, up to 90-degree tilting, severe but
brief occlusion, arbitrary camera movement, and multiple
moving people in the background.
IEEE Conference on Computer
Vision and Pattern Recognition,
Santa Barbara, California,
1998 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or
promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted
component of this work in other works, must be obtained from the IEEE.
Introduction
Robust, reliable visual tracking of an object in a complex
environment will require the integration of several different visual modules, each using a different criterion and
each employing different assumptions about the incoming
images. The modules must be selected so that theirassumptions are, as much as possible, orthogonal to each other so
that when one module fails the other one can come to its
According to elementary set theory, every closed set
in the plane can be decomposed into two disjoint sets:
the boundary and the interior .
Since these two sets
are complementary (in the true, mathematical sense), it
stands to reason that the failure modes of a tracking module
focusing on the object‚Äôs boundary will be orthogonal to
those of a module focusing on the object‚Äôs interior.
In this paper, we present a method for object tracking
*This research was sponsored by Autodesk.
that combines the output of two different modules: one that
matches the intensitygradients along the object‚Äôs boundary
and one that matches the color histogram of the object‚Äôs
interior. The present work applies the method to tracking a
person‚Äôs head, primarily because of the number of applications that could beneÔ¨Åt from such a system, such as video
conferencing, distance learning, automatic video analysis,
and surveillance. Moreover, the head is well approximated
bya simpletwo-dimensionalmodel, namely an ellipse, thus
simplifying the present investigation.
Despite their complementarity, the gradient and color
modules operate in a symmetric fashion, thus making the
combination step trivial and obviating the need for complicated sensor fusion techniques. The result is a robust
tracker that is accurate enough to actively control the camera‚Äôs pan, tilt, and zoom for long periods of time in order
to keep the subject centered in the Ô¨Åeld of view at a desired
size. The algorithm is insensitive to out-of-plane rotation,
tilting, severe but brief occlusion, arbitrary camera movement, and multiple moving people in the background.
Searching for the Head
Assume that we have an estimate of the position
 of the head in an image. The head is modeled as
a vertical ellipse with a Ô¨Åxed aspect ratio of
:, so that
) is the center of the ellipse and
 is the length of the
minor axis. We will use the notation s
head‚Äôs state or location.
 The tracking task is to update
the state by Ô¨Ånding the location whose image values best
match the values in the model. This is accomplished via
a hypothesize-and-test procedure in which the goodness of the match is dependent upon the intensity gradients
around the object‚Äôs boundary and the color histogram of
the object‚Äôs interior:
Throughout this paper, the term position refers to
), while location or state refers to
) are the matching scores based on
intensity gradients and color histograms, respectively.
The search space
S is the set of all states within some
range of the predicted location, using velocity prediction
 . Somewhat surprisingly, this simple prediction scheme
greatly improves the behavior of the tracker because it removes any restriction on the maximum lateral velocity of
the subject ‚Äî only the amount of acceleration is limited.
We nowexamine the gradient and colormodules, inturn.
Gradient Module
Perhaps the most natural way to measure the goodness of
match around the object‚Äôs boundary is to compute the normalized sum of the gradient magnitude around the perimeter of the ellipse:
(i) is the intensity gradient at perimeter pixel
the ellipse at location s, and
 is the number of pixels on
the perimeter of an ellipse with size
Except for the Ô¨Åxed shape of the object‚Äôs perimeter, the
above formulation is nearly identical to that employed by
most contour trackers . One minor difference is that
the gradient is summed around the entire perimeter rather
than just at select points. A more signiÔ¨Åcant difference is
that the current hypothesize-and-test paradigm allows
all of the data to be examined before a decision is made, in
contrast to the typical contour tracker in which each control
point independently decides how to move based on purely
local information.
A more sophisticated measure than the one in (2) is the
one proposed by Nishihara . Rather than just desiring large gradient magnitudes around the perimeter, it also
desires the gradient direction to be perpendicular to the
perimeter:
(i) is the unit vector normal to the ellipse at pixel
() denotes the dot product. The gradient magnitude
still plays a part here since the gradient is unnormalized
(Our experiments have indicated that normalization greatly
increases sensitivity to image noise).
In practice, the performance of the gradient magnitude
moduleisinferiortothat ofthegradient dot product module.
Therefore, the term gradient module will hereafter refer to
the latter, unless speciÔ¨Åcally stated otherwise.
To facilitate adding the gradient score to the color score,
the former is converted to a percentage by subtracting the
minimum and dividing by the range:
Color Module
Many researchers have exploited the relative uniqueness of
skin color to track faces . A weakness of
these systems is their heavy reliance upon skin color that
forbids skin-colored objects in the background and, more
importantly, forbids the subject from turning around so that
the back of his head, rather than his face, is visible. The
color of human heads is complex, however, being at the
very least bimodal due to the skin and hair, and any system
attemptingtohandle out-of-planerotationmust address this
The color histogram is well suited to this task because of its ability to implicitly capture complex, multimodal patterns of color. Moreover, because it disregards
all geometric information, it remains relatively invariant to
many complicated, non-rigid motions.
The procedure is as follows.
Off-line, the subject
presents a three-quarters view to the camera in order to
capture both face and hair, and a model histogram is constructed by countingthe pixels inside the ellipse (the ellipse
can be either manually placed or automatically placed via
the gradient module, either of which takes about one to
thirty seconds of user time). Then, at run time, the histogram intersection is computed between the model
M and the image histogram
I at each hypothesized location:
(i) are the numbers of pixels in the
bin of the histograms, and
N is the number of bins.
The power of histogram intersection results from the
() function, which matches no more image pixels of
a certain color than are present in the model histogram.
Thus, for example, the measure is more satisÔ¨Åed with a
region containing both facial and hair color than a region
containing all facial color.
Our color space consists of scaled versions of the three
R. The Ô¨Årst twocontainthe
chrominance information and are sampled into eight bins
This equation is identical to the one in .
At Ô¨Årst glance the
denominator may look different, but this is because our goal is to match a
single model to the best image patch, rather than to match a single image
patch to the best model.
Figure 1: A cluttered background. (a) Gradient magnitude.
(b) Horizontal and (c) vertical components of gradient.
each, while the last one contains the luminance information
and is sampled more coarsely into four bins . Some researchers have ignored luminance information completely
 , but this is dangerous with out-of-plane rotation because, based on chrominance alone, dark brown hair looks
similar to a white wall.
As in the case of the gradient scores, the color scores are
converted to percentages:
Experimental Results
Inthissectionweexaminetheperformanceoftheindividual
modules, the ways in which they complement each other,
and the robust behavior achieved with the complete system.
Gradient module alone
It is somewhat surprising that the simple gradient module
is sufÔ¨Åcient to control the camera‚Äôs pan and tilt in order to
track a person walking around an untextured, unmodiÔ¨Åed
room . Even in the rather cluttered environment shown
in Figure 1, the gradient module was able to consistently
track the subject‚Äôs slowly-moving head for about Ô¨Åfty pixels
or so of image motion before becoming distracted by the
background (The gradient magnitude performed slightly
worse than the gradient dot product).
However, in cluttered environments the gradient module
fails too often for reliable tracking.
Moreover, even in
untextured environments the module is unable to control
the camera‚Äôs zoom because the ellipse tends to become
attracted to gradients inside the face. Finally, the gradient
score function has a small basin of attraction that prevents
large accelerations (see below).
Color module alone
In nearly every respect, the performance of the color module is superior to that of the gradient module, which should
not be too surprising since it looks at more pixels. Alone,
this module is capable of controlling the camera‚Äôs pan, tilt,
Figure 2: The subject in front of a skin-colored board. All
the white pixels in (a) have the same quantized color, and
similarly for (b). The logical OR is shown in (c).
Figure 3: (a) A situation in which the ellipse was far from
the true solution. (b,c) The matching scores as a function
y at a particular scale for the gradient and color
modules, respectively. Looking at the maximum, we see
the former incorrectly pulling the ellipse left but the latter
correctly pulling to the right.
and zoom in order to track a person in an unmodiÔ¨Åed environment, even when there are skin-colored objects in the
background. For example, the subject was able to move in
front of the board shown in Figure 2 without causing the
tracker to become lost. Although the ellipse‚Äôs location became unstable while the subject was in front of the board,
yet it remained onthe subject because of the non-skinpixels
such as the hair, eyes, and mouth. When the subject subsequently moved away from the board, the ellipse‚Äôs location
quickly stabilized onto the head.
Not only can the color module control zoom and handle
background clutter, it has the added advantage of a large
basin of attraction. For example, Figure 3 shows a situation
in which the ellipse was barely hanging on to the head
because of the subject‚Äôsquick acceleration and thecamera‚Äôs
slow dynamics (notice the skin-colored board behind the
subject). Although the gradient module was distracted by
the background and tried to pull the ellipse to the left, the
color module correctly pulled to the right, even though
a large percentage of the ellipse‚Äôs interior contained the
potentially distracting skin-colored board.
Module complementarity
It is therefore clear that the color module greatly helps the
gradient module by ignoring background clutter, correctly
Figure 4: People in the experiments.
handling changes in scale, and providing a larger basin of
attraction.
In a similar manner, sometimes the gradient
module helps the color module.
One situation occurred when the subject turned his back
toward the camera and moved farther or closer. Left to
itself, the color module had difÔ¨Åculty Ô¨Ånding the correct
scale when the face was thus invisible; but with the gradient module, the tracker was able to succeed. In another
scenario, the subject moved away from the camera while
the skin-colored board remained behind him. Althoughthe
color module did not lose the position of the subject due
to his hair, it was unable to properly scale the head since
the board looked like skin. By adding the gradient module,
the ellipse correctly scaled to the subject‚Äôs head and the
camera zoomed in. Finally, the color module was found to
occasionally slip down to the subject‚Äôs neck, a problem that
was solved by adding the gradient module, which tended
to get a strong response from the outline of the top of the
Demonstrations of robust performance
To demonstrate the tracker‚Äôs robust behavior in various
situations,it was tested on twelve people with a wide variety
of facial complexion,hair color, amount of hair, head shape,
and type and color of shirt, as shown in Figure 4. Several
of the people wore spectacles, and one of them had a beard.
Due to hardware restrictions, all the testing was performed
in the same environment.
During the tests, the output of the tracker was used to
automatically control the camera‚Äôs pan, tilt, and zoom to
computing time per frame (ms)
+/‚àí8 pixels in x and y, +/‚àí1 pixel in size
Real time (30 Hz)
Figure 5: From left to right, the computing time for the
gradient magnitude, gradient dot product, color, color and
gradient magnitude, and color and gradient dot product
modules, for two different search ranges.
keep the subject centered in the Ô¨Åeld of view at a desired
A few snapshots from the various video clips are
shown in Figure 6. The tracker was able to handle full
360-degree rotation and up to 90-degree tilting of the head,
arbitrary camera motion, and severe occlusion (at least for
short periods of time). With multiple people in the scene the
tracker usually succeeded but was occasionally distracted
when the two faces occupied adjacent regions in the image.
For example, in the fourth row of the Ô¨Ågure the ellipse
temporarily preferred one of the background people but
quickly returned to the subject when his continued motion
caused the other person to be occluded. Had the subject
changed direction at that point, the tracker probably would
have lost him.
Computing times
Figure 5 shows the raw computing times using a 200 MHz
Pentium Pro microprocessor. Notice that increasing the
number of search locations by 257% caused only about
an 80% increase in computing time, which indicates that
the Ô¨Åxed time needed to convolve the image, transform the
color space, and set up the search is much greater than the
time needed to actually conduct the search.
Comparison with Previous Work
Compared with previous work, this tracker is the only one
of which we are aware that can handle signiÔ¨Åcant outof-plane rotation, arbitrary camera motion, textured foregrounds and backgrounds, and multiple moving people in
Simultaneous translation, occlusion, and out-of-plane rotation
Complete occlusion of the subject by another person
Zooming and rotation
Three people trying to steal the ellipse from the subject
Figure 6: Demonstration of the tracker‚Äôs performance in various situations. These and other MPEG sequences are available
from 
the background, all simultaneously.
Template- and neural network-based trackers , as well as trackers based on facial color , cannot handle severe out-of-plane rotation because
such a rotation causes the face to disappear. The colorbased techniques also tend to have difÔ¨Åculty with skincolored objects or other people in the background.
Trackers utilizing some form of background differencing either require a static camera
or restrict the camera‚Äôs motion to rotation about its focal point.
 Moreover, many of these techniques perform
motion-based Ô¨Ågure-ground segmentation, which tends to
fail when the camera zooms or when multipleobjects move
In , the camera may move occasionally but not continuously.
in the scene.
Reliabletrackingwas reported bycombininga templatebased tracker with stereo depth . However, besides the
additional hardware, it is not clear whether this system
would be able to handle multiple people at a similar depth
as the subject.
Also, promising results have been achieved using a
shape-based contour tracker that is more sophisticated
than ours because it allows the shape to deform over time.
However, in its present implementation the tracking criterion is the gradient magnitude alone, which will probably
fail with quick movements in cluttered scenes.
Finally, it must be mentioned that some of the systems
cited above contain multiple modules. However, it is often the case that one of the modules utilizes backgrounddifferencing and another uses facial color. Although the
former can handle out-of-plane rotation and the latter can
handle a dynamic camera, the system resulting from combining the two cannot handle both situations simultaneously.
Conclusion
In this paper we have presented a method for robustly
tracking a person‚Äôs head undergoingcomplex motions such
as 360-degree out-of-plane rotation, severe occlusion, and
scale changes in front of a dynamic, unstructured background. Robustness is achieved using two orthogonal modules, one based on the intensity gradient around the head‚Äôs
perimeter and another based on the color histogram of the
head‚Äôs interior.
One limitation of the current work is that the color histogram of the model is not adaptive. Therefore, changing
lighting conditions or automatic gain adjustments by the
camera will cause the color module to become confused.
To solve this problem,the histogram must be able to quickly
update itself because conditionscan change quickly (imagine the subject walking in front of a window through which
sunlight shines), but the ellipse does not always provide a
good segmentation of the head at each frame, thus preventing the update routine from completely trusting the pixels
inside the ellipse. An additional module is needed.
Another problem with the current system is that the
head‚Äôs acceleration is fairly limited when the background
is confusing, but this is really a limit of the 30 Hz NTSC
video signal and the speed of the computer than it is a
limitation of the algorithm. A higher temporal sampling
rate, coupled with a slightly faster machine, should cause a
noticeable improvement in this area.
Acknowledgments
This work was largely inspired by several discussions I had
withJ. Brian Burns. Many thanks to him and to the subjects
who willingly gave their time. S.D.G.