The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)
CBNet: A Novel Composite Backbone Network Architecture for Object Detection
Yudong Liu,1 Yongtao Wang,1∗Siwei Wang,1 Tingting Liang,1 Qijie Zhao,1 Zhi Tang,1 Haibin Ling2
1Wangxuan Institute of Computer Technology, Peking University
2Department of Computer Science, Stony Brook University
{bahuangliuhe, wyt, wangsiwei17, liangtingting, zhaoqijie, tangzhi}@pku.edu.cn
 
In existing CNN based detectors, the backbone network is a
very important component for basic feature1 extraction, and
the performance of the detectors highly depends on it. In this
paper, we aim to achieve better detection performance by
building a more powerful backbone from existing ones like
ResNet and ResNeXt. Speciﬁcally, we propose a novel strategy for assembling multiple identical backbones by composite connections between the adjacent backbones, to form a
more powerful backbone named Composite Backbone Network (CBNet). In this way, CBNet iteratively feeds the output features of the previous backbone, namely high-level features, as part of input features to the succeeding backbone,
in a stage-by-stage fashion, and ﬁnally the feature maps of
the last backbone (named Lead Backbone) are used for object
detection. We show that CBNet can be very easily integrated
into most state-of-the-art detectors and signiﬁcantly improve
their performances. For example, it boosts the mAP of FPN,
Mask R-CNN and Cascade R-CNN on the COCO dataset by
about 1.5 to 3.0 points. Moreover, experimental results show
that the instance segmentation results can be improved as
well. Speciﬁcally, by simply integrating the proposed CBNet
into the baseline detector Cascade Mask R-CNN, we achieve
a new state-of-the-art result on COCO dataset (mAP of 53.3)
with a single model, which demonstrates great effectiveness
of the proposed CBNet architecture. Code will be made available at 
Introduction
Object detection is one of the most fundamental problems in
computer vision, which can serve a wide range of applications such as autonomous driving, intelligent video surveillance, remote sensing, and so on. In recent years, great progresses have been made for object detection thanks to the
booming development of the deep convolutional networks
 , and a few excellent detectors have been proposed, e.g., SSD . All rights reserved.
1Here and after, ”basic feature” refers in particular to the features that are extracted by the backbone network and used as the input to other functional modules in the detector like detection head,
RPN and FPN.
Figure 1: Illustration of the proposed Composite Backbone
Network (CBNet) architecture for object detection. CBNet
assembles multiple identical backbones (Assistant Backbones and Lead Backbone) by composite connections between the parallel stages of the adjacent backbones. This
way, it iteratively feeds the output features of the previous
backbone as part of input to the succeeding backbone, in
a stage-by-stage fashion, and ﬁnally outputs the features of
the last backbone (i.e., Lead Backbone) for object detection.
The red arrows represent composite connections.
2016), Faster R-CNN , RetinaNet , FPN , Mask R-CNN , Cascade R-CNN , etc.
Generally speaking, in a typical CNN based object detector, a backbone network is used to extract basic features for detecting objects, which is usually designed for
the image classiﬁcation task and pretrained on the ImageNet
dataset . Not surprisingly, if a backbone
can extract more representational features, its host detector will perform better accordingly. In other words, a more
powerful backbone can bring better detection performance,
as demonstrated in Table 1. Hence, starting from AlexNet
 , deeper and larger
(i.e., more powerful) backbones have been exploited by the
Backbone Network
ResNeXt101
ResNeXt152
Dual-ResNeXt152 (ours)
Triple-ResNeXt152 (ours)
Table 1: Results of the state-of-the-art detector Cascade
Mask R-CNN on the COCO test-dev dataset with different existing backbones and the proposed Composite Backbone Networks (Dual-ResNeXt152
and Triple-ResNeXt152), which are reproduced by Detectron . It
shows that deeper and larger backbones bring better detection performance, while our Composite Backbone Network
architecture can further strengthen the existing very powerful backbones for object detection such as ResNeXt152.
state-of-the-art detectors, such as VGG , ResNet , DenseNet , ResNeXt . Despite encouraging
results achieved by the state-of-the-art detectors based on
deep and large backbones, there is still plenty of room for
performance improvement. Moreover, it is very expensive
to achieve better detection performance by designing a novel
more powerful backbone and pre-training it on ImageNet. In
addition, since almost all of the existing backbone networks
are originally designed for image classiﬁcation, directly employing them to extract basic features for object detection
may result in suboptimal performance.
To deal with the issues mentioned above, as illustrated in
Figure 1, we propose to assemble multiple identical backbones, in a novel way, to build a more powerful backbone
for object detection. In particular, the assembled backbones
are treated as a whole that we call Composite Backbone Network (CBNet). More speciﬁcally, CBNet consists of multiple identical backbones (specially called Assistant Backbones and Lead Backbone) and composite connections between neighbor backbones. From left to right, the output of
each stage in an Assistant Backbone, namely higher-level
features, ﬂows to the parallel stage of the succeeding backbone as part of inputs through composite connections. Finally, the feature maps of the last backbone named Lead
Backbone are used for object detection. Obviously, the features extracted by CBNet for object detection fuse the highlevel and low-level features of multiple backbones, hence
improve the detection performance. It is worth mentioning
that, we do not need to pretrain CBNet for training a detector integrated with it. For instead, we only need to initialize each assembled backbone of CBNet with the pretrained
model of the single backbone that is widely and freely available today, such as ResNet and ResNeXt. In other words,
adopting the proposed CBNet is more economical and efﬁcient than designing a novel more powerful backbone and
pre-training it on ImageNet.
On the widely tested MS-COCO benchmark , we conduct experiments by applying the proposed
Composite Backbone Network to several state-of-the-art object detectors, such as FPN , Mask R-
CNN and Cascade R-CNN . Experimental results show that the mAPs
of all the detectors consistently increase by 1.5 to 3.0
points, which demonstrates the effectiveness of our Composite Backbone Network. Moreover, with our Composite Backbone Network, the results of instance segmentation
are also improved. Speciﬁcally, using Triple-ResNeXt152,
i.e., Composite Backbone Network architecture of three
ResNeXt152 backbones, we achieve the
new state-of-the-art result on COCO dataset, that is, mAP of
53.3, outperforming all the published object detectors.
To summarize, the major contributions of this work are
• We propose a novel method to build a more powerful
backbone for object detection by assembling multiple
identical backbones, which can signiﬁcantly improve the
performances of various state-of-the-art detectors.
• We achieve the new state-of-the-art result on the
MSCOCO dataset with a single model, that is, the mAP
of 53.3 for object detection.
In the rest of the paper, after reviewing related work in
Sec. 2, we describe in details the proposed CBNet for object detection in Sec. 3. Then, we report the experimental
validation in Sec. 4, and draw the conclusion in Sec. 5.
Related work
Object detection Object detection is a fundamental problem
in computer vision. The state-of-the-art methods for general
object detection can be brieﬂy categorized into two major
branches. The ﬁrst branch contains one-stage methods such
as YOLO , SSD , Retinanet , FSAF 
and NAS-FPN . The other branch
contains two-stage methods such as Faster R-CNN , FPN , Mask R-CNN , Cascade R-CNN and Libra R-CNN . Although breakthrough has
been made and encouraging results have been achieved by
the recent CNN based detectors, there is still large room
for performance improvement. For example, on MS COCO
benchmark , the best publicly reported mAP
is only 52.5 , which is achieved by model
ensemble of four detectors.
Backbone for Object detection Backbone is a very important component of a CNN based detector to extract basic
features for object detection. Following the original works
 and OverFeat ) of applying deep learning to object detection, almost all of the recent detectors adopt the pretraining and ﬁne-tuning paradigm, that is, directly use the networks which are pre-trained for ImageNet classiﬁcation task
as their backbones. For instance, VGG , ResNet , ResNeXt are widely used by the state-of-the-art detectors. Since
these backbone networks are originally designed for image
Figure 2: Comparison between (a). our proposed CBNet architecture (K = 2) and (b). the unrolled architecture of
RCNN (T = 2).
classiﬁcation task, directly employing them to extract basic
features for object detection may result in suboptimal performance. More recently, two sophisticatedly designed backbones, i.e., DetNet and FishNet , are proposed for object detection. These two backbones are speciﬁcally designed for the object detection task,
and they still need to be pretrained for ImageNet classiﬁcation task before training (ﬁne tuning) the detector based on
them. It is well known that designing and pretraining a novel
and powerful backbone like them requires much manpower
and computation cost. In an alternative way, we propose a
more economic and efﬁcient solution to build a more powerful backbone for object detection, by assembling multiple
identical existing backbones (e.g., ResNet and ResNeXt).
Recurrent Convolution Neural Network As shown in
Figure 2, the proposed architecture of Composite Backbone
Network shares some similarity with an unfolded recurrent
convolutional neural network (RCNN) 
architecture, but is signiﬁcantly different from RCNN. First,
as illustrated in Figure 2, the architecture of CBNet is very
different, especially for the connections between the parallel stages. Second, in RCNN, the parallel stages of different
time steps share the parameters, while in the proposed CB-
Net, the parallel stages of backbones do not share the parameters. Moreover, if we use RCNN as the backbone of a
detector, we need to pretrain it on ImageNet. By contrast,
no pretraining for CBNet is needed, since it instead takes
existing backbones directly.
Proposed method
This section elaborates the proposed CBNet in detail. We
ﬁrst describe its architecture and variants in Section 3 and
Section 3 respectively. And then, we describe the structure
of detection network with CBNet in Section 3.
Architecture of CBNet
The architecture of the proposed CBNet consists of K identical backbones (K ≥2). Specially, we call the case of K
= 2 (as shown in Figure 2.a) as Dual-Backbone (DB) for
simplicity, and the case of K=3 as Triple-Backbone (TB).
As illustrated in Figure 1, the CBNet architecture consists of two types of backbones: the Lead Backbone BK and
the Assistant Backbones B1, B2, ..., BK−1. Each backbone
comprises L stages (generally L = 5), and each stage consists of several convolutional layers with feature maps of the
same size. The l-th stage of the backbone implements a nonlinear transformation F l(·).
In the traditional convolutional network with only one
backbone, the l-th stage takes the output (denoted as xl−1) of
the previous l −1-th stage as input, which can be expressed
xl = F l(xl−1), l ≥2.
Unlike this, in the CBNet architecture, we novelly employ
Assistant Backbones B1, B2, ..., Bk−1 to enhance the features of the Lead Backbone Bk, by iteratively feeding the
output features of the previous backbone as part of input features to the succeeding backbone, in a stage-by-stage fashion. To be more speciﬁc, the input of the l-th stage of the
backbone Bk is the fusion of the output of the previous l−1th stage of Bk (denoted as xl−1
) and the output of the parallel stage of the previous backbone Bk−1 (denoted as xl
This operation can be formulated as following:
k−1)), l ≥2,
where g(·) denotes the composite connection, which consists of a 1×1 convolutional layer and batch normalization
layer to reduce the channels and an upsample operation. As
a result, the output features of the l-th stage in Bk−1 is transformed to the input of the same stage in Bk, and added to the
original input feature maps to go through the corresponding
layers. Considering that this composition style feeds the output of the adjacent higher-level stage of the previous backbone to the succeeding backbone, we call it as Adjacent
Higher-Level Composition (AHLC).
For object detection task, only the output of Lead Backbone xl
K(l = 2, 3, . . . , L) are taken as the input of the
RPN/detection head, while the output of each stage of Assistant Backbones is forwarded into its adjacent backbone.
Moreover, the B1, B2, ..., BK−1 in CBNet can adopt various backbone architectures, such as or
ResNeXt , and can be initialized from the
pre-trained model of the single backbone directly.
Other possible composite styles
Same Level Composition (SLC)
An intuitive and simple
composite style is to fuse the output features from the same
stage of backbones. This operation of Same Level Composite (SLC) can be formulated as:
k−1), l ≥2.
To be more speciﬁc, Figure 3.b illustrates the structure of
SLC when K = 2.
Figure 3: Four kinds of composite styles for Dual-Backbone architecture (an Assistant Backbone and a Lead Backbone). (a)
Adjacent Higher-Level Composition (AHLC). (b) Same Level Composition (SLC). (c) Adjacent Lower-Level Composition
(ALLC). (d) Dense Higher-Level Composition (DHLC). The composite connection denotes in blue boxes represents some
simple operations, i.e., element-wise operation, scaling, 1×1 Conv layer and BN layer.
Adjacent Lower-Level Composition (ALLC)
to AHLC, another intuitive composite style is to feed the
output of the adjacent lower-level stage of the previous backbone to the succeeding backbone. This operation of Adjacent Lower-Level Composition (ALLC). The operation of
Inverse Level Composite (ILC) can be formulated as:
k−1)), l ≥3.
To be more speciﬁc, Figure 3.c illustrates the structure of
ILC when K = 2.
Higher-Level
Composition
DenseNet , each layer is connected
to all subsequent layers to build a dense connection in
a stage. Inspired by it, we can utilize dense composite
connection in our CBNet architecture. The operation of
DHLC can be expressed as follows:
As shown in Figure 3.d, when K = 2, we assemble the features from all the higher-level stages in the Assistant Backbone, and add the composite features to the output features
of the previous stage in the Lead Backbone.
Architecture of detection network with CBNet
The CBNet architecture is applicable with various off-theshelf object detectors without additional modiﬁcations to the
network architectures. In practice, we attach layers of the
Lead Backbone with functional networks, RPN , detection head .
Experiments
In this section, we present experimental results on the
bounding box detection task and instance segmentation
task of the challenging MS-COCO benchmark . Following the protocol in MS-COCO, we use the
trainval35k set for training, which is a union of 80k
images from the train split and a random 35k subset of images from the 40k image validation split. We report COCO
AP on the test-dev split for comparisons, which is tested
on the evaluation server.
Implementation details
Baselines methods in this paper are reproduced by ourselves
based on the Detectron framework . All
the baselines are trained with the single-scale strategy, except Cascade Mask R-CNN ResNeXt152. Speciﬁcally, the
short side of input image is resized to 800, and the longer
side is limited to 1,333. We conduct experiments on a machine with 4 NVIDIA Titan X GPUs, CUDA 9.2 and cuDNN
7.1.4 for most experiments. In addition, we train Cascade
Mask R-CNN with Dual-ResNeXt152 on a machine with 4
NVIDIA P40 GPUs and Cascade Mask R-CNN with Triple-
ResNeXt152 on a machine with 4 NVIDIA V100 GPUs.
The data augmentation is simply ﬂipping the images. For
most of the original baselines, batch size on a single GPU
is two images. Due to the limitation of GPU memory for
CBNet, we put one image on each GPU for training the detectors using CBNet. Meanwhile, we set the initial learning
rate as half of the default value and train for the same epochs
as the original baselines. It is worth noting that, we do not
change any other conﬁguration of these baselines except the
reduction of the initial learning rate and batch size.
During the inference, we completely use the conﬁguration in the original baselines . For Cascade Mask R-CNN with different backbones, we run both
single-scale test and multi-scale test. And for other baseline
detectors, we run single-scale test, in which the short side of
input image is resized to 800, and the longer side is limited
to 1,333. It is noted that we do not utilize Soft-NMS during the inference for fair comparison .
Baseline detector
FPN + ResNet101
Mask R-CNN + ResNet101
Cascade R-CNN + ResNet101
Cascade Mask R-CNN + ResNeXt152
Table 2: Detection results on the MS-COCO test-dev set. We report both object detection and instance segmentation results
on four kinds of detectors to demonstrate the effectiveness of CBNet. Single: with/without baseline backbone. DB: with/without
Dual-Backbone architecture. TB: with/without Triple-Backbone architecture. Column 5-7 show the results of object detection
while column 8-10 show the results of instance segmentation.
Detection results
To demonstrate the effectiveness of the proposed CBNet, we
conduct a series of experiments with the baselines of stateof-the-art detectors, i.e., FPN , Mask R-
CNN and Cascade R-CNN , and the results are reported in Table 2. In
each row of Table 2, we compare a baseline ) with its variants using the
proposed CBNet, and one can see that our CBNet consistently improves all of these baselines with a signiﬁcant margin. More speciﬁcally, the mAPs of these baselines increase
by 1.5 to 3 percent.
Furthermore, as presented in Table 3, a new state-ofthe-art detection result of 53.3 mAP on the MS-COCO
benchmark is achieved by Cascade Mask R-CNN baseline
equipped with the proposed CBNet. Notably, this result is
achieved just by single model, without any other improvement for the baseline besides taking CBNet as backbone.
Hence, this result demonstrates great effectiveness of the
proposed CBNet architecture.
Moreover, as shown in Table 2, the proposed CBNet also
improves the performances of the baselines for instance segmentation. Compared with bounding boxes prediction (i.e.,
object detection), pixel-wise classiﬁcation (i.e., instance segmentation) tends to be more difﬁcult and requires more representational features. And these results demonstrate the effectiveness of CBNet again.
Comparisons of different composite styles
We further conduct experiments to compare the suggested
composite style AHLC with other possible composite styles
illustrated in Figure 3, including SLC, ALLC, and DHLC.
All of these experiments are conducted based on the Dual-
Backbone architecture and the baseline of FPN ResNet101.
SLC v.s. AHLC As presented in Table 4, SLC gets even
worse result than the original baseline. We think the major
reason is that the architecture of SLC will bring serious parameter redundancy. To be more speciﬁc, the features extracted by the same stage of the two backbones in CBNet are
similar, hence SLC cannot learn more semantic information
than using single backbone. In other words, the network parameters are not fully utilized, but bring much difﬁculty on
training, leading to a worse result.
ALLC v.s. AHLC As shown in Table 4, there is a great
gap between ALLC and AHLC. We infer that, in our CBNet,
if we directly add the lower-level (i.e., shallower) features of
the previous backbone to the higher-level (i.e., deeper) ones
of the succeeding backbone, the semantic information of the
latter ones will be largely harmed. On the contrary, if we add
the deeper features of the previous backbone to the shallow
ones of the succeeding backbone, the semantic information
of the latter ones can be largely enhanced.
DHLC v.s. AHLC The results in Table 4 show that DHLC
does not bring performance improvement as AHLC, although it adds more composite connections than AHLC.
We infer that, the success of Composite Backbone Network
lies mainly in the composite connections between adjacent
stages, while the other composite connections do not enrich
much feature since they are too far away.
Obviously, CBNets of these composite styles have same
amount of the network parameter (i.e., about twice amount
of the network parameters than single backbone), but only
AHLC brings optimal detection performance improvement.
These experiment results prove that only increasing parameters or adding additional backbone may not bring better
result. Moreover, these experiment also show that composite
connections should be added properly. Hence, these experiment results actually demonstrate that the suggested composite style AHLC is effective and nontrivial.
Sharing weights for CBNet
Due to the fuse of more backbones, CBNet increases the
number of network parameters. To further demonstrate that
the improvement of detection performance mainly comes
from the composite architecture rather than the increase of
network parameters, we conduct experiments on FPN, with
one stage:
SSD512 
RetinaNet 
ResNeXt101
ReﬁneDet *
CornerNet *
Hourglass-104
M2Det *
FSAF *
ResNext-101
NAS-FPN 
two stage:
Faster R-CNN 
R-FCN 
FPN 
Mask R-CNN 
Cascade R-CNN 
Libra R-CNN 
ResNext-101
SNIP (model ensemble) *
SINPER *
Cascade Mask R-CNN *
ResNeXt152
MegDet (model ensemble) *
ours:(single model)
Cascade Mask R-CNN *
Dual-ResNeXt152
Cascade Mask R-CNN *
Triple-ResNeXt152
Table 3: Object detection comparison between our methods and state-of-the-art detectors on COCO test-dev set. * : utilizing
multi-scale testing.
Composite style
Table 4: Comparison between different composite styles,
the baseline is FPN ResNet101 . DB:
with/without Dual-Backbone. ”SLC” represents Same Level
Composition, ”ALLC” represents Adajacent Lower-Level
Composition, ”DHLC” is Dense Higher-Level Composition
and ”AHLC” is Adjacent Higher-Level Composition.
the conﬁguration of sharing the weighs of two backbones
in Dual-ResNet101, and the results are shown in Table 5.
We can see that when sharing the weights of backbones in
CBNet, the increment of parameters is negligible, but the
detection result is still much better than the baseline (e.g.,
mAP 40.4 v.s. 39.4). However, when we do not share the
weights, the improvement is minor (mAP from 40.4 to 41.0),
which proves that it is the composite architecture that boosts
the performance dominantly, rather than the increase of network parameters.
Number of backbones in CBNet
We conduct experiments to investigate the relationship between the number of backbones in CBNet and the detection
performance by taking FPN-ResNet101 as the baseline, and
the results are shown in Figure 4. It can be noted that the
detection mAP steadily increases with the number of back-
Baseline detector
FPN + ResNet101
Table 5: Comparison of with/without sharing weights
for Dual-Backbone architecture. DB: with/without Dual-
Backbone. Share: with/without sharing weights. APbox: detection results on COCO test-dev dataset. mb: the model
bones, and tends to converge when the number of backbones
reaches three. Hence, considering the speed and memory
cost, we suggest to use Dual-Backbone and Triple-Backbone
architectures.
An accelerated version of CBNet
The major drawback of the proposed CBNet is that it will
slows down the inference speed of the baseline detector
since it uses more backbones to extract features thus increases the computation complexity. For example, as shown
in Table 6, DB increases the AP of FPN by 1.6 percent but
slows down the detection speed from 8.1 fps to 5.5 fps. To alleviate this problem, we further propose an accelerated version of the CBNet as illustrated in Figure 5, by removing
the two early stages of the Assistant Backbone. As demonstrated in Table 6, this accelerated version can signiﬁcantly
improve the speed (from 5.5 fps to 6.9 fps) while not harming the detection accuracy (i.e., AP) a lot (from 41.0 to 40.8).
Figure 4: Object detection results on the MS-COCO
test-dev dataset using different numbers of backbones
in CBNet architecture based on FPN ResNet101.
Baseline detector
FPN + ResNet101
Table 6: Performance comparison between the original
DB and the accelerated version. DB: with/without Dual-
Backbone. Ψ: with/without the acceleration modiﬁcation of
the CBNet architecture illustrated in Figure 5.
Effectiveness of basic feature enhancement by
We think the critical reason for CBNet to outperform the
single backbone network is: it can extract more representational basic features than the original single backbone network, which is originally designed for classiﬁcation problem. To verify this, as illustrated in Figure 6, we visualize
and compare the intermediate feature maps extracted by our
CBNet and the original single backbone in the detectors for
some examples. The example image in Figure 6 contains
two foreground objects: a person and a tennis ball. Obviously, the person is a large-size object and the tennis ball is
a small-size one. Hence, we correspondingly visualize the
large scale feature maps (for detecting small objects) and
the small scale feature maps (for detecting large objects) extracted by our CBNet and the original single backbone. One
can see that, the feature maps extracted by our CBNet consistently have stronger activation values at the foreground
object and weaker activation values at the background. This
visualization example shows that our CBNet is more effective to extract representational basic features for object detection.
Figure 5: An accelerated version of CBNet (K = 2).
Figure 6: Visualization comparison of the features extracted
by our CBNet (Dual-ResNet101) and the original backbone
(ResNet101). The baseline detector is FPN-ResNet101. For
each backbone, we visualize the Res2 and Res5 according
to the size of the foreground objects, by averaging feature
maps along channel dimension. Best viewed in color.
Conclusion
In this paper, a novel network architecture called Composite
Backbone Network (CBNet) is proposed to boost the performance of state-of-the-art object detectors. CBNet consists
of a series of backbones with same network structure and
uses composite connections to link these backbones. Specifically, the output of each stage in a previous backbone ﬂows
to the parallel stage of the succeeding backbone as part of
inputs through composite connections. Finally, the feature
maps of the last backbone namely Lead Backbone are used
for object detection. Extensive experimental results demonstrate that the proposed CBNet is beneﬁcial for many stateof-the-art detectors, such as FPN, Mask R-CNN, and Cascade R-CNN, to improve their detection accuracy. To be
more speciﬁc, the mAPs of the detectors mentioned above
on the COCO dataset are increased by about 1.5 to 3 points,
and a new state-of-the art result on COCO with the mAP of
53.3 is achieved by simply integrating CBNet into the Cascade Mask R-CNN baseline. Simultaneously, experimental
results show that it is also very effective to improve the instance segmentation performance. Additional ablation studies further demonstrate the effectiveness of the proposed architecture and the composite connection module.
Acknowledgment
This work is supported by National Natural Science Foundation of China under Grant 61673029. This work is also a research achievement of Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology). Ling is supported in part by
Yahoo Faculty Research and Engagement Program Award
and Amazon AWS Machine Learning Research Award.