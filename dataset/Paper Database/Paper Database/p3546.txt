Appears in 3rd International Workshop on Semantic Learning and Applications in Multimedia, Anchorage, 2008.
A Study of Query by Semantic Example
Nikhil Rasiwasia
Nuno Vasconcelos
Department of Electrical and Computer Engineering
University of California, San Diego
 , 
In recent years, query-by-semantic-example (QBSE) has
become a popular approach to do content based image retrieval .
QBSE extends the well established
query-by-example retrieval paradigm to the semantic domain. While various authors have pointed out the beneﬁts of
QBSE, there are still various open questions with respect to
this paradigm. These include a lack of precise understanding of how the overall performance depends on various different parameters of the system. In this work, we present a
systematic experimental study of the QBSE framework. This
can be broadly divided into three categories. First, we examine the space of low-level visual features for its effects
on the retrieval performance. Second, we study the space
of learned semantic concepts, herein denoted as the “semantic space”, and show that not all semantic concepts are
equally informative for retrieval. Finally, we present a study
of the intrinsic structure of the semantic space, by analyzing
the contextual relationships between semantic concepts and
show that this intrinsic structure is crucial for the performance improvements.
1. Introduction
Content based image retrieval has been an active subject
of research over the last decades , when three different retrieval paradigms have gained popularity. In the early years,
the predominant paradigm was query-by-visual-example
(QBVE) . Under QBVE, each image is decomposed into a number of low-level visual features (e.g.
color, texture or shape histograms) and retrieval is based
on an example (query) image. One signiﬁcant limitation of
this paradigm is that the similarity of low-level image descriptors does not always correlate with human judgments
of similarity. This motivated the introduction of query-bykeyword paradigm . Under this paradigm, users
specify their queries through a natural language description
of the desired concepts. Such a paradigm requires the images to be annotated with semantic keywords. Since manual
image annotation is a labor intensive process, research was
focused on semantic labeling systems . The advantages of query-by-keyword lies in its ability to perform
retrieval at a higher level of query abstraction. However, it
is limited by the size of the vocabulary of concepts which
the retrieval system is trained to recognize.
Realizing that the shortcomings and advantages of
QVBE and query-by-keyword are in many respects complementary, several authors have proposed their combination
which is rapidly gaining popularity .
This combination extends the query-by-example paradigm
to the semantic domain, and can be formulated as a two
stage process. In the ﬁrst stage, as is common in queryby-keyword, images are fed to a semantic labeling system which detects pre-deﬁned semantic concepts. An image is then represented as a vector of posterior concept
probabilities.
These probabilities can be interpreted as
high-level semantic features, rendered by projection of the
image onto the abstract space of semantic concepts supported by the labeling system. This space is commonly referred to as the “semantic space” or the “model
space” . The second stage performs all classiﬁcation decisions on this higher-level semantic space, using the
query-by-example principle: the concept probability vector of the query image is used to ﬁnd the database images
with concept distributions closest to that of the query. Using the terminology of , we refer to this framework as
“query-by-semantic-example” (QBSE) in the remainder of
this work.
While various authors have pointed out the beneﬁts of
QBSE, there are still various open questions with respect to
this paradigm. These include a lack of precise understanding of how the overall performance depends on the accuracy
of each of the stages, and how the performance improvements are related to the structure of the intermediate semantic space. In this work, we present the results of a systematic
experimental study of the performance of a QBSE system,
which addresses these questions. The experiments undertaken can be broadly divided into three categories: studies
of how 1) the low-level visual space, and 2) the high-level
semantic space affect the overall retrieval performance, and
3) a study of the intrinsic structure of the semantic space. To
analyze the impact of the low-level visual space, we have
built semantic spaces from various combinations of standard representations for color and texture. With regards to
color, we consider a number of colorspaces, viz. “YBR”
(luminance, normalized blue, normalized red), perceptually
uniform “LAB”, “HSV” (hue, saturation, luminance) and
“Y” (luminance only). In what concerns texture, we apply
a standard feature transformation (in this paper we use the
discrete cosine transform, although similar results were obtained with wavelets) and vary the number of dimensions
in a coarse-to-ﬁne manner. By varying the dimensionality
(adding more or less high-frequencies) it is possible to vary
the accuracy of the low-level visual representation, and examine its impact on the overall retrieval accuracy.
To analyze the impact of the high-level semantic space,
we then vary the dimensions of the latter, by gradually eliminating non-informative semantic features. We show that
the overall retrieval performance is directly proportional
to the number of informative dimensions of the semantic
space. Finally, we characterize the intrinsic structure of this
space, by analyzing contextual relationships between concepts. We also show that these relationships play a crucial
role in the retrieval operation. This is further substantiated
by building a semantic space devoid of any (meaningful)
structure, which is shown to obliterate the beneﬁts (in retrieval accuracy) of QBSE over QBVE.
The paper is organized as follows. Section 2 discusses
the related work on semantic spaces and QBSE. In Section 3, we review implementations of QBVE and
query-by-keyword , based on the minimum probability
of error (MPE) formulation of image retrieval . This
MPE formulation has also been successfully applied to
QBSE , which we review in Section 4. An extensive experimental study of the performance of QBSE is presented
in Section 5. Finally, we present conclusions, and some
ideas for future research in Section 6.
2. Related Work
The idea of representing documents on semantic spaces
is commonly used in information retrieval .
In image retrieval, earliest efforts on building semantic spaces
were based on semantic information extracted from metadata . Later on, semantic spaces were also constructed
with resort to active learning, based on user relevance feedback . However, it is not always clear how the learned
semantic information could be combined with the visual
search at the core of the retrieval system.
A solution to this was pioneered by Smith et al. by
extending query-by-example to the semantic domain. This
was done by learning a semantic space, by learning a separate statistical model for each concept, and performing
Figure 1. An image and its associated semantic representation.
Note that, while most of the concepts of largest probability are
present in the image, signiﬁcant probability is also assigned to
“bridge” and “arch”. This is due to the presence of a geometric
structure similar to that of “bridge” and “arch”, shown on the image close-up.
query-by-example in the space of resulting semantic concepts. They later extended QBSE to perform retrieval on
video databases in . A semantic space representation of images was also used by Lu et al. in to perform
automatic image annotation, rather than image retrieval. A
QBSE system based on the semantic labeling algorithm
of was presented in .
The authors highlight the
superiority of QBSE over QBVE on benchmark datasets.
In , the authors showed that this superiority also holds
outside the space of learned semantic concepts, using multiple image queries. Another approach to QBSE, using the
semantic labeling system of , is presented in .
Although laying the foundations for QBSE, these previous works lack a systematic study of the QBSE paradigm.
In this work, using the QBSE implementation of , we
address this problem by studying some of the parameters
that affect the performance of a QBSE system. In particular, we examine the dependence of the retrieval performance
on both the low-level visual space and the high-level semantic space. We also characterize the intrinsic structure of the
semantic space, by analyzing the contextual relationships
between the semantic concepts. We use the implementation
of , because it allows the control of various parameters of the system, for example, the dimensions of the two
spaces, in a systematic and ﬁne-grained manner.
3. Minimum probability of error retrieval
The retrieval architecture adopted for the implementation of all retrieval strategies discussed in this work is that
of minimum probability of error (MPE) retrieval . We
adopt this architecture as it has been shown to perform
well in all retrieval contexts discussed herein: QBVE ,
query-by-keyword and QBSE . Moreover, it is also
conducive to the examination of various relevant parameters of a QBSE system. We start by brieﬂy reviewing this
architecture.
3.1. Visual-level retrieval system
Under the MPE framework, images are characterized as
observations from a random variable X, deﬁned on some
visual feature space X. The starting point for an image retrieval system is an image database D = {I1, . . . , ID}. In
the absence of any labels, each image is considered an observation from a different class. The class is determined by
a random variable Y deﬁned on {1, . . . , D}. Given a query
image Iq, the MPE decision rule for retrieval is to assign it
to the class of largest posterior probability, i.e.
y∗= arg max
PY |X(y|Iq).
At the visual level, each image is represented as a set of
n feature vectors I = {x1, . . . , xn}, xi ∈X. It is assumed that the feature vectors which compose any image
I are sampled independently.
PX|Y (I|y) =
PX|Y(xj|y).
Although any type of visual features are acceptable, we only
consider localized features, i.e., features of limited spatial
In this work, the distributions PX|Y (x|y) are modeled
as Gaussian mixtures. The parameters of the distributions
are learned from the training sample (the n feature vectors
{x1, . . . , xn} per image) using the well known expectationmaximization (EM) algorithm.
Image retrieval is based on the mapping g : X
{1, . . . , D} of (1), implemented by combining (2) and
Bayes rule. Although any prior class distribution PY (i) can
be supported, we assume a uniform distribution. In the remainder of this work we refer to the nearest-neighbor operation of (1), at the visual level, as query-by-visual-example
3.2. Semantic-level retrieval system
A semantic-level retrieval system augments the database D with a vocabulary L
{w1, . . . , wL} of
semantic concepts or keywords wi,
and each image
with a pre-speciﬁed caption ci,
{(I1, c1), . . . , (ID, cD)}.
Note that ci is a binary Ldimensional vector such that ci,j = 1 if the ith image was
annotated with the jth keyword in L.
The database is said to be weakly labeled if the absence
of a keyword from caption ci does not necessarily mean that
the associated concept is not present in Ii. For example, an
image containing “sky” may not be explicitly labeled with
that keyword. This is usually the case in practical scenarios, since each image is likely to be annotated with a small
caption that only identiﬁes the semantics deemed as most
relevant to the labeler. We assume weak labeling throughout this work.
Sem antic sim plex
x – database
vegetation
vegetation
Figure 2. Under QBSE the user provides a query image, posterior
probabilities (given the image) are computed for all concepts, and
the image represented by the concept probability distribution.
Concepts are determined by the random variable W,
which takes values in {1, . . . , L}, so that W = i if and only
if x is a sample from the concept wi. Each concept induces
a probability density {PX|W (x|i)}L
i=1 on X. At the semantic level images are assumed to be independently sampled
from concept distributions,
PX|W (I|w) =
PX|W(xj|w).
For each concept w, the semantic class density PX|W (x|w)
is learned from the set Dw of all training images labeled
with the wth label in L. In the implementation of , this
is based on a hierarchical procedure , which estimates
semantic class densities directly from the image densities
used for QBVE, in (2).
To support retrieval from the database using natural language queries, the unlabeled images are ﬁrst annotated with
the concepts of high posterior probability.
w∗= arg max
PW |X(w|I).
Given a query concept wq, the optimal retrieval decision (in
the MPE sense) is then to select the image for which wq has
the largest posterior annotation probability.
4. Query by Semantic Example
A QBSE retrieval system operates at the semantic level,
representing images by vectors of concept counts I =
(c1, . . . , cL)T . Each feature vector extracted from an image
is assumed to be sampled from the probability distribution
of a semantic class (concept), and ci is the number of feature vectors drawn from the ith concept. The count vector
for the yth image is drawn from a multinomial variable T
of parameters πy = (π1
y, . . . , πL
PT|Y (I|y; πy)
y is the probability that an image feature vector
is drawn from the ith concept.
Given an image I =
{x1, . . . , xn}, the posterior concept probabilities
πw = PW |X(w|I)
are maximum a posteriori estimates of the parameters πi,
and can be computed by combining (3) and Bayes rule, assuming a uniform prior concept distribution PW (w).
The random variable T is the result of a feature transformation from the space of visual features X to the Ldimensional probability simplex SL. This mapping establishes a one-to-one correspondence between images and
points πy ∈SL. We refer to the probability vector πy
as the semantic multinomial (SMN) that characterizes the
yth image. For example, in Fig. 1 this is a 371-dimensional
The QBSE system, then performs a nearest neighbor operation on the simplex SL, according to a similarity mapping f : SL →{1, . . . , D} such that
f(π) = arg min
where π is the query SMN, πy the SMN of the yth database
image, and d(·, ·) an appropriate dissimilarity function. In
this work, the dissimilarity between two SMNs, π and π′ is
measured using the Kullback-Leibler divergence, i.e.
d(π, π′) =
This is the asymptotic limit of (1), when Y is uniformly
distributed. Similarity matching in semantic space is also
illustrated in Fig. 2, which depicts a query and the two closest database matches.
The mapping of visual features to the L-dimensional
probability simplex SL can be seen as an abstract projection of the image onto a semantic space where each concept
probability πw, w = 1, . . . , L can be thought of as a semantic feature, as illustrated by Fig. 2. Features (semantic
concepts) that are not the part of semantic vocabulary de-
ﬁne directions that are orthogonal to this semantic space.
Their projection onto the learned semantic simplex enables
QBSE to generalize beyond the known semantic concepts,
and hence achieves better performance even outside the semantic space. This is exempliﬁed by Fig. 8 where images of
‘construction’ (a concept absent from the semantic vocabulary) are successfully retrieved from the database. In this
case, the projection of ‘construction’ images on the learned
semantic simplex assigns higher probabilities to (known)
concepts such as ‘people’, ‘buildings’, ‘streets’, ‘tables’ etc.
Since these are an effective alternative characterization for
the ‘construction’ concept, the retrieval operation succeeds.
Table 1. Retrieval and Query Database
Semantic Space
# Retrieval Images
# Query Images
5. Experimental evaluation
In this section, we report on the experimental study of
the QBSE system. First, we examine the dependence of
retrieval performance on both the low-level visual and the
high-level semantic spaces. This is done by considering
two cases: 1) where the query and database images contain
semantic concepts known to the semantic labeling system,
and 2) where this is not true. We refer to the former as retrieval inside the semantic space and to the latter as retrieval
outside the semantic space. Next, we also present a study
of the structure of the semantic space, showing that it captures contextual relationships between semantic concepts.
This intrinsic structure is also shown to be essential for the
success of the overall retrieval operation. In all cases, performance is measured with precision-recall (PR) curves and
mean average precision (MAP) .
5.1. Databases
The study of a QBSE system requires three databases:
a training database, used by the semantic labeling system
to learn concept probabilities, a retrieval database, from
which images are to be retrieved, and a database of query
images, which plays the role of test set. All experiments are
conducted on datasets used in . Table 1 summarizes
the composition of the databases used. The retrieval database of Corel50 is used as the training database to learn the
semantic space.
Note that the use of multiple-image queries has been
shown to outperform single-image queries in . In this
work, we restrict our attention to single-image queries, as
the aim is not so much to maximize performance, but to
obtain a deeper understanding of the QBSE system.
5.2. Low-level visual space
In all experiments, images are normalized to a maximum
of 180 pixels on the longest side, keeping the aspect ratio
constant. To represent images at the low-level, they are converted to various colorspaces, including various 3-channel
colorspaces (“YBR”, “HSV”, and “Lab”) and one singlechannel colorspace (“Y”, luminance only). Image observations are derived from 8 × 8 patches obtained with a sliding
window, moved in a raster-scan fashion. A feature transformation is applied by computing the 8 × 8 discrete cosine transform (DCT) coefﬁcients per patch and color chan-
(a) Corel50
(b) Corel15
(c) Flickr18
Figure 3. PR curves achieved with different color spaces on the three retrieval databases. (a) Inside the semantic space (Corel50). (b,c)
Outside the semantic space (Corel15, Flickr18).
nel. These DCT coefﬁcients are then ordered by decreasing variance, producing a 64 dimensional feature vector.
For 3-channel colorspaces, features from different channels
are interleaved, e.g., the “YBR” channels are interleaved
according to a “YBRYBR...” pattern. The parameters of
the semantic class mixture hierarchies are learned in a subspace of these DCT coefﬁcients. We evaluate subspaces of
various dimensionalities, ranging from 3 to 64 dimensions
per channel. Typically, low-dimensional subspaces capture
low-frequency information, producing a coarse image representation. As the dimensionality increases, so does the
accuracy of the low-level visual representation.
this choice of features enables a number of possibilities for
color and texture representation: from perceptual to nonperceptual color spaces, to texture only, in each case controlling the amount of texture representation by varying the
subspace dimensionality.
Colorspace
Retrieval experiments were conducted with four different
colorspaces, viz.
“YBR”, “LAB”, “HSV”, “Y”. Fig. 3
presents the PR curves obtained on different databases. Inside the semantic space (Fig. 3(a)), the performance of 3channel colorspaces supersedes that of luminance only colorspace signiﬁcantly. This indicates, that the color correlations are a signiﬁcant source of information for this database. Among the different 3-channel colorspaces, “YBR”
performs better than the perceptually uniform “LAB” and
the cylindrical co-ordinate based “HSV” spaces. The MAP
scores for the three colorspaces are 0.197, 0.152 and 0.174
respectively, the chance performance stands at 0.0200.
Outside the semantic space, the experiments reveal a
different behavior (Fig. 3(b)(c)). Interestingly, the performance of the “Y” colorspace is only marginally lower than
those of the 3-channel colorspaces.
That is, using just
the “texture” (“Y” colorspace) information, the retrieval
system performs as well as when color is also available
(“texture+color” representation with any of the “YBR”,
112 128 144 160 176 192
Dimensions of low−level visual space
Figure 5. MAP scores of QBSE for different dimensions of the
low-level visual space across all the databases.
“LAB”, and “HSV” colorspaces). This suggests that only
the learned “texture” correlations are informative for generalization with respect to previously unseen concepts. If
true, this assertion would imply that color features capture
information that, while characteristic of the images in database, is not characteristic of the underlying concepts. This
could be due to the existence of certain global regularities
within each class in the database (e.g. most images taken at
certain times of the day or year) that create commonalities
of color distribution which, although artiﬁcial, are not easily
detected by visual inspection. It is an interesting assertion,
given the long history of research in color-based image retrieval. While further experiments will be required to reach
deﬁnitive conclusions, these results have lead us to adopt
the “YBR” colorspace in the remaining experiments. Fig. 4
shows a query and the corresponding retrieved images for
the ’YBR’ and ’Y’ colorspace.
Dimensionality of visual space
Since the visual representation is based on a subspace of the
192-dimensional space of DCT coefﬁcients, it is possible
to control the accuracy of visual representation by simply
varying the dimension of this subspace. As the number of
Query Image
Top 5 retrieved images using QBSE
Adventure Sailing
Figure 4. An example of a query and corresponding retrieved images from Corel15 dataset. The ﬁrst and the second row shows results
using “YBR” and “Y” colorspace respectively. This ﬁgure is best viewed in color.
visual features decreases, the performance of QBSE, tends
to degrade. Fig. 5 shows the MAP score as the subspace
dimension varies from 8 to 192 for the interleaved “YBR”
colorspace. The performance across the three databases are
qualitatively similar, it increases rapidly from 8 to 64 dimensions and then remains fairly stable over a large range
of dimensions. This suggests that 1) accuracy of low-level
visual space is an important parameter for retrieval, and 2)
the system is robust to the noise, introduced by the high frequency components of the DCT features. We use the ﬁrst
64 dimensions of the interleaved “YBR” colorspace for rest
of the experiments.
5.3. High-level Semantic space
We next study the dependence of QBSE performance on
the number of informative semantic dimensions. Assuming
that the dimensions of the learned semantic space are not
equally useful for retrieval, it should be possible to achieve
improved performance with feature selection.
It should,
nevertheless, be noted that standard feature extraction techniques, such as principal component analysis or latent semantic indexing, do not preserve the semantic meaning of
the dimensions of the space. To avoid this problem, we investigated the beneﬁts of feature selection by simply 1) ordering the semantic features by decreasing variance of their
posterior probabilities (over the retrieval database) and 2)
selecting the top k, for values of k ranging from 4 to 371.
Fig 6 shows the MAP score obtained on the three databases, as a function of k. In each ﬁgure, the right verticalaxis shows the percent of the variance (over the retrieval
database) explained by the top k features, as a function of k.
It can be observed that retrieval performance improves proportionally to the increase in the number of informative semantic dimensions. This is explained by the fact that more
features enable a greater diversity of contextual correlations
between concepts, and the similarity judgments are more
robust. However, there is a saturation effect, i.e. not all 371
semantic concepts are equally informative. In particular,
the MAP score saturated faster on Flickr18, Corel15 than
Table 2. Semantic feature pairs with highest mutual information.
Feature Pair
Feature Pair
’polar-bear’
’sun-sunset’
’beach-sand’
’stone-ruins’
’plane-jet’
’leaf-ﬂowers’
’light-restaurant’
’sky-tree’
’restaurant-tables’
’sunset-sea’
’statue-pillar’
’sky-beach’
’petals-leaf’
’sky-mountain’
’tree-mountain’
on Corel50, as almost all the variability is explained by less
than 100 concepts for the former while more than 200 are
needed for the latter. However, unlike most learning problems, the inclusion of uninformative features does not seem
to degrade retrieval performance. We have, therefore, used
all semantic features in the remaining experiments.
5.4. Structure of the semantic space
In this section we demonstrate that 1) the labeling
process does seem to produce a space with semantic structure, and 2) this semantic structure is a necessary condition
for the success of QBSE.
Relationship between semantic features
To unveil some of the structure of the semantic space, we
analyzed the relationship between pairs of semantic features, by measuring their mutual information (MI) 
I(w1; w2) =
p(w1, w2) log
p(w1) p(w2), (9)
where p(wi) is estimated from the posterior probability of
the semantic feature wi in a given set of SMNs.
MI is a measure of the statistical dependence between variables, it should be strong for pairs of concepts that are either
synonyms or frequently appear together in natural imagery.
Table 2 presents the most dependent concept pairs for the
SMNs in the retrieval dataset of Corel15. Note that, even
Semantic Space Dimensions
% Variance
% Variance
(a) Corel50
Semantic Space Dimensions
% Variance
% Variance
(b) Corel15
Semantic Space Dimensions
% Variance
% Variance
(c) Flickr18
Figure 6. MAP scores for all the databases, as it varies with the dimensions of the semantic space. (a) Inside the semantic space (Corel50).
(b,c) Outside the semantic space (Corel15, Flickr18). Also shown are the % variance of the semantic dimensions, as it varies across the
respective retrieval database (on the right Y-axis).
restaurant
Figure 7. A visualization of the semantic correlations in Corel15
dataset. The mutual information of the top 30 concepts (sorted
according to their variance), is used to learn an embedding in a
two-dimensional space, by non-metric multidimensional scaling.
0.038 people
0.033 buildings
0.031 post
0.030 tables
0.027 statue
0.026 street
0.022 door
0.045 statue
0.041 people
0.039 door
0.038 buildings
0.034 tables
0.033 street
0.027 guard
0.048 people
0.047 pillar
0.041 tables
0.041 statue
0.037 buildings
0.031 restaurant
0.024 street
0.048 people
0.047 pillar
0.041 tables
0.041 statue
0.037 buildings
0.031 restaurant
0.024 street
TOP RESULTS
Figure 8. Query from class ‘commercial construction’ and top
QBSE matches. Shown below each image are the semantic features of highest posterior probability.
though none of the images in this set was used to train the
semantic space, all pairs consist of words which are, indeed,
semantically correlated. Fig.7 presents a visualization of the
semantic correlations amongst the top 30 concepts (selected
according to highest variance) in Corel15. To obtain this visualization, the mutual informations between concepts were
used to learn a two-dimensional embedding of the semantic space, with non-metric multidimensional scaling .
These correlations show that the semantic space encodes
contextual relationships.
To further substantiate this claim, Fig 8 shows a query
image from the class ‘Commercial construction’ (Corel15).
Although the ‘construction’ concept is absent from the semantic vocabulary, the top retrieved images are all in this
class. This illustrates how the QBSE system is effectively
able to rely on contextual correlations to retrieve semantically similar images. Analyzing the SMN’s of the query
and retrieved images, it is clear that the semantic features of
largest probability (shown below each image) include various words that are contextually related to the concept of
‘construction’. This shows that outside the semantic space,
retrieval success is purely due to the effectiveness of such
contextual relationships.
Meaningless semantic space
The fact that QBSE signiﬁcantly outperforms QBVE both
inside and outside the semantic space is strong evidence
for the beneﬁts of image retrieval on semantic spaces. To
study the beneﬁts of the contextual structure of the semantic
space, QBSE was applied to a meaningless semantic space a semantic space without any contextual structure. This was
achieved by replicating the QBSE experiments with random image groupings. That is, instead of a semantic space
composed of concepts like ‘sky’ (learned from images containing sky), we created a semantic space of nameless concepts learned from random collections of images. Fig. 9
compares (on Corel50) the PR obtained with QBSE on this
“meaningless semantic space”, with the previous results of
QBVE and QBSE. Although, as before, the classiﬁcation
is performed on a semantic space (albeit meaningless), the
absence of true semantic structure leads to very poor QBSE
performance, even clearly inferior to that of QBVE. This
suggests that the gains previously observed for QBSE are
intrinsic to the semantic nature of the image representation,
and strengthens the claim that the contextual correlations of
the underlying semantic space are the reason for its advantages over QBVE.
Meaningless semantic space
Figure 9. Comparison of precision-recall curve for the retrieval
results using meaningless semantic space to that of QBSE and
QBVE inside the semantic space (Corel50).
6. Conclusion
We have presented an extensive study of the QBSE image retrieval framework. This study supports various conclusions. First, experiments on the low-level visual space,
reveal that 1) inside the semantic space colorspaces play
an important role in retrieval performance, with the “YBR”
color space achieving the best results, but 2) outside the semantic space there are only small differences across colorspaces. Second, experiments on the high-level semantic
space, reveal that 1) semantic features are not all equally
informative for retrieval, and 2) the number of informative
features grows proportionally to the variance of the semantic multinomials. Third, a study of the intrinsic structure of
the semantic space revealed the presence of contextual relationships between concepts, that seems to substantially improve the robustness of similarity judgments. Finally, it was
shown that, in the absence of meaningful semantic structure, QBSE performs worse than QBVE.
It should be noted that our current implementation
does not incorporate spatial scene information, current evidence favoring integration of weak spatial information.
Furthermore, although our visual representations is based
on DCT features, the current success of scale invariant features such as SIFT warrants a preference for them. At
the semantic level, instead of using variance based feature
selection, more sophisticated feature extraction techniques
which conserve the semantic meaning of the space, such
as probabilistic latent semantic indexing , can also be
used. We intend to investigate these question in future work.