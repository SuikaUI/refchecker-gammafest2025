Econometric Computing with HC and HAC
Covariance Matrix Estimators
Achim Zeileis
Universität Innsbruck
This introduction to the R package sandwich is a (slightly) modiﬁed version of Zeileis
 , published in the Journal of Statistical Software.
A follow-up paper on object
object-oriented computation of sandwich estimators is available in .
Data described by econometric models typically contains autocorrelation and/or heteroskedasticity of unknown form and for inference in such models it is essential to use
covariance matrix estimators that can consistently estimate the covariance of the model
parameters.
Hence, suitable heteroskedasticity-consistent (HC) and heteroskedasticity
and autocorrelation consistent (HAC) estimators have been receiving attention in the
econometric literature over the last 20 years.
To apply these estimators in practice,
an implementation is needed that preferably translates the conceptual properties of the
underlying theoretical frameworks into computational tools. In this paper, such an implementation in the package sandwich in the R system for statistical computing is described
and it is shown how the suggested functions provide reusable components that build on
readily existing functionality and how they can be integrated easily into new inferential
procedures or applications. The toolbox contained in sandwich is extremely ﬂexible and
comprehensive, including speciﬁc functions for the most important HC and HAC estimators from the econometric literature. Several real-world data sets are used to illustrate
how the functionality can be integrated into applications.
Keywords: covariance matrix estimators, heteroskedasticity, autocorrelation, estimating functions, econometric computing, R.
1. Introduction
This paper combines two topics that play an important role in applied econometrics: computational tools and robust covariance estimation.
Without the aid of statistical and econometric software modern data analysis would not be
possible: hence, both practitioners and (applied) researchers rely on computational tools that
should preferably implement state-of-the-art methodology and be numerically reliable, easy
to use, ﬂexible and extensible.
In many situations, economic data arises from time-series or cross-sectional studies which
typically exhibit some form of autocorrelation and/or heteroskedasticity. If the covariance
structure were known, it could be taken into account in a (parametric) model, but more
often than not the form of autocorrelation and heteroskedasticity is unknown. In such cases,
model parameters can typically still be estimated consistently using the usual estimating
Econometric Computing with HC and HAC Covariance Matrix Estimators
functions, but for valid inference in such models a consistent covariance matrix estimate is
essential. Over the last 20 years several procedures for heteroskedasticity consistent (HC) and
for heteroskedasticity and autocorrelation consistent (HAC) covariance estimation have been
suggested in the econometrics literature and are now routinely used in econometric
Many statistical and econometric software packages implement various HC and HAC estimators for certain inference procedures, so why is there a need for a paper about econometric computing with HC and HAC estimators? Typically, only certain special cases of such
estimators—and not the general framework they are taken from—are implemented in statistical and econometric software packages and sometimes they are only available as options
to certain inference functions. It is desirable to improve on this for two reasons: First, the
literature suggested conceptual frameworks for HC and HAC estimation and it would only
be natural to translate these conceptual properties into computational tools that reﬂect the
ﬂexibility of the general framework. Second, it is important, particularly for applied research,
to have covariance matrices not only as options to certain tests but as stand-alone functions
which can be used as modular building blocks and plugged into various inference procedures.
This is becoming more and more relevant, because today, as Cribari-Neto and Zarkos 
point out, applied researchers typically cannot wait until a certain procedure becomes available in the software package of their choice but are often forced to program new techniques
themselves. Thus, just as suitable covariance estimators are routinely plugged into formulas in theoretical work, programmers should be enabled to plug in implementations of such
estimators in computational work. Hence, the aim of this paper is to present an econometric computing approach to HC and HAC estimation that provides reusable components that
can be used as modular building blocks in implementing new inferential techniques and in
applications.
All functions described are available in the package sandwich implemented in the R system
for statistical computing which is currently not the most
popular environment for econometric computing but which is ﬁnding increasing attention
among econometricians . Both R
itself and the sandwich package (as well as all other packages used in this paper) are available
at no cost under the terms of the general public licence (GPL) from the comprehensive R
archive network (CRAN, R has no built-in support for HC
and HAC estimation and at the time we started writing sandwich there was only one package
that implements HC (but not HAC) estimators but which does
not allow for as much ﬂexibility as the tools presented here. sandwich provides the functions
vcovHC and vcovHAC implementing general classes of HC and HAC estimators. The names of
the functions are chosen to correspond to vcov, R’s generic function for extracting covariance
matrices from ﬁtted model objects.
Below, we focus on the general linear regression model estimated by ordinary least squares
(OLS), which is typically ﬁtted in R using the function lm from which the standard covariance
matrix (assuming spherical errors) can be extracted by vcov. Using the tools from sandwich,
HC and HAC covariances matrices can now be extracted from the same ﬁtted models using
vcovHC and vcovHAC. Due to the object orientation of R, these functions are not only limited to
the linear regression model but can be easily extended to other models. The HAC estimators
are already available for generalized linear models (ﬁtted by glm) and robust regression (ﬁtted
Achim Zeileis
by rlm in package MASS). Another important feature of R that is used repeatedly below is
that functions are ﬁrst-level objects—i.e., functions can take functions as arguments and
return functions—which is particularly useful for deﬁning certain procedures for data-driven
computations such as the deﬁnition of the structure of covariance matrices in HC estimation
and weighting schemes for HAC estimation.
The remainder of this paper is structured as follows: To ﬁx notations, Section 2 describes
the linear regression model used and motivates the following sections. Section 3 gives brief
literature reviews and describes the conceptual frameworks for HC and HAC estimation respectively and then shows how the conceptual properties are turned into computational tools
in sandwich.
Section 4 provides some illustrations and applications of these tools before
a summary is given in Section 5. More details about the R code used are provided in an
2. The linear regression model
To ﬁx notations, we consider the linear regression model
(i = 1, . . . , n),
with dependent variable yi, k-dimensional regressor xi with coeﬃcient vector β and error
term ui. In the usual matrix notation comprising all n observations this can be formulated
as y = Xβ + u.
In the general linear model, it is typically assumed that the errors have zero mean and variance
VAR[u] = Ω. Under suitable regularity conditions , the
coeﬃcients β can be consistently estimated by OLS giving the well-known OLS estimator ˆβ
with corresponding OLS residuals ˆui:
(In −H) y = . Their covariance matrix
Ψ is usually denoted in one of the two following ways:
Ψ = VAR[ˆβ]
where Φ = n−1X⊤ΩX is essentially the covariance matrix of the scores or estimating functions
Vi(β) = xi(yi −x⊤
i β). The estimating functions evaluated at the parameter estimates ˆVi =
Vi(ˆβ) have then sum zero.
For inference in the linear regression model, it is essential to have a consistent estimator for
Ψ. What kind of estimator should be used for Ψ depends on the assumptions about Ω: In the
classical linear model independent and homoskedastic errors with variance σ2 are assumed
Econometric Computing with HC and HAC Covariance Matrix Estimators
yielding Ω= σ2In and Ψ = σ2(X⊤X)−1 which can be consistently estimated by plugging
in the usual OLS estimator ˆσ2 = (n −k)−1 Pn
i . But if the independence and/or homoskedasticity assumption is violated, inference based on this estimator ˆΨconst = ˆσ(X⊤X)−1
will be biased. HC and HAC estimators tackle this problem by plugging an estimate ˆΩor
ˆΦ into (4) or (5) respectively which are consistent in the presence of heteroskedasticity and
autocorrelation respectively. Such estimators and their implementation are described in the
following section.
3. Estimating the covariance matrix Ψ
3.1. Dealing with heteroskedasticity
If it is assumed that the errors ui are independent but potentially heteroskedastic—a situation
which typically arises with cross-sectional data—their covariance matrix Ωis diagonal but has
nonconstant diagonal elements. Therefore, various HC estimators ˆΨHC have been suggested
which are constructed by plugging an estimate of type ˆΩ= diag(ω1, . . . , ωn) into Equation (4).
These estimators diﬀer in their choice of the ωi, an overview of the most important cases is
given in the following:
where hi = Hii are the diagonal elements of the hat matrix, ¯h is their mean and δi =
min{4, hi/¯h}.
The ﬁrst equation above yields the standard estimator ˆΨconst for homoskedastic errors. All
others produce diﬀerent kinds of HC estimators. The estimator HC0 was suggested in the
econometrics literature by White and is justiﬁed by asymptotic arguments. The estimators HC1, HC2 and HC3 were suggested by MacKinnon and White to improve
the performance in small samples. A more extensive study of small sample behaviour was
carried out by Long and Ervin which arrive at the conclusion that HC3 provides the
best performance in small samples as it gives less weight to inﬂuential observations.
Recently, Cribari-Neto suggested the estimator HC4 to further improve small sample
performance, especially in the presence of inﬂuential observations.
All of these HC estimators ˆΨHC have in common that they are determined by ω = (ω1, . . . , ωn)⊤
which in turn can be computed based on the residuals ˆu, the diagonal of the hat matrix h
and the degrees of freedom n −k. To translate these conceptual properties of this class of
HC estimators into a computational tool, a function is required which takes a ﬁtted regres-
Achim Zeileis
sion model and the diagonal elements ω as inputs and returns the corresponding ˆΨHC. In
sandwich, this is implemented in the function vcovHC which takes the following arguments:
vcovHC(lmobj, omega = NULL, type = "HC3", ...)
The ﬁrst argument lmobj is an object as returned by lm, R’s standard function for ﬁtting linear
regression models. The argument omega can either be the vector ω or a function for datadriven computation of ω based on the residuals ˆu, the diagonal of the hat matrix h and the
residual degrees of freedom n−k. Thus, it has to be of the form omega(residuals, diaghat,
e.g., for computing HC3 omega is set to function(residuals, diaghat, df)
residuals^2/(1 - diaghat)^2.
As a convenience option, a type argument can be set to "const", "HC0" (or equivalently
"HC"), "HC1", "HC2", "HC3" (the default) or "HC4" and then vcovHC uses the corresponding
omega function. As soon as omega is speciﬁed by the user, type is ignored.
In summary, by specfying ω—either as a vector or as a function—vcovHC can compute arbitrary HC covariance matrix estimates from the class of estimators outlined above. In Section 4,
it will be illustrated how this function can be used as a building block when doing inference
in linear regression models.
3.2. Dealing with autocorrelation
If the error terms ui are not independent, Ωis not diagonal and without further speciﬁcation of
a parametic model for the type of dependence it is typically burdensome to estimate Ωdirectly.
However, if the form of heteroskedasticity and autocorrelation is unknown, a solution to this
problem is to estimate Φ instead which is essentially the covariance matrix of the estimating
functions1. This is what HAC estimators do: ˆΨHAC is computed by plugging an estimate ˆΦ
into Equation (5) with
w|i−j| ˆVi ˆV ⊤
where w = (w0, . . . , wn−1)⊤is a vector of weights. An additional ﬁnite sample adjustment can
be applied by multiplication with n/(n −k). For many data structures, it is a reasonable assumption that the autocorrelations should decrease with increasing lag ℓ= |i −j|—otherwise
β can typically not be estimated consistently by OLS—so that it is rather intuitive that the
weights wℓshould also decrease. Starting from White and Domowitz and Newey and
West , diﬀerent choices for the vector of weights w have been suggested in the econometrics literature which have been placed by Andrews in a more general framework
of choosing the weights by kernel functions with automatic bandwidth selection. Andrews
and Monahan show that the bias of the estimators can be reduced by prewhitening
the estimating functions ˆVi using a vector autoregression (VAR) of order p and applying the
estimator in Equation (6) to the VAR(p) residuals subsequently. Lumley and Heagerty 
suggest an adaptive weighting scheme where the weights are chosen based on the estimated
autocorrelations of the residuals ˆu.
1Due to the use of estimating functions, this approach is not only feasible in linear models estimated by OLS,
but also in nonlinear models using other estimating functions such as maximum likelihood (ML), generalized
methods of moments (GMM) or Quasi-ML.
Econometric Computing with HC and HAC Covariance Matrix Estimators
All the estimators mentioned above are of the form (6), i.e., a weighted sum of lagged products
of the estimating functions corresponding to a ﬁtted regression model. Therefore, a natural
implementation for this class of HAC estimators is the following:
vcovHAC(lmobj, weights,
prewhite = FALSE, adjust = TRUE, sandwich = TRUE,
order.by, ar.method, data)
The most important arguments are again the ﬁtted linear model2 lmobj—from which the estimating functions ˆVi can easily be extracted using the generic function estfun(lmobj)—and
the argument weights which specifys w. The latter can be either the vector w directly or a
function to compute it from lmobj.3 The argument prewhite speciﬁes wether prewhitening
should be used or not4 and adjust determines wether a ﬁnite sample correction by multiplication with n/(n−k) should be made or not. By setting sandwich it can be controlled wether
the full sandwich estimator ˆΨHAC or only the “meat” ˆΦ/n of the sandwich should be returned.
The remaining arguments are a bit more technical: order.by speciﬁes by which variable the
data should be ordered (the default is that they are already ordered, as is natural with time
series data), which ar.method should be used for ﬁtting the VAR(p) model (the default is
OLS) and data provides a data frame from which order.by can be taken (the default is the
environment from which vcovHAC is called).5
As already pointed out above, all that is required for specifying an estimator ˆΨHAC is the
appropriate vector of weights (or a function for data-driven computation of the weights).
For the most important estimators from the literature mentioned above there are functions for computing the corresponding weights readily available in sandwich.
all of the form weights(lmobj, order.by, prewhite, ar.method, data), i.e., functions
that compute the weights depending on the ﬁtted model object lmobj and the arguments
order.by, prewhite, data which are only needed for ordering and prewhitening. The function weightsAndrews implements the class of weights of Andrews and weightsLumley
implements the class of weights of Lumley and Heagerty . Both functions have convenience interfaces: kernHAC calls vcovHAC with weightsAndrews (and diﬀerent defaults for
some parameters) and weave calls vcovHAC with weightsLumley. Finally, a third convenience
interface to vcovHAC is available for computing the estimator(s) of Newey and West suggested to use linearly decaying weights
where L is the maximum lag, all other weights are zero. This is implemented in the
function NeweyWest(lmobj, lag = NULL, ...) where lag speciﬁes L and ... are
2Note, that not only HAC estimators for ﬁtted linear models can be computed with vcovHAC. See Zeileis
 for details.
3If weights is a vector with less than n elements, the remaining weights are assumed to be zero.
4The order p is set to as.integer(prewhite), hence both prewhite = 1 and prewhite = TRUE lead to a
VAR(1) model, but also prewhite = 2 is possible.
5More detailed technical documentation of these and other arguments of the functions described are available
in the reference manual included in sandwich.
Achim Zeileis
(here, and in the following) further arguments passed to other functions, detailed information is always available in the reference manual. If lag is set to NULL (the default)
the non-parametric bandwidth selection procedure of Newey and West is used.
This is also available in a stand-alone function bwNeweyWest, see also below.
Quadratic Spectral
Tukey−Hanning
Kernel functions for kernel-based HAC estimation.
• Andrews placed this and other estimators in a more general class of kernelbased HAC estimators with weights of the form wℓ= K(ℓ/B) where K(·) is a kernel
function and B the bandwidth parameter used. The kernel functions considered are the
truncated, Bartlett, Parzen, Tukey-Hanning and quadratic spectral kernel which are
depicted in Figure 1. The Bartlett kernel leads to the weights used by Newey and West
 in Equation (7) when the bandwidth B is set to L+1. The kernel recommended
by Andrews and probably most used in the literature is the quadratic spectral
kernel which leads to the following weights:
where z = 6π/5·ℓ/B. The deﬁnitions for the remaining kernels can be found in Andrews
 . All kernel weights mentioned above are available in weightsAndrews(lmobj,
kernel, bw, ...) where kernel speciﬁes one of the kernels via a character string
("Truncated", "Bartlett", "Parzen", "Tukey-Hanning" or "Quadratic Spectral")
and bw the bandwidth either as a scalar or as a function. The automatic bandwidth
selection described in Andrews via AR(1) or ARMA(1,1) approximations is implemented in a function bwAndrews which is set as the default in weightsAndrews.
For the Bartlett, Parzen and quadratic spectral kernels, Newey and West suggested a diﬀerent nonparametric bandwidth selection procedure, which is implemented
in bwNeweyWest and which can also be passed to weightsAndrews.
As the ﬂexibility of this conceptual framework of estimators leads to a lot of knobs and switches in
Econometric Computing with HC and HAC Covariance Matrix Estimators
the computational tools, a convenience function kernHAC for kernel-based HAC estimation has been added to sandwich that calls vcovHAC based on weightsAndrews and
bwAndrews with defaults as motivated by Andrews and Andrews and Monahan : by default, it computes a quadratic spectral kernel HAC estimator with
VAR(1) prewhitening and automatic bandwidth selection based on an AR(1) approximation. But of course, all the options described above can also be changed by the user
when calling kernHAC.
• Lumley and Heagerty suggested a diﬀerent approach for specifying the weights
in (6) based on some estimate ˆϱℓof the autocorrelation of the residuals ˆui at lag
0 = 1, . . . , n −1.
They suggest either to use truncated weights wℓ= I{n ˆϱ2
(where I(·) is the indicator function) or smoothed weights wℓ= min{1, C n ˆϱ2
for both a suitable constant C has to be speciﬁed. Lumley and Heagerty suggest using a default of C = 4 and C = 1 for the truncated and smoothed weights
respectively. Note, that the truncated weights are equivalent to the truncated kernel
from the framework of Andrews but using a diﬀerent method for computing
the truncation lag. To ensure that the weights |wℓ| are decreasing, the autocorrelations have to be decreasing for increasing lag ℓwhich can be achieved by using isotonic
regression methods. In sandwich, these two weighting schemes are implemented in a
function weightsLumley with a convenience interface weave (which stands for weighted
empirical adaptive variance estimators) which again sets up the weights and then calls
vcovHAC. Its most important arguments are weave(lmobj, method, C, ...) where
method can be either "truncate" or "smooth" and C is by default 4 or 1 respectively.
To sum up, vcovHAC provides a simple yet ﬂexible interface for general HAC estimation as
deﬁned in Equation (6). Arbitrary weights can be supplied either as vectors or functions for
data-driven computation of the weights. As the latter might easily become rather complex,
in particular due to the automatic choice of bandwidth or lag truncation parameters, three
strategies suggested in the literature are readily available in sandwich: First, the Bartlett
kernel weights suggested by Newey and West are used in NeweyWest which by
default uses the bandwidth selection function bwNeweyWest. Second, the weighting scheme
introduced by Andrews for kernel-based HAC estimation with automatic bandwidth
selection is implemented in weightsAndrews and bwAndrews with corresponding convenience
interface kernHAC. Third, the weighted empirical adaptive variance estimation scheme suggested by Lumley and Heagerty is available in weightsLumley with convenience interface weave.
It is illustrated in the following section how these functions can be easily used in applications.
4. Applications and illustrations
In econometric analyses, the practitioner is only seldom interested in the covariance matrix ˆΨ
(or ˆΩor ˆΦ) per se, but mainly wants to compute them to use them for inferential procedures.
Therefore, it is important that the functions vcovHC and vcovHAC described in the previous
section can be easily supplied to other procedures such that the user does not necessarily have
to compute the variances in advance.
A typical ﬁeld of application for HC and HAC covariances are partial t or z tests for assessing
Achim Zeileis
whether a parameter βj is signiﬁcantly diﬀerent from zero.
Exploiting the (asymptotic)
normality of the estimates, these tests are based on the t ratio ˆβj/
ˆΨjj and either use
the asymptotic normal distribution or the t distribution with n −k degrees of freedom for
computing p values . This procedure is available in the R package lmtest in the generic function coeftest which has a default method applicable
to ﬁtted "lm" objects.
coeftest(lmobj, vcov = NULL, df = NULL, ...)
where vcov speciﬁes the covariances either as a matrix (corresponding to the covariance matrix
estimate) or as a function computing it from lmobj (corresponding to the covariance matrix
estimator). By default, it uses the vcov method which computes ˆΨconst assuming spherical
The df argument determines the degrees of freedom: if df is ﬁnite and positive,
a t distribution with df degrees of freedom is used, otherwise a normal approximation is
employed. The default is to set df to n −k.
Inference based on HC and HAC estimators is illustrated in the following using three realworld data sets: testing coeﬃcients in two models from Greene and a structural change
problem from Bai and Perron .
To make the results exactly reproducible for the reader, the commands for the inferential
procedures is given along with their output within the text. A full list of commands, including
those which produce the ﬁgures in the text, are provided (without output) in the appendix
along with the versions of R and the packages used. Before we start with the examples, the
sandwich and lmtest package have to be loaded:
R> library("sandwich")
R> library("lmtest")
4.1. Testing coeﬃcients in cross-sectional data
A quadratic regression model for per capita expenditures on public schools explained by
per capita income in the United States in 1979 has been analyzed by Greene and
re-analyzed in Cribari-Neto . The corresponding cross-sectional data for the 51 US
states is given in Table 14.1 in Greene and available in sandwich in the data frame
PublicSchools which can be loaded by:
R> data("PublicSchools")
R> ps <- na.omit(PublicSchools)
R> ps$Income <- ps$Income * 0.0001
where the second line omits a missing value (NA) in Wisconsin and assigns the result to a
new data frame ps and the third line transforms the income to be in USD 10, 000.
quadratic regression can now easily be ﬁt using the function lm which ﬁts linear regression
models speciﬁed by a symbolic formula via OLS.
R> fm.ps <- lm(Expenditure ~ Income + I(Income^2), data = ps)
Econometric Computing with HC and HAC Covariance Matrix Estimators
The ﬁtted "lm" object fm.ps now contains the regression of the variable Expenditure on
the variable Income and its sqared value, both variables are taken from the data frame ps.
The question in this data set is whether the quadratic term is really needed, i.e., whether
the coeﬃcient of I(Income^2) is signiﬁcantly diﬀerent from zero. The partial quasi-t tests
(or z tests) for all coeﬃcients can be computed using the function coeftest. Greene 
assesses the signiﬁcance using the HC0 estimator of White .
R> coeftest(fm.ps, df = Inf, vcov = vcovHC(fm.ps, type = "HC0"))
z test of coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept)
I(Income^2)
Signif. codes:
0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
The vcov argument speciﬁes the covariance matrix as a matrix (as opposed to a function)
which is returned by vcovHC(fm.ps, type = "HC0"). As df is set to inﬁnity (Inf) a normal
approximation is used for computing the p values which seem to suggest that the quadratic
term might be weakly signiﬁcant. In his analysis, Cribari-Neto uses his HC4 estimator
(among others) giving the following result:
R> coeftest(fm.ps, df = Inf, vcov = vcovHC(fm.ps, type = "HC4"))
z test of coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept)
I(Income^2)
The quadratic term is clearly non-signiﬁcant. The reason for this result is depicted in Figure 2
which shows the data along with the ﬁtted linear and quadratic model—the latter being
obviously heavily inﬂuenced by a single outlier: Alaska. Thus, the improved performance of
the HC4 as compared to the HC0 estimator is due to the correction for high leverage points.
4.2. Testing coeﬃcients in time-series data
Greene also anayzes a time-series regression model based on robust covariance matrix
estimates: his Table 15.1 provides data on the nominal gross national product (GNP), nominal gross private domestic investment, a price index and an interest rate which is used to
formulate a model that explains real investment by real GNP and real interest. The corresponding transformed variables RealInv, RealGNP and RealInt are stored in the data frame
Investment in sandwich which can be loaded by:
R> data("Investment")
Achim Zeileis
per capita income
per capita spending on public schools
Expenditure on public schools and income with ﬁtted models.
Subsequently, the ﬁtted linear regression model is computed by:
R> fm.inv <- lm(RealInv ~ RealGNP + RealInt, data = Investment)
and the signiﬁcance of the coeﬃcients can again be assessed by partial z tests using coeftest.
Greene uses the estimator of Newey and West without prewhitening and with
lag L = 4 for this purpose which is here passed as a matrix (as opposed to a function) to
R> coeftest(fm.inv, df = Inf, vcov = NeweyWest(fm.inv, lag = 4, prewhite = FALSE))
z test of coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept) -12.5336
<2e-16 ***
Signif. codes:
0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
If alternatively the automatic bandwidth selection procedure of Newey and West with
prewhitening should be used, this can be passed as a function to coeftest.
R> coeftest(fm.inv, df = Inf, vcov = NeweyWest)
z test of coefficients:
Econometric Computing with HC and HAC Covariance Matrix Estimators
Estimate Std. Error z value Pr(>|z|)
(Intercept) -12.5336
7.4e-13 ***
Signif. codes:
0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
For illustration purposes, we show how a new function implementing a particular HAC estimator can be easily set up using the tools provided by sandwich. This is particularly helpful
if the same estimator is to be applied several times in the course of an analysis. Suppose,
we want to use a Parzen kernel with VAR(2) prewhitening, no ﬁnite sample adjustment and
automatic bandwidth selection according to Newey and West . First, we set up the
function parzenHAC and then pass this function to coeftest.
R> parzenHAC <- function(x, ...) kernHAC(x, kernel = "Parzen", prewhite = 2,
adjust = FALSE, bw = bwNeweyWest, ...)
R> coeftest(fm.inv, df = Inf, vcov = parzenHAC)
z test of coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept) -12.5336
4.7e-16 ***
Signif. codes:
0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
The three estimators leads to slightly diﬀerent standard errors, but all tests agree that real
GNP has a highly signiﬁcant inﬂuence while the real interest rate has not. The data along
with the ﬁtted regression are depicted in Figure 3.
4.3. Testing and dating structural changes in the presence of
heteroskedasticity and autocorrelation
To illustrate that the functionality provided by the covariance estimators implemented in
sandwich cannot only be used in simple settings, such as partial quasi-t tests, but also for
more complicated tasks, we employ the real interest time series analyzed by Bai and Perron
 . This series contains changes in the mean (see Figure 4, right panel) which Bai and
Perron detect using several structural change tests based on F statistics and date using
a dynamic programming algorithm. As the visualization suggests, this series exhibits both
heteroskedasticity and autocorrelation, hence Bai and Perron use a quadratic spectral
kernel HAC estimator in their analysis. Here, we use the same dating procedure but assess
the signiﬁcance using an OLS-based CUSUM test based on the
same HAC estimator. The data are available in the package strucchange as the quarterly
time series RealInt containing the US ex-post real interest rate from 1961(1) to 1986(3) and
they are analyzed by a simple regression on the mean.
Achim Zeileis
120 140 160 180 200 220 240 260
Investment equation data with ﬁtted model.
Under the assumptions in the classical linear model with spherical errors, the test statistic of
the OLS-based CUSUM test is
If autocorrelation and heteroskedasticity are present in the data, a robust variance estimator
should be used: if xi is equal to unity, this can simply be achieved by replacing ˆσ2 with ˆΦ or
nˆΨ respectively. Here, we use the quadratic spectral kernel HAC estimator of Andrews 
with VAR(1) prewhitening and automatic bandwidth selection based on an AR(1) approximation as implemented in the function kernHAC. The p values for the OLS-based CUSUM
test can be computed from the distribution of the supremum of a Brownian bridge . This and other methods for testing, dating and monitoring
structural changes are implemented in the R package strucchange which contains the function gefp for ﬁtting and assessing ﬂuctuation processes including OLS-based CUSUM processes .
After loading the package and the data,
R> library("strucchange")
R> data("RealInt", package = "strucchange")
the command
R> ocus <- gefp(RealInt ~ 1, fit = lm, vcov = kernHAC)
ﬁts the OLS-based CUSUM process for a regression on the mean (RealInt ~ 1), using the
function lm and estimating the variance using the function kernHAC. The ﬁtted OLS-based
Econometric Computing with HC and HAC Covariance Matrix Estimators
CUSUM process can then be visualized together with its 5% critical value (horizontal lines)
by plot(scus) which leads to a similar plot as in the left panel of Figure 4 (see the appendix
for more details). As the process crosses its boundary, there is a signiﬁcant change in the
mean, while the clear peak in the process conveys that there is at least one strong break in
the early 1980s. A formal signiﬁcance test can also be carried out by sctest(ocus) which
leads to a highly signiﬁcant p value of 0.0082. Similarly, the same quadratic spectral kernel
HAC estimator could also be used for computing and visualizing the supF test of Andrews
 , the code is provided in the appendix.
Finally, the breakpoints in this model along with their conﬁdence intervals can be computed
R> bp <- breakpoints(RealInt ~ 1)
R> confint(bp, vcov = kernHAC)
Confidence intervals for breakpoints
of optimal 3-segment partition:
confint.breakpointsfull(object = bp, vcov. = kernHAC)
Breakpoints at observation number:
2.5 % breakpoints 97.5 %
Corresponding to breakdates:
breakpoints 97.5 %
1 1970(1) 1972(3)
2 1980(1) 1980(3)
The dating algorithm breakpoints implements the procedure described in Bai and Perron
 and estimates the timing of the structural changes by OLS. Therefore, in this step
no covariance matrix estimate is required, but for computing the conﬁdence intervals using a
consistent covariance matrix estimator is again essential. The confint method for computing
conﬁdence intervals takes again a vcov argument which has to be a function (and not a matrix)
because it has to be applied to several segments of the data. By default, it computes the
breakpoints for the minimum BIC partition which gives in this case two breaks.6 The ﬁtted
three-segment model along with the breakpoints and their conﬁdence intervals is depicted in
the right panel of Figure 4.
5. Summary
6By choosing the number of breakpoints with sequential tests and not the BIC, Bai and Perron 
arrive at a model with an additional breakpoint which has rather wide conﬁdence intervals 
Achim Zeileis
Empirical fluctuation process
Real interest rate
OLS-based CUSUM test (left) and ﬁtted model (right) for real interest data.
This paper brieﬂy reviews a class of heteroskedasticity-consistent (HC) and a class of heteroskedasticity and autocorrelation consistent (HAC) covariance matrix estimators suggested
in the econometric literature over the last 20 years and introduces uniﬁed computational tools
that reﬂect the ﬂexibility and the conceptual ideas of the underlying theoretical frameworks.
Based on these general tools, a number of special cases of HC and HAC estimators is provided
including the most popular in applied econometric research. All the functions suggested are
implemented in the package sandwich in the R system for statistical computing and designed
in such a way that they build on readily available model ﬁtting functions and provide building blocks that can be easily integrated into other programs or applications. To achieve this
ﬂexibility, the object orientation mechanism of R and the fact that functions are ﬁrst-level
objects are of prime importance.
Acknowledgments
We are grateful to Thomas Lumley for putting his code in the weave package at disposal and
for advice in the design of sandwich, and to Christian Kleiber for helpful suggestions in the
development of sandwich.