Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 489–500
Brussels, Belgium, October 31 - November 4, 2018. c⃝2018 Association for Computational Linguistics
Understanding Back-Translation at Scale
Sergey Edunov△
Michael Auli△
David Grangier▽∗
△Facebook AI Research, Menlo Park, CA & New York, NY.
▽Google Brain, Mountain View, CA.
An effective method to improve neural machine translation with monolingual data is
to augment the parallel training corpus with
back-translations of target language sentences.
This work broadens the understanding of
back-translation and investigates a number
of methods to generate synthetic source sentences. We ﬁnd that in all but resource poor
settings back-translations obtained via sampling or noised beam outputs are most effective.
Our analysis shows that sampling or
noisy synthetic data gives a much stronger
training signal than data generated by beam or
greedy search. We also compare how synthetic
data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences
and achieve a new state of the art of 35 BLEU
on the WMT’14 English-German test set.
Introduction
Machine translation relies on the statistics of large
parallel corpora, i.e. datasets of paired sentences
in both the source and target language. However,
bitext is limited and there is a much larger amount
of monolingual data available. Monolingual data
has been traditionally used to train language models which improved the ﬂuency of statistical machine translation .
In the context of neural machine translation
 , there has been extensive
work to improve models with monolingual data,
including language model fusion , back-translation and dual learning . These methods have different advantages and can be combined to reach high accuracy .
*Work done while at Facebook AI Research.
We focus on back-translation (BT) which operates in a semi-supervised setup where both bilingual and monolingual data in the target language
are available. Back-translation ﬁrst trains an intermediate system on the parallel data which is used
to translate the target monolingual data into the
source language. The result is a parallel corpus
where the source side is synthetic machine translation output while the target is genuine text written
by humans. The synthetic parallel corpus is then
simply added to the real bitext in order to train a ﬁnal system that will translate from the source to the
target language. Although simple, this method has
been shown to be helpful for phrase-based translation , NMT as well as unsupervised MT .
In this paper, we investigate back-translation
for neural machine translation at a large scale
by adding hundreds of millions of back-translated
sentences to the bitext. Our experiments are based
on strong baseline models trained on the public bitext of the WMT competition. We extend previous
analysis of back-translation in several ways. We provide a comprehensive analysis of different methods to generate synthetic source sentences and we
show that this choice matters: sampling from the
model distribution or noising beam outputs outperforms pure beam search, which is typically
used, by 1.7 BLEU on average across several test
sets. Our analysis shows that synthetic data based
on sampling and noised beam search provides a
stronger training signal than synthetic data based
on argmax inference. We also study how adding
synthetic data compares to adding real bitext in
a controlled setup with the surprising ﬁnding that
synthetic data can sometimes match the accuracy
of real bitext. Our best setup achieves 35 BLEU
on the WMT’14 English-German test set by rely-
ing only on public WMT bitext as well as 226M
monolingual sentences. This outperforms the system of DeepL by 1.7 BLEU who train on large
amounts of high quality non-benchmark data. On
WMT’14 English-French we achieve 45.6 BLEU.
Related work
This section describes prior work in machine
translation with neural networks as well as semisupervised machine translation.
Neural machine translation
We build upon recent work on neural machine
translation which is typically a neural network
with an encoder/decoder architecture.
The encoder infers a continuous space representation of
the source sentence, while the decoder is a neural
language model conditioned on the encoder output. The parameters of both models are learned
jointly to maximize the likelihood of the target
sentences given the corresponding source sentences from a parallel corpus . At inference, a target sentence is generated by left-to-right decoding.
Different neural architectures have been proposed with the goal of improving efﬁciency
and/or effectiveness. This includes recurrent networks , convolutional networks and transformer networks .
Recent work relies on attention mechanisms where the encoder
produces a sequence of vectors and, for each
target token, the decoder attends to the most
relevant part of the source through a contextdependent weighted-sum of the encoder vectors .
Attention has been reﬁned with multi-hop attention , self-attention and multi-head
attention . We use a transformer architecture .
Semi-supervised NMT
Monolingual target data has been used to improve
the ﬂuency of machine translations since the early
IBM models . In phrase-based
systems, language models (LM) in the target language increase the score of ﬂuent outputs during
decoding .
A similar strategy can be applied to NMT . Besides improving accuracy during
decoding, neural LM and NMT can beneﬁt from
deeper integration, e.g.
by combining the hidden states of both models .
Neural architecture also allows multi-task learning
and parameter sharing between MT and target-side
LM .
Back-translation (BT) is an alternative to leverage monolingual data. BT is simple and easy to
apply as it does not require modiﬁcation to the MT
training algorithms. It requires training a targetto-source system in order to generate additional
synthetic parallel data from the monolingual target data. This data complements human bitext to
train the desired source-to-target system. BT has
been applied earlier to phrase-base systems . For these systems, BT
has also been successful in leveraging monolingual data for domain adaptation . Recently, BT
has been shown beneﬁcial for NMT . It has been
found to be particularly useful when parallel data
is scarce .
Currey et al. show that low resource
language pairs can also be improved with synthetic data where the source is simply a copy of
the monolingual target data. Concurrently to our
work, Imamura et al. show that sampling
synthetic sources is more effective than beam
search. Speciﬁcally, they sample multiple sources
for each target whereas we draw only a single sample, opting to train on a larger number of target
sentences instead. Hoang et al. and Cotterell and Kreutzer suggest an iterative procedure which continuously improves the quality of
the back-translation and ﬁnal systems. Niu et al.
 experiment with a multilingual model that
does both the forward and backward translation
which is continuously trained with new synthetic
There has also been work using source-side
monolingual data . Furthermore, Cheng et al. ; He et al. ;
Xia et al. show how monolingual text from
both languages can be leveraged by extending
back-translation to dual learning: when training
both source-to-target and target-to-source models
jointly, one can use back-translation in both directions and perform multiple rounds of BT. A simi-
lar idea is applied in unsupervised NMT . Besides monolingual data, various approaches have been introduced to beneﬁt
from parallel data in other language pairs .
Data augmentation is an established technique
in computer vision where a labeled dataset is supplemented with cropped or rotated input images.
Recently, generative adversarial networks (GANs)
have been successfully used to the same end as well
as models that learn distributions over image transformations .
Generating synthetic sources
Back-translation typically uses beam search or just greedy search to generate synthetic source sentences. Both are approximate algorithms to identify the maximum a-posteriori (MAP) output, i.e.
the sentence with the largest estimated probability given an input. Beam is generally successful in
ﬁnding high probability outputs .
However, MAP prediction can lead to less rich
translations since it always favors the most likely alternative in case of ambiguity. This is particularly problematic in tasks where
there is a high level of uncertainty such as dialog and story generation . We argue that this is also problematic for a data augmentation scheme such as backtranslation. Beam and greedy focus on the head of
the model distribution which results in very regular synthetic source sentences that do not properly
cover the true data distribution.
As alternative, we consider sampling from the
model distribution as well as adding noise to beam
search outputs. First, we explore unrestricted sampling which generates outputs that are very diverse but sometimes highly unlikely.
we investigate sampling restricted to the most
likely words . At each time step, we select the k
most likely tokens from the output distribution, renormalize and then sample from this restricted set.
This is a middle ground between MAP and unrestricted sampling.
As a third alternative, we apply noising Lample et al. to beam search outputs. Adding
noise to input sentences has been very beneﬁcial for the autoencoder setups of which is inspired by denoising autoencoders .
particular, we transform source sentences with
three types of noise: deleting words with probability 0.1, replacing words by a ﬁller token with
probability 0.1, and swapping words which is implemented as a random permutation over the tokens, drawn from the uniform distribution but restricted to swapping words no further than three
positions apart.
Experimental setup
The majority of our experiments are based on data
from the WMT’18 English-German news translation task. We train on all available bitext excluding the ParaCrawl corpus and remove sentences
longer than 250 words as well as sentence-pairs
with a source/target length ratio exceeding 1.5.
This results in 5.18M sentence pairs. For the backtranslation experiments we use the German monolingual newscrawl data distributed with WMT’18
comprising 226M sentences after removing duplicates. We tokenize all data with the Moses tokenizer and learn a joint source
and target Byte-Pair-Encoding with 35K types. We develop on newstest2012 and report ﬁnal results on newstest2013-
2017; additionally we consider a held-out set from
the training data of 52K sentence-pairs.
We also experiment on the larger WMT’14
English-French task which we ﬁlter in the same
way as WMT’18 English-German. This results in
35.7M sentence-pairs for training and we learn a
joint BPE vocabulary of 44K types. As monolingual data we use newscrawl2010-2014, comprising 31M sentences after language identiﬁcation
 . We use newstest2012
as development set and report ﬁnal results on
newstest2013-2015.
The majority of results in this paper are in terms
of case-sensitive tokenized BLEU but we also report test accuracy with detokenized BLEU using sacreBLEU .
Model and hyperparameters
We re-implemented the Transformer model in pytorch using the fairseq toolkit.1 All experiments
 
pytorch/fairseq
are based on the Big Transformer architecture with
6 blocks in the encoder and decoder. We use the
same hyper-parameters for all experiments, i.e.,
word representations of size 1024, feed-forward
layers with inner dimension 4096. Dropout is set
to 0.3 for En-De and 0.1 for En-Fr, we use 16 attention heads, and we average the checkpoints of
the last ten epochs. Models are optimized with
Adam using β1 = 0.9,
β2 = 0.98, and ϵ = 1e −8 and we use the same
learning rate schedule as Vaswani et al. . All
models use label smoothing with a uniform prior
distribution over the vocabulary ϵ = 0.1 . We run experiments on DGX-1 machines with 8 Nvidia V100
GPUs and machines are interconnected by Inﬁniband. Experiments are run on 16 machines and
we perform 30K synchronous updates. We also
use the NCCL2 library and the torch distributed
package for inter-GPU communication. We train
models with 16-bit ﬂoating point operations, following Ott et al. .
For ﬁnal evaluation,
we generate translations with a beam of size 5 and
with no length penalty.
Our evaluation ﬁrst compares the accuracy of
back-translation generation methods (§5.1) and
analyzes the results (§5.2). Next, we simulate a
low-resource setup to experiment further with different generation methods (§5.3). We also compare synthetic bitext to genuine parallel data and
examine domain effects arising in back-translation
(§5.4). We also measure the effect of upsampling
bitext during training (§5.5). Finally, we scale to a
very large setup of up to 226M monolingual sentences and compare to previous research (§5.6).
Synthetic data generation methods
We ﬁrst investigate different methods to generate synthetic source translations given a backtranslation model, i.e., a model trained in the
reverse language direction (Section 5.1).
consider two types of MAP prediction: greedy
search (greedy) and beam search with beam size 5
(beam). Non-MAP methods include unrestricted
sampling from the model distribution (sampling),
restricting sampling to the k highest scoring outputs at every time step with k = 10 (top10) as well
as adding noise to the beam outputs (beam+noise).
Restricted sampling is a middle-ground between
Total training data
BLEU (newstest2012)
beam+noise
Accuracy of models trained on different amounts of back-translated data obtained
with greedy search, beam search (k = 5), randomly sampling from the model distribution, restricting sampling over the ten most likely words
(top10), and by adding noise to the beam outputs
(beam+noise). Results based on newstest2012 of
WMT English-German translation.
beam search and unrestricted sampling, it is less
likely to pick very low scoring outputs but still
preserves some randomness. Preliminary experiments with top5, top20, top50 gave similar results
We also vary the amount of synthetic data and
perform 30K updates during training for the bitext only, 50K updates when adding 3M synthetic
sentences, 75K updates for 6M and 12M sentences and 100K updates for 24M sentences. For
each setting, this corresponds to enough updates to
reach convergence in terms of held-out loss. In our
128 GPU setup, training of the ﬁnal models takes
3h 20min for the bitext only model, 7h 30min for
6M and 12M synthetic sentences, and 10h 15min
for 24M sentences. During training we also sample the bitext more frequently than the synthetic
data and we analyze the effect of this in more detail in §5.5.
Figure 1 shows that sampling and beam+noise
outperform the MAP methods (pure beam search
and greedy) by 0.8-1.1 BLEU. Sampling and
beam+noise improve over bitext-only (5M) by between 1.7-2 BLEU in the largest data setting.
Restricted sampling (top10) performs better than
beam and greedy but is not as effective as unrestricted sampling (sampling) or beam+noise.
Table 1 shows results on a wider range of
+ sampling
+ beam+noise
Table 1: Tokenized BLEU on various test sets of WMT English-German when adding 24M synthetic
sentence pairs obtained by various generation methods to a 5.2M sentence-pair bitext (cf. Figure 1).
Training perplexity
beam+noise
Figure 2: Training perplexity (PPL) per epoch for
different synthetic data. We separately report PPL
on the synthetic data and the bitext. Bitext PPL is
averaged over all generation methods.
test sets .
Sampling and
beam+noise perform roughly equal and we adopt
sampling for the remaining experiments.
Analysis of generation methods
The previous experiment showed that synthetic
source sentences generated via sampling and beam
with noise perform signiﬁcantly better than those
obtained by pure MAP methods. Why is this?
Beam search focuses on very likely outputs
which reduces the diversity and richness of the
generated source translations.
Adding noise to
beam outputs and sampling do not have this problem: Noisy source sentences make it harder to predict the target translations which may help learning, similar to denoising autoencoders . Sampling is known to better approximate the data distribution which is richer than the
argmax model outputs . There-
Perplexity
human data
beam+noise
Table 2: Perplexity of source data as assigned by a
language model (5-gram Kneser–Ney). Data generated by beam search is most predictable.
fore, sampling is also more likely to provide a
richer training signal than argmax sequences.
To get a better sense of the training signal provided by each method, we compare the loss on
the training data for each method. We report the
cross entropy loss averaged over all tokens and
separate the loss over the synthetic data and the
real bitext data. Speciﬁcally, we choose the setup
with 24M synthetic sentences. At the end of each
epoch we measure the loss over 500K sentence
pairs sub-sampled from the synthetic data as well
as an equally sized subset of the bitext. For each
generation method we choose the same sentences
except for the bitext which is disjoint from the synthetic data. This means that losses over the synthetic data are measured over the same target tokens because the generation methods only differ
in the source sentences. We found it helpful to upsample the frequency with which we observe the
bitext compared to the synthetic data (§5.5) but we
do not upsample for this experiment to keep conditions as similar as possible. We assume that when
the training loss is low, then the model can easily
ﬁt the training data without extracting much learning signal compared to data which is harder to ﬁt.
Figure 2 shows that synthetic data based on
Diese gegenstzlichen Auffassungen von Fairness liegen nicht nur der politischen Debatte
These competing principles of fairness underlie not only the political debate.
These conﬂicting interpretations of fairness are not solely based on the political debate.
Mr President, these contradictory interpretations of fairness are not based solely on the
political debate.
Those conﬂicting interpretations of fairness are not solely at the heart of the political
beam+noise
conﬂicting BLANK interpretations BLANK are of not BLANK based on the political
Table 3: Example where sampling produces inadequate outputs. ”Mr President,” is not in the source.
BLANK means that a word has been replaced by a ﬁller token.
greedy or beam is much easier to ﬁt compared to
data from sampling, top10, beam+noise and the
bitext. In fact, the perplexity on beam data falls
below 2 after only 5 epochs. Except for sampling,
we ﬁnd that the perplexity on the training data is
somewhat correlated to the end-model accuracy
(cf. Figure 1) and that all methods except sampling have a lower loss than real bitext.
These results suggest that synthetic data obtained with argmax inference does not provide
as rich a training signal as sampling or adding
noise. We conjecture that the regularity of synthetic data obtained with argmax inference is not
optimal. Sampling and noised argmax both expose
the model to a wider range of source sentences
which makes the model more robust to reordering and substitutions that happen naturally, even if
the model of reordering and substitution through
noising is not very realistic.
Next we analyze the richness of synthetic outputs and train a language model on real human text
and score synthetic source sentences generated by
beam search, sampling, top10 and beam+noise.
We hypothesize that data that is very regular
should be more predictable by the language model
and therefore receive low perplexity. We eliminate a possible domain mismatch effect between
the language model training data and the synthetic
data by splitting the parallel corpus into three nonoverlapping parts:
1. On 640K sentences pairs, we train a backtranslation model,
2. On 4.1M sentence pairs, we take the source
side and train a 5-gram Kneser-Ney language
model ,
3. On the remaining 450K sentences, we apply
the back-translation system using beam, sampling and top10 generation.
For the last set, we have genuine source sentences as well as synthetic sources from different
generation techniques. We report the perplexity of
our language model on all versions of the source
data in Table 2. The results show that beam outputs receive higher probability by the language
model compared to sampling, beam+noise and
real source sentences. This indicates that beam
search outputs are not as rich as sampling outputs
or beam+noise. This lack of variability probably
explains in part why back-translations from pure
beam search provide a weaker training signal than
alternatives.
Closer inspection of the synthetic sources (Table 3) reveals that sampled and noised beam outputs are sometimes not very adequate, much more
so than MAP outputs, e.g., sampling often introduces target words which have no counterpart
in the source.
This happens because sampling
sometimes picks highly unlikely outputs which are
harder to ﬁt (cf. Figure 2).
Low resource vs. high resource setup
The experiments so far are based on a setup with a
large bilingual corpus. However, in resource poor
settings the back-translation model is of much
lower quality. Are non-MAP methods still more
effective in such a setup? To answer this question, we simulate such setups by sub-sampling
the training data to either 80K sentence-pairs or
640K sentence-pairs and then add synthetic data
from sampling and beam search.
We compare
these smaller setups to our original 5.2M sentence bitext conﬁguration.
The accuracy of the
Total training data
BLEU (newstest2012)
sampling 80K
sampling 640K
sampling 5M
Figure 3: BLEU when adding synthetic data from
beam and sampling to bitext systems with 80K,
640K and 5M sentence pairs.
German-English back-translation systems steadily
increases with more training data:
On newstest2012 we measure 13.5 BLEU for 80K bitext,
24.3 BLEU for 640K and 28.3 BLEU for 5M.
Figure 3 shows that sampling is more effective
than beam for larger setups (640K and 5.2M bitexts) while the opposite is true for resource poor
settings (80K bitext). This is likely because the
back-translations in the 80K setup are of very poor
quality and the noise of sampling and beam+noise
is too detrimental for this brittle low-resource setting. When the setup is very small the very regular MAP outputs still provide useful training signal
while the noise from sampling becomes harmful.
Domain of synthetic data
Next, we turn to two different questions: How
does real human bitext compare to synthetic data
in terms of ﬁnal model accuracy? And how does
the domain of the monolingual data affect results?
To answer these questions, we subsample 640K
sentence-pairs of the bitext and train a backtranslation system on this set. To train a forward
model, we consider three alternative types of data
to add to this 640K training set. We either add:
• the remaining parallel data (bitext),
• the back-translated target side of the remaining parallel data (BT-bitext),
• back-translated newscrawl data (BT-news).
The back-translated data is generated via sampling. This setup allows us to compare synthetic
data to genuine data since BT-bitext and bitext
share the same target side. It also allows us to
estimate the value of BT data for domain adaptation since the newscrawl corpus (BT-news) is
pure news whereas the bitext is a mixture of europarl and commoncrawl with only a small newscommentary portion. To assess domain adaptation
effects, we measure accuracy on two held-out sets:
• newstest2012, i.e. pure newswire data.
• a held-out set of the WMT training data
(valid-mixed), which is a mixture of europarl, commoncrawl and the small newscommentary portion.
Figure 4 shows the results on both validation
sets. Most strikingly, BT-news performs almost
as well as bitext on newstest2012 (Figure 4a) and
improves the baseline (640K) by 2.6 BLEU. BTbitext improves by 2.2 BLEU, achieving 83% of
the improvement with real bitext. This shows that
synthetic data can be nearly as effective as real human translated data when the domains match.
Figure 4b shows the accuracy on valid-mixed,
the mixed domain valid set. The accuracy of BTnews is not as good as before since the domain of
the BT data and the test set do not match. However, BT-news still improves the baseline by up to
1.2 BLEU. On the other hand, BT-bitext matches
the domain of valid-mixed and improves by 2.7
BLEU. This trails the real bitext by only 1.3 BLEU
and corresponds to 67% of the gain achieved with
real human bitext.
In summary, synthetic data performs remarkably well, coming close to the improvements
achieved with real bitext for newswire test data,
or trailing real bitext by only 1.3 BLEU for validmixed. In absence of a large parallel corpus for
news, back-translation therefore offers a simple,
yet very effective domain adaptation technique.
Upsampling the bitext
We found it beneﬁcial to adjust the ratio of bitext
to synthetic data observed during training. In particular, we tuned the rate at which we sample data
from the bitext compared to synthetic data. For
example, in a setup of 5M bitext sentences and
10M synthetic sentences, an upsampling rate of 2
means that we double the frequency at which we
Amount of data
(a) newstest2012
Amount of data
(b) valid-mixed
Figure 4: Accuracy on (a) newstest2012 and (b) a mixed domain valid set when growing a 640K bitext
corpus with (i) real parallel data (bitext), (ii) a back-translated version of the target side of the bitext
(BT-bitext), (iii) or back-translated newscrawl data (BT-news).
visit bitext, i.e. training batches contain on average an equal amount of bitext and synthetic data
as opposed to 1/3 bitext and 2/3 synthetic data.
Figure 5 shows the accuracy of various upsampling rates for different generation methods in a
setup with 5M bitext sentences and 24M synthetic
sentences. Beam and greedy beneﬁt a lot from
higher rates which results in training more on the
bitext data. This is likely because synthetic beam
and greedy data does not provide as much training
signal as the bitext which has more variation and
is harder to ﬁt. On the other hand, sampling and
beam+noise require no upsampling of the bitext,
which is likely because the synthetic data is already hard enough to ﬁt and thus provides a strong
training signal (§5.2).
Large scale results
To conﬁrm our ﬁndings we experiment on
WMT’14 English-French translation where we
show results on newstest2013-2015. We augment
the large bitext of 35.7M sentence pairs by 31M
newscrawl sentences generated by sampling. To
train this system we perform 300K training updates in 27h 40min on 128 GPUs; we do not upsample the bitext for this experiment.
shows tokenized BLEU and Table 5 shows detokenized BLEU.2 To our knowledge, our baseline
2sacreBLEU signatures:
BLEU+case.mixed+lang.enfr+numrefs.1+smooth.exp+test.SET+tok.13a+version.1.2.7
with SET ∈{wmt13, wmt14/full, wmt15}
bitext upsample rate
BLEU (newstest2012)
beam+noise
Accuracy when changing the rate at
which the bitext is upsampled during training.
Rates larger than one mean that the bitext is observed more often than actually present in the
combined bitext and synthetic training corpus.
is the best reported result in the literature for newstest2014, and back-translation further improves
upon this by 2.6 BLEU (tokenized).
Finally, for WMT English-German we train
on all 226M available monolingual training sentences and perform 250K updates in 22.5 hours
on 128 GPUs.
We upsample the bitext with a
rate of 16 so that we observe every bitext sentence
3sacreBLEU signatures:
BLEU+case.mixed+lang.en-
LANG+numrefs.1+smooth.exp+test.wmt14/full+
tok.13a+version.1.2.7 with LANG ∈{de,fr}
Table 4: Tokenized BLEU on various test sets for
WMT English-French translation.
Table 5: De-tokenized BLEU (sacreBLEU) on various test sets for WMT English-French.
16 times more often than each monolingual sentence.
This results in a new state of the art of
35 BLEU on newstest2014 by using only WMT
benchmark data. For comparison, DeepL, a commercial translation engine relying on high quality bilingual training data, achieves 33.3 tokenized
BLEU .4 Table 6 summarizes our results and compares to other work in the literature. This shows
that back-translation with sampling can result in
high-quality translation models based on benchmark data only.
Conclusions and future work
Back-translation is a very effective data augmentation technique for neural machine translation.
Generating synthetic sources by sampling or by
adding noise to beam outputs leads to higher accuracy than argmax inference which is typically
In particular, sampling and noised beam
outperforms pure beam by 1.7 BLEU on average
on newstest2013-2017 for WMT English-German
translation. Both methods provide a richer training signal for all but resource poor setups. We
also ﬁnd that synthetic data can achieve up to 83%
of the performance attainable with real bitext. Finally, we achieve a new state of the art result of 35
BLEU on the WMT’14 English-German test set
by using publicly available benchmark data only.
In future work, we would like to investigate
an end-to-end approach where the back-translation
model is optimized to output synthetic sources that
are most helpful to the ﬁnal forward model.
4 
a. Gehring et al. 
b. Vaswani et al. 
c. Ahmed et al. 
d. Shaw et al. 
Our result
detok. sacreBLEU3
BLEU on newstest2014 for WMT
English-German (En–De) and English-French
The ﬁrst four results use only WMT
bitext (WMT’14, except for b, c, d in En–De
which train on WMT’16).
DeepL uses proprietary high-quality bitext and our result relies on
back-translation with 226M newscrawl sentences
for En–De and 31M for En–Fr. We also show detokenized BLEU (SacreBLEU).