Probab. Theory Relat. Fields 115, 549–578 
A particle approximation of the solution
of the Kushner–Stratonovitch equation
D. Crisan⋆, T. Lyons⋆⋆
Department of Mathematics, Imperial College, 180 Queen’s Gate, London, SW7 2BZ, UK
e-mail: ; 
Received: 4 February 1997 / Revised version: 26 October 1998
Abstract. We construct a sequence of branching particle systems αn convergent in measure to the solution of the Kushner–Stratonovitch equation.
The algorithm based on this result can be used to solve numerically the
ﬁltering problem. We prove that the rate of convergence of the algorithm is
of order n
4 . This paper is the third in a sequence, and represents the most
efﬁcient algorithm we have identiﬁed so far.
Mathematics Subject Classiﬁcation : 93E11, 60G57, 65U05
1. Introduction
Stochastic ﬁltering studies the problem of estimating a ‘signal’ process
X = {Xt; t ≥0}, given an associated ‘noisy’ observation process
h(Xs)ds + Wt,
where {Wt; t ≥0} is a standard m-dimensional Brownian motion independent of X and h maps the state space of the signal process into Rm; in other
words the objective is to obtain/approximate the conditional distribution of
Xt given Yt = σ(Ys; s ∈[0, t]). In practical applications, one is interested
in computing estimates of the form E[ϕ(Xt)|Yt], where ϕ is a bounded
⋆Supported by EPSRC Grant No. GR/K69704 and the British Council.
⋆⋆Supported by EPSRC Senior Fellowship No. B/93/SF/445, EPSRC Grant No. GR/K69704
and EEC Grant SC1*-CT92-0784.
D. Crisan, T. Lyons
Borel function. When X is a Markov process with generator A and ϕ is in
the domain of A, then πt(ϕ)
△= E[ϕ(Xt)|Yt] satisﬁes the evolution equation
called the Kushner – Stratonovitch equation:
dπt(ϕ) = πt(Aϕ)dt + (πt(h∗ϕ) −πt(h∗)πt(ϕ))(dYt −πt(h)dt).
One can also prove that there exists an unique Yt-measurable, c`adl`ag, measure valued version α of π such that
(α(t), ϕ) = πt(ϕ)
and α satisﬁes the measure valued process equivalent of (1).
The nonlinear character of the equation makes direct numerical work
difﬁcult. Using branching particle systems αn, we produce approximations
to the conditional distribution of X, and based on these, we construct a
numerical algorithm (αn will converge to the measure valued process α).
The algorithm is particularly suitable to high dimensional problems.
We construct the particle systems up to a ﬁxed horizon, i.e. on the ﬁxed
interval , the construction being identical for any interval [0, T ]. Then,
using an argument based on the Carath`eodory extension theorem, the construction can be extended for the whole positive axis.
The authors explained in how particle approximations in the spirit
of Dawson and Watanabe can be used to approximate the measure valued
process of nonlinear ﬁltering. These approximations permitted us to analyse
a class of nonlinear problems where conventional numerical approaches to
solving the parabolic SPDE’s would have failed.
The algorithm in introduced the idea of minimising the randomness
of the particle systems subject to constraints on the expected number of offsprings. It had a number of advantages over and was orders of magnitude
more efﬁcient.
This paper moves one step further. We show how it is possible to directly approximate the normalised solution to the ﬁltering problem in a way
that controls the number of particles. We would expect this to yield the
most efﬁcient algorithm so-far for ergodic but essentially nonlinear ﬁltering
problems. However, although we have determined an upper bound for the
order of convergence, we have not yet proved it to be optimal.
TheﬁrstversionofthismanuscriptwassubmittedforpublicationinApril
1997, however the authors took it back to change the arguments in Section
5 and to add Section 6 to take into account very helpful comments from
the referees of the papers . Since those early days, we have become
aware of a number of other papers which use “particle” based algorithms
for solving the ﬁltering problem: , , , , etc. It makes sense
Solution of the Kushner–Stratonovitch equation
to attempt a comparison between these various algorithms. Unfortunately,
to date, no-one has identiﬁed adequate and generic criteria for comparing
different algorithms (including the Monte Carlo method) and of accurately
measuring the numerical errors.
We try to make a few comparative remarks, but do not pretend that they
are satisfactory, or better than heuristic.
Allmethodsaimtorecursivelyapproximatetheevolutionoftheposterior
measure πt over discrete time steps. The common feature of the papers
above is that the approximation is a two stage process. These algorithms
produce, over each time step (ti, ti+1], two numerical approximations to
πti denoted by η(ti) and ˜η(ti). Proceeding recursively, suppose that η(ti) is
the best or primary approximation to πti given information up to time ti.
Introduce ˜η(ti+1) as an attempt to predict πti+1 given the observation up to
time ti; η(ti+1) is obtained from ˜η(ti+1) using the observations in the interval
(ti, ti+1].
Roughly, the prediction step makes the assumption that πti = η(ti) and
computes the approximate a-priori distribution of πti+1 using the Markovian
structure of Xt and/or the semigroup property to produce an estimate for
the measure E
The second step rebalances the measure using the likelihood function
associated to the next piece of observational data. 1
The following can all be viewed in this light:
1. The classical Monte Carlo method : A cloud of particles whose initial positions form a random sample from the initial distribution of the signal
moves according to the law of X (particles move independently of the each
other). Correction is done by modifying the mass of the particles according
to the likelihood function associated to the next piece of observational data.
2. The ﬁnite-difference splitting approach : Weights
placed on the points of a ﬁxed non-random (but possibly adaptive) grid
are used to approximate the evolving measure. (We may therefore view the
approximations as providing a cloud of particles with known positions but
evolving masses). Here, a discretised form of the transition semigroup of
the signal is used to move the approximation forward. Then, a correction is
done by modifying the masses of the particles at the grid locations according
to the likelihood function associated to the next piece of observational data.
3. The genetic resampling method : In contrast to the method
in 2. and in common with 1. the measure is approximated at any time by
1 There are situations where a direct handle can be obtained on L(Xti+1|Xti, Yti+1) and
in this case it is possible and advantageous to interchange the order of the rebalancing and
the diffusion in the algorithm.
D. Crisan, T. Lyons
particles which can be at any location. The prediction step is achieved by
allowing the particles diffuse forward using independent simulations for the
unconditional Markov diffusion modelling the law of the signal. It is the
second step that brings an essential difference. The rebalancing is achieved
by independently sampling with replacement from these particles, using
probabilities proportional to the likelihood function associated to the next
piece of observational data. This resampling provides an approximation
with equally weighted particles.
4. The minimal variance branching method: The method, as presented
in this paper, approximates the measure πt by equally weighted particles
which can be at any location. As in 1. and 3. the prediction step is achieved
by allowing the particles diffuse forward using independent simulations of
the Markov diffusion. However, the rebalancing is achieved by branching
each particle with random number of offspring. The mean of the offspring
distribution for a particle is proportional to the likelihood function, while
the distribution of this integer valued variable is chosen to have minimal
variance subject to this constraint.
The last two methods are effective because particles with small weights,
i.e., particles with unlikely trajectories/positions are not carried forward
uselessly while the most probable regions are explored more thoroughly.
This provides an automated compression technique similar to those used
in wavelet approaches to the numerical solution of PDE’s. By choosing the
truncation randomly one keeps the one step error with mean zero which
should further improve matters.
We explain below some of the differences between the methods 1. 3. and
4. but ﬁrst we should discuss the prediction step a little. At this point our
objective is to take an atomic measure, and move it forward according to the
diffusion equation, and then project it back onto a particle approximation.
The simple Monte Carlo approach used in 1. 3. and 4. is easy to programme,
and has the advantage that it works in high dimensions. Equally signiﬁcantly
it produces a random approximation that has as its expectation the true predicted measure. Its disadvantage, particularly in low dimensions, is that the
particles do not try to equidistribute themselves, as in general an n-point
i.i.d. random sample from a measure is not the best way to approximate the
measure with an n-point atomic measure and for low dimensions classical
PDE methods start to have a real advantage. It would not be difﬁcult, although to our knowledge it has not been done, to combine the PDE methods
with the minimal variance branching methods. In this case the idea would be
that the computer represents the solution via elements, (the type would depend on the approach: ﬁnite difference, ﬁnite element,...) and only stores the
elements that have positive weight. The predictive step would use standard
Solution of the Kushner–Stratonovitch equation
numerical PDE techniques to allocate mass to a new set of elements. After
modifying these with the likelihood function, the rebalancing step would
then subdivide or kill elements randomly but with minimal variance so as
to ensure that, as far as possible, the elements of small mass are eliminated
in a fair way compatible with the amount of computation we are willing to
invest in the computation.
The (minimal variance) branching algorithm shares the prediction step
with the genetic algorithm, so comparing the rebalancing step:
(i) The errors it produces in rebalancing over one step with the using the
same predicted measure are an order of magnitude greater in the genetic
resampling approach than in the minimal variance approach – at least
if variance or relative entropy are used as measures of approximation.
From this one can conclude, using large deviations, that the probability of making a serious error is exponentially smaller in the minimal
variance in the discrete time case (see ).
(ii) It is easier to program. The resampling approach involves taking a
sample of size n from a multinomial distribution (with the order of
the average computational time which could be as big as n log n) at
each correction time, while the minimal variance branching approach
involves n binary distributions with an average computational complexity of order n (n is the number of particles).
(iii) The minimal variance approach has much less sensitivity to the ratio δt
where δt is the inter-branching time and n is the number of particles. We
make the following conjecture: the particle systems generated by the
genetic resampling algorithm converges to the following (as n →∞)
– πt, if O (δt) = n−1+ε, where 0 < ε < 1.
– a Fleming-Viot type measure valued process (whose conditional expectation given the observation process is πt) if δt = n−1.
– a one particle process if O (δt) = n−1−ε.
By contrast, the particle systems generated by the branching algorithm converges to the following (as n →∞)
– πt, if O (δt) = n−2+ε, where 0 < ε < 2.
– a non-degenerate measure valued process (whose conditional expectation given the observation process is πt) if δt = n−2.
– it diverges if O (δt) = n−2−ε.
One interesting case where there is a clear distinction between the two
is in the case where no information is coming in. In this case the genetic
algorithm undergoes genetic drift and resampling continues unabated. In
the minimal variance case, the branching disappears and the population
remains static. At least in this special case the latter is clearly preferable to
the former.
D. Crisan, T. Lyons
Weemphasisethatthenoveltyoftheminimalvariancebranchingmethod
is concentrated in the rebalancing aspect of the algorithm. However, it seems
on some reﬂection, that the advantages of using it, in contrast with the
genetic algorithm, will not be fully realised if one uses the raw Monte
Carlo simulation for the prediction step. The point is that the errors in this
approach to prediction seem to be of the same type and magnitude as those
in the genetic rebalancing so the gains of improving the rebalancing step,
while quite real, are not as dramatic as one might otherwise expect. On the
other hand, PDE approaches have a smaller error in the prediction step,
so viable combinations of the two approaches, if they could be made to
work in intermediate dimensions, have potential to make quite impressive
improvements.
2. The nonlinear ﬁltering problem
We will set up the assumptions on the ﬁltering problem so that we will have
existence and uniqueness for the measure valued equivalent of the equation
(1). For this, we will use the results contained in . Let (, F, Ft≥0, P)
be a complete ﬁltered probability space and E be a locally compact complete
separable metric space (a locally compact Polish space). We will denote by
B(E) the space of bounded, Borel measurable functions on E , by Cb(E) the
space of continuous bounded functions on E and by C0(E) the space of continuous functions which vanish at inﬁnity, all endowed with the supremum
norm ∥· ∥. Let MF(E) be the space of ﬁnite measures deﬁned on the Borel
sets of E endowed with the topology of weak convergence, i.e. the topology
in which µn →µ if and only if (µn, f ) →(µ, f ) for all f ∈Cb(E) and
P(E) be the set of probability measures deﬁned on the Borel sets of E endowed with the same topology; let M′
F(E) be the space of ﬁnite measures
over E endowed with the topology of vague convergence, i.e., the topology
in which µn →µ if and only if (µn, f ) →(µ, f ) for all f ∈C0(E).
2 Let also DMF (E) , DP(E) , DM′
F (E) be the spaces of c`adl`ag
paths deﬁned on with values in MF(E), P(E), respectively, M′
endowed with the Skorohod topology.
Let X = {Xt, Ft; t ≥0} be an E-valued c`adl`ag Markov process
(the ‘signal’ process), solution of the martingale problem associated with
the inﬁnitesimal generator A : D(A) ⊂B(E) →B(E) with A1 = 0 and
the initial distribution π0 ∈P(E) (the initial law of the signal is, indeed,
equal with the initial conditional law of the signal). Let A0 be the restric-
2 Note that, if p, (pn)n>0 are probability measures, then pn →p weakly if and only if
pn →p vaguely.
Solution of the Kushner–Stratonovitch equation
tion of A to C0(E). We assume that the domain of A0 is a dense algebra
in C0(E) (in particular that if f ∈D(A0), then f 2 ∈D(A0)) and that the
martingale problem for A0 is well posed.
Let W = {Wt, Ft; t ≥0} be an m-dimensional standard Brownian
motion independent of X and Y = {Yt, Ft; t ≥0} be the Rm-valued
process deﬁned by the following formula
h(Xs)ds + Wt,
where h : E →Rm is a continuous, bounded Borel measurable function
and denote ∥h∥= supx∈E,1≤i≤m ∥hi(x)∥. The process Y is usually called the
‘observation’ process. We denote Yt = σ(Ys, 0 ≤s ≤t) W N where N
is the set of P-null sets of F. The ﬁltering problem consists of computing
= E[ϕ(Xt)|Yt].
Under the assumptions from above, it can be shown that πt(ϕ) satisﬁes
the evolution equation (1) for ϕ ∈D(A) and, moreover, Lemma 1.1 and
Remark (iii) from assure us that there exists a DP(E)[0, ∞)-valued,
Yt-adapted process {α(t), t ≥0} for which for all t ∈[0, T ] and for all
ϕ ∈B(E), πt(ϕ) = (α(t), ϕ) a.s.. It follows that α will satisfy the measure
valued equivalent of (1):
(α(t), ϕ) = (π0, ϕ) +
(α(s), Aϕ)ds
 (α(s), h∗ϕ) −(α(s), h∗)(α(s), ϕ)
× (dYs −(α(s), h)ds) ,
Also, under these assumptions, the following uniqueness theorem, again
taken from , holds true.
Theorem 1. Assume that A0 satisﬁes the conditions from above, i.e., A0 :
D(A0) ⊂C0(E) →C0(E), D(A0) is a dense algebra of C0(E), the martingale problem for A0 is well posed and f hi ∈C0(E) for all f ∈D(A0),
1 ≤i ≤m. Let α′(t) be an Yt-adapted c`adl`ag P(E)-valued process such
that, for every ϕ ∈D(A0),
(α′(t), ϕ) = (π0, ϕ) +
(α′(s), Aϕ)ds
 (α′(s), h∗ϕ)−(α′(s), h∗)(α′(s), ϕ)
  dYs−(α′(s), h)ds
D. Crisan, T. Lyons
Then α′(t) = α(t) for all t ≤T a.s..
In the following we will use the same notation for both πt and α(t).
Using Girsanov’s theorem, one can ﬁnd a new probability measure ˜P
absolutely continuous with respect to P (and vice versa), so that Yt is a
Brownian motion under ˜P, independent of X and
πt(ϕ) = ρt(ϕ)
ρt(ϕ) = ˜E
h∗(Xs)dYs −1
h∗(Xs)h(Xs)ds
and ˜E is the expectation with respect to ˜P . ρt is called the unnormalised
conditional distribution of the signal and uniquely satisﬁes the following
evolution equation, called the Zakai equation
ρt(ϕ) = π0(ϕ)+
ρs(h∗ϕ)dYs,
Although we consider A and h to be time independent all the results in the
paper are true in the time-dependent case (which can be viewed as a time
independent case with the signal process (Xt, t)).
Proofs and more details for the ﬁltering problem can be found, for instance, in , and the reference therein.
3. The branching particle systems an
Let {αn(t), Ft; 0 ≤t ≤1} be a sequence of branching particle systems on
(, F, P ) with values in MF(E) deﬁned as follows:
a. The initial state of the systems, αn(0), is the occupation measure of
n particles of mass 1
n, i.e., αn(0) = 1
i , where xn
i :  →E are
independent random variables with distribution π0 for 1 ≤i ≤n and n > 0.
b.Atthetime i
n,i = 0, 1, ..., n−1,theprocessconsistsoftheoccupation
measure of an( i
n) particles of mass 1
n (we will denote the number of particles
alive at time t by an(t)).
c. During the interval
the particles move independently with the
same law as the signal X. Let bn,i
j (s), s ∈
, 1 ≤j ≤an( i
the trajectories of the an( i
n) particles alive in this interval (where possible,
we will use the simpler notation bj(s)) . The number of particles remains
constant during the interval.
Solution of the Kushner–Stratonovitch equation
d. At the end of the interval, the particles branch into a random number
of offspring with a mechanism depending on the recent past of the whole
e. The mechanism is chosen so that it has ﬁnite second moment and the
mean number of offspring of the j-th particle given the σ-ﬁeld F i+1
σ(Fs, s < i+1
n ) of events up to time i+1
k=1 βin(bk)
h∗(bj(t))dYt −1
h∗h(bj(t))dt
and the minimal variance δi
n(bj) consistent with the number of offspring
being an integer. We will also assume that the branching mechanism of any
two different particles is negatively correlated (given F i+1
n −), i.e.,
where ξj and ξk are the number of offsprings of 2 different particles.
Remark 1. Although the case when the particles branch independently is
included in the result, independence is not necessary. Indeed, we know of examples where better rates of convergence are obtained by taking advantage
of this possibility.
We observe that the minimal variance is connected to the mean by the
following formula
n(bj) = (γ i
n(bj) −[γ i
n(bj)])([γ i
n(bj)] + 1 −γ i
and is always less that 1
4 ([x] is the largest integer smaller that x). Just before
the (i + 1)-th branching, we will have ai
n) particles. Let us denote
by αn( i+1
n −) the state of the process just before the (i +1)-th branching and
j (s), s ∈[ i
n ), j = 1, ..., ai
n, the processes which represents the
trajectories of the particles alive during the interval [ i
n ) corresponding
to the n-th particle system. As before, we will identify the particles with
their path, i.e., the particle bn,i
is the particle whose path is the process
j (s), s ∈[ i
n ) and vice versa.
Remark 2. The branching systems αn will never die and will never explode.
D. Crisan, T. Lyons
Proof. Changes in the number of particles take place only at branching
times. Due to the fact that the variance of the branching mechanism is the
minimal one, the number of offspring for the j-th particle in the interval
is either [γ i
j )], or [γ i
j )] + 1. So
j )] + 1) ≤
j ) + 1) ≤2ai
Also, since Pai
n, at least one γ i
n is larger than or equal to 1,
hence at least one particle has offsprings.
In the remaining part of this section we present several results which
will provide some rough bounds on our processes. In the later sections we
will get much better bounds, but we will need to start with these estimates.
Proposition 2. The process {ai
n −; i = 0, ..., n} is a bounded martingale and E[(ai
n)2] < 2n2.
Proof. From the construction of the particle systems we have that
j=1 βin(bn,i
i = 0, .., n −1.
and the inequality (9) implies that ai
n ≤2in for 0 ≤i ≤n. Since the
branching mechanisms are non-negatively correlated given F i
n −, we obtain
n)2] = E[⟨ai
n⟩] + n2 = E
] + n2 < 2n2,
which completes the proof of the proposition.
Corollary 3. The real valued random variables supt≤1(αn(t), 1) form a
tight sequence.
Proof. We have (αn(t), 1) = 1
(αn(t), 1) ≥k) = P(sup
Solution of the Kushner–Stratonovitch equation
and the last term is actually equal to 1
(αn(t), 1) ≥k) = 0.
which proves our claim.
Remark 3. For arbitrary n ≥1 and ϕ ∈B(E), the random variables
(αn(t), ϕ), 0 ≤t ≤1 are uniformly bounded.
Proof. Straightforwardfromtheinequality(9)andthefactthatϕ isbounded.
Proposition 4. If ϕ, ϕ2 ∈D(A), then the process (αn(t), ϕ) satisﬁes the
following evolution equation
(αn(t), ϕ) = (αn(0), ϕ) +
(αn(s), Aϕ)ds + Sϕ
n (t) + Mϕ
where {(Sϕ
n (t), Ft), t ∈ } is a bounded martingale with quadratic
n ⟩(t) = 1
(αn(s), Aϕ2 −2ϕAϕ)ds
n (l), F l+1
n −), l = 0, 1, ..., n} is a discrete martingale. If ϕ has
constant sign on E, then the conditional quadratic variation of Mϕ
n satisﬁes
and, in general (i.e., if ϕ does not necessarily have constant sign on E),
n ⟩(l) ≤10 ||ϕ||2
Proof. From the construction of the particle systems we have that
D. Crisan, T. Lyons
and also, if ϕ has constant sign on E
If ϕ does not have constant sign on E, we get using (15)
 (ϕ −m)2 + m2
≤10 ||ϕ||2
where m = minx∈E ϕ (x). In between two branches the particles move
according to the prescribed law, hence for t in the interval [ i
(αn(t), ϕ) =
(αn(s), Aϕ)ds + Sϕ,i
where {(Sϕ,i
n (t), Ft), t ∈[ i
n ]} is a bounded martingale (we use Proposition 2) with the quadratic variation
Solution of the Kushner–Stratonovitch equation
n ⟩(t) = 1
(αn(s), Aϕ2 −2ϕAϕ)ds.
In order to compute the evolution equation of (αn(t), ϕ), we need to add all
the parts coming from the particles motion and all the parts coming from
the particle branching, which gives
(αn(t), ϕ) = (αn(0), ϕ) +
(αn(s), Aϕ)ds + Sϕ
where {(Sϕ
n (t), Ft), t ∈ } is the bounded martingale
1= Sϕ,[nt]
with the quadratic variation presented in (11). We then split the term coming
from the branching into a martingale part and a bounded variation part and
(αn(t), ϕ) = (αn(0), ϕ) +
(αn(s), Aϕ)ds + Sϕ
n (t) + Mϕ
n (l), F l+1
n −), l = 0, 1, ..., n} is the square integrable martingale
with (conditional) quadratic variation
The proposition follows now easily from (14), (15), (18) and (19).
D. Crisan, T. Lyons
We introduce also the notation
j (t))dYt −1
k=1 lin(bn,i
Using Itˆo calculus, we get that
k , r)h∗(bn,i
k , r)h(bn,i
4. A-priori estimates for the sequence αn
In this section we derive several useful estimates for the sequence αn. Unless
we say otherwise, we will work under the probability ˜P , as introduced in
Section 2, with ˜E being the expectation with respect to ˜P . We remind the
reader that, under ˜P, Y is a Brownian motion and the law of the signal X
remains unchanged. Also the laws of the processes bn,i
do not change under
the new measure ˜P. In the following, we will use indexed constants ck.
Proposition 5. For all i = 0,..., n −1, we have
where c1 ≡c1(h) is a constant depending only on h.
Proof. From the identity (20) one shows using Gronwall’s inequality that
there exists a constant c2 = c2(h) depending only on h such that
Solution of the Kushner–Stratonovitch equation
We also know that if we have an integer random variable with mean µ and
minimal variance ν, then ν ≤|µ −1|, thus
where c1 = √c2.
Corollary 6.
i. For all n > 0 and t ∈ , ˜E
((αn(t), 1) −1)2
ii. For all n > 0 and t ∈ , ˜E
h supt∈ (αn(t), 1)
where c3 and c4 are constants depending only on h.
i. Straightforward from (10), (12) and (21).
ii. Straightforward from (10), (12), i. and Doob’s maximal inequality.
Proposition 5 tells us that the total amount of randomness introduced
at branching time (measured by the sum of the variances of the individual
branching mechanisms) converges to 0 as n →∞. As a consequence we
get an upper bound for the mass of the processes and also the mass of
αn converges to 1 with a rate of order n−1
4 . The next three results basically
show that αn are asymptotically continuous. Let W be the following set
1= {ϕ, ϕ2 ∈D(A)| s.t. ||ϕ|| , ||Aϕ|| ,
then we have:
Proposition 7. There exist two constants c5 and c6 such that for all n > 0,
i. supϕ∈W ˜E[|(αn(ρ), ϕ)−(αn(τ), ϕ)|] ≤c5(√|ρ −τ|+ 1
√n) for all ρ, τ ∈
ii. supϕ∈W ˜E
|(αn(t), ϕ) −(αn( k
all t ∈[ k
i. Using (10), we get
˜E[|(αn(ρ), ϕ) −(αn(τ), ϕ)|]
(αn(s), Aϕ)ds|] + ˜E[|Sϕ
D. Crisan, T. Lyons
n ([nρ]) −Mϕ
n ([nτ])|]
j ) −1)|].
It is trivial to show that we have an upper bound of the required type for the
ﬁrst three terms of the right hand side of (22), we only prove this for the last
one. Based on the identity (20), we have the following
×(h∗(bn,[nr]
, r)h∗(bn,[nr]
, r)h(bn,[nr]
By taking the modulus then the expectation and using the fact that
we obtain from (23)
(αn(t), 1)2 [nρ] −[nτ]
(αn(t), 1)[nρ] −[nτ]
n + ∥h∥(ρ −τ + 1
which, in turn implies the inequality in i.
Solution of the Kushner–Stratonovitch equation
ii. Follows easily, using (10) as in i.
Let M be a countable dense set in C0(E) such that ϕ, ϕ2 ∈D(A) for
ϕ ∈M. Let ||·||M be the following norm on the set of ﬁnite signed measures
where ϕ0 = 1, (ϕk)k>0 are the elements of M and
∥ϕk∥⊥= ||ϕ|| + ||Aϕ|| +
Then, Proposition 7 implies the following corollary.
Corollary 8. For all n > 0, we have
||αn(ρ) −αn(τ)||M
|ρ −τ| + 1
Using a similar proof as for Proposition 7 we get
Corollary 9. For all n, k > 0, we have for k
and for all ρ ≤τ
(αn (ρ) , 1) ,
where||µ||Z
1= supϕ∈Z(µ, ϕ)andZ = {ϕ ∈D(A)| s.t. ||ϕ|| , ||Aϕ|| ≤1} ,
both inequalities holding ˜P −a.s..
Remark 4. The norm ||·||Z is stronger than the norm ||·||M.
In order to prove the convergence of αn(t) to π(t) we need to ‘unnormalise’ the mass process. For this, we introduce a discrete process
{ξn(k), F k
n ; k = 0, ..., n −1} as follows
Remark 5. The process {ξn(k), F k
n ; k = 0, ..., n−1} is a positive martingale with mean 1.
D. Crisan, T. Lyons
Proposition 10. There exist constants c7, c8, c9 independent of k such that
≤c7, ∀k = 0, ..., n −1.
ξn (k)2 Pak−1
∀k = 0, ..., n −1.
iii. ˜E[(ξn(k)ak−1
)2] ≤c9n2, ∀k = 1, ..., n −1.
i. We have
j (r))dYr,
hence, using the fact that
and Gronwall’s inequality we get from (27) that
≤ec10(r−i
n ) ∀r ∈[ i
(ξn(k −1))2 ˜E
(ξn(k −1))2
By induction we obtain from (28) that ˜E[ξn(k)2] ≤e
≤ec10 ≡c7.
ii. We have
ξn (k −1)2
Solution of the Kushner–Stratonovitch equation
ξn (k −1)2
Using the fact that δk−1
one proves by Itˆo
calculus that
ξn (k −1)2 ak−1
Note that the random variable ξn(k −1) is measurable with respect to the
σ-ﬁeld F k−1
n −. Using this and the martingale property of an we get that
ξn(k −1)2ak−1
ξn(k −1)2 ˜E
ξn (k −2)2 ak−2
ξn (k −2)2 ak−2
hence by induction
ξn (k −1)2 ak−1
∀k = 1, ..., n.
From (29) and (30) we ﬁnally get ii.
iii. We have consecutively that
˜E[(ξn(k)ak−1
)2] = ˜E[(ξn(k −1)2(
≤˜E[(ξn(k −1)2(ak−1
n ˜E[(ξn(k −1)2(ak−2
ξn (k −1)2
Hence from ii. we get that
˜E[(ξn(k)ak−1
n ˜E[(ξn(k −1)2(ak−2
which by induction gives iii.
D. Crisan, T. Lyons
5. The convergence of the sequence to the solution of the K−S equation
We remind the reader that, under ˜P, Y is a Brownian motion. Let {ψs, s ∈
[0, t]} be the solution of the backward stochastic partial differential equation
dψs = −Aψsds −h∗(s)ψs ¯dYs,
where ϕ ∈Cb(E). The equation (31) can be written in the integral form as
h∗(p)ψp ¯dYp r, s ∈[0, t].
In (32), we took
s h∗(p)ψp ¯dYp to be a backward Itˆo integral. Written in
Stratonovitch form, (32) becomes
h∗(p)ψp ◦dYp + 1
h∗h(p)ψpdp. (33)
In order to prove the convergence of the sequence αn(t) to π(t) we introduce
the following condition:
U. For all t ∈ , there exists a countable set M = {ϕk}k>0, uniformly
dense in C0(E), such that for all ϕ ∈M∪{1} the SPDE (31) has a solution
ψs ∈D (A0) , s ∈[0, t] and there exists a constant B1 such that, for all
≤B1 ||ϕ||2
s −2ψsAψs∥
≤B2 ||ϕ||2
where ||ϕ||∗= ||ϕ|| +
Remark 6. Using the monotonicity of (31), assumption (34) holds true for
all ϕ ∈M∪{1} if ˜E
sups∈[0,t] ∥ψs∥2
≤B1 for the solution corresponding
to the ﬁnal condition ϕ ≡1.
Remark 7. IfE = Rd,A = Pd
∂xi∂xj +Pd
∂xi ,where
is a constant positive matrix and fi are smooth real valued functions with
boundedderivatives,h = (hi)m
i=1 isasmoothboundedfunctionwithbounded
derivatives and hi ∈D(A) for 1 ≤i ≤m, then the condition U is satisﬁed
(cf. ).
Solution of the Kushner–Stratonovitch equation
Lemma 11. Let {Vr, Fr, r ∈[s, t]} be a process solution of the martingale
problem associated with the inﬁnitesimal generator A such that the σ-ﬁeld
σ(Vr −Vs, r ∈[s, t]) is independent of Y. Then, under the assumption U,
ψr(Vr) exp
h∗(Vp) ◦dYp −1
≤B2 ||ϕ||2
Proof. From (33) we obtain that
ψr(Vr) = ψs(Vs)−
h∗(Vp)ψp(Vp)◦dYp+1
h∗h(Vp)ψp(Vp)dp+Mψ
r , Fr ∨Y, r ∈[s, t]} is a square integrable martingale (due to
(34)) with quadratic variation
p(Vp) −2ψp(Vp)Aψp(Vp)dp.
h∗(Vp) ◦dYp −1
h∗h(Vp)dp),
ξph∗(Vp) ◦dYp −1
ξph∗h(Vp)dp
ψr(Vr)ξr = ψs(Vs) + M
r , Fr ∨Y, r ∈[s, t]} is a square integrable martingale with
quadratic variation
p(Vp) −2ψp(Vp)Aψp(Vp)dp.
˜E[(ψr(Vr)ξr −ψs(Vs))2|Fs]
p(Vp) −2ψp(Vp)Aψp(Vp)|Fs]dp
p −2ψpAψp∥|Fs
≤B2 ||ϕ||2
D. Crisan, T. Lyons
The last inequality holds true since Aψ2
p −2ψpAψp is independent of ξp
and Fr and ˜E
Fs ∨σ(Yq1 −Yq2; q1, q2 ≥p)
Proposition 12. Under the assumption U there exists a constant c15 such
that, for all ϕ ∈M ∪{1} and k = 1, 2, ..., n −1, we have
≤c15 ||ϕ||2
Proof. By using Itˆo calculus (see also (4.6.57) pp. 124 in ), we observe
n (ϕ) = ρ0(ψ0) = π0(ψ0),
where ψ is deﬁned as in (31) but with the ﬁnal condition at time k
n being ϕ.
We also have (using the independence of xn
˜E[((αn(0), ψ0) −ρ0(ψ0))2] = ˜E[ ˜E[((αn(0), ψ0) −ρ0(ψ0))2 |Y]]
˜E[ ˜E[(ψ0(xn
i ) −ρ0(ψ0))2 |Y]]
≤B1 ||ϕ||2
From (39) and (40) we see that it is enough to prove that there exists a
constant c16 such that, for all ϕ ∈M ∪{1} and k = 1, 2, ..., n −1, we have
−(αn (0) , ψ0)
≤c16 ||ϕ||2
For this we use the following identity
(ξn([nt])αn
n ) −(αn(0), ψ0)
Solution of the Kushner–Stratonovitch equation
and prove that a bound of the required type holds for both terms on the right
hand side of (41). For the ﬁrst term we have
(ξn(i)αn( i
n ) −˜E[(ξn(i)αn( i
(ξn(i)αn( i
n ) −˜E[(ξn(i)αn( i
n (x)|2ξn (i)2
n (x)|2] ˜E[ξn (i)2
≤10B1c8 ||ϕ||2
where the ﬁrst inequality is obtained via the same argument used for proving
(13), the second one results from the independence of the random variables
supx∈E |ψ i
n (x)|2 and ξn (i)2 Pai−1
) and the last inequality is
true because of Proposition 10 ii. Therefore the ﬁrst term from the right
hand side of (41) is of order
ξn(i −1)αn
and using the independence of the trajectories bn,i−1
, 1 ≤j ≤ai−1
n ∨Y, we have
˜E[(ξn(i)αn( i
n −∨Y] −(ξn(i −1)αn(i −1
ξn (i −1)2
From (8), (30) and (35), we have that
≤B2 ||ϕ||2
D. Crisan, T. Lyons
for j = 1, ..., ai
n and i = 0, ..., n−1 and using (43) we obtain the following
B2c13 ||ϕ||2
for the expression (42) which is, in fact, the second term from the right hand
side of (41). Therefore (38) holds true.
Proposition 13. Under the assumption U, for all ϕ ∈M∪{1} and t ∈ 
˜E[|(αn(t), ϕ) −πt(ϕ)|] ≤c17∥ϕ∥⊥
where c17 is a constant independent of n and ϕ and M is the countable and
uniformly dense set in C0(E) deﬁned above and ∥ϕk∥⊥is the norm deﬁned
Proof. First we observe that
|(αn(t), ϕ) −πt(ϕ)| =
(αn(t), ϕ) −(αn([nt]
πt(ϕ) −π [nt]
n ), ϕ) −π [nt]
Using Proposition 7 we immediately get a bound of the required type for the
ﬁrst term of the right hand side of the identity (45) and, using Itˆo calculus,
one obtains from (3) (6) the following upper bounds
πt(ϕ) −π [nt]
n (1)−2] ≤c19,
which, in particular, give us an upper bound on the second term of the right
hand side of the identity (45). Thus, to complete the proof we only need
a bound on ˜E[|(αn( k
n), ϕ) −π k
n (ϕ)|] for k = 1, 2, . . . , n −1. The idea of
the proof is to ‘unnormalise’ αn(t) by multiplying it by the suitably chosen
‘mass’ process ξn([nt]). We have
n), ϕ)−π k
n (ϕ) = (αn(k
n), ϕ)(1−ξn(k)
n (1))+(ξn(k)(αn( k
Solution of the Kushner–Stratonovitch equation
For the ﬁrst term of (47) we obtain, consecutively,
n), ϕ)(1 −ξn(k)
n (1))| ≤||ϕ||(αn(k
n), 1)(1 −ξn(k)
≤||ϕ||(|(αn(k
n), 1)−1| + |(αn(k
n), 1) ξn(k)
n (1) −1|.
From Corollary 6 we have ˜E[(αn(t), 1) −1)2] ≤
√n and using Proposition
12, H¨older’s inequality and the second bound from (46), we get that
n), 1) ξn(k)
n (1) −1|]
˜E[(ξn(k)(αn(k
n), 1) −ρ k
n (1))2] ˜E[ρ [nt]
So we have the required upper bound for the ﬁrst term in (47). Finally, we
treat the second term in (47) in the same way as in (48):
˜E [|(αn(k
n), ϕ) ξn(k)
˜E[(ξn(k)(αn(k
n), ϕ) −ρ k
n (ϕ))2] ˜E[ρ [nt]
√c15c19 ||ϕ||∗
By putting together (45), (47), (48) and (49), we get (44).
Let ||·||M be the norm on the set of ﬁnite signed measures on E deﬁned
as in (24). Then, Proposition 7 implies the following corollary:
Corollary 14. For all n > 0, we have
||αn(ρ) −αn(τ)||M
|ρ −τ| + 1
Proposition 13 implies, in turn, the following theorem:
Theorem 15. Under the assumption U, for all t ∈ , we have that
||αn(t) −πt||M
and, in particular, this implies that the sequence αn(t) is convergent in
measure to πt in MF(E), i.e., to the distribution of the signal process X(t)
given the observation σ-ﬁeld Yt.
D. Crisan, T. Lyons
Proof . From (44) and (24) we have, immediately,
||αn(t) −πt||M
˜E [|(αn(t), ϕk) −πt(ϕk)|]
Theorem 16. Under the assumption U, the sequence αn converges in distribution to π in DM
F (E) . Furthermore if there exists a sequence of
compact sets Kk ∈E and a sequence of functions {ϕk}k>0 ⊂D(A0),
ϕk : E → such that limk→∞ϕk = 1 π0-a.s., limk→∞∥Aϕk∥= 0
CKk+1 = 0 (CA
1=E −A), then αn converges in distribution to π in
DMF (E) .
Proof. Since the proof of the theorem is similar to that from we give
here only a sketch of it. We need to show ﬁrst that the sequence is tight
over the space DM′
F (E) (resp., DMF (E) ). For this we need to prove
(cf. ) that, for each ﬁxed i, the processes {(αn(s), ϕi), s ∈ } form
a tight sequence, where {ϕi}i≥0 is deﬁned as follows: ϕ0 is the constant
function 1 and {ϕi}i≥1 is a dense set in C0(E). We will take {ϕi}i≥1 from the
domain of A0 (the restriction of A to C0(E)), which is assumed to be a dense
algebra in C0(E). In order to prove that {(αn(s), ϕi), s ∈ } form a tight
sequence, we use Aldous’ criteria (cf. ) and show that the mass processes
form a tight sequence (Corollary 3) and that for any arbitrary sequence of
stopping times {τn}n>0 (with respect to the natural ﬁltration of {αn}n>0) and
any sequence {ρn}n>0 of positive real numbers with limn→∞ρn →0, we
have (αn(τn + ρn), ϕi) −(αn(τn), ϕi) →0 in probability (the proof is
similar to that of Proposition 7).
Once we know that the sequence is tight over DM′
F (E) , if the extra
condition holds true, then there exists a sequence of compact sets Kk ∈E
such that, for every ε > 0,
k→∞lim sup
(Un(t), ICKk) > ε) = 0,
which implies that the sequence is tight over DMF (E) .
Since for any ﬁxed t, αn(t) converges in measure to π(t), for all ﬁnite
sequences 0 ≤t1 ≤t2 ≤· · · ≤tk ≤1 (αn(t1), αn(t2), ..., αn(tk)) converges
in measure to (πt1, πt2, ..., πtk) and hence also in distribution, i.e., the ﬁnite
dimensional distributions of αn converge to the ﬁnite dimensional distributions of π. This, together with the tightness of the sequence gives us the
required convergence in distribution of the processes αn to π.
Solution of the Kushner–Stratonovitch equation
6. The computational effort
In order to discuss the convergence of the algorithm we must identify the
notion of distance between measures, for example by choosing an appropriate norm (in our case ||·||M). It is important to understand that norms
are not equivalent in inﬁnite dimensional spaces, so different norms might
mean different rates of convergence, as the following example shows us:
Example 1. Let µ be the uniform measure on the interval and µn be
the empirical measure associated to a sample of n independent particles
Deﬁne ||·||p to be the following norm on the set of ﬁnite signed measures
|ρtk −ρtk−1|p
where π = {0 = t0 < t1 < · · · < tmπ = t} is an arbitrary partition
of the interval (the supremum is taken over all the partitions) and
ρt : → is the distribution function of ν. In other words, ||ν||p is
the p-variation of the distribution function of ν. Then, for p ∈ , there
exist constants cp
22 independent of n such that
||µn −µ||p
Our intention is to control the error with respect to the norm ||·||M
deﬁned in the previous section. We are not particularly proud of this choice
of norm and would be pleased to see an improvement on this. The main result
shows that if we require an error of order 1
n then we need to do roughly n8
computations. The next proposition tells us what the amount of computation
needed in order to compute αn(t), which we will denote by 4n(t), is
Proposition 17. There exists a constant c22 independent of n and t such
4n(t) = c22n2
(αn(s), 1)ds.
D. Crisan, T. Lyons
Proof. Let us denote by c23 the amount of computation needed to calculate
the trajectory of a particle in an arbitrary interval [ k
n ] and by c24 the
amount of computation needed to calculate the number of offsprings of
that particle at the end of the interval. We ﬁx the constants so that they
are independent of n. By a simple calculation we get then that in order to
compute αn(t), we need (c23 + c24) P[nt]
n, hence (51) holds true.
Corollary 18. There exists a constant c25 independent of n and t such that
˜E [4n(t)] ≤c25n2.
Proof. Straightforward from (51) and Corollary 6.
Theorem 19. If 4(t) is the amount of computation we do for approximating πt by αn(t) then the error is, almost surely, of order o(4−1
8 +ι) for all
Proof. We have that
4n(t)γ ||αn(t) −πt||δ
1−δ ]1−δ ˜E
||αn(t) −πt||M
0 (αn(s), 1)ds
≤c27n2γ −δ
4 if γ ≤2(1 −δ).
We need 2γ −δ
4 < 0 ⇐⇒γ ≤δ
8 < 2(1 −δ). We take then a subsequence
(nk)k>0 such that
(for instance nk = 2k) and get that
4nk(t)γ ||αn(t) −πt||δ
which implies that
4nk(t)γ ||αn(t) −πt||δ
M < ∞, ˜P −a.s.,
which, in turn implies that
k→∞4nk(t)γ ||αn(t) −πt||δ
M = 0, ˜P −a.s.,
Solution of the Kushner–Stratonovitch equation
hence, for all ε there exists kε such that
||αn(t) −πt||M ≤ε4nk(t)−γ
≤ε4nk(t)−1
for all k ≥kε (ι