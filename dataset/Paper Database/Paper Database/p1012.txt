Explanation Methods in Deep Learning:
Users, Values, Concerns and Challenges∗
Gabri¨elle Ras, Marcel van Gerven, Pim Haselager
Radboud University, Donders Institute for Brain, Cognition and Behaviour,
Nijmegen, the Netherlands
{g.ras, m.vangerven, w.haselager}@donders.ru.nl
Issues regarding explainable AI involve four components: users, laws & regulations, explanations and algorithms. Together these components provide a context in which explanation
methods can be evaluated regarding their adequacy. The goal of this chapter is to bridge the
gap between expert users and lay users. Diﬀerent kinds of users are identiﬁed and their concerns revealed, relevant statements from the General Data Protection Regulation are analyzed
in the context of Deep Neural Networks (DNNs), a taxonomy for the classiﬁcation of existing
explanation methods is introduced, and ﬁnally, the various classes of explanation methods are
analyzed to verify if user concerns are justiﬁed. Overall, it is clear that (visual) explanations can
be given about various aspects of the inﬂuence of the input on the output. However, it is noted
that explanation methods or interfaces for lay users are missing and we speculate which criteria
these methods / interfaces should satisfy. Finally it is noted that two important concerns are
diﬃcult to address with explanation methods: the concern about bias in datasets that leads to
biased DNNs, as well as the suspicion about unfair outcomes.
Introduction
Increasingly, Artiﬁcial Intelligence (AI) is used in order to derive actionable outcomes from data
(e.g. categorizations, predictions, decisions). The overall goal of this chapter is to bridge the
gap between expert users and lay users, highlighting the explanation needs of both sides and
analyzing the current state of explainability. We do this by taking a more detailed look at each
component mentioned above and in Figure 1. Finally we address some concerns in the context
The components of explainability
Issues regarding explainable AI (XAI) involve (at least) four components: users, laws and
regulations, explanations and algorithms.
Together these components provide a context in
which explanation methods can be evaluated regarding their adequacy. We brieﬂy discuss these
components in Figure 1.
Users and laws
AI has a serious impact on society, due to the large scale adoption of digital automation techniques that involve information processing and prediction.
Deep Neural Networks (DNNs)
∗This article will appear as a chapter in Explainable and Interpretable Models in Computer Vision and Machine
Learning, a Springer series on Challenges in Machine Learning.
 
belong to this set of automation techniques and are used increasingly because of their capability to extract meaningful patterns from raw input. DNNs are fed large quantities of digital
information that are easily collected from users. Currently there is much debate regarding the
safety of and trust in data processes in general, leading to investigations regarding the explainability of AI-supported decision making. The level of concern about these topics is reﬂected by
oﬃcial regulations such as the General Data Protection Regulation (GDPR), also mentioned in
 , incentives to promote the ﬁeld of explainability and institutional initiatives to ensure the safe development of AI such as
OpenAI. As the technology becomes more widespread, DNNs in particular, the dependency on
said technology increases and ensuring trust in DNN technology becomes a necessity. Current
DNNs are achieving unparalleled performance in areas of Computer Vision (CV) and Natural
Language Processing (NLP). They are also being used in real-world applications in e.g. medical
imaging , autonomous driving and legislation (Lockett
Explanation and DNNs
The challenge with DNNs in particular lies in providing insight into the processes leading to
their outcomes, and thereby helping to clarify under which circumstances they can be trusted to
perform as intended and when they cannot. Unlike other methods in Machine Learning (ML),
such as decision trees or Bayesian networks, an explanation for a certain decision made by a
DNN cannot be retrieved by simply scrutinizing the inference process. The learned internal
representations and the ﬂow of information through the network are hard to analyze: As architectures get deeper, the number of learnable parameters increases. It is not uncommon to
have networks with millions of parameters. Furthermore, network architecture is determined by
various components (unit type, activation function, connectivity pattern, gating mechanisms)
and the result of a complex learning procedure, which itself depends on various properties (regularization, adaptive mechanisms, employed cost function). The net result of the interaction
between these components cannot be predicted in advance. Because of these complications,
DNNs are often called black box models, as opposed to glass-box models . Fortunately, these problems have not escaped the attention of the ML/Deep Learning
(DL) community . Research on
how to interpret and explain the decision process of Artiﬁcial Neural Networks (ANNs) has been
going on since the late 1980’s . The objective of explanation
methods is to make speciﬁc aspects of a DNN’s internal representations and information ﬂow
interpretable by humans.
Users and their concerns
Various kinds of DNN users can be distinguished. Users entertain certain values; these include
ethical values such as fairness, neutrality, lawfulness, autonomy, privacy or safety, or functional
values such as accuracy, usability, speed or predictability. Out of these values certain concerns
regarding DNNs may arise, e.g. apprehensions about discrimination or accuracy. These concerns
get translated into questions about the system, e.g. “did the factor race inﬂuence the outcome
of the system” or “how reliable was the data used?” In this section we identify at least two
general types of users: the expert users and the lay users, that can be further categorized into
six speciﬁc kinds of users. Note that there could be (and there regularly is) overlap between the
users described below, such that a particular user can be classiﬁed as belonging to more than
one of the categories.
1. Expert users are the system builders and/or modiﬁers that have direct inﬂuence on the
implementation of the network. Two kinds of experts can be identiﬁed:
(a) DNN engineers are generally researchers involved in extending the ﬁeld and have
detailed knowledge about the mathematical theories and principles of DNNs. DNN
Figure 1: Issues regarding explainable DNNs involve (at least) four components: users, algorithms,
laws and explanations. Together these components provide a context in which explanations can be
evaluated regarding their adequacy.
engineers are interested in explanations of a functional nature, e.g.
the eﬀects of
various hyperparameters on the performance of the network or methods that can be
used for model debugging.
(b) DNN developers are generally application builders who make software solutions
that can be used by lay people. DNN developers often make use of oﬀ-the-shelf DNNs,
often re-training the DNN along with tuning certain hyperparameters and integrating
them with various software components, resulting in a functional application. The
DNN developer is concerned with the goals of the overall application and assesses
whether they have been met by the DNN solution. DNN developers are interested in
explanation methods that allow them to understand the behavior of the DNN in the
various use cases of the integrated software application.
2. Lay users do not and need not have knowledge of how the DNN was implemented and
the underlying mathematical principles, nor do they require knowledge of how the DNN
was integrated with other software components resulting in a ﬁnal functional application.
At least four lay users are identiﬁed:
(a) The owner of the software application in which the DNN is embedded. The owner
is usually an entity that acquires the application for possible commercial, practical or
personal use. For example, an owner can be an organization (e.g. a hospital or a car
manufacturer) that purchases the application for end users (e.g. employees (doctors)
or clients (car buyers)), but the owner can also be a consumer that purchases the application for personal use. In the latter case the categorization of owner fully overlaps
with the next category of users which are the end users. The owner is concerned with
explainability questions about the capabilities of the application, e.g. justiﬁcation of
a prediction or a prediction given the input data, and aspects of accountability, e.g.
to what extent can application malfunction be attributed to the DNN component?
(b) The end user for whom the application was intended to be used by. The end user
uses the application as part of their profession or for personal use. The end user is
concerned with explainability about the capabilities of the application, e.g. justiﬁcation of a prediction given the input data, and explainability regarding the behavior of
the application, e.g. why does the application not do what it was advertised to do?
(c) The data subject is the entity whose information is being processed by the application or the entity which is directly aﬀected by the application outcome. An outcome
is the output of the application in the context of the use case. Sometimes the data
subject is the same entity as the end user, for example in the case that the application
is meant for personal use. The data subject is mostly concerned with the ethical and
moral aspects that result from the actionable outcomes. An actionable outcome is
an outcome that has consequences or an outcome on which important decisions are
(d) Stakeholders are people or organizations without a direct connection to either the
development, use or outcome of the application and who can reasonably claim an
interest in the process, for instance when its use runs counter to particular values
they protect. Governmental and non-governmental organizations may put forward
legitimate information requests regarding the operations and consequences of DNNs.
Stakeholders are often interested in the ethical and legal concerns raised in any phase
of the process.
Case study: autonomous driving
In this section the diﬀerent users are presented in the context of a self-driving car.
1. The DNN engineer creates a DL solution to the problem of object segmentation and object
classiﬁcation by experimenting with various types of networks. Given raw video input the
DL solution gives the output of the type of object and the location of the object in the
2. The DNN developer creates a planning system which integrates the output of the DL
solution with other components in the system. The planning system decides which actions
the car will take.
3. The owner acquires the planning system and produces a car in which the planning system
is operational.
4. The end user purchases the car and uses the car to travel from point A to point B.
5. The data subjects are all the entities from which information is captured along the route
from point A to point B: pedestrians, private property such as houses, other cars.
6. The stakeholders are governmental institutions which formulate laws regulating the use
of autonomous vehicles, or insurance companies that have to assess risk levels and their
consequences.
Laws and regulations
An important initiative within the European Union is the General Data Protection Regulation
(GDPR) 1 that was approved on April 14, 2016, and became enforceable on May 25, 2018. The
GDPR distinguishes between personal data, data subjects, data processors and data controllers
(Article 4, Deﬁnitions, Paragraphs 1, 7 & 8). Personal data is deﬁned as “any information
relating to an identiﬁed or identiﬁable natural person (data subject)”. A data processor is the
natural or legal person, public authority, agency or other body which processes data on behalf
of the data controller, who determines the purposes, conditions and means of the processing.
Hence, the DNN can function as a tool to be used by the data processor, whereas owners or end
users can ﬁll the role of data controllers.
The GDPR focuses in part on proﬁling: “any form of automated processing of personal
data consisting of the use of personal data to evaluate certain personal aspects relating to a
1 
natural person, in particular to analyse or predict aspects concerning that natural person’s
performance at work, economic situation, health, personal preferences, interests, reliability,
behaviour, location or movements” (Article 4, Deﬁnitions, Paragraph 4). According to articles
13, 14 and 15, when personal data is collected from a data subject for automated decisionmaking, the data subject has the right to access, and the data controller is obliged to provide,
“meaningful information about the logic involved.” Article 12 stipulates that the provision of
information to data subjects should be in “concise, transparent, intelligible and easily accessible
form, using clear and plain language.”
Explanation
The right to meaningful information translates into the demand that actionable outcomes of
DNNs need to be explained, i.e.
be made transparent, interpretable or comprehensible to
humans. Transparency refers to the extent to which an explanation makes a speciﬁc outcome
understandable to a particular (group of) users. Understanding, in this context, amounts to a
person grasping how a particular outcome was reached by the DNN. Note that this need not
imply agreeing with the conclusion, i.e. accepting the outcome as valid or justiﬁed. In general,
transparency may be considered as recommendable, leading to e.g. a greater (societal) sense of
control and acceptance of ML applications. Transparency is normally also a precondition for
accountability: i.e. the extent to which the responsibility for the actionable outcome can be
attributed to legally (or morally) relevant agents (governments, companies, expert or lay users,
etc.). However, transparency may also have negative consequences, e.g. regarding privacy or by
creating possibilities for manipulation (of data, processing or training).
In relation to the (perceived) need for explanation, two reasons for investigation stand out
in particular. First, a DNN may appear to dysfunction, i.e. fail to operate as intended, e.g.
through bugs in the code (process malfunction). Second, it may misfunction, e.g. by producing
unintended or undesired (side-)eﬀects that are
deemed to be societally or ethically unacceptable (outcome malfunction). Related to dysfunction
is a ﬁrst category of explanations. This category is based on the information necessary in order
to understand the system’s basic processes, e.g. to assess whether it is functioning properly,
as intended, or whether it dysfunctions (e.g. suboptimal or erroneous results). This type of
explanation is normally required by DNN developers and expert users.
The information is
used to interpret, predict, monitor, diagnose, improve, debug or repair the functioning of a
system .
Once an application is made available to non-expert users, normally certain guarantees regarding the systems proper functioning are in place. Generally speaking, owners, end users,
data subjects and stakeholders are more interested in a second category of explanations, where
suspicions about a DNN’s misfunctioning (undesired outcomes) leads to requests for “local explanations”. Users may request information about how a particular outcome was reached by
the DNN, which aspects of input data, which learning factors or other parameters of the system
inﬂuenced its decision or prediction. This information is then used to assess the appropriateness
of the outcome in relation to the concerns and values of users . The aim of local explanations is to strengthen
the conﬁdence and trust of users that the system is not (or will not be) conﬂicting with their
values, i.e. that it does not violate fairness or neutrality. Note that this implies that the oﬀered
explanations should match (within certain limits) the particular user’s capacity for understanding , as indicated by the GDPR.
Explanation methods
So far the users, the GDPR, and the role of explanations have been discussed. To bridge the
gap from that area to the more technical area of explanation methods, we need to be able to
evaluate the capabilities of existing methods, in the context of the users and their needs. We
bridge the gap in two ways. First, we identify, on a high level, desirable properties of explanation
methods. Second, we introduce a taxonomy to categorize all types of explanation methods and
third, assess the presence of the desirable properties in the categories in our taxonomy.
Desirable properties of explainers
Based on a survey of the literature, we arrive at the following properties which any explainer
should have:
1. High Fidelity The degree to which the interpretation method agrees with the inputoutput mapping of the DNN. This term appears in . Fidelity is arguably the most important property
that an explanation model should possess. If an explanation method is not faithful to the
original model then it cannot give valid explanations because the input-output mapping
is incorrect.
2. High Interpretabiliy To what extent a user is able to obtain true insight into how
actionable outcomes are obtained. We distinguish interpretability into the following two
subproperties:
(a) High Clarity The degree to which the resulting explanation is unambiguous. This
property is extremely important in safety-critical applications 
where ambiguity is to be avoided. introduces a quantiﬁable
measure of clarity (unambiguity) for their method.
(b) High Parsimony This refers to the complexity of the resulting explanation.
explanation that is parsimonious is a simple explanation. This concept is generally
related to Occam’s razor and in the case of explaining DNNs the principle is also of
importance. The optimal degree of parsimony can in part be dependent on the user’s
capabilities.
3. High Generalizability The range of architectures to which the explanation method can
be applied. This increases the usefulness of the explanation method. Methods that are
model-agnostic are the highest in generalizability.
4. High Explanatory Power In this context this means how many phenomena the method
can explain. This roughly translates to how many diﬀerent kinds of questions the method
can answer. Previously in Section 2 we have identiﬁed a number of questions that users
may have. It is also linked to the notion that the explainer should be able to take a global
perspective , in the sense that it can explain the behaviour of the
model rather than only accounting for individual predictions.
A taxonomy for explanation methods
Over a relatively short period of time a plethora of explanation methods and strategies have
come into existence, driven by the need of expert users to analyze and debug their DNNs.
However, apart from a non-exhaustive overview of existing methods and
classiﬁcation schemes for purely visual methods , little is known about eﬀorts to rigorously map the landscape of
explanation methods and isolate the underlying patterns that guide explanation methods. In
this section a taxonomy for explanation methods is proposed. Three main classes of explanation
methods are identiﬁed and their features described. The taxonomy was derived by analyzing
the historical and contemporary trends surrounding the topic of interpretation of DNNs and
explainable AI. We realize that we cannot foresee the future developments of DNNs and their
explainability methods.
As such it is possible that in the future the taxonomy needs to be
modiﬁed. We propose the following taxonomy:
Rule-extraction methods
Extract rules that approximate the decision-making process in a DNN by utilizing the
input and output of the DNN.
Attribution methods
Measures the importance of a component by changing to the input or internal components
and recording how much the changes aﬀect model performance. Methods known by other
names that fall in this category are occlusion, perturbation, erasure, ablation and inﬂuence. Attribution methods are often visualized and sometimes referred to as visualization
Intrinsic methods
Aim to improve the interpretability of internal representations with methods that are part
of the DNN architecture.
Intrinsic methods increase ﬁdelity, clarity and parsimony in
attribution methods.
In the following subsections we will describe the main features of each class and give examples
from current research.
Rule-extraction methods
Rule-extraction methods extract human interpretable rules that approximate the decisionmaking process in a DNN. Older genetic algorithm based rule extraction methods for ANNs
can be found in . Andrews et al.
 specify three categories of rule extraction methods:
Decompositional approach
Decomposition refers to breaking down the network into smaller individual parts. For the
decompositional approach, the architecture of the network and/or its outputs are used in
the process. Zilke et al. uses a decompositional algorithm that extracts rules for
each layer in the DNN. These rules are merged together in a ﬁnal merging step to produce
a set of rules that describe the network behaviour by means of its inputs. Murdoch and
Szlam succeeded in extracting rules from an LSTM by applying a decompositional
Pedagogical approach
Introduced by Craven and Shavlik and named by Andrews et al. the pedagogical approach involves “viewing rule extraction as a learning task where the target
concept is the function computed by the network and the input features are simply the
network’s input features” . The pedagogical approach has the
advantage that it is inherently model-agnostic. Recent examples are found in .
Eclectic approach
According to Andrews et al. “membership in this category is assigned to techniques
which utilize knowledge about the internal architecture and/or weight vectors in the trained
artiﬁcial neural network to complement a symbolic learning algorithm.”
In terms of ﬁdelity, local explanations are more faithful than global explanations. For ruleextraction this means that rules that govern the result of a speciﬁc input, or a neighborhood of
inputs are more faithful than rules that govern all possible inputs. Rule extraction is arguably
the most interpretable category of methods in our taxonomy considering that the resulting set
of rules can be unambiguously be interpreted by a human being as a kind of formal language.
Therefore we can say that it has a high degree of clarity. In terms of parsimony we can say that
if the ruleset is ”small enough” the parsimony is higher than when the ruleset is “too large”.
What determines “small enough” and “too large” is diﬃcult to quantify formally and is also
dependent on the user (expert vs. lay). In terms of generalizability it can go both ways: if
a decompositional approach is used it is likely that the method is not generalizable, while if a
pedagogical approach is used the method is highly generalizable. In terms of explanatory power,
rule-extraction methods can 1) validate whether the network is working as expected in terms of
overall logic ﬂow, and 2) explain which aspects of the input data had an eﬀect that lead to the
speciﬁc output.
Attribution methods
Attribution, a term introduced by Ancona et al. , also referred to as relevance , contribution , class saliency or inﬂuence , aims to reveal components of high importance in the input to the DNN and their eﬀect as the input is propagated through the network.
Because of this property we can categorize the following methods to the attribution category:
occlusion , erasure , perturbation , adversarial examples and prediction diﬀerence analysis . Other methods that belong to this category are found in . It is worth mentioning that attribution methods
do not only apply to image input but also to other forms of input, such as text processing by
LSTMs . The deﬁnition of attribution methods in this chapter is similar
to that of saliency methods , but more general than the deﬁnition of
attribution methods in akin to the deﬁnition in .
The majority of explanation methods for DNNs visualize the information obtained by attribution methods. Visualization methods were popularized by in recent years and are concerned with how the important
features are visualized. Zeng identiﬁes that current methods focus on three aspects of
visualization: feature visualization, relationship visualization and process visualization. Overall
visualization methods are very intuitive methods to gain a variety of insight about a DNN decision process on many levels including architecture assessment, model quality assessment and
even user feedback integration, e.g. Olah et al. create intuitive visualization interfaces
for image processing DNNs.
Kindermans et al. has shown recently that attribution methods “lack reliability when
the explanation is sensitive to factors that do not contribute to the model prediction.” Furthermore they introduce the notion of input invariance as a prerequisite for accurate attribution. In
other words, if the attribution method does not satisfy input invariance, we can consider it to
have low ﬁdelity. In terms of clarity, there is a degree of ambiguity that is inherent with these
methods because visual explanations can be interpreted in multiple ways by diﬀerent users, even
by users in the same user category. In contrast to the precise results of rule-extraction methods,
the information that results from attribution methods has less structure. In addition, the degree
of clarity is dependent on the degree of ﬁdelity of the method: low ﬁdelity can cause incorrect
attribution, resulting in noisy output with distracting attributions that increase ambiguity. The
degree of parsimony depends on the method of visualization itself. Methods that visualize only
the signiﬁcant attributions exhibit a higher degree of parsimony. The degree of generalizability
depends on which components are used to determine attribution. Methods that only use the input and output are inherently model agnostic, resulting in the highest degree of generalizability.
Following this logic, methods that make use of internal components are generalizable to the degree that other models share these components. For example, deconvolutional networks can be applied to models that make use of convolutions to extract features from
input images. In terms of explanatory power, this class of methods can reﬂect intuitively with
visual explanations which factors in the input dimension had a signiﬁcant impact on the output of the DNN. However these methods do not explain the reason for the importance of the
particular factor attribution.
Intrinsic methods
The previous categories are designed to make explainable some aspects of a DNN in a process
separate from training the DNN. In contrast, this category aims to improve the interpretability
of internal representations with methods that are part of the DNN architecture, e.g. as part
of the loss function , modules that add additional capabilities , or as part of the architecture structure, in terms of operations
between layers .
Dong et al. provide an interpretive loss function to increase the visual ﬁdelity of the
learned features.
More importantly Dong et al. show that by training DNNs with
adversarial data and a consistent loss, we can trace back errors made by the DNN to individual
neurons and identify whether the data was adversarial. Santoro et al. give a DNN the
ability to answer relational reasoning questions about a speciﬁc environment, by introducing a
relational reasoning module that learns a relational function, which can be applied to any DNN.
Palm et al. build on work by Santoro et al. and introduces a recurrent relational
network which can take the temporal component into account. Li et al. introduce an
explicit structure to DNNs for visual recognition by building in an AND-OR grammar directly
in the network structure.
This leads to better interpretation of the information ﬂow in the
network, hence increased parsimony in attribution methods. Louizos et al. make use of
generative neural networks perform causal inference and Goudet et al. use generative
neural networks to learn functional causal models. Intrinsic methods do not explicitly explain
anything by themselves.
Instead they increase ﬁdelity, clarity and parsimony in attribution
methods. This class of methods is diﬀerent from attribution methods because it tries to make the
DNN inherently more interpretable by changing the architecture of the DNN, where attribution
methods use what is there already and only transform aspects of the representation to something
meaningful after the network is trained.
Addressing general concerns
As indicated in Figure 1, users have certain values, that in relation to a particular technology
may lead to concerns, that in relation to particular applications can lead to speciﬁc questions.
Mittelstadt et al. and Danks and London distinguish various concerns that users
may have. The kinds of concerns they discuss focus to a large extent on the inconclusiveness,
inscrutability or misguidedness of used evidence. That is, they concern to a signiﬁcant extent
the reliability and accessibility of used data (data mining, generally speaking).
In addition
to apprehensions about data, there are concerns that involve aspects of the processing itself,
e.g. the inferential validity of an algorithm. Also, questions may be raised about the validity
of a training process (e.g.
requiring information about how exactly a DNN is trained).
the following, we provide a list of general concerns that should be addressed when developing
predictive models such as DNNs:
Flawed data collection
Data collection may be ﬂawed in several ways. Large labeled datasets that are used to
train DNNs are either acquired by researchers (often via crowdsourcing) or by companies
that ‘own’ the data. However, data quality may depend on multiple factors such as noise
or censoring and there is no strict control on whether data is annotated correctly. Furthermore, the characteristics of the workers who annotated the data may introduce unwanted
biases . These biases may be due to preferences that do not
generalize across cultures or due to stereotyping, where sensitivity to irrelevant attributes
such as race or gender may induce unfair actionable outcomes. The same holds for the selection of the data that is used for annotation in the ﬁrst place. Used data may reﬂect the
status quo, which is not necessarily devoid of biases . Furthermore,
selection bias may have as a result that data collected in one setting need not generalize
to other settings. For example, video data used to train autonomous driving systems may
not generalize to other locations or conditions.
Inscrutable data use
The exact use of the data to train DNNs may also be opaque. Users may worry about
what (part of the) data exactly has led to the outcome. Often it is not even known to
the data subject which personal data is being used for what purposes. A case in point
is the use of person data for risk proﬁling by governmental institutions.
For example,
criticisms have been raised about the way the Dutch SyRI system uses data to detect
fraud.2 Furthermore, the involvement of expert users who may be prone to biases as well
may have an implicit inﬂuence on DNN training.
2 
Suboptimal inferences
The inferences made by DNNs are of a correlational rather than a causal nature. This
implies that subtle correlations between input features may inﬂuence network output,
which themselves may be driven by various biases. Work is in progress to mitigate or
remove the inﬂuence of sensitive variables that should not aﬀect decision outcomes by
embracing causal inference procedures . Note further that the
impact of suboptimal inferences is domain dependent. For example, in medicine and the
social sciences, suboptimal inferences may directly aﬀect the lives of individuals or whole
populations whereas in the exact sciences, suboptimal inferences may aﬀect evidence for
or against a speciﬁc scientiﬁc theory.
Undesirable outcomes
End users or data subjects may feel that the outcome of the DNN is somehow undesirable in relation to the particular values they hold, e.g. violating fairness or privacy.
Importantly, actionable outcomes should take into account preferences of the stakeholder,
which can be an individual (e.g. when deciding on further medical investigation) as well
as the community as a whole (e.g. in case of policies about autonomous driving or predictive policing). These considerations demand the involvement of domain experts and
ethicists already in the earliest stages of model development. Finally, model predictions
may be of a statistical rather than deterministic nature. This speaks for the inclusion of
decision-theoretic constructs in deciding on optimal actionable outcomes .
Adversarial attacks
Images and audio can
easily be distorted with modiﬁcations that are imperceptible to humans. Such distortions
cause DNNs to make incorrect inferences and can be done with the purpose of intentionally
misleading DNNs (e.g. yielding predictions in favor of the perpetrator). Work in progress
shows that there are methods to detect adversarial instances and to
mitigate the attacks . However further research is needed to increase the
robustness of DNNs against adversarial attacks as there are no methods in existence that
fully diminish the eﬀects of adversarial attacks.
As stated by Doran et al. , explanation methods may make predictive models such as
DNNs more comprehensible. However, explanation methods alone not completely resolve the
raised concerns.
Discussion
In this chapter we set out to analyze the question of “What can be explained?”
users and their needs, laws and regulations, and existing explanation methods. Speciﬁcally, we
looked at the capabilities of explanation methods and analyzed which questions/concerns about
explainability these methods address in the context of DNNs.
Overall, it is clear that (visual) explanations can be given about various aspects of the
inﬂuence of the input on the output (e.g.
given the input data, which aspects of the data
lead to the output?), by making use of both rule-extraction and attribution methods. Also,
when used in combination with attribution methods, intrinsic methods lead to more explainable
It is likely that in the future we will see the rise of a new category of explanation
methods that combine aspects of rule-extraction, attribution and intrinsic methods, to answer
speciﬁc questions in a simple human interpretable language.
Furthermore, it is obvious that current explanation methods are tailored to expert users,
since the interpretation of the results require knowledge of the DNN process. As far as we are
aware, explanation methods, e.g. intuitive explanation interfaces, for lay users do not exist.
Ideally, if such explanation methods would exist, they should be able to answer, in a simple
human language, questions about every operation that the application performs. This is not
an easy task since the number of conceivable questions one could ask about the working of an
application is substantial.
Two particular concerns, which are diﬃcult to address with explanation methods, is the
concern about bias in datasets that leads to biased DNNs, as well as the suspicion about unfair
outcomes: Can we indicate that the DNN is biased, and if so, can we remove the bias? Has
the DNN been applied responsibly?
These are not problems that are directly solvable with
explanation methods. However, explanation methods alleviate the ﬁrst problem to the extent
that learned features can be visualized (using attribution methods) and further analyzed for
bias using other methods that are not explanation methods. For the second problem, more
general measures, such as regulations and laws, will need to be developed.