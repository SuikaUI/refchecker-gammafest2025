Agnostic Active Learning
Maria-Florina Balcan
 
School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213-3891
Alina Beygelzimer
 
IBM T. J. Watson Research Center, Hawthorne, NY 10532
John Langford
 
Toyota Technological Institute at Chicago, Chicago, IL 60637
We state and analyze the ﬁrst active learning algorithm which works in the presence
of arbitrary forms of noise. The algorithm,
A2 (for Agnostic Active), relies only upon
the assumption that the samples are drawn
from a ﬁxed distribution.
that A2 achieves an exponential improvement
(i.e., requires only O
samples to ﬁnd
an ϵ-optimal classiﬁer) over the usual sample
complexity of supervised learning, for several
settings considered before in the realizable
case. These include learning threshold classiﬁers and learning homogeneous linear separators with respect to an input distribution
which is uniform over the unit sphere.
1. Introduction
What distinguishes active learning from the more typical batch learning is that the algorithm initially sees
only the unlabeled portion of a pool of examples drawn
from some underlying distribution. The algorithm can
then pay for the label of any example in the pool,
and the hope is that a good classiﬁer can be learned
with signiﬁcantly fewer labels by actively directing the
queries to informative examples. There is a signiﬁcant
practical interest in minimizing the number of labeled
examples in settings where there is no shortage of unlabeled data but labels are expensive.
Most active learning strategies are noise seeking on
many natural learning problems.
In particular, the
process of trying to ﬁnd an optimal separation between
one class and another often involves label queries for
examples with a large conditional noise rate. Thus the
Appearing in Proceedings of the 23 rd International Conference on Machine Learning, Pittsburgh, PA, 2006. Copyright 2006 by the author(s)/owner(s).
most informative examples are also the ones that are
typically the most noise-prone.
Consider the active learning algorithm which searches
for the optimal threshold on an interval using binary
search. This example is often used to demonstrate the
potential of active learning in the noise-free case when
there is a perfect threshold separating the classes . Binary search needs O(ln 1
ϵ ) labeled
examples to learn a threshold with error less than ϵ,
while learning passively requires O
labels. A fundamental drawback of this algorithm is that a small
amount of adversarial noise can force the algorithm
to behave badly. Is this extreme brittleness to small
amounts of noise essential? Can we still achieve an
exponential decrease in sample complexity? Can we
avoid making assumptions about the mechanism producing noise? These are the questions addressed here.
Previous Work on Active Learning
been substantial work on active learning under additional assumptions.
For example, the Query by Committee analysis assumes realizability (i.e., there exists a perfect classiﬁer in a known set) and a correct
Bayesian prior on the set of hypotheses. Recent work
by Dasgupta has identiﬁed suﬃcient and seminecessary conditions for active learning given only the
additional realizability assumption. also assume realizability.
there exists a perfect separator in our class, we can use
any informative querying strategy to direct the learning process without the need to worry about the distribution it induces—any inconsistent hypothesis can
be eliminated based on a single query, regardless of
which distribution this query comes from. In the agnostic case, however, a hypothesis that performs badly
on the query distribution may well be the optimal hypothesis with respect to the input distribution. This
is the main challenge in agnostic active learning that
Agnostic Active Learning
is not present in the non-agnostic case.
Burnashev and Zigangirov allow noise, but require a correct Bayesian prior on threshold functions.
Other assumptions include speciﬁc noise models. For
example, Castro et al.
 have analyzed active
learning in a setting with a constant noise rate everywhere.
The membership-query setting 
is similar to active learning considered here except
that no unlabeled data is given. Instead, the learning algorithm is allowed to query examples of its own
This is problematic in several applications
because natural oracles, such as hired humans, have
diﬃculty labeling synthetic examples . Ulam’s Problem ), where the goal is ﬁnd a distinguished element
in a set by asking subset membership queries, is also
related. The quantity of interest is the smallest number of such queries required to ﬁnd the element, given a
bound on the number of queries that can be answered
incorrectly. But both types of results do not apply here
since an active learning strategy can only buy labels of
the examples it observes. For example, a membership
query algorithm can be used to quickly ﬁnd a separating hyperplane in a high-dimensional space. An active
learning algorithm can not do so when the data distribution does not support queries close to the decision
When active learning can help
It is important to
keep in mind that the speedups achievable with active
learning depend on the match between the distribution
over example-label pairs and the hypothesis class, and
therefore on the target hypothesis in the class. There
are simple examples where active learning does not
help at all, even if there is no noise ). Obviously, all such lower bounds
apply in our setting as well.
It is also important to note that we cannot hope to
achieve speedups when the noise rate η is large, due
to a lower bound of Ω( η2
ϵ2 ) on the sample complexity
of any active learner .
Summary of results
In section 3, we present an
Agnostic Active learning algorithm, A2. The only assumption we rely upon is that samples are drawn i.i.d.
from some underlying distribution. In particular, we
make no assumptions about the mechanism producing
noise (e.g., class/target misﬁt, fundamental randomization, adversarial situations).
As far as we know,
this is the ﬁrst result of this form.
Section 3.1 proves that A2 is correct, and section 3.2
shows that A2 is never harmful, i.e., it never requires
signiﬁcantly more samples than batch learning. Section 4 shows the potential of A2 by establishing exponential speedups in several settings previously analyzed without noise. In particular, we show that we
can achieve exponential speedups for the simple case of
learning threshold functions; the result holds for arbitrary distributions provided that the noise rate is suf-
ﬁciently small with respect to the desired error ϵ. We
also show that our algorithm achieves an exponential
improvement if the hypothesis class consists of homogeneous (through the origin) linear separators and the
data is distributed uniformly over the unit sphere in
Rd. The last example has been the most encouraging
theoretical result so far in the realizable case . The A2 analysis also achieves an
almost contradictory property: for some sets of classi-
ﬁers, we can choose an ϵ-optimal classiﬁer with fewer
samples than are needed to estimate the error rate of
the chosen classiﬁer with precision ϵ.
2. Preliminaries
Let X be an instance space and Y = {−1, 1} be the set
of possible labels. Let H be the hypothesis class, a set
of functions mapping from X to Y . We assume there
is a distribution D over instances in X, and that the
instances are labeled by a possibly randomized oracle
O. The error rate of a hypothesis h with respect to
a distribution P over X × Y is deﬁned as errP (h) =
Prx,y∼P [h(x) ̸= y]. Let η = min
h∈H (errD,O(h)) denote
the minimum error rate of any hypothesis in H with
respect to the distribution (D, O) induced by D and
the labeling oracle O. The goal is to ﬁnd a hypothesis
h ∈H with errD,O(h) within ϵ of η, where ϵ is some
target error.
The algorithm A2 relies on a subroutine, which computes a lower bound LB(S, h, δ) and an upper bound
UB(S, h, δ) on the true error rate errP (h) of h by using
a sample S of examples drawn i.i.d. from P. Each of
these bounds must hold for all h simultaneously with
probability at least 1 −δ. The subroutine is formally
deﬁned below.
Deﬁnition 1 A subroutine for computing LB(S, h, δ)
and UB(S, h, δ) is said to be legal if for all distributions P over X × Y , and for all m ∈N,
LB(S, h, δ) ≤errP (h) ≤UB(S, h, δ)
holds for all h ∈H simultaneously, with probability
1 −δ over the draw of S according to P m.
Examples of such subroutines are the VC bound and the Occam Razor
bound .
3. The A2 Agnostic Active Learner
At a high level, A2 can be viewed as a robust version of the selective sampling algorithm of Cohen et al.
 . Selective sampling is a sequential process that
keeps track of two spaces—the current version space
Hi, deﬁned as the set of hypotheses in H consistent
with all labels revealed so far, and the current region
of uncertainty Ri, deﬁned as the set of all x ∈X, for
which there exists a pair of hypotheses in Hi that disagrees on x. In round i, the algorithm picks a random
unlabeled example from Ri and queries it, eliminating all hypotheses in Hi inconsistent with the received
The algorithm then eliminates those x ∈Ri
on which all surviving hypotheses agree, and recurses.
This process fundamentally relies on the assumption
that there exists a consistent hypothesis in H. In the
agnostic case, we cannot eliminate a hypothesis based
on its disagreement with a single example. We need
to be more conservative, or we risk eliminating best
hypotheses in the class.
A formal speciﬁcation of A2 is given in Algorithm 1.
Let Hi be the set of hypotheses still under consideration by A2 in round i.
If all hypotheses in Hi
agree on some region of the instance space, this region can be safely eliminated. To help us keep track of
progress in decreasing the region of uncertainty, deﬁne
DisagreeD(Hi) as the probability that there exists a
pair of hypotheses in Hi that disagrees on a random
example drawn from D:
DisagreeD(Hi) = Prx∼D[∃h1, h2 ∈G : h1(x) ̸= h2(x)].
Hence DisagreeD(Hi) is the volume of the current
region of uncertainty with respect to D.
Let Di be the distribution D restricted to the current
region of uncertainty. Formally, Di = D(x | ∃h1, h2 ∈
Hi : h1(x) ̸= h2(x)). In round i, A2 samples a set of
examples Si from Di, O, and uses it to compute upper and lower bounds for all hypotheses in Hi. It then
eliminates all hypotheses whose lower bound is greater
than the minimum upper bound. Figure 3.1 shows the
algorithm in action for the case when the data lie in
the interval on the real line, and H is the set
of thresholding functions. The horizontal axis denotes
both the instance space and the hypothesis space, superimposed. The vertical axis shows the error rates.
Round i completes when Si is large enough to eliminate at least half of the current region of uncertainty.
Since we eliminate only those examples on which the
surviving hypotheses agree, an optimal hypothesis in
Hi with respect to Di remains an optimal hypothesis
in Hi+1 with respect to Di+1. Since each round i cuts
DisagreeD(Hi) down by half, the number of rounds
is bounded by log 1
ϵ . Sections 4 gives examples of distributions and hypothesis classes for which A2 requires
only a small number of labeled examples to transition
between rounds, yielding an exponential improvement
in sample complexity.
When evaluating bounds during the course of Algorithm 1, we choose a schedule of δ according to the
following rule: we evaluate bound k with conﬁdence
k(k+1), for k ≥1.
Algorithm 1 A2 (allowed error rate ϵ, sampling oracle
for D, labeling oracle O, hypothesis class H)
Set i = 1, Di = D, Hi = H, Si = ∅, and k = 1.
DisagreeD(Hi)( min
h∈Hi UB(Si, h, δk)
h∈Hi LB(Si, h, δk)) > ϵ
1. set Si = ∅, H′
i = Hi, k ←k + 1.
2. while DisagreeD(H′
2DisagreeD(Hi)
DisagreeD(H′
UB(Si, h, δk) −
LB(Si, h, δk)) ≤ϵ
return h = argmin
UB(Si, h, δk).
i = Rejection sample 2|Si| + 1 samples
x from D satisfying:
∃h1, h2 ∈Hi : h1(x) ̸= h2(x)
ii. Si ←Si ∪{(x, O(x)) : x ∈S′
i}; k ←k +1;
LB(Si, h, δk, )
h′∈Hi UB(Si, h′, δk)}, k ←k + 1.
3. Hi+1 ←H′
Di+1 ←Di conditioned on the disagreement
∃h1, h2 ∈Hi : h1(x) ̸= h2(x);
Return h = argmin
h∈Hi UB(Si, h, δk).
Agnostic Active Learning
true error rates
upper bounds
lower bounds
min upper bound
Figure 3.1. A2 in action: Sampling, Bounding, Eliminating.
3.1. Correctness
Theorem 3.1 (Correctness)
(D, O), for all valid subroutines for computing UB and
LB, with probability 1−δ, A2 returns an ϵ-optimal hypothesis or does not terminate.
Note 1 For most “reasonable” subroutines for computing UB and LB, A2 terminates with probability at
least 1 −δ. For more discussion and a proof of this
fact see Section 3.2.
We ﬁrst prove that all bound evaluations are
valid simultaneously with probability at least 1−δ, and
then show that the procedure produces an ϵ-optimal
hypothesis upon termination.
To prove the ﬁrst claim, notice that the samples on
which each bound is evaluated are drawn i.i.d. from
some distribution over X × Y . This can be veriﬁed
by noting that the distribution Di used in round i
is precisely that given by drawing x from the underlying distribution D conditioned on the disagreement
∃h1, h2 ∈Hi : h1(x) ̸= h2(x), and then labeling according to the oracle O. The k-th bound evaluation
fails with probability at most
k(k+1). By the Union
bound, the probability that any bound fails is less then
the sum of the probabilities of individual bound failures. This sum is bounded by P∞
k(k+1) = δ.
To prove the second claim, notice ﬁrst that since every
bound evaluation is correct, we can be certain that step
2(c)(iii) never eliminates a hypothesis that has minimum error rate with respect D. Let us now introduce
the following notation. For a hypothesis h ∈H and
G ⊆H deﬁne:
eD,G,O(h) = Prx,y∼D,O|∃h1,h2∈G:h1(x)̸=h2(x)[h(x) ̸= y],
fD,G,O(h) = Prx,y∼D,O|∀h1,h2∈G:h1(x)=h2(x)[h(x) ̸= y].
Notice that eD,G,O(h) is in fact errDG,O(h), where DG
is D conditioned on the disagreement ∃h1, h2 ∈G :
h1(x) ̸= h2(x). Moreover, given any G ⊆H, we can
decompose the error rate of every hypothesis h into
two parts as follows:
errD,O(h) = eD,G,O(h) · DisagreeD(G) + fD,G,O(h) ·
(1−DisagreeD(G)) = errDG,O(h)·DisagreeD(G)+
fD,G,O(h) · (1 −DisagreeD(G)).
Notice that the only term that varies with h ∈G
in the above decomposition, is eD,G,O(h).
Consequently, if we want to ﬁnd an ϵ-optimal hypothesis
we need only bound the error rate of errDG,O(h) ·
DisagreeD(G) to precision ϵ.
But this is exactly
what the negation of the main while-loop guard does,
and this is also the condition we use in Step 2(a)
of the algorithm. In other words, upon termination
DisagreeD(Hi)(minh∈Hi UB(Si, h, δk) −
minh∈Hi LB(Si, h, δk)) ≤ϵ, which proves the desired
3.2. Fallback Analysis
This section shows that A2 is never much worse than
a standard batch, bound-based algorithm1 in terms
of the number of samples required in order to learn,
assuming that UB and LB are “sane”.
Deﬁne the sample complexity m(ε, δ, H) required by
a batch algorithm that uses a subroutine for computing LB(S, h, δ) and UB(S, h, δ) as the minimum number of samples m such that for all S ∈Xm, we have
|UB(S, h, δ) −LB(S, h, δ)| ≤ε for all h ∈H.
We use the following bound on m(ϵ, δ, H) stated as
Theorem A.1 in Appendix A:
m(ϵ, δ, H) = 64
Here V is the VC-dimension of H.
Assume that
m(2ϵ, δ, H) ≤m(ϵ,δ,H)
, and also that the function m
1A standard example of a bound-based learning algorithm is Empirical Risk Minimization (ERM).
Agnostic Active Learning
is monotonically increasing in 1/δ. These conditions
are satisﬁed by many subroutines for computing UB
and LB, including those based on the VC-bound and the Occam’s Razor
bound .
Theorem 3.2 For all H, for all (D, O), for all UB
and LB satisfying the assumption above, the algorithm
A2 makes at most 2m(ϵ, δ′, H) calls to the oracle O,
where δ′ =
N(ϵ,δ,H)(N(ϵ,δ,H)+1) and N(ϵ, δ, H) satisﬁes:
N(ϵ, δ, H) ≥ln 1
N(ϵ,δ,H)(N(ϵ,δ,H)+1), H). Here
m(ϵ, δ, H) is the sample complexity of UB and LB.
k(k+1) be the conﬁdence parameter used in the k-th application of the subroutine for
computing UB and LB. We will determine an upper
bound N(ϵ, δ, H) on the number of bound evaluations
throughout the life of the algorithm. This will imply
that the conﬁdence parameter δk will always be greater
N(ϵ,δ,H)(N(ϵ,δ,H)+1).
Consider i = 1.
If condition 2 of Algorithm A2 is
repeatedly satisﬁed then after labeling m(ϵ, δ′, H) examples from D1 we have that, uniformly, for all hypotheses h ∈H1, |UB(S1, h, δ′) −LB(S1, h, δ′)| ≤ϵ.
Note that in these conditions we safely halt. Notice
also that the number of bound evaluations during this
process is at most ln m(ϵ, δ′, H).
On the other hand, if loop (2) ever completes and i
increases, then it is enough to have uniformly for all
h ∈H2, |UB(S2, h, δ′) −LB(S2, h, δ′)| ≤
follows from the exit conditions we use in the outer
while-loop and in Step 2(a) of A2.) Clearly, in order
to uniformly have that for all hypotheses h ∈H2 their
true upper and lower bounds are within 2ϵ from each
other, we only need m(2ϵ, δ′, H) ≤
examples from D2 and the number of bounds evaluations in round i = 2 is at most ln m(ϵ, δ′, H).
In general, in round i it is enough to have uniformly for
all h ∈Hi, |UB(Si, h, δ′) −LB(Si, h, δ′)| ≤2i−1ϵ, and
in order to obtain this we only need m(2i−1ϵ, δ′, H) ≤
labeled examples from Di.
Also the number of bounds evaluations in round i is at most
ln m(ϵ, δ′, H).
Since the number of rounds is bounded by ln 1
follows that the maximum number of bound evaluation throughout the life of the algorithm is at most
ϵ ln m(ϵ, δ′, H). This implies that in order to determine an upper bound N(ϵ, δ, H) we just need to ﬁnd
a solution of the following inequality: N(ϵ, δ, H) ≥
N(ϵ,δ,H)(N(ϵ,δ,H)+1), H).
Finally, adding up the number of calls to the oracle
in all rounds, we get that the number of calls to the
oracle throughout the life of the algorithm is at most
2m(ϵ, δ′, H).
denote the VC-dimension of H,
m(ϵ, δ, H) be the number of examples required by the
ERM algorithm. As we state in Theorem A.1 in Appendix A a classic bound on m(ϵ, δ, H) is m(ϵ, δ, H) =
. Then using Theorem 3.2, we
can show the following corollary.
Corollary 3.3 For all hypothesis classes H of VCdimension V , for all distributions (D, O) over X × Y ,
the algorithm A2 requires at most ˜O
ϵ2 (V ln 1
labeled examples drawn i.i.d. from (D, O).2
We use the form of m(ϵ, δ, H) and Theorem 3.2 to upper bound N
N(ϵ, δ, H).
is enough to ﬁnd the smallest N satisfying N
. Using the inequality ln a ≤ab −ln b −1 for all a, b > 0 and some
simple algebraic manipulations, we get the desired upper bound on N(ϵ, δ, H). The result then follows from
Theorem 3.2.
4. Active Learning Speedups
In this section, we show that it is possible to achieve
exponential sample complexity improvements even
with arbitrary noise for some sets of classiﬁers.
4.1. Learning threshold functions
We begin by analyzing the simple class of threshold
functions. As mentioned in the introduction, it turns
out that even for this simple class of functions exponential reduction in sample complexity is not achievable when the noise rate η is large .
Therefore we prove two results: one shows an exponential sample complexity improvement when the noise
rate is small, while the other simply shows a slower improvement when the noise rate is large. In the extreme
where the noise rate is 1/2, there is no improvement.
Theorem 4.1 Let H be the set of thresholds on an interval with LB and UB the VC bound. For all distributions (D, O), for any ϵ < 1
16, the algorithm
A2 makes O
calls to the oracle
O on examples drawn i.i.d. from D, with probability
2Here and in the rest of the paper, the ˜O(·) notation
is used to hide factors logarithmic in the factors present
explicitly.
Agnostic Active Learning
Each execution of loop 2 decreases
DisagreeD(Hi) by at least a factor of 2, implying that
the number of executions is bounded by log 1
ϵ . Consequently, we are done if only O
labeled samples
are required per loop.3
Let h∗be any minimum error rate hypothesis.
h1, h2 ∈Hi, let di(h1, h2) be the probability that
predict diﬀerently on a random example drawn according to Di, i.e., di(h1, h2)
Prx∼Di(h1(x) ̸= h2(x)).
Consider i ≥1.
Let [loweri, upperi] be the support of Di.
Note that for any hypothesis h ∈Hi
we have errDi,O(h) ≥di(h, h∗) −errDi,O(h∗).
also clearly have errDi,O(h∗) ≤η/Zi, where Zi =
Prx∼D(x ∈[loweri, upperi]).
(So Zi is a shorthand
for DisagreeD(Hi).)
Now notice that at least a 1
2-fraction (measured with
respect to Di) of thresholds in Hi satisfy di(h, h∗) ≥1
and these thresholds are located at the “ends” of the
interval [loweri, upperi]. Formally, assume ﬁrst that
both di(h∗, loweri) ≥1
4 and di(h∗, upperi) ≥1
let li and ui be the hypotheses to the left and to the
right of h∗, respectively, that satisfy di(h∗, li) =
and di(h∗, ui) =
We clearly have that all h ∈
[loweri, li]∪[ui, upperi] satisfy di(h∗, h) ≥1
4 and moreover Prx∼Di(x ∈[loweri, li] ∪[ui, upperi]) ≥1
assume without loss of generality that di(h∗, loweri) ≤
4. Let ui be the hypothesis to the right of h∗with
di(h, upperi) =
Then we clearly have that all
h ∈[ui, upperi] satisfy di(h∗, h) ≥
4 and moreover
Prx∼Di(x ∈[ui, upperi]) ≥1
Using the VC bound, we get that with probability 1−δ′
then uniformly for all hypotheses h ∈Hi, we have
|UB(Si, h, δ) −LB(Si, h, δ)| ≤1
Consider a hypothesis h ∈Hi with di(h, h∗) ≥
For any such h we have errDi,O(h) ≥di(h, h∗) −
errDi,O(h∗) ≥1
Zi , and so LB(Si, h, δ) ≥1
8. On the other hand, errDi,O(h∗) ≤
and so UB(Si, h∗, δ) ≤
8. Thus A2 eliminates all h ∈Hi with di(h, h∗) ≥1
4. But that means
DisagreeD(H′
2DisagreeD(Hi), ter-
3Notice that the diﬀerence (minh∈Hi UB(Si, h, Di, O)−
minh∈Hi LB(Si, h, Di, O)) appearing in Step 2(a) is always
minating round i. 4
Finally notice that A2 makes O
to the oracle, where δ′ =
N2(ϵ,δ,H) and N(ϵ, δ, H)
is an upper bound on the number of bound evaluations throughout the life of the algorithm.
clearly have that the number of bound evaluations
required in round i is O
This implies
that the number of bound evaluations throughout
the life of the algorithm N(ϵ, δ, H) should satisfy
≤N(ϵ, δ, H), for some constant
c. Solving this inequality, we get the desired result.
Theorem 4.2 Let H be the set of thresholds on an
interval with LB and UB the VC bound. Suppose that
2 and η > ϵ. For all D, with probability 1 −δ, the
algorithm A2 will require at most ˜O
Proof Sketch:
The proof is similar to the previous
Theorem 4.1 implies that loop (2) will complete log 1
η −4 times.
At this point, the noise becomes suﬃcient so that the algorithm may only halt
via the return step in loop (2). In this case, we have
DisagreeD(H) = Θ(η) implying that the number of
samples required is ˜O
Note that Theorem 4.2 asymptotically matches a lower
bound of Kaariainen .
4.2. Linear Separators under the Uniform
Distribution
A commonly analyzed case for which active learning
is known to give exponential savings in the number of
labeled examples is when the data is drawn uniformly
from the unit sphere in Rd, and the labels are consistent with a linear separator going through the origin.
We show that A2 also gives exponential savings in this
case, in the presence of noise.
Let X = {x ∈Rd : ∥x∥= 1}, the unit sphere in
Assume that D is uniform over X, and let H
be the class of linear separators through the origin.
Any h ∈H is a homogeneous hyperplane represented
by a unit vector w ∈X with the classiﬁcation rule
h(x) = sign(w·x). The distance between two hypotheses u and v in H with respect to a distribution D (i.e.,
the probability that they predict diﬀerently on a random example drawn from D) is given by dD(u, v) =
4The assumption in the theorem statement can clearly
be weakened to η <
d for any constant ∆> 0.
Agnostic Active Learning
arccos(u·v)
. Finally, let θ(u, v) = arccos(u · v). Thus
dD(u, v) = θ(u,v)
Theorem 4.3 Let X, H, and D be as deﬁned above,
and let LB and UB be the VC bound. Then for any
2, 0 < η <
d, and δ > 0, with probability
1 −δ, A2 requires
d ln d + ln 1
calls to the labeling oracle, where δ′ =
N2(ϵ,δ,H) and
N(ϵ, δ, H) = O
d2 ln d + ln d ln 1
Let w∗∈H be a hypothesis with the
minimum error rate η.
Denote the region of uncertainty in round i by Ri.
Thus Prx∼D[x ∈Ri] =
DisagreeD(Hi).
Consider round i of A2. Initially i = 1, corresponding
to Hi = H, Di = D, and DisagreeD(Hi) = 1.
Theorem A.1 says that it suﬃces to query the oracle
on a set S of O(d2 ln d + d ln 1
δ′ ) examples from Di to
guarantee, with probability 1 −δ′, that for all w ∈Hi,
|errDi,O(w) −c
errDi,O(w)| < 1
where ri is a shorthand for DisagreeD(Hi). (By assumption, η <
d. We also have DisagreeD(Hi) ≥
ϵ. Thus the precision above is at least
implies that UB(S, w, δ′)−errDi,O(w) <
errDi,O(w) −LB(S, w, δ′) <
ri . Consider any
w ∈Hi with dDi(w, w∗) ≥
d. For any such w, we
have errDi,O(w) ≥
ri , and so
LB(S, w, δ′) >
But we also know that errDi,O(w∗) =
ri , and thus
UB(S, w∗, δ′) < η
d, so A2 will eliminate w in step 2(c)(iii).
Thus round i eliminates all hypotheses w ∈Hi with
dDi(w, w∗) ≥
d. Since all hypotheses in Hi agree
on every x ̸∈Ri, we have
dDi(w, w∗) = 1
dD(w, w∗) = θ(w, w∗)
Thus round i eliminates all hypotheses w ∈Hi with
θ(w, w∗) ≥πri
d. But since 2θ/π ≤sin θ, for θ ∈(0, π
it certainly eliminates all w with sin θ(w, w∗) ≥
5The assumption in the theorem statement can clearly
be weakened to η <
d for any constant ∆> 0.
Consider any x ∈Ri+1 and the value |w∗· x| =
cos θ(w∗, x). There must exist a hypothesis w ∈Hi+1
that disagrees with w∗on x; otherwise x would not
be in Ri+1.
But then we must have cos θ(w∗, x) ≤
2 −θ(w, w∗)) = sin θ(w, w∗) <
d, where the
last inequality is due to the fact that A2 eliminated all
w with sin θ(w, w∗) ≥
d. Thus any x ∈Ri+1 must
satisfy |w∗· x| <
Using the fact that Pr(A | B) = Pr(AB)
any A and B, we can write
Prx∼Di [x ∈Ri+1] ≤Prx∼Di
Prx∼D[x ∈Ri]
where the second inequality follows from Lemma A.2.
Thus DisagreeD(Hi+1) ≤1
2DisagreeD(Hi), as desired.
Figure 4.1. The region of uncertainty after the ﬁrst iteration (schematic).
To ﬁnish the argument, it suﬃces to notice that since
every round cuts DisagreeD(Hi) at least in half, the
total number of rounds is bounded by log 1
Notice also that A2 makes O
 d2 ln d + d ln 1
to the oracle, where δ′ =
N 2(ϵ,δ,H) and N(ϵ, δ, H)
is an upper bound on the number of bound evaluations throughout the life of the algorithm.
clearly have that the number of bound evaluations
required in round i is O
 d2 ln d + d ln 1
. This implies that the number of bound evaluations throughout the life of the algorithm N(ϵ, δ, H) should satisfy c
d2 ln d + d ln
≤N(ϵ, δ, H)
for some constant c. Solving this inequality, we get
the desired result.
Agnostic Active Learning
comparison,
complexity
Perceptron-based active learner of Dasgupta et al.
 , is O(d ln 1
δ +ln ln 1
ϵ )), for the same H, X,
and D, but only for the realizable case when η = 0.
5. Discussion and Open Questions
The results here should be regarded as a ﬁrst-case
proof-of-possibility.
A2 suggests a number of interesting open questions:
1. On what other (hypothesis spaces, distribution)
pairs can we observe exponential speedups?
there an algorithm that is more sample eﬃcient
than A2? Does A2 always achieve speedups comparable to the ones achieved by the selective sampling algorithm , but in the
presence of (limited) noise?
2. Are there concept classes for which A2 (or some
other algorithm) can be made computationally ef-
Checking for disagreement amongst all
remaining classiﬁers can be very computationally
intensive.
3. What conditions are suﬃcient and necessary for
active learning to succeed in the agnostic case?