One-Shot Learning of Object Categories
Li Fei-Fei, Member, IEEE, Rob Fergus, Student Member, IEEE, and Pietro Perona, Member, IEEE
Abstract—Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that
it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than
learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these
categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models.
Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object
category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on
a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to
models learned from by Maximum Likelihood (ML) and Maximum A Posteriori (MAP) methods. We find that on a database of more
than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other
methods to operate successfully.
Index Terms—Recognition, object categories, learning, few images, unsupervised, variational inference, priors.
INTRODUCTION
ECOGNITION is one of the most useful functions of our
visual system. We recognize materials (marble, orange
peel), surface properties (rough, cold), objects (my car, a
willow tree), and scenes (a thicket of trees, my kitchen) at a
glance and without touching them. We recognize both
individuals (my mother, my office), as well as categories . By the time we are six years old,
we recognize more than 104 categories of objects , and
keep learning more throughout our life. As we learn, we
organize both objects and categories into useful and
informative taxonomies and relate them to language.
Replicating these abilities in the machines that surround us
would profoundly affect the practical aspects of our lives,
mostly for the better. Certainly, this is the most exciting and
difficult puzzle that faces computational vision scientists and
engineers in this decade.
A rich palette of diverse ideas has been proposed during
the past few years, especially on the problem of recognizing
objects and object categories (see our brief review of the
literature below). There is broad consensus on the fact that
models need to capture the great diversity of forms and
appearances of the objects that surround us. This means
models containing hundreds, sometimes thousands, of
parameters. It is common knowledge in statistics that
estimating a given number of parameters requires a
many-fold larger number of training examples—as a
consequence, learning one object category requires a batch
process involving thousands or tens of thousands of
training examples , , , .
Unfortunately, it is often difficult and expensive to
acquire large sets of training examples. Compounding this
problem, most algorithms for learning categories require
that each training exemplar be aligned (typically by hand)
with a prototype. This becomes particularly problematic
when fiducial points are not readily identifiable (can we
find a natural alignment for images of octopuses, of
cappuccino machines, of bonsai trees?). This is a large
practical obstacle on the way to learning thousands of object
categories. It would be far better if we managed to find
ways to train new categories with few examples.
Is there any hope? We believe so. A young child learns
many categories per day . It seems unlikely that this
would require a large set of training images for each
category as well as much supervision.
We hypothesize that, once a few categories have been
learned the hard way, some information may be abstracted
from that process to make learning further categories more
efficient. In other words, we should be able to make use of the
knowledge that has been gained so far rather than starting
from scratch each time we learn a new category. We pursue
herethis hypothesis in a Bayesian setting: We extract “general
knowledge” from previously learned categories and represent it in the form of a prior probability density function in the
space of model parameters. Given a training set, no matter
how small, we update this knowledge and produce a
posterior density, which is then used for detection/recognition.Ourexperimentsshowthatthisisaproductiveapproach
and that, indeed, some useful information about categories
may be obtained from a few, even one, training example.
We beginwith abriefreviewof theliterature inSection 2.A
detailed review of the mathematical framework of our
recognition system follows in Section 3. Section 4 briefly
introduces our methods for learning the model. Detailed
derivations are given in . We then proceed to test our ideas
experimentally. In Section 5, we give implementational
details for each stage of the system, from feature detection
(Section 5.1) to the experimental setup for learning and
recognition (Section 6.2). In Section 6.3, we demonstrate the
algorithm with a walkthough for the motorbike category. We
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
APRIL 2006
. L. Fei-Fei is with the University of Illinois Urbana-Champaign, 405 N.
Mathews Ave., MC 251, Urbana, IL 61801. E-mail: .
. R. Fergus is with the University of Oxford, Parks Road, Oxford, OX1 3PJ,
UK. E-mail: .
. P. Perona is with the California Institute of Technology, Mail Code 136-93,
Pasadena, CA 91125. E-mail: .
Manuscript received 30 Aug. 2004; revised 12 July 2005; accepted 12 July
2005; published online 14 Feb. 2006.
Recommended for acceptance by R. Basri.
For information on obtaining reprints of this article, please send e-mail to:
 , and reference IEEECS Log Number TPAMI-0460-0804.
0162-8828/06/$20.00  2006 IEEE
Published by the IEEE Computer Society
then contrast our Bayesian learning algorithm with the
traditional ML approach in Section 6.4. In Section 6.5, we
show a variety of experiments run on the 101 object
categories, including: a comparison with MAP algorithm,
deliberate degradation of the system, and a confusion table
for all 101 categories. Section 7 concludes the paper. For
convenience, we will denote our algorithm as the “Bayesian
One-Shot” algorithm throughout the text.
LITERATURE REVIEW
Researchers in this area face three main challenges. Representation: How should we model objects and categories?
Learning: How may we acquire such models? Detection/
recognition: Given a new image, how do we detect the
presence of a known object/category amongst clutter, and
despite occlusion,viewpoint,andlightingchanges?Thegreat
richness and diversity of methods and ideas in the literature
indicatesthattheseissuesarefarfrombeingsettled.However,
there is broad consensus on a few significant points. First of
all,theshapeandappearanceoftheobjectsthatsurroundusis
complex and diverse; therefore, models should be rich (lots of
parameters, heterogeneous descriptors). Second, the appearanceofobjectswithinagivencategorymaybehighlyvariable,
therefore, models should be flexible enough to handle this.
Third, in order to handle intraclass variability and occlusion,
models should be composed of features, or parts, which are
not required to be detected in all instances; the mutual
position of these parts constitutes further model information.
Fourth, it is difficult, if not impossible, to model class
variability using principled a priori techniques; it is best to
learn the models from training examples. Fifth, computational efficiency must be kept in mind.
Work on recognition may be divided into two groups:
recognition of individual objects , , , and
recognition of categories , , , , , , , ,
 , , . Individual objects are easier to handle,
therefore, more progress has been made on efficient recognition , lighting-invariant , , and viewpoint-invariant , representations and recognition. Categories are
more general, requiring more complex representations and
are more difficult to learn; most work has therefore focused
on modeling and learning. Viewpoint and lighting have not
been treated explicitly (exceptions include , ), but
rather treated as an additional source of in-class variability.
We are interested in the problem of learning and
recognition of categories (as opposed to individual objects).
While the literature proposed learning methods that require
batch processing of thousands of training examples, the
present work focuses on the previously unexplored problem
of efficient learning: How could we estimate models of
categories from very few, one in the limit, training examples?
Most researchers have focused on special-interest categories:
human faces , , pedestrians , handwritten digits
 , and automobiles , . Instead, we wish to develop
techniques that apply equally well to any category that a
human would readily recognize. With this objective in mind,
we carried out our experiments on a large number of
categories.
Another aspect that we wish to emphasize is the ability to
learn with minimal supervision. We prefer to develop
methods that do not rely on hand-alignment of the training
examples, for the reasons mentioned in the introduction. For
this reason, we use statistical models and probabilistic
detection techniques developed by , , , , which
will bereviewed in Section 3.2. A comprehensive treatment of
these models may be found in Weber’s and Fergus’ 
PhD theses.
THEORETICAL APPROACH
Overall Bayesian Framework
Let’s say that we are looking for a flamingo bird in a query
image that is presented to us. To decide whether there is a
flamingo bird or not, we compare the probability of a
flamingo being present in the image with the probability of
only background clutter being present in the image. The
decision is simple: If the probability of a flamingo being
presentis higher, wedecide this image contains an instance of
a flamingo. If it is the other way around, we decide there is no
flamingo. To compute the probability of a flamingo being
present in an image, we need a model of a flamingo, which we
learn from a set of training images containing examples of
flamingos. Then, we could compare this probability with the
background model and, in turn, make our final decision.
We can now translate the above events into a probabilistic
framework. Let I be the query image, which may contain an
example of the foreground category Ofg. The alternative is
that it contains background clutter belonging to a generic
background category Obg. I t is the set of training images that
we have used as the foreground category. Now, the decision
of whether this query image I has the foreground object or
not can be written in the following way:
R ¼ pðOfgjI; I tÞ
pðObgjI; I tÞ ¼ pðIjIt; OfgÞ pðOfgÞ
pðIjI t; ObgÞ pðObgÞ :
If R, the ratio of the class posteriors, is greater than some
threshold, T,then wedecidethe image contains aninstance of
the object. If it is less than T, then the image does not contain
the object. In (1), we use Bayes Rule for the expansion, giving
us a ratio of likelihoods and a ratio of priors on the object
categories. We can now further expand (1) by introducing a
parametric model for the foreground and background
category, whose parameters are  and bg, respectively:
R pðIj; OfgÞpðjI t; OfgÞ d
pðIjbg; ObgÞpðbgjI t; ObgÞ dbg
pðIjÞpðjI t; OfgÞ d
R pðIjbgÞpðbgjI t; ObgÞ dbg
The ratio of priors, pðOfgÞ
pðObgÞ , is a constant, thus it is omitted in (2)
since it maybe be incorporated into the decision threshold. In
addition, we have simplified pðIj; OfgÞ and pðItjbg; ObgÞ
into pðIjÞ and pðItjbgÞ, respectively. The learning procedure involves estimating pðjIt; OfgÞ, the distribution of
model parameters given the training images. Once this is
known, we can evaluate R by integrating out over . We now
look at the particular object model used.
The Object Category Model
Our chosen representation is a Constellation model , ,
 . Given a query image, I, we find a set of N interesting
regions in the image. From these N regions, we obtain two
variables: X—the locations of the regions and A—the
appearances of the regions. Section 5.1 gives details of how
FEI-FEI ET AL.: ONE-SHOT LEARNING OF OBJECT CATEGORIES
X and A are obtained. It is X and A that we now model, I no
longer being used directly. Similarly, in the case of the
training images I t, we obtain Xt and At. Thus, (2) becomes:
pðX; Aj; OfgÞpðjXt; At; OfgÞ d
pðX; Ajbg; ObgÞpðbgjXt; At; ObgÞ dbg
R pðX; AjÞpðjXt; At; OfgÞ d
pðX; AjbgÞpðbgjX t; At; ObgÞ dbg
We now examine likelihoods pðX; AjÞ and pðX; AjbgÞ,
where, in the general case, we have a mixture of constellation models, with  components:
pðX; Aj Þ ¼
pðX; A; h; wj Þ
p Ajh; A
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
Appearance
p Xjh; X
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
pðhjwÞ;
where  ¼ f; A; Xg and pðhjwÞ is a constant. The shape, X,
and appearance, A, are assumed to be independent.
Typically, a constellation model would have P (3  7)
diagnostic features, or parts. But, there are N (up to 100)
interest points, or candidate features in the image. We
therefore introduce an indexing variable h, which we call a
hypothesis. h is a vector of length P, where each entry is
between 1 and N, which allocates a particular feature to a
model part. Any unallocated features are assumed to belong
to the background of the image. The set of all hypotheses H
consists of all valid allocations of features to the parts;
consequently, jHj, the total number of hypotheses is OðNPÞ.
For simplicity, we assume the background model is fixed and
has a single parameter value, bg, thus the integral in the
denominator of (3) collapses to pðX; AjbgÞ. If we believe no
object to be present (the Obg case), then only one hypothesis
exists, h0, the null hypothesis, where all detections are
assigned to the background. Hence, the denominator
pðX; AjbgÞ ¼ pðX; A; h0jbgÞ
¼ p Ajh0; A
p Xjh0; X
p h0jbg
Since this expression is constant for given X and A, we can
use it to cancel terms in the numerator of (3).
The model encompasses the important properties of an
object: shape and appearance, both in a probabilistic way.
This allows the model to represent both geometrically
constrained objects (where the shape density would have a
small covariance, e.g., a face) and objects with distinctive
appearance but lacking geometric form (the appearance
densities would be tight, but the shape density would now be
looser, e.g., an animal principally defined by its texture such
as a zebra). The following assumptions are made in the
model: Shape is independent of appearance; for shape, the
joint covariance of the parts’ position is modeled, while for
appearance, each part is modeled independently. In the
experiments reported here, we use a slightly simplified
version of the model presented in by removing the terms
involving occlusion and statistics of the feature finder since
these are relatively unimportant when we only have a few
images to train from.
3.2.1 Appearance
Each feature’s appearance is represented as a point in some
appearance space, defined in Section 5.1. For now, we could
think of each feature being represented by a vector whose
values are related to the gray-value pixel intensities of the
small neighborhood of the feature. For a given mixture
component, each part p has a Gaussian density within this
space, with mean and precision parameters A
p;w ¼ fA
p;wg, which is independent of other parts’ densities. The
background model has the same form, with fixed parameters
bg ¼ fA
bgg. Note that A
p;! and A
bg are diagonal matrices.
Eachfeatureselectedbythehypothesisisevaluatedunderthe
appropriate part density with features not selected being
evaluated under the background model:
p Ajh; A
G AðhhhpÞjA
j¼1; j nhhh
G AðjÞjA
where G is the Gaussian distribution and j represents
features not assigned to a part in hypothesis h. In addition,
we adopt the notation hp in (6) to indicate the feature
belonging to the pth part of h. If no object is present, then all
features are modeled by the background:
pðAjh0; A
G AðjÞjA
Note that pðAjh0; A
bgÞ is a constant for a given image;
therefore, it can be brought inside the integral and
summation over all hypotheses in (3) and (4). This cancels
with all other background hypotheses except the true
foreground hypothesis h in (6):
p Ajh; A
p Ajh0; A
G AðhhhpÞjA
G AðhhhpÞjA
3.2.2 Shape
The shape of each constellation model component may be
represented by a joint Gaussian density of the locations of
features for a hypothesis, after they have been transformed
into a scale and translation-invariant space. Translation
invariance is achieved by using the left-most feature in h as
a landmark and translating all feature locations relative to it.
Scale invariance is obtained by taking the scale of the
landmark feature and using it to normalize the relative
locations of the other features . We assume a uniform
density 1 for the position of the object, where  is the image
area. The relative location of the parts is modeled by a
2ðP  1Þ-dimensional Gaussian, with a uniform background
model for unallocated features:
p Xjh; X
¼ 1 G XðhÞjX
where X
w ¼ f; X
wg. For the null hypothesis, pðXjh0; X
¼ N, which is a constant, so we cancel with all other
background hypotheses except the true foreground hypothesis h in (9):
p Xjh; X
p Xjh0; X
 ¼ P1 G XðhÞjX
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
APRIL 2006
Additionally, to reduce the number of hypotheses that
must be considered in each frame, we impose an ordering
constraint on each hypothesis’s shape, such that the
x-coordinate of each part must be monotonically increasing. This reduces the number of hypotheses that must be
considered by P! and provides a useful constraint in the
learning process.
Discussion of the Model
We make some comments concerning the model:
XðhÞ 2 IR2P2 and AðhÞ 2 IRkP. Thus, for k ¼ 10
(dimension of the appearance descriptor), P ¼ 4
(number of parts), the shape term has 6 þ 21 ¼ 27
(mean + full covariance matrix) parameters. The
appearance term has 40 þ 40 ¼ 80 (mean + diagonal
covariance matrix) parameters, thus the model has
27 þ 80 ¼ 107 parameters in total.
The total number of hyperparameters for k ¼ 10,
P ¼ 4 is 109 since m
m and BBB have the same
dimensionality as , . Additionally,  and a (both
real numbers) exist for both shape and appearance
terms: 107 þ 2 ¼ 109.
The constellation model is a generative model of the
output of an interest region detector, not the image
pixels. Hence, the performance of the model is
dependent on the performance of the detectors
themselves. See Section 6.5.3 for an investigation
into this dependency.
In our representation, there is nothing to prevent
patches from overlapping, which could lead to
overcounting of the evidence for the model. However, given a relatively low number of features per
image, this should not be a major problem.
The shape model presented above uses a joint
density over all parts, thus, the data association
problem has complexity OðNPÞ. While this is the
most thorough approach to modeling the location of
parts, it presents a major computational bottleneck.
Imposing conditional independence by the use of a
tree-structured model would reduce the complexity
to OðN2PÞ in learning and OðNPÞ in recognition
 , . However, in doing so, other issues arise,
such as how the optimal graph structure should be
chosen. Since these issues are in themselves complex
and are outside the focus of this paper, for the sake of
simplicity, we stick with the complete representation, despite its drawbacks.
Our model and representation of shape is suited to
compact objects which do not have large amounts of
articulation (e.g., human bodies). For such categories,
different graph structures and coordinate frames (i.e.,
the angles between parts) may be more appropriate.
Our feature representation is currently confined to
textured image patches. Alternative representations
such as curve contours, which model the outline of the
object, could also be used with little modification to
the underlying model , . This would allow the
model to handle categories where the outline of the
object is more important than its interior (e.g., bottles).
Currently, the background model is very simple: A
uniform shape distribution and a single Gaussian
distribution for appearance. Their crude nature is a
consequence of the requirement, for efficiency, that
the denominator in (3) must be able to cancel with
the numerator, making evaluation of the likelihood
ratio simple. The parametric assumptions of the
background model were tested by examining the
distribution of thousands of detections from an
assorted collection of images. Our observation was
that these assumptions were reasonably accurate.
The framework describes object detection (i.e., object
present or absent), however, it can easily be extended
to localization by using the best hypothesis in each
image (e.g., by taking a bounding box around it).
Multiple instances per image can also be found by a
greedy approach: finding the best hypothesis; summing over all hypotheses around its neighborhood to
give a value of R for a subwindow of the image;
removing all features within the subwindow and
repeating until no subwindows with R greater than a
given threshold can be found.
10. Our model is formulated as a mixture of Gaussians
(4). In practice, we use a single mixture component
in this paper for all of the experiments. Weber et al.
have demonstrated that, by increasing the number of
mixture components, the model is capable of
representing different aspects of the object due to
pose variations .
Form of the Parameter Posterior
In computing R, we must evaluate the integral
pðX; AjÞ
pðjXt; At; OÞ d. In Section 3.2, the form of pðX; AjÞ was
considered. We now look at the posterior of , pðjXt; At; OÞ.
Before we consider how this density might be estimated, its
form must be decided upon. Since the integral above is
typically impossible to solve analytically, we look at various
forms of pðjX t; At; OÞ that approximate the true density
while making the integral tractable.
3.4.1 Maximum Likelihood (ML) and Maximum
A Posteriori (MAP)
If we assume that the model distribution pðjXt; At; OÞ is
highly peaked, we could approximate it with a  function at
: ð  Þ. This allows the integral in (3) to collapse to
pðX; AjÞ, whose functional form is given by (4).
There are two ways of obtaining . The simplest one is
Maximum Likelihood (ML) estimation , . Here,  ¼
ML is computed by picking the  that gives rise to the
highest likelihood value of the training data:
 ¼ ML ¼ argmax
pðXt; AtjÞ:
If we had some prior knowledge about , we could also use
this information to help estimate . The idea is to weigh the
likelihood of training examples at  by the prior probability
of  at that point. This is called the Maximum A Posteriori
(MAP) estimation.
 ¼ MAP ¼ argmax
pðXt; AtjÞpðÞ:
TheformofpðÞneedstobechosencarefullytoensurethatthe
estimation procedure is efficient. In , we shall give a more
detailed account of pðÞ and methods for estimating MAP.
Both ML and MAP assume a well peaked pðjX t; At; OÞ so
that ð  Þ is a suitable estimate of the entire distribution.
FEI-FEI ET AL.: ONE-SHOT LEARNING OF OBJECT CATEGORIES
But, when there is a limited number of training examples, the
distribution may not be well peaked, in which case both ML
and MAP are likely to yield poor models.
3.4.2 Other Inference Methods
Sampling methods. At the other extreme, we can use
numerical methods such as Gibbs Sampling or Markov-
Chain Monte-Carlo (MCMC) to give an accurate estimate
of the integral in (3), but these can be computationally very
expensive. In the constellation model, the dimensionality of 
is large ( 100) for a reasonable number of parts, making
MCMC methods impractical for our problem. Additionally,
the use of sampling-based methods is something of an art:
Issues such as what sampling regime to use have no simple
answer. Hence, they are less attractive as compared with
methods giving a distinct solution.
Recursive Approximations. A variety of variational
approximations exist that are recursive or incremental in
nature , . In such schemes, the data points are
processed sequentially with the (approximate) marginal
posterior pðjXt; At; OÞ being updated after each new data
point. We explore one of such methods in .
3.4.3 Conjugate Densities
The final approach is to assume that pðjXt; At; OfgÞ has a
specific parametric form, such that the integral in (3) has a
closed-form solution. Recalling the numerator of (3):
pðX; AjÞpðjXt; At; OfgÞ d:
Our goal is to find a parametric form of pðÞ such that the
learning of pðjXt; At; OfgÞ is feasible and the evaluation of
(13) is tractable. This could be achieved by taking advantage
of a class of prior distributions that are conjugate to their
posterior distributions. In other words, a conjugate prior for a
given probabilistic model is one for which the resulting
posterior has the same functional form as the prior . In the
caseofpðjX t; At; OfgÞ,weuseaNormal-Wishartdistribution
as its conjugate prior. Given that pðX; AjÞ was chosen to be a
product of Gaussians(in Section 3.2),theentire integral of (13)
becomes a multivariate Student’s T distribution. Efficient
learning schemes exist for estimating the hyper-parameters
of the Normal-Wishart distribution , having the same
computational complexity as standard ML methods. These
are introduced in Section 4.
Recognition Using a Conjugate Density
Parameter Posterior
Having specified a functional form for the parameter posterior, we now give the actual equations for use in recognition.
3.5.1 Parameter Distribution
Recall the mixture of constellation models from (4):
pðX; AjÞ ¼
p XðhÞjX
p AðhÞjA
Each component ! has a mixing coefficient !, a mean of
shapeandappearanceX
! ,andaprecisionmatrixofshape
and appearance X
! . Collecting all mixture components
and their corresponding parameters together, we obtain an
overall parameter vector  ¼ f; X; A; X; Ag. Assuming
we have now learned the model distribution pðjXt; AtÞ from
a set of training data Xt and At, we define the model
distribution in the following way:
pðjXt; AtÞ ¼ pðÞ
where the mixing component is a symmetric Dirichlet:
pðÞ ¼ Dirð!IIIÞ, the distribution over the shape precisions
is a Wishart: pðX
! Þ ¼ WðX
! Þ, and the distribution
over the shape mean conditioned on the precision matrix is
Normal: pðX
! Þ ¼ GðX
! Þ. Together, the shape
distribution pðX
! Þ is a Normal-Wishart density , .
Note that f!; a!; BBB!; m
m!; !g are hyper-parameters for
defining distributions of model parameters. Identical expressions apply to the appearance component in (15). We will
show an empirical way of obtaining these hyper-parameters
in Section 6.3.
3.5.2 Closed-Form Calculation of R
Recall that:
R ¼ pðX; AjX t; At; OfgÞ
pðX; AjX t; At; ObgÞ
pðX; AjÞpðjX t; At; OfgÞ d
R pðX; AjbgÞpðbgjX t; At; ObgÞ dbg
Due to the use of conjugate densities, the integral in the
numerator becomes a multimodal multivariate Student’s T
distribution (denoted by S):
pðX; AjXt; At; OfgÞ ¼
~! S X hj gX
where g! ¼ a! þ 1  d and ! ¼ ! þ 1
BBB! and ~! ¼
Note that d is the dimensionality of the parameter vector .
Thedenominatorof (16)isaconstant,sinceweonlyconsidera
single value of bg: ML
bg , i.e., pðbgjXt; At; ObgÞ ¼ ðbg  ML
LEARNING USING A CONJUGATE DENSITY
PARAMETER POSTERIOR
The process of learning an object category is weakly
supervised , . The algorithm is presented with a
number of training images labeled as “foreground images.” It
assumes that there is an instance of the object category to be
learned in each image. But, no other information, e.g.,
location, size, shape, appearance, etc., is provided apart from
minimal preprocessing (see Section 6.1 for details). The
algorithm first detects interesting features in these training
images and then estimates the parameters of the model
densities from these regions. Since the model is linear and
Gaussian with conjugate priors, it should have a closed-form
solution. However, the discrete indexing variable h, representing the assignment of features to parts, prevents such a
solution. Instead, an iterative variational method that
resembles the Expectation-Maximization (EM) algorithm 
is used to estimate the variational posterior. Afterward,
recognition is performed on a query image by repeating the
process of detecting regions and then evaluating the regions
usingthemodelparametersestimatedinthelearningprocess.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
APRIL 2006
The goal of learning is to obtain a posterior distribution
pðjX t; At; OfgÞ of the model parameters given a set of
training data fXt; Atg as well as some prior information. We
formulate this learning problem using Variational Bayesian
Expectation Maximization (VBEM), applied to a multidimensional Gaussian mixture model as introduced by
Attias . Detailed derivations of VBEM are given in .
In addition, we also give a detailed derivation of the MAP
parameter estimation in .
IMPLEMENTATION
Feature Detection and Representation
We use the same features as in . They are found using the
detector of Kadir and Brady . This method finds regions
that are salient over both location and scale. Gray-scale
images are used as the input. The most salient regions are
clustered over location and scale to give a reasonable number
of features per image, each with an associated scale. The
coordinates of the center of each feature give us X. This
particular feature detector was chosen as it tends to give a
small number of informative features per image as compared
to other detectors, such as multiscale Harris which give
hundreds or thousands of less distinctive features. Fig. 1
illustrates thison imagesfromfourdatasets.Oncetheregions
are identified, they are cropped from the image and rescaled
to the size of a small (11  11) pixel patch. Each patch exists in
a 121-dimensional space. We then reduce this dimensionality
by using principle component analysis (PCA). A fixed PCA
basis,precalculatedfromthebackgrounddatasets,isusedfor
this task. We then collect the coefficients of the first
10 principal components from each patch to form A.
We now discuss the practical aspects of the Bayesian One-
Shot learning procedure—the choice of the prior density,
pðÞ, and details of the Bayesian One-Shot implementation.
5.2.1 Choice of Prior
One critical issue is the choice of priors for the Dirichlet and
Norm-Wishart distributions. In this paper, learning is
performed using a single mixture component, i.e.,  ¼ 1.
So,  is set to 1, since ! will always be 1. Ideally, the values
for the shape and appearance priors should reflect object
models in the real world. In other words, if we have already
learned a sufficient number of categories of objects (e.g.,
hundreds or thousands), we would have a pretty good idea
of the average shape/appearance mean and variances given
a new object category. In reality, we do not have the luxury
of such a number of object categories. We use three
categories of object models learned in a ML manner from
 to form our priors. They are: spotted cats, faces, and
airplanes. The hyper-parameters of the prior are then
estimated from the parameters of the existing category
models. An example of this process is given in Section 6.3.
5.2.2 Details of the Bayesian One-Shot Algorithm
Initial conditions are chosen in the following way:
Shape and appearance means are set to the means of
the training data itself. Covariances are chosen
randomly within a sensible range. Namely, they
are initialized to be roughly in the order of the
average dimensions of the training images.
Learning is halted when the largest parameter
change per iteration (across all parameters) falls
below a certain threshold (104) or the maximum
number of iterations is exceeded (typically 500). In
general, convergence occurs within 100 iterations.
Since the model is a generative one, the background
images are not used in learning except for one
instance: The appearance model has a distribution in
appearance space modeling background features.
Estimating this from foreground data proven inaccurate, so the parameters are estimated from a set
of background images and not updated within the
Bayesian One-Shot iteration.
Learning a category takes roughly less than a minute
on a 2.8 GHz machine when the number of training
images is less than 10 and the model is composed of
four parts. The algorithm is implemented in Matlab. It
is also worth mentioning that the current algorithm
does not utilize any efficient search methods, unlike
 . It has been shown that increasing the number of
parts in a constellation model results in greater
FEI-FEI ET AL.: ONE-SHOT LEARNING OF OBJECT CATEGORIES
Fig. 1. Output of the feature detector on sample images from four categories. (a) Elephant. (b) Grand piano. (c) Hawksbill. (d) Bonsai tree.
recognition power provided enough training examples are given . Were efficient search techniques
used, 6-7 parts could be learned, since the Bayesian
One-Shot update equations require the same amount
of computation as the traditional ML ones. However,
all our experiments currently use four part models for
both the current algorithm and ML.
EXPERIMENTAL RESULTS
In the first set of experiments, the same four object
categories as in , were used,1 namely, human faces,
motorbikes, airplanes, and spotted cats. These data sets
contain a fair amount of background clutter and scale
variation, although each category is presented from a
consistent viewpoint.
In addition, two naive subjects collected another data set
of 97 object categories for the second set of experiments. The
97 categories were combined with the motorbikes, airplanes, faces, and spotted cats to give a data set of 101 object
categories. The names of the 97 new categories were
generated by flipping through the pages of the Webster
Collegiate Dictionary , picking a subset of categories that
were associated with a drawing. Using a script, all images
returned by the Google Image Search engine for each
category name were downloaded. The two subjects then
sorted through the images for each category, getting rid of
irrelevant images (e.g., a zebra-patterned shirt for the
“zebra” category). Fig. 2 shows examples from 101 foreground object categories as well as the background clutter
category (obtained by typing “things” into Google).
Minimal preprocessing was performed on the categories.
Categories such as motorbike, airplane, cannon, etc., where
two mirror image views were present, were manually
flipped, so all instances faced in the same direction.
Additionally, categories with a predominantly vertical
structure were rotated to an arbitrary angle. This is due to
the convention that the left-most part of each hypothesis is
used as a reference point to translate the rest of the parts
(see Section 3.2.2). With vertically orientated structures, the
horizontal ordering of the features will be somewhat
arbitrary, so artificially giving a large vertical variability.
Experimental Setup
Each experiment is carried out as follows: Each data set is
randomly split into two disjoint sets of equal size. N training
imagesaredrawnrandomlyfromthefirst.Afixedsetof50are
selected from the second, forming the test set. We then learn
models using Variational Bayesian, ML, and MAP approaches and evaluate their performance on the test set. For
evaluation purposes, we also use 50 images from a background data set. For each category, we vary N from 1 to 6,
repeating the experiments 10 times for each value (using a
different set of N training images each time) to obtain a more
robust estimate of performance. When N ¼ 1, ML, and MAP
failtoconverge,soweonlyshowresults fortheBayesian One-
Shot algorithm in this case.
When evaluating the models, the task is a binary
decision—object present or absent. All performance values
are quoted as equal error rates from the receiver-operating
characteristic curve (ROC) (i.e., p (True positive) ¼ 1  p
(False alarm)). The ROC curve is obtained by testing the
model on 50 foreground test images and 50 background
images. For example, a value of 85 percent means that 85
percent of the foreground images are correctly classified but
15 percent of the background images are incorrectly classified
(i.e., false alarms).
In all the experiments, the following parameters are used:
number of parts in model = 4, number of PCA dimensions for
each part appearance = 10, and average number of detections
of interest point for each image = 20. It is also important to
point out that all parameters remain the same for learning all
different categories. In other words, exactly the same piece of
software was used in all experiments.
Walkthrough for the Motorbike Category
We now go through the experimental procedure step-by-step
for the motorbike category. Six training images are selected
(examples of which are shown in Fig. 3a). The Kadir and
Brady interest operator is applied to them, giving Xt. Each of
these regions is then transformed into the fixed PCA basis, to
Next, we consider the prior we will use in learning. This
has been constructed from models trained using ML from
the three other data sets: spotted cats, faces, and airplanes.
Ten ML models were trained for each category, giving a
total of 30 models, each being a point in -space. The
parameters of the prior, fm
m0; 0; a0; BBB0g for both the shape
and appearance components of the model are then directly
computed from these points in the following manner:
m0 is estimated by computing the mean of ML over
the M ¼ 30 ML models: m
a0 is fixed to be number of degress of freedom in
the precision matrix ML, which differs between
and appearance
0 ¼ 2ðP  1ÞðP  2Þ, while aA
BBB0 is estimated by letting a0BBB1
0 , the mean of the
precision, be
m ML and using the previously
calculated value of a0 to give BBB0.
0 is estimated as the ratio between the precision
the mean and
the mean of the
precision:
Fig. 4 illustrates both the ML models (as points colored by
category) and the prior density fitted to them. Since the
parameter space is high-dimensional, it is difficult to
visualize. But, by considering each appearance descriptor
separately, the mean and variance of the part from each
model can be plotted in 2D. Note that all parts use the same
prior density for appearance. For shape, the mean and
variance of location of each part relative to the landmark part
is shown. To understand how the prior assists in learning,
models were trained on background data alone and their
parameters also plotted in Fig. 4 (as magenta ’s). The prior
density was estimated only from the ML category models,not
these background models. However, they serve to illustrate
the point that models lacking visual consistency occupy a
different part of the parameter space to coherent models. The
prior captures this knowledge, then, in thelearning process,it
biases pðjXt; At; OfgÞ to areas of -space corresponding to
visually consistent models.
Now that the prior and training data, X t and At, have
been obtained, we commence the learning process described in Section 4. We only use one mixture component,
so  ¼ 1. The initial values of the hyper-parameters
f!; a!; BBB!; m
m!; !g are initialized as in Fig. 3b. Note that,
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
APRIL 2006
1. Available from www.vision.caltech.edu.
since we only have one component, we do not need to
worry about setting .
The initial posterior densities are illustrated in green in
Fig. 5. Then, we run the Bayesian One-Shot algorithm until
convergence is reached. Fig. 5 shows the learned parameter
densities in red. They can be seen to be much tighter then
the initial density, often lying close to the prior density (in
black), which is likely to exert a large influence with so few
training images. The model corresponding to the mean of
the parameter density is shown in Fig. 6.
In the recognition phase, the learned model is applied to
50 images containing motorbikes and 50 images of scenes
not containing motorbikes. Fig. 6 shows the ROC curve for
the model, along with sample images when the threshold,
T, is set so as to give equal numbers of false alarms and
missed detections.
FEI-FEI ET AL.: ONE-SHOT LEARNING OF OBJECT CATEGORIES
Fig. 2. The 101 object categories and the background clutter category. Each category contains between 45 and 400 images. Two randomly chosen
samples are shown for each category. The categories were selected prior to the experiments and the images collected by operators not associated
with the experiment. The last row shows examples from the background data set. This data set is obtained by collecting images through the Google
image search engine (www.google.com). The keyword “things” is used to obtain the background data set. Note that only gray-scale information is
used in our system. Complete data sets can be found at 
Caltech 4 Data Set
We first tested our algorithm on the four object categories
used by Weber et al. and Fergus et al. . They are
faces, motorbikes, airplanes, and spotted cats. Our experiments demonstrate the benefit of using prior information as
well as using a full Bayesian algorithm in learning new
object categories (Figs. 7 and 8). In Figs. 7 and 8, given zero
training images, the detection rate for each category is at
chance level 50 percent. This tells us that, given only the
prior model, it is not sufficient to capture characteristic
information of the particular categories we are interested in.
Only by incorporating this prior knowledge into the
training data is the algorithm capable of learning a sensible
model with only one training example. For instance, in
Fig. 7c, we see that the 4-part model has captured the
essence of a face (e.g., eyes and nose). In this case, it
achieves an average detection rate of 82 percent, given only
one training example.
Caltech 101 Data Set
We have tested our algorithm on a large data set of 101 object
categories (Fig. 2). We summarize different aspects of our
experiments in the following sections.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
APRIL 2006
Fig. 5. The learningprocess.(a) Appearance parameter space,showingthe meanand variancedistributionsfor eachof the models’ four parts forthe first
four descriptors. The parameter densities are colored as follows: black for the prior, green for the initial posterior density, and red for the density after
30 iterations of Bayesian One-Shot, when convergence is reached. (b) X component of the shape term for each of the model parts. (c) Y component of
shape. Note that, in both (b) and (c), only the variance terms along the diagonal are visualized—not the covariance terms. This figure is best viewed in
color with magnification.
Fig. 4. A visualization of the prior parameter density, estimated from ML models of spotted cats (green s), faces (red þs), and airplanes (blue s).
Models trained on background data are shown as magenta s, but are not used in estimating the prior density. In all figures, the mean is plotted on
the x-axis and the variance on the y-axis. (a) Appearance parameter space for the first four descriptors. (b) X component of the shape term for each
of the nonlandmark model parts. (c) Y component of shape. This figure is best viewed in color with magnification.
Fig. 3. (a) Sample training images for the motorbike category, with the output of the feature detector overlaid. (b) Initial values of the hyperparameters of the parameter posterior for shape and appearance.
6.5.1 Overall Results: ML versus MAP versus Bayesian
Using the Bayesian formulation, we are able to incorporate
prior knowledge of the object world into the learning scheme.
In addition, we are also capable of averaging over the
uncertainties of models by integrating over the model
distributions. Do both of these two factors contribute to the
efficient learning of our algorithm? Or is it only the prior that
truly matters?
We are able to answer this question by comparing the
detection result of the Bayesian One-Shot algorithm not only
to the ML method, but also to the MAP algorithm (as derived
in ). Both the Bayesian One-Shot and the MAP algorithms
are given exactly the same prior distributions for learning for
each of the 101 categories. While Fig. 9 illustrates that prior
knowledge helps in learning new object categories, the
introduction of priors alone cannot account for all the
advantages of our Bayesian formulation. The Bayesian
algorithm consistently performed better than both the ML
and MAP methods given a few training examples. While
MAP learning takes advantage of the prior density, it is
fundamentally the same as maximum likelihood in that a
single parameter set is estimated for the object category.
Given few training examples, such an assumption is likely to
overfit the data points. The Bayesian algorithm reduces the
overfit by averaging over model uncertainties.
6.5.2 Good Models and Bad Models
Figs. 10 and 11 show in detail the results from the grandpiano and cougar-face categories, both of which have
achieved reasonable performances given few training
examples (equal error rates of 84 percent and 85 percent,
respectively, for 15 training examples). In the left-most
columns, four examples of feature detection results are
presented. The center of each detection circle indicates the
location of the feature detected while the size of the circle
indicates its scale. The second column shows the resulting
shape model for the Bayesian One-Shot method for
f1; 3; 6; 15g training images. As the number of training
examples increases, we observe that the shape model is
more defined and structured with a reduction in variance.
This is expected since the algorithm should be more and
more confident of what is to be learned. The third column
shows examples of the part appearance that are closest to
the mean distribution of the appearance. Notice that
distinctive features such as keyboards for the piano and
eyes or whiskers for the cougar-face are successfully
learned by the algorithm. Two learning methods’ performances are compared in the top panel of the last column.
The Bayesian methods clearly show a big advantage over
the ML method when the training number is small.
It is also useful to look at the other end of the performance
spectrum—those categories that have low recognition performance. We give some informal observations into the cause
of the poor performance. Feature detection is a crucial step for
both learning and recognition. On both the crocodile and
mayfly figures in Fig. 12, notice that some testing images
marked “INCORRECT” have few detection points on the
target object itself. When feature detection fails either in
learning or recognition, it affects the performance results
greatly. Furthermore, Fig. 10a shows that a variety of viewpoints are present in each category. In this set of experiments,
we have only used one mixture component, hence, only a
single viewpoint can be accommodated. Our model is also a
FEI-FEI ET AL.: ONE-SHOT LEARNING OF OBJECT CATEGORIES
Fig. 6. The mean posterior model. (a) The shape component of the model. The four þs and ellipses indicate the mean and variance in position of
each part. The interpart covariance terms are not shown. (b) The mean appearance distributions for the first three PCA dimensions. Each color
indicates one of the four parts. The background density is shown in black. (c) The detected feature patches in the training image closest to the mean
of the appearance densities for each of the four parts. (d) Some examples of foreground test images for the model, with a mix of correct and incorrect
classifications. The pink dots are features found on each image and the colored circles indicate the best hypothesis in the image. The size of the
circles indicates the score of the hypothesis (the bigger the better). (e) The model running on some background query images. (f) The ROC curve for
the model on the test set. The equal error rate is around 18 percent.
simplified version Burl et al.’s constellation model , ,
 as it ignores the possibility of occluded parts.
6.5.3 A Further Investigation on Prior Models and
Feature Detectors
One usefulquestion to ask is whether learning is improved by
constructing the prior model from more categories. To
investigate this, we randomly select 20 object categories that
will incrementally contribute to the prior model. We learn a
model for each of the 20 categories, forming a set of models C.
We also randomly select 30 object categories from the rest of
the data set, calling this set S. We train a model for each
category in S using a prior constructed from N models drawn
fromC.WevaryN from0to20.ForN ¼ 0,thepriormodelisa
broad, noninformative distribution over the shape and
appearance space. For N > 0, we pick a model from C and
update the prior as a weighted average between the old prior
modelandthenewcategorymodel,theweightingbeingN  1
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
APRIL 2006
Fig. 7. Summary of face model. (a) Test performances of the algorithm given 0  6 number of training image(s) (red line). Zero number of training
images is when only the prior model is used. Note the prior alone is not sufficient for categorization. Each data point is obtained by 10 repeated runs
with different randomly drawn training and testing images. Error bars show one standard deviation from the mean performance. This result is
compared with the maximum-likelihood (ML) method (green). Note that ML cannot learn the degenerate case of a single training image. (b) Sample
ROC curves for the Bayesian One-Shot algorithm (red) compared with the ML algorithm (green line). The curves shown here use typical models
drawn from the repeated runs summarized in (a). (c), (d), (e), and (f) show typical models learned with one and five training images. (c) Shape model,
appearance samples, and appearance densities (of the first three descriptors) for a model trained on one image. (e) Sample foreground test images
for the model shown in (c). (d) and (f) correspond to a model trained on five images. Note that the size of the open circles on (e) and (f) indicates the
strength of the hypothesis (scaled according to the log likelihood score), not the variance of the part locations. In other words, the bigger the circles,
the stronger the algorithm believes in the recognition decision. Only the mean locations of the parts are shown here in pink dots. (c) Model from one
training example. (d) Model from five training example. (e) Testing examples from model in (c). (f) Testing examples from model in (d).
and 1, respectively. Fig. 13a shows the relationship between
the number of categories contributing to the prior model and
the performances averaged over all categories in S. We see a
trendofdecreasingerrorwhenthenumberofcategoriesinthe
prior model is between one and eight, although this trend
becomes less clear beyond eight.
We also explored the effect of feature detections on the
overall object detection performances. Two human subjects
annotated the whole data set, giving ground truth information of the location and the contours of the objects within each
image. Given this information, we are able to compute the
proportion of features detected within the object boundary as
a fraction of the total number in the image. In Fig. 13b, we
show the relationship between the quality of the feature
detections and the performances for each training number. In
general, a very weak positive correlation is observed between
FEI-FEI ET AL.: ONE-SHOT LEARNING OF OBJECT CATEGORIES
Fig. 8.Summaryof the threeothercategoriesfromthe Caltech4 datasets: motorbike, spottedcat,and airplaneforone trainingexample.Notethat, in (b),
the sample patches are of relatively low resolution. This is due to the lower resolution of the original images of the spotted cat category. (a) Summary of
motorbike model. (b) Summary of spotted cat model. (c) Summary of airplane model. (d) Testing examples from models in (a), (b), and (c). respectively.
feature detection quality and performance. This correlation
seems to increase slightly as the training number increases.
6.5.4 Bayesian One-Shot Algorithm: Shape-Only versus
App-Only versus Shape-App models
In Section 3.2, we detailed the formulation of object category
models. Each model of an object category carries two sources
of information: shape and appearance. We show in Fig. 14
that the contributions of shape and appearance components
of the model vary when the object category to be learned
differs. While some categories depend more on the shape
component (e.g., faces, electrical guitars, side view of cars,
etc.), others rely more on the appearance (leopards, octopi,
ketch, etc.). Overall, a full model has a significant advantage
over a shape-only or appearance-only model in terms of
categorization performances.
6.5.5 Bayesian One-Shot Algorithm: Discrimination
among 101 Categories
Sofar,wehavetestedouralgorithminadetectionscenario:For
aparticularobjectcategory,weareonlydecidingifitispresent
or not. We now test the algorithm in a discrimination scenario:
one where we have multiple categories (i.e., more than two)
andmustcorrectly classify thequery imagesfromeach.In our
experiment, we first learn a model for each of the 101 object
categories. Query images are then drawn from the test set of
each category in turn and evaluated by all 101 models. For a
given image, the assignment of the category it belongs to is in
the “winner-take-all” fashion. In other words, the category
modelthatachievedthehighestlikelihoodscoreisassignedto
the image. For each category of images, we repeat the
experiment 50 times with different randomly chosen training
and test images. This gives a vector of 101 entries, each being
the average of the “winner-take-all” assignment over the
50 repetitions. We do this for each of the 101 categories, so
obtaining the confusion table in Fig. 15. By averaging the
correctdiscriminationrates,i.e.,theentriesalongthediagonal
of Fig. 15a, we obtain the average correct discrimination rates
for 3, 6, and 15 training examples of, respectively, 10.4, 13.9,
and 17.7 percent. These rates would be approximately
1 percent if the classifiers were making random decisions.
6.5.6 Discussions
Our results highlight a number of issues that we continue to
investigate.Themostimportantoneisthechoiceofpriors.We
have used a very general prior constructed from three
categories and would like to further explore the effects of
different priors. Notice that, in Fig. 9, the Maximum
Likelihood method, on average, gives a similar level of
performance to the Bayesian One-Shot algorithm for 15 training images. This is surprising, given the large number of
parameters in each model and, therefore, a few hundred
training examples are, in principle, required by a maximum
likelihood method—one might have expected that the ML
method to converge with the Bayesian One-Shot method at
onlyaround 100 training examples.The most likelyreason for
this result is that the prior that we employ is very simple.
Similarly, this overly simple prior (along with other weaknesses of the model) might also be responsible for a lack of
moredramaticimprovementoftheperformancesobservedin
Figs. 7 and 8. Bayesian methods live and die by the quality of
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
APRIL 2006
Fig. 9. Performance on 101 categories using three different learning methods: Maximum Likelihood (ML), Maximum A Posteriori (MAP), and the
Bayesian One-Shot algorithm. (a), (b), (c), and (d) show the performance given training number(s) 1, 3, 6, and 15 and compare them with
performance of the prior alone. “Percent correct” is measured as 1  Eq: Error Rate. (e) summarizes the four panels above, showing the mean
performance (Eq. Error Rate). The error bars indicate one standard deviation.
FEI-FEI ET AL.: ONE-SHOT LEARNING OF OBJECT CATEGORIES
Fig. 11. Results for the “cougar face” category.
Fig. 10. Results for the “grand-piano” category. Column 1 shows examples of feature detection. Column 2 shows the shape models learned from
f1; 3; 6; 15g training images. Column 3 shows the appearance patches for the model learned from f1; 3; 6; 15g training images. The top panel of Column 4
shows the comparative results between ML and Bayesian methods (the error bars show the variation over the 10 runs). The bottom panel of Column 4
shows the recognition result for the Bayesian One-Shot algorithm for one training image. Pink dots indicate the center of detected interest points.
the prior that is used. Our prior density is derived from only
three object categories. Given the variability of our training
set, it is realistic that a prior based on many more categories
would yield a better performance. We have tested this
hypothesis using a simple, synthetic example in Figs. 15b
and 15c. Our goal is to learn a simple triangular shape model
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
APRIL 2006
Fig. 12. Two categories with poor performance. (a) Crocodile (equal error rate = 35 percent for one training example). (b) Mayfly (equal error rate =
42 percent for one training example).
Fig. 13. (a) Effect of the number of object categories in the prior model on the performance of testing categories. There are 20 randomly drawn object
categories for training the prior model. There are 30 other randomly drawn object categories in the testing category set. The x-axis indicates the
number of object categories in the prior model. The y-axis indicates the average performance error of the 30 test categories given the prior model.
(b) Quality of feature detection compared with object detection performances of the 101 categories given f1; 3; 6; 15g training images. The x-axis of
each plot is the detection performance of the model. The y-axis is the quality of feature detection, defined by the percentage of detection points
landing within the outline of the object over the total number of detections. For each category, we average the percentage over all images within it.
Fig. 14. Shape only models and appearance only models compared with models using shape and appearance for each of the 101 categories given
f1; 3; 6; 15g training images ((a), (b), (c), and (d)). The x-axis of each plot is the detection performance of models using both shape and appearance.
The y-axis is the detection performance of shape-only models and appearance-only models for each category. (a) TrainNum = 1. (b) TrainNum = 3.
(c) TrainNum = 6. (d) TrainNum = 15.
(Fig. 15b). We test the effect of priors on the Bayesian One-
Shot algorithm by giving the system three different priors: a
triangular shape prior (similar to the synthetic model in
Fig. 15b used to generate the data. Note that a fourth part of
the triangle model is located off the vertex), a trapezium
shape prior, and a square shape prior. The Bayesian One-Shot
algorithm with three different priors is compared to the
maximum likelihood method. We observe that it takes more
than 100 training examples for the ML method to “catch up”
with the Bayesian One-Shot learning method given the
triangular shape prior. On the contrary, it takes much smaller
number of training examples for the ML method to converge
with the other two Bayesian One-Shot learning method with
noneffective priors.
CONCLUSIONS AND FUTURE WORK
We have demonstrated that, contrary to intuition, useful
aspects of a new object category may be learned from a
single training example (or just a few). As Table 1 shows,
this is beyond the capability of existing algorithms.
The key insight we have exploited is that categories we
have already learned give us information that helps us to
learn new categories with fewer training examples. To
pursue this idea, we developed a Bayesian learning framework based on representing object categories with probabilistic models. Prior information from previously learned
categories is represented with a suitable prior probability
density function on the parameters of their models. These
prior models are updated with the few training examples
available to produce posteriors which, in turn, may be used
for both detection and discrimination.
Our experiments, conducted on images from 101 categories, are encouraging in that they show that very few (1 to 5)
training examples produce models that are already able to
achieve a detection performance of around 70-95 percent.
Furthermore, that the categories from which the prior
knowledge is learned do not need to be visually similar to
the categories that one wishes to learn.
FEI-FEI ET AL.: ONE-SHOT LEARNING OF OBJECT CATEGORIES
Fig. 15. (a) A confusion table for six training examples. The x-axis enumerates the category models, one for each category, giving 101 in total. The
y-axis is the ground truth category for the query image. The intensity of an entry in the table corresponds to the probability of a given query image
being classified as a given category. Since the categories are consistently ordered on both axes, the ideal case would consist of a completely black
diagonal line, showing perfect discrimination power of all category models over all categories of objects. (b) The synthetic triangle model used in (c).
Note the triangle is characterized by a 4-part model. (c) Effect of different priors for learning a triangle object category. Note that the point of
convergence between the ML method and the Bayesian One-Shot method depends on the choice of prior distribution. When a prior is very effective
(e.g., a triangular prior for learning a triangular model), it takes more than 100 training examples to converge. But, when the prior is not very effective
(e.g., square or trapezium priors for learning a triangular model), it takes less than 30 training examples for the two methods to converge.
A Comparison between a Variety of Object Recognition Approaches
The framework column specifies if the approach is generative (Gen.) or discriminative (Disc.) or both. The hand alignment and segmented columns
indicate if the training data needs to be hand-aligned or hand-segmented for a given approach.
While our experiments are very encouraging, they are by
no means satisfactory from a practical standpoint. Much
can be done toward the goal of obtaining better error rates,
as our current implementation is, at the moment, just a toy.
In order to curtail the complexity of our experiments, we
have simplified the probabilistic models that are used for
representing objects. For example, a probabilistic model for
occlusion ( , , ) was not implemented, and we only
used four parts in our models, definitely not enough to
represent the full complexity of object appearance. Furthermore, we only used three known categories to derive a
prior. This is clearly a very small set which ought to be
substantially broadened in a real-world situation.
However, at this point, it is probably more important to
make progress at the conceptual level and much still needs
to be done. For example, would a more sophisticated,
multimodal prior be beneficial in learning? Is it easier to
learn new categories which are similar to some of the
“prior” categories? How should one best represent prior
knowledge? Is there any other productive point of view,
besides the Bayesian one which we have adopted here, that
allows one to incorporate prior knowledge? In addition, it
would be highly valuable to learn incrementally where
each training example will update the probability density
function defined on the parameters of each object category;
we presented a few ideas toward this in .
One last note of optimism: We feel that the problem of
recognizing automatically hundreds, perhaps thousands, of
object categories does not belong to a hopelessly far future.
We hope that the positive outcome of our experiments on
the large majority of 101 very diverse and challenging
categories, despite the simplicity of our implementation and
the rudimentary prior we employ, will encourage other
vision researchers to test their algorithms on larger and
more diverse data sets .
ACKNOWLEDGMENTS
The authors would like to thank Andrew Zisserman, David
Mackay, Brian Ripley, and Joel Lindop. This work was
supported by the Caltech CNSE, the UK EPSRC, and EC
Project CogViSys.