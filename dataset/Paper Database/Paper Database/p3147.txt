Proceedings of the 6th Workshop on Representation Learning for NLP , pages 29–33
Bangkok, Thailand (Online), August 6, 2021. ©2021 Association for Computational Linguistics
Larger-Scale Transformers for Multilingual Masked Language Modeling
Naman Goyal
Jingfei Du
Giri Anantharaman
Alexis Conneau
{naman, jingfeidu, myleott, giriman, aconneau}@fb.com
Facebook AI
Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding.
study, we present the results of two larger multilingual masked language models, with 3.5B
and 10.7B parameters.
Our two new models dubbed XLM-RXL and XLM-RXXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms
the RoBERTa-Large model on several English
tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This
suggests larger capacity models for language
understanding may obtain strong performance
on both high- and low-resource languages. We
make our code and models publicly available.1
Introduction
The goal of this paper is to present a study of
the impact of larger capacity models on crosslingual language understanding (XLU). We scale
the capacity of XLM-R by almost two orders
of magnitude while training on the same CC100
dataset . Our two new multilingual masked language model dubbed XLM-RXL
and XLM-RXXL, with 3.5 and 10.7 billion parameters respectively, signiﬁcantly outperform the previous XLM-R model on cross-lingual understanding
benchmarks and obtain competitive performance
with the multilingual T5 models . We show that they can even outperform RoBERTa-Large on the
GLUE benchmark .
Recent multilingual masked language models
(MLM) like mBERT or
XLM improved crosslingual language understanding by pretraining large
Transformer models on mul-
1 
tiple languages at once. The XLM-R model extended that approach by scaling the amount of data by two orders of magnitude, from Wikipedia to Common-Crawl and training longer, similar to RoBERTa .
These models are particularly effective for lowresource languages, where both labeled and unlabeled data is scarce. They enable supervised
cross-lingual transfer, where labeled data in one
language can be used to solve the same task in other
languages, and unsupervised cross-lingual transfer,
where low-resource language self-supervised representations are improved using additional unlabeled
data from higher-resource languages. Furthermore,
they reduce the need for training one model per
language, and allows the use of a single - potentially much larger - pretrained model that is then
ﬁne-tuned on annotated data from many languages.
The better performance of self-supervised crosslingual models on low-resource languages comes
however at the cost of lower performance on higherresource languages .
When the number of languages becomes large,
Conneau et al. even observed an overall
decrease of performance on all languages. It was
hypothesized that when multilingual models get
more capacity, they may showcase strong performance on both high-resource languages and lowresource languages. With only 550M parameters,
the XLM-R model is now relatively small compared to new standards. Recent work scaled language models to hundreds of billions or even multiple trillion parameters , showing consistent gains in doing
so. Recently, multilingual T5 showed impressive
increase in performance by scaling the model capacity to tens of billions of parameters. Our study
complements these ﬁndings by showing the impact
of larger capacity models on the important pretraining task of multilingual masked language model-
ing. We show promising results for cross-lingual
understanding: XLM-RXXL can both obtain a new
state of the art on some cross-lingual understanding
benchmarks and outperform the RoBERTa-Large
model on the English GLUE benchmark . This suggests that very large-scale
multilingual models may be able to beneﬁt from the
best of both worlds: obtaining strong performance
on high-resource languages while still allowing for
zero-shot transfer and low-resource language understanding. We make the following contributions:
• We scale XLM capacity by two orders of magnitude, and publicly release XLM-RXL and
XLM-RXXL with 3.5B and 10.7B parameters.
• We show that those two models obtain very
strong performance on cross-lingual benchmarks while outperforming RoBERTaLarge on
the GLUE benchmark.
Pretraining and evaluation
In this section, we describe the model we use and
how we scale it, as well as the data and tasks we
use for pretraining and evaluation.
Multilingual masked language models
We use a Transformer model 
trained with the multilingual MLM objective using
only monolingual data. We sample streams of text
from each language and train the model to predict
the masked tokens in the input. We use the same
learning procedure as XLM-R. We apply subword
tokenization directly on raw text data using Sentence Piece with a
unigram language model just like
in XLM-R. We sample batches from different languages using the same sampling distribution as
Conneau et al. , with α = 0.3, and without
language embeddings. We use a large vocabulary
size of 250K with a full softmax and train two different models: XLM-RXL (L = 36, H = 2560, A
= 32, 3.5B params) and XLM-RXXL (L = 48, H
= 4096, A = 32, 10.7B params). We pretrain the
models on the CC100 dataset, which corresponds
to 167B tokens in 100 languages. We compare our
approach to previous results as well as the mT5
baselines, which were pretrained on the larger mC4
corpus of 6.4T tokens.
Evaluation
We consider three evaluation benchmarks. For
cross-lingual understanding, we use cross-lingual
natural language inference and question answering, and use the GLUE benchmark to evaluate the
English performance.
Cross-lingual
Inference.
The XNLI dataset comes
with ground-truth dev and test sets in 15 languages,
and a ground-truth English training set. The training set has been machine-translated to the remaining 14 languages, providing synthetic training data
for these languages as well. We evaluate our model
on cross-lingual transfer from English to other languages. We also consider two machine translation
baselines: (i) translate-test: dev and test sets are
machine-translated to English and a single English
model is used (ii) translate-train-all: the English
training set is machine-translated to each language
and we ﬁne-tune a multilingual model on all training sets. For translations, we use the original XNLI
data for consistency.
Cross-lingual Question Answering.
We use the
MLQA and XQuad benchmark from Lewis et al.
 and Artetxe et al. , which extends
the English SQuAD benchmark to more languages.
We report the F1 score as well as the exact match
(EM) score for cross-lingual transfer from English.
The English GLUE Benchmark.
Finally, we
evaluate the English performance of our model
on the GLUE benchmark 
which gathers multiple classiﬁcation tasks, such
as MNLI , SST-2 , or QNLI .
Training details
We use model parallelism based on tensor parallel for scaling models. XLM-
RXL uses model parallel size of 2 and XLM-RXXL
used 8. Compared to previous XLM-R models, we
reduce the batch size and number of updates signiﬁcantly to keep the compute of the new models
similar (see Table 5). For both models, we use
batch size of 2048 and train for 500,000 updates.
We use pre-LayerNorm setting for both the models
which was more stable during training.
For all the tasks in ﬁnetuning, we use batch size
of 32 and train for 10 epochs. We do early stopping based on the average valid metrics across all
languages and report test results.
Data (#tok)
Fine-tune multilingual model on English training set (Cross-lingual Transfer)
XLM-RLarge
Translate everything to English and use English-only model (TRANSLATE-TEST)
Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)
XLM-RLarge
Table 1: Results on cross-lingual classiﬁcation (XNLI). We report the accuracy on each of the 15 XNLI languages
and average accuracy, and specify the dataset and its corresponding size in number of tokens. We report results of
XLM-R models with increasing capacity, from 270M (Base), 550M (Large), 3.5B (XL) to 10.7B (XXL) parameters.
Analysis and Results
In this section, we present our results and compare
XLM-RXL and XLM-RXXL performance to other
methods from previous work.
Cross-lingual
understanding
XNLI, we observe in Table 1 that scaling the
capacity from XLM-RLarge to XLM-RXL leads
to an average accuracy improvement of 1.4 on
zero-shot cross-lingual transfer and 1.8 on multilingual ﬁne-tuning. When scaling even further
to XLM-RXXL, we observe a total improvement
of 2.2 on zero-shot and 2.4 on translate-train-all
compared to XLM-RXL, with a new state of
the art on French, Vietnamese and Hindi.
MLQA, in Table 4, we observe even larger
gains for cross-lingual zero-shot transfer, where
scaling from XLM-RLarge to XLM-RXXL leads
to improvements of 4.1 F1 and 3.9 EM scores
on average.
Similarly, on XQuad we observe
improvements of 4.4 F1 and 5.5 scores, with new
state-of-the-art results on Arabic, German, Greek
and Russian (see Table 3).
Comparison to monolingual English model.
For smaller-capacity models like the Base and
Large version of XLM-R, it was shown that the
more languages are considered the lower the performance , in particular on highresource languages.
For instance, XLM-RLarge
was outperformed by RoBERTaLarge by 1% accuracy on average on several downstream tasks
from the GLUE benchmark, as illustrated in Table2. With larger capacity, we now observe that
XLM-RXXL is able to outperform RoBERTaLarge
by 0.3 dev points, going from 92.9 to 93.2 average accuracy, while handling 99 more languages.
While a RoBERTaXXL model may outperform
XLM-RXXL, we believe it interesting to notice that
with more capacity, a multilingual model can get
strong high-resource performance while not losing
its cross-lingual transfer ability for lower-resource
languages. Given the compute needed for training
such large-scale models, the possibility of training
a single very large model on hundreds of languages
with state-of-the-art performance on high-resource
languages is an encouraging and positive result.
XLM-RLarge
Table 2: GLUE dev results
Cross-lingual zero-shot transfer (models ﬁne-tune on English data only)
88.4 / 77.3
75.2 / 56.7
80.0 / 62.9
77.5 / 57.6
81.8 / 64.2
73.4 / 56.6
74.7 / 56.9
73.4 / 62.0
76.5 / 56.3
79.4 / 60.3
75.9 / 65.5
77.8 / 61.5
88.8 / 78.1
77.4 / 60.8
80.4 / 63.5
80.4 / 61.2
82.7 / 64.5
76.1 / 60.3
76.2 / 58.8
74.2 / 62.5
77.7 / 58.4
80.5 / 60.8
80.5 / 71.0
79.5 / 63.6
90.9 / 80.1
80.3 / 62.6
83.1 / 65.5
83.3 / 65.5
85.1 / 68.1
81.7 / 65.9
79.3 / 63.6
77.8 / 66.1
80.2 / 60.9
83.1 / 63.6
83.1 / 73.4
82.5 / 66.8
XLM-RLarge
86.5 / 75.7
68.6 / 49.0
80.4 / 63.4
79.8 / 61.7
82.0 / 63.9
76.7 / 59.7
80.1 / 64.3
74.2 / 62.8
75.9 / 59.3
79.1 / 59.0
59.3 / 50.0
76.6 / 60.8
89.5 / 79.0
78.4 / 61.6
81.3 / 64.1
82.3 / 63.9
84.6 / 66.2
78.8 / 63.2
81.5 / 65.0
76.0 / 65.5
73.9 / 57.9
81.7 / 61.8
72.3 / 66.1
80.0 / 64.9
89.3 / 79.4
80.1 / 63.7
82.7 / 65.8
83.4 / 65.5
83.8 / 66.0
80.7 / 65.4
82.4 / 65.4
76.6 / 65.6
76.8 / 61.7
82.2 / 63.0
74.1 / 67.4
81.1 / 66.3
Table 3: XQuad results (F1/EM) for each language.
Cross-lingual zero-shot transfer (models ﬁne-tune on English data only)
84.9 / 70.7
65.3 / 44.6
68.9 / 51.8
73.5 / 54.1
66.9 / 47.7
72.5 / 50.7
66.2 / 42.0
71.2 / 51.7
85.5 / 71.9
68.0 / 47.4
70.5 / 54.4
75.2 / 56.3
70.5 / 51.0
74.2 / 52.8
70.5 / 47.2
73.5 / 54.4
86.7 / 73.5
70.7 / 50.4
74.0 / 57.8
76.8 / 58.4
75.6 / 57.3
76.4 / 56.0
71.8 / 48.8
76.0 / 57.4
XLM-RLarge
80.6 / 67.8
74.1 / 56.0
68.5 / 53.6
63.1 / 43.5
69.2 / 51.6
71.3 / 50.9
68.0 / 45.4
70.7 / 52.7
85.1 / 72.6
66.7 / 46.2
70.5 / 55.5
74.3 / 56.9
72.2 / 54.7
74.4 / 52.9
70.9 / 48.5
73.4 / 55.3
85.5 / 72.4
68.6 / 48.4
72.7 / 57.8
75.4 / 57.6
73.7 / 55.8
76.0 / 55.0
71.7 / 48.9
74.8 / 56.6
Table 4: MLQA results (F1/EM) for each language.
Discussion and comparison to mT5.
and XLM-R models obtain strong performance on
cross-lingual understanding benchmarks, as well
as high performance on English benchmarks (see
the score of 91.6 of mT5XXL on English XNLI).
Many hyperparameters are however different between mT5 and XLM-R models which makes difﬁcult an apple-to-apple comparison. First, as shown
in Table 5, the mT5 models are pretrained on the
much larger mC4 dataset which contains around
6.4T tokens, which is 38 times bigger than CC100
(167B tokens). While XLM-RLarge was pretrained
with more updates (6T tokens), the XLM-RXL and
XLM-RXXL models have seen less tokens (0.5T)
during pretraining than their mT5 counterparts, although it also uses a bigger batch size . Another difference is the context
sequence length of 512 for XLM-R and 1024 for
mT5. The mT5-XXL model also has slightly more
parameters (13B over 10.7B). The larger number
of updates combined with the larger dataset size
may explain the larger improvement from the XL
model to the XXL model in the case of mT5 (+3 average accuracy on XNLI), in which the additional
parameters
training tokens
XLM-RLarge
Table 5: Comparison of datasets and pretraining details
between XLM-R and mT5. We report dataset sizes and
number of updates in terms of number of tokens.
capacity can exploit the large quantity of unlabeled
mC4 data. We note however that the mT5XL is
outperformed by XLM-RXL on XNLI by 0.6% on
average, on XQuad by 1.3% and on MLQA by
0.9% when considering average EM score. In comparison, gains of XLM-R from the XL to the XXL
architecture are only of 0.6 on average. Another
explanation may be that generative models scale
better than masked language models. The difference in the nature of the pretraining dataset is particularly striking when looking at the variance of
performance across languages. For example the
mT5XXL outperforms XLM-RXXL by 8.4 points on
Swahili on XNLI zero-shot, while it only outperforms XLM-RXXL by 1.4 average accuracy. These
results may suggest that the CC100 dataset gets
saturated with current larger-capacity models.
Conclusion
In this study, we scaled the model capacity of the
XLM-R model up to 10.7B parameters and obtained stronger performance than previous XLM-
R models on cross-lingual understanding benchmarks. We also show that the additional capacity allows a multilingual model to outperform a
the RoBERTaLarge baseline on English benchmarks.
Our technical study thus suggests that larger capacity multilingual model can obtain state-of-the-art
cross-lingual understanding results while maintaining strong performance on high-resource languages.
Our work provides an alternative to mT5 models,
with new state-of-the-art performance on some languages. We release our code and models publicly.