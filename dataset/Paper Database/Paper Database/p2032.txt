Analysis and Design of Optimization
Algorithms via Integral Quadratic
Constraints
Laurent Lessard
Benjamin Recht
Andrew Packard
This manuscript develops a new framework to analyze and design iterative optimization algorithms built on the notion of Integral Quadratic Constraints (IQC) from
robust control theory. IQCs provide suﬃcient conditions for the stability of complicated interconnected systems, and these conditions can be checked by semideﬁnite
programming. We discuss how to adapt IQC theory to study optimization algorithms,
proving new inequalities about convex functions and providing a version of IQC theory adapted for use by optimization researchers. Using these inequalities, we derive
numerical upper bounds on convergence rates for the Gradient method, the Heavy-ball
method, Nesterov’s accelerated method, and related variants by solving small, simple
semideﬁnite programming problems. We also brieﬂy show how these techniques can be
used to search for optimization algorithms with desired performance characteristics,
establishing a new methodology for algorithm design.
Introduction
Convex optimization algorithms provide a powerful toolkit for robust, eﬃcient, large-scale
optimization algorithms. They provide not only eﬀective tools for solving optimization
problems, but are guaranteed to converge to accurate solutions in provided time budgets , are robust to errors and time delays , and are amendable to declarative modeling that decouples the algorithm design from the problem formulation .
However, as we push up against the boundaries of the convex analysis framework, try to
build more complicated models, and aim to deploy optimization systems in highly complex
environments, the mathematical guarantees of convexity start to break. The standard
proof techniques for analyzing convex optimization rely on deep insights by experts and
are devised on an algorithm-by-algorithm basis. It is thus not clear how to extend the
toolkit to more diverse scenarios where multiple objectives—such as robustness, accuracy,
and speed—need to be delicately balanced.
This paper marks an attempt at providing a systematized approach to the design and
analysis optimization algorithms using techniques from control theory. Our strategy is
to adapt the notion of an integral quadratic constraint from robust control theory .
These constraints link sequences of inputs and outputs of operators, and are ideally suited
to proving algorithmic convergence. We will see that for convex functions, we can derive
these constraints using only the standard ﬁrst-order characterization of convex functions,
and that these inequalities will be suﬃcient to reduce the analysis of ﬁrst-order methods to
the solution of a very small semideﬁnite program. Our IQC framework puts the analysis
of algorithms in a uniﬁed proof framework, and enables new analyses of algorithms by
minor perturbations of existing proofs. This new system aims to simplify and automate
the analysis of optimization programs, and perhaps to open new directions for algorithm
 
Our methods are inspired by the recent work of Drori and Teboulle .
manuscript, the authors propose writing down the ﬁrst-order convexity inequality for
all steps of an algorithmic procedure. They then derive a semideﬁnite program that analytically veriﬁes very tight bounds for the convergence rate for the Gradient method, and
numerically precise bounds for convergence of Nesterov’s method and other ﬁrst-order
methods. The main drawback of the Drori and Teboulle approach is that the size of the
semideﬁnite program scales with the number of time steps desired. Thus, it becomes
computationally laborious to analyze algorithms that require more than a few hundred
iterations.
Integral quadratic constraints will allow us to circumvent this issue. A typical example
of one of our semideﬁnite programs might have a 3 × 3 positive semideﬁnite decision
variable, 3 scalar variables, a 5 × 5 semideﬁnite cone constraint, and 4 scalar constraints.
Such a problem can be solved in less than 10 milliseconds on a laptop with standard
We are able to analyze a variety of methods in our framework. We show that our
framework recovers the standard rates of convergence for the Gradient method applied to
strongly convex functions. We show that we can numerically estimate the performance of
Nesterov’s method. Indeed, our analysis provides slightly sharper bounds than Nesterov’s
proof. We show how our system fails to certify the stability of the popular Heavy-ball
method of Polyak for strongly convex functions whose condition ratio is larger than 18.
Based on this analysis, we are able to construct a one-dimensional strongly convex function
whose condition ratio is 25 and prove analytically that the Heavy-ball method fails to
ﬁnd the global minimum of this function. This suggests that our tools can also be used
as a way to guide the construction of counterexamples.
We show that our methods extend immediately to the projected and proximal variants
of all the ﬁrst order methods we analyze. We also show how to extend our analysis to
functions that are convex but not strongly convex, and provide bounds on convergence
that are within a logarithmic factor of the best upper bounds. We also demonstrate that
our methods can bound convergence rates when the gradient is perturbed by relative
deterministic noise.
We show how diﬀerent parameter settings lead to very diﬀerent
degradations in performance bounds as the noise increases.
Finally, we turn to algorithm design. Since our semideﬁnite program takes as input
the parameters of our iterative scheme, we can search over these parameters. For simple
two-step methods, our algorithms are parameterized by 3 parameters, and we show how
we can derive ﬁrst-order methods that achieve nearly the same rate of convergence as
Nesterov’s accelerated method but are more robust to noise.
The manuscript is organized as follows. We begin with a discussion of discrete-time
dynamical system and how common optimization algorithms can be viewed as feedback
interconnections between a known linear system with an uncertain nonlinear component.
We then turn to show how quadratic Lyapunov functions can be used to certify rates
of convergence for optimization problems and can be found by semideﬁnite programming. This immediately leads to the notion of an integral quadratic constraint. Another
contribution of this work is a new form of IQC analysis geared speciﬁcally toward rate-ofconvergence conclusions, and accessible to optimization researchers. We also discuss their
history in robust control theory and how they can be derived. With these basic IQCs
in hand, we then turn to analyzing the Gradient method and Nesterov method, their
projected and proximal variants, and their robustness to noise. We discuss one possible
brute-force technique for designing new algorithms, and how we can outperform existing
methods. Finally, we conclude with many directions for future work.
Notation and conventions
Common matrices.
The d×d identity matrix and zero matrix are denoted Id and 0d,
respectively. Subscripts are omitted when they are to be inferred by context.
Norms and sequences.
We deﬁne ℓn
2e to be the set of all one-sided sequences x : N →
Rn. We sometimes omit n and simply write ℓ2e when the superscript is clear from context.
The notation ∥· ∥: Rn →R denotes the standard 2-norm. The subset ℓ2 ⊂ℓ2e consists
of all square-summable sequences. In other words, x ∈ℓ2 if and only if P∞
k=0 ∥xk∥2 is
convergent.
Convex functions.
For a given 0 < m < L, we deﬁne S(m, L) to be the set of functions
f : Rd →R that are continuously diﬀerentiable, strongly convex with parameter m, and
have Lipschitz gradients with parameter L. In other words, f satisﬁes
m∥x −y∥2 ≤(∇f(x) −∇f(y))T(x −y) ≤L∥x −y∥2
for all x, y ∈Rd
We call κ := L/m the condition ratio of f ∈S(m, L). We adopt this terminology to
distinguish the condition ratio of a function from the related concept of condition number
of a matrix.
The connection is that if f is twice diﬀerentiable, we have the bound:
cond(∇2f(x)) ≤κ for all x ∈Rd, where cond(·) is the condition number.
Kronecker product
The Kronecker product of two matrices A ∈Rm×n and B ∈Rp×q
is denoted A ⊗B ∈Rmp×nq and given by:
Two useful properties of the Kronecker product are that (A ⊗B)T = AT ⊗BT and that
(A ⊗B)(C ⊗D) = (AC) ⊗(BD) whenever the matrix dimensions are such that the
products AC and BD make sense.
Optimization algorithms as dynamical systems
A linear dynamical system is a set of recursive linear equations of the form
ξk+1 = Aξk + Buk
yk = Cξk + Duk .
At each timestep k = 0, 1, . . . , uk ∈Rd is the input, yk ∈Rd is the output, and ξk ∈Rm is
the state. We can write the dynamical system (2.1) compactly by stacking the matrices
into a block using the notation
We can connect this linear system in feedback with a nonlinearity φ by deﬁning the rule
ξk+1 = Aξk + Buk
yk = Cξk + Duk
uk = φ(yk) .
In this case, the output is transformed by the map φ : Rd →Rd and is then used as the
input to the linear system.
In this paper, we will be interested in the case when the interconnected nonlinearity
has the form φ(y) = ∇f(y) where f ∈S(m, L). In particular, we will consider algorithms
designed to solve the optimization problem
as dynamical systems and see how this new viewpoint can give us insights into convergence
analysis. Section 5.3 considers variants of (2.3) where the decision variable x is constrained
or f is non-smooth.
Standard ﬁrst order methods such as the Gradient method, Heavy-ball method, and
Nesterov’s accelerated method, can all be cast in the form (2.2). In all cases, the nonlinearity is the mapping φ(y) = ∇f(y). The state transition matrices A, B, C, D diﬀer for
each algorithm. The Gradient method can be expressed as
To verify this, substitute (2.4) into (2.2) and obtain
ξk+1 = ξk −αuk
uk = ∇f(yk)
Eliminating yk and uk and renaming ξ to x yields
xk+1 = xk −α∇f(xk)
which is the familiar Gradient method with constant stepsize.
Nesterov’s accelerated
method for strongly convex functions is given by the dynamical system
Verifying that (2.5) is equivalent to Nesterov’s method takes only slightly more eﬀort
than it did for the Gradient method. Substituting (2.5) into (2.2) now yields
k+1 = (1 + β)ξ(1)
k+1 = ξ(1)
yk = (1 + β)ξ(1)
uk = ∇f(yk)
Note that (2.6b) asserts that the partial state ξ(2) is a delayed version of the state ξ(1).
Substituting (2.6b) into (2.6a) gives the simpliﬁed system
k+1 = (1 + β)ξ(1)
yk = (1 + β)ξ(1)
uk = ∇f(yk)
Eliminating uk and renaming ξ(1) to x yields the common form of Nesterov’s method
xk+1 = yk −α∇f(yk)
yk = (1 + β)xk −βxk−1 .
Note that other variants of this algorithm exist for which the α and β parameters are updated at each iteration. In this paper, we restrict our analysis to the constant-parameter
version above. The Heavy-ball method is given by
One can check by similar analysis that (2.7) is equivalent to the update rule
xk+1 = xk −α∇f(xk) + β(xk −xk−1) .
Proving algorithm convergence
Convergence analysis of convex optimization algorithms typically follows a two step procedure. First one must show that the algorithm has a ﬁxed point that solves the optimization problem in question. Then, one must verify that from a reasonable starting
point, the algorithm converges to this optimal solution at a speciﬁed rate.
In dynamical systems, such proofs are called stability analysis. By writing common
ﬁrst order methods as dynamical systems, we can unify their stability analysis. For a
general problem with minimum occurring at y⋆, a necessary condition for optimality is
that u⋆= ∇f(y⋆) = 0. Substituting into (2.1), the ﬁxed point satisﬁes
In particular, A must have an eigenvalue of 1. If the blocks of A are diagonal as in the
Gradient, Heavy-ball, or Nesterov methods shown above, then the eigenvalue of 1 will
have a geometric multiplicity of at least d.
Proving that all paths lead to the optimal solution requires more eﬀort and constitutes
the bulk of what is studied herein. Before we proceed for general convex f, it is instructive
to study what happens for quadratic f.
Quadratic problems
Suppose f is a convex, quadratic function f(y) = 1
2yTQy−pTy+r, where mId ⪯Q ⪯LId
in the positive deﬁnite ordering. The gradient of f is simply ∇f(y) = Qy −p and the
optimal solution is y⋆= Q−1p.
What happens when we run a ﬁrst order method on a quadratic problem? Assume
throughout this section that D = 0. Substituting the equation for y⋆and ∇f(y) back
into (2.2), we obtain the system of equations:
ξk+1 = Aξk + Buk
uk = ∇f(yk) = Qyk −p = Q(yk −y⋆)
Now make use of the ﬁxed-point equations y⋆= Cξ⋆and ξ⋆= Aξ⋆and we obtain
uk = QC(ξk −ξ⋆). Eliminating yk and uk from the above equations, we obtain
ξk+1 −ξ⋆= (A + BQC)(ξk −ξ⋆)
Let T := A + BQC denote the closed-loop state transition matrix.
A necessary and
suﬃcient condition for ξk to converge to ξ⋆is that the spectral radius of T is strictly less
than 1. Recall that the spectral radius of a matrix M is deﬁned as the largest magnitude
of the eigenvalues of M. We denote the spectral radius by ρ(M). It is a fact that
ρ(M) ≤∥M k∥1/k
for all k and
ρ(M) = lim
k→∞∥M k∥1/k
where ∥· ∥is the induced 2-norm. Therefore, for any ε > 0, we have for all k suﬃciently
large that ρ(T)k ≤∥T k∥≤(ρ(T) + ε)k. Hence, we can bound the convergence rate:
∥ξk −ξ⋆∥= ∥T k(ξ0 −ξ⋆)∥≤∥T k∥∥ξ0 −ξ⋆∥≤(ρ(T) + ε)k∥ξ0 −ξ⋆∥.
So the spectral radius also determines the rate of convergence of the algorithm. With only
bounds on the eigenvalues of Q, we can provide conditions under which the algorithms
above converge for quadratic f.
Proposition 1 The following table gives worst-case rates for diﬀerent algorithms and
parameter choices when applied to a class of convex quadratic functions. We assume
here that f : Rd →R where f(x) = 1
2xTQx −pTx + r and Q is any matrix that satisﬁes
mId ⪯Q ⪯LId. We also deﬁne κ := L/m.
Parameter choice
Rate bound
popular choice
standard choice
optimal tuning
optimal tuning
Heavy-ball
L+√m)2 , β =
optimal tuning
All of these results are proven by elementary linear algebra and the bounds are tight.
In other words, there exists a quadratic function that achieves the worst-case ρ.
Appendix A for more detail.
Unfortunately, the proof technique used in Proposition 1 does not extend to the case
where f is a more general strongly convex function. However, a diﬀerent characterization
of stability does generalize and will be described in Section 3. It turns out that for linear
systems, stability is equivalent to the feasibility of a particular semideﬁnite program. We
will see in the sequel that similar semideﬁnite programs can be used to certify stability
of nonlinear systems.
Proposition 2 Suppose T ∈Rd×d. Then ρ(T) < ρ if and only if there exists a P ≻0
satisfying T TPT −ρ2P ≺0.
The proof of Proposition 2 is elementary so we omit it.
The use of Linear Matrix
Inequalities (LMI) to characterize stability of a linear time-invariant system dates back
to Lyapunov , and we give a more detailed account of this history in Section 3.4.
Now suppose we are studying a dynamical system of the form ξk+1 −ξ⋆= T(ξk −ξ⋆) as
in (2.8). Then, if there exists a P ≻0 satisfying T TPT −ρ2P ≺0,
(ξk+1 −ξ⋆)TP(ξk+1 −ξ⋆) < ρ2(ξk −ξ⋆)TP(ξk −ξ⋆)
along all trajectories.
If ρ < 1, then the sequence {ξk}k≥0 converges linearly to ξ⋆.
Iterating (2.9) down to k = 0, we see that
(ξk −ξ⋆)TP(ξk −ξ⋆) < ρ2k(ξ0 −ξ⋆)TP(ξ0 −ξ⋆)
which implies that
cond(P) ρk∥ξ0 −ξ⋆∥
where cond(P) is the condition number of P. In what follows, we will generalize this
semideﬁnite programming approach to yield feasibility problems that are suﬃcient to
characterize when the closed loop system (2.2) converges and which provide bounds on
the distance to optimality as well. The function
V (ξ) = (ξ −ξ⋆)TP(ξ −ξ⋆)
is called a Lyapunov function for the dynamical system. This function strictly decreases
over all trajectories and hence certiﬁes that the algorithm is stable, i.e., converges to
nominal values. The conventional method for proving stability of an electromechanical
system is to show that some notion of total energy always decreases over time. Lyapunov
functions provide a convenient mathematical formulation of this notion of total energy.
The question for the remainder of the paper is how can we search for Lyapunov-like
functions that guarantee algorithmic convergence when f is not quadratic.
Proving convergence using integral quadratic constraints
When the function being minimized is quadratic as explored in Section 2.2, its gradient
is aﬃne and the interconnected dynamical system is a simple linear diﬀerence equation
whose stability and convergence rate is analyzed solely in terms of eigenvalues of the
closed-loop system. When the cost function is not quadratic, the gradient update is not
an aﬃne function and hence a diﬀerent analysis technique is required.
A popular technique in the control theory literature is to use integral quadratic constraints (IQCs) to capture features of the behavior of partially-known components. The
term IQC was introduced in the seminal paper by Megretski and Rantzer . In that
work, the authors analyzed continuous time dynamical systems and the constraints involved integrals of quadratic functions, hence the name IQC.
In the development that follows, we repurpose the classical IQC theory for use in
algorithm analysis. This requires using discrete time dynamical systems so our constraints
will involve sums of quadratics rather than integrals. We also adapt the theory in a way
that allows us to certify a speciﬁc convergence rate in addition to stability.
An introduction to IQCs
IQCs provide a convenient framework for analyzing interconnected dynamical systems
that contain components that are noisy, uncertain, or otherwise diﬃcult to model. The
idea is to replace this troublesome component by a quadratic constraint on its inputs and
outputs that is known to be satisﬁed by all possible instances of the component. If we can
certify that the newly constrained system performs as desired, then the original system
must do so as well.
Suppose φ : ℓ2e →ℓ2e is the troublesome function we wish to analyze. The equation
u = φ(y) can be represented using a block diagram, as in Figure 1.
Figure 1: Block-diagram representation of the map φ.
Although we do not know φ exactly, we assume that we have some knowledge of the
constraints it imposes on the pair (y, u). For example, suppose it is known that φ satisﬁes
the following properties:
(i) φ is static and memoryless: φ(y0, y1, . . . ) = (g(y0), g(y1), . . . ) for some g : Rd →Rd.
(ii) g is L-Lipschitz: ∥g(y1) −g(y2)∥≤L∥y1 −y2∥for all y1, y2 ∈Rd.
Now suppose that y = (y0, y1, . . . ) is an arbitrary sequence of vectors in Rd, and u = φ(y)
is the output of the unknown function applied to y. Property (ii) implies that ∥uk −u⋆∥≤
L∥yk −y⋆∥for all k, where (y⋆, u⋆) is any pair of vectors satisfying u⋆= g(y⋆) that will
serve as a reference point. In matrix form, this is
for k = 0, 1, . . .
Core idea behind IQC.
Instead of analyzing a system that contains φ, we analyze
the system where φ is removed, but we enforce the constraints (3.1) on the signals (y, u).
Since (3.1) is true for all admissible choices of φ, then any properties we can prove for
the constrained system must hold for the original system as well.
Note that (3.1) is rather special in that the quadratic coupling of (y, u) is pointwise;
it only manifests itself as separate quadratic constraints on each (yk, uk). It is possible
to specify more general quadratic constraints that couple diﬀerent k values, and the key
insight above still holds. To do this, introduce auxiliary sequences ζ, z ∈ℓ2e together
with a map Ψ characterized by the matrices (AΨ, By
Ψ) and the recursion
ζk+1 = AΨζk + By
zk = CΨζk + Dy
where we will deﬁne the initial condition ζ⋆shortly. The equations (3.2) deﬁne an aﬃne
map z = Ψ(y, u).
Assuming a reference point (y⋆, u⋆) as before, we can deﬁne the
associated reference (ζ⋆, z⋆) that is a ﬁxed point of (3.2). In other words,
ζ⋆= AΨζ⋆+ By
z⋆= CΨζ⋆+ Dy
We will require that ρ(AΨ) < 1, which ensures that (3.3) has a unique solution (ζ⋆, z⋆)
for any choice of (y⋆, u⋆). Note that the reference points are deﬁned in such a way that if
we use y = (y⋆, y⋆, . . . ) and u = (u⋆, u⋆, . . . ) in (3.2), we will obtain ζ = (ζ⋆, ζ⋆, . . . ) and
z = (z⋆, z⋆, . . . ).
We then consider the quadratic forms (zk −z⋆)TM(zk −z⋆) for a given symmetric
matrix M (typically indeﬁnite).
Note that each such quadratic form is a function of
(y0, . . . , yk, u0, . . . , uk) that is determined by our choice of (Ψ, M, y⋆, u⋆). In our previous
example (3.1), Ψ has no dynamics and the corresponding Ψ and M are
In other words, if we use the deﬁnitions (3.4), then (zk −z⋆)TM(zk −z⋆) ≥0 is the same
as (3.1). In general, these sorts of quadratic constraints are called IQCs. We consider
four diﬀerent types of IQCs, which we now deﬁne.
Deﬁnition 3 Suppose φ : ℓd
2e is an unknown map and Ψ : ℓd
given linear map of the form (3.2) with ρ(AΨ) < 1. Suppose (y⋆, u⋆) ∈R2d is a given
reference point and let (ζ⋆, z⋆) be the unique solution of (3.3). Suppose y ∈ℓd
arbitrary square-summable sequence. Let u = φ(y) and let z = Ψ(y, u) as in (3.2). We
say that φ satisﬁes the
1. Pointwise IQC deﬁned by (Ψ, M, y⋆, u⋆) if for all y ∈ℓd
2e and k ≥0,
(zk −z⋆)TM(zk −z⋆) ≥0
2. Hard IQC deﬁned by (Ψ, M, y⋆, u⋆) if for all y ∈ℓd
2e and k ≥0,
(zt −z⋆)TM(zt −z⋆) ≥0
3. ρ-Hard IQC deﬁned by (Ψ, M, ρ, y⋆, u⋆) if for all y ∈ℓd
2e and k ≥0,
ρ−2t(zt −z⋆)TM(zt −z⋆) ≥0
4. Soft IQC deﬁned by (Ψ, M, y⋆, u⋆) if for all y ∈ℓd
(zt −z⋆)TM(zt −z⋆) ≥0
(and the sum is convergent)
Note that the example (3.1) is a pointwise IQC. Examples of the other types of IQCs
will be described in Section 3.3. Note that the sets of maps satisfying the various IQCs
deﬁned above are nested as follows:
{all pointwise IQCs} ⊂{all ρ-hard IQCs, ρ < 1} ⊂{all hard IQCs} ⊂{all soft IQCs}
For example, if φ satisﬁes a pointwise IQC deﬁned by (Ψ, M, y⋆, u⋆) then it must also
satisfy the hard IQC deﬁned by the same (Ψ, M, y⋆, u⋆). The notions of hard IQC and
the more general soft IQC (sometimes simply called IQC) were introduced in and
their relationship is discussed in . These concepts are useful in proving that a dynamic
system is stable, but do not directly allow for the derivation of useful bounds on convergence rates. The deﬁnitions of pointwise and ρ-hard IQCs are new, and were created for
the purpose of better characterizing convergence rates, as we will see in Section 3.2.
Finally, note that y⋆and u⋆are nominal inputs and outputs for the unknown φ, and
they can be tuned to certify diﬀerent ﬁxed points of the interconnected system. We will
see in Section 3.2 that certifying a particular convergence rate to some ﬁxed point does
not require prior knowledge of ﬁxed point; only knowledge that the ﬁxed point exists.
Stability and performance results
In this section, we show how IQCs can be used to prove that iterative algorithms converge
and to bound the rate of convergence. In both cases, the certiﬁcation requires solving
a tractable convex program. We note that the original work on IQCs only proved
stability (boundedness). Some other works have addressed exponential stability , but the emphasis of these works is on proving the existence of an exponential decay
rate, and so the rates constructed are very conservative. We require rates that are less
conservative, and this is reﬂected in the inclusion of ρ in the LMI of our main result,
Theorem 4.
We will now combine the dynamical system framework of Section 2 and the IQC theory
of Section 3.1. Suppose G : ℓd
2e is an aﬃne map u 7→y described by the recursion
ξk+1 = Aξk + Buk
where (A, B, C) are matrices of appropriate dimensions. The map is aﬃne rather than
linear because of the initial condition ξ0. As in Section 2, G is the iterative algorithm we
wish to analyze, and using the general formalism of Section 3.1, φ is the nonlinear map
(y0, y1, . . . ) 7→(u0, u1, . . . ) that characterizes the feedback. Of course, this framework
subsumes the special case of interest in which uk = ∇f(yk) for each k. We assume that
φ satisﬁes an IQC, and this IQC is characterized by a map Ψ and matrix M. We can
interpret z = Ψ(y, u) as a ﬁltered version of the signals u and y. These equations can be
represented using a block-diagram as in Figure 2a.
(a) The auxiliary system Ψ produces z, which
is a ﬁltered version of the signals y and u.
(b) The nonlinearity φ is replaced by a constraint on z, so we may remove φ entirely.
Figure 2: Feedback interconnection between a system G and a nonlinearity φ. An IQC is
a constraint on (y, u) satisﬁed by φ. We only analyze the constrained system and so we
may remove the φ block entirely.
Consider the dynamics of G and Ψ from (3.5) and (3.2), respectively. Upon eliminating y, the recursions may be combined to obtain
More succinctly, (3.6) can be written as
xk+1 = ˆAxk + ˆBuk
zk = ˆCxk + ˆDuk
where we deﬁned xk :=
The dynamical system (3.7) is represented in Figure 2b by the dashed box. Our main
result is as follows.
Theorem 4 (Main result) Consider the block interconnection of Figure 2a. Suppose G
is given by (3.5) and Ψ is given by (3.2). Deﬁne ( ˆA, ˆB, ˆC, ˆD) as in (3.6)–(3.7). Suppose
(ξ⋆, ζ⋆, y⋆, u⋆, z⋆) is a ﬁxed point of (3.5) and (3.2). In other words,
ξ⋆= Aξ⋆+ Bu⋆
ζ⋆= AΨζ⋆+ By
z⋆= CΨζ⋆+ Dy
Suppose φ satisﬁes the ρ-hard IQC deﬁned by (Ψ, M, ρ, y⋆, u⋆) where 0 ≤ρ ≤1. Consider the following LMI.
 ˆATP ˆA −ρ2P
If (3.9) is feasible for some P ≻0 and λ ≥0, then for any ξ0, we have
cond(P) ρk ∥ξ0 −ξ⋆∥
where cond(P) is the condition number of P.
Proof. Let x, u, z ∈ℓ2e be a set of sequences that satisﬁes (3.7). Suppose (P, λ) is a
solution of (3.9). Multiply (3.9) on the left and right by
(uk −u⋆)T
its transpose, respectively. Making use of (3.7)–(3.8), we obtain
(xk+1 −x⋆)TP(xk+1 −x⋆)−ρ2 (xk −x⋆)TP(xk −x⋆)+λ (zk −z⋆)TM(zk −z⋆) ≤0 (3.10)
Multiply (3.10) by ρ−2k for each k and sum over k. The ﬁrst two terms yield a telescoping
sum and we obtain
ρ−2k+2(xk −x⋆)TP(xk −x⋆) −ρ2(x0 −x⋆)TP(x0 −x⋆)
ρ−2t(zt −z⋆)TM(zt −z⋆) ≤0
Because φ satisﬁes the ρ-hard IQC deﬁned by (Ψ, M, ρ, y⋆, u⋆), the summation part of
the inequality is nonnegative for all k. Therefore,
(xk −x⋆)TP(xk −x⋆) ≤ρ2k(x0 −x⋆)TP(x0 −x⋆)
for all k and consequently ∥xk −x⋆∥≤
cond(P) ρk ∥x0 −x⋆∥. Recall from (3.7) that
xk = (ξk, ζk) and from (3.2a) that ζ0 = ζ⋆. Therefore,
∥ξk −ξ⋆∥2 ≤∥xk −x⋆∥2
≤cond(P)ρ2k∥x0 −x⋆∥2
= cond(P)ρ2k ∥ξ0 −ξ⋆∥2 + ∥ζ0 −ζ⋆∥2
= cond(P)ρ2k∥ξ0 −ξ⋆∥2
and this completes the proof.
We now make several comments regarding Theorem 4.
Pointwise and hard IQCs
Theorem 4 can easily be adapted to other types of IQCs.
1. If the pointwise IQC deﬁned by some (Ψ, M, y⋆, u⋆) is satisﬁed, then so is the ρhard IQC deﬁned by (Ψ, M, ρ, y⋆, u⋆) for any ρ. Therefore, we may apply Theorem 4
directly and ignore the ρ-hardness constraint. The smallest ρ that makes (3.9) feasible
will correspond to the best exponential rate we can guarantee.
2. Hard IQCs are a special case of ρ-hard IQCs with ρ = 1. Therefore, if the LMI (3.9) is
feasible, Theorem 4 guarantees that ∥ξk −ξ⋆∥≤
cond(P)∥ξ0 −ξ⋆∥. In other words,
the iterates are bounded (but not necessarily convergent).
3. If a ρ1-hard IQC is satisﬁed, then so is the ρ-hard IQC for any ρ ≥ρ1. Also, if (3.9)
is feasible for some ρ2, it will also be feasible for any ρ ≥ρ2. Therefore, if we use
a ρ1-hard IQC and (3.9) is feasible for ρ2, then the smallest exponential rate we can
guarantee is ρ = max(ρ1, ρ2).
Multiple IQCs
Theorem 4 can also be generalized to the case where φ satisﬁes multiple
IQCs. Suppose φ satisﬁes the ρ-hard IQCs deﬁned by (Ψi, Mi, ρ, y(i)
⋆) for i = 1, . . . , r.
Simply redeﬁne the matrices ( ˆA, ˆB, ˆC, ˆD) in a manner analogous to (3.7), but where the
output is now (z(1)
k , . . . , z(r)
k ). Instead of (3.9), use
 ˆATP ˆA −ρ2P
where λ1, . . . , λr ≥0. Thus, when (3.11) is multiplied out as in (3.10), we now obtain
(xk+1 −x⋆)TP(xk+1 −x⋆) −ρ2 (xk −x⋆)TP(xk −x⋆)
⋆)TMi(z(i)
and the rest of the proof proceeds as in Theorem 4.
Remark on Lyapunov functions
In the quadratic case treated in Section 2.2, a
quadratic Lyapunov function is constructed from the solution P in (2.12). In the case of
IQCs, such a quadratic function cannot serve as a Lyapunov function because it does not
strictly decrease over all trajectories. Nevertheless, Theorem 4 shows how ρ-hard IQCs can
be used to certify a convergence rate and no Lyapunov function is explicitly constructed.
We can explain this diﬀerence more explicitly. If V (x) is a Lyapunov function, then by
deﬁnition it satisﬁes the properties;
(i) λ1∥x −x⋆∥2 ≤V (x) ≤λ2∥x −x⋆∥2 for all x and k.
(ii) V (xk+1) ≤ρ2V (xk) for all system trajectories {xk}k≥0.
Property (ii) implies that
V (xk) ≤ρ2kV (x0)
which, combined with Property (i) implies that ∥xk −x⋆∥≤
λ2/λ1 ρk∥x0 −x⋆∥. In
Theorem 4, we use V (x) = (x −x⋆)TP(x −x⋆), which satisﬁes (i) but not (ii). So V (x) is
not a Lyapunov function in the technical sense. Nevertheless, we prove directly that (3.12)
holds, and so the desired result still holds. That is, V (x) serves the same purpose as a
Lyapunov function.
IQCs for convex functions
We will derive three IQCs that are useful for describing gradients of strongly convex
functions: the sector (pointwise) IQC, the oﬀ-by-one (hard) IQC, and weighted oﬀ-byone (ρ-hard) IQC. In general, gradients of strongly convex functions satisfy an inﬁnite
family of IQCs, originally characterized by Zames and Falb for the single-input-singleoutput case . A generalization of the Zames-Falb IQCs to multidimensional functions
is derived in . Both the sector and oﬀ-by-one IQCs are special cases of Zames-Falb,
while the weighted oﬀ-by-one IQC is a convex combination of the sector and oﬀ-by-one
IQCs. While the Zames-Falb family is inﬁnite, the three simple IQCs mentioned above
are the only ones used in this paper. IQCs can be used to describe many other types
of functions as well, and further examples are available in .
We begin with some
fundamental inequalities that describe strongly convex function.
Proposition 5 (basic properties) Suppose f ∈S(m, L). Then the following properties hold for all x, y ∈Rd.
f(y) ≤f(x) + ∇f(x)T(y −x) + L
(∇f(y) −∇f(x))T(y −x) ≥1
L ∥∇f(y) −∇f(x)∥2
f(y) ≥f(x) + ∇f(x)T(y −x) + 1
2L∥∇f(y) −∇f(x)∥2
∇f(y) −∇f(x)
T  −2mLId
∇f(y) −∇f(x)
Property (3.13a) follows from the deﬁnition of Lipschitz gradients.
Properties (3.13b) and (3.13c) are commonly known as co-coercivity. To prove (3.13d), deﬁne
g(x) := f(x) −m
2 ∥x∥2 and note that g ∈S(0, L −m). Applying (3.13b) to g and rearranging, we obtain
(L + m)(∇f(y) −∇f(x))T(y −x) ≥mL∥y −x∥2 + ∥∇f(y) −∇f(x)∥2
which is precisely (3.13d).
Detailed derivations of these properties can be found for
example in .
Lemma 6 (sector IQC) Suppose fk ∈S(m, L) for each k, and (y⋆, u⋆) is a common
reference point for the gradients of fk. In other words, u⋆= ∇fk(y⋆) for all k ≥0. Let
φ := (∇f0, ∇f1, . . . ). If u = φ(y), then φ satisﬁes the pointwise IQC deﬁned by
The corresponding quadratic inequality is that for all y ∈ℓd
2 and k ≥0, we have
T  −2mLId
Proof. Equation (3.14) follows immediately from (3.13d) by using (f, x, y) →(fk, y⋆, yk).
It can be veriﬁed that
and therefore (3.14) is equivalent to (zk −z⋆)TM(zk −z⋆) ≥0 as required.
Remark 7 In Lemma 6, we use a slight abuse of notation in representing the map Ψ :
2e. In writing Ψ as a matrix in R2d×2d, we mean that Ψ is a static map that
operates pointwise on (y, u). In other words,
for all k.
Lemma 8 (oﬀ-by-one IQC) Suppose f ∈S(m, L) and (y⋆, u⋆) is a reference for the
gradient of f. In other words, u⋆= ∇f(y⋆). Let φ := (∇f, ∇f, . . . ). Then φ satisﬁes the
hard IQC deﬁned by
The corresponding quadratic inequality is that for all y ∈ℓd
2 and k ≥0, we have
(˜u0 −m˜y0)T(L˜y0 −˜u0) +
(˜ut −m˜yt)T L(˜yt −˜yt−1) −(˜ut −˜ut−1)
where we have deﬁned ˜yk := yk −y⋆and ˜uk := uk −u⋆.
Proof. Deﬁne the function
g(x) := f(x) −f(y⋆) −m
2 ∥x −y⋆∥2
It is straightforward to check that g ∈S(0, L −m), and g(x) ≥g(y⋆) = 0 for all x ∈Rd.
Applying (3.13c) using (f, x, y) →(g, y⋆, yk), we observe that
qk := (L −m)g(yk) −1
2∥∇g(yk)∥2 ≥0
for all k ≥0
Moreover, ∇g(yk) = ∇f(yk) −m(yk −y⋆) = ˜uk −m˜yk. Therefore, we may manipulate the
ﬁrst term in (3.15) to eliminate ˜u0 and obtain
(˜u0 −m˜y0)T(L˜y0 −˜u0) = ∇g(y0)T((L −m)˜y0 −∇g(y0))
= (L −m)∇g(y0)T˜y0 −∥∇g(y0)∥2
≥(L −m)g(y0) −1
2∥∇g(y0)∥2
where the inequality follows from applying (3.13c) using (f, x, y) →(g, y0, y⋆). Similarly,
the tth term in the sum in (3.15) can be bounded by eliminating ˜ut and ˜ut−1.
(˜ut −m˜yt)T(L(˜yt −˜yt−1) −(˜ut −˜ut−1))
= (L −m)∇g(yt)T(˜yt −˜yt−1) −∇g(yt)T(∇g(yt) −∇g(yt−1))
≥(L −m)(g(yt) −g(yt−1)) −1
2∥∇g(yt)∥2 + 1
2∥∇g(yt−1)∥2
= qt −qt−1
where the inequality follows this time from applying (3.13c) using (f, x, y) →(g, yt, yt−1).
Substituting (3.17) and (3.18) into the left-hand side of (3.15), the sum telescopes and
we obtain the lower bound qk, which is nonnegative from (3.16).
To verify the IQC factorization, note that the state equations for Ψ given in the statement of Lemma 8 are
ζk+1 = −Lyk + uk
ζk + Lyk −uk
ζ⋆+ Ly0 −u0
L(yk −yk−1) −(uk −uk−1)
Moreover, the solution to the ﬁxed-point equations (3.3) are
ζ⋆= −Ly⋆+ u⋆
Therefore, we conclude that
 L˜y0 −˜u0
−m˜y0 + ˜u0
L(˜yk −˜yk−1) −(˜uk −˜uk−1)
−m˜yk + ˜uk
and it follows that Pk
t=0(zt −z⋆)TM(zt −z⋆) ≥0 is equivalent to (3.15), as required.
Note that the sector IQC (3.14) is a special case of the oﬀ-by-one IQC when k = 0.
The oﬀ-by-one IQC is itself a special case of the Zames-Falb IQC, which we now describe.
Lemma 9 (Zames-Falb IQC) Suppose f
∈S(m, L) has the optimal point u⋆=
∇f(y⋆) = 0. Let φ := (∇f, ∇f, . . . ) and let h1, h2, . . . be any sequence of real numbers that
(i) {hτ}τ≥1 is ﬁnitely nonzero, and hs is the last nonzero component.
(ii) 0 ≤hτ ≤1 for all τ ≥1.
τ=1 hτ ≤1.
Then φ satisﬁes the hard IQC deﬁned by
The corresponding quadratic inequality is that for all y ∈ℓd
2 and k ≥0, we have
(˜ut −m˜yt)T
where we have deﬁned ˜yk := yk −y⋆and ˜uk := uk −u⋆.
Proof. We will construct a proof for a general sequence h1, h2, . . . by ﬁrst considering a
speciﬁc set of sequences. Fix some j ≥1 and consider the case where
For t < j, the terms in the sum (3.19) have the form
(˜ut −m˜yt)T(L˜yt −˜ut)
which are bounded below by qt ≥0, as proven in Lemma 8, (3.16)–(3.17). For t ≥j, the
terms in the sum (3.19) have the form
(˜ut −m˜yt)T(L(˜yt −˜yt−j) −(˜ut −˜ut−j))
which are bounded below by qt −qt−j, as proven in (3.18). Summing up (3.19) for all t
yields a telescoping sum, thereby proving that (3.19) holds. This can be thought of an
“oﬀ-by-j” IQC. Indeed, when j = 1, we recover the oﬀ-by-one IQC of Lemma 8.
Now note that if we take a convex combination of the inequalities (3.19) corresponding
to each oﬀ-by-j IQC and let the associated coeﬃcient be hj, we have proven (3.19) for
the case of a general sequence h1, h2, . . . .
Though we will not make use of the more general Zames-Falb family of inequalities,
we include them as they are interesting in their own right and may ﬁnd applications in
future work. We conclude this section with a ρ-hard version of the oﬀ-by-one IQC. This
ﬁnal IQC will be critical for deriving convergence rates.
Lemma 10 (weighted oﬀ-by-one IQC) Suppose f ∈S(m, L) and (y⋆, u⋆) is a reference for the gradient of f. In other words, u⋆= ∇f(y⋆). Let φ := (∇f, ∇f, . . . ). Then
for any (¯ρ, ρ) satisfying 0 ≤¯ρ ≤ρ ≤1, φ satisﬁes the ρ-hard IQC deﬁned by
The corresponding quadratic inequality is that for all y ∈ℓd
2 and k ≥0, we have
(˜u0 −m˜y0)T(L˜y0 −˜u0)+
ρ−2t(˜ut −m˜yt)T L(˜yt −¯ρ2˜yt−1)−(˜ut −¯ρ2˜ut−1)
where we have deﬁned ˜yk := yk −y⋆and ˜uk := uk −u⋆.
Proof. Note that the weighted oﬀ-by-one IQC is a Zames-Falb IQC with h = (¯ρ2, 0, . . . ).
Thus the hardness and the factorization (Ψ, M) follows from Lemma 9. In order to prove
ρ-hardness (3.20), a bit more work is required. First, observe (see remarks on pointwise
and hard IQCs after Theorem 4) that it suﬃces to show ¯ρ-hardness, and this will imply
ρ-hardness. The tth term in the sum in (3.20) can be bounded as follows. First, deﬁne
the general terms in the sector (Lemma 6) and oﬀ-by-one (Lemma 8) inequalities:
st := (˜ut −m˜yt)T(L˜yt −˜ut)
pt := (˜ut −m˜yt)T L(˜yt −˜yt−1) −(˜ut −˜ut−1)
Algebraic manipulations reveal that the general term in the sum (3.20) satisﬁes
(˜ut −m˜yt)T L(˜yt −¯ρ2˜yt−1) −(˜ut −¯ρ2˜ut−1)
= (1 −¯ρ2)st + ¯ρ2pt
≥(1 −¯ρ2)qt + ¯ρ2(qt −qt−1)
= qt −¯ρ2qt−1
where the inequalities follow from (3.17) and (3.18). Substituting the general term back
into (3.20) with ρ = ¯ρ, the ¯ρ−2t coeﬃcient causes the sum to telescope and we are left
with ¯ρ−2kqk, which is nonnegative from (3.16). This completes the proof.
Remark 11 In implementing the weighted oﬀ-by-one IQC, one can simply set ¯ρ = ρ.
However, a less conservative approach is to keep ¯ρ as an additional degree of freedom. In
Theorem 4, the IQC constraint is included in (3.9) in the ﬁnal term and is multiplied by
the constant λ ≥0. When using the weighted oﬀ-by-one IQC, this amounts to:
 (1 −¯ρ2)st + ¯ρ2pt
with the constraints: 0 ≤¯ρ ≤ρ and λ ≥0
By deﬁning λ1 = λ(1 −¯ρ2) and λ2 = λ¯ρ2, an equivalent expression is
λ1st + λ2pt
with the constraints: λ1, λ2 ≥0 and λ2 ≤ρ2(λ1 + λ2)
Historical context of IQCs and Lyapunov theory
Constructing Lyapunov functions has a long history in control and dynamical systems,
and the central focus of this paper is borrowing tools from this literature to see how we
can generalize our analysis from quadratic functions to more general, nonlinear convex
functions.
One of the most fundamental problems in control theory is certifying the stability of
nonlinear systems. In interconnected systems such as electric circuits or chemical plants,
individual components are typically modeled using diﬀerential (or diﬀerence) equations.
Interconnected systems often contain nonlinearities or components that are otherwise
diﬃcult to model. The earliest results on such systems date back to the work of Lur’e
and Postnikov . The goal was to prove stability under a wide range of admissible
uncertainties. This notion of robust stability was called absolute stability. Indeed, Lur’e
studied precisely the model we are concerned with: a known linear system interconnected
in feedback to an uncertain nonlinear system.
In the 1960’s and 70’s, several suﬃcient conditions for absolute stability were expressed
as frequency-domain conditions. In other words, the main objects of interest are ratios
of the Laplace transforms of the outputs to the inputs, also known as transfer functions.
Examples include the Popov criterion , the small-gain theorem, the circle criterion,
and passivity theory . Frequency-domain conditions were popular at the time because
they could be veriﬁed graphically. The work of Willems uniﬁed many of the existing
results by casting them in the time domain in a framework called dissipativity theory.
This notion is on one hand a generalization of Lyapunov functions to include systems
with exogenous inputs, and on the other hand a generalization of passivity theory and
the small-gain theorem. These ideas form the core of modern nonlinear control theory,
and are covered in many textbooks such as Khalil .
With the advent of computers, graphical methods were no longer required. The connection between frequency-domain conditions and Linear Matrix Inequalities (LMIs) was
made by Kalman and Yakubovich and culminated in the Kalman-Yakubovich-
Popov (KYP) lemma, also known as the Positive-Real lemma. This paved the way for the
use of modern computational tools such as semideﬁnite programming. Another important
development is the concept of the structured singular value , also known as µ-analysis.
While previous theory had been used to describe static nonlinearities or uncertainties,
µ-analysis is a computationally tractable framework for describing a system containing
multiple dynamic uncertainties. A survey of µ-related techniques and results is given
in . For a comprehensive overview of the history and development of LMIs in control
theory, we refer the reader to .
Integral Quadratic Constraints (IQCs) were ﬁrst introduced by Yakubovich, who considered the notion of imposing quadratic constraints on an inﬁnite-horizon control problem , and combining multiple constraints via the S-procedure .
The deﬁnitive
work on IQCs is Megretski and Rantzer . In this seminal paper, the authors showed
that dissipativity theory, as well as all the frequency-domain conditions, could be formulated as IQCs. Furthermore, the KYP lemma in conjunction with the S-procedure allows
stability to be veriﬁed by solving an LMI.
The seminal paper on IQCs develops the theory primarily in the frequency domain,
but also alludes to time-domain versions of the results by introducing hard IQCs. This
notion of hard IQCs is pursued in , where the main IQC stability theorem is rederived
entirely in the time domain. In the time domain, these constraints parallel the development of Nesterov, where we are able to construct inequalities linking multiple inputs
and outputs of uncertain functions. This allows us to provide a wholly self-contained
development of the theory. Moreover, we are able to enhance the techniques of , providing new IQCs and considerably sharper rates of convergence than those discussed in
the earlier work. In this sense, our work provides useful methods for control theorists
interested in estimating rates of stabilization of their control systems.
Case studies
We now use the results of Section 3 to rederive some existing results from the literature
on iterative large-scale algorithms. The IQC approach gives a uniﬁed method to analyze
many diﬀerent algorithms. In addition to verifying existing results, we also present a
negative result that was not previously known.
Computational approach
Given an iterative algorithm, our ﬁrst step is to express it as a feedback interconnection
of a discrete linear time-invariant dynamical system with a nonlinearity representing ∇f.
This procedure is explained in Section 2 and yields matrices (A, B, C).
The next step is to decide which IQCs will be used to characterize the nonlinearity. A
simple but conservative choice is the sector IQC deﬁned in Lemma 6. A less conservative
choice is the weighted oﬀ-by-one IQC of Lemma 10. For the chosen (Ψ, M), we ﬁnd the
smallest ρ such that the semideﬁnite program (SDP) (3.9) of Theorem 4 is feasible. In
the case of the sector IQC, the SDP has variables (P, λ, ρ). For the weighted oﬀ-by-one
IQC, the SDP has variables (P, λ1, λ2, ρ) as explained in Remark 11. The resulting ρ is
an upper bound for the worst-case convergence rate of the algorithm. Speciﬁcally,
cond(P) ρk∥ξ0 −ξ⋆∥.
To solve the SDP (3.9) numerically, observe that it is a quasiconvex program.
particular, for every ﬁxed ρ, (3.9) is an LMI. The simplest way to solve (3.9) is to use a
bisection search on ρ. For a ﬁxed ρ, the SDP (3.9) or (3.11) become an LMI and can be
eﬃciently solved using interior-point methods. Popular implementations include SDPT3,
SeDuMi, and Mosek. This approach was used for all the simulations presented herein.
More sophisticated methods exist to solve (3.9) as well. A quasiconvex program of
the type (3.9) is known as a generalized eigenvalue optimization problem (GEVP) .
The GEVP is well-studied and modiﬁed interior-point methods such as the method of
centers and the long-step method of analytic centers can be used to solve it.
Lossless dimensionality reduction
The size of the SDP in (3.9) is proportional to d, the size of the state ξk in the optimization algorithm. This can be problematic in cases where d is large because it can
be computationally costly to solve large SDPs. In many cases of interest, however, the
algorithms we wish to analyze have a block-diagonal structure. For example, Nesterov’s
accelerated method has the form (2.5), which is
Each of the matrices (A, B, C) is a block matrix with repeated diagonal blocks. Using
Kronecker product notation (see Section 1.1, this means for example that
and similarly for B and C. Moreover, the IQCs we use to describe ∇f have the same
sort of structure. That is, (AΨ, By
Ψ) are block matrices with repeated
diagonal blocks. Now consider the SDP (3.9) from Theorem 4.
 ˆATP ˆA −ρ2P
Based on the discussion above, each of the matrices ( ˆA, ˆB, ˆC, ˆD, M) have the form e.g.
A0 ⊗Id. Rather than looking for a general P ∈Rnd×nd with P ≻0nd, if we restrict our
search to P = P0 ⊗Id with P0 ∈Rn×n and P0 ≻0n, then the SDP reduces to
0 P0 ˆA0 −ρ2P0
The resulting SDP no longer depends on d and is eﬀectively the same as if we had solved
the original problem with d = 1. As it turns out, there is no loss of generality in assuming
a P of this form. To see why this is so, ﬁrst suppose P0 ≻0 satisﬁes (4.3). Then clearly
P = P0 ⊗Id satisﬁes (4.2). Conversely, suppose P ≻0 satisﬁes (4.2). Then deﬁne the
matrix P0 := (In ⊗e1)TP(In ⊗e1) where e1 =
0T ∈Rd×1. Note that P0 is
an n × n principal submatrix of P, and therefore P0 ≻0 because P ≻0. Multiplying the
left-hand side of (4.2) by (In ⊗e1)T on the left and (In ⊗e1) on the right, we conclude
that P0 satisﬁes (4.3). Thus, ˆP = P0 ⊗Id is also a solution to (4.2). In other words, (4.2)
is feasible if and only if (4.3) is feasible.
Known bounds for ﬁrst-order optimization algorithms
The following proposition summarizes some of the known bounds for optimizing strongly
convex functions.
Proposition 12 The following table gives worst-case rate bounds for diﬀerent algorithms
and parameter choices when applied to a class of strongly convex functions.
assume here that f : Rd →R where f ∈S(m, L). Again, we deﬁne κ := L/m.
Parameter choice
Rate bound
popular choice
standard choice
optimal tuning
The Gradient bounds in the table above follow from the bound ρ ≤
L+m , which
is proven in . A tighter Gradient bound ρ ≤max
|1 −αm|, |1 −αL|
is proven in 
but makes the additional assumption that f is twice diﬀerentiable. The Nesterov bound
in Proposition 12 is proven in using the technique of estimate sequences. There are
no known global convergence guarantees for the Heavy-ball method in the case of strongly
convex functions, but it is proven in that the Heavy-ball method converges locally
with the same rate as in Proposition 1.
In the following sections, we will use IQC machinery to demonstrate that the ﬁrst two
bounds in Proposition 12 are loose. We will construct tighter bounds for the strongly convex case without requiring additional assumptions about locality or twice-diﬀerentiability.
We will then use our framework to help guide a refutation of the convergence of the Heavyball method.
The Gradient method
The Gradient method with constant stepsize is among the simplest optimization schemes.
The recursion is given by
ξk+1 = ξk −α∇f(ξk)
We will analyze this algorithm by applying Theorem 4.
Since f ∈S(m, L), we may
use the sector IQC of Lemma 6 and (3.9) together with the dimensionality reduction of
Section 4.2 yields the following SDP.
Note that P is 1 × 1, so we may set P = 1 without loss of generality and we obtain the
following LMI in (ρ2, λ).
Using Schur complements, (4.6) is equivalent to
ρ2 ≥1 −2mLλ −(α −(L + m)λ)2
By analyzing the lower bound on ρ in (4.7), we can ﬁnd the optimal choice of λ as a
function of the stepsize α. Omitting the details, we eventually obtain the simple expression
|1 −αm|, |1 −αL|
. This is precisely the bound found for the quadratic case,
as derived in Appendix A. However, we have shown something much stronger here, since
the only assumption we made about f is that ∇f satisﬁes the sector IQC of Lemma 6. In
particular, the Gradient method rates in Proposition 1 hold not only for quadratics, but
also for strongly convex functions, and even for functions that change or switch over time
(either stochastically, adversarially, or otherwise), so long as each function satisﬁes the
pointwise sector constraint. Note that (4.6) can be transformed using Schur complements:
And now (4.8) is linear in (ρ2, λ, α).
This formulation allows one to directly answer
questions such as “what range of stepsizes can yield a given rate?”.
Nesterov’s accelerated method
Nesterov’s accelerated method with constant stepsize converges at a linear rate. There
exists some c > 0 such that for any initial condition ξ0,
∥ξk −ξ⋆∥≤cρk∥ξ0 −ξ⋆∥
when applied to functions f ∈S(m, L). In this case, the parameters are the standard
parameters from Proposition 12, which are α := 1/L and β := (
 . Nesterov also showed that a lower bound on convergence rate for any algorithm of
the form (2.2) and for any f ∈S(m, L) is given by
∥ξk −ξ⋆∥≥ρk
opt∥ξ0 −ξ⋆∥
Since ρ and ρopt behave similarly as L/m →∞, Nesterov’s accelerated method is sometimes called “optimal” or “nearly optimal”.
We computed the rate bounds using Theorem 4 using either the sector IQC of Lemma 6,
or a combination of the sector IQC and the weighted oﬀ-by-one IQC of Lemma 10. It is
important to note that unlike the Gradient method case, the LMI (3.9) is no longer linear
in ρ2. Therefore, we found the minimal ρ by performing a bisection search on ρ, see the
ﬁrst plot in Figure 3.
Condition ratio L/m
Convergence rate ρ
LMI (sector)
LMI (weighted oﬀ-by-one)
Optimal Gradient rate
Nesterov (strongly convex)
Nesterov (quadratics)
Theoretical lower bound
Condition ratio L/m
Iterations to convergence
LMI (sector)
LMI (weighted oﬀ-by-one)
Optimal Gradient rate
Nesterov (strongly convex)
Nesterov (quadratics)
Theoretical lower bound
Figure 3: Upper bounds for Nesterov’s accelerated method applied to f ∈S(m, L) using
the standard tuning in Proposition 12. We tested both the sector IQC and the weighted
oﬀ-by-one IQC. The ﬁrst plot shows convergence rate and the second plot shows number
of iterations required to achieve convergence to a speciﬁed tolerance. The theoretical lower
bound ρopt is given in (4.9). The rate that can be certiﬁed using the LMI approach is
strictly better than the rate proved in using estimate sequences.
The rate obtained using the sector IQC alone is very poor. To understand why, recall
from Lemma 6 that the sector IQC allows for fk to be diﬀerent at each iteration. Unlike
the Gradient method, Nesterov’s accelerated method is not robust to having a changing fk. However, convergence can nevertheless be guaranteed as long as ρ < 1, which
corresponds approximately to L/m < 11.7.
The rate obtained using the weighted oﬀ-by-one IQC improves upon the rate proven
in using the estimate sequence approach (see Proposition 12). Note that we do not
have an analytical expression for the improved bound; it was found numerically by solving
the LMI of Theorem 4.
Given that ∥xk∥≤
cond(P)ρk∥x0∥, if we seek the smallest k such that ∥xk∥≤ε,
then it suﬃces that
cond(P)ρk∥x0∥≤ε. This implies that
cond(P)∥x0∥2
For the second plot in Figure 3, we plotted −1/ log ρ versus L/m to get a sense of how
the relative iteration count scales as a function of condition number.
As we can see
from Figure 3, Nesterov’s method applied to quadratics is within a factor of 2 of the
theoretical lower bound, and the bound we can prove for Nesterov’s method applied to
strongly convex functions is within a factor of 1.4 of the bound for quadratics.
Finally, we must also ensure that P is reasonably well-conditioned. In Figure 4, we
see that cond(P) appears to be proportional to L/m, which agrees with the scale factor
found by Nesterov .
If we repeat the above experiments, but instead using the optimal tuning of Nesterov’s
method given in Proposition 1, the resulting plots are virtually identical. The only diﬀerences are that the curves are shifted down slightly because the optimal rate for quadratics
is now 1 −
√3κ+1 instead of 1 −
√κ. The sector-IQC curve goes unstable a little sooner
as well, at around L/m ≈10. Roughly speaking, if we use the optimal tuning we can
guarantee slightly faster convergence but slightly less robustness.
Condition ratio L/m
Scale factor cond(P)
LMI (sector)
LMI (weighted oﬀ-by-one)
Slope of 1
Figure 4: Condition number cond(P) when using the weighted oﬀ-by-one IQC. It is
within a constant factor of L/m. Note that log cond(P) appears in (4.10) for computing
minimum iterations to convergence.
The Heavy-ball method
The optimal Heavy-ball rate for quadratics in Proposition 1 matches Nesterov’s lower
bound (4.9) for strongly convex functions. Although the Heavy-ball method and Nesterov’s accelerated method have similar recursions, Figures 3 and 5 tell very diﬀerent
stories. When we allow for a diﬀerent fk at every iteration (sector IQC), we can guarantee stability when L/m ≈6 or less. When we include the weighted oﬀ-by-one IQC as
well, we can only guarantee stability when L/m ≈18 or less. While it seems possible
that using more IQCs could potentially improve this upper bound, it turns out that the
poor quality of these bounds is due to something more serious: the Heavy-ball method
optimized for quadratics does not converge for general f ∈S(m, L).
Condition ratio L/m
Convergence rate ρ
LMI (sector)
LMI (weighted oﬀ-by-one)
Optimal Gradient rate
Theoretical lower bound
Condition ratio L/m
Iterations to convergence
LMI (sector)
LMI (weighted oﬀ-by-one)
Optimal Gradient iterations
Theoretical lower bound
Figure 5: Upper bounds for the Heavy-ball method, using either the sector IQC or the
weighted oﬀ-by-one IQC. Convergence rate (ﬁrst plot) and number of iterations required
to achieve convergence to a speciﬁed tolerance (second plot). Note that the theoretical
lower bound is equal to the optimal Heavy-ball rate for quadratics. The theoretical lower
bound ρopt is given in (4.9).
To ﬁnd an example of an f(x) that leads to a non-convergent Heavy-ball method,
Figure 5 indicates that we should search for L/m > 18. The following one-dimensional
example does the job.
It is easy to check that ∇f(x) is continuous and monotone, and so f ∈S(m, L) with
m = 1 and L = 25. When using an initial condition in the interval 3.07 ≤x0 ≤3.46,
the Heavy-ball method produces a limit cycle with oscillations that never damp out. The
ﬁrst 50 iterates for x0 = 3.3 are shown in Figure 6, and a plot of f(x) with the limit cycle
overlaid is shown in Figure 7.
Iteration number k
Heavy-ball iterate xk
Iteration history of the Heavy-ball method when optimizing f(x) deﬁned
in (4.11). Dashed lines separate the pieces of f(x). The iterates tend to a limit cycle, so
the Heavy-ball method does not converge for this particular strongly convex function.
Figure 7: Graph of f(x) deﬁned in (4.11) with the limit cycle overlaid on top.
For a detailed proof that f can indeed converge to a limit cycle, see Appendix B. We
further investigate the stability of the Heavy-ball method in Section 5.
Further applications
Stability of the Heavy-ball method
We saw in Section 4.6 that the Heavy-ball method that uses α and β optimized for
quadratic functions is unstable for general strongly convex functions. A natural question
to ask is whether the Heavy-ball method is stable over the class S(m, L) for some choice
of α and β. This experiment is easy to carry out in our framework, because choosing new
values of α and β simply amounts to changing parameters in the LMI. We chose α = 1
and for a sampling of points in β ∈ , we evaluated the corresponding Heavy-ball
method using Theorem 4 together with the weighted oﬀ-by-one IQC. See Figure 8.
The ﬁrst plot shows convergence rate. When β = 0, the Heavy-ball method becomes
the Gradient method, which is always convergent. However, we can improve upon the
gradient rate by optimizing over β. The best achievable rate is given by the black curve.
The black curve lies strictly above the optimal Heavy-ball rate for quadratics, but below
the optimal gradient rate.
In the second plot, we show the iterations required to achieve convergence. Again, the
black curve represents the optimal parameter choice. As L/m gets large, the envelope
veers away from the optimal Heavy-ball curve and becomes parallel to the optimal gradient
Condition ratio L/m
Convergence rate ρ
Heavy-ball optimized rate using grid search
Gradient rate with α = 1
Theoretical lower bound
Condition ratio L/m
Iterations to convergence
Heavy-ball optimized iterations using grid search
Gradient iterations with α = 1
Theoretical lower bound
Figure 8: Upper bounds for the Heavy-ball method. We ﬁxed α = 1
L, and for each L/m,
we picked β that led to the optimal rate. The result is the solid black curve. We plotted
convergence rate (ﬁrst plot) and number of iterations required to achieve convergence to
a speciﬁed tolerance (second plot). The theoretical lower bound ρopt is given in (4.9) and
is the same as the optimal Heavy-ball rate for quadratics.
curve. So when L/m is large, even when β is chosen optimally, the Heavy-ball method is
comparable to the Gradient method in worst-case for general strongly convex functions.
Multiplicative gradient noise
A common consideration is the inclusion of noise in the gradient computation.
possible model is relative deterministic noise where we assume the gradient error is proportional to the distance to optimality . Instead of directly observing ∇f(y), we see
uk = ∇f(yk) + rk , where
∥rk∥≤δ∥∇f(yk)∥
for some small nonnegative δ. The IQC framework can be used to analyze such situations
to study the robustness of various algorithms to this type of noise.
If wk is the true gradient, we actually measure uk = ∆kwk, where the gradient error
is bounded above by a quantity proportional to the true gradient. In other words, we
assume there is some δ > 0 such that ∥uk −wk∥≤δ∥wk∥. Squaring both sides of the
inequality and rearranging, we obtain the IQC
Note that this is simply the sector IQC with m = 1 −δ and L = 1 + δ. We make no
assumptions on how the noise is generated; it may be the output of a stochastic process,
or could even be chosen adversarially. The modiﬁed block-diagram is shown in Figure 9.
Figure 9: Block-diagram representation of the standard interconnection with an additional
block ∆representing multiplicative noise.
By making a small modiﬁcation, we can apply Theorem 4. We will look to show that
the following inequality holds over all trajectories
k+1Pxk+1 −ρ2 xT
kPxk + λ1zT
k Mzk + λ2
for some λ1, λ2 ≥0. In order to formulate an LMI that implies a solution to (5.1), we
use the signal
. Consequently, the matrices ( ˆA, ˆB, ˆC, ˆD) from (3.6)–(3.7)
now become a map (wk, uk) 7→zk. This leads to an LMI of the form (3.11) which is
now block-3 × 3 instead of the 2 × 2 LMI of Theorem 4. The proof is identical to that of
Theorem 4.
Gradient method
Our ﬁrst experiment is to test the Gradient method. We used noise
values of δ ∈{0.01, 0.02, 0.05, 0.1, 0.2, 0.5}. See Figure 10.
In examining Figure 10, we observe that the Gradient method with stepsize
not very robust to multiplicative noise. Even with noise as low as 1% (δ = 0.01), the
Gradient method is no longer stable for L/m > 100. An explanation for this phenomenon
is that in choosing the stepsize α, we are trading oﬀconvergence rate with robustness.
The choice
L+m yields the minimum worst-case rate, but is fragile to noise. If we pick a
more conservative stepsize such as the popular choice α = 1
L, we obtain a very diﬀerent
picture. See Figure 11.
Notice that with the updated stepsize of α = 1
L, the Gradient method is now robust
to multiplicative noise. Robustness comes at the expense of a degradation in the best
achievable convergence rate.
This degradation manifests itself as a gap in Figure 11
between the black curves and the other ones.
Nesterov’s accelerated method
We can carry out an experiment similar to the one
we did with the Gradient method, but now with Nesterov’s method. As before, we examine the trade-oﬀbetween the magnitude of the multiplicative noise and the degradation
of the optimal convergence rate. This time, we use δ ∈{0.05, 0.1, 0.2, 0.3, 0.4, 0.5}. See
Figure 12.
Condition ratio L/m
Convergence rate ρ
Rates for diﬀerent values of δ
δ ∈{0.01, 0.02, 0.05, 0.1, 0.2, 0.5}
Noise-free Gradient rate, α =
Condition ratio L/m
Iterations to convergence
Iterations for diﬀerent values of δ
δ ∈{0.01, 0.02, 0.05, 0.1, 0.2, 0.5}
Noise-free Gradient iterations, α =
Figure 10: Convergence rate and iterations to convergence for the Gradient method with
L+m, for various noise parameters δ. This method is not robust to noise.
As with our ﬁrst Gradient method test, Nesterov’s method is not robust to multiplicative noise. For moderate L/m, the degradation is minor, but eventually leads to
instability when we reach a certain threshold. The idea that accelerated methods are
sensitive to noise and can lead to an accumulation of error was noted in the recent work
 , using a diﬀerent notion of gradient perturbation.
Robustness of Nesterov’s method can be improved by modiﬁying the α and β parameters. Choosing a smaller α pushes back the instability threshold, while choosing a smaller
β simultaneously pushes back the instability threshold and degrades the rate. In the
limit β →0, Nesterov’s method becomes the Gradient method, so we recover the plots of
Figure 11.
Proximal point methods
Suppose we are interested in solving a problem of the form
f(x) + P(x)
subject to
Condition ratio L/m
Convergence rate ρ
Rates for diﬀerent values of δ
δ ∈{0.01, 0.02, 0.05, 0.1, 0.2, 0.5}
Noise-free Gradient rate, α =
Condition ratio L/m
Iterations to convergence
Iterations for diﬀerent values of δ
δ ∈{0.01, 0.02, 0.05, 0.1, 0.2, 0.5}
Noise-free Gradient iterations, α =
Figure 11: Convergence rate and iterations to convergence for the Gradient method with
L, for various noise parameters δ. This method is robust to noise, but at the expense
of a gap in performance compared to the optimal stepsize of α =
where f ∈S(m, L), and P is an extended-real-valued convex function on Rn. An example
of such a problem is constrained optimization, where we require that x ∈C. In this case,
we simply let P be the indicator function of C. We will now show how the IQC framework
can be used to analyze algorithms involving a proximal operator. Deﬁne the proximal
operator of P as
Πν(x) := arg min
2∥x −y∥2 + νP(y)
As an illustrative example, we will show how to analyze the proximal version of Nesterov’s
algorithm. Iterations take the form:
ξk+1 = Πν (yk −α∇f(yk))
yk = ξk + β(ξk −ξk−1)
Note that when Πν = I, we recover the standard Nesterov algorithm. When β = 0, we
recover the proximal gradient method.
In order to analyze this algorithm, we must characterize Πν using IQCs. To this end,
let T := ∂P be the subdiﬀerential of P.
Then, Πν(x) is the unique point such that
Condition ratio L/m
Convergence rate ρ
Rates for diﬀerent values of δ
δ ∈{0.05, 0.1, 0.2, 0.3, 0.4, 0.5}
Standard Nesterov (quadratics)
Condition ratio L/m
Iterations to convergence
Iterations for diﬀerent values of δ
δ ∈{0.05, 0.1, 0.2, 0.3, 0.4, 0.5}
Standard Nesterov (quadratics)
Figure 12: Convergence rate and iterations to convergence for Nesterov’s method with
standard tuning, for various noise parameters δ.
x −Πν(x) ∈νT(Πν(x)). Or, written another way,
Πν = (I + νT)−1
Since T is a subdiﬀerential, it satisﬁes the incremental passivity condition. Namely,
(T(x) −T(y))T(x −y) ≥0
for all x, y ∈Rn
Therefore, T satisﬁes the sector IQC with m = 0 and L = ∞. In fact, via minor modiﬁcations of Lemma 8 and Lemma 9 using the deﬁnition of a subdiﬀerential rather than (3.13c),
T satisﬁes the oﬀ-by-one and weighted oﬀ-by-one IQCs as well. Now transform (5.2) by
introducing the auxiliary signals uk := ∇f(yk), wk := Πν(yk −αuk), vk := νT(wk). The
deﬁnitions of wk and vk together with (5.3) immediately imply that wk = yk −αuk −vk.
Therefore, we can rewrite (5.2) as
ξk+1 = ξk + β(ξk −ξk−1) −vk −αuk
wk = ξk + β(ξk −ξk−1) −vk −αuk
yk = ξk + β(ξk −ξk−1)
uk = ∇f(yk)
vk = νT(wk)
Figure 13: Block-diagram representation of the standard interconnection with an additional block νT representing a scaled subdiﬀerential.
These equations may be succinctly represented as a block diagram, as in Figure 13.
Analyzing this interconnection is done by accounting for the IQCs for both unknown
blocks. If (Ψ1, M1) is the IQC for ∇f with output z1
k and (Ψ2, M2) is the IQC for νT
with output z2
k, then we seek to show that for all trajectories satisfy
k+1Pxk+1 −ρ xT
kPxk + λ1(z1
k) + λ2(z2
where xk now includes the states ξk as well as the internal states of Ψ1 and Ψ2. As in
the proof of Theorem 4, for each ﬁxed ρ, we can write (5.4) as an LMI in the variables
P ≻0, λ1 ≥0, λ2 ≥0.
Applying this approach to the proximal version of Nesterov’s accelerated method, we
recover the exact same plots as in Figure 3. This is to be expected because it is known that
the proximal gradient and accelerated methods achieves the same worst-case convergence
rates as their unconstrained counterparts . We conjecture that any algorithm
G of the form (2.2) which converges with rate ρ has a proximal variant that converges at
precisely the same rate.
Weakly convex functions
With minor modiﬁcations to our analysis, we can immediately extend our results to the
case where the function to be optimized is convex, but not strongly convex. Speciﬁcally,
we will assume throughout this subsection that f ∈S(0, L). The following development
is due to Elad Hazan .
Suppose we want to minimize f over a compact, convex domain D for which we can
readily compute the Euclidean projection. Let R denote the diameter of the set D. Deﬁne
the function fε(x) := f(x) +
2R2 ∥x∥2. Note that fε is diﬀerentiable and strongly convex;
it satisﬁes fε ∈S( ε
R2 ). Therefore, we may apply our analysis to fε.
Suppose we execute on fε an algorithm with interleaved projections as in Section 5.3.
Let x⋆be any minimizer of f on D and x(ε)
be the minimizer of fε. Let ρ denote the
rate of convergence achieved when the condition ratio is set as κ = (1 + LR2/ε) and let
Pε ≻0 be the associated solution to the LMI. Let σ := cond(Pε). After k steps,
f(xk) −f(x⋆) = fε(xk) −fε(x⋆) +
 ∥x⋆∥2 −∥xk∥2
≤fε(xk) −fε(x(ε)
 ∥x⋆∥2 −∥xk∥2
≤fε(xk) −fε(x(ε)
Now apply (3.13a) from Proposition 5 using (f, x, y) = (fε, x(ε)
⋆, xk) and obtain
f(xk) −f(x⋆) ≤LR2 + ε
σρ2k∥x0 −x(ε)
 (LR2 + ε) σρ2k + ε
Where the last inequality follows from the deﬁnition of set diameter. Therefore, if
 (1 + LR2/ε) σ
2 log(ρ−1)
then f(xk) −f(x⋆) ≤ε. Substituting the rates found algebraically for the quadratic case
(Section 2.2) or our numerical results for the strongly convex case (Sections 4.4–4.5), the
convergence rate ρ satisﬁes
log(ρ−1) ∝κ = (1 + LR2/ε)
for the Gradient method, and
log(ρ−1) ∝κ1/2 = (1 + LR2/ε)1/2
for Nesterov’s accelerated method.
Finally, note that σ = cond(Pε) also depends on ε. We can control the growth of σ
directly by including a constraint of the form I ⪯P ⪯σI when solving the SDP of
Theorem 4. Alternatively, we can observe (see Figure 4) that σ ∝κ = (1 + LR2/ε).
Therefore, we conclude that
for the Gradient method, and
for Nesterov’s accelerated method.
This analysis matches the standard bounds up to the logarithmic terms .
Algorithm design
In this section, we show one way in which the IQC analysis framework can be used for
algorithm design. We saw in Section 5.2 that the Gradient method can be very robust
to noise (Figure 11), or not robust at all (Figure 10), depending on whether we use a
stepsize of α = 1/L or α = 2/(L + m), respectively.
A natural question to ask is whether such a trade-oﬀbetween performance and robustness exists with Nesterov’s method as well. As can be seen in Figure 12, Nesterov’s
method is only somewhat robust to noise. In the sequel, we will synthesize variants of
Nesterov’s method that explore the performance-robustness trade-oﬀspace.
Consider an algorithm of the form (2.2). Based on the discussion in Section 2.1, we
know A must have an eigenvalue of 1. Moreover, given any invertible T, the algorithms
(A, B, C, D) and (TAT −1, TB, CT −1, D) are equivalent realizations in the sense that if
one is stable with rate ρ, the other is stable with rate ρ as well.
Indeed, if the ﬁrst
algorithm has state ξk, the second algorithm has state Tξk. We limit our search to the
case A ∈R2×2 and D = 0. Three parameters are required to characterize all possible
algorithms in this family (modulo equivalences due to a choice of T).
One possible
parameterization is given by
(α, β1, β2) ∈R3
In light of the discussion in Section 2, we see that the Gradient, Heavy-ball, and Nesterov
methods are all special cases of (6.1). In particular,
(α, β1, β2) is equal to:
for the Gradient method
for the Heavy-ball method
for Nesterov’s method
We may also rewrite (6.1) in more familiar recursion form as
ξk+1 = ξk −α∇f(yk) + β1(ξk −ξk−1)
yk = ξk + β2(ξk −ξk−1)
Our approach is straightforward:
for each choice of condition ratio L/m and noise
strength δ, we generate a large grid of tuples (α, β1, β2) and use the approach of Section 5.2 to evaluate each algorithm. We then choose the algorithm with the lowest ρ.
In other words, given bounds on the condition ratio and noise strength, we choose the
algorithm for which we can certify the best possible convergence rate over all admissible
choices of f and gradient noise. The performance of each optimized algorithm is plotted
in Figure 14.
By design, this new family of algorithms must have a performance superior to the
Gradient method, Nesterov’s method, and the Heavy-ball method for any choice of tuning
parameters.
In the limit δ →0, we appear to recover the performance of Nesterov’s
method when it is applied to quadratics.
That is, we have used numerical search to
ﬁnd an algorithm whose worst case performance guarantee is slightly better than what is
guaranteed by Nesterov’s method.
In the second plot of Figure 14, the algorithms robust to higher noise levels have greater
slopes. When the noise level is low (δ = 0.01), we approach a slope of 0.5, the same as
Nesterov. When the noise level is high (δ = 0.5), the slope is roughly 0.75. Note that the
Gradient method, which was robust for all noise levels, has a slope of 1. Therefore, the
new algorithms we found explore the trade-oﬀbetween noise robustness and performance,
and may be useful in instances where Nesterov’s method would be too fragile and the
Gradient method would be too slow.
Future work
We are only beginning to get a sense of what IQCs can tell us about optimization schemes,
and there are many more control theory tools and techniques left to adapt to the context
of optimization and machine learning. We conclude this paper with several interesting
directions for future work.
Analytic proofs
One of the drawbacks of our numerical proofs is that we are always
pushing up against numerical error and conditioning error. Analytic proofs would alleviate this issue and could provide more interpretable results about how parameters of
algorithms should vary to meet performance and robustness demands. To provide such
analytic proofs, one would have to solve small LMIs in closed form. This amounts to
solving small semideﬁnite programming problems, and this may be doable using analytic
tools from algebraic geometry .
Lower Bounds
Our IQC conditions are merely suﬃcient for verifying the convergence
of an optimization problem. However, as pointed out by Megretski and Rantzer, the
derived conditions are necessary in a restricted sense . If we fail to ﬁnd a solution
Condition ratio L/m
Convergence rate ρ
Rates for diﬀerent values of δ
δ ∈{0.01, 0.1, 0.2, 0.3, 0.4, 0.5}
Standard Nesterov (quadratics)
Condition ratio L/m
Iterations to convergence
Iterations for diﬀerent values of δ
δ ∈{0.01, 0.1, 0.2, 0.3, 0.4, 0.5}
Standard Nesterov (quadratics)
Figure 14: Upper bounds found using a brute-force search over the three-parameter
family of algorithms described by (6.2).
Convergence rate is shown (ﬁrst plot) as is
the number of iterations required to achieve convergence to a speciﬁed tolerance (second
plot). Although the bounds assume strongly convex functions, we also show the worstcase rate for quadratics as a comparison.
to our LMI, then there is necessarily a sequence of point that satisfy all of the IQC
constraints and that do not converge to an equilibrium . It is thus possible that
this tool can be used to construct a convex function to serve as a counterexample for
convergence. This intuition was what guided our construction of a counterexample for
the convergence of the Heavy-ball method. It may be possible that this construction can
be generalized to systematically produce counterexamples.
Time-varying algorithms
In many practical scenarios, we know neither the Lipschitz
constant L nor the strong convexity parameter m. Under such conditions, some sort of
estimation scheme is used to choose the appropriate step size. This could be a simple
backoﬀscheme to ensure a suﬃcient decrease, or a more intricate search method to ﬁnd
the appropriate parameters . From our control vantage point, it may be possible to use
techniques from adaptive control to certify when such line search methods are stable. In
particular, these could be used to diﬀerentiate between the diﬀerent sorts of schemes used
to choose the parameters of the nonlinear conjugate gradient method. Useful connections
are made between robustness analysis of adaptive controllers and Lyapunov theory in .
A related area of study is that of linear parameter varying (LPV) systems. This extension of linear systems analysis considers parameterized variations in the dynamical system
matrices (A, B, C, D). Algorithms with variable stepsize are examples of LPV systems.
Some recent work discussing IQCs applied to LPV systems appeared in . Another
possible direction would be to use optimal control techniques directly to choose algorithm
parameters, possibly solving a small SDP at every iteration to choose new assignments.
Algorithm synthesis
Perhaps even more ambitiously than using our framework for
parameter selection, our initial results show that we can use IQCs as a way of designing
new algorithms.
We restricted our attention to algorithms with one-step of memory,
as then we only had to search over 3 parameters. However, new techniques would be
necessary to explore more complicated algorithms. Local search heuristics could be used
here to probe the feasible region of the associated LMIs, but convex methods and convex
relaxations may also be applicable and should be investigated for these searches.
Noise analysis
Our robustness analysis only allows us to consider certain forms of
deterministic noise. Expanding our techniques to study stochastic noise would expand
the applicability of our techniques and could provide new insight into popular stochastic
optimization algorithms such as stochastic coordinate descent and stochastic gradient descent . Many of the most common techniques for proving convergence of stochastic
methods rely on Lyapunov-type arguments, and we may be able to generalize this approach to account for the variety of diﬀerent methods. In order to expand our techniques
to this space, we would need to introduce IQCs that were valid in expectation. Stability
methods from stochastic control may be applicable to such investigations.
Beyond convexity
Since our analysis decouples the derivation of constraints on function classes from the algorithm analysis, it is possible that it can be generalized to nonconvex optimization. If we can characterize the function class by reasonable quadratic
constraints, our framework immediately applies, and may lead to entirely new analyses
for nonconvex function classes. For example, IQCs for saturating nonlinearities are readily available in the controls literature . From a complementary perspective, if we
know that our function is not merely convex, but has additional structure, this can be
incorporated as additional IQCs. With extra constraints, it is possible that we can derive
faster rates or more robustness for smaller function classes.
Non-quadratic Lyapunov functions
There has been substantial work in the past
decade on eﬃcient algorithms to search over non-quadratic Lyapunov functions .
These techniques use sum-of-squares hierarchies to certify that non-quadratic polynomials
are nonnegative, and still reduce to solving small semideﬁnite programming problems.
This more general class of Lyapunov functions could be better matched to certain classes
of functions than quadratics, and we could perhaps analyze more complicated algorithms
and interconnections.
Large-scale composite system analysis
Perhaps the most ambitious goal of this
program is to move beyond convex models and attempt to analyze complicated optimization systems used in science and industry. Powerful modeling languages like AMPL or
GAMS allow for local analysis of large, complex systems, and certifying that the decisions
about these systems are valid and safe would have impact in a variety of ﬁelds including process technology, web-scale analytics, and power management. Since our methods
nicely abstract beyond two interconnected systems, it is our hope that they can be extended to analyze the variety of optimization algorithms deployed to handle large, high
throughput data processing.
Acknowledgments
We would like to thank Peter Seiler for many helpful pointers on time-domain IQCs, Elad
Hazan for his suggestion of how to analyze functions that are not strongly convex, and
Bin Hu for pointing out a misreading of Nesterov’s results in an earlier draft of this paper.
We would also like to thank Ali Jadbabaie, Pablo Parrilo, and Stephen Wright for many
helpful discussions and suggestions.
LL and AP are partially supported by AFOSR award FA9550-12-1-0339 and NASA
Grant No. NRA NNX12AM55A. BR is generously supported by ONR awards N00014-
11-1-0723 and N00014-13-1-0129, NSF award CCF-1148243, AFOSR award FA9550-13-
1-0138, and a Sloan Research Fellowship.
This research was also supported in part
by NSF CISE Expeditions Award CCF-1139158, LBNL Award 7076018, and DARPA
XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, SAP,
The Thomas and Stacey Siebel Foundation, Adobe, Apple, Inc., C3Energy, Cisco, Cloudera, EMC, Ericsson, Facebook, GameOnTalis, Guavus, HP, Huawei, Intel, Microsoft,
NetApp, Pivotal, Splunk, Virdata, Fanuc, VMware, and Yahoo!.