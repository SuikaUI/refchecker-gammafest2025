Mach Learn 77: 27–59
DOI 10.1007/s10994-009-5108-8
Cutting-plane training of structural SVMs
Thorsten Joachims · Thomas Finley ·
Chun-Nam John Yu
Received: 15 October 2007 / Revised: 2 October 2008 / Accepted: 30 January 2009 /
Published online: 9 May 2009
Springer Science+Business Media, LLC 2009
Abstract Discriminative training approaches like structural SVMs have shown much
promise for building highly complex and accurate models in areas like natural language
processing, protein structure prediction, and information retrieval. However, current training algorithms are computationally expensive or intractable on large datasets. To overcome
this bottleneck, this paper explores how cutting-plane methods can provide fast training not
only for classiﬁcation SVMs, but also for structural SVMs. We show that for an equivalent
“1-slack” reformulation of the linear SVM training problem, our cutting-plane method has
time complexity linear in the number of training examples. In particular, the number of iterations does not depend on the number of training examples, and it is linear in the desired
precision and the regularization parameter. Furthermore, we present an extensive empirical
evaluation of the method applied to binary classiﬁcation, multi-class classiﬁcation, HMM
sequence tagging, and CFG parsing. The experiments show that the cutting-plane algorithm
is broadly applicable and fast in practice. On large datasets, it is typically several orders of
magnitude faster than conventional training methods derived from decomposition methods
like SVM-light, or conventional cutting-plane methods. Implementations of our methods are
available at www.joachims.org.
Keywords Structural SVMs · Support vector machines · Structured output prediction ·
Training algorithms
Editor: Tony Jebara.
T. Joachims · T. Finley · C.-N.J. Yu ()
Department of Computer Science, Cornell University, Ithaca, NY, USA
e-mail: 
T. Joachims
e-mail: 
e-mail: 
Mach Learn 77: 27–59
1 Introduction
Consider the problem of learning a function with complex outputs, where the prediction is
not a single univariate response (e.g., 0/1 for classiﬁcation or a real number for regression),
but a complex multivariate object. For example, the desired prediction is a tree in natural
language parsing, or a total ordering in web search, or an alignment between two amino acid
sequences in protein threading. Further instances of such structured prediction problems are
ubiquitous in natural language processing, bioinformatics, computer vision, and many other
application domains.
Recent years have provided intriguing advances in extending methods like Logistic Regression, Perceptrons, and Support Vector Machines (SVMs) to global training of such structured prediction models . In contrast to conventional generative training, these methods are discriminative (e.g., conditional likelihood, empirical risk minimization). Akin to moving from Naive Bayes to an SVM for classiﬁcation, this provides greater
modeling ﬂexibility through avoidance of independence assumptions, and it was shown to
provide substantially improved prediction accuracy in many domains . By eliminating
the need to model statistical dependencies between features, discriminative training enables
us to freely use more complex and possibly interdependent features, which provides the
potential to learn models with improved ﬁdelity. However, training these rich models with
a sufﬁciently large training set is often beyond the reach of current discriminative training
algorithms.
We focus on the problem of training structural SVMs in this paper. Formally, this can
be thought of as solving a convex quadratic program (QP) with a large (typically exponential or inﬁnite) number of constraints. Existing algorithm fall into two groups. The
ﬁrst group of algorithms relies on an elegant polynomial-size reformulation of the training
problem , which is possible for the special case
of margin-rescaling with linearly decomposable loss. These
smaller QPs can then be solved, for example, with general-purpose optimization methods or decomposition methods similar to SMO . Unfortunately, decomposition methods are known to scale super-linearly with
the number of examples , and so do general-purpose optimizers,
since they do not exploit the special structure of this optimization problem. But most signiﬁcantly, the algorithms in the ﬁrst group are limited to applications where the polynomial-size
reformulation exists. Similar restrictions also apply to the extragradient method , which applies only to problems where subgradients of the QP can be computed via
a convex real relaxation, as well as exponentiated gradient methods , which require the ability to compute “marginals” (e.g. via the
sum-product algorithm). The second group of algorithms works directly with the original,
exponentially-sized QP. This is feasible, since a polynomially-sized subset of the constraints
from the original QP is already sufﬁcient for a solution of arbitrary accuracy . Such algorithms either take stochastic subgradient steps , or build a cutting-plane model which
is easy to solve directly . The algorithm in Tsochantaridis et al.
 shows how such a cutting-plane can be constructed efﬁciently. Compared to the subgradient methods, the cutting-plane approach does not take a single gradient step, but always takes an optimal step in the current cutting-plane model. It requires only the existence
of an efﬁcient separation oracle, which makes it applicable to many problems for which no
Mach Learn 77: 27–59
polynomially-sized reformulation is known. In practice, however, the cutting-plane method
of Tsochantaridis et al. is known to scale super-linearly with the number of training
examples. In particular, since the size of the cutting-plane model typically grows linearly
with the dataset size , QPs of increasing size
need to be solved to compute the optimal steps, which leads to the super-linear runtime.
In this paper, we explore an extension of the cutting-plane method presented in Joachims
 for training linear structural SVMs, both in the margin-rescaling and in the slackrescaling formulation . In contrast to the cutting-plane method
presented in Tsochantaridis et al. , we show that the size of the cutting-plane models
and the number of iterations are independent of the number of training examples n. Instead,
their size and the number of iterations can be upper bounded by O( C
ε ), where C is the regularization constant and ε is the desired precision of the solution (see Optimization Problems
OP2 and OP3). Since each iteration of the new algorithm takes O(n) time and memory, it
also scales O(n) overall with the number of training examples both in terms of computation time and memory. Empirically, the size of the cutting-plane models and the QPs that
need to be solved in each iteration is typically very small (less than a few hundred) even for
problems with millions of features and hundreds of thousands of examples.
A key conceptual difference of the new algorithm compared to the algorithm of Tsochantaridis et al. and most other SVM training methods is that not only individual data
points are considered as potential Support Vectors (SVs), but also linear combinations of
those. This increased ﬂexibility allows for solutions with far fewer non-zero dual variables,
and it leads to the small cutting-plane models discussed above.
The new algorithm is applicable to all structural SVM problems where the separation
oracle can be computed efﬁciently, which makes it just as widely applicable as the most
general training algorithms known to date. Even further, following the original publication
in Joachims , Teo et al. have already shown that the algorithm can also be
extended to Conditional Random Field training. We provide a theoretical analysis of the
algorithm’s correctness, convergence rate, and scaling behavior for structured prediction.
Furthermore, we present empirical results for several structured prediction problems (i.e.,
multi-class classiﬁcation, part-of-speech tagging, and natural language parsing), and compare against conventional algorithms also for the special case of binary classiﬁcation. On all
problems, the new algorithm is substantially faster than conventional decomposition methods and cutting-plane methods, often by several orders of magnitude for large datasets.
2 Structural support vector machines
Structured output prediction describes the problem of learning a function
where X is the space of inputs, and Y is the space of (multivariate and structured) outputs.
In the case of natural language parsing, for example, X is the space of sentences, and Y is
the space of trees over a given set of non-terminal grammar symbols. To learn h, we assume
that a training sample of input-output pairs
S = ((x1,y1),...,(xn,yn)) ∈(X × Y)n
Mach Learn 77: 27–59
is available and drawn i.i.d.1 from a distribution P (X,Y). The goal is to ﬁnd a function h
from some hypothesis space H that has low prediction error, or, more generally, low risk
Δ(y,h(x))dP (x,y).
Δ(y, ¯y) is a loss function that quantiﬁes the loss associated with predicting ¯y when y is the
correct output value. Furthermore, we assume that Δ(y,y) = 0 and Δ(y, ¯y) ≥0 for y ̸= ¯y.
We follow the Empirical Risk Minimization Principle to infer a function h
from the training sample S. The learner evaluates the quality of a function h ∈H using the
empirical risk RΔ
S (h) on the training sample S.
Δ(yi,h(xi)).
Support Vector Machines select an h ∈H that minimizes a regularized empirical risk on S.
For conventional binary classiﬁcation where Y = {−1,+1}, SVM training is typically formulated as the following convex quadratic optimization problem2 .
Optimization Problem 1 (Classiﬁcation SVM (primal))
∀i ∈{1,...,n} : yi(wT xi) ≥1 −ξi.
It was shown that SVM training can be generalized to structured outputs , leading to an optimization problem that
is similar to multi-class SVMs and extending the Perceptron
approach described in Collins . The idea is to learn a discriminant function f : X ×
Y →ℜover input/output pairs from which one derives a prediction by maximizing f over
all y ∈Y for a speciﬁc given input x.
hw(x) = argmax
We assume that fw(x,y) takes the form of a linear function
fw(x,y) = wT Ψ (x,y),
where w ∈ℜN is a parameter vector and Ψ (x,y) is a feature vector relating input x and
output y. Intuitively, one can think of fw(x,y) as a compatibility function that measures
how well the output y matches the given input x. The ﬂexibility in designing Ψ allows us to
employ SVMs to learn models for problems as diverse as natural language parsing 77: 27–59
et al. 2004; Tsochantaridis et al. 2004), protein sequence alignment , learning
ranking functions that optimize IR performance measures , and segmenting
images .
For training the weights w of the linear discriminant function, the standard SVM optimization problem can be generalized in several ways . This paper uses the formulations given
in Tsochantaridis et al. , which subsume all other approaches. We refer to these as the
“n-slack” formulations, since they assign a different slack variable to each of the n training
examples. Tsochantaridis et al. identify two different ways of using a hinge loss to
convex upper bound the loss, namely “margin-rescaling” and “slack-rescaling”. In marginrescaling, the position of the hinge is adapted while the slope is ﬁxed,
ΔMR(y,hw(x)) = max
¯y∈Y {Δ(y, ¯y) −wT Ψ (x,y) + wT Ψ (x, ¯y)} ≥Δ(y,hw(x))
while in slack-rescaling, the slope is adjusted while the position of the hinge is ﬁxed.
ΔSR(y,hw(x)) = max
¯y∈Y {Δ(y, ¯y)(1 −wT Ψ (x,y) + wT Ψ (x, ¯y))} ≥Δ(y,hw(x)).
This leads to the following two training problems, where each slack variable ξi is equal to
the respective ΔMR(yi,hw(xi)) or ΔSR(yi,hw(xi)) for training example (xi,yi).
Optimization Problem 2 (n-Slack Structural SVM with Margin-Rescaling (primal))
∀¯y1 ∈Y : wT [Ψ (x1,y1) −Ψ (x1, ¯y1)] ≥Δ(y1, ¯y1) −ξ1,
∀¯yn ∈Y : wT [Ψ (xn,yn) −Ψ (xn, ¯yn)] ≥Δ(yn, ¯yn) −ξn.
Optimization Problem 3 (n-Slack Structural SVM with Slack-Rescaling (primal))
∀¯y1 ∈Y : wT [Ψ (x1,y1) −Ψ (x1, ¯y1)] ≥1 −
Δ(y1, ¯y1),
∀¯yn ∈Y : wT [Ψ (xn,yn) −Ψ (xn, ¯yn)] ≥1 −
Δ(yn, ¯yn).
The objective is the conventional regularized risk used in SVMs. The constraints state
that for each training example (xi,yi), the score wT Ψ (xi,yi) of the correct structure yi
must be greater than the score wT Ψ (xi, ¯y) of all incorrect structures ¯y by a required margin.
This margin is 1 in slack-rescaling, and equal to the loss Δ(yi, ¯y) in margin rescaling. If the
margin is violated, the slack variable ξi of the example becomes non-zero. Note that ξi is
Mach Learn 77: 27–59
Algorithm 1 for training Structural SVMs (with margin-rescaling) via the n-Slack Formulation (OP2).
1: Input: S = ((x1,y1),...,(xn,yn)), C, ε
2: Wi ←∅, ξi ←0 for all i = 1,...,n
for i = 1,...,n do
ˆy ←argmax ˆy∈Y{Δ(yi, ˆy) −wT [Ψ (xi,yi) −Ψ (xi, ˆy)]}
if Δ(yi, ˆy) −wT [Ψ (xi,yi) −Ψ (xi, ˆy)] > ξi + ε then
Wi ←Wi ∪{ ˆy}
(w,ξ) ←argminw,ξ≥0
s.t. ∀¯y1 ∈W1 : wT [Ψ (x1,y1) −Ψ (x1, ¯y1)] ≥Δ(y1, ¯y1) −ξ1
∀¯yn ∈Wn : wT [Ψ (xn,yn) −Ψ (xn, ¯yn)] ≥Δ(yn, ¯yn) −ξn
11: until no Wi has changed during iteration
12: return(w,ξ)
shared among constraints from the same example. The correct labels yi’s are not excluded
from the constraints because they correspond to non-negativity constraints on the slack variables ξi. It is easy to verify that for both margin-rescaling and for slack-rescaling, ξi is an
upper bound on the empirical risk RΔ
S (h) on the training sample S.
It is not immediately obvious that Optimization Problems OP2 and OP3 can be solved
efﬁciently since they have O(n|Y|) constraints. |Y| is typically extremely large (e.g., all
possible alignments of two amino-acid sequence) or even inﬁnite (e.g., real-valued outputs).
For the special case of margin-rescaling with linearly decomposable loss functions Δ, Taskar
et al. have shown that the problem can be reformulated as a quadratic program with
only a polynomial number of constraints and variables.
A more general algorithm that applies to both margin-rescaling and slack-rescaling under
a large variety of loss functions was given in Tsochantaridis et al. . The algorithm relies on the theoretical result that for any desired precision ε, a greedily constructed
cutting-plane model of OP2 and OP3 requires only O( n
ε2 ) many constraints . This greedy algorithm for the case of margin-rescaling is Algorithm 1, for slack-rescaling it leads to Algorithm 2. The algorithms iteratively construct a
working set W = W1 ∪··· ∪Wn of constraints, starting with an empty working set W = ∅.
The algorithms iterate through the training examples and ﬁnd the constraint that is violated
most by the current solution w,ξ (Line 5). If this constraint is violated by more than the
desired precision ε (Line 6), the constraint is added to the working set (Line 7) and the QP
is solved over the extended W (Line 8). The algorithms terminate when no constraint is
added in the previous iteration, meaning that all constraints in OP2 or OP3 are fulﬁlled up to
a precision of ε. The algorithm is provably efﬁcient whenever the most violated constraint
can be found efﬁciently. The procedure in Line 5 for ﬁnding the most violated constraint
is called the “separation oracle”. The argmax in Line 5 has an efﬁcient solution for a wide
variety of choices for Ψ , Y, and Δ , and often it involves the same algorithm for making predictions (see (1)).
Related to Algorithm 1 is the method proposed in Anguelov et al. , which applies
to the special case where the argmax in Line 5 can be computed as a linear program. This
Mach Learn 77: 27–59
Algorithm 2 for training Structural SVMs (with slack-rescaling) via the n-Slack Formulation (OP3).
1: Input: S = ((x1,y1),...,(xn,yn)), C, ε
2: Wi ←∅, ξi ←0 for all i = 1,...,n
for i = 1,...,n do
ˆy ←argmax ˆy∈Y{Δ(yi, ˆy)(1 −wT [Ψ (xi,yi) −Ψ (xi, ˆy)])}
if Δ(yi, ˆy)(1 −wT [Ψ (xi,yi) −Ψ (xi, ˆy)]) > ξi + ε then
Wi ←Wi ∪{ ˆy}
(w,ξ) ←argminw,ξ≥0
s.t. ∀¯y1 ∈W1 : wT Δ(y1, ¯y1)[Ψ (x1,y1)−Ψ (x1, ¯y1)] ≥Δ(y1, ¯y1)−ξ1
∀¯yn ∈Wn : wT Δ(yn, ¯yn)[Ψ (xn,yn)−Ψ (xn, ¯yn)] ≥Δ(yn, ¯yn)−ξn
11: until no Wi has changed during iteration
12: return(w,ξ)
allows them not to explicitly maintain a working set, but implicitly represent it by folding
n linear programs into the quadratic program OP2. To this special case also applies the
method of Taskar et al. , which casts the training of max-margin structured predictors
as a convex-concave saddle-point problem. It provides improved scalability compared to an
explicit reduction to a polynomially-sized QP, but involves the use of a special min-cost
quadratic ﬂow solver in the projection steps of the extragradient method.
Exponentiated gradient methods, originally proposed for online learning of linear predictors , have also been applied to the training of structured
predictors . They solve the optimization problem
in the dual, and treat conditional random ﬁeld and structural SVM within the same framework using Bregman divergences. Stochastic gradient methods 
have been applied to the training of conditional random ﬁeld on large scale problems, and
exhibit faster rate of convergence than BFGS methods. Recently, subgradient methods and
their stochastic variants have also been proposed to solve the optimization problem in max-margin structured prediction. While not yet explored for structured
prediction, the PEGASOS algorithm has shown promising performance for binary classiﬁcation SVMs. Related to such online methods is also the MIRA
algorithm , which has been used for training structured predictors . However, to deal with the exponential size of Y, heuristics
have to be used (e.g. only using a k-best subset of Y), leading to only approximate solutions
of Optimization Problem OP2.
3 Training algorithm
While polynomial runtime was established for most algorithms discussed above, training
general structural SVMs on large-scale problems is still a challenging problem. In the following, we present an equivalent reformulation of the training problems for both marginrescaling and slack-rescaling, leading to a cutting-plane training algorithm that has not only
Mach Learn 77: 27–59
provably linear runtime in the number of training examples, but is also several orders of
magnitude faster than conventional cutting-plane methods on
large-scale problems. Nevertheless, the new algorithm is equally general as Algorithms 1
3.1 1-slack formulation
The ﬁrst step towards the new algorithm is a reformulation of the optimization problems for
training. The key idea is to replace the n cutting-plane models of the hinge loss—one for
each training example—with a single cutting plane model for the sum of the hinge-losses.
Since there is only a single slack variable in the new formulations, we refer to them the
“1-slack” formulations.
Optimization Problem 4 (1-Slack Structural SVM with Margin-Rescaling (primal))
2wT w + Cξ
∀( ¯y1,..., ¯yn) ∈Yn : 1
Ψ (xi,yi) −Ψ (xi, ¯yi)
Δ(yi, ¯yi) −ξ.
Optimization Problem 5 (1-Slack Structural SVM with Slack-Rescaling (primal))
2wT w + Cξ
∀( ¯y1,..., ¯yn) ∈Yn : 1
Δ(yi, ¯yi)
Ψ (xi,yi) −Ψ (xi, ¯yi)
Δ(yi, ¯yi) −ξ.
While OP4 has |Y|n constraints, one for each possible combination of labels
( ¯y1,..., ¯yn) ∈Yn, it has only one slack variable ξ that is shared across all constraints.
Each constraint corresponds to a tangent to RΔMR
(h) and RΔSR
(h) respectively, and the set
of constraints forms an equivalent model of the risk function. Speciﬁcally, the following
theorems show that ξ ∗= RΔMR
(hw∗) at the solution (w∗,ξ ∗) of OP4, and ξ ∗= RΔSR
the solution (w∗,ξ ∗) of OP5, since the n-slack and the 1-slack formulations are equivalent
in the following sense.
Theorem 1 (Equivalence of OP2 and OP4) Any solution w∗of OP4 is also a solution of
OP2 (and vice versa), with ξ ∗= 1
Proof Generalizing the proof in Joachims , we will show that both optimization problems have the same objective value and an equivalent set of constraints. In particular, for
every w the smallest feasible ξ and n
i ξi are equal.
For a given w, each ξi in OP2 can be optimized individually, and the smallest feasible ξi
given w is achieved for
¯yi∈Y{Δ(yi, ¯yi) −wT [Ψ (xi,yi) −Ψ (xi, ¯yi)]}.
Mach Learn 77: 27–59
For OP4, the smallest feasible ξ for a given w is
( ¯y1,..., ¯yn)∈Yn
Δ(yi, ¯yi) −wT 1
Ψ (xi,yi) −Ψ (xi, ¯yi)
Since the function can be decomposed linearly in ¯yi, for any given w, each ¯yi can be optimized independently.
nΔ(yi, ¯yi) −1
nwT [Ψ (xi,yi) −Ψ (xi, ¯yi)]
Therefore, the objective functions of both optimization problems are equal for any w given
the corresponding smallest feasible ξ and ξi. Consequently this is also true for w∗and its
corresponding smallest feasible slacks ξ ∗and ξ ∗
Theorem 2 (Equivalence of OP3 and OP5) Any solution w∗of OP5 is also a solution of
OP3 (and vice versa), with ξ ∗= 1
Proof Analogous to Theorem 1.
3.2 Cutting-plane algorithm
What could we possibly have gained by moving from the n-slack to the 1-slack formulation, exponentially increasing the number of constraints in the process? We will show in
the following that the dual of the 1-slack formulation has a solution that is extremely sparse,
with the number of non-zero dual variables independent of the number of training examples. To ﬁnd this solution, we propose Algorithms 3 and 4, which are generalizations of the
algorithm in Joachims to structural SVMs.
Similar to the cutting-plane algorithms for the n-slack formulations, Algorithms 3 and 4
iteratively construct a working set W of constraints. In each iteration, the algorithms compute the solution over the current W (Line 4), ﬁnd the most violated constraint (Lines 5–7),
and add it to the working set. The algorithm stops once no constraint can be found that is
violated by more than the desired precision ε (Line 9). Unlike in the n-slack algorithms,
Algorithm 3 for training Structural SVMs (with margin-rescaling) via the 1-Slack Formulation (OP4).
1: Input: S = ((x1,y1),...,(xn,yn)), C, ε
(w,ξ) ←argminw,ξ≥0
2wT w + Cξ
s.t. ∀( ¯y1,..., ¯yn) ∈W : 1
i=1[Ψ (xi,yi)−Ψ (xi, ¯yi)] ≥1
i=1 Δ(yi, ¯yi)−ξ
for i = 1,...,n do
ˆyi ←argmax ˆy∈Y{Δ(yi, ˆy) + wT Ψ (xi, ˆy)}
W ←W ∪{( ˆy1,..., ˆyn)}
9: until 1
i=1 Δ(yi, ˆyi) −1
i=1[Ψ (xi,yi) −Ψ (xi, ˆyi)] ≤ξ + ε
10: return(w,ξ)
Mach Learn 77: 27–59
Algorithm 4 for training Structural SVMs (with slack-rescaling) via the 1-Slack Formulation (OP5).
1: Input: S = ((x1,y1),...,(xn,yn)), C, ε
(w,ξ) ←argminw,ξ≥0
2wT w + Cξ
s.t. ∀( ¯y1,..., ¯yn) ∈W :
i=1 Δ(yi, ¯yi)[Ψ (xi,yi) −Ψ (xi, ¯yi)] ≥1
i=1 Δ(yi, ¯yi) −ξ
for i = 1,...,n do
ˆyi ←argmax ˆy∈Y{Δ(yi, ˆy)(1 −wT [Ψ (xi,yi) −Ψ (xi, ˆy)])}
W ←W ∪{( ˆy1,..., ˆyn)}
9: until 1
i=1 Δ(yi, ˆyi) −1
i=1 Δ(yi, ˆyi)[Ψ (xi,yi) −Ψ (xi, ˆyi)] ≤ξ + ε
10: return(w,ξ)
only a single constraint is added in each iteration. The following theorems characterize the
quality of the solutions returned by Algorithms 3 and 4.
Theorem 3 (Correctness of Algorithm 3) For any training sample S = ((x1,y1),...,
(xn,yn)) and any ε > 0, if (w∗,ξ ∗) is the optimal solution of OP4, then Algorithm 3 returns a point (w,ξ) that has a better objective value than (w∗,ξ ∗), and for which (w,ξ +ε)
is feasible in OP4.
Proof We ﬁrst verify that Lines 5–7 in Algorithm 3 compute the vector ( ˆy1,..., ˆyn) ∈Yn
that maximizes
( ˆy1,..., ˆyn)∈Yn
Δ(yi, ˆyi) −1
Ψ (xi,yi) −Ψ (xi, ˆyi)
ξ ′ is the minimum value needed to fulﬁll all constraints in OP4 for the current w. The
maximization problem is linear in the ˆyi, so one can maximize over each ˆyi independently.
Δ(yi, ˆy) −wT 
Ψ (xi,yi) −Ψ (xi, ˆy)
wT Ψ (xi,yi) + 1
Δ(yi, ˆy) + wT Ψ (xi, ˆy)
Since the ﬁrst sum in (4) is constant, the second term directly corresponds to the assignment
in Line 6. As checked in Line 9, the algorithm terminates only if ξ ′ does not exceed the ξ
from the solution over W by more than ε as desired.
Since the (w,ξ) returned by Algorithm 3 is the solution on a subset of the constraints
from OP4, it holds that 1
2w∗T w∗+ Cξ ∗≥1
2wT w + Cξ.
Theorem 4 (Correctness of Algorithm 4) For any training sample S = ((x1,y1),...,
(xn,yn)) and any ε > 0, if (w∗,ξ ∗) is the optimal solution of OP5, then Algorithm 4 returns a point (w,ξ) that has a better objective value than (w∗,ξ ∗), and for which (w,ξ +ε)
is feasible in OP5.
Mach Learn 77: 27–59
Proof Analogous to the proof of Theorem 3.
Using a stopping criterion based on the accuracy of the empirical risk ξ ∗is very intuitive and practically meaningful, unlike the stopping criteria typically used in decomposition
methods. Intuitively, ε can be used to indicate how close one wants to be to the empirical
risk of the best parameter vector. In most machine learning applications, tolerating a training error that is suboptimal by 0.1% is very acceptable. This intuition makes selecting the
stopping criterion much easier than in other training methods, where it is usually deﬁned
based on the accuracy of the Kuhn-Tucker Conditions of the dual .
Nevertheless, it is easy to see that ε also bounds the duality gap of the solution by Cε.
Solving the optimization problems to an arbitrary but ﬁxed precision of ε is essential in our
analysis below, making sure that computation time is not wasted on computing a solution
that is more accurate than necessary.
We next analyze the time complexity of Algorithms 3 and 4. It is easy to see that each
iteration of the algorithm takes n calls to the separation oracle, and that for the linear kernel
the remaining work in each iteration scales linearly with n as well. We show next that the
number of iterations until convergence is bounded, and that this upper bound is independent
The argument requires the Wolfe-dual programs, which are straightforward to derive (see
Appendix). For a more compact notation, we denote vectors of labels as ¯y = ( ¯y1,..., ¯yn) ∈
Yn. For such vectors of labels, we then deﬁne Δ(¯y) and the inner product HMR(¯y, ¯y′) as
follows. Note that yi and yj denote correct training labels, while ¯yi and ¯y′
j denote arbitrary
Δ(yi, ¯yi),
HMR(¯y, ¯y′) = 1
Ψ (xi,yi)T Ψ (xj,yj) −Ψ (xi,yi)T Ψ (xj, ¯y′
−Ψ (xi, ¯yi)T Ψ (xj,yj) + Ψ (xi, ¯yi)T Ψ (xj, ¯y′
The inner products Ψ (x,y)T Ψ (x′,y′) are computed either explicitly or via a Kernel
K(x,y,x′,y′) = Ψ (x,y)T Ψ (x′,y′). Note that it is typically more efﬁcient to compute
HMR(¯y, ¯y′) = 1
(Ψ (xi,yi) −Ψ (xi, ¯yi))
(Ψ (xj,yj) −Ψ (xj, ¯y′
if no kernel is used. The dual of the 1-slack formulation for margin-rescaling is:
Optimization Problem 6 (1-Slack Structural SVM with Margin-Rescaling (dual))
α≥0 D(α) =
Δ(¯y)α¯y −1
α¯yα¯y′HMR(¯y, ¯y′)
Mach Learn 77: 27–59
For the case of slack-rescaling, the respective H(¯y, ¯y′) is as follows. There is an analogous factorization that is more efﬁcient to compute if no kernel is used:
HSR(¯y, ¯y′) = 1
Δ(yi, ¯yi)Δ(yj, ¯y′
Ψ (xi,yi)T Ψ (xj,yj) −Ψ (xi,yi)T Ψ (xj, ¯y′
−Ψ (xi, ¯yi)T Ψ (xj,yj) + Ψ (xi, ¯yi)T Ψ (xj, ¯y′
Δ(yi, ¯yi)(Ψ (xi,yi) −Ψ (xi, ¯yi))
j)(Ψ (xj,yj) −Ψ (xj, ¯y′
The dual of the 1-slack formulation is:
Optimization Problem 7 (1-Slack Structural SVM with Slack-Rescaling (dual))
α≥0 D(α) =
Δ(¯y)α¯y −1
α¯yα¯y′HSR(¯y, ¯y′)
Using the respective dual solution α∗, one can compute inner products with the weight
vector w∗solving the primal via
w∗T Ψ (x,y) =
Ψ (x,y)T Ψ (xj,yj) −Ψ (x,y)T Ψ (xj, ¯yj)
Ψ (xj,yj) −Ψ (xj, ¯yj)
for margin-rescaling and via
w∗T Ψ (x,y) =
Δ(yj, ¯yj)
Ψ (x,y)T Ψ (xj,yj) −Ψ (x,y)T Ψ (xj, ¯yj)
Δ(yj, ¯yj)
Ψ (xj,yj) −Ψ (xj, ¯yj)
for slack-rescaling. We will show in the following that only a small (i.e., polynomial) number
of the α¯y is non-zero at the solution. In analogy to classiﬁcation SVMs, we will refer to
those ¯y with non-zero α¯y as Support Vectors. However, note that Support Vectors in the
1-slack formulation are linear combinations of multiple examples. We can now state the
theorem giving an upper bound on the number of iterations of the 1-slack algorithms. The
proof extends the one in Joachims to general structural SVMs, and is based on the
Mach Learn 77: 27–59
technique introduced in Joachims and generalized in Tsochantaridis et al. .
The ﬁnal step of the proof uses an improvement developed in Teo et al. .
Theorem 5 (1-Slack Margin-Rescaling SVM Iteration Complexity) For any 0 < C,
0 < ε ≤4R2C and any training sample S = ((x1,y1),...,(xn,yn)), Algorithm 3 terminates
after at most
iterations. R2 = maxi, ¯y ∥Ψ (xi,yi) −Ψ (xi, ¯y)∥2, Δ = maxi, ¯y Δ(yi, ¯y), and ⌈..⌉is the integer
ceiling function.
Proof We will show that adding each new constraint to W increases the objective value at
the solution of the quadratic program in Line 4 by at least some constant positive value.
Since the objective value of the solution of OP6 is upper bounded by CΔ (since w = 0 and
ξ = Δ is a feasible point in the primal), the algorithm can only perform a constant number
of iterations before termination. The amount by which the solution increases by adding one
constraint that is violated by more than ε (i.e., the criteria in Line 9 of Algorithm 3 and
Algorithm 4) to W can be lower bounded as follows.
Let ˆy be the newly added constraint and let α be the solution of the dual before the
addition. To lower bound the progress made by the algorithm in each iteration, consider the
increase in the dual that can be achieved with a line search
0≤β≤C {D(α + βη)} −D(α).
The direction η is constructed by setting ηˆy = 1 and η¯y = −1
C α¯y for all other ¯y. Note that
the constraints on β and the construction of η ensure that α + βη never leaves the feasible
region of the dual.
To apply Lemma 2 (see Appendix) for computing the progress made by a line search, we
need a lower bound for ∇D(α)T η and an upper bound for ηT Hη. Starting with the lower
bound for ∇D(α)T η, note that
α¯y′HMR(¯y, ¯y′) = ξ
for all ¯y with non-zero α¯y at the solution over the previous working set W. For the newly
added constraint ˆy and some γ > 0,
α¯y′HMR(ˆy, ¯y′) = ξ + γ ≥ξ + ε
by construction due to Line 9 of Algorithms 3. It follows that
∇D(α)T η = ξ + γ −
Mach Learn 77: 27–59
The following gives an upper bound for ηT Hη, where H¯y¯y′ = HMR(¯y, ¯y′) for ¯y, ¯y′ ∈W ∪{ˆy}.
ηT Hη = HMR(ˆy, ˆy) −2
α¯yHMR(¯y, ˆy) + 1
α¯yα¯y′HMR(¯y, ¯y′)
The bound uses that −R2 ≤HMR(¯y, ˆy) ≤R2.
Plugging everything into the bound of Lemma 2 shows that the increase of the objective
is at least
0≤β≤C {D(α + βη)} −D(α) ≥min
Note that the ﬁrst case applies whenever γ ≥4R2C, and that the second case applies otherwise.
The ﬁnal step of the proof is to use this constant increase of the objective value in each
iteration to bound the maximum number of iterations. First, note that α¯y = 0 for all incorrect
vectors of labels ¯y and α¯y∗= C for the correct vector of labels ¯y∗= (y1,...,yn) is a feasible
starting point α0 with a dual objective of 0. This means the initial optimality gap δ(0) =
D(α∗) −D(α0) is at most CΔ, where α∗is the optimal dual solution. An optimality gap
of δ(i) = D(α∗) −D(αi) ensures that there exists a constraint that is violated by at least
C . This means that the ﬁrst case of (20) applies while δ(i) ≥4R2C2, leading to a
decrease in the optimality gap of at least
δ(i + 1) ≤δ(i) −1
in each iteration. Starting from the worst possible optimality gap of δ(0) = CΔ, the algorithm needs at most
iterations until it has reached an optimality gap of δ(i1) ≤4R2C2, where the second case of
(20) becomes valid. As proposed in Teo et al. , the recurrence equation
δ(i + 1) ≤δ(i) −
8R2C2 δ(i)2
for the second case of (20) can be upper bounded by solving the differential equation ∂δ(i)
8R2C2 δ(i)2 with boundary condition δ(0) = 4R2C2. The solution is δ(i) ≤8R2C2
i+2 , showing
that the algorithms does not need more than
iterations until it reaches an optimality gap of Cε when starting at a gap of 4R2C2, where ε
is the desired target precision given to the algorithm. Once the optimality gap reaches Cε, it
Mach Learn 77: 27–59
is no longer guaranteed that an ε-violated constraint exists. However, such constraints may
still exist and so the algorithm does not yet terminate. But since each such constraint leads
to an increase in the dual objective of at least
8R2 , only
can be added before the optimality gap becomes negative. The overall bound results from
adding i1, i2, and i3.
Note that the proof of the theorem requires only a line search in each step, while Algorithm 4 actually computes the full QP solution. This suggests the following. On the one
hand, the actual number of iterations in Algorithm 4 might be substantially smaller in practice than what is predicted by the bound. On the other hand, it suggests a variant of Algorithm 4, where the QP solver is replaced by a simple line search. This may be beneﬁcial in
structured prediction problems where the separation oracle in Line 6 is particularly cheap to
Theorem 6 (1-Slack Slack-Rescaling SVM Iteration Complexity) For any 0 < C, 0 < ε ≤
4Δ2R2C and any training sample S = ((x1,y1),...,(xn,yn)), Algorithm 4 terminates after
iterations. R2 = maxi, ¯y ∥Ψ (xi,yi) −Ψ (xi, ¯y)∥2, Δ = maxi, ¯y Δ(yi, ¯y), and ⌈..⌉is the integer
ceiling function.
Proof The proof for the case of slack-rescaling is analogous. The only difference is that
−Δ2R2 ≤HSR( ¯y, ¯y′) ≤Δ2R2.
ε ) convergence rate in the bound is tight, as the following example shows. Consider a multi-class classiﬁcation problem with inﬁnitely many classes Y = {1,...,∞} and
a feature space X = ℜthat contains only one feature. This problem can be encoded using a
feature map Ψ (x,y) which takes value x in position y and 0 everywhere else. For a training
set with a single training example (x,y) = ((1),1) and using the zero/one-loss, the 1-slack
quadratic program for both margin-rescaling and slack-rescaling is
2wT w + Cξ
wT [Ψ (x,1) −Ψ (x,2)] ≥1 −ξ,
wT [Ψ (x,1) −Ψ (x,3)] ≥1 −ξ,
wT [Ψ (x,1) −Ψ (x,4)] ≥1 −ξ,
Let’s assume without loss of generaltity that Algorithm 3 (or equivalently Algorithm 4)
introduces the ﬁrst constraint in the ﬁrst iteration. For C ≥1
2 the solution over this working
Mach Learn 77: 27–59
set is wT = ( 1
2,0,0,...) and ξ = 0. All other constraints are now violated by 1
one of them is selected at random to be added to the working set in the next iteration. It
is easy to verify that after adding k constraints, the solution over the working set is wT =
k+1,0,0,...) for C ≥1
2, and all constraints outside the working set are
violated by ε =
k+1. It therefore takes O( 1
ε ) iterations to reach a desired precision of ε.
The O(C) scaling with C is tight as well, at least for small values of C. For C ≤1
solution over the working set after adding k constraints is wT = (C,−C
k ,0,0,...).
This means that after k constraints, all constraints outside the working set are violated by
k . Consequently, the bounds in (10) and (26) accurately reﬂect the scaling with C up to
the log-term for C ≤1
The following theorem summarizes our characterization of the time complexity of the
1-slack algorithms. In real applications, however, we will see that Algorithm 3 scales much
better than what is predicted by these worst-case bounds both w.r.t. C and ε. Note that a
“support vector” (i.e. point with non-zero dual variable) no longer corresponds to a single
data point in the 1-slack dual, but is typically a linear combination of data points.
Corollary 1 (Time Complexity of Algorithms 3 and 4 for Linear Kernel) For any n training examples S = ((x1,y1),...,(xn,yn)) with maxi, ¯y ∥Ψ (xi,yi) −Ψ (xi, ¯y)∥2 ≤R2 < ∞
and maxi, ¯y Δ(yi, ¯y) ≤Δ < ∞for all n, the 1-slack cutting plane Algorithms 3 and 4 with
constant ε and C using the linear kernel
• require at most O(n) calls to the separation oracle,
• require at most O(n) computation time outside the separation oracle,
• ﬁnd a solution where the number of support vectors (i.e. the number of non-zero dual
variables in the cutting-plane model) does not depend on n,
for any ﬁxed value of C > 0 and ε > 0.
Proof Theorems 5 and 6 show that the algorithms terminate after a constant number of
iterations does not depend on n. Since only one constraint is introduced in each iteration,
the number of support vectors is bounded by the number of iterations. In each iteration, the
algorithm performs exactly n calls to the separation oracle, which proves the ﬁrst statement.
Similarly, the QP that is solved in each iteration is of constant size and therefore requires
only constant time. It is easily veriﬁed that the remaining operations in each iteration can be
done in time O(n) using (7) and (9).
We further discuss the time complexity for the case of kernels in the following section. Note that the linear-time algorithm proposed in Joachims for training binary
classiﬁcation SVMs is a special case of the 1-slack methods developed here. For binary
classiﬁcation X = ℜN and Y = {−1,+1}. Plugging
Ψ (x,y) = 1
if y = y′,
into either n-slack formulation OP2 or OP3 produces the standard SVM optimization problem OP1. The 1-slack formulations and algorithms are then equivalent to those in Joachims
 . However, the O( 1
ε ) bound on the maximum number of iterations derived here is
tighter than the O( 1
ε2 ) bound in Joachims . Using a similar argument, it can also be
shown the ordinal regression method in Joachims is a special case of the 1-slack
algorithm.
Mach Learn 77: 27–59
3.3 Kernels and low-rank approximations
For problems where a (non-linear) kernel is used, the computation time in each iteration is
O(n2) instead of O(n), since (7) and (9) no longer apply. However, the 1-slack algorithm
can easily exploit rank-k approximations, which we will show reduces the computation time
outside of the separation oracle from O(n2) to O(nk + k3). Let (x′
1),...,(x′
set of basis functions so that the subspace spanned by Ψ (x′
1),...,Ψ (x′
k) (approximately) contains the solution w∗of OP4 and OP5 respectively. Algorithms for ﬁnding such
approximations have been suggested in Keerthi et al. ; Fukumizu et al. ; Smola
and Schölkopf for classiﬁcations SVMs, and at least some of them can be extended
to structural SVMs as well. In the simplest case, the set of k basis functions can be chosen
randomly from the set of training examples.
For a kernel K(.) and the resulting Gram matrix K with Kij = Ψ (x′
j), we can compute the inverse L−1 of the Cholesky Decomposition L of K
in time O(k3). Assuming that w∗actually lies in the subspace, we can equivalently rewrite
the 1-slack optimization problems as
Optimization Problem 8 (1-Slack Structural SVM with Margin-Rescaling and k Basis
Functions (primal))
2βT β + Cξ
∀( ¯y1,..., ¯yn) ∈Yn :
K(xi,yi,x′
1) −K(xi, ¯yi,x′
K(xi,yi,x′
k) −K(xi, ¯yi,x′
Δ(yi, ¯yi) −ξ.
Optimization Problem 9 (1-Slack Structural SVM with Slack-Rescaling and k Basis Functions (primal))
2βT β + Cξ
∀( ¯y1,..., ¯yn) ∈Yn :
Δ(yi, ¯yi)
K(xi,yi,x′
1) −K(xi, ¯yi,x′
K(xi,yi,x′
k) −K(xi, ¯yi,x′
Δ(yi, ¯yi) −ξ.
Intuitively, the values of the kernel K(.) with each of the k basis functions form a new
feature vector Ψ ′(x,y)T = (K(x,y,x′
1),...,K(x,y,x′
k))T describing each example
(x,y). After multiplication with L−1, OP8 and OP9 become identical to a problem with
linear kernel and k features, and it is straightforward to see that Algorithms 3 and 4 apply to
this new representation.
Corollary 2 (Time Complexity of Algorithms 3 and 4 for Non-Linear Kernel) For any
n training examples S = ((x1,y1),...,(xn,yn)) with maxi, ¯y ∥Ψ (xi,yi) −Ψ (xi, ¯y)∥2 ≤
R2 < ∞and maxi, ¯y Δ(yi, ¯y) ≤Δ < ∞for all n, the 1-slack cutting plane Algorithms 3
and 4 using a non-linear kernel
Mach Learn 77: 27–59
• require at most O(n) calls to the separation oracle,
• require at most O(n2) computation time outside the separation oracle,
• require at most O(nk + k3) computation time outside the separation oracle, if a set of
k ≤n basis functions is used,
• ﬁnd a solution where the number of support vectors does not depend on n,
for any ﬁxed value of C > 0 and ε > 0.
Proof The proof is analogous to that of Corollary 1. For the low-rank approximation, note
that it is more efﬁcient to once compute wT = βT L−1 before entering the loop in Line 5, than
to compute L−1Ψ ′(x,y) for each example. k3 is the cost of the Cholesky Decomposition,
but this needs to be computed only once.
4 Implementation
We implemented both the n-slack algorithms and the 1-slack algorithms in a software package called SVMstruct, which we make publicly available for download at 
joachims.org. SVMstruct uses SVM-light as the optimizer for solving the QP sub-problems.
Users may adapt SVMstruct to their own structural learning tasks by implementing API functions corresponding to task-speciﬁc Ψ , Δ, separation oracle, and inference. User API functions are in C. A popular extension is SVMpython, which allows users to write API functions
in Python instead, and eliminates much of the drudge work of C including model serialization/deserialization and memory management.
An efﬁcient implementation of the algorithms required a variety of design decisions,
which are summarized in the following. These design decisions have a substantial inﬂuence
on the practical efﬁciency of the algorithms.
Restarting the QP Sub-Problem Solver from the Previous Solution
Instead of solving each
QP subproblem from scratch, we restart the optimizer from the dual solution of the previous
working set as the starting point. This applies to both the n-slack and the 1-slack algorithms.
Batch Updates for the n-Slack Algorithm
Algorithm 1 recomputes the solution of the QP
sub-problem after each update to the working set. While this allows the algorithm to potentially ﬁnd better constraints to be added in each step, it requires a lot of time in the QP
solver. We found that it is more efﬁcient to wait with recomputing the solution of the QP
sub-problem until 100 constraints have been added.
Managing the Accuracy of the QP Sub-Problem Solver
In the initial iterations, a relatively
low precision solution of the QP sub-problems is sufﬁcient for identifying the next violated
constraint to add to the working set. We therefore adjust the precision of the QP sub-problem
optimizer throughout the optimization process for all algorithms.
Removing Inactive Constraints from the Working Set
For both the n-slack and the 1-slack
algorithm, constraints that were added to the working set in early iterations often become
inactive later in the optimization process. These constraints can be removed without affecting the theoretical convergence guarantees of the algorithm, leading to smaller QP’s being
solved in each iteration. At the end of each iteration, we therefore remove constraints from
the working set that have not been active in the last 50 QP sub-problems.
Mach Learn 77: 27–59
Caching Ψ (xi,yi) −Ψ (xi, ˆyi) in the 1-Slack Algorithm
If the separation oracle returns a
label ˆyi for an example xi, the constraint added in the n-slack algorithm ensures that this
label will never again produce an ε-violated constraint in a subsequent iteration. This is
different, however, in the 1-slack algorithm, where the same label can be involved in an
ε-violated constraint over and over again. We therefore cache the f most recently used
Ψ (xi,yi) −Ψ (xi, ˆyi) for each training example xi (typically f = 10 in the following experiments). Let’s denote the cache for example xi with Ci. Instead of asking the separation
oracle in every iteration, the algorithm ﬁrst tries to construct a sufﬁciently violated constraint
from the caches via
for i = 1,...,n do
ˆyi ←max ˆy∈Ci{Δ(yi, ˆy) + wT Ψ (xi, ˆy)}
or the analogous variant for the case of slack-rescaling. Only if this fails will the algorithm
ask the separation oracle. The goal of this caching strategy is to decrease the number of
calls to the separation oracle. Note that in many applications, the separation oracle is very
expensive (e.g., CFG parsing).
Parallelization
While currently not implemented, the loop in Lines 5–7 of the 1-slack algorithms can easily be parallelized. In principle, one could make use of up to n parallel
threads, each computing the separation oracle for a subset of the training sample. For applications like CFG parsing, where more than 98% of the overall runtime is spent on the
separation oracle (see Sect. 5), parallizing this loop will lead to a substantial speed-up that
should be almost linear in the number of threads.
Solving the Dual of the QP Sub-Problems in the 1-Slack Algorithm
As indicated by Theorems 5 and 6, the working sets in the 1-slack algorithm stay small independent of the size of
the training set. In practice, typically less then 100 constraints are active at the solutions and
we never encountered a single instance where the working set grew beyond 1000 constraints.
This makes it advantageous to store and solve the QP sub-problems in the dual instead of
in the primal, since the dual is not affected by the dimensionality of Ψ (x,y). The algorithm
explicitly stores the Hessian H of the dual and adds or deletes a row/column whenever a
constraint is added or removed from the working set. Note that this is not feasible for the
n-slack algorithm, since the working set size is typically orders of magnitude larger (often
>100,000 constraints).
5 Experiments
For the experiments in this paper we will consider the following four applications, namely
binary classiﬁcation, multi-class classiﬁcation, sequence tagging with linear chain HMMs,
and CFG grammar learning. They cover the whole spectrum of possible applications, from
multi-class classiﬁcation involving a simple Y of low cardinality and with a very inexpensive
separation oracle, to CFG parsing with large and complex structural objects and an expensive
separation oracle. The particular setup for the different applications is as follows.
Binary Classiﬁcation
For binary classiﬁcation X = ℜN and Y = {−1,+1}. Using
Ψ (x,y) = 1
Δ(y, ¯y) = 100[y ̸= ¯y] =
if y = ¯y,
Mach Learn 77: 27–59
in the 1-slack formulation, OP4 results in the algorithm presented in Joachims and
implemented in the SVM-perf software.3 In the n-slack formulation, one immediately recovers Vapnik et al.’s original classiﬁcation SVM formulation of OP1 (up to the more convenient percentage-scale rescaling of the loss function and
the absence of the bias term), which we solve using SVM-light.
Multi-Class Classiﬁcation
This is another simple instance of a structual SVM, where
X = ℜN and Y = {1,...,k}. Using Δ(y, ¯y) = 100[y ̸= ¯y] and
Ψmulti(x,y) =
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
where the feature vector x is stacked into position y, the resulting n-slack problem becomes
identical to the multi-class SVM of Crammer and Singer . Our SVM-multiclass
(V2.13) implementation3 is also built via the SVMstruct API. The argmax for the separation
oracle and the prediction are computed by explicit enumeration.
We use the Covertype dataset of Blackard, Jock & Dean as our benchmark for the multiclass SVM. It is a 7-class problem with n = 522,911 examples and 54 features. This means
that the dimensionality of Ψ (x,y) is N = 378.
Sequence Tagging with Linear Chain HMMs
In sequence tagging (e.g., Part-of-Speech
Tagging) each input x = (x1,...,xl) is a sequence of feature vectors (one for each word),
and y = (y1,...,yl) is a sequence of labels yi ∈{1,...,k} of matching length. Isomorphic
to a linear chain HMM, we model dependencies between each yi and xi, as well as dependencies between yi and yi−1. Using the deﬁnition of Ψmulti(x,y) from above, this leads to a
joint feature vector of
ΨHMM((x1,...,xl),(y1,...,yl)) =
Ψmulti(xi,yi)
[yi = 1][yi−1 = 1]
[yi = 1][yi−1 = 2]
[yi = k][yi−1 = k]
We use the number of misclassiﬁed tags Δ((y1,...,yl),(y′
i=1[yi ̸= y′
the loss function. The argmax for prediction and the separation oracle are both computed
via the Viterbi algorithm. Note that the separation oracle is equivalent to the prediction
argmax after adding 1 to the node potentials of all incorrect labels. Our SVM-HMM (V3.10)
implementation based on SVMstruct is also available online.3
We evaluate on the Part-of-Speech tagging dataset from the Penn Treebank corpus . After splitting the dataset into training and test set, it has n = 35,531 training
3Available at svmlight.joachims.org.
Mach Learn 77: 27–59
examples (i.e., sentences), leading to a total of 854,022 tags over k = 43 labels. The feature
vectors xi describing each word consist of binary features, each indicating the presence of
a particular preﬁx or sufﬁx in the current word, the previous word, and the following word.
All preﬁxes and sufﬁxes observed in the training data are used as features. In addition, there
are features encoding the length of the word. The total number of features is approximately
430,000, leading to a ΨHMM(x,y) of dimensionality N = 18,573,781.
Parsing with Context Free Grammars
We use natural language parsing as an example
application where the cost of computing the separation oracle is comparatively high. Here,
each input x = (x1,...,xl) is a sequence of feature vectors (one for each word), and y is a
tree with x as its leaves. Admissible trees are those that can be constructed from a given set
of grammar rules—in our case, all grammar rules observed in the training data. As the loss
function, we use Δ(y, ¯y) = 100[y ̸= ¯y], and ΨCFG(x,y) has one feature per grammar rule
that counts how often this rule was applied in y. The argmax for prediction can be computed
efﬁciently using a CKY parser. We use the CKY parser implementation4 of Johnson .
For the separation oracle the same CKY parser is used after extending it to also return the
second best solution. Again, our SVM-CFG (V3.01) implementation based on SVMstruct is
available online.3
For the following experiments, we use all sentences with at most 15 words from the
Penn Treebank corpus . Restricting the dataset to short sentences is
not due to a limitation of SVMstruct, but due to the CKY implementation we are using. It
becomes very slow for long sentences. Faster parsers that use pruning could easily handle
longer sentences as well. After splitting the data into training and test set, we have n = 9,780
training examples (i.e., sentences) and ΨCFG(x,y) has a dimensionality of N = 154,655.
5.1 Experiment setup
Unless noted otherwise, the following parameters are used in the experiments reported below. Both the 1-slack (SVMstruct options “-w 3” and “-w 4” with caching) and the n-slack
algorithms (option “-w 0”) use ε = 0.1 as the stopping criterion (option “-e 0.1”). Given
the scaling of the loss for multi-class classiﬁcation and CFG parsing, this corresponds to a
precision of approximately 0.1% of the empirical risk for the 1-slack algorithm, and it is
slightly higher for the HMM problem. For the n-slack problem it is harder to interpret the
meaning of this ε, but we will see in Sect. 5.7 that it gives solutions of comparable precision.
As the value of C, we use the setting that achieves the best prediction performance on the test
set when using the full training set (C = 10,000,000 for multi-class classiﬁcation, C = 5,000
for HMM sequence tagging, and C = 20,000 for CFG parsing) (option “-c”). As the cache
size we use f = 10 (option “-f 10”). For multi-class classiﬁcation, margin-rescaling and
slack-rescaling are equivalent. For the others two problems we use margin-rescaling (option
“-o 2”). Whenever possible, runtime comparisons are done on the full training set. All
experiments are run on 3.6 GHz Intel Xeon processors with 4 GB of main memory under
5.2 How fast is the 1-slack algorithm compared to the n-slack algorithm?
We ﬁrst examine absolute runtimes of the 1-slack algorithm, and then analyze and explain
various aspects of its scaling behavior in the following. Table 1 shows the CPU-time that
4Available at 
Mach Learn 77: 27–59
Table 1 Training CPU-time (in hours), number of calls to the separation oracle, and number of support
vectors for both the 1-Slack (with caching) and the n-Slack Algorithm. n is the number of training examples
and N is the number of features in Ψ (x,y)
# Sep. oracle
# Support vec.
10,981,131
18,573,781
both the 1-slack and the n-slack algorithm take on the multi-class, sequence tagging, and
parsing benchmark problems. For all problems, the 1-slack algorithm is substantially faster,
for multi-class and HMM by several orders of magnitude.
The speed-up is largest for the multi-class problem, which has the least expensive separation oracle. Not counting constraints constructed from the cache, less than 1% of the time
is spent on the separation oracle for the multi-class problem, while it is 15% for the HMM
and 98% for CFG parsing. Therefore, it is interesting to also compare the number of calls to
the separation oracle. In all cases, Table 1 shows that the 1-slack algorithm requires a factor
between 2 and 4 fewer calls, accounting for much of the time saved on the CFG problem.
The most striking difference between the two algorithms lies in the number of support
vectors they produce (i.e., the number of dual variables that are non-zero). For the n-slack
algorithm, the number of support vectors lie in the tens or hundreds of thousands, while
all solutions produced by the 1-slack algorithm have only about 100 support vectors. This
means that the working sets that need to be solved in each iteration are orders of magnitude
smaller in the 1-slack algorithm, accounting for only 26% of the overall runtime in the multiclass experiment compared to more than 99% for the n-slack algorithm. We will further
analyze this in the following.
5.3 How fast is the 1-slack algorithm compared to conventional SVM training algorithms?
Since most work on training algorithms for SVMs was done for binary classiﬁcation, we
compare the 1-slack algorithms against algorithms for the special case of binary classiﬁcation. While there are training algorithms for linear SVMs that scale linearly with n (using the ξ 2
i loss), Proximal SVM
 (using an L2 regression loss), and Interior Point Methods
 ), they use the Sherman-Morrison-Woodbury formula (or matrix
factorizations) for inverting the Hessian of the dual. This requires operating on N × N matrices, which makes them applicable only for problems with small N. The L2-SVM-MFN
method avoids explicitly representing N × N matrices using
conjugate gradient techniques. While the worst-case cost is still O(snmin(n,N)) per iteration for feature vectors with sparsity s, they observe that their method empirically scales
much better. The discussion in Joachims concludes that runtime is comparable to the
1-slack algorithm implemented in SVM-perf. The 1-slack algorithm scales linearly in both
n and the sparsity s of the feature vectors, even if the total number N of features is large
 . Note that it is unclear whether any of the conventional algorithms can be
extended to structural SVM training.
The most widely used algorithms for training binary SVMs are decomposition methods like SVM-light , SMO , and others 77: 27–59
Table 2 Training CPU time (in seconds) for ﬁve binary classiﬁcation problems comparing the 1-slack algorithm (without caching) with SVM-light. n is the number of training examples, N is the number of features,
and s is the fraction of non-zero elements of the feature vectors. The SVM-light results are quoted from
Joachims , the 1-slack results are re-run with the latest version of SVM-struct using the same experiment setup as in Joachims 
# Support vec.
Reuters CCAT
Reuters C11
ArXiv Astro-ph
Covertype 1
KDD04 Physics
Collobert and Bengio 2001). Taskar et al. extended the SMO algorithm to structured
prediction problems based on their polynomial-size reformulation of the n-slack optimization problem OP2 for the special case of decomposable models and decomposable loss functions. In the case of binary classiﬁcation, their SMO algorithm reduces to a variant of the
traditional SMO algorithm, which can be seen as a special case of the SVM-light algorithm.
We therefore use SVM-light as a representative of the class of decomposition methods.
Table 2 compares the runtime of the 1-slack algorithm to SVM-light on ﬁve benchmark
problems with varying numbers of features, sparsity, and numbers of training examples.
The benchmarks include two text classiﬁcation problems from the Reuters RCV1 collection5 , a problem of classifying ArXiv abstracts, a binary classiﬁer for
class 1 of the Covertype dataset6 of Blackard, Jock & Dean, and the KDD04 Physics
task from the KDD-Cup 2004 . In all cases, the 1-slack algorithm is
faster than SVM-light, which is highly optimized to binary classiﬁcation. On large datasets,
the difference spans several orders of magnitude.
After the 1-slack algorithm was originally introduced, new stochastic subgradient descent methods were proposed that are competitive in runtime for classiﬁcation SVMs,
especially the PEGASOS algorithm . While currently only
explored for classiﬁcation, it should be possible to extend PEGASOS also to structured prediction problems. Unlike exponentiated gradient methods , PEGASOS does not require the computation of marginals, which
makes it equally easy to apply as cutting-plane methods. However, unlike for our cuttingplane methods where the theory provides a practically effective stopping criterion, it is less
clear when to stop primal stochastic subgradient methods. Since they do not maintain a
dual program, the duality gap cannot be used to characterize the quality of the solution at
termination. Furthermore, there is a question of how to incorporate caching into stochastic
subgradient methods while still maintaining fast convergence. As shown in the following,
caching is essential for problems where the separation oracle (or, equivalently, the computation of subgradients) is expensive (e.g. CFG parsing).
5 
6 
Mach Learn 77: 27–59
5.4 How does training time scale with the number of training examples?
A key question is the scalability of the algorithm for large datasets. While Corollary 1 shows
that an upper bound on the training time scales linearly with the number of training examples, the actual behavior underneath this bound could potentially be different. Figure 1
shows how training time relates to the number of training examples for the three structural
prediction problems. For the multi-class and the HMM problem, training time does indeed
scale at most linearly as predicted by Corollary 1, both with and without using the cache.
However, the cache helps for larger datasets, and there is a large advantage from using the
cache over the whole range for CFG parsing. This is to be expected, given the high cost of
the separation oracle in the case of parsing.
As shown in Fig. 2, the scaling behavior of the 1-slack algorithm remains essentially
unchanged even when the regularization parameter C is not held constant, but is set to the
value that gives optimal prediction performance on the test set for each training set size. The
scaling with C is analyzed in more detail in Sect. 5.9.
The n-slack algorithm scales super-linearly for all problems, but so does the 1-slack
algorithm for CFG parsing. This can be explained as follows. Since the grammar is constructed from all rules observed in the training data, the number of grammar rules grows
with the number of training examples. Even from the second-largest to the largest training set, the number of rules in the grammar still grows by almost 70% (3550 rules vs.
5182 rules). This has two effects. First, the separation oracle becomes slower, since its time
scales with the number of rules in the grammar. In particular, the time the CFG parser takes
to compute a single argmax increases more than six-fold from the smallest to the largest
training set. Second, additional rules (in particular unary rules) introduce additional features and allow the construction of larger and larger “wrong” trees ¯y, which means that
R2 = maxi, ¯y ∥Ψ (xi,yi) −Ψ (xi, ¯y)∥2 is not constant but grows. Indeed, Fig. 3 shows that—
consistent with Theorem 5—the number of iterations of the 1-slack algorithm is roughly
Fig. 1 Training times for multi-class classiﬁcation (left) HMM part-of-speech tagging (middle) and CFG
parsing (right) as a function of n for the n-slack algorithm, the 1-slack algorithm, and the 1-slack algorithm
with caching
Mach Learn 77: 27–59
Fig. 2 Training times as a function of n using the optimal value of C at each training set size for the 1-slack
algorithm (left) and the 1-slack algorithm with caching (right)
Fig. 3 Number of iterations as a function of n for the the 1-slack algorithm (left) and the 1-slack algorithm
with caching (right)
constant for multi-class classiﬁcation and the HMM,7 while it grows slowly for CFG parsing.
Finally, note that in Fig. 3 the difference in the number of iterations of the algorithm
without caching (left) and with caching (right) is small. Despite the fact that the constraint
from the cache is typically not the overall most violated constraint, but only a sufﬁciently
violated constraint, both versions of the algorithm appear to make similar progress in each
iteration.
7Note that the HMM always considers all possible rules in the regular language, so that there is no growth in
the number of rules once all symbols are added.
Mach Learn 77: 27–59
5.5 What is the size of the working set?
As already noted above, the size of the working set and its scaling has a substantial inﬂuence
on the overall efﬁciency of the algorithm. In particular, large (and growing) working sets
will make it expensive to solve the quadratic programs. While the number of iterations is
an upper bound on the working set size for the 1-slack algorithm, the number of support
vectors shown in Fig. 4 gives a much better idea of its size, since we are removing inactive
constraints from the working set. For the 1-slack algorithm, Fig. 4 shows that the number
of support vectors does not systematically grow with n for any of the problems, making it
easy to solve the working set QPs even for large datasets. This is very much in contrast to
the n-slack algorithm, where the growing number of support vectors makes each iteration
increasingly costly, and is starting to push the limits of what can be kept in main memory.
5.6 How often is the separation oracle called?
Next to solving the working set QPs in each iteration, computing the separation oracle is
the other major expense in each iteration. We now investigate how the number of calls to
the separation oracle scales with n, and how this is inﬂuenced by caching. Figure 5 shows
that for all algorithms the number of calls scales linearly with n for the multi-class problem
and the HMM. It is slightly super-linear for CFG parsing due to the increasing number of
iterations as discussed above. For all problems and training set sizes, the 1-slack algorithm
with caching requires the fewest calls.
The size of the cache has surprisingly little inﬂuence on the reduction of calls to the
separation oracle. Figure 6 shows that a cache of size f = 5 already provides all of the
beneﬁts, and that larger cache sizes do not further reduce the number of calls. However, we
conjecture that this might be an artifact of our simple least-recently-used caching strategy,
and that improved caching methods that selectively call the separation oracle for only a
well-chosen subset of the examples will provide further beneﬁts.
Fig. 4 Number of support vectors for multi-class classiﬁcation (left) HMM part-of-speech tagging (middle)
and CFG parsing (right) as a function of n for the n-slack algorithm, the 1-slack algorithm, and the 1-slack
algorithm with caching
Mach Learn 77: 27–59
Fig. 5 Number of calls to the separation oracle for multi-class classiﬁcation (left) HMM part-of-speech
tagging (middle) and CFG parsing (right) as a function of n for the n-slack algorithm, the 1-slack algorithm,
and the 1-slack algorithm with caching
Fig. 6 Number of calls to the
separation oracle as a function of
cache size for the 1-slack
5.7 Are the solutions different?
Since the stopping criteria are different in the 1-slack and the n-slack algorithm, it remains
to verify that they do indeed compute a solution of comparable effectiveness. The plot in
Fig. 7 shows the dual objective value of the 1-slack solution relative to the n-slack solution.
A value below zero indicates that the n-slack solution has a better dual objective value, while
a positive value shows by which fraction the 1-slack objective is higher than the n-slack
objective. For all values of C the solutions are very close for the multi-class problem and for
CFG parsing, and so are their prediction performances on the test set (see table in Fig. 7).
This is not surprising, since for both the n-slack and the 1-slack formulation the respective
ε bound the duality gap by Cε.
For the HMM, however, this Cε is a substantial fraction of the objective value at the
solution, especially for large values of C. Since the training data is almost linearly separable
for the HMM, Cε becomes a substantial part of the slack contribution to the objective value.
Furthermore, note the different scaling of the HMM loss 77: 27–59
1-slack n-slack
MultiC Accuracy
Token accuracy 96.71
Bracket F1
Fig. 7 Relative difference in dual objective value of the solutions found by the 1-slack algorithm and by
the n-slack algorithm as a function of C at the maximum training set size (left), and test-set prediction
performance for the optimal value of C (right)
Fig. 8 Number of iterations for the 1-slack algorithm (left) and number of calls to the separation oracle for
the 1-slack algorithm with caching (right) as a function of ε at the maximum training set size
the sentence), which is roughly 5 times smaller than the loss function on the other problems
(i.e., 0 to 100 scale). So, an ε = 0.1 on the HMM problem is comparable to an ε = 0.5 on
the other problems. Nevertheless, with a per-token test error rate of 3.29% for the 1-slack
solution, the prediction accuracy is even slightly better than the 3.31% error rate of the
n-slack solution.
5.8 How does the 1-slack algorithm scale with ε?
While the scaling with n is the most important criterion from a practical perspective, it is also
interesting to look at the scaling with ε. Theorem 5 shows that the number of iterations (and
therefore the number of calls to the separation oracle) scales O( 1
ε ) in the worst cast. Figure 8,
however, shows that the scaling is much better in practice. In particular, the number of calls
Mach Learn 77: 27–59
Fig. 9 Number of iterations for the 1-slack algorithm (left) and number of calls to the separation oracle for
the 1-slack algorithm with caching (right) as a function of C at the maximum training set size
to the separation oracle is largely independent of ε and remains constant when caching is
used. It seems like the additional iterations can be done almost entirely from the cache.
5.9 How does the 1-slack algorithm scale with C?
With increasing training set size n, the optimal value of C will typically change (some theoretical results suggest an increase on the order of √n). In practice, ﬁnding the optimal value
of C typically requires training for a large range of C values as part of a cross-validation
experiment. It is therefore interesting to know how the algorithm scales with C. While Theorem 5 bounds the number of iterations with O(C), Fig. 9 shows that the actual scaling is
again much better. The number of iterations increases slower than Ω(C) on all problems.
Furthermore, as already observed for ε above, the additional iterations are almost entirely
based on the cache, so C has hardly any inﬂuence on the number of calls to the separation
6 Conclusions
We presented a cutting-plane algorithm for training structural SVMs. Unlike existing cutting plane methods for these problems, the number of constraints that are generated does not
depend on the number of training examples n, but only on C and the desired precision ε.
Empirically, the new algorithm is substantially faster than existing methods, in particular
decomposition methods like SMO and SVM-light, and it includes the training algorithm of
Joachims for linear binary classiﬁcation SVMs as a special case. An implementation of the algorithm is available online with instances for multi-class classiﬁcation, HMM
sequence tagging, CFG parsing, and binary classiﬁcation.
Acknowledgements
We thank Evan Herbst for implementing a prototype of the HMM instance of
SVMstruct, which was used in some of our preliminary experiments. This work was supported in part through
the grant NSF IIS-0713483 from the National Science Foundation and through a gift from Yahoo!.
Mach Learn 77: 27–59
α≥0 D(α) =
Δ(¯y)α¯y −1
α¯yα¯y′HMR(¯y, ¯y′)
and the Wolfe-Dual of the 1-slack optimization problem OP5 for slack-rescaling is
α≥0 D(α) =
Δ(¯y)α¯y −1
α¯yα¯y′HSR(¯y, ¯y′)
Proof The Lagrangian of OP4 is
L(w,ξ,α) = 1
2wT w + Cξ
Δ(yi, ¯yi) −ξ −1
Ψ (xi,yi) −Ψ (xi, ¯yi)
Differentiating with respect to w and setting the derivative to zero gives
[Ψ (xi,yi) −Ψ (xi, ¯yi)]
Similarly, differentiating with respect to ξ and setting the derivative to zero gives
Plugging w into the Lagrangian with constraints on α we obtain the dual problem:
[Ψ (xi,yi) −Ψ (xi, ¯yi)]
[Ψ (xj,yj) −Ψ (xj, ¯y′
Δ(yi, ¯yi)
∀¯y ∈Yn : α¯y ≥0.
The derivation of the dual of OP5 is analogous.
Mach Learn 77: 27–59
Lemma 2 For any unconstrained quadratic program
α∈ℜn {Θ(α)} < ∞,
Θ(α) = hT α −1
with positive semi-deﬁnite H, and derivative ∂Θ(α) = h −Hα, a line search starting at α
along an ascent direction η with maximum step-size C > 0 improves the objective by at least
0≤β≤C {Θ(α + βη)} −Θ(α) ≥1
C, ∇Θ(α)T η
Proof For any β and η, it is easy to verify that
Θ(α + βη) −Θ(α) = β
∇Θ(α)T η −1
Maximizing this expression with respect to an unconstrained β by setting the derivative to
zero, the solution β∗is
β∗= ∇Θ(α)T η
Note that ηT Hη is non-negative, since H is positive semi-deﬁnite. Furthermore, ηT Hη ̸= 0,
since otherwise η being an ascent direction would contradict maxα∈ℜn{Θ(α)} < ∞. Plugging β∗into (34) shows that
β∈ℜ{Θ(α + βη)} −Θ(α) = 1
(∇Θ(α)T η)2
It remains to check whether the unconstrained solution β∗fulﬁlls the constraints 0 ≤
β∗≤C. Since η is an ascent direction, β∗is always non-negative. But one needs to consider the case that β∗> C, which happens when ∇Θ(α)T η > CηT Hη. In that case, the
constrained optimum is at β = C due to convexity. Plugging C into (34) shows that
β∈ℜ{Θ(α + βη)} −Θ(α) = C∇Θ(α)T η −1
2C∇Θ(α)T η.
The inequality follows from C ≤∇Θ(α)T η