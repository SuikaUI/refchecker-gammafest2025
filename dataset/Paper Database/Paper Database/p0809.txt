Online Ensemble Model Compression using
Knowledge Distillation
Devesh Walawalkar, Zhiqiang Shen, and Marios Savvides
Carnegie Mellon University, Pittsburgh PA 15213, USA
 {zhiqians,marioss}@andrew.cmu.edu
Abstract. This paper presents a novel knowledge distillation based
model compression framework consisting of a student ensemble. It enables distillation of simultaneously learnt ensemble knowledge onto each
of the compressed student models. Each model learns unique representations from the data distribution due to its distinct architecture. This
helps the ensemble generalize better by combining every model’s knowledge. The distilled students and ensemble teacher are trained simultaneously without requiring any pretrained weights. Moreover, our proposed
method can deliver multi-compressed students with single training, which
is eﬃcient and ﬂexible for diﬀerent scenarios. We provide comprehensive experiments using state-of-the-art classiﬁcation models to validate
our framework’s eﬀectiveness. Notably, using our framework a 97% compressed ResNet110 student model managed to produce a 10.64% relative accuracy gain over its individual baseline training on CIFAR100
dataset. Similarly a 95% compressed DenseNet-BC (k=12) model managed a 8.17% relative accuracy gain.
Keywords: Deep Model Compression, Image Classiﬁcation, Knowledge
Distillation, Ensemble Deep Model Training
Introduction
Deep Learning based neural networks have provided tremendous improvements
over the past decade in various domains of Computer Vision. These include Image Classiﬁcation , Object Detection , Semantic Segmentation among others. The drawbacks of these methods however
include the fact that a large amount of computational resources are required to
achieve state-of-the-art accuracy. A trend started setting in where constructing
deeper and wider models provided better accuracy at the cost of considerable resource utilization . The diﬀerence in resource utilization is considerable
compared to traditional computer vision techniques. To alleviate this gap, model
compression techniques started being developed to reduce these large computational requirements. These techniques can broadly be classiﬁed into four types
 i.e. Parameter Pruning , Low Rank Factorization , Transferred Convolutional Filters and Knowledge Distillation methods
 
D. Walawalkar et al.
Fig. 1: Overview of our model compression framework for a 3 student ensemble. Each
student is composed of the base block and one of the network branches on top of it.
The original model is the ﬁrst student in the ensemble, termed as pseudo teacher due
to its simultaneous knowledge transfer capability and training from scratch properties.
The ensemble teacher is a weighted combination of all student’s output logits. Each
student is divided into four blocks such that each block incorporates approximately
the same number of layers. The layer channels for the compressed student branches
are reduced by a speciﬁc ratio with respect to the original model. For a 3 student
ensemble, the layer channels assigned are M, 2M/3 and M/3, where M is the original
layer channel count. Channel adaptation layers help map the compressed student’s
block output channels to the pseudo teacher’s output channels in order to compute an
intermediate feature representation loss
 . Each of these types was able to provide impressive computational reductions while simultaneously managing to keep the accuracy degradation to a minimum.
Knowledge Distillation (KD) in particular has provided great model compression capabilities using a novel teacher-student model concept . Here a teacher,
the original model trained for a speciﬁc task is used to teach a compressed or
replicated version of itself referred to as student. The student is encouraged to
mimic the teacher output distribution, which helps the student generalize much
better and in certain cases leads to the student performing better than the
teacher itself. Drawbacks of these methods include the fact that a pre-trained
model is required to distill knowledge onto a student model. To solve this, simultaneous distillation methods were developed wherein multiple students
were being trained with an ensemble teacher learning on the ﬂy with the students itself in an ensemble training scheme. These methods however primarily
focused on replicating the teacher multiple times, causing the ensemble to gain
sub-optimal generalized knowledge due to model architecture redundancy. Also
in parallel, methods were developed which focused on distilling not only
the output knowledge but also the intermediate representations of teacher onto
the student to have more eﬀective knowledge transfer. These techniques are
eﬃcient, however still suﬀering from the drawback of requiring teacher model
pre-training in addition to the student training.
Online Ensemble Model Compression
In this paper, we present a novel model compression and ensemble training
framework which improves on all aforementioned drawbacks and provides a new
perspective for ensemble model compression techniques. We present a framework which enables multiple student model training using knowledge transfer
from an ensemble teacher. Each of these student models represents a version of
the original model compressed to diﬀerent degrees. Knowledge is distilled onto
each of the compressed student through the ensemble teacher and also through
intermediate representations from a pseudo teacher. The original model is the
ﬁrst student in our ensemble, termed as pseudo teacher due to its simultaneous
knowledge transfer capability and training from scratch property. Moreover this
framework simultaneously provides multiple compressed models having diﬀerent computational budget with each having beneﬁted from every other model’s
training. Our framework facilitates the choice of selecting a student that ﬁts the
resource budget for a particular use case with the knowledge that every student
provides decent comparable performance to the original model. Also, the ensemble training signiﬁcantly reduces the training time of all student combined,
compared to when they are trained individually. We speciﬁcally focus on image
classiﬁcation task while providing extensive experimentation on popular image
classiﬁcation models and bench-marked datasets.
Our contributions from this paper can be thus summarized as follows:
1. We present a novel ensemble model compression framework based on knowledge distillation that can generate multiple compressed networks simultaneously with a single training run and is completely model agnostic.
2. We provide extensive experiments on popular classiﬁcation models on standard datasets with corresponding baseline comparisons of each individual
student training to provide evidence of our framework’s eﬀectiveness.
3. We provide hyper parameter ablation studies which help provide insights into
the eﬀective ensemble knowledge distillation provided by our framework.
Related Works
Model Compression using Knowledge Distillation
Hinton et al. introduced the concept of distilling knowledge from a larger
teacher model onto a smaller compressed student model. Mathematically this
meant training the student on softened teacher output distribution in addition
to the traditional cross entropy with dataset labels. The paper argued that the
teacher distribution provided much richer information about an image compared
to just one hot labels. For e.g. consider a classiﬁcation task of diﬀerentiating
between various breeds of dogs. The output distribution of a higher capability
teacher provides the student with the information of how alike one breed of dogs
looks to the other. This helps the student learn more generalized features of
each dog breed compared to providing just one hot labels which fails to provide
any comparative knowledge. Also, in process of trying to mimic the distribution
of a much deeper teacher model the student tries to ﬁnd a compact series of
D. Walawalkar et al.
transformation that tries to mimic the teacher’s larger series of transformations.
This inherently helps the student negate the accuracy loss due to compression.
Impressively, in certain cases the student manages to outperform its teacher due
to this superior generalization training. Further works extended this concept
by using internal feature representations , adversarial learning and inner
product between feature maps .
Online Knowledge Distillation
The single practical drawback of Knowledge Distillation is the fact that a pretrained model is required to play the role of a teacher. This entails multiple
sequential training runs on the same dataset. Anil et al. came up with a
method to train the student in parallel with the teacher termed as codistillation.
Here the student is an exact replica of the teacher and roles of teacher and student
were continuously interchanged between the models during training with one
training the other iteratively. The primary distinguishing property between the
models was their distinct parameter initialization. This enabled each model to
learn unique features which were then distilled from one to the other as training
progressed.
Zhang et al. employed a unique KD loss, by using KL divergence loss between two model output distributions to penalize the diﬀerences between them.
Each model’s training involved a combination loss of KL divergence loss with
other model’s distribution and traditional cross entropy loss. Both models acting
as teacher and student simultaneously were trained jointly in an online fashion.
Lan et al. extended this online KD concept by having multiple replicas of
a given model in a multi branch architecture fashion. Multi branch architecture
designs became popular with the image classiﬁcation models like Resnet ,
Inception and ResNext . In this paper, the multiple replicated models
have a common base of block of layers with each model represented as a branch
on top of this base with subsequent layer blocks from the original model architecture right until the ﬁnal fully connected layers. The teacher in this concept
was the combined output of all the student models in the ensemble. Each student
model learnt from the ensemble joint knowledge represented by the teacher outputs. Our paper builds on this core concept however is fundamentally diﬀerent
as we incorporate compressed student branches, more eﬃcient training procedure, incorporate intermediate representation distillation in addition to the ﬁnal
output distillation among others.
Intermediate Representation Knowledge Distillation
A separate branch of Knowledge Distillation focuses on training the student to
mimic the intermediate representations obtained in form of feature maps from
certain intermediate layer blocks within the teacher. This provides a more stricter
learning regime for the student who has to focus not only on the ﬁnal teacher
output distribution but also on its intermediate layer feature maps. Romero
et al. provide one of the preliminary works in this direction by distilling
Online Ensemble Model Compression
a single intermediate layer’s knowledge onto the student which they term as
providing hint to the student. Mathematically, hint training involves minimizing
a combination of L2 loss between the features maps at an immediate layer of
the two models and the regular KD (KL divergence loss) between the output
distributions.
Koratana et al. extend this concept by comparing feature maps at not
just one but multiple intermediate locations within the model which can be
related to as multiple hint training. This method however again requires a pretrained teacher model which might be time consuming and compute expensive
for certain real world scenarios. Our method incorporates multiple hint training
for all student models with respect to pseudo teacher, which is also the ﬁrst
student in the ensemble. A network’s depth measures its function modeling capacity in general. Koratana et al. compress the model by removing blocks
of layers from the network which severely aﬀects the network depth. Our work
incorporates a more robust compression logic where the number of channels in
every student layer are simply reduced by a certain percent, thus preserving the
model depth.
Methodology
An overview of our ensemble compression framework is presented in Figure 1,
which can be split up into three major sections for the ease of understanding.
We would be going into their details in the following sections.
Ensemble Student Model Compression
First, the entire architecture of a given neural network is broken down into a
series of layer blocks, ideally into four blocks. The ﬁrst block is designated as a
common base block and the rest of the blocks are replicated in parallel to create
branches as shown in Figure 1. A single student model can be viewed as a series
of base block and one of the branches on top of it. As previously mentioned,
the original network is designated as the ﬁrst student model, also termed as
pseudo teacher. For every successive student branch the number of channels in
each layer of its blocks is reduced by a certain ratio with respect to the pseudo
teacher. This ratio becomes higher for every new student branch created. For
example, for a four student ensemble and C being number of channels in pseudo
teacher, the channels in other three students are assigned to be 0.75C, 0.5C and
0.25C. The students are compressed versions of the original model to varying
degrees, which still manage to maintain the original network depth. The channels
in the common base block are kept the same as original whose main purpose is
to provide constant low level features to all the student branches.
The output logits from all the student models are averaged together to create
the ensemble teacher logits. This ensemble teacher output distribution represents
the joint knowledge of the ensemble. During inference stage, any of the individual
D. Walawalkar et al.
Table 1: Model size and test set accuracy comparison of every student in a ﬁve student
ensemble, with their percent relative size in respect to the original model and their
CIFAR10 test accuracies achieved using our ensemble framework.
Classiﬁcation
Student Model size and accuracy (%)
Size Accuracy Size Accuracy Size Accuracy Size Accuracy Size Accuracy
ResNet20 
ResNet33 
ResNet44 
ResNet110 
DenseNet (k=12) 
ResNext50 (32 × 4d) 100.0
EﬃcientNet-B0 
EﬃcientNet-B2 
EﬃcientNet-B4 
student models can be selected from the ensemble depending on the computational hardware constraints. In case of lenient constraints, the entire ensemble
can be used with the ensemble teacher providing inference based on the learnt
ensemble knowledge. From our studies we ﬁnd that having 5 students (inclusive of pseudo teacher) provides an optimal trade oﬀbetween training time and
eﬀective model compression. Table 1 provides an overview of compressed student model sizes and their CIFAR10 trained accuracies for a ﬁve student
ensemble based on various classiﬁcation model architectures.
Intermediate Knowledge Distillation
The intermediate block knowledge (feature map representation) is additionally
distilled onto every compressed student from the pseudo teacher. This provides a
more stricter training and distillation regime for all the compressed students. The
loss in every student’s representational capacity due to compression is countered,
by making each student block try and learn the intermediate feature map of its
corresponding pseudo teacher block. The feature map pairs are compared using
traditional Mean Squared Error loss, on which the network is trained to reduce
any diﬀerences between them. Since the number of feature map channels varies
across every corresponding student and pseudo teacher block, an adaptation
layer consisting of pointwise convolution (1×1 kernel) is used to map compressed
student block channels to its pseudo teacher counterpart. Figure 2 (a) presents
this idea in detail for an EﬃcientNet-B0 based ensemble. The intermediate
knowledge is transferred at three locations corresponding to the three blocks in
every student branch as shown in Figure 1.
Knowledge Distillation Based Training
The overall ensemble is trained using a combination of three separate losses
which are described in detail as follows:
Cross-Entropy Loss Each student model is trained individually on classical cross entropy loss [Equation 1, 2] with one hot vector of labels and student
Online Ensemble Model Compression
output logits. This loss helps each student directly train on a given dataset.
This loss procedure makes the pseudo teacher learn alongside the compressed
students as the framework doesn’t use pretrained weights of any sort. It also
helps the ensemble teacher gain richer knowledge of the dataset as it incorporates combination of every student’s learnt knowledge. It additionally enables
the framework to avoid training the ensemble teacher separately and is trained
implicitly through the students. The output softmax distribution and combined
normal loss can be expressed as follows,
k=1 exp(xijk)
−1jk log(Xijk)
where i, j, k represents student, batch sample and class number indices respectively. 1jk is an one hot label indicator function for jth sample and kth class.
Similarly, xijk is a single output logit from ith student model for jth batch sample
and kth class and Xijk is its corresponding softmax output.
Intermediate Loss For every pseudo teacher and compressed student pair,
the output feature maps from every pseudo teacher block are compared to the
ones at its corresponding compressed student block. In order to facilitate an eﬀective knowledge distillation between these respective map pairs, the compressed
student maps are ﬁrst passed through an adaptation layer which as mentioned
earlier is a simple 1 × 1 convolution, mapping the student map channels to the
pseudo teacher map channels. A Mean Squared Error loss is used to compare
each single element of a given pseudo teacher-student feature map pair. This loss
is averaged across the batch. The loss for a single block across all students can
be expressed as follows,
lintermediate
m is a feature map of size H × W × C corresponding to mth batch
sample of the lth student model. |.|2 represents element wise squared L2 norm.
l = 1 represents the pseudo teacher, also designated as PT in xP T
which is the
corresponding pseudo teacher feature map. The overall intermediate loss can be
expressed as:
Lintermediate =
lintermediate
This loss is used to update only the compressed student model parameters in
order to have the compressed student learn from the pseudo teacher and not the
other way round. In our experiments we observed that the mean of adaptation
layer weights is on average lower for larger student models. This in turn propagates a smaller model response term in the intermediate loss equation, thus
D. Walawalkar et al.
increasing their losses slightly compared to thinner students. This helps balance
this loss term across all students.
Knowledge Distillation Loss To facilitate global knowledge transfer from
the ensemble teacher to each of the students, a KD loss in form of Kullback-
Leibler Divergence Loss is incorporated between the ensemble teacher and student outputs. The outputs of the ensemble teacher and each respective student
are softened using a certain temperature T to help students learn eﬃciently
from highly conﬁdent ensemble teacher predictions where the wrong class outputs are almost zero. The softened softmax and overall KD loss can be expressed
as follows,
k=1 exp( xijk
where Xijk is the softened softmax output of the ith student for jth batch sample
and kth class. Similarly XT
jk represents the ensemble teacher softened softmax
output for jth batch sample and kth class.
Combined Loss The above presented three losses are combined using a
weighted combination, on which the entire framework is trained to reduce this
overall loss. This can be mathematically expressed as,
L = αLNormal + βLintermediate + γLKD
The optimal weight value combination which was found out to be α = 0.7,
β = 0.15, γ = 0.15 is discussed in detail in an ablation study presented in later
Experiments
Datasets. We incorporate four major academic datasets: (1) CIFAR10 dataset
 which contains 50,000/10,000 training/test samples drawn from 10 classes.
Each class has 6,000 images included in both training and test set sized at
32 × 32 pixels. (2) CIFAR100 dataset which contains 50,000/10,000 training/test samples drawn from 100 classes. Each class has 600 images included in
both training and test set sized at 32 × 32 pixels. (3) SVHN dataset which
contains 73,257/26,032 training/test samples drawn from 10 classes. Each class
represents a digit from 0 to 9. Each image is sized at 32×32 pixels. (4) ImageNet
dataset is a comprehensive database containing around 1.2 million images,
speciﬁcally 1,281,184/50,000 training/testing images drawn from 1000 classes.
Experimental Hypothesis. Experiments are conducted in order to: (1) compare every compressed student’s test set performance trained using our ensemble framework versus simply training each one of them individually without any
Online Ensemble Model Compression
Table 2: Individual Test Set performance comparison for ﬁve compressed students
trained using our ensemble and using baseline training on CIFAR10 dataset. Reported
results are averaged over ﬁve individual experimental runs.
Classiﬁcation
Student Test Accuracy (%)
Baseline Ensemble Baseline Ensemble Baseline Ensemble Baseline Ensemble Baseline Ensemble
Resnet20 
Resnet32 
Resnet44 
Resnet110 
Densenet-BC (k=12) 
ResNext50 (32 × 4d) 
EﬃcientNet-B0 
EﬃcientNet-B2 
EﬃcientNet-B4 
Table 3: Individual Test Set performance comparison for ﬁve compressed students
trained using our ensemble and using baseline training on CIFAR100. Reported results
are averaged over ﬁve individual experimental runs.
Classiﬁcation
Student Test Accuracy (%)
Baseline Ensemble Baseline Ensemble Baseline Ensemble Baseline Ensemble Baseline Ensemble
Resnet32 
Resnet44 
Resnet56 
Resnet110 
Densenet-BC (k=12) 
ResNeXt50 (32 × 4d) 
EﬃcientNet-B0 
EﬃcientNet-B2 
EﬃcientNet-B4 
knowledge distillation component. These experiments help validate the advantages of using an ensemble teacher and intermediate knowledge transfer for every
compressed student compared to using only the traditional individual cross entropy loss based training. (2) Compare the test set accuracy of our ensemble
teacher to other notable ensemble knowledge distillation based techniques in
literature on all four mentioned datasets to prove our framework’s overall superiority and eﬀectiveness, which are presented in Table 4. (3) Compare the time
taken for training our ﬁve student based ensemble versus the combined time
taken for training each of those students individually. This comparison helps
substantiate the training time beneﬁts of our hybrid multi-student architecture
compared to training each student alone either sequentially or in parallel. These
are presented in Figure 2 (b).
Performance Metrics. We compare the test set accuracy (Top-1) of each of
our student models within the ensemble, trained using our framework and as an
individual baseline model with only the traditional cross entropy loss. For each
of our ensemble students, this test set accuracy is computed as an average of the
best student test accuracies achieved during each of ﬁve conducted runs.
D. Walawalkar et al.
Table 4: Comparison of notable knowledge distillation and ensemble based techniques
with our ensemble teacher reported test accuracy performance (Error rate %). The
best performing model accuracy is chosen for DML.
ResNet-32 ResNet-110 ResNet-32 ResNet-110 ResNet-32 ResNet-110 Resnet-18 ResNeXt-50
KD-ONE 
Snopshot Ensemble 
Table 5: Individual Test Set performance comparison for ﬁve compressed students
trained using our ensemble and using baseline training on SVHN. Reported results are
averaged over ﬁve individual experimental runs.
Classiﬁcation
Student Test Accuracy (%)
Baseline Ensemble Baseline Ensemble Baseline Ensemble Baseline Ensemble Baseline Ensemble
Resnet20 
Resnet32 
Resnet44 
Resnet110 
Densenet-BC (k=12) 
ResNext50 (32 × 4d) 
EﬃcientNet-B0 
EﬃcientNet-B2 
EﬃcientNet-B4 
Experimental Setup. For fair comparison, we keep the training schedule the
same for both our ensemble framework and baseline training. Speciﬁcally, for
ResNet, DenseNet and ResNeXt models SGD is used with Nesterov momentum
set to 0.9, following a standard learning rate schedule that drops from 0.1 to
0.01 at 50% training and to 0.001 at 75%. For EﬃcientNet models RMSProp
optimizer is implemented with decay 0.9 and momentum 0.9 and initial learning rate of 0.256 that decays by 0.97 every 3 epochs. The models are trained
for 350/450/50/100 epochs each for the CIFAR10/CIFAR100/SVHN/ImageNet
datasets respectively.
Evaluation of our online model compression framework
Results on CIFAR10 and CIFAR100. Tables 2,3 present our experimental results for CIFAR10 and CIFAR100 dataset respectively. Each compressed
student’s test set performance is on an average 1% better using our ensemble
framework as compared to the simple baseline training for both the datasets. Our
ensemble teacher also provides the best Test set accuracy when compared to the
teacher accuracies of three other ensemble knowledge distillation techniques for
ResNet32 and ResNet110 models as presented in Table 4. Our framework provides substantial training time beneﬁts for all models tested with CIFAR10 and
CIFAR100 datasets as presented in Fig 2 (b). For fair comparison the ensemble
Online Ensemble Model Compression
Table 6: Individual Test Set performance comparison for ﬁve compressed students
trained using our ensemble and using baseline training on ImageNet (Top-1 accuracy).
Reported results are averaged over ﬁve individual experimental runs.
Classiﬁcation
Student Test Accuracy (Top-1 accuracy %)
Baseline Ensemble Baseline Ensemble Baseline Ensemble Baseline Ensemble Baseline Ensemble
Resnet18 
Resnet34 
Resnet50 
Resnet101 
Densenet-121 
ResNext50 (32 × 4d) 
Pseudo Teacher Layer block 3
Width channels Multiplier = 1.0
5th Student Layer block 3
Width Channels Multiplier = 1/5
Block Output
Feature map
Elementwise
Intermediate
7 x 7 x 320
7 x 7 x 64
1 x 1 x 320 x 64
7 x 7 x 320
Adaptation
Layer Kernel
Fig. 2: (a) Channel Adaptation Logic for mapping output channels of an EﬃcientNet-B0
model based ensemble, depicting Block 3 outputs of the pseudo Teacher and 5th student
in the ensemble. (b) Comparison of our ensemble framework training time (Blue) to the
combined training time of individual baseline students performed sequentially (Orange)
and in parallel (Green). Timings recorded for training carried out on a GPU cluster of
Nvidia GTX 1080Ti.
and each of the baseline students are trained for the same number epochs on
both datasets. Notably, training a ﬁve student ensemble of an EﬃcientNet-B4 architecture is roughly 7.5K GPU minutes quicker as compared to their combined
individual baseline training.
Results on SVHN and ImageNet. Tables 5,6 present our experimental results for SVHN and ImageNet datasets respectively. Again, each compressed
student test set performance is on an average 1% better using our ensemble
framework as compared to the simple baseline training for both the datasets.
Notably for both datasets, the heavily compressed fourth and ﬁfth students perform around 3% on average better than their baseline counterparts. This provide
an excellent evidence of our framework’s eﬃcient knowledge transfer capabilities
for the heavily compressed student cases. Similar to the aforementioned datasets,
D. Walawalkar et al.
Table 7: Ablation Study for α contribution ratio grid search conducted with ResNet110
 model on CIFAR100 dataset. Weighted average technique used for calculating
eﬀective student model accuracy with higher weight given to more compressed students.
Student Test Accuracy (%) Ensemble Teacher Weighted
Test Accuracy (%) Average
0.3 69.58 64.47 63.21 55.64 39.57
0.4 68.27 65.36 62.35 57.54 40.14
0.5 71.32 69.58 67.92 62.69 45.16
0.6 72.11 67.82 65.55 60.93 43.19
0.7 71.25 69.32 67.29 62.16 47.23
0.8 71.05 70.21 68.01 62.61 45.10
0.9 69.21 66.56 63.12 58.85 45.76
our ensemble teacher provides the best Test set accuracy when compared to the
ensemble teacher accuracy of three other ensemble knowledge distillation based
techniques for ResNet32 and ResNet110 models as presented in Table 4.
Ablation Studies
Loss Contribution Ratios. A selective grid search was conducted for the optimal values of loss contribution ratios, speciﬁcally α, β, γ referenced in Equation
7. The grid search was conducted with the constraint that the ratio should sum
to one which would represent the overall loss factor. The study was carried out
using ResNet110 model on the CIFAR100 dataset. Firstly, a grid search
was conducted for α which is the normal loss contribution ratio. The other two
contribution ratio namely β, γ were kept equal to half the fraction left from
subtracting the grid search α from 1. This study is presented in Table 7. All accuracies are averaged over ﬁve runs to reduce any weight initialization eﬀect. The
value of 0.7 provided the best weighted average accuracy across all the students.
Weights of 1
5, 1 were assigned to the students, with higher importance
given to accuracy achieved by more compressed student.
With the value of α set as 0.7, a grid search was then conducted for the value
of β and γ. This study is presented in Table 8. Here also the same weighted
average technique was used to ﬁnd the eﬀective student test accuracy. β = 0.15
and γ = 0.15 gave the optimal performance and were thus selected as the ﬁnal contribution ratios for our framework. These ﬁnal ratios seem to indicate
the major importance of cross entropy loss with α = 0.7 in individual student’s
training and equal importance of intermediate and output distribution knowledge transfer with β = γ = 0.15 for the knowledge distillation process.
Knowledge distillation Temperature. A temperature variable T is used to
soften the student and ensemble teacher model logits before computing its respective softmax distribution as referenced in Equation 5. A grid search was conducted for its optimal value, which would facilitate optimum knowledge transfer
from the ensemble teacher to each student model. Similar to the previous study,
Online Ensemble Model Compression
Table 8: Ablation Study for β, γ contribution ratios grid search conducted with
ResNet110 model on CIFAR100 dataset. Weighted average technique used for
calculating combined student model test accuracy with higher weight given to more
compressed students. α is set at optimal value of 0.7 referred from Table 7.
Student Test Accuracy (%) Ensemble Teacher Weighted
Test Accuracy (%) Average
0.05 0.25 68.57 66.69 64.34 59.92 47.26
68.72 68.19 65.94 62.09 44.88
0.15 0.15 69.37 68.39 66.44 62.17 46.82
66.79 64.92 62.46 57.78 41.34
0.25 0.05 67.97 67.22 65.37 61.08 46.69
Table 9: Ablation Study for softmax temperature (T) grid search conducted with
ResNet110 model on CIFAR100 dataset. Mean accuracy computed using only
student test accuracies.
Temperature Student Test Accuracy (%) Ensemble Teacher
Test Accuracy (%) Accuracy (%)
70.29 67.16 64.22 62.46 44.75
71.89 69.56 68.43 59.48 47.26
69.14 68.18 65.33 57.52 46.33
68.35 66.77 64.36 56.41 46.45
66.58 66.95 65.67
66.92 66.28 65.33 55.97 43.06
we incorporate a ResNet110 model to train on CIFAR100 dataset. This
study is presented in Table 9. The resulting optimal value of 2 is used for all of
our conducted experiments. The study results provide evidence to the fact that
higher temperature values tend to over-soften the output logits leading to sub
optimal knowledge transfer and test accuracy gains.
Discussion
The performed experiments provide a strong evidence of the eﬃcient compression
and generalization capabilities of our framework over individual baseline training
for every compressed student model. In most of the experiments the ensemble
teacher’s test accuracy is much better than any of its ensemble students and
their baseline counterparts. This additional test accuracy gain can be attributed
to the joint ensemble knowledge learnt by the framework.
The intermediate knowledge transfer from the pseudo teacher onto each
one of the compressed students helps guide every student compressed block
to reproduce the same transformations its respective higher capacity pseudo
teacher block is learning. Enabling the low capacity compressed block to try and
imitate the higher capacity pseudo teacher block helps reduce any redundant
(sub-optimal) transformations inside the student block that would generally be
D. Walawalkar et al.
Fig. 3: Gradient Class Activation Mapping (Grad CAM) comparison of a
EﬃcientNet-B4 based ensemble pseudo teacher and one of its compressed students
with that of its respective individually trained student. The ensemble student’s CAM
is more accurate compared to that of baseline student. Also the former follows the
pseudo teacher more closely as compared to the latter, which provides evidence of the
eﬀective knowledge distillation taking place in our ensemble framework.
present during baseline training. This is substantiated by the fact that the test
accuracy gains of heavily compressed students, speciﬁcally the fourth and ﬁfth
students in the ensemble are substantial over their baseline counterparts. Figure
3 presents a comparison of the gradient based class activation mapping (Grad-
CAM) of the last block of an EﬃcientNet-B4 framework pseudo teacher and
one of its compressed students. These are compared to the Grad-CAM of the
same compressed student with baseline training. The smaller diﬀerences between
the Grad-CAMs of pseudo teacher and its ensemble student compared to those
between the pseudo teacher and the baseline student provide evidence of how
our eﬃcient knowledge distillation helps the student imitate the pseudo teacher
and learn better as compared to the baseline student.
Conclusion
We present a novel model compression technique using an ensemble knowledge
distillation learning procedure without requiring the need of any pretrained
weights. The framework manages to provide multiple compressed versions of
a given base (pseudo teacher) model simultaneously, providing gains in each
of the participating model’s test performance and in overall framework’s training time compared to each model’s individual baseline training. Comprehensive
experiments conducted using a variety of current state-of-the-art image classi-
ﬁcation based models and benchmarked academic datasets provide substantial
evidence of the framework’s eﬀectiveness. It also provides an account of the
highly modular nature of the framework which makes it easier to incorporate
any existing classiﬁcation model into the framework without any major modiﬁcations. It manages to provide multiple eﬃcient versions of the same, compressed
to varying degree without making any major manual architecture changes on the
user’s part.
Online Ensemble Model Compression