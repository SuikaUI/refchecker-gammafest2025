Overcoming catastrophic forgetting in neural
James Kirkpatricka, Razvan Pascanua, Neil Rabinowitza, Joel Venessa, Guillaume Desjardinsa,
Andrei A. Rusua, Kieran Milana, John Quana, Tiago Ramalhoa, Agnieszka Grabska-Barwinska a,
Demis Hassabisa, Claudia Clopathb, Dharshan Kumarana, and Raia Hadsella
aDeepMind, London, N1C 4AG, United Kingdom
bBioengineering department, Imperial College London, SW7 2AZ, London, United Kingdom
The ability to learn tasks in a sequential fashion is crucial to the development of
artiﬁcial intelligence. Neural networks are not, in general, capable of this and it
has been widely thought that catastrophic forgetting is an inevitable feature of
connectionist models. We show that it is possible to overcome this limitation and
train networks that can maintain expertise on tasks which they have not experienced
for a long time. Our approach remembers old tasks by selectively slowing down
learning on the weights important for those tasks. We demonstrate our approach is
scalable and effective by solving a set of classiﬁcation tasks based on the MNIST
hand written digit dataset and by learning several Atari 2600 games sequentially.
Introduction
Achieving artiﬁcial general intelligence requires that agents are able to learn and remember many
different tasks Legg and Hutter . This is particularly difﬁcult in real-world settings: the
sequence of tasks may not be explicitly labelled, tasks may switch unpredictably, and any individual
task may not recur for long time intervals. Critically, therefore, intelligent agents must demonstrate a
capacity for continual learning: that is, the ability to learn consecutive tasks without forgetting how
to perform previously trained tasks.
Continual learning poses particular challenges for artiﬁcial neural networks due to the tendency for
knowledge of previously learnt task(s) (e.g. task A) to be abruptly lost as information relevant to the
current task (e.g. task B) is incorporated. This phenomenon, termed catastrophic forgetting [French,
1999, McCloskey and Cohen, 1989, McClelland et al., 1995, Ratcliff, 1990], occurs speciﬁcally when
the network is trained sequentially on multiple tasks because the weights in the network that are
important for task A are changed to meet the objectives of task B. Whilst recent advances in machine
learning and in particular deep neural networks have resulted in impressive gains in performance
across a variety of domains , little progress has
been made in achieving continual learning. Current approaches have typically ensured that data from
all tasks are simultaneously available during training. By interleaving data from multiple tasks during
learning, forgetting does not occur because the weights of the network can be jointly optimized for
performance on all tasks. In this regime—often referred to as the multitask learning paradigm—deep
learning techniques have been used to train single agents that can successfully play multiple Atari
games [Rusu et al., 2015, Parisotto et al., 2015]. If tasks are presented sequentially, multitask learning
can only be used if the data are recorded by an episodic memory system and replayed to the network
during training. This approach ,
is impractical for learning large numbers of tasks, as in our setting it would require the amount of
memories being stored and replayed to be proportional to the number of tasks. The lack of algorithms
 
to support continual learning thus remains a key barrier to the development of artiﬁcial general
intelligence.
In marked contrast to artiﬁcial neural networks, humans and other animals appear to be able to learn
in a continual fashion [Cichon and Gan, 2015]. Recent evidence suggests that the mammalian brain
may avoid catastrophic forgetting by protecting previously-acquired knowledge in neocortical circuits
[Cichon and Gan, 2015, Hayashi-Takagi et al., 2015, Yang et al., 2009, 2014]. When a mouse acquires
a new skill, a proportion of excitatory synapses are strengthened; this manifests as an increase in
the volume of individual dendritic spines of neurons [Yang et al., 2009]. Critically, these enlarged
dendritic spines persist despite the subsequent learning of other tasks, accounting for retention of
performance several months later [Yang et al., 2009]. When these spines are selectively “erased”, the
corresponding skill is forgotten [Hayashi-Takagi et al., 2015, Cichon and Gan, 2015]. This provides
causal evidence that neural mechanisms supporting the protection of these strengthened synapses
are critical to retention of task performance. Together, these experimental ﬁndings—together with
neurobiological models [Fusi et al., 2005, Benna and Fusi, 2016]—suggest that continual learning
in the mammalian neocortex relies on a process of task-speciﬁc synaptic consolidation, whereby
knowledge about how to perform a previously acquired task is durably encoded in a proportion of
synapses that are rendered less plastic and therefore stable over long timescales.
In this work, we demonstrate that task-speciﬁc synaptic consolidation offers a novel solution to the
continual learning problem for artiﬁcial intelligence. We develop an algorithm analogous to synaptic
consolidation for artiﬁcial neural networks, which we refer to as elastic weight consolidation (EWC
for short). This algorithm slows down learning on certain weights based on how important they are to
previously seen tasks. We show how EWC can be used in supervised learning and reinforcement
learning problems to train several tasks sequentially without forgetting older ones, in marked contrast
to previous deep-learning techniques.
Elastic weight consolidation
In brains, synaptic consolidation enables continual learning by reducing the plasticity of synapses that
are vital to previously learned tasks. We implement an algorithm that performs a similar operation in
artiﬁcial neural networks by constraining important parameters to stay close to their old values. In
this section we explain why we expect to ﬁnd a solution to a new task in the neighbourhood of an
older one, how we implement the constraint, and ﬁnally how we determine which parameters are
important.
A deep neural network consists of multiple layers of linear projection followed by element-wise
non-linearities. Learning a task consists of adjusting the set of weights and biases θ of the linear
projections, to optimize performance. Many conﬁgurations of θ will result in the same performance
[Nielsen, 1989, Sussmann, 1992]; this is relevant for EWC: over-parameterization makes it likely that
there is a solution for task B, θ∗
B, that is close to the previously found solution for task A, θ∗
learning task B, EWC therefore protects the performance in task A by constraining the parameters to
stay in a region of low error for task A centered around θ∗
A, as shown schematically in Figure 1. This
constraint is implemented as a quadratic penalty, and can therefore be imagined as a spring anchoring
the parameters to the previous solution, hence the name elastic. Importantly, the stiffness of this
spring should not be the same for all parameters; rather, it should be greater for those parameters that
matter most to the performance during task A.
In order to justify this choice of constraint and to deﬁne which weights are most important for a task,
it is useful to consider neural network training from a probabilistic perspective. From this point of
view, optimizing the parameters is tantamount to ﬁnding their most probable values given some data
D. We can compute this conditional probability p(θ|D) from the prior probability of the parameters
p(θ) and the probability of the data p(D|θ) by using Bayes’ rule:
log p(θ|D) = log p(D|θ) + log p(θ) −log p(D)
Note that the log probability of the data given the parameters log p(D|θ) is simply the negative of the
loss function for the problem at hand −L(θ). Assume that the data is split into two independent parts,
one deﬁning task A (DA) and the other task B (DB). Then, we can re-arrange equation 1:
log p(θ|D) = log p(DB|θ) + log p(θ|DA) −log p(DB)
Note that the left hand side is still describing the posterior probability of the parameters given the
entire dataset, while the right hand side only depends on the loss function for task B log p(DB|θ).
Figure 1: elastic weight consolidation (EWC) ensures task A is remembered whilst training on task B. Training
trajectories are illustrated in a schematic parameter space, with parameter regions leading to good performance
on task A (gray) and on task B (cream). After learning the ﬁrst task, the parameters are at θ∗
A. If we take gradient
steps according to task B alone (blue arrow), we will minimize the loss of task B but destroy what we have learnt
for task A. On the other hand, if we constrain each weight with the same coefﬁcient (green arrow) the restriction
imposed is too severe and we can only remember task A at the expense of not learning task B. EWC, conversely,
ﬁnds a solution for task B without incurring a signiﬁcant loss on task A (red arrow) by explicitly computing how
important weights are for task A.
All the information about task A must therefore have been absorbed into the posterior distribution
p(θ|DA). This posterior probability must contain information about which parameters were important
to task A and is therefore the key to implementing EWC. The true posterior probability is intractable,
so, following the work on the Laplace approximation by Mackay [MacKay, 1992], we approximate
the posterior as a Gaussian distribution with mean given by the parameters θ∗
A and a diagonal precision
given by the diagonal of the Fisher information matrix F. F has three key properties [Pascanu and
Bengio, 2013]: (a) it is equivalent to the second derivative of the loss near a minimum, (b) it can
be computed from ﬁrst-order derivatives alone and is thus easy to calculate even for large models,
and (c) it is guaranteed to be positive semi-deﬁnite. Note that this approach is similar to expectation
propagation where each subtask is seen as a factor of the posterior [Eskin et al., 2004]. Given this
approximation, the function L that we minimize in EWC is:
L(θ) = LB(θ) +
2 Fi(θi −θ∗
where LB(θ) is the loss for task B only, λ sets how important the old task is compared to the new one
and i labels each parameter.
When moving to a third task, task C, EWC will try to keep the network parameters close to the
learned parameters of both task A and B. This can be enforced either with two separate penalties, or
as one by noting that the sum of two quadratic penalties is itself a quadratic penalty.
EWC allows continual learning in a supervised learning context
We start by addressing the problem of whether elastic weight consolidation could allow deep neural
networks to learn a set of complex tasks without catastrophic forgetting. In particular, we trained a
fully connected multilayer neural network on several supervised learning tasks in sequence. Within
each task, we trained the neural network in the traditional way, namely by shufﬂing the data and
processing it in small batches. After a ﬁxed amount of training on each task, however, we allowed no
further training on that task’s dataset.
We constructed the set of tasks from the problem of classifying hand written digits from the MNIST
[LeCun et al., 1998] dataset, according to a scheme previously used in the continual learning literature
[Srivastava et al., 2013, Goodfellow et al., 2014]. For each task, we generated a ﬁxed, random
permutation by which the input pixels of all images would be shufﬂed. Each task was thus of equal
difﬁculty to the original MNIST problem, though a different solution would be required for each.
Detailed description of the settings used can be found in Appendix 4.1.
Training on this sequence of tasks with plain stochastic gradient descent (SGD) incurs catastrophic
forgetting, as demonstrated in Figure 2A. The blue curves show performance on the testing sets of
two different tasks. At the point at which the training regime switches from training on the ﬁrst
task (A) to training on the second (B), the performance for task B falls rapidly, while for task A it
climbs steeply. The forgetting of task A compounds further with more training time, and the addition
Figure 2: Results on the permuted MNIST task. A: Training curves for three random permutations A, B and C
using EWC(red), L2 regularization (green) and plain SGD(blue). Note that only EWC is capable of mantaining
a high performance on old tasks, while retaining the ability to learn new tasks. B: Average performance across
all tasks using EWC (red) or SGD with dropout regularization (blue). The dashed line shows the performance
on a single task only. C: Similarity between the Fisher information matrices as a function of network depth for
two different amounts of permutation. Either a small square of 8x8 pixels in the middle of the image is permuted
(grey) or a large square of 26x26 pixels is permuted (black). Note how the more different the tasks are, the
smaller the overlap in Fisher information matrices in early layers.
of subsequent tasks. This problem cannot be countered by regularizing the network with a ﬁxed
quadratic constraint for each weight (green curves, L2 regularization): here, the performance in task
A degrades much less severely, but task B cannot be learned properly as the constraint protects all
weights equally, leaving little spare capacity for learning on B. However, when we use EWC, and thus
take into account how important each weight is to task A, the network can learn task B well without
forgetting task A (red curves). This is exactly the expected behaviour described diagrammatically in
Previous attempts to solve the continual learning problem for deep neural networks have relied upon
careful choice of network hyperparameters, together with other standard regularization methods, in
order to mitigate catastrophic forgetting. However, on this task, they have only achieved reasonable
results on up to two random permutations [Srivastava et al., 2013, Goodfellow et al., 2014]. Using a
similar cross-validated hyperparameter search as [Goodfellow et al., 2014], we compared traditional
dropout regularization to EWC. We ﬁnd that stochastic gradient descent with dropout regularization
alone is limited, and that it does not scale to more tasks (Figure 2B). In contrast, EWC allows a large
number of tasks to be learned in sequence, with only modest growth in the error rates.
Given that EWC allows the network to effectively squeeze in more functionality into a network with
ﬁxed capacity, we might ask whether it allocates completely separate parts of the network for each
task, or whether capacity is used in a more efﬁcient fashion by sharing representation. To assess this,
we determined whether each task depends on the same sets of weights, by measuring the overlap
between pairs of tasks’ respective Fisher information matrices (see Appendix 4.3). A small overlap
means that the two tasks depend on different sets of weights (i.e. EWC subdivides the network’s
weights for different tasks); a large overlap indicates that weights are being used for both the two tasks
(i.e. EWC enables sharing of representations). Figure 2C shows the overlap as a function of depth.
As a simple control, when a network is trained on two tasks which are very similar to each other
(two versions of MNIST where only a few pixels are permutated), the tasks depend on similar sets of
weights throughout the whole network (grey curve). When then the two tasks are more dissimilar
from each other, the network begins to allocate separate capacity (i.e. weights) for the two tasks
(black line). Nevertheless, even for the large permutations, the layers of the network closer to the
output are indeed being reused for both tasks. This reﬂects the fact that the permutations make the
input domain very different, but the output domain (i.e. the class labels) is shared.
EWC allows continual learning in a reinforcement learning context
We next tested whether elastic weight consolidation could support continual learning in the far
more demanding reinforcement learning (RL) domain. In RL, agents dynamically interact with
the environment in order to develop a policy that maximizes cumulative future reward. We asked
whether Deep Q Networks (DQNs)—an architecture that has achieved impressive successes in such
challenging RL settings [Mnih et al., 2015]—could be harnessed with EWC to successfully support
continual learning in the classic Atari 2600 task set [Bellemare et al., 2013]. Speciﬁcally, each
experiment consisted of ten games chosen randomly from those that are played at human level or
above by DQN. At training time, the agent was exposed to experiences from each game for extended
periods of time. The order of presentation of the games was randomized and allowed for returning to
the same games several times. At regular intervals we would also test the agent’s score on each of the
ten games, without allowing the agent to train on them (Figure 3A).
Notably, previous reinforcement learning approaches to continual learning have either relied on either
adding capacity to the network [Ring, 1998, Rusu et al., 2016] or on learning each task in separate
networks, which are then used to train a single network that can play all games[Rusu et al., 2015,
Parisotto et al., 2015]. In contrast, the EWC approach presented here makes use of a single network
with ﬁxed resources (i.e. network capacity) and has minimal computational overhead.
In addition to using EWC to protect previously-acquired knowledge, we used the RL domain to
address a broader set of requirements that are needed for successful continual learning systems: in
particular, higher-level mechanisms are needed to infer which task is currently being performed,
detect and incorporate novel tasks as they are encountered, and allow for rapid and ﬂexible switching
between tasks [Collins and Frank, 2013]. In the primate brain, the prefrontal cortex is widely viewed
as supporting these capabilities by sustaining neural representations of task context that exert topdown gating inﬂuences on sensory processing, working memory, and action selection in lower-level
regions [O’Reilly and Frank, 2006, Mante et al., 2013, Miller and Cohen, 2001, Doya et al., 2002].
Inspired by this evidence, we used an agent very similar to that described in [van Hasselt et al.,
2016] with few differences: (a) a network with more parameters, (b) a smaller transition table, (c)
task-speciﬁc bias and gains at each layer, (d) the full action set in Atari, (e) a task-recognition model,
and (e) the EWC penalty. Full details of hyper-parameters are described in Appendix app:atari. Here
we brieﬂy describe the two most important modiﬁcations to the agent: the task-recognition module,
and the implementation of the EWC penalty.
We treat the task context as the latent variable of a Hidden Markov Model. Each task is therefore
associated to an underlying generative model of the observations. The main distinguishing feature of
our approach is that we allow for the addition of new generative models if they explain recent data
better than the existing pool of models by using a training procedure inspired by the forget me not
process[Kieran et al., 2016] (see Appendix 4.2).
In order to apply EWC, we compute the Fisher information matrix at each task switch. For each
task, a penalty is added with anchor point given by the current value of the parameters and with
weights given by the Fisher information matrix times a scaling factor λ which was optimized by
hyperparameter search. We only added an EWC penalty to games which had experienced at least 20
million frames.
We also allowed the DQN agents to maintain separate short-term memory buffers for each inferred
task: these allow action values for each task to be learned off-policy using an experience replay
mechanism [Mnih et al., 2015]. As such, the overall system has memory on two time-scales: over
short time-scales, the experience replay mechanism allows learning in DQN to be based on the
interleaved and uncorrelated experiences [Mnih et al., 2015]. At longer time scales, know-how across
tasks is consolidated by using EWC. Finally, we allowed a small number of network parameters to be
game-speciﬁc, rather than shared across games. In particular, we allowed each layer of the network
to have biases and per element multiplicative gains that were speciﬁc to each game.
We compare the performance of agents which use EWC (red) with ones that do not (blue) over sets
of ten games in Figure 3. We measure the performance as the total human-normalized score across
all ten games. We average across random seeds and over the choice of which ten games were played
(see Appendix 4.2). We also clip the human-normalized score for each game to 1. Our measure of
performance is therefore a number with a maximum of 10 (at least at human level on all games)
where 0 means the agent is as good as a random agent. If we rely on plain gradient descent methods
as in [Mnih et al., 2015], the agent never learns to play more than one game and the harm inﬂicted
by forgetting the old games means that the total human-normalized score remains below one. By
using EWC, however, the agents do indeed learn to play multiple games. As a control, we also
considered the beneﬁt to the agent if we explicitly provided the agent with the true task label (Figure
3B, brown), rather than relying on the learned task recognition through the FMN algorithm (red). The
improvement here was only modest.
Figure 3: Results on Atari task. A: Schedule of games. Black bars indicate the sequential training periods
(segments) for each game. After each training segment, performance on all games is measured. The EWC
constraint is only activated to protect an agent’s performance on each game once the agent has experienced 20
million frames in that game. B: Total scores for each method across all games. Red curve denotes the network
which infers the task labels using the Forget Me Not algorithm; brown curve is the network provided with the
task labels. The EWC and SGD curves start diverging when games start being played again that have been
protected by EWC. C: Sensitivity of a single-game DQN, trained on Breakout, to noise added to its weights. The
performance on Breakout is shown as a function of the magnitude (standard deviation) of the weight perturbation.
The weight perturbation is drawn from a zero mean Gaussian with covariance that is either uniform (black; i.e.
targets all weights equally), the inverse Fisher ((F + λI)−1; blue; i.e. mimicking weight changes allowed by
EWC), or uniform within the nullspace of the Fisher (orange; i.e. targets weights that the Fisher estimates that
the network output is entirely invariant to). To evaluate the score, we ran the agent for ten full game episodes,
drawing a new random weight perturbation for every timestep.
While augmenting the DQN agent with EWC allows it to learn many games in sequence without
suffering from catastrophic forgetting, it does not reach the score that would have been obtained by
training ten separate DQNs (see Figure 1 in Appendix 4.2). One possible reason for this is that we
consolidated weights for each game based on a tractable approximation of parameter uncertainty, the
Fisher Information. We therefore sought to test the quality of our estimates empirically. To do so, we
trained an agent on a single game, and measured how perturbing the network parameters affected the
agent’s score. Regardless of which game the agent was trained on, we observed the same patterns,
shown in Figure 3C. First, the agent was always more robust to parameter perturbations shaped by the
inverse of the diagonal of the Fisher Information (blue), as opposed to uniform perturbations (black).
This validates that the diagonal of the Fisher is a good estimate of how important a certain parameter
is. Within our approximation, perturbing in the nullspace should have no effect on performance at
all on performance. Empirically, however, we observe that perturbing in this space (orange) has the
same effect as perturbing in the inverse Fisher space. This suggests that we are over-conﬁdent about
certain parameters being unimportant: it is therefore likely that the chief limitation of the current
implementation is that it under-estimates parameter uncertainty.
Discussion
We present a novel algorithm, elastic weight consolidation, that addresses the signiﬁcant problem
continual learning poses for neural networks. EWC allows knowledge of previous tasks to be
protected during new learning, thereby avoiding catastrophic forgetting of old abilities. It does so by
selectively decreasing the plasticity of weights, and thus has parallels with neurobiological models of
synaptic consolidation. We implement EWC as a soft, quadratic constraint whereby each weight is
pulled back towards its old values by an amount proportional to its importance for performance on
previously-learnt tasks. To the extent that tasks share structure, networks trained with EWC reuse
shared components of the network. We further show that EWC can be effectively combined with
deep neural networks to support continual learning in challenging reinforcement learning scenarios,
such as Atari 2600 games.
The EWC algorithm can be grounded in Bayesian approaches to learning. Formally, when there
is a new task to be learnt, the network parameters are tempered by a prior which is the posterior
distribution on the parameters given data from previous task(s). This enables fast learning rates on
parameters that are poorly constrained by the previous tasks, and slow learning rates for those which
are crucial.
There has been previous work [French and Chater, 2002, Eaton and Ruvolo, 2013] using a quadratic
penalty to approximate old parts of the dataset, but these applications have been limited to small
models. Speciﬁcally, [French and Chater, 2002] used random inputs to compute a quadratic approximation to the energy surface. Their approach is slow, as it requires re-computing the curvature at
each sample. The ELLA algorithm described in [Eaton and Ruvolo, 2013] requires computing and
inverting matrices with a dimensionality equal to the number of parameters being optimized, therefore
it has been mainly applied to linear and logistic regressions. In contrast, EWC has a run time which is
linear in both the number of parameters and the number of training examples. We could only achieve
this low computational complexity by making several simpliﬁcations, most notably by approximating
the posterior distribution of the parameters on a task (i.e. the weight uncertainties) by a factorized
Gaussian, and by computing its variance using a point-estimate of the parameters, via the diagonal of
the Fisher Information matrix. Despite its low computational cost and empirical successes—even in
the setting of challenging RL domains—our use of a point estimate of the posterior’s variance (as in a
Laplace approximation) does constitute a signiﬁcant weakness (see Fig 4C). Our initial explorations
suggest that one might improve on this local estimate by using Bayesian neural networks [Blundell
et al., 2015].
While this paper has primarily focused on building an algorithm out of neurobiological observations, it
is also instructive to consider whether the algorithm’s successes can feed back into our understanding
of the brain. In particular, we see considerable parallels between EWC and two computational
theories of synaptic plasticity.
In this respect, the perspective we offer here aligns with a recent proposal that each synapse not
only stores its current weight, but also an implicit representation of its uncertainty about that weight
[Aitchison and Latham, 2015]. This idea is grounded in observations that post-synaptic potentials are
highly variable in amplitude (suggestive of sampling from the weight posterior during computation),
and that those synapses which are more variable are more amenable to potentiation or depression
(suggestive of updating the weight posterior). While we do not explore the computational beneﬁts
of sampling from a posterior here, our work aligns with the notion that weight uncertainty should
inform learning rates. We take this one step further, to emphasize that consolidating the high precision
weights enables continual learning over long time scales. With EWC, three values have to be stored
for each synapse: the weight itself, its variance and its mean. Interestingly, synapses in the brain also
carry more than one piece of information. For example, the state of the short-term plasticity could
carry information on the variance [Aitchison and Latham, 2015, Pﬁster et al., 2010]. The weight for
the early phase of plasticity [Clopath et al., 2008] could encode the current synaptic strength, whereas
the weight associated with the late-phase of plasticity or the consolidated phase could encode the
mean weight.
The ability to learn tasks in succession without forgetting is a core component of biological and
artiﬁcial intelligence. In this work we show that an algorithm that supports continual learning—which
takes inspiration from neurobiological models of synaptic consolidation—can be combined with deep
neural networks to achieve successful performance in a range of challenging domains. In doing so,
we demonstrate that current neurobiological theories concerning synaptic consolidation do indeed
scale to large-scale learning systems. This provides prima facie evidence that these principles may be
fundamental aspects of learning and memory in the brain.
Acknowledgements. We would like to thank P. Dayan, D. Wierstra, S. Mohamed, Yee Whye Teh
and K. Kavukcuoglu.