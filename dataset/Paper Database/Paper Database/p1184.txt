Learning Entity and Relation Embeddings for Knowledge Graph Completion
Yankai Lin1, Zhiyuan Liu1∗, Maosong Sun1,2, Yang Liu3, Xuan Zhu3
1 Department of Computer Science and Technology, State Key Lab on Intelligent Technology and Systems,
National Lab for Information Science and Technology, Tsinghua University, Beijing, China
2 Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China
3 Samsung R&D Institute of China, Beijing, China
Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the
approach of knowledge graph embeddings. Recently,
models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these
models simply put both entities and relations within
the same semantic space. In fact, an entity may have
multiple aspects and various relations may focus on
different aspects of entities, which makes a common
space insufﬁcient for modeling. In this paper, we propose TransR to build entity and relation embeddings in
separate entity space and relation spaces. Afterwards,
we learn embeddings by ﬁrst projecting entities from
entity space to corresponding relation space and then
building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classiﬁcation and relational fact extraction. Experimental results show significant and consistent improvements compared to stateof-the-art baselines including TransE and TransH. The
source code of this paper can be obtained from https:
//github.com/mrlyk423/relation extraction.
Introduction
Knowledge graphs encode structured information of entities and their rich relations. Although a typical knowledge
graph may contain millions of entities and billions of relational facts, it is usually far from complete. Knowledge
graph completion aims at predicting relations between entities under supervision of the existing knowledge graph.
Knowledge graph completion can ﬁnd new relational facts,
which is an important supplement to relation extraction from
plain texts.
Knowledge graph completion is similar to link prediction
in social network analysis, but more challenging for the following reasons: (1) nodes in knowledge graphs are entities
with different types and attributes; and (2) edges in knowledge graphs are relations of different types. For knowledge
graph completion, we not only determine whether there is
∗Corresponding author: Zhiyuan Liu ( ).
Copyright c⃝2015, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
a relation between two entities or not, but also predict the
speciﬁc type of the relation.
For this reason, traditional approach of link prediction
is not capable for knowledge graph completion. Recently,
a promising approach for the task is embedding a knowledge graph into a continuous vector space while preserving
certain information of the graph. Following this approach,
many methods have been explored, which will be introduced
in detail in Section “Related Work”.
Among these methods, TransE and
TransH are simple and effective, achieving the state-of-the-art prediction performance. TransE, inspired by , learns vector embeddings
for both entities and relationships. These vector embeddings
are set in Rk and we denote with the same letters in boldface. The basic idea behind TransE is that, the relationship
between two entities corresponds to a translation between
the embeddings of entities, that is, h + r ≈t when (h, r, t)
holds. Since TransE has issues when modeling 1-to-N, Nto-1 and N-to-N relations, TransH is proposed to enable an
entity having different representations when involved in various relations.
Both TransE and TransH assume embeddings of entities
and relations being in the same space Rk. However, an entity may have multiple aspects, and various relations focus
on different aspects of entities. Hence, it is intuitive that
some entities are similar and thus close to each other in
the entity space, but are comparably different in some speciﬁc aspects and thus far away from each other in the corresponding relation spaces. To address this issue, we propose a
new method, which models entities and relations in distinct
spaces, i.e., entity space and multiple relation spaces (i.e.,
relation-speciﬁc entity spaces), and performs translation in
the corresponding relation space, hence named as TransR.
The basic idea of TransR is illustrated in Fig. 1. For each
triple (h, r, t), entities in the entity space are ﬁrst projected
into r-relation space as hr and tr with operation Mr, and
then hr + r ≈tr. The relation-speciﬁc projection can make
the head/tail entities that actually hold the relation (denoted
as colored circles) close with each other, and also get far
away from those that do not hold the relation (denoted as
colored triangles).
Moreover, under a speciﬁc relation, head-tail entity pairs
usually exhibit diverse patterns. It is insufﬁcient to build
Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence
Entity Space
Relation Space of r
Figure 1: Simple illustration of TransR.
only a single relation vector to perform all translations from
head to tail entities. For example, the head-tail entities of
the relation “location location contains” have many patterns
such as country-city, country-university, continent-country
and so on. Following the idea of piecewise linear regression
 , we extend TransR by clustering
diverse head-tail entity pairs into groups and learning distinct relation vectors for each group, named as cluster-based
TransR (CTransR).
We evaluate our models with the tasks of link prediction,
triple classiﬁcation and relation fact extraction on benchmark datasets of WordNet and Freebase. Experiment results
show signiﬁcant and consistent improvements compared to
state-of-the-art models.
Related Models
TransE and TransH
As mentioned in Section “Introduction”, TransE wants h+r ≈t when (h, r, t) holds. This indicates
that (t) should be the nearest neighbor of (h + r). Hence,
TransE assumes the score function
fr(h, t) = ∥h + r −t∥2
is low if (h, r, t) holds, and high otherwise.
TransE applies well to 1-to-1 relations but has issues for
N-to-1, 1-to-N and N-to-N relations. Take a 1-to-N relation r
for example. ∀i ∈{0, . . . , m}, (hi, r, t) ∈S. This indicates
that h0 = . . . = hm, which does not comport with the facts.
To address the issue of TransE when modeling N-to-1,
1-to-N and N-to-N relations, TransH is
proposed to enable an entity to have distinct distributed representations when involved in different relations. For a relation r, TransH models the relation as a vector r on a hyperplane with wr as the normal vector. For a triple (h, r, t), the
entity embeddings h and t are ﬁrst projected to the hyperplane of wr, denoted as h⊥and t⊥. Then the score function
is deﬁned as
fr(h, t) = ∥h⊥+ r −t⊥∥2
If we restrict ∥wr∥2 = 1, we will have h⊥= h −w⊤
and t⊥= t−w⊤
r twr. By projecting entity embeddings into
relation hyperplanes, it allows entities playing different roles
in different relations.
Other Models
Besides TransE and TransH, there are also many other methods following the approaches of knowledge graph embedding. Here we introduce several typical models, which will
also be compared as baselines with our models in experiments.
Unstructured Model (UM). UM model was proposed as a naive version of TransE by
assigning all r = 0, leading to score function fr(h, t) =
2. This model cannot consider differences of relations.
Structured Embedding (SE). SE model designs two relation-speciﬁc matrices for head and
tail entities, i.e., Mr,1 and Mr,2, and deﬁnes the score function as an L1 distance between two projected vectors, i.e.,
fr(h, t) = ∥Mr,1h −Mr,2t∥1. Since the model has two
separate matrices for optimization, it cannot capture precise
relations between entities and relations.
Single Layer Model (SLM). SLM model was proposed
as a naive baseline of NTN . The score
function of SLM model is deﬁned as
fr(h, t) = u⊤
r g(Mr,1h + Mr,2t),
where Mr,1 and Mr,2 are weight matrices, and g() is the
tanh operation. SLM is a special case of NTN when the
tensor in NTN is set to 0.
Semantic Matching Energy (SME). SME model aims to capture correlations between entities and relations via multiple matrix products and
Hadamard product. SME model simply represents each relation using a single vector, which interacts with entity vectors via linear matrix products, with all relations share the
same parameters. SME considers two deﬁnitions of semantic matching energy functions for optimization, including
the linear form
fr(h, t) = (M1h + M2r + b1)⊤(M3t + M4r + b2), (4)
and the bilinear form
fr(h, t) =
 (M1h)⊗(M2r)+b1
⊤ (M3t)⊗(M4r)+b2
where M1, M2, M3 and M4 are weight matrices, ⊗is the
Hadamard product, b1 and b2 are bias vectors. In , the bilinear form of SME is re-deﬁned with 3-way
tensors instead of matrices.
Latent Factor Model (LFM). LFM model 
considers second-order correlations between entity embeddings using a quadratic form, and deﬁnes a bilinear score
function fr(h, t) = h⊤Mrt.
Neural Tensor Network (NTN). NTN model deﬁnes an expressive score function for graph embedding as follows,
fr(h, t) = u⊤
r g(h⊤Mrt + Mr,1h + Mr,2t + br),
where ur is a relation-speciﬁc linear layer, g() is the
tanh operation, Mr ∈Rd×d×k is a 3-way tensor, and
Mr,1, Mr,2 ∈Rk×d are weight matrices. Meanwhile, the
corresponding high complexity of NTN may prevent it from
efﬁciently applying on large-scale knowledge graphs.
In experiments we will also compare with RESCAL, a
collective matrix factorization model presented in .
Our Method
To address the representation issue of TransE and TransH,
we propose TransR, which represent entities and relations
in distinct semantic space bridged by relation-speciﬁc matrices.
Both TransE and TransH assume embeddings of entities and
relations within the same space Rk. But relations and entities are completely different objects, it may be not capable
to represent them in a common semantic space. Although
TransH extends modeling ﬂexibility by employing relation
hyperplanes, it does not perfectly break the restrict of this assumption. To address this issue, we propose a new method,
which models entities and relations in distinct spaces, i.e.,
entity space and relation spaces, and performs translation
in relation space, hence named as TransR.
In TransR, for each triple (h, r, t), entities embeddings are
set as h, t ∈Rk and relation embedding is set as r ∈Rd.
Note that, the dimensions of entity embeddings and relation
embeddings are not necessarily identical, i.e., k ̸= d.
For each relation r, we set a projection matrix Mr ∈
Rk×d, which may projects entities from entity space to relation space. With the mapping matrix, we deﬁne the projected
vectors of entities as
The score function is correspondingly deﬁned as
fr(h, t) = ∥hr + r −tr∥2
In practice, we enforce constraints on the norms of the embeddings h, r, t and the mapping matrices, i.e. ∀h, r, t,
we have ∥h∥2 ≤1, ∥r∥2 ≤1, ∥t∥2 ≤1, ∥hMr∥2 ≤
1, ∥tMr∥2 ≤1.
Cluster-based TransR (CTransR)
The above mentioned models, including TransE, TransH and
TransR, learn a unique vector for each relation, which may
be under-representative to ﬁt all entity pairs under this relation, because these relations are usually rather diverse. In
order to better model these relations, we incorporate the idea
of piecewise linear regression to
extend TransR.
The basic idea is that, we ﬁrst segment input instances
into several groups. Formally, for a speciﬁc relation r, all
entity pairs (h, t) in the training data are clustered into multiple groups, and entity pairs in each group are expected to
exhibit similar r relation. All entity pairs (h, t) are represented with their vector offsets (h −t) for clustering, where
h and t are obtained with TransE. Afterwards, we learn a
separate relation vector rc for each cluster and matrix Mr
for each relation, respectively. We deﬁne the projected vectors of entities as hr,c = hMr and tr,c = tMr, and the
score function is deﬁned as
fr(h, t) = ∥hr,c + rc −tr,c∥2
2 + α∥rc −r∥2
where ∥rc −r∥2
2 aims to ensure cluster-speciﬁc relation vector rc not too far away from the original relation vector r,
and α controls the effect of this constraint. Besides, same to
TransR, CTransR also enforce constraints on norm of embeddings h, r, t and mapping matrices.
Training Method and Implementation Details
We deﬁne the following margin-based score function as objective for training
(h′,r,t′)∈S′
 0, fr(h, t) + γ −fr(h′, t′)
where max(x, y) aims to get the maximum between x and
y, γ is the margin, S is the set of correct triples and S′ is the
set of incorrect triples.
Existing knowledge graphs only contain correct triples. It
is routine to corrupt correct triples (h, r, t) ∈S by replacing
entities, and construct incorrect triples (h′, r, t′) ∈S′. When
corrupting the triple, we follow and assign different probabilities for head/tail entity replacement.
For those 1-to-N, N-to-1 and N-to-N relations, by giving
more chance to replace the “one” side, the chance of generating false-negative instances will be reduced. In experiments, we will denote the traditional sampling method as
“unif” and the new method in as “bern”.
The learning process of TransR and CTransR is carried
out using stochastic gradient descent (SGD). To avoid over-
ﬁtting, we initialize entity and relation embeddings with results of TransE, and initialize relation matrices as identity
Experiments and Analysis
Data Sets and Experiment Setting
In this paper, we evaluate our methods with two typical
knowledge graphs, built with WordNet and
Freebase . WordNet provides semantic knowledge of words. In WordNet, each entity is a synset
consisting of several words, corresponding to a distinct word
sense. Relationships are deﬁned between synsets indicating their lexical relations, such as hypernym, hyponym,
meronym and holonym. In this paper, we employ two data
sets from WordNet, i.e., WN18 used in 
and WN11 used in . WN18 contains 18 relation types and WN11 contains 11. Freebase provides general facts of the world. For example, the triple (Steve Jobs,
founded, Apple Inc.) builds a relation of founded between
the name entity Steve Jobs and the organization entity Apple
Inc. In this paper, we employ two data sets from Freebase,
i.e., FB15K used in and FB13 used in
 . We list statistics of these data sets in
Table 1: Statistics of data sets.
Link Prediction
Link prediction aims to predict the missing h or t for a relation fact triple (h, r, t), used in . In this task, for each position of missing entity, the
system is asked to rank a set of candidate entities from the
knowledge graph, instead of only giving one best result. As
set up in , we conduct experiments
using the data sets WN18 and FB15K.
In testing phase, for each test triple (h, r, t), we replace
the head/tail entity by all entities in the knowledge graph,
and rank these entities in descending order of similarity
scores calculated by score function fr. Following , we use two measures as our evaluation metric: (1) mean rank of correct entities; and (2) proportion of
correct entities in top-10 ranked entities (Hits@10). A good
link predictor should achieve lower mean rank or higher
Hits@10. In fact, a corrupted triple may also exist in knowledge graphs, which should be also considered as correct.
However, the above evaluation may under-estimate those
systems that rank these corrupted but correct triples high.
Hence, before ranking we may ﬁlter out these corrupted
triples which have appeared in knowledge graph. We name
the ﬁrst evaluation setting as “Raw” and the latter one as
Since we use the same data sets, we compare our models with baselines reported in . For experiments of TransR and CTransR, we select learning rate λ for SGD among {0.1, 0.01, 0.001}, the
margin γ among {1, 2, 4} , the dimensions of entity embedding k and relation embedding d among {20, 50, 100} , the
batch size B among {20, 120, 480, 1440, 4800}, and α for
CTransR among {0.1, 0.01, 0.001}. The best conﬁguration
is determined according to the mean rank in validation set.
The optimal conﬁgurations are λ = 0.001, γ = 4, k = 50,
d = 50, B = 1440, α = 0.001 and taking L1 as dissimilarity on WN18; λ = 0.001, γ = 1, k = 50, d = 50,
B = 4800, α = 0.01 and taking L1 as dissimilarity on
FB15K. For both datasets, we traverse all the training triplets
for 500 rounds.
Evaluation results on both WN18 and FB15K are shown
in Table 2. From the table we observe that: (1) TransR
and CTransR outperform other baseline methods including
TransE and TransH signiﬁcantly and consistently. It indicates that TransR ﬁnds a better trade-off between model
complexity and expressivity. (2) CTransR performs better
than TransR, which indicates that we should build ﬁnegrained models to handle complicated internal correlations
under each relation type. CTransR is a preliminary exploration; it will be our future work to build more sophisticated models for this purpose. (3) The “bern” sampling
trick works well for both TransH and TransR, especially on
FB15K which have much more relation types.
In Table 3, we show separate evaluation results by mapping properties of relations 1 on FB15K. We can see TransR
achieves great improvement consistently on all mapping categories of relations, especially when (1) predicting “1-to-1”
relations, which indicates that TransR provides more precise
representation for both entities and relation and their complex correlations, as illustrated in Fig. 1; and (2) predicting
the 1 side for “1-to-N” and “N-to-1” relations, which shows
the ability of TransR to discriminate relevant from irrelevant
entities via relation-speciﬁc projection.
Table 4: ⟨Head, Tail⟩examples of some clusters for the relation “location location contains”.
⟨Head, Tail⟩
⟨Africa, Congo⟩, ⟨Asia, Nepal⟩, ⟨Americas, Aruba⟩,
⟨Oceania, Federated States of Micronesia⟩
⟨United States of America, Kankakee⟩, ⟨England, Bury St
Edmunds⟩, ⟨England, Darlington⟩, ⟨Italy, Perugia⟩
⟨Georgia, Chatham County⟩, ⟨Idaho, Boise⟩, ⟨Iowa, Polk
County⟩, ⟨Missouri, Jackson County⟩, ⟨Nebraska, Cass
⟨Sweden, Lund University⟩, ⟨England, King’s College
at Cambridge⟩, ⟨Fresno, California State University at
Fresno⟩, ⟨Italy, Milan Conservatory⟩
Table 4 gives some cluster examples for the relation “location location contains” in FB15K training triples. We can
ﬁnd obvious patterns that: Cluster#1 is about continent containing country, Cluster#2 is about country containing city,
Cluster#3 is about state containing county, and Cluster#4
is about country containing university. It is obvious that by
clustering we can learn more precise and ﬁne-grained relation embeddings, which can further help improve the performance of knowledge graph completion.
Triple Classiﬁcation
Triple classiﬁcation aims to judge whether a given triple
(h, r, t) is correct or not. This is a binary classiﬁcation
task, which has been explored in for evaluation. In this task, we use three
data sets, WN11, FB13 and FB15K, following , where the ﬁrst two datasets are used in already have negative triples, which are
obtained by corrupting correct triples. As FB15K with negative triples has not been released by previous work, we construct negative triples following the same setting in . For triple classiﬁcation, we set a relationspeciﬁc threshold δr. For a triple (h, r, t), if the dissimilarity
score obtained by fr is below δr, the triple will be classiﬁed
1Mapping properties of relations follows the same rules in .
Table 2: Evaluation results on link prediction.
Hits@10 (%)
Hits@10 (%)
Unstructured 
RESCAL 
SE 
SME (linear) 
SME (bilinear) 
LFM 
TransE 
TransH (unif) 
TransH (bern) 
TransR (unif)
TransR (bern)
CTransR (unif)
CTransR (bern)
Table 3: Evaluation results on FB15K by mapping properties of relations. (%)
Predicting Head(Hits@10)
Predicting Tail(Hits@10)
Relation Category
Unstructured 
SE 
SME (linear) 
SME (bilinear) 
TransE 
TransH (unif) 
TransH (bern) 
TransR (unif)
TransR (bern)
CTransR (unif)
CTransR (bern)
as positive, otherwise negative. δr is optimized by maximizing classiﬁcation accuracies on the validation set.
For WN11 and FB13, we compare our models with baseline methods reported in who used the
same data sets. As mentioned in , for a
fair comparison, all reported results are without combination with word embedding.
Since FB15K is generated by ourselves according to the
strategy in , the evaluation results are not
able to compare directly with those reported in . Hence, we implement TransE and TransH, and use
the NTN code released by , and evaluate
on our FB15K data set for comparison.
For experiments of TransR we select learning rate λ for
SGD among {0.1, 0.01, 0.001, 0.0001}, the margin γ among
{1, 2, 4} , the dimensions of entity embedding k , relation
embedding d among {20, 50, 100} and the batch size B
among {20, 120, 480, 960, 4800} . The best conﬁguration is
determined according to accuracy in validation set.The optimal conﬁgurations are: λ = 0.001, γ = 4, k, d = 20, B =
120 and taking L1 as dissimilarity on WN11; λ = 0.0001,
γ = 2, k, d = 100 and B = 480 and taking L1 as dissimilarity on FB13. For both datasets, we traverse all the training
triples for 1000 rounds.
Evaluation results of triple classiﬁcation is shown in Table
5. From Table 5, we observe that: (1) On WN11, TransR signiﬁcantly outperforms baseline methods including TransE
and TransH. (2) Neither TransE, TransH nor TransR can
outperform the most expressive model NTN on FB13. In
contrast, on the larger data set FB15K, TransE, TransH and
TransR perform much better than NTN. The results may correlate with the characteristics of data sets: There are 1, 345
relation types in FB15K and only 13 relations types in FB13.
Meanwhile, the number of entities and relational facts in the
two data sets are close. As discussed in ,
the knowledge graph in FB13 is much denser than FB15K
and even WN11. It seems that the most expressive model
NTN can learn complicated correlations using tensor transformation from the dense graph of FB13. In contrast, simpler
models are able to better handle the sparse graph of FB15K
with good generalization. (3) Moreover, the “bern” sampling
technique improves the performance of TransE, TransH and
TransR on all three data sets.
As shown in , the training time of
TransE and TransH are about 5 and 30 minutes, respectively.
The computation complexity of TransR is higher than both
TransE and TransH, which takes about 3 hours for training.
Table 5: Evaluation results of triple classiﬁcation. (%)
SME (bilinear)
TransE (unif)
TransE (bern)
TransH (unif)
TransH (bern)
TransR (unif)
TransR (bern)
CTransR (bern)
Relation Extraction from Text
Relation extraction aims to extract relational fact from largescale plain text, which is an important information source
to enrich knowledge graphs. Most exiting methods take knowledge graphs
as distant supervision to automatically annotate sentences
in large-scale text corpora as training instances, and then
extract textual features to build relation classiﬁers. These
methods only use plain texts to reason new relational fact;
meanwhile knowledge graph embeddings perform link prediction only based on existing knowledge graphs.
It is straightforward to take advantage of both plain texts
and knowledge graphs to infer new relational facts. In , TransE and text-based extraction model
were combined to rank candidate facts and achieved promising improvement. Similar improvements are also found over
TransH . In this section, we will investigate the performance of TransR when combining with textbased relation extraction model.
We adopt NYT+FB, also used in , to
build text-based relation extraction model. In this data set,
entities in New York Times Corpus are annotated with Stanford NER and linked to Freebase.
In our experiments, we implement the same text-based extraction model proposed in which is
named as Sm2r. For the knowledge graph part, used a subset restricted to the top 4 million entities
with 23 thousand relation types. As TransH has not released
the dataset and TransR will take too long to learn from 4
million entities, we generate a smaller data set FB40K ourselves, which contains all entities in NYT and 1, 336 relation
types. For test fairness, from FB40K we remove all triples
whose entity pairs have appeared in the testing set of NYT.
As compared to previous results in , we ﬁnd that learning with FB40K does
not signiﬁcantly reduce the effectiveness of TransE and
TransH. Hence we can safely use FB40K to demonstrate the
effectiveness of TransR.
Following the same method in ,
we combine the scores from text-based relation extraction
model with the scores from knowledge graph embeddings to
rank test triples, and get precision-recall curves for TransE,
TransH and TransR. Since the freebase part of our data set
is built by ourselves, different from the ones in , the evaluation results cannot be compared directly
with those reported in . Hence, we implement TransE, TransH and TransR by ourselves. We set
the embedding dimensions k, d = 50, the learning rate
λ = 0.001, the margin γ = 1.0, B = 960, and dissimilarity metric as L1. Evaluation curves are shown in Figure
Figure 2: Precision-recall curves of TransE, TransH and
TransR for relation extraction from text.
From the table we observe that TransR outperforms
TransE and is comparable with TransH when recall ranges
[0, 0.05], and outperforms all baselines including TransE and
TransH when recall ranges [0.05, 1].
Recently, the idea of embeddings has also been widely
used for representing words and texts , which may be used for text-based relation extraction.
Conclusion and Future Work
In this paper, we propose TransR, a new knowledge graph
embedding model. TransR embeds entities and relations in
distinct entity space and relation space, and learns embeddings via translation between projected entities. In addition,
we also propose CTransR, which aims to model internal
complicated correlations within each relation type based on
the idea of piecewise linear regression. In experiments, we
evaluate our models on three tasks including link prediction,
triple classiﬁcation and fact extraction from text. Experiment
results show that TransR achieves consistent and signiﬁcant
improvements compared to TransE and TransH.
We will explore the following further work:
• Existing models including TransR consider each relational fact separately. In fact, relations correlate with each
other with rich patterns. For example, if we know (gold-
ﬁsh, kind of, ﬁsh) and (ﬁsh, kind of, animal), we can
infer (goldﬁsh, kind of, animal) since the relation type
kind of is transitive. We may take advantages of these
relation patterns for knowledge graph embeddings.
• In relational fact extraction from text, we simply perform
linear weighted average to combine the scores from text-
side extraction model and the knowledge graph embedding model. In future, we may explore a uniﬁed embedding model of both text side and knowledge graph.
• CTransR is an initial exploration for modeling internal
correlations within each relation type. In future, we will
investigate more sophisticated models for this purpose.
Acknowledgments
2014CB340501), the National Natural Science Foundation of China (NSFC No. 61133012 and 61202140) and
Tsinghua-Samsung Joint Lab. We thank all anonymous reviewers for their constructive comments.