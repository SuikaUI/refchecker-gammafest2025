Reading Text in the Wild with Convolutional Neural Networks
Max Jaderberg · Karen Simonyan · Andrea Vedaldi · Andrew Zisserman
Abstract In this work we present an end-to-end system for text spotting – localising and recognising text in
natural scene images – and text based image retrieval.
This system is based on a region proposal mechanism
for detection and deep convolutional neural networks
for recognition. Our pipeline uses a novel combination
of complementary proposal generation techniques to ensure high recall, and a fast subsequent ﬁltering stage
for improving precision. For the recognition and ranking of proposals, we train very large convolutional neural networks to perform word recognition on the whole
proposal region at the same time, departing from the
character classiﬁer based systems of the past. These
networks are trained solely on data produced by a synthetic text generation engine, requiring no human labelled data.
Analysing the stages of our pipeline, we show stateof-the-art performance throughout. We perform rigorous experiments across a number of standard end-toend text spotting benchmarks and text-based image retrieval datasets, showing a large improvement over all
previous methods. Finally, we demonstrate a real-world
application of our text spotting system to allow thousands of hours of news footage to be instantly searchable via a text query.
1 Introduction
The automatic detection and recognition of text in natural images, text spotting, is an important challenge for
visual understanding.
Max Jaderberg · Karen Simonyan · Andrea Vedaldi · Andrew
Department of Engineering Science, University of Oxford
E-mail: {max,karen,vedaldi,az}@robots.ox.ac.uk
Text, as the physical incarnation of language, is one
of the basic tools for preserving and communicating information. Much of the modern world is designed to be
interpreted through the use of labels and other textual
cues, and so text ﬁnds itself scattered throughout many
images and videos. Through the use of text spotting, an
important part of the semantic content of visual media
can be decoded and used, for example, for understanding, annotating, and retrieving the billions of consumer
photos produced every day.
Traditionally, text recognition has been focussed on
document images, where OCR techniques are well suited
to digitise planar, paper-based documents. However,
when applied to natural scene images, these document
OCR techniques fail as they are tuned to the largely
black-and-white, line-based environment of printed documents. The text that occurs in natural scene images is
hugely variable in appearance and layout, being drawn
from a large number of fonts and styles, suﬀering from
inconsistent lighting, occlusions, orientations, noise, and,
in addition, the presence of background objects causes
spurious false-positive detections. This places text spotting as a separate, far more challenging problem than
document OCR.
The increase of powerful computer vision techniques
and the overwhelming increase in the volume of images
produced over the last decade has seen a rapid development of text spotting methods. To eﬃciently perform
text spotting, the majority of methods follow the intuitive process of splitting the task in two: text detection
followed by word recognition . Text detection involves generating candidate character or word region
detections, while word recognition takes these proposals and infers the words depicted.
In this paper we advance text spotting methods,
making a number of key contributions as part of this.
 
Max Jaderberg et al.
Fig. 1 The end-to-end text spotting pipeline proposed. a) A combination of region proposal methods extracts many word
bounding box proposals. b) Proposals are ﬁltered with a random forest classiﬁer reducing number of false-positive detections.
c) A CNN is used to perform bounding box regression for reﬁning the proposals. d) A CNN performs text recognition on
each of the reﬁned proposals. e) Detections are merged based on proximity and recognition results and assigned a score. f)
Thresholding the detections results in the ﬁnal text spotting result.
Our main contribution is a novel text recognition
method – this is in the form of a deep convolutional
neural network (CNN) which takes the whole word
image as input to the network. Evidence is gradually
pooled from across the image to perform classiﬁcation
of the word across a huge dictionary, such as the 90kword dictionary evaluated in this paper. Remarkably,
our model is trained purely on synthetic data, without
incurring the cost of human labelling. We also propose
an incremental learning method to successfully train a
model with such a large number of classes. Our recognition framework is exceptionally powerful, substantially
outperforming previous state of the art on real-world
scene text recognition, without using any real-world labelled training data.
Our second contribution is a novel detection strategy for text spotting: the use of fast region proposal
methods to perform word detection. We use a combination of an object-agnostic region proposal method and
a sliding window detector. This gives very high recall
coverage of individual word bounding boxes, resulting
in around 98% word recall on both ICDAR 2003 and
Street View Text datasets with a manageable number
of proposals. False-negative candidate word bounding
boxes are ﬁltered with a stronger random forest classi-
ﬁer and the remaining proposals adjusted using a CNN
trained to regress the bounding box coordinates.
Our third contribution is the application of our pipeline
for large-scale visual search of text in video. In a fraction of a second we are able to retrieve images and
videos from a huge corpus that contain the visual rendering of a user given text query, at very high precision.
We expose the performance of each part of the pipeline
in experiments, showing that we can maintain the high
recall of the initial proposal stage while gradually boosting precision as more complex models and higher order
information is incorporated. The recall of the detection stage is shown to be signiﬁcantly higher than that
of previous text detection methods, and the accuracy
of the word recognition stage higher than all previous
methods. The result is an end-to-end text spotting system that outperforms all previous methods by a large
margin. We demonstrate this for the annotation task
(localising and recognising text in images) across a large
range of standard text spotting datasets, as well as in
a retrieval scenario (retrieving a ranked list of images
that contain the text of a query string) for standard
datasets. In addition, the use of our framework for retrieval is further demonstrated in a real-world application – being used to instantly search through thousands
of hours of archived news footage for a user-given text
The following section gives an overview of our pipeline.
We then review a selection of related work in Section 3.
Sections 4-7 present the stages of our pipeline. We extensively test all elements of our pipeline in Section 8
and include the details of datasets and the experimental
setup. Finally, Section 9 summarises and concludes.
Our word recognition framework appeared previously as a tech report and at the NIPS 2014 Deep
Reading Text in the Wild with Convolutional Neural Networks
Learning and Representation Learning Workshop, along
with some other non-dictionary based variants.
2 Overview of the Approach
The stages of our approach are as follows: word bounding box proposal generation (Section 4), proposal ﬁltering and adjustments (Section 5), text recognition
(Section 6) and ﬁnal merging for the speciﬁc task (Section 7). The full process is illustrated in Fig. 1.
Our process loosely follows the detection/recognition
separation – a word detection stage followed by a word
recognition stage. However, these two stages are not
wholly distinct, as we use the information gained from
word recognition to merge and rank detection results
at the end, leading to a stronger holistic text spotting
The detection stage of our pipeline is based on weakbut-fast detection methods to generate word boundingbox proposals. This draws on the success of the R-
CNN object detection framework of Girshick et al. 
where region proposals are mapped to a ﬁxed size for
CNN recognition. The use of region proposals avoids the
computational complexity of evaluating an expensive
classiﬁer with exhaustive multi-scale, multi-aspect-ratio
sliding window search. We use a combination of Edge
Box proposals and a trained aggregate channel features detector to generate candidate word bounding
boxes. Due to the large number of false-positive proposals, we then use a random forest classiﬁer to ﬁlter the
number of proposals to a manageable size – this is a
stronger classiﬁer than those found in the proposal algorithms. Finally, inspired by the success of bounding
box regression in DPM and R-CNN , we regress
more accurate bounding boxes from the seeds of the
proposal algorithms which greatly improves the average
overlap ratio of positive detections with groundtruth.
However, unlike the linear regressors of we train
a CNN speciﬁcally for regression. We discuss these design choices in each section.
The second stage of our framework produces a text
recognition result for each proposal generated from the
detection stage. We take a whole-word approach to recognition, providing the entire cropped region of the word
as input to a deep convolutional neural network. We
present a dictionary model which poses the recognition task as a multi-way classiﬁcation task across a
dictionary of 90k possible words. Due to the mammoth
training data requirements of classiﬁcation tasks of this
scale, these models are trained purely from synthetic
data. Our synthetic data engine is capable of rendering
suﬃciently realistic and variable word image samples
that the models trained on this data translate to the
domain of real-world word images giving state-of-theart recognition accuracy.
Finally, we use the information gleaned from recognition to update our detection results with multiple
rounds of non-maximal suppression and bounding box
regression.
3 Related Work
In this section we review the contributions of works
most related to ours. These focus solely on text detection , text recognition , or on combining both in end-to-end systems
 .
3.1 Text Detection Methods
Text detection methods tackle the ﬁrst task of the standard text spotting pipeline : producing segmentations or bounding boxes of words in natural scene images. Detecting instances of words in noisy and cluttered images is a highly non-trivial task, and the methods developed to solve this are based on either character regions or sliding windows .
Character region methods aim to segment pixels into
characters, and then group characters into words. Epshtein
et al. ﬁnd regions of the input image which have
constant stroke width – the distance between two parallel edges – by taking the stroke width transform (SWT).
Intuitively characters are regions of similar stroke width,
so clustering pixels together forms characters, and characters are grouped together into words based on geometric heuristics. In , Neumann and Matas revisit
the notion of characters represented as strokes and use
gradient ﬁlters to detect oriented strokes in place of the
SWT. Rather than regions of constant stroke width,
Neumann and Matas use Extremal Regions 
as character regions. Huang et al. expand on the
use of Maximally Stable Extremal Regions by incorporating a strong CNN classiﬁer to eﬃciently prune the
trees of Extremal Regions leading to less false-positive
detections.
Sliding window methods approach text detection as
a classical object detection task. Wang et al. use a
random ferns classiﬁer trained on HOG features 
in a sliding window scenario to ﬁnd characters in an
image. These are grouped into words using a pictorial structures framework for a small ﬁxed lexicon. Wang & Wu et al. show that CNNs trained
for character classiﬁcation can be used as eﬀective sliding window classiﬁers. In some of our earlier work ,
Max Jaderberg et al.
we use CNNs for text detection by training a text/notext classiﬁer for sliding window evaluations, and also
CNNs for character and bigram classiﬁcation to perform word recognition. We showed that using feature
sharing across all the CNNs for the diﬀerent classiﬁcation tasks resulted in stronger classiﬁers for text detection than training each classiﬁer independently.
Unlike previous methods, our framework operates
in a low-precision, high-recall mode – rather than using
a single word location proposal, we carry a suﬃciently
high number of candidates through several stages of our
pipeline. We use high recall region proposal methods
and a ﬁltering stage to further reﬁne these. In fact, our
“detection method” is only complete after performing
full text recognition on each remaining proposal, as we
then merge and rank the proposals based on the output of the recognition stage to give the ﬁnal detections,
complete with their recognition results.
3.2 Text Recognition Methods
Text recognition aims at taking a cropped image of a
single word and recognising the word depicted. While
there are many previous works focussing on handwriting or historical document recognition ,
these methods don’t generalise in function to generic
scene text due to the highly variable foreground and
background textures that are not present with documents.
For scene text recognition, methods can be split into
two groups – character based recognition and whole word based recognition .
Character based recognition relies on an individual
character classiﬁer for per-character recognition which
is integrated across the word image to generate the full
word recognition. In , Yao et al. learn a set of midlevel features, strokelets, by clustering sub-patches of
characters. Characters are detected with Hough voting, with the characters identiﬁed by a random forest
classiﬁer acting on strokelet and HOG features.
The works of all use CNNs as character classiﬁers. and over-segment the word image
into potential character regions, either through unsupervised binarization techniques or with a supervised
classiﬁer. Alsharif et al. then use a complicated combination of segmentation-correction and character recognition CNNs together with an HMM with a ﬁxed lexicon to generate the ﬁnal recognition result. The PhotoOCR system uses a neural network classiﬁer acting on the HOG features of the segments as scores
to ﬁnd the best combination of segments using beam
search. The beam search incorporates a strong N-gram
language model, and the ﬁnal beam search proposals
are re-ranked with a further language model and shape
model. Our own previous work uses a combination
of a binary text/no-text classiﬁer, a character classi-
ﬁer, and a bigram classiﬁer densely computed across
the word image as cues to a Viterbi scoring function in
the context of a ﬁxed lexicon.
As an alternative approach to word recognition other
methods use whole word based recognition, pooling features from across the entire word sub-image before performing word classiﬁcation. The works of Mishra et al. 
and Novikova et al. still rely on explicit character
classiﬁers, but construct a graph to infer the word, pooling together the full word evidence. Goel et al. use
whole word sub-image features to recognise words by
comparing to simple black-and-white font-renderings
of lexicon words. Rodriguez et al. use aggregated
Fisher Vectors and a Structured SVM framework
to create a joint word-image and text embedding.
Almazan et al. further explore the notion of word
embeddings, creating a joint embedding space for word
images and representations of word strings. This is extended in where Gordo makes explicit use of character level training data to learn mid-level features. This
results in performance on par with but using only a
small fraction of the amount of training data.
While not performing full scene text recognition,
Goodfellow et al. had great success using a CNN
with multiple position-sensitive character classiﬁer outputs to perform street number recognition. This model
was extended to CAPTCHA sequences up to 8 characters long where they demonstrated impressive performance using synthetic training data for a synthetic
problem (where the generative model is known). In contrast, we show that synthetic training data can be used
for a real-world data problem (where the generative
model is unknown).
Our method for text recognition also follows a whole
word image approach. Similarly to , we take the
word image as input to a deep CNN, however we employ a dictionary classiﬁcation model. Recognition is
achieved by performing multi-way classiﬁcation across
the entire dictionary of potential words.
In the following sections we describe the details of
each stage of our text spotting pipeline. The sections
are presented in order of their use in the end-to-end
4 Proposal Generation
The ﬁrst stage of our end-to-end text spotting pipeline
relies on the generation of word bounding boxes. This
Reading Text in the Wild with Convolutional Neural Networks
is word detection – in an ideal scenario we would be
able to generate word bounding boxes with high recall and high precision, achieving this by extracting the
maximum amount of information from each bounding
box candidate possible. However, in practice a precision/recall tradeoﬀis required to reduce computational
complexity. With this in mind we opt for a fast, high
recall initial phase, using computationally cheap classi-
ﬁers, and gradually incorporate more information and
more complex models to improve precision by rejecting false-positive detections resulting in a cascade. To
compute recall and precision in a detection scenario, a
bounding box is said to be a true-positive detection if
it has overlap with a groundtruth bounding box above
a deﬁned threshold. The overlap for bounding boxes b1
and b2 is deﬁned as the ratio of intersection over union
(IoU): |b1∩b2|
Though never applied to word detection before, region proposal methods have gained a lot of attention for
generic object detection. Region proposal methods aim to generate object region proposals with
high recall, but at the cost of a large number of falsepositive detections. Even so, this still reduces the search
space drastically compared to sliding window evaluation of the subsequent stages of a detection pipeline.
Eﬀectively, region proposal methods can be viewed as
a weak detector.
In this work we combine the results of two detection mechanisms – the Edge Boxes region proposal algorithm ( , Section 4.1) and a weak aggregate channel
features detector ( , Section 4.2).
4.1 Edge Boxes
We use the formulation of Edge Boxes as described
in . The key intuition behind Edge Boxes is that
since objects are generally self contained, the number
of contours wholly enclosed by a bounding box is indicative of the likelihood of the box containing an object.
Edges tend to correspond to object boundaries, and so
if edges are contained inside a bounding box this implies
objects are contained within the bounding box, whereas
edges which cross the border of the bounding box suggest there is an object that is not wholly contained by
the bounding box.
The notion of an object being a collection of boundaries is especially true when the desired objects are
words – collections of characters with sharp boundaries.
Following , we compute the edge response map
using the Structured Edge detector and perform
Non-Maximal Suppression orthogonal to the edge responses, sparsifying the edge map. A candidate bounding box b is assigned a score sb based on the number of
edges wholly contained by b, normalised by the perimeter of b. The full details can be found in .
The boxes b are evaluated in a sliding window manner, over multiple scales and aspect ratios, and given a
score sb. Finally, the boxes are sorted by score and nonmaximal suppression is performed: a box is removed if
its overlap with another box of higher score is more than
a threshold. This results in a set of candidate bounding
boxes for words Be.
4.2 Aggregate Channel Feature Detector
Another method for generating candidate word bounding box proposals is by using a conventional trained
detector. We use the aggregate channel features (ACF)
detector framework of for its speed of computation.
This is a conventional sliding window detector based on
ACF features coupled with an AdaBoost classiﬁer. ACF
based detectors have been shown to work well on pedestrian detection and general object detection, and here
we use the same framework for word detection.
For each image I a number of feature channels are
computed, such that channel C = Ω(I), where Ωis
the channel feature extraction function. We use channels similar to those in : normalised gradient magnitude, histogram of oriented gradients (6 channels), and
the raw greyscale input. Each channel C is smoothed,
divided into blocks and the pixels in each block are
summed and smoothed again, resulting in aggregate
channel features.
The ACF features are not scale invariant, so for
multi-scale detection we need to extract features at
many diﬀerent scales – a feature pyramid. In a standard
detection pipeline, the channel features for a particular
scale s are computed by resampling the image and recomputing the channel features Cs = Ω(Is) where Cs
are the channel features at scale s and Is = R(I, s) is
the image resampled by s. Resampling and recomputing the features at every scale is computationally expensive. However, as shown in , the channel features
at scale s can be approximated by resampling the features at a diﬀerent scale, such that Cs ≈R(C, s)·s−λΩ,
where λΩis a channel speciﬁc power-law factor. Therefore, fast feature pyramids can be computed by evaluating Cs = Ω(R(I, s)) at only a single scale per octave (s ∈{1, 1
4, . . . }) and at intermediate scales, Cs
is computed using Cs = R(Cs′, s/s′)(s/s′)−λΩwhere
4, . . . }. This results in much faster feature
pyramid computation.
The sliding window classiﬁer is an ensemble of weak
decision trees, trained using AdaBoost , using the
aggregate channel features. We evaluate the classiﬁer on
Max Jaderberg et al.
every block of aggregate channel features in our feature
pyramid, and repeat this for multiple aspect ratios to
account for diﬀerent length words, giving a score for
each box. Thresholding on score gives a set of word
proposal bounding boxes from the detector, Bd.
Discussion. We experimented with a number of region
proposal algorithms. Some were too slow to be useful . A very fast method, BING , gives good recall when re-trained speciﬁcally for word detection but
achieves a low overlap ratio for detections and poorer
overall recall. We found Edge Boxes to give the best
recall and overlap ratio.
It was also observed that independently, neither Edge
Boxes nor the ACF detector achieve particularly high
recall, 92% and 70% recall respectively (see Section 8.2
for experiments), but when the proposals are combined
achieve 98% recall (recall is computed at 0.5 overlap).
In contrast, combining BING, with a recall of 86% with
the ACF detector gives a combined recall of only 92%.
This suggests that the Edge Box and ACF detector
methods are very complementary when used in conjunction, and so we compose the ﬁnal set of candidate
bounding boxes B = {Be ∪Bd}.
5 Filtering & Reﬁnement
The proposal generation stage of Section 4 produces a
set of candidate bounding boxes B. However, to achieve
a high recall, thousands of bounding boxes are generated, most of which are false-positive. We therefore
aim to use a stronger classiﬁer to further ﬁlter these to
a number that is computationally manageable for the
more expensive full text recognition stage described in
Section 5.1. We also observe that the overlap of many
of the bounding boxes with the groundtruth is unsatisfactorily low, and therefore train a regressor to reﬁne
the location of the bounding boxes, as described in Section 5.2.
5.1 Word Classiﬁcation
To reduce the number of false-positive word detections,
we seek a classiﬁer to perform word/no-word binary
classiﬁcation. For this we use a random forest classi-
ﬁer acting on HOG features .
For each bounding box proposal b ∈B we resample
the cropped image region to a ﬁxed size and extract
HOG features, resulting in a descriptor h. The descriptor is then classiﬁed with a random forest classiﬁer, with
decision stump nodes. The random forest classiﬁes every proposal, and the proposals falling below a certain
Fig. 2 The shortcoming of using an overlap ratio for text
detection of 0.5. The two examples of proposal bounding
boxes (green solid box) have approximately 0.5 overlap with
groundtruth (red dashed box). In the bottom case, a 0.5 overlap is not satisfactory to produce accurate text recognition
threshold are rejected, leaving a ﬁltered set of bounding
5.2 Bounding Box Regression
Although our proposal mechanism and ﬁltering stage
give very high recall, the overlap of these proposals can
be quite poor.
While an overlap of 0.5 is usually acceptable for general object detection , for accurate text recognition
this can be unsatisfactory. This is especially true when
one edge of the bounding box is predicted accurately
but not the other – e.g. if the height of the bounding
box is computed perfectly, the width of the bounding
box can be either double or half as wide as it should
be and still achieve 0.5 overlap. This is illustrated in
Fig. 2. Both predicted bounding boxes have 0.5 overlap with the groundtruth, but for text this can amount
to only seeing half the word if the height is correctly
computed, and so it would be impossible to recognise
the correct word in the bottom example of Fig. 2. Note
that both proposals contain text, so neither are ﬁltered
by the word/no-word classiﬁer.
Due to the large number of region proposals, we
could hope that there would be some proposals that
would overlap with the groundtruth to a satisfactory
degree, but we can encourage this by explicitly reﬁning
the coordinates of the proposed bounding boxes – we
do this within a regression framework.
Our bounding box coordinate regressor takes each
proposed bounding box b ∈Bf and produces an updated estimate of that proposal b∗. A bounding box is
Reading Text in the Wild with Convolutional Neural Networks
parametrised by its top-left and bottom-right corners,
such that bounding box b = (x1, y1, x2, y2). The full
image I is cropped to a rectangle centred on the region
b, with the width and height inﬂated by a scale factor.
The resulting image is resampled to a ﬁxed size W ×H,
giving Ib, which is processed by the CNN to regress the
four values of b∗. We do not regress the absolute values of the bounding box coordinates directly, but rather
encoded values. The top-left coordinate is encoded by
the top-left quadrant of Ib, and the bottom left coordinate by the bottom-left quadrant of Ib as illustrated
by Fig. 3. This normalises the coordinates to generally
fall in the interval , but allows the breaking of this
interval if required.
In practice, we inﬂate the cropping region of each
proposal by a factor of two. This gives the CNN enough
context to predict a more accurate location of the proposal bounding box. The CNN is trained with example
pairs of (Ib, bgt) to regress the groundtruth bounding
box bgt from the sub-image Ib cropped from I by the
estimated bounding box b. This is done by minimising
the L2 loss between the encoded bounding boxes, i.e.
∥g(Ib; Φ) −q(bgt)∥2
over the network parameters Φ on a training set Btrain,
where g is the CNN forward pass function and q is the
bounding box coordinate encoder.
Discussion. The choice of which features and the classiﬁer and regression methods to use was made through
experimenting with a number of diﬀerent choices. This
included using a CNN for classiﬁcation, with a dedicated classiﬁcation CNN, and also by jointly training a
single CNN to perform both classiﬁcation and regression simultaneously with multi-task learning. However,
the classiﬁcation performance of the CNN was not signiﬁcantly better than that of HOG with a random forest, but requires more computations and a GPU for
processing. We therefore chose the random forest classiﬁer to reduce the computational cost of our pipeline
without impacting end results.
The bounding box regression not only improves the
overlap to aid text recognition for each individual sample, but also causes many proposals to converge on the
same bounding box coordinates for a single instance of
a word, therefore aiding the voting/merging mechanism
described in Section 8.3 with duplicate detections.
6 Text Recognition
At this stage of our processing pipeline, a pool of accurate word bounding box proposals has been gener-
Fig. 3 The bounding box regression encoding scheme showing the original proposal (red) and the adjusted proposal
(green). The cropped input image shown is always centred on
the original proposal, meaning the original proposal always
has implied encoded coordinates of (0.5, 0.5, 0.5, 0.5).
ated as described in the previous sections. We now turn
to the task of recognising words inside these proposed
bounding boxes. To this end we use a deep CNN to
perform classiﬁcation across a pre-deﬁned dictionary of
words – dictionary encoding – which explicitly models natural language. The cropped image of each of the
proposed bounding boxes is taken as input to the CNN,
and the CNN produces a probability distribution over
all the words in the dictionary. The word with the maximum probability can be taken as the recognition result.
The model, described fully in Section 6.2, can scale
to a huge dictionary of 90k words, encompassing the
majority of the commonly used English language (see
Section 8.1 for details of the dictionary used). However,
to achieve this, many training samples of every diﬀerent
possible word must be amassed. Such a training dataset
does not exist, so we instead use synthetic training data,
described in Section 6.1, to train our CNN. This synthetic data is so realistic that the CNN can be trained
purely on the synthetic data but still applied to real
world data.
6.1 Synthetic Training Data
This section describes our scene text rendering algorithm. As our CNN models take whole word images as
input instead of individual character images, it is essential to have access to a training dataset of cropped
word images that covers the whole language or at least
a target lexicon. While there are some publicly available
Max Jaderberg et al.
Fig. 4 (a) The text generation process after font rendering, creating and colouring the image-layers, applying projective
distortions, and after image blending. (b) Some randomly sampled data created by the synthetic text engine.
datasets from ICDAR , the Street View
Text (SVT) dataset , the IIIT-5k dataset , and
others, the number of full word image samples is only
in the thousands, and the vocabulary is very limited.
The lack of full word image samples has caused previous work to rely on character classiﬁers instead (as
character data is plentiful), or this deﬁcit in training
data has been mitigated by mining for data or having
access to large proprietary datasets . However,
we wish to perform whole word image based recognition
and move away from character recognition, and aim to
do this in a scalable manner without requiring human
labelled datasets.
Following the success of some synthetic character
datasets , we create a synthetic word data generator, capable of emulating the distribution of scene
text images. This is a reasonable goal, considering that
much of the text found in natural scenes is restricted
to a limited set of computer-generated fonts, and only
the physical rendering process (e.g. printing, painting)
and the imaging process (e.g. camera, viewpoint, illumination, clutter) are not controlled by a computer algorithm.
Fig. 4 illustrates the generative process and some resulting synthetic data samples. These samples are composed of three separate image-layers – a background
image-layer, foreground image-layer, and optional border/shadow image-layer – which are in the form of an
image with an alpha channel. The synthetic data generation process is as follows:
1. Font rendering – a font is randomly selected from a
catalogue of over 1400 fonts downloaded from Google
Fonts. The kerning, weight, underline, and other
properties are varied randomly from arbitrarily de-
ﬁned distributions. The word is rendered on to the
foreground image-layer’s alpha channel with either
a horizontal bottom text line or following a random
2. Border/shadow rendering – an inset border, outset
border, or shadow with a random width may be rendered from the foreground.
3. Base colouring – each of the three image-layers are
ﬁlled with a diﬀerent uniform colour sampled from
clusters over natural images. The clusters are formed
by k-means clustering the RGB components of each
image of the training datasets of into three clusters.
4. Projective distortion – the foreground and border/shadow
image-layers are distorted with a random, full projective transformation, simulating the 3D world.
5. Natural data blending – each of the image-layers are
blended with a randomly-sampled crop of an image from the training datasets of ICDAR 2003 and
SVT. The amount of blend and alpha blend mode
(e.g. normal, add, multiply, burn, max, etc.) is dictated by a random process, and this creates an eclectic range of textures and compositions. The three
image-layers are also blended together in a random
manner, to give a single output image.
6. Noise – Elastic distortion similar to , Gaussian
noise, blur, resampling noise, and JPEG compression artefacts are introduced to the image.
This process produces a wide range of synthetic data
samples, being drawn from a multitude of random distributions, mimicking real-world samples of scene text
images. The synthetic data is used in place of real-world
data, and the labels are generated from a corpus or dictionary as desired. By creating training datasets many
orders of magnitude larger than what has been available
before, we are able to use data-hungry deep learning algorithms to train a richer, whole-word-based model.
Reading Text in the Wild with Convolutional Neural Networks
Fig. 5 A schematic of the CNN used for text recognition by word classiﬁcation. The dimensions of the featuremaps at each
layer of the network are shown.
6.2 CNN Model
This section describes our model for word recognition.
We formulate recognition as a multi-class classiﬁcation
problem, with one class per word, where words w are
constrained to be selected in a pre-deﬁned dictionary
W. While the dictionary W of a natural language may
seem too large for this approach to be feasible, in practice an advanced English vocabulary, including diﬀerent
word forms, contains only around 90k words, which is
large but manageable.
In detail, we propose to use a CNN classiﬁer where
each word w ∈W in the lexicon corresponds to an output neuron. We use a CNN with ﬁve convolutional layers and three fully-connected layers, with the exact details described in Section 8.2. The ﬁnal fully-connected
layer performs classiﬁcation across the dictionary of
words, so has the same number of units as the size of
the dictionary we wish to recognise.
The predicted word recognition result w∗out of the
set of all dictionary words W in a language L for a given
input image x is given by
w∗= arg max
w∈W P(w|x, L).
Since P(w|x, L) can be written as
P(w|x, L) = P(w|x)P(w|L)P(x)
P(x|L)P(w)
and with the assumptions that x is independent of L
and that prior to any knowledge of our language all
words are equally probable, our scoring function reduces to
w∗= arg max
w∈W P(w|x)P(w|L).
The per-word output probability P(w|x) is modelled
by the softmax output of the ﬁnal fully-connected layer
of the recognition CNN, and the language based word
prior P(w|L) can be modelled by a lexicon or frequency
counts. A schematic of the network is shown in Fig. 5.
One limitation of this CNN model is that the input
x must be a ﬁxed, pre-deﬁned size. This is problematic
for word images, as although the height of the image
is always one character tall, the width of a word image
is highly dependent on the number of characters in the
word, which can range between one and 23 characters.
To overcome this issue, we simply resample the word
image to a ﬁxed width and height. Although this does
not preserve the aspect ratio, the horizontal frequency
distortion of image features most likely provides the
network with word-length cues. We also experimented
with diﬀerent padding regimes to preserve the aspect
ratio, but found that the results are not quite as good
as performing naive resampling.
To summarise, for each proposal bounding box b ∈
Bf for image I we compute P(w|xb, L) by cropping
the image to Ib = c(b, I), resampling to ﬁxed dimensions W × H such that xb = R(Ib, W, H), and compute
P(w|xb) with the text recognition CNN and multiply
by P(w|L) (task dependent) to give a ﬁnal probability
distribution over words P(w|xb, L).
7 Merging & Ranking
At this point in the pipeline, we have a set of word
bounding boxes for each image Bf with their associated
word probability distributions PBf = {pb : b ∈Bf},
where pb = P(w|b, I) = P(w|xb, L). However, this set
of detections still contains a number of false-positive
and duplicate detections of words, so a ﬁnal merging
and ranking of detections must be performed depending
on the task at hand: text spotting or text based image
retrieval.
7.1 Text Spotting
The goal of text spotting is to localise and recognise
the individual words in the image. Each word should
be labelled by a bounding box enclosing the word and
the bounding box should have an associated text label.
For this task, we assign each bounding box in b ∈Bf
a label wb and score sb according to b’s maximum word
Max Jaderberg et al.
probability:
wb = arg max
w∈W P(w|b, I),
w∈W P(w|b, I)
To cluster duplicate detections of the same word instance, we perform a greedy non maximum suppression
(NMS) on detections with the same word label, aggregating the scores of suppressed proposals. This can be
seen as positional voting for a particular word. Subsequently, we perform NMS to suppress non-maximal
detections of diﬀerent words with some overlap.
Our text recognition CNN is able to accurately recognise text in very loosely cropped word sub-images. Because of this, we ﬁnd that some valid text spotting results have less than 0.5 overlap with groundtruth, but
we require greater than 0.5 overlap for some applications (see Section 8.3).
To improve the overlap of detection results, we additionally perform multiple rounds of bounding box regression as in Section 5.2 and NMS as described above
to further reﬁne our detections. This can be seen as a
recurrent regressor network. Each round of regression
updates the prediction of the each word’s localisation,
giving the next round of regression an updated context
window to perform the next regression, as shown in
Fig. 6. Performing NMS between each regression causes
bounding boxes that have become similar after the latest round of regression to be grouped as a single detection. This generally causes the overlap of detections
to converge on a higher, stable value with only a few
rounds of recurrent regression.
The reﬁned results, given by the tuple (b, wb, sb), are
ranked by their scores sb and a threshold determines
the ﬁnal text spotting result. For the direct comparison of scores across images, we normalise the scores of
the results of each image by the maximum score for a
detection in that image.
7.2 Image Retrieval
For the task of text based image retrieval, we wish to
retrieve the list of images which contain the given query
words. Localisation of the query word is not required,
only optional for giving evidence for retrieving that image.
This is achieved by, at query time, assigning each
image I a score sQ
I for the query words Q = {q1, q2, . . .},
and sorting the images in the database I in descending order of score. It is also required that the score
for all images can be computed fast enough to scale to
databases of millions of images, allowing fast retrieval
of visual content by text search. While retrieval is often
performed for just a single query word (Q = {q}), we
generalise our retrieval framework to be able to handle
multiple query words.
We estimate the per-image probability distribution
across word space P(w|I) by averaging the word probability distributions across all detections Bf in an image
pI = P(w|I) =
This distribution is computed oﬄine for all I ∈I.
At query time, we can simply compute a score for
each image sQ
I representing the probability that the image I contains any of the query words Q. Assuming
independence between the presence of query words
where pI(q) is just a lookup of the probability of word
q in the word distribution pI. These scores can be computed very quickly and eﬃciently by constructing an
inverted index of pI ∀I ∈I.
After a one-time, oﬄine pre-processing to compute
pI and assemble the inverted index, a query can be
processed across a database of millions of images in less
than a second.
8 Experiments
In this section we evaluate our pipeline on a number of
standard text spotting and text based image retrieval
benchmarks.
We introduce the various datasets used for evaluation in Section 8.1, give the exact implementation details and results of each part of our pipeline in Section 8.2, and ﬁnally present the results on text spotting
and image retrieval benchmarks in Section 8.3 and Section 8.4 respectively.
8.1 Datasets
We evaluate our pipeline on an extensive number of
datasets. Due to diﬀerent levels of annotation, the datasets
are used for a combination of text recognition, text
spotting, and image retrieval evaluation. The datasets
are summarised in Table 1, Table 2, and Table 3. The
smaller lexicons provided by some datasets are used to
reduce the search space to just text contained within
the lexicons.
The Synth dataset is generated by our synthetic
data engine of Section 6.1. We generate 9 million 32 ×
100 images, with equal numbers of word samples from
Reading Text in the Wild with Convolutional Neural Networks
Fig. 6 An example of the improvement in localisation of the word detection pharmacy through multiple rounds of recurrent
regression.
a 90k word dictionary. We use 900k of these for a testing dataset, 900k for validation, and the remaining for
training. The 90k dictionary consists of the English
dictionary from Hunspell , a popular open source
spell checking system. This dictionary consists of 50k
root words, and we expand this to include all the pre-
ﬁxes and suﬃxes possible, as well as adding in the test
dataset words from the ICDAR, SVT and IIIT datasets
– 90k words in total. This dataset is publicly available
at 
ICDAR 2003 (IC03) , ICDAR 2011 (IC11)
 , and ICDAR 2013 (IC13) are scene text
recognition datasets consisting of 251, 255, and 233 full
scene images respectively. The photos consist of a range
of scenes and word level annotation is provided. Much
of the test data is the same between the three datasets.
For IC03, Wang deﬁnes per-image 50 word lexicons
(IC03-50) and a lexicon of all test groundtruth words
(IC03-Full). For IC11, deﬁnes a list of 538 query
words to evaluate text based image retrieval.
The Street View Text (SVT) dataset consists of 249 high resolution images downloaded from
Google StreetView of road-side scenes. This is a challenging dataset with a lot of noise, as well as suﬀering
from many unannotated words. Per-image 50 word lexicons (SVT-50) are also provided.
The IIIT 5k-word dataset contains 3000
cropped word images of scene text and digital images
obtained from Google image search. This is the largest
dataset for natural image text recognition currently available. Each word image has an associated 50 word lexicon (IIIT5k-50) and 1k word lexicon (IIIT5k-1k).
IIIT Scene Text Retrieval (STR) is a text
based image retrieval dataset also collected with Google
image search. Each of the 50 query words has an associated list of 10-50 images that contain the query word.
There are also a large number of distractor images with
no text downloaded from Flickr. In total there are 10k
images and word bounding box annotation is not provided.
The IIIT Sports-10k dataset is another
text based image retrieval dataset constructed from frames
of sports video. The images are low resolution and often noisy or blurred, with text generally located on ad-
Recall (%)
# proposals
# proposals
Fig. 7 The recall and the average number of proposals per
image at each stage of the pipeline on IC03. (a) Edge Box
proposals, (b) ACF detector proposals, (c) Proposal ﬁltering,
(d) Bounding box regression, (e) Regression NMS round 1,
(f) Regression NMS round 2, (g) Regression NMS round 3.
The recall computed is detection recall across the dataset (i.e.
ignoring the recognition label) at 0.5 overlap.
vertisements and signboards, making this a challenging
retrieval task. 10 query words are provided with 10k
total images, without word bounding box annotations.
BBC News is a proprietary dataset of frames from
the British Broadcasting Corporation (BBC) programmes
that were broadcast between 2007 and 2012. Around
5000 hours of video (approximately 12 million frames)
were processed to select 2.3 million keyframes at 1024×
768 resolution. The videos are taken from a range of
diﬀerent BBC programmes on news and current aﬀairs,
including the BBC’s Evening News programme. Text is
often present in the frames from artiﬁcially inserted labels, subtitles, news-ticker text, and general scene text.
No labels or annotations are provided for this dataset.
8.2 Implementation Details
We train a single model for each of the stages in our
pipeline, and hyper parameters are selected using training datasets of ICDAR and SVT. Exactly the same
pipeline, with the same models and hyper parameters
are used for all datasets and experiments. This highlights the generalisability of our end-to-end framework
to diﬀerent datasets and tasks. The progression of de-
Max Jaderberg et al.
Text Recognition Datasets
Description
Our synthetically generated test dataset.
ICDAR 2003 test dataset.
ICDAR 2003 test dataset with ﬁxed lexicon.
ICDAR 2003 test dataset with ﬁxed lexicon.
SVT test dataset.
SVT test dataset with ﬁxed lexicon.
ICDAR 2013 test dataset.
IIIT5k test dataset with ﬁxed lexicon.
IIIT5k test dataset with ﬁxed lexicon.
Table 1 A description of the various text recognition datasets evaluated on.
Text Spotting Datasets
Description
ICDAR 2003 test dataset.
ICDAR 2003 test dataset with ﬁxed lexicon.
ICDAR 2003 test dataset with ﬁxed lexicon.
SVT test dataset.
SVT test dataset with ﬁxed lexicon.
ICDAR 2011 test dataset.
ICDAR 2013 test dataset.
Table 2 A description of the various text spotting datasets evaluated on.
Text Retrieval Datasets
Description
ICDAR 2011 test dataset.
SVT test dataset.
IIIT STR text retrieval dataset.
IIIT Sports-10k text retrieval dataset.
A dataset of keyframes from BBC News video.
Table 3 A description of the various text retrieval datasets evaluated on.
tection recall and the number of proposals as the pipeline
progresses can be seen in Fig. 7.
8.2.1 Edge Boxes & ACF Detector
The Edge Box detector has a number of hyper parameters, controlling the stride of evaluation and non maximal suppression. We use the default values of α = 0.65
and β = 0.75 (see for details of these parameters).
In practice, we saw little eﬀect of changing these parameters in combined recall.
For the ACF detector, we set the number of decision
trees to be 32, 128, 512 for each round of bootstrapping.
For feature aggregation, we use 4 × 4 blocks smoothed
with [1 2 1]/4 ﬁlter, with 8 scales per octave. As the
detector is trained for a particular aspect ratio, we perform detection at multiple aspect ratios in the range
[1, 1.2, 1.4, . . . , 3] to account for variable sized words.
We train on 30k cropped 32 × 100 positive word samples amalgamated from a number of training datasets as
outlined in , and randomly sample negative patches
from 11k images which do not contain text.
Fig. 8 shows the performance of our proposal generation stage. The recall at 0.5 overlap of groundtruth labelled words in the IC03 and SVT datasets is shown as
a function of the number of proposal regions generated
per image. The maximum recall achieved using Edge
Boxes is 92%, and the maximum recall achieved by the
ACF detector is around 70%. However, combining the
proposals from each method increases the recall to 98%
at 6k proposals and 97% at 11k proposals for IC03 and
SVT respectively. The average maximum overlap of a
particular proposal with a groundtruth bounding box
is 0.82 on IC03 and 0.77 on SVT, suggesting the region
proposal techniques produce some accurate detections
amongst the thousands of false-positives.
This high recall and high overlap gives a good starting point to the rest of our pipeline, and has greatly
reduced the search space of word detections from the
tens of millions of possible bounding boxes to around
10k proposals per image.
Reading Text in the Wild with Convolutional Neural Networks
Average num proposals / image
IC03 Edge Boxes (0.92 recall)
SVT Edge Boxes (0.92 recall)
IC03 ACF (0.70 recall)
SVT ACF (0.74 recall)
IC03 Combined (0.98 recall)
SVT Combined (0.97 recall)
Fig. 8 The 0.5 overlap recall of diﬀerent region proposal algorithms. The recall displayed in the legend for each method
gives the maximum recall achieved. The curves are generated
by decreasing the minimum score for a proposal to be valid,
and terminate when no more proposals can be found.
8.2.2 Random Forest Word Classiﬁer
The random forest word/no-word binary classiﬁer acts
on cropped region proposals. These are resampled to a
ﬁxed 32 × 100 size, and HOG features extracted with
a cell size of 4, resulting in h ∈R8×25×36, a 7200dimensional descriptor. The random forest classiﬁer consists of 10 trees with a maximum depth of 64.
For training, region proposals are extracted as we
describe in Section 4 on the training datasets of IC-
DAR and SVT, with positive bounding box samples
deﬁned as having at least 0.5 overlap with groundtruth,
and negative samples as less than 0.3 with groundtruth.
Due to the abundance of negative samples, we randomly
sample an equal number of negative samples to positive
samples, giving 300k positive and 400k negative training samples.
Once trained, the result is a very eﬀective falsepositive ﬁlter. We select an operating probability threshold of 0.5, giving 96.6% and 94.8% recall on IC03 and
SVT positive proposal regions respectively. This ﬁltering reduces the total number of region proposals to on
average 650 (IC03) and 900 (SVT) proposals per image.
8.2.3 Bounding Box Regressor
The bounding box regression CNN consists of four convolutional layers with stride 1 with
{filter size, number of filters} of {5, 64}, {5, 128},
{3, 256}, {3, 512} for each layer from input respectively,
followed by two fully-connected layers with 4k units and
4 units (one for each regression variable). All hidden
layers are followed by rectiﬁed linear non-linearities, the
inputs to convolutional layers are zero-padded to preserve dimensionality, and the convolutional layers are
followed by 2 × 2 max pooling. The ﬁxed sized input
to the CNN is a 32 × 100 greyscale image which is zero
centred by subtracting the image mean and normalised
by dividing by the standard deviation.
The CNN is trained with stochastic gradient descent
(SGD) with dropout on the fully-connected layers
to reduce overﬁtting, minimising the L2 distance between the estimated and groundtruth bounding boxes
(Equation 1). We used 700k training examples of bounding box proposals with greater than 0.5 overlap with
groundtruth computed on the ICDAR and SVT training datasets.
Before the regression, the average positive proposal
region (with over 0.5 overlap with groundtruth) had an
overlap of 0.61 and 0.60 on IC03 and SVT. The CNN
improves this average positive overlap to 0.88 and 0.70
for IC03 and SVT.
8.2.4 Text Recognition CNN
The text recognition CNN consists of eight weight layers – ﬁve convolutional layers and three fully-connected
layers. The convolutional layers have the following
{filter size, number of filters}: {5, 64}, {5, 128},
{3, 256}, {3, 512}, {3, 512}. The ﬁrst two fully-connected
layers have 4k units and the ﬁnal fully-connected layer
has the same number of units as as number of words
in the dictionary – 90k words in our case. The ﬁnal
classiﬁcation layer is followed by a softmax normalisation layer. Rectiﬁed linear non-linearities follow every
hidden layer, and all but the fourth convolutional layers are followed by 2 × 2 max pooling. The inputs to
convolutional layers are zero-padded to preserve dimensionality. The ﬁxed sized input to the CNN is a 32×100
greyscale image which is zero centred by subtracting the
image mean and normalised by dividing by the standard
deviation.
We train the network on Synth training data, backpropagating the standard multinomial logistic regression loss. Optimisation uses SGD with dropout regularisation of fully-connected layers, and we dynamically
lower the learning rate as training progresses. With uniform sampling of classes in training data, we found the
SGD batch size must be at least a ﬁfth of the total
number of classes in order for the network to train.
For very large numbers of classes (i.e. over 5k classes),
the SGD batch size required to train eﬀectively becomes
large, slowing down training a lot. Therefore, for large
dictionaries, we perform incremental training to avoid
requiring a prohibitively large batch size. This involves
Max Jaderberg et al.
initially training the network with 5k classes until partial convergence, after which an extra 5k classes are
added. The original weights are copied for the previously trained classes, with the extra classiﬁcation layer
weights being randomly initialised. The network is then
allowed to continue training, with the extra randomly
initialised weights and classes causing a spike in training error, which is quickly trained away. This process of
allowing partial convergence on a subset of the classes,
before adding in more classes, is repeated until the full
number of desired classes is reached.
At evaluation-time we do not do any data augmentation. If a lexicon is provided, we set the language
prior P(w|L) to be equal probability for lexicon words,
otherwise zero. In the absence of a lexicon, P(w|L) is
calculated as the frequency of word w in a corpus (we
use the opensubtitles.org English corpus) with power
law normalisation. In total, this model contains around
500 million parameters and can process a word in 2.2ms
on a GPU with a custom version of Caﬀe .
Recognition Results. We evaluate the accuracy of our
text recognition model over a wide range of datasets
and lexicon sizes. We follow the standard evaluation
protocol by and perform recognition on the words
containing only alphanumeric characters and at least
three characters.
The results are shown in Table 4, and highlight the
exceptional performance of our deep CNN. Although
we train on purely synthetic data, with no human annotation, our model obtains signiﬁcant improvements on
state-of-the-art accuracy across all standard datasets.
On IC03-50, the recognition problem is largely solved
with 98.7% accuracy – only 11 mistakes out of 860 test
samples – and we signiﬁcantly outperform the previous
state-of-the-art on SVT-50 by 5% and IC13 by 3%.
Compared to the ICDAR datasets, the SVT accuracy,
without the constraints of a dataset-speciﬁc lexicon, is
lower at 80.7%. This reﬂects the diﬃculty of the SVT
dataset as image samples can be of very low quality,
noisy, and with low contrast. The Synth dataset accuracy shows that our model really can recognise word
samples consistently across the whole 90k dictionary.
Synthetic Data Eﬀects. As an additional experiment,
we look into the contribution that the various stages
of the synthetic data generation engine in Section 6.1
make to real-world recognition accuracy. We deﬁne two
reduced recognition models (for speed of computation)
with dictionaries covering just the IC03 and SVT full
lexicons, denoted as DICT-IC03-Full and DICT-SVT-
Full respectively, which are tested only on their respective datasets. We repeatedly train these models from
Recognition Accuracy %
DICT−IC03−Full
DICT−SVT−Full
Fig. 9 The recognition accuracies of text recognition models
trained on just the IC03 lexicon (DICT-03-Full) and just the
SVT lexicon (DICT-SVT-Full), evaluated on IC03 and SVT
respectively. The models are trained on purely synthetic data
with increasing levels of sophistication of the synthetic data.
(a) Black text rendered on a white background with a single
font, Droid Sans. (b) Incorporating all of Google fonts. (c)
Adding background, foreground, and border colouring. (d)
Adding perspective distortions. (e) Adding noise, blur and
elastic distortions. (f) Adding natural image blending – this
gives an additional 6.2% accuracy on SVT. The ﬁnal accuracies on IC03 and SVT are 98.1% and 87.0% respectively.
scratch, with the same training procedure, but with increasing levels of sophistication of synthetic data. Fig. 9
shows how the test accuracy of these models increases
as more sophisticated synthetic training data is used.
The addition of random image-layer colouring causes
a notable increase in performance (+44% on IC03 and
+40% on SVT), as does the addition of natural image blending (+1% on IC03 and +6% on SVT). It is
interesting to observe a much larger increase in accuracy through incorporating natural image blending on
the SVT dataset compared to the IC03 dataset. This is
most likely due to the fact that there are more varied
and complex backgrounds to text in SVT compared to
8.3 Text Spotting
In the text spotting task, the goal is to localise and
recognise the words in the test images. Unless otherwise stated, we follow the standard evaluation protocol
by and ignore all words that contain alphanumeric
characters and are not at least three characters long. A
positive recognition result is only valid if the detection
bounding box has at least 0.5 overlap (IoU) with the
groundtruth.
Table 5 shows the results of our text spotting pipeline
compared to previous methods. We report the global
F-measure over all images in the dataset. Across all
datasets, our pipeline drastically outperforms all previ-
Reading Text in the Wild with Convolutional Neural Networks
Cropped Word Recognition Accuracy (%)
Synth IC03-50 IC03-Full
SVT-50 SVT IC13 IIIT5k-50 IIIT5k-1k
Baseline (ABBYY) 
Mishra 
Novikova 
Wang & Wu 
PhotoOCR 
Alsharif 
Almazan 
Jaderberg 
Gordo 
Table 4 Comparison to previous methods for text recognition accuracy – where the groundtruth cropped word image is given
as input. The ICDAR 2013 results given are case-insensitive. Bold results outperform previous state-of-the-art methods. The
baseline method is from a commercially available document OCR system. *Recognition is constrained to a dictionary of 50k
End-to-End Text Spotting (F-measure %)
IC03-50 IC03-Full IC03 IC03+
SVT-50 SVT IC11 IC11+
Neumann 
Wang & Wu 
Neumann 
Alsharif 
Jaderberg 
Proposed (0.3 IoU)
Table 5 Comparison to previous methods for end-to-end text spotting. Bold results outperform previous state-of-the-art
methods. *Recognition is constrained to a dictionary of 50k words. +Evaluation protocol described in .
Wang & Wu (F: 0.72)
Alsharif (F: 0.77)
Jaderberg (F: 0.80, AP: 0.78)
Proposed (F: 0.90, AP: 0.83)
Alsharif (0.70)
Jaderberg (F: 0.75, AP: 0.69)
Proposed (F: 0.86, AP: 0.79)
Jaderberg (F: 0.56, AP: 0.45)
Proposed (F: 0.76, AP: 63)
Fig. 10 The precision/recall curves on (a) the IC03-50 dataset, (b) the IC03-Full dataset, and (c) the SVT-50 dataset. The
lines of constant F-measure are shown at the maximum F-measure point of each curve. The results from were extracted
from the papers.
ous methods. On SVT-50, we increase the state-of-theart by +20% to a P/R/F (precision/recall/F-measure)
of 0.85/0.68/0.76 compared to 0.73/0.45/0.56 in .
Similarly impressive improvements can be seen on IC03,
where in all lexicon scenarios we improve F-measure
by at least +10%, reaching a P/R/F of 0.96/0.85/0.90.
Looking at the precision/recall curves in Fig. 10, we can
see that our pipeline manages to maintain very high recall, and the recognition score of our text recognition
system is a strong cue to the suitability of a detection.
We also give results across all datasets when no lexicon is given. As expected, the F-measure suﬀers from
the lack of lexicon constraints, though is still signiﬁcantly higher than other comparable work. It should
Max Jaderberg et al.
Text Based Image Retrieval
IC11 (mAP) SVT (mAP) STR (mAP) Sports (mAP) Sports (P@10) Sports (P@20)
Wang *
Neumann *
Mishra 
Table 6 Comparison to previous methods for text based image retrieval. We report mean average precision (mAP) for IC11,
SVT, STR, and Sports, and also report top-n retrieval to compute precision at n (P@n) on Sports. Bold results outperform
previous state-of-the-art methods. *Experiments were performed by Mishra et al. in , not by the original authors.
1.00/1.00/1.00
1.00/1.00/1.00
1.00/1.00/1.00
1.00/1.00/1.00
1.00/0.88/0.93
1.00/1.00/1.00
Fig. 11 Some example text spotting results from SVT-50 (top row) and IC11 (bottom row). Red dashed shows groundtruth
and green shows correctly localised and recognised results. P/R/F ﬁgures are given above each image.
be noted that the SVT dataset is only partially annotated. This means that the precision (and therefore Fmeasure) is much lower than the true precision if fully
annotated, since many words that are detected are not
annotated and are therefore recorded as false-positives.
We can however report recall on SVT-50 and SVT of
71% and 59% respectively.
Interestingly, when the overlap threshold is reduced
to 0.3 (last row of Table 5), we see a small improvement
across ICDAR datasets and a large +8% improvement
on SVT-50. This implies that our text recognition CNN
is able to accurately recognise even loosely cropped detections. Ignoring the requirement of correctly recognising the words, i.e. performing purely word detection, we
get an F-measure of 0.85 and 0.81 for IC03 and IC11.
Some example text spotting results are shown in
Fig. 11. Since our pipeline does not rely on connected
component based algorithms or explicit character recognition, we can detect and recognise disjoint, occluded
and blurry words.
A common failure mode of our system is the missing
of words due to the lack of suitable proposal regions, especially apparent for slanted or vertical text, something
which is not explicitly modelled in our framework. Also
the detection of sub-words or multiple words together
can cause false-positive results.
8.4 Image Retrieval
We also apply our pipeline to the task of text based image retrieval. Given a text query, the images containing
the query text must be returned.
This task is evaluated using the framework of ,
with the results shown in Table 6. For each deﬁned
query, we retrieve a ranked list of all the images of
the dataset and compute the average precision (AP)
Reading Text in the Wild with Convolutional Neural Networks
hollywood – P@100: 100%
boris johnson – P@100: 100%
vision – P@100: 93%
Fig. 12 The top two retrieval results for three queries on our BBC News dataset – hollywood, boris johnson, and vision.
The frames and associated videos are retrieved from 5k hours of BBC video. We give the precision at 100 (P@100) for these
queries, equivalent to the ﬁrst page of results of our web application.
for each query, reporting the mean average precision
(mAP) over all queries. We signiﬁcantly outperform
Mishra et al. across all datasets – we obtain an mAP
on IC11 of 90.3%, compared to 65.3% from . Our
method scales seamlessly to the larger Sports dataset,
where our system achieves a precision at 20 images
(P@20) of 92.5%, more than doubling that of 43.4%
from . Mishra et al. also report retrieval results on SVT for released implementations of other text
spotting algorithms. The method from Wang et al. 
achieves 21.3% mAP, the method from Neumann et al.
 acheives 23.3% mAP and the method proposed by
 itself achieves 56.2% mAP, compared to our own
result of 86.3% mAP.
However, as with the text spotting results for SVT,
our retrieval results suﬀer from incomplete annotations
on SVT and Sports datasets – Fig. 13 shows how precision is hurt by this problem. The consequence is that
the true mAP on SVT is higher than the reported mAP
Depending on the image resolution, our algorithm
takes approximately 5-20s to compute the end-to-end
results per image on a single CPU core and single GPU.
We analyse the time taken for each stage of our pipeline
on the SVT dataset, which has an average image size of
1260 × 860, showing the results in Table 7. Since we reduce the number of proposals throughout the pipeline,
we can allow the processing time per proposal to increase while keeping the total processing time for each
stage stable. This aﬀords us the use of more computa-
# proposals Time Time/proposal
(a) Edge Boxes
(b) ACF detector
(c) RF ﬁlter
(d) CNN regression
(e) CNN recognition
Table 7 The processing time for each stage of the pipeline
evaluated on the SVT dataset on a single CPU core and single
GPU. As the pipeline progresses from (a)-(e), the number
of proposals is reduced (starting from all possible bounding
boxes), allowing us to increase our computational budget per
proposal while keeping the overall processing time for each
stage comparable.
tionally complex features and classiﬁers as the pipeline
progresses. Our method can be trivially parallelised,
meaning we can process 1-2 images per second on a
high-performance workstation with 16 physical CPU
cores and 4 commodity GPUs.
The high precision and speed of our pipeline allows
us to process huge datasets for practical search applications. We demonstrate this on a 5000 hour BBC News
dataset. Building a search engine and front-end web application around our image retrieval pipeline allows a
user to instantly search for visual occurrences of text
within the huge video dataset. This works exceptionally well, with Fig. 12 showing some example retrieval
results from our visual search engine. While we do not
have groundtruth annotations to quantify the retrieval
performance on this dataset, we measure the precision
Max Jaderberg et al.
at 100 (P@100) for the test queries in Fig. 12, showing
a P@100 of 100% for the queries hollywood and boris
johnson, and 93% for vision. These results demonstrate the scalable nature of our framework.
9 Conclusions
In this work we have presented an end-to-end text reading pipeline – a detection and recognition system for
text in natural scene images. This general system works
remarkably well for both text spotting and image retrieval tasks, signiﬁcantly improving the performance
on both tasks over all previous methods, on all standard
datasets, without any dataset-speciﬁc tuning. This is
largely down to a very high recall proposal stage and a
text recognition model that achieves superior accuracy
to all previous systems. Our system is fast and scalable
– we demonstrate seamless scalability from datasets of
hundreds of images to being able to process datasets of
millions of images for instant text based image retrieval
without any perceivable degradation in accuracy. Additionally, the ability of our recognition model to be
trained purely on synthetic data allows our system to
be easily re-trained for recognition of other languages
or scripts, without any human labelling eﬀort.
We set a new benchmark for text spotting and image
retrieval. Moving into the future, we hope to explore
additional recognition models to allow the recognition
of unknown words and arbitrary strings.
Acknowledgements
This work was supported by the EPSRC and ERC grant
VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the
GPUs used for this research. We thank the BBC and
in particular Rob Cooper for access to data and video
processing resources.