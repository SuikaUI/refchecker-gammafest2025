MIT Open Access Articles
Inference on Treatment Effects after
Selection among High-Dimensional Controls
The MIT Faculty has made this article openly available. Please share
how this access benefits you. Your story matters.
Citation: Belloni, A.; Chernozhukov, V. and Hansen, C. “Inference on Treatment Effects after
Selection Among High-Dimensional Controls.” The Review of Economic Studies 81, no. 2
 : 608–650.
As Published: 
Publisher: Oxford University Press
Persistent URL: 
Version: Original manuscript: author's manuscript prior to formal peer review
Terms of use: Creative Commons Attribution-Noncommercial-Share Alike
 
INFERENCE ON TREATMENT EFFECTS AFTER SELECTION
AMONGST HIGH-DIMENSIONAL CONTROLS
A. BELLONI, V. CHERNOZHUKOV, AND C. HANSEN
Abstract. We propose robust methods for inference on the eﬀect of a treatment variable on
a scalar outcome in the presence of very many controls. Our setting is a partially linear model
with possibly non-Gaussian and heteroscedastic disturbances where the number of controls
may be much larger than the sample size. To make informative inference feasible, we require
the model to be approximately sparse; that is, we require that the eﬀect of confounding factors
can be controlled for up to a small approximation error by conditioning on a relatively small
number of controls whose identities are unknown. The latter condition makes it possible to
estimate the treatment eﬀect by selecting approximately the right set of controls. We develop a
novel estimation and uniformly valid inference method for the treatment eﬀect in this setting,
called the “post-double-selection” method. Our results apply to Lasso-type methods used for
covariate selection as well as to any other model selection method that is able to ﬁnd a sparse
model with good approximation properties.
The main attractive feature of our method is that it allows for imperfect selection of the
controls and provides conﬁdence intervals that are valid uniformly across a large class of models. In contrast, standard post-model selection estimators fail to provide uniform inference
even in simple cases with a small, ﬁxed number of controls. Thus our method resolves the
problem of uniform inference after model selection for a large, interesting class of models. We
illustrate the use of the developed methods with numerical simulations and an application to
the eﬀect of abortion on crime rates.
Key Words: treatment eﬀects, partially linear model, high-dimensional-sparse regression,
inference under imperfect model selection, uniformly valid inference after model selection
Date: First version: May 2010. This version is of May 10, 2012. This is a revision of a 2011 ArXiv/CEMMAP
paper entitled “Estimation of Treatment Eﬀects with High-Dimensional Controls”.
We thank Ted Anderson, Takeshi Amemiya, St´ephane Bonhomme, Mathias Cattaneo, Gary Chamberlain,
Denis Chetverikov, Graham Elliott, Eric Gretchen, Bruce Hansen, James Hamilton, Han Hong, Guido Imbens,
Tom MaCurdy, Anna Mikusheva, Whitney Newey, Alexei Onatsky, Joseph Romano, Andres Santos, Chris Sims,
and participants of 10th Econometric World Congress in Shanghai 2010, CIREQ-Montreal, Harvard-MIT, UC
San-Diego, Princeton, Stanford, and Infometrics Workshop for helpful comments.
BELLONI CHERNOZHUKOV HANSEN
1. Introduction
Many empirical analyses in economics focus on estimating the structural, causal, or treatment eﬀect of some variable on an outcome of interest. For example, we might be interested
in estimating the causal eﬀect of some government policy on an economic outcome such as
employment. Since economic policies and many other economic variables are not randomly
assigned, economists rely on a variety of quasi-experimental approaches based on observational
data when trying to estimate such eﬀects. One important method is based on the assumption
that the variable of interest can be taken as randomly assigned once a suﬃcient set of other
factors has been controlled for. Economists, for example, might argue that changes in statelevel public policies can be taken as randomly assigned relative to unobservable factors that
could aﬀect changes in state-level outcomes after controlling for aggregate macroeconomic activity, state-level economic activity, and state-level demographics; see, for example, Heckman,
LaLonde, and Smith or Imbens .
A problem empirical researchers face when relying on an identiﬁcation strategy for estimating
a structural eﬀect that relies on a conditional on observables argument is knowing which
controls to include. Typically, economic intuition will suggest a set of variables that might be
important but will not identify exactly which variables are important or the functional form
with which variables should enter the model. This lack of clear guidance about what variables
to use leaves researchers with the problem of selecting a set of controls from a potentially vast
set of control variables including raw regressors available in the data as well as interactions
and other transformations of these regressors. A typical economic study will rely on an ad hoc
sensitivity analysis in which a researcher reports results for several diﬀerent sets of controls
in an attempt to show that the parameter of interest that summarizes the causal eﬀect of the
policy variable is insensitive to changes in the set of control variables. See Donohue III and
Levitt , which we use as the basis for the empirical study in this paper, or examples in
Angrist and Pischke among many other references.
We present an approach to estimating and performing inference on structural eﬀects in
an environment where the treatment variable may be taken as exogenous conditional on observables that complements existing strategies. We pose the problem in the framework of a
partially linear model
yi = diα0 + g(zi) + ζi
where di is the treatment/policy variable of interest, zi is a set of control variables, and ζi
is an unobservable that satisﬁes E[ζi | di, zi] = 0.1
The goal of the econometric analysis
is to conduct inference on the treatment eﬀect α0. We examine the problem of selecting a
1 We note that di does not need to be binary.
INFERENCE AFTER MODEL SELECTION
set of variables from among p potential controls xi = P(zi), which may consist of zi and
transformations of zi, to adequately approximate g(zi) allowing for p > n. Of course, useful
inference about α0 is unavailable in this framework without imposing further structure on the
data. We impose such structure by assuming that exogeneity of di may be taken as given once
one controls linearly for a relatively small number s < n of variables in xi whose identities
are a priori unknown. This assumption implies that a linear combination of these s unknown
controls provides an approximation to g(zi) which produces relatively small approximation
errors.2 This assumption, which is termed approximate sparsity or simply sparsity, allows us
to approach the problem of estimating α0 as a variable selection problem. This framework
allows for the realistic scenario in which the researcher is unsure about exactly which variables
or transformations are important for approximating g(zi) and so must search among a broad
set of controls.
The assumed sparsity includes as special cases the most common approaches to parametric
and nonparametric regression analysis. Sparsity justiﬁes the use of fewer variables than there
are observations in the sample. When the initial number of variables is high, the assumption
justiﬁes the use of variable selection methods to reduce the number of variables to a manageable size. In many economic applications, formal and informal strategies are often used to
select such smaller sets of potential control variables. Most of these standard variable selection strategies are non-robust and may produce poor inference.3 In an eﬀort to demonstrate
robustness of their conclusions, researchers often employ ad hoc sensitivity analyses which
examine the robustness of inferential conclusions to variations in the set of controls. Such
sensitivity analyses are useful but lack rigorous justiﬁcation. As a complement to these ad
hoc approaches, we propose a formal, rigorous approach to inference allowing for selection of
controls. Our proposal uses modern variable selection methods in a novel manner which results
in robust and valid inference.
The main contributions of this paper are providing a robust estimation and inference method
within a partially linear model with potentially very high-dimensional controls and developing
the supporting theory. The method relies on the use of Lasso-type or other sparsity-inducing
procedures for variable selection. Our approach diﬀers from usual post-model-selection methods that rely on a single selection step. Rather, we use two diﬀerent variable selection steps
followed by a ﬁnal estimation step as follows:
2We carefully deﬁne what we mean by small approximation errors in Section 2.
3An example of inference going wrong is given in Figure 1 (left panel), presented in the next section, where
a standard post-model selection estimator has a bimodal distribution which sharply deviates from the standard
normal distribution. More examples are given in Section 6 where we document the poor inferential performance
of a standard post-model selection method.
BELLONI CHERNOZHUKOV HANSEN
1. In the ﬁrst step, we select a set of control variables that are useful for predicting the
treatment di. This step helps to insure robustness by ﬁnding control variables that are
strongly related to the treatment and thus potentially important confounding factors.
2. In the second step, we select additional variables by selecting control variables that
predict yi.
This step helps to insure that we have captured important elements in
the equation of interest, ideally helping keep the residual variance small as well as
intuitively providing an additional chance to ﬁnd important confounds.
3. In the ﬁnal step, we estimate the treatment eﬀect α0 of interest by the linear regression
of yi on the treatment di and the union of the set of variables selected in the two
variable selection steps.
We provide theoretical results on the properties of the resulting treatment eﬀect estimator
and show that it provides inference that is uniformly valid over large classes of models and
also achieves the semi-parametric eﬃciency bound under some conditions. Importantly, our
theoretical results allow for imperfect variable selection in either of the two variable selection
steps as well as allowing for non-Gaussianity and heteroscedasticity of the model’s errors.4
We illustrate the theoretical results through an examination of the eﬀect of abortion on
crime rates following Donohue III and Levitt . In this example, we ﬁnd that the formal
variable selection procedure produces a qualitatively diﬀerent result than that obtained through
the ad hoc set of sensitivity results presented in the original paper. By using formal variable
selection, we select a small set of between eight and fourteen variables depending on the
outcome, compared to the set of eight variables considered by Donohue III and Levitt .
Once this set of variables is linearly controlled for, the estimated abortion eﬀect is rendered
imprecise. It is interesting that the key variable selected by the variable selection procedure
is the initial condition for the abortion rate. The selection of this initial condition and the
resulting imprecision of the estimated treatment eﬀect suggest that one cannot determine
precisely whether the eﬀect attributed to abortion found when this initial condition is omitted
from the model is due to changes in the abortion rate or some other persistent state-level
factor that is related to relevant changes in the abortion rate and current changes in the crime
rate.5 It is interesting that Foote and Goetz raise a similar concern based on intuitive
grounds and additional data in a comment on Donohue III and Levitt .
Goetz ﬁnd that a linear trend interacted with crime rates before abortion could have
4In a companion paper that presents an overview of results for ℓ1-penalized estimators, Belloni, Chernozhukov, and Hansen , we provide similar results in the idealized Gaussian homoscedastic framework.
5Note that all models are estimated in ﬁrst-diﬀerences to eliminate any state-speciﬁc factors that might be
related to both the relevant level of the abortion rate and the level of the crime rate.
INFERENCE AFTER MODEL SELECTION
had an eﬀect renders the estimated abortion eﬀects imprecise.6 Overall, ﬁnding that a formal,
rigorous approach to variable selection produces a qualitatively diﬀerent result than a more ad
hoc approach suggests that these methods might be used to complement economic intuition in
selecting control variables for estimating treatment eﬀects in settings where treatment is taken
as exogenous conditional on observables.
Relationship to literature. We contribute to several existing literatures. First, we contribute to the literature on series estimation of partially linear models , H¨ardle, Liang, and Gao , Robinson , and others). We diﬀer from most of
the existing literature which considers p ≪n series terms by allowing p ≫n series terms from
which we select bs ≪n terms to construct the regression ﬁts. Considering an initial broad set
of terms allows for more reﬁned approximations of regression functions relative to the usual
approach that uses only a few low-order terms. See, for example, Belloni, Chernozhukov, and
Hansen for a wage function example and Section 5 for theoretical examples. However,
our most important contribution is to allow for data-dependent selection of the appropriate
series terms. The previous literature on inference in the partially linear model generally takes
the series terms as given without allowing for their data-driven selection. However, selection
of series terms is crucial for achieving consistency when p ≫n and is needed for increasing
eﬃciency even when p = Cn with C < 1. That the standard estimator can be be highly
ineﬃcient in the latter case follows from results in Cattaneo, Jansson, and Newey .7 We
focus on Lasso for performing this selection as a theoretically and computationally attractive
device but note that any other method, such as selection using the traditional generalized
cross-validation criteria, will work as long as the method guarantees suﬃcient sparsity in its
solution. After model selection, one may apply conventional standard errors or the reﬁned
standard errors proposed by Cattaneo, Jansson, and Newey .8
complicated
speciﬁcation
supplement
 we provide additional results based on Donohue III
and Levitt . The conclusions are similar to those obtained in this paper in that we ﬁnd the estimated
abortion eﬀect becomes imprecise once one allows for a broad set of controls and selects among them. However,
the speciﬁcation of Donohue III and Levitt relies on a large number of district cross time ﬁxed eﬀects
and so does not immediately ﬁt into our regularity conditions. We conjecture the methodology continues to
work in this case but leave veriﬁcation to future research.
7 Cattaneo, Jansson, and Newey derive properties of series estimator under p = Cn, C < 1, asymptotics. It follows from their results that under homoscedasticity the series estimator achieves the semiparametric
eﬃciency bound only if C →0.
8If the selected number of terms bs is a substantial fraction of n, we recommend using Cattaneo, Jansson, and
Newey standard errors after applying our model selection procedure.
BELLONI CHERNOZHUKOV HANSEN
Second, we contribute to the literature on the estimation of treatment eﬀects. We note that
the policy variable di does not have to be binary in our framework. However, our method has
a useful interpretation related to the propensity score when di is binary. In the ﬁrst selection
step, we select terms from xi that predict the treatment di, i.e. terms that explain the propensity score. We also select terms from xi that predict yi, i.e. terms that explain the outcome
regression function. Then we run a ﬁnal regression of yi on the treatment di and the union of
selected terms. Thus, our procedure relies on the selection of variables relevant for both the
propensity score and the outcome regression. Relying on selecting variables that are important
for both objects allows us to achieve two goals: we obtain uniformly valid conﬁdence sets for
α0 despite imperfect model selection and we achieve full eﬃciency for estimating α0 in the
homoscedastic case. The relation of our approach to the propensity score brings about interesting connections to the treatment eﬀects literature. Hahn , Heckman, Ichimura, and
Todd , and Abadie and Imbens have constructed eﬃcient regression or matchingbased estimates of average treatment eﬀects. Hahn also shows that conditioning on the
propensity score is unnecessary for eﬃcient estimation of average treatment eﬀects. Hirano,
Imbens, and Ridder demonstrate that one can eﬃciently estimate average treatment
eﬀects using estimated propensity score weighting alone. Robins and Rotnitzky have
shown that using propensity score modeling coupled with a parametric regression model leads
to eﬃcient estimates if either the propensity score model or the parametric regression model is
correct. While our contribution is quite distinct from these approaches, it also highlights the
important robustness role played by the propensity score model in the selection of the right
control terms for the ﬁnal regression.
Third, we contribute to the literature on estimation and inference with high-dimensional
data and to the uniformity literature.
There has been extensive work on estimation and
perfect model selection in both low and high-dimensional contexts,9 but there has been little
work on inference after imperfect model selection. Perfect model selection relies on unrealistic
assumptions, and model selection mistakes can have serious consequences for inference as has
been shown in P¨otscher , Leeb and P¨otscher , and others. In work on instrument
selection for estimation of a linear instrumental variables model, Belloni, Chen, Chernozhukov,
and Hansen have shown that model selection mistakes do not prevent valid inference
about low-dimensional structural parameters due to the inherent adaptivity of the problem:
Omission of a relevant instrument does not aﬀect consistency of an IV estimator as long as
there is another relevant instrument. The partially linear regression model (1.1) does not have
the same adaptivity structure, and model selection based on the outcome regression alone
9For reviews focused on econometric applications, see, e.g., Hansen and Belloni, Chernozhukov, and
Hansen .
INFERENCE AFTER MODEL SELECTION
produces non-robust conﬁdence intervals.10 Our post-double selection procedure creates the
necessary adaptivity by performing two separate model selection steps, making it possible
to perform robust/uniform inference after model selection. The uniformity holds over large,
interesting classes of high-dimensional sparse models. In that regard, our contribution is in the
spirit and builds upon the classical contribution by Romano on the uniform validity of
t-tests for the univariate mean. It also shares the spirit of recent contributions, among others,
by Mikusheva on uniform inference in autoregressive models, by Andrews and Cheng
 on uniform inference in moment condition models that are potentially unidentiﬁed, and
by Andrews, Cheng, and Guggenberger on a generic framework for uniformity analysis.
Finally, we contribute to the broader literature on high-dimensional estimation. For variable
selection we use ℓ1-penalization methods, though our method and theory will allow for the use
of other methods. ℓ1-penalized methods have been proposed for model selection problems in
high-dimensional least squares problems, e.g. Lasso in Frank and Friedman and Tibshirani , in part because they are computationally eﬃcient. Many ℓ1-penalized methods
have been shown to have good estimation properties even when perfect variable selection is
not feasible; see, e.g., Candes and Tao , Meinshausen and Yu , Bickel, Ritov, and
Tsybakov , Huang, Horowitz, and Wei , Belloni and Chernozhukov and
the references therein. Such methods have also been shown to extend suitably to nonparametric and non-Gaussian cases as in Bickel, Ritov, and Tsybakov and Belloni, Chen,
Chernozhukov, and Hansen . These methods also produce models with a relatively small
set of variables. The last property is important in that it leaves the researcher with a set of
variables that may be examined further; in addition it corresponds to the usual approach in
economics that relies on considering a small number of controls.
Paper Organization. In Section 2, we formally present the modeling environment including the key sparsity condition and develop our advocated estimation and inference method.
We establish the consistency and asymptotic normality of our estimator of α0 uniformly over
large classes of models in Section 3. In Section 4, we present a generalization of the basic procedure to allow for model selection methods other than Lasso. In Section 5, we present a series
of theoretical examples in which we provide primitive condition that imply the higher-level
conditions of Section 3. In Section 6, we present a series of numerical examples that verify our
theoretical results numerically, and we apply our method to the abortion and crime example
of Donohue III and Levitt in Section 7. In appendices, we provide the proofs.
10The poor performance of inference on a treatment eﬀect after model selection on only the outcome equation
is shown through simulations in Section 6.
BELLONI CHERNOZHUKOV HANSEN
Notation. In what follows, we work with triangular array data {(ωi,n, i = 1, ..., n) , n =
1, 2, 3, ...} deﬁned on probability space (Ω, A, Pn), where P = Pn can change with n. Each
ωi,n = (y′
i,n)′ is a vector with components deﬁned below, and these vectors are i.n.i.d.
– independent across i, but not necessarily identically distributed. Thus, all parameters that
characterize the distribution of {ωi,n, i = 1, ..., n} are implicitly indexed by Pn and thus by
n. We omit the dependence on these objects from the notation in what follows for notational
simplicity. We use array asymptotics to better capture some ﬁnite-sample phenomena and
to insure the robustness of conclusions with respect to perturbations of the data-generating
process P along various sequences. This robustness, in turn, translates into uniform validity
of conﬁdence regions over certain regions of data-generating processes.
We use the following empirical process notation, En[f] := En[f(ωi)] := Pn
i=1 f(ωi)/n, and
Gn(f) := Pn
i=1(f(ωi) −E[f(ωi)])/√n. Since we want to deal with i.n.i.d.
data, we also
introduce the average expectation operator: ¯E[f] := EEn[f] = EEn[f(ωi)] = Pn
i=1 E[f(ωi)]/n.
The l2-norm is denoted by ∥· ∥, and the l0-norm, ∥· ∥0, denotes the number of non-zero
components of a vector. We use ∥· ∥∞to denote the maximal element of a vector. Given a
vector δ ∈Rp, and a set of indices T ⊂{1, . . . , p}, we denote by δT ∈Rp the vector in which
δTj = δj if j ∈T, δTj = 0 if j /∈T. We use the notation (a)+ = max{a, 0}, a ∨b = max{a, b},
and a ∧b = min{a, b}. We also use the notation a ≲b to denote a ⩽cb for some constant
c > 0 that does not depend on n; and a ≲P b to denote a = OP (b). For an event E, we say
that E wp →1 when E occurs with probability approaching one as n grows. Given a p-vector
b, we denote support(b) = {j ∈{1, ..., p} : bj ̸= 0}.
2. Inference on Treatment and Structural Effects Conditional on
Observables
2.1. Framework. We consider the partially linear model
yi = diα0 + g(zi) + ζi,
E[ζi | zi, di] = 0,
di = m(zi) + vi,
E[vi | zi] = 0,
where yi is the outcome variable, di is the policy/treatment variable whose impact α0 we would
like to infer, zi represents confounding factors on which we need to condition, and ζi and vi are
disturbances. The parameter α0 is the average treatment or structural eﬀect under appropriate
conditions given, for example, in Heckman, LaLonde, and Smith or Imbens and
is of major interest in many empirical studies.
INFERENCE AFTER MODEL SELECTION
The confounding factors zi aﬀect the policy variable via the function m(zi) and the outcome
variable via the function g(zi). Both of these functions are unknown and potentially complicated. We use linear combinations of control terms xi = P(zi) to approximate g(zi) and m(zi),
writing (2.2) and (2.3) as
yi = diα0 + x′
iβg0 + rgi
iβm0 + rmi
iβg0 and x′
iβm0 are approximations to g(zi) and m(zi), and rgi and rmi are the corresponding approximation errors. In order to allow for a ﬂexible speciﬁcation and incorporation
of pertinent confounding factors, the vector of controls, xi = P(zi), can have a dimension
p = pn which can be large relative to the sample size. Speciﬁcally, our results only require
log p = o(n1/3) along with other technical conditions. High-dimensional regressors xi = P(zi)
could arise for diﬀerent reasons. For instance, the list of available controls could be large, i.e.
xi = zi as in e.g. Koenker . It could also be that many technical controls are present;
i.e. the list xi = P(zi) could be composed of a large number of transformations of elementary
regressors zi such as B-splines, dummies, polynomials, and various interactions as in Newey
 or Chen .
Having very many controls creates a challenge for estimation and inference. A key condition
that makes it possible to perform constructive estimation and inference in such cases is termed
sparsity. Sparsity is the condition that there exist approximations x′
iβg0 and x′
iβm0 to g(zi)
and m(zi) in (2.4)-(2.5) that require only a small number of non-zero coeﬃcients to render the
approximation errors rgi and rmi suﬃciently small relative to estimation error. More formally,
sparsity relies on two conditions. First, there exist βg0 and βm0 such that at most s = sn ≪n
elements of βm0 and βg0 are non-zero so that
∥βm0∥0 ⩽s and ∥βg0∥0 ⩽s.
Second, the sparsity condition requires the size of the resulting approximation errors to be
small compared to the conjectured size of the estimation error:
s/n and {¯E[r2
Note that the size of the approximating model s = sn can grow with n just as in standard
series estimation.
The high-dimensional-sparse-model framework outlined above extends the standard framework in the treatment eﬀect literature which assumes both that the identities of the relevant
controls are known and that the number of such controls s is much smaller than the sample
BELLONI CHERNOZHUKOV HANSEN
size. Instead, we assume that there are many, p, potential controls of which at most s controls
suﬃce to achieve a desirable approximation to the unknown functions g(·) and m(·) and allow
the identity of these controls to be unknown. Relying on this assumed sparsity, we use selection methods to select approximately the right set of controls and then estimate the treatment
2.2. The Method: Least Squares after Double Selection. We propose the following
method for estimating and performing inference about α0. The most important feature of this
method is that it does not rely on the highly unrealistic assumption of perfect model selection
which is often invoked to justify inference after model selection. To the best of our knowledge,
our result is the ﬁrst of its kind in this setting. This result extends our previous results on
inference under imperfect model selection in the instrumental variables model given in Belloni,
Chen, Chernozhukov, and Hansen .
The problem is fundamentally more diﬃcult in
the present paper due to lack of adaptivity in estimation which we overcome by introducing
additional model selection steps. The construction of our advocated procedure reﬂects our
eﬀort to oﬀer a method that has attractive robustness/uniformity properties for inference.
The estimator is √n-consistent and asymptotically normal under mild conditions and provides
conﬁdence intervals that are robust to various perturbations of the data-generating process
that preserve approximate sparsity.
To deﬁne the method, we ﬁrst write the reduced form corresponding to (2.2)-(2.3) as:
i ¯β0 + ¯ri + ¯ζi,
iβm0 + rmi + vi,
where ¯β0 := α0βm0 + βg0,
¯ri := α0rmi + rgi,
¯ζi := α0vi + ζi.
We have two equations and hence can apply model selection methods to each equation to
select control terms.
The chief method we discuss is the Lasso method described in more
detail below.
Given the set of selected controls from (2.6) and (2.7), we can estimate α0
by a least squares regression of yi on di and the union of the selected controls.
on α0 may then be performed using conventional methods for inference about parameters
estimated by least squares.
Intuitively, this procedure works well since we are more likely
to recover key controls by considering selection of controls from both equations instead of
just considering selection of controls from the single equation (2.4) or (2.6). In ﬁnite-sample
experiments, single-selection methods essentially fail, providing poor inference relative to the
double-selection method outlined above. This performance is also supported theoretically by
INFERENCE AFTER MODEL SELECTION
the fact that the double-selection method requires weaker regularity conditions for its validity
and for attaining the eﬃciency bound11 than the single selection method.
Now we formally deﬁne the post-double-selection estimator: Let bI1 = support(bβ1) denote
the control terms selected by a feasible Lasso estimator bβ1 computed using data (˜yi, ˜xi) =
(di, xi), i = 1, ..., n. Let bI2 = support(bβ2) denote the control terms selected by a feasible Lasso
estimator bβ2 computed using data (˜yi, ˜xi) = (yi, xi), i = 1, ..., n. The post-double-selection
estimator ˇα of α0 is deﬁned as the least squares estimator obtained by regressing yi on di and
the selected control terms xij with j ∈bI ⊇bI1 ∪bI2:
(ˇα, ˇβ) = argmin
α∈R,β∈Rp{En[(yi −diα −x′
iβ)2] : βj = 0, ∀j ̸∈bI}.
The set bI may contain variables that were not selected in the variable selection steps with
indices in bI3 that the analyst thinks are important for ensuring robustness. We call bI3 the
amelioration set. Thus, bI = bI1 ∪bI2 ∪bI3; let bs = |bI| and bsj = |bIj| for j = 1, 2, 3.
We deﬁne feasible Lasso estimators below and note that other selection methods could be
used as well. When a feasible Lasso is used to construct bI1 and bI2, we refer to the post-doubleselection estimator as the post-double-Lasso estimator. When other model selection devices
are used to construct bI = bI1 and bI2, we shall refer the estimator as the generic post-doubleselection estimator.
The main theoretical result of the paper shows that the post-double-selection estimator ˇα
i ]−1¯E[v2
i ]−1)−1/2√n(ˇα −α0) ⇝N(0, 1)
under approximate sparsity conditions, uniformly within a rich set of data generating processes. We also show that the standard plug-in estimator for standard errors is consistent
in these settings. All of these results imply uniform validity of conﬁdence regions over large,
interesting classes of models. Figure 2.2 (right panel) illustrates the result (2.9) by showing
that the ﬁnite-sample distribution of our post-double-selection estimator is very close to the
normal distribution. In contrast, Figure 2.2 (left panel) illustrates the classical problem with
the traditional post-single-selection estimator based on (2.4), showing that its distribution is
bimodal and sharply deviates from the normal distribution. Finally, it is worth noting that
the estimator achieves the semi-parametric eﬃciency bound under homoscedasticity.
11Semi-parametric eﬃciency is attained in the homoscedastic case.
BELLONI CHERNOZHUKOV HANSEN
−8 −7 −6 −5 −4 −3 −2 −1
post-single-selection estimator
−8 −7 −6 −5 −4 −3 −2 −1
post-double-selection estimator
Distributions of Studentized Estimators
Figure 1. The ﬁnite-sample distributions (densities) of the standard post-single selection
estimator (left panel) and of our proposed post-double selection estimator (right panel). The
distributions are given for centered and studentized quantities. The results are based on 10000
replications of Design 1 described in Section 6, with R2’s in equation (2.6) and (2.7) set to
2.3. Selection of controls via feasible Lasso Methods. Here we describe feasible variable
selection via Lasso. Note that each of the regression equations above is of the form
where f(˜zi) is the regression function, ˜x′
iβ0 is the approximation based on the dictionary
˜xi = P(˜zi), ri is the approximation error, and ǫi is the error. The Lasso estimator is deﬁned
as a solution to
β∈Rp En[ and . The kinked
nature of the penalty function induces the solution bβ to have many zeroes, and thus the Lasso
solution may be used for model selection. The selected model bT = support(bβ) is often used for
further reﬁtting by least squares, leading to the so called post-Lasso or Gauss-Lasso estimator,
see, e.g., Belloni and Chernozhukov . The Lasso estimator/selector is computationally
attractive because it minimizes a convex function. In the homoskedastic Gaussian case, a basic
choice for penalty level suggested by Bickel, Ritov, and Tsybakov is
λ = 2 · cσ
2n log(2p/γ),
INFERENCE AFTER MODEL SELECTION
where c > 1, 1 −γ is a conﬁdence level that needs to be set close to 1, and σ is the standard
deviation of the noise. The formal motivation for this penalty is that it leads to near-optimal
rates of convergence of the estimator under approximate sparsity. The good behavior of the
estimator of β0 in turn implies good approximation properties of the selected model bT, as
noted in Belloni and Chernozhukov . Unfortunately, even in the homoskedastic case
the penalty level speciﬁed above is not feasible since it depends on the unknown σ.
Belloni, Chen, Chernozhukov, and Hansen formulate a feasible Lasso estimator/selector
bβ geared for heteroscedastic, non-Gaussian cases, which solves
β∈Rp En[(˜yi −˜x′
where bΨ = diag(bl1, . . . ,blp) is a diagonal matrix of penalty loadings. The penalty level λ and
loadings blj’s are set as
λ = 2 · c√nΦ−1(1 −γ/2p) and blj = lj + oP (1),
i ], uniformly in j = 1, . . . , p,
where c > 1 and 1 −γ is a conﬁdence level.12 The lj’s are ideal penalty loadings that are
not observed, and we estimate lj by blj obtained via an iteration method given in Appendix
A. We refer to the resulting feasible Lasso method as the Iterated Lasso. The estimator bβ
has statistical performance that is similar to that of the (infeasible) Lasso described above in
Gaussian cases and delivers similar performance in non-Gaussian, heteroscedastic cases; see
Belloni, Chen, Chernozhukov, and Hansen . In this paper, we only use bβ as a model
selection device. Speciﬁcally, we only make use of
bT = support(bβ),
the labels of the regressors with non-zero estimated coeﬃcients. We show that the selected
model bT has good approximation properties for the regression function f under approximate
sparsity in Section 3.
Belloni, Chernozhukov, and Wang propose another feasible variant of Lasso called
the Square-root Lasso estimator, bβ, deﬁned as a solution to
En[(˜yi −˜x′
with the penalty level
λ = c · √nΦ−1(1 −γ/2p),
12Practical recommendations include the choice c = 1.1 and γ = .05.
BELLONI CHERNOZHUKOV HANSEN
where c > 1, γ ∈(0, 1) is a conﬁdence level, and bΨ = diag(bl1, . . . ,blp) is a diagonal matrix of
penalty loadings. The main attractive feature of (2.14) is that one can set blj = {En[˜x2
which depends only on observed data in the homoscedastic case.
In the heteroscedastic case, we would like to choose blj so that
lj + oP(1) ⩽blj ≲P lj, where lj = {En[˜x2
i ]]/En[ǫ2
i ]}1/2, uniformly in j = 1, ..., p.
As a simple bound, we could use blj = 2{En[˜x4
ij]}1/4 since
i ]]/En[ǫ2
i ]}1/2 ⩽{En[˜x4
ij]}1/4{En[ǫ4
i ]}1/4/{En[ǫ2
This bound gives lj + oP (1) ⩽blj if {En[ǫ4
i ]}1/4/{En[ǫ2
i ]}1/2 ⩽2 + oP (1), which covers a wide
class of marginal distributions for error ǫi. For example, all t-distributions with degrees of
freedom greater than ﬁve satisfy this condition. As in the previous case, we can also iteratively
re-estimate the penalty loadings using estimates of the ǫi’s to approximate the ideal penalty
blj = lj + oP (1), uniformly in j = 1, ..., p.
The resulting Square-root Lasso and post-Square-root Lasso estimators based on these penalty
loadings achieve near optimal rates of convergence even in non-Gaussian, heteroscedastic cases.
This good performance implies good approximation properties for the selected model bT.
In what follows, we shall use the term feasible Lasso to refer to either the Iterated Lasso
estimator bβ solving (2.12)-(2.13) or the Square-root Lasso estimator bβ solving (2.14)-(2.16)
with c > 1 and 1 −γ set such that
γ = o(1) and log(1/γ) ≲log(p ∨n).
3. Theory of Estimation and Inference
3.1. Regularity Conditions. In this section, we provide regularity conditions that are suf-
ﬁcient for validity of the main estimation and inference result. We begin by stating our main
condition, which contains the previously deﬁned approximate sparsity as well as other more
technical assumptions. Throughout the paper, we let c, C, and q be absolute constants, and
let ℓn ր ∞, δn ց 0, and ∆n ց 0 be sequences of absolute positive constants. By absolute
constants, we mean constants that are given, and do not depend the dgp P = Pn.
We assume that for each n the following condition holds on dgp P = Pn.
Condition ASTE (P). (i) {(yi, di, zi), i = 1, ..., n} are i.n.i.d. vectors on (Ω, F, P) that
obey the model (2.2)-(2.3), and the vector xi = P(zi) is a dictionary of transformations of zi,
which may depend on n but not on P. (ii) The true parameter value α0, which may depend
INFERENCE AFTER MODEL SELECTION
on P, is bounded, |α0| ⩽C. (iii) Functions m and g admit an approximately sparse form.
Namely there exists s ⩾1 and βm0 and βg0, which depend on n and P, such that
m(zi) = x′
iβm0 + rmi,
∥βm0∥0 ⩽s,
mi]}1/2 ⩽C
g(zi) = x′
iβg0 + rgi,
∥βg0∥0 ⩽s,
gi]}1/2 ⩽C
(iv) The sparsity index obeys s2 log2(p ∨n)/n ⩽δn and the size of the amelioration set obeys
bs3 ⩽C(1 ∨bs1 ∨bs2). (v) For ˜vi = vi + rmi and ˜ζi = ζi + rgi we have |¯E[˜v2
i ] −¯E[v2
and ¯E[|˜vi|q + |˜ζi|q] ⩽C for some q > 4. Moreover, maxi⩽n ∥xi∥2
∞sn−1/2+2/q ⩽δn wp 1 −∆n.
Comment 3.1. The approximate sparsity (iii) and the growth condition (iv) are the main
conditions for establishing the key inferential result. We present a number of primitive examples
to show that these conditions contain standard models used in empirical research as well as
more ﬂexible models. Condition (iv) requires that the size bs3 of the amelioration set bI3 should
not be substantially larger than the size of the set of variables selected by the Lasso method.
Simply put, if we decide to include controls in addition to those selected by Lasso, the total
number of additions should not dominate the number of controls selected by Lasso.
and other conditions will ensure that the total number bs of controls obeys bs ≲P s, and we
also require that s2 log2(p ∨n)/n →0.
This condition can be relaxed using the samplesplitting method of Fan, Guo, and Hao , which is done in the Supplementary Appendix.
Condition (v) is simply a set of suﬃcient conditions for consistent estimation of the variance of
the double selection estimator. If the regressors are uniformly bounded and the approximation
errors are going to zero a.s., it is implied by other conditions stated below; and it can also be
demonstrated under other sorts of more primitive conditions.
The next condition concerns the behavior of the Gram matrix En[xix′
i]. Whenever p > n, the
empirical Gram matrix En[xix′
i] does not have full rank and in principle is not well-behaved.
However, we only need good behavior of smaller submatrices. Deﬁne the minimal and maximal
m-sparse eigenvalue of a semi-deﬁnite matrix M as
φmin(m)[M] :=
and φmax(m)[M] :=
To assume that φmin(m)[En[xix′
i]] > 0 requires that all empirical Gram submatrices formed
by any m components of xi are positive deﬁnite. We shall employ the following condition as a
suﬃcient condition for our results.
Condition SE (P).
There is an absolute sequence of constants ℓn →∞such that the
maximal and minimal ℓns-sparse eigenvalues are bounded from below and away from zero,
namely with probability at least 1 −∆n,
κ′ ⩽φmin(ℓns)[En[xix′
i]] ⩽φmax(ℓns)[En[xix′
BELLONI CHERNOZHUKOV HANSEN
where 0 < κ′ < κ′′ < ∞are absolute constants.
Comment 3.2. It is well-known that Condition SE is quite plausible for many designs of
interest. For instance, Condition SE holds if
(a) xi, i = 1, . . . , n, are i.i.d. zero-mean sub-Gaussian random vectors that have population
Gram matrix E[xix′
i] with minimal and maximal s log n-sparse eigenvalues bounded
away from zero and from above by absolute constants where s(log n)(log p)/n ⩽δn →0;
(b) xi, i = 1, . . . , n, are i.i.d.
bounded zero-mean random vectors with ∥xi∥∞⩽Kn
that have population Gram matrix E[xix′
i] with minimal and maximal s log nsparse eigenvalues bounded from above and away from zero by absolute constants
ns(log3 n){log(p ∨n)}/n ⩽δn →0.
The claim (a) holds by Theorem 3.2 in Rudelson and Zhou 
and Baraniuk, Davenport, DeVore, and Wakin ) and claim (b) holds by Lemma 1 in
Belloni and Chernozhukov or by Theorem 1.8 Rudelson and Zhou .
that a standard assumption in econometric research is to assume that the population Gram
matrix E[xix′
i] has eigenvalues bounded from above and away from zero, see e.g. Newey .
The conditions above allow for this and more general behavior, requiring only that the s log n
sparse eigenvalues of the population Gram matrix E[xix′
i] are bounded from below and from
The next condition imposes moment conditions on the structural errors and regressors.
Condition SM (P).
There are absolute constants 0 < c < C < ∞and 4 < q < ∞such
that for (˜yi, ǫi) = (yi, ζi) and (˜yi, ǫi) = (di, vi) the following conditions hold:
(i) ¯E[|di|q] ⩽C,
i | xi, vi] ⩽C and c ⩽E[v2
i | xi] ⩽C a.s. 1 ⩽i ⩽n,
(ii) ¯E[|ǫi|q] + ¯E[˜y2
1⩽j⩽p{¯E[x2
i ] + ¯E[|x3
i |] + 1/¯E[x2
(iii) log3 p/n ⩽δn,
1⩽j⩽p{|(En −¯E)[x2
i ]| + |(En −¯E)[x2
i ]|} + max
1⩽i⩽n ∥xi∥2
s log(n ∨p)
⩽δn wp 1 −∆n.
These conditions, which are rather mild, ensure good model selection performance of feasible
Lasso applied to equations (2.6) and (2.7). These conditions also allow us to invoke moderate
deviation theorems for self-normalized sums from Jing, Shao, and Wang to bound some
important error components.
INFERENCE AFTER MODEL SELECTION
3.2. The Main Result. The following is the main result of this paper. It shows that the
post-double selection estimator is root-n consistent and asymptotically normal. Under homoscedasticity this estimator achieves the semi-parametric eﬃciency bound. The result also
veriﬁes that plug-in estimates of the standard errors are consistent.
Theorem 1 (Estimation and Inference on Treatment Eﬀects). Let {Pn} be a sequence of datagenerating processes. Assume conditions ASTE (P), SM (P), and SE (P) hold for P = Pn for
each n. Then, the post-double-Lasso estimator ˇα, constructed in the previous section, obeys as
√n(ˇα −α0) ⇝N(0, 1),
i ]−1¯E[v2
i ]−1. Moreover, the result continues to apply if σ2
n is replaced by
n = [Enbv2
i ]−1En[bv2
i ]−1, for bζi := [yi −diˇα −x′
i ˇβ]{n/(n −bs −1)}1/2 and bvi := di −x′
i = 1, . . . , n where bβ ∈arg minβ{En[(di −x′
iβ)2] : βj = 0, ∀j /∈bI}.
A consequence of this result is the following corollary.
Corollary 1 (Uniformly Valid Conﬁdence Intervals). (i) Let Pn be the collection of all
data-generating processes P for which conditions ASTE(P), SM (P), and SE (P) hold for given
n. Let c(1 −ξ) = Φ−1(1 −ξ/2). Then as n →∞, uniformly in P ∈Pn
 α0 ∈[ˇα ± c(1 −ξ)bσn/√n]
(ii) Let P = ∩n⩾n0Pn be the collection of data-generating processes for which the conditions
above hold for all n ⩾n0 for some n0. Then as n →∞, uniformly in P ∈P
 α0 ∈[ˇα ± c(1 −ξ)bσn/√n]
By exploiting both equations (2.4) and (2.5) for model selection, the post-double-selection
method creates the necessary adaptivity that makes it robust to imperfect model selection.
Robustness of the post-double selection method is reﬂected in the fact that Theorem 1 permits
the data-generating process to change with n. Thus, the conclusions of the theorem are valid
for a wide variety of sequences of data-generating processes which in turn deﬁne the regions P
of uniform validity of the resulting conﬁdence sets. These regions appear to be substantial, as
we demonstrate via a sequence of theoretical and numerical examples in Section 5 and 6. In
contrast, the standard post-selection method based on (2.4) generates non-robust conﬁdence
intervals.
Comment 3.3. Our approach to uniformity analysis is most similar to that of Romano ,
Theorem 4. It proceeds under triangular array asymptotics, with the sequence of dgps obeying
certain constraints; then these results imply uniformity over sets of dgps that obey the constraints for all sample sizes. This approach is also similar to the classical central limit theorems
BELLONI CHERNOZHUKOV HANSEN
for sample means under triangular arrays, and does not require the dgps to be parametrically
(or otherwise tightly) speciﬁed, which then translates into uniformity of conﬁdence regions.
This approach is somewhat diﬀerent in spirit to the generic uniformity analysis suggested by
Andrews, Cheng, and Guggenberger .
Comment 3.4. Uniformity holds over a large class of approximately sparse models, which
cover conventional models used in series estimation of partially linear models as shown in
Section 5.
Of course, for every interesting class of models and any inference method, one
could ﬁnd an even bigger class of models where the uniformity does not apply. In particular,
our models do not cover models with many small coeﬃcients.
In the series case, a model
with many small coeﬃcients corresponds to a deviation from smoothness towards highly nonsmooth functions, namely functions generated as realized paths of an approximate white noise
process. The fact that our results do not cover such models motivates further research work
on inference procedures that have robustness properties to deviations from the given class of
models that are deemed important. In the simulations in Section 6, we consider incorporating
the ridge ﬁt along the other controls to be selected over using lasso to build extra robustness
against “many small coeﬃcients” deviations away from approximately sparse models.
3.3. Auxiliary Results on Model Selection via Lasso and Post-Lasso. The postdouble-selection estimator applies the least squares estimator to the union of variables selected
for equations (2.6) and (2.7) via feasible Lasso. Therefore, the model selection properties of
feasible Lasso as well as properties of least squares estimates for m and g based on the selected
model play an important role in the derivation of the main result. The purpose of this section
is to describe these properties. The proof of Theorem 1 relies on these properties.
Note that each of the regression models (2.6)-(2.7) obeys the following conditions.
Condition ASM. Let {Pn} be a sequence of data-generating processes. For each n, we
have data {(˜yi, ˜zi, ˜xi = P(˜zi)) : 1 ⩽i ⩽n} deﬁned on (Ω, A, Pn) consisting of i.n.i.d vectors
that obey the following approximately sparse regression model for each n:
˜yi = f(˜zi) + ǫi = ˜x′
iβ0 + ri + ǫi,
E[ǫi | ˜xi] = 0, ¯E[ǫ2
∥β0∥0 ⩽s, ¯E[r2
i ] ≲σ2s/n.
Let bT denote the model selected by the feasible Lasso estimator bβ:
bT = support(bβ) = {j ∈{1, . . . , p} : |bβj| > 0},
INFERENCE AFTER MODEL SELECTION
The Post-Lasso estimator eβ is is ordinary least squares applied to the data after removing the
regressors that were not selected by the feasible Lasso:
eβ ∈arg min
β∈Rp En[(˜yi −˜x′
βj = 0 for each j /∈bT.
The following regularity conditions are imposed to deal with non-Gaussian, heteroscedastic
Condition RF. In addition to ASTE, we have
(i) log3 p/n →0 and s log(p ∨n)/n →0,
(ii) ¯E[˜y2
i ] + max1⩽j⩽p{¯E[˜x2
i ] + ¯E[|˜x3
i |] + 1/¯E[˜x2
1⩽j⩽p{|(En −¯E)[˜x2
i ]| + |(En −¯E)[˜x2
i ]|} + max
1⩽i⩽n ∥˜xi∥2
s log(n ∨p)
The main auxiliary result that we use in proving the main result is as follows.
Lemma 1 (Model Selection Properties of Lasso and Properties of Post-Lasso). Let {Pn} be
a sequence of data-generating processes. Suppose that conditions ASM and RF hold, and that
Condition SE (Pn) holds for En[˜xi˜x′
i]. Consider a feasible Lasso estimator with penalty level
and loadings speciﬁed as in Section 3.3.
(i) Then the data-dependent model bT selected by a feasible Lasso estimator satisﬁes with
probability approaching 1:
bs = | bT| ≲s
β∈Rp: βj=0 ∀j̸∈bT
En[f(˜zi) −˜x′
s log(p ∨n)
(ii) The Post-Lasso estimator obeys
En[f(˜zi) −˜x′
i eβ]2 ≲P σ
s log(p ∨n)
∥eβ −β0∥≲P
iβ0}2] ≲P σ
s log(p ∨n)
Lemma 1 was derived in Belloni, Chen, Chernozhukov, and Hansen for Iterated Lasso
and by Belloni, Chernozhukov, and Wang for Square-root Lasso. These analyses build
on the rate analysis of infeasible Lasso by Bickel, Ritov, and Tsybakov and on sparsity
analysis and rate analysis of Post-Lasso by Belloni and Chernozhukov . Lemma 1 shows
that feasible Lasso methods select a model bT that provides a high-quality approximation to
the regression function f(˜zi); i.e. they ﬁnd a sparse model that can approximate the function
BELLONI CHERNOZHUKOV HANSEN
at the “near-oracle” rate
log(p ∨n).
If we knew the “best” approximating model
T = support(β0), we could achieve the “oracle” rate of
Note that Lasso methods
generally will not recover T perfectly. Moreover, no method can recover T perfectly in general,
except under the restrictive condition that all non-zero coeﬃcients in β0 are bounded away
from zero by a factor that exceeds estimation error. We do not require this condition to hold
in our results. All that we need is that the selected model bT can approximate the regression
function well and that the size of the selected model, bs = | bT|, is of the same stochastic order
as s = |T|. This condition holds in many cases in which some non-zero coeﬃcients are close to
The lemma above also shows that feasible Post-Lasso achieves the same near-oracle rate
as feasible Lasso. The coincidence in rates occurs despite the fact that feasible Lasso will in
general fail to correctly select the best-approximating model T as a subset of the variables
selected; that is, T ̸⊆bT. The intuition for this result is that any components of T that feasible
Lasso misses are unlikely to be important; otherwise, (3.24) would be impossible. This result
was ﬁrst derived in the context of median regression by Belloni and Chernozhukov and
extended to least squares in reference cited above.
4. Generalization: Inference after Double Selection by a Generic Selection
The conditions provided so far are simply a set suﬃcient conditions that are tied to the use
of Lasso as the model selector. The purpose of this section is to prove that the main results
apply to any other model selection method that is able to select a sparse model with good
approximation properties. As in the case of Lasso, we allow for imperfect model selection.
Next we state a high-level condition that summarizes a suﬃcient condition on the performance
of a model selection method that allows the post-double selection estimator to attain good
inferential properties.
Condition HLMS (P). A model selector provides possibly data-dependent sets bI1 ∪bI2 ⊆
bI ⊂{1, ..., p} of covariate names such that, with probability 1 −∆n, |bI| ⩽Cs and
β:βj=0,j̸∈bI1
En[(m(zi) −x′
iβ)2] ⩽δnn−1/4 and
β:βj=0,j̸∈bI2
En[(g(zi) −x′
iβ)2] ⩽δnn−1/4.
Condition HLMS requires that with high probability the selected models are sparse and
generates a good approximation for the functions g and m. Examples of methods producing
such models include the Dantzig selector , feasible Dantzig selector
 , Bridge estimator , SCAD
INFERENCE AFTER MODEL SELECTION
penalized least squares , and thresholded Lasso , to name a few. We emphasize that, similarly to the previous arguments, we allow for
imperfect model selection.
The following result establishes the inferential properties of a generic post-double-selection
estimator.
Theorem 2 (Estimation and Inference on Treatment Eﬀects under High-Level Model Selection). Let {Pn} be a sequence of data-generating processes and the model selection device be
such that conditions ASTE (P), SM (P), SE (P), and HLSM(P) hold for P = Pn for each n.
Then the generic post-double-selection estimator ˇα based on bI, as deﬁned in (2.8), obeys
i ]−1¯E[v2
i ]−1)−1/2√n(ˇα −α0) ⇝N(0, 1).
Moreover, the result continues to apply if ¯E[v2
i ] and ¯E[v2
i ] are replaced by En[bv2
i ] and En[bv2
for bζi := [yi −diˇα −x′
i ˇβ]{n/(n −bs −1)}1/2 and bvi := di −x′
i bβ, i = 1, . . . , n where bβ ∈
arg minβ{En[(di −x′
iβ)2] : βj = 0, ∀j /∈bI}.
Theorem 2 can also be used to establish uniformly valid conﬁdence intervals as shown is the
following corollary.
Corollary 2 (Uniformly Valid Conﬁdence Intervals). (i) Let Pn be the collection of all
data-generating processes P for which conditions ASTE(P), SM (P), SE (P), and HLSM (P)
hold for given n. Let c(1 −ξ) = Φ−1(1 −ξ/2). Then as n →∞, uniformly in P ∈Pn
 α0 ∈[ˇα ± c(1 −ξ)bσn/√n]
(ii) Let P = ∩n⩾n0Pn be the collection of data-generating processes for which the conditions
above hold for all n ⩾n0 for some n0. Then as n →∞, uniformly in P ∈P
 α0 ∈[ˇα ± c(1 −ξ)bσn/√n]
5. Theoretical Examples
The purpose of this section is to give a sequence of examples – progressing from simple
to somewhat involved – that highlight the range of the applicability and robustness of the
proposed method. In these examples, we specify primitive conditions which cover a broad
range of applications including nonparametric models and high-dimensional parametric models.
We emphasize that our main regularity conditions cover even more general models which
combine various features of these examples such as models with both nonparametric and highdimensional parametric components.
BELLONI CHERNOZHUKOV HANSEN
In all examples, the model is
yi = diα0 + g(zi) + ζi,
E[ζi | zi, vi] = 0,
di = m(zi) + vi,
E[vi | zi] = 0,
however, the structure for g and m will vary across examples, and so will the assumptions on
the error terms ζi and vi.
We start out with a simple example, in which the dimension p of the regressors is ﬁxed.
In practical terms this example approximates cases with p small compared to n. This simple example is important since standard post-single-selection methods fail even in this simple
case. Speciﬁcally, they produce conﬁdence intervals that are not valid uniformly in the underlying data-generating process; see Leeb and P¨otscher . In contrast, the post-doubleselection method produces conﬁdence intervals that are valid uniformly in the underlying datagenerating process.
Example 1. (Parametric Model with Fixed p.) Consider (Ω, A, P) as the probability space,
on which we have (yi, zi, di) as i.i.d. vectors for i = 1, ..., n obeying the model (5.26) with
j=1 βg0jzij,
j=1 βm0jzij.
For estimation we use xi = (zij, j = 1, ..., p)′. We assume that there are some absolute constants
0 < b < B < ∞, qx ⩾q > 4, with 4/qx + 4/q < 1, such that
E[∥xi∥qx] ⩽B,
∥α0∥+ ∥βg0∥+ ∥βm0∥⩽B,
b ⩽λmin(E[xix′
i | xi, vi],
i | | xi, vi] ⩽B,
i | | xi] ⩽B.
Let P be the collection of all regression models P that obey the conditions set forth above
for all n for the given constants (p, b, B, qx, q).
Then, as established in Appendix F, any
P ∈P obeys Conditions ASTE (P) with s = p, SE (P), and SM (P) for all n ⩾n0, with the
constants n0 and (κ′, κ′′, c, C) and sequences ∆n and δn in those conditions depending only on
(p, b, B, qx, q). Therefore, the conclusions of Theorem 1 hold for any sequence Pn ∈P, and the
conclusions of Corollary 1 on the uniform validity of conﬁdence intervals apply uniformly in
The next examples are more substantial and include inﬁnite-dimensional models which we
approximate with linear functional forms with potentially very many regressors, p ≫n. The
key to estimation in these models is a smoothness condition which requires regression coeﬃcients to decay at some rates. In series estimation, this condition is often directly connected
to smoothness of the regression function.
INFERENCE AFTER MODEL SELECTION
Let a and A be positive constants. We shall say that a sequence of coeﬃcients
θ = {θj, j = 1, 2, ...}
is a-smooth with constant A if
|θj| ⩽Aj−a, j = 1, 2, ...,
which will be denoted as θ ∈Sa
A. We shall say that a sequence of coeﬃcients θ = {θj, j =
1, 2, ...} is a-smooth with constant A after p-rearrangement if
|θ(j)| ⩽Aj−a, j = 1, 2, ..., p,
|θj| ⩽Aj−a, j = p + 1, p + 2, ...,
which will be denoted as θ ∈Sa
A(p), where {|θ(j)|, j = 1, ..., p} denotes the decreasing rearrangement of the numbers {|θj|, j = 1, ..., p}. Since Sa
A(p), the second kind of smoothness is
strictly more general than the ﬁrst kind.
Here we use the term “smoothness” motivated by Fourier series analysis where smoothness of
functions often translates into smoothness of the Fourier coeﬃcients in the sense that is stated
above; see, e.g., Kerkyacharian and Picard . For example, if a function h : d 7→R
possesses r > 0 continuous derivatives uniformly bounded by a constant M and the terms Pj are
compactly supported Daubechies wavelets, then h can be represented as h(z) = P∞
j=1 Pj(z)θhj,
with |θhj| ⩽Aj−r/d−1/2 for some constant A; see Kerkyacharian and Picard . We also
note that the second kind of smoothness is considerably more general than the ﬁrst since it
allows relatively large coeﬃcients to appear anywhere in the series of the ﬁrst p coeﬃcients. In
contrast, the ﬁrst kind of smoothness only allows relatively large coeﬃcients among the early
terms in the series. Lasso-type methods are speciﬁcally designed to deal with the generalized
smoothness of the second kind and perform equally well under both kinds of smoothness. In
the context of series applications, smoothness of the second kind allows one to approximate
functions that exhibit oscillatory phenomena or spikes, which are associated with “high order”
series terms. An example of this is the wage function example given in Belloni, Chernozhukov,
and Hansen .
Before we proceed to other examples we discuss a way to generate sparse approximations
in inﬁnite-dimensional examples. Consider, for example, a function h that can be represented
a.s. as h(zi) = P∞
j=1 θhjPj(zi) with coeﬃcients θh ∈Sa
A(p). In this case we can construct
sparse approximations by simply thresholding to zero all coeﬃcients smaller than 1/√n and
with indices j ⩾p. This generates a sparsity index s ⩽A
2a . The non-zero coeﬃcient could
be further reoptimized by using the least squares projection. More formally, given a sparsity
index s > 0, a target function h(zi), and terms xi = (Pj(zi) : j = 1, . . . , p)′ ∈Rp, we let
∥β∥0⩽s E[(h(zi) −x′
BELLONI CHERNOZHUKOV HANSEN
and deﬁne x′
iβh0 as the best s-sparse approximation to h(zi).
Example 2. (Gaussian Model with Very Large p.) Consider (Ω, A, P) as the probability
space on which we have (yi, zi, di) as i.i.d. vectors for i = 1, ..., n obeying the model (5.26)
j=1 θgjzij,
j=1 θmjzij.
Assume that the inﬁnite dimensional vector wi = (z′
i, ζi, vi)′ is jointly Gaussian with minimal
and maximal eigenvalues of the matrix (operator) E[wiw′
i] bounded below by an absolute
constant κ > 0 and above by an absolute constant κ < ∞.
The main assumption that guarantees approximate sparsity is the smoothness condition on
the coeﬃcients. Let a > 1 and 0 < A < ∞be some absolute constants. We require that the
coeﬃcients of the expansions in (5.30) are a-smooth with constant A after p-rearrangement,
θm = (θmj, j = 1, 2, ...) ∈Sa
θg = (θgj, j = 1, 2, ...) ∈Sa
For estimation purposes we shall use xi = (zij, j = 1, ..., p)′, and assume that ∥α0∥⩽B and
p = pn obeys
n[(1−a)/a]+χ log2(p ∨n) ⩽¯δn,
log3 p/n ⩽¯δn,
for some absolute sequence ¯δn ց 0 and absolute constants B and χ > 0.
Let Pn be the collection of all dgp P that obey the conditions set forth in this example for
a given n and for the given constants (κ, κ, a, A, B, χ) and sequences p = pn and ¯δn. Then, as
established in Appendix F, any P ∈Pn obeys Conditions ASTE (P) with s = A1/an
(P), and SM (P) for all n ⩾n0, with constants n0 and (κ′, κ′′, c, C) and sequences ∆n and δn in
those conditions depending only on (κ, ¯κ, a, A, B, χ), p, and ¯δn. Therefore, the conclusions of
Theorem 1 hold for any sequence Pn ∈Pn, and the conclusions of Corollary 1 on the uniform
validity of conﬁdence intervals apply uniformly for any P ∈Pn. In particular, these conclusions
apply uniformly in P ∈P = ∩n⩾n0Pn.
Example 3. (Series Model with Very Large p.) Consider (Ω, A, P) as the probability space,
on which we have (yi, zi, di) as i.i.d. vectors for i = 1, ..., n obeying the model:
j=1 θgjPj(zi),
j=1 θmjPj(zi),
where zi has support d with density bounded from below by constant f > 0 and above by
constant ¯f, and {Pj, j = 1, 2, ..} is an orthonormal basis on L2 d with bounded elements,
i.e. maxz∈ d |Pj(z)| ⩽B for all j = 1, 2, .... Here all constants are taken to be absolute.
Examples of such orthonormal bases include canonical trigonometric bases.
INFERENCE AFTER MODEL SELECTION
Let a > 1 and 0 < A < ∞be some absolute constants. We require that the coeﬃcients of
the expansions in (5.31) are a-smooth with constant A after p-rearrangement, namely
θm = (θmj, j = 1, 2, ...) ∈Sa
θg = (θgj, j = 1, 2, ...) ∈Sa
For estimation purposes we shall use xi = (Pj(zi), j = 1, ..., p)′, and assume that p = pn
n(1−a)/a log2(p ∨n) ⩽¯δn,
2a ⩽p¯δn and
log3 p/n ⩽¯δn,
for some sequence of absolute constants ¯δn ց 0. We assume that there are some absolute
constants b > 0, B < ∞, q > 4, with (1 −a)/a + 4/q < 0, such that
∥α0∥⩽B, b ⩽E[ζ2
i | xi, vi],
i | | xi, vi] ⩽B,
i | | xi] ⩽B.
Let Pn be the collection of all regression models P that obey the conditions set forth above
for a given n.
Then, as established in Appendix F, any P ∈Pn obeys Conditions ASTE
(P) with s = A1/an
2a , SE (P), and SM (P) for all n ⩾n0, with absolute constants in those
conditions depending only on (f, ¯f, a, A, b, B, q) and ¯δn. Therefore, the conclusions of Theorem
1 hold for any sequence Pn ∈Pn, and the conclusions of Corollary 1 on the uniform validity
of conﬁdence intervals apply uniformly for any P ∈Pn. In particular, as a special case, the
same conclusion applies uniformly in P ∈P = ∩n⩾n0Pn.
6. Monte-Carlo Examples
In this section, we examine the ﬁnite-sample properties of the post- double-selection method
through a series of simulation exercises and compare its performance to that the standard postsingle-selection method.
All of the simulation results are based on the structural model
iθg + σy(di, xi)ζi,
ζi ∼N(0, 1)
where p = dim(xi) = 200, the covariates xi ∼N(0, Σ) with Σkj = (0.5)|j−k|, α0 = .5, and the
sample size n is set to 100. In each design, we generate
iθm + σd(xi)vi,
vi ∼N(0, 1)
with E[ζivi] = 0. Inference results for all designs are based on conventional t-tests with standard
errors calculated using the heteroscedasticity consistent jackknife variance estimator discussed
in MacKinnon and White . Another option would be to use the standard error estimator
recently proposed in Cattaneo, Jansson, and Newey .
BELLONI CHERNOZHUKOV HANSEN
We report results from three diﬀerent dgp’s. In the ﬁrst two dgp’s, we set θg,j = cyβ0,j and
θm,j = cdβ0,j with β0,j = (1/j)2 for j = 1, ..., 200. The ﬁrst dgp, which we label “Design 1,” uses
homoscedastic innovations with σy = σd = 1. The second dgp, “Design 2,” is heteroscedastic
with σd,i =
iβ0)2 and σy,i =
(1+α0di+x′
En(1+α0di+x′
iβ0)2 . The constants cy and cd are chosen
to generate desired population values for the reduced form R2’s, i.e. the R2’s for equations
(2.6) and (2.7). For each equation, we choose cy and cd to generate R2 = 0, .2, .4, .6, and
.8. In the heteroscedastic design, we choose cy and cd based on R2 as if (6.33) and (6.34)
held with vi and ζi homoscedastic and label the results by R2 as in Design 1. In the third
design (“Design 3”), we use a combination of deterministic and random coeﬃcients. For the
deterministic coeﬃcients, we set θg,j = cy(1/j)2 for j ≤5 and θm,j = cd(1/j)2 for j ≤5. We
then generate the remaining coeﬃcients as iid draws from (θg,j, θm,j)′ ∼N(02×1, (1/p)I2). For
each equation, we choose cy and cd to generate R2 = 0, .2, .4, .6, and .8 in the case that all
of the random coeﬃcients were exactly equal to 0 and label the results by R2 as in Design 1.
We draw new x’s, ζ’s, and v’s at every simulation replication, and we also generate new θ’s at
every simulation replication in Design 3.
We consider Designs 1 and 2 to be baseline designs.
These designs do not have exact
sparse representations but have coeﬃcients that decay quickly so that approximately sparse
representations are available. Design 3 is meant to introduce a modest deviation from the
approximately sparse model towards a model with many small, uncorrelated coeﬃcients. Using
this we shall document that our proposed procedure still performs reasonably well, although
it could be improved by incorporation of a ridge ﬁt as one of regressors over which selection
occurs. In a working paper version of this paper Belloni, Chernozhukov, and Hansen ,
we present results for 26 additional designs. The results presented in this section are suﬃcient
to illustrate the general patterns from the larger set of results.13
We report results for ﬁve diﬀerent procedures. Two of the procedures are infeasible benchmarks: Oracle and Double-Selection Oracle estimators, which use of knowledge of the true
coeﬃcient structures θg and θm and are thus unavailable in practice. The Oracle estimator is
the ordinary least squares of yi −x′
iθg on di, and the Double-Selection Oracle is the ordinary
least squares of y −x′
iθg on di −x′
iθm. The other procedures we consider are feasible. In all
of them, we rely on Lasso and set λ according to the algorithm outlined in Appendix A with
1 −γ = .95. One procedure is the standard post-single selection estimator – the Post-Lasso
13 In particular, the post-double-Lasso performed very well across all simulations designs where approximate sparsity provides a reasonable description of the dgp. Unsurprisingly, the performance deteriorates as
one deviates from the smooth/approximately sparse case. However, in no design was the post-double-Lasso
outperformed by other feasible procedures. In extensive initial simulations, we also found that Square-Root
Lasso and Iterated Lasso performed very similarly and thus only report Lasso results.
INFERENCE AFTER MODEL SELECTION
– which applies Lasso to equation (6.33) without penalizing α, the coeﬃcient on d, to select
additional control variables from among x. Estimates of α0 are then obtained by OLS regression of y on d and the set of additional controls selected in the Lasso step and inference using
the Post-Lasso estimator proceeds using conventional heteroscedasticity robust OLS inference
from this regression.
Post-Double-Selection or Post-Double-Lasso is the feasible procedure
advocated in this paper. We run Lasso of y on x to select a set of predictors for y and run
Lasso of d on x to select a set of predictors for d.
α0 is then estimated by running OLS
regression of y on d and the union of the sets of regressors selected in the two Lasso runs,
and inference is simply the usual heteroscedasticity robust OLS inference from this regression.
Post-Double-Selection + Ridge is an ad hoc variant of Post-Double-Selection in which we add
the ridge ﬁt from equation (6.34) as an additional potential regressor that may be selected by
Lasso. The ridge ﬁt is obtained with a single ridge penalty parameter that is chosen using
10-fold cross-validation. This procedure is motivated by a desire to add further robustness in
the case that many small coeﬃcients are suspected. Further exploration of procedures that
perform well, both theoretically and in simulations, in the presence of many small coeﬃcients
is an interesting avenue for additional research.
We start by summarizing results in Table 1 for (R2
d) = (0, .2), (0, .8), (.8, .2), and (.8, .8)
y is the population R2 from regressing y on x (Structure R2) and R2
d is the population
R2 from regressing d on x (First Stage R2). We report root-mean-square-error (RMSE) for
estimating α0 and size of 5% level tests (Rej.
As should be the case, the Oracle
and Double-Selection Oracle, which are reported to provide the performance of an infeasible
benchmark, perform well relative to the feasible procedures across the three designs.
do see that the feasible Post-Double-Selection procedures perform similarly to the Double-
Selection Oracle without relying on ex ante knowledge of the coeﬃcients that go in to the
control functions, θg and θm. On the other hand, the Post-Lasso procedure generally does
not perform as well as Post-Double-Selection and is very sensitive to the value of R2
Post-Lasso performs adequately when R2
d is small, its performance deteriorates quickly as
d increases. This lack of robustness of traditional variable selection methods such as Lasso
which were designed with forecasting, not inference about treatment eﬀects, in mind is the
chief motivation for our advocating the Post-Double-Selection procedure when trying to infer
structural or treatment parameters.
We provide further details about the performance of the feasible estimators in Figures 1,
2, and 3 which plot size of 5% level tests, bias, and standard deviation for the Post-Lasso,
Double-Selection (DS), and Double-Selection Oracle (DS Oracle) estimators of the treatment
eﬀect across the full set of R2 values considered. Figure 1, 2, and 3 respectively report the
results from Design 1, 2, and 3. The ﬁgures are plotted with the same scale to aid comparability
BELLONI CHERNOZHUKOV HANSEN
and for readability rejection frequencies for Post-Lasso were censored at .5. Perhaps the most
striking feature of the ﬁgures is the poor performance of the Post-Lasso estimator. The Post-
Lasso estimator performs poorly in terms of size of tests across many diﬀerent R2 combinations
and can have an order of magnitude more bias than the corresponding Post-Double-Selection
estimator. The behavior of Post-Lasso is quite non-uniform across R2 combinations, and Post-
Lasso does not reliably control size distortions or bias except in the case where the controls
are uncorrelated with the treatment (where First-Stage R2 equals 0) and thus ignorable. In
contrast, the Post-Double-Selection estimator performs relatively well across the full range
of R2 combinations considered.
The Post-Double-Selection estimator’s performance is also
quite similar to that of the infeasible Double-Selection Oracle across the majority of R2 values
considered.
Comparing across Figures 1 and 2, we see that size distortions for both the
Post-Double-Selection estimator and the Double-Selection Oracle are somewhat larger in the
presence of heteroscedasticity but that the basic patterns are more-or-less the same across the
two ﬁgures. Looking at Figure 3, we also see that the addition of small independent random
coeﬃcients results in somewhat larger size distortions for the Post-Double-Selection estimator
than in the other homoscedastic design, Design 1, though the procedure still performs relatively
In the ﬁnal ﬁgure, Figure 4, we compare the performance of the Post-Double-Selection
procedure to the ad hoc Post-Double-Selection procedure which selects among the original
set of variables augmented with the ridge ﬁt obtained from equation (6.34). We see that the
addition of this variable does add robustness relative to Post-Double-Selection using only the
raw controls in the sense of producing tests that tend to have size closer to the nominal level.
This additional robustness is a good feature, though it comes at the cost of increased RMSE
which is especially prominent for small values of the ﬁrst-stage R2.
The simulation results are favorable to the Post-Double-Selection estimator. In the simulations, we see that the Post-Double-Selection procedure provides an estimator of a treatment
eﬀect in the presence of a large number of potential confounding variables that performs similarly to the infeasible estimator that knows the values of the coeﬃcients on all of the confounding variables. Overall, the simulation evidence supports our theoretical results and suggests
that the proposed Post-Double-Selection procedure can be a useful tool to researchers doing
structural estimation in the presence of many potential confounding variables. It also shows,
as a contrast, that the standard Post-Single-Selection procedure provides poor inference and
therefore can not be a reliable tool to these researchers.
INFERENCE AFTER MODEL SELECTION
7. Empirical Example: Estimating the Effect of Abortion on Crime
In the preceding sections, we have provided results demonstrating how variable selection
methods, focusing on the case of Lasso-based methods, can be used to estimate treatment effects in models in which we believe the variable of interest is exogenous conditional on observables. We further illustrate the use of these methods in this section by reexamining Donohue
III and Levitt’s study of the impact of abortion on crime rates. In the following, we
brieﬂy review Donohue III and Levitt and then present estimates obtained using the
methods developed in this paper.
Donohue III and Levitt discuss two key arguments for a causal channel relating
abortion to crime. The ﬁrst is simply that more abortion among a cohort results in an otherwise
smaller cohort and so crime 15 to 25 years later, when this cohort is in the period when its
members are most at risk for committing crimes, will be otherwise lower given the smaller
cohort size. The second argument is that abortion gives women more control over the timing
of their fertility allowing them to more easily assure that childbirth occurs at a time when a
more favorable environment is available during a child’s life. For example, access to abortion
may make it easier to ensure that a child is born at a time when the family environment is
stable, the mother is more well-educated, or household income is stable. This second channel
would mean that more access to abortion could lead to lower crime rates even if fertility rates
remained constant.
The basic problem in estimating the causal impact of abortion on crime is that state-level
abortion rates are not randomly assigned, and it seems likely that there will be factors that
are associated to both abortion rates and crime rates. It is clear that any association between
the current abortion rate and the current crime rate is likely to be spurious. However, even
if one looks at say the relationship between the abortion rate 18 years in the past and the
crime rate among current 18 year olds, the lack of random assignment makes establishing a
causal link diﬃcult without adequate controls. An obvious confounding factor is the existence
of persistent state-to-state diﬀerences in policies, attitudes, and demographics that are likely
related to the overall state level abortion and crime rates.
It is also important to control
ﬂexibly for aggregate trends.
For example, it could be the case that national crime rates
were falling over this period while national abortion rates were rising but that these trends
were driven by completely diﬀerent factors. Without controlling for these trends, one would
mistakenly associate the reduction in crime to the increase in abortion. In addition to these
overall diﬀerences across states and times, there are other time varying characteristics such as
state-level income, policing, or drug-use to name a few that could be associated with current
crime and past abortion.
BELLONI CHERNOZHUKOV HANSEN
To address these confounds, Donohue III and Levitt estimate a model for state-level
crime rates running from 1985 to 1997 in which they condition on a number of these factors.
Their basic speciﬁcation is
ycit = αacit + w′
itβ + δi + γt + εit
where i indexes states, t indexes times, c ∈{violent, property, murder} indexes type of crime,
δi are state-speciﬁc eﬀects that control for any time-invariant state-speciﬁc characteristics,
γt are time-speciﬁc eﬀects that control ﬂexibly for any aggregate trends, wit are a set of
control variables to control for time-varying confounding state-level factors, acit is a measure
of the abortion rate relevant for type of crime c,14 and ycit is the crime-rate for crime type c.
Donohue III and Levitt use the log of lagged prisoners per capita, the log of lagged police
per capita, the unemployment rate, per-capita income, the poverty rate, AFDC generosity at
time t−15, a dummy for concealed weapons law, and beer consumption per capita for wit, the
set of time-varying state-speciﬁc controls. Tables IV and V in Donohue III and Levitt 
present baseline estimation results based on (7.35) as well as results from diﬀerent models
which vary the sample and set of controls to show that the baseline estimates are robust to
small deviations from (7.35). We refer the reader to the original paper for additional details,
data deﬁnitions, and institutional background.
For our analysis, we take the argument that the abortion rates deﬁned above may be taken as
exogenous relative to crime rates once observables have been conditioned on from Donohue III
and Levitt as given. Given the seemingly obvious importance of controlling for state
and time eﬀects, we account for these in all models we estimate. We choose to eliminate the
state eﬀects via diﬀerencing rather than including a full set of state dummies but include a full
set of time dummies in every model. Thus, we will estimate models of the form
ycit −ycit−1 = α(acit −acit−1) + z′
itκ + γt + ηit.
We use the same state-level data as Donohue III and Levitt but delete Alaska, Hawaii,
and Washington, D.C. which gives a sample with 48 cross-sectional observations and 12 time
series observations for a total of 576 observations. With these deletions, our baseline estimates
using the same controls as in (7.35) are quite similar to those reported in Donohue III and
Levitt . Baseline estimates from Table IV of Donohue III and Levitt and our
14This variable is constructed as weighted average of abortion rates where weights are determined by the
fraction of the type of crime committed by various age groups.
For example, if 60% of violent crime were
committed by 18 year olds and 40% were committed by 19 year olds in state i, the abortion rate for violent
crime at time t in state i would be constructed as .6 times the abortion rate in state i at time t −18 plus .4
times the abortion rate in state i at time t −19. See Donohue III and Levitt for further detail and exact
construction methods.
INFERENCE AFTER MODEL SELECTION
baseline estimates based on the diﬀerenced version of (7.35) are given in the ﬁrst and second
row of Table 2 respectively.
Our main point of departure from Donohue III and Levitt is that we allow for a much
richer set zit than allowed for in wit in model (7.35). Our zit includes higher-order terms and
interactions of the control variables deﬁned above. In addition, we put initial conditions and
initial diﬀerences of wit and ait into our vector of controls zit. This addition allows for the
possibility that there may be some feature of a state that is associated both with its growth
rate in abortion and its growth rate in crime. For example, having an initially high-levels of
abortion could be associated with having high-growth rates in abortion and low growth rates
in crime. Failure to control for this factor could then lead to misattributing the eﬀect of this
initial factor, perhaps driven by policy or state-level demographics, to the eﬀect of abortion.
Finally, we allow for more general trends by allowing for an aggregate quadratic trend in zit
as well as interactions of this quadratic trend with control variables. This gives us a set of 251
control variables to select among in addition to the 12 time eﬀects that we include in every
Note that interpreting estimates of the eﬀect of abortion from model (7.35) as causal relies
on the belief that there are no higher-order terms of the control variables, no interaction terms,
and no additional excluded variables that are associated both to crime rates and the associated
abortion rate. Thus, controlling for a large set of variables as described above is desirable from
the standpoint of making this belief more plausible. At the same time, naively controlling
lessens our ability to identify the eﬀect of interest and thus tends to make estimates far less
precise. The eﬀect of estimating the abortion eﬀect conditioning on the full set of 251 potential
controls described above is given in the third row of Table 2. As expected, all coeﬃcients are
estimated very imprecisely. Of course, very few researchers would consider using 251 controls
with only 576 observations due to exactly this issue.
We are faced with a tradeoﬀbetween controlling for very few variables which may leave
us wondering whether we have included suﬃcient controls for the exogeneity of the treatment
and controlling for so many variables that we are essentially mechanically unable to learn
about the eﬀect of the treatment.
The variable selection methods developed in this paper
oﬀer one resolution to this tension. The assumed sparse structure maintains that there is a
small enough set of variables that one could potentially learn about the treatment but adds
substantial ﬂexibility to the usual case where a researcher considers only a few control variables
by allowing this set to be found by the data from among a large set of controls. Thus, the
15The exact identities of the 251 potential controls is available upon request.
It consists of linear and
quadratic terms of each continuous variable in wit, interactions of every variable in wit, initial levels and initial
diﬀerences of wit and ait, and interactions of these variables with a quadratic trend.
BELLONI CHERNOZHUKOV HANSEN
approach should complement the usual careful speciﬁcation analysis by providing a researcher
an eﬃcient, data-driven way to search for a small set of inﬂuential confounds from among a
sensibly chosen broad set of potential confounding variables.
In the abortion example, we use the post-double-selection estimator deﬁned in Section 2.2
for each of our dependent variables. For violent crime, ten variables are selected in the abortion
equation,16 and one is selected in the crime equation.17 For property crime, eight variables are
selected in the abortion equation,18 and six are selected in the crime equation.19 For murder,
eight variables are selected in the abortion equation,20 and none were selected in the crime
Estimates of the causal eﬀect of abortion on crime obtained by searching for confounding
factors among our set of 251 potential controls are given in the fourth row of Table 2. Each of
these estimates is obtained from the least squares regression of the crime rate on the abortion
rate and the 11, 14, and eight controls selected by the double-post-Lasso procedure for violent
crime, property crime, and murder respectively. The estimates for the eﬀect of abortion on
violent crime and the eﬀect of abortion on murder are quite imprecise, producing 95% con-
ﬁdence intervals that encompass large positive and negative values. The estimated eﬀect for
property crime is roughly in line with the previous estimates though it is no longer signiﬁcant
at the 5% level but is signiﬁcant at the 10% level. Note that the double-post-Lasso produces
models that are not of vastly diﬀerent size than the “intuitive” model (7.35). As a ﬁnal check,
we also report results that include all of the original variables from (7.35) in the amelioration
set in the ﬁfth row of the table. These results show that the conclusions made from using
only the variable selection procedure do not qualitatively change when the variables used in
the original Donohue III and Levitt are added to the equation. For a quick benchmark
16The selected variables are AFDC generosity squared, beer consumption squared, the initial poverty change,
initial income, initial income squared, the initial change in prisoners per capita squared interacted with the trend,
initial income interacted with the trend, the initial change in the abortion rate, the initial change in the abortion
rate interacted with the trend, and the initial level of the abortion rate.
17The initial level of the abortion rate interacted with time is selected.
18The selected variables are income, the initial poverty change, the initial change in prisoners per capita
squared, the initial level of prisoners per capita, initial income, the initial change in the abortion rate, the initial
change in the abortion rate interacted with the trend, and the initial level of the abortion rate.
19The six variables are the initial level of AFDF generosity, the initial level of income interacted with the
trend and the trend squared, the initial level of income squared interacted with the trend and the trend squared,
and the initial level of the abortion rate interacted with the trend.
20The selected variables are AFDC generosity, beer consumption squared, the change in beer consumption
squared, the change in beer consumption squared times the trend and the trend squared, initial income times
the trend, the initial change in the abortion rate interacted with the trend, and the initial level of the abortion
INFERENCE AFTER MODEL SELECTION
relative to the simulation examples, we note that the R2 obtained by regressing the crime rate
on the selected variables are .0395, .1185, and .0044 for violent crime, property crime, and the
murder rate respectively and that the R2’s from regressing the abortion rate on the selected
variables are .9447, .9013, and .9144 for violent crime, property crime, and the murder rate
respectively. These values correspond to regions of the R2 space considered in the simulation
where the double selection procedure substantially outperformed simple Lasso procedures.
It is very interesting that one would draw qualitatively diﬀerent conclusions from the estimates obtained using formal variable selection than from the estimates obtained using a small
set of intuitively selected controls. Looking at the set of selected control variables, we see that
initial conditions and interactions with trends are selected across all dependent variables. The
selection of this set of variables suggests that there are initial factors which are associated with
the change in the abortion rate. We also see that we cannot precisely determine the eﬀect of
the abortion rate on crime rates once one accounts for initial conditions. Of course, this does
not mean that the eﬀects of the abortion rate provided in the ﬁrst two rows of Table 2 are
not representative of the true causal eﬀects. It does, however, imply that this conclusion is
strongly predicated on the belief that there are not other unobserved state-level factors that
are correlated to both initial values of the controls and abortion rates, abortion rate changes,
and crime rate changes. Interestingly, a similar conclusion is given in Foote and Goetz 
based on an intuitive argument.
We believe that the example in this section illustrates how one may use modern variable
selection techniques to complement causal analysis in economics. In the abortion example,
we are able to search among a large set of controls and transformations of variables when
trying to estimate the eﬀect of abortion on crime. Considering a large set of controls makes
the underlying assumption of exogeneity of the abortion rate conditional on observables more
plausible, while the methods we develop allow us to produce an end-model which is of manageable dimension. Interestingly, we see that one would draw quite diﬀerent conclusions from the
estimates obtained using formal variable selection. Looking at the variables selected, we can
also see that this change in interpretation is being driven by the variable selection method’s
selecting diﬀerent variables, speciﬁcally initial values of the abortion rate and controls, than
are usually considered. Thus, it appears that the usual interpretation hinges on the prior belief
that initial values should be excluded from the structural equation.
8. Conclusion
In this paper, we consider estimation of treatment eﬀects or structural parameters in an
environment where the treatment is believed to be exogenous conditional on observables. We
BELLONI CHERNOZHUKOV HANSEN
do not impose the conventional assumption that the identities of the relevant conditioning variables and the functional form with which they enter the model are known. Rather, we assume
that the researcher believes there is a relatively small number of important factors whose identities are unknown within a much larger known set of potential variables and transformations.
This sparsity assumption allows the researcher to estimate the desired treatment eﬀect and
infer a set of important variables upon which one needs to condition by using modern variable
selection techniques without ex ante knowledge of which are the important conditioning variables. Since naive application of variable selection methods in this context may result in very
poor properties for inferring the treatment eﬀect of interest, we propose a “double-selection”
estimator of the treatment eﬀect, provide a formal demonstration of its properties for estimating the treatment eﬀect, and provide its approximate distribution under technical regularity
conditions and the assumed sparsity in the model.
In addition to the theoretical development, we illustrate the potential usefulness of our
proposal through a number of simulation studies and an empirical example. In Monte Carlo
simulations, our procedure outperforms simple variable selection strategies for estimating the
treatment eﬀect across the designs considered and does relatively well compared to an infeasible
estimator that uses the identities of the relevant conditioning variables. We then apply our
estimator to attempt to estimate the causal impact of abortion on crime following Donohue III
and Levitt . We ﬁnd that our procedure selects a small number of conditioning variables.
After conditioning on these selected variables, one would draw qualitatively diﬀerent inference
about the eﬀect of abortion on crime than would be drawn if one assumed that the correct
set of conditioning variables was known and the same as those variables used in Donohue III
and Levitt . Taken together, the empirical and simulation examples demonstrate that
the proposed method may provide a useful complement to other sorts of speciﬁcation analysis
done in applied research.
Appendix A. Iterated Estimation of Penalty Loadings
In the case of Lasso under heteroscedasticity, we must specify for the penalty loadings (2.13).
Here we state algorithms for estimating these loadings.
Let I0 be an initial set of regressors with bounded number of elements, including for example intercept. Let ¯β(I0) be the least squares estimator of the coeﬃcients on the covariates
associated with I0, and deﬁne blj0 :=
i ¯β(I0))2].
An algorithm for estimating the penalty loadings using Post-Lasso is as follows:
INFERENCE AFTER MODEL SELECTION
Algorithm 1 (Estimation of Lasso loadings using Post-Lasso iterations). Set blj,0 := bljI0,
j = 1, . . . , p. Set k = 0, and specify a small constant ν ⩾0 as a tolerance level and a constant
K > 1 as an upper bound on the number of iterations. (1) Compute the Post-Lasso estimator eβ
based on the loadings blj,k. (2) For bs = ∥eβ∥0 = | bT| set lj,k+1 :=
n/(n −bs).
(3) If max1⩽j⩽p |blj,k −blj,k+1| ⩽ν or k > K, set the loadings to blj,k+1, j = 1, . . . , p and stop;
otherwise, set k ←k + 1 and go to (1).
A similar algorithm can be deﬁned for using with Post-Square-root Lasso instead of Post-
Algorithm 2 (Estimation of Square-root Lasso loadings using Post-Square-root Lasso iterations). Set k = 0, and specify a small constant ν ⩾0 as a tolerance level and a constant K > 1
as an upper bound on the number of iterations. (1) Compute the Post-Square-root Lasso estimator eβ based on the loadings blj,k. (2) Set blj,k+1 :=
En[(yi −x′
(3) If max1⩽j⩽p |blj,k −blj,k+1| ⩽ν or k > K, set the loadings to blj,k+1, j = 1, . . . , p, and stop;
otherwise set k ←k + 1 and go to (1).
Appendix B. Proof of Theorem 1
The proof proceeds under given sequence of probability measures {Pn}, as n →∞.
Let Y = [y1, ..., yn]′, X = [x1, ..., xn]′, D = [d1, ..., dn]′, V = [v1, ..., vn]′, ζ = [ζ1, ..., ζn]′,
m = [m1, ..., mn]′, Rm = [rm1, ..., rmn]′, g = [g1, ..., gn]′, Rg = [rg1, ..., rgn]′, and so on. For
A ⊂{1, ..., p}, let X[A] = {Xj, j ∈A}, where {Xj, j = 1, ..., p} are the columns of X. Let
PA = X[A](X[A]′X[A])−X[A]′
be the projection operator sending vectors in Rn onto span[X[A]], and let MA = In −PA be
the projection onto the subspace that is orthogonal to span[X[A]]. For a vector Z ∈Rn, let
˜βZ(A) := arg min
b∈Rp ∥Z −X′b∥2 : bj = 0, ∀j ̸∈A,
be the coeﬃcient of linear projection of Z onto span[X[A]]. If A = ∅, interpret PA = 0n, and
Finally, denote φmin(m) = φmin(m)[En[xix′
i]] and φmax(m) = φmax(m)[En[xix′
Step 1.(Main) Write ˇα =
−1 [D′MbIY/n] so that
√n(ˇα −α0) =
−1 [D′MbI(g + ζ)/√n] =: ii−1 · i.
21The algorithms can also be modiﬁed in the obvious manner for Lasso or Square-root Lasso.
BELLONI CHERNOZHUKOV HANSEN
By Steps 2 and 3,
ii = V ′V/n + oP (1) and i = V ′ζ/√n + oP (1).
Next note that V ′V/n = E[V ′V/n] + oP (1) by Chebyshev, and because E[V ′V/n] is bounded
away from zero and from above uniformly in n by Condition SM, we have ii−1 = E[V ′V/n]−1 +
By Condition SM σ2
i ]−1¯E[ζ2
i ]−1 is bounded away from zero and from above,
uniformly in n. Hence
√n(ˇα −α0) = n−1/2
zi,n + oP (1),
where zi,n := σ−1
n viζi are i.n.i.d. with mean zero. For δ > 0 such that 4 + 2δ ⩽q
¯E|zi,n|2+δ ≲¯E
|vi|2+δ|ζi|2+δi
¯E|vi|4+2δ
¯E|ζi|4+2δ ≲1,
by Condition SM. This condition veriﬁes the Lyapunov condition and thus application of the
Lyapunov CLT for i.n.i.d. triangular arrays implies that
Zn ⇝N(0, 1).
Step 2. (Behavior of i.) Decompose, using D = m + V ,
i = V ′ζ/√n + m′MbIg/√n
+ m′MbIζ/√n
+ V ′MbIg/√n
−V ′PbIζ/√n
First, by Step 5 and 6 below we have
|ia| = |m′MbIg/√n| ⩽√n∥MbIg/√n∥∥MbIm/√n∥≲P
[s log(p ∨n)]2/n = o(1),
where the last bound follows from the assumed growth condition s2 log2(p ∨n) = o(n).
Second, using that m = Xβm0 + Rm and m′MbIζ = R′
mζ −(˜βm(bI) −βm0)′X′ζ , conclude
mζ/√n| + |(˜βm(bI) −βm0)′X′ζ/√n| ≲P
[s log(p ∨n)]2/n = oP (1).
This follows since
R′mRm/n ≲P
holding by Chebyshev inequality and Conditions SM and ASTE(iii), and
|(˜βm(bI) −βm0)′X′ζ/√n| ⩽∥˜βm(bI) −βm0∥1∥X′ζ/√n∥∞≲P
[s2 log(p ∨n)]/n
log(p ∨n).
The latter bound follows by (a)
∥˜βm(bI) −βm0∥1 ⩽
bs + s∥˜βm(bI) −βm0∥≲P
[s2 log(p ∨n)]/n
INFERENCE AFTER MODEL SELECTION
holding by Step 5 and by bs ≲P s implied by Lemma 1, and (b) by
∥X′ζ/√n∥∞≲P
holding by Step 4 under Condition SM.
Third, using similar reasoning, decomposition g = Xβg0 + Rg, and Steps 4 and 6, conclude
gV/√n| + |(˜βg(bI) −βg0)′X′V/√n| ≲P
[s log(p ∨n)]2/n = oP (1).
Fourth, we have
|id| ⩽|˜βV (bI)′X′ζ/√n| ⩽∥˜βV (bI)∥1∥X′ζ/√n∥∞≲P
[s log(p ∨n)]2/n = oP (1),
since by Step 4 below ∥X′ζ/√n∥∞≲P
log(p ∨n), and
∥˜βV (bI)∥1
bs∥˜βV (bI)∥⩽
bs∥(X[bI]′X[bI]/n)−1X[bI]′V/n∥
bs∥X′V/√n∥∞/√n ≲P s
[log(p ∨n)]/n.
The latter bound follows from bs ≲P s, holding by Lemma 1, so that φ−1
min(bs) ≲P 1 by Condition
SE, and from ∥X′V/√n∥∞≲P
log(p ∨n) holding by Step 4.
Step 3. (Behavior of ii.) Decompose
ii = (m + V )′MbI(m + V )/n = V ′V/n + m′MbIm/n
+ 2m′MbIV/n
−V ′PbIV/n
Then |iia| ≲P [s log(p∨n)]/n = oP (1) by Step 5, |iib| ≲P [s log(p∨n)]/n = oP (1) by reasoning
similar to deriving the bound for |ib|, and |iic| ≲P [s log(p∨n)]/n = oP(1) by reasoning similar
to deriving the bound for |id|.
Step 4. (Auxiliary: Bounds on ∥X′ζ/√n∥∞and ∥X′V/√n∥∞) Here we show that
(a) ∥X′ζ/√n∥∞≲P
log(p ∨n) and (b)∥X′V/√n∥∞≲P
log(p ∨n).
To show (a), we use Lemma 4 stated in Appendix F on the tail bound for self-normalized
deviations to deduce the bound. Indeed, we have that wp →1 for some ℓn →∞but so slowly
that 1/γ = ℓn ≲log n, with probability 1 −o(1)
2 log(2ℓnp) ≲
log(p ∨n).
By Lemma 4 the ﬁrst inequality in (B.37) holds, provided that for all n suﬃciently large the
following holds,
BELLONI CHERNOZHUKOV HANSEN
Since we can choose ℓn to grow as slowly as needed, a suﬃcient condition for this are the
conditions:
log p = o(n1/3) and
1⩽j⩽p Mj ≳1,
which both hold by Condition SM. Finally,
1⩽j⩽p En[x2
by Condition SM. Therefore (a) follows from the bounds (B.37) and (B.38). Claim (b) follows
similarly.
Step 5. (Auxiliary: Bound on ∥MbIm∥and related quantities.) This step shows that
(a) ∥MbIm/√n∥≲P
[s log(p ∨n)]/n and (b) ∥˜βm(bI) −βm0∥≲P
[s log(p ∨n)]/n.
Observe that
[s log(p ∨n)]/n ≳P
∥MbI1m/√n∥≳P
where inequality (1) holds since by Lemma 1 ∥MbI1m/√n∥⩽∥(X ˜βD(bI1) −m)/√n∥≲P
[s log(p ∨n)]/n, and (2) holds by bI1 ⊆bI by construction. This shows claim (a). To show
claim (b) note that
∥MbIm/√n∥≳P
|∥X(˜βm(bI) −βm0)/√n∥−∥Rm/√n∥|
where (3) holds by the triangle inequality.
Since ∥Rm/√n∥≲P
s/n by Chebyshev and
Condition ASTE(iii), conclude that
[s log(p ∨n)]/n
∥X(˜βm(bI) −βm0)/√n∥
φmin(bs + s)∥˜βm(bI) −βm0∥≳P ∥˜βm(bI) −βm0∥,
since bs ≲P s by Lemma 1 so that 1/φmin(bs + s) ≲P 1 by condition SE. This shows claim (b).
Step 6. (Auxiliary: Bound on ∥MbIg∥and related quantities.) This step shows that
(a) ∥MbIg/√n∥≲P
[s log(p ∨n)]/n and (b) ∥˜βg(bI) −βg0∥≲P
[s log(p ∨n)]/n.
Observe that
[s log(p ∨n)]/n
∥MbI2(α0m + g)/√n∥
∥MbI(α0m + g)/√n∥
|∥MbIg/√n∥−∥MbIα0m/√n∥|
INFERENCE AFTER MODEL SELECTION
where inequality (1) holds since by Lemma 1 ∥MbI2(α0m + g)/√n∥⩽∥(X ˜βY1(bI2) −α0m −
[s log(p ∨n)]/n, (2) holds by bI2 ⊆bI, and (3) by the triangle inequality. Since
∥α0∥is bounded uniformly in n by assumption, by Step 5, ∥MbIα0m/√n∥≲P
[s log(p ∨n)]/n.
Hence claim (a) follows by the triangle inequality:
[s log(p ∨n)]/n ≳P ∥MbIg/√n∥
To show claim (b) we note that
∥MbIg/√n∥⩾|∥X(˜βg(bI) −βg0)/√n∥−∥Rg/√n∥|
where ∥Rg/√n∥≲P
s/n by Condition ASTE(iii). Then conclude similarly to Step 5 that
[s log(p ∨n)]/n
∥X(˜βg(bI) −βg0)/√n∥
φmin(bs + s)∥˜βg(bI) −βg0∥≳P ∥˜βg(bI) −βg0∥.
Step 7. (Variance Estimation.) Since bs ≲P s = o(n), (n −bs −1)/n = oP (1), and since
i ] and ¯E[v2
i ] are bounded away from zero and from above uniformly in n by Condition
SM, it suﬃces to show that
i ] −¯E[v2
i ] −¯E[v2
The second relation was shown in Step 3, so it remains to show the ﬁrst relation.
Let ˜vi = vi + rmi and ˜ζi = ζi + rgi. Recall that by Condition ASTE(v) we have ¯E[˜v2
i ] →0, and En[˜v2
i ] −¯E[˜v2
i ] →P 0 by Vonbahr-Esseen’s inequality in von Bahr and
Esseen since ¯E[|˜vi˜ζi|2+δ] ⩽(¯E[|˜vi|4+2δ]¯E[|˜ζi|4+2δ])1/2 is uniformly bounded for 4+2δ ⩽q.
Thus it suﬃces to show that En[bv2
i ] −En[˜v2
By the triangular inequality
i ]| ⩽|En[(bv2
Then, expanding bζ2
⩽2En[{di(α0 −ˇα)}2bv2
i ] + 2En[{x′
i(ˇβ −βg0)}2bv2
+|2En[˜ζidi(α0 −ˇα)bv2
i ]| + |2En[˜ζix′
i(ˇβ −βg0)bv2
=: iiia + iiib + iiic + iiid = oP (1)
where the last bound follows by the relations derived below.
First, we note
i |α0 −ˇα|2En[bv2
i ] ≲P n(2/q)−1 = o(1)
i⩽n {|˜ζi||di|}En[bv2
i ]|α0 −ˇα| ≲P n(2/q)−(1/2) = o(1)
BELLONI CHERNOZHUKOV HANSEN
which holds by the following argument. Condition SM assumes that E[|di|q] which in turn
implies that E[maxi⩽n d2
i ] ≲n2/q. Similarly Condition ASTE implies that E[maxi⩽n ˜ζ2
and E[maxi⩽n ˜v2
i ] ≲n2/q. Thus by Markov inequality
i⩽n |di| + |˜ζi| + |˜vi| ≲P n1/q.
Moreover, En[bv2
i ] ≲P 1 and |ˇα −α0| ≲P n−1/2 by the previous steps. These bounds and q > 4
imposed in Condition SM imply (B.39)-(B.40).
Next we bound,
i⩽n |˜ζi| max
i(ˇβ −βg0)|En[bv2
s log(p ∨n)
using (B.41) and that for bTg = support(βg0) ∪bI, we have
i(ˇβ −βg0)}2 ⩽max
i⩽n ∥xi bTg∥2∥ˇβ −βg0∥2,
i⩽n ∥xi bTg∥2 ⩽| bTg| max
by the sparsity assumption in ASTE and the sparsity bound in Lemma 1, and since ˇβ[bI] =
(X[bI]′X[bI])−X[bI]′(ζ + g −(ˇα −α0)D) we have
∥ˇβ −βg0∥⩽∥˜βg(bI) −βg0∥+ ∥˜βζ(bI)∥+ |ˇα −α0| · ∥˜βD(bI)∥≲P
s log(p ∨n)/n
by Step 6(b), by
∥˜βζ(bI)∥⩽
min(bs)∥X′ζ/n∥∞≲P
s log(p ∨n)/n
holding by Condition SE and by bs ≲P s from Lemma 1, and by Step 4, |ˇα −α0| ≲P 1/√n by
Step 1, and
∥˜βD(bI)∥⩽φ−1
1⩽j⩽p |En[xijdi]| ⩽φ−1
by Condition SE, bs ≲P s by the sparsity bound in Lemma 1, and Condition SM.
The ﬁnal conclusion in (B.42) then follows by condition ASTE (iv) and (v).
Next, using the relations above and condition ASTE (iv) and (v), we also conclude that
i(ˇβ −βg0)}2En[bv2
s log(p ∨n)
INFERENCE AFTER MODEL SELECTION
Finally, the argument for iv = oP (1) follows similarly to the argument for iii = oP (1) and
the result follows.
Appendix C. Proof of Corollary 1
Let Pn be a collection of probability measures P for which conditions ASTE (P), SM (P), SE
(P), and R (P) hold for the given n. Consider any sequence {Pn}, with index n ∈{n0, n0+1, ...},
with Pn ∈Pn for each n ∈{n0, n0 + 1, ...}. By Theorem 1 we have that, for c = Φ−1(1 −γ/2),
limn→∞Pn (α0 ∈[ˇα ± cbσn/√n]) = Φ(c) −Φ(−c) = 1 −γ. This means that for every further
subsequence {Pnk} with Pnk ∈Pnk for each k ∈{1, 2, ...}
k→∞Pnk (α0 ∈[ˇα ± cbσnk/√nk]) = 1 −γ.
Suppose that the claim of corollary does not hold, i.e.
 α0 ∈[ˇα ± cbσn/√n]
Hence there is a subsequence {Pnk} with Pnk ∈Pnk for each k ∈{1, 2, ...} such that:
k→∞Pnk (α0 ∈[ˇα ± cbσnk/√nk]) ̸= 1 −γ.
This gives a contradiction to (C.44). The claim (i) follows. Claim (ii) follows from claim (i),
since P ⊆Pn for all n ⩾n0.
Appendix D. Proof of Theorem 2
We use the same notation as in Theorem 1. Using that notation the approximations bounds
stated in Condition HLMS are equivalent to ∥MbIg∥⩽δnn1/4 and ∥MbIm∥⩽δnn1/4.
Step 1. It follows the same reasoning as Step 1 in the proof of Theorem 1.
Step 2. (Behavior of i.) Decompose, using D = m + V
i = V ′ζ/√n + m′MbIg/√n
+ m′MbIζ/√n
+ V ′MbIg/√n
−V ′PbIζ/√n
First, by Condition HLMS we have ∥MbIg∥= oP (n1/4) and ∥MbIm∥= oP (n1/4). Therefore
|ia| = |m′MbIg/√n| ⩽√n∥MbIg/√n∥∥MbIm/√n∥≲P o(1).
Second, using that m = Xβm0 + Rm and m′MbIζ = R′
mζ −(˜βm(bI) −βm0)′X′ζ, we have
mζ/√n| + |(˜βm(bI) −βm0)′X′ζ/√n|
mζ/√n| + ∥˜βm(bI) −βm0∥1∥X′ζ/√n∥∞
s/n + √s {o(n−1/4) +
log(p ∨n) = o(1).
BELLONI CHERNOZHUKOV HANSEN
This follows because
R′mRm/n ≲P
by Chebyshev inequality and Conditions SM and ASTE(iii),
∥˜βm(bI) −βm0∥1 ⩽
bs + s∥˜βm(bI) −βm0∥≲P
√s {o(n−1/4) +
by Step 4 and bs = |bI| ≲P s by Condition HLMS, and
∥X′ζ/√n∥∞≲P
holding by Step 4 in the proof of Theorem 1.
Third, using similar reasoning and the decomposition g = Xβg0 + Rg conclude
gV/√n| + |(˜βg(bI) −βg0)′X′V/√n|
s/n + √s {o(n−1/4) +
log(p ∨n) = oP (1).
Fourth, we have
|id| ⩽|˜βV (bI)′X′ζ/√n| ⩽∥˜βV (bI)∥1∥X′ζ/√n∥∞≲P
[s log(p ∨n)]2/n = oP (1),
since ∥X′ζ/√n∥∞≲P
log(p ∨n) by Step 4 of the proof of Theorem 1, and
∥˜βV (bI)∥1
bs∥˜βV (bI)∥⩽
bs∥(X[bI]′X[bI]/n)−1X[bI]′V/n∥
bs∥X′V/√n∥∞/√n ≲P s
[log(p ∨n)]/n.
The latter bound follows from bs ≲P s by condition HLMS so that φ−1
min(bs) ≲P 1 by condition SE,
and again invoking Step 4 of the proof of Theorem 1 to establish ∥X′V/√n∥∞≲P
log(p ∨n).
Step 3. (Behavior of ii.) Decompose
ii = (m + V )′MbI(m + V )/n = V ′V/n + m′MbIm/n
+ 2m′MbIV/n
−V ′PbIV/n
Then |iia| ≲P o(n1/2)/n = oP(n−1/2) by condition HLMS, |iib| = o(n−1/2) by reasoning similar
to deriving the bound for |ib|, and |iic| ≲P [s log(p ∨n)]/n = oP (1) by reasoning similar to
deriving the bound for |id|.
Step 4. (Auxiliary: Bounds on ∥˜βm(bI) −βm0∥and ∥˜βg(bI) −βg0∥.) To establish a bound on
∥˜βg(bI) −βg0∥note that
∥MbIg/√n∥⩾| ∥X(˜βg(bI) −βg0)/√n∥−∥Rg/√n∥|
INFERENCE AFTER MODEL SELECTION
where ∥Rg/√n∥≲P
s/n holds by Chebyshev inequality and Condition ASTE(iii). Moreover,
by Condition HLMS we have ∥MbIg/√n∥= oP(n−1/4) and bs = |bI| ≲P s. Thus
o(n−1/4) +
≳P ∥X(˜βg(bI) −βg0)/√n∥
φmin(s + bs)∥˜βg(bI) −βg0∥
≳P ∥˜βg(bI) −βg0∥
φmin(s + bs) ≳P 1 by Condition SE.
The same logic yields ∥˜βm(bI) −βm0∥≲P
s/n + o(n−1/4).
Step 5. (Variance Estimation.) It follows similarly to Step 7 in the proof of Theorem 1 but
using Condition HLMS instead of Lemma 1.
Appendix E. Proof of Corollary 2
The proof is similar to the proof of Corollary 1.
Appendix F. Verification of Conditions for the Examples
F.1. Veriﬁcation for Example 1. Let P be the collection of all regression models P that
obey the conditions set forth above for all n for the given constants (p, b, B, qx, q). Below we
provide explicit bounds for κ′, κ′′, c, C, δn and ∆n that appear in Conditions ASTE, SE and
SM that depend only on (p, b, B, qx, q) and n which in turn establish these conditions for any
Condition ASTE(i) is assumed. Condition ASTE(ii) holds with ∥α0∥⩽CASTE
= B. Condition ASTE(iii) holds with s = p and rgi = rmi = 0.
Condition ASTE(iv) holds with δASTE
:= p2 log2(p ∨n)/n →0 since s = p is ﬁxed. Finally,
we verify ASTE(v). Because ˜vi = vi, ˜ζi = ζi and the moment condition E[|vq
i |] + E[|ζq
= 2B with q > 4, the ﬁrst two requirements follow. To show the last requirement, note
that because E[∥xi∥qx] ⩽B we have
1⩽i⩽n ∥xi∥∞> t1n
⩽nE[∥xi∥qx]/tqx
1n ⩽nB/tqx
1n =: ∆ASTE
BELLONI CHERNOZHUKOV HANSEN
Let t1n = (n log n)1/qxB1/qx so that ∆ASTE
= 1/ log n. Thus we have with probability 1 −
1⩽i⩽n ∥xi∥2
∞sn−1/2+2/q ⩽(n log n)2/qxB2/qxpn−1/2+2/q =: δASTE
It follows that δASTE
→0 by the assumption that 4/qx + 4/q < 1.
To verify Condition SE note that
P(∥En[xix′
i] −E[xix′
ij] + E[x4
⩽pE[∥xi∥4]
Setting t2n := b/2 we have ∆SE
1n = (2/b)2B4/qxp/n →0 since p is ﬁxed. Then, with probability
1n we have
λmin(En[xix′
⩾λmin(E[xix′
i]) −∥En[xix′
i] −E[xix′
i]∥⩾b/2 =: κ′,
λmax(En[xix′
⩽λmax(E[xix′
i]) + ∥En[xix′
i] −E[xix′
i]∥⩽E[∥xi∥2] + b/2 ⩽2B2/qx =: κ′′.
In the veriﬁcation of Condition SM note that the second and third requirements in Condition
SM(i) hold with cSM
= b and CSM
= B2/q. Condition SM(iii) holds with δSM
:= log3 p/n →0
since p is ﬁxed.
The ﬁrst requirement in Condition SM(i) and Condition SM(ii) hold by the stated moment
assumptions, for ǫi = vi and ǫi = ζi, ˜yi = di and ˜yi = yi,
⩽2q−1E[|x′
iβm0|q] + 2q−1E[|vq
i |] ⩽2q−1E[∥xi∥q]∥βm0∥q + 2q−1E[|vq
⩽2q−1(Bq/qxBq + B) =: A2
⩽23(B4/qxB4 + B) =: A′
⩽33∥α0∥4E[d4
i ] + 33∥βg0∥4E[∥xi∥4] + 33E[ζ4
2 + 33B4B4/qx + 33B4/q =: A3
1⩽j⩽p(E[x4
ij])1/2(E[˜y4
i ])1/2 ⩽B2/qx(E[˜y4
i ])1/2 ⩽B2/qx(A′
2 ∨A3)1/2 =: A4
1⩽j⩽p E[|xijǫi|3]
1⩽j⩽p E[|x3
i | | xi]] ⩽B3/q max
1⩽j⩽p E[|x3
ij|] ⩽B3/q+3/qx =: A5
1⩽j⩽p 1/E[x2
⩽1/λmin(E[xix′
i]) ⩽1/b =: A6
since 4 < q ⩽qx. Thus these conditions hold with CSM
= A2∨(A1+(A′
2∨A3)1/2+A4+A5+A6).
Next we show Condition SM(iv). By (F.45) we have max1⩽i⩽n ∥xi∥2
∞⩽(n log n)2/qxB2/qx
with probability 1 −∆ASTE
, thus with the same probability
s log(n ∨p)
⩽(B log n)2/qx n2/qxp log(p ∨n)
INFERENCE AFTER MODEL SELECTION
since qx > 4 and s = p is ﬁxed.
Next for ǫi = vi and ǫi = ζi we have
1⩽j⩽p|(En −E)[x2
i ]| > δSM
2n )2 ⩽pB4/q+4/qx
by the union bound, Chebyshev inequality and by E[x4
i ] = E[x4
i | xi]] ⩽B4/q+4/qx.
Letting δSM
= B2/q+2/qxn−1/4 →0 we have ∆SM
= p/n1/2 →0 since p, B, q and qx are ﬁxed.
Next for ˜yi = di and ˜yi = yi we have
1⩽j⩽p |(En −E)[x2
i ]| > δSM
3n )2 ⩽pB4/qxA4/q
by the union bound, Chebyshev inequality and by
i ] ⩽E[x˜q
ij]4/˜qE[˜yq
i ]4/q ⩽E[xqx
ij ]4/qxE[˜yq
i ]4/q ⩽B4/qxA4/q
holding by H¨older inequality where 4 < ˜q ⩽qx such that 4/q + 4/˜q = 1, and
⩽(1 + 3q−1∥α0∥q)E[dq
i ] + 3q−1∥βg0∥qE[∥xi∥q] + 3q−1E[ζq
⩽3q(A2 + BqA2 + BqBq/qx + B) =: A8.
Letting δSM
= B4/qxA4/q
n−1/4 →0 we have ∆SM
2n = p/n1/2 →0 since p, B, q and qx are ﬁxed.
Finally, we set c = cSM
C = max{CASTE
}, δn = max{δASTE
3n } →0, and ∆n = max{∆ASTE
We will make use of the following technical lemma in the veriﬁcation of examples 2, 3, and
Lemma 2 (Uniform Approximation). Let hi = x′
iθh + ρi be a function whose coeﬃcients
A(p), and κ ⩽λmin(E[xix′
i]) ⩽λmax(E[xix′
i]) ⩽¯κ. For s = A1/an1/2a, a > 1, deﬁne βh0
as in (5.29), rhi = hi −x′
iβh0, for i = 1, . . . , n. Then we have
|rhi| ⩽∥xi∥∞(¯κ/κ)3/2
Proof. Let Th denote the support of βh0 and S denote the support of the s largest components
of θh. Note that |Th| = |S| = s. First we establish some auxiliary bounds on the ∥θh[T c
h]∥1. By the optimality of Th and βh0 we have that
E[(xi[Sc]′θh[Sc] + ρi)2] ⩽
¯κ∥θh[Sc]∥+
i(θh −βh0) + ρi}2] ⩾√κ∥θh[T c
BELLONI CHERNOZHUKOV HANSEN
Thus we have ∥θh[T c
¯κ/κ∥θh[Sc]∥+ 2
i ]/κ. Moreover, since θh ∈Sa
A(p), we have
∥θh[Sc]∥2 =
j−2a ⩽A2s−2a+1/[2a −1] ⩽A2s−2a+1
since a > 1. Combining these relations we have
¯κ/κAs−a+1/2 + 2
The second bound follows by observing that
⩽√s∥θh[T c
h ∩S]∥+ ∥θh[Sc]∥1 ⩽√s∥θh[T c
h]∥+ As−a+1/[a −1]
i ]/κ + (s/√n)/[a −1]
¯κ/κ a/[a −1] + 2
By the ﬁrst-order optimality condition of the problem (5.29) that deﬁnes βh0, we have
E[xi[Th]xi[Th]′](βh0[Th] −θh[Th]) = E[xi[Th]xi[T c
h]′]θh[T c
h] + E[xi[Th]ρi].
Thus, since ∥E[xi[Th]ρi]∥= sup∥η∥=1 E[η′xi[Th]ρi] ⩽sup∥η∥=1
E[(η′xi[Th])2]
i ] we have
κ∥βh0 −θh[Th]∥
⩽¯κ∥θh[T c
s/n (¯κ3/2/√κ) +
i ] √¯κ(1 + 2
where the last inequality follows from the deﬁnition of s = A1/an1/2a. Therefore
iβh0| = |x′
i(θh −βh0)| + |ρi|
⩽∥xi∥∞∥θh −βh0∥1 + |ρi|
⩽√s∥xi∥∞∥θhTh −βh0∥+ ∥xi∥∞∥θhT c
h∥1 + |ρi|
s2/n (¯κ/κ)3/2 +
¯κ/κ(1 + 2
¯κ/κ a/[a −1] + 2
i ]/κ) + |ρi|
⩽∥xi∥∞(¯κ/κ)3/2{2a−1
i ]/κ} + |ρi|.
F.2. Veriﬁcation for Example 2. Let P be the collection of all regression models P that
obey the conditions set forth above for all n for the given constants (κ, ¯κ, a, A, B, χ) and
sequences pn and ¯δn. Below we provide explicit bounds for κ′, κ′′, c, C, δn and ∆n that appear
in Conditions ASTE, SE and SM that depend only on (κ, ¯κ, a, A, B, χ), p, ¯δn and n which in
turn establish these conditions for any P ∈P. In what follows we exploit Gaussianity of wi
INFERENCE AFTER MODEL SELECTION
and use that (E[|η′wi|k])1/k ⩽Gk(E[|η′wi|2])1/2 for any vector η, ∥η∥< ∞, where the constant
Gk depends on k only.
Conditions ASTE(i) is assumed.
Condition ASTE(ii) holds with ∥α0∥⩽B =: CASTE
Because θm, θg ∈Sa
A(p), Condition ASTE(iii) holds with
s = A1/an1/2a,
rmi = m(zi) −
and rgi = g(zi) −
where ∥βm0∥0 ⩽s and ∥βg0∥0 ⩽s. Indeed, we have
θm(j)zi(j)
m(j) ⩽¯κA2s−2a+1/[2a −1] ⩽¯κs/n
where the ﬁrst inequality follows by the deﬁnition of βm0 in (5.29), the second inequality
follows from θm ∈Sa
A(p), and the last inequality because s = A1/an1/2a. Similarly we have
j⩾s+1 θg(j)zi(j))2] ⩽¯κA2s−2a+1/[2a −1] ⩽¯κs/n. Thus let CASTE
Condition ASTE(iv) holds with δASTE
:= A2/an1/a−1 log2(p ∨n) →0 since s = A1/an1/2a,
A is ﬁxed, and the assumed condition n(1−a)/a log2(p ∨n) log2 n ⩽¯δn →0.
The moment restrictions in Condition ASTE(v) are satisﬁed by the Gaussianity. Indeed, we
have for q = 4/χ (where χ < 1 by assumption)
⩽2q−1E[|ζq
i |] + 2q−1E[|rq
gi|] ⩽2q−1Gq
i ]q/2 + E[r2
q{¯κq/2 + ¯κq/2(s/n)q/2}
q¯κq/2 =: CASTE
for s ⩽n, i.e., n ⩾nASTE
:= A2/[2a−1]. Similarly, E[|˜vi|q] ⩽CASTE
. Moreover,
mi] + E[r2
i ] + E[r2
4¯κ2{2 + ¯κs/n}s/n =: δASTE
Next note that by Gaussian tail bounds and λmax(E[wiw′
i]) ⩽¯κ we have
maxi⩽n ∥xi∥∞
⩽∥E[xi]∥∞+ maxi⩽n ∥xi −E[xi]∥∞
2¯κ log(pn) with probability at least 1 −∆ASTE
where ∆ASTE
2¯κ log(pn).
The last requirement in Condition ASTE(v) holds with
∞sn−1/2+2/q ⩽6¯κ log(pn)A1/an
2+χ/2 =: δASTE
with probability 1 −∆ASTE
. By the assumption on a, p, χ, and n, δASTE
BELLONI CHERNOZHUKOV HANSEN
To verify Condition SE with ℓn = log n note that the minimal and maximal eigenvalues of
i] are bounded away from zero by κ > 0 and from above by ¯κ < ∞uniformly in n. Also,
let µ = E[xi] so that xi = ˜xi +µ where ˜xi is zero mean. By constriction E[xix′
i] = E[˜xi˜x′
and ∥µ∥⩽√¯κ.
For any η ∈Rp, ∥η∥0 ⩽k := s log n and ∥η∥= 1, we have that
En[(η′xi)2] −E[(η′xi)2] = En[(η′˜xi)2] −E[(η′˜xi)2] + 2η′En[˜xi] · η′µ.
Moreover, by Gaussianity of xi, with probability 1 −∆SE
1n , where ∆SE
2¯κ log(pn),
|η′En[˜xi]|
⩽∥η∥1∥En[˜xi]∥∞⩽
2¯κ log(pn)/√n
⩽∥η∥∥µ∥⩽√¯κ.
By the sub-Gaussianity of ˜xi = (E[xix′
i] −µµ′)−1/2Ψi, where Ψi ∼N(0, Ip), by Theorem 3.2
in Rudelson and Zhou (restated in Lemma 10 in Appendix G) with τ = 1/6, k = s log n,
8/3, provided that
n ⩾Nn := 80(α4/τ 2)(s log n) log(12ep/[τs log n]),
(1 −τ)2E[(η′˜xi)2] ⩽En[(η′˜xi)2] ⩽(1 + τ)2E[(η′˜xi)2]
with probability 1 −∆SE
1n , where ∆SE
1n = 2exp(−τ 2n/80α4). Note that under ASTE(iv) we
01 := max{n : n ⩽Nn} ⩽max{(12e/τ)2aA−2, 802(α8/τ 4)A2/a, n∗}
where n∗is the smallest n such that ¯δn < 1.
Therefore, with probability 1 −∆SE
1n and n ⩾nSE
01 , we have for any η ∈Rp, ∥η∥0 ⩽k and
En[(η′xi)2]
⩾E[(η′xi)2] −|En[(η′xi)2] −E[(η′xi)2]|
⩾E[(η′xi)2] −|En[(η′˜xi)2] −E[(η′˜xi)2]| −2|η′En[˜xi]| · |η′µ|
⩾E[(η′xi)2]{1 −2τ −τ 2} −2¯κ
2k log(pn)/√n
⩾E[(η′xi)2]/2 −2¯κ
2k log(pn)/√n
since τ = 1/6 and E[(η′˜xi)2] ⩽E[(η′xi)2]. So for n ⩾nSE
02 := 288k(¯κ/κ)2 log(pn) we have
φmin(s log n)[En[xix′
i]] ⩾κ/3 =: κ′.
INFERENCE AFTER MODEL SELECTION
Similarly, we have
En[(η′xi)2]
⩽E[(η′xi)2] + |En[(η′xi)2] −E[(η′xi)2]|
⩽E[(η′xi)2] + |En[(η′˜xi)2] −E[(η′˜xi)2]| + 2|η′En[˜xi]| · |η′µ|
⩽E[(η′xi)2]{1 + 2τ + τ 2} + 2¯κ
2k log(pn)/√n
⩽2E[(η′xi)2] + 2¯κ
2k log(pn)/√n
since τ = 1/6 and E[(η′˜xi)2] ⩽E[(η′xi)2]. So for n ⩾nSE
03 := 2k log(pn) we have
φmax(s log n)[En[xix′
i]] ⩽4¯κ =: κ′′.
The second and third requirements in Conditions SM(i) holds by the Gaussianity of wi,
E[ζi | xi, vi] = 0, E[vi | xi] = 0, and the assumption that the minimal and maximum eigenvalues
of the covariance matrix (operator) E[wiw′
i] are bounded below and above by positive absolute
constants.
The ﬁrst requirement in Condition SM(i) and Condition SM(ii) also hold by Gaussianity.
Indeed, we have for ǫi = vi and ǫi = ζi, ˜yi = di and ˜yi = yi
i |] + E[|ζq
i ])q/2 + (E[ζ2
i ])q/2} ⩽2qGq
q¯κq/2 =: A1
⩽2q−1E[|θ′
mz|q] + 2q−1E[|vq
i |] ⩽2q−1Gq
mz|2])q/2 + 2q−1Gq
q∥θm∥q¯κq/2 + 2q−1Gq
q¯κq/2 ⩽2qGq
q¯κq/2(1 + (2A)q) =: A2
mzi|2] + 2E[v2
i ] ⩽2¯κ∥θm∥2 + 2¯κ ⩽2¯κ(4A2 + 1) =: A′
⩽3|α0|2E[d2
i ] + 3E[|θ′
mz|2] + 3E[ζ2
i ] ⩽3B2A′
2 + 3¯κ =: A3
max1⩽j⩽p E[x2
⩽max1⩽j⩽p(E[x4
ij])1/2(E[˜y4
i ])1/2 ⩽G4
4 max1⩽j⩽p E[x2
2 ∨A3) =: A4
max1⩽j⩽p E[|xijǫi|3]
⩽max1⩽j⩽p(E[x6
ij])1/2(E[ǫ6
i ])1/2 ⩽G6
6 max1⩽j⩽p(E[x2
ij])3/2(E[ǫ2
6¯κ3 =: A5
max1⩽j⩽p 1/E[x2
⩽1/λmin(E[wiw′
i]) ⩽1/κ =: A6
because ∥θm∥⩽2A and ∥θg∥⩽2A since θm, θg ∈Sa
Thus the ﬁrst requirement in
Condition SM(i) holds with CSM
= A2. Condition SM(ii) holds with CSM
A4 + A5 + A6.
Condition SM(iii) is assumed.
To verify Condition SM(iv) note that for ǫi = vi and ǫi = ζi, by (F.46), with probability
2¯κ log(pn)} maxj⩽p
BELLONI CHERNOZHUKOV HANSEN
By Lemma 3 with k = 4 we have with probability 1 −∆SM
1n , where ∆SM
⩽∥E[xi]∥∞+ maxj⩽p
En[(xij −E[xij])4]
⩽√¯κ + √¯κ2 ¯C + √¯κn−1/4p
2 log(2pn) ⩽4 ¯C√¯κ
for n ⩾nSM
= 4 log2(2pn). Also, Lemma 3 with k = 8 and p = 1 we have with probability
i ] ⩽2¯κ8 ¯C2 + 2¯κn−1/42 log(2n) ⩽20 ¯C2¯κ
for n ⩾nSM
= 16 log4(2n). Moreover, we have
Applying Lemma 6, for τ = 2∆ASTE
1n , with probability 1 −8τ we have
j⩽p |(En −¯E)[x2
2 log(2p/τ)
1⩽j⩽p En[x4
i ], 1 −τ) ∨2
where by (F.47), (F.48) and (F.49) we have
Q(max1⩽j⩽p
i ], 1 −τ)
2 log(pn)80 ¯C3.
So we let δSM
= 640 ¯C3¯κ2
√n →0 under the condition that log2(p∨
n)/n ⩽¯δn.
Similarly for ˜yi = di and ˜yi = yi, by Lemma 3, we have with probability 1 −∆SM
⩽|E[˜yi]| +
En[(˜yi −E[˜yi])8]
2 ∨A3]1/2 + (20 ¯C2E[˜y2
i ])1/2 ⩽6 ¯C[A′
2 ∨A3]1/2.
2 ∨A3]. Therefore by Lemma 6, for τ = 2∆ASTE
with probability 1 −8τ we have by the arguments in (F.47), (F.48), and (F.50)
j⩽p |(En −¯E)[x2
2 log(2p/τ)
6¯κ log(pn)4 ¯C
¯κ(36 ¯C2[A′
2 ∨A3]) ∨2
→0 under the condition log2(p ∨n)/n ⩽¯δn →0.
We have that the last term in Condition SM(iv) satisﬁes with probability 1 −∆ASTE
s log(p ∨n)
⩽6¯κ log(pn)A1/an−1+1/2a log(p ∨n) =: δSM
Under ASTE(iv) and s = A1/an1/2a we have δSM
INFERENCE AFTER MODEL SELECTION
Finally, we set n0 = max{nASTE
02 }, C = max{CASTE
}, δn = max{¯δn, δASTE
3n } →0, and ∆n =
max{33∆ASTE
Lemma 3. Let fij ∼N(0, σ2
j ), σj ⩽σ, independent across i = 1, . . . , n, where j = 1, . . . , p.
Then, for some universal constant ¯C ⩾1, we have that for any k ⩾2 and γ ∈(0, 1)
1⩽j⩽p{En[|f k
ij|]}1/k ⩾σ ¯C
k + σn−1/kp
2 log(2p/γ)
Proof. Note that P(En[|f k
ij|] > M) = P(∥f·j∥k
k > Mn) = P(∥f·j∥k > (Mn)1/k).
Since |∥f∥k −∥g∥k| ⩽∥f −g∥k ⩽∥f −g∥, we have that ∥· ∥k is 1-Lipschitz for k ⩾2.
E[∥f·j∥k] ⩽(E[∥f·j∥k
k])1/k = (
ij|])1/k = n1/k(E[|f k
j 2k/2Γ((k + 1)/2)/Γ(1/2)}1/k ⩽n1/kσ
By Ledoux and Talagrand , page 21 equation (1.6), we have
P(∥f·j∥k > (Mn)1/k) ⩽2 exp(−{(Mn)1/k −E[∥f·j∥k]}2/2σ2
Setting M := {σ
k ¯C+σn−1/kp
2 log(2p/γ)}k, so that (Mn)1/k = n1/kσ
2 log(2p/γ)
we have by the union bound and σ ⩾σj
1⩽j⩽p En[|f k
ij|] ⩾M) ⩽p max
1⩽j⩽pP(En[|f k
ij|] ⩾M) ⩽γ.
F.3. Veriﬁcation for Example 3. Let P be the collection of all regression models P that
obey the conditions set forth above for all n for the given constants (f, ¯f, a, A, b, B, q) and the
sequence ¯δn. Below we provide explicit bounds for κ′, κ′′, c, C, δn and ∆n that appear in
Conditions ASTE, SE and SM that depend only on (f, ¯f, a, A, b, B, q) and ¯δn which in turn
establish these conditions for all P ∈P.
Conditions ASTE(i) is assumed.
Condition ASTE(ii) holds with ∥α0∥⩽B =: CASTE
Because θm, θg ∈Sa
A(p), Condition ASTE(iii) holds with
rmi = m(zi) −
βm0jPj(zi) and rgi = g(zi) −
βg0jPj(zi)
BELLONI CHERNOZHUKOV HANSEN
where ∥βm0∥0 ⩽s and ∥βg0∥0 ⩽s. Indeed, we have
θm(j)P(j)(zi)
m(j) ⩽¯fA2s−2a+1/[2a −1] = ¯fs/n
where the ﬁrst inequality follows by the deﬁnition of βm0 in (5.29), the second inequality
follows from the upper bound on the density and orthogonality of the basis, the third inequality
follows from θm ∈Sa
A(p), and the last inequality because s = A1/an1/2a. Similarly we have
j⩾s+1 θg(j)zi(j))2] ⩽¯fA2s−2a+1/[2a −1] = ¯fs/n. Let CASTE
Condition ASTE(iv) holds with δASTE
:= A2/an1/a−1 log2(p ∨n) →0 since s = A1/an1/2a,
A is ﬁxed, and the assumed condition n(1−a)/a log2(p ∨n) ⩽¯δn →0.
Next we establish the moment restrictions in Condition ASTE(v). Because f ⩽λmin(E[xix′
λmax(E[xix′
i]) ⩽¯f, by the assumption on the density and orthonormal basis, and maxi⩽n ∥xi∥∞⩽
B, by Lemma 2 with ρi = 0 we have
1⩽i⩽n |rmi| ∨|rgi| ⩽max
1⩽i⩽n ∥xi∥∞( ¯f/f)3/2 2a −1
s2/n ⩽B( ¯f/f)3/2 2a −1
s2/n =: δASTE
where δASTE
→0 under s = A1/an1/2a and a > 1.
Thus we have
⩽2q−1E[|ζq
i |] + 2q−1E[|rq
gi|] ⩽2q−1B + 2q−1(δASTE
⩽2q−1B + 2q−1(δASTE
)q =: CASTE
Similarly, E[|˜vi|q] ⩽CASTE
. Moreover, since δASTE
→0 we have
mi] + E[r2
i ] + E[r2
⩽2B2/q(δASTE
)2 + (δASTE
)4 =: δASTE
Finally, the last requirement holds because (1 −a)/a + 4/q < 0 implies
∞sn−1/2+2/q ⩽B2A1/an1/2a−1/2+2/q =: δASTE
since s = A1/an1/2a and maxi⩽n ∥xi∥∞⩽B.
To show Condition SE with ℓn = log n note that regressors are uniformly bounded, and
minimal and maximal eigenvalues of E[xix′
i] are bounded below by f and above by ¯f uniformly
in n. Thus Condition SE follows by Corollary 4 in the supplementary material in Belloni and
Chernozhukov (restated in Lemma 9 in Appendix G) which is based on Rudelson and
Vershynin . Let
1n := 2 ¯CB
s log n log(1 + s log n)
INFERENCE AFTER MODEL SELECTION
1n := (2/f)(δSE
1n )2 + δSE
1n (2 ¯f/f), where ¯C is an universal constant. By this result and
the Markov inequality, we have with probability 1 −∆SE
κ′ := f/2 ⩽φmin(s log n)[En[xix′
i]] ⩽φmax(s log n)[En[xix′
i]] ⩽2 ¯f =: κ′′.
We need to show that ∆SE
1n →0 which follows from δSE
1n →0. We have that
1n ⩽2 ¯CB(1 + A)2√
n1/2a log2(n)
= 2 ¯CB(1 + A)2
n1/2a log4 n
By assumption we have log3 p/n ⩽¯δn →0 and a > 1 we have δSE
The second and third requirements in Condition SM(i) hold with CSM
= B2/q and cSM
by assumption. Condition SM(iii) is assumed.
The ﬁrst requirement in Condition SM(i) and Condition SM(ii) follow by, for ǫi = vi and
ǫi = ζi, ˜yi = di and ˜yi = yi
i |] + E[|ζq
⩽2q−1E[|θ′
mxi|q] + 2q−1E[|vq
i |] ⩽2q−1∥θm∥q
∞] + 2q−1B
⩽2q−1(2A)qBq + 2q−1B =: A2
⩽2 ¯f∥θm∥2 + 2E[v2
i ] ⩽8 ¯fA2 + 2B2/q =: A′
⩽3|α0|2E[d2
i ] + 3∥θg∥2
∞] + 3E[ζ2
2 + 12A2B2 + 3B2/q =: A3
max1⩽j⩽p E[x2
i ] ⩽B2(A′
2 ∨A3) =: A4
max1⩽j⩽p E[|xijǫi|3]
i |] ⩽B3B3/q =: A5
max1⩽j⩽p 1/E[x2
⩽1/λmin(E[xix′
i]) ⩽1/f =: A6
where we used that maxi⩽n ∥xi∥∞⩽B, the moment assumptions of the disturbances, ∥θm∥⩽
∥θm∥1 ⩽2A, ∥θg∥1 ⩽2A since θm, θg ∈Sa
A(p) for a > 1.
Thus the ﬁrst requirement in
Condition SM(i) holds with CSM
= A2. Condition SM(ii) holds with CSM
:= A1 + (A′
A3) + A4 + A5 + A6.
To verify Condition SM(iv) note that for ǫi = vi and ǫi = ζi we have by Lemma 6 with
probability 1 −8τ, where τ = 1/ log n,
1⩽j⩽p |(En −¯E)[x2
2 log(2p/τ)
i ], 1 −τ) ∨
2 log(2p/τ)
i ], 1 −τ) ∨2B2√
2 log(2p log n)
B2B2/q log n =: δSM
where we used E[ǫ4
i ] ⩽B4/q and the Markov inequality. By the deﬁnition of τ and the assumed
rate log3(p ∨n)/n ⩽¯δn →0, we have δSM
BELLONI CHERNOZHUKOV HANSEN
Similarly, we have for ˜yi = di and ˜yi = yi, with probability 1 −8τ
1⩽j⩽p|(En −¯E)[x2
2 log(2p/τ)
i ], 1 −τ) ∨
2 log(2p/τ)
i ], 1 −τ) ∨
2 log(2p log n)
B2A7 log n =: δSM
where we used the Markov inequality and
i ] + 33|α0|4E[d4
i ] + 33∥θg∥4
∞] + 33E[ζ4
+ 33B4A4/q
+ 33(2A)4B4 + 33B4/q =: A7.
By the deﬁnition of τ and the assumed rate log3(p ∨n)/n ⩽¯δn →0, we have δSM
The last term in the requirement of Condition SM(iv), because maxi⩽n ∥xi∥∞⩽B and
Condition ASTE(iv) holds, is bounded by δSM
:= B2A1/an1/2a log(p ∨n)/n →0.
Finally, we set c = cSM
, C = max{CASTE
max{¯δn, δASTE
3n } →0, ∆n = max{16/ log n, ∆SE
Appendix G. Tools
G.1. Moderate Deviations for a Maximum of Self-Normalized Averages. We shall be
using the following result, which is based on Theorem 7.4 in .
Lemma 4 (Moderate Deviation Inequality for Maximum of a Vector). Suppose that
where Uij are independent variables across i with mean zero. We have that
1⩽j⩽p |Sj| > Φ−1(1 −γ/2p)
where A is an absolute constant, provided for ℓn > 0
0 ⩽Φ−1(1 −γ/(2p)) ⩽n1/6
i=1 E[|U 3
The proof of this result, given in Belloni, Chen, Chernozhukov, and Hansen , follows
from a simple combination of union bounds with the bounds in Theorem 7.4 in de la Pe˜na,
Lai, and Shao .
INFERENCE AFTER MODEL SELECTION
G.2. Inequalities based on Symmetrization. Next we proceed to use symmetrization arguments to bound the empirical process. In what follows for a random variable Z let Q(Z, 1−τ)
denote its (1 −τ)-quantile.
Lemma 5 (Maximal inequality via symmetrization). Let Z1, . . . , Zn be arbitrary independent
stochastic processes and F a ﬁnite set of measurable functions. For any τ ∈(0, 1/2), and
δ ∈(0, 1) we have that with probability at least 1 −4τ −4δ
f∈F |Gn(f(Zi))| ⩽
2 log(2|F|/δ) Q
En[f(Zi)2], 1 −τ
|Gn(f(Zi))|, 1
Proof. Let
2 log(2|F|/δ) Q
En[f(Zi)2], 1 −τ
|Gn(f(Zi))|, 1
and the event E = {maxf∈F
En [f 2(Zi)] ⩽Q
En[f 2(Zi)], 1 −τ
} which satisﬁes
P(E) ⩾1 −τ. By the symmetrization Lemma 2.3.7 of van der Vaart and Wellner (by
deﬁnition of e2n we have βn(x) ⩾1/2 in Lemma 2.3.7) we obtain
P {maxf∈F |Gn(f(Zi))| > 4e1n ∨2e2n}
⩽4P {maxf∈F |Gn(εif(Zi))| > e1n}
⩽4P {maxf∈F |Gn(εif(Zi))| > e1n|E} + 4τ
where εi are independent Rademacher random variables, P(εi = 1) = P(εi = −1) = 1/2.
Thus a union bound yields
f∈F |Gn(f(Zi))| > 4e1n ∨2e2n
⩽4τ + 4|F| max
f∈F P {|Gn(εif(Zi))| > e1n|E} .
We then condition on the values of Z1, . . . , Zn and E, denoting the conditional probability
measure as Pε. Conditional on Z1, . . . , Zn, by the Hoeﬀding inequality the symmetrized process
Gn(εif(Zi)) is sub-Gaussian for the L2(Pn) norm, namely, for f ∈F, Pε{|Gn(εif(Zi))| > x} ⩽
2 exp(−x2/{2En[f 2(Zi)]}). Hence, under the event E, we can bound
Pε {|Gn(εif(Zi))| > e1n|Z1, . . . , Zn, E}
1n/[2En[f 2(Zi)])
2 exp(−log(2|F|/δ)).
Taking the expectation over Z1, . . . , Zn does not aﬀect the right hand side bound. Plugging in
this bound yields the result.
The following specialization will be convenient.
Lemma 6. Let τ ∈(0, 1) and {(x′
i, ǫi)′ ∈Rp × R, i = 1, . . . , n} be random vectors that are
independent across i. Then with probability at least 1 −8τ
1⩽j⩽p |En[x2
i ] −¯E[x2
2 log(2p/τ)
1⩽j⩽pEn[x4
BELLONI CHERNOZHUKOV HANSEN
Proof. Let Zi = xiǫi, fj(Zi) = x2
i , F = {f1, . . . , fp}, so that n−1/2Gn(fj(Zi)) = En[x2
i ]. Also, for τ1 ∈(0, 1/2) and τ2 ∈(0, 1), let
2 log(2p/τ1)
1⩽j⩽p En[x4
i ], 1 −τ2
and e2n = max
1⩽j⩽p Q(|Gn(x2
i )|, 1/2)
where we have e2n ⩽max1⩽j⩽p
i ] by Chebyshev.
By Lemma 5 we have
1⩽j⩽p|En[x2
i ] −¯E[x2
i ]| > 4e1n ∨2e2n
⩽4τ1 + 4τ2.
The result follows by setting τ1 = τ2 = τ < 1/2. Note that for τ ⩾1/2 the result is trivial.
G.3. Moment Inequality. We shall be using the following result, which is based on Markov
inequality and .
Lemma 7 (Vonbahr-Esseen’s LLN). Let r ∈ , and independent zero-mean random variables Xi with ¯E[|Xi|r] ⩽C. Then for any ℓn > 0
> ℓnn−(1−1/r)
G.4. Matrices Deviation Bounds. In this section we collect matrices deviation bounds.
We begin with a bound due to Rudelson for the case that p < n.
Lemma 8 ). Let xi, i = 1, . . . , n, be independent random
vectors in Rp and set
1⩽i⩽n ∥xi∥2].
for some universal constant ¯C. Then, we have
(α′xi)2 −E[(α′xi)2]
n + δn sup
¯E[(α′xi)2].
Based on results in Rudelson and Vershynin , the following lemma for bounded regressors was derived in the supplementary material of Belloni and Chernozhukov 
Lemma 9 ). Let xi, i = 1, . . . , n,
be independent random vectors in Rp be such that
E[max1⩽i⩽n ∥xi∥2∞] ⩽K.
k log(1 + k)
log(p ∨n)√log n
/√n, where ¯C is the universal constant. Then,
∥α∥0⩽k,∥α∥=1
(α′xi)2 −E[(α′xi)2]
∥α∥0⩽k,∥α∥=1
¯E[(α′xi)2].
INFERENCE AFTER MODEL SELECTION
Proof. Let
∥α∥0⩽k,∥α∥=1
(α′xi)2 −E[(α′xi)2]
Then, by a standard symmetrization argument , page 804)
sup∥α∥0⩽k,∥α∥=1
i=1 εi(α′xi)2
∥α∥0⩽k,∥α∥⩽1
En[(α′xi)2] and ϕ(k) =
∥α∥0⩽k,∥α∥=1
¯E[(α′xi)2],
we have φ(k) ⩽ϕ(k) + Vk and by Lemma 3.8 in Rudelson and Vershynin to bound the
expectation in ε,
k log(1 + k)
log(p ∨n)√log n
maxi⩽n ∥xi∥∞
k log(1 + k)
log(p ∨n)√log n
Ex [maxi⩽n ∥xi∥2∞] Ex [φ(k)]
k log(1 + k)
log(p ∨n)√log n
ϕ(k) + E[Vk].
The result follows by noting that for positive numbers v, A, B, v ⩽A(v + B)1/2 implies v ⩽
The following result establishes an approximation bound for sub-Gaussian regressors and
was developed in Rudelson and Zhou . Recall that a random vector Z ∈Rp is isotropic
if E[ZZ′] = I, and it is called ψ2 with a constant α if for every w ∈Rp we have
∥Z′w∥ψ2 := inf{t : E[exp( (Z′w)2/t2)] ⩽2} ⩽α∥w∥2.
Lemma 10 ). Let Ψi, i = 1, . . . , n,
be i.i.d. isotropic random vectors in Rp that are ψ2 with a constant α. Let xi = Σ1/2Ψi so that
Σ = E[xix′
i]. For m ⩽p and τ ∈(0, 1) assume that
Then with probability at least 1 −2 exp(−τ 2n/80α4), for all u ∈Rp, ∥u∥0 ⩽m, we have
(1 −τ)∥Σ1/2u∥2 ⩽
iu)2] ⩽(1 + τ)∥Σ1/2u∥2.
For example, Lemma 10 covers the case of xi ∼N(0, Σ) by setting Ψi ∼N(0, I) which is
isotropic and ψ2 with a constant α =
BELLONI CHERNOZHUKOV HANSEN