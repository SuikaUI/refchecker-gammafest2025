REVIEW VERSION
Noname manuscript No.
(will be inserted by the editor)
Using Basic Image Features for Texture Classiﬁcation
M Crosier · L D Griﬃn
Received: date / Accepted: date
Abstract Representing texture images statistically as
histograms over a discrete vocabulary of local features
has proven widely eﬀective for texture classiﬁcation tasks.
Images are described locally by vectors of, for example,
responses to some ﬁlter bank; and a visual vocabulary is
deﬁned as a partition of this descriptor-response space,
typically based on clustering. In this paper, we investigate the performance of an approach which represents
textures as histograms over a visual vocabulary which
is deﬁned geometrically, based on the Basic Image Features of , rather than by clustering. BIFs provide a natural mathematical quantization of a ﬁlter-response space into qualitatively distinct
types of local image structure. We also extend our approach to deal with intra-class variations in scale. Our
algorithm is simple: there is no need for a pre-training
step to learn a visual dictionary, as in methods based
on clustering, and no tuning of parameters is required
to deal with diﬀerent datasets. We have tested our implementation on three popular and challenging texture
datasets and ﬁnd that it produces consistently good
classiﬁcation results on each, including what we believe
to be the best reported for the UIUCTex and KTH-
TIPS databases.
Keywords Texture classiﬁcation · Basic Image
Features · textons
Computer Science, University College London, Gower Street,
London WC1E 6BT, UK
Tel.: +44 20 7679 7214
E-mail: 
Computer Science, University College London
E-mail: 
1 Introduction
Eﬀective general-purpose analysis of texture in images
is an important step towards a variety of computer vision applications, from industrial inspection to scene
and object recognition. Its challenge lies in the wide
variety of possible textures (ranging in nature from regular to stochastic and in origin from albedo variations
to 3D surface structure) and conditions under which
they are imaged .
Images are probed locally by considering, for example,
the responses to a ﬁlter bank or the greyscale values of a
local image patch. These descriptor responses are then
assigned to discrete bins according to some partition of
the feature space.
This model encompasses two approaches to image
representation. In the ﬁrst , every image in the dataset is represented
as a histogram over a common dictionary and some
form of histogram comparison measure is used to compare images. This dictionary is most often deﬁned by
a once-and-for-all clustering of feature vectors from a
REVIEW VERSION
subset of images from the dataset, as described below.
The second approach uses a separate dictionary for each image and
represents the image as a ‘signature’: a table of feature
deﬁnitions (e.g. cluster centres) with the corresponding numbers of occurrences in the image. Image signatures are compared using a measure such as the Earth
Mover’s Distance. This dictionary is most often deﬁned
by clustering feature vectors from the single image to
be represented.
Various classiﬁcation schemes have been explored
for both of these approaches, from nearest-neighbour
matching to kernel-based SVMs . Although the superiority of SVMs
for texture classiﬁcation has been clearly demonstrated
 , nearest-neighbour is still often used as an uncommitted mechanism to compare texture representations
due to its simplicity and absence of parameters that
need to be tuned.
Of these three dimensions of statistical texture representation – the choice of histogram or signature representation; the descriptive space over which the histogram bins are deﬁned; and the actual choice of histogram bins – the ﬁrst two have been well-studied.
The relative merits of histogram- and signature-based
approaches are explored in tandem with classiﬁcation
schemes, and a variety of local descriptors have been
proposed including:
– The joint responses of various ﬁlter banks , made up of
e.g. Gaussian derivative ﬁlters .
– Grey-scale image patches 
or points sampled in some regular local conﬁguration ; and the related notion
of Markov Random Fields and intensity domain
SPIN images worked around this problem by
limiting the number of ﬁlters used and adaptively calculating bin widths for each dimension based on data
from the training set, but limitations of this kind remain undesirable.
The solution to this problem which has come to to
dominate involves controlling the number of bins by
deﬁning a partition of feature space through unsupervised clustering of feature vectors into textons . Local descriptors
calculated from a number of training images for a given
texture class are used to populate a feature space which
is partitioned into a pre-selected number (typically 10-
40) of regions, each represented by a cluster-centre. This
is repeated for each texture class in the dataset and the
combined list of cluster-centres (containing perhaps 250
to 2500 elements, depending on the clustering parameters and number of texture classes) used to Voronoi
partition feature space, by labeling new descriptor vectors according to the nearest cluster-centre in feature
Varma and Zisserman 
investigated reducing redundancy in this representation
by combining textons whose cluster-centres fall close
to each other in feature space. This produces a slight
degradation of classiﬁcation performance, as does learning textons from only a subset (around half) of the total
classes in a dataset.
This unsupervised clustering step almost universally
employs the k-means algorithm. Jurie and Triggs noted that k-means produces poor
dictionaries of features for describing natural images
(for which similar descriptions to those used for texture have been studied) because of the highly nonuniform distribution of descriptor responses. This results
in most k-means cluster-centres being concentrated in
high-density regions of feature space, with Voronoi cells
radiating outwards, so that the assignment of labels to
potentially informative mid-frequency (of occurrence)
descriptor responses is dominated by less informative
(and potentially noisy) high-frequency responses. Although this non-uniformity is less severe for texture images (which may be one of the reasons why unadorned
REVIEW VERSION
bag-of-words representations have proved more sucessful in this domain), the problems with k-means still
apply – including the question of how to choose a suitable value of k. compares kmeans with an acceptance-radius based clusterer for
visual dictionary generation and demonstrates signiﬁcant improvements in object classiﬁcation results from
the latter.
There are other more general problems with schemes
which use unsupervised clustering to generate a feature
dictionary. The need to populate feature space suﬃciently to allow clustering still imposes restrictions on
the choice of local description, although this can be
ameliorated by sampling descriptions from a greater
number of training images. More problematic is the cost
of performing a nearest neighbour computation to assign each new descriptor response – at every point in
an image – to a texton.
1.2 Keypoint detection as feature space quantization
Specifying the quantization of feature space used to de-
ﬁne a visual dictionary can also be seen as encompassing the choice of how to sample features from an image,
which is often described as an additional dimension of
statistical texture representation.
An image representation histogram can be populated from the image either densely (considering every
point), or from keypoints only ). Detectors used to select these
keypoints are generally tuned to local aspects of the image diﬀerent than the descriptors. A dual way of contrasting these two approaches is as alternative partitions of some feature space. Consider a feature space
consisting of the joint response of i) the descriptor and
ii) the information used in the keypoint detector, e.g.
in the case of the Harris corner detector, x- and yderivatives at each point in a local window . Then, in the same way that methods
which describe an image densely correspond to a dense
(generally Voronoi) partitioning of feature space, those
using keypoint detection assign labels only to those
points which fall within an appropriate sub-region of
feature space as determined by the rules of the keypoint detector, ignoring the remainder, i.e. a non-dense
partition is induced. That is, detecting keypoints in an
image can be seen as equivalent to performing some
form of implicit feature selection in this joint response
1.3 A geometrically deﬁned partition of feature space
In this paper, we investigate the classiﬁcation performance of an approach which represents textures as histograms over a feature dictionary which is deﬁned mathematically – by the type of local geometry – rather than
by clustering.
We describe an image locally at some scale using
a family of six Gaussian derivative ﬁlters and base our
visual dictionary on the partition of this response space
deﬁned by the Basic Image Features of . The idea is to assign each ﬁlter response
vector to one of a set of Basic Image Features (BIFs),
each corresponding to a qualitatively diﬀerent type of
local geometric structure, based on a study of types of
local symmetry (see section 2). In our current scheme
there are seven such BIFs which are calculated mathematically by deciding which of seven simple combinations of ﬁlter response values is largest.
As well as avoiding the problems inherent in using
k-means clustering, our approach has the advantages
over clustering methods of simplicity – there is no need
for a pre-training step to learn a visual dictionary – and
computational eﬃciency, since we assign ﬁlter responses
to histogram bins without needing to perform a nearestneighbour computation.
1.4 Related work
Statistical texture representations which are based on
visual dictionaries derived by clustering feature vectors
are discussed above.
One approach which, like ours, provides a datasetindependent dictionary of local features over which textures are represented statistically, is Local Binary Patterns (LBPs) . Images are probed locally by sampling greyscale values at a point gc and P
points g0, . . . , gP −1 spaced equidistantly around a circle of radius R (the choice of which acts as a surrogate
for controlling the scale of description) centred at gc,
as shown in ﬁgure 1a. The resulting feature space of
P + 1 greyscale values can be partitioned according to
one of a nested set of progressively more invariant LBP
– The ﬁrst deﬁnes Local Binary Patterns themselves.
The greyscale value at gc is subtracted from those
at g0, . . . , gP −1 and the resulting values thresholded
about zero to produce a Local Binary Pattern (as in
ﬁgure 1b), LBPP,R, given by sign[g0−gc], . . . , sign[gP −1−
gc], which is by deﬁnition invariant to any monotonic greyscale transformation.
REVIEW VERSION
Fig. 1 Local Binary Patterns. a) Sampling points from the image, with P = 8, R = 1. b) Binarisation to get LBP8,1. c) The
set of Uniform patterns LBPriu2
– Rotation invariance is built in by factoring out cyclic
relabelling of g0, . . . , gP −1, i.e. representing each group
of LBPs which are equal under some cyclic relabelling of g0, . . . , gP −1 by a single canonical LBP
(denoted LBPri
– Since the dimensionality of the representation (which
grows exponentially with P) is still high, a form of
feature selection based on complexity is employed.
Uniform LBPs (LBPriu2
P,R ) are those (rotationally invariant) patterns which contain at most two transitions between 0 and 1, as shown in ﬁgure 1c. In many
cases, the majority of patterns observed in texture
images are classiﬁed as one of these P + 1 Uniform
LBPs. All other LBPs are grouped together into a
single ‘other’ category, producing a P + 2 dimensional representation.
LBPs are similar to our approach in that they are
based upon a pre-deﬁned visual dictionary rather than
one derived with reference to the dataset to be analysed. They therefore share the advantages listed above
over methods based on clustering. They also possess
similar invariances to our method. The central diﬀerence results from the local description used: we probe
an image locally using Gaussian derivative ﬁlters where
as LBPs sample greyscale values. This allows us to make
use of some powerful mathematical properties of Gaussian derivatives in order to study the local geometry of
the image in a way that allows a more geometrically
rigorous treatment of invariances and partitioning of
feature space. For example, the steerability of Gaussian derivative ﬁlters allows
us to achieve exact rotation invariance rather than the
approximate rotation invariance of LBPs.
Fig. 2 Our ﬁlter bank, consisting of one zeroth-order, two ﬁrst
order and three second order Gaussian derivative ﬁlters, all at
the same scale. We refer to the vector of responses as a local jet.
The remainder of the paper is structured as follows:
In section 2 we introduce Basic Image Features and our
BIF-based texture representation. In section 3 we evaluate this approach against a selection of state-of-the-art
alternatives on a commonly used texture dataset. In
section 4 we extend our approach to incorporate scale
invariance: this involves extending our representation
and developing a multi-scale texture comparison metric for classiﬁcation. Results are presented on two additional datasets which contain signiﬁcant intra-class
changes in scale.
2 Basic Image Features (BIFs)
Basic Image Features are deﬁned by a partition of the
ﬁlter-response space (jet space) of a set of six Gaussian
derivative ﬁlters (Figure 2). This set of ﬁlters describes
an image locally up to second order at some scale.
Jet space is partitioned into seven regions – which
we refer to as BIFs – each corresponding to one of
seven qualitatively distinct types of local image structure, based on symmetry types (ﬁgure 3). Algorithm 1
deﬁnes this partition by assigning a given ﬁlter response
vector to one of the seven BIFs. An example of an image ‘labelled’ with BIFs in this way is given in ﬁgure
There are two stages to the derivation of this partition. In the ﬁrst, information which is intrinsic to the
local structure of the scene is separated from ‘extrinsic’ information resulting from uninteresting changes
in imaging setup. In the second, this intrinsic component is quantized into regions corresponding to diﬀerent
types of local image symmetries.
The transformations which are considered uninteresting for the purpose of calculating BIFs are rotations,
reﬂections, intensity multiplications and addition of a
REVIEW VERSION
1. Measure ﬁlter responses cij, and from these calculate the
scale-normalised ﬁlter responses sij = σi+jcij
2. Compute λ = s20 + s02, γ =
(s20 −s02)2 + 4s2
3. Classify
01, ±λ, 2−1
2 (γ ± λ), γ
Algorithm 1: Calculation of BIFs. The single parameter ε controls what amplitude of structure is tolerated
before a region is no longer considered suﬃciently uniform to be assigned to the ‘ﬂat’ (pink) BIF category
(see ﬁgure 3), and is given another label.
, 2
Fig. 3 Top: Stereotypical image patches demonstrating the type
of structure / symmetry represented by each of the seven BIFs
deﬁned by step 3 of Algorithm 1. Bottom: An image of bark from
the UIUCTex database , densely labelled
with BIFs computed at scales σ = 1 and σ = 4 (both with ε = 0),
according to the colours of the key above, in order to show where
diﬀerent BIFs occur in a real-world texture image.
constant intensity. Jet space is factored 
by these extrinsic transformation groups to produce an
intrinsic component in which all ﬁlter responses diﬀering only in one of these extrinsic factors are mapped to
the same point. Any partition of this intrinsic component will therefore produce a set of features which are
invariant to rotations, reﬂections and these grey-scale
transformations.
The partition of the intrinsic component of jet space
which deﬁnes the Basic Image Features is based on deciding which type of symmetry of the local image geometry is most nearly consistent with the local jet.
A test has been developed which
shows whether a ﬁlter is sensitive to a certain local
symmetry, i.e. whether it is able to detect invariance under a group of transformations (a prospective automorphism group). The type of transformations considered
are image isometries : spatial isometries
combined with intensity isometries. The possible automorphism groups of 2D images relative to the class
of image isometries, excluding cases containing discrete
periodic translations, have been determined. Hence we
can use our test to decide which ﬁlters in the span of the
second order Gaussian derivative family of ﬁgure 2 (i.e.
which linear combinations of the ﬁlters) are sensitive to
each of these symmetries. This allows the regions of the
intrinsic component of jet space which represent each
type of image symmetry to be identiﬁed.
Since most image structures are not perfectly symmetrical, we base our partitioning scheme on deciding
which symmetry most approximately holds. By selecting an appropriate subset of symmetry types (which
deals with the problem of some automorphism groups
being subgroups of others) and partitioning the intrinsic
component into Voronoi cells around their corresponding regions using a metric induced by the ﬁlter response
space , we achieve this approximate symmetry classiﬁcation.
2.1 A BIF-based texture representation
By providing a natural quantization of ﬁlter response
space into qualitatively distinct types of local image
structure, with an appropriate set of in-built invariances, BIFs oﬀer a basis for a viable mathematical alternative to visual dictionaries based on clustering. As
discussed above, the advantages of this include avoidance of biases introduced by the clustering algorithm;
elimination of a clustering pre-training step; and computational eﬃciency since image locations are classiﬁed
into BIFs simply, using algorithm 1, rather than by a
costly nearest-neighbour computation.
However, simply modelling an image as a histogram
over our 7 categories produces too coarse a representation. Using a simple 7-bin BIF-histogram texture representation and the classiﬁcation framework of section 3,
only 65% of images from the CUReT dataset are classiﬁed correctly; state-of-the-art approaches score in the
high nineties percent (see sections 3 and 4). We need
a way of combining this seven letter ‘alphabet’ into a
suﬃciently descriptive collection of ‘words’ to make up
our dictionary.
One way to achieve this is to look at local conﬁgurations of BIFs, i.e. how the type of local structure
in the image changes with location and/or scale. The
conﬁguration which we evaluate in this paper is a stack
of BIFs calculated, at the same spatial location, across
four octave-separated scales. We refer to these ‘scale
templates’ as BIF-columns, and deﬁne σbase to be the
ﬁnest scale in a BIF-column. Informally, we have found
that this selection of four scales seems to produce a representation which captures the right trade-oﬀbetween
speciﬁcity and generality. By considering how BIFs vary
REVIEW VERSION
over scale, rather than space, we retain the rotationinvariance of BIFs, which has been shown to be advantageous for texture classi-
The single parameter ε of Algorithm 1 controls how
much ‘noise’ is tolerated before a region is no longer
considered suﬃciently uniform to be assigned to the
‘ﬂat’ (pink) BIF category, and is given another label.
For texture analysis we do not want any ‘ﬂattening’ of
potentially informative low-contrast structure and so
we set ε = 0, with the result that this BIF is never
selected. Hence we reduce our alphabet to six letters,
resulting in a 64 = 1296 dimensional representation by
BIF-columns. In practise this also means that we need
not compute responses to our zeroth order ﬁlter, so assignment of image points to BIF-columns is fully determined by the responses of 5 × 4 = 20 ﬁlters.
We populate our histogram by counting occurrences
of BIF-columns at every pixel in an image, rather than
at keypoints. Further, we include description at points
which are too close to the edge of the image to accommodate the full spatial support of the ﬁlters. Where
full support is unavailable, we compensate by wrapping
around to the opposite edge of the image. Traditionally, this ought to decrease the accuracy of our models.
However, we have observed the opposite: that removing
edge-points from our description degrades classiﬁcation
performance to a similar degree to removing the same
number of points at locations randomly sampled from
across the image. We oﬀer the explanation that this
result is a combination of (i) the eﬀects of poorer sampling when these points are removed, with (ii) suﬃcient
homogeneity in the images which we have analysed so
that they can reasonably be treated as cyclical.
Thus our texture representation at scale σbase comprises:
1. Compute a stack of four BIF-images at scales σbase, 2σbase, 4σbase, 8σbase
by convolving the image with a second-order family of Gaussian derivative ﬁlters and applying Algorithm 1 (with ε = 0). Transpose to form an array of
BIF-columns representing each image pixel.
2. Populate a 1296-bin histogram representation by
counting occurrences of BIF-columns.
3 Evaluation
We test our BIF-column texture representation by classifying images from the CUReT dataset . CUReT consists of 61 texture classes each containing 205 images of a physical texture sample photographed under a (calibrated) range of viewing and
lighting angles, but without signiﬁcant variation in scale
or in-plane rotation. CUReT is a challenging test of local image description because of the signiﬁcant intraclass changes in appearance resulting from varying directional light falling on the 3D texture samples. In line
with other classiﬁcation studies using CUReT, we consider only the 92 images per class which aﬀord the extraction of a 200x200 pixel foreground region of texture.
Since our focus is on representation, we use a simple nearest-neighbour classiﬁer rather than a more sophisticated classiﬁer such as support vector machines
which has been shown to produce superior results 
but requires more tuning of parameters. The classiﬁer is
trained by computing representation histograms of all
images in the training set; and a novel image classiﬁed
according to the shortest distance from its representation to each stored training histogram. The most commonly used histogram comparison metric for this purpose is the χ2 statistic, although others such as a loglikelihood measure have been used .
We employ a simpliﬁed form of the Bhattacharyya distance, 1 −√g.
h, which is theoretically better suited
than χ2 to calculating distances between distant points
in high dimensional space . However, we have also experimented with the χ2 metric in a
limited set of experiments and have found no signiﬁcant
diﬀerence in the results produced. One possible cause
for this is that in a nearest-neighbour classiﬁer all but
the smallest distances are eﬀectively ignored and, for
small distances, the Bhattacharyya measure approximates the χ2 measure .
For our BIF-column representation, we set the single
scale parameter σbase = 1 (a multi-scale approach is
developed in Section 4).
We compare histograms of BIF-columns with four
other state-of-the-art histogram representations, using
the same classiﬁcation framework in each case. These
VZ-MR8 (610 textons) : After being grey-scale normalised, images are probed
locally using the (normalised) MR8 ﬁlter bank, which
consists of a Gaussian; a Laplacian of Gaussian; and
collections of elongated ﬁrst order and second order Gaussian derivative ﬁlters, each at three scales
and six orientations of which only the response with
greatest magnitude at each scale is recorded. Thus
ﬁlter response vectors are eight dimensional in total
(although 38 ﬁlters are computed in their calculation), are approximately invariant to rotation and,
like BIF-columns, describe the local deep structure
of an image. To generate a dictionary of textons,
ﬁlter responses densely sampled from 13 randomly
selected images per texture class are clustered using
REVIEW VERSION
BIF-columns
VZ-MR8 (2440 textons)
VZ-MR8 (610 textons)
VZ-Joint 7x7
Proportion of test images classified correctly
Number of training images per class
Fig. 4 The mean proportion of correctly classiﬁed images over 100 random splits of the CUReT dataset into training/test data, for
a range of training set sizes. The best result for BIF-columns (with 43 training images per class) is 98.1±0.3%.
k-means to produce 10 cluster-centre textons per
class. Aggregated over the 61 CUReT classes, these
610 textons Voronoi-partition feature space.
VZ-MR8 (2440 textons) 
: As VZ-MR8 (610 textons) above, except that 40
cluster-centre textons are learnt per CUReT category resulting in a 2440 dimensional representation.
VZ-Joint 7x7 : After being grey-scale normalised, images are described locally by the collected grey-scale values of a 7x7 pixel
image patch. The resulting 49-dimensional feature
space is partitioned into 610 textons using clustering in the same way as for VZ-MR8 (610 textons).
24,3 : Rotation-invariant uniform
Local Binary Patterns as described in Section 1.4,
with 24 points sampled around a circle of radius 3
pixels, resulting in a 26-dimensional representation.
Note the low-dimensionality of this representation
compared to the others tested.
Each of the ﬁve (including BIF-columns) representations which we test contain some degree of invariance
to grey-scale transformations. For the VZ methods, this
is a global (per image) rather than local invariance, although the normalisation of ﬁlter responses will add
some degree of local invariance as well. BIF-columns
are invariant to additions and linear multiplications of
intensity, while LBPs are invariant to any monotonic
grey-scale transformations. Similarly, with the one exception of VZ-Joint, each representation exhibits some
degree of rotation-invariance. VZ-MR8 and LBPs are
invariant to small discrete rotations and hence approximately invariant to continuous rotations, while BIFcolumns are fully invariant to continuous rotations.
Our classiﬁcation task consists of training with a
given number of images randomly chosen from each texture class and assigning all of the remaining images to
one of the 61 categories. We repeat this experiment with
100 diﬀerent random selections of training and test data
 ) and report the mean fraction
of images correctly classiﬁed along with the standard
deviation. Figure 4 shows results for a range of training
set sizes.
First, note that the performance ranking of the ﬁve
representations tested remains the same regardless of
the number of images in the training set. This can
be seen as conﬁrming the uncommitted nature of the
nearest neighbour classiﬁer used with each of the representations. BIF-columns score highest, followed by the
two MR8 based representations (with the richer 2440bin representation slightly superior) and then 7x7 image patches. The performance of uniform Local Binary
Patterns is signiﬁcantly below those of the other approaches for all but the smallest collections of training
images. However, it should be noted that this representation is only 26-dimensional, compared to a mini-
REVIEW VERSION
mum of 610 dimensions for other methods. This reﬂects
its design goal of being able to cope with smaller images: fewer bins produce a less precise representation
but one which can be populated more accurately when
the quantity of data available is a limiting factor. However, the proximity of the two MR8-based approaches
suggest that the dimensionality of representation is not
a major cause of variation in performance between the
other four (more consonant) representations.
The relative similarity in performance of the best
four methods for large numbers of training images begs
the question of whether we are pushing against a ceiling
of a minority of images which are particularly diﬃcult
for histogram-based texture representations to cope with.
Figure 6 suggests that this is not the case: although
there is some correlation between the distributions of
images misclassiﬁed by diﬀerent representations, in the
majority of cases it is fairly weak, i.e. in general diﬀerent
representations mis-classify diﬀerent images. One notable exception to this is the strong correlation between
the two representations using the same (MR8) local description, which diﬀer only in the number of cells into
which their feature spaces are partitioned. The particular types of texture which appear problematic for each
representation defy easy characterization (ﬁgure 5).
4 Multi-scale histogram matching
Although our representation describes the local deep
structure in an image, it is not scale invariant. The
scale of the base of our BIF-columns, σbase, remains
ﬁxed. In order to be able to usefully describe sets of
textures which, unlike CUReT, contain signiﬁcant variation in scale, we extend our representation and introduce a multi-scale histogram comparison.
There are two related problems which should be addressed in an appropriate scale-treatment of texture.
First, images of the same texture should be recognised
as such despite being taken from diﬀerent distances
(scale invariance). Second, the texture representation
should incorporate description at (and representations
should be compared across) a range of scales as multi-resolution analysis), rather than at one ﬁxed scale which is chosen as
a compromise for the given dataset, or at one intrinsic scale. This ensures (i) that the image is probed at
scales matching those of important local structure in
that image, and (ii) that where (as frequently happens)
images contain informative structure at a number of
scales, full use is made of this information: rather than,
for example, having to choose whether a brick wall is
best represented by the layout of the bricks or the microstructure of the clay.
BIF-columns
VZ-MR8 (610 textons)
VZ-Joint 7x7
VZ-MR8 (2440 textons)
Fig. 5 The (1st, 3rd, 5th, 7th and 9th) most frequently misclassiﬁed images over the 100 trials (top), and the images for which
they were most often mistaken (bottom). For each representation,
some images are perceptually similar to those for which they are
mistaken and some are not.
REVIEW VERSION
(610 textons)
(2440 textons)
VZ-Joint 7x7
BIF-columns
(610 textons)
(2440 textons)
VZ-Joint 7x7
BIF-columns
Fig. 6 Correlation between the marginal distributions by class of incorrectly classiﬁed images (taken across all 100 training/test splits
with 43 training images per class) for each pair of representations. There is strong correlation between the two representations using
the same (MR8) local description, but only weak correlation between other representations. The strongest correlation for LBPs is
with VZ-Joint (although the converse is false). This could be explained by the relative similarity of these two representations in using
greyscale-based descriptions and in the extents of their local regions of support, despite the very diﬀerent forms of their feature space
quantization. Similarly, the strongest correlation for BIF-columns is with the two MR8-based methods, which also probe the image
using Gaussian derivative ﬁlters.
Hayman et al. adopt a pure
learning approach which addresses the ﬁrst of these
problems (and, to an extent, part (i) of the second) by,
in eﬀect, augmenting the training data with artiﬁcially
rescaled versions of the original training images. By decoupling the descriptions of textures at each scale it
makes the implicit assumption that textures need only
be matched at a single dominant intrinsic scale; thus
although it works well for the datasets tested, it may
not extend to the more general problem.
Our approach retains the links between representations of the same texture at diﬀerent scales by modelling an image as a stack of BIF-column histograms
computed over a range of scales (indexed by σbase) in
the same way as for the single-scale representation described in section 2.1. The range of σbases which we
have found to be eﬀective (as a trade-oﬀbetween descriptiveness and computational complexity) increment
in quarter-octaves from 2−1/4 to 23/2 meaning that,
with the four-octave span of our BIF-columns, the total range of scales analysed runs from 0.84 to 22.6 pixels. We emphasize the diﬀerence between BIF-columns
which describe the local variation in structure around
some point in scale space; and histogram stacks which
represent the global variation over scale of the texture
The second of the above criteria is then addressed
by comparing histogram-stack texture representations
using a multi-scale metric, based on the Bhattacharyya
distance, which computes a weighted average of the distances between histograms at each scale. The ﬁrst criterion (scale invariance) is realised by allowing histogram
stacks to be shifted in scale relative to one another before calculation of this distance, as shown in ﬁgure 7
(scale-shifting).
More speciﬁcally, to compare stacks of normalised
BIF-column histograms for images A and B, calculated at column-base scales σbase = σA1, σA2, . . . , σAn
and σB1, σB2, . . . , σBn respectively (see ﬁgure 7, right),
our multi-scale metric calculates a weighted average
of squared Bhattacharyya distances computed at each
pair of base scales (σAi, σBi),
h(A;σAi ).√
h(B;σBi ))
where h(I; σj) is the normalised BIF-column histogram of image I computed at base scale σj and σ2
Bi. The weighting by
Bi discriminates against
REVIEW VERSION
Fig. 7 Multi-scale comparison of images A and B. Left: Scale shifting: Histogram stacks containing n histograms are shifted relative
to each other in scale in each of 2n −1 possible ways, to allow matching of similar features appearing at diﬀerent scales in each image.
Right: The notation used in equation 1.
poorly sampled coarse scale representations. Normalisation commensurates distances for diﬀerently shifted
comparisons, allowing the multi-scale scheme to be incorporated directly into our nearest neighbour classi-
ﬁer: the distance between two images is eﬀectively taken
to be the minimum of the distances calculated between
those images in each of the 2n −1 possible ways illustrated in ﬁgure 7.
4.1 Evaluation
We have tested our multi-scale scheme by classifying
texture images from three datasets: the CUReT dataset
as used in Section 3, KTH-TIPS 
and UIUCTex . We emphasize
that our method is exactly the same for each dataset,
with no tuning of parameters.
The KTH-TIPS dataset extends CUReT by imaging
new samples of 10 of the CUReT textures at a subset of
the viewing and lighting angles used in CUReT but also
over a range of scales, producing 81 200x200 pixel images per class. Although KTH-TIPS is designed in such
a way that it is possible to combine it with CUReT in
testing, we follow in treating it as a
stand-alone dataset. UIUCTex contains 25 classes, each
of 40 640x480 pixel images. The dataset is uncalibrated
and classes contain images taken at a variety of scales
and viewpoints, and sometimes with non-rigid deformations of the samples. However, variations in lighting
geometry are less severe than for the other two datasets.
As in Section 3, results are reported as the mean
proportion of images correctly classiﬁed over 100 random splits into training and test data, along with one
standard deviation. We use 43, 40 and 20 training images per class respectively for CUReT, KTH-TIPS and
Results )
are shown in Table 1. Despite not being modiﬁed to
suit each dataset, our multi-scale BIF-columns scheme
scores well across all three datasets, producing what we
believe to be the best reported results on the UIUCTex
and KTH-TIPS images; and the best reported results
out of those which use a nearest-neighbour classiﬁer
on CUReT. The overall best performance on CUReT is
from Broadhurst’s conference paper ,
which achieved 99.22% correct classiﬁcation using a Gaussian Bayes classiﬁer with marginal ﬁlter distributions.
Analysis of the behaviour of our multi-scale approach
shows that the two component parts – the multi-scale
comparison metric and histogram-stack scale-shifting –
complement each other appropriately (ﬁgure 8). Our
multi-scale metric improves performance over our single scale scheme on both the UIUCTex and CUReT
datasets, conﬁrming that texture comparison at a range
of scales is important even in the absence of signiﬁcant intra-class variation in scale; where as the scaleshifting part of our algorithm is useful only when scaleinvariance is called for. Indeed, for the CUReT data,
distances between matching images are nearly always
smaller when no scale-shifting is used, meaning that, for
these images, our multi-scale algorithm rarely acts any
diﬀerently than if this component were absent (ﬁgure
9). By contrast, shifting occurs frequently for UIUCTex
images. That is, most of the time, scale-shifting is used
only when scale-invariance is called for.
Note in ﬁgure 8 that, on the UIUCTex images, the
method which produces the next-best results to our full
multi-scale scheme is scale-shifting without the multiscale comparison metric, which is the method most similar to Hayman et al.’s approach .
Figure 10 shows examples of images which are misclassiﬁed by our multi-scale scheme.
REVIEW VERSION
Proportion of images classified correctly
Multi-scale
Multi-scale
metric and
Multi-scale
Multi-scale
metric and
Fig. 8 The proportion of images correctly categorized by each of the two components of our multi-scale classiﬁer (the multi-scale
metric and scale-shifting); our full multi-scale classiﬁer (these components combined); and our single scale classiﬁer as evaluated in
section 3. We use 43 training images per CUReT class and 5 training images per UIUCTex class, and report the mean and standard
deviation over 100 trials of the fraction of remaining images correctly classiﬁed. For CUReT, which does not contain signiﬁcant
intra-class variation in scale, there is no beneﬁt to be gained by using scale shifting. However, comparing images at a range of scales
using our multi-scale metric does result in improved performance, suggesting that images contain informative structure at multiple
scales. For UIUCTex, which does contain signiﬁcant intra-class scale variations, both the multi-scale metric and scale-shifting produce
improvements over our single scale classiﬁer, with the combination of the two in our full multi-scale scheme giving the best performance.
Fig. 10 Examples of images from the UIUCTex dataset which are mis-classiﬁed by our multi-scale algorithm (top); the training
images for which they are most often mistaken (centre); and the most frequently corresponding ‘nearest misses’ from the correct class
(bottom). Left to right, the images are the ﬁrst (‘fur’ mistaken for ‘marble’), second (‘bark 2’ mistaken for ‘granite’), third (‘marble’
mistaken for ‘bark 2’), fourteenth (‘bark 3’ mistaken for ‘fur’) and seventeenth (‘brick 1’ mistaken for ‘glass 1’) most frequently misclassiﬁed UIUCTex images, counted over 100 random splits into 20 training and 20 test images per class. Mis-classiﬁed images are
often perceptually similar, on a local level, to those for which they are mistaken, as in the middle three examples. However, the most
frequently mis-classiﬁed image (left) bears little resemblence to the training image selected by our algorithm. The right-most example
demonstrates a lack of sensitivity to the regularity property of the brick texture, a limitation inherent in the representation of images
as histograms.
REVIEW VERSION
Table 1 Classiﬁcation scores on the CUReT, UIUCTex and KTH-TIPS datasets. Scores are as originally reported, except for those
marked † which are taken from the comparative study in .
43 training images
20 training images
40 training images
Multi-scale BIF-columns
Varma & Zisserman - MR8 
Varma & Zisserman - Joint 
78.4±2.0%†
92.4±2.1%†
Hayman et al. 
98.46±0.09%
92.0±1.3%†
94.8±1.2%†
Lazebnik et al. 
72.5±0.7%†
91.3±1.4%†
Zhang et al. 
Broadhurst 
99.22±0.34%
Proportion of correctly classified images
Degree of scale-shifting
Fig. 9 The proportion of images, out of those which are correctly
classiﬁed, in which the nearest training image representation is
found using the given degree of histogram-stack scale-shifting (ﬁg.
7), for CUReT (red) and UIUCTex (black) images. For CUReT,
the distance calculated between histogram-stack representations
after shifting is nearly always larger than the distance calculated
with no shifting, i.e. the closest training image is most frequently
found using no scale-shifting: as is appropriate in the absence
of intra-class scale changes. For UIUCTex, which does contain
intra-class variations in scale, it is more common for a distance
calculated after shifting to be smaller than the distance with no
shifting, resulting in a ﬂatter distribution.
We have developed a statistical texture representation
which models images as histograms over a dictionary of
features which is based on the qualitative type of local
geometric structure, encoded by Basic Image Features,
rather than a dictionary based on clustering. Our features are naturally invariant to rotation and reﬂection,
and addition and linear multiplication of illumination
intensity; and we have extended the approach to incorporate invariance to changes in scale.
Our approach has the advantages over methods which
use clustering of simplicity – there is no need for a pretraining step to learn a visual dictionary – and computational eﬃciency, since we assign feature vectors to
histogram bins without needing to perform a nearestneighbour computation. In addition, it avoids the potential introduction of biases by clustering algorithms
poorly suited to the data.
We have tested our implementation on three popular and challenging texture datasets and ﬁnd that it
produces consistently good classiﬁcation results on each,
including what we believe to be the best reported for
the UIUCTex and KTH-TIPS databases. Further, it
does this without requiring modiﬁcation or tuning of
parameters between datasets.
Acknowledgements EPSRC-funded project ‘Basic Image Features’ EP/D030978/1.