Reinforcement learning in the brain
Psychology Department & Princeton Neuroscience Institute, Princeton University
Abstract: A wealth of research focuses on the decision-making processes that animals and humans employ when selecting actions in the face of reward and punishment. Initially such work stemmed from psychological investigations of conditioned
behavior, and explanations of these in terms of computational models. Increasingly,
analysis at the computational level has drawn on ideas from reinforcement learning, which provide a normative framework within which decision-making can be
analyzed. More recently, the fruits of these extensive lines of research have made
contact with investigations into the neural basis of decision making. Converging evidence now links reinforcement learning to speciﬁc neural substrates, assigning them
precise computational roles. Speciﬁcally, electrophysiological recordings in behaving animals and functional imaging of human decision-making have revealed in the
brain the existence of a key reinforcement learning signal, the temporal difference
reward prediction error. Here, we ﬁrst introduce the formal reinforcement learning
framework. We then review the multiple lines of evidence linking reinforcement
learning to the function of dopaminergic neurons in the mammalian midbrain and
to more recent data from human imaging experiments. We further extend the discussion to aspects of learning not associated with phasic dopamine signals, such
as learning of goal-directed responding that may not be dopamine-dependent, and
learning about the vigor (or rate) with which actions should be performed that has
been linked to tonic aspects of dopaminergic signaling. We end with a brief discussion of some of the limitations of the reinforcement learning framework, highlighting questions for future research.
A fundamental question in behavioral neuroscience concerns the decision-making processes by
which animals and humans select actions in the face of reward and punishment, and their neural
realization. In behavioral psychology, this question has been investigated in detail through the
paradigms of Pavlovian (classical) and instrumental (operant) conditioning, and much evidence
has accumulated regarding the associations that control different aspects of learned behavior. The
computational ﬁeld of reinforcement learning has provided a normative
framework within which such conditioned behavior can be understood. In this, optimal action
selection is based on predictions of long-run future consequences, such that decision making is
aimed at maximizing rewards and minimizing punishment. Neuroscientiﬁc evidence from lesion
studies, pharmacological manipulations and electrophysiological recordings in behaving animals
have further provided tentative links to neural structures underlying key computational constructs
in these models. Most notably, much evidence suggests that the neuromodulator dopamine provides basal ganglia target structures with phasic signals that convey a reward prediction error that
can inﬂuence learning and action selection, particularly in stimulus-driven habitual instrumental
behavior .
From a computational perspective, Pavlovian conditioning is considered as a prototypical instance of prediction learning – learning the predictive relationships between events in the environment such as the fact that the scent of home-cooking usually predicts
a tasty meal . Instrumental conditioning, on the other hand, involves
learning to select actions that will increase the probability of rewarding events and decrease the
probability of aversive events . Computationally, such decision
making is treated as attempting to optimize the consequences of actions in terms of some longterm measure of total obtained rewards (and/or avoided punishments) . Thus,
the study of instrumental conditioning is an inquiry into perhaps the most fundamental form of
rational decision-making. This capacity to select actions that inﬂuence the environment to one’s
subjective beneﬁt is the mark of intelligent organisms, and although animals such as pigeons and
rats are capable of modifying their behaviors in response to the contingencies provided by the
environment, choosing those behaviors that will maximize rewards and minimize punishments
in an uncertain, often changing, and computationally complex world is by no means a trivial
In recent years computational accounts of these two classes of conditioned behavior have drawn
heavily from the framework of reinforcement learning (RL) in which models all share in common the use of a scalar reinforcement signal to direct learning. Importantly, RL provides a
normative framework within which to analyze and interpret animal conditioning. That is, RL
models 1) generate predictions regarding the molar and molecular forms of optimal behavior,
2) suggests a means by which optimal prediction and action selection can be achieved, and 3)
expose explicitly the computations that must be realized in the service of these. Different from
(and complementary to) descriptive models that describe behavior as it is, normative models
study behavior from the point of view of its hypothesized function, that is, they study behavior
as it should be if it were to accomplish speciﬁc goals in an optimal way. The appeal of normative
models derives from two primary sources. First, because throughout evolution animal behavior
has been shaped and constrained by its inﬂuence on ﬁtness, it is not unreasonable to view particular behaviors as optimal or near-optimal adaptations to some set of problems .
This allows for the generation of computationally explicit and directly testable hypotheses about
the characteristics of those behaviors. Second, discrepancies between observed behavior and
the predictions of normative models are often illuminating as they can shed light on the neural
and/or informational constraints under which animals make decisions, or suggest that animals
are, in fact, optimizing something other than what the model has assumed.
Adopting Marr’s famous terminology, normative computational models span both the
computational level in which the problem is deﬁned (as they stem from an objective, such as
maximizing future reward) and the algorithmic level of its principled solution. The relevance
of RL models to human and animal learning and decision-making has recently been strengthened by research linking directly the computational and algorithmic levels to the implementation
level. Speciﬁcally, extracellular recordings in behaving animals and functional imaging of human decision-making have revealed in the brain the existence of a key RL signal, the temporal
difference reward prediction error. In this review we will focus on these links between the theory
of reinforcement learning and its implementation in animal and human neural processing.
The link to the level of a neural implementation requires a (perhaps not obviously motivated)
leap beyond the computer-science realm of RL, into an inquiry of how the brains of animals and
humans bring about complex behavior. We believe that this connection between neuroscience
and reinforcement learning stands to beneﬁt both lines of research, making (at least) two important contributions. First, although behavioral predictions are extremely useful for the purpose of
testing the relevance of RL to animal and human decision-making, neural data provide an important source of support and constraints, grounding the theory in another level of empirical support.
This is especially true for a theory that makes clear predictions about learning – a fundamentally
unobservable process, and its underlying hidden variables (such as prediction errors). Because
different learning processes can lead to similar choice behavior, neural evidence is key to selecting one model of learning over another. Prime examples of this are the arbitration between
different variants of RL based on dopaminergic ﬁring patterns , or the separation versus combination of model-based and model-free approaches to RL
based on lesion studies , which we will discussed below. The fact that animals
and humans clearly solve the RL problem successfully despite severe constraints on real-time
neural computation suggests that the neural mechanisms can also provide a source for new theoretical developments such as approximations due to computational limitations and mechanisms
for dealing with continuous and noisy sensory experience. A second contribution that a wedding
of the computational and algorithmic levels to the neural implementation level allows, which is
of even greater importance, is to our understanding of the neural processes underlying decisionmaking in the normal and abnormal brain. The potential advantages of understanding learning
and action selection at the level of dopamine-dependent function of the basal ganglia can not
be exaggerated: dopamine is implicated in a huge variety of disorders ranging from Parkinson’s
disease, through schizophrenia, major depression, attentional deﬁcit hyperactive disorder etc,
and ending in decision-making aberrations such as substance abuse and addiction. Understanding the computational and algorithmic role of dopamine in learning and action selection is a ﬁrst
step to reversing or treating such unfortunate conditions.
In the following, we ﬁrst introduce the formal RL framework . We then review (in Section 2) the multiple
lines of evidence linking RL to the function of dopaminergic neurons in the mammalian midbrain. These data demonstrate the strength of the computational model and normative framework
for interpreting and predicting a wide range of (otherwise confusing) neural activity patterns.
Section 3 extends these results to more recent data from human imaging experiments. In these
experiments, the combination of RL models of choice behavior and online imaging of wholebrain neural activity has allowed the detection of speciﬁc ‘hidden variables’ controlling behavior
(such as the subjective value of different options) in the human brain. In Section 4, we discuss
aspects of learning not associated with phasic dopamine signals, such as goal directed learning (which may be relatively dopamine-independent) and learning about the vigor (or rate) with
which actions should be performed (whose neural underpinning has been suggested to be tonic
levels of dopamine in the striatum). We conclude with a discussion of some of the limitations of
the RL framework of learning, and highlight several open questions.
Reinforcement learning: Theoretical background
The modern form of RL arose historically from two separate and parallel lines of research. The
ﬁrst axis is mainly associated with Richard Sutton, formerly an undergraduate psychology major, and his doctoral thesis advisor, Andrew Barto, a computer scientist. Interested in artiﬁcial
intelligence and agent-based learning and inspired by the psychological literature on Pavlovian
and instrumental conditioning, Sutton and Barto developed what is today the core algorithms and
concepts of RL . In the second
axis, stemming from a different background of operations research and optimal control, electrical
engineers such as Dimitri Bertsekas and John Tsitsiklis developed stochastic approximations to
dynamic programming methods (which they termed ‘neuro-dynamic programming’), which led
to similar reinforcement learning rules . The fusion of these
two lines of research couched the behaviorally-inspired heuristic reinforcement learning algorithms in more formal terms of optimality, and provided tools for analyzing their convergence
properties in different situations.
The Rescorla-Wagner model
The early impetus for the artiﬁcial intelligence trajectory can be traced to the early days of the
ﬁeld of ‘mathematical psychology’ in the 1950’s, within which statistical models of learning
were considered for the ﬁrst time. In a seminal paper Bush and Mosteller developed
one of the ﬁrst detailed formal accounts of learning. Together with Kamin’s insight
that learning should occur only when outcomes are ‘surprising’, the Bush and Mosteller ‘linear
operator’ model found its most popular expression in the now-classic Rescorla-Wagner model of
Pavlovian conditioning . The Rescorla-Wagner model, arguably the
most inﬂuential model of animal learning to date, explained puzzling behavioral phenomena such
as blocking, overshadowing and conditioned inhibition (see below) by postulating that learning
occurs only when events violate expectations. For instance, in a conditioning trial in which two
conditional stimuli CS1 and CS2 (say, a light and a tone) are presented, as well as an affective
stimulus such as food or a tail-pinch (the unconditional stimulus; US), Rescorla and Wagner
postulated that the associative strength of each of the conditional stimuli V(CSi) will change
according to
Vnew(CSi) = Vold(CSi)+η
In this error correcting learning rule, learning is driven by the discrepancy between what was
predicted (∑iV(CSi) where i indexes all the CSs present in the trial) and what actually happened
(λUS, whose magnitude is related to the worth of the unconditional stimulus, and which quantiﬁes
the maximal associative strength that the unconditional stimulus can support). η is a learning
rate that can depend on the salience properties of both the unconditional and the conditional
stimuli being associated.
At the basis of the Rescorla-Wagner model are two important (and innovative) assumptions or
hypotheses: 1) learning happens only when events are not predicted, and 2) predictions due to
different stimuli are summed to form the total prediction in a trial. Due to these assumptions,
the model could explain parsimoniously several anomalous features of animal learning such as
why an already predicted unconditional stimulus will not support conditioning of an additional
conditional stimulus ; why differently salient conditional stimuli
presented together might become differentially associated with an unconditional stimulus ; and why a stimulus that predicts the absence of an expected
unconditional stimulus acquires a negative associative strength . Furthermore, the model predicted correctly previously unknown phenomena such as over-expectation .
The Rescorla-Wagner model explains a large collection of behavioral data with one elegant learning rule, however, it suffers from two major shortcomings. First, by treating the conditional and
unconditional stimuli as qualitatively different, it does not extend to the important phenomenon
of second order conditioning. In second order conditioning if stimulus B predicts an affective
outcome (say, fruit juice, or electric shock) and stimulus A predicts stimulus B, then stimulus A
also gains reward predictive value. This laboratory paradigm is especially important given the
prevalence of second (or higher) order conditioning in every-day life, a prime example for which
is the conditioning of humans to monetary outcomes, which are second order predictors of a
wide range of affectively desirable unconditional stimuli such as food and shelter. The second
shortcoming of the Rescorla-Wagner rule is that its basic unit of learning is a conditioning trial
as a discrete temporal object. Not only does this impose an experimenter-oriented parsing of
otherwise continuous events, but it also fails to account for the sensitivity of conditioning to the
different temporal relations between the conditional and the unconditional stimuli within a trial
(that is, whether they appeared simultaneously or serially, their order of appearance, and whether
there was a time lag between them).
Temporal Difference learning
To overcome these two problems, Sutton and Barto suggested the temporal difference
learning rule as a model of prediction learning in Pavlovian conditioning. Temporal-difference
(TD) learning is an extension of the Rescorla-Wagner model that also takes into account the
timing of different events. Prima facie the distinctions between the two model are subtle (see
below). However, the differences allow the TD model to account for higher order conditioning
and make it sensitive to the temporal relationships within a learning trial . As will be discussed in section 2, the TD model is also more consistent with ﬁndings
regarding the neural underpinnings of RL.
In TD learning, the goal of the learning system (the ‘agent’) is to estimate the values of different
states or situations, in terms of the future rewards or punishments that they predict. For example,
from a learning standpoint, the TD model assumes that the goal of a rat running in a novel
arena is to learn the value of various positions in the arena in terms of obtaining any available
rewards. One way to do this would be to estimate for each location the average total amount
of reward that the rat could expect to receive in the future, when starting from that location.
This departure from Rescorla and Wagner’s framework, in which predictions are only of the
immediately forthcoming reward, turns out to be key.
In order to formally introduce TD learning, let us depart for the moment from animal conditioning and human decision-making. Consider a dynamic process (called a Markov chain) in which
different states S ∈S follow one another according to some predeﬁned probability distribution
P(St+1|St), and rewards are observed at each state with probability P(r|S). As mentioned, a
useful quantity to predict in such a situation is the expected sum of all future rewards, given the
current state St, which we will call the value of state St, denoted V(St). Thus
rt +γrt+1 +γ2rt+2 +...
where γ ≤1 discounts the effect of rewards distant in time on the value of the current state.
The discount rate was ﬁrst introduced in order to ensure that the sum of future rewards is ﬁnite,
however, it also aligns well with the fact that humans and animals prefer earlier rewards to later
ones, and such exponential discounting is equivalent to an assumption of a constant ‘interest
rate’ per unit time on obtained rewards, or a constant probability of exiting the task per unit
time. The expectation here is with respect to both the probability of transitioning from one state
to the next, and the probability of reward in each state. From this deﬁnition of state values it
follows directly that
E [rt|St]+γE [rt+1|St]+γ2E [rt+2|St]+... =
E [rt|St]+γ ∑
P(St+1|St)(E [rt+1|St+1]+γE [rt+2|St+1]+...) =
P(r|St)+γ ∑
P(St+1|St)V(St+1)
(assuming here for simplicity that rewards are Bernoulli distributed with a constant probability
P(r|St) for each state). This recursive relationship or consistency between consecutive state
values lies at the heart of TD learning. The key to learning these values is that the consistency
holds only for correct values (ie, those that correctly predict the expected discounted sum of
future values). If the values are incorrect, there will be a discrepancy between the two sides of
the equation, which is called the temporal difference prediction error
δt = P(r|St)+γ ∑
P(St+1|St)V(St+1)−V(St).
This prediction error is a natural ‘error signal’ for improving estimates of the function V(St). If
we substitute this prediction error for the ‘surprise’ term in the Rescorla-Wagner learning rule,
V(St)new = V(St)old +η·δt,
which will update and improve the state values until all prediction errors are 0, that is, until the
consistency relationship between all values holds, and thus the values are correct.
However, returning to prediction learning in real-world scenarios, we note that this updating
scheme has one major problem: it requires knowledge of the dynamics of the
environment, that is, P(r|St) and P(St+1|St) (the “world model”) must be known in order to
compute the prediction error δt in equation (6). This is clearly an unreasonable assumption
when considering an animal in a Pavlovian conditioning task, or a human predicting the trends
of a stock. Werbos in his “heuristic dynamic programming methods”, and later Barto,
Sutton, and Watkins and Bertsekas and Tsitsiklis , suggested that in a “model-free”
case in which we can not assume knowledge of the dynamics of the environment, the environment itself can supply this information stochastically and incrementally. Every time an animal is
in the situation that corresponds to state St, it can sample the reward probability in this state, and
the probabilities of transitions from this state to another. As it experiences the different states
repeatedly within the task, the animal will obtain unbiased samples of the reward and transition
probabilities. Updating the estimated values according to these stochastic samples (with a decreasing learning rate or ‘step-size’) will eventually lead to the correct predictive values. Thus
the stochastic prediction error
δt = rt +γV(St+1)−V(St)
(where rt is the reward observed at time t, when in state St, and St+1 is the next observed state
of the environment) can be used as an approximation to equation (6), in order to learn in a
“model-free” way the true predictive state values. The resulting learning rule is
Vnew(St) = Vold(St)+η(rt +γV(St+1)−V(St)).
Finally, incorporating into this learning rule the Rescorla-Wagner assumption that predictions
due to different stimuli Si comprising the state of the environment are additive , we get for all Si present at time t
Vnew(Si,t) = Vold(Si,t)+η
Vold(Sk,t+1)−∑
Vold(Sj,t)
which is the TD learning rule proposed by Sutton and Barto . As detailed above, the
formal justiﬁcation for TD learning as a method for optimal RL derives from its direct relation to
dynamic programming methods .
This ensures that using TD learning, animals can learn the optimal (true) predictive values of
different events in the environment, even when this environment is stochastic and its dynamics
are unknown.
Indeed this rule is similar, but not identical, to the Rescorla-Wagner rule. As in the Rescorla-
Wagner rule, η is a learning rate or step-size parameter, and learning is driven by discrepancies between available and expected outcomes. However, one difference is that in TD learning
time within a trial is explicitly represented and learning occurs at every timepoint within a trial.
Moreover, in the speciﬁc tapped delay line representation variant of TD learning described in
equation (10), stimuli create long-lasting memory traces (representations), and a separate value
V(Si,t) is learned for every timepoint of this trace (for instance, a stimulus might predict a reward
exactly ﬁve seconds after its presentation). A second and more important difference is in how
predictions, or expectations, are construed in each of the models. In TD learning, the associative
strength of the stimuli (and traces) at time t is taken to predict not only the immediately forthcoming reward rt, but also the future predictions due to those stimuli that will still be available
in the next time-step ∑Sj@t+1V(Sj,t+1), with γ ≤1 discounting these future delayed predictions.
Optimal action selection
The above holds whenever the probabilities of transitioning between different states of the environment are ﬁxed, as in Pavlovian conditioning (in which the animal can not inﬂuence events
by means of its actions) or in situations in which the animal has a ﬁxed behavioral policy . But what about improving action selection in order to obtain more rewards? That is,
what about instrumental conditioning? Since the environment rewards us for our actions, not our
predictions (be they correct as they may), one might argue that the ultimate goal of prediction
learning is to aid in action selection.
The problem of optimal action selection is especially difﬁcult in those (very common) cases
in which actions have long-term consequences (such as in a game of checkers), or in which
attaining outcomes requires a series of actions. The main problem, in these cases, is that of credit
assignment – how to ﬁgure out, when
reaching the outcome (for instance, a win or a loss), what actions (perhaps in the distant past)
were key to obtaining this outcome. The correct assignment of credit is crucial for learning to
improve the behavioral policy: those actions that ultimately lead to rewards should be repeated,
and those that lead to punishment should be avoided. This is true in the animal domain as well:
when reaching a dead-end in a maze, how will a rat know which of its previous actions was the
erroneous one? RL methods solve the credit assignment problem by basing action selection not
only on immediate outcomes, but also on future value predictions such as those we discussed
above, which embody predictions of long-term outcomes.
How does action selection then interact with state evaluation (for instance, using TD learning
as above)? First, note that given predictive state values, the best action to select is the one that
leads to the state with the highest value . In fact, Samuel’s 
checker player, the ﬁrst notable application of TD learning (even prior to its conception in its
modern form), used this method to select actions. However, this necessitates knowledge of how
transitions between states depend on actions, that is, what is the probability of transitioning to
each state, given a speciﬁc action. What if such knowledge is not available? For example,
imagine deciding whether to buy or to sell a stock on the stock market – clearly this decision
would be trivial if only you knew whether the stock’s price would increase or decrease as a result
of your (and the rest of the market’s) actions. But what can a human or a rat do in the completely
model-free case, ie, without knowledge of how different actions will inﬂuence the state of the
environment?
Actor/Critic methods
In one of the ﬁrst RL papers, which was inspired by neural-network models of learning, Barto
et al. showed that the credit assignment problem can be effectively solved by a learning system comprised of two neuron-like elements. One unit, termed the “adaptive critic element (ACE),” constructed an evaluation of different states of the environment, using a temporaldifference-like learning rule from which the TD learning rule above was later developed. This
evaluation was used to augment the external reinforcement signal and train through a trial-anderror process a second unit, the “associative search element (ASE)”, to select the correct action
at each state. These two elements were the precursors of the modern-day Actor/Critic framework
for model-free action selection which has been closely associated with reinforcement learning
and action selection in the brain.
The insight in the ASE-ACE model, ﬁrst due to , is that even when the external
reinforcement for a task is delayed (as when playing checkers), a temporal difference prediction error can convey, at every timestep, a surrogate ‘reinforcement’ signal that embodies both
immediate outcomes and future prospects, to the action just chosen. This is because, in the
absence of external reinforcement (ie,rt = 0), the prediction error δt in equation (8) becomes
γV(St+1)−V(St), that is, it compares the values of two consecutive states and conveys information regarding whether the chosen action has led to a state with a higher value than the previous
state (ie, to a state predictive of more future reward) or not. This means that whenever a positive
prediction error is encountered, the current action has improved prospects for future rewards,
and should be repeated. The opposite is true for negative prediction errors, which signal that the
action should be chosen less often in the future. Thus the agent can learn an explicit policy –
a probability distribution over all available actions at each state π(S,a) = p(a|S), by using the
following learning rule at every timestep
π(S,a)new = π(S,a)old +ηπδt
where ηπ is the policy learning rate and δt is the prediction error from equation (8).
Thus, in Actor/Critic models, a Critic module uses TD learning to estimate state values V(S)
from experience with the environment, and the same TD prediction error is also used to train the
Actor module, which maintains and learns a policy π (Figure 1). This method is closely related to
policy improvement methods in dynamic programming , and Williams and
Sutton et al. have shown that in some cases the Actor/Critic can be construed as a gradient
climbing algorithm for learning a parameterized policy, which converges to a local maximum
 . However, in the general case Actor/Critic methods are not
guaranteed to converge on an optimal behavioral policy . Nevertheless, some of the strongest links between RL methods and neurobiological data
regarding animal and human decision making have been related to the Actor/Critic framework.
Speciﬁcally, Actor/Critic methods have been extensively linked to instrumental action selection
and Pavlovian prediction learning in the basal ganglia , as will be detailed below.
State-action values
An alternative to Actor/Critic methods for model-free RL, is to explicitly learn the predictive
value (in terms of future expected rewards) of taking a speciﬁc action at a certain state, that is,
learning the value of the state-action pair, denoted Q (S,a). In his PhD thesis, Watkins 
suggested Q -learning as a modiﬁcation of TD learning that allows one to learn such Q -values
 . The learning rule is quite similar to the state-value learning rule above
Q (St,at)new = Q (St,at)old +ηδt
Environment
action (a)
S evaluation
function V(S)
Figure 1: Actor/Critic architecture: The state
St and reinforcement signal rt are conveyed to
the Critic by the environment. The Critic then
computes a temporal difference prediction error (equation 8) based on these.
The prediction error is used to train the state value predictions V(S) in the Critic, as well as the policy
π(S,a) in the Actor. Note that the Actor does
not receive direct information regarding the actual outcomes of its actions. Rather, the TD prediction error serves as a surrogate reinforcement
signal, telling the Actor whether the (immediate and future expected) outcomes are better or
worse than previously expected. Adapted from
Sutton & Barto, 1998.
albeit with a slightly different TD prediction error driving the learning process
δt = rt +max
a γQ (St+1,a)−Q (St,at)
where the max operator means that the temporal difference is computed with respect to what is
believed to be the best action at the subsequent state St+1. This method is considered ‘off-policy’
as it takes into account the best future action, even if this will not be the action that is actually
taken at St+1. In an alternative ‘on-policy’ variant called SARSA (the acronym for state-actionreward-state-action), the prediction error takes into account the next chosen action, rather than
the best possible action, resulting in a prediction error of the form:
δt = rt +γQ (St+1,at+1)−Q (St,at).
In both cases, action selection is easy given Q -values, as the best action at each state S is that
which has the highest Q (S,a) value. That is, learning Q -values obviates the need for separately learning a policy. Furthermore, dynamic programming results regarding the soundness
and convergence of ‘policy iteration’ methods ensure that if the proper conditions on the learning rate are met and all state-action pairs
are visited inﬁnitely often, both Q -learning and SARSA will indeed converge to the true optimal (in case of Q -learning) or policy-dependent (in the case of SARSA) state-action values.
Interestingly, recent electrophysiological recordings in non-human primates 
and in rats suggest that dopaminergic neurons in the brain may indeed be
conveying a prediction error that is based on state-action values (rather than state values, as in
the Actor/Critic model), with the former study supporting a Q -learning prediction error, and
the latter a SARSA prediction error. Whether these results mean that the brain is not using an
Actor/Critic scheme at all, or whether the Actor/Critic framework could be modiﬁed to use stateaction values (and indeed, the potential advantages of such a scheme) is still an open question
 
Neural correlates of reinforcement learning
In recent years, RL models such as those brieﬂy described above have been applied to a wide
range of neurobiological and behavioral data. In particular, the computational function of neuromodulators such as dopamine, acetylcholine, and serotonin have been addressed using the RL
framework. Among these neuromodulatory systems, the dopamine system is the most studied,
perhaps due to its implication in conditions such as Parkinson’s disease, schizophrenia, and drug
addiction as well as its long-suspected functions in reward learning and working memory. It is
in elucidating the role of dopamine signals in the brain, that computational models of learning
in general, and TD learning in particular, have had their most profound impact on neuroscience.
The link between dopamine and RL was made in the mid ’90s. On the background of a dominant hypothesis that viewed dopamine as the brain’s reward signal ,
pioneering extracellular recordings in the midbrain of awake and behaving moneys for the lab of
Wolfram Schultz showed that dopaminergic neurons did not simply signal the primary motivational value of rewarding stimuli such as food and water. In these experiments, recordings were
done while the monkeys underwent simple instrumental or Pavlovian conditioning . Surprisingly, although the recorded
cells showed phasic bursts of activity when the monkey was given a rewarding sip of juice or a
morsel of apple, if food delivery was consistently preceded by a tone or a light, after a number
of trials the dopaminergic response to reward disappeared. Contrary to the “dopamine equals
reward” hypothesis, the disappearance of the dopaminergic response to reward delivery did not
accompany extinction, but rather it followed acquisition of the conditioning relationship – as
the cells ceased to respond to rewards the monkeys began showing conditioned responses of
anticipatory licking and arm movements to the reward-predictive stimulus. Indeed, not only the
monkeys responded to the tone – the neurons now began responding to the tone as well, showing
distinct phasic bursts of activity whenever the tone came on. This was also true for the difference between self-initiated reaching for reward, in which case dopamine neurons responded
phasically to touching the reward, versus cue-initiated movements, in which case the neurons
responded to the cue rather than to the reward. These results were extremely puzzling, as is
evident by the conclusions of those early papers, which portray a handful of separate functions
attributed to different types of dopaminergic responses, and reﬂect the dire need for a unifying
The reward prediction error hypothesis of dopamine
And a unifying theoretical interpretation was not long to follow. In the mid ’90s a number of
theoreticians interested in computer science and computational neuroscience recognized the unmistakable ﬁngerprint of reinforcement learning signals in these data, and suggested that the
phasic ﬁring of dopaminergic neurons reﬂects a reward prediction error . Indeed, the hallmark of temporal difference
prediction errors is that they occur only when motivationally signiﬁcant events are unpredicted.
This explains why dopaminergic neurons show burst ﬁring to rewards early in training (when
they were unexpected), but not later in training, after the animal has learned to expect reward
10th trial
Figure 2: (a-c) Temporal difference prediction errors in a Pavlovian conditioning task. A tone CS is
presented at random times, followed 2 seconds later by a juice US. (a) In the beginning of training,
the juice is not predicted, resulting in prediction errors at the time of the juice US. With learning, the
prediction error propagates backward within the trial as
predictive values are learned (equation 9). (b) After learning, the now-predicted US no longer generates a
prediction error. Rather, the unpredicted occurrence of the tone CS is accompanied by a prediction error.
(c) The unexpected omission of the US causes a negative prediction error at the time in which the US was
expected, as in this trial reality was worse than expected. In these simulations the CS was represented
over time with the commonly used serial compound state representation , and there was no discounting (γ = 1). Other representation schemes make different predictions
for how the prediction error propagates backward, but do not differ in their predictions for the activity
patterns in a fully learned task. (d-f) Firing patterns of dopaminergic neurons in monkeys performing an
analogous instrumental conditioning task. Each raster plot shows action potentials (dots) with different
rows representing different trials, aligned on the time of the cue (or the reward). Histograms show activity
summed over the trials plotted below. (d) When a reward unexpectedly obtained, dopaminergic neurons
respond with a phasic burst of ﬁring. (e) After conditioning with a predictive visual cue (which, in this
task, predicted a food reward if the animal quickly performed the correct reaching response), the reward
no longer elicits a burst of activity, and the phasic burst now occurs at the presentation of the predictive
cue. (f) When the food reward was unexpectedly omitted, dopaminergic neurons showed a preciselytimed pause in ﬁring, below their standard background ﬁring rate. Subplots (d-f) adapted from Schultz et
al. .
on every trial. Similarly, early in training neutral cues that precede the reward should not cause
a prediction error (as they themselves are not rewarding), but later in training, once they have
acquired predictive value (ie, V(cue) > 0), an unexpected onset of such a cue should generate
a prediction error (as δt = rt + γV(cue) −V(no cue) = γV(cue) > 0), and thus dopaminergic
ﬁring. Figure 2 illustrates these effects in a simulation of TD learning, and, for comparison, in
the activity of dopaminergic neurons . The simulation is of a Pavlovian conditioning scenario in which a tone CS is followed two seconds later by a food US; the
electrophysiological recordings are from an analogous instrumental task in which a cue signaled
the availability of reward, provided the monkey responded correctly with a rapid reaching movement. Panels (a,d) illustrate the prediction error to the appetitive US early in training, and panels
(b,e) show responses after training – now shifted to the time of the unexpected CS, rather than
the US. Moreover, in trials in which the US is not delivered, a negative reward prediction error
occurs at the precise time of the expected US delivery, as is illustrated by panels (c,e). The
discrepancies between the simulation and the dopamine neuron ﬁring patterns in terms of the
magnitude and spread of the prediction errors at the time of the reward likely result from the
temporal noise in reward delivery in the instrumental task, and the asymmetric representation
of negative and positive prediction errors around the baseline ﬁring rate of these neurons . Note that the prediction error to the CS occurs only if this cue is not itself predicted
by earlier events. For instance, training with an earlier cue (CS2) that reliably precedes this CS,
would result in the dopaminergic response shifting to CS2, that is, to the earliest possible cue
that predicts the reward . The fact that an unexpected cue that predicts
reward generates a prediction error similar in all aspects to that generated by an unexpected reward, is the reason that second order conditioning can occur, with a predictive cue supporting
new conditioning as if it were itself a reward.
The close correspondence between the phasic dopaminergic ﬁring patterns and the characteristics of a temporal difference prediction error led Montague et al. to suggest the reward
prediction error hypothesis of dopamine . Within this theoretical
framework, it was immediately clear why dopamine is necessary for reward mediated learning
in the basal ganglia. The link with RL theory provided a normative basis for understanding
not only why dopamine neurons ﬁre when they do, but also what the function of these ﬁring
patterns might be. If dopamine signals a reward prediction error, this could be used for prediction learning and for action learning in dopaminergic targets. Indeed, behaviorally the shift in
dopaminergic activity from the time of reward to the time of the predictor 
resembles the shift of behavioral responding from the time of the US to that of the CS in Pavlovian conditioning experiments . Furthermore,
there is physiological evidence for dopamine-dependent (or even dopamine-gated) plasticity in
the synapses between the cortex and the striatum .
The above basic characteristics of phasic dopaminergic responding have since been replicated
in many variants . In fact, recent work investigating the detailed quantitative implications of the prediction error hypothesis has demonstrated that the correspondence
between phasic dopaminergic ﬁring and TD prediction errors goes far beyond the three basic
characteristics depicted in Figure 2. For instance, using general linear regression, Bayer and
or stimulus off
or stimulus off
Figure 3: Dopaminergic ﬁring patterns comply with the predictions of TD learning. (a) Phasic responses
to a cue predicting reward are proportional to the magnitude of the predicted reward . (b,c) When different cues predict the same reward but with different probabilities, the
prediction error at the time of the cue is proportional to the predicted probability of reward (red rectangles;
compare panel b (simulation) to panel c (data)). However, due to the low baseline ﬁring rate of midbrain
dopaminergic neurons, negative prediction errors can only be encoded asymmetrically about the base
ﬁring rate, with a shallower ‘dip’ in ﬁring rate to encode negative prediction errors as compared to the
height of the ‘peak’ by which positive prediction errors are encoded. As a result, when rewards are
probabilistic, averaging over rewarded and unrewarded trials will create an apparent ramp leading up to
the time of the reward (green rectangles; compare panel b (simulation) to panel c (data)). Panel b adapted
from Niv et al., 2005, panel c adapted from Fiorillo et al., 2003.
colleagues have rigorously shown that the contribution of previously experienced rewards to the
dopaminergic response to the current reward is exactly according to an exponentially weighted
average of past experience, as is implied by the TD learning rule . Moreover, conditioned stimuli predicting probabilistic rewards or rewards
Average ﬁring rate of 19 dopaminergic neurons, recorded in rats performing an odordiscrimination task in which one of the odors predicted that a reward would be delivered in a food-well,
with some delay. Color indicates the length of the delay preceding reward delivery from 0.5 to 7 seconds.
Activity is aligned on odor onset (left) and food-well entry (right). Note that the response to the (not fully
predicted) reward is similar in all trial types (with the earliest rewards perhaps better predicted, and thus
accompanied by smaller prediction errors), but the response at the time of the predictive cue depends on
the predicted delay of the reward, with longer predicted delays eliciting a smaller dopaminergic response.
Adapted from Roesch et al. .
of different magnitudes have been shown to elicit a phasic dopaminergic response that is proportional to the magnitude and/or probability of the expected reward , and ﬁring patterns in tasks involving probabilistic
rewards are in accord with a constantly back-propagating error signal . With regard to delayed rewards, recent results from recordings in rodents show that
dopaminergic activity to a cue predicting a delayed reward is attenuated in proportion to the
delay (Figure 4), as would be expected from a signal predicting the expected sum of discounted
future rewards . Impressively, even in sophisticated conditioning tasks such
as blocking and appetitive conditioned inhibition, dopaminergic responses are in line with the
predictions of TD learning . Finally, measurements
of extracellular dopamine in behaving rodents using fast scan cyclic voltammetry have conﬁrmed
that phasic changes in the level of dopamine in target structures (speciﬁcally, in the striatum) also
conform quantitatively to a prediction error signal , despite the nonlinear relationship between dopamine neuron ﬁring and actual synaptic discharge of the transmitter .
The prediction error theory of dopamine is a computationally precise theory of how phasic
dopaminergic ﬁring patterns are generated. It suggests that the input that dopaminergic neurons receive from their diverse afferents conveys information about
current motivationally signiﬁcant events (rt), and the predictive value of the current state V(St),
and that the circuitry in the dopaminergic nuclei uses this information to compute a temporal
difference reward prediction error. Moreover, it suggests that dopamine provides target areas
with a neural signal that is theoretically appropriate for controlling learning of both predictions
and reward-optimizing actions. Following the analogy between the dopamine signal and the
temporal difference prediction error signal in Actor/Critic models , it has been
suggested that dopaminergic signals originating in the ventral tegmental area and terminating in
ventral striatal and frontal areas are used to train predictions, as in the Critic , while a similar signal reported by dopaminergic neurons in the substantia nigra pars
compacta to dorsal striatal target areas, is used to learn an action-selection policy, as in the Actor
 .
As should be the case when researching the basic characteristics of a neural signal, the studies
mentioned above mostly used rather simple Pavlovian or instrumental tasks, in which trials include one unambiguous stimulus and one reward. Given the accumulation of positive results, it
seems that the time is now ripe to test the reward prediction error theory of dopamine in more
complex scenarios, for instance situations in which there are a number of conﬂicting predictive
cues, tasks in which several actions are necessary to obtain an outcome, or tasks in which there
are several possible outcomes to choose from. In these cases the theory is not as prescriptive –
there are different ways to combine predictive cues, or to generate a prediction error that does or
does not depend on the actual chosen action that we did not
detail), thus electrophysiological evidence is key to informing the RL theory and constraining
the algorithm actually used by the brain.
Several studies have recently made progress in this direction. Morris et al. trained monkeys in a standard instrumental task in which cues predicted reward with different probabilities.
In some trials, however, the monkeys were given a choice between two of these cues. Single
unit recordings in the substantia nigra pars compacta showed that in these trials the cue-elicited
dopaminergic ﬁring matched best the prediction errors corresponding to the cue that would subsequently be chosen (even though the monkey could only signal its choice seconds later). This
is contrary to the straightforward predictions of an Actor/Critic mechanism, and more in line
with SARSA learning. Interestingly, recordings from the ventral tegmental area of rats performing a more dynamic odor-discrimination task were taken to suggest that
dopmainergic activity in choice scenarios was better described by Q -learning. There are many
differences between these two studies, including the animal species, the dopaminergic nuclei in
which recordings were made, the task and the amount of training (or overtraining) of the animals,
any of which could be invoked to explain their different results. However, the detailed activation
patterns in the latter study, as well as results from a task in which monkeys engaged in a difﬁcult
random-dot motion discrimination task , suggest that predictions (and thus
prediction errors) can be sensitive to the information available at every timepoint, with stimuli
represented before a choice is made, and chosen cues represented only later. These ﬁndings
suggest a possible reconciling between the different results in terms of different representations
in the tasks, and further highlights the need to study, from a theoretical point of view, as well as
an experimental one, the effects of different state representations on TD learning.
As precise as the prediction error hypothesis of dopamine is, other open questions are abounding. Many of these will likely require modiﬁcations and enhancements of the currently highly
simpliﬁed basic theory . One extremely pressing issue is that of prediction
of aversive events such as pain. Interestingly, dopaminergic neurons do not seem to be involved
in the signaling or prediction errors for aversive outcomes , although they do signal negative prediction errors due to the
absence of appetitive outcomes . Despite the behavioral similarities between
the loss of a reward and the receipt of a punishment, these seem to be separated in terms of
prediction learning, and it is currently far from clear what the substrate for aversive prediction
learning might be .
We should also mention that there are alternative psychological theories regarding the role of
dopamine in conditioned behavior . These include Redgrave and colleagues’ ‘incentive salience’ , Berridge and Robbinson’s ‘wanting’ versus ‘liking’ , and ideas about dopamine signaling uncertainty . A
discussion of the merits and pitfalls of the different theories is beyond the scope of this review.
Moreover, such a discussion unfortunately involves the unsatisfactory comparison of qualitative
suggestions to a quantitatively precise theory, rendering it difﬁcult for any deﬁnitive conclusions
to be reached. Nevertheless, it is our personal opinion that, in as far as these theories are indeed fundamentally different from the prediction error theory (which is not always clear), to
date no alternative has mustered as convincing and multidirectional experimental support as the
prediction error theory of dopamine.
RL correlates in functional imaging of human decision-making
Although animals can display complex decision-making behavior that is still well beyond our
current understanding of the brain, ultimately we are interested in understanding human decisionmaking and how (and whether) it is related to the RL framework. While the characteristics of
human conditioning are similar to those of animal conditioning, the possibility of instructing
subjects verbally allows for much more elaborate paradigms in human experiments. Of course,
our ability to measure neural processes in humans is much more limited. One technique that
has been used extensively to study the underpinnings of RL in the human brain is functional
magnetic resonance imaging (fMRI), in which correlates of neural activity can be measured
non-invasively, albeit with a low signal-to-noise ratio (necessitating averaging over many trials
or subjects) and poor temporal and spatial resolution (seconds and millimeters, respectively).
One advantage of fMRI is that it allows imaging of activity throughout the entire brain, rather
than in only a small population of neurons. This also has disadvantages, in terms of the statistical
analysis of the enormous volume of data collected even in a single experimental session. One
might argue that fMRI thus places a premium on using precise computational models for data
analysis. A model-driven analysis allows us to make precise hypotheses regarding hidden variables that control learning and decision-making, such as state values or prediction errors, and
search for these in the brain. Using the computational model we can quantitatively specify the
dynamics of the hidden variable even within a non-stationary task, and search the brain for a signal with a similar temporal proﬁle. Identifying a neural correlate for such a signal advances our
understanding of the brain in a way that would not be possible without the model, and can also
lend powerful support for the model that gave rise to the speciﬁc values of the hidden variable.
A more pervasive disadvantage of fMRI investigations, however, is that the neural underpinnings
of the measured blood oxygen level dependent (BOLD) signal are unclear. Some studies suggest
that the BOLD signal in a brain area correlates with local ﬁeld potentials in that area (signals that
are themselves poorly understood, but are thought to be related to the overall dendritic synaptic
activity within a volume of tissue), and thus reﬂects the input activity impinging on an area,
rather than the (output) spiking activity of neurons within that area . However, in these studies the correlation between BOLD and local ﬁeld
potentials was only slightly stronger than the (weak) correlation with spiking activity, making the
results inconclusive. Furthermore, it is not clear that BOLD reﬂect the same underlying neural
processes in all brain areas. For instance, because dopamine can directly affect the dilation and
contraction of local blood vessels, it can affect BOLD measurements directly in brain areas in
which extracellular concentration of dopamine is pronounced . Dopamine is also known to affect local oscillatory activity , a
prominent determinant of local ﬁeld potentials, and thus perhaps BOLD. Indeed, these caveats go
both ways: since dopamine is a major neuromodulator of interest in RL, its direct measurement
is actually of major interest. However, despite its possible direct effect on BOLD signals, one
can not interpret BOLD measurements as reﬂecting dopaminergic activity per se . Keeping these caveats in mind, we now turn to the speciﬁc use of RL models in
fMRI studies of learning and decision making in the human brain.
The ﬁrst fMRI studies to search for prediction errors in humans implicated the nucleus accumbens and the orbitofrontal cortex ,
both major dopaminergic targets. O’Doherty et al. and McClure et al. then used
a hidden-variable analysis such as the one described above, to identify the neural correlates of
model-derived TD prediction errors. These studies again implicated the nucleus accumbens (the
ventral striatum) as well as the putamen (the dorsolateral striatum). O’Doherty et al. 
then showed that fMRI correlates of prediction error signals can be dissociated in the dorsal
and ventral striatum according to whether active choice behavior is required in order to obtain
reward (ie, instrumental conditioning) or not (Pavlovian conditioning). In the passive predictionlearning task the reward prediction error was evident only in the ventral striatum, while in the
active choice task it was evident in both the ventral and the dorsolateral striatum. These ﬁndings
supported a previously suggested mapping of an Actor/Critic architecture in the basal ganglia,
according to which the ventral striatum includes a prediction-learning Critic, and the dorsal striatum hosts a policy-learning Actor .
expected value Vt
TD prediction error δt
Time course of different
‘hidden variables’ of interest in the TD
The bell predicts a rewarding cup of coffee some time later. At
the time of the cue, the phasic prediction error δt = rt +Vt+1 −Vt equals the
magnitude of the predicted reward Vt+1
(assuming here, for simplicity, γ = 1).
The expected value signal corresponding to Vt also becomes positive at this
time, and stays elevated until the time
of the expected reward.
At the time
of the reward, a phasic cue might signal the occurrence of the reward, but
no prediction error occurs if the reward
was predicted.
Figure adapted from
Niv & Schoenbaum .
Indeed, correlates of prediction errors in the dorsal and ventral striatum have now been seen in
multiple studies . As mentioned, although single unit recordings do not show prediction error encoding in the striatum,
these results are in line with the fact that the striatum is a major target of dopaminergic inﬂuence. Indeed, dopaminergic manipulations (eg. administration of dopamine enhancers (agonists)
or dopamine receptor blockers (antagonists) in such tasks have been shown to inﬂuence both the
BOLD measurement of prediction-error activity and learning and action selection , and recent results show that better learners show a higher correlation of striatal
BOLD with a reward prediction error . However, fMRI results cannot
isolate dopaminergic activity from other activity in the brain (speciﬁcally, from the effects of
other striatal afferents), and might not differentiate inhibitory from excitatory afferent activity,
as is illustrated by the fact that BOLD correlates of positive prediction errors for pain and punishment have also been found in the striatum .
Note, also, that most fMRI analyses consist of searching for areas in the brain where the measured BOLD is correlated with some signal of interest. In particular, the assumption is that
multiple signals in one brain area may be linearly multiplexed, and one can uncover the component signals via linear regression. As a result, it is not easy to distinguish between different
correlated components of RL models, for instance, prediction errors and state values, especially
at the time of the predictive cue (Figure 5). This is because the prediction error at the time of the
cue is δt = V(cue)−V(baseline), which, is obviously linearly related to V(cue). Indeed, many
studies have implicated the striatum in representing the anticipated value of outcomes , and it is not always clear whether
the measured activation is distinct from that attributable to a prediction error. In any case, electrophysiological data show that the striatum is deﬁnitely a viable candidate for representing state
values .
Studies involving both gains and losses have further implicated the striatum in the anticipation
of losses and not only gains, with decreases in BOLD signals correlated with the anticipated
loss. Moreover, the degree of deactivation to losses compared to activation to gains (‘neural
loss aversion’) in the nucleus accumbens and the prefrontal cortex was predictive of individual
differences in behavioral loss aversion . Finally, outcome values themselves
(as well as subjective preferences) have been associated with activations in areas such as the
ventromedial prefrontal cortex and the orbitofrontal cortex . These activations as well need to be
convincingly dissociated from other potentially correlated signals such as TD errors.
While outlining the contribution of fMRI to elucidating the neural underpinnings of RL, it is clear
from the above that fMRI results can reveal only so much. One way to overcome the different
interpretational caveats is to synthesize results from electrophysiological recordings with those
from fMRI. Another approach that is gaining popularity is the use of functional imaging in
combination with pharmacological challenges or with radioligand labeled positron emission tomography to test
in humans more directly the causal predictions and pharmacological hypotheses of RL models,
respectively. In any case, the promise of model-driven analysis of imaging data has yet to be
fully realized, and the link between computational models of learning and the brain does not end
with the identiﬁcation of the reward prediction error signal. For example, recent work has used
such a ‘hidden-variable’ analysis within an RL framework to investigate the neural substrates of
exploration and a hierarchical RL model has been used to demonstrate that
the brain tracks the volatility (or rate of change) of the environment .
Evidence for multiple reinforcement learning systems in the brain
While dopamine is critical to many learning and behavioral processes in the brain, animals can
learn to select actions correctly even in the absence of dopamine . This is
perhaps not so surprising, as converging lines of evidence suggest that animals and humans have
a number of parallel decision-making systems at their disposal , only a subset of which are dopamine dependent. Key to identifying these
different systems is the fact that a certain behavior (for instance, simple lever-pressing by a rat)
can have wholly different characteristics in different situations: early in training this behavior can
show ﬂexibility and sensitivity to changes in the task or in the animal’s motivational state, while
the same lever-pressing behavior can become inﬂexible and slow to adapt to any change after
considerable training . The amount of training is not the only determinant of the degree of ﬂexibility of learned
behavior , but the link between over-training
and the folk-psychological notions of “habits” has bestowed upon the inﬂexible form of behavior
the name “habitual responding”, while the more ﬂexible instrumental actions are called “goaldirected” . To complicate matters further, some forms of behavior seem wholly inﬂexible in that
they are innately speciﬁed reactions to learned predictions . These fall into
the class of Pavlovian responses, and it is not clear whether they are also driven by two types
of value predictions: ﬂexible outcome-speciﬁc predictions (similar to those underlying goaldirected instrumental behavior), and less-ﬂexible general affective predictions (as in habitual
instrumental behavior).
This suggested multiplicity of neural controllers may be surprising – why not use the best controllers at all times? Careful consideration of the characteristics of real-world learning situations,
together with the properties of different RL strategies, can offer insight into the advantages of
combining different controllers as well as explain their different behavioral characteristics. Recall that the exposition of RL above began with a deﬁnition of predictive values (as the expected
sum of future rewards), and then suggested different ways to learn or estimate such values. If
different methods are available, perhaps the brain uses more than one? Daw et al. have
suggested that since each method has different advantages and disadvantages, the brain should
use each method in the circumstances for which it is best. They suggested to identify habitual
action selection with behavior based on cached values – those values learned through prediction
errors and slow trial-and-error experience with the environment. These are the classic “modelfree” RL methods, for which action selection is easy, however, much training is needed in order
to obtain accurate value estimates. Goal-directed behavior, on the other hand, was identiﬁed
with online dynamic-programming-like computation of values through forward search or forward simulation of the consequences of actions using an estimated model of the environment
(ie, estimated probabilities of transitions between states and reward probabilities at each state).
These “model-based” methods are more accurate and adjust ﬂexibly to changes in circumstances,
however, their computation is costly in terms of neural resources and time, and so they should
be used only sparingly – perhaps only in those situations in which there is not yet enough data to
inform the “model-free” system. Daw et al. further postulated that the brain arbitrates between these parallel systems based on the uncertainty associated with their evaluations: when the
two systems ‘recommend’ different courses of action, the recommendation that is most accurate
is the one that should be followed. Assessing the accuracy of the evaluations of the two systems
normatively depends on variables such as amount of training (which decreases uncertainty in the
habitual system) and depth of the necessary forward search (which increases uncertainty in the
goal-directed system).
These theoretical considerations align surprisingly well with both behavioral and neural data.
Behaviorally, the circumstances that favor goal-directed behavior are those in which there is not
yet sufﬁcient experience for the model-free system to build on, such as early in training or when
there are several actions leading to several outcomes (each of which has been sampled to a lesser
extent). To the contrary, when a rather long sequence of possible events needs to be mentally
simulated in order to evaluate a course of action in the model-based system, behavior tends to
be habitual, determined by model-free evaluations instead. Other conditions favoring habitual
responding are excessive training and simple scenarios in which only one action needs be evaluated. Even the fact that behavior on some schedules of reinforcement habitizes more readily
than on other schedules (namely, interval and ratio schedules, respectively) can be understood
within this framework: in interval schedules many behavioral policies lead to similar rates of
reward, and thus policy learning can enjoy a large degree of generalization. In contrast, in ratio
schedules different policies lead to different amounts of reward, and learning about one policy
can not generalize to another. This means that, given the same number of training sessions, the
effective amount of learning experience per policy is smaller in a ratio schedule as compared
to an interval schedule, and thus the uncertainty associated with the habitual system would be
higher and behavior would remain goal-directed in ratio schedules.
Neurally, work from the lab of Bernard Balleine has implicated separate cortico-basal-ganglia
loops in each of these evaluation and decision
making systems . Speciﬁcally, the so-called ‘limbic loop’,
including areas such as the ventral striatum, the basolateral amygdala and the orbitofrontal cortex, has been associated with Pavlovian prediction learning and evaluation . The acquisition and expression
of ‘action-outcome’ (forward model) associations in the goal-directed instrumental system has
been localized to the ‘associative loop’ which includes the dorsolateral prefrontal cortex (or its
homologue in rats, the prelimbic cortex) and the caudate nucleus (dorsomedial striatum in rats)
 . Finally,
‘stimulus-response’ (cached policy) habitual behavior, which had previously been associated
with the striatum in general, has recently been more speciﬁcally localized to the ‘sensorimotor
loop’ originating in sensorimotor cortices, and involving the putamen (dorsolateral striatum in
rats) .
The interactions between these interconnected loops , and indeed the implementation of arbitration between the different systems, are less well understood. One candidate area for arbitration between goal-directed and habitual control is the rat infralimbic cortex
 , but the story here is only beginning to unfold. Another avenue for
future research, the ﬂexible model-based action selection system is, as of yet, under-constrained
both computationally and behaviorally. This complex system may be easier to study in humans,
where the use of instructions can prevent the need for extensive training (and thus habitization
of responding). Indeed, recent fMRI investigations in the lab of John O’Doherty have begun to
see fruit from such a program. In one study, human goal-directed behavior was tested using the
outcome devaluation protocol that has been developed for such studies in rats . The neural results implicated the orbitofrontal cortex in the ﬂexible evaluation of expected outcomes that is at the heart of goal-directed behavior. Another set of papers in which
model-free TD learning was contrasted with model-based learning algorithms that exploit the
higher order structure of a serial-reversal task, implicates the ventromedial prefrontal cortex and
the anterior cingulate cortex in computations that are explicitly based on task structure .
Tonic dopamine and the choice of response vigor
Keen-eyed readers may have noticed that there is one aspect critically missing in our various
accounts of reinforcement learning and decision-making in the brain. Indeed, it is something of
a curiosity that although the tradition in animal experimentation is to investigate the determinants
of rates of responding (as in Skinner’s investigations of key-pecking in pigeons or leverpressing
in rats, so called ‘free-operant’ experiments because the animal is free to choose when to respond
and no trial structure is imposed on behavior), RL models of conditioning have concentrated
exclusively on discrete choices of actions at pre-speciﬁed timepoints. Our issue is not only with
laboratory paradigms: real-life decisions most often take place in continuous time, and one could
argue that every choice of action, even that in a discrete trial setting, is accompanied by a choice
of the speed or vigor with which that action will be performed. Such a decision gives rise to
response rates in free operant behavior, to running times in mazes, and to reaction time data
in discrete settings. It also interacts with the effects of motivation on behavior – a hungry rat
running down a maze in search of food will run faster than a sated rat.
In this ﬁnal subsection we will brieﬂy discuss the application of RL methods to decisions about
how fast (or with what vigor) to behave, and the neural implications of such a model. Despite
the emphasis on discrete actions, RL theory does exist that deals with continuous time: this
is average reward reinforcement learning in a semi-Markov decision process . Building on this theoretical framework, we have recently
proposed an RL model of optimal rates of responding . In this model of instrumental conditioning, every choice of action is accompanied by a choice of a latency with
which to perform that action. Furthermore, responding incur costs inversely proportional to the
chosen latency, such that vigor is costly. Finally, the goal of the decision-maker is to obtain the
highest possible net rate of rewards minus costs per unit time. The results of the model showed
that the fundamental characteristics of free operant response rates could be explained as the consequences of the optimal choice of response rates in different tasks . Moreover, the
model was used to derive a normative explanation for how motivational states should affect the
rates of responding .
Importantly, the average reward RL framework highlights an important factor that determines
optimal responding: the net rate of rewards, that acts as the opportunity cost of time. Consider,
for instance, a rat pressing a lever in order to obtain food. Suppose that its presses had previously earned food at an average rate of four pellet per minute. When contemplating whether
to devote ﬁve seconds to executing the next leverpress, the potential beneﬁt of this action (ie,
the probability of its generating reward, and the magnitude of this reward) should be weighed
against both the costs of performing the action at this speed, and the opportunity cost of time,
ie, the potential loss of (on average) 1/3 reward pellet due to devoting time to this action rather
than continuing to behave according to the previous policy. Because the opportunity cost of time
is similar for all actions, the model predicts that when the net reward rate is higher (for instance,
due to a benevolent experimenter, or due to the rat being hungry, which renders each food pellet
subjectively more valuable), all actions should optimally be performed faster , and affect not only synaptic plasticity, but also membrane potentials
and neural excitability . Indeed, the effects of dopaminergic manipulations such as lesions,
antagonism or agonism, are ﬁrst and foremost seen in the vigor of ongoing behavior, rather than
in learning processes. For instance, 6-hydroxydopamine injections into the ventral striatum that
kill dopaminergic neurons projecting to that area, profoundly reduce the rate of instrumental responding . As a result, dopamine in the striatum has
been linked to invigorating Pavlovian and instrumental responding .
Combining these lines of evidence, we have suggested that the net rate of reward, the critical
determinant of response rates across the board, might be represented by tonic levels of dopamine
in the striatum . Different from phasic dopaminergic activity that changes
on the timescale of milliseconds, and presumably exerts its main effect inside its target synapses,
the tonic level of dopamine is the slowly-changing background level of the neuromodulator in
the extrasynaptic ﬂuid, hypothesized to change very slowly (on the order of minutes), bridging
across events such as trials in an experiment. If tonic dopamine really does convey the net reward
rate, it is now clear why higher levels of dopamine (for instance, as a result of amphetamine
administration) would result in overall faster responding, and why dopamine depletion (as in
Parkinson’s disease) would induce lethargy. Recent work supporting this hypothesis has shown
that the overall vigor of instrumental responding depends on the balance between the so-called
direct and indirect pathways from the striatum to the output of the basal ganglia , and that the lateral habenula controls the tonic levels of dopamine in the striatum, with
manipulations exerting prolonged effects (more than one hour long) on the degree of locomotion
of rats . Moreover, It conveniently turns out that if the tonic level of
dopamine simply reﬂects spillover from phasic prediction error signals, averaged over a longer
timeframe due to slow reuptake, it follows computationally that it would, by default, equal the
net rate of obtained rewards. This ‘tonic dopamine hypothesis’ thus dovetails neatly both with
the computational prediction error theory of phasic dopamine and with psychological theories
about dopamine’s role in energizing responses. It provides the ﬁrst normative explanation for
the critical role that tonic levels of dopamine play in determining the vigor of responding, and
suggests a route by which dopamine could mediate the effects of motivation on response vigor.
Challenges and future directions
RL models are now used routinely to design and interpret a wide range of learning and decisionmaking experiments. However, one of the reasons that RL models have been successful is that
they are highly simpliﬁed models, accounting for fundamental phenomena while eschewing the
necessary complexities that accompany more detailed explanations. We have already discussed
some possible extensions and fertile areas for future work throughout this review. In this last
section, we highlight a few more theoretical challenges that await this area of active research.
The ﬁrst challenge is hinted to by responses of dopamine neurons to stimuli not clearly related
to reward prediction. It has long been known that novel stimuli cause phasic bursts in dopamine
neurons , although they are not (yet) predictive of any outcome, aversive or appetitive. However, new learning is not done on the background of a blank slate. It is reasonable
to think that generalization to previously encountered stimuli would play a role in the initial
appraisal of a novel stimulus. If the experimental (or the general ecological) scenario is such
that animals have learned to expect that stimuli predict rewards (as is the case in many experimental situations), it is not surprising that new stimuli will be treated optimistically. Kakade
and Dayan addressed this possibility directly, and furthermore suggested that the novelty
responses can function as ‘novelty bonuses’ – quantities that are added to other available rewards
= rt + novelty(St)) and enhance exploration of novel stimuli. Kakade and Dayan showed
how this simple idea can account in detail for the reported novelty responses of dopamine neu-
rons (for instance, for the observation that the novelty burst is frequently followed immediately
by a dip of the ﬁring rate below baseline) yet still explain how they also communicate a reward
prediction error. Recent fMRI work has demonstrated directly the existence of such additive
novelty bonuses and their inﬂuence on choice behavior, in a situation in which novelty was explicitly not related to reward predictions . The general issue which this
line of work only begins to touch upon, is that of generalization: how does learning from one
task affect subsequent learning. This fundamental question is still, for the most part, awaiting a
normative computational account.
A second intriguing avenue of research deals with more complex tasks, for instance, those which
have hierarchical structure. A quintessential example of this is the everyday task of making
coffee, which comprises several high-level ‘modules’ such as ‘grind beans’, ‘pour water’, ‘add
sugar’, each of which, in turn, comprises many lower-level motor actions. Hierarchical reinforcement learning is an active
area of research exploring the ways in which RL systems can take advantage of the hierarchical
structure of real-world tasks, in order to mix-and-match previously learned action modules. One
question that frequently arises in these theoretical studies is, where do these previously learned
modules come from, or, more importantly, how does an agent learn useful modules . In animal learning, at least part of this answer is clear:
useful modules come from previous tasks. This again raises the issue of generalization – how
to transfer learning from one task to another effectively. But, above and beyond the issue of
learning of modules, the hierarchical RL framework raises many tantalizing questions regarding
the neural implementation of hierarchical control .
A third challenge is due to the nature of temporal difference learning, and speciﬁcally, its strong
dependence on temporal representations. Behavioral results suggest that interval timing is extremely inaccurate, with the standard deviation of the prediction of an interval being proportional
to the mean length of the interval . Simple variants
of TD learning are extremely sensitive to timing noise, with even very small amounts of noise
devastating the predictive power of the model, and resulting in pronounced prediction errors at
the time of reward delivery, even in thoroughly learned tasks. The puzzle is whether the high degree of noise in behavioral timing is consistent with the temporal sensitivity displayed by neural
prediction error signals. In any case, deriving such precisely-timed predictions despite considerable timing noise likely necessitates a more complex account for timing within a semi-Markov
framework 
The ﬁnal area of research that we would like to mention here, has actually been waiting in the
sidelines all along. Even simple experimental paradigms such as extinction (in which a once
predicted reward ceases to appear) and conditioned inhibition (in which a cue predicts that an
otherwise expected reward will not occur) do not yet have satisfactory computational models.
The RL framework is exposed for its excessive simplicity by basic behavioral phenomena such
as spontaneous recovery from extinction, or the fact that the inhibitory value of a conditioned
inhibitor does not extinguish when this cue is presented alone. Temporal difference models
that treat extinction merely as the unlearning of predictive values, can not explain spontaneous
recovery. Similarly, modeling conditioned inhibitors as having negative predictive value can not
explain why this value is maintained even when it is consistently paired with no reward (“0”)
rather than a negative outcome. Clearly, conditioning is more than learning and unlearning of
additively combined values. One recent model that suggested that negative prediction errors
result in the inference of a new state S and new learning about this state, explained how both the
original conditioned values and the new extinction knowledge can co-exist .
Further modeling work awaits.
Conclusions
To summarize, computational models of learning have done much to advance our understanding
of decision making in the last couple of decades. Temporal difference reinforcement learning
models have suggested a framework for optimal online model-free learning, which can be used
by animals and humans interacting with the environment in order to learn to predict events in the
future and to choose actions such as to bring about those events that are more desirable. Investigations into the decision-making behavior of both animals and humans support the existence of
such a mechanism, controlling at least some types of decision-making behavior. The prediction
error hypothesis of dopamine has further linked these algorithmic ideas to possible underlying
neural substrates, speciﬁcally, to learning and action selection in the basal ganglia modulated by
phasic dopaminergic signals. Converging evidence from a wide variety of recording and imaging methods supports this hypothesis. Neural investigations of the underpinnings of RL, in turn,
have highlighted some holes in the current theory (eg. dopaminergic novelty responses), and
have suggested extensions to the RL framework (eg. combining different RL controllers within
one agent).
It thus seems that reinforcement learning has been most powerful (and unfortunately for neuroscience, almost unique), in tying together Marr’s three levels: computation, algorithm
and implementation, into one coherent framework that is used not only for gleaning understanding, but also for shaping the next generation of experimental investigations. Whether the theory
can be elaborated to account for results of future experimentation without losing its simplicity
and elegance, or whether it is eventually abandoned and replaced by a newer generation of computational learning theories, reinforcement learning has already left its permanent mark on the
study of decision making in the brain.
Acknowledgments
The author wishes to thank P. Read Montague and Paul Glimcher for comments and contribution
to earlier versions of this paper, and Michael Todd for comments on a previous draft. Parts of
this work were published as a chapter in “Neuroeconomics: Decision making and the brain” (P.
W. Glimcher, C. Camerer, E. Fehr, and R. Poldrack, editors). This work was supported by the
Human Frontiers Science Program.