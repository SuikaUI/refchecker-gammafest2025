Quantifying Self-Organization with Optimal Predictors
Cosma Rohilla Shalizi,1, ∗Kristina Lisa Shalizi,2, † and Robert Haslinger3, 4, ‡
1Center for the Study of Complex Systems, University of Michigan, Ann Arbor, MI 48109§
2Statistics Department, University of Michigan, Ann Arbor, MI 48109
3MGH-NMR Center, Department of Radiology, Massachusetts General Hospital, Charlestown, MA 02129
4Center for Nonlinear Studies, Los Alamos National Laboratory,Los Alamos, NM 87545¶
Despite broad interest in self-organizing systems, there are few quantitative, experimentallyapplicable criteria for self-organization. The existing criteria all give counter-intuitive results for
important cases.
In this Letter, we propose a new criterion, namely an internally-generated increase in the statistical complexity, the amount of information required for optimal prediction of the
system’s dynamics. We precisely deﬁne this complexity for spatially-extended dynamical systems,
using the probabilistic ideas of mutual information and minimal suﬃcient statistics.
This leads
to a general method for predicting such systems, and a simple algorithm for estimating statistical
complexity. The results of applying this algorithm to a class of models of excitable media (cyclic
cellular automata) strongly support our proposal.
PACS numbers: 05.65.+b, 02.50.Tt, 89.75.Fb, 89.75.Kd
Keywords: Self-organization, cellular automata, excitable media, information theory, statistical complexity,
spatio-temporal prediction, minimal suﬃcient statistics
The term “self-organization” was coined in the 1940s
 to label processes in which systems become more
highly organized over time, without being ordered by outside agents or by external programs. It has become one
of the leading concepts of nonlinear science, without ever
having been properly deﬁned. The prevailing “I know
it when I see it” standard prevents the development of
a theory of self-organization. Thus some say that “selforganizing” implies “dissipative” , and others that they
can exhibit reversible self-organization , and no one
knows if both groups are talking about the same idea.
A deﬁnition of self-organization should be mathematically precise, so we can build theories around it, and experimentally applicable, so we can use empirical data to
say whether something self-organizes. The goal of such
a deﬁnition should be both to match our informal notions in easy cases, where intuition is clear and consensual, and to extend unambiguously to intuitively hard or
disputed cases. If our informal notions allow for comparative, “more than” judgments, a formalization should
match those, too. Generally there are many ways to formalize a single concept, and competing formalizations
must be judged by their scientiﬁc fruitfulness; diﬀering
formalizations may be appropriate in diﬀerent contexts.
(For more on such methodological issues, see .)
We believe we have a formal criterion for selforganization that meets the key requirements. It is precise, unambiguous, and operational. We check its conformity with intuition against cellular automata, speciﬁcally
cyclic cellular automata (CA). They are ideal test cases:
their dynamics are completely known (because we specify
them) and can easily be simulated exactly. They are reasonable qualitative models of excitable media, and there
is an analytical theory of the patterns they form. We
show that our deﬁnition works, at least in this case. Two
of us discussed preliminary work in ; here we present
the (concurring) results of larger, more extensive simulations.1
Measuring Organization Few attempts have been
made to measure self-organization quantitatively. Thermodynamic entropy is an obvious measure of organization for physicists, and several works claim to measure
self-organization by ﬁnding spontaneous declines in entropy . But thermodynamic entropy is a bad
measure of organization in complex systems .
Entropy is proportional to the logarithm of the accessible
volume in phase space, which has no necessary connection to any kind of organization. Thus low-temperature
states of Ising systems or Fermi ﬂuids have very low entropy, but no discernible organization .
Biological
organisms are never in pure, low-entropy states, but are
organized, if anything is. Some kinds of biological selforganization are, in fact, thermodynamically driven by
increasing entropy .
After “fall in entropy”, the leading idea on how to
measure self-organization, advanced in , is a rise in
complexity.
While there are many proposed measures
of physical complexity, the general view is that complex
phenomena are ones which cannot be described concisely
and accurately (see for a general survey).
proposals use algorithmic descriptions, and are limited
by inherent uncomputability. Here we take a stochastic
point of view, aiming to statistically describe ensembles
of conﬁgurations. We follow Grassberger in deﬁn-
1 Strictly speaking, we quantify system organization.
In isolated systems, as in our simulations, this is necessarily selforganization. Distinguishing self- from external organization in
systems receiving structured input is tricky; we discuss some possible approaches below.
In any case, our subject is distinct from “self-organized criticality” , a term labeling non-equilibrium systems whose attractors show power-law ﬂuctuations and long-range correlations.
We plan to address whether such systems are self-organizing in
our sense in future work.
ing the complexity of a process as the least amount of
information about its state needed for maximally accurate prediction. Crutchﬁeld and Young extended this
concept, by giving operational deﬁnitions of “maximally
accurate prediction” and “state”.
The Grassberger-Crutchﬁeld-Young “statistical complexity”, C, is the information content of the minimal suf-
ﬁcient statistic for predicting the process’s future . In
thermodynamic settings, this is the amount of information a full set of macrovariables contains about the system’s microscopic state . We now sketch the formalism allowing us to use statistical complexity to characterize spatially-extended dynamical systems of arbitrary
dimension, after .
Let x(⃗r, t) be an n + 1D ﬁeld, possibly stochastic, in
which interactions between diﬀerent space-time points
propagate at speed c.
As in , deﬁne the past light
cone of the space-time point (⃗r, t) as all points which
could inﬂuence x(⃗r, t), i.e., all points (⃗q, u) where u < t
and ||⃗q −⃗r|| ≤c(t −u). The future light cone of (⃗r, t) is
the set of all points which could be inﬂuenced by what
happens at (⃗r, t). l−(⃗r, t) is the conﬁguration of the ﬁeld
in the past light cone, and l+(⃗r, t) the same for the future
light cone. The distribution of future light cone conﬁgurations, given the conﬁguration in the past, is P(l+|l−).
Any function η of l−deﬁnes a local statistic. It summarizes the inﬂuence of all the space-time points which
could aﬀect what happens at (⃗r, t).
Such local statistics should tell us something about “what comes next,”
which is l+. ( explains why we must use local predictors, and the advantages of basing them on light cones,
as ﬁrst suggested by .)
Information theory lets us
quantify how informative diﬀerent statistics are.
The information about variable x in variable y is
where P(x, y) is joint probability, P(x) is marginal probability, and ⟨·⟩is expectation .
The information a
statistic η conveys about the future is I[l+; η(l−)].
statistic is suﬃcient if it is as informative as possible
 , here if and only if I[l+; η(l−)] = I[l+; l−]. This is
the same as requiring that P(l+|η(l−)) = P(l+|l−).
A suﬃcient statistic retains all the predictive information in the data. Decision theory tells us that maximally accurate and precise prediction needs only a suﬃcient statistic, not the original data; in fact, any predictor
which does not use a suﬃcient statistic can be replaced
by a superior one which does.
Since we want optimal
prediction, we conﬁne ourselves to suﬃcient statistics.
If we use a suﬃcient statistic η for prediction, we must
describe or encode it.
Since η(l−) is a function of l−,
this encoding takes I[η(l−); l−] bits. If knowing η1 lets
us compute η2, which is also suﬃcient, then η2 is a more
concise summary, and I[η1(l−); l−] ≥I[η2(l−); l−].
minimal suﬃcient statistic can be computed from
any other suﬃcient statistic. We now construct one.
Take two past light cone conﬁgurations, l−
Each has some conditional distribution over future light
cone conﬁgurations, P(l+|l−
1 ) and P(l+|l−
2 ) respectively.
The two past conﬁgurations are equivalent, l−
those conditional distributions are equal. The set of con-
ﬁgurations equivalent to l−is [l−]. Our statistic is the
function which maps past conﬁgurations to their equivalence classes:
ǫ(l−) ≡[l−] =
λ : P(l+|λ) = P(l+|l−)
Clearly, P(l+|ǫ(l−)) = P(l+|l−), and so I[l+; ǫ(l−)] =
I[l+; l−], making ǫ a suﬃcient statistic.
The equivalence classes, the values ǫ can take, are the causal states
 . Each causal state is a set of speciﬁc past
light-cones, and all the cones it contains are equivalent,
predicting the same possible futures with the same probabilities. Thus there is no advantage to subdividing the
causal states, which are the coarsest set of predictively
suﬃcient states.
For any suﬃcient statistic η, P(l+|l−) = P(l+|η(l−)).
So if η(l−
1 ) = η(l−
2 ), then P(l+|l−
1 ) = P(l+|l−
2 ), and the
two pasts belong to the same causal state. Since we can
get the causal state from η(l−), we can use the latter to
compute ǫ(l−). Thus, ǫ is minimal. Moreover, ǫ is the
unique minimal suﬃcient statistic : any other just
relabels the same states.
Because ǫ is minimal, I[ǫ(l−); l−] ≤I[η(l−); l−], for
any other suﬃcient statistic η. Thus we can speak objectively about the minimal amount of information needed
to predict the system, which is how much information
about the past of the system is relevant to predicting its
own dynamics. This quantity, I[ǫ(l−); l−], is a characteristic of the system, and not of any particular model. We
deﬁne the statistical complexity as
C ≡I[ǫ(l−); l−]
C is the amount of information required to describe the
behavior at that point, and equals the log of the eﬀective
number of causal states, i.e., of diﬀerent distributions for
the future. Complexity lies between disorder and order
 , and C = 0 both when the ﬁeld is completely
disordered (all values of x are independent) and completely ordered (x is constant). C grows when the ﬁeld’s
dynamics become more ﬂexible and intricate, and more
information is needed to describe the behavior.
We now sketch an algorithm to recover the causal
states from data, and so estimate C. ( provides details, including pseudocode; cf. .) At each time t, list
the observed past and future light-cone conﬁgurations,
and put the observed past conﬁgurations in some arbitrary order,
. (In practice, we must limit how far
light-cones extend into the past or future.) For each past
conﬁguration l−
i , estimate Pt(l+|l−
i ). We want to estimate the states, which ideally are groups of past cones
with the same conditional distribution over future cone
conﬁgurations.
Not knowing the conditional distributions a priori, we must estimate them from data, and
with ﬁnitely many samples, such estimates always have
some error. Thus, we approximate the true causal states
by clusters of past light-cones with similar distributions
over future light-cones; the conditional distribution for a
cluster is the weighted mean of those of its constituent
past cones. Start by assigning the ﬁrst past, l−
ﬁrst cluster. Thereafter, for each l−
i , go down the list of
existing clusters and check whether Pt(l+|l−
i ) diﬀers signiﬁcantly from each cluster’s distribution, as determined
by a ﬁxed-size χ2 test. (We used α = 0.05 in our simulations below.) If the discrepancy is insigniﬁcant, add l−
to the ﬁrst matching cluster, updating the latter’s distribution. Make a new cluster if l−
i does not match any
existing cluster. Continue until every l−
is assigned to
some cluster. The clusters are then the estimated causal
states at time t. Finally, obtain the probabilities of the
diﬀerent causal states from the empirical probabilities of
their constituent past conﬁgurations, and calculate C(t).
This procedure converges on the correct causal states as
it gets more data, independent of the order of presentation of the past light-cones, the ordering of the clusters,
or the size α of the signiﬁcance test . For ﬁnite data,
the order of presentation matters, but we ﬁnesse this by
randomizing the order.
We say a system has organized between times t1 and
t2 if (I) C(t2) −C(t1) ≡∆C > 0. It has self-organized
if (II) some of the rise in complexity is not due to external agents. We can check condition (I) by estimating
∆C. We know condition (II) holds for many systems,
because they either have no external inputs (e.g., deterministic CA), or only unstructured inputs (e.g., chemical
pattern-formers exposed to thermal noise). For systems
with structured input, we need, but lack, a way to say
how much of ∆C is due to that input. We could, perhaps, treat this as a causal inference problem , with
∆C as the response variable, and the input as the treatment. Alternately, we could see how much ∆C changes if
we replace the input with statistically-similar noise .
Numerical Experiments and Results Having developed a quantitative criterion for self-organization, we
now check it experimentally. Our test systems are cyclic
cellular automata (CCA), which are models of pattern
formation in excitable media . Each site in a square
lattice has one of κ colors. A cell of color k will change
its color to k + 1 mod κ if there are already at least T
(“threshold”) cells of that color in its neighborhood, i.e.,
within a distance r (“range”) of that cell.
Otherwise,
the cell keeps its current color. (In normal excitable media, which have a unique quiescent state, the role of the
threshold is slightly diﬀerent .) All cells update their
colors in parallel.
CCA have three generic long-run behaviors, depending on the ratio of the threshold to the range. At high
FIG. 1: (Color online.) Phases of the cyclic CA. Parameters
are as described in the text, started from uniform random
initial conditions. Color ﬁgures were prepared with . From
the top left: (a) Local oscillations (T = 1), in which the CA
oscillates with period 4, each cell cycling through all colors;
(b) Spiral waves (T = 2); (c) The “turbulent” phase (T = 3);
(d) Fixation with solid color blocks (T = 4).
thresholds, CCA form homogeneous blocks of solid colors, which are completely static (“ﬁxation”).
low thresholds, the entire lattice eventually oscillates periodically; sometimes rotating spiral waves grow to engulf
the entire lattice. With intermediate thresholds, incoherent traveling waves form, propagate, collide and disperse;
this, metaphorically, is “turbulence”. With a range one
Moore (box) neighborhood and κ = 4, the phenomenology is as follows (see Fig. 1). T = 1 and T = 2 are
both locally periodic, but T = 2 produces spiral waves,
while T = 1 quenches incoherent local oscillations. T = 3
leads to meta-stable turbulence — spiral waves can form
and entrain the entire CA, but turbulence can persist indeﬁnitely on ﬁnite lattices. Fixation occurs with T ≥4.
All CCA phases self-organize when started from uniform
noise. (This is best appreciated by viewing simulations
 .) By the same intuitive standard, the ﬁxation phase
is less organized than turbulence (which has dynamic,
large-scale spatial structures), which in turn is less organized than spiral waves (which has more intricate structures). It is hard to say, by eye, whether incoherent local oscillations are more or less organized than simple
ﬁxation. All four regimes lead to stable stationary distributions. Thus, C should start at zero (reﬂecting the
totally random initial conditions), rise to a steady value,
and stay there. T = 2 should have the highest long-run
complexity, followed by T = 3.
We ran κ = 4, r = 1 CCA on 300 × 300 lattices with
periodic boundary conditions, for T from 1 to 4. Figure
2 shows the results of applying our proposed measure
of self-organization to these simulations. We used lightcones extending 1 time-step into both past and future;
longer light-cones did not, here, lead to diﬀerent states.
The agreement with expectations is clear. All four curves
climb steadily to plateaus, leveling oﬀwhen the distribu-
Complexity (bits per site)
Complexity versus Time
FIG. 2: (Color online.) Complexity over time for CCA with
diﬀerent thresholds T , averaging 30 independent simulations
at each value of T . The T = 2 curve has the highest asymptote, followed by T = 3, T = 4 and T = 1.
Error bars:
standard error of the complexity.
tion of CA conﬁgurations become stationary. Sampling
noise leads to ﬂuctuations around the asymptotic values
 . The slight fall in complexity for T = 3 occurs when
spirals try to form but break up, and their debris limit
further spiral formation. Additional simulations at diﬀerent lattice sizes L show the estimated long-run complexity growing with L, approaching a limit as O(L−1). This
rate combines ﬁnite-size eﬀects with the negative bias of
our information estimator, which is at least O(L−2) .
We hope in the future to precisely determine both our estimation bias and the ﬁnite-size scaling of the complexity.
Conclusion A theory of self-organization should predict when and why diﬀerent systems will assume diﬀerent
kinds and degrees of organization. This will require an
adequate characterization of self-organization.
We argue that “internally-caused rise in complexity” works,
if we deﬁne complexity as the amount of information
needed for optimal statistical prediction.
We can reliably estimate this statistical complexity from data, and
for CCA, the estimates match intuitive judgments about
self-organization. The methods used are not limited to
CA, but apply to all kinds of discrete random ﬁelds,
including ones on complex networks .
They would
work equally well on discretized empirical data, e.g., digital movies of chemical pattern formation experiments.
This is a ﬁrst step towards a physical theory of selforganization.
Acknowledgments We thank D. Abbott, J. Crutchﬁeld,
R. D’Souza, D. Feldman, D. Griﬀeath, C. Moore, S. Page,
M. Porter, E. Smith, J. Usinowicz and our referees.
∗Electronic address: 
† Electronic address: 
‡ Electronic address: 
§ Supported by a grant from the James S. McDonnell Foundation.
¶ Supported by DR Project 200153, and the Department
of Energy, under contract W-7405-ENG-36.
 W. R. Ashby, J. General Psychology 37, 125 .
 G. Nicolis and I. Prigogine, Self-Organization in Nonequilibrium Systems .
 R. M. D’Souza and N. H. Margolus, Phys. Rev. E 60,
264 , URL cond-mat/9810258.
 E. Smith, Phys. Rev. E 68, 046114 .
 W. V. O. Quine, From a Logical Point of View .
 R. Fisch, J. Gravner, and D. Griﬀeath, Stat. Comput. 1,
23 , URL psoup.math.wisc.edu/papers/tr.zip.
 C. R. Shalizi and K. L. Shalizi, in Noise in Complex Systems and Stochastic Dynamics, edited by L. Schimansky-
Geier et al. , pp.
108–117, URL bactra.org/research/FN03.pdf.
 P. Bak, C. Tang, and K. Wiesenfeld, Phys. Rev. Lett.
59, 381 .
 S. Wolfram, Rev. Mod. Phys. 55, 601 .
 J. C. Krepeau and L. K. Isaacson, J. Noneq. Thermo. 15,
115 .
 Y. L. Klimontovich, Turbulent Motion and the Structure
of Chaos .
 R. F. Fox, Energy and the Evolution of Life .
 G. L. Sewell, Quantum Mechanics and Its Emergent
Macrophysics .
 R. Badii and A. Politi, Complexity: Hierarchical Structures and Scaling in Physics .
 P. L. Privalov and S. J. Gill, Adv. Protein Chem. 39, 191
 C. H. Bennett, in Emerging Syntheses in Science, edited
by D. Pines , pp. 215–234.
 P. Grassberger, Int. J. Theor. Phys. 25, 907 .
 J. P. Crutchﬁeld and K. Young, Phys. Rev. Lett. 63, 105
 C. R. Shalizi and J. P. Crutchﬁeld, J. Stat. Phys. 104,
817 , URL cond-mat/9907176.
 C. R. Shalizi and C. Moore, Studies Hist. Phil. Mod.
Phys. submitted , URL cond-mat/0303625.
 C. R. Shalizi,
Discrete Math. Theor. Comput. Sci.
AB(DMCS), 11 , URL math.PR/0305160.
 U. Parlitz and C. Merkwirth, Phys. Rev. Lett. 84, 1890
 S. Kullback, Information Theory and Statistics .
 D. Blackwell and M. A. Girshick, Theory of Games and
Statistical Decisions .
 J. Pearl, Causality: Models, Reasoning, and Inference
 .
 J. Delgado and R. V. Sol´e, Physical Review E 55, 2338
 J. J. Tyson and J. P. Keener, Physica D 32, 327 .
 M. W´ojtowicz, Cellebration, Online software ,
URL psoup.math.wisc.edu/mcell/.
 J. D. Victor, Neural Computation 12, 2797 .