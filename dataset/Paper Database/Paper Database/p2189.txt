The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)
Joint Domain Alignment and Discriminative Feature
Learning for Unsupervised Deep Domain Adaptation∗
Chao Chen,† Zhihong Chen, Boyuan Jiang, Xinyu Jin
Institute of Information Science and Electronic Engineering
Zhejiang University, Hangzhou, China
{chench,zhihongchen,byjiang,jinxy}@zju.edu.cn
Recently, considerable effort has been devoted to deep domain adaptation in computer vision and machine learning
communities. However, most of existing work only concentrates on learning shared feature representation by minimizing the distribution discrepancy across different domains.
Due to the fact that all the domain alignment approaches can
only reduce, but not remove the domain shift, target domain
samples distributed near the edge of the clusters, or far from
their corresponding class centers are easily to be misclassi-
ﬁed by the hyperplane learned from the source domain. To
alleviate this issue, we propose to joint domain alignment
and discriminative feature learning, which could beneﬁt both
domain alignment and ﬁnal classiﬁcation. Speciﬁcally, an
instance-based discriminative feature learning method and a
center-based discriminative feature learning method are proposed, both of which guarantee the domain invariant features
with better intra-class compactness and inter-class separability. Extensive experiments show that learning the discriminative features in the shared feature space can signiﬁcantly
boost the performance of deep domain adaptation methods.
Introduction
Domain adaptation, which focuses on the issues of how to
adapt the learned classiﬁer from a source domain with a
large amount of labeled samples to a target domain with limited or no labeled target samples even though the source and
target domains have different, but related distributions, has
received more and more attention in recent years. According
to , there are three commonly used domain adaptation approaches: feature-based
domain adaptation, instance-based domain adaptation and
classiﬁer-based domain adaptation. The feature-based methods, which aim to learn a shared feature representation by
minimizing the distribution discrepancy across different domains, can be further distinguished by: (a) the considered
class of transformations , (b) the types of discrepancy metrics, such as Maximum Mean Discrepancy (MMD)
∗This work was supported by the opening foundation of the
State Key Laboratory (No. 2014KF06), and the National Science
and Technology Major Project (No. 2013ZX03005013).
†Chao Chen and Zhihong Chen contributed equally.
Copyright c⃝2019, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Figure 1: The necessity of joint domain alignment and discriminant features learning. Red: Source samples; Blue: Target samples; Black line: Hyperplane learned from the source
domain; Circle, Square and Star indicate three different categories, respectively. (a) Source Only, due to the domain shift,
the hyperplane learned from the source samples will misclassify a large amount of target samples. (b) Domain Alignment Only, the domain shift has been greatly reduced, but
not removed, by the domain alignment. Therefore, the hyperplane learned from the source will still misclassify a few
target samples which are almost distributed near the edge of
the clusters, or far from their corresponding class centers. (c)
Joint Domain Alignment and Discriminative Feature Learning, the hyperplane learned from the source can perfectly
classify the target samples due to the discriminative-ness of
the domain invariant features. (Best Viewed in Color)
 , Correlation Alignment (CORAL) , Center Moment Discrepancy (CMD)
 , etc. The instance reweighting (also
called landmarks selection) is another typical strategy for
domain adaptation , which considers
that some source instances may not be relevant to the target even in the shared subspace. Therefore, it minimizes the
distribution divergence by reweighting the source samples
or selecting the landmarks, and then learns from those samples that are more similar to the target samples. Apart of this,
the classiﬁer-based domain adaptation represents another independent line of work , which adapts
the source model to the target by regularizing the difference
between the source and target model parameters.
As mentioned above, the most recent work only devote to
mitigating the domain shift by domain alignment. However,
all the domain alignment approaches can only reduce, but
not remove, the domain discrepancy. Therefore, the target
samples distributed near the edge of the clusters, or far from
their corresponding class centers are most likely to be misclassiﬁed by the hyperplane learned from the source domain.
To alleviate this issue, a practical way is to enforce the target
samples with better intra-class compactness. In this way, the
number of samples that are far from the high density region
and easily to be misclassiﬁed will be greatly reduced. Similarly, another feasible measure is to eliminate the harmful
effects of the domain mismatch in the aligned feature space
by enlarging the difference across different categories. However, under the unsupervised domain adaptation setting, it is
quite difﬁcult and inaccurate to obtain the category or cluster
information of the target samples. Therefore, to enforce the
target samples with better intra-class compactness and interclass separability directly is somehow a hard work. Fortunately, recall that the source domain and target domain are
highly-related and have similar distributions in the shared
feature space. In this respect, it is reasonable to make the
source features in the aligned feature space more discriminative, such that the target features maximally aligned with the
source domain will become discriminative automatically.
In this work, we propose to joint domain alignment
and discriminative feature learning for unsupervised deep
domain adaptation (JDDA). As can be seen in Fig. 1, we illustrate the necessity of joint domain alignment and discriminative feature learning. The merits of this paper include:
(1) As far as we know, this is the ﬁrst attempt to jointly learn
the discriminative deep features for deep domain adaptation.
(2) Instance-based and center-based discriminative learning
strategies are proposed to learn the deep discriminative features.
(3) We analytically and experimentally demonstrate that
the incorporation of the discriminative shared representation
will further mitigate the domain shift and beneﬁt the ﬁnal
classiﬁcation, which would signiﬁcantly enhance the transfer performance.
Related Work
Recently, a great deal of efforts have been made for domain adaptation based on the deep architectures. Among
them, most of the deep domain adaptation methods follow
the Siamese CNN architectures with two streams, representing the source model and target model respectively. In
 ,
the two-stream CNN shares the same weights between the
source and target models, while explores the two-stream CNN with related but non-shared parameters. As concluded in , the most commonly used deep domain adaptation
approaches can be roughly classiﬁed into three categories:
(1) Discrepancy-based methods, (2) Reconstruction-based
methods and (3) Adversarial adaptation methods.
The typical discrepancy-based methods can be seen in
 .
They are usually achieved by adding an additional loss to
minimize the distribution discrepancy between the source
and target domains in the shared feature space. Specially,
Zeng et al. explores the Maximum Mean
Discrepancy (MMD) to align the source and target domains,
while Long et al. extend the MMD to multi-kernel MMD
 which aligns the joint distributions
of multiple domain-speciﬁc layers across domains. Another
impressive work is DeepCORAL ,
which extends the CORAL to deep architectures, and aligns
the covariance of the source and target features. Besides,
the recently proposed Center Moment Discrepancy (CMD)
 diminishes the domain shift by aligning the central moment of each order across domains.
Another important line of work is the reconstructionbased deep domain adaptation , which
jointly learns the shared encoding representation for both
source label prediction and unlabeled target samples reconstruction. In contrast, domain separation networks (DSN)
 introduce the notion of a private
subspace for each domain, which captures domain speciﬁc
properties using the encoder-decoder architectures. Besides,
Tan et al. propose a Selective Learning Algorithm (SLA)
 , which gradually selects the useful unlabeled data from the intermediate domains using the reconstruction error. The adversarial adaptation method is another
increasingly popular approach. The representative work is
to optimize the source and target mappings using the standard minimax objective , the symmetric confusion objective or the inverted label objective .
Recently, there is a trend to improve the performance of
CNN by learning even more discriminative features. Such as
contrastive loss and center loss , which are proposed to learn discriminative deep features for face recognition and face veriﬁcation. Besides, the
large-margin softmax loss (L-Softmax) is
proposed to generalize the softmax loss to large margin softmax, leading to larger angular separability between learned
features. Inspired by these methods, we propose two discriminative feature learning methods, i.e., Instance-Based
discriminative feature learning and Center-Based discriminative feature learning. By jointing domain alignment and
discriminative feature learning, the shared representations
could be better clustered and more separable, which can evidently contribute to domain adaptation.
Our Approach
In this section, we present our proposed JDDA in detail.
Following their work , the two-stream CNN architecture
with shared weights is adopted. As illustrated in Fig. 2, the
ﬁrst stream operates the source data and the second stream
operates the target data. What distinguishes our work from
others is that an extra discriminative loss is proposed to
encourage the shared representations to be more discriminative, which is demonstrated to be good for both domain
alignment and ﬁnal classiﬁcation.
In this work, following the settings of unsupervised domain adaptation, we deﬁne the labeled source data as Ds =
Conv Layer
Conv Layer
Conv Layer
Conv Layer
Conv Layer
Conv Layer
Bottleneck
Bottleneck
Discrepancy
Discriminative
Figure 2: The proposed two-stream CNN for domain adaptation. We introduce a discriminative loss, which enforces
the domain invariant features with smaller intra-class scatter
and better inter-class separability. Note that both the domain
discrepancy loss and the discriminative loss are applied in
the bottleneck layer.
{Xs, Ys} = {(xs
i=1 and deﬁne the unlabeled target
data as Dt = {Xt} = {xt
i=1, where xs and xt have the
same dimension xs(t) ∈Rd. Let Θ denotes the shared parameters to be learned. Hs ∈Rb×L and Ht ∈Rb×L denote
the learned deep features in the bottleneck layer regard to the
source stream and target stream, respectively. b indicates the
batch size during the training stage and L is the number of
hidden neurons in the bottleneck layer. Then, the networks
can be trained by minimizing the following loss function.
L(Θ|Xs, Ys, Xt) = Ls + λ1Lc + λ2Ld
Lc = CORAL(Hs, Ht)
Ld = Jd(Θ|Xs, Ys)
Here, Ls, Lc and Ld represent the source loss, domain
discrepancy loss and discriminative loss, respectively. λ1
and λ2 are trade-off parameters to balance the contributions of the domain discrepancy loss and the discriminative loss. Speciﬁcally, c(Θ|xs
i ) denotes the standard classiﬁcation loss with respect to the source data.
CORAL(Hs, Ht) denotes the domain discrepancy loss measured by the correlation alignment (CORAL)
 .
Jd(Θ|Xs, Ys) indicates our proposed discriminative loss,
which guarantees the domain invariant features with better
intra-class compactness and inter-class separability.
Correlation Alignment
To learn the domain invariant features, the CORAL is
adopted, which diminishes the domain discrepancy by aligning the covariance of the source and target features. The
domain discrepancy loss measured by CORAL can be expressed as
Lc = CORAL(Hs, Ht) =
4L2 ∥Cov(Hs) −Cov(Ht)∥2
where ∥· ∥2
F denotes the squared matrix Frobenius norm.
Cov(Hs) and Cov(Ht) denote the covariance matrices
of the source and target features in the bottleneck layer,
which can be computed as Cov(Hs) = H⊤
s JbHs, and
Cov(Ht) = H⊤
t JbHt. Jb = Ib −1
n is the centralized matrix, where 1b ∈Rb is an all-one column vector, and b is the batch-size. Note that the training process is implemented by mini-batch SGD, therefore, only a
batch of training samples are aligned in each iteration. Interested readers may refer for more details.
Discriminative Feature Learning
In order to enforce the two-stream CNN to learn even more
discriminative deep features, we propose two discriminative
feature learning methods, i.e., the Instance-Based discriminative feature learning and the Center-Based discriminative
feature learning. It is worth noting that the whole training
stage is based on mini-batch SGD. Therefore, the discriminative loss presented below is based on a batch of samples.
Instance-Based Discriminative Loss
The motivation of
the Instance-Based discriminative feature learning is that the
samples from the same class should be as closer as possible
in the feature space, and the samples from different classes
should be distant from each other by a large margin. In this
respect, the Instance-Based discriminative loss LI
formulated as
max(0, ∥hs
max(0, m2 −∥hs
i ∈RL (L is the number of neurons in the bottleneck layer) denotes the i-th deep feature of bottleneck layer
w.r.t. the i-th training sample, and Hs = [hs
2; · · · ; hs
Cij = 1 means that hs
j are from the same class, and
Cij = 0 means that hs
j are from different classes.
As can be seen in (6)(7), the discriminative loss will enforce the distance between intra-class samples no more than
m1 and the distance between the paired inter-class samples at least m2 (m2 should be larger than m1). Intuitively,
this penalty will undoubtedly enforce the deep features to
be more discriminative. For brevity, we denote the pairwise
distance of the deep features Hs as DH ∈Rb×b, where
j∥2. Let L ∈Rb×b denotes the indictor matrix, Lij = 1 if the i-th and j-th samples are from the same
class and Lij = 0 if they are from different classes. Then,
the Instance-Based discriminative loss can be simpliﬁed to
d = α∥max(0, DH −m1)2 ◦L∥sum
+ ∥max(0, m2 −DH)2 ◦(1 −L)∥sum
where the square operate denotes element-wise square and
”◦” denotes element-wise multiplication. ∥·∥sum represents
the sum of all the elements in the matrix. α is the tradeoff parameter introduced to balance the intra-class compactness and inter-class separability. Note that the Instance-
Based discriminative learning method is quite similar with
the manifold embedding related methods. Both of them encourage the similar samples to be closer
and dissimilar samples to be far from each other in the embedding space. The difference is that the similarity in our
proposal is deﬁned by the labels, while the manifold embedding is an unsupervised approach and deﬁnes the similarity
by the distance in the input space.
Center-Based Discriminative Loss
To calculate the
Instance-Based discriminative loss, the calculation of pairwise distance is required, which is computationally expensive. Inspired by the Center Loss which
penalizes the distance of each sample to its corresponding
class center, we proposed the Center-Based discriminative
feature learning as below.
max(0, ∥hs
i,j=1,i̸=j
max(0, m2 −∥ci −cj∥2
where β is the trade-off parameter, m1 and m2 are two constraint margins (m1 < m2). The cyi ∈Rd denotes the yi-th
class center of the deep features, yi ∈{1, 2, · · · , c} and c
is the number of class. Ideally, the class center ci should be
calculated by averaging the deep features of all the samples.
Due to the fact that we perform the update based on minibatch, it is quite difﬁcult to average the deep features by the
whole training set. Herein, we make a necessary modiﬁcation. For the second term of the discriminative loss in (9),
the ci and cj used to measure the inter-class separability are
approximately computed by averaging the current batch of
deep features, which we call the ”batch class center”. Instead, the cyi used to measure the intra-class compactness
should be more accurate and closer to the ”global class center”. Therefore, we updated the cyi in each iteration as
i=1 δ(yi = j)(cj −hs
i=1 δ(yi = j)
j −γ · ∆ct
The ”global class center” is initialized as the ”batch class
center” in the ﬁrst iteration and updated according to the
coming batch of samples via (10)(11) in each iteration,
where γ is the learning rate to update the ”global class center”. For brevity, (9) can be simpliﬁed to
d = β∥max(0, Hc −m1)∥sum+
∥max(0, m2 −Dc) ◦M∥sum
where Hc = [hc
2; . . . ; hc
b] has the same size as Hs, and
2 denotes the distance between the i-th deep
feature hs
i and its corresponding center cyi. Dc ∈Rc×c
denotes the pairwise distance of the ”batch class centers”,
ij = ∥ci −cj∥2
2. M = 1b1⊤
b −Ib, and ”◦” denotes
the element-wise multiplication. Different from the Center
Loss, which only considers the intra-class compactness. Our
proposal not only penalizes the distances between the deep
features and their corresponding class centers, but also enforces large margins among centers across different categories.
Discussion
Whether it is Instance-Based method or
Center-Based method, it can make the deep features more
discriminative. Besides, these two methods can be easily implemented and integrated into modern deep learning
frameworks. Compared with the Instance-Based method, the
computation of the Center-Based method is more efﬁcient.
Speciﬁcally, The computational complexity of Center-Based
method is theoretically O(nsc + c2) and O(bc + c2) when
using mini-batch SGD, while the Instance-Based method
needs to compute the pairwise distance, therefore, its complexity is O(n2
s) in theory and O(b2) when using mini-batch
SGD. Besides, the Center-Based method should converge
faster intuitively (this can also be evidenced in our experiments), because it takes the global information into consideration in each iteration, while the Instance-Based method
only regularizes the distance of pairs of instances.
Both the proposed Instance-Based joint discriminative domain adaptation (JDDA-I) and Center-Based joint discriminative domain adaptation (JDDA-C) can be easily implemented via the mini-batch SGD. For the JDDA-I, the total
loss is given as L = Ls+λ1Lc+λI
d, while the source loss
is deﬁned by the conventional softmax classiﬁer. Lc deﬁned
in (5) and LI
d deﬁned in (8) are both differentiable w.r.t. the
inputs. Therefore, the parameters Θ can be directly updated
by the standard back propagation
Θt+1 = Θt −η ∂ ,
the other is a large-scale digital recognition dataset. The
source code of the JDDA has been released online1
is a standard benchmark for domain adaptation
in computer vision, comprising 4,110 images in 31 classes
collected from three distinct domains: Amazon (A), which
contains images downloaded from amazon.com, Webcam
(W) and DSLR (D), which contain images taken by web
camera and digital SLR camera with different photographical settings, respectively. We evaluate all methods across
all six transfer tasks A→W, W→A, W→D, D→W, A→D
1 
and D→A as in . These tasks represent
the performance on the setting where both source and target
domains have small number of samples.
Digital recognition dataset
contains ﬁve widely used
benchmarks: Street View House Numbers (SVHN) , MNIST , MNIST-M
 , USPS and synthetic digits dataset (syn digits) , which consist
10 classes of digits. We evaluate our approach over four
cross-domain pairs: SVHN→MNIST, MNIST→MNIST-
M, MNIST→USPS and synthetic digits→MNIST. Different from Ofﬁce-31 where different domains are of small but
different sizes, each of the ﬁve domains has a large-scale and
a nearly equal number of samples, which makes it a good
complement to Ofﬁce-31 for more controlled experiments.
Compared Approaches
We mainly compare our proposal
with Deep Domain Confusion (DDC) ,
Deep Adaptation Network (DAN) , Domain Adversarial Neural Network (DANN) , Center Moment Discrepancy (CMD) , Adversarial Discriminative Domain Adaptation
(ADDA) and Deep Correlation Alignment (CORAL) since these
approaches and our JDDA are all proposed for learning domain invariant feature representations.
Implementation Details
For the experiments on Ofﬁce-31, both the compared approaches and our proposal are trained by ﬁne-tuning the
ResNet pre-trained on ImageNet, and the activations of the
last layer pool5 are used as image representation. We follow standard evaluation for unsupervised adaptation and use all source examples with labels and all
target examples without labels. For the experiments on digital recognition dataset, we use the modiﬁed LeNet to verify
the effectiveness of our approach. We resize all images to 32
× 32 and convert RGB images to grayscale. For all transfer
tasks, we perform ﬁve random experiments and report the
averaged results across them. For fair comparison, all deep
learning based models above have the same architecture as
our approach for the label predictor.
Note that all the above methods are implemented via tensorﬂow and trained with Adam optimizer. When ﬁne-tuning
the ResNet (50 layers), we only update the weights of the
full-connected layers (fc) and the ﬁnal block (scale5/block3)
and ﬁx other layers due to the small sample size of the
Ofﬁce-31. For each approach we use a batch size of 256
samples in total with 128 samples from each domain, and set
the learning rate η to 10−4 and the learning rate of ”global
class center” γ to 0.5. When implementing the methods proposed by others, instead of ﬁxing the adaptation factor λ, we
gradually update it from 0 to 1 by a progressive schedule:
1+exp(−µp) −1, where p is the training progress linearly changing from 0 to 1 and µ = 10 is ﬁxed throughout
experiments . This progressive strategy reduces parameter sensitivity and eases the selection of models. As our approach can work stably across different transfer tasks, the hyper-parameter λ2 is ﬁrst selected according
to accuracy on SVHN→MNIST (results are shown in the
Figure 6) and then ﬁxed as λI
2 = 0.03 (JDDA-I) and λC
0.01 (JDDA-C) for all other transfer tasks. We also ﬁxed
the constraint margins as m1 = 0 and m2 = 100 throughout
experiments.
Result and Discussion
The unsupervised adaptation results on the Ofﬁce-31 dataset
based on ResNet are shown in Table 1. As can be seen,
our proposed JDDA outperforms all comparison methods
on most transfer tasks. It is worth noting that our approach
improves the classiﬁcation accuracy substantially on hard
transfer tasks, e.g. A→W where the source and target domains are remarkably different and W→A where the size of
the source domain is even smaller than the target domain,
and achieves comparable performance on the easy transfer
tasks, D→W and W→D, where source and target domains
are similar. Thus we can draw a conclusion that our proposal
has the ability to learn more transferable representations and
can be applied to small-scale datasets adaption effectively
by using a pre-trained deep model.
The results reveal several interesting observations. (1)
Discrepancy-based methods achieve better performance
than standard deep learning method (ResNet), which con-
ﬁrms that embedding domain-adaption modules into deep
networks (DDC, DAN, CMD, CORAL) can reduce the
domain discrepancy and improve domain adaptation performance. (2) Adversarial adaptation methods (DANN,
ADDA) outperform source-only method, which validates
that adversarial training process can learn more transferable
features and thus improve the generalization performance.
(3) The JDDA model performs the best and sets new stateof-the-art results. Different from all previous deep transfer learning methods that the distance relationship of the
source samples in the feature space is unconsidered during
training, we add a discriminative loss using the information
of the source domain labels, which explicitly encourages
intra-class compactness and inter-class separability among
learned features.
In contrast to Ofﬁce-31 dataset, digital recognition dataset
has a much larger domain size. With these large-scale transfer tasks, we are expecting to testify whether domain adaptation improves when domain sizes is large. Table 2 shows
the classiﬁcation accuracy results based on the modiﬁed
LeNet. We observe that JDDA outperforms all comparison
methods on all the tasks. In particular, our approach improves the accuracy by a huge margin on difﬁcult transfer
tasks, e.g. SVHN→MNIST and MNIST→MNIST-M. In the
task of SVHN→MNIST, the SVHN dataset contains signiﬁcant variations (in scale, background clutter, blurring,
slanting, contrast and rotation) and there is only slightly
variation in the actual digits shapes, that makes it substantially different from MNIST. In the domain adaption scenario of MNIST→MNIST-M. The MNIST-M quite distinct
from the dataset of MNIST, since it was created by using
each MNIST digit as a binary mask and inverting with it the
colors of a background image randomly cropped from the
Berkeley Segmentation Data Set . The
above results suggest that the proposed discriminative loss
Table 1: results (accuracy %) on Ofﬁce-31 dataset for unsupervised domain adaptation based on ResNet
ResNet 
DDC 
DAN 
DANN 
CMD 
CORAL 
Table 2: results (accuracy %) on digital recognition dataset for unsupervised domain adaptation based on modiﬁed LeNet
SVHN→MNIST
MNIST→MNIST-M
USPS→MNIST
Modiﬁed LeNet
DDC 
DAN 
DANN 
CMD 
ADDA 
CORAL 
Ld is also effective for large-scale domain adaption.
Feature Visualization
To better illustrate the effectiveness of our approach, we randomly select 2000 samples in
the source domain, set the feature (input of the softmax loss)
dimension as 2 and then plot the 2D features in Figure 3.
Compared with the features obtained by methods without
the proposed discriminative loss Ld (Figure 3a and 3c), the
features obtained by the methods with our discriminative
loss (Figure 3b and 3d) become much more compact and
well separated. In particular, the features given by Source
Only (Figure 3a) are in the form of a strip, while the features
given by Source Only with our discriminative loss (Figure
3b) are tightly clustered, and there exists a great gap between
the clusters. This demonstrates that our proposal can make
the model learn more distinguishing features.
The visualizations of the learned features in Figure 3 show
great discrimination in the source domain. But this does not
mean that our method is equally effective on the target domain. Therefore, we visualize the t-SNE embeddings of the last hidden layer learned by CORAL
or JDDA on transfer task SVHN→MNIST in Figures 4a-4b
(with category information) and Figures 4c-4d (with domain
information). We can make intuitive observations. (1) Figure
4a (Ls + Lc) has more scattered points distributed on the
inter-class gap than Figure 4b (Ls + Lc + Ld) , which suggests that the features learned by JDDA are discriminated
much better than that learned by CORAL (larger class-toclass distances). (2) As shown in Figures 4c (Ls + Lc) and
Figures 4d (Ls + Lc + Ld), with CORAL features, categories
are not aligned very well between domains, while with features learned by JDDA, the categories are aligned much better. All of the above observations can demonstrate the advantages of our approach: whether in the source or target
domain, the model can learn more transferable and more distinguishing features with the incorporation of our proposed
discriminative loss.
Convergence Performance
We evaluate the convergence
performance of our method through the test error of the
training process. Figure 5 shows the test errors of different methods on SVHN→MNIST and MNIST→MNIST-M,
which reveals that incorporating the proposed discriminative
loss helps achieve much better performance on the target domain. What’s more, the trend of convergence curve suggests
that JDDA-C converges fastest due to it considers the global
cluster information of the domain invariant features during
training. In general, our approach converges fast and stably
to a lowest test error, meaning it can be trained efﬁciently
and stably to enable better domain transfer.
Parameter Sensitivity
We investigate the effects of the
parameter λ2 which balances the contributions of our proposed discriminative loss. The larger λ2 would lead to
more discriminating deep features, and vice versa. The left
one in the Figure 6 shows the variation of average accuracy as λI
2 ∈{0.0001, 0.001, 0.003, 0.01, 0.03, 0.1, 1, 10}
2 ∈{0.001, 0.005, 0.01, 0.03, 0.05, 0.1, 1, 10} on task
SVHN→MNIST. We ﬁnd that the average accuracy increases ﬁrst and then decreases as λ2 increases and shows a
bell-shaped curve, which demonstrates a proper trade-off between domain alignment and discriminative feature learning
(a) Ls (2D)
(b) Ls + Ld (2D)
(c) Ls + Lc (2D)
(d) Ls + Lc + Ld (2D)
Figure 3: features visualization (without our discriminative loss (a)(c) VS. with our discriminative loss (b)(d)) in SVHN dataset.
It is worth noting that we set the feature (input of the Softmax loss) dimension as 2, and then plot them by class information.
(a) Ls + Lc (t-SNE)
(b) Ls + Lc + Ld (t-SNE)
(c) Ls + Lc (t-SNE)
(d) Ls + Lc + Ld (t-SNE)
Figure 4: The t-SNE visualization of the SVHN→MNIST task. (a)(b) are generated from category information and each color
in (a)(b) represents a category. (c)(d) are generated from domain information. Red and blue points represent samples of source
and target domains, respectively.
Number of Iterations (×104)
Test Error
Number of Iterations (×104)
Test Error
Figure 5: Comparison between JDDA and other stateof-the-art methods in the convergence performance on
SVHN→MNIST (right) and MNIST→MNIST-M (left).
can improve transfer performance. The right one in the Figure 6 gives an illustration of the relationship between convergence performance and λC
2 . We can observe that the model
can achieve better convergence performance as λC
2 is appropriately increased. This conﬁrms our motivation that when
the speed of feature alignment can keep up with the changing speed of the source domain feature under the inﬂuence
of our discriminative loss, we can get a domain adaptation
model with fast convergence and high accuracy.
Conclusion
In this paper, we propose to boost the transfer performance
by jointing domain alignment and discriminative feature
learning. Two discriminative feature learning methods are
0.001 0.005 0.01
Average Accuracy(%)
10−3 0.003 0.01
Number of Iterations (×103)
Test Error
Figure 6: Parameter sensitivity analysis of our approach.
The ﬁgure on the left shows the Average Accuracy w.r.t.
λ2 (the λI
2 is the hyper-parameter of JDDA-I and the λC
the hyper-parameter of JDDA-C) and the ﬁgure on the right
shows the convergence performance w.r.t. λC
proposed to enforce the shared feature space with better
intra-class compactness and inter-class separability, which
can beneﬁt both domain alignment and ﬁnal classiﬁcation.
There are two reasons that the discriminative-ness of deep
features can contribute to domain adaptation. On the one
hand, since the shared deep features are better clustered, the
domain alignment can be performed much easier. On the
other hand, due to the better inter-class separability, there
is a large margin between the hyperplane and each cluster.
Therefore, the samples distributed near the edge, or far from
the center of each cluster in the target domain are less likely
to be misclassiﬁed. Future researches may focus on how to
further mitigate the domain shift in the aligned feature space
by other constrains for the domain invariant features.