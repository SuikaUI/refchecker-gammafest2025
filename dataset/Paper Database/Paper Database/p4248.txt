Exponential concentration in quantum
kernel methods
Supanut Thanasilp
, Samson Wang
4, M. Cerezo
5,6 & Zoë Holmes2,5
Kernel methods in Quantum Machine Learning (QML) have recently gained
signiﬁcant attention as a potential candidate for achieving a quantum advantage in data analysis. Among other attractive properties, when training a
kernel-based model one is guaranteed to ﬁnd the optimal model’s parameters
due to the convexity of the training landscape. However, this is based on the
assumption that the quantum kernel can be efﬁciently obtained from quantum
hardware. In this work we study the performance of quantum kernel models
from the perspective of the resources needed to accurately estimate kernel
values. We show that, under certain conditions, values of quantum kernels
over different input data can be exponentially concentrated (in the number of
qubits) towards some ﬁxed value. Thus on training with a polynomial number
of measurements, one ends up with a trivial model where the predictions on
unseen inputs are independent of the input data. We identify four sources that
can lead to concentration including expressivity of data embedding, global
measurements, entanglement and noise. For each source, an associated concentration bound of quantum kernels is analytically derived. Lastly, we show
that when dealing with classical data, training a parametrized data embedding
with a kernel alignment method is also susceptible to exponential concentration. Our results are veriﬁed through numerical simulations for several
QML tasks. Altogether, we provide guidelines indicating that certain features
should be avoided to ensure the efﬁcient evaluation of quantum kernels and so
the performance of quantum kernel methods.
Quantum machine learning (QML) has generated tremendous amounts
of excitement, but it is important not to over-hype its potential. On the
one hand, a family of impressive results have recently established a
provable separation between the power of classical and quantum
machine learning methods in a range of contexts1–10. On the other,
many proposals remain heuristic and there are signiﬁcant questions yet
to be answered on the efﬁcient scalability of QML methods.
Quantum kernel methods, which involve embedding classical
data into quantum states and then computing their inner-products
(i.e., their kernels), or in the case of quantum data directly computing
input state overlaps, are widely viewed as particularly promising family
of QML algorithms to achieve a practical quantum advantage. To
ensure provable quantum speed-up over classical algorithms, the key
is to constructthe embedding (also called a quantum feature map) that
is capable of recognizing classically intractable complex patterns6–8.
Quantum kernels are expected to ﬁnd use in a mix of scientiﬁc and
practical applications including classifying types of supernovae in
cosmology11, probing phase transitions in quantum many-body
Received: 21 November 2022
Accepted: 31 May 2024
Check for updates
1Centre for Quantum Technologies, National University of Singapore, 3 Science Drive 2, Singapore. 2Institute of Physics, Ecole Polytechnique Fédérale de
Lausanne (EPFL), Lausanne, Switzerland. 3Chula Intelligent and Complex Systems, Department of Physics, Faculty of Science, Chulalongkorn University,
Bangkok, Thailand. 4Imperial College London, London, UK. 5Information Sciences, Los Alamos National Laboratory, Los Alamos, NM, USA. 6Quantum Science
Center, Oak Ridge, TN, USA.
e-mail: ; 
Nature Communications| 15:5200
1234567890():,;
1234567890():,;
physics12, and detecting fraud in ﬁnance13. Moreover, kernel methods
are famously said to enjoy trainability guarantees due to the convexity
of their loss landscapes14–17.
This is in contrast to Quantum Neural Networks (QNNs) where the
loss landscape is generally non-convex18,19 and can exhibit Barren Plateaus (BPs). A barren plateau is a cost landscape where the magnitudes
of gradients vanish exponentially with growing problem size20–32.
There are a number of causes that can lead to barren plateaus,
including using variational ansatze that are too expressive20,26,33 or too
entangling23,34. However, barren plateaus can even arise for inexpressive and low-entangling QNNs if the cost function relies on measuring
global properties of the system21 or if the training dataset is too
random29,35. Hardware errors can also wash out landscape features
leading to noise-induced barren plateaus28,36.
Here we argue that quantum kernel methods experience a similar
barrier to barren plateaus. Crucially, the trainability guarantees
enjoyed by kernel methods only become meaningful when the values
of the kernel can be efﬁciently estimated to a sufﬁcient precision such
that the statistical estimates contain information about the input data.
We show that under certain conditions, the value of quantum kernels
can exponentially concentrate (with increasing number of qubits)
around a ﬁxed value. In such cases, the number of shots required to
resolve the kernels to a sufﬁciently high accuracy scales exponentially.
This indicates that the efﬁcient evaluation of quantum kernels cannot
always be taken for granted. Consequently, when the kernel values are
estimated with a polynomial number of measurements, the trained
model with high probability becomes independent of input data. That
is, the predictions of the model on unseen data are the same for any
target problem that suffers from exponential concentration and thus
the learnt model is, for all intents and purposes, useless. This is summarized in Fig. 1.
This concentration of quantum kernels can in broad terms be
viewed as a result of the fact that it can be extremely difﬁcult to extract
any useful information from the (necessarily) exponentially large Hilbert space (especially in the presence of noise). We show that analogous to the causes of BPs for QNNs there are at least three different
mechanisms that can lead to the exponential concentration of the
encoded quantum states, including (i) the expressivity of the encoded
quantum state ensemble, (ii) the entanglement in encoded quantum
states with a local observable and (iii) the effect of noise. We further
show that for the case of the commonly used ﬁdelity kernel15,37, the
dependence of global measurements to evaluate the kernel can lead to
exponential concentration even when the expressivity of the embedding and the entanglement of the data states are low. In all cases, we
establish exponential concentration by deriving an analytic bound
(summarized in Table 1). We further provide numerical results
demonstrating these effects for different learning tasks.
Our work on embedding-induced concentration suggests that
problem-inspired embeddings should be used over problem-agnostic
embeddings (which are typically highly expressive and entangling).
For instance, one can construct embeddings encoding the geometrical
properties of the data38–42. However, additional care should be taken if
such embeddings are to be found through optimizing embedding
architectures, since we show this training embedding process can also
exhibit barren plateaus. Furthermore, we consider the projected
quantum kernel which is constructed by measuring local subsystems
and has been shown to maintain good generalization in a situation
where the ﬁdelity kernel fails to generalize6,7.
In contrast to QNNs where the trainability barrier caused by BPs is
now common knowledge, the community is generally less aware of the
problems posed by exponential concentration for quantum kernel
methods. The problem of exponential concentration for the ﬁdelity
quantum kernel was ﬁrst observed in ref. 6 and later analyzed in
refs. 7,43,44 in the context of generalization. ref. 7 discusses exponential concentration in the context of a projected quantum kernel for
a speciﬁc example embedding. On the other hand, refs. 8,16 provide a
rigorous study of the number of measurement shots required to successfully train the ﬁdelity kernel but do not address the issue of
exponential concentration. Here we provide a systematic treatment of
the causes and effects of exponential concentration in the presence of
shot noise. We intend our results to be viewed as a guideline to the
types of kernels and embeddings to be avoided for successful training.
Fig. 1 | Exponential concentration and its implications on kernel methods. The
exponential concentration (in the number of qubits n) of quantum kernels κðx,x0Þ,
over all possible input data pairs x,x0, can be seen to stem from the difﬁculty of
information extraction from data quantum states due to various sources (illustrated in panels a and b). The kernel concentration has a detrimental impact on the
performance of quantum kernel-based methods. As shown in panel c, for a polynomial (in n) number of measurement shots, the statistical estimates of the offdiagonal elements in the Gram matrix ^κðxi,xjÞ contain no information about the
input data (with high probability) i.e., each ^κðxi,xjÞ = ^κij. The exact behavior of the
estimated kernel value depends on the measurement strategy: for the Loschmidt
Echo test (i.e., the overlap test), ^κij concentrates to 0 for i ≠j (corresponding to the
estimated Gram matrix being an identity 1) and for the SWAP test ^κi,j for i ≠j is
indistinguishable from a data-independent random variable (corresponding to the
estimated Gram matrix being a random matrix). Ultimately, this leads to a trivial
model where the predictions on unseen inputs are independent of the training data.
 
Nature Communications| 15:5200
Moreover, our results on noise-induced kernel concentration serve as
a warning against using deep encoding schemes in the near-term. For a
more detailed survey of how our results ﬁt in the context of prior work
see Supplementary Note I.
Our results apply generally to any method that involves quantum kernels. This includes both supervised learning tasks such as regression
and classiﬁcation tasks, as well as unsupervised learning tasks such as
generative modeling and dimensional reduction. However, for concreteness we focus on supervised learning on classical data. Here, one is
given repeated access to a training dataset S :¼ fxi,yigNs
i = 1, where xi 2 X
are input vectors and yi 2 Y are associated labels. The input vectors and
labels are related by some unknown target function f : X ! Y. Our task
is to use the dataset to train a parameterized QML model ha, i.e. a
function ha : X ! Y parameterized by a, to approximate f.
The model can be trained by introducing an empirical loss La
which quantiﬁes the degree to which the model ha agrees with the
target function f over the training data S. The optimal parameters of
the model are given by
aopt :¼ arg minaLaðSÞ,
and can be obtained by minimizing the empirical loss. Once trained,
the model is tested on some unseen data. The hope is that if the dataset
is sufﬁciently large and appropriately chosen, the optimized function
haopt not only agrees on the training set but also accurately predicts the
correct labels on unseen inputs. This is exactly the question of
generalization6–10,43–63: does successful training on the training data
imply good predictive power on unseen data?
In what follows we focus on quantum kernel methods. Here, each
individual input data point xi is encoded into an n-qubit data-encoded
quantum state ρ(xi) using a data-embedding unitary U(xi), so that
ρðxiÞ = UðxiÞρ0UyðxiÞ,
for some initial state ρ0. Consequently, the training input dataset can
be seen as an ensemble of data-encoded quantum states. For now, we
leave the choice of U(xi) entirely arbitrary, and thus this framework
includes all unitary embedding schemes.
For a given input data pair x and x0 we evaluate a similarity measure κðx,x0Þ between two encoded quantum states on a quantum
computer. Formally, this is a function κ : X × X ! R corresponding to
an inner product of data states, and is known as a quantum kernel6,15,37.
Here, we consider two common choices of quantum kernels. First, we
study the ﬁdelity quantum kernel15,37, which is deﬁned as
κFQðx, x0Þ = Tr½ρðxÞρðx0Þ:
Second, we consider the projected quantum kernel6, given by
κPQðx, x0Þ = exp γ
kρkðxÞ  ρkðx0Þk2
where ρk(x) is the reduced state of ρ(x) on the k-th qubit, ∥⋅∥2 is the
Schatten 2-norm and γ is a positive hyperparameter.
The power of kernel-based learning methods stems from the fact
that they map data from X to a higher-dimensional feature space (in
this case the 2n-dimensional Hilbert space) where inner products are
taken and a decision boundary such as a support vector machine can
be trained64. Notably, thanks to the Representer Theorem, the optimal
kernel-based model is guaranteed to be expressed as a linear combination of the kernels evaluated over the training dataset (see Chapter 5
in ref. 64). More concretely, for a kernel-based QML model ha depends
on the input data through the inner product between states. We have
that the optimal solution is given by
haoptðxÞ =
optκðx, xiÞ,
where aopt = ðað1Þ
opt, . . . ,aðNsÞ
opt Þ. Additionally, if the loss L is appropriately
chosen, then the loss landscape can be guaranteed to be convex. It
follows that by constructing the Gram matrix K whose entries are
kernels over training input pairs,
½Kij = κðxi, xjÞ,
where xi,xj 2 S, the optimal parameters aopt can be found by solving
the convex optimization problem in Eq. (1). Thus if the Gram matrix
can be calculated exactly, kernel-based methods are perfectly
trainable.
As an example, in kernel ridge regression, we consider a square
loss function LaðSÞ = 1
i = 1ðhaðxiÞ  yiÞ2 + λ
H with a regularization
λ and a norm in a feature space kak2
H. The optimal parameters can be
analytically shown to be of the form
aopt = ðK  λ1Þ1y,
where y is a training label vector with the ith component yi. Another
common example is a support vector machine where we consider a
classiﬁcation
corresponding
y ∈{ −1, + 1}. Using a hinge loss function with no regularization, i.e.
i = 1 maxð0,1  haðxiÞyiÞ, the optimization problem in Eq.
(1) can be reformulated as
aopt = argmaxa
aðiÞaðjÞyiyjKij
subject to 0 ⩽a(i) for all i. Assuming that the Gram matrix K can be
accurately and efﬁciently obtained, solving for the optimal parameters
can done with a number of iterations in OðpolyðNsÞÞ.
Why exponential concentration is problematic
By virtue of their convex optimization landscapes, kernel methods are
guaranteed to obtain the optimal model from a given Gram matrix.
However, due to the probabilistic nature of quantum devices, in
practice the entries of the Gram matrix can only be estimated via
repeated measurements on a quantum device. Thus the model is only
ever trained on a statistical estimate of the Gram matrix, ^K, instead of
the exact one, K. The resulting statistical uncertainty, as we will argue
here, inhibits how well quantum kernel methods may perform.
The heart of the problem is that, in a wide range of circumstances,
the value of quantum kernels exponentially concentrate. That is, as the
size of the problem increases, the difference between kernel values
become increasingly small and so, more shots are required to distinguish between kernel entries. With a polynomial shot budget this leads
to an optimized model which is insensitive to the input data and cannot generalize well.
Table 1 | Summary of our main results
Sources of concentration
Expressivity
refs. 20,25
Global measurements
Proposition 3
Entanglement
Corollary 2
refs. 23,34
This table summarizes our key analytical results on different sources that lead to the exponential
concentration in quantum kernels as compared with BP results of QNNs in the literature.
 
Nature Communications| 15:5200
More generally, exponential concentration can be formally
deﬁned as follows.
Deﬁnition 1. (Exponential concentration) Consider a quantity X(α) that
depends on a set of variables α and can be measured from a quantum
computer as the expectation of some observable. X(α) is said to be
deterministically exponentially concentrated in the number of qubits n
towards a certain α-independent value μ if
jXðαÞ  μj ⩽β 2 Oð1=bnÞ ,
for some b > 1 and all α. Analogously, X(α) is probabilistically exponentially concentrated if
Prα½jXðαÞ  μj⩾δ ⩽β
δ2 , β 2 Oð1=bnÞ,
for b > 1. That is, the probability that X(α) deviates from μ by a small
amount δ is exponentially small for all α.
If μ additionally exponentially vanishes in the number of qubits
i.e., μ 2 Oð1=b0nÞ for some b0 > 1, we say that X(α) exponentially concentrates towards an exponentially small value.
We remark that using Chebyshev’s inequality, probabilistic
exponential concentration can also be diagnosed by analyzing the
variance of X(α). That is, X(α) is exponentially concentrated towards its
mean μ = Eα½XðαÞ if
Varα½XðαÞ 2 Oð1=bnÞ,
for b > 1, thus satisfying Deﬁnition 1. Here the variance is taken over α. If
0 ⩽X(α) ⩽1 for all α (as for quantum kernels) one can demonstrate
exponential concentration by showing that the mean μ = Eα½XðαÞ is
exponentially small which directly implies that Varα½XðαÞ 2 Oð1=bnÞ.
Furthermore, in this context when μ vanishes exponentially, we can say
that the probability of deviating from zero by an arbitrary constant
amount is exponentially small.
Deﬁnition 1 is rather general and applies to a number of QML
frameworks. In the case of quantum neural networks, X(α) = C(θ),
where α = θ and C(θ) is a cost function that depends on some variational ansatz parameters θ. In the context of quantum landscape theory, such concentration is central to studying the BP phenomenon. In
particular, the equivalence between exponentially concentrating costs
and vanishing gradients cost gradients is demonstrated in ref. 65. In
the context of quantum kernels, the quantity of interest is the quantum
kernel, i.e., XðαÞ = κðx,x0Þ where the set of variables is a pair of input
data α = fx, x0g. Hence the probability in Eq. (10) and the variance in Eq.
(11) is now taken over all possible pairs of input data fx, x0g.
To understand the problems caused by exponential concentration, let us ﬁrst consider the ﬁdelity kernel. In practice, the kernel value
is statistically estimated from measuring N samples where (on all but
classically simulable quantum devices) we assume we are restricted to
N 2 OðpolyðnÞÞ. For a given input data pair x and x0, we consider two
common measurement strategies to estimate the kernel value: (i) the
Loschmidt Echo test (i.e., the overlap test) and (ii) the SWAP test. In
either case, the ﬁdelity quantum kernel is equivalent to the expectation
value of an observable O for some quantum state ρ with the exact
expression for O and ρ depending on the strategy used. If we write the
eigendecomposition of the observable as O = P
ioi∣oiihoi∣where oi and
are the eigenvalues and eigenvectors of O respectively, then the
statistical estimate after N measurements is given by
FQðx,x0Þ = 1
Here λm is the outcome of the mth measurement and can be treated as a
probability pi = Tr½∣oiihoi∣ρ.
The behavior of the statistical estimate depends on the measurement strategy taken. When employing the Loschmidt Echo test,
the kernel value corresponds to the probability of observing the allzero bitstring. To estimate this probability, we assign + 1 to the outcome of obtaining the all-zero bitstring and assign 0 to other bitstrings. If the kernel value concentrates to an exponentially small value
i.e., μ 2 Oð1=bnÞ, then the chance of never obtaining the all-zero bitstring from N samples is (1−μ)N ≈1 −Nμ. That is, with a polynomial
number of samples N 2 OðpolyðnÞÞ, it is very likely that none are the allzero bitstring and hence likely that the statistical estimate of the kernel
is zero. This is formalized in the following proposition (proven in
Supplementary Note III A 1).
Proposition 1. Consider the ﬁdelity quantum kernel as deﬁned in Eq.
(3). Assume that the kernel values κFQðx,x0Þ exponentially concentrate
towards an exponentially small value as per Deﬁnition 1. Supposing an
N 2 OðpolyðnÞÞ shot Loschmidt Echo test is used to estimate the Gram
matrix for a training dataset S = fxi,yig of sizeNs then, with a probability
exponentially close to 1, the statistical estimate of the Gram matrix bK is
equal to the identity matrix. That is,
⩾1  δ0 , δ0 2 OðcnÞ
for some c > 1.
In the case of the SWAP test the measurement outcomes are
either + 1 with probability p + = 1=2 + κFQðx,x0Þ=2, or −1 with probability
1 −p+. Thus computing the kernel value amounts to determining the
perturbation from the uniform distribution where the + 1 and −1 outcomes occur with equal probabilities.Intuitively, when the kernel value
concentrates to an exponentially small value, the perturbation cannot
be detected with a polynomial number of measurement shots. In other
words, a statistical estimate using only a polynomial number of shots
does not contain information about the input data pair with probability exponentially close to 1. This is formally stated in the following
proposition which is derived by reducing the problem of distinguishing distributions (i.e., one associated with a kernel value and the uniform distribution) to a hypothesis testing task.
Proposition 2. Assume that the ﬁdelity quantum kernel κFQðx,x0Þ
exponentially concentrates towards some exponentially small value as
per Deﬁnition 1. Suppose an N 2 OðpolyðnÞÞ shot SWAP test is used to
estimate the Gram matrix for a training dataset S = fxi,yig of size Ns.
Then, with probability exponentially close to 1 (i.e., probability at least
1  δ0 such that δ0 2 OðcnÞ for some c > 1), the estimate of the Gram
matrix ^K is statistically indistinguishable from the matrix bK
diagonal elements are 1 and off-diagonal elements are instances of
where each eλm takes either + 1 or −1 with equal probability. We note
does not contain any information about the input
data S = fxi,yig.
We refer the readers to Supplementary Note II for an introduction
to some preliminary tools for a hypothesis testing and Supplementary
Note III A 2 for further technical details regarding the SWAP test, which
includes formal deﬁnitions of statistical indistinguishability (i.e., Deﬁnition 2 for distributions and Deﬁnition 3 for outputs), and a proof of
the proposition.
 
Nature Communications| 15:5200
Although statistical estimates of the kernel behave differently
depending on the choice of measurement strategy, they are both in
effect independent of the input data for large n. Thus training with this
estimated Gram matrix leads to a model whose predictions are independent of the input training data. We present numerical simulations
to support our theoretical ﬁndings in Supplementary Note III A 3.
Crucially, this conclusion applies generally beyond kernel ridge
regression to other kernel methods including both supervised learning
tasks and unsupervised learning tasks. As a concrete example, we
consider the optimal solution for kernel ridge regression in the presence of exponential concentration.
Corollary 1. Consider a kernel ridge regression task with a squared loss
function and regularization λ using the same assumptions as Proposition 1. Denote y as a vector with its ith elements equal to yi.
For the Loschmidt Echo test, the optimal parameters are
found to be
a0ðy, λÞ =
with probability at least 1 −δ with δ 2 OðbnÞ for some b > 1.
For a test data point x=2S, the model prediction is 0 with probability at least 1  δ0 such that δ0 2 Oðb0nÞ for some b0>1.
For the SWAP test, the optimal parameters are statistically indistinguishable from the vector
arandðy, λÞ =
with probability at least 1  ~δ with ~δ 2 Oð~b
nÞ for some ~b > 1. Here,
is a data-independent random matrix whose diagonal elements
are 1 and off-diagonal elements are instances of bκ
in Eq. (14).
In addition, with probability exponentially close to 1, the model
prediction on unseen data is statistically indistinguishable from the
data-independent random variables that result from measuring bK
Corollary 1 shows that, regardless of the measurement strategy
to estimate the kernel value, exponential concentration leads to a
trained model where the predictions on unseen inputs are independent of the training data. A visual illustration of the effect of
exponential concentration in the presence of shot noise on model
predictions is provided in Fig. 2. We note that these propositions
and corollary presented here are simpliﬁed versions and refer the
readers to Supplementary Note III A for the full statements and
It is natural to ask whether the problems caused by exponential
concentration should be viewed as a barrier to training or generalization. Since the estimated Gram matrix is still positive semi-deﬁnite,
the loss landscape remains convex when the model is trained and the
optimal model with respect to this estimated Gram matrix is guaranteed to be obtained. Although this trained model is independent of
input data (as explained above), the model can still perform well on the
training phase and achieve small training errors in the limit of small
regularization. This is because the training output data is trivially
‘cooked’ into the model via the optimization process (independently
of the kernel values).
On the other hand, the data independence of the kernel values
means that the predictions of the trained model are completely independent of the training data and so the trained model in general performs trivially on unseen data. That is, the model generalizes terribly.
By incorporating the effect of shot noise, this has a different ﬂavor to
typical barriers to generalization in that crucially it arises from using
not enough shots (rather than not enough training data). Moreover, in
our numerics below we concretely see that this barrier cannot be
resolved by training on more input data points.
Figure 3 numerically demonstrates this effect on an engineered
dataset for a 40 qubit simulation. In the main plot, the generalization is
studied as a function of increasing training data SNs and whether the
training is performed with exact or estimated kernel values. Particularly, to observe the improvement due to increasing data, we plot a
relative loss on a test dataset Stest with respect to its initial value
(NS = 10) i.e., ηðNsÞ =
LaðStestjSNs Þ
LaðStestjSNs = 10Þ. That is, η(Ns) < 1 for Ns > 10 indicates
better generalization with increasing training data. This is observed to
be the case for the training on the exact kernel value where the model
gradually generalizes better. In fact, this learning task is synthesized
such that when training on the whole dataset, with an access to the
exact kernel values, the trained model generalizes perfectly. Even with
this dataset which is heavily favorable for the ﬁdelity kernel, the
Fig. 2 | Schematic of effect of exponential concentration and shot noise on
training and generalization performance. For the unseen (test) data, the behavior depends on how kernel values are statistically estimated. In the case of the
Loschmidt Echo test, the model predictions are zero with high probability. On using
the SWAP test, the model predictions ﬂuctuate around zero (due to shot noise). On
the other hand, for the training data, the training labels are effectively hard-coded
by the optimization process. (For simplicity we here consider the limit of no
regularization).
Fig. 3 | Effect of exponential concentration on training and generalization
performance. We consider a tensor product encoding for an engineered data set
where each component is uniformly drawn from [0, 2π] and the true label is
ytrueðxÞ = PNs
i = 1 wiκFQðxi,xÞ where wi is randomly chosen from . We train on
Ns = 150 data points. In the main plot, the loss on a test dataset Stest relative to its
initial value (without training) is plotted as a function of increasing training data. In
the inset, an absolute training error is plotted as a function of the increasing data.
We note that each kernel value is estimated with N = 1000 and the number testing
data points is 20. The training is done with no regularization λ = 0. We repeat this
experiment 10 times. The solid curves represent averages of respective losses and
the shaded areas represent standard deviations.
 
Nature Communications| 15:5200
performance on unseen data with the estimated kernel values shows
no improvement with the increasing training data. Speciﬁcally, when
the Loschmidt Echo test is used to evaluate kernel values, the statistical
estimates accumulate at exactly zero, leading to η(Ns) = 1 for all Ns. In
addition, for the SWAP measurement strategy, there is no improvement with increasing data and the behavior of η(Ns) aligns with the one
where the model is trained on a random matrix where each offdiagonal element is a data-independent random variable in Eq. (14). On
the other hand, as demonstrated in the inset of Fig. 3, the trained
model performs perfectly on the training set SNs and achieves zero
training errors in all cases. This is again because the training label
information is hard-coded in the optimization process. These empirical observations are all in good agreement with our theoretical
predictions.
Finally, the analysis in the case of the projected quantum kernel is
slightly more complicated as estimating the kernel requires us to ﬁrst
obtain the statistical estimates of the 2-norms between the reduced
data encoding states on all individual qubits from quantum computers. Two common strategies to to do so include (i) the full tomography of the single qubit reduced density matrices and (ii) the local
SWAP tests. In Supplementary Note III B, we again use a hypothesis
testing framework to analyze the effect of exponential concentration
on the projected kernel for these strategies. Similarly to the ﬁdelity
kernel we ﬁnd that the ﬁnal trained model is in effect independent of
the training data.
Sources of exponential concentration
Given that exponential concentration leads to trivial data-independent
models, it is important to determine when kernel values will, or will
not, concentrate. In this section, we investigate the causes of exponential concentration for quantum kernels.
In broad terms, the exponential concentration of quantum kernels may be viewed as stemming from the fact in certain situations it
can be difﬁcult to extract information from quantum states. In particular, we identify four key features that can severely hinder the
information extraction process via kernels. These include the expressivity of the data embedding, entanglement, global measurements and
noise (see Fig. 1). For each source, we derive an associated concentration bound. As summarized in Table 1 each of these theorems
has an analog for QNN. All the proofs of our main results are presented
in the Supplementary Information.
1. Expressivity-induced concentration. In broad terms, the expressivity of an ensemble of unitaries is deﬁned as how close the ensemble
uniformly covers the unitary group. To introduce the concept of the
expressivity of the data embedding U(x), we ﬁrst consider the unitary
ensemble generated via the data embedding U(x) over all possible
input data vectors x 2 X. That is, the data embedding deﬁnes a map
U : X ! Ux  UðdÞ, where UðdÞ is the total space of unitaries of
dimension d = 2n, and
Ux = fUðxÞ j x 2 Xg :
In addition, for some initial state ρ0, we can deﬁne an ensemble of the
data-embedded quantum states Sx = fUðxÞρ0UyðxÞg for all UðxÞ 2 Ux.
Consequently, performing an average over all the input data is
equivalent to an average over the ensemble of data-encoded unitaries
Ux, or the data encoded states Sx.
More concretely, we can measure the expressivity of a given
ensemble U by how close it is from a 2-design (a pseudo-random
distribution that agrees with the random distribution up to the second
expressivity
refs. 25,66,67, the following superoperator formally quantiﬁes the
distance between U and an ensemble that forms a 2-design,
AUðÞ :¼ VHaarðÞ 
dUU2ðÞ2ðUyÞ2:
Here VHaarðÞ =
UðdÞdμðVÞV 2ðÞ2ðV yÞ
2 is an integral over Haar
ensemble and the second term is an integral over U. In our case, we
have the data-encoded ensemble as our ensemble of interest i.e.,
U = Ux. Given an input state ρ0, the trace norm
εUx :¼ kAUxðρ0Þk1,
can be chosen as a data-dependent expressivity measure. The datadependence of εUx stems from the dependence of Ux, Eq. (17), on the
input data X. Thus εUx takes into account not just the expressivity of
the embedding but also the randomness of the input dataset. The
measure equals zero, εUx = 0, only when Ux is maximally expressive
(i.e., when it agrees with the uniform distribution up to at least the
second moment).
To understand why expressivity can be an issue for kernel-based
methods, let us consider the ﬁdelity quantum kernel of Eq. (3). This
kernel requires computing the inner product between two vectors in
an exponentially large Hilbert space. As such, for highly expressive
embeddings we are essentially evaluating the inner product between
two approximately random (and hence orthogonal) vectors, thus
leading to typical kernel values being exponentially small. That is,
kernel values tend to concentrate with increased expressivity. The
following theorem establishes the formal relationship between the
expressivity of the unitary embedding and the concentration of
quantum kernels.
Theorem 1. (Expressivity-induced concentration) Consider the ﬁdelity
quantum kernel as deﬁned in Eq. (3) and the projected quantum kernel
as deﬁned in Eq. (4). Assume that input data x and x0 are drawn from
the same distribution, leading to an ensemble of unitaries Ux as
deﬁned in Eq. (17). We have
Prx,x0 jκðx,x0Þ  Ex,x0½κðx,x0Þj ⩾δ
where εUx = k AUxðρ0Þk1 is the data-dependent expressivity measure
over Ux deﬁned in Eq. (19), and GnðεUxÞ is a function of εUx deﬁned
For the ﬁdelity quantum kernel κðx,x0Þ = κFQðx,x0Þ, we have
GnðεUxÞ = βHaar + εUx
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
where βHaar =
2n1ð2n + 1Þ
For the projected quantum kernel κðx,x0Þ = κPQðx,x0Þ, we have
GnðεUxÞ = 4γn ~βHaar + εUx
where ~βHaar =
2n + 1 + 2.
Theorem 1 establishes that higher embedding expressivity leads
to greater quantum kernel concentration. That is, the upper bound on
the kernel concentration becomes smaller when U(x) is more expressive. In the limit where Ux forms an ensemble that is exponentially
close to a 2-design (corresponding to εUx 2 Oð1=bnÞ for b > 1), the
kernel exponentially concentrates, and so exponentially many measurement shots are required to evaluate the kernel on a quantum
device. Note that the ﬁdelity kernel exponentially concentrates to
some exponentially small value i.e., μ = Ex,x0 ∼Haar½κFQðx,x0Þ = 1=2n.
 
Nature Communications| 15:5200
We stress that the proof of Theorem 1 makes no assumptions on
the form of U(x). This means the theorem holds for a wide range of
embedding architectures, including both problem-agnostic11,15,29,37,46,68
and problem-inspired embeddings6–8. In Supplementary Note IV A, we
generalize Theorem 1 by relaxing the assumption that x and x0 are
drawn from the same distribution. This is relevant, for example, in
binary classiﬁcation tasks where one might want to analyze the behavior of the kernel when a pair of inputs are drawn from different
training ensembles. Here, we ﬁnd a similar conclusion, where higher
expressivity leads to more concentrated kernel values.
Although Theorem 1 is stated in terms of the unitary embedding
of classical data, the theorem is also applicable to quantum data. That
is, it can also be applied when the input data is a collection of pure
quantum states generated directly from some quantum process of
interest. This follows from the fact that given a set of quantum data
states, there is a (potentially unknown) underlying ensemble of unitaries associated with preparing this set of states. Since we can
associate each of these unitaries with a classical label, we can associate
the quantum data with an encoding ensemble Ux and apply Theorem 1
as before. For example, consider a Hamiltonian H(x) where x are
parameters describing the Hamiltonian (e.g. perhaps on-site energies
or interaction strengths). The quantum data generated by an evolution
under H(x) for time T can be expressed as fUðxiÞ∣0i = eiHðxiÞT∣0igi.
We numerically probe the dependence of the concentration of
quantum kernel values on the expressivity of the data embedding. To
do so, we consider a Hardware Efﬁcient Embedding (HEE)29, comprised
of L layers of data-dependent single-qubit rotations around the x-axis
followed by entangling gates (Fig. 4). We further consider a data reuploading strategy where an input data point is repeatedly uploaded
into the data embedding15,29,69,70. In particular, the ith-component of a
data point x is encoded as the rotation angle of qubit i in every
HEE layer.
We chose to focus on the binary classiﬁcation task of distinguishing handwritten ‘0’ and ‘1’ digits from the MNIST dataset71. As
sketched in Fig. 5a, each individual image (i.e., an input data point) is
dimensionally reduced to a real-valued vector of length n using principle component analysis. We refer the reader to Appendix F of ref. 29
for more details.
For a dataset S = fxi,yigNs
i = 1 of size Ns, we evaluate the kernel values
over all possible different pairs of inputs in S. Thus we consider the set
of values: KS = fκðx1,x2Þ,κðx1,x3Þ, . . . ,κðxNs1,xNsÞg. We note that kernel
values for pairs of identical inputs are always 1 and are so excluded
from KS. To study the degree to which the quantum kernels probabilistically concentrate, we compute the variance Varx,x0½κðx,x0Þ
Figure 6 shows results for the scaling of the kernel variance as a
function of the number of qubits n and HEE layers L. As L increases, the
expressivity of the ansatz increases, and for sufﬁciently large L we
observe exponential concentration of both the ﬁdelity and projected
quantum kernels. We note that while the projected quantum kernel
reaches the exponential decay regime at shorter depths (i.e., roughly
L ⩾16 for the projected kernel, compared to L ⩾75 for the ﬁdelity
kernel), we generally observe smaller variances (and so stronger concentration) for the ﬁdelity kernel than the projected kernel.
Theorem 1 and the numerics presented in Fig. 6 highlight the
importance of the expressivity of quantum kernels. Namely, highly
expressive encodings (whether using ﬁdelity or projected kernels)
concretely,
unstructured
dataembeddings11,15,46,68 should generally be avoided and the data structure should be taken into account when designing a data-embedding
(for instance by constructing geometrically inspired embedding
schemes38–41).
2. Entanglement-induced concentration. In the previous section we
saw that high expressivity can be an issue due to the fact that kernels
Fig. 4 | Hardware Efﬁcient Embedding (HEE). A layer is composed of single qubit
x-rotations where the rotation angle on qubit k is given by the kth component of the
input data point x. After each layer of rotations, one applies entangling gates acting
on adjacent pairs of qubits.
Fig. 5 | Datasets. a An input data point x is obtained from dimensionally reducing
an original MNIST image to n features using principal component analysis. We
assign label −1 if the original image is digit ‘0’ and 1 if the original image is digit ‘1’.
b A hypercube of width 2π/21/n is centered at the origin. An input data point x with
each of its component bounded between −π and π has an associated label y = 1 if
the point is inside the hypercube (represented by a circle) and y = −1, otherwise
(represented by a cross).
Fig. 6 | Effect of expressivity on quantum kernels. We plot variances of the (a)
ﬁdelity and (b) projected quantum kernels, as a function of n and L. The classical
data from the MNIST dataset (Ns = 40) is encoded via an L-layer HEE.
 
Nature Communications| 15:5200
(such as the ﬁdelity kernel) compare inner products of objects in
exponentially large spaces. This issue can be mitigated using projected
kernels, which reduce the dimension of the feature space. However, a
different issue arises here due to the non-local correlations between
the qubits. Namely, the entanglement of the encoded state is another
potential source of concentration. Intuitively, this follows from the fact
that tracing out qubits in very entangled encoded states, leads to local
states that are close to maximally mixed.
Theorem 2. (Entanglement-induced concentration) Consider the
projected quantum kernel as deﬁned in Eq. (4). For a given pair of dataencoded states associated with x and x0, we have
∣1  κPQðx,x0Þ∣⩽ð2 ln 2ÞγΓsðx,x0Þ,
Γsðx,x0Þ =
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
S ρkðxÞ 1k
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
S ρkðx0Þ 1k
where we denote S  k 
Þ as the quantum relative entropy, ρk as a
reduced state on qubit k, and 1k as the maximally mixed state on
Theorem 2 upper bounds the deviation of kernel values from a
ﬁxed value of 1 with the relative entropy between the reduced states of
the encoded data and a maximally mixed state of a single qubit. In
addition, unlike the results in the previous sections, the exponential
concentration bounds here are deterministic. In the case where the
entanglement of the encoded states obeys a volume law, that is
SðρkðxÞ k 1k
2 Þ,Sðρkðx0Þ k 1k
2 Þ 2 Oð1=2n1Þ for all subsystems, the kernel
values deterministically exponentially concentrate to 1. For encoded
states that obey an area-law scaling, i.e. SðρkðxÞ k 1k
2 Þ,Sðρkðx0Þ k 1k
Oð1Þ for all subsystems, the story is more complex. Theorem 2, as an
upper bound, allows (but does not guarantee) that such data states do
not concentrate exponentially.
It is worth highlighting that the entanglement-induced bound in
Theorem 2 is stated for a given pair of data-encoded states, and not as
an average over all possible data pairs. Hence, it is thus natural to
determine classes of data and embeddings where concentration will
arise with high probability, e.g., cases when the encoded states obey a
volume law of entanglement. First, we note that if the ensemble of
encoded data states forms at least a 4-design, then most of the encoded states to obey a volume-law scaling72,73. However, in this case, our
bound on expressivity already implies that the kernel’s exponentially
concentrate so the entanglement-induced result is redundant.
Entanglement-induced concentration can also occur in cases
where the embedding is not highly expressive but still leads to states
satisfying a volume-law. Here, Theorem 2 implies that the kernel values
of the projected quantum kernels will exponentially concentrate. In
this case performing any supervised learning task with the projected
quantum kernels will fail with a polynomial number of measurement
shots. As an example, consider binary classiﬁcation and assume that
one manages to construct a U(x) that maps the input data into one of
the two orthogonal sets of volume-law entangled states depending on
the true label of the input. In this setting, the trained model with the
ﬁdelity kernel should not face issues associated with exponential
concentration. However, if we use the projected quantum kernel, we
cannot perform the task better than random guessing without
spending an exponential number of shots. This statement is formalized in the following corollary.
Corollary 2. Consider the projected quantum kernel as deﬁned in Eq.
(4). If all the states in the ensemble Strain generated from the training
dataset obey volume law scaling, we have
∣1  κPQðx,x0Þ∣2 Oðn2nÞ,
for all x and x0 in the training data.
Thus, when using projected kernels, highly entangling encodings
should be avoided to ensure predictability on unseen data. We note
that ﬁdelity kernels (with pure input states) are not affected by
entanglement in this manner as they do not require tracing qubit out.
Lastly, we stress that Theorem 2 and Corollary 2 are readily applied to
quantum data. Indeed, entanglement-induced concentration may well
be more problematic in this case since if the quantum data is already
highly entangled then there is little that one can do. (In contrast, for
classical data one may simply avoid highly entangled embeddings). As
an example, consider a quantum dataset generated by evolving different initial states with either U1 or U2 where U1 and U2 are unitaries
drawn from the Haar measure over the unitary group. Since Haar
random evolution leads to a volume-law scaling, classifying whether a
given state is evolved by U1 or U2 cannot be done efﬁciently using
projected quantum kernels.
3. Global-measurement-induced concentration. Global measurements can be another source of exponential concentration. A global
measurement is a measurement that acts non-trivially on all n qubits.
Such global measurements are required by design to compute ﬁdelity
kernels but not projected kernels. In broad terms global measurements can lead to concentration because we are attempting to extract
global information about a state that lives in an exponentially large
Hilbert space. While projected quantum kernels do not face these
difﬁculties due to their local construction, we argue that global measurements can lead to problems for the ﬁdelity kernel.
To illustrate this problem, we provide an example where the
data embedding has low expressivity and contains no entanglement
and yet it is still possible to have exponential concentration. Consider the tensor product unitary data embedding UðxÞ = Nn
k = 1UkðxkÞ
with xk being a k-th component of x, and Uk being a single-qubit
proposition holds.
Proposition 3. (Global-measurement-induced concentration) Consider
the ﬁdelity quantum kernel as deﬁned in Eq. (3) where the data
embedding is of the form UðxÞ = Nn
k = 1UkðxkÞ with xk being an input
component encoded in the qubit k, and Uk being a single-qubit rotation
about the y-axis on the k-th qubit. For an input data point x, assume that
all components of x are independent and uniformly sampled in [ −π, π].
Given a product initial state ρ0 = Nn
k = 1∣0i 0
h ∣, we have,
Prx,x0 jκFQðx,x0Þ  1=2nj ≥δ
Intuitively, the result in Proposition 3 can be understood as following from the fact that the ﬁdelity between two product states is
usually exponentially small. In Supplementary Note VI A, we further
generalize this proposition to the case when Uk is a general unitary,
which also leads to a concentration result.
We remark that the assumptions underlying Proposition 3 can be
relevant in practice. For example, consider classifying whether or not a
given point x in n −dimensional space (with each component bounded
between [ −π, π]) stays inside a hypercube centered at the origin with
the width of 2π
21=n (see Fig. 5(b)). Note that the width of the hypercube is
chosen so there is a 0.5 probability of a randomly chosen point being in
or out of the hypercube. For this task, an individual data point in the
training dataset is generated by uniformly drawing each vector component from the range [ −π, π]. Since here data points are obtained via
 
Nature Communications| 15:5200
uniformly sampling each component independently, the above
assumptions are satisﬁed.
We numerically study the concentration of the kernels for this
classiﬁcation task in Fig. 7. To reduce the effects of expressivity and
entanglement, we ﬁrst select the data embedding to be a single layer of
one qubit rotations (Rx, Ry, Hadamard followed by Rz). Similar to the
HEE, each component of an individual data point is embedded as a
rotation angle. We observe an exponential decay in the variance of the
ﬁdelity kernel in a good agreement with our theoretical predictions.
While Proposition 3 is derived with a tensor product embedding,
similar results are expected when dealing with more general unstructured embeddings such as hardware efﬁcient embeddings. This is
because the additional complexity from using an unstructured
embedding can only increase the kernel concentration (due to
increased expressivity and entanglement). This is highlighted in Fig. 7
where we additionally consider an L-layered HEE and see that
increasing expressivity can accelerate the exponential decay.
Nonetheless, it is important to stress that global measurements
do not always lead to exponential concentration. For example, if the
encoded quantum states are not too far away in Hilbert space, such
that the ﬁdelity kernel values concentrate no worse than polynomially
in n, their overlap can be efﬁciently resolved. For example, the MNIST
classiﬁcation task does not satisfy the assumptions of Proposition 3. As
a result, as shown in Fig. 6, global measurements do not lead to the
exponential concentration of the ﬁdelity kernel for low depth ansatze.
This demonstrates that the structure of the training data matters and
global measurements do not always lead to exponential concentration.
Thus, the key message here is that when using global measurements to evaluate the kernel, the embedding must be chosen particularly carefully such that the ﬁdelity between any pair of encoded
quantum states is at least in Ω(1/poly(n)). To achieve this, one can
either take the problem’s structure into consideration when building
the embedding8,38,39,42 or further reduce the expressivity of problemagnostic embeddings43.
4. Noise-induced concentration. Hardware noise may disrupt and
destroy information in the encoded quantum states, providing
another source of concentration. To analyze the effect of noise, we
here further suppose the data-embedding can be decomposed into L
layers of data-encoding unitaries
where xl is an input associated with x that is encoded in the layer l.
We remark that from our construction, xl can be either the l
component of the input data x or a ﬁxed vector x that is encoded
repeatedly. Although the form of the data embedding is slightly less
general than the one described in the noiseless sections, it still covers
a large class of data embedding ansatze including the Hardware
(HEE)11,15,29,37,46,68,
Alternative
Operator Ansatz (QAOA)74, the Hamiltonian Variational Embedding
Instantaneous
Polynomial
embedding29,37,43.
We model the hardware noise as a Pauli noise channel applied
before and after every layer of the embedding, similar to the model
considered in ref. 28. The output state of the noisy embedding circuit
is given by
~ρðxÞ = N  ULðxLÞ  N  .. .  N  U1ðx1Þ  N ðρ0Þ
where UlðxlÞ is the channel corresponding to the unitary Ul(xl) and
N = N 1  . . .  N n is a local Pauli noise channel. Speciﬁcally, in this
work we consider unital channels such that the effect of N j on each
local Pauli operator σ ∈{X, Y, Z} is given by
N j ðσÞ = qσσ,
where −1 < qσ < 1. We remark that the noiseless regime corresponds to
qσ = 1 for all qubits. The strength of the noise can be quantiﬁed by a
characteristic noise parameter which is deﬁned as
q = maxfjqXj,jqYj,jqZjg:
The following theorem summarizes the impact of noise on
quantum kernels.
Theorem 3. (Noise-induced concentration) Consider the L-layered
data embedding circuit deﬁned in Eq. (27) with input state ρ0 and the
layerwise Pauli noise model deﬁned in Eq. (28) with characteristic noise
parameter q < 1. The concentration of quantum kernel values may be
bounded as follows
∣~κðx,x0Þ  μ∣⩽Fðq,LÞ:
For the ﬁdelity quantum kernel ~κðx,x0Þ = ~κFQðx,x0Þ, we have μ = 1/
Fðq,LÞ = q2L + 1 ρ0  1
For the projected quantum kernel ~κðx, x0Þ = ~κPQðx,x0Þ, we have
μ = 1, and
Fðq,LÞ = ð8 ln 2ÞγnqbðL + 1ÞS2 ρ0
where S2( ⋅∥⋅) denotes the sandwiched 2-Rényi relative entropy
and b = 1=ð2 lnð2ÞÞ ≈0:72.
Additionally, the noisy data-encoded quantum state ~ρðxÞ concentrates towards the maximally mixed state as
⩽qL + 1 ρ0  1
Theorem 3 shows that the concentration of quantum kernels due
to noise is exponential in the number of layers L for both the ﬁdelity
and projected quantum kernels. This is a consequence of the encoded
state concentrating towards the maximally mixed state, as captured in
Eq. (34). In addition, we note that the noise-induced concentration
Fig. 7 | Global-measurement concentration of quantum kernels. We plot the
variance of the ﬁdelity kernel as a function of n using different data-embeddings,
namely a single layer of one qubit rotations (Rx, Ry, Hadamard followed by Rz) and
HEE with L layers. The components of Ns = 40 input data points are independent
and uniformly drawn from [ −π, π]).
 
Nature Communications| 15:5200
bounds here are deterministic due to the noise acting independently
of the input data.
If quantum kernel-based methods are to provide any quantum
advantage, the data embedding part must be hard to classically
simulate. For example, when using embeddings with local connectivity, we are largely interested in the regime of moderately deep
circuits where L scales at least linearly in n37. However, it is precisely
this regime in which our bounds suggest kernels will exponentially
concentrate due to an effect of noise. In particular, when the number
of layers L scales polynomially with the number of qubits n, F(q, L)
decays exponentially in the number of qubits. We stress that the
exponential decay nature of the concentration bounds persists for all
q < 1 and different values of noise characteristics only lead to different
exponential decay rates.
The impact of noise on the concentration of kernels is studied in
Fig. 8 where we plot the average of ∣κFQðx,x0Þ  1=2n∣and ∣κPQðx,x0Þ  1∣
for the MNIST dataset as a function of the depth L of the HEE embedding
and the noise characteristic q. We observe exponential concentration
with L, with the concentration stronger for higher noise levels q, in
agreement with Theorem 3. We note that in our numerical simulations,
noise only acts before and after single-qubit gates and we assume
noiseless implementations of entangling gates. Therefore, in real
experiments, where gate ﬁdelity of entangling gates is generally worse
than single-qubit gates, we expect the noise to dominate at a faster pace.
In Supplementary Note VIII, we argue that, similar to noise-induced
BPs in VQAs32,75,76, the exponential concentration cannot be resolved
with current common error mitigation techniques including Zero-Noise
Extrapolation77–80, Clifford Data Regression81, Virtual Distillation82,83 and
Probabilistic Error Cancellation78,79. Hence, noise-induced concentration results poses a signiﬁcant barrier to the successful implementation
of quantum kernel methods on near term hardware.
Training parameterized quantum kernels
Given the problems associated with expressivity-induced concentration, it is generally advisable to avoid problem-agnostic embeddings
and instead try and take advantage of the data structure of the problem. However, in many cases, constructing such problem-inspired
embeddings is highly non-trivial. An alternative is to allow the data
embedding itself to be parametrized and then train the embedding.
Such strategies have been shown to improve generalization of the
kernel-based quantum model68,74. We note that this is an additional
process to train and select an appropriate embedding before implementing the standard quantum kernel algorithm (with this selected
embedding).
Here we consider a parametrized data embedding U(x, θ), where θ
is a vector of trainable parameters (typically corresponding to single
qubit rotation angles). For a given input data vector x, an ensemble of
data embedding unitaries can be generated by varying the parameters
θ. This in turn generates a family of parametrized quantum kernels
κθðx,x0Þ. Let θ = θ* be the optimal parameters found by training the
embedding. The optimally embedded kernel now corresponds to
κðx,x0Þ = κθ*ðx,x0Þ and the remaining process to obtain the optimal
model is the same as that described in Section IA.
The standard approach to obtain the optimal kernel is to train the
parameters θ* via standard optimization techniques18,68,74, which in turn
requires deﬁning a loss function one needs to minimize. For instance,
in a binary classiﬁcation task where the true labels are either +1 or −1,
the ideal kernel is +1 if the input data are in the same class and is −1
otherwise. In practice, however, one can only approximate the ideal
κidealðxi, xjÞ = yiyj,
using the given training data S. The kernel target alignment measures
the similarity between the parameterized kernel and the approximated
ideal kernel68,84
i,jyiyjκθðxi,xjÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
i,jðκθðxi,xjÞÞ2
i,jð yiyjÞ2
As minimizing the target alignment corresponds to aligning the parametrized kernel to the ideal kernel, we can use TA(θ) as a loss function.
Crucially, unlike the training of the model itself, the associated loss
function for training the embedding is generally non-convex.
Training the parameterized data-embedding U(x, θ) has been
recently proposed as an approach to improve generalization quantum
kernel-based methods68,74. In particular, ref. 68 showed that optimizing
the kernel target alignment TA(θ) of Eq. (36) leads to data-embedding
schemes with better performance than unstructured embeddings for
MNIST-based
classiﬁcation
assumes that one can successfully train the target alignment.
Here we study the trainability of TA(θ). Namely, we discuss what
features of the parameterized embedding U(x, θ) can lead to exponential concentration and therefore to exponentially ﬂat parameter
landscapes (i.e., a BP). First, we show that the variance of TA(θ) with
respect to the variational parameters θ is upper bounded by the variances of the parameterized quantum kernels κθðx,x0Þ
Proposition 4. (Concentration of kernel target alignment) Consider an
arbitrary parameterized kernel κθðx,x0Þ and a training dataset fxi,yigNs
for binary classiﬁcation with yi = ± 1. The probability that the kernel
target alignment TA(θ) (deﬁned in Eq. (36)) deviates from its mean
value is approximately bounded as
Prθ jTAðθÞ  Eθ½TAðθÞj ≥δ
i,jVarθ½κθðxi,xjÞ
s 9ðNs1Þ2 + 16
Fig. 8 | Effect of noise. We plot the average of the difference between the quantum
kernels and their respective ﬁxed point μ over different input data points and for
different number of layers L and noise parameter q. We consider the ﬁdelity
quantum kernel in a with μ = 1/2n and the projected quantum kernel in b with μ = 1.
We use the MNIST dataset with Ns = 40 and n = 8.
 
Nature Communications| 15:5200
If Varθ[κθ(xi, xj)] vanishes exponentially in the number of qubits
for all pairs in the training data, the probability that TA(θ) deviates by
an amount δ from its mean vanishes exponentially with the size of the
problem. In this case, the parameter landscape of TA(θ) becomes
exponentially ﬂat and hence TA(θ) is untrainable with a polynomial
number of measurement shots.
In Supplementary Note X, we analyze features leading to exponentially vanishing variances Varθ[κθ(xi, xj)] and ﬁnd that the same
ones that lead to BPs for QNNs lead to BPs here. Namely, features that
are deemed detrimental for trainability in QNNs such as deep
unstructured circuits20,25 and global measurements21 also lead to BPs
here. Thus these features should be avoided when designing parameterized data embeddings for quantum kernels.
We numerically demonstrate the effect of global measurements
on training an embedding in Fig. 9. The data embedding consists of a
single layer of parameterized single-qubit rotations around y-axis followed by a single layer of HEE. We study the variance of the kernel
target alignment TA(θ) (which determines the ﬂatness of the training
landscape20,21) for 500 random initialization of the parameters θ. As
expected, since the parametrized block acts globally on all qubits,
TA(θ) exponentially concentrates when one averages over the trainable parameters θ.
Discussion
Quantum kernels stand out as a promising candidate for achieving a
practical quantum advantage in data analysis. This is in part due to the
common belief that the optimal quantum kernel-based model can
always be obtained14–17 due to the convexity of the problem. Although
this is true, provided that the kernel values can be efﬁciently obtained
to a sufﬁciently high precision, here we show that there exist scenarios
where quantum kernels are exponentially concentrated towards some
ﬁxed value and so exponential resources are required to accurately
estimate the kernel values. With only a polynomial number of shots,
the predictions of the trained model become insensitive to input data
and the model performs trivially on unseen data, that is, generalizes
poorly. Crucially, in this context generalization cannot be improved by
training on more input data points but rather by increasing the number
of measurement shots (or using a more appropriate embedding). It is
worth stressing that as we assume very little on the form of the data
embedding U(x), our analytical bounds hold for a wide range of
embedding architectures and schemes, including both problemagnostic and problem-inspired embeddings.
Our results highlight four aspects to carefully consider when
choosing a data embedding for quantum kernels. While much of the
literature currently focuses on using problem-agnostic quantum
embeddings for quantum kernels11,15,46,68; these are typically highly
expressive and as such should generally be avoided. Entanglement can
also be detrimental when combined with local quantum kernels such
as the projected quantum kernels, and suggests that one should be
mindful about using embeddings leading to states satisfying volumelaws of entanglement. Our results on global measurements demonstrate that the ﬁdelity kernel can exponentially concentrate even with a
simple embedding that has low expressivity and no entanglement.
Consequently, the ﬁdelity kernel should only be used for datasets
where the data-embedded states are ‘not too distant’ in the Hilbert
space. Finally, our study of noise suggests that polynomial-depth data
embeddings in noisy hardware suffer from exponential concentration,
thus presenting a serious barrier to achieve a meaningful quantum
advantage in the near term.
In addition, we show that training parametrized quantum kernels
using kernel target alignment suffers from an exponentially ﬂat training landscape under similar conditions to those leading to barren
plateaus in QNNs. That is, when constructing the parametrized part of
the data embeddings, one should avoid features that induce BPs as
QNNs such as global measurements and deep unstructured circuits.
Our work provides a systematic study of the barriers to the successful scaling up of quantum kernel methods posed by exponential
concentration. Prior work on BPs motivated the community to search
for ways to avoid or mitigate BPs such as employing correlated
parameters85 using tools from quantum optimal control22,86, or developing the ﬁeld of geometrical quantum machine learning38–41. In a
similar manner, we stress our results should not be understood as
condemning quantum kernel methods, but rather a prompt to develop
exponential-concentration-free embeddings for quantum kernels.
Crucially, incorporating quantum aspects to machine learning does
not always lead to better performance. Indeed, often it willonly worsen
the performance of the learning models. In particular, if one remains
restricted to mimicking the classical techniques without carefully
taking into account quantum phenomena, it is unlikely that one will
achieve a quantum advantage. Hence distinctly quantum approaches,
using specialized quantum structures/symmetries, may prove to be
the way forward37,42.
Data availability
Data generated and analyzed during the current study are available at
repository:
 
Thanasilp/Exponential-concentration-in-quantum-kernel-methods.
Code availability
Code used for the current study are available at the following GitHub
repository: 
ntration-in-quantum-kernel-methods.