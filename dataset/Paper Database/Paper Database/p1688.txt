Large Kernel Matters ——
Improve Semantic Segmentation by Global Convolutional Network
Xiangyu Zhang
Guiming Luo
School of Software, Tsinghua University, { , }
Megvii Inc. (Face++), {zhangxiangyu, yugang, sunjian}@megvii.com
One of recent trends in network architecture design is stacking small ﬁlters (e.g., 1x1 or 3x3) in the
entire network because the stacked small ﬁlters is more ef-
ﬁcient than a large kernel, given the same computational
complexity.
However, in the ﬁeld of semantic segmentation, where we need to perform dense per-pixel prediction,
we ﬁnd that the large kernel (and effective receptive ﬁeld)
plays an important role when we have to perform the classiﬁcation and localization tasks simultaneously. Following
our design principle, we propose a Global Convolutional
Network to address both the classiﬁcation and localization
issues for the semantic segmentation. We also suggest a
residual-based boundary reﬁnement to further reﬁne the object boundaries. Our approach achieves state-of-art performance on two public benchmarks and signiﬁcantly outperforms previous results, 82.2% (vs 80.2%) on PASCAL VOC
2012 dataset and 76.9% (vs 71.8%) on Cityscapes dataset.
1. Introduction
Semantic segmentation can be considered as a per-pixel
classiﬁcation problem.
There are two challenges in this
task: 1) classiﬁcation: an object associated to a speciﬁc semantic concept should be marked correctly; 2) localization:
the classiﬁcation label for a pixel must be aligned to the appropriate coordinates in output score map. A well-designed
segmentation model should deal with the two issues simultaneously.
However, these two tasks are naturally contradictory. For
the classiﬁcation task, the models are required to be invariant to various transformations like translation and rotation.
But for the localization task, models should be
transformation-sensitive, i.e., precisely locate every pixel
for each semantic category. The conventional semantic segmentation algorithms mainly target for the localization issue, as shown in Figure 1 B. But this might decrease the
Figure 1. A: Classiﬁcation network; B: Conventional segmentation
network, mainly designed for localization; C: Our Global Convolutional Network.
classiﬁcation performance.
In this paper, we propose an improved net architecture,
called Global Convolutional Network (GCN), to deal with
the above two challenges simultaneously. We follow two
design principles: 1) from the localization view, the model
structure should be fully convolutional to retain the localization performance and no fully-connected or global pooling
layers should be used as these layers will discard the localization information; 2) from the classiﬁcation view, large
kernel size should be adopted in the network architecture
to enable densely connections between feature maps and
per-pixel classiﬁers, which enhances the capability to handle different transformations. These two principles lead to
our GCN, as in Figure 2 A. The FCN -like structure
is employed as our basic framework and our GCN is used
to generate semantic score maps. To make global convolution practical, we adopt symmetric, separable large ﬁlters to
reduce the model parameters and computation cost. To further improve the localization ability near the object boundaries, we introduce boundary reﬁnement block to model the
boundary alignment as a residual structure, shown in Figure 2 C. Unlike the CRF-like post-process , our boundary
 
reﬁnement block is integrated into the network and trained
end-to-end.
Our contributions are summarized as follows: 1) we propose Global Convolutional Network for semantic segmentation which explicitly address the “classiﬁcation” and “localization” problems simultaneously; 2) a Boundary Reﬁnement block is introduced which can further improve the localization performance near the object boundaries; 3) we
achieve state-of-art results on two standard benchmarks,
with 82.2% on PASCAL VOC 2012 and 76.9% on the
Cityscapes.
2. Related Work
In this section we quickly review the literatures on semantic segmentation. One of the most popular CNN based
work is the Fully Convolutional Network (FCN) . By
converting the fully-connected layers into convolutional
layers and concatenating the intermediate score maps, FCN
has outperformed a lot of traditional methods on semantic
segmentation. Following the structure of FCN, there are
several works trying to improve the semantic segmentation
task based on the following three aspects.
Context Embedding in semantic segmentation is a hot
topic. Among the ﬁrst, Zoom-out proposes a handcrafted hierarchical context features, while ParseNet 
adds a global pooling branch to extract context information.
Further, Dilated-Net appends several layers after the
score map to embed the multi-scale context, and Deeplab-
V2 uses the Atrous Spatial Pyramid Pooling, which is a
combination of convolutions, to embed the context directly
from feature map.
Resolution Enlarging is another research direction in
semantic segmentation. Initially, FCN proposes the
deconvolution (i.e. inverse of convolution) operation to increase the resolution of small score map. Further, Deconv-
Net and SegNet introduce the unpooling operation
(i.e. inverse of pooling) and a glass-like network to learn
the upsampling process. More recently, LRR argues
that upsampling a feature map is better than score map. Instead of learning the upsampling process, Deeplab and
Dilated-Net propose a special dilated convolution to
directly increase the spatial size of small feature maps, resulting in a larger score map.
Boundary Alignment tries to reﬁne the predictions near
the object boundaries. Among the many methods, Conditional Random Field (CRF) is often employed here because
of its good mathematical formation. Deeplab directly
employs denseCRF , which is a CRF-variant built on
fully-connected graph, as a post-processing method after
CNN. Then CRFAsRNN models the denseCRF into
a RNN-style operator and proposes an end-to-end pipeline,
yet it involves too much CPU computation on Permutohedral Lattice . DPN makes a different approximation on denseCRF and put the whole pipeline completely on
GPU. Furthermore, Adelaide deeply incorporates CRF
and CNN where hand-crafted potentials is replaced by convolutions and nonlinearities. Besides, there are also some
alternatives to CRF. presents a similar model to CRF,
called Bilateral Solver, yet achieves 10x speed and comparable performance. introduces the bilateral ﬁlter to
learn the speciﬁc pairwise potentials within CNN.
In contrary to previous works, we argues that semantic
segmentation is a classiﬁcation task on large feature map
and our Global Convolutional Network could simultaneously fulﬁll the demands of classiﬁcation and localization.
3. Approach
In this section, we ﬁrst propose a novel Global Convolutional Network (GCN) to address the contradictory aspects
— classiﬁcation and localization in semantic segmentation.
Then using GCN we design a fully-convolutional framework for semantic segmentation task.
3.1. Global Convolutional Network
The task of semantic segmentation, or pixel-wise classi-
ﬁcation, requires to output a score map assigning each pixel
from the input image with semantic label. As mentioned in
Introduction section, this task implies two challenges: classiﬁcation and localization. However, we ﬁnd that the requirements of classiﬁcation and localization problems are
naturally contradictory: (1) For classiﬁcation task, models
are required invariant to transformation on the inputs — objects may be shifted, rotated or rescaled but the classiﬁcation results are expected to be unchanged. (2) While for localization task, models should be transformation-sensitive
because the localization results depend on the positions of
In deep learning, the differences between classiﬁcation
and localization lead to different styles of models. For classiﬁcation, most modern frameworks such as AlexNet ,
VGG Net , GoogleNet or ResNet employ the ”Cone-shaped” networks shown in Figure 1 A:
features are extracted from a relatively small hidden layer,
which is coarse on spatial dimensions, and classiﬁers
are densely connected to entire feature map via fullyconnected layer or global pooling layer ,
which makes features robust to locally disturbances and allows classiﬁers to handle different types of input transformations. For localization, in contrast, we need relatively
large feature maps to encode more spatial information. That
is why most semantic segmentation frameworks, such as
FCN , DeepLab , Deconv-Net , adopt
”Barrel-shaped” networks shown in Figure 1 B. Techniques
such as Deconvolution , Unpooling and Dilated-
Convolution are used to generate high-resolution
feature maps, then classiﬁers are connected locally to each
Figure 2. An overview of the whole pipeline in (A). The details of Global Convolutional Network (GCN) and Boundary Reﬁnement (BR)
block are illustrated in (B) and (C), respectively.
spatial location on the feature map to generate pixel-wise
semantic labels.
We notice that current state-of-the-art semantic segmentation models mainly follow the design principles for localization, however, which may be suboptimal
for classiﬁcation. As classiﬁers are connected locally rather
than globally to the feature map, it is difﬁcult for classi-
ﬁers to handle different variations of transformations on the
input. For example, consider the situations in Figure 3: a
classiﬁer is aligned to the center of an input object, so it is
expected to give the semantic label for the object. At ﬁrst,
the valid receptive ﬁled (VRF)1 is large enough to hold the
entire object. However, if the input object is resized to a
large scale, then VRF can only cover a part of the object,
which may be harmful for classiﬁcation. It will be even
worse if larger feature maps are used, because the gap between classiﬁcation and localization becomes larger.
Based on above observation, we try to design a new architecture to overcome the drawbacks. First from the localization view, the structure must be fully-convolutional without any fully-connected layer or global pooling layer that
used by many classiﬁcation networks, since the latter will
1Feature maps from modern networks such as GoolgeNet or ResNet
usually have very large receptive ﬁeld because of the deep architecture.
However, studies show that network tends to gather information
mainly from a much smaller region in the receptive ﬁeld, which is called
valid receptive ﬁeld (VRF) in this paper.
discard localization information. Second from the classi-
ﬁcation view, motivated by the densely-connected structure
of classiﬁcation models, the kernel size of the convolutional
structure should be as large as possible. Specially, if the kernel size increases to the spatial size of feature map (named
global convolution), the network will share the same beneﬁt with pure classiﬁcation models. Based on these two
principles, we propose a novel Global Convolutional Network (GCN) in Figure 2 B. Instead of directly using larger
kernel or global convolution, our GCN module employs a
combination of 1 × k + k × 1 and k × 1 + 1 × k convolutions, which enables densely connections within a large
k×k region in the feature map. Different from the separable
kernels used by , we do not use any nonlinearity after
convolution layers. Compared with the trivial k × k convolution, our GCN structure involves only O( 2
k) computation
cost and number of parameters, which is more practical for
large kernel sizes.
3.2. Overall Framework
Our overall segmentation model are shown in Figure 2.
We use pretrained ResNet as the feature network and
FCN4 as the segmentation framework. Multi-scale
feature maps are extracted from different stages in the feature network. Global Convolutional Network structures are
used to generate multi-scale semantic score maps for each
Figure 3. Visualization of valid receptive ﬁeld (VRF) introduced by . Regions on images show the VRF for the score map located at
the center of the bird. For traditional segmentation model, even though the receptive ﬁeld is as large as the input image, however, the VRF
just covers the bird (A) and fails to hold the entire object if the input resized to a larger scale (B). As a comparison, our Global Convolution
Network signiﬁcantly enlarges the VRF (C).
class. Similar to , score maps of lower resolution
will be upsampled with a deconvolution layer, then added
up with higher ones to generate new score maps. The ﬁnal
semantic score map will be generated after the last upsampling, which is used to output the prediction results.
In addition, we propose a Boundary Reﬁnement (BR)
block shown in Figure 2 C. Here, we models the boundary
alignment as a residual structure. More speciﬁcally, we de-
ﬁne ˜S as the reﬁned score map: ˜S = S + R(S), where S is
the coarse score map and R(·) is the residual branch. The
details can be referred to Figure 2.
4. Experiment
We evaluate our approach on the standard benchmark
PASCAL VOC 2012 and Cityscapes .
CAL VOC 2012 has 1464 images for training, 1449 images
for validation and 1456 images for testing, which belongs
to 20 object classes along with one background class. We
also use the Semantic Boundaries Dataset as auxiliary
dataset, resulting in 10,582 images for training. We choose
the state-of-the-art network ResNet 152 (pretrained on
ImageNet ) as our base model for ﬁne tuning. During the training time, we use standard SGD with batch
size 1, momentum 0.99 and weight decay 0.0005 . Data
augmentations like mean subtraction and horizontal ﬂip are
also applied in training. The performance is measured by
standard mean intersection-over-union (IoU). All the experiments are running with Caffe tool.
In the next subsections, ﬁrst we will perform a series of
ablation experiments to evaluate the effectiveness of our approach. Then we will report the full results on PASCAL
VOC 2012 and Cityscapes.
4.1. Ablation Experiments
In this subsection, we will make apple-to-apple comparisons to evaluate our approaches proposed in Section 3. As
mentioned above, we use PASCAL VOC 2012 validation
set for the evaluation. For all succeeding experiments, we
pad each input image into 512 × 512 so that the top-most
feature map is 16 × 16.
Figure 4. (A) Global Convolutional Network. (B) 1 × 1 convolution baseline. (C) k × k convolution. (D) stack of 3 × 3 convolutions.
Global Convolutional Network — Large Kernel
In Section 3.1 we propose Global Convolutional Network (GCN) to enable densely connections between classiﬁers and features. The key idea of GCN is to use large
kernels, whose size is controlled by the parameter k (see
Figure 2 B). To verify this intuition, we enumerate different k and test the performance respectively. The overall
network architecture is shown as in Figure 2 A except that
Boundary Reﬁnement block is not applied. For better comparison, a naive baseline is added just to replace GCN with a
simple 1×1 convolution (shown in Figure 4 B). The results
are presented in Table 1.
We try different kernel sizes ranging from 3 to 15. Note
that only odd size are used just to avoid alignment error. In
the case k = 15, which roughly equals to the feature map
size (16×16), the structure becomes “really global convoluk
Table 1. Experimental results on different k settings of Global
Convolutional Network. The score is evaluated by standard mean
IoU(%) on PASCAL VOC 2012 validation set.
tional”. From the results, we can ﬁnd that the performance
consistently increases with the kernel size k. Especially,
the “global convolutional” version (k = 15) surpasses the
smallest one by a signiﬁcant margin 5.5%. Results show
that large kernel brings great beneﬁt in our GCN structure,
which is consistent with our analysis in Section 3.1.
Further Discussion:
In the experiments in Table 1,
since there are other differences between baseline and different versions of GCN, it seems not so conﬁrmed to attribute the improvements to large kernels or GCN. For example, one may argue that the extra parameters brought by
larger k lead to the performance gain. Or someone may
think to use another simple structure instead of GCN to
achieve large equivalent kernel size. So we will give more
evidences for better understanding.
(1) Are more parameters helpful? In GCN, the number
of parameters increases linearity with kernel size k, so one
natural hypothesis is that the improvements in Table 1 are
mainly brought by the increased number of parameters. To
address this, we compare our GCN with the trivial large kernel design with a trivial k×k convolution shown in Figure 4
C. Results are shown in Table 2. From the results we can see
that for any given kernel size, the trivial convolution design
contains more parameters than GCN. However, the latter is
consistently better than the former in performance respectively. It is also clear that for trivial convolution version,
Score (GCN)
Score (Conv)
# of Params (GCN)
# of Params (Conv)
Table 2. Comparison experiments between Global Convolutional
Network and the trivial implementation. The score is measured
under standard mean IoU(%), and the 3rd and 4th rows show number of parameters of GCN and trivial Convolution after res-5.
larger kernel will result in better performance if k ≤5, yet
for k ≥7 the performance drops. One hypothesis is that
too many parameters make the training suffer from overﬁt,
which weakens the beneﬁts from larger kernels. However,
in training we ﬁnd trivial large kernels in fact make the network difﬁcult to converge, while our GCN structure will not
suffer from this drawback. Thus the actual reason still needs
further study.
(2) GCN vs. Stack of small convolutions. Instead of
GCN, another trivial approach to form a large kernel is to
use stack of small kernel convolutions(for example, stack
of 3 × 3 kernels in Figure 4 D), , which is very common
in modern CNN architectures such as VGG-net . For
example, we can use two 3×3 convolutions to approximate
a 5 × 5 kernel. In Table 3, we compare GCN with convolutional stacks under different equivalent kernel sizes. Different from , we do not apply nonlinearity within convolutional stacks so as to keep consistent with GCN structure.
Results shows that GCN still outperforms trivial convolution stacks for any large kernel sizes.
Score (GCN)
Score (Stack)
Table 3. Comparison Experiments between Global Convolutional
Network and the equivalent stack of small kernel convolutions.
The score is measured under standard mean IoU(%). GCN is still
better with large kernels (k > 7).
For large kernel size (e.g. k = 7) 3 × 3 convolutional
stack will bring much more parameters than GCN, which
may have side effects on the results. So we try to reduce
the number of intermediate feature maps for convolutional
stack and make further comparison. Results are listed in Table 4. It is clear that its performance suffers from degradation with fewer parameters. In conclusion, GCN is a better
structure compared with trivial convolutional stacks.
2048 (GCN)
# of Params
Table 4. Experimental results on the channels of stacking of small
kernel convolutions. The score is measured under standard mean
IoU. GCN outperforms the convolutional stack design with less
parameters.
(3) How GCN contributes to the segmentation results? In
Section 3.1, we claim that GCN improves the classiﬁcation
capability of segmentation model by introducing densely
connections to the feature map, which is helpful to handle large variations of transformations. Based on this, we
can infer that pixels lying in the center of large objects may
beneﬁt more from GCN because it is very close to “pure”
classiﬁcation problem. As for the boundary pixels of objects, however, the performance is mainly affected by the
localization ability.
To verify our inference, we divide the segmentation
score map into two parts: a) boundary region, whose pixels locate close to objects’ boundary (distance ≤7), and b)
internal region as other pixels. We evaluate our segmentation model (GCN with k = 15) in both regions. Results
are shown in Table 5. We ﬁnd that our GCN model mainly
improves the accuracy in internal region while the effect in
boundary region is minor, which strongly supports our argument. Furthermore, in Table 5 we also evaluate the boundary reﬁnement (BF) block referred in Section 3.2. In contrary to GCN structure, BF mainly improves the accuracy in
boundary region, which also conﬁrms its effectiveness.
Boundary (acc.)
Internal (acc. )
Overall (IoU)
Table 5. Experimental results on Residual Boundary Alignment.
The Boundary and Internal columns are measured by the per-pixel
accuracy while the 3rd column is measured by standard mean IoU.
Global Convolutional Network for Pretrained
In the above subsection our segmentation models are
ﬁnetuned from ResNet-152 network.
Since large kernel
plays a critical role in segmentation tasks, it is nature to apply the idea of GCN also on the pretrained model. Thus we
propose a new ResNet-GCN structure, as shown in Figure 5.
We remove the ﬁrst two layers in the original bottleneck
structure used by ResNet, and replace them with a GCN
module. In order to keep consistent with the original, we
also apply Batch Normalization and ReLU after each
of the convolution layers.
A: the bottleneck module in original ResNet. B: our
Global Convolutional Network in ResNet-GCN.
We compare our ResNet-GCN structure with the original
ResNet model. For fair comparison, sizes for ResNet-GCN
are carefully selected so that both network have similar
computation cost and number of parameters. More details
are provided in the appendix. We ﬁrst pretrain ResNet-GCN
on ImageNet 2015 and ﬁne tune on PASCAL VOC
2012 segmentation dataset. Results are shown in Table 6.
Note that we take ResNet50 model (with or without GCN)
for comparison because the training of large ResNet152 is
very costly. From the results we can see that our GCNbased ResNet is slightly poorer than original ResNet as an
ImageNet classiﬁcation model. However, after ﬁnetuning
on segmentation dataset ResNet-GCN model outperforms
original ResNet signiﬁcantly by 5.5%. With the application of GCN and boundary reﬁnement, the gain of GCNbased pretrained model becomes minor but still prevails.
We can safely conclude that GCN mainly helps to improve
segmentation performance, no matter in pretrained model
or segmentation-speciﬁc structures.
Pretrained Model
ResNet50-GCN
ImageNet cls err (%)
Seg. Score (Baseline)
Seg. Score (GCN + BR)
Table 6. Experimental results on ResNet50 and ResNet50-GCN.
Top-5 error of 224×224 center-crop on 256×256 image is used in
ImageNet classiﬁcation error. The segmentation score is measured
under standard mean IoU.
4.2. PASCAL VOC 2012
In this section we discuss our practice on PASCAL VOC
2012 dataset. Following , we employ the Microsoft COCO dataset to pre-train our model. COCO
has 80 classes and here we only retain the images including
the same 20 classes in PASCAL VOC 2012. The training
phase is split into three stages: (1) In Stage-1, we mix up all
the images from COCO, SBD and standard PASCAL VOC
2012, resulting in 109,892 images for training. (2) During
the Stage-2, we use the SBD and standard PASCAL VOC
2012 images, the same as Section 4.1. (3) For Stage-3, we
only use the standard PASCAL VOC 2012 dataset. The input image is padded to 640 × 640 in Stage-1 and 512 × 512
for Stage-2 and Stage-3. The evaluation on validation set is
shown in Table 7.
Stage-1(%)
Stage-2(%)
Stage-3(%)
Stage-3-MS(%)
Stage-3-MS-CRF(%)
Table 7. Experimental results on PASCAL VOC 2012 validation
set. The results are evaluated by standard mean IoU.
Our GCN + BR model clearly prevails, meanwhile the
post-processing multi-scale and denseCRF also bring
beneﬁts. Some visual comparisons are given in Figure 6.
We also submit our best model to the on-line evaluation
server, obtaining 82.2% on PASCAL VOC 2012 test set,
Figure 6. Examples of semantic segmentation results on PASCAL VOC 2012. For every row we list input image (A), 1 × 1 convolution
baseline (B), Global Convolutional Network (GCN) (C), Global Convolutional Network plus Boundary Reﬁnement (GCN + BR) (D), and
Ground truth (E).
as shown in Table 8. Our work has outperformed all the
previous state-of-the-arts.
mean-IoU(%)
FCN-8s-heavy 
TTI zoomout v2 
MSRA BoxSup 
DeepLab-MSc-CRF-LargeFOV 
Oxford TVG CRF RNN COCO 
CUHK DPN COCO 
Oxford TVG HO CRF 
CASIA IVA OASeg 
Adelaide VeryDeep FCN VOC 
LRR 4x ResNet COCO 
Deeplabv2-CRF 
CentraleSupelec Deep G-CRF 
Our approach
Table 8. Experimental results on PASCAL VOC 2012 test set.
4.3. Cityscapes
Cityscapes is a dataset collected for semantic segmentation on urban street scenes. It contains 24998 images
from 50 cities with different conditions, which belongs to
30 classes without background class. For some reasons,
only 19 out of 30 classes are evaluated on leaderboard. The
images are split into two set according to their labeling quality. 5,000 of them are ﬁne annotated while the other 19,998
are coarse annotated. The 5,000 ﬁne annotated images are
further grouped into 2975 training images, 500 validation
images and 1525 testing images.
The images in Cityscapes have a ﬁxed size of 1024 ×
2048, which is too large to our network architecture. Therefore we randomly crop the images into 800 × 800 during
training phase. We also increase k of GCN from 15 to 25
as the ﬁnal feature map is 25 × 25. The training phase is
split into two stages: (1) In Stage-1, we mix up the coarse
annotated images and the training set, resulting in 22,973
images. (2) For Stage-2, we only ﬁnetune the network on
training set. During the evaluation phase, we split the images into four 1024×1024 crops and fuse their score maps.
The results are given in Table 9.
Stage-1(%)
Stage-2(%)
Stage-2-MS(%)
Stage-2-MS-CRF(%)
Table 9. Experimental results on Cityscapes validation set. The
standard mean IoU is used here.
We submit our best model to the on-line evaluation
server, obtaining 76.9% on Cityscapes test set as shown
in Table 10. Once again, we outperforms all the previous
publications and reaches the new state-of-art.
5. Conclusion
According to our analysis on classiﬁcation and segmentation, we ﬁnd that large kernels is crucial to relieve the
contradiction between classiﬁcation and localization. Following the principle of large-size kernels, we propose the
Global Convolutional Network. The ablation experiments
show that our proposed structures meet a good trade-off
between valid receptive ﬁeld and the number of parameters, while achieves good performance. To further reﬁne
the object boundaries, we present a novel Boundary Re-
ﬁnement block.
Qualitatively, our Global Convolutional
Network mainly improve the internal regions while Boundary Reﬁnement increase performance near boundaries. Our
best model achieves state-of-the-art on two public benchmarks:
PASCAL VOC 2012 (82.2%) and Cityscapes