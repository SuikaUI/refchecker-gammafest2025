KANAZAWA-16-04
Restricted Boltzmann Machines for the Long Range Ising Models
Ken-Ichi Aoki1, ∗and Tamao Kobayashi2, †
1Institute for Theoretical Physics, Kanazawa University, Kanazawa 920-1192, Japan
2National Institute of Technology, Yonago College, Yonago 683-8502, Japan
We set up Restricted Boltzmann Machines (RBM) to reproduce the Long Range Ising (LRI)
models of the Ohmic type in one dimension. The RBM parameters are tuned by using the standard
machine learning procedure with an additional method of Conﬁguration with Probability (CwP).
The quality of resultant RBM are evaluated through the susceptibility with respect to the magnetic
external ﬁeld. We compare the results with those by Block Decimation Renormalization Group
(BDRG) method, and our RBM clear the test with satisfactory precision.
∗Electronic address: 
†Electronic address: 
 
INTRODUCTION
Quite recently the deep learning machines have drawn much attention since they are very
eﬀective for image processing and Go game etc. The deep learning machines can be
regarded as a method of information reduction keeping as much macroscopically important
features as possible. This policy or idea resembles to the renormalization group method in
physics , and the intrinsic relationship between them has been argued , and further
investigation is strongly desired from both sides.
Here in this article we make Restricted Boltzmann Machines (RBM) to reproduce
the Long Range Ising (LRI) models in one dimension. The LRI models have its own long
history , since they work as most simpliﬁed models to investigate the quantum dissipation issues which are still unveiled subjects lying in between classical and quantum physics
 . The LRI models have a critical point (temperature) to exhibit the spontaneous
magnetization . The critical point depends on the long range nature determined by
the power exponent of the interactions . Moreover, functional renormalization group
approaches revealed Ising model critical phenomena . They organized new framework
using the ﬁeld-theoretic formulation and developed powerful techniques with including higher
order diagrams. In this article, however, we are attempting to utilize the ﬁnite range scaling
method and direct comparison of our results with the above mentioned ones are not possible
yet. Here we adopt the Ohmic case where the power exponent is −2 and it is known to give
a marginal point of bringing about the ﬁnite critical temperature (K1c = 0.657) .
We are interested in the procedure of how we can make up appropriate RBM to reproduce
the LRI models. We deﬁne RBM with link parameters respecting the translational and
Parity invariance. We generate sample data set of the LRI models, and make RBM to learn
the data by tuning the RBM parameters to reach the maximum likelihood point.
Here we introduce a new method in setting up the input data for RBM learning. We
slice out the learning procedures into many small steps, where the sample data set is deﬁned
by a set of pair of Conﬁguration with its corresponding Probability (CwP). This method
resembles to the multi-canonical ensemble method where a part of the Hamiltonian is moved
out of the conﬁguration measure into the physical quantity to be averaged.
Finally we evaluate the total quality of the tuned up RBM. Starting with completely random spins, we produce output set of conﬁgurations, step by step, checking the susceptibility
of each set. After the onset of equilibrium, we examine the susceptibility. We compare our
results with those calculated by a renormalization group method called the Block Decimation
Renormalization Group (BDRG) which gives the exact susceptibility numerically .
This article is a short report of our work and full analysis will be published elsewhere.
RESTRICTED BOLTZMANN MACHINES FOR LONG RANGE ISING MOD-
We introduce the standard RBM consisting of visible variables v and hidden variables h.
The total probability distribution is deﬁned by
P(v, h) = 1
Z e−H(v,h) ,
, he−H(v,h) ,
where H is the Hamiltonian (energy function) and the partition function Z is the total
normalization constant. We integrate out the hidden variables to get the probability distribution function for v,
The standard restriction of RBM requires the Hamiltonian to take the following form,
H(v, h) = −
where we omit the external ﬁeld terms (linear in v, h) here.
Our target system is one dimensional Ising system and all variables v, h takes +1 or
−1 respectively. The number of hidden variables is exactly half of that of visible variables
and we adopt the periodic boundary condition. The links and their weights are deﬁned
as drawn in Fig. 1. Although other types of linking pattern may be considered, we take
this type since it has a well-deﬁned nearest neighbor solution. We respect the translational
and Parity invariance of the system, and therefore the link weights are all common to each
hidden variables and also are left-right symmetric k−n = kn. Note that precisely speaking,
the translational invariance holds for the hidden sector only, and in the visible sector, odd
and even site spins are not equivalent. Hereafter our RBM are denoted by
P(v, h; k) ,
FIG. 1: Deﬁnition of Restricted Boltzmann Machines.
where the machine parameter k represents {k0, k1, · · · }. The RBM visible probability distribution is given similarly,
P(v, h; k) .
THE LONG RANGE ISING MODELS
The LRI model is deﬁned by the following statistical weights,
PLRI(v) = 1
where Kn is the coupling constant for range n. We take the Ohmic type of the long range
Our purpose in this article is to tune the RBM parameter k so that the RBM visible
probability distribution may best reproduce the LRI probability distribution,
P(v; k∗) ≃PLRI(v) .
We divide the LRI Hamiltonian into two parts which are the nearest neighbor base part
and other long range part,
Z exp(−H0(v)) exp(−HL(v)) .
We set up the input data for RBM as follows. In this section we omit the Hamiltonian slicing
procedure for simplicity, which will be explained in the next section. We generate a set of
spin conﬁgurations exactly respecting the nearest neighbor part probability distribution.
This is done most quickly via the domain wall representation where the domain wall
exists with probability,
1 + exp(2K1) ,
and there is no correlation among domain walls. Therefore we can set each domain wall
independently, expect for caring the periodic boundary condition. We express this set of
conﬁguration by {v(µ)} where µ denotes discriminator of each conﬁguration.
Then the second part of the weight is considered as the physical quantity side.
calculate the additional probability which should be assigned to each conﬁguration generated
vµ =⇒pµ ∝exp(−HL(vµ)) .
Now our target probability distribution to be learned by RBM is deﬁned by a set of pair of
Conﬁguration with corresponding Probability, CwP:
{v(µ); pµ} .
The normalization of the probability is taken to be
where N is the total number of conﬁgurations.
Now we deﬁne the likelihood of RBM to produce the above CwP as follows:
P(v(µ), h; k).
Note that the probability pµ is included representing the eﬀective number of occurrences
of the corresponding conﬁguration. To search for the stationary point of the likelihood,
we diﬀerentiate the logarithm of likelihood function with respect to k. Using the explicit
deﬁnition of our RBM, we have the following derivative,
∂log(L(k))
j+n tanh(λj) −1
E [vj+nhj; k] ,
where jM is the total number of hidden variables h, λj is deﬁned by
and E[ · ] denotes the expectation value of operator by the RBM,
E [vj+nhj; k] =
vj+nhjP(v, h; k) .
Using the derivative above we adopt the steepest descent method to ﬁnd the maximum
likelihood position of RBM parameters. The expectation value part is evaluated by the
contrastive divergence method with several times of sample updates .
MACHINE LEARNING PROCEDURE AND RESULTS
Our purpose here is to make appropriate RBM to generate high quality distribution of spin
chain for the 1D Ising model with long range interactions. If the interactions among spins are
limited to the nearest neighbor type, the model is easily solved exactly and the corresponding
RBM solution is also obtained straightforwardly, although the practical machine learning
process is not trivial. However if the interactions are not nearest neighbor, the model cannot
be solved analytically, and its RBM counterpart is far from trivial.
LRI (Long Range Ising)
machine learning
comparison
FIG. 2: View of RBM learning procedures.
Our total strategy here is drawn in Fig. 2, although we omit the slicing processes explained
below for simplicity. Slicing is necessary since the diﬀerence of the probabilities in a set
should not be too large. The large deviation of probabilities causes drastic loss of sample
quality and the eﬀective size of data set is shrunk. We tune the slicing width so that the
averaged probability might be limited by some small size.
Now the slicing procedure is explained in some detail.
First of all we prepare sliced
Hamiltonians ∆Hm (m = 1, 2, · · · mMax) so that they satisfy the following properties:
∆Hm(v) = HM(v) ,
HM(v)|M=mMax = HL(v).
At each slicing step (m-th step here), the initial input conﬁgurations, denoted by
{v(µ)[m]},
is regarded as sarisfyng the probability distribution,
Pm(v) ∝exp(−H0(v)) exp(−Hm−1(v)).
Then we assign additional probability factor given by
pµ[m] = N exp(−∆Hm(v(µ)[m])
exp(−∆Hm(v(µ)[m])
where ∆Hm is a current slice of the remaining part of the Hamiltonian. Using this Conﬁguration with Probability: {v(µ)[m], pµ[m]} as the target data, RBM parameters are tuned up
(km →km+1),
{v(µ)[m], pµ[m]} ⇐⇒RBM(km)
=⇒RBM(km+1) =⇒{v(µ)[m + 1]} ,
and the output data set {v(µ)[m + 1]} by RBM (km+1) is expected to obey the probability
distribution,
Pm+1(v) ∝exp(−H0(v)) exp(−Hm(v)).
Then this data set works for the input conﬁguration set for the next sliced step and is
coupled with probability pµ[m + 1] deﬁned through ∆Hm+1.
In this serial procedures of learning, the set of conﬁguration is simultaneously updated.
The set is updated at each step of the steepest descent move of the machine through the
contrastive divergence iteration. At the stationary point of the machine, the ﬁnal set of
conﬁguration is used as the initial set of conﬁguration for the next slice, that is, each
conﬁguration is assigned the probability coming from the next sliced Hamiltonian eﬀect.
Actually our slicing order respects the range of interactions as follows. Starting with the
nearest neighbor conﬁgurations, where the probability of conﬁguration is all 1 (constant), we
add the non-nearest neighbor interactions of range 2, but sliced (divided) by some number.
We proceed RBM learning slice by slice, to reach the range 2 full interactions. Then we add
range 3 interactions, again with a slice. Proceeding this way further, ﬁnally we reach the
maximum range interactions, which is 9 in this article.
Practical and full analysis of RBM learning procedures are reported in the future full
paper and here we show the tuned RBM parameters and its evaluation by checking the
susceptibility estimates. The size of the system is 128 spins (the number of visible variables
v). The RBM links contains up to k12, that is, RBM has 13 machine parameters. The
total number of conﬁgurations for input is 1024. We take 64 random number series to get
averaged RBM machine parameters. The initial values of parameters are taken to be normal
0.037 -0.003 -0.002
0.023 -0.001 -0.002
0.000 -0.001
0.064 -0.004 -0.004
0.002 -0.001
0.000 -0.004
0.005 -0.002 -0.002
0.000 -0.005
0.088 -0.004 -0.005
0.004 -0.001 -0.002
0.001 -0.008
0.003 -0.001
0.009 -0.002 -0.001
0.001 -0.003
0.112 -0.009
0.009 -0.013
0.036 -0.001 -0.001
0.054 -0.001
TABLE I: Tuned Restricted Boltzmann Machines and their Evaluation.
values k0 = 1, k1 = 1, kn = 1/n2 (for n > 1). The total structure of the likelihood function
in the multi-dimensional space of k will not be discussed here. In fact, 64 machines give
well-converged results and we take averaged machine parameters to deﬁne the tuned up
RBM in the following results.
Table 1 is the results of the averaged RBM parameters, where K1 is the nearest neighbor
coupling constant and n is the maximum rang of the target LRI model interactions. For
n > 7, optimized kn are all small numbers and are not listed in the table.
In order to evaluate the quality of tuned RBM, we compare the susceptibility given by
RBM with those calculated by the Block Decimation Renormalization Group (BDRG). We
refer to a half of the logarithm of susceptibility χ,
X = log(χ)/2 ,
For the nearest neighbor case, X coincides with the coupling constant,
exactly in the inﬁnite size limit.
RBM Iteration
FIG. 3: RBM iteration of output data.
Starting with a set of 1024 perfectly random spin conﬁgurations (high temperature limit
ensemble), we operate the tuned up RBM. We evaluate the susceptibility, step by step, which
is seen in Fig. 3 as an example, where the target system is K1 = 0.6 and n = 9. The value
X starts from the vanishing value of random spins and it increases rather quickly. Finally it
slowly approaches towards the target value (1.565 in this case) which is drawn by a straight
line. After the equilibrium, the thermal ﬂuctuation is observed, whose size will be argued
in a separate paper. After the onset of thermalized equilibrium, we read out the parameter
X of the RBM by averaging over 100 iterations.
The results are listed in Table 1 and are shown in Fig. 4, where all data of four K values are
plotted (K = 0.2, 0.4, 0.6, 0.8 from bottom to top). The coincidence looks very good and our
tuned RBM well reproduce the LRI model results for the wide range of parameter values
of K and n. As for large susceptibility region, however, there appears small diﬀerences,
Long range interaction range
FIG. 4: Evaluation of RBM by comparing with BDRG.
some part of which might come from the fact that our system is ﬁnite, periodic 128 spins,
and shortage of the number of input conﬁgurations and/or iteration sets of learning and
evaluation. These will be discussed in a separate paper.
It should be noted here that the susceptibility is just one physical quantity though it
is most important, and we will investigate the RBM output conﬁgurations in detail to
further check the total equivalence or quality of probability distribution. Also we will clarify
the intimate relation between RBM and renormalization group method through multi-layer
RBM systems, which will give us a new viewpoint to understand the physical features of
LRI models.
We thank fruitful discussions with Shin-Ichiro Kumamoto, Hiromitsu Goto and Daisuke
Sato. This work is ﬁrst motivated by the general lecture given by Muneki Yasuda and we
thank him much for telling us basic notions of recent development of deep machine learning.
This work was partially supported by JSPS KAKENHI Grant Number 25610103 and the
2015th Research Grant of Yonago National College of Technology.
 J. Xie, L. Xu, E. Chen, Advances Neural Inform .Process. Syst. 25 350.
 David Silver et.al., Nature 529 ,7587 484
 K. G. Wilson, Rev. Mod. Phys. 47, 773.
 J. Berges, N. Tetradis, and C. Wetterich, Phys. Rep. 363, 223
 B. Delamotte, arXiv:cond-mat/0702365 .
 K-I. Aoki, Int. J. Mod. Phys. 14 , 1249;
K-I. Aoki, A. Horikoshi, M. Taniguchi and H. Terao, Phys. Rev. Lett. 108 , 572;
K-I. Aoki and A. Horikoshi, Phys. Lett. A 314 ,177; Phys. Rev. A 66 , 042105.
 P Mehta, DJ Schwab, arXiv.org e-Print archive, stat/1410.3831 .
 Smolensky, P., Information processing in dynamical systems: Foundations of harmony theory.
In D. E. Rumelhart, J. L. McClelland & the PDP Research Group, Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations. Cambridge,
MA: MIT Press/Bradford Books. 194.
 Hinton, GE., Neural computation 14, 8 1771.
 R. B. Griﬃths, J. Math. Phys. 8 478; Commun. Math. Phys.6 121.
D. Ruelle, Commun. Math. Phys.9 267.
F. J. Dyson, Commun. Math. Phys. 12 91.
M. Aizenman and R. Fern´andez, Lett. Math. Phys.16 39.
 A. O. Caldeira and A. J. Leggett, Phys. Rev. Lett. 46 211; Ann. of Phys. 149 
 K. Fujikawa, S. Iso, M. Sasaki and H. Suzuki, Phys. Rev. Lett. 68 1093. K. Fujikawa,
S. Iso, M. Sasaki and H. Suzuki, Phys.Rev. B46 10295.
 T. Matsuo, Y. Natsume and T. Kato, J. Phys. Soc. Jpn.75 103002.
T. Matsuo, Y. Natsume and T. Kato, J. Phys. Soc. Jpn. 75 103002; Phys. Rev. B 77
 184304.
 S. Chakravarty, Phys. Rev. Lett. 49 681. A. J. Bray, M. A. Moore, Phys. Rev. Lett.
49 1545.
 A. S. Kapoyannisa and N. Tetradisa, Phys. Lett. A276 225. D. Zappala, Phys. Lett.
A290 35.
 J. Froehlich and T. Spencer, Commun. Math. Phys. 84 87.
M. Aizenman and R. Fern´andez, Let. Math. Phys. 16 39.
M. Aizenman, J. T. Chayes, L. Chayes and C. M. Newman, J. Stat. Phys. 50 1.
J. Z. Imbrie and C. M. Newman, Commun. Math. Phys. 118 303.
 P. W. Anderson and G. Yuval, J. Phys. C4 607. J. M. Kosterlitz and D. J. Thouless,
J. Phys. C6 1181. J. M. Kosterlitz, J. Phys. C7 1046. J. M. Kosterlitz, Phys.
Rev. Lett. 37 1577. J. L. Cardy, J. Phys. A14 1407.
 J. Bhattacharjee, S. Chakravarty, J. L. Richardson and D. J.Scalapino, Phys. Rev. B24 
S. A. Cannas and A. C. N. de Magalhaes, J. Phys. A30 3345.
E. Bayong, H. T. Diep, and V. Dotsenko, Phys. Rev. Lett. 83 14.
Erik Luijten and Henk W. J. Bl¨ote, Phys. Rev. B 56 8945.
Erik Luijten and Holger. Meßingfeld, Phys. Lev. Lett. 86 5305.
 K-I. Aoki, T. Kobayashi and H. Tomita, Prog. Theor. Phys. 119 509.
 K-I. Aoki and T. Kobayashi, Mod. Phys. Lett. B 26 1250202.
 K-I. Aoki, T. Kobayashi and H. Tomita, Int. J. Mod. Phys. B 23 3739.