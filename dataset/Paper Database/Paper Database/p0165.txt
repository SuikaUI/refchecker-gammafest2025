Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6066–6080
July 5 - 10, 2020. c⃝2020 Association for Computational Linguistics
Word-level Textual Adversarial Attacking as Combinatorial Optimization
Yuan Zang1∗, Fanchao Qi1∗, Chenghao Yang2∗†, Zhiyuan Liu1‡,
Meng Zhang3, Qun Liu3, Maosong Sun1
1Department of Computer Science and Technology, Tsinghua University
Institute for Artiﬁcial Intelligence, Tsinghua University
Beijing National Research Center for Information Science and Technology
2Columbia University
3Huawei Noah’s Ark Lab
{zangy17,qfc17}@mails.tsinghua.edu.cn, 
{liuzy,sms}@tsinghua.edu.cn, {zhangmeng92,qun.liu}@huawei.com
Adversarial attacks are carried out to reveal the
vulnerability of deep neural networks.
Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring signiﬁcant change to the original input.
Word-level attacking, which can
be regarded as a combinatorial optimization
problem, is a well-studied class of textual attack methods. However, existing word-level
attack models are far from perfect, largely because unsuitable search space reduction methods and inefﬁcient optimization algorithms are
employed. In this paper, we propose a novel
attack model, which incorporates the sememebased word substitution method and particle
swarm optimization-based search algorithm to
solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT
on three benchmark datasets. Experimental results demonstrate that our model consistently
achieves much higher attack success rates and
crafts more high-quality adversarial examples
as compared to baseline methods. Also, further experiments show our model has higher
transferability and can bring more robustness
enhancement to victim models by adversarial
training. All the code and data of this paper
can be obtained on 
thunlp/SememePSO-Attack.
Introduction
Adversarial attacks use adversarial examples
 ,
which are maliciously crafted by perturbing the
original input, to fool the deep neural networks
∗Indicates equal contribution.
Yuan developed the
method, designed and conducted most experiments; Fanchao
formalized the task, designed some experiments and wrote
the paper; Chenghao made the original research proposal, performed human evaluation and conducted some experiments.
†Work done during internship at Tsinghua University
‡Corresponding author
produce shows
Original Input
Substitute
Search Space
Adversarial
Figure 1: An example showing search space reduction
with sememe-based word substitution and adversarial
example search in word-level adversarial attacks.
(DNNs). Extensive studies have demonstrated that
DNNs are vulnerable to adversarial attacks, e.g.,
minor modiﬁcation to highly poisonous phrases
can easily deceive Google’s toxic comment detection systems . From another
perspective, adversarial attacks are also used to
improve robustness and interpretability of DNNs
 . In the ﬁeld of natural language processing (NLP) which widely employs
DNNs, practical systems such as spam ﬁltering
 and malware detection
 have been broadly used,
but at the same time the concerns about their security are growing. Therefore, the research on textual
adversarial attacks becomes increasingly important.
Textual adversarial attacking is challenging. Different from images, a truly imperceptible perturbation on text is almost impossible because of its
discrete nature. Even a slightest character-level
perturbation can either (1) change the meaning and,
worse still, the true label of the original input, or
(2) break its grammaticality and naturality. Unfortunately, the change of true label will make the
adversarial attack invalid. For example, supposing an adversary changes “she” to “he” in an input
sentence to attack a gender identiﬁcation model, although the victim model alters its prediction result,
this is not a valid attack. And the adversarial examples with broken grammaticality and naturality
(i.e., poor quality) can be easily defended .
Various textual adversarial attack models have
been proposed , ranging from
character-level ﬂipping to
sentence-level paraphrasing .
Among them, word-level attack models, mostly
word substitution-based models, perform comparatively well on both attack efﬁciency and adversarial
example quality .
Word-level adversarial attacking is actually a
problem of combinatorial optimization , as its goal is to craft adversarial examples which can successfully fool the
victim model using a limited vocabulary. In this
paper, as shown in Figure 1, we break this combinatorial optimization problem down into two steps
including (1) reducing search space and (2) searching for adversarial examples.
The ﬁrst step is aimed at excluding invalid or
low-quality potential adversarial examples and retaining the valid ones with good grammaticality
and naturality. The most common manner is to
pick some candidate substitutes for each word in
the original input and use their combinations as the
reduced discrete search space. However, existing
attack models either disregard this step or adopt unsatisfactory substitution
methods that do not perform well in the trade-off
between quality and quantity of the retained adversarial examples . The second step is supposed to ﬁnd adversarial examples that can successfully fool the
victim model in the reduced search space. Previous
studies have explored diverse search algorithms
including gradient descent ,
genetic algorithm and greedy
algorithm . Some of them like
gradient descent only work in the white-box setting
where full knowledge of the victim model is required. In real situations, however, we usually have
no access to the internal structures of victim models. As for the other black-box algorithms, they are
not efﬁcient and effective enough in searching for
adversarial examples.
These problems negatively affect the overall attack performance of existing word-level adversarial attacking. To solve the problems, we propose
a novel black-box word-level adversarial attack
model, which reforms both the two steps. In the
ﬁrst step, we design a word substitution method
based on sememes, the minimum semantic units,
which can retain more potential valid adversarial
examples with high quality. In the second step,
we present a search algorithm based on particle
swarm optimization ,
which is very efﬁcient and performs better in ﬁnding adversarial examples. We conduct exhaustive
experiments to evaluate our model. Experimental
results show that, compared with baseline models,
our model not only achieves the highest attack success rate (e.g., 100% when attacking BiLSTM on
IMDB) but also possesses the best adversarial example quality and comparable attack validity. We
also conduct decomposition analyses to manifest
the advantages of the two parts of our model separately. Finally, we demonstrate that our model
has the highest transferability and can bring the
most robustness improvement to victim models by
adversarial training.
Background
In this section, we ﬁrst brieﬂy introduce sememes,
and then we give an overview of the classical particle swarm optimization algorithm.
In linguistics, a sememe is deﬁned as the minimum
semantic unit of human languages . The meaning of a word can be represented
by the composition of its sememes.
In the ﬁeld of NLP, sememe knowledge bases
are built to utilize sememes in practical applications, where sememes are generally regarded as
semantic labels of words (as shown in Figure 1).
HowNet is the most wellknown one. It annotates over one hundred thousand
English and Chinese words with a predeﬁned sets
of about 2,000 sememes. Its sememe annotations
are sense-level, i.e., each sense of a (polysemous)
word is annotated with sememes separately. With
the help of HowNet, sememes have been successfully applied to many NLP tasks including word
representation learning , sentiment
analysis , semantic composition , sequence modeling ,
reverse dictionary , etc.
Particle Swarm Optimization
Inspired by the social behaviors like bird ﬂocking,
particle swarm optimization (PSO) is a kind of
metaheuristic population-based evolutionary computation paradigms .
It has been proved effective in solving the optimization problems such as image classiﬁcation , part-of-speech tagging and text clustering .
Empirical studies have proven it is more efﬁcient
than some other optimization algorithms like the
genetic algorithm .
PSO exploits a population of interacting individuals to iteratively search for the optimal solution in
the speciﬁc space. The population is called a swarm
and the individuals are called particles. Each particle has a position in the search space and moves
with an adaptable velocity.
Formally, when searching in a D-dimensional
continuous space S ⊆RD with a swarm containing
N particles, the position and velocity of each particle can be represented by xn ∈S and vn ∈RD
respectively, n ∈{1, · · · , N}. Next we describe
the PSO algorithm step by step.
(1) Initialize. At the very beginning, each particle is randomly initialized with a position xn in
the search space and a velocity vn. Each dimension of the initial velocity vn
d ∈[−Vmax, Vmax],
d ∈{1, · · · , D}.
(2) Record. Each position in the search space
corresponds to an optimization score. The position
a particle has reached with the highest optimization
score is recorded as its individual best position. The
best position among the individual best positions
of all the particles is recorded as the global best
(3) Terminate. If current global best position
has achieved the desired optimization score, the
algorithm terminates and outputs the global best
position as the search result.
(4) Update. Otherwise, the velocity and position
of each particle are updated according to its current
position and individual best position together with
the global best position. The updating formulae are
d + c1 × r1 × . Therefore, the words with the same sememe
annotations should have the same meanings, and
they can serve as the substitutes for each other.
Compared with other word substitution methods, mostly including word embedding-based , language model-based and synonym-based methods , the sememe-based
word substitution method can achieve a better
trade-off between quality and quantity of substitute words.
For one thing, although the word embedding and
language model-based substitution methods can
ﬁnd as many substitute words as we want simply by
relaxing the restrictions on embedding distance and
language model prediction score, they inevitably
introduce many inappropriate and low-quality substitutes, such as antonyms and semantically related
but not similar words, into adversarial examples
which might break the semantics, grammaticality
and naturality of original input. In contrast, the
sememe-based and, of course, the synonym-based
substitution methods does not have this problem.
For another, compared with the synonym-based
method, the sememe-based method can ﬁnd more
substitute words and, in turn, retain more potential
adversarial examples, because HowNet annotates
sememes for all kinds of words. The synonymbased method, however, depends on thesauri like
WordNet , which provide no synonyms for many words like proper nouns and the
number of a word’s synonyms is very limited. An
empirical comparison of different word substitution
methods is given in Section 4.6.
In our sememe-based word substitution method,
to preserve grammaticality, we only substitute content words1 and restrict the substitutes to having
the same part-of-speech tags as the original words.
Considering polysemy, a word w can be substituted by another word w∗only if one of w’s senses
has the same sememe annotations as one of w∗’s
senses. When making substitutions, we conduct
lemmatization to enable more substitutions and
delemmatization to avoid introducing grammatical
PSO-based Adversarial Example Search
Before presenting our algorithm, we ﬁrst explain
what the concepts in the original PSO algorithm
correspond to in the adversarial example search
Different from original PSO, the search space of
word-level adversarial example search is discrete.
A position in the search space corresponds to a sentence (or an adversarial example), and each dimension of a position corresponds to a word. Formally,
1 · · · wn
d · · · wn
d), where D is
the length (word number) of the original input, wo
is the d-th word in the original input, and V . We repeat mutation N
times to initialize the positions of N particles. Each
dimension of each particle’s velocity is randomly
1Content words are the words that carry meanings and
consist mostly of nouns, verbs, adjectives and adverbs.
initialized between −Vmax and Vmax.
For the Record step, our algorithm keeps the
same as the original PSO algorithm. For the Terminate step, the termination condition is the victim
model predicts the target label for any of current
adversarial examples.
For the Update step, considering the discreteness of search space, we follow Kennedy and Eberhart to adapt the updating formula of velocity to
d + (1 −ω) × [I(pn
where ω is still the inertia weight, and I(a, b) is
Following Shi and Eberhart , we let the
inertia weight decrease with the increase of numbers of iteration times, aiming to make the particles
highly dynamic to explore more positions in the
early stage and gather around the best positions
quickly in the ﬁnal stage. Speciﬁcally,
ω = (ωmax −ωmin) × T −t
where 0 < ωmin < ωmax < 1, and T and t are the
maximum and current numbers of iteration times.
The updating of positions also needs to be adjusted to the discrete search space. Inspired by
Kennedy and Eberhart , instead of making
addition, we adopt a probabilistic method to update the position of a particle to the best positions.
We design two-step position updating. In the ﬁrst
step, a new movement probability Pi is introduced,
with which a particle determines whether it moves
to its individual best position as a whole. Once a
particle decides to move, the change of each dimension of its position depends on the same dimension
of its velocity, speciﬁcally with the probability of
sigmoid(vn
d ). No matter whether a particle has
moved towards its individual best position or not, it
would be processed in the second step. In the second step, each particle determines whether to move
to the global best position with another movement
probability Pg. And the change of each position
dimension also relies on sigmoid(vn
d ). Pi and Pg
vary with iteration to enhance search efﬁciency
by adjusting the balance between local and global
search, i.e., encouraging particles to explore more
BiLSTM %ACC
Sentiment Analysis
Sentiment Analysis
Table 1: Details of datasets and their accuracy results of victim models. “#Class” means the number of classiﬁcations. “Avg. #W” signiﬁes the average sentence length (number of words). “Train”, “Val” and “Test” denote
the instance numbers of the training, validation and test sets respectively. “BiLSTM %ACC” and “BERT %ACC”
means the classiﬁcation accuracy of BiLSTM and BERT.
space around their individual best positions in the
early stage and search for better position around
the global best position in the ﬁnal stage. Formally,
Pi = Pmax −t
T × (Pmax −Pmin),
Pg = Pmin + t
T × (Pmax −Pmin),
where 0 < Pmin < Pmax < 1.
Besides, to enhance the search in unexplored
space, we apply mutation to each particle after
the update step. To avoid excessive modiﬁcation,
mutation is conducted with the probability
Pm(xn) = min
0, 1 −kE(xn, xo)
where k is a positive constant, xo represents the
original input, and E measures the word-level edit
distance (number of different words between two
sentences). E(xn,xo)
is deﬁned as the modiﬁcation
rate of an adversarial example. After mutation, the
algorithm returns to the Record step.
Experiments
In this section, we conduct comprehensive experiments to evaluate our attack model on the tasks of
sentiment analysis and natural language inference.
Datasets and Victim Models
For sentiment analysis, we choose two benchmark
datasets including IMDB and
SST-2 . Both of them are binary sentiment classiﬁcation datasets. But the average sentence length of SST-2 (17 words) is much
shorter than that of IMDB (234 words), which renders attacks on SST-2 more challenging. For natural language inference (NLI), we use the popular Stanford Natural Language Inference (SNLI)
dataset . Each instance in
SNLI comprises a premise-hypothesis sentence
pair and is labelled one of three relations including
entailment, contradiction and neutral.
As for victim models, we choose two widely
used universal sentence encoding models, namely
bidirectional LSTM (BiLSTM) with max pooling
 and BERTBASE (BERT) . For BiLSTM, its hidden states
are 128-dimensional, and it uses 300-dimensional
pre-trained GloVe word
embeddings. Details of the datasets and the classi-
ﬁcation accuracy results of the victim models are
listed in Table 1.
Baseline Methods
We select two recent open-source word-level adversarial attack models as the baselines, which are
typical and involve different search space reduction
methods (step 1) and search algorithms (step 2).
The ﬁrst baseline method 
uses the combination of restrictions on word embedding distance and language model prediction
score to reduce search space. As for search algorithm, it adopts genetic algorithm, another popular metaheuristic population-based evolutionary
algorithm. We use “Embedding/LM+Genetic” to
denote this baseline method.
The second baseline chooses
synonyms from WordNet as substitutes and designs a saliency-based greedy algorithm as the search algorithm. We call this method
“Synonym+Greedy”. This baseline model is very
similar to another attack model TextFooler , which has extra semantic similarity checking when searching adversarial examples.
But we ﬁnd the former performs better in almost
all experiments, and thus we only select the former
as a baseline for comparison.
In addition, to conduct decomposition analyses
of different methods in the two steps separately, we
combine different search space reduction methods
(Embedding/LM, Synonym and our sememe-based
substitution method (Sememe)), and search algorithms (Genetic, Greedy and our PSO).
Evaluation Method
Success Rate
Human (Valid Attack Rate)
Modiﬁcation Rate
Grammaticality
Auto (Error Increase Rate)
Auto (Perplexity)
Naturality
Human (Naturality Score)
Details of evaluation metrics.
and “Human” represent automatic and human evaluations respectively. “Higher” and “Lower” mean the
higher/lower the metric, the better a model performs.
Experimental Settings
For our PSO, Vmax is set to 1, ωmax and ωmin
are set to 0.8 and 0.2, Pmax and Pmin are also
set to 0.8 and 0.2, and k in Equation (6) is set to
2. All these hyper-parameters have been tuned on
the validation set. For the baselines, we use their
recommended hyper-parameter settings. For the
two population-based search algorithms Genetic
and PSO, we set the maximum number of iteration
times (T in Section 3.2) to 20 and the population
size (N in Section 3.2) to 60, which are the same
as Alzantot et al. .
Evaluation Metrics
To improve evaluation efﬁciency, we randomly
sample 1, 000 correctly classiﬁed instances from
the test sets of the three datasets as the original input to be perturbed. For SNLI, only the hypotheses
are perturbed. Following Alzantot et al. , we
restrict the length of the original input to 10-100,
exclude the out-of-vocabulary words from the substitute sets, and discard the adversarial examples
with modiﬁcation rates higher than 25%.
We evaluate the performance of attack models
including their attack success rates, attack validity
and the quality of adversarial examples. The details
of our evaluation metrics are listed in Table 2.
(1) The attack success rate is deﬁned as the percentage of the attacks which craft an adversarial
example to make the victim model predict the target
label. (2) The attack validity is measured by the percentage of valid attacks to successful attacks, where
the adversarial examples crafted by valid attacks
have the same true labels as the original input. (3)
For the quality of adversarial examples, we divide
it into four parts including modiﬁcation rate, grammaticality, ﬂuency and naturality. Grammaticality
is measured by the increase rate of grammatical
error numbers of adversarial examples compared
with the original input, where we use Language-
Tool2 to obtain the grammatical error number of a
sentence. We utilize the language model perplexity (PPL) to measure the ﬂuency with the help of
GPT-2 . The naturality re-
ﬂects whether an adversarial example is natural and
indistinguishable from human-written text.
We evaluate attack validity and adversarial example naturality only on SST-2 by human evaluation
with the help of Amazon Mechanical Turk3. We
randomly sample 200 adversarial examples, and
ask the annotators to make a binary sentiment classiﬁcation and give a naturality score (1, 2 or 3,
higher better) for each adversarial example and
original input. More annotation details are given in
Appendix A.
Attack Performance
Attack Success Rate
The attack success rate results of all the models are listed in Table 3. We
observe that our attack model (Sememe+PSO)
achieves the highest attack success rates on all
the three datasets (especially the harder SST-
2 and SNLI) and two victim models, proving
the superiority of our model over baselines. It
attacks BiLSTM/BERT on IMDB with a notably 100.00%/98.70% success rate, which clearly
demonstrates the vulnerability of DNNs. By comparing three word substitution methods (search
space reduction methods) and three search algorithms, we ﬁnd Sememe and PSO consistently outperform their counterparts. Further decomposition
analyses are given in a later section.
Validity and Adversarial Example Quality
We evaluate the attack validity and adversarial example quality of our model together with the two
baseline methods (Embedding/LM+Genetic and
Synonym+Greedy). The results of automatic and
human evaluations are displayed in Table 4 and
5 respectively.4 Note that the human evaluations
including attack validity and adversarial example
naturality are conducted on SST-2 only. We ﬁnd
that in terms of automatic evaluations of adversarial example quality, including modiﬁcation rate,
grammaticality and ﬂuency, our model consistently
outperforms the two baselines on whichever victim
model and dataset. As for attack validity and adver-
2 
3 
4Automatic evaluation results of adversarial example quality of all the combination models are shown in Appendix B.
Word Substitution
Embedding/LM
Table 3: The attack success rates (%) of different attack models.
Attack Model
Embedding/LM+Genetic
Synonym+Greedy
Sememe+PSO
Embedding/LM+Genetic
Synonym+Greedy
Sememe+PSO
Table 4: Automatic evaluation results of adversarial example quality. “%M”, “%I” and “PPL” indicate the modiﬁcation rate, grammatical error increase rate and language model perplexity respectively.
Attack Model
Original Input
Embedding/LM+Genetic
Synonym+Greedy
Sememe+PSO
Embedding/LM+Genetic
Synonym+Greedy
Sememe+PSO
Table 5: Human evaluation results of attack validity
and adversarial example naturality on SST-2, where
the second row additionally lists the evaluation results
of original input. “%Valid” refers to the percentage
of valid attacks. “NatScore” is the average naturality
score of adversarial examples.
sarial example naturality, our Sememe+PSO model
obtains a slightly higher overall performance than
the two baselines. But its adversarial examples
are still inferior to original human-authored input,
especially in terms of validity (label consistency).
We conduct Student’s t-tests to further measure
the difference between the human evaluation results
of different models, where the statistical signiﬁcance threshold of p-value is set to 0.05. We ﬁnd
that neither of the differences of attack validity and
adversarial example naturality between different
models are signiﬁcant. In addition, the adversarial
examples of any attack model have signiﬁcantly
worse label consistency (validity) than the original
input, but possesses similar naturality. More details
of statistical signiﬁcance test are given in Appendix
For Embedding/LM, relaxing the restrictions on
embedding distance and language model prediction score can improve its attack success rate but
sacriﬁces attack validity. To make a speciﬁc comparison, we adjust the hyper-parameters of Embedding/LM+Genetic5 to increase its attack success rates to 96.90%, 90.30%, 58.00%, 93.50%,
83.50% and 62.90% respectively on attacking the
two victim models on the three datasets (in the
same order as Table 3). Nonetheless, its attack validity rates against BiLSTM and BERT on SST-2
dramatically fall to 59.5% and 56.5%. In contrast,
ours are 70.5% and 72.0%, and their differences
are signiﬁcant according to the results of signiﬁcance tests in Appendix D.
Decomposition Analyses
In this section, we conduct detailed decomposition analyses of different word substitution methods (search space reduction methods) and different
search algorithms, aiming to further demonstrate
the advantages of our sememe-based word substitution method and PSO-based search algorithm.
5The detailed hyper-parameter settings are given in Appendix C.
Word Substitution Method
Embedding/LM
Table 6: The average number of substitutes provided
by different word substitution methods.
She breaks the pie dish and screams out that she is not handicapped.
Embedding/LM
tart, pizza, apple,
shoemaker, cake
cheesecake
cheese, popcorn, ham, cream,
break, cake, pizza, chocolate,
and 55 more
Table 7: A real case showing the substitutes found by
three word substitution methods, where the original
word is colored green and appropriate substitutes are
colored red.
Maximum Number of Iteration Times
Attack Success Rate
Sememe+PSO
Synonym+PSO
Sememe+Genetic
Synonym+Genetic
Figure 2: Attack success rates of different models with
different maximum numbers of iteration times. The xcoordinate is in log-2 scale.
Word Substitution Method
Table 6 lists the average number of substitutes provided by different
word substitution methods on the three datasets.
It shows Sememe can ﬁnd much more substitutes
than the other two counterparts, which explains
the high attack success rates of the models incorporating Sememe. Besides, we give a real case
from SST-2 in Table 7 which lists substitutes found
by the three methods. We observe that Embedding/LM ﬁnd many improper substitutes, Synonym
cannot ﬁnd any substitute because the original word
“pie” has no synonyms in WordNet, and only Sememe ﬁnds many appropriate substitutes.
population-based search algorithms Genetic and
PSO by changing two important hyper-parameters,
namely the maximum number of iteration times T
and the population size N. The results of attack
success rate are shown in Figure 2 and 3. From the
two ﬁgures, we ﬁnd our PSO outperforms Genetic
Population Size
Attack Success Rate
Sememe+PSO
Synonym+PSO
Sememe+Genetic
Synonym+Genetic
Figure 3: Attack success rates of different models with
population sizes. The x-coordinate is in log-2 scale.
Attack Model
Embedding/LM+Genetic
Synonym+Greedy
Sememe+PSO
Embedding/LM+Genetic
Synonym+Greedy
Sememe+PSO
Table 8: The classiﬁcation accuracy of transferred adversarial examples on the three datasets. Lower accuracy reﬂects higher transferability.
consistently, especially in the setting with severe
restrictions on maximum number of iteration
times and population size, which highlights the
efﬁciency of PSO.
Transferability
The transferability of adversarial examples reﬂects
whether an attack model can attack a DNN model
without any access to it .
It has been widely used as an important evaluation metric in adversarial attacks. We evaluate
the transferability of adversarial examples by using BiLSTM to classify the adversarial examples
crafted for attacking BERT, and vice versa. Table 8 shows the classiﬁcation accuracy results of
transferred adversarial examples. Note that lower
accuracy signiﬁes higher transferability. The lower
the accuracy is, the higher the transferability is.
We ﬁnd compared with the two baselines, our Sememe+PSO crafts adversarial examples with overall higher transferability.
Adversarial Training
Adversarial training is proposed to improve the
robustness of victim models by adding adversarial examples to the training set . In this experiment, for each attack model,
we craft 692 adversarial examples (10% of the original training set size) by using it to attack BiL-
STM on the training set of SST-2. Then we add
the adversarial examples to the training set and retrain a BiLSTM. We re-evaluate its robustness by
calculating the attack success rates of different attack models. Table 9 lists the results of adversarial
training. Note larger attack success rate decrease
signiﬁes greater robustness improvement. We ﬁnd
that adversarial training can improve the robustness
of victim models indeed, and our Sememe+PSO
model brings greater robustness improvement than
the two baselines, even when the attack models
are exactly themselves.6 From the perspective of
attacking, our Sememe+PSO model is still more
threatening than others even under the defense of
adversarial training.
We also manually select 692 valid adversarial
examples generated by Sememe+PSO to conduct
adversarial training, which leads to even greater
robustness improvement (last column of Table 9).
The results show that adversarial example validity
has big inﬂuence on adversarial training effect.
Related Work
Existing textual adversarial attack models can be
classiﬁed into three categories according to the perturbation levels of their adversarial examples.
Sentence-level attacks include adding distracting sentences , paraphrasing and
performing perturbations in the continuous latent
semantic space . Adversarial
examples crafted by these methods usually have
profoundly different forms from original input and
their validity are not guaranteed.
Character-level attacks are mainly random character manipulations including swap, substitution,
deletion, insertion and repeating . In addition, gradient-based character substitution methods have also been explored, with the
help of one-hot character embeddings or visual character embeddings . Although character-level attacks can
achieve high success rates, they break the grammaticality and naturality of original input and can be
easily defended .
6For instance, using Embedding/LM+Genetic in adversarial training to defend its attack declines the attack success rate
by 2.60% while using our Sememe+PSO model declines by
Att \Adv.T
The attack success rates of different attack models when attacking BiLSTM on SST-2 and
their decrements brought by adversarial training. “Att”
and “Adv.T” denote “Attack Model” and “Adversarial
Training”. E/L+G, Syn+G and Sem+P represent Embedding/LM+Genetic, Synonym+Greedy and our Sememe+PSO, respectively. “Sem+P*” denotes only using the valid adversarial examples generated by Sememe+PSO in adversarial training.
As for word-level attacks, following our twostep modeling, their adversarial example space reduction methods (step 1) involve using word embeddings or language model
 to ﬁlter words, selecting synonyms as substitutes , and their combinations .
The search algorithms (step 2) include gradient descent , genetic algorithm , Metropolis-Hastings sampling , saliency-based greedy algorithm . In
comparison, our model adopts new methods in both
steps which are more powerful.
Conclusion and Future Work
In this paper, we propose a novel word-level attack
model comprising the sememe-based word substitution method and particle swarm optimization-based
search algorithm. We conduct extensive experiments to demonstrate the superiority of our model
in terms of attack success rate, adversarial example
quality, transferability and robustness improvement
to victim models by adversarial training. In the future, we will try to increase the robustness gains of
adversarial training and consider utilizing sememes
in adversarial defense model.
Acknowledgments
This work is supported by the National Key Research and Development Program of China (No.
2018YFB1004503) and the National Natural Science Foundation of China (NSFC No. 61732008,
61772302). We also thank the anonymous reviewers for their valuable comments and suggestions.