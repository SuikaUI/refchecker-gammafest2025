Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 458–467,
Seattle, Washington, USA, 18-21 October 2013. c⃝2013 Association for Computational Linguistics
Automatic Feature Engineering for Answer Selection and Extraction
Aliaksei Severyn
DISI, University of Trento
38123 Povo (TN), Italy
 
Alessandro Moschitti
Qatar Computing Research Institue
5825 Doha, Qatar
 
This paper proposes a framework for automatically engineering features for two important
tasks of question answering: answer sentence
selection and answer extraction. We represent
question and answer sentence pairs with linguistic structures enriched by semantic information, where the latter is produced by automatic classiﬁers, e.g., question classiﬁer and
Named Entity Recognizer. Tree kernels applied to such structures enable a simple way to
generate highly discriminative structural features that combine syntactic and semantic information encoded in the input trees. We conduct experiments on a public benchmark from
TREC to compare with previous systems for
answer sentence selection and answer extraction. The results show that our models greatly
improve on the state of the art, e.g., up to 22%
on F1 (relative improvement) for answer extraction, while using no additional resources
and no manual feature engineering.
Introduction
Question Answering (QA) systems are typically
built from three main macro-modules: (i) search and
retrieval of candidate passages; (ii) reranking or selection of the most promising passages; and (iii) answer extraction. The last two steps are the most interesting from a Natural Language Processing viewpoint since deep linguistic analysis can be carried
out as the input is just a limited set of candidates.
Answer sentence selection refers to the task of selecting the sentence containing the correct answer
among the different sentence candidates retrieved by
a search engine.
Answer extraction is a ﬁnal step, required for
factoid questions, consisting in extracting multiwords constituting the synthetic answer, e.g., Barack
Obama for a question: Who is the US president?
The deﬁnition of rules for both tasks is conceptually
demanding and involves the use of syntactic and semantic properties of the questions and its related answer passages.
For example, given a question from TREC QA1:
Q: What was Johnny Appleseed’s real
and a relevant passage, e.g., retrieved by a search
A: Appleseed, whose real name was John
Chapman, planted many trees in the early
a rule detecting the semantic links between Johnny
Appleseed’s real name and the correct answer
John Chapman in the answer sentence has to
be engineered.
This requires the deﬁnition of
other rules that associate the question pattern
real name ?(X) with real name is(X) of
the answer sentence. Although this can be done by
an expert NLP engineer, the effort for achieving the
necessary coverage and a reasonable accuracy is not
negligible.
An alternative to manual rule deﬁnition is the use
of machine learning, which often shifts the problem
1We use it as our running example in the rest of the paper.
to the easier task of feature engineering. Unfortunately, when the learning task is semantically dif-
ﬁcult such as in QA, e.g., features have to encode
combinations of syntactic and semantic properties.
Thus their extraction modules basically assume the
shape of high-level rules, which are, in any case, essential to achieve state-of-the-art accuracy. For example, the great IBM Watson system uses a learning to rank algorithm fed with
hundreds of features. The extraction of some of the
latter requires articulated rules/algorithms, which,
in terms of complexity, are very similar to those
constituting typical handcrafted QA systems.
immediate consequence is the reduced adaptability
to new domains, which requires a substantial reengineering work.
In this paper, we show that tree kernels can be applied to
automatically learn complex structural patterns for
both answer sentence selection and answer extraction. Such patterns are syntactic/semantic structures
occurring in question and answer passages. To make
such information available to the tree kernel functions, we rely on the shallow syntactic trees enriched
with semantic information , e.g., Named Entities (NEs)
and question focus and category, automatically derived by machine learning modules, e.g., question
classiﬁer (QC) or focus classiﬁer (FC).
More in detail, we (i) design a pair of shallow
syntactic trees (one for the question and one for the
answer sentence); (ii) connect them with relational
nodes (i.e., those matching the same words in the
question and in the answer passages); (iii) label the
tree nodes with semantic information such as question category and focus and NEs; and (iv) use the NE
type to establish additional semantic links between
the candidate answer, i.e., an NE, and the focus word
of the question. Finally, for the task of answer extraction we also connect such semantic information
to the answer sentence trees such that we can learn
factoid answer patterns.
We show that our models are very effective in producing features for both answer selection and extraction by experimenting with TREC QA corpora
and directly comparing with the state of the art,
e.g., . The results show that our methods greatly improve on both
tasks yielding a large improvement in Mean Average
Precision for answer selection and in F1 for answer
extraction: up to 22% of relative improvement in F1,
when small training data is used. Moreover, in contrast to the previous work, our model does not rely
on external resources, e.g., WordNet, or complex
features in addition to the structural kernel model.
The reminder of this paper is organized as follows, Sec. 2 describes our kernel-based classiﬁers,
Sec. 3 illustrates our question/answer relational
structures also enriched with semantic information,
Sec. 4 describes our model for answer selection and
extraction, Sec. 5 illustrates our comparative experiments on TREC data, Sec. 6 reports on our error
analysis, Sec. 7 discusses the related work, and ﬁnally, Sec. 8 derives the conclusions.
Structural Kernels for classiﬁcation
This section describes a kernel framework where the
input question/answer pairs are handled directly in
the form of syntactic/semantic structures.
Feature vector approach to object pair
classiﬁcation
A conventional approach to represent a question/answer pairs in linear models consists in deﬁning a set of similarity features {xi} and computing
the simple scalar product h(xxx) = www · xxx = P
where www is the model weight vector learned on the
training data.
Hence, the learning problem boils
down to estimating individual weights of each of
the similarity features xi. Such features often encode various types of lexical, syntactic and semantic
similarities shared between a question and its candidate. Previous work used a rich number of distributional semantic, knowledge-based, translation and
paraphrase resources to build explicit feature vector
representations. One evident potential downside of
using feature vectors is that a great deal of structural
information encoded in a given text pair is lost.
Pair Classiﬁcation using Structural Kernels
A more versatile approach in terms of the input
representation relies on kernels.
A typical kernel machine, e.g., SVM, classiﬁes a test input xxx
using the following prediction function: h(xxx) =
i αiyiK(xxx,xxxi), where αi are the model parameters estimated from the training data, yi are target
variables, xxxi are support vectors, and K(·, ·) is a kernel function. The latter can measure the similarity
between question and answer pairs.
We deﬁne each question/answer pair xxx as a triple
composed of a question tree TTT q and answer sentence
tree TTT s and a similarity feature vector vvv, i.e., xxx =
⟨TTT q,TTT s,vvv⟩. Given two triples xxxi and xxxj, we deﬁne
the following kernel:
K(xxxi,xxxj)
Kv(vvvi,vvvj),
where KTK computes a structural kernel, e.g., tree
kernel, and Kv is a kernel over feature vectors, e.g.,
linear, polynomial, gaussian, etc. Structural kernels
can capture the structural representation of a question/answer pair whereas traditional feature vectors
can encode some sort of similarity, e.g., lexical, syntactic, semantic, between a question and its candidate answer.
We prefer to split the kernel computation over a
question/answer pair into two terms since tree kernels are very efﬁcient and there are no efﬁcient
graph kernels that can encode exhaustively all graph
fragments. It should be noted that the tree kernel
sum does not capture feature pairs. Theoretically,
for such purpose, a kernel product should be used.
However, our experiments revealed that using the
product is actually worse in practice. In contrast,
we solve the lack of feature pairing by annotating
the trees with relational tags which are supposed
to link the question tree fragments with the related
fragments from the answer sentence.
Such relational information is very important to
improve the quality of the pair representation as well
as the implicitly generated features. In the next section, we show simple structural models that we used
in our experiments for question and answer pair classiﬁcation.
Partial Tree Kernels
The above framework can use any kernel for
structural data.
We use the Partial Tree Kernel
(PTK) to compute KTK(·, ·) as it
is the most general convolution tree kernel, which
at the same time shows rather good efﬁciency. PTK
can be effectively applied to both constituency and
dependency parse trees. It generalizes the syntactic
tree kernel (STK) , which
maps a tree into the space of all possible tree fragments constrained by the rule that sibling nodes cannot be separated. In contrast, the PTK fragments
can contain any subset of siblings, i.e., PTK allows
for breaking the production rules in syntactic trees.
Consequently, PTK generates an extremely rich feature space, which results in higher generalization
Relational Structures
This section introduces relational structures designed to encode syntactic and shallow semantic
properties of question/answer pairs. We ﬁrst deﬁne a
simple to construct shallow syntactic tree representation derived from a shallow parser. Next, we introduce a relational linking scheme based on a plain
syntactic matching and further augment it with additional semantic information.
Shallow syntactic tree
Our shallow tree structure is a two-level syntactic
hierarchy built from word lemmas (leaves), part-ofspeech tags that organized into chunks identiﬁed by
a shallow syntactic parser (Fig. 1). We deﬁned a
similar structure in 
for answer passage reranking, which improved on
feature vector baselines.
This simple linguistic representation is suitable
for building a rather expressive answer sentence selection model. Moreover, the use of a shallow parser
is motivated by the need to generate text spans to
produce candidate answers required by an answer
extraction system.
Tree pairs enriched with relational links
It is important to establish a correspondence between question and answer sentence aligning related
concepts from both.
We take on a two-level approach, where we ﬁrst use plain lexical matching to
connect common lemmas from the question and its
candidate answer sentence. Secondly, we establish
semantic links between NEs extracted from the answer sentence and the question focus word, which
encodes the expected lexical answer type (LAT). We
use the question categories to identify NEs that have
Figure 1: Shallow tree representation of the example q/a pair from Sec. 1. Dashed arrows (red) indicate the tree
fragments (red dashed boxes) in the question and its answer sentence linked by the relational REL tag, which is
established via syntactic match on the word lemmas. Solid arrows (blue) connect a question focus word name with the
related named entities of type Person corresponding to the question category (HUM) via a relational tag REL-HUM.
Additional ANS tag is used to mark chunks containing candidate answer (here the correct answer John Chapman).
higher probability to be correct answers following a
mapping deﬁned in Table 1.
Next, we brieﬂy introduce our tree kernel-based
models for building question focus and category
classiﬁers.
Lexical Answer Type. Question Focus represents
a central entity or a property asked by a question
 . It can be used to search for semantically compatible candidate answers, thus greatly reducing the search space . While several machine learning approaches based on manual
features and syntactic structures have been recently
explored, e.g. , we opt for
the latter approach where tree kernels handle automatic feature engineering.
To build an automatic Question Focus detector we
use a tree kernel approach as follows: we (i) parse
each question; (ii) create a set of positive trees by
labeling the node exactly covering the focus with
FC tag; (iii) build a set of negative trees by labeling
any other constituent node with FC; (iii) we train
the FC node classiﬁer with tree kernels. At the test
time, we try to label each constituent node with FC
generating a set of candidate trees. Finally, we select
the tree and thus the constituent associated with the
highest SVM score.
Question classiﬁcation. Our question classiﬁcation
model is simpler than before: we use an SVM multiclassiﬁer with tree kernels to automatically extract
the question class. To build a multi-class classiﬁer
we train a binary SVM for each of the classes and
apply a one-vs-all strategy to obtain the predicted
Table 1: Expected Answer Type (EAT) →named entity
Named Entity types
Organization, Person, Misc
Date, Time, Number
Number, Percentage
Money, Number
class. We use constituency trees as our input representation.
Our question taxonomy is derived from the
UIUIC dataset which deﬁnes
6 coarse and 50 ﬁne grain classes.
In particular,
our set of question categories is formed by adopting 3 coarse classes: HUM (human), LOC (location), ENTY (entities) and replacing the NUM (numeric) coarse class with 3 ﬁne-grain classes: CUR-
RENCY, DATE, QUANTITY2. This set of question
categories is sufﬁcient to capture the coarse semantic answer type of the candidate answers found in
TREC. Also using fewer question classes results in
a more accurate multi-class classiﬁer.
Semantic tagging. Question focus word speciﬁes
the lexical answer type capturing the target information need posed by a question, but to make this piece
of information effective, the focus word needs to
be linked to the target candidate answer. The focus
word can be lexically matched with words present in
2This class is composed by including all the ﬁne-grain
classes from NUMERIC coarse class except for CURRENCY
the answer sentence, or the match can be established
using semantic information. Clearly, the latter approach is more appealing since it helps to alleviate
the lexical gap problem, i.e., it improves the coverage of the n¨aive string matching of words between a
question and its answer.
Hence, we propose to exploit a question focus
along with the related named entities (according to
the mapping from Table 1) of the answer sentence
to establish relational links between the tree fragments. In particular, once the question focus and
question category are determined, we link the focus word wfocus in the question, with all the named
entities whose type matches the question class (Table 1). We perform tagging at the chunk level and
use a relational tag typed with a question class, e.g.,
REL-HUM. Fig. 1 shows an example q/a pair where
the typed relational tag is used in the shallow syntactic tree representation to link the chunk containing
the question focus name with the named entities of
the corresponding type Person, i.e., Appleseed and
John Chapman.
Answer Sentence Selection and Answer
Keyword Extraction
This section describes our approach to (i) answer
sentence selection used to select the most promising
answer sentences; and (ii) answer extraction which
returns the answer keyword (for factoid questions).
Answer Sentence Selection
We cast the task of answer sentence selection as
a classiﬁcation problem. Considering a supervised
learning scenario, we are given a set of questions
i=1 where each question qi is associated with
a list of candidate answer sentences {(ri, si)}N
with ri ∈{−1, +1} indicating if a given candidate
answer sentence si contains a correct answer (+1)
or not (−1). Using this labeled data, our goal is to
learn a classiﬁer model to predict if a given pair of
a question and an answer sentence is correct or not.
We train a binary SVM with tree kernels3 to train an
answer sentence classiﬁer. The prediction scores obtained from a classiﬁer are used to rerank the answer
candidates (pointwise reranking), s.t. the sentences
that are more likely to contain correct answers will
3disi.unitn.it/moschitti/Tree-Kernel.htm
be ranked higher than incorrect candidates. In addition to the structural representation, we augment our
model with basic bag-of-word features (unigram and
bigrams) computed over lemmas.
Answer Sentence Extraction
The goal of answer extraction is to extract a text span
from a given candidate answer sentence. Such span
represents a correct answer phrase for a given question. Different from previous work that casts the answer extraction task as a tagging problem and apply
a CRF to learn an answer phrase tagger , we take on a simpler approach using a kernelbased classiﬁer.
In particular, we rely on the shallow tree representation, where text spans identiﬁed by a shallow syntactic parser serve as a source of candidate answers.
Algorithm 1 speciﬁes the steps to generate training
data for our classiﬁer.
In particular, for each example representing a triple ⟨a, Tq, Ts⟩composed of
the answer a, the question and the answer sentence
trees, we generate a set of training examples E with
every candidate chunk marked with an ANS tag (one
at a time). To reduce the number of generated examples for each answer sentence, we only consider NP
chunks, since other types of chunks, e.g., VP, ADJP,
typically do not contain factoid answers. Finally, an
original untagged tree is used to generate a positive
example (line 8), when the answer sentence contains
a correct answer, and a negative example (line 10),
when it does not contain a correct answer.
At the classiﬁcation time, given a question and a
candidate answer sentence, all NP nodes of the sentence are marked with ANS (one at a time) as the
possible answer, generating a set of tree candidates.
Then, such trees are classiﬁed (using the kernel from
Eq. 1) and the one with the highest score is selected.
If no tree is classiﬁed as positive example we do not
extract any answer.
Experiments
We provide the results on two related yet different
tasks: answer sentence selection and answer extraction.
The goal of the former is to learn a model
scoring correct question and answer sentence pairs
to bring in the top positions sentences containing the
correct answers. Answer extraction derives the cor-
Algorithm 1 Generate training data for answer extraction
1: for all ⟨a, Tq, Ts⟩∈D
for all chunk ∈extract chunks(Ts) do
if not chunk == NP then
s ←tagAnswerChunk(Ts, chunk)
if contains answer(a, chunk) then
e ←build example ,
Mooney GeoQuery and
the dataset from . The
SeCo dataset contains 600 questions. The Mooney
GeoQuery contains 250 question targeted at geographical information in the U.S. The ﬁrst two
datasets are very domain speciﬁc, while the dataset
from is more generic
containing the ﬁrst 2,000 questions from the answer
type dataset from Li and Roth annotated with focus words. We removed questions with implicit and
multiple focuses.
Question Classiﬁcation.
We used the UIUIC
dataset which contains 5,952
factoid questions 4 to train a multi-class question
classiﬁer.
Table 2 summarizes the results of question focus
and category classiﬁcation.
4We excluded questions from TREC to ensure there is no
overlap with the data used for testing models trained on TREC
Table 2: Accuracy (%) of focus (FC) and question classi-
ﬁers (QC) using PTK.
TREC 11-12
Answer Sentence Selection
We used the train and test data from to enable direct comparison with previous
work on answer sentence selection.
The training
data is composed by questions drawn from TREC
8-12 while questions from TREC 13 are used for
The data provided for training comes as
two sets: a small set of 94 questions (TRAIN) that
were manually curated for errors5 and 1,229 questions from the entire TREC 8-12 that contain at least
one correct answer sentence (ALL). The latter set
represents a more noisy setting, since many answer
sentences are marked erroneously as correct as they
simply match a regular expression. Table 3 summarizes the data used for training and testing.
Table 4 compares our kernel-based structural
model with the previous state-of-the-art systems for
answer sentence selection. In particular, we compare with four most recent state of the art answer
sentence reranker models , which report their performance on
the same questions and candidate sets from TREC
13 as provided by .
Our simple shallow tree representation delivers state-of-the-art accuracy largely improving on previous work.
Finally, augmenting the structure with semantic linking yields additional improvement in MAP and MRR. This suggests the
utility of using supervised components, e.g., question focus and question category classiﬁers coupled
with NERs, to establish semantic mapping between
words in a q/a pair.
5In TREC correct answers are identiﬁed by regex matching
using the provided answer pattern ﬁles
Table 3: Summary of TREC data for answer extraction
used in .
candidates
Table 4: Answer sentence reranking on TREC 13.
Wang et al. 
Heilman & Smith 
Wang & Manning 
Yao et al. 
shallow tree 
+ semantic tagging
It is worth noting that our kernel-based classiﬁer
is conceptually simpler than approaches in the previous work, as it relies on the structural kernels, e.g.,
PTK, to automatically extract salient syntactic patterns relating questions and answers.
only includes the most basic feature vector (uni- and
bi-grams) and does not rely on external sources such
as WordNet.
Answer Extraction
Our experiments on answer extraction replicate the
setting of , which is the most recent
work on answer extraction reporting state-of-the-art
Table 5 reports the accuracy of our model in recovering correct answers from a set of candidate answer sentences for a given question. Here the focus is on the ability of an answer extraction system
to recuperate as many correct answers as possible
from each answer sentence candidate. The set of
extracted candidate answers can then be used to select a single best answer, which is the ﬁnal output
of the QA system for factoid questions. Recall (R)
encodes the percentage of correct answer sentences
for which the system correctly extracts an answer
(for TREC 13 there are a total of 284 correct answer
sentences), while Precision (P) reﬂects how many
answers extracted by the system are actually correct.
Clearly, having a high recall system, allows for correctly answering more questions. On the other hand,
a high precision system would attempt to answer less
questions (extracting no answers at all) but get them
We compare our results to a CRF model of augmented with WordNet features (without forced voting) 6. Unlike the CRF model which
obtains higher values of precision, our system acts
as a high recall system able to recover most of the
answers from the correct answer sentences. Having
higher recall is favorable to high precision in answer
extraction since producing more correct answers can
help in the ﬁnal voting scheme to come up with a
single best answer. To solve the low recall problem
of their CRF model, Yao et al. apply fairly
complex outlier resolution techniques to force answer predictions, thus aiming at increasing the number of extracted answers.
To further boost the number of answers produced
by our system we exclude negative examples (answer sentences not containing the correct answer)
from training, which slightly increases the number
of pairs with correctly recovered answers. Nevertheless, it has a substantial effect on the number of
questions that can be answered correctly (assuming
perfect single best answer selection). Clearly, our
system is able to recover a large number of answers
from the correct answer sentences, while low precision, i.e., extracting answer candidates from sentences that do not contain a correct answer, can be
overcome by further applying various best answer
selection strategies, which we explore in the next
Best Answer Selection
Since the ﬁnal step of the answer extraction module
is to select for each question a single best answer
from a set of extracted candidate answers, an answer
selection scheme is required.
We adopt a simple majority voting strategy, where
we aggregate the extracted answers produced by our
answer extraction model.
Answers sharing similar lemmas (excluding stop words) are grouped together. The prediction scores obtained by the an-
6We could not replicate the results obtained in with the forced voting strategy. Thus such result is not
included in Table 5.
Table 5: Results on answer extraction. P/R - precision
and recall; pairs - number of QA pairs with a correctly extracted answer, q - number of questions with at least one
correct answer extracted, F1 sets an upper bound on the
performance assuming the selected best answer among
extracted candidates is always correct. *-marks the setting where we exclude incorrect question answer pairs
from training.
Yao et al. 
Yao et al. 
Table 6: Results on ﬁnding the best answer with voting.
Yao et al. 
Yao et al. 
swer extraction classiﬁer are used as votes to decide
on the ﬁnal rank to select the best single answer.
Table 6 shows the results after the majority voting is applied to select a single best answer for each
candidate. A rather na¨ıve majority voting scheme
already produces satisfactory outcome demonstrating better results than the previous work. Our voting scheme is similar to the one used by , yet it is much simpler since we do not perform any additional hand tuning to account for the
weight of the “forced” votes or take any additional
steps to catch additional answers using outlier detection techniques applied in the previous work.
Discussion and Error Analysis
There are several sources of errors affecting the ﬁnal performance of our answer extraction system: (i)
chunking, (ii) named entity recognition and semantic linking, (iii) answer extraction, (iv) single best
answer selection.
Chunking. Our system uses text spans identiﬁed by
a chunker to extract answer candidates, which makes
it impossible to extract answers that lie outside the
chunk boundaries. Nevertheless, we found this to
be a minor concern since for 279 out of total 284
candidate sentences from TREC 13 the answers are
recoverable within the chunk spans.
Semantic linking. Our structural model relies heavily on the ability of NER to identify the relevant entities in the candidate sentence that can be further
linked to the focus word of the question.
our answer extraction model is working on all the
NP chunks, the semantic tags from NER serve as a
strong cue for the classiﬁer that a given chunk has
a high probability of containing an answer. Typical
off-the-shelf NER taggers have good precision and
low recall, s.t. many entities as potential answers are
missed. In this respect, a high recall entity linking
system, e.g., linking to wikipedia entities , is required to boost the quality of candidates considered for answer extraction. Finally,
improving the accuracy of question and focus classiﬁers would allow for having more accurate input
representations fed to the learning algorithm.
Answer Extraction. Our answer extraction model
acts as a high recall system, while it suffers from
low precision in extracting answers for many incorrect sentences. Improving the precision without sacriﬁcing the recall would ease the successive task of
best answer selection, since having less incorrect answer candidates would result in a better ﬁnal performance. Introducing additional constraints in the
form of semantic tags to allow for better selection of
answer candidates could also improve our system.
Best Answer Selection. We apply a na¨ıve majority
voting scheme to select a single best answer from
a set of extracted answer candidates. This step has
a dramatic impact on the ﬁnal performance of the
answer extraction system resulting in a large drop
of recall, i.e., from 82.0 to 70.8 before and after voting respectively. Hence, a more involved model, i.e.,
performing joint answer sentence re-ranking and answer extraction, is required to yield a better performance.
Related Work
Tree kernel methods have found many applications
for the task of answer reranking which are reported
in .
However, their methods lack the use of important
relational information between a question and a candidate answer, which is essential to learn accurate
relational patterns. In this respect, a solution based
on enumerating relational links was given in for
the textual entailment task but it is computationally
too expensive for the large dataset of QA. A few solutions to overcome computational issues were suggested in .
In contrast, this paper relies on structures directly
encoding the output of question and focus classiﬁers
to connect focus word and good candidate answer
keywords (represented by NEs) of the answer passage. This provides more effective relational information, which allows our model to signiﬁcantly improve on previous rerankers. Additionally, previous
work on kernel-based approaches does not target answer extraction.
One of the best models for answer sentence selection has been proposed in . They
use the paradigm of quasi-synchronous grammar to
model relations between a question and a candidate
answer with syntactic transformations.
and Smith, 2010) develop an improved Tree Edit
Distance (TED) model for learning tree transformations in a q/a pair. They search for a good sequence
of tree edit operations using complex and computationally expensive Tree Kernel-based heuristic.
 develop a probabilistic
model to learn tree-edit operations on dependency
parse trees. They cast the problem into the framework of structured output learning with latent variables. The model of has reported
an improvement over the Wang’s et al. system. It applies linear chain CRFs with features derived from TED and WordNet to automatically learn
associations between questions and candidate answers.
Different from previous approaches that use treeedit information derived from syntactic trees, our
kernel-based learning approach also use tree structures but with rather different learning methods, i.e.,
SVMs and structural kernels, to automatically extract salient syntactic patterns relating questions and
answers. In , we have shown
that such relational structures encoding input text
pairs can be directly used within the kernel learning
framework to build state-of-the-art models for predicting semantic textual similarity. Furthermore, semantically enriched relational structures, where automatic have been previously explored for answer
passage reranking in . This paper demonstrates that this
model also works for building a reranker on the sentence level, and extends the previous work by applying the idea of automatic feature engineering with
tree kernels to answer extraction.
Conclusions
Our paper demonstrates the effectiveness of handling the input structures representing QA pairs directly vs. using explicit feature vector representations, which typically require substantial feature engineering effort. Our approach relies on a kernelbased learning framework, where structural kernels,
e.g., tree kernels, are used to handle automatic feature engineering. It is enough to specify the desired
type of structures, e.g., shallow, constituency, dependency trees, representing question and its candidate answer sentences and let the kernel learning
framework learn to use discriminative tree fragments
for the target task.
An important feature of our approach is that it
can effectively combine together different types of
syntactic and semantic information, also generated
by additional automatic classiﬁers, e.g., focus and
question classiﬁers.
We augment the basic structures with additional relational and semantic information by introducing special tag markers into the
tree nodes. Using the structures directly in the kernel learning framework makes it easy to integrate
additional relational constraints and semantic information directly in the structures.
The comparison with previous work on a public
benchmark from TREC suggests that our approach
is very promising as we can improve the state of the
art in both answer selection and extraction by a large
margin (up to 22% of relative improvement in F1 for
answer extraction). Our approach makes it relatively
easy to integrate other sources of semantic information, among which the use of Linked Open Data can
be the most promising to enrich the structural representation of q/a pairs.
To achieve state-of-the-art results in answer sentence selection and answer extraction, it is sufﬁcient
to provide our model with a suitable tree structure
encoding relevant syntactic information, e.g., using
shallow, constituency or dependency formalisms.
Moreover, additional semantic and relational information can be easily plugged in by marking tree
nodes with special tags. We believe this approach
greatly eases the task of tedious feature engineering
that will ﬁnd its applications well beyond QA tasks.
Acknowledgements
This research is partially supported by the EU’s
7th Framework Program (#288024
LIMOSINE project) and an Open Collaborative Research (OCR) award from IBM Research. The ﬁrst
author is supported by the Google Europe Fellowship 2013 award in Machine Learning.