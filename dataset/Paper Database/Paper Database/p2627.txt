The Annals of Applied Statistics
2007, Vol. 1, No. 2, 302–332
DOI: 10.1214/07-AOAS131
© Institute of Mathematical Statistics, 2007
PATHWISE COORDINATE OPTIMIZATION
BY JEROME FRIEDMAN,1 TREVOR HASTIE,2 HOLGER HÖFLING3
AND ROBERT TIBSHIRANI4
Stanford University
We consider “one-at-a-time” coordinate-wise descent algorithms for a
class of convex optimization problems. An algorithm of this kind has been
proposed for the L1-penalized regression (lasso) in the literature, but it seems
to have been largely ignored. Indeed, it seems that coordinate-wise algorithms
are not often used in convex optimization. We show that this algorithm is
very competitive with the well-known LARS (or homotopy) procedure in
large lasso problems, and that it can be applied to related methods such as the
garotte and elastic net. It turns out that coordinate-wise descent does not work
in the “fused lasso,” however, so we derive a generalized algorithm that yields
the solution in much less time that a standard convex optimizer. Finally, we
generalize the procedure to the two-dimensional fused lasso, and demonstrate
its performance on some image smoothing problems.
1. Introduction.
In this paper we consider statistical models that lead to convex optimization problems with inequality constraints. Typically, the optimization
for these problems is carried out using a standard quadratic programming algorithm. The purpose of this paper is to explore “one-at-a-time” coordinate-wise
descent algorithms for these problems. The equivalent of a coordinate descent algorithm has been proposed for the L1-penalized regression (lasso) in the literature,
but it is not commonly used. Moreover, coordinate-wise algorithms seem too simple, and they are not often used in convex optimization, perhaps because they only
work in specialized problems. We ourselves never appreciated the value of coordinate descent methods for convex statistical problems before working on this paper.
In this paper we show that coordinate descent is very competitive with the wellknown LARS (or homotopy) procedure in large lasso problems, can deliver a
path of solutions efﬁciently, and can be applied to many other convex statistical
problems such as the garotte and elastic net. We then go on to explore a nonseparable problem in which coordinate-wise descent does not work—the “fused
lasso.” We derive a generalized algorithm that yields the solution in much less
time that a standard convex optimizer. Finally, we generalize the procedure to
Received May 2007; revised August 2007.
1Supported in part by NSF Grant DMS-97-64431.
2Supported in part by NSF Grant DMS-05-50676 and NIH Grant 2R01 CA 72028-07.
3Supported by an Albion Walter Hewlett Stanford Graduate Fellowship.
4Supported in part by NSF Grant DMS-99-71405 and NIH Contract N01-HV-28183.
Key words and phrases. Coordinate descent, lasso, convex optimization.
PATHWISE COORDINATE OPTIMIZATION
the two-dimensional fused lasso, and demonstrate its performance on some image smoothing problems.
A key point here: coordinate descent works so well in the class of problems that
we consider because each coordinate minimization can be done quickly, and the
relevant equations can be updated as we cycle through the variables. Furthermore,
often the minimizers for many of the parameters don’t change as we cycle through
the variables, and hence, the iterations are very fast.
Consider, for example, the lasso for regularized regression [Tibshirani ].
We have predictors xij,j = 1,2,...,p, and outcome values yi for the ith
observation, for i = 1,2,...,n. Assume that the xij are standardized so that
i xij/n = 0, 
ij = 1. The lasso solves
subject to
The bound s is a user-speciﬁed parameter, often chosen by a model selection procedure such as cross-validation. Equivalently, the solution to (1) also minimizes
the “Lagrange” version of the problem
where γ ≥0. There is a one-to-one correspondence between γ and the bound s—
if ˆβ(γ ) minimizes (2), then it also solves (1) with s = p
j=1 | ˆβj(γ )|. In the signal
processing literature, the lasso and L1 penalization is known as “basis pursuit”
[Chen et al. ].
There are efﬁcient algorithms for solving this problem for all values of s or γ ;
see Efron et al. , and the homotopy algorithm of [Osborne et al. ].
There is another, simpler algorithm for solving this problem for a ﬁxed value γ . It
relies on solving a sequence of single-parameter problems, which are assumed to
be simple to solve.
With a single predictor, the lasso solution is very simple, and is a softthresholded version [Donoho and Johnstone ] of the least squares estimate ˆβ:
ˆβlasso(γ ) = S( ˆβ,γ ) ≡sign( ˆβ)(| ˆβ| −γ )+
if ˆβ > 0 and γ < | ˆβ|,
if ˆβ < 0 and γ < | ˆβ|,
if γ ≥| ˆβ|.
J. FRIEDMAN, T. HASTIE, H. HÖFLING AND R. TIBSHIRANI
The lasso problem with a single standardized predictor leads to soft thresholding. In each
case the solid vertical line indicates the lasso estimate, and the broken line the least-squares estimate.
This simple expression arises because the convex-optimization problem (2) reduces to a few special cases when there is a single predictor. Minimizing the criterion (2) with a single standardized x and β simpliﬁes to the equivalent problem
2(β −ˆβ)2 + γ |β|,
where ˆβ = 
i xiyi is the simple least-squares coefﬁcient. If β > 0, we can differentiate (5) to get
dβ = β −ˆβ + γ = 0.
This leads to the solution β = ˆβ −γ (left panel of Figure 1) as long as ˆβ > 0 and
γ < ˆβ, otherwise 0 is the minimizing solution (right panel). Similarly, if ˆβ < 0, if
γ < −ˆβ, then the solution is β = ˆβ + γ , else 0.
With multiple predictors that are uncorrelated, it is easily seen that once again
the lasso solutions are soft-thresholded versions of the individual least squares
estimates. This is not the case for general (correlated) predictors. Consider instead
a simple iterative algorithm that applies soft-thresholding with a “partial residual”
as a response variable. We write (2) as
f ( ˜β) = 1
xik ˜βk −xijβj
| ˜βj| + γ |βj|,
where all the values of βk for k ̸= j are held ﬁxed at values ˜βk(γ ). Minimizing
w.r.t. βj, we get
˜βj(γ ) ←S
yi −˜y(j)
PATHWISE COORDINATE OPTIMIZATION
Diabetes data: iterates for each coefﬁcient from algorithm (9). The algorithm converges to
the lasso estimates shown on the right side of the plot.
where ˜y(j)
k̸=j xik ˜βk(γ ). This is simply the univariate regression coefﬁcient
of the partial residual yi −˜y(j)
on the (unit L2-norm) jth variable; hence, this has
the same form as the univariate version (3) above. The update (7) is repeated for
j = 1,2,...,p,1,2,... until convergence.
An equivalent form of this update is
˜βj(γ ) ←S
xij(yi −˜yi),γ
j = 1,2,...p,1,2,....
Starting with any values for βj, for example, the univariate regression coefﬁcients, it can be shown that the ˜βj(γ ) values converge to ˆβlasso.
Figure 2 shows an example, using the diabetes data from Efron et al. .
This data has 442 observations and 10 predictors. We applied algorithm (9) with
γ = 88. It produces the iterates shown in the ﬁgure and converged after 14 steps to
the lasso solution ˆβlasso(88).
This approach provides a simple but fast algorithm for solving the lasso, especially useful for large p. It was proposed in the “shooting” procedure of Fu 
and re-discovered by [Daubechies, Defrise and De Mol ]. Application of the
same idea to the elastic net procedure [Zhou and Hastie ] was proposed by
[Van der Kooij ].
2. Pathwise coordinatewise optimization algorithms.
The procedure described in Section 1 is a successive coordinate-wise descent algorithm for minimizing the function f (β) = 1
j xijβj)2 + λ p
j=1 |βj|. The idea is to
apply a coordinate-wise descent procedure for each value of the regularization parameter, varying the regularization parameter along a path. Each solution is used
as a warm start for the next problem.
J. FRIEDMAN, T. HASTIE, H. HÖFLING AND R. TIBSHIRANI
This approach is attractive whenever the single-parameter problem is easy to
solve. Some of these algorithms have already been proposed in the literature, and
we give appropriate references. Here are some other examples:
• The nonnegative garotte. This method, a precursor to the lasso, was proposed
by Breiman and solves
subject to
Here ˆβj are the usual least squares estimates (we assume that p ≤n). Using
partial residuals as in (7), one can show that the the coordinate-wise update has
˜βj ˆβj −λ
where ˜βj = n
i=1 xij(yi −˜y(j)
), and ˜y(j)
k̸=j xikck ˆβk.
• Least absolute deviation regression and LAD-lasso. Here the problem is
We can write this as
(yi −˜y(j)
holding all but βj ﬁxed. This quantity is minimized over βj by a weighted median of the values (yi −˜y(j)
)/xij. Hence, coordinate-wise minimization is just
a repeated computation of medians. This approach is studied and reﬁned by Li
and Arce .
The same approach may be used in the LAD-lasso [Wang et al. ]. Here
we add an L1 penalty to the least absolute deviation loss:
This can be converted into an LAD problem (12) by augmenting the dataset
with p artiﬁcial observations. In detail, let X denote n × (p + 1) model matrix (with the column of 1s for the intercept). The extra p observations have
response yi equal to zero, and predictor matrix equal to λ · (0:Ip). Then the
above coordinate-wise algorithm for the LAD problem can be applied.
PATHWISE COORDINATE OPTIMIZATION
• The elastic net. This method [due to Zhou and Hastie ] adds a second
constraint p
j ≤s2 to the lasso (1). In Lagrange form, we solve
The coordinate-wise update has the form
i=1 xij(yi −˜y(j)
Thus, we compute the simple least squares coefﬁcient on the partial residual, apply soft-thresholding to take care of the lasso penalty, and then apply a proportional shrinkage for the ridge penalty. This algorithm was suggested by Van der
Kooij .
• Grouped lasso [Yuan and Lin ]. This is like the lasso, except variables
occur in groups (such as dummy variables for multi-level factors). Suppose Xj
is an N × pj orthonormal matrix that represents the jth group of pj variables,
j = 1,...,m, and βj the corresponding coefﬁcient vector. The grouped lasso
where λj = λ√pj. Other choices of λj ≥0 are possible; this one penalizes large
groups more heavily. Notice that the penalty is a weighted sum of L2 norms (not
squared); this has the effect of selecting the variables in groups. Yuan and Lin
 argue for a coordinate-descent algorithm for solving this problem, and
show through the Karush–Kuhn–Tucker conditions that the coordinate updates
are given by
˜βj ←(∥Sj∥2 −λj)+
where Sj = XT
j (y −˜y(j)), and here ˜y(j) = 
k̸=j Xk ˜βk.
• The “Berhu” penalty. This method is due to Owen , and is another compromise between an L1 and L2 penalty. In Lagrange form, we solve
|βj| · I(|βj| < δ) +
· I(|βj| ≥δ)
J. FRIEDMAN, T. HASTIE, H. HÖFLING AND R. TIBSHIRANI
This penalty is the reverse of a “Huber” function—initially absolute value, but
then blending into quadratic beyond δ from zero. The coordinate-wise update
has the form
yi −˜y(j) ,λ
if |βj| < δ,
yi −˜y(j) /(1 + λ/δ),
if |βj| ≥δ.
This is a lasso-style soft-thresholding for values less than δ, and ridge-style beyond δ.
Tseng [see also Tseng ] has established that coordinate descent
works in problems like the above. He considers minimizing functions of the form
f (β1,...,βp) = g(β1,...,βp) +
where g(·) is differentiable and convex, and the hj(·) are convex. Here each βj
can be a vector, but the different vectors cannot have any overlapping members.
He shows that coordinate descent converges to the minimizer of f . The key to
this result is the separability of the penalty function p
j=1 hj(βj), a sum of functions of each individual parameter. This result implies that the coordinate-wise
algorithms for the lasso, the grouped lasso and elastic net, etc. converge to their
optimal solutions.
Next we examine coordinate-wise descent in a more complicated problem, the
“fused lasso” [Tibshirani, Saunders, Rosset, Zhu and Knight ], which we
represent in Lagrange form:
|βj −βj−1|.
The ﬁrst penalty encourages sparsity in the coefﬁcients; the second penalty encourages sparsity in their differences; that is, ﬂatness of the coefﬁcient proﬁles βj
as a function of the index set j. Note that f (β) is strictly convex, and hence has a
unique minimum.
The left panel of Figure 3 shows an example of an application of the fused lasso,
in a special case where the feature matrix {xij} is the identity matrix—this is called
fused lasso signal approximation, discussed in the next section. The data represents
Comparative Genomic Hybridization (CGH) measurements from two Glioblastoma Multiforme (GBM) tumors. The measurements are “copy numbers”—logratios of the number of copies of each gene in the tumor versus normal tissue. The
data are displayed in the left panel, and the red line in the right panel represents the
PATHWISE COORDINATE OPTIMIZATION
Fused lasso applied to some Glioblastoma Multiforme data. The data are shown in the left
panel, and the jagged line in the right panel represents the inferred copy number ˆβ from the fused
lasso. The horizontal line is for y = 0.
smoothed signal ˆβ from the fused lasso. The regions of the nonzero estimated signal can be used to call “gains” and “losses” of genes. Tibshirani and Wang 
report excellent results in the application of the fused lasso, ﬁnding that the method
outperforms other popular techniques for this problem.
Somewhat surprisingly, coordinate-wise descent does not work for the fused
lasso. Proposition 2.7.1 of Bertsekas , for example, shows that every limit
point of successive coordinate-wise minimization of a continuously differentiable
function is a stationary point for the overall minimization, provided that the minimum is uniquely obtained along each coordinate. However, f (β) is not continuously differentiable, which means that coordinate-wise descent can get stuck.
Looking at Tseng’s result, the penalty function for the fused lasso is not separable,
and hence, Tseng’s theorem cannot be applied in that case.
Figure 4 illustrates the difﬁculty. We created a fused lasso problem with 100
parameters, with the solutions for two of the parameters, β63 and β64, being equal
to about −1. The top panels shows slices of the function f (β) varying β63 and β64,
with the other parameters set to the global minimizers. We see that the coordinatewise descent algorithm has got stuck in a corner of the response surface, and is
stationary under single-coordinate moves. In order to advance to the minimum, we
have to move both β63 and β64 together.
Despite this, it turns out that the coordinate-wise descent procedure can be modiﬁed to work for the fused lasso, yielding an algorithm that is much faster than a
general quadratic-program solver for this problem.
3. The fused lasso signal approximator.
Here we consider a variant of the
fused lasso (22) for approximating one- and higher-dimensional signals, which we
call the fused-lasso signal approximator (FLSA). For one-dimensional signals we
J. FRIEDMAN, T. HASTIE, H. HÖFLING AND R. TIBSHIRANI
Failure of coordinate-wise descent in a fused lasso problem with 100 parameters. The
optimal values for two of the parameters, β63 and β64, are both −1.05, as shown by the dot in the
right panel. The left and middle panels shows slices of the objective function f (β) as a function of
β63 and β64, with the other parameters set to the global minimizers. The coordinate-wise minimizer
over both β63 and β64 (separately) is −0.69, rather than −1.05. The right panel shows contours of
the two-dimensional surface. The coordinate-descent algorithm is stuck at (−0.69, −0.69). Despite
being strictly convex, the surface has corners, in which the coordinate-wise procedure can get stuck.
In order to travel to the minimum we have to move both β63 and β64 together.
β f (β) = 1
(yi −βi)2 + λ1
|βi −βi−1|.
The measurements yi are made along a one-dimensional index i, and there is one
parameter per observation. Later we consider images as well. In the special case
of λ1 = 0, the fused-lasso signal approximator is equivalent to a discrete version
of the “total variation denoising” procedure [Rudin et al. ] used in signal
processing. We make this connection clear in Section 6. Thus, the algorithm that
we present here also provides a fast implementation for total variation denoising.
Figure 5 illustrates an example of FLSA with 1000 simulated data points, and
the ﬁt is shown for s1 = 269.2,s2 = 10.9.
We now describe a modiﬁed coordinate-wise algorithm for the diagonal fused
lasso (FLSA) using the Lagrange form (23). The algorithm can also be extended to
the general fused lasso problem; details are given in the Appendix. However, it is
not guaranteed to give the exact solution for that problem, as we later make clear.
Our algorithm, for a ﬁxed value λ1, delivers a sequence of solutions corresponding to an increasing sequence of values for λ2. First we make an observation that
makes it easy to obtain the solution over a grid of λ1 and λ2 values:
PROPOSITION 1.
The solutions to (23) for all (λ′
1 > λ1,λ2) can be obtained
by soft-thresholding the solution for (λ1,λ2).
This is proven in the Appendix for the FLSA, two-dimensional penalty fused
lasso and even more general penalty functionals. Thus, our overall strategy for
PATHWISE COORDINATE OPTIMIZATION
Fused lasso solution in a constructed example.
obtaining the solution over a grid is to solve the problem for λ1 = 0 over a grid
of values of λ2, and then use this result to obtain the solution for all values of λ1.
However, for a single (especially large) value of λ1, we demonstrate that it is faster
to obtain the solution directly for that value of λ1 (Table 2). Hence, we present our
algorithm for ﬁxed but arbitrary values of λ1.
Two keys for the algorithm are assumptions (A1) and (A2) below, stating that
for ﬁxed λ1, small increments in the value of λ2 can only cause pairs of parameters
to fuse and they do not become unfused for the larger of λ2. This allows us to
efﬁciently solve the problem for a path of λ2 values, keeping λ1 ﬁxed.
The algorithm has three nested cycles:
Descent cycle: Here we run coordinate-wise descent for each parameter βj, holding all the others ﬁxed.
Fusion cycle: Here we consider the fusion of a neighboring pair of parameters,
followed by coordinate-wise descent.
Smoothing cycle: Here we increase the penalty λ2 a small amount, and rerun the
two previous cycles.
We now describe each in more detail.
Descent cycle.
Consider the derivative of (23), holding all βk = ˜βk,k ̸= i ﬁxed
at their current values:
= −(yi −βi) + λ1 · sign(βi)
−λ2 · sign( ˜βi+1 −βi) + λ2 · sign(βi −˜βi−1),
assuming that βi /∈{0, ˜βi−1, ˜βi+1}.
The algorithm for coordinate-wise minimization of f (β) works as follows. The
expression in (24) is piecewise linear in βi with breaks at 0, ˜βi−1 and ˜βi+1. Suppose, for example, at a given iteration we have 0 ≤˜βi−1 ≤˜βi+1. We check for
J. FRIEDMAN, T. HASTIE, H. HÖFLING AND R. TIBSHIRANI
Example of the one-dimensional search in the coordinate descent cycle for a FLSA problem with 5 parameters. Shown is the gradient for β3, with the other 4 parameters set at the global
minimizing values. There are discontinuities at ˜β2, ˜β4 and zero. We look for a zero-crossing in each
of the intervals (−∞, ˜β4),( ˜β4,0),(0, ˜β2),( ˜β2,∞), and if none is found, take the minimum of f (β)
over the set of discontinuities. In this case, the minimum is at a discontinuity, with ˜β3 = ˜β4.
a zero of (24) in each of the four intervals (−∞,0],(0, ˜βi−1],( ˜βi−1, ˜βi+1] and
( ˜βi+1,∞). Since the function is piecewise linear, there is an explicit solution,
when one exists. If no solution is found, we examine the three active-constraint
values for βi: 0, ˜βi−1 and ˜βi+1, and ﬁnd the one giving the smallest value of the
objective function f (β). Figure 6 illustrates the procedure on a simulated example.
Other orderings among 0, ˜βi−1 and ˜βi+1 are handled similarly, and at the endpoints i = 1 and i = p, there are only three intervals to check, rather than four.
Fusion cycle.
The descent cycle moves parameters one at a time. Inspection
of Figure 4 shows that this approach can get stuck. One way to get around this is
to consider a potential fusion of parameters, when a move of a single βi fails to
improve the loss criterion. This amounts to enforcing |βi −βi−1| = 0 by setting
βi = βi−1 = γ . With this constraint, we try a descent move in γ . Equation (24)
now becomes
= −(yi−1 −γ ) −(yi −γ )
+ 2λ1 · sign(γ ) −λ2 · sign( ˜βi+1 −γ )
+ λ2 · sign(γ −˜βi−2).
If the optimal value for γ decreases the criterion, we accept the move setting βi =
βi−1 = γ .
Notice that the fusion step is equivalent to temporarily collapsing the problem
to one with p −1 parameters:
• we replace the pair yi−1 and yi with the average response ¯y = (yi−1 +yi)/2 and
an observation weight of 2;
PATHWISE COORDINATE OPTIMIZATION
• the pair of parameters βi−1 and βi are replaced by a single γ , with a penalty
weight of 2 for the ﬁrst penalty.
At the end of the entire process of descent and fusion cycles for a given λ2, we
identify adjacent nonzero solution values that are equal and collapse the data accordingly, maintaining a weight vector to assign weights to the observations averages and the contributions to the ﬁrst penalty.
Smoothing cycle.
Although the fusion step often leads to a decrease in f ( ˜β),
it is possible to construct examples where, for a particular value of λ2, no fusion
of two neighbors causes a decrease, but a fusion of three or more can. Our ﬁnal
strategy is to solve a series of fused lasso problems sequentially, ﬁxing λ1, but
varying λ2 through a range of values increasing in small increments δ from 0.
The smoothing cycle is then as follows:
1. Start with λ2 = 0, hence, with the lasso solution with penalty parameter λ1.
2. Increment λ2 ←λ2 + δ, and run the descent and fusion cycles repeatedly until
no further changes occur. After convergence of the process for a given value λ2,
identify neighboring solution values that are equal and nonzero and collapse the
problem as described above, updating the weights.
3. Repeat step 2 until a target value of λ2 is reached (or a target bound s2).
Our strategy relies on the following assumptions:
(A1) If the increments δ are sufﬁciently small, fusions will occur between no more
than two neighboring points at a time.
(A2) Two parameters that are fused in the solution for (λ1,λ2) will be fused for
all (λ1,λ′
By collapsing the data after each solution, we can achieve long fusions by a
sequence of pairwise fusions. Note that each of the fused parameters can represent
more than one parameter in the original problem. For example, if βj has a weight
of 3, and βi+1 a weight of 2, then the merged parameter has a weight of 5, and
represents 5 neighboring parameters in the original problem.
After m fusions, the problem has the form
wi(yi −βi)2 + λ1
|βi −βi−1|.
Initially, m = 0, wi = 1, and C0 = 0. If the (m+1)st fusion is between βi−1 and βi,
then the following updates occur:
• ¯y ←(wi−1yi−1 + wiyi)/(wi−1 + wi).
• w+ ←wi−1 + wi.
• Cm+1 = Cm + 1
2[wi−1 · (yi−1 −¯y)2 + wi · (yi −¯y)2].
• yi−1 ←¯y, wi−1 ←w+.
J. FRIEDMAN, T. HASTIE, H. HÖFLING AND R. TIBSHIRANI
Small example of the fused lasso. λ1 is ﬁxed at 0.01; as λ2 increases, the number of fused
parameters increases.
• Discard observation i, and reduce all indices greater than i by 1.
Note that we don’t actually need to carry out the update for Cm, because no parameters are involved.
Figure 7 shows an example with just 9 data points. We have ﬁxed λ1 = 0.01
and show the solutions for four values of λ2. As λ2 increases, the number of fused
parameters increases.
Assumption (A1) requires that the data have some randomness (i.e., no preexisting ﬂat plateaus exist). Assumption (A2) holds in general. We prove that both
assumptions hold for the FLSA procedure in the next section.
Numerical experiments show that (A2) does not always hold for the general
fused lasso. Hence, the extension of this algorithm to the general fused lasso (detailed in the Appendix) is not guaranteed to yield the exact solution. Note that each
descent and fusion cycle can only decrease the convex objective and, hence, must
converge. We terminate this pair of cycles when the change in parameter estimates
is less than some threshold. The smoothing cycle is done over a discrete grid of λ2
4. Optimality conditions.
In this section we derive the optimality conditions
for the FLSA problem, and use them to show that our algorithms’ assumptions
(A1) and (A2) are satisﬁed.
PATHWISE COORDINATE OPTIMIZATION
We consider the Lagrangian form (23) for the fused lasso. The standard Karush–
Kuhn–Tucker conditions for this problem are fairly complicated, since we need
to express each parameter in terms of its positive and negative parts in order to
make the penalty differentiable. A more convenient formulation is through the subgradient approach [see, e.g., Bertsekas , Proposition B.24]. The equations
for the subgradient have the form
−(y1 −β1) + λ1s1 −λ2t2 = 0,
−(yj −βj) + λ1sj + λ2(tj −tj+1) = 0,
j = 2,...,n,
with sj = sign(βj) if βj ̸= 0 and sj ∈[−1,1] if βj = 0. Similarly, tj = sign(βj −
βj−1) if βj ̸= βj−1 and tj ∈[−1,1] if βj = βj−1. These n equations are necessary and sufﬁcient for the solution. We restate assumptions (A1) and (A2) more
precisely, and then prove they hold.
PROPOSITION 2.
For the fused-lasso signal approximation algorithm detailed
in Section 3:
(A1′) If the sequence yi are in general position—speciﬁcally, no two consecutive
yj values are equal—and the increments δ are sufﬁciently small, fusions will
occur between no more than two neighboring points at a time.
(A2′) Two parameters that are fused in the solution for (λ1,λ2) will be fused for
all (λ1,λ′
We ﬁrst prove (A2′). Suppose we have a stretch of nonzero solutions
ˆβj−k, ˆβj−k+1,..., ˆβj that are equal for some value λ2, and ˆβj−k−1 and ˆβj+1 are
not equal to this common value. Then tj−k and tj+1 each take a value in {−1,1};
we denote these boundary values by Tj−k and Tj+1. Although the parameters
tj−k+1,...,tj can vary in [−1,1] as λ2 changes (while the fused group remains
intact), the values depend on only the (j −k + 1)st through jth equations in the
system (27), because the boundary values are ﬁxed. Taking pairwise differences,
and using the fact that ˆβj−k = ˆβj−k+1 = ··· = ˆβj, this subgroup of equations simpliﬁes to
yj−k+1 −yj+k
yj−k+2 −yj−k+1
yj−1 −yj−2
J. FRIEDMAN, T. HASTIE, H. HÖFLING AND R. TIBSHIRANI
Write this system symbolically as Mt = 1
λ2 y + T , and let C = M−1. The explicit form for C given in Schlegel gives Cℓ1 = (n −ℓ+ 1)/(n + 1),Cℓn =
ℓ/(n + 1). It is easy to check for all three possibilities for T that CT ∈[−1,1] elementwise. We know that t = ( 1
λ2 Cy + CT ) ∈[−1,1] elementwise as well, since
t is a solution to (23) at λ2. For λ′
2 > λ2, the elements of the ﬁrst terms shrink, and
hence the values t(λ′
2) remain in [−1,1]. This implies that the fused set remains
fused as we increase λ2. These equations describe the path t(λ2) for λ2 increasing,
and only change when one of the boundary points (fused sets) is fused with the
current set, and the argument is repeated. This proves (A2′).
We now address (A1′). Suppose the data are in general position (e.g., have a
random noise component), and we have the lasso-solution ˆβj for λ1. Because of
the randomness, no neighboring nonzero parameters ˆβj will be exactly the same.
This means for each nonzero value ˆβj, we can write an equation of the form (27)
where we know exactly the values for sj, tj and tj+1 (each will be one of the values
{−1,+1}). This means that we can calculate exactly the path of each such βj as
we increase λ2 from zero, until an event occurs that changes the sj, tj. By looking
at all pairs, we can identify the time of the ﬁrst fusion of such pairs. The data are
then fused together and reduced, and the problem is repeated. Fusions occur oneat-a-time in this fashion, at a distinct sequence of values for λ2. Hence, for δ small
enough in our smoothing step, we can ensure that we encounter these fusions one
at a time.
5. Comparison of run times.
In this section we compare the run times of the
coordinate-wise algorithm to standard algorithms, for both the lasso and diagonal
fused lasso (FLSA) problems. All timings were carried out on a Intel Xeon 2.80GH
processor.
5.1. Lasso speed trials.
We generated Gaussian data with n observations and
p predictors, with each pair of predictors Xj,Xj′ having the same population correlation ρ. We tried a number of combinations of n and p, with ρ varying from
zero to 0.95. The outcome values were generated by
βjXj + k · Z,
where βj = (−1)j exp(−2(j −1)/20), Z ∼N(0,1) and k is chosen so that the
signal-to-noise ratio is 3.0. The coefﬁcients are constructed to have alternating
signs and to be exponentially decreasing.
Table 1 shows the average CPU timings for the coordinatewise algorithm, two
versions of the LARS procedure and lasso2, an implementation of the homotopy
algorithm of Osborne et al. . All algorithms are implemented as R language
functions. The coordinate-wise algorithm does all of its numerical work in Fortran,
PATHWISE COORDINATE OPTIMIZATION
Run times (CPU seconds) for lasso problems of various sizes n, p and different correlation between
the features. Methods are the coordinate-wise optimization (Fortran), LARS (R and Fortran
versions) and lasso2 (C language)—the homotopy procedure of Osborne et al. 
Population correlation between features
n = 100,p = 1000
coord-Fort
n = 100,p = 5000
coord-Fort
LARS-Fort would not run
lasso2 would not run
n = 100,p = 20,000
coord-Fort
LARS-Fort would not run
lasso2 would not run
n = 1000,p = 100
coord-Fort
n = 5000,p = 100
coord-Fort
while lasso2 does its numerical work in C. LARS-R is the “production version” of
LARS (written by Efron and Hastie), doing much of its work in R, calling Fortran
routines for some matrix operations. LARS-Fort (due to Ji Zhu) is a version of
LARS that does all of its numerical work in Fortran. Comparisons between differ-
J. FRIEDMAN, T. HASTIE, H. HÖFLING AND R. TIBSHIRANI
CPU times for coordinate descent, for the same problem as in Table 1, for different values
of n and p. In each case the times are averaged over ﬁve runs and averaged over the set of values of
the other parameter (n or p).
ent programs are always tricky: in particular, the LARS procedure computes the
entire path of solutions, while the coordinate-wise procedure and lasso2 solve the
problems for a set of pre-deﬁned points along the solution path. In the orthogonal
case, LARS takes min(n,p) steps: hence, to make things roughly comparable, we
called the latter two algorithms to solve a total of min(n,p) problems along the
Not surprisingly, the coordinate-wise algorithm is fastest when the correlations
are small, and gets slower when they are large. It seems to be very competitive with
the other two algorithms in general, and offers some potential speedup, especially
when n > p.
Figure 8 shows the CPU times for coordinate descent, for the same problem as
in Table 1. We varied n and p, and averaged the times over ﬁve runs. We see that
the times are roughly linear in n and in p.
A key to the success of the coordinate-wise algorithm for lasso is the fact that,
for squared error loss, the ingredients needed for each coordinate step can be easily
updated as the algorithm proceeds. We can write the second term in (9) as
xij(yi −˜yi) = ⟨xj,y⟩−
⟨xj,xk⟩˜βk,
where ⟨xj,y⟩= n
i=1 xijyi, and so on. Hence, we need to compute inner products
of each feature with y initially, and then each time a feature xk enters the model,
we need to compute its inner product with all the rest. But importantly, O(n) calculations do not have to be made at every step. This is the case for all penalized
procedures with squared error loss.
Friedlander and Saunders do a thorough comparison of the LARS (homotopy) procedure to a number of interior point QP procedures for the lasso problem, and ﬁnd that LARS is generally much faster. Our ﬁnding that coordinate de-
PATHWISE COORDINATE OPTIMIZATION
scent is very competitive with LARS therefore suggests that also will outperform
interior point methods.
Finally, note that there is another approach to solving the FLSA problem for
λ1 = 0. We can transform to parameters θj = βj −βj−1, and we get a new lasso
problem in these new parameters. One can use coordinate descent to solve this
lasso problem, and then Proposition 1 gives the FLSA solution for other values
of λ1. However, this new lasso problem has a dense data matrix, and hence, the
coordinate descent procedure is many times slower than the procedure described
in this section. The procedure developed here exploits the near-diagonal structure
of the problem in the original parametrization.
5.2. Fused lasso speed trials.
For the example of Figure 5, we compared the
pathwise coordinate algorithm to the two-phase active set algorithm sqopt of
Gill, Murray and Saunders . Both algorithms are implemented as R functions, but do all but the setup and wrapup computations in Fortran. Table 2 shows
Run times (CPU seconds) for fused lasso (FLSA) problems of various sizes n for different values of
the regularization parameters λ1,λ2. The methods compared are the pathwise coordinate
optimization, and “standard”-two-phase active set algorithm sqopt of
Gill, Murray and Saunders . The number of active constraints in
the solution is shown in each case
J. FRIEDMAN, T. HASTIE, H. HÖFLING AND R. TIBSHIRANI
Run times (CPU seconds) for pathwise coordinate optimization applied
to fused lasso (FLSA) problems with a large number of parameters n
averaged over different values of the regularization parameters λ1,λ2
Average CPU sec
the timings for the two algorithms for a range of values of λ1 and λ2. The resulting number of active constraints (i.e., βj = 0 or βj −βj−1 = 0) is also shown.
In the second part of the table, we increased the sample size to 5000. We see that
the coordinate algorithm offers substantial speedups, by factors of 50 up to 300 or
In these tables, each entry for the pathwise coordinate procedure is the computation time for the entire path of solutions leading to the given values λ1,λ2.
In practice, one could obtain all of the solutions for a given λ1 from a single run
of the algorithm, and hence, the numbers in the table are very conservative. But
we reported the results in this way to make a fair comparison with the standard
procedure since it can also exploit warm starts.
In Table 3 we show the run times for pathwise coordinate optimization for larger
values of n. As in the previous table, these are the averages of run times for the
entire path of solutions for a given λ1, and hence, are conservative. We were unable
to run the standard algorithm for these cases.
6. The two-dimensional fused lasso.
Suppose we have a total of n2 cells,
laid out in a n × n grid (the square grid is not essential). We can generalize the
diagonal fused lasso (FLSA) to two-dimensions as follows:
(yii′ −βii′)2
subject to
|βii′| ≤s1,
|βi,i′ −βi,i′−1| ≤s2,
|βi,i′ −βi−1,i′| ≤s3.
The penalties encourage the parameter map βii′ to be both sparse and spatially
PATHWISE COORDINATE OPTIMIZATION
The fused lasso is related to signal processing methods such as “total variation denoising” [Rudin et al. ], which uses a continuous smoothness penalty
analogous to the second penalty in the fused lasso. The TV criterion is written in
subject to
∥u −y∥2 = σ 2,
where y is the data, u is the approximation with allowable error σ 2,
is a bounded
convex region in Rd, | · | denotes Euclidean norm in Rd and ∥· ∥denotes the norm
). Thus, in d = 1 dimension, this is a continuous analogue of the fused
lasso, but without the (ﬁrst) L1 penalty on the coefﬁcients. In d = 2 dimensions,
the TV approach is different: the discretized version uses the Euclidean norm of
the ﬁrst differences in u, rather than the sum of the absolute values of the ﬁrst
differences.
This problem (32) can be solved by a general purpose quadratic-programming
algorithm; we give details in the Appendix. However, for a p × p grid, there are
7p2 variables and 3p2 + 3 constraints, in addition to nonnegativity constraints on
the variables. For p = 256, for example, this is 458,752 variables and 196,611
constraints, so that ﬁnding the exact solution is impractical.
Hence, we focus on the pathwise coordinate algorithm. The algorithm has the
same form as in the one-dimensional case, except that rather than checking the
three active constraint values 0, βj−1 and βj+1, we check 0 and the four values to
the right, left, above and below (its four-neighborhood). The number of constraint
values reduces to 4 at the edges and 3 at the corners. The algorithm starts with individual pixels as the groups, and the four-neighborhood pixels are its “distance-1
neighbors.” In each fusion cycle we try to fuse a group with its distance-1 neighbors. If the fusion is accepted, then the distance-1 neighbors of the fused group
are the union of the distance-1 neighbors of the two groups being joined (with the
groups themselves removed). Now one pixel might be the distance-1 neighbor to
each of the two groups being fused, and some careful bookkeeping is required to
keep track of this through appropriate weights. Full details are given in the Appendix.
We do not provide a proof of the correctness of this procedure. However, in
our (limited) experiments we have found that it gives the exact solution to the
two-dimensional fused lasso. We guess that a proof along the lines of that in the
one-dimensional case can be constructed, although some additional assumptions
may be required.
As in the one-dimensional FLSA (Proposition 1), if we write the problem in
terms of Lagrange multipliers (λ1,λ2,λ3), the solution for (λ′
1 > λ1,λ2,λ3) can
be obtained by soft-thresholding the solutions for (λ1,λ2,λ3).
6.1. Example 1.
Figure 9 shows a toy example. The data are in the top left,
representing a “+”-shaped image with N(0,1) noise added. The reconstruction by
J. FRIEDMAN, T. HASTIE, H. HÖFLING AND R. TIBSHIRANI
A toy example: the data are in the top left, representing a “+”-shaped image with added
noise. The reconstructions by the lasso and fused lasso are shown in the other panels. In each case
we did a grid search over the tuning parameters using a kind of two-fold validation.
the lasso and fused lasso are shown in the other panels. In each case we did a grid
search over the tuning parameters using a kind of two-fold validation. We created
a training set of the odd pixels (1,3,5... in each direction) and tested it on the
even pixels. For illustration only, we chose the values that minimized the squared
reconstruction error over the test set. We see that the fused lasso has successfully
exploited the spatial smoothness and provided a much better reconstruction than
the lasso.
Table 4 shows the number of CPU seconds required for the standard and pathwise coordinate descent algorithms, as n increases. We were unable to apply the
standard algorithm for n = 256 (due to memory requirements), and have instead
estimated the time by crude quadratic extrapolation.
PATHWISE COORDINATE OPTIMIZATION
2D fused lasso applied to the toy problem. The table shows the number of
CPU seconds required for the standard and pathwise coordinate descent
algorithms, as n increases. The regularization parameters were set at
the values that yielded the solution in the bottom left panel of Figure 9
6.2. Example 2.
Figure 10 shows another example. The noiseless image (top
panel) was randomly generated with 512 × 512 pixels. The background pixels are
zero, while the signal blocks have constant values randomly chosen between 1 and
4. The top right panel shows the reconstruction by the fused lasso: as expected, it is
perfect. In the bottom left we have added Gaussian noise with standard deviation
1.5. The reconstruction by the fused lasso is shown in the bottom right panel,
using two-fold validation to estimate λ1,λ2. The reconstruction is still quite good,
capturing most of the important features. In this example we did a search over
10 λ1 values. The entire computation for the bottom right panel of Figure 10,
including the two-fold validation to estimate the optimal values for λ1 and λ2,
took 11.3 CPU minutes.
6.3. Example 3.
The top left panel of Figure 11 shows a 256 × 256 gray scale
image of statistician Sir Ronald Fisher. In the top right we have added Gaussian
noise with a standard deviation 2.5. We explore the use of the two-dimensional
fused lasso for denoising this image. However, the ﬁrst (lasso) penalty doesn’t
make sense here, as zero does not represent a natural baseline. So instead, we
tried a pure fusion model, with λ1 = 0. We found the best value of λ2, in terms
of reconstruction error from the original noiseless image. The solution shown in
the bottom right panel gives a reasonable approximation to the original image and
reduces the reconstruction error from 6.18 to 1.15. In the bottom left panel we
have set λ2 = 0, and optimized over λ1. As expected, this pure lasso solution does
poorly, and the optimal value of λ1 turned out to be 0.
6.4. Applications to higher dimensions and other problems.
The general strategy for the two-dimensional fused lasso can be directly applied in higher dimensional problems, the difference being that each cell would have more potential distance-1 neighbors. In fact, the same strategy might be applicable to non-
Euclidean problems. All one needs is a notion of distance-1 neighbors and the
J. FRIEDMAN, T. HASTIE, H. HÖFLING AND R. TIBSHIRANI
A second toy example. The 100 × 100 noiseless and noisy images are shown on the left,
while the corresponding fused lasso reconstructions are shown on the right. In each case we did a
grid search over the tuning parameters λ1,λ2 using a kind of two-fold validation.
property that the distance-1 neighbors of a fusion of two groups are the union of
the distance-1 neighbors of the two groups, less the fused group members themselves.
7. Discussion.
Coordinate-wise descent algorithms deserve more attention
in convex optimization. They are simple and well-suited to large problems. We
have found that for the lasso, coordinate-wise descent is very competitive with
the LARS algorithm, probably the fastest procedure for that problem to-date.
Coordinate-wise descent algorithms can be applied to problems in which the constraints decouple, and a generalized version of coordinate-wise descent like the one
presented here can handle problems in which each parameter is involved in only a
limited number of constraints. This procedure is ideally suited for a special case of
PATHWISE COORDINATE OPTIMIZATION
Top-left panel: 256 × 256 gray scale image of statistician Sir Ronald Fisher. Top-right
panel: Gaussian noise with standard deviation 2.5 has been added. Bottom-left panel: best solution with λ2 set to zero (pure lasso penalty); this gives no improvement in reconstruction error.
Bottom-right panel: best solution with λ1 set to zero (pure fusion penalty). This reduces the reconstruction error from 6.18 to 1.15.
the fused lasso—the fused lasso signal approximator, and runs many times faster
than a standard convex optimizer. On the other hand, it is not guaranteed to work
for the general fused lasso problem, as it can get stuck away from the solution.
7.1. Software.
Both Fortran and R language routines for the lasso, and the
one- and two-dimensional fused lasso will be made freely available.
J. FRIEDMAN, T. HASTIE, H. HÖFLING AND R. TIBSHIRANI
A.1. Proof of Proposition 1.
Suppose that we are optimizing a function of
(yi −βi)2 + λ1
λi,j|βi −βj|,
where (i,j) ∈C if the difference |βi −βj| is L1 penalized with penalty parameter
λi,j. This general form for the penalty includes the following models discussed
Fused lasso signal approximator: Here, (i,j) ∈C if i = j −1. Furthermore, λi,j =
Two-dimensional fused lasso: Here i itself is a two-dimensional coordinate i =
(i1,i2). Let (i,j) ∈C if |i1 −j1| + |i2 −j2| = 1. If |i1 −j1| = 1, then λi,j = λ2,
otherwise λi,j = λ3.
Now we prove a soft thresholding result.
LEMMA A.1.
Assume that the solution for λ1 = 0 and λ2 ≥0 is known and
denote it by ˆβ(0,λ2). Then, the solution for λ1 > 0 is
ˆβi(λ1,λ2) = sign( ˆβi(0,λ2))
| ˆβi(0,λ2)| −λ1
First ﬁnd the subgradient equations for β1,...,βn, which are
gi = −(yi −βi) + λ1si +
λ2tj,i = 0,
where si = sign(βi) if βi ̸= 0 and si ∈[−1,1] if βi = 0. Also, ti,j = sign(βi −
βj) for βi ̸= βj and ti,j ∈[−1,1] if βi = βj. These equations are necessary and
sufﬁcient for the solution.
As it is assumed that a solution for λ1 = 0 is known, let s(0) and t(0) denote
the values of the parameters for this solution. Speciﬁcally, si(0) = sign( ˆβi(0)) for
ˆβi(0) ̸= 0 and for ˆβi(0) = 0, it can be chosen arbitrarily, so set si(0) = 0. Note that
as λ2 is constant throughout the whole proof, the dependence of β, s and t on λ2
is suppressed for notational convenience.
In order to ﬁnd t(λ1), observe that soft thresholding of β(0) does not change
the ordering of pairs ˆβi(λ1) and ˆβj(λ1) for those pairs for which at least one
of the two is not 0 and, therefore, it is possible to deﬁne ti,j(λ1) = ti,j(0). If
ˆβi(λ1) = ˆβj(λ1) = 0, then ti,j can be chosen arbitrarily in [−1,1] and, therefore,
let ti,j(λ1) = ti,j(0). Thus, without violating restrictions on ti,j, t(λ1) = t(0) for
all λ1 > 0. s(λ1) will be chosen appropriately below so that the subgradient equations hold.
Now insert ˆβi(λ1) = sign( ˆβi(0))(| ˆβi(0)| −λ1)+ into the subgradient equations.
For λ1 > 0, look at 2 cases:
PATHWISE COORDINATE OPTIMIZATION
| ˆβi(0)| ≥λ1
gi(λ1) = −yi + ˆβi(0) −λ1si(0) + λ1si(λ1)
j : (i,j)∈C
λ2ti,j(λ1) −
j : (j,i)∈C
λj,itj,i(λ1)
= −yi + ˆβi(0) +
j : (i,j)∈C
λ2ti,j(0) −
j : (j,i)∈C
λj,itj,i(0) = 0
by setting si(λ1) = si(0), using the deﬁnition of t(λ1) and noting that ˆβ(0) was
assumed to be a solution.
| ˆβi(0)| < λ1. Here, ˆβ(λ1) = 0 and, therefore,
gi(λ1) = −yi + λ1si(λ1) +
j : (i,j)∈C
λ2ti,j(λ1) −
j : (j,i)∈C
λj,itj,i(λ1)
= −yi + ˆβi(0) +
j : (i,j)∈C
λ2ti,j(0) −
j : (j,i)∈C
λj,itj,i(0) = 0
by choosing si(λ1) = ˆβi(0)/λ1 ∈[−1,1] and again using that ˆβ(0) is optimal.
As the subgradient equations hold for every λ1 > 0, soft thresholding gives the
solution. Note that we have assumed that λi,j = λ2, but this proof works for any
ﬁxed values λi,j.
Using this theorem, it is possible to make a more general statement.
PROPOSITION A.1.
Let ˆβ(λ1,λ2) be a minimum of f (β) for (λ1,λ2). Then
the solution for the parameters (λ′
1,λ2) with λ′
1 > λ1 is a soft thresholding of
ˆβ(λ1,λ2), that is,
1,λ2) = sign( ˆβ(λ1,λ2))
 ˆβ(λ1,λ2) −(λ′
As a solution for minimizing f (β) exists and is unique for all
λ1,λ2 ≥0, the solution ˆβ(0,λ2) exists and ˆβ(λ1,λ2) as well as ˆβ(λ′
1,λ2) are softthresholded versions of it using the previous theorem. Therefore,
ˆβi(λ1,λ2) = sign( ˆβi(0,λ2))
| ˆβi(0,λ2)| −λ1
1,λ2) = sign( ˆβi(0,λ2))(| ˆβi(0,λ2)| −λ′
for i = 1,...,n. If ˆβi(λ1,λ2) = 0, then also ˆβi(λ′
1,λ2) = 0. For ˆβi(λ1,λ2) > 0, the
soft-thresholding implies that the sign did not change and, thus, sign( ˆβi(λ1,λ2)) =
sign( ˆβi(0,λ2)). It then follows
1,λ2) = sign( ˆβi(0,λ2))
| ˆβi(0,λ2)| −λ′
= sign( ˆβi(λ1,λ2))
| ˆβi(λ1,λ2)| −(λ′
J. FRIEDMAN, T. HASTIE, H. HÖFLING AND R. TIBSHIRANI
Therefore, ˆβ(λ′
1,λ2) is a soft-thresholded version of ˆβ(λ1,λ2).
Quadratic programming solution for the two-dimensional fused lasso.
we outline the solution to the two-dimensional fused lasso using the general purpose sqopt package of Gill, Murray and Saunders .
Let βii′ = β+
ii′ with β+
ii′ ≥0. Deﬁne θh
ii′ = βi,i′ −βi−1,i′ for i > 1,
ii′ = βi,i′ −βi,i′−1 for i′ > 1, and θ11 = β11. Let θh
ii′ with θh+
0, and similarly for θv
ii′. We string out each set of parameters into one long vector,
starting with the 11 entry in the top left, and going across each row.
Let L1 and L2 be the n × n matrices that compute horizontal and vertical differences. Let e be a column n-vector of ones, and I be the n × n identity matrix.
Then the constraints can be written as
Here a0 = (∞,0,0...0). Since β1i′ = θ1i′, setting its bounds at ±∞avoids a
“double” penalty for |β1i′| and similarly for β1i′. Similarly, e0 equals e, with the
ﬁrst component set to zero.
Pathwise coordinate optimization for the general one-dimensional fused lasso.
This algorithm has exactly the same form as that for the fused lasso signal approximator given earlier. The form of the basic equations is all that changes.
Equation (24) becomes
xik ˜βk −xijβj
+ λ1 · sign(βj) −λ2 · sign( ˜βj+1 −βj)
+ λ2 · sign(βj −˜βj−1),
assuming that βj /∈{0, ˜βj−1, ˜βj+1}.
Similarly, expression (25) becomes
k/∈{j,j+1}
xik ˜βk −ziγ
+ 2λ1 · sign(γ ) −λ2 · sign( ˜βj+2 −γ )
+ λ2 · sign(γ −˜βj−1),
PATHWISE COORDINATE OPTIMIZATION
where zi = xij + xi,j+1. If the optimal value for γ decreases the criterion, we
accept the move setting βj = βj−1 = γ .
Pathwise coordinate optimization for two-dimensional fused lasso signal approximator.
Consider a set (grid) G of pixels p = (i,j), with 1 ≤i ≤n1,
1 ≤j ≤n2. Associated with each pixel is a data value yp = yij. The goal is to
obtain smoothed values ˆβp = ˆβij for the pixels that jointly solve
(yij −βij)2 + λ1
|βij −βi−1,j| +
|βij −βi,j−1|.
Deﬁning the distance between two pixels p = (i,j) and p′ = (i′,j′) as d(p,p′) =
|i −i′| + |j −j′|, (36) can be expressed as
|βp| + (λ2/2)
|βp −βp′|.
Consider a partition of G into K contiguous groups {Gk}, G = Gk and Gk ∩
Gk′ = 0, k ̸= k′. A group Gk is contiguous if any p ∈Gk can be reached from any
other p′ ∈Gk by a sequence of distance-one steps within Gk. Deﬁne the distance
between two groups Gk and Gk′ as
D(k,k′) = min
Suppose one seeks the solution to (37) under the constraints that all pixels in the
same group have the same parameter value. That is, for each Gk, {βp = γk}p∈Gk.
The corresponding optimal group parameter values ˆγk are the solution to the unconstrained problem
Nk( ¯yk −γk)2
Nk|γk| + (λ2/2)
wkk′|γk −γk′|,
where Nk is the number of pixels in Gk, ¯yk is the mean of the pixel data values
in Gk, and
I[d(p,p′) = 1].
J. FRIEDMAN, T. HASTIE, H. HÖFLING AND R. TIBSHIRANI
Note that (38) is equivalent to (37) when all groups contain only one pixel.
Further, suppose that for a given partition one wishes to obtain the solution to
(38) with the additional constrain that two adjacent groups Gl and Gl′, D(l,l′) = 1,
have the same parameter value γm; that is, γl = γl′ = γm, or equivalently, {βp =
γm}p∈Gl∪Gl′. This can be accomplished by deleting groups l and l′ from the sum
in (38) and adding the corresponding “fused” group Gm = Gl ∪Gl′, with Nm =
Nl + Nl′, ¯ym = (Nlyl + Nl′yl′)/Nm,
{Gk′}D(m,k′)=1
= {Gk′}D(l,k′)=1 ∪{Gk′}D(l′,k′)=1 −Gl −Gl′,
and wmk′ = wlk′ + wl′k′.
The strategy for solving (36) is based on (38). As with FLSA (Section 3), there
are three basic operations: descent, fusion and smoothing. For a ﬁxed value of λ1,
we start at λ2 = 0 and n1 · n2 groups each consisting of a single pixel. The initial
λ2 = 0 solution of each γk is obtained by soft-thresholding γk = S( ¯yk,λ1) as in (3).
Starting at this solution, the value of λ2 is incremented by a small amount λ2 ←
λ2 +δ. Beginning with γ1, the descent operation solves (38) for each γk holding all
other {γk′}k′̸=k at their current values. The derivative of the criterion in (38) with
respect to γk is piecewise linear with breaks at 0, {γk′}D(k,k′)=1. The solution for
γk is thus obtained in the same manner as that for the one–dimensional problem
described in Section 3. If this solution fails to change the current value of γk,
successive provisional fusions of Gk with each Gk′ for which D(k,k′) = 1 are
considered, and the solution for the corresponding fused parameter γm is obtained.
The derivative of the criterion in (38) with respect to γm is piecewise linear with
breaks at 0, {γk′}D(m,k′)=1 (39). If any of these fused solutions for γm improves
the criterion, one provisionally sets γk = γk′ = γm. If not, the current value of γk
remains unchanged.
One then applies the descent/fusion strategy to the next parameter, k ←k + 1,
and so on until a complete pass over all parameters {γk} has been made. These
passes (cycles) are then repeated until one complete pass fails to change any parameter value, at which point the solution for the current value of λ2 has been
At this point each current group Gk is permanently fused (merged) with all
groups Gk′ for which γk = γk′, γk′ ̸= 0, and D(k,k′) = 1, producing a new criterion (38) with potentially fewer groups. The value of λ2 is then further incremented
λ2 ←λ2 + δ and the above process is repeated starting at the solution for the previous λ2 value. This continues until a speciﬁed maximum value of λ2 has been
reached or until only one group remains.
Acknowledgments.
We thank Anita van der Kooij for informing us about her
work on the elastic net, Michael Saunders and Guenther Walther for helpful discussions, and Balasubramanian Narasimhan for valuable help with our software.
PATHWISE COORDINATE OPTIMIZATION
A special thanks to Stephen Boyd for telling us about the subgradient form (28).
While writing this paper, we learned of concurrent, independent work on coordinate optimization for the lasso and other convex problems by Ken Lange and
Tongtong Wu. We thank the Editors and two referees for comments led to substantial improvements in the manuscript.