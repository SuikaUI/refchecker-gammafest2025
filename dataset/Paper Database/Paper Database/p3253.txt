BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks
for Semantic Segmentation
Jifeng Dai
Kaiming He
Microsoft Research
{jifdai,kahe,jiansun}@microsoft.com
Recent leading approaches to semantic segmentation
rely on deep convolutional networks trained with humanannotated, pixel-level segmentation masks.
Such pixelaccurate supervision demands expensive labeling effort and
limits the performance of deep networks that usually beneﬁt
from more training data. In this paper, we propose a method
that achieves competitive accuracy but only requires easily obtained bounding box annotations. The basic idea is
to iterate between automatically generating region proposals and training convolutional networks. These two steps
gradually recover segmentation masks for improving the
networks, and vise versa. Our method, called “BoxSup”,
produces competitive results (e.g., 62.0% mAP for validation) supervised by boxes only, on par with strong baselines (e.g., 63.8% mAP) fully supervised by masks under
the same setting. By leveraging a large amount of bounding
boxes, BoxSup further unleashes the power of deep convolutional networks and yields state-of-the-art results on PAS-
CAL VOC 2012 and PASCAL-CONTEXT .
1. Introduction
In the past few months, tremendous progress has been
made in the ﬁeld of semantic segmentation . Deep convolutional neural networks (CNNs) 
that play as rich hierarchical feature extractors are a key to
these methods. These networks are trained on large-scale
datasets as classiﬁers, and transferred to the semantic segmentation tasks based on the annotated segmentation
masks as supervision.
But pixel-level mask annotations are time-consuming,
frustrating, and in the end commercially expensive to obtain. According to the annotation report of the large-scale
Microsoft COCO dataset , the workload of labeling segmentation masks is more than 15 times heavier than that of
spotting object locations. Further, the crowdsourcing annotators need to be specially trained for the tedious and difﬁcult task of labeling per-pixel masks. These facts limit the
amount of available segmentation mask annotations, and
thus hinder the performance of CNNs that in general desire large-scale data for training. On the contrary, bounding
box annotations are more economical than masks. There
have already existed a large number of available box-level
annotations in datasets like PASCAL VOC 20071 and
ImageNet . Though these box-level annotations are less
precise than pixel-level masks, their amount may help improve training deep networks for semantic segmentation.
In addition, current leading approaches have not fully
utilized the detailed pixel-level annotations. For example,
in the Convolutional Feature Masking (CFM) method ,
the ﬁne-resolution masks are used to generate very lowresolution (e.g., 6 × 6) masks on the feature maps. In the
Fully Convolutional Network (FCN) method , the network predictions are regressed to the ground-truth masks
using a large stride (e.g., 8 pixels). These methods yield
competitive results without explicitly harnessing the ﬁner
masks. If we consider the box-level annotations as very
coarse masks, can we still retain comparably good results
without using the segmentation masks?
In this work, we investigate bounding box annotations
as an alternative or extra source of supervision to train convolutional networks for semantic segmentation2. We resort
to unsupervised region proposal methods to generate candidate segmentation masks. The convolutional network is trained under the supervision of these approximate
masks. The updated network in turn improves the estimated
masks used for training. This process is iterated. Although
the masks are coarse at the beginning, they are gradually
improved and then provide useful information for network
training. Fig. 1 illustrates our training algorithm.
We extensively evaluate our method, called “BoxSup”,
on the PASCAL segmentation benchmarks .
1The PASCAL VOC 2007 dataset only has bounding box annotations.
2The idea of using bounding box annotations for CNN-based semantic
segmentation is developed concurrently and independently in . We
also compare with the results of .
 
train image
with gt boxes
candidate masks
BoxSup training
estimated masks
network for
segmentation
Figure 1: Overview of our training approach supervised by bounding boxes.
box-supervised (i.e., using bounding box annotations)
method shows a graceful degradation compared with its
mask-supervised (i.e., using mask annotations) counterpart.
As such, our method waives the requirement of pixel-level
masks for training. Further, our semi-supervised variant in
which 9/10 mask annotations are replaced with bounding
box annotations yields comparable accuracy with the fully
mask-supervised counterpart. This suggests that we may
save expensive labeling effort by using bounding box annotations dominantly. Moreover, our method makes it possible
to harness the large number of available box annotations to
improve the mask-supervised results. Using the limited provided mask annotations and extra large-scale bounding box
annotations, our method achieves state-of-the-art results on
both PASCAL VOC 2012 and PASCAL-CONTEXT 
benchmarks.
Why can a large amount of bounding boxes help improve convolutional networks? Our error analysis reveals
that a BoxSup model trained with a large set of boxes effectively increases the object recognition accuracy (the accuracy in the middle of an object), and its improvement on
object boundaries is secondary. Though a box is too coarse
to contain detailed segmentation information, it provides an
instance for learning to distinguish object categories. The
large-scale object instances improve the feature quality of
the learned convolutional networks, and thus impact the
overall performance for semantic segmentation.
2. Related Work
Deep convolutional networks in general have better accuracy with the growing size of training data, as is evidenced in . The ImageNet classiﬁcation dataset 
is one of the largest datasets with quality labels, but the current available datasets for object detection, semantic segmentation, and many other vision tasks mostly have orders
of magnitudes fewer labeled samples. The milestone work
of R-CNN proposes to pre-train deep networks as classi-
ﬁers on the large-scale ImageNet dataset and go on training
(ﬁne-tuning) them for other tasks that have limited number
of training data. This transfer learning strategy is widely
adopted for object detection , semantic segmentation , visual tracking , and other
visual recognition tasks.
With the continuously improving deep convolutional models ,
the accuracy of these vision tasks also improves thanks to
the more powerful generic features learned from large-scale
Although pre-training partially relieves the problem of
limited data, the amount of the task-speciﬁc data for ﬁnetuning still matters.
In , it has been found that augmenting the object detection training set by combining the
VOC 2007 and VOC 2012 sets improves object detection
accuracy compared with using VOC 2007 only. In ,
the training set for object detection is augmented by visual
tracking results obtained from videos and improves detection accuracy. These experiments demonstrate the importance of dataset sizes for task-speciﬁc network training.
For semantic segmentation, there have been existing papers that investigate exploiting bounding box annotations instead of masks. But the box-level annotations have
not been used to supervised deep convolutional networks in
those works.
3. Baseline
Our BoxSup method is in general applicable for many
existing CNN-based mask-supervised semantic segmentation methods, such as FCN , improvements on FCN
 , and others . In this paper, we adopt our
implementation of the FCN method reﬁned by CRF 
as the mask-supervised baseline, which we brieﬂy introduce
(d) GrabCut
(a) training image
(b) ground-truth
(c) rectangles
Figure 2: Segmentation masks used as supervision. (a) A training image. (b) Ground-truth. (c) Each box is na¨ıvely considered
as a rectangle mask. (d) A segmentation mask is generated by GrabCut . (e) For our method, the supervision is estimated
from region proposals (MCG ) by considering bounding box annotations and network feedbacks.
as follows.
The network training of FCN is formulated as a perpixel regression problem to the ground-truth segmentation
masks. Formally, the objective function can be written as:
e(Xθ(p), l(p)),
where p is a pixel index, l(p) is the ground-truth semantic label at a pixel, and Xθ(p) is the per-pixel labeling produced by the fully convolutional network with parameters θ.
e(Xθ(p), l(p)) is the per-pixel loss function. The network
parameters θ are updated by back-propagation and stochastic gradient descent (SGD). A CRF is used to post-process
the FCN results .
The objective function in Eqn.(1) demands pixel-level
segmentation masks l(p) as supervision. It is not directly
applicable if only bounding box annotations are given as
supervision. Next we introduce our method for addressing
this problem.
4. Approach
4.1. Unsupervised Segmentation for Supervised Training
To harness the bounding boxes annotations, it is desired
to estimate segmentation masks from them. This is a widely
studied supervised image segmentation problem, and can
be addressed by, e.g., GrabCut . But GrabCut can only
generate one or a few samples from one box, which may be
insufﬁcient for deep network training.
We propose to generate a set of candidate segments using unsupervised region proposal methods (e.g., Selective
Search ) due to their nice properties. First, region proposal methods have high recall rates of having a good
candidate in the proposal pool. Second, region proposal
methods generate candidates of greater variance, which provide a kind of data augmentation for network training.
We will show by experiments the improvements of these
properties.
The candidate segments are used to update the deep convolutional network. The semantic features learned by the
network are then used to pick better candidates. This procedure is iterated. We formulate this procedure as an objective
function as we will describe below.
It is worth noticing that the region proposal is only used
for networking training. For inference, the trained FCN is
directly applied on the image and produces pixel-wise predictions. So our usage of region proposals does not impact
the test-time efﬁciency.
4.2. Formulation
As a pre-processing, we use a region proposal method to
generate segmentation masks. We adopt Multiscale Combinatorial Grouping (MCG) by default, while other methods are also evaluated.
The proposal candidate
masks are ﬁxed throughout the training procedure. But during training, each candidate mask will be assigned a label
which can be a semantic category or background. The labels assigned to the masks will be updated.
With a ground-truth bounding box annotation, we expect
it to pick out a candidate mask that overlaps the box as much
as possible. Formally, we deﬁne an overlapping objective
function Eo as:
(1 −IoU(B, S))δ(lB, lS).
Here S represents a candidate segment mask, and B represents a ground-truth bounding box annotation. IoU(B, S) ∈
 is the intersection-over-union ratio computed from the
ground-truth box B and the tight bounding box of the segment S. The function δ is equal to one if the semantic label
lS assigned to segment S is the same as the ground-truth
label lB of the bounding box B, and zero otherwise. Minimizing Eo favors higher IoU scores when the semantic labels are consistent. This objective function is normalized
by the number of candidate segments N.
training image
Figure 3: Update of segmentation masks during training. Here we show the masks in epoch #1, epoch #5, and epoch #20.
Each segmentation mask will be used as the supervision for the next epoch.
With the candidate masks and their estimated semantic
labels, we can supervise the deep convolutional network as
in Eqn.(1). Formally, we consider the following regression
objective function Er:
e(Xθ(p), lS(p)).
Here lS is the estimated semantic label used as supervision
for the network training. This objective function is the same
as Eqn.(1) except that its regression target is the estimated
candidate segment.
We minimize an objective function that combines the
above two terms:
(Eo + λEr)
Here the summation P
i runs over the training images, and
λ = 3 is a ﬁxed weighting parameter. The variables to
be optimized are the network parameters θ and the labeling
{lS} of all candidate segments {S}. If only the term Eo
exists, the optimization problem in Eqn.(4) trivially ﬁnds a
candidate segment that has the largest IoU score with the
box; if only the term Er exists, the optimization problem in
Eqn.(4) is equivalent to FCN. Our formulation simultaneously considers both cases.
4.3. Training Algorithm
The objective function in Eqn.(4) involves a problem of
assigning labels to the candidate segments. Next we propose a greedy iterative solution to ﬁnd a local optimum.
With the network parameters θ ﬁxed, we update the semantic labeling {lS} for all candidate segments.
implementation, we only consider the case in which one
ground-truth bounding box can “activate” (i.e., assign a
non-background label to) one and only one candidate. As
such, we can simply update the semantic labeling by selecting a single candidate segment for each ground-truth bounding box, such that its cost Eo + λEr is the smallest among
all candidates. The selected segment is assigned the groundtruth semantic label associated with that bounding box. All
other pixels are assigned the background label.
The above winner-takes-all selection tends to repeatedly
use the same or very similar candidate segments, and the optimization procedure may be trapped in poor local optima.
To increase the sample variance for better stochastic training, we further adopt a random sampling method to select
the candidate segment for each ground-truth bounding box.
Instead of selecting the single segment with the largest cost
Eo + λEr, we randomly sample a segment from the ﬁrst k
segments with the largest costs. In this paper we use k = 5.
This random sampling strategy improves the accuracy by
about 2% on the validation set.
With the semantic labeling {lS} of all candidate segments ﬁxed, we update the network parameters θ. In this
case, the problem becomes the FCN problem as in
Eqn.(1). This problem is minimized by SGD.
We iteratively perform the above two steps, ﬁxing one set
of variables and solving for the other set. For each iteration,
we update the network parameters using one training epoch
(i.e., all training images are visited once), and after that we
update the segment labeling of all images. Fig.3 shows the
gradually updated segmentation masks during training. The
network is initialized by the model pre-trained in the ImageNet classiﬁcation dataset, and our algorithm starts from
the step of updating segment labels.
Our method is applicable for the semi-supervised case
(the ground-truth annotations are mixtures of segmentation
masks and bounding boxes). The labeling l(p) is given by
candidate proposals as above if a sample only has groundtruth boxes, and is simply assigned as the true label if a
sample has ground-truth masks.
In the SGD training of updating the network, we use a
mini-batch size of 20, following . The learning rate
is initialized to be 0.001 and divided by 10 after every 15
epochs. The training is terminated after 45 epochs.
5. Experiments
In all our experiments, we use the publicly released
VGG-16 model3 that is pre-trained on ImageNet .
The VGG model is also used by all competitors [22, 13, 6,
3www.robots.ox.ac.uk/˜vgg/research/very_deep/
VOC train + COCO
supervision
Table 1: Comparisons of supervision in PASCAL VOC
2012 validation.
5, 23] compared in this paper.
5.1. Experiments on PASCAL VOC 2012
We ﬁrst evaluate our method on the PASCAL VOC
2012 semantic segmentation benchmark . This dataset
involves 20 semantic categories of objects.
We use the
“comp6” evaluation protocol. The accuracy is evaluated by
mean IoU scores. The original training data has 1,464 images. Following , the training data with ground-truth
segmentation masks are augmented to 10,582 images. The
validation and test sets have 1,449 and 1,456 images respectively. When evaluating the validation set or the test set, we
only use the training set for training. A held-out 100 random validation images are used for cross-validation to set
hyper-parameters.
Comparisons of Supervision Strategies
Table 1 compares the results of using different strategies
of supervision on the validation set. When all ground-truth
masks are used as supervision, the result is our implementation of the baseline DeepLab-CRF . Our reproduction
has a score of 63.8 (Table 1, “mask only”), which is very
close to 63.74 reported in under the same setting. So we
believe that our reproduced baseline is convincing.
When all 10,582 training samples are replaced with
bounding box annotations, our method yields a score of
62.0 (Table 1, “box only”).
Though the supervision information is substantially weakened, our method shows a
graceful degradation (1.8%) compared with the strongly supervised baseline of 63.8. This indicates that in practice we
can avoid the expensive mask labeling effort by using only
bounding boxes, with small accuracy loss.
Table 1 also shows the semi-supervised result of our
This result uses the ground-truth masks of the
original 1,464 training images and the bounding box annotations of the rest 9k images. The score is 63.5 (Table 1,
“semi”), on par with the strongly supervised baseline. Such
semi-supervision replaces 9/10 of the segmentation mask
annotations with bounding box annotations. This means
that we can greatly reduce the labeling effort by dominantly
using bounding box annotations.
As a proof of concept, we further evaluate using a sub-
 
 
ground-truth
Figure 4: Error analysis on the validation set. Top: (from
left to right) image, ground-truth, boundary regions marked
as white, interior regions marked as white).
boundary and interior mean IoU, using VOC masks only
(blue) and using extra COCO boxes (red).
stantially larger set of boxes. We use the Microsoft COCO
dataset that has 123,287 images with available groundtruth segmentation masks. This dataset has 80 semantic categories, and we only use the 20 categories that also present
in PASCAL VOC. For our mask-supervised baseline, the
result is a score of 68.1 (Table 1). Then we replace the
ground-truth segmentation masks in COCO with their tight
bounding boxes. Our semi-supervised result is 68.2 (Table 1), on par with the strongly supervised baseline. Fig. 5
shows some visual results in the validation set.
The semi-supervised result (68.2) that uses VOC+COCO
is considerably better than the strongly supervised result
(63.8) that uses VOC only. The 4.4% gain is contributed
by the extra large-scale bounding boxes in the 123k COCO
images. This comparison suggests a promising strategy we may make use of the larger amount of existing bounding
boxes annotations to improve the overall semantic segmentation results, as further analyzed below.
Error Analysis
Why can a large set of bounding boxes help improve
convolutional networks? The error in semantic segmentation can be roughly thought of as two types: (i) recognition error that is due to confusions of recognizing object
categories, and (ii) boundary error that is due to misalignments of pixel-level labels on object boundaries. Although
the bounding box annotations have no information about the
object boundaries, they provide extra object instances for
recognizing them. We may expect that the large amount of
rectangles
ours w/o sampling
Table 2: Comparisons of estimated masks for supervision
in PASCAL VOC 2012 validation. All methods only use
10,582 bounding boxes as annotations, with no groundtruth segmentation mask used.
Table 3: Comparisons of the effects of region proposal
methods on our method in PASCAL VOC 2012 validation.
All methods only use 10,582 bounding boxes as annotations, with no ground-truth segmentation mask used.
boxes mainly improve the recognition accuracy.
To analyze the error, we separately evaluate the performance on the boundary regions and interior regions. Following , we generate a “trimap” near the ground-truth
boundaries (Fig. 4, top). We evaluate mean IoU scores inside/outside the bands, referred to as boundary/interior regions. Fig. 4 (bottom) shows the results of using different
band widths for the trimaps.
For the interior region, the accuracy of using the extra
COCO boxes (red solid line, Fig. 4) is considerably higher
than that of using VOC masks only (blue solid line). On the
contrary, the improvement on the boundary regions is relatively smaller (red dash line vs. blue dash line). Note that
correctly recognizing the interior may also help improve the
boundaries (e.g., due to the CRF post-processing). So the
improvement of the extra boxes on the boundary regions is
secondary.
Because the accuracy in the interior region is mainly determined by correctly recognizing objects, this analysis suggests that the large amount of boxes improve the feature
quality of a learned BoxSup model for better recognition.
Comparisons of Estimated Masks for Supervision
In Table 2 we evaluate different methods of estimating
masks from bounding boxes for supervision. As a na¨ıve
baseline, we ﬁll each bounding box with its semantic label, and consider it as a rectangular mask (Fig. 2(c)). Using these rectangular masks as the supervision throughout
training, the score is 52.3 on the validation set. We also use
GrabCut to generate segmentation masks from boxes
(Fig. 2(d)).
With the GrabCut masks as the supervision
DeepLabCRF 
V07+C 133k
V07+C 133k
Table 4: Results on PASCAL VOC 2012 test set. In the supervision (“sup”) column, “mask” means all training samples are with segmentation mask annotations, “box” means
all training samples are with bounding box annotations, and
“semi” means mixtures. “V” denotes the VOC data, “C”
denotes the COCO data, and “V07” denotes the VOC 2007
data which only has bounding boxes available.
throughout training, the score is 55.2. In both cases, the
masks are not updated by the network feedbacks.
Our method has a score 62.0 (Table 2) using the same
set of bounding box annotations. This is a considerable gain
over the baseline using ﬁxed GrabCut masks. This indicates
the importance of the mask quality for supervision. Fig. 3
shows that our method iteratively updates the masks by the
network, which in turn improves the network training.
We also evaluate a variant of our method where each
time the updated mask is the candidate with the largest cost,
instead of randomly sampled from the ﬁrst k candidates (see
Sec. 4.3). This variant has a lower score of 59.7 (Table 2).
The random sampling strategy, which is data augmentation
and increases sample variances, is beneﬁcial for training.
Table 2 also shows the result of the concurrent method
WSSL under the same evaluation setting. Its results is
58.5. This result suggests that our method estimates more
accurate masks than for supervision.
Comparisons of Region Proposals
Our method resorts to unsupervised region proposals for
training. In Table 3, we compare the effects of various region proposals on our method: Selective Search (SS) ,
Geodesic Object Proposals (GOP) , and MCG . Table 3 shows that MCG has the best accuracy, which is
consistent with its segmentation quality evaluated by other
metrics in . Note that at test-time our method does not
need region proposals. So the better accuracy of using MCG
implies that our method effectively makes use of the higher
quality segmentation masks to train a better network.
Comparisons on the Test Set
Next we compare with the state-of-the-art methods on
(c) box, VOC
(b) mask, VOC
(d) semi, VOC mask +COCO box
Figure 5: Example semantic segmentation results on PASCAL VOC 2012 validation using our method. (a) Images. (b)
Supervised by masks in VOC. (c) Supervised by boxes in VOC. (d) Supervised by masks in VOC and boxes in COCO.
the PASCAL VOC 2012 test set. In Table 4, the methods
are based on the same FCN baseline and thus fair comparisons are made to evaluate the impact of mask/box/semisupervision.
As shown in Table 4, our box-supervised result that only
uses VOC bounding boxes is 64.6. This compares favorably with the WSSL counterpart (60.4) under the same
setting. On the other hand, our box-supervised result has
a graceful degradation (1.8%) compared with the masksupervised DeepLab-CRF (66.4 ) using the VOC training
data. Moreover, our semi-supervised variant which replaces
9/10 segmentation mask annotations with bounding boxes
has a score of 66.2. This is on par with the mask-supervised
counterpart of DeepLab-CRF, but the supervision information used by our method is much weaker.
In the WSSL paper , by using all segmentation
mask annotations in VOC and COCO, the strongly masksupervised result is 70.4.
Our semi-supervised method
shows a higher score of 71.0. Remarkably, our result uses
the bounding box annotations from the 123k COCO images.
So our method has a more accurate result but uses much
weaker annotations than .
On the other hand, compared with the DeepLab-CRF result (66.4), our method has a 4.6% gain enjoyed from exploiting the bounding box annotations of the COCO dataset.
This comparison demonstrates the power of our method that
exploits large-scale bounding box annotations to improve
Exploiting Boxes in PASCAL VOC 2007
To further demonstrate the effect of BoxSup, we exploit
the bounding boxes in the PASCAL VOC 2007 dataset .
This dataset has no mask annotations. It is a de facto dataset
which mask-supervised methods are not able to use.
We exploit all 10k images in the VOC 2007 trainval and
test sets. We train a BoxSup model using the union set of
VOC 2007 boxes, COCO boxes, and the augmented VOC
2012 training set. The score improves from 71.0 to 73.1 (Table 4) because of the extra box training data. It is reasonable
for us to expect further improvement if more bounding box
annotations are available.
Baseline Improvement
Although our focus is mainly on exploiting boxes as supervision, it is worth noticing that our method may also
beneﬁt from other improvements on the mask-sup baseline
(FCN in our case). Concurrent with our work, there are a series of improvements made on FCN, which achieve
excellent results using strong mask-supervision from VOC
and COCO data.
To show the potential of our BoxSup method in parallel
with improvements on the baseline, we use a simple testtime augmentation to boost our results. Instead of comput-
(c) baseline
(b) ground-truth
(d) BoxSup
Figure 6: Example results on PASCAL-CONTEXT validation. (a) Images. (b) Results of our baseline (35.7 mean IoU),
trained using VOC masks. (c) Results of BoxSup (40.5 mean IoU), trained using VOC masks and COCO boxes.
Table 5: Results on PASCAL-CONTEXT validation.
Our baseline is our implementation of FCN+CRF. “V” denotes the VOC data, and “C” denotes the COCO data.
ing pixel-wise predictions on a single scale, we compute
the score maps from two extra scales (±20% of the original image size) and bilinearly re-scale the score maps to
the original size. The scores from three scales are averaged. This simple modiﬁcation boosts our result from 73.1
to 75.2 (BoxSup+, Table 4) in the VOC 2012 test set. This
result is on par with the latest results using strong masksupervision from both VOC and COCO, but in our case the
COCO dataset only provides bounding boxes.
5.2. Experiments on PASCAL-CONTEXT
We further perform experiments on the recently labeled
PASCAL-CONTEXT dataset . This dataset provides
ground-truth semantic labels for the whole scene, including
object and stuff (e.g., grass, sky, water). Following the protocol in , the semantic segmentation is performed
on the most frequent 59 categories (identiﬁed by ) plus
a background category. The accuracy is measured by mean
IoU scores. The training and evaluation are performed on
the training and validation sets that have 4,998 and 5,105
images respectively.
To train a BoxSup model for this dataset, we ﬁrst use the
box annotations from all 80 object categories in the COCO
dataset to train the FCN (using VGG-16). This network
ends with an 81-way (with an extra one for background)
layer. Then we remove this last layer and add a new 60way layer for the 59 categories of PASCAL-CONTEXT. We
ﬁne-tune this model in the 5k training images of PASCAL-
CONTEXT. A CRF for post-processing is also used. We do
no use the test-time scale augmentation.
Table 5 shows the results in PASCAL-CONTEXT. The
methods of CFM and FCN are both based on the
VGG-16 model. Our baseline method, which is our implementation of FCN+CRF, has a score of 35.7 using masks
of the 5k training images. Using our BoxSup model pretrained using the COCO boxes, the result is improved to
40.5. The 4.8% gain is solely because of the bounding box
annotations in COCO that improve our network training.
Fig. 6 shows some examples of our results for joint object
and stuff segmentation.
6. Conclusion
The proposed BoxSup method can effectively harness
bounding box annotations to train deep networks for semantic segmentation. Our BoxSup method that uses 133k
bounding boxes and 10k masks achieves state-of-the-art results. Our error analysis suggests that semantic segmentation accuracy is hampered by the failure of recognizing
objects, which large-scale data may help with.