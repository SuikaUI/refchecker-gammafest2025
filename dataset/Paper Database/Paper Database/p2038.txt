TECHNICAL WORKING PAPER SERIES
PARAMETRIC AND NONPARAMETRIC VOLATILITY MEASUREMENT
Torben G. Andersen
Tim Bollerslev
Francis X. Diebold
Technical Working Paper 279
 
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
August 2002
This paper is prepared for Yacine Aït-Sahalia and Lars Peter Hansen (eds.), Handbook of Financial
Econometrics, Amsterdam: North Holland. We are grateful to the National Science Foundation for research
support, and to Nour Meddahi, Neil Shephard and Sean Campbell for useful discussions and detailed
comments on earlier drafts. The views expressed in this paper are those of the authors and not necessarily
those of the National Bureau of Economic Research.
© 2001 by Torben G. Andersen, Tim Bollerslev and Francis X. Diebold. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.
Parametric and Nonparametric Volatility Measurement
Torben G. Andersen, Tim Bollerslev and Francis X. Diebold
NBER Technical Working Paper No. 279
August 2002
JEL No. C1
Volatility has been one of the most active areas of research in empirical finance and time series
econometrics during the past decade. This chapter provides a unified continuous-time, frictionless,
no-arbitrage framework for systematically categorizing the various volatility concepts, measurement
procedures, and modeling procedures. We define three different volatility concepts: (i) the notional
volatility corresponding to the ex-post sample-path return variability over a fixed time interval, (ii) the
ex-ante expected volatility over a fixed time interval, and (iii) the instantaneous volatility corresponding
to the strength of the volatility process at a point in time. The parametric procedures rely on explicit
functional form assumptions regarding the expected and/or instantaneous volatility. In the discrete-time
ARCH class of models, the expectations are formulated in terms of directly observable variables, while
the discrete- and continuous-time stochastic volatility models involve latent state variable(s). The
nonparametric procedures are generally free from such functional form assumptions and hence afford
estimates of notional volatility that are flexible yet consistent (as the sampling frequency of the underlying
returns increases). The nonparametric procedures include ARCH filters and smoothers designed to
measure the volatility over infinitesimally short horizons, as well as the recently-popularized realized
volatility measures for (non-trivial) fixed-length time intervals.
Torben G. Andersen
Tim Bollerslev
Department of Finance
Depts. of Economics and Finance
Kellogg School of Management
Duke University
Northwestern University
Durham, NC 27708
Evanston, IL 60208
 
 
Francis X. Diebold
Depts. of Economics, Finance and Statistics
University of Pennsylvania
Philadelphia, PA 19104
 
Table of Contents
1. Introduction
2. Volatility Definitions
2.1. Continuous-Time No-Arbitrage Pricing
2.2. Notional, Expected, and Instantaneous Volatility
2.3. Volatility Models and Measurements
3. Parametric Methods
3.1. Discrete-Time Models
3.1.1. ARCH Models
3.1.2. Stochastic Volatility Models
3.2. Continuous-Time Models
3.2.1. Continuous Sample Path Diffusions
3.2.1. Jump Diffusions and Lévy Driven Processes
4. Nonparametric Methods
4.1. ARCH Filters and Smoothers
4.2. Realized Volatility
5. Conclusion
References
1 See, for example, Bollerslev, Chou and Kroner .
1. INTRODUCTION
Since Engle’s seminal paper on ARCH models, the econometrics literature has focused
considerable attention on time-varying volatility and the development of new tools for volatility
measurement, modeling and forecasting. These advances have in large part been motivated by
the empirical observation that financial asset return volatility is time-varying in a persistent
fashion, across assets, asset classes, time periods, and countries.1 Asset return volatility,
moreover, is central to finance, whether in asset pricing, portfolio allocation, or risk
management, and standard financial econometric methods and models take on a very different,
conditional, flavor when volatility is properly recognized to be time-varying.
The combination of powerful methodological advances and tremendous relevance in empirical
finance produced explosive growth in the financial econometrics of volatility dynamics, with the
econometrics and finance literatures cross-fertilizing each other furiously. Initial developments
were tightly parametric, but the recent literature has moved in less parametric, and even fully
nonparametric, directions. Here we review and provide a unified framework for interpreting
both the parametric and nonparametric approaches.
In section 2, we define three different volatility concepts: (i) the notional volatility
corresponding to the ex-post sample-path return variability over a fixed time interval, (ii) the exante expected volatility over a fixed time interval, and (iii) the instantaneous volatility
corresponding to the strength of the volatility process at a point in time.
In section 3, we survey parametric approaches to volatility modeling, which are based on explicit
functional form assumptions regarding the expected and/or instantaneous volatility. In the
discrete-time ARCH class of models, the expectations are formulated in terms of directly
observable variables, while the discrete- and continuous-time stochastic volatility models both
involve latent state variable(s).
In section 4, we survey nonparametric approaches to volatility modeling, which are generally
free from such functional form assumptions and hence afford estimates of notional volatility that
are flexible yet consistent (as the sampling frequency of the underlying returns increases). The
nonparametric approaches include ARCH filters and smoothers designed to measure the
volatility over infinitesimally short horizons, as well as the recently-popularized realized
volatility measures for (non-trivial) fixed-length time intervals.
We conclude in section 5 by highlighting promising directions for future research.
2. VOLATILITY DEFINITIONS
Here we introduce a unified framework for defining and classifying different notions of return
volatility in a continuous-time no-arbitrage setting. We begin by outlining the minimal set of
regularity conditions invoked on the price processes and establish the notation used for the
decomposition of returns into an expected, or mean, return and an innovation component. The
resulting characterization of the price process is central to the development of our different
volatility measures, and we rely on the concepts and notation introduced in this section
throughout the chapter.
2.1. Continuous-Time No-Arbitrage Price Processes
Measurement of return volatility requires determination of the component of a given price
increment that represents a return innovation as opposed to an expected price movement. In a
discrete-time setting this identification may only be achieved through a direct specification of the
conditional mean return, for example through an asset pricing model, as economic principles
impose few binding constraints on the price process. However, within a frictionless continuoustime framework the no-arbitrage requirement quite generally guarantees that the return
innovation is an order of magnitude larger than the mean return. This result is not only critical to
the characterization of arbitrage-free continuous-time price processes, but it also has important
implications for the approach one may employ for measurement and modeling of volatility over
short intradaily return horizons.
We take as given a univariate risky logarithmic price process p(t) defined on a complete
probability space, (S,ö,P). The price process evolves in continuous time over the interval
[0,T], where T is a (finite) integer. The associated natural filtration is denoted (öt )t0[0,T] f ö,
where the information set, öt , contains the full history (up to time t) of the realized values of the
asset price and other relevant (possibly latent) state variables, and is otherwise assumed to satisfy
the usual conditions. It is sometimes useful to consider the information set generated by the
asset price history alone. We refer to this coarser filtration, consisting of the initial conditions
and the history of the asset prices only, by (Ft )t0[0,T] f F / FT , so that by definition, Ft f öt .
Finally, we assume there is an asset guaranteeing an instantaneously risk-free rate of interest,
although we shall not refer to this rate explicitly. Many more risky assets may, of course, be
available but we explicitly retain a univariate focus for notational simplicity. The extension to
the multivariate setting is conceptually straightforward as discussed in specific instances below.
The continuously compounded return over the time interval [t-h,t] is then
r(t,h) = p(t) - p(t-h), 0#h#t#T .
We also adopt the following short hand notation for the cumulative return up to time t, i.e., the
return over the [0,t] time interval:
r(t) / r(t,t) = p(t) - p(0), 0#t#T.
These definitions imply a simple relation between the period-by-period and the cumulative
returns that we use repeatedly in the sequel:
r(t,h) = r(t) - r(t-h), 0#h#t#T.
A maintained assumption throughout is that - almost surely (P) (henceforth denoted (a.s.) ) - the
asset price process remains strictly positive and finite, so that p(t) and r(t) are well defined over
[0,T] (a.s.). It follows that r(t) has only countably many jump points over [0,T], and we adopt
the convention of equating functions that have identical left and right limits everywhere.
Defining r(t-) / limJ6 t, J<t r(J) and r(t+) / limJ6 t, J>t r(J), uniquely determines the right-continuous,
left-limit (càdlàg) version of the process, for which r(t)=r(t+) (a.s.), and the left-continuous,
right-limit (càglàd) version, for which r(t)=r(t-) (a.s.), for all t in [0,T]. In the following, we
assume without loss of generality that we are working with the càdlàg version of the various
return processes and components.
The jumps in the cumulative price and return process are then
)r(t) / r(t) - r(t -), 0#t#T.
Obviously, at continuity points for r(t), we have )r(t) = 0. Notice also that, because there are at
most a countably infinite number of jumps, a jump occurrence is unusual in the sense that we
generically have
P( )r(t) … 0 ) = 0 ,
for an arbitrarily chosen t in [0,T]. This does not imply that jumps necessarily are rare. In fact,
equation (2.5) is consistent with there being a (countably) infinite number of jumps over any
discrete interval - a phenomenon referred to as an explosion. Jump processes that do not explode
are termed regular. For regular processes, the anticipated jump frequency is conveniently
characterized by the instantaneous jump intensity, i.e., the probability of a jump over the next
instant of time, and expressed in units that reflect the expected (and finite) number of jumps per
unit time interval.
In the following, we invoke the standard assumptions of no arbitrage opportunities and a finite
expected return. Within our frictionless setting, it is well known that these conditions imply that
the log-price process must constitute a (special) semi-martingale . This, in
turn, affords the following unique canonical return decomposition .
PROPOSITION 1 - Return Decomposition
2 In other words, even though the conditional mean is locally predictable, all return components in the
special semi-martingale decomposition are generally stochastic: not only volatility, but also the jump intensity, the
jump size distribution and the conditional mean process may evolve randomly over a finite interval.
Any arbitrage-free logarithmic price process subject to the regularity conditions outlined above
may be uniquely represented as
r(t) / p(t) - p(0) = µ(t) + M(t) = µ(t) + Mc(t) + MJ(t),
where µ(t) is a predictable and finite variation process, M(t) is a local martingale which may be
further decomposed into Mc(t), a continuous sample path, infinite variation local martingale
component, and MJ(t), a compensated jump martingale. All components may be assumed to have
initial conditions normalized such that µ(0) / M(0) / Mc(0) / MJ(0) / 0, which implies that r(t) /
Proposition 1 provides a unique decomposition of the instantaneous return into an expected
return component and a (martingale) innovation. Over discrete intervals, the relation becomes
slightly more complex. Letting the expected returns over [t-h,t] be denoted by m(t,h), equation
(2.6) implies
m(t,h) / E[r(t,h)|öt-h ] = E[µ(t,h)|öt-h ] , 0<h#t#T,
µ(t,h) / µ(t) - µ(t-h) , 0<h#t#T,
and the return innovation takes the form
r(t,h) - m(t,h) = ( µ(t,h) - m(t,h) ) + M(t,h) , 0<h#t#T.
The first term on the right hand side of (2.9) signifies that the expected return process, even
though it is (locally) predictable, may evolve stochastically over the [t-h,t] interval.2 If µ(t,h) is
predetermined (measurable with respect to öt-h), and thus known at time t-h, then the discretetime return innovation reduces to M(t,h) / M(t) - M(t-h). However, any shift in the expected
return process during the interval will generally render the initial term on the right hand side of
(2.9) non-zero and thus contribute to the return innovation over [t-h,t].
Although the discrete-time return innovation incorporates two distinct terms, the martingale
component, M(t,h), is generally the dominant contributor to the return variation over short
intervals, i.e., for h small. In order to discuss the intuition behind this result, which we formalize
in the following section, it is convenient to decompose the expected return process into a purely
continuous, predictable finite variation part, µc(t), and a purely predictable jump part, µJ(t).
3 This point is perhaps most readily understood by analogy to a discrete-time setting. When there is a
predictable jump at time t, the instant from t- to t is effectively equivalent to a trading period, say from t-1 to t,
within a discrete-time model. In that setting, no asset can earn a positive (or negative) excess return relative to the
risk-free rate over (t-1,t] without bearing genuine risk as this would otherwise imply a trivial arbitrage opportunity.
A predictable price jump without an associated positive probability of a jump innovation works entirely analogous.
Because the continuous component, µc(t), is of finite variation it is locally an order of magnitude
smaller than the corresponding contribution from the continuous component of the innovation
term, Mc(t). The reason is - loosely speaking - that an asset earning, say, a positive expected
return over the risk-free rate must have innovations that are an order of magnitude larger than the
expected return over infinitesimal intervals. Otherwise, a sustained long position (infinitely
many periods over any interval) in the risky asset will tend to be perfectly diversified due to a
Law of Large Numbers, as the martingale part is uncorrelated. Thus, the risk-return relation
becomes unbalanced. Only if the innovations are large, preventing the Law of Large Numbers
from becoming operative, will this not constitute a violation of the no-arbitrage condition . The presence of a non-trivial MJ(t) component may similarly
serve to eliminate arbitrage and retain a balanced risk-return trade-off relationship.
Analogous considerations apply to the jump component for the expected return process, µJ(t), if
this factor is present. There cannot be a predictable jump in the mean - i.e., a perfectly
anticipated jump in terms of both time and size - unless it is accompanied by large jump
innovation risk as well, so that Pr()M(t) … 0) > 0. Again - intuitively - if there was a known,
say, positive jump then this induces arbitrage (by going long the asset) unless there is offsetting
(jump) innovation risk.3 Most of the continuous-time asset pricing literature ignores predictable
jumps, even if they are logically consistent with the framework. One reason may be that their
existence is fragile in the following sense. A fully anticipated jump must be associated with
release of new (price relevant) information at a given point in time. However, if there is any
uncertainty about the timing of the announcement so that it is only known to occur within a given
minute, or even a few seconds, then the timing of the jump is more aptly modelled by a
continuous hazard function where the jump probability at each point in time is zero, and the
predictable jump event is thus eliminated. In addition, even if there were predictable jumps
associated with scheduled news releases, the size of the predictable component of the jump is
likely much smaller than the size of the associated jump innovation, so that the descriptive power
lost by ignoring the possibility of predictable jumps is minimal. Thus, rather than modify the
standard setup to allow for the presence of predictable (but empirically negligible) jumps, we
follow the tradition in the literature and assume away such jumps.
Although we will not discuss specific model classes at length until later sections, it may be useful
to briefly consider two simple examples to illustrate the somewhat abstract definitions given in
the current section.
EXAMPLE 1: Stochastic Volatility Jump Diffusion with Non-Zero Mean Jumps
Consider the following continuous-time jump diffusion expressed in stochastic differential
equation (sde) form,
dp(t) = ( µ + $F2(t) ) dt + F(t) dW(t) + 6(t) dq(t) , 0#t#T,
where F(t) is a strictly positive continuous sample path process (a.s.), W(t) denotes a standard
Brownian motion, q(t) is a pure jump process with q(t)=1 corresponding to a jump at time t and
q(t)=0 otherwise, while 6(t) refers to the size of the corresponding jumps. We assume the jump
size distribution has a constant mean of µ6 and variance of F6
2. Finally, the jump intensity is
assumed constant (and finite) at a rate 8 per unit time. In the notation of Proposition 1, we then
have the return components,
µ(t) = µc(t) = µ@ t + $ I0
t F2(s) ds + 8@ µ6@ t ,
Mc(t) = I0
t F(s) dW(s) ,
MJ(t) = E0#s#t 6 (s) q(s) - 8@ µ6@ t .
Notice that the last term of the mean representation captures the expected contribution coming
from the jumps, while the corresponding term is subtracted from the jump innovation process to
provide a unique (compensated) jump martingale representation for MJ.
EXAMPLE 2: Discrete-Time Stochastic Volatility (ARCH) Model
Consider the discrete-time (jump) process for p(t) defined over the unit time interval,
p(t) = p(t-1) + µ + $ F2(t) + F(t) z(t) , t = 1, 2, ..., T,
where (implicitly),
p(t+J) / p(t) ,
t = 0, 1, ..., T-1,
and z(t) denotes a martingale difference sequence with unit variance, while F(t) is a (possibly
latent) positive (a.s.) stochastic process that is measurable with respect to öt-1. Of course, in this
situation the continuous-time no-arbitrage arguments based on infinitely many long-short
positions over fixed length time intervals are no longer operative. Nonetheless, we shall later
argue that the orders of magnitude of the corresponding terms in the decomposition in
Proposition 1 remain suggestive and useful from an empirical perspective. In fact, we may still
think of the price process evolving in continuous time, but the market being open and prices
observed only at discrete points in time. In this specific model we have, for t = 1, 2, ..., T,
µ(t) = µ@ t + $ Es=1,...,t F2(s) ,
M(t) / MJ(t) = Es=1,...,t F(s) z(s) .
A final comment is in order. We purposely express the price changes and associated returns in
Proposition 1 over a discrete time interval. The concept of an instantaneous return employed in
the formulation of continuous-time models, that are given in sde form (as in Example 1 above), is
pure short-hand notation that is formally defined only through the corresponding integral
representation, such as equation (2.6). Although this is a technical point, it has an important
empirical analogy: real-time price data are not available at every instant and, due to pertinent
microstructure features, prices are invariably constrained to lie on a discrete grid, both in the
price and time dimension. Hence, there is no real-world counterpart to the notion of a continuous
sample path martingale with infinite variation over arbitrarily small time intervals (say, less than
a second). It is only feasible to measure return (and volatility) realizations over discrete time
intervals. Moreover, sensible measures can typically only be constructed over much longer
horizons than given by the minimal interval length for which consecutive trade prices or quotes
are recorded. We return to this point later. For now, we simply note that our main
conceptualization of volatility in the next section conforms directly with the focus on realizations
measured over non-trivial discrete time intervals rather than vanishing, or instantaneous, interval
2.2. Notional, Expected, and Instantaneous Volatility
This section introduces three distinct volatility concepts that serve to formalize the process of
measuring and modeling volatility within our frictionless, arbitrage-free setting.
By definition volatility seeks to capture the strength of the (unexpected) return variation over a
given period of time. However, two distinct features importantly differentiate the construction of
all (reasonable) volatility measures. First, given a set of actual return observations, how is the
realized volatility computed? Here, the emphasis is explicitly on ex-post measurement of the
volatility. Second, decision making often requires forecasts of future return volatility. The focus
is then on ex-ante expected volatility. The latter concept naturally calls for a model that may be
used to map the current information set into a volatility forecast. In contrast, the (ex-post)
realized volatility may be computed (or approximated) without reference to any specific model,
thus rendering the task of volatility measurement essentially a nonparametric procedure.
It is natural first to concentrate on the behavior of the martingale component in the return
decomposition (2.6). However, a prerequisite for observing the M(t) process is that we have
access to a continuous record of price data. Such data are simply not available and, even for
extremely liquid markets, microstructure effects (discrete price grids, bid-ask bounce effects,
etc.) prevent us from ever getting really close to a true continuous sample path realization.
Consequently, we focus on measures that represent the (average) volatility over a discrete time
4 Of course, by choosing the interval very small, one may in principle approximate the notion of point-intime volatility, as discussed further below.
-8interval, rather than the instantaneous (point-in-time) volatility.4 This, in turn, suggests a natural
and general notion of volatility based on the quadratic variation process for the local martingale
component in the unique semi-martingale return decomposition.
Specifically, let X(t) denote any (special) semi-martingale. The unique quadratic variation
process, [X,X]t , t0[0,T], associated with X(t) is then formally defined by
[X,X]t / X(t)2 - 2 I0
t X(s- ) dX(s), 0<t#T,
where the stochastic integral of the adapted càglàd process, X(s- ), with respect to the càdlàg
semi-martingale, X(s), is well-defined . It follows directly that the quadratic
variation, [X,X], is an increasing stochastic process. Also, jumps in the sample path of the
quadratic variation process necessarily occur concurrent with the jumps in the underlying semimartingale process, )[X,X] =()X)2.
Importantly, if M is a locally square integrable martingale, then the associated (M2 - [M,M])
process is a local martingale,
E[ M(t,h)2 - ( [M,M]t - [M,M]t-h ) | öt-h ] = 0, 0 < h # t # T.
This relation, along with the following well-known result, provide the key to the interpretation of
the quadratic variation process as one of our volatility measures.
PROPOSITION 2 - Theory of Quadratic Variation
Let a sequence of possibly random partitions of [0,T], (Jm ), be given s.t. (Jm ) / {Jm,j }j$0 , m =
1,2, ... where Jm,0 # Jm,1 # Jm,2 # ... satisfy, with probability one, for m 6 4 ,
Jm,0 6 0 ; supj$1 Jm,j 6 T; supj$0 (Jm,j+1 - Jm,j ) 6 0 .
Then, for t 0 [0,T],
limm64 { Ej$1 ( X(tvJm,j) - X(tvJm,j-1) )2 } 6 [X,X]t ,
where tvJ / min (t,J), and the convergence is uniform in probability.
Intuitively, the proposition says that the quadratic variation process represents the (cumulative)
realized sample path variability of X(t) over the [0,t] time interval. This observation, together
-9with the martingale property of the quadratic variation process in (2.11), immediately point to the
following theoretical notion of ex-post return variability.
DEFINITION 1 - Notional Volatility
The Notional Volatility over [t-h,t], 0<h # t # T, is
L2(t,h) / [M,M]t - [M,M]t-h = [Mc,Mc]t - [Mc,Mc]t-h + Et-h<s#t )M2(s) .
This same volatility concept has recently been highlighted in a series of papers by Andersen,
Bollerslev, Diebold and Labys and Barndorff-Nielsen and Shephard . The
latter authors term the corresponding concept Actual Volatility.
Under the maintained assumption of no predictable jumps in the return process, and noting that
the quadratic variation of any finite variation process, such as µc(t), is zero, we also have
L2(t,h) / [r,r]t - [r,r]t-h = [Mc,Mc]t - [Mc,Mc]t-h + Et-h<s#t )r2(s) .
Consequently, the notional volatility equals (the increment to) the quadratic variation for the
return series. Equation (2.13) and Proposition 2 also suggest that (ex-post) it is possible to
approximate the notional volatility arbitrarily well through the accumulation of ever finely
sampled high-frequency squared return, and that this approach remains consistent independent of
the expected return process. We shall return to a much more detailed analysis of this idea in our
discussion of nonparametric ex-post volatility measures in section 4 below.
Similarly, from (2.13) and Proposition 2 it is evident that the notional volatility, L2(t,h), directly
captures the sample path variability of the log-price process over the [t-h,t] time interval. In
particular, the notional volatility explicitly incorporates the effect of (realized) jumps in the price
process: jumps contribute to the realized return variability and forecasts of volatility must
account for the potential occurrence of such jumps. It also follows, from the properties of the
quadratic variation process, that
E[L2(t,h)|öt-h ] = E[M(t,h)2 |öt-h ] = E[M2(t)|öt-h ] - M2(t-h),
Hence, the expected notional volatility represents the expected future (cumulative) squared return
innovation. As argued in section 2.1, this component is typically the dominant determinant of
the expected return volatility.
For illustration, consider again the two examples introduced in section 2.1 above. Alternative
more complicated specifications and issues related to longer horizon returns are considered in
section 3.
EXAMPLE 1: Stochastic Volatility Jump Diffusion with Non-Zero Mean Jumps (Revisited)
The log-price process evolves according to
dp(t) = ( µ + $F2(t) ) dt + F(t) dW(t) + 6(t) dq(t) , 0#t#T.
The notional volatility is then
L2(t,h) = I0
h F 2(t-h+s) ds + Et-h<s#t 62(s) .
The expected notional volatility involves taking the conditional expectation of this expression.
Without an explicit model for the volatility process, this cannot be given in closed form.
However, for small h the (expected) notional volatility is typically very close to the value
attained if volatility is constant. In particular, to a first-order approximation,
E[L2(t,h)|öt-h ] . F 2(t-h) · h + 8 · h ·( µ6
2 ) . [F 2(t-h) + 8 ( µ6
2 ) ] · h,
m(t,h) . [ µ + $ ·F 2(t-h) + 8 ·µ6 ] · h .
Thus, the expected notional volatility is of order h, the expected return is of order h (and the
variation of the mean return of order h2 ), whereas the martingale (return innovation) is of the
order h1/2, and hence an order of magnitude larger for small h.
EXAMPLE 2: Discrete-Time Stochastic Volatility (ARCH) Model (Revisited)
The discrete-time (jump) process for p(t) defined over the unit time interval is
p(t) = p(t-1) + µ + $ F2(t) + F(t) z(t) , t = 1, 2, ..., T.
The one-period notional volatility measure is then
L2(t,1) = F 2(t) z2(t),
while, of course, the expected notional volatility is
E[L2(t,1)|öt-1 ] = F 2(t),
and the expected return is
m(t,1) = µ + $ F2(t).
Although direct comparison of the order of magnitude of the mean return relative to the return
innovation is not feasible here, empirical studies based on daily and weekly returns invariably
find the mean parameters, such as µ and $, to be very small relative to the expected notional
volatility, F 2(t). In practice, the contribution of innovations in the mean process to the overall
return variability is negligible over such horizons, as also suggested by Example 1.
It is obvious that - when volatility is stochastic - the ex-post (realized) notional volatility will not
correspond to the ex-ante expected volatility. More importantly, equation (2.9) implies that even
the ex-ante expected notional volatility generally is not identical to the usual notion of return
volatility as an ex-ante characterization of future return variability over a discrete holding period.
The fact that the latter quantity is highly relevant for financial decision making motivates the
standard discrete-time expected volatility concept defined below.
DEFINITION 2 - Expected Volatility
The Expected Volatility over [t-h,t], 0<h # t # T, is defined by
h 2(t,h) / E[ {r(t,h) - E(µ(t,h)|öt-h )}2 |öt-h ]
= E[ { r(t,h) - m(t,h) }2 |öt-h ].
If the µ(t,h) process is not measurable with respect to öt-h , the expected volatility will typically
differ from the expected notional volatility in equation (2.14). Specifically, the future return
variability in equation (2.15) reflects both genuine return innovations, as in equation (2.14), and
intra-period innovations to the conditional mean process. Trivially, however, for models with an
assumed constant mean return, or for one-period-ahead discrete-time volatility forecasts with
given conditional mean representation (as in Example 2 above), the two concepts coincide.
Continuous-time models often portray the volatility process as perpetually evolving. From this
perspective, the focus on volatility measurement over a fixed interval length, h, is ultimately
arbitrary. A more natural theoretical concept is provided by the expected instantaneous
volatility, measured as the current strength of the volatility process expressed per unit of time,
limh60 h2(t,h)/h = limh60 [ E { ( [M,M]t - [M,M]t-h ) / h} |öt-h ].
This is especially true when the underlying logarithmic price path is continuous, i.e., MJ(t)/ 0, in
which case the (scaled) notional and expected instantaneous volatilities coincide,
limh60 h2(t,h)/h = limh60 L2(t,h)/h = limh60 [ E { ( [Mc,Mc]t - [Mc,Mc]t-h ) / h } |öt-h ].
Inspired by these results, we adopt the corresponding definition of instantaneous volatility.
5 A definition of instantaneous volatility similar to equation (2.17) is suggested by Comte and Renault in
their discussion of alternative inference procedures for a continuous-time long-memory volatility model.
DEFINITION 3 - Instantaneous Volatility
The Instantaneous Volatility at time t, 0 # t # T, is
2 / limh60 [ E { ( [Mc,Mc]t - [Mc,Mc]t-h ) / h} |öt-h ].
This definition is consistent with the terminology commonly employed in the literature on
continuous-time parametric stochastic volatility models.5 Barndorff-Nielsen and Shephard
 , in a slightly different setting, refer to the corresponding concept as Spot Volatility.
Although the instantaneous volatility is a natural concept, and we refer to it frequently below,
practical volatility measurement invariably takes place over discrete time intervals. In the sequel
we stress the general relationship between the various volatility concepts for alternative
parametric volatility models and nonparametric volatility measurements.
2.3. Volatility Modeling and Measurement
This section provides a qualitative overview of the different approaches to volatility modeling
and measurement. Specific characterization of the volatility estimates and measurements are
postponed to the following sections, where we present a more detailed study for each major class
of models. The approaches for empirically quantifying volatility naturally falls into two separate
categories, namely procedures based on estimation of parametric models and more direct
nonparametric measurements. Within the parametric volatility classification, alternative models
exploit different assumptions regarding the expected volatility, h2(t,h), through distinct
functional forms and the nature of the variables in the information set, öt-h . In contrast, the datadriven or nonparametric volatility measurements typically quantify the notional volatility, L2(t,h),
directly. Both set of procedures differ importantly in terms of the choice of time interval for
which the volatility measure applies, e.g., a discrete interval, h>0, or a point-in-time
(instantaneous) measure, obtained as the limiting case for h6 0.
Within the discrete-time parametric models, the most significant distinction concerns the
character of the variables in the information set, öt-h , which in turn governs the type of
estimation and inference techniques that are required for their practical implementation. In the
ARCH class of models, the expected volatility, h2(t,h), is parameterized as a function of past
returns only, or Ft-h , although other observable variables could easily be included in öt-h. In
contrast, the parameterized expectations in the Stochastic Volatility (SV) class of models
explicitly rely on latent state variables. As we move to continuous-time parametric
representations of either model, the assumption that all past returns are observable implies that
the distinction between the two classes of models effectively vanishes, as the latent volatility
state variables may be extracted without error from the frictionless, continuous-time price record.
6 Most prominent among these procedures are, of course, the Black-Scholes option implied volatilities based on
the assumption of an underlying continuous-time Random Walk model first analyzed empirically by Latané and
Rendleman . More detailed empirical analyses of Black-Scholes implied volatilities along with generalizations to
allow for more realistic price dynamics have been the subject of an enormous literature, an incomplete list of which
includes Bakshi, Cao and Chen , Canina and Figlewski , Chernov and Ghysels , Christensen and
Prabhala , Day and Lewis , Duan , Dumas, Fleming and Whaley , Fleming , Heston
 , Heston and Nandi , Hull and White , and Wiggins .
7 The 30-day VIX implied volatility index for the S&P100 and the more recent VXN index for the NASDAQ-
100 traded on the Chicago Board Options Exchange (CBOE) are based on weighted averages of Black-Scholes put and
call implied volatilities; see Fleming, Ostdiek and Whaley and Whaley for further discussion. A similar
construct underlies the 45-day VDAX for the DAX index on the Deutsche Termin Börse (DTB).
The following definition formalizes these categorizations.
DEFINITION 4 - Parametric Volatility Models
Discrete-Time Parametric Volatility Models explicitly parameterize the expected volatility,
h2(t,h), h>0, as a non-trivial function of the time t-h information set, öt-h. In the ARCH class of
models, öt-h depends on past returns and other directly observable variables only. In the
Stochastic Volatility (SV) class of models öt-h explicitly incorporates past returns as well as
latent state variables. Continuous-Time Volatility Models provide an explicit parameterization of
the instantaneous volatility, Ft
2 , as a (non-trivial) function of the öt information set, with
additional volatility dynamics possibly introduced through time variation in the process
governing jumps in the price path.
In addition to these three separate model classes, so-called Implied Volatility approaches also
figure prominently in the literature. The implied volatilities are based on a parametric volatility
model for the returns, as defined above, along with an asset pricing model and an augmented
information set consisting of options prices and/or term structure variables. Intuitively, if the
number of derivatives prices at time t-h pertaining to the price of the asset at time t included in
the augmented information set, öt-h , exceeds the number of latent state variables in the
parametric model for the returns, it is possible to back out a value for h2(t,h) by inverting the
theoretical asset pricing model.6,7 Importantly, these procedures all depend (implicitly) on
specific assumptions about the price of volatility risk. The statistical measurements surveyed in
this chapter are void of any such assumptions; see e.g., Bates , Renault and the
chapters by Garcia, Ghysels, and Renault , and Piazzesi in this Handbook for a
discussion of the extensive literature on options implied volatilities and related procedures.
In contrast to the parametric procedures categorized above, the nonparametric volatility
measurements are generally void of any specific functional form assumptions about the
stochastic process(es) governing the local martingale, M(t), as well as the predictable and finite
variation process, µ(t), in the unique return decomposition. These procedures also differ
importantly from the parametric models in their focus on providing measures of the notional
-14volatility, L2(t,h), rather than the expected volatility, h2(t,h). In addition, the nonparametric
procedures generally restrict the measurements to be functions of the coarser filtration, Ft ,
generated by the return on the asset only. In parallel to the parametric measures, the
nonparametric procedures may be further differentiated depending upon whether they let h60
and thus provide measures of instantaneous volatility, or whether they explicitly operate with a
strictly positive h>0 resulting in realized volatility measures over a discrete time interval.
DEFINITION 5 - Nonparametric Volatility Measurement
Nonparametric Volatility Measurement utilizes the ex-post returns, or FJ , in extracting measures
of the notational volatility. ARCH Filters and Smoothers rely on continuous sample paths, or
MJ/0, in measuring the instantaneous volatility Ft
2 = limh60 h2(t+h,h)/h. The filters only utilize
information up to time J=t, while the smoothers are based on J>t. Realized Volatility measures
directly quantify the notional volatility,L2(t,h), over (non-trivial) fixed-length time intervals, h>0.
Within the class of instantaneous volatility measures, the ARCH filters rely only on the past
return record, typically through a weighted rolling regression, while the smoothers, or two-sided
filters, also exploit (ex-post) the future price record. Realized volatility approaches may
similarly be categorized according to whether the measurement of L2(t,h) exploits only price
observations within the time interval [t-h,t] itself, or filtering/smoothing techniques are used to
also incorporate return observations outside of [t-h,t]. An important advantage of realized
volatility procedures, which exploit only interval specific information, is that they provide
asymptotically unbiased measures, and therefore approximately serially uncorrelated
measurement errors, under quite general conditions. A potential drawback is that useful
information from adjacent intervals is ignored. Consistency of both ARCH filters and smoothers
and realized volatility procedures generally require that the length of the underlying sampling
interval for the returns within the [t-h,t] time interval approaches zero (even for the ARCH filters
and smoothers where h itself is shrinking). We next turn to a more detailed discussion of these
different procedures for modeling and measuring volatility within the context of the general
setup in section 2.1.
3. PARAMETRIC METHODS
Parametric volatility models and their implementation constitute one of the cornerstones of
modern empirical asset pricing, and a large econometrics and statistics literature has been
devoted to the development and theoretical foundation of differently parameterized volatility
models. A thorough review of this literature is beyond the scope of this chapter. Instead, we
shall merely highlight the most important ideas as they relate specifically to the volatility
measurement for the different model classes.
3.1. Discrete-Time Models
8 The information set, öt-h , is (implicitly) restricted to the corresponding discrete-time realizations of the
process along with any other discrete-time (possibly latent) state variables.
Even if trading and pricing evolves in continuous time within the frictionless no-arbitrage setting
outlined in section 2, it is sometimes more convenient to formulate a model for the associated
discrete-time returns. Of course, such an approach is naturally motivated by situations in which
the prices are only observed at regular fixed time intervals (e.g. daily closing prices, end-of-themonth prices). Alternatively, if trading is only feasible at the given discrete points in time, the
relevant return distribution is captured fully by the conditional discrete-time dynamics. Either
perspective allows us to embed the discrete-time ARCH and SV models in our basic continuoustime setting. Hence, for the remainder of this section, we assume that prices are only observed
(and trades only possible) at discrete and equally spaced points in time, t = 0, h, 2·h, ... , T-h, T.
The discrete-time models, at a minimum, assume that the correct specification of the h-stepahead conditional mean and variances is known up to a low-dimensional parameter vector. That
is, the models (parsimoniously) parameterize the first two conditional return moments,8
m(t,h) = E[ r(t,h) |öt-h ] = E[µ(t,h)|öt-h ] = µ(t,h)
h 2(t,h) = E[ (r(t,h) - m(t,h))2 |öt-h ] ,
where m(t,h) and µ(t,h) coincide because the h-step-ahead conditional mean is predictable. Of
course, in general the first two conditional moments of the h-period returns do not fully
characterize the dynamic return distribution, although this is true for continuous sample path
diffusion models (h60), which may be defined completely through the instantaneous drift and
volatility coefficients alone.
The restriction of only observing prices at equidistant points in time is readily interpreted, within
the continuous-time setting, as a pure jump process with known jump times but random jump
sizes. In the notation of the previous section,
)M(t) = r(t,h) - µ(t,h) = r(t,h) - m(t,h),
t = h, ... , T .
As such, it follows directly from the definition of the notional volatility over [t-h,t] that
L2(t,h) / [M,M]t - [M,M]t-h = )M2(t).
Moreover, the expected notional volatility over [t-h,t] simply equals the conditional h-periodahead variance as specified by the model,
h 2(t,h) = E[L2(t,h)|öt-h ] = E[)M2(t)|öt-h ].
This same result is generally not true for multi-period forecasts, or volatilities over longer
horizons, [t-k@ h, t] where k>1. In this situation, any variation in the conditional mean process
-16within the forecast horizon will contribute to the return variation, so the expected notional
volatility typically is not equal to the expected volatility. However, as discussed further below,
the contribution from the variation in the conditional mean will usually only be of second order
importance unless the forecast horizon is very long. This same basic insights carry over to the
continuous-time processes discussed in more detail in section 3.2 below.
In order to more explicitly clarify the relationship between the notional volatility and total return
variability within the multi-period setting, recall the generic return decomposition for a discretetime pure-jump process in Proposition 1,
r(t) = µ(t) + MJ(t) ,
t = 0, h, 2·h, ... , T-h, T,
µ(t) = EJ=1,..,t/h E(r(J·h,h)|ö(J-1)·h) = EJ=1,..,t/h µ(J·h,h) ,
MJ(t) = EJ=1,..,t/h )M(J) = EJ=1,..,t/h (r(J·h,h) - µ(J·h,h)).
Now, for any integer k>0,
µ(t,k·h) = µ(t) - µ(t-k·h) = EJ=1,..,k µ(t-(k-J)·h,h).
This µ(t,k·h) term represents the cumulative conditional h-period-ahead expected return and not
the conditional multi h-period expected return. Specifically, for k>1, the term µ(t,k·h) is
generally not equal to E( r(t,k·h)|öt-k·h ) = m(t,k·h), even though
E[µ(t,k·h)|öt-k·h ] = m(t,k·h).
Hence, we obtain the decomposition of the k-period expected volatility over [t-k·h,t],
h 2(t,k·h) = E[ (r(t,k·h)-m(t,k·h))2 |öt-k·h ]
= E[M2(t,k·h) + (µ(t,k·h)- m(t,k·h))2 + 2·M(t,k·h)·µ(t,k·h)|öt-k·h ]
= Var[ M(t,k·h)|öt-k·h ] + Var[ µ(t,k·h)|öt-k·h ]
+ 2 · Cov[ M(t,k·h), µ(t,k·h)|öt-k·h ]
= E( [M,M]t - [M,M]t-k·h |öt-k·h ) + Var[ µ(t,k·h)|öt-k·h ]
+ 2 · Cov[ M(t,k·h), µ(t,k·h)|öt-k·h ] .
Trivially, as noted above, for the one-period-ahead forecasts, or k=1, there cannot be any withinperiod variability in the conditional mean process, so that the last two terms in (3.10) vanishes,
9 Of course, the exact terms involved in the multiple-period volatility forecasts will depend upon the specific
functional form and the underlying distributional assumptions. Their practical computation may not be trivial, or even
feasible in closed form, necessitating the use of numerical simulation techniques . We shall not
be concerned with these more computationally oriented aspects of the problem in this chapter.
-17and the expected volatility equals the expected notional volatility. However, for multiple-period
forecasts the stochastic evolution of the conditional mean within the interval contributes to the
overall return variability, both through the variation in the conditional mean itself and through
the covariance between the return innovations and future (within forecast horizon) changes in the
conditional mean return. However, the period-by-period conditional mean is generally much
smaller than the volatility, and the shifts in the conditional mean smaller yet. Hence, the
expected notional volatility, or quadratic variation, remains the dominant component for the
multi-period return variability in empirically realistic situations.9
Further, notice that the so-called Leverage Effect impacts only the expected
notional volatility (quadratic variation), and none of the other terms. The hypothesis stipulates a
(negative) correlation between the return innovations, )M, and the size of future return
innovations, ()M)2, essentially predicting a left-skewed distribution for the return innovations.
With no impact on the conditional mean, only the quadratic variation process is affected. The
closely related Volatility Feedback Effect has an impact via the covariance term, but it remains
limited by the size of the shifts in the conditional mean. Again, the hypothesis essentially
implies a leftward skew in the return innovation distribution. Intuitively, given a positive
volatility risk premium, large negative return innovations are magnified while large positive
innovations are dampened due to the increase in the expected future return required to
compensate for a positive and persistent shock to future volatility. Hence, technically, the
volatility feedback tends to raise the expected volatility directly, but it also induces a negative
correlation between the return innovations and the future expected mean returns.
Returning to the basic discrete-time setup, the conditional moments in equations (3.1) and (3.2)
allow for relatively easy and consistent statistical inference concerning the unknown parameters
by a standard Generalized Methods of Moments (GMM) estimator , or for
stochastic volatility and latent state variable(s), a Simulated Method of Moments (SMM) type
estimator . Of course, simple method-of-moments estimators with
ill chosen moment conditions may behave poorly, both asymptotically and in finite samples
 , and much of the literature on discrete-time volatility
models is concerned with the development of more efficient estimation procedures under
auxiliary assumptions. In particular, assuming that the standardized innovations, (r(t,h)-
µ(t,h))/h(t,h), belong to a specific parametric family of distributions, Maximum Likelihood
Estimation (MLE) and corresponding Gaussian Quasi-MLE (QMLE) procedures are both conceptually straightforward to implement for the ARCH models,
while more complicated procedures are required for stochastic volatility models.
Next, we briefly review some of the popular discrete-time parametric volatility models. The key
distinguishing features for each class of models consist of the functional form for the conditional
-18moments in equations (3.1) and (3.2), the variables in the information set öt-h, along with any
additional distributional assumptions. The performance of the different models, such as the fit to
the data and precision of forecasts, as well as the ease of computing parameter estimates and the
various terms in the volatility forecast expressions, depend importantly on these features.
3.1.1. ARCH Models
The ARCH class of models was first introduced in the seminal paper by Engle . It has
since enjoyed unprecedented empirical success along with a myriad of extensions and further
theoretical developments. Indeed, most of our empirical knowledge to date concerning the
temporal dependencies in financial market volatility have arguably been gleaned from estimation
and inference with ARCH type models. Several surveys of this burgeoning literature already
exist , and we will not attempt yet another comprehensive review.
However, it is useful to briefly summarize the key developments and model formulations within
the current framework.
The ARCH class of models differ from the discrete-time stochastic volatility models discussed
below by the assumption that the filtration, öt-h, underlying the parameterized conditional
expectations in equation (3.1) and (3.2) depends exclusively on observable variables. This
assumption greatly facilitates statistical inference vis-a-vis stochastic volatility models, and the
widespread empirical use of ARCH style models in part stems from the ease with which
traditional (quasi-) maximum likelihood based procedures may be implemented.
Any time series model in which the conditional variance depends non-trivially on the time t-h
observable information set is now commonly referred to as an ARCH model. This terminology
is explained by the particular parametric formulation first adapted by Engle . Specifically,
in the so-called ARCH(p) model, h 2(t,h) is parameterized as an autoregressive distributed lag of
p squared innovations,
h 2(t,h) = T + 3j=1,..,p "j(r(t-j@ h,h)-µ(t-j@ h,h))2 / T + "(L,h)(r(t,h)-µ(t,h))2,
where T>0 and "j $0 in order to ensure positivity of h 2(t,h) (a.s.). A more parsimonious
characterization of the intertemporal volatility dependencies is often obtained by the Generalized
ARCH, or GARCH(p,q), model ,
h 2(t,h) = T + 3j=1,..,p "j (r(t-j@ h,h)-µ(t-j@ h,h))2 + 3i=1,..,q $i h 2(t-j@ h,h)
/ T + "(L,h) (r(t,h)-µ(t,h))2 +$(L,h) h 2(t,h).
For the popular GARCH(1,1) model, T>0, "1 $0, and $1$0 obviously guarantees positivity of h
2(t,h). Corresponding conditions for the general case are presented in Nelson and Cao .
Rearranging the terms, the GARCH(p,q) model is readily interpreted as an ARMA model for
[r(t,h)-µ(t,h)]2 in which the autoregressive and moving average polynomials are given by ["(L,h)
+ $(L,h)] and [1 - $(L,h)], respectively. Hence, provided that all the roots of the characteristic
equation, "(x,h)+$(x,h)=1, have norm greater than one, the model is covariance stationary, and
the unconditional h-period (one-period) variance equals E[h 2(t,h)] = T(1-"(1,h)+$(1,h))-1.
Weaker conditions for strict stationarity have been derived by Nelson and Bougerol and
Picard , while higher order moment conditions have recently been developed by Ling and
McAleer .
The leverage effect, briefly discussed earlier, stipulates a negative correlation between current
return innovations and future expected conditional variances. The GJR-GARCH model
 in which the "j coefficients in "(L,h) in equation (3.12)
depend on the sign of the corresponding return innovations, r(t-j@ h,h)-µ(t-j@ h,h), was specifically
designed to accommodate such asymmetries. A similar motivation underlies the EGARCH
model in Nelson . Defining the standardized innovations,
z(t,h) / (r(t,h)-µ(t,h))/h(t,h),
the EGARCH(p,q) model takes the form,
log[h 2(t,h)] = T + "(L,h){2 @ z(t,h) + ( @ [|z(t,h))| - E(|z(t,h))|)]}
+ $(L,h) log[h 2(t,h)],
where as before "(L,h) and $(L,h) denote pth and qth order lag polynomials respectively. The
log-transform complicates the calculation of (unbiased) multi-step conditional variance forecasts,
but conveniently avoids having to impose non-negativity constraints on the parameters.
Obviously, for 2<0 the model predicts a negative relation between current returns and future
conditional variances.
Alternatively, as discussed above asymmetries in the return-volatility relationship may also be
attributed to the so-called volatility feedback effect. This feature is captured by the ARCH-in-
Mean type formulation , in which the functional form for the
conditional mean, µ(t,h), depends explicitly on the conditional variance of the process, h 2(t,h).
Which of these competing specifications is best able to capture the empirically observed
asymmetry in equity return volatility has been the subject of several empirical studies .
Another important empirical finding concerns the strong degree of volatility persistence
estimated with most daily and weekly financial rates of return. This is manifest by the
autoregressive polynomials in the GARCH(p,q) formulations, 1-"(x,h)-$(x,h), and the EGARCH
formulations, 1-$(x,h), having (their largest) roots very close to unity. The IGARCH model of
Engle and Bollerslev directly imposes this condition; i.e., "(1,h)+$(1,h)=1. However, the
imposition of a unit-root in the conditional variance arguably exaggerates the true dynamic
-20dependencies, and several alternative long-memory, or fractionally integrated, ARCH type
formulations have recently been estimated and analyzed more formally in the literature .
Possible explanations for the apparent long-memory dependencies based on the aggregation of
multiple volatility components and/or stochastic regime switching models have been explored by
Andersen and Bollerslev , Diebold and Inoue , and Liu among others . This remains a very active area of
current research.
Our focus in this chapter has been almost exclusively univariate. Nonetheless, most interesting
questions in asset pricing finance and risk management call for a multivariate framework
involving not just conditional variances but also time-varying conditional covariances. From a
conceptual view point, the extension of the univariate ARCH class of models to a multivariate
setting presents few new issues. However, conditions to ensure that the parameterized
conditional covariance matrices are positive definite (a.s.) and involve only a manageable (small)
number of parameters are both important considerations from a practical perspective. In the
Diagonal GARCH model of Bollerslev, Engle and Wooldridge the conditional variances
and covariances are parameterized as univariate GARCH(p,q) processes; i.e., the ijth element in
the conditional covariance matrix depends on a distributed lag of past values of the same element
and the cross-products of the corresponding innovations. The related BEKK GARCH
formulation guarantees that the covariance matrices are positive
definite. The constant conditional correlation model in Bollerslev is empirically among
the most frequently applied multivariate ARCH models. This model has recently been extended
to incorporate parsimoneously parameterized time-varying conditional correlations by Engle
 . Other multivariate formulations that allow for relatively easy implementation in large
dimensions include the Flexible GARCH model of Ledoit, Santa-Clara and Wolf and the
R-GARCH model in Gallant and Tauchen .
Most industry applications entailing large scale covariance matrix measurements rely on J.P.
Morgan’s RiskMetrics . The RiskMetrics procedure is based on exponential
smoothing, and as such corresponds directly to a Diagonal IGARCH(1,1) model in which all of
the intercepts in the conditional covariance matrix are fixed at zero and identical values of " and
$/1-" are employed across all assets. The use of the same smoothing parameter ($ = 0.94 with
daily data) obviously facilitates the implementation and automatically guarantees that the
covariance matrix measurements are positive definite. Nonetheless, when viewed as a data
generating process as opposed to a filter, the RiskMetrics procedure is formally degenerate
 .
One major theoretical drawback to the GARCH class of models concerns their lack of closedform aggregation. This is true both inter-temporally and cross-sectionally. For example, if daily
asset returns follow a univariate GARCH(p,q) model, the corresponding weekly returns are not
GARCH(p,q). Similarly, if a collection of asset returns follow a multivariate GARCH(p,q)
model, (non-trivial) portfolio returns are not GARCH(p,q). The Weak GARCH class of models
-21was explicitly introduced by Drost and Nijman and Nijman and Sentana to address
this issue. In a Weak GARCH model, h2(t,h) has the interpretation of a parameterized linear
projection for the squared innovation. In contrast to the conditional expectations underlying the
standard ARCH formulations, the linear projections are closed under temporal (and in the
multivariate case cross-sectional) aggregation. However, the linear projections do not easily
translate into the volatility concepts in section 2 and, as emphasized by Meddahi and Renault
 , asset pricing relationships are based on conditional expectations as opposed to
linear projections. Thus, even though the difference between the linear projections and the true
conditional expectations may be numerically small in empirical realistic situations, this limits the
applicability and the formal interpretation of the Weak GARCH class of models. The discretetime Square-Root Stochastic Autoregressive Volatility (SR-SARV) models provide an
alternative formulation which circumvent these problems. We next turn to a discussion of this
and other discrete-time stochastic volatility models.
3.1.2. Stochastic Volatility Models
The stochastic volatility models differ from the ARCH class of models in that the information
set, öt-h , underlying the conditional expectations in equation (3.1) and (3.2) is not directly
measurable with respect to the time t-h observable filtration. This is typically the result of the
inclusion of two separate stochastic innovations - one innovation term relating the conditional
mean of the process to the actually observed return, a second innovation relating the latent
volatility process to its conditional mean. This type of formulation is typically motivated by the
Mixture-of-Distributions Hypothesis (MDH) and the idea of a latent information arrival process.
The MDH was originally put forth by Clark as a way of conceptualizing the distributional
characteristics of speculative returns, and the basic hypothesis has subsequently been extended
and analyzed empirically by Epps and Epps , Taylor , Tauchen and Pitts ,
Andersen , Andersen and Bollerslev , Ané and Geman , among many others,
to allow for more realistic temporal dependencies in the underlying latent information arrival
process(es). We shall return to a discussion of these ideas in section 4.2 below. The actual
parameterizations of the most popular discrete-time stochastic volatility models are often
rationalized through the discretization of specific continuous-time stochastic volatility models.
We do not provide an exhaustive review of the pertinent discrete-time stochastic volatility class
of models here, but simply refer to the excellent surveys offered in Taylor , Shephard
 , and Ghysels, Harvey and Renault .
In parallel to the GARCH and EGARCH class of models discussed above, most of the parametric
stochastic volatility models employed in the literature are based on an autoregressive formulation
for a continuous function of the (now) latent volatility process,
ƒ[h 2(t,h)] = T + $(L,h)ƒ[h 2(t,h)] + u(t,h),
where $(L,h) denotes a pth order distributed lag polynomial, and u(t,h) is a martingale difference
sequence; i.e., E[u(t,h)|öt-h ]=0. This class of models is commonly referred to as a Stochastic
10 Formal conditions under which the u(t,h) term in the autoregressive formulation cannot be integrated out of
the conditional expectations in (3.1) and (3.2), resulting in a genuine stochastic volatility model, are presented in
Andersen .
11 This terminology derives from Andersen who parameterizes an AR(1) model for ƒ -1[h(t,h)].
Similarly, the lognormal stochastic volatility model is sometimes referred to as an Exponential SARV model.
12 This motivates the extension of the QMLE procedure for the lognormal stochastic volatility model to a non-
Gaussian state space in Kim, Shephard and Chib .
AutoRegresive Volatility, or SARV(p), model. Intuitively, it is the innovation term, u(t,h),
which distinguishes the stochastic volatility from the ARCH class of models.10 Of course,
analogous to the GARCH class of models discussed above, for the SARV(p) model in (3.15) to
be well defined h 2(t,h) must be positive (a.s.). Depending on the functional form for ƒ(@ ), this
restricts the admissible parameters in $(L,h) and/or the support of u(t,h). Most of the models
estimated in the literature have included only a single lag in the $(L,h) polynomial. Conditions
to ensure ergodicity and stationarity for the general SARV(1) model are presented in Andersen
 . The two leading cases are given by the lognormal stochastic autoregressive volatility
model in which ƒ(x)/ log(x) and u(t,h) is assumed to be Gaussian, and the square-root11, or SR-
SARV, model corresponding to ƒ(x)/ x.
The lognormal stochastic volatility models was first analyzed by Taylor and subsequently
popularized in influential papers by Harvey, Ruiz and Shephard and Jacquier, Polson and
Rossi . The logarithmic volatility model arises naturally from the standard return
formulation, r(t,h) = µ(t,h) + h(t,h)@ z(t,h), in which z(t,h) is an i.i.d. mean zero, unit variance,
white noise process. Rearranging the terms, squaring both sides, and taking logarithms, it
follows that
y(t,h) / log[r(t,h) - µ(t,h)] 2 = log[h 2(t,h)] + log[z(t,h)2].
Assuming the mean to be known, this may be interpreted as the measurement equation in a state
space representation of the model, with corresponding transition equation defined by the
parametric model for log[h2(t,h)]. In particular, for the lognormal SARV(1) model,
log[h2(t,h)] = T + $ @ log[h 2(t-h,h)] + u(t,h).
In this situation, filtered and smoothed measurements of the latent log[h 2(t,h)] volatility process
are readily available by linear Kalman filtering which, as pointed out by Nelson and
Harvey, Ruiz and Shephard , in turn allows for relatively easy to compute Gaussian
QMLE parameter estimates. Of course, the innovations in the measurement equation will
generally not be Gaussian, so the Kalman may result in poor measurements of the latent volatility
state variable and correspondingly highly inefficient parameter estimates.12
The lognormal SARV(1) formulation may also be justified as a discrete-time approximation to
the popular Ornstein-Uhlenbeck diffusion for the logarithmic instantaneous volatility,
-23dlog(F2(t)) = - $ (log(F2(t)) - ") dt + R dV(t) ,
where V(t) denotes a Standard Brownian Motion, or Wiener Process. We will return to a more
detailed discussion of this and other continuous-time parametric stochastic volatility models in
section 3.2.1 below. For now note that by a standard Euler scheme, the discrete-time version of
the model in (3.18) takes the form
log[h2(t,h)] = log[h2(t-h,h)] - h @ $ @ {log[h2(t-h,h) - "] + h1/2 @ R @ [V(t,h) - V(t-h,h)].
The actual parameterization is, of course, different from the model in equation (3.17), but the
structure corresponds exactly to that of the lognormal SARV(1) model. Interestingly, the
continuous-time Ornstein-Uhlenbeck process in equation (3.18) also has the interpretation of
being the diffusion limit of the discrete-time EGARCH model, in the sense that a sequence of
appropriately parameterized EGARCH(1,1) models (as discussed in section 4.1 below)
converges weakly to this model as the length of the sampling interval, h, approaches zero.
Although the latent logarithmic volatility in (3.17) takes the form of an AR(1) model, this
translates into an ARMA(1,1) correlation structure for the demeaned logarithmic returns in
(3.16), y(t,h). Moreover, following Taylor and Harvey this same approximate
correlation structure is present for any positive power transform of the squared returns; i.e.,
exp[y(t,h)] c for c>0. Hence, the shape of the autocorrelogram for the squared returns from the
log-normal SARV(1) model mimics that of the empirically popular GARCH(1,1) model.
The second leading class of stochastic volatility models is given by the SR-SARV(p) model.
Following Meddahi and Renault , the SR-SARV(p) model for h 2(t,h) is naturally defined
by the marginalization of a p-dimensional latent VAR(1) process. As emphasized by Meddahi
and Renault , this class of models has the advantage of being closed under temporal
(and in the multivariate setting cross sectional) aggregation. To appreciate this result, suppose
that the true underlying continuous-time volatility is determined by the Constant Elasticity of
Variance (CEV) diffusion (see section 3.2.1 below for further details),
dF2(t) = (T - 2@F2(t))dt + %2·"·(F2(t))*
where * $½ in order to ensure that the process for F2(t) is stationary and non-negative. By
standard arguments the exact discretization of the process then adheres to the basic SR-SARV(1)
model structure,
h 2(t,h) = T + $@h 2(t-h,h) + u(t,h) ,
where E[u(t,h)|öt-h ]=0. Of course, the h-period time interval is arbitrary, so that the expected
volatility for the temporally aggregated process, h 2(t·k,h·k), where k>1 and t = 0, 1, 2 ... , must
be governed the same AR(1) model structure. As discussed further in section 4.1 below, the
CEV model in (3.19) with *=1 may also be interpreted as the diffusion limit of the GARCH(1,1)
The discrete-time AR(1) formulations in (3.17) and (3.20) are, of course, somewhat restrictive.
In parallel to the developments within the parametric ARCH class of models discussed above,
long-memory, or fractionally integrated stochastic volatility models, better suited at capturing the
apparent long-run dependencies in the volatility, have recently been estimated by Breidt, Crato
and de Lima and Harvey . In a related context, Comte and Renault have
proposed a continuous-time stochastic volatility model with discrete-time long-memory
implications.
Direct extensions of the univariate discrete-time stochastic volatility models discussed above to a
multivariate setting was first explored by King, Sentana and Wadhwani , while earlier
work by Diebold and Nerlove employed a related univariate latent ARCH factor structure
in parameterizing time-varying conditional covariances. Recent proposals to allow for more
flexible inference and the estimation of large-dimensional systems include Jacquier, Polson and
Rossi and Chib, Nardari and Shephard .
A more detailed discussion of statistical inference procedures for the stochastic volatility class of
models (both univariate and multivariate) is provided by other chapters in the Handbook as well
as the aforementioned surveys by Ghysels, Harvey and Renault and Shephard .
Intuitively, because of the latent information structure, any inference procedure must either rely
on a (potentially noisy) proxy for the latent volatility or integrate out the latent stochastic
variable(s) from the model. In addition to the method of moments, GMM, and SMM estimators
implemented by Taylor , Melino and Turnbull , and Duffie and Singleton ,
respectively, and the Gaussian QMLE procedures in Harvey, Ruiz and Shephard briefly
discussed above, other noteworthy developments include the Bayesian Markov Chain Monte
Carlo (MCMC) method in Jacquier, Polson and Rossi and Kim, Shephard and Chib
 , the Efficient Method of Moments (EMM) procedure implemented by Gallant, Hsieh and
Tauchen , the simulated maximum likelihood technique of Danielsson , the Monte
Carlo maximum likelihood approach of Sandmann and Koopman , the direct maximum
likelihood estimation through recursive numerical integration in Fridman and Harris , and
the recent eigenfunction expansion of the latent volatility process in Meddahi .
Importantly, from the perspective of volatility measurements, many of these procedures in turn
result in (approximately optimal) filtered and/or smoothed measurements of the functional latent
volatility process, ƒ[h 2(t,h)], conditional on the underlying parametric model and the observable
information.
Optimal measurements may in theory be obtained by non-linear filtering and smoothing
procedures . However, the direct implementation of such procedures
involves high-dimensional integration which is generally prohibitively expensive from a
computational point of view . Recent advances along
these lines to allow for the practical numerical calculation and extraction of latent volatility
measurements include the particle filters in Pitt and Shephard and the reprojection
13 In a related context, Alizadeh, Brandt and Diebold and Gallant, Hsu and Tauchen have
recently proposed the use of intraperiod high-low, or range, data for more efficiently extracting discrete-time stochastic
volatility measures.
-25approach advocated by Gallant and Tauchen .13 We next turn to a discussion of analogous
volatility modeling and measurement procedures based on continuous-time parametric
formulations.
3.2. Continuous-Time Models
Much of the theoretical asset pricing literature is cast in continuous time. Within this tradition,
the sample path of the price process is also commonly assumed to be continuous. This approach
is convenient because the representation in Proposition 1 then ensures that - locally - the mean
and variance are of the same order. Consequently, the framework effectively involves a dynamic
mean-variance trade-off, which typically allows for a tractable analysis of asset pricing and
portfolio choice problems. On the other hand, we usually do not observe a record of
continuously evolving asset prices, and all but the very simplest specifications tend to imply
intractable conditional return distributions for the corresponding discretely observed returns.
This issue has historically inhibited empirical work on estimation and inference for realistic
continuous-time asset price processes, although a burst of research activity in this area over the
last few years has allowed important headway to be made. As a result, the parametric approach
to continuous-time modeling is beginning to have a practical impact on return volatility
modeling. We will not discuss estimation and inference techniques for this class of model in any
detail, however, but rather outline the conceptual issues that distinguish this approach from the
discrete-time modeling approached discussed above and the nonparametric volatility
measurement discussed subsequently. Other chapters in this Handbook offer extensive coverage
of parametric and semi-(non)parametric estimation techniques for diffusion processes 
The continuous-time parametric models are directly compatible with the no-arbitrage framework
outlined in section 2, so the specific volatility concepts carry over without modification.
However, the specifications of the models traditionally adapted in the literature differ from the
general semi-martingale representation in section 2, as the models typically are expressed (in
short-hand format) as stochastic differential equations driven by underlying Brownian motions
and - in the case of discontinuities - Poisson jump processes.
Before exploring the manifestation of the different volatility concepts for some of the more
popular model specifications, it is instructive to briefly reiterate the relationship between the
concepts of expected notional volatility and expected volatility within a multi-period setting. Of
course, for a continuous-time model, any volatility forecast over a discrete time interval
invariably entails multi-period considerations (typically a continuum). Calculations similar to
those presented in equation (3.10) , show that the expected volatility may generally be expressed as
h 2(t,h) = E[ (r(t,h) - m(t,h))2 |öt-h ]
= E[L2(t,h)|öt-h ] + Var[ µ(t,h)|öt-h ] + 2·Cov[M(t,h), µ(t,h)|öt-h ].
Again, the total expected return volatility involves the expected notional volatility (quadratic
variation) as well as two terms induced by future within-forecast-period variation in the
conditional mean. The intuition is identical to the one for the discrete-time setting discussed in
section 3.1. The random variation in the mean component is a direct source of future return
variation, and covariation between the return and conditional mean innovations will further
impact the return variability. However, under standard conditions and moderate forecast
horizons, the dominant factor is indisputably the expected notional volatility, as the innovations
to the mean return process generally will be very small relative to the cumulative return
innovations. Importantly, this does not rule out asymmetric effects from current return
innovations to future return volatility, as the leverage and volatility feedback effects operate,
respectively, exclusively or primarily through the impact on the notional volatility process.
3.2.1. Continuous Sample Path Diffusions
The continuous-time models in the theoretical asset and derivatives pricing literature frequently
assume that the sample paths are continuous, with the corresponding diffusion processes given in
the form of stochastic differential equations (as in Example 1 in section 2 above), rather than
through (abstract) integral representations for continuous sample path semi-martingales along the
lines of Proposition 1. This does not involve any loss of generality, as illustrated by the
following well-known result .
PROPOSITION 3 - Martingale Representation Theorem
For any univariate, square-integrable, continuous sample path logarithmic price process, which is
not locally riskless, there exists a representation such that for all 0 # t # T, a.s.(P),
r(t,h) = µ(t,h) + M(t,h)
h µ(t-h+s) ds + I0
h F(t-h+s) dW(s),
where µ(s) is an integrable, predictable and finite variation stochastic process, F(s) is a strictly
positive càglàd stochastic process satisfying
h F 2(t-h+s) ds < 4 ] = 1,
and W(s) is a standard Brownian motion.
The integral representation (3.22) is equivalent to the standard (short-hand) sde specification for
-27the logarithmic price process,
dp(t) = µ(t) dt + F(t) dW(t), 0#t#T.
Hence, within the class of continuous sample path semi-martingale (diffusion) models, there are
no consequential restrictions involved in stating the model directly through a stochastic
differential equation. In accordance with Definition 3 in section 2, the volatility coefficient
process in this formulation, {Ft
}t0[0,T] , is usually termed the Instantaneous Volatility Process. We
have the following direct link between these alternative volatility representations,
F2(t) = limh60 F2(t-h) = limh60 ( I0
h F2(t-h+s) ds / h ).
It is also immediately evident from Proposition 3 that in this situation the notional volatility (or
the increment to the quadratic variation process) equals the so-called Integrated Volatility,
L2(t,h) = [M,M]t - [M,M]t-h = I0
h F 2(t-h+s) ds.
The (expected) integrated volatility plays a key role in the stochastic volatility option pricing
literature. In particular, following Hull and White and ignoring the return variation
associated with the conditional mean, options prices only depend on the expected notional
volatility .
The number of alternative continuous-time specifications for asset returns employed in the
literature is much too large for a comprehensive review at this point. For illustrative purposes,
we simply consider the relevant volatility concepts implied by a few standard formulations.
The simplest possible case is provided by the time-invariant diffusion,
dp(t) = µ dt + F dW(t), 0#t#T,
which underlies the Black-Scholes option pricing formula. Obviously, this process has a
deterministic mean return so the expected return volatility trivially equals the expected notional
volatility. Moreover, because the volatility is also constant, the expected notional volatility is
identical to the notional volatility. Formally, we thus have for the Black-Scholes setting,
h 2(t,h) = E[ (r(t,h) - m(t,h))2 |öt-h ] = L2(t,h) = I0
h F 2(t-h+s) ds = F 2 · h.
As discussed further in section 4.1 below, this model is also straightforward to estimate from
discretely sampled data by, e.g., maximum likelihood, as the returns are i.i.d. and normally
distributed. Of course, the model is overwhelmingly rejected for moderately frequently sampled
data (say, daily, weekly, or monthly), as it fails to accommodate the well-documented strong
intertemporal volatility dependencies.
For some price series (notably real commodity prices and exchange rates) it is often sensible to
-28postulate a stationary logarithmic price process. Popular models for such series - inspired by the
interest rate literature - include the Ornstein-Uhlenbeck (OU) processes and the square-root, or
Cox, Ingersoll and Ross (CIR), processes. These models take the general form
dp(t) = N ( µ - p(t) ) dt + F(t) dW(t), 0#t#T.
The drift specification ensures mean reversion in the process, given appropriate regularity
conditions and a well-behaved diffusion (volatility) coefficient process. Letting F(s) / F results
in the standard OU model, while having F(s) / F p((s) produces a constant elasticity of variance
(CEV) model, with the CIR model as a special case for , and further popularized for
interest rates by Chan, Karolyi, Longstaff and Sanders . The attraction of the specific OU
and CIR formulations stems primarily from the tractable distributions for discretely observed
data, and the accompanying closed-form solutions for many related asset and derivatives pricing
problems. Explicit solutions for the expected volatility and the expected notional volatility may
be derived from existing results in the literature. One immediate observation is that these two
volatility concepts now differ, as the return innovations will impact the mean process randomly
over the forecast horizon. Nonetheless, the expected notional volatility will remain the dominant
component in empirically realistic situations.
To exemplify the points above, consider the simple OU process which allows for explicit
expressions for the different expected volatility components over the interval, [t-h,t]. For
simplicity, and without loss of generality, we let t-h=0, corresponding to the interval [0,h] with
return r(h,h)/ r(h) and associated expected volatility h 2(h,h) = E[ (r(h) - m(h))2 |ö0 ]. The
explicit solution to the OU stochastic differential equation may be written directly as
r(h) = µ(h) + M(h)
= { p(0) (exp(-N h)-1) + F I0
h [exp(-N(h-s)) - 1] dW(s) } + F I0
The martingale component is, of course, simply M(h) = F I0
h dW(s) = F ·W(h), so the notional
volatility equals L2(h,h) = F 2·h. The mean component accounts for the two first terms on the
right hand side of (3.28). Notice that the drift coefficient, as required, only depends on the
martingale innovation process through a weighted average of past realizations where, at time
s=h, the current realization of W(h) receives a zero weight from [exp(-N(h-s)) - 1]. This
representation reflects the negative correlation between the return innovations and the drift, as a
(positive) shock to returns decreases the future drift coefficient via a mean-reverting term.
Given the (conditional) normality of the OU process, it is straightforward to compute the various
components of the expected volatility explicitly. We present the closed-form expressions along
with simpler approximations that apply for a short horizon, h. First, for the overall expected
volatility,
h 2(h,h) = [(1- exp(- 2N h) ) / ( 2N ) ] F 2 . F 2 · h - N h2 F 2 + (2/3) · F 2 N2 h3
14 A simple numerical example illustrates the orders of magnitude. The OU process is typically estimated, or
calibrated, to capture slowly evolving long-run swings in the logarithmic price process (or interest rate) away from the
unconditional mean. Such movements induce a relatively small degree of predictability in the short-term asset returns,
but long-term mean reversion, as manifest by a small mean reversion parameter for data calibrated to an annual
frequency, say N=0.1. At the daily frequency, or h=1/250, clearly N@ h2 .0, so that the difference between the expected
volatility and the notional volatility is negligible. Even at the quarterly frequency, or h=1/4, the deviation is a modest
2.5%. Of course, this number is somewhat sensitive to the assumed strength of the mean reversion.
. E[L2(h,h)|ö0 ] - N h2 F 2 + (2/3) · F 2 N2 h3.
The expected volatility is thus locally smaller than the notional volatility. This occurs because of
the mean-reverting drift coefficient. Large return innovations will tend to be partially undone
over the forecast horizon. However, to first order in h, expected volatility equals notional
volatility, confirming the crucial role of expected notional volatility.14
Further, in reference to equation (3.21), straightforward calculations reveal that
Var[ µ(h)|ö0 ] = [ h + {1 - exp(-2N h) /(2N)} - 2{1 - exp(- N h) /N }] ·F 2
. (N 2 /3) h3 F 2,
Cov[ M(h), µ(h)|ö0 ] = [{1 - exp(-N h) / N } - h] ·F 2 . - (N /2) h2 F 2 + (N2
/6) h3 F 2,
where the approximating expressions are computed to order h3 to avoid the variation of the mean
process from vanishing. Obviously, the contribution of the variation in the drift process is
generally (locally) negligible. Hence, the main contribution to the expected volatility (beyond
the notional volatility) stems from the covariance between return innovations and the future path
of the mean process. The negative correlation between these components lowers the overall
expected volatility (albeit the effect typically is small). Although the calculations are specific to
the OU process, the orders of magnitude are indicative of the relative importance of the
components governing the expected volatility. In fact, the OU process displays a very strong
covariance between the return innovations and the expected returns process, suggesting that this
example, if anything, overstates the typical contribution of the terms beyond the (expected)
notional volatility in determining the expected volatility for many asset classes.
Unfortunately, the entire class of one-factor models covered by equation (3.27) falter
dramatically when confronted with actual price or return data. In order to obtain more
satisfactory empirical fits, the literature has moved towards multi-factor parametric formulations.
A natural approach is to let the volatility process be governed by an independent source of
random variation, leading to a (genuine) continuous-time stochastic volatility model. An
influential specification is given by the square-root volatility model popularized by Heston
 corresponding to *=½ in the CEV diffusion,
dF 2(t) = (T - 2 F 2(t)) dt + 0 (F 2(t))½ dV(t), 0#t#T,
15 Alternatively, as shown by Chen, Hansen and Carrasco long-memory dependencies may also be
represented through (appropriate) nonlinearities in the scalar diffusion process. This idea also underlies the eigenfunction
stochastic volatility class of models recently developed by Meddahi .
-30where the standard Brownian motion process, V(t), may be correlated with the W(t) process
driving the returns, thus introducing an asymmetric return-volatility relation into the asset price
dynamics. This model is particularly attractive as it allows for closed-form solutions for option
prices. An extensive analysis of multivariate square-root (or affine) processes in modeling termstructure dynamics is provided in Dai and Singleton .
An alternative popular choice is to represent the logarithmic volatility process by the OU
diffusion in equation (3.18). As discussed in section 3.1.2 above, this formulation corresponds to
an (approximate) discrete-time SARV(1) model. In either case, the relation between the expected
volatility and the expected notional volatility may be found from the general formula in equation
(3.21). If the two Wiener innovation processes are correlated, all three terms become operative,
although the expected notional volatility (expected quadratic variation) continues to dominate
empirically.
The stochastic volatility diffusions above are considerably harder to estimate from discretely
observed data than the classical one-factor models of the OU or CIR variety. However, recent
progress has made relatively efficient inference possible through simulation-based procedures
such as efficient method of moments (EMM) or Markov chain Monte Carlo (MCMC). The
evidence indicates that these models provide major improvements over the traditional one-factor
models although they continue to be decidedly rejected by both equity return and nominal
interest rate data .
These failures have recently prompted a number of authors to add additional parametrically
specified diffusion factors . In light of the general representation in equation (3.22), it is evident that such
multi-factor models simply provide an alternative way of specifying the return dynamics that
ultimately may be reduced to a single factor representation for the univariate process. The
advantage is that the system may be defined through a sum of different factors, each following a
simple dynamic process, rather than a single factor with a more complex specification. For
example, one may approximate (apparent) long-range dependencies in the volatility process
through a sum of multiple distinct AR(1) factors .15
The ability to produce a simple parametric representation is extremely convenient, if not critical,
for economic interpretation and implementation of tractable estimation strategies through
standard (simulation based) likelihood and method of moments techniques. To illustrate this
idea, consider the general k-factor model,
r(t) / p(t) - p(0) = I0
h µ(t-h+s) ds + 3j=1,..,k I0
h Fj(t-h+s) dWj(s),
16 Formally, P[q(s)=0, t-h<s#t] = 1 - I0
h 8(t-h+s)ds + o(h), P[q(s)=1, t-h<s#t] = I0
h 8(t-h+s)ds + o(h), and
P[q(s)$2, t-h<s#t] = o(h).
-31where the Fj(t) refer to the jth volatility factor and W(t) = (W1(t), ..., Wk(t)), denotes a kdimensional vector process of independent standard Brownian motions. The notional volatility
then follows straightforwardly as the sum of the integrated constituent components,
L2(t,h) = I0
h F 2(t-h+s) ds = I0
h {3j=1,..,k Fj
2(t-h+s)} ds = 3j=1,..,k I0
2(t-h+s) ds.
As such, none of the general principles change, but the requisite calculations for, say, the
different terms in equation (3.21) may certainly become more involved.
It is arguably premature to judge the empirical performance of the parametric multi-factor
continuous sample path (pure diffusion) volatility models for asset returns, as this work truly is in
its infancy. It is clear, nonetheless, that such models serve as alternatives as well as complements
for the parametric jump-diffusion models that we turn to next.
3.2.2. Jump Diffusions and Lévy Driven Processes
At the highest sampling frequencies, there is compelling evidence of the existence of jumps in
asset price processes. Specifically, the arrival of important news such as macroeconomic
announcements (at the aggregate level) or earnings reports (at the firm level) typically induce a
discrete jump associated with an immediate revaluation of the asset; see, e.g., Andersen and
Bollerslev and Andersen, Bollerslev, Diebold and Vega for direct parametric
modeling of such jumps or Johannes for nonparametric specification tests for the
existence of jumps. Likewise, much evidence from the implied volatility literature - which
extracts information about market expectations concerning the future return distribution directly
from option prices - points towards the importance of incorporating discrete jump probabilities
into the analysis of the return dynamics; see, e.g., Bates and Bakshi, Cao and Chen
In the same way that the Brownian motion constitutes the basic building block of continuoustime martingales, the standard Poisson jump process serves as the basic building block for pure
(compensated) jump martingales . Thus, one may accommodate the relevant
jump features in an arbitrage-free continuous-time logarithmic price process by adding a Poisson
jump component with appropriate time variation in the jump intensity and/or the jump
distribution (as in Example 1 in section 2 above). In line with this reasoning, let q(t) denote a
Poisson process, with q(t)=1 indicating a jump at time t, and q(t)=0 otherwise, and (possibly
time-varying) jump intensity denoted 8(t).16 With the random jump size denoted 6(t), we then
have the general representation
= µ(t,h) + M(t,h)
h µ(t-h+s) ds + I0
h F(t-h+s) dW(s) + Et-h#s#t 6(s)·q(s).
The associated notional volatility process explicitly incorporates the jumps,
L2(t,h) / [M,M]t - [M,M]t-h = [Mc,Mc]t - [Mc,Mc]t-h + Et-h#s#t )M2(s)
h F 2(t-h+s) ds + Et-h#s#t 6 2(s)·q(s).
The computation of the corresponding expressions for the expected notional volatility and the
expected volatility will depend on the specific parametric formulation.
To illustrate, consider the simple jump diffusion in Merton with constant mean and
diffusion volatility coefficients as well as i.i.d. jumps; i.e., F(t)/ F, 8(t)/ 8. Also, denote the
mean and the variance of the jump distribution by µ6 and F6
2, respectively. The notional
volatility is now a stochastic variable, reflecting the random occurrence of jumps. Moreover,
only if the mean jump size is zero, µ6 = 0, will the expected notional volatility coincide with the
expected volatility (expected volatility does not contain the squared mean term below),
E[L2(t,h)|öt-h ] = F 2· h + E[Et-h#s#t )M2(s)|öt-h ] = F 2 · h + 8 · h · ( µ6
The corresponding expected instantaneous volatility, defined in equation (2.16), is
limh60 h2(t,h)/h = F 2 + 8 ·( µ6
which differs from the instantaneous volatility associated with the continuous martingale
component, as defined in equation (2.17), and here restricted to be constant, F2(t)/F2.
In this context, it is also important to recognize that the (implicit) càdlàg assumption on the F(t)
process in equation (3.31), and the corresponding continuous sample path martingale
representation in Proposition 3, does not rule out jumps in the F(t) process. Notice, however, that
the presence of jumps in either F(t) and/or r(t,h) invalidates the consistency arguments
underlying the nonparametric ARCH filters and smoothers, but not for the so-called realized
volatility measures, as discussed further in the next section.
As for the multi-factor parametric diffusion representations, the empirical evidence on jump
diffusion models for asset returns is still inconclusive. Early work estimated (overly) simple
representations in line with the time-invariant jump diffusion discussed above , but these
models are clearly at odds with the data. More realistic models have recently been explored by,
e.g., Andersen, Benzoni and Lund , Duffie, Pan and Singleton , Eraker, Johannes
Polson , Pan , and Eraker . While these formulations improve dramatically
on the fit of traditional univariate diffusion and standard stochastic volatility representations, a
general consensus about the relative performance of the various alternative specifications remains
17 As noted by Barndorff-Nielsen and Shephard this type of formulation also allows for the derivation
of analytical option pricing formula under quite general conditions, as studied more thoroughly by Nicolato and Venardos
18 Of course, the discrete-time parametric ARCH and stochastic volatility models discussed in section 3.1
above may also be given the interpretation of fixed length interval filters for extracting L2(t,h), h>0. However, these type
of filters are difficult to characterize and formally justify outside the realm of a specific parametric framework.
-33elusive at this (early) point.
Another recent proposal is to retain the continuous sample path strategy for the asset returns, but
model the volatility process as a non-Gaussian OU process driven by pure upward Lévy jumps
 . A primary motivation for this approach is to
retain analytic tractability of the temporal aggregation process involved in the construction of
volatility forecasts within a reasonably descriptive continuous-time setting.17 Technically, the
jumps in the volatility process introduces no new conceptual theoretical issues, as the Lévy
processes are semi-martingales, and as such the general apparatus for diffusion processes
discussed above applies directly. The empirical implementation of this approach is just
beginning, but the results reported so far are intriguing .
4. NONPARAMETRIC METHODS
The data-driven, or nonparametric volatility measurements afford direct ex-post empirical
appraisals of the notional volatility, L2(t,h), without any specific functional form assumptions.
The most obvious such measure is, of course, given by the ex-post squared return spanning the
[t-h,t] time interval. However, even though the (demeaned) squared return generally provides
an unbiased estimator for L2(t,h), it is also a very noisy estimate. The nonparametric
measurements more generally achieve consistency by measuring the volatility as (weighted)
sample averages of increasingly finer sampled squared (or absolute) returns over (and possible
outside) the [t-h,t] interval. This immediately raises important issues of efficiency, rates of
convergence, and the (asymptotic) distributions for the measurement errors associated with
different weighting schemes. At a more fundamental level, however, the nonparametric
procedures differ importantly in their assumptions about the length of the time interval, h. The
instantaneous volatility filters, or ARCH filters and smoothers, discussed next, are based on the
assumption of ever more observations over ever finer time intervals (a double limit theory), while
the realized volatility measures build on the idea of an increasing number of observations over
fixed length time intervals (a single limit theory).18
4.1. ARCH Filters and Smoothers
Parametric ARCH models were designed to parsimoneously model the expected volatility as an
explicit function of discretely observed returns; i.e., a parameterized conditional expectation, h
2(t,h)=E[(r(t,h)-m(t,h))2|Ft-h ], where h>0 and Ft-h denotes the information set generated by the
past returns r(t-h,h), r(t-2h,h), ... However, as observed by Nelson , these same discretetime parametric models may alternatively be given a nonparametric interpretation as filters
designed to extract information about the (latent) instantaneous volatility. In particular, assuming
that the sample path of the price and the corresponding instantaneous volatility processes are both
continuous, then - although formally misspecified at all discrete sampling frequencies, h>0 -
an appropriately parameterized sequence of ARCH models, or expected (scaled) volatilities h
2(t,h)/h, will consistently (as h64) estimate the instantaneous volatility, F2(t), at each point in
To grasp the intuition behind this powerful result, consider the simple continuous-time random
walk model in equation (3.26), previously studied by Merton in this context,
dp(t) = µ dt + F dW(t) , 0#t#T.
Suppose that observations are only available at n+1 equally spaced points over the [t-h,t] time
interval, where 0#h<t#T ; i.e., t-h, t-h+(h/n), ... , t-h+(n-1)·(h/n), t. By the definition of the
process, the corresponding sequence of i = 1, 2, ..., n, discrete (h/n)-period returns,
r(t-h+i·(h/n) , h/n) / p(t-h+i·(h/n)) - p(t-h+(i-1)·(h/n)),
is then i.i.d. normally distributed with mean µ·(h/n) and variance F2·(h/n). Hence, the MLE of the
drift is simply given by the sample mean of the (scaled) returns,
8µn / n-1·Ei=1,..,n (h/n)-1·r(t-h+i·(h/n) , h/n) / r(t,h)/h.
It follows immediately that
E( 8µn ) = µ.
This fixed-interval, or fill-in asymptotic, estimator for the drift only depends on h and not n. The
sampling frequency is irrelevant, only the span of the data matters. Thus, although 8µn is an
unbiased estimator for µ, it is not consistent as n64.
Consider now the (unadjusted) estimator for F 2 defined by the sum of the (scaled) squared
2 / n-1·Ei=1,..,n (h/n)-1 · r(t-h+i·(h/n) , h/n)2 = h-1·Ei=1,..,n r(t-h+i·(h/n) , h/n)2 .
E[ r(t-h+i·(h/n),h/n)2 ] = F2·(h/n) + µ2·(h/n)2 ,
it follows readily that
2 ) = F2 + µ2·(h/n) .
Hence, the drift induces only a second order bias, or O(n-1) term, in the estimation of F2 for n64.
Moreover, this estimator for the diffusion coefficient is consistent as n64. To see this, note that
E[ r(t-h+i·(h/n),h/n)3 ] = 3·µ·F2·(h/n)2 + µ3·(h/n)3 ,
E[ r(t-h+i·(h/n),h/n)4 ] = 3·F4·(h/n)2 + 6·µ2·F2·(h/n)3 + µ4·(h/n)4 ,
which along with the second moment given above, and the fact that the returns are i.i.d., implies
2 ) = 2·F4·n-1 + 4·µ2·F2·n-2.
Hence by a standard Law of Large Numbers,
plim n 64 ˆF n
The consistency result for the sample variance estimator for the time-invariant diffusion hinges
on the true volatility being constant over [t-h,t]. Increasing the number of (scaled) squared return
observations over the interval then produces an increasing number of unbiased and uncorrelated
measures of F2, and simply averaging these yields a consistent estimator.
This basic idea may, given appropriate regularity conditions, be extended to the general class of
continuous sample path diffusions considered in equation (3.23) above,
dp(t) = µ(t) dt + F(t) dW(t), 0#t#T.
under the additional assumption that the sample path for the F(t) process also is continuous. The
main difference between this general model and the time-invariant diffusion F(t)/F analyzed in
detail above, is that the length of the sampling interval, h, now also must shrink to zero as the
sampling intensity within the interval, n, increases.
At an intuitive level, by the assumed sample path continuity, the temporal variation in F2(t) is
readily bounded by restricting the length of the time interval, h, over which the variation is
œ > > 0, › h > 0 : supt-h#J#t |F2(J) - F2(t)| < > , (a.s).
Using this result and refining the arguments above, it is possible to show that the analogous time
t (unadjusted) sample variance estimator,
,h(t) / n-1·Ei=1,..,n (h/n)-1 · r(t-h+i·(h/n), h/n)2,
19 For earlier work on nonparametric diffusion estimation based on stronger assumptions and different
asymptotic arguments; see, e.g., Banon , Dohnal , Genon-Catalot, Laredo and Picard , and Florens-
Zmirou .
20 Note, this rules out the Lévy driven Ornstein-Uhlenbeck volatility processes favored by Barndorff-Nielsen
and Shephard .
-36consistently estimates the instantaneous volatility provided h60 and n64 at the proper rates,
plim n64,h60 ˆFn
,h(t) = F2(t),
where the convergence is pointwise in probability.
The tradeoff between the length of the sampling interval, h60, and the number of observations,
n6 4, is analogous to the usual bias-variance tradeoff encountered in nonparametric kernel
estimation. Similarly, the sample variance estimator in equation (4.1) corresponds to a flat kernel
scheme and the efficiency of this estimator may generally be improved by using a weighted oneor two-sided average of squared returns. That is the motivation behind the ARCH filters and
smoothers developed in a series of papers by Nelson , Nelson and Foster and Nelson and Schwartz .19
To illustrate, consider the GARCH(1,1) filter for the (1/n)-period returns defined in Nelson
2(t) = Tn + "n·r(t,1/n)2 + $n·ˆFn
= Tn ·(1-$n )-1 + Ei=0,..,4 "n ·$n
i·r(t-i/n,1/n)2,
"n = " · (1/n)½,
$n = 1 - " · (1/n)½ - 2/n,
and where T>0, ">0, 2>0, corresponding to p=q=1, h 2(t,1/n)/ ˆFn
2(t), and µ(t,1/n) / 0 in
equation (3.12) above. This filter again achieves consistency as n64 for F2(t),
plim n64 ˆFn
2(t) = F2(t),
and, as before, the convergence is pointwise in probability. Importantly, this result explicitly
excludes any jumps, or discontinuities, in either the price or diffusion processes,20 so that the
sample path for the instantaneous volatility process, F2(t), is continuous and coincides with that
of the expected (scaled) instantaneous volatility, limh60 L2(t,h)/h.
Heuristically, the GARCH(1,1) filter works analogously to the sample variance estimator in
-37equation (4.1) by utilizing an (infinite weighted) average of increasingly finer sampled squared
returns ever closer to time t. However, the continuous record, or fill-in, asymptotics of ever more
observations per interval, n64, over ever smaller time intervals, h64, is here achieved by a single
asymptotic device dictating both the return sampling frequency and the simultaneous downweighting of the more distant squared return observations, r(t-i/n,1/n)2, for large values of i.
Note that the parameter configuration in (4.2) underlying this result implies that
lim n64 ( "n + $n ) = lim n64 ( 1 - 2/n ) = 1,
so that the sequence of GARCH(1,1) filters approaches an IGARCH model, as discussed in
section 3.1.1. above, in the limit.
Besides providing a consistent volatility filter, such sequences of GARCH models have other
interesting and useful properties. For example, if the standardized returns,
z(t,1/n) / n1/2 · r(t,1/n) / ˆFn(t-1/n) ,
are i.i.d. normally distributed then, as shown by Nelson , the sequence of GARCH(1,1)
models defined implicitly by (4.2) converges weakly to the following CEV diffusion process for
the instantaneous volatility ,
dp(t) = F2(t) dW(t),
dF2(t) = (T - 2·F2(t) )dt + %2·"·F2(t) dV(t),
where the two Wiener processes are uncorrelated, Corr( dW(t) , dV(t) ) = 0. Of course, it remains
true, that when interpreted as a filter, the sequence of GARCH(1,1) models in (4.2) underlying
this diffusion limit consistently extracts the instantaneous volatility, F2(t), for any continuous
sample path diffusion.
Many other appropriately parameterized ARCH models share this important property.
Specifically, consider the sequence of EGARCH(0,1) models defined by
2(t)) = Tn + $n·log(ˆFn
2(t-1/n) ) + 2n·z(t,1/n) + (n·[ |z(t,1/n)| - (2/B)½ ]
Tn = "·$/n, $n = 1 - $/n, 2n = D·R·(1/n)½, (n = R·(1-D2)·(1-(2/B))-½·(1/n)½,
where $>0, R > 0, and the standardized innovations are defined as in equation (4.3). Interpreted
as a sequence of filters, this similarly provides consistent estimates (as n64) of the instantaneous
volatility at each point in time for any continuous sample path diffusion of the general form in
equation (3.23). In parallel to the consistent GARCH(1,1) filter,
lim n64 $n = 1,
-38so that the root in the autoregressive polynomial dictating the exponential decay in the weights
associated with the past absolute standardized returns approaches unity. Under the additional
assumption of i.i.d. normally distributed standardized returns, the sequence of EGARCH(0,1)
models defined by equation (4.5) converges weakly to the OU diffusion for log(F2(t)),
dp(t) = F(t) dW(t)
dlog(F2(t)) = - $ [log(F2(t)) - "] dt + R dV(t) ,
where the instantaneous correlation between the two Wiener processes is determined by the
leverage parameter D ; i.e., Corr( dW(t) , dV(t) ) = D dt.
Because many candidate ARCH models may serve as consistent filters for the instantaneous
volatility, this naturally raises the question of efficiency. The asymptotic distribution theory for
the filter errors developed by Nelson and Foster and Nelson allows for a formal
analysis of this issue. Intuitively, in the diffusion limit (with continuous sample paths) the
process is completely characterized by the first two conditional moments, and the optimal ARCH
filter matches both of these. These results for continuous-time stochastic differential equations
carry over to the design of optimal ARCH filters for the type of stochastic difference equations
employed in the formulation of the discrete-time stochastic volatility models discussed in section
3.1.2. In this situation, if the conditional distribution of the innovations are sufficiently fat tailed,
estimating F2(t) by squaring a distributed lag of past absolute returns, as originally proposed by
Taylor and Schwert , may be more efficient than using a distributed lag of past
squared returns. A detailed discussion of these results is beyond the scope of this chapter.
However, it is worth noting that the comparisons in Nelson and Foster related to the
diffusion in equation (4.6) show that asymptotically (for n 64) the efficiency loss in extracting
log(F2(t)) based on the lognormal SARV(1) model in equation (3.18) coupled with the (suboptimal) linear Kalman filter can be substantial relative to the (asymptotically) optimal ARCH
filter (which essentially looks like the EGARCH filter defined in equation 4.5). Of course, this
still entails an efficiency loss relative to the optimal non-linear extraction filter , but as discussed above, the numerical integration involved in the implementation of such
filters is computationally much more demanding than the simple recursions underlying the
filtered volatility estimates from ARCH models.
The ARCH filters explicitly restrict the information set used in the extraction of F2(t) to past and
current returns only; i.e., Ft . Asymptotic (for n64) optimal ARCH smoothers involving both
lagged and future returns have been developed by Nelson . The basic idea behind the
construction of optimal ARCH smoothers exploit principles similar to those involved in the
extension of the Kalman filter to a Kalman smoother . It is
noteworthy that in contrast to the optimal ARCH filters, the resulting optimal ARCH smoothers
do not necessarily match the first two conditional moments of the true distribution. An
alternative asymptotic distribution theory for analyzing smoothed volatility measurements is
provided by the rolling regression approach in Foster and Nelson . We return to a
discussion of some of these results in the following section.
4.2. Realized Volatility
The use of historical, ex-post sample variances computed from higher frequency return data as
lower frequency volatility measures has many precedents within the empirical finance literature.
For example, Poterba and Summers , French, Schwert and Stambaugh , and Schwert
 rely on monthly sample variances computed from daily returns, while Schwert ,
Hsieh and Taylor and Xu exploit intraday data to produce daily sample return
variance measures. In spite of the intuitive appeal of using sample variance estimators over fixed
horizons as simple nonparametric volatility measures, they appear hard to justify theoretically if
volatility truly is time-varying. However, by connecting the sample variances, termed Realized
Volatility in financial economics, to the theory of quadratic variation it is possible to more
formally justify and assess the properties of such measures. Moreover, this approach to volatility
measurement has inspired promising and ongoing new research into volatility modeling based on
general distributional assumptions. The formal definition is straightforward.
DEFINITION 6 - Realized Volatility
The Realized Volatility over [t-h,t], for 0<h # t # T, is defined by
L2(t,h;n) / Ei=1,..,n r(t-h+(i/n)·h , h/n )2.
The realized volatility is simply the second (uncentered) sample moment of the return process
over a fixed interval of length h, scaled by the number of observations n (corresponding to the
sampling frequency 1/n), so that it provides a volatility measure calibrated to the h-period
measurement interval. Although the definition is stated in terms of equally spaced observations,
most results discussed below carry over to situations in which the realized volatility is based on
the sum of unevenly, but increasingly finely sampled squared returns.
The realized volatility measure is closely related to, but different from, the theoretical volatility
concepts introduced in section 2. For example, if the mean return is zero, µ(t) / 0, the realized
volatility represents the ex-post sample variance computed from n discretely sampled (h/n)period returns over [t-h,t]. In this case the realized volatility is (ex-ante) unbiased for the
expected volatility, h 2(t,h). Formally, we have the following slight extension of equation (2.14)
 .
PROPOSITION 4 - Realized Volatility as an Unbiased Volatility Estimator
If the return process is square-integrable and µ(t) / 0, then for any value of n $ 1 and h > 0,
h 2(t,h) = E[L2(t,h)|öt-h ] = E[M2(t-h)|öt-h ] = E[L2(t,h;n)|öt-h ].
21 Andreou and Ghysels provide some intriguing simulation-based efficiency comparisons of the
realized volatility measure in Definition 6 with alternative ex-post volatility filters motivated by the Nelson and Foster
 rolling regression theory.
As such, the ex-post realized volatility is an unbiased estimator of ex-ante expected volatility. Of
course, the zero mean assumption is highly restrictive but, as we discuss later, the result remains
approximately true for a stochastically evolving mean return process over relevant horizons
under weak auxiliary conditions, as long as the underlying returns are sampled at sufficiently
high frequencies.
Another link to our previous discussion is provided by the theory of rolling sample variance
estimators within the continuous sample path (diffusion) setting . This
theory implies that the realized volatility based on increasingly many return observations over
finer and finer time intervals is consistent for the corresponding instantaneous volatility. That is,
for h 6 0 and n 6 4 (at proper rates),
plimn64,h60 L2(t,h;n)/h = plimh60 L2(t,h)/h = F2(t).
Although this result is of theoretical interest, it is less robust and less useful in practice.21 One
constraint is that the theory excludes jumps in both the return and volatility processes. More
importantly, from a practical perspective, the result hinges on the length of the time interval
going to zero and the number of observations going to infinity (over the vanishing interval)
simultaneously. This construction is hard to mimic in any relevant sense. Market microstructure
features invariably limit the number of (effectively) uncorrelated return observations, so even for
highly liquid markets, it is not possible to measure returns (or volatilities) instantaneously. We
discuss these practical issues in more detail below.
The rolling regression procedures and associated ARCH filters and smoothers for the
instantaneous volatilities are also usually based on long (weighted) averages of the returns.
Adjacent instantaneous volatility measures will therefore involve overlapping return
observations. This renders formal statistical analysis of the time-series properties of any such
derived volatility series complex. The realized volatility approach explicitly seeks to avoid such
difficulties by fixing h>0 and interpreting L2(t,h;n) as a measure of the overall volatility for the
[t-h,t] time interval. We turn now towards a general discussion of this approach.
The theoretical properties of realized volatility have been discussed from different perspectives in
a number of recent studies including Andersen, Bollerslev, Diebold and Labys ,
Barndorff-Nielsen and Shephard , and Comte and Renault . A simple
yet fundamental result follows directly by combining the theory of quadratic variation in
Proposition 2 with the Definitions 1 and 6.
PROPOSITION 5 - Consistency of Realized Volatility
The Realized Volatility provides a consistent nonparametric measure of the Notional Volatility,
plimn64 L2(t,h;n) = L2(t,h),
0<h # t # T,
where the convergence is uniform in probability.
The notional volatility plays a crucial role in the return dynamics. From the relation between
expected notional volatility and expected volatility in equation (3.21), the ex-ante expected
notional volatility is also the critical determinant of expected volatility. Any empirical measures
of (ex-ante expected) notional volatility based on (3.21) will necessarily depend on the assumed
parametric model structure. Proposition 5 implies that, in the limit for increasingly finely
sampled returns, or n 6 4, realized volatility is a consistent (nonparametric) estimator of the
(realized) notional volatility over any fixed length time interval, h > 0.
It is natural to combine the unbiasedness property of realized volatility in Proposition 4 and the
consistency result in Proposition 5 to think of ex-post realized volatility measures, in general, as
approximately unbiased estimators, and the ex-ante expected values of the realized volatility
measures as consistent estimators for the ex-ante expected notional volatility. These conjectures
warrant some formal discussion. The former result is, indeed, only approximately correct, while
the latter requires a minor auxiliary assumption to be generally valid.
PROPOSITION 6 - Consistency of Ex-Ante Expected Realized Volatility
The Expected Realized Volatility provides a consistent estimator of the Expected Notional
Volatility,
plimn64 E[L2(t,h;n)|öt-h ] = E[L2(t,h)|öt-h ], 0<h # t # T,
provided the return process is bounded and the expected return, µ(s), is of integrable variation
for s in [t-h,t].
The conclusion embodied in equation (4.10) follows from (4.9) if convergence in probability
implies convergence in mean. This is not generally true, however, even if the result is valid for
martingale processes, as stated in Proposition 4. The assumption of a bounded return process
provides a simple sufficient condition for convergence in mean, although a weaker uniform
integrability condition would suffice; e.g., Hoffmann-Jørgensen , sections 3.22-3.25.
Proposition 6 establishes that the ex-post realized notional volatility provides an asymptotically
(for n 6 4 ) unbiased estimator of the ex-ante expected notional volatility, as long as the return is
bounded over the relevant horizon. This condition is obviously not satisfied by all admittable
price processes in Proposition 1, and will be violated for many typical return specifications, but
-42still does not appear particularly restrictive. For example, one may imagine a bound on the return
that prevents a small investment in the asset from ever producing a return that exceeds a (large)
multiple of the expected value of all resources available in the world-wide economy. Moreover,
it is only a sufficient condition, and the proposition will apply in most cases, even when the
boundedness condition is violated. Nonetheless, some specifications are excluded .
Although the above consistency result for the realized volatility estimators leaves important
considerations regarding the size of potential error terms and any finite-sample biases
unanswered, the reasoning suggests that the simple realized volatility measure could serve as a
constructive basis for volatility measurement (and modeling) under general conditions. We
discuss the issue of the measurement errors involved in such an approach more formally below.
It is instructive, however, first to decompose the realized volatility measure into the separate
terms associated with the potential sources of error and bias. In the frictionless arbitrage-free
setting, the return on a risky asset over time intervals of length (h/n) has a martingale innovation
of order (h/n)½, while the corresponding mean component is at most of order (h/n). In particular,
following the notation in equation (4.11), we have
L2(t,h;n) = Ei=1,..,n [µi
2+ 2 · µi Mi + Mi
= Op(h2/n) + Op( h3/2/ n½) + Ei=1,..,n Mi
= L2(t,h) + Op(n-1) + Op(n-½) + [ Ei=1,..,n Mi
2 - L2(t,h) ].
It is apparent that the realized volatility may differ from the notional volatility for two distinct
reasons. First, the second and third terms on the right-hand-side of the last equation in (4.12)
reflect the mean returns which only truly vanishes in the limit for n 6 4. However, the expected
return over short intervals (large n) are necessarily small, so the contribution from these terms
will be empirically negligible. This conclusion is only reinforced by noting that the component
of the largest order, Op(n-½), represents a covariance term that is limited by the size of the
innovations to the expected return over the (h/n) time interval, which typically will be very small.
Second, the last term on the right-hand-side of (4.12) has the interpretation of a measurement
error term, as Proposition 4 shows that the cumulative squared martingale innovations provide
unbiased estimators for the corresponding notional volatility (quadratic variation). Hence, this
term has a zero expected value. Nonetheless, for any given value of n it induces a measurement
error that is unrelated to the mean return. This component is the source of empirically relevant
deviations between realized volatility and (realized) notional volatility.
The actual size and exact distribution of the errors obviously depend on the particular return
process and must be analyzed on a case-by-case basis. Barndorff-Nielsen and Shephard 
provide specific evidence for the Ornstein-Uhlenbeck specification with a background-driving
Lévy process. Similarly, Meddahi presents explicit expressions for the different terms in
equation (4.12) for the class of eigenfunction stochastic volatility models, and goes on to
22 This result also underlies the simple GMM estimation procedure for parametric continuous-time stochastic
volatility models in Barndorff-Nielsen and Shephard and Bollerslev and Zhou based on matching sample
moments of realized volatility with corresponding model implied moments for notional (integrated) volatility.
-43numerically compare the size of the unconditional variance of the measurement error to the
unconditional variance of the notional (integrated) volatility for some of the continuous-time
diffusions of the general form in equation (3.23) that have been estimated in the existing
literature.
The preceding discussion implies that realized volatility is approximately (apart from minor
biases induced by the mean component) unbiased for the corresponding notional volatility.
Importantly, it also follows from the local martingale property in (2.11) and the decomposition in
(4.12), that the associated measurement errors are approximately uncorrelated, i.e.,
E[(L2(t+j,h;n)-L2(t+j,h) )@ (L2(t,h;n)-L2(t,h) ) ] = Op(n-1 ),
where j…0. Again, the O(n-1 ) term is identically equal to zero in the case of constant mean
returns, and otherwise likely to be small in empirically realistic situations. This confirms that
realized volatilities provide meaningful and theoretically well founded volatility measurements.
Moreover, they constitute natural and convenient inputs into modeling and inference procedures
concerning the expected notional volatility and, by extension, the expected return volatility.22
It is worth reiterating that the fixed h, large n asymptotics, or realized volatility asymptotics,
underlying the results discussed above, is pivotal in practice. In particular, in spite of the
theoretical desirability of letting interval size, h, shrink indefinitely as an increasing number of
high-frequency return observations is used within each (vanishing) interval (as in the ARCH
filters and smoothers discussed in section 4.1), this idea is difficult (impossible) to mimic in
practice. The number of data points, n, that adhere (approximately) to the underlying noarbitrage semi-martingale property over short time intervals is severely limited by various market
microstructure frictions. This invariably puts an effective (asset and/or market specific) lower
bound on the highest sampling frequency that is applicable in empirical work, say 1/n >1/N. We
return to this important practical consideration below.
In summary, the realized volatility approach exploiting intraday return observations allow for
directly observable return volatility measures that are consistent, approximately unbiased and
have uncorrelated measurement errors. It is natural to exploit these properties by building a time
series model directly for the observed realized volatility measures through standard ARMA style
modeling. Importantly, such procedures sidestep the complex task of providing an appropriate
model for the intraday volatility patterns while still exploiting the inherent information in the
high-frequency data for lower frequency volatility movements. Of course, the use of
nonparametric volatility measurements invariably entails a loss in statistical efficiency relative to
the use of a fully (and by assumption correctly) specified parametric volatility model. We
comment further on this issue, and the practical merits of reduced form realized volatility
modeling below.
23 In an interesting paper Bai, Russell and Tiao extend this expression for the unconditional variance to
allow for serially correlated, leptokurtic high-frequency returns subject to GARCH effects and intraday deterministic
volatility patterns.
The imposition of additional restrictions on the return process allows for important additional
insight into the size and asymptotic distribution of the realized volatility errors. In particular,
consider the class of continuous sample path diffusions, characterized by the sde,
dp(t) = µ(p(t),F(t-)) dt + F(t-) dW(t) , 0 # t # T,
where µ(p(t),F(t)) is predictable and of finite variation and the F(t) process is independent of the
Brownian motion, W(t) (c.f., Proposition 3 and the corresponding sde in equation 3.23). Recent
results by Barndorff-Nielsen and Shephard then provide the asymptotic (for n 6 4)
distribution of the measurement errors in this setting.
PROPOSITION 7 - Asymptotic Mixed Normality of Realized Volatility
The Realized Volatility Errors for the continuous sample path diffusion in equation (4.14) is
distributed as
[L2(t,h;n) - L2(t,h)]·[ b @ L (t,h;n) ] -1/2 6 N(0,1),
for n64, where
L (t,h;n) / Ei=1,..,n r(t-h+i@(h/n) , h/n )4.
This proposition strengthens the convergence of realized volatility to notional volatility (in
probability) result considerably by providing the asymptotic distribution of the corresponding
errors. Formally, the variance of the realized volatility errors is given by b@I t
t-h F 4(J)dJ, which
is consistently estimated by b@L (t,h;n) as defined in equation (4.16). Hence, the magnitude of
the errors depends upon the level of the (latent) volatility. This result represents a fundamental
extension of the corresponding expression for the variance of the (uncentered) sample variance
for the continuous-time random walk model discussed in section 4.1 above, 2@F 4@ h2@ n-1 + O(n-
1).23 Of course, the general expression for the variance of the realized volatility errors, or
L (t,h;n), is straightforward to calculate in practice.
The more powerful distributional result in Proposition 7, compared to the weak convergence in
Proposition 4, comes at the cost of the stronger assumption on the underlying log-price process.
Although the additional conditions, most notably the absence of jumps in the price path, likely
are violated empirically at the highest sampling frequencies, the asymptotic distribution should
nonetheless serve as a very useful theoretical benchmark for assessing the properties of the
realized volatility measures and further assist in guiding empirical procedures.
24 This is also consistent with the recent empirical evidence in Andersen, Bollerslev, Diebold and Labys
 and Andersen, Bollerslev, Diebold and Ebens suggesting that the unconditional distribution of realized
volatility is approximately log-normal.
25 The presence of jumps in the price process will generally render the corresponding distribution of the
(standardized) returns non-normal. This could be exploited in the formulation of tests for (the importance of) jumps .
In this regard, Barndorff-Nielsen and Shephard find that an improved finite-sample
(finite n) approximation may be obtained by the log-linearization,
[log(L2(t,h;n)) - log(L2(t,h)) +½@ s(t,h;n)2 ]·s(t,h;n) -1 - N(0,1),
s(t,h;n)2 / max{ b L (t,h;n)·L2(t,h;n)-2 , 2/n }.
The upper bound of 2/n in equation (4.18) arises from imposing the theoretical lower bound for n
6 4 on the first ratio. This approximation seems to work well, even for moderately sized n (say n
$10), in a (stylized) simulation setting. The improvement is related to the logarithm delivering a
variance-stabilizing transformation. In that sense, the improved finite-sample distribution
obtained by equations (4.17) and (4.18) is directly in line with the evidence for the parametric
discrete-time stochastic volatility models discussed in section 3.1.2, for which the innovation
process for the formulations involving the logarithmic volatility typically exhibits much reduced
(conditional) heteroskedasticity.24
The above results speak to the precision in extracting information about the (realized) notional
volatility from the realized volatility measures. Realizations of the notional volatility are, of
course, of direct interest as indicators of return variability. However, they also provide an
indication of the character of the underlying return distribution itself. In particular, it follows under the identical conditions underlying Proposition 7 - that the returns, r(t,h), conditional on
the notional volatility (and the mean return) over the [t-h,t] return interval will be Gaussian.
PROPOSITION 8 - Normal Mixture Distribution
The discrete-time returns r(t,h) over [t-h,t], 0<h # t # T, for the continuous sample path diffusion
in equation (4.14) is distributed as a normal mixture,
r(t,h) | F {µ(t,h),L2(t,h)} - N( µ(t,h) , L2(t,h) ).
Of course, the (ex-ante) mean return and the notional volatility is not directly observable.
However, integrating out F {µ(t,h),L2(t,h)}, the proposition implies that the return distribution
conditional on time t-h information should be governed by a normal mixture distribution.25 This
is directly in line with the implications of the Mixture-of-Distributions Hypothesis (MDH)
pioneered by Clark which, as discussed in section 3.1.2, has motivated the formulation of
26 Of course, as discussed above the realized volatility invariably differs from the true notional volatility for
finite n. However, the measurement errors are (approximately) serially uncorrelated, and therefore effectively averaged
out in any reduced form time series model for L2(t,h;n).
27 Other interesting recent empirical studies highlighting the potential benefits of realized volatility procedures,
in addition to the ones already cites, include Areal and Taylor , Galbraith and Zinde-Walsh , Maheu and
McCurdy , and Thomakos and Wang .
-46some of the most widely used empirical discrete-time stochastic volatility models.
The consistency of the realized volatility for the notional volatility in Proposition 4 along with
the approximate log-normality of the realized volatility distribution and the normal mixture
distribution in Proposition 8 suggest a simple alternative empirical return-volatility modeling
strategy. Assume that the demeaned returns standardized by the realized volatilities, [r(t,h) -
µ(t,h)] ·L2(t,h;n)-1, are (approximately) Gaussian, coupled with a simple reduced form
(approximately) Gaussian time series model for the logarithmic realized volatilities,
log[L2(t,h;n)]. Effectively, this modeling strategy relies exclusively on forecasts for the
distribution of the future notional volatilities through the observed realized volatilities, and as
such is in principle straightforward to implement in practice.26 Moreover, leverage effects, or
asymmetries, in the notional volatility are easily incorporated by allowing the time series model
for log[L2(t,h;n)] to depend (non-trivially) on the level of the (past) returns. This empirical
modeling framework has recently been pursued successfully by Andersen, Bollerslev, Diebold,
and Labys . Related empirical work by Fleming, Kirby and Ostdiek and Martens
and Zein suggests that important improvement can be obtained by using this realized
volatility modeling approach in lieu of standard volatility modeling procedures for practical
financial decision making as exemplified by better portfolio allocations and more accurate option
pricing calculations.27
As mentioned repeatedly, the realized volatility approach, of holding h >0 fixed, is motivated by
the fact that it is undesirable, and due to the presence of market microstructure frictions indeed
practically infeasible, to sample returns infinitely often (n64) over infinitesimally short time
intervals (h60). Model specific calculations and simulations by Andersen and Bollerslev
 , Andersen, Bollerslev and Lange , Andreou and Ghysels , Bai, Russell and
Tiao , Barndorff-Nielsen and Shephard , Barucci and Renò , Bollerslev and
Zhou , and Zumbach, Corsi and Trapletti , among others, illustrate the effects of
finite n (and h) for a variety of settings. The discrepancies in the underlying model formulation
and character of the assumed frictions render a general assessment of the results difficult.
Moreover, the size of the measurement errors are often computed unconditionally rather than
conditional on the realization of the realized volatility statistic. Nonetheless, it is evident that the
measurement errors typically are non-trivial. In turn, this has motivated the development of
alternative realized volatility measures designed to circumvent the microstructure biases. One
common adjustment involves a correction for serial correlation in the high-frequency returns,
28 This is manifest in the volatility signature plots in Andersen, Bollerslev, Diebold and Labys . In a
multivariate setting, the so-called Epps effect and the breakdown of the sample autocorrelations at the highest intraday
frequencies is also directly related to this phenomenon .
29 This mirrors (in many ways) earlier developments related to the estimation of CAPM beta’s in the presence
of asynchronous trading effects by Scholes and Williams , and the adjustment to the sample variance in French,
Schwert and Stambaugh obtained by including the cross-product between successive returns. Another recent
approach developed in Malliavin and Mancino exploit frequency-domain methods, thereby avoiding any
(arbitrary) assumptions about the length of the high-frequency return interval. However, this necessarily involves another
(arbitrary) assumption about the (latent) price path between two successive observations .
-47supposedly induced by asynchronous trading and quoting, as well as bid-ask bounce.28 This
typically takes the form of a simple MA(1) type filter, examples of which include Zhou ,
Andersen, Bollerslev, Diebold, and Ebens , and Corsi, Zumbach, Müller and Dacorogna
Alternatively, it may be desirable to focus on volatility estimators that are robust to
microstructure noise. One example is the range. Alizadeh, Brandt and Diebold and
Brandt and Diebold show that range-based volatility is robust both to bid-ask bounce and
to asynchronous trading. This feature must be weighed against the fact that formal statistical
analysis of range based estimators require specific distributional assumptions avoided with the
realized volatility and power based variation measures discussed at length in the current section.
A second example involves using power transformations of the absolute returns rather than the
squared returns . For instance, as noted in section 4.1 above, the
optimal ARCH filters for discrete-time stochastic volatility models may entail a distributed lag of
past absolute returns as opposed to the squared returns . These results
have a direct analogue for the continuous sample path diffusion in equation (4.14). The
following two formal definitions of Notional and Realized Power Variation measures, adapted
from Barndorff-Nielsen and Shephard , directly parallel the notional and realized
volatility concepts in the general frictionless arbitrage-free setting.
DEFINITION 7 - Power Variation Measures
The Notional sth Order Power Variation and the Realized sth Order Power Variation, s>0, for the
diffusion in equation (4.14) over [t-h,t], 0<h # t # T, are defined, respectively, by
L[s](t,h) / I t
t-h F s(J) dJ
L[s](t,h;n) / Ei=1,..,n |r(t-h+i·(h/n) , h/n )| s .
It is apparent that, for s=2, the definitions correspond directly to the notional volatility and the
realized volatility concepts discussed above; i.e., L (t,h)/L2(t,h) and L (t,h;n)/L2(t,h;n),
-48respectively. However, other values of s may allow for more robust measurements. In particular,
recent developments by Barndorff-Nielsen and Shephard extend the distributional results
for the (standard) realized volatility in Proposition 5 to the generalized power variation measures.
PROPOSITION 9 - Asymptotic Mixed Normality of Realized Power Variation
The Realized sth Order Power Variation Errors, s>0, for the continuous sample path diffusion in
equation (4.11) is distributed as
6s(h/n)s/2-1·[6s
-1(h/n)1-s/2·L[s](t,h;n) - L[s](t,h)]·[62s
-1·Ts·L[2s](t,h;n) ] -1/2 6 N(0,1),
for n64, where 6s=E(|Z|s ), Ts=Var(|Z|s ), and Z denotes a standard normal distribution.
The special case corresponding to s=1 is naturally termed Absolute Variation. The realized
absolute variation is, of course, simply constructed by the summation of the n absolute returns, |
r(t-h+i·(h/n) , h/n )| , i = 1, 2, ..., n, within the [t-h,t] time interval. From Proposition 9, the
asymptotic (for n 6 4) distribution of the corresponding measurement error for the notional
absolute variation satisfies
[(B/2)½(h/n)1/2L (t,h;n) - L (t,h)]·[(h/n)(B/2 - 1)L2(t,h;n) ] -1/2 6 N(0,1).
This result holds the promise of improved volatility measurements and more accurate inference
than the (standard) realized volatility (quadratic variation) measure in Proposition 7. The
absolute variation measures also provide a theoretical basis for gauging the empirical results in
Andersen and Bollerslev among others based on L (t,h;n). Similarly, the so-called
Taylor Effect according to which the autocorrelations of power
transforms of the absolute returns are maximized (empirically) for values of s close to unity may
be directly related to these new results.
Research in the realized volatility area has evolved rapidly over the past few years, and it is still
too early to draw firm conclusions or consensus opinion about the preferred procedures.
However, the theoretical and empirical results reported to date have been very promising.
Current research into multivariate extensions of Propositions 7 and 9 may help form a firm
theoretical foundation for new realized CAPM betas and factor loadings that are being explored
empirically in ongoing work. Extensions of the continuous sample path diffusion in equation
(4.14) to allow for more general processes (including asymmetries or leverage effects) within the
frictionless, arbitrage-free setup is another theoretically challenging problem. As discussed
repeatedly, the development of reliable empirical procedures for dealing with the inherent market
microstructure frictions at the highest sampling frequencies across different assets and markets is
also of utmost importance from a practical perspective .
5. DIRECTIONS FOR FUTURE RESEARCH
In the last ten years, there has been a movement toward the use of newly-available highfrequency asset return data, and away from restrictive and hard-to-estimate parametric models
toward flexible and computationally simple nonparametric approaches. Those trends will
continue. Two related, directions for future research are apparent: (1) continued development of
methods for exploiting the volatility information in high-frequency data, and (2) volatility
modeling and forecasting in the high-dimensional multivariate environments of practical
financial economic relevance. The realized volatility concept readily tackles both: it incorporates
the powerful information in high-frequency data while dispensing with the need to actually
model the high-frequency data, and it requires only the most trivial of computations, thereby
bringing within reach the elusive goal of accurate and high-dimensional volatility measurement,
modeling and forecasting. We look forward to realization of that goal in the not-too-distant