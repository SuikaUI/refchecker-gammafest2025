New Regularized Algorithms for Transductive Learning
Partha Pratim Talukdar and Koby Crammer
Computer & Information Science Department
University of Pennsylvania
Philadelphia, PA 19104
{partha,crammer}@cis.upenn.edu
Abstract. We propose a new graph-based label propagation algorithm for transductive learning. Each example is associated with a vertex in an undirected graph
and a weighted edge between two vertices represents similarity between the two
corresponding example. We build on Adsorption, a recently proposed algorithm
and analyze its properties. We then state our learning algorithm as a convex
optimization problem over multi-label assignments and derive an efﬁcient algorithm to solve this problem. We state the conditions under which our algorithm is
guaranteed to converge. We provide experimental evidence on various real-world
datasets demonstrating the effectiveness of our algorithm over other algorithms
for such problems. We also show that our algorithm can be extended to incorporate additional prior information, and demonstrate it with classifying data where
the labels are not mutually exclusive.
Keywords: label propagation, transductive learning, graph based semi-supervised
Introduction
Supervised machine learning methods have achieved considerable success in a wide
variety of domains ranging from Natural Language Processing, Speech Recognition to
Bioinformatics. Unfortunately, preparing labeled data for such methods is often expensive and time consuming, while unlabeled data are widely available in many cases. This
was the major motivation that led to the development of semi-supervised algorithms
which learn from limited amounts of labeled data and vast amounts of freely available
unannotated data.
Recently, graph based semi-supervised algorithms have achieved considerable attention . Such methods represent instances as vertices in a graph with edges
between vertices encoding similarities between them. Graph-based semi-supervised algorithms often propagate the label information from the few labeled vertices to the
entire graph. Most of the algorithms tradeoff between accuracy (initially labeled nodes
should retain those labels, relaxations allowed by some methods) with smoothness (adjacent vertices in the graph should be assigned similar labels). Most algorithms only
output label information to the unlabeled data in a transductive setting, while some
algorithms are designed for the semi-supervised framework and build a classiﬁcation
model which can be applied to out-of-sample examples.
W. Buntine et al. (Eds.): ECML PKDD 2009, Part II, LNAI 5782, pp. 442–457, 2009.
c⃝Springer-Verlag Berlin Heidelberg 2009
New Regularized Algorithms for Transductive Learning
Adsorption is one such recently proposed graph based semi-supervised algorithm
which has been successfully used for different tasks, such as recommending YouTube
videos to users and large scale assignment of semantic classes to entities within
Information Extraction . Adsorption has many desirable properties: it can perform
multiclass classiﬁcation, it can be parallelized and hence can be scaled to handle large
data sets which is of particular importance for semi-supervised algorithms. Even though
Adsorption works well in practice, to the best of our knowledge it has never been analyzed before and hence our understanding of it is limited. Hoping to ﬁll this gap, we
make the following contributions in this paper:
– We analyze the Adsorption algorithm and show that there does not exist an
objective function whose local optimization would be the output of the Adsorption
algorithm.
– Motivated by this negative result, we propose a new graph based semi-supervised
algorithm (Modiﬁed Adsorption, MAD), which shares Adsorption’s desirable properties, yet with some important differences.
– We state the learning problem as an optimization problem and develop efﬁcient
(iterative) methods to solve it. We also list the conditions under which the optimization algorithm – MAD – is guaranteed to converge.
– The transition to an optimization based learning algorithm provides a ﬂexible and
general framework that enables us to specify a variety requirements. We demonstrate this framework using data with non-mutually exclusive labels, resulting in
the Modiﬁed Adsorption for Dependent Labels (MADDL, pronounced medal) algorithm.
– We provide experimental evidence demonstrating the effectiveness of our proposed
algorithm on various real world datasets.
Adsorption Algorithm
Adsorption is a general algorithmic framework for transductive learning where the
learner is often given a small set of labeled examples and a very large set of unlabeled
examples. The goal is to label all the unlabeled examples, and possibly under the assumption of label-noise, also to relabel the labeled examples.
As many other related algorithms , Adsorption assumes that the learning
problem is given in a graph form, where examples or instances are represented as nodes
or vertices and edges code similarity between examples. Some of the nodes are associated with a pre-speciﬁed label, which is correct in the noise-free case, or can be subject
to label-noise. Additional information can be given in the form of weights over the labels. Adsorption propagates label-information from the labeled examples to the entire
set of vertices via the edges. The labeling is represented using a non-negative score for
each label, with high score for some label indicating high-association of a vertex (or its
corresponding instance) with that label. If the scores are additively normalized they can
be thought of as a conditional distribution over the labels given the node (or example)
More formally, Adsorption is given an undirected graph G = (V, E, W), where a
node v ∈V corresponds to an example, an edge e = (a, b) ∈V × V indicates that the
P.P. Talukdar and K. Crammer
label of the two vertices a, b ∈V should be similar and the weight Wab ∈R+ reﬂects
the strength of this similarity.
We denote the total number of examples or vertices by n = |V | , by nl the number
of examples for which we have prior knowledge of their label and by nu the number of
unlabeled examples to be labeled. Clearly nl + nu = n. Let L be the set of possible
labels, their total number is denoted by m = |L| and without loss of generality we
assume that the possible labels are L = {1 . . . m}. Each instance v ∈V is associated
with two row-vectors Yv, ˆYv ∈Rm
+. The lth element of the vector Yv encodes the
prior knowledge for vertex v. The higher the value of Yvl the stronger we a-priori
believe that the label of v should be l ∈L and a value of zero Yvl = 0 indicates
no prior about the label l for vertex v. Unlabeled examples have all their elements set
to zero, that is Yvl = 0 for l = 1 . . . m. The second vector ˆYv ∈Rm
+ is the output
of the algorithm, using similar semantics as Yv. For example, a high value of ˆYvl
indicates that the algorithm believes that the vertex v should have the label l. We denote
by Y, ˆY ∈Rn×m
the matrices whose rows are Yv and ˆYv respectively. Finally, we
denote by 0d the all-zeros row vector of dimension d.
Random-Walk View
The Adsorption algorithm can be viewed as a controlled random walk over the graph
G. The control is formalized via three possible actions: inject, continue and
abandon (denoted by inj, cont, abnd) with pre-deﬁned probabilities pinj
0 per vertex v ∈V . Clearly their sum is unit: pinj
= 1. To label any
vertex v ∈V (either labeled or unlabeled) we initiate a random-walk starting at v facing three options: with probability pinj
the random-walk stops and return (i.e. inject)
the pre-deﬁned vector information Yv. We constrain pinj
= 0 for unlabeled vertices
v. Second, with probability pabnd
the random-walk abandons the labeling process and
return the all-zeros vector 0m. Third, with probability pcont
the random-walk continues
to one of v’s neighbors v′ with probability proportional to Wv′v ≥0. Note that by definition Wv′v = 0 if (v, v′) /∈E. We summarize the above process with the following
set of equations. The transition probabilities are,
Pr [v′|v] =
u : (u,v)∈E
(v′, v) ∈E
The (expected) score ˆYv for node v ∈V is given by,
ˆYv = pinj
× Yv + pcont
v′ : (v′,v)∈E
Pr [v′|v] ˆYv′ + pabnd
Averaging View
For this view we add a designated symbol called the dummy label denoted by ν /∈L.
This additional label explicitly encodes ignorance about the correct label and it means
New Regularized Algorithms for Transductive Learning
Algorithm 1. Adsorption Algorithm
G = (V, E, W )
- Prior labeling: Yv ∈Rm+1 for v ∈V
- Probabilities: pinj
- Label Scores: ˆYv for v ∈V
1: ˆYv ←Yv for v ∈V {Initialization}
for all v ∈V do
× Yv + pcont
× Dv + pabnd
8: until convergence
that a dummy label can be used instead. Explicitly, we add an additional column to all
the vectors deﬁned above, and have that Yv, ˆYv ∈Rm+1
and Y, ˆY ∈Rn×(m+1)
set Yvν = 0, that is, a-priori no vertex is associated with the dummy label, and replace
the zero vector 0m with the vector r ∈Rm+1
where rl = 0 for l ̸= ν and rν = 1. In
words, if the random-walk is abandoned, then the corresponding labeling vector is zero
for all true labels in L, and an arbitrary value of unit for the dummy label ν. This way,
there is always positive score for at least one label, the ones in L or the dummy label.
The averaging view then deﬁnes a set of ﬁxed-point equations to update the predicted
labels. A summary of the equations appears in Algorithm 1.
The algorithm is run
until convergence which is achieved when the label distribution on each node ceases to
change within some tolerance value. Since Adsorption is memoryless, it scales to tens
of millions of nodes with dense edges and can be easily parallelized .
Baluja et. al. show that up to the additional dummy label, these two views are
equivalent. It remains to specify the values of pinj
. For the experiments
reported in Section 6, we set their value using the following heuristics (adapted from
Baluja et. al. ) which depends on a parameter β which we set to β = 2. For each
node v we deﬁne two quantities: cv and dv and deﬁne
The ﬁrst quantity cv ∈ is monotonically decreasing with the number of neighbors
for node v in the graph G. Intuitively, the higher the value of cv, the lower the number
of neighbors of vertex v and higher the information they contain about the labeling
of v. The other quantity dv ≥0 is monotonically increasing with the entropy (for
labeled vertices), and in this case we prefer to use the prior-information rather than the
computed quantities from the neighbors.
P.P. Talukdar and K. Crammer
Speciﬁcally we ﬁrst compute the entropy of the transition probabilities for each node,
Pr [u|v] log Pr [u|v] ,
and then pass it through the following monotonically decreasing function,
log(β + ex)) .
Note that f(0) = log(β)/ log(β + 1) and that f(x) goes to zero, as x goes to inﬁnity.
cv = f (H[v]) .
Next we deﬁne,
the vertex v is labeled
the vertex v is unlabled
Finally, to ensure proper normalization of pcont
, we deﬁne,
zv = max(cv + dv, 1) ,
= 1 −pcont
Thus, abandonment occurs only when the continuation and injection probabilities are
low enough. This is most likely to happen at unlabeled nodes with high degree. Once
the random walk reaches such a node (v), the walk is terminated with probability
. This, in effect, prevents the Adsorption algorithm from propagating information through high degree nodes. We note that the probabilities pinj
node v may be set with heuristics other than the fan-out entropy heuristics shown above
to suit speciﬁc application contexts.
Analysis of the Adsorption Algorithm
Our next goal is to ﬁnd an objective function that the Adsorption algorithm minimizes.
Our starting point is line 6 of Algorithm 1. We note that when the algorithm converges,
both sides of the assignment operator equal each other before the assignment takes
place. Thus when the algorithm terminates, we have for all v ∈V :
ˆYv = pinj
× Yv + pcont
Wuv ˆYu + pabnd
New Regularized Algorithms for Transductive Learning
The last set of equalities is equivalent to,
= 0 for v ∈V ,
where we deﬁne,
× Yv + pcont
Wuv ˆYu + pabnd
× r −ˆYv .
Now, if the Adsorption algorithm was minimizing some objective function (denoted by
), the termination condition of Eq. (3) was in fact a condition on the
vector of its partial derivatives where we would identify
Since the functions Gv are linear (and thus has continuous derivatives), necessary conditions for the existence of a function Q such that (4) holds is that the derivatives of Gv
are symmetric , that is,
Computing and comparing the derivatives we get,
Gv = pcont
which is true since in general Nu ̸= Nv and pcont
. We conclude:
Theorem 1. There does not exists a function Q with continuous second partial derivatives such that the Adsorption algorithm convergences when gradient of Q are equal to
In other words, we searched for a (well-behaved) function Q such that its local optimal
would be the output of the Adsorption algorithm, and showed that this search will always fail. We use this negative results to deﬁne a new algorithm, which builds on the
Adsorption algorithm and is optimizing a function of the unknowns ˆYv for v ∈V .
New Algorithm: Modiﬁed Adsorption (MAD)
Our starting point is Sec. 2.2 where we assume to have been given a weighted-graph
G = (V, E, W) and a matrix Y ∈Rn×(m+1)
and are seeking for a labeling-matrix
ˆY ∈Rn×(m+1)
. In this section it is more convenient to decompose the matrices Y and
ˆY into their columns, rather than rows. Speciﬁcally, we denote by Yl ∈Rn
column of Y and similarly by ˆYl ∈Rn
+ the lth column of ˆY. We distinguish the rows
and columns of the matrices Y and ˆY using their indices, the columns are indexed with
the label index l, while the rows are indexed are with a vertex index v (or u).
P.P. Talukdar and K. Crammer
We build on previous research and construct an objective that reﬂects three
requirements as follows. First, for the labeled vertices we like the output of the algorithm to be close to the a-priori given labels, that is Yv ≈ˆYv. Second, for pair of
vertices that are close according to the input graph, we would like their labeling to be
close, that is ˆYu ≈ˆYv if Wuv is large. Third, we want the output to be as uninformative as possible, this serves as additional regularization, that is ˆYv ≈r. We now further
develop the objective in light of the three requirements.
We use the Euclidian distance to measure discrepancy between two quantities, and
start with the ﬁrst requirement above,
where we deﬁne the diagonal matrix S ∈Rn×n and Svv = pinj
if vertex v is labeled
and Svv = 0 otherwise. The matrix S captures the intuition that for different vertices
we enforce the labeling of the algorithm to match the a-priori labeling with different
Next, we modify the similarity weight between vertices to take into account the difference in degree of various vertices. In particular we deﬁne W
vu = pcont
Thus, a vertex u will not be similar to a vertex v if either the input weights Wvu are
low or the vertex v has a large-degree (pcont
is low). We write the second requirement
ˆYvl −ˆYul
ˆYvl −ˆYul
∥ˆYul∥2 −2
vu ˆYul ˆYvl
L = D + ¯D −W
and D, ¯D are n × n diagonal matrices with
Finally we deﬁne the matrix R ∈Rn×(m+1)
where the vth row of R equals pabnd
(we deﬁne r in Sec 2.2). In other words the ﬁrst m columns of R equal zero, and the
last (m+1th column) equal the elements of pabnd
. The third requirement above is thus
written as,
New Regularized Algorithms for Transductive Learning
We combine the three terms above into a single objective (which we would like to
minimize), giving to each term a different importance using the weights μ1, μ2, μ3.
l L ˆYl + µ3
 ˆYl −Rl
The objective in Equation 5 is similar to the Quadratic Cost Criteria , with the exception that the matrices S and L have different constructions. We remind the reader
that ˆYl, Yl, Rl are the lth columns (each of size n × 1) of the matrices ˆY, Y and R
respectively.
Solving the Optimization Problem
We now develop an algorithm to optimize (5) similar to the quadratic cost criteria .
Differentiating Equation 5 w.r.t. ˆYl we get,
= μ1S( ˆYl −Yl) + μ2L ˆYl + μ3( ˆYl −Rl)
= (μ1S + μ2L + μ3I) ˆYl −(μ1SYl + μ3Rl) .
Differentiating once more we get,
δ ˆYlδ ˆYl
= μ1S + μ2L + μ3I ,
and since both S and L are symmetric and positive semideﬁnite matrices (PSD), we get
that the Hessian is PSD as well. Hence, the optimal minima is obtained by setting the
ﬁrst derivative (i.e. Equation (6)) to 0 as follows,
(μ1S + μ2L + μ3I) ˆYl = (μ1SYl + μ3Rl) .
Hence, the new labels ( ˆY) can be obtained by a matrix inversion followed by matrix
multiplication. However, this can be quite expensive when large matrices are involved.
A more efﬁcient way to obtain the new label scores is to solve a set of linear equations
using Jacobi iteration which we now describe.
Jacobi Method
Given the following linear system (in x)
the Jacobi iterative algorithm deﬁnes the approximate solution at the (t+1)th iteration
given the solution at tth iteration as follows,
P.P. Talukdar and K. Crammer
We apply the iterative algorithm to our problem by substituting x = ˆYl, M = μ1S +
μ2L + μ3I and b = μ1SYl + μ3Rl in (7),
⎝μ1(SYl)v + μ3Rvl −
Let us compute the values of (SYl)i, Mij(j̸=i) and Mii. First,
Mvu(v̸=u) = μ1Svu + μ2Lvu + μ3Ivu .
Note that since S and I are diagonal, we have that Svu = 0 and Ivu = 0 for u ̸= v.
Substituting the value of L we get,
Mvu(v̸=u) = μ2Lvu = μ2
Dvu + ¯Dvu −W
and as before the matrices D and ¯D are diagonal and thus Dvu + ¯Dvu = 0. Finally,
substituting the values of W
uv we get,
Mvu(v̸=u) = −μ2 × (pcont
× Wvu + pcont
We now compute the second quantity,
(SYl)vu = SvvYvv +
SvtYtu = pinj
where the second term equals zero since S is diagonal. Finally, the third term,
Mvv = μ1Svv + μ2Lvv + μ3Ivv
= μ1 × pinj
+ μ2(Dvv + ¯Dvv −W
= μ1 × pinj
Wvu + pcont
Wuv) + μ3 .
Plugging the above equations into (8) and using the fact that the diagonal elements of
W are zero, we get,
Wvu + pcont
u + µ3 pabnd
We call the new algorithm MAD for Modiﬁed-Adsorption and it is summarized in
Algorithm 2. Note that for graphs G that are invariant to permutations of the vertices,
and setting μ1 = 2 × μ2 = μ3 = 1, MAD reduces to the Adsorption algorithm.
Convergence
A sufﬁcient condition for the iterative process of Equation (7) to converge is that M is
strictly diagonally dominant , that is if,
for all values of v
New Regularized Algorithms for Transductive Learning
Algorithm 2. Modiﬁed Adsorption (MAD) Algorithm
G = (V, E, W )
- Prior labeling: Yv ∈Rm+1 for v ∈V
- Probabilities: pinj
- Label Scores: ˆYv for v ∈V
1: ˆYv ←Yv for v ∈V {Initialization}
2: Mvv ←µ1 × pinj
u̸=v(pcont
Wvu + pcont
Wvu + pcont
for all v ∈V do
× Yv + µ2 × Dv + µ3 × pabnd
8: until convergence
|Mvu| = μ1×pinj
×Wvu + pcont
×Wvu + pcont
Note that pinj
≥0 for all v and that μ3 is a free parameter in (11). Thus we can
guarantee a strict diagonal dominance (and hence convergence) by setting μ3 > 0.
Extensions: Non-mutually Exclusive Labels
In many learning settings, labels are not mutually exclusive. For example, in hierarchical classiﬁcation, labels are organized in a tree. In this section, we extend the MAD
algorithm to handle dependence among labels. This can be easily done using our new
formulation which is based on objective optimization. Speciﬁcally, we shall add additional terms to the objective for each pair of dependent labels. Let C be a m×m matrix
where m is the number of labels (excluding the dummy label) as before. Each entry,
Cll′ , of this matrix C represents the dependence or similarity among the labels l and l
By encoding dependence in this pairwise fashion, we can capture dependencies among
labels represented as arbitrary graphs. The extended objective is shown in Equation 12.
l L ˆYl + μ3
 ˆYl −Rl
Cll′ ( ˆYil −ˆYil′ )2
P.P. Talukdar and K. Crammer
The last term in Equation 12 penalizes the algorithm if similar labels (as determined by
the matrix C) are assigned different scores, with severity of the penalty controlled by
μ4. Now, analyzing the objective in Equation 12 in the manner outlined in Section 4,
we arrive at the update rule shown in Equation 13.
Wvu + pcont
Cll′ ˆYil′
vv = μ1 × pinj
Wvu + pcont
Wuv) + μ3 + μ4
Replacing Line 6 in MAD (Algorithm 2) with Equation 13, we end up with a new
algorithm: Modiﬁed Adsorption for Dependent Labels (MADDL). In Section 6.4, we
shall use MADDL to obtain smooth ranking for sentiment classiﬁcation.
Experimental Results
We compare MAD with various state-of-the-art learning algorithms on two tasks, text
classiﬁcation (Sec. 6.1) and sentiment analysis (Sec. 6.2), and demonstrate its effectiveness. In Sec. 6.3, we also provide experimental evidence showing that MAD is
quite insensitive to wide variation of values of its hyper-parameters. In Sec. 6.4, we
present evidence showing how MADDL can be used to obtain smooth ranking for sentiment prediction, a particular instantiation of classiﬁcation with non-mutually exclusive
labels. For the experiments reported in this section involving Adsorption, MAD and
MADDL, the a-priori label matrix Y was column-normalized so that all labels have
equal overall injection score. Also, the dummy label was ignored during evaluation as
its main role is to add regularization during learning phase only.
Text Classiﬁcation
World Wide Knowledge Base (WebKB) is a text classiﬁcation dataset widely used for
evaluating transductive learning algorithms. Most recently, the dataset was used by
Subramanya and Bilmes , who kindly shared their preprocessed complete WebKB
graph with us. There are a total of 4, 204 vertices in the graph, with the nodes labeled
with one of four categories: course, faculty, project, student. A K-NN graph is created
from this complete graph by retaining only top K neighbors of each node, where the
value of K is treated as a hyper-parameter.
We follow the experimental protocol in . The dataset was randomly partitioned
into four sets. A transduction set was generated by ﬁrst selecting one of the four splits at
random and then sampling nl documents from it; the remaining three sets are used as the
test set for evaluation. This process was repeated 21 times to generate as many trainingtest splits. The ﬁrst split was used to tune the hyper-parameters, with search over the
New Regularized Algorithms for Transductive Learning
Table 1. PRBEP for the WebKB data set with nl = 48 training and 3148 testing instances. All
results are averages over 20 randomly generated transduction sets. The last row is the macroaverage over all the classes. MAD is the proposed approach. Results for SVM, TSVM, SGT, LP
and AM are reproduced from Table 2 of .
Class SVM TSVM SGT LP AM Adsorption MAD
course 46.5
29.9 45.0 67.6
faculty 14.5
42.9 40.3 42.5
project 15.8
17.5 27.8 42.3
student 15.0
56.6 51.8 55.0
average 23.0
36.8 41.2 51.9
Fig. 1. PRBEP (macro-averaged) for the WebKB dataset with 3148 testing instances. All results
are averages over 20 randomly generated transduction sets.
following: K ∈{10, 50, 100, 500, 1000, 2000, 4204},μ2, μ3 ∈{1e−8, 1e−4, 1e−2, 1,
10, 1e2, 1e3}. The value of μ1 was set to 1 for this experiment. Both for Adsorption
and MAD, the optimal value of K was 1, 000. Furthermore, the optimal value for
the other parameters were found to be μ2 = μ3 = 1. As in previous work , we
use Precision-Recall Break Even Point (PRBEP) as the evaluation metric. Same
evaluation measure, dataset and the same experimental protocol makes the results reported here directly comparable to those reported previously . For easier readability, the results from Table 2 of Subramanya and Bilmes are cited in Table 1 of this
paper, comparing performance of Adsorption based methods (Adsorption and MAD)
to many previously proposed approaches: SVM , Transductive-SVM , Spectral
Graph Transduction (SGT) , Label Propagation (LP) and Alternating Minimization (AM) . The ﬁrst four rows in Table 1 shows PRBEP for individual categories,
with the last line showing the macro-averaged PRBEP across all categories. The MAD
algorithm achieves the best performance overall (for nl = 48).
Performance comparison of MAD and Adsorption for increasing nl are shown in
Figure 1. Comparing these results against Fig. 2 in Subramanya and Bilmes , it
seems that MAD outperforms all other methods compared (except AM ) for all
P.P. Talukdar and K. Crammer
Fig. 2. Precision for the Sentiment Analysis dataset with 3568 testing instances. All results are
averages over 4 randomly generated transduction sets.
Table 2. Average prediction loss at ranks 1 & 2 (for various values of µ4) for sentiment prediction.
All results are averaged over 4 runs. See Section 6.4 for details.
100 1e3 1e4
Prediction Loss (L1) at rank 1 0.93 0.93 0.92 0.90 0.90 0.90
Prediction Loss (L1) at rank 2 1.21 1.20 1.12 0.96 0.97 0.97
values of nl. MAD performs better than AM for nl = 48, but achieves second best
solution for the other three values of nl. We are currently investigating why MAD is
best for settings with fewer labeled examples.
Sentiment Analysis
The goal of sentiment analysis is to automatically assign polarity scores to text collections, with a high score reﬂecting positive sentiment (user likes) and a low score reﬂecting negative sentiment (user dislikes). In this section, we report results on sentiment
classiﬁcation in the transductive setting. From Section 6.1 and , we observe that
Label Propagation (LP) is one of the best performing L2-norm based transductive
learning algorithm. Hence, we compare the performance of MAD against Adsorption
For the experiments in this section, we use a set of 4, 768 user reviews from the
electronics domain . Each review is assigned one of the four scores: 1 (worst), 2,
3, 4 (best). We create a K-NN graph from these reviews by using cosine similarity
as the measure of similarity between reviews. We created 5 training-test splits from
this data using the process described in Section 6.1. One split was used to tune the
hyper-parameters while the rest were used for training and evaluation. Hyper-parameter
search was carried over the following ranges: K ∈{10, 100, 500}, μ1 ∈{1, 100},
New Regularized Algorithms for Transductive Learning
Count of Top Predicted Pair in MAD Output
Fig. 3. Plot of counts of top predicted
label pairs (order ignored) in MAD’s
predictions with µ1 = µ2 = 1, µ3 =
Count of Top Predicted Pair in MADDL Output
Fig. 4. Plot of counts of top label pairs
(order ignored) in MADDL’s predictions (Section 5), with µ1 = µ2 = 1,
µ3 = 100, µ4 = 1e3
μ2 ∈{1e−4, 1, 10}, μ3 ∈{1e−8, 1, 100, 1e3}. Precision is used as the evaluation
metric. Comparison of different algorithms for varying number of labeled instances are
shown in Figure 2. From this, we note that MAD and Adsorption outperform LP, while
Adsorption and MAD are competitive.
Parameter Sensitivity
We evaluated the sensitivity of MAD to variations of its μ2 and μ3 hyper-parameters,
with all other hyper-parameters ﬁxed. We used a 2000-NN graph constructed from the
WebKB dataset and a 500-NN graph constructed from the Sentiment dataset. In both
cases, 100 nodes were labeled. We tried three values each for μ2 and μ3, ranging in at
least 3 order of magnitude. For the WebKB, the PRBEP varied between 43.1−49.9 and
for the sentiment data, the precision varied in the range 31.4−36.4 with μ2 ≤μ3 while
precision dropped to 25 with μ2 > μ3. This underscores the need for regularization in
these models, which is enforced with high μ3. We note that in both cases the algorithm
is less sensitive to the value of μ2 than the value of μ3. In general, we have found that
setting μ3 to one or two order magnitude more than μ2 is a reasonable choice. We have
also found that the MAD algorithm is quite insensitive to variations in μ1. For example
on the sentiment dataset, we tried two values for μ1 ranging two order of magnitude,
with other hyper-parameters ﬁxed. In this case, precision varied in the range 36.2 - 36.3.
Smooth Ranking for Sentiment Analysis
We revisit the sentiment prediction problem in Section 6.2, but with the additional requirement that ranking of the labels (1, 2, 3, 4) generated by the algorithm should be
P.P. Talukdar and K. Crammer
smooth i.e. we prefer the ranking 1 > 2 > 3 > 4 over the ranking 1 > 4 > 3 > 2, where
3 > 2 means that the algorithm ranks label 3 higher than label 2. The ranking 1 > 2 >
3 > 4 is smoother as it doesn’t involve rough transition 1 > 4 which is present in 1 > 4
> 3 > 2. We use the framework of stating requirements as an objective to be optimized.
We use the MADDL algorithm of Sec. 5 initializing the matrix C as follows (assuming
that labels 1 and 2 are related, while labels 3 and 4 are related):
C12 = C21 = 1
C34 = C43 = 1
with all other entries in matrix C set to 0. Such constraints (along with appropriate μ4
in Equation (12)) will force the algorithm to assign similar scores to dependent labels,
thereby assigning them adjacent ranks in the ﬁnal output. MAD and MADDL were
then used to predict ranked labels for vertices on a 1000-NN graph constructed from
the sentiment data used in Sec. 6.2, with 100 randomly selected nodes labeled. For this
experiment we set μ1 = μ2 = 1, μ3 = 100. The L1-loss between the gold label and
labels predicted at ranks r = 1, 2 for increasing values of μ4 are given in Table 2. Note
that, MADDL with μ4 = 0 corresponds to MAD. From Table 2 we observe that with
increasing μ4, MADDL is ranking at r = 2 a label which is related (as per C) to the
top ranked label at r = 1, but at the same time maintain the quality of prediction at
r = 1 (ﬁrst row of Table 2), thereby ensuring a smoother ranking. From Table 2, we
also observe that MADDL is insensitive to variations of μ4 beyond a certain range. This
suggests that μ4 may be set to a (high) value and that tuning it may not be necessary.
Another view of the same phenomenon is shown in Fig. 3 and Fig. 4. In these ﬁgures,
we plot the counts of top predicted label pair (order of prediction is ignored for better
readability) generated by the MAD and MADDL algorithms. By comparing these two
ﬁgures we observe that label pairs (e.g. (2,1) and (4,3)) favored by C (above) are more
frequent in MADDL’s predictions than in MAD’s. At the same time, non-smooth predictions (e.g. (4, 1)) are virtually absent in MADDL’s predictions while they are quite
frequent in MAD’s. These clearly demonstrate MADDL’s ability to generate smooth
predictions in a principled way, and more generally the ability to handle data with nonmutually exclusive or dependent labels.
Related Work
LP is one of the ﬁrst graph based semi-supervised algorithms. Even though there
are several similarities between LP and MAD, there are important differences: (1) LP
doesn’t allow the labels on seeded nodes to change (while MAD does). As was pointed
out previously , this can be problematic in case of noisy seeds. (2) There is no way for
LP to express label uncertainty about a node. MAD can accomplish this by assigning
high score to the dummy label. More recently, a KL minimization based algorithm
was presented in . Further investigation is necessary to determine the merits of
each approach. For a general introduction to the area of graph-based semi-supervised
learning, the reader is referred to a survey by Zhu .
Conclusion
In this paper we have analyzed the Adsorption algorithm and proposed a new
graph based semi-supervised learning algorithm, MAD. We have developed efﬁcient
New Regularized Algorithms for Transductive Learning
(iterative) solution to solve our convex optimization based learning problem. We have
also listed the conditions under which the algorithm is guaranteed to converge. Transition to an optimization based learning algorithm allows us to easily extend the algorithm
to handle data with non-mutually exclusive labels, resulting in the MADDL algorithm.
We have provided experimental evidence demonstrating effectiveness of our proposed
methods. As part of future work, we plan to evaluate the proposed methods further
and apply the MADDL method in problems with dependent labels (e.g. Information
Extraction).
Acknowledgment
This research is partially supported by NSF grant #IIS-0513778.The authors would like
to thank F. Pereira and D. Sivakumar for useful discussions.