Published as a conference paper at ICLR 2020
ALBERT: A LITE BERT
FOR SELF-SUPERVISED
LEARNING OF LANGUAGE REPRESENTATIONS
Zhenzhong Lan1
Mingda Chen2∗
Sebastian Goodman1
Kevin Gimpel2
Piyush Sharma1
Radu Soricut1
1Google Research
2Toyota Technological Institute at Chicago
{lanzhzh, seabass, piyushsharma, rsoricut}@google.com
{mchen, kgimpel}@ttic.edu
Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and
longer training times. To address these problems, we present two parameterreduction techniques to lower memory consumption and increase the training
speed of BERT . Comprehensive empirical evidence shows
that our proposed methods lead to models that scale much better compared to
the original BERT. We also use a self-supervised loss that focuses on modeling
inter-sentence coherence, and show it consistently helps downstream tasks with
multi-sentence inputs. As a result, our best model establishes new state-of-the-art
results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available
at 
INTRODUCTION
Full network pre-training has led to a series of breakthroughs in language representation learning. Many nontrivial NLP tasks, including those that have limited training data, have greatly beneﬁted from these
pre-trained models. One of the most compelling signs of these breakthroughs is the evolution of machine performance on a reading comprehension task designed for middle and high-school English
exams in China, the RACE test : the paper that originally describes the task and formulates the modeling challenge reports then state-of-the-art machine accuracy at 44.1%; the latest
published result reports their model performance at 83.2% ; the work we present
here pushes it even higher to 89.4%, a stunning 45.3% improvement that is mainly attributable to
our current ability to build high-performance pretrained language representations.
Evidence from these improvements reveals that a large network is of crucial importance for achieving state-of-the-art performance . It has become common
practice to pre-train large models and distill them down to smaller ones for real applications. Given the importance of model size, we ask: Is having better NLP
models as easy as having larger models?
An obstacle to answering this question is the memory limitations of available hardware. Given that
current state-of-the-art models often have hundreds of millions or even billions of parameters, it is
easy to hit these limitations as we try to scale our models. Training speed can also be signiﬁcantly
hampered in distributed training, as the communication overhead is directly proportional to the
number of parameters in the model.
Existing solutions to the aforementioned problems include model parallelization and clever memory management .
∗Work done as an intern at Google Research, driving data processing and downstream task evaluations.
 
Published as a conference paper at ICLR 2020
These solutions address the memory limitation problem, but not the communication overhead. In
this paper, we address all of the aforementioned problems, by designing A Lite BERT (ALBERT)
architecture that has signiﬁcantly fewer parameters than a traditional BERT architecture.
ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling
pre-trained models. The ﬁrst one is a factorized embedding parameterization. By decomposing
the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden
layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden
size without signiﬁcantly increasing the parameter size of the vocabulary embeddings. The second
technique is cross-layer parameter sharing. This technique prevents the parameter from growing
with the depth of the network. Both techniques signiﬁcantly reduce the number of parameters for
BERT without seriously hurting performance, thus improving parameter-efﬁciency. An ALBERT
conﬁguration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster.
The parameter reduction techniques also act as a form of regularization that stabilizes the training
and helps with generalization.
To further improve the performance of ALBERT, we also introduce a self-supervised loss for
sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed
to address the ineffectiveness of the next sentence prediction
(NSP) loss proposed in the original BERT.
As a result of these design decisions, we are able to scale up to much larger ALBERT conﬁgurations
that still have fewer parameters than BERT-large but achieve signiﬁcantly better performance. We
establish new state-of-the-art results on the well-known GLUE, SQuAD, and RACE benchmarks
for natural language understanding. Speciﬁcally, we push the RACE accuracy to 89.4%, the GLUE
benchmark to 89.4, and the F1 score of SQuAD 2.0 to 92.2.
RELATED WORK
SCALING UP REPRESENTATION LEARNING FOR NATURAL LANGUAGE
Learning representations of natural language has been shown to be useful for a wide range of NLP
tasks and has been widely adopted . One of the most signiﬁcant changes
in the last two years is the shift from pre-training word embeddings, whether standard or contextualized ,
to full-network pre-training followed by task-speciﬁc ﬁne-tuning . In this line of work, it is often shown that larger model size improves
performance. For example, Devlin et al. show that across three selected natural language
understanding tasks, using larger hidden size, more hidden layers, and more attention heads always
leads to better performance. However, they stop at a hidden size of 1024, presumably because of the
model size and computation cost problems.
It is difﬁcult to experiment with large models due to computational constraints, especially in terms
of GPU/TPU memory limitations. Given that current state-of-the-art models often have hundreds of
millions or even billions of parameters, we can easily hit memory limits. To address this issue, Chen
et al. propose a method called gradient checkpointing to reduce the memory requirement to be
sublinear at the cost of an extra forward pass. Gomez et al. propose a way to reconstruct each
layer’s activations from the next layer so that they do not need to store the intermediate activations.
Both methods reduce the memory consumption at the cost of speed. Raffel et al. proposed
to use model parallelization to train a giant model. In contrast, our parameter-reduction techniques
reduce memory consumption and increase training speed.
CROSS-LAYER PARAMETER SHARING
The idea of sharing parameters across layers has been previously explored with the Transformer
architecture , but this prior work has focused on training for standard encoderdecoder tasks rather than the pretraining/ﬁnetuning setting. Different from our observations, Dehghani et al. show that networks with cross-layer parameter sharing (Universal Transformer,
UT) get better performance on language modeling and subject-verb agreement than the standard
Published as a conference paper at ICLR 2020
transformer. Very recently, Bai et al. propose a Deep Equilibrium Model (DQE) for transformer networks and show that DQE can reach an equilibrium point for which the input embedding
and the output embedding of a certain layer stay the same. Our observations show that our embeddings are oscillating rather than converging. Hao et al. combine a parameter-sharing
transformer with the standard one, which further increases the number of parameters of the standard
transformer.
SENTENCE ORDERING OBJECTIVES
ALBERT uses a pretraining loss based on predicting the ordering of two consecutive segments
of text. Several researchers have experimented with pretraining objectives that similarly relate to
discourse coherence. Coherence and cohesion in discourse have been widely studied and many
phenomena have been identiﬁed that connect neighboring text segments . Most objectives found effective in practice are quite simple. Skipthought and FastSent sentence embeddings are learned by using
an encoding of a sentence to predict words in neighboring sentences. Other objectives for sentence
embedding learning include predicting future sentences rather than only neighbors 
and predicting explicit discourse markers . Our loss is most
similar to the sentence ordering objective of Jernite et al. , where sentence embeddings are
learned in order to determine the ordering of two consecutive sentences. Unlike most of the above
work, however, our loss is deﬁned on textual segments rather than sentences. BERT uses a loss based on predicting whether the second segment in a pair has been swapped
with a segment from another document. We compare to this loss in our experiments and ﬁnd that
sentence ordering is a more challenging pretraining task and more useful for certain downstream
tasks. Concurrently to our work, Wang et al. also try to predict the order of two consecutive
segments of text, but they combine it with the original next sentence prediction in a three-way
classiﬁcation task rather than empirically comparing the two.
THE ELEMENTS OF ALBERT
In this section, we present the design decisions for ALBERT and provide quantiﬁed comparisons
against corresponding conﬁgurations of the original BERT architecture .
MODEL ARCHITECTURE CHOICES
The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer encoder with GELU nonlinearities . We follow the
BERT notation conventions and denote the vocabulary embedding size as E, the number of encoder
layers as L, and the hidden size as H. Following Devlin et al. , we set the feed-forward/ﬁlter
size to be 4H and the number of attention heads to be H/64.
There are three main contributions that ALBERT makes over the design choices of BERT.
Factorized embedding parameterization.
In BERT, as well as subsequent modeling improvements such as XLNet and RoBERTa , the WordPiece embedding
size E is tied with the hidden layer size H, i.e., E ≡H. This decision appears suboptimal for both
modeling and practical reasons, as follows.
From a modeling perspective, WordPiece embeddings are meant to learn context-independent representations, whereas hidden-layer embeddings are meant to learn context-dependent representations.
As experiments with context length indicate , the power of BERT-like representations comes from the use of context to provide the signal for learning such context-dependent
representations. As such, untying the WordPiece embedding size E from the hidden layer size H
allows us to make a more efﬁcient usage of the total model parameters as informed by modeling
needs, which dictate that H ≫E.
From a practical perspective, natural language processing usually require the vocabulary size V to
be large.1 If E ≡H, then increasing H increases the size of the embedding matrix, which has size
1Similar to BERT, all the experiments in this paper use a vocabulary size V of 30,000.
Published as a conference paper at ICLR 2020
V ×E. This can easily result in a model with billions of parameters, most of which are only updated
sparsely during training.
Therefore, for ALBERT we use a factorization of the embedding parameters, decomposing them
into two smaller matrices. Instead of projecting the one-hot vectors directly into the hidden space of
size H, we ﬁrst project them into a lower dimensional embedding space of size E, and then project
it to the hidden space. By using this decomposition, we reduce the embedding parameters from
O(V × H) to O(V × E + E × H). This parameter reduction is signiﬁcant when H ≫E. We
choose to use the same E for all word pieces because they are much more evenly distributed across
documents compared to whole-word embedding, where having different embedding size ; Baevski & Auli ; Dai et al. ) for different words is important.
Cross-layer parameter sharing.
For ALBERT, we propose cross-layer parameter sharing as another way to improve parameter efﬁciency. There are multiple ways to share parameters, e.g., only
sharing feed-forward network (FFN) parameters across layers, or only sharing attention parameters.
The default decision for ALBERT is to share all parameters across layers. All our experiments
use this default decision unless otherwise speciﬁed. We compare this design decision against other
strategies in our experiments in Sec. 4.5.
Similar strategies have been explored by Dehghani et al. (Universal Transformer, UT) and
Bai et al. (Deep Equilibrium Models, DQE) for Transformer networks. Different from our
observations, Dehghani et al. show that UT outperforms a vanilla Transformer. Bai et al.
 show that their DQEs reach an equilibrium point for which the input and output embedding
of a certain layer stay the same. Our measurement on the L2 distances and cosine similarity show
that our embeddings are oscillating rather than converging.
L2 distance
BERT-large
ALBERT-large
Cosine Similarity (Degree)
BERT-large
ALBERT-large
Figure 1: The L2 distances and cosine similarity (in terms of degree) of the input and output embedding of each layer for BERT-large and ALBERT-large.
Figure 1 shows the L2 distances and cosine similarity of the input and output embeddings for each
layer, using BERT-large and ALBERT-large conﬁgurations (see Table 1). We observe that the transitions from layer to layer are much smoother for ALBERT than for BERT. These results show that
weight-sharing has an effect on stabilizing network parameters. Although there is a drop for both
metrics compared to BERT, they nevertheless do not converge to 0 even after 24 layers. This shows
that the solution space for ALBERT parameters is very different from the one found by DQE.
Inter-sentence coherence loss.
In addition to the masked language modeling (MLM) loss , BERT uses an additional loss called next-sentence prediction (NSP). NSP is a
binary classiﬁcation loss for predicting whether two segments appear consecutively in the original
text, as follows: positive examples are created by taking consecutive segments from the training
corpus; negative examples are created by pairing segments from different documents; positive and
negative examples are sampled with equal probability. The NSP objective was designed to improve
performance on downstream tasks, such as natural language inference, that require reasoning about
the relationship between sentence pairs. However, subsequent studies found NSP’s impact unreliable and decided to eliminate it, a decision supported by an improvement in downstream task performance across several tasks.
We conjecture that the main reason behind NSP’s ineffectiveness is its lack of difﬁculty as a task,
as compared to MLM. As formulated, NSP conﬂates topic prediction and coherence prediction in a
Published as a conference paper at ICLR 2020
Parameters
Parameter-sharing
Table 1: The conﬁgurations of the main BERT and ALBERT models analyzed in this paper.
single task2. However, topic prediction is easier to learn compared to coherence prediction, and also
overlaps more with what is learned using the MLM loss.
We maintain that inter-sentence modeling is an important aspect of language understanding, but we
propose a loss based primarily on coherence. That is, for ALBERT, we use a sentence-order prediction (SOP) loss, which avoids topic prediction and instead focuses on modeling inter-sentence
coherence. The SOP loss uses as positive examples the same technique as BERT (two consecutive segments from the same document), and as negative examples the same two consecutive segments but with their order swapped. This forces the model to learn ﬁner-grained distinctions about
discourse-level coherence properties. As we show in Sec. 4.6, it turns out that NSP cannot solve the
SOP task at all (i.e., it ends up learning the easier topic-prediction signal, and performs at randombaseline level on the SOP task), while SOP can solve the NSP task to a reasonable degree, presumably based on analyzing misaligned coherence cues. As a result, ALBERT models consistently
improve downstream task performance for multi-sentence encoding tasks.
MODEL SETUP
We present the differences between BERT and ALBERT models with comparable hyperparameter
settings in Table 1. Due to the design choices discussed above, ALBERT models have much smaller
parameter size compared to corresponding BERT models.
For example, ALBERT-large has about 18x fewer parameters compared to BERT-large, 18M versus 334M. An ALBERT-xlarge conﬁguration with H = 2048 has only 60M parameters and an
ALBERT-xxlarge conﬁguration with H = 4096 has 233M parameters, i.e., around 70% of BERTlarge’s parameters. Note that for ALBERT-xxlarge, we mainly report results on a 12-layer network
because a 24-layer network (with the same conﬁguration) obtains similar results but is computationally more expensive.
This improvement in parameter efﬁciency is the most important advantage of ALBERT’s design
choices. Before we can quantify this advantage, we need to introduce our experimental setup in
more detail.
EXPERIMENTAL RESULTS
EXPERIMENTAL SETUP
To keep the comparison as meaningful as possible, we follow the BERT setup in
using the BOOKCORPUS and English Wikipedia for pretraining baseline models. These two corpora consist of around 16GB of uncompressed text. We format
our inputs as “[CLS] x1 [SEP] x2 [SEP]”, where x1 = x1,1, x1,2 · · · and x2 = x1,1, x1,2 · · · are
two segments.3 We always limit the maximum input length to 512, and randomly generate input
sequences shorter than 512 with a probability of 10%. Like BERT, we use a vocabulary size of
30,000, tokenized using SentencePiece as in XLNet .
2Since a negative example is constructed using material from a different document, the negative-example
segment is misaligned both from a topic and from a coherence perspective.
3A segment is usually comprised of more than one natural sentence, which has been shown to beneﬁt
performance by Liu et al. .
Published as a conference paper at ICLR 2020
We generate masked inputs for the MLM targets using n-gram masking , with the
length of each n-gram mask selected randomly. The probability for the length n is given by
We set the maximum length of n-gram (i.e., n) to be 3 (i.e., the MLM target can consist of up to a
3-gram of complete words, such as “White House correspondents”).
All the model updates use a batch size of 4096 and a LAMB optimizer with learning rate
0.00176 . We train all models for 125,000 steps unless otherwise speciﬁed. Training was done on Cloud TPU V3. The number of TPUs used for training ranged from 64 to 512,
depending on model size.
The experimental setup described in this section is used for all of our own versions of BERT as well
as ALBERT models, unless otherwise speciﬁed.
EVALUATION BENCHMARKS
INTRINSIC EVALUATION
To monitor the training progress, we create a development set based on the development sets from
SQuAD and RACE using the same procedure as in Sec. 4.1. We report accuracies for both MLM and
sentence classiﬁcation tasks. Note that we only use this set to check how the model is converging;
it has not been used in a way that would affect the performance of any downstream evaluation, such
as via model selection.
DOWNSTREAM EVALUATION
Following Yang et al. and Liu et al. , we evaluate our models on three popular benchmarks: The General Language Understanding Evaluation (GLUE) benchmark ,
two versions of the Stanford Question Answering Dataset ,
and the ReAding Comprehension from Examinations (RACE) dataset . For completeness, we provide description of these benchmarks in Appendix A.3. As in ,
we perform early stopping on the development sets, on which we report all comparisons except for
our ﬁnal comparisons based on the task leaderboards, for which we also report test set results. For
GLUE datasets that have large variances on the dev set, we report median over 5 runs.
OVERALL COMPARISON BETWEEN BERT AND ALBERT
We are now ready to quantify the impact of the design choices described in Sec. 3, speciﬁcally the
ones around parameter efﬁciency. The improvement in parameter efﬁciency showcases the most
important advantage of ALBERT’s design choices, as shown in Table 2: with only around 70% of
BERT-large’s parameters, ALBERT-xxlarge achieves signiﬁcant improvements over BERT-large, as
measured by the difference on development set scores for several representative downstream tasks:
SQuAD v1.1 (+1.9%), SQuAD v2.0 (+3.1%), MNLI (+1.4%), SST-2 (+2.2%), and RACE (+8.4%).
Another interesting observation is the speed of data throughput at training time under the same training conﬁguration (same number of TPUs). Because of less communication and fewer computations,
ALBERT models have higher data throughput compared to their corresponding BERT models. If we
use BERT-large as the baseline, we observe that ALBERT-large is about 1.7 times faster in iterating
through the data while ALBERT-xxlarge is about 3 times slower because of the larger structure.
Next, we perform ablation experiments that quantify the individual contribution of each of the design
choices for ALBERT.
FACTORIZED EMBEDDING PARAMETERIZATION
Table 3 shows the effect of changing the vocabulary embedding size E using an ALBERT-base
conﬁguration setting (see Table 1), using the same set of representative downstream tasks. Under
the non-shared condition (BERT-style), larger embedding sizes give better performance, but not by
Published as a conference paper at ICLR 2020
Parameters
Table 2: Dev set results for models pretrained over BOOKCORPUS and Wikipedia for 125k steps.
Here and everywhere else, the Avg column is computed by averaging the scores of the downstream
tasks to its left (the two numbers of F1 and EM for each SQuAD are ﬁrst averaged).
much. Under the all-shared condition (ALBERT-style), an embedding of size 128 appears to be the
best. Based on these results, we use an embedding size E = 128 in all future settings, as a necessary
step to do further scaling.
Parameters
not-shared
all-shared
Table 3: The effect of vocabulary embedding size on the performance of ALBERT-base.
CROSS-LAYER PARAMETER SHARING
Table 4 presents experiments for various cross-layer parameter-sharing strategies, using an
ALBERT-base conﬁguration (Table 1) with two embedding sizes (E = 768 and E = 128). We
compare the all-shared strategy (ALBERT-style), the not-shared strategy (BERT-style), and intermediate strategies in which only the attention parameters are shared (but not the FNN ones) or only
the FFN parameters are shared (but not the attention ones).
The all-shared strategy hurts performance under both conditions, but it is less severe for E = 128 (-
1.5 on Avg) compared to E = 768 (-2.5 on Avg). In addition, most of the performance drop appears
to come from sharing the FFN-layer parameters, while sharing the attention parameters results in no
drop when E = 128 (+0.1 on Avg), and a slight drop when E = 768 (-0.7 on Avg).
There are other strategies of sharing the parameters cross layers. For example, We can divide the L
layers into N groups of size M, and each size-M group shares parameters. Overall, our experimental results shows that the smaller the group size M is, the better the performance we get. However,
decreasing group size M also dramatically increase the number of overall parameters. We choose
all-shared strategy as our default choice.
Parameters
all-shared
shared-attention
shared-FFN
not-shared
all-shared
shared-attention
shared-FFN
not-shared
Table 4: The effect of cross-layer parameter-sharing strategies, ALBERT-base conﬁguration.
Published as a conference paper at ICLR 2020
SENTENCE ORDER PREDICTION (SOP)
We compare head-to-head three experimental conditions for the additional inter-sentence loss: none
(XLNet- and RoBERTa-style), NSP (BERT-style), and SOP (ALBERT-style), using an ALBERTbase conﬁguration. Results are shown in Table 5, both over intrinsic (accuracy for the MLM, NSP,
and SOP tasks) and downstream tasks.
Intrinsic Tasks
Downstream Tasks
Table 5: The effect of sentence-prediction loss, NSP vs. SOP, on intrinsic and downstream tasks.
The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP
task (52.0% accuracy, similar to the random-guess performance for the “None” condition). This
allows us to conclude that NSP ends up modeling only topic shift. In contrast, the SOP loss does
solve the NSP task relatively well (78.9% accuracy), and the SOP task even better (86.5% accuracy).
Even more importantly, the SOP loss appears to consistently improve downstream task performance
for multi-sentence encoding tasks (around +1% for SQuAD1.1, +2% for SQuAD2.0, +1.7% for
RACE), for an Avg score improvement of around +1%.
WHAT IF WE TRAIN FOR THE SAME AMOUNT OF TIME?
The speed-up results in Table 2 indicate that data-throughput for BERT-large is about 3.17x higher
compared to ALBERT-xxlarge. Since longer training usually leads to better performance, we perform a comparison in which, instead of controlling for data throughput (number of training steps),
we control for the actual training time (i.e., let the models train for the same number of hours). In
Table 6, we compare the performance of a BERT-large model after 400k training steps (after 34h
of training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model
with 125k training steps (32h of training).
BERT-large
ALBERT-xxlarge
Table 6: The effect of controlling for training time, BERT-large vs ALBERT-xxlarge conﬁgurations.
After training for roughly the same amount of time, ALBERT-xxlarge is signiﬁcantly better than
BERT-large: +1.5% better on Avg, with the difference on RACE as high as +5.2%.
ADDITIONAL TRAINING DATA AND DROPOUT EFFECTS
The experiments done up to this point use only the Wikipedia and BOOKCORPUS datasets, as in
 . In this section, we report measurements on the impact of the additional data
used by both XLNet and RoBERTa .
Fig. 2a plots the dev set MLM accuracy under two conditions, without and with additional data, with
the latter condition giving a signiﬁcant boost. We also observe performance improvements on the
downstream tasks in Table 7, except for the SQuAD benchmarks (which are Wikipedia-based, and
therefore are negatively affected by out-of-domain training material).
No additional data
With additional data
Table 7: The effect of additional training data using the ALBERT-base conﬁguration.
We also note that, even after training for 1M steps, our largest models still do not overﬁt to their
training data. As a result, we decide to remove dropout to further increase our model capacity. The
Published as a conference paper at ICLR 2020
Steps (1e4)
Dev accuracy (MLM) %
W/O additional data
W additional data
(a) Adding data
Steps (1e4)
Dev accuracy (MLM) %
W/ Dropout
W/O Dropout
(b) Removing dropout
Figure 2: The effects of adding data and removing dropout during training.
plot in Fig. 2b shows that removing dropout signiﬁcantly improves MLM accuracy. Intermediate
evaluation on ALBERT-xxlarge at around 1M training steps (Table 8) also conﬁrms that removing
dropout helps the downstream tasks. There is empirical and theoretical evidence showing that a combination of batch normalization and dropout in Convolutional Neural Networks may have harmful results. To the best of our knowledge, we are the ﬁrst to
show that dropout can hurt performance in large Transformer-based models. However, the underlying network structure of ALBERT is a special case of the transformer and further experimentation
is needed to see if this phenomenon appears with other transformer-based architectures or not.
With dropout
Without dropout
Table 8: The effect of removing dropout, measured for an ALBERT-xxlarge conﬁguration.
CURRENT STATE-OF-THE-ART ON NLU TASKS
The results we report in this section make use of the training data used by Devlin et al. , as
well as the additional data used by Liu et al. and Yang et al. . We report state-of-the-art
results under two settings for ﬁne-tuning: single-model and ensembles. In both settings, we only do
single-task ﬁne-tuning4. Following Liu et al. , on the development set we report the median
result over ﬁve runs.
Single-task single models on dev
BERT-large
XLNet-large
RoBERTa-large
ALBERT (1M)
ALBERT (1.5M)
Ensembles on test 
Adv-RoBERTa
Table 9: State-of-the-art results on the GLUE benchmark. For single-task single-model results, we
report ALBERT at 1M steps (comparable to RoBERTa) and at 1.5M steps. The ALBERT ensemble
uses models trained with 1M, 1.5M, and other numbers of steps.
The single-model ALBERT conﬁguration incorporates the best-performing settings discussed: an
ALBERT-xxlarge conﬁguration (Table 1) using combined MLM and SOP losses, and no dropout.
4Following Liu et al. , we ﬁne-tune for RTE, STS, and MRPC using an MNLI checkpoint.
Published as a conference paper at ICLR 2020
The checkpoints that contribute to the ﬁnal ensemble model are selected based on development set
performance; the number of checkpoints considered for this selection range from 6 to 17, depending
on the task. For the GLUE (Table 9) and RACE (Table 10) benchmarks, we average the model
predictions for the ensemble models, where the candidates are ﬁne-tuned from different training
steps using the 12-layer and 24-layer architectures. For SQuAD (Table 10), we average the prediction scores for those spans that have multiple probabilities; we also average the scores of the
“unanswerable” decision.
Both single-model and ensemble results indicate that ALBERT improves the state-of-the-art significantly for all three benchmarks, achieving a GLUE score of 89.4, a SQuAD 2.0 test F1 score of
92.2, and a RACE test accuracy of 89.4. The latter appears to be a particularly strong improvement,
a jump of +17.4% absolute points over BERT , +7.6% over
XLNet , +6.2% over RoBERTa , and 5.3% over DCMI+ , an ensemble of multiple models speciﬁcally designed for reading comprehension tasks.
Our single model achieves an accuracy of 86.5%, which is still 2.4% better than the state-of-the-art
ensemble model.
SQuAD1.1 dev
SQuAD2.0 dev
SQuAD2.0 test
RACE test (Middle/High)
Single model 
BERT-large
72.0 (76.6/70.1)
81.8 (85.5/80.2)
83.2 (86.5/81.3)
XLNet + SG-Net Veriﬁer++
ALBERT (1M)
86.0 (88.2/85.1)
ALBERT (1.5M)
86.5 (89.0/85.5)
Ensembles 
BERT-large
XLNet + SG-Net Veriﬁer
XLNet + DAAF + Veriﬁer
84.1 (88.5/82.3)
89.4 (91.2/88.6)
Table 10: State-of-the-art results on the SQuAD and RACE benchmarks.
DISCUSSION
While ALBERT-xxlarge has less parameters than BERT-large and gets signiﬁcantly better results, it
is computationally more expensive due to its larger structure. An important next step is thus to speed
up the training and inference speed of ALBERT through methods like sparse attention and block attention . An orthogonal line of research, which could provide
additional representation power, includes hard example mining and more
efﬁcient language modeling training . Additionally, although we have convincing
evidence that sentence order prediction is a more consistently-useful learning task that leads to better
language representations, we hypothesize that there could be more dimensions not yet captured by
the current self-supervised training losses that could create additional representation power for the
resulting representations.
ACKNOWLEDGEMENT
The authors would like to thank Beer Changpinyo, Nan Ding, Noam Shazeer, and Tomer Levinboim
for discussion and providing useful feedback on the project; Omer Levy and Naman Goyal for
clarifying experimental setup for RoBERTa; Zihang Dai for clarifying XLNet; Brandon Norick,
Emma Strubell, Shaojie Bai, Chas Leichner, and Sachin Mehta for providing useful feedback on the
paper; Jacob Devlin for providing the English and multilingual version of training data; Liang Xu,
Chenjie Cao and the CLUE community for providing the training data and evaluation benechmark
of the Chinese version of ALBERT models.
Published as a conference paper at ICLR 2020