Optimal Multi-scale Patterns in Time Series Streams
Spiros Papadimitriou
IBM T.J. Watson Research Center
Hawthorne, NY, USA
 
Philip S. Yu
IBM T.J. Watson Research Center
Hawthorne, NY, USA
 
We introduce a method to discover optimal local patterns,
which concisely describe the main trends in a time series.
Our approach examines the time series at multiple time
scales (i.e., window sizes) and eﬃciently discovers the key
patterns in each.
We also introduce a criterion to select
the best window sizes, which most concisely capture the
key oscillatory as well as aperiodic trends. Our key insight
lies in learning an optimal orthonormal transform from the
data itself, as opposed to using a predetermined basis or
approximating function (such as piecewise constant, shortwindow Fourier or wavelets), which essentially restricts us
to a particular family of trends. Our method lifts that limitation, while lending itself to fast, incremental estimation
in a streaming setting. Experimental evaluation shows that
our method can capture meaningful patterns in a variety of
settings. Our streaming approach requires order of magnitude less time and space, while still producing concise and
informative patterns.
INTRODUCTION
Data streams have recently received much attention in several communities (e.g., theory, databases, networks, data
mining) because of several important applications (e.g., network traﬃc analysis, moving object tracking, ﬁnancial data
analysis, sensor monitoring, environmental monitoring, scientiﬁc data processing).
Many recent eﬀorts concentrate on summarization and pattern discovery in time series data streams . Typical approaches for pattern discovery and summarization of time series rely on ﬁxed transforms, with a predetermined set of bases or approximating functions
 . For example, the short-window Fourier transform
uses translated sine waves of ﬁxed length and has been successful in speech processing .
Wavelets use translated
and dilated sine-like waves and have been successfully applied to more bursty data, such as images and video streams
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGMOD 2006, June 27–29, 2006, Chicago, Illinois, USA.
Copyright 2006 ACM 1-59593-256-9/06/0006 ...$5.00.
 . Furthermore, selecting a wavelet basis to match the
time series’ characteristics is a non-trivial problem . Thus, even though these approaches have been useful
for pattern discovery in a number of application domains,
there is no single method that is best for arbitrary time
We propose to address this problem by learning the appropriate approximating functions directly from the data, instead of using a ﬁxed set of such functions. Our method
estimates the “eigenfunctions” of the time series in ﬁnitelength time windows. Similar ideas have been used in other
areas (e.g., image denoising or motion capture). However,
our approach examines the data at multiple time scales and,
to the best of our knowledge, the problem of estimating these
eigenfunctions incrementally has not been studied before.
Automobile
Figure 1: Automobile traﬃc data (aggregate counts
from a west coast interstate).
We will illustrate the main intuition and motivation with a
real example. Figure 1 shows automobile traﬃc counts in a
large, west coast interstate. The data exhibit a clear daily
periodicity. Also, in each day there is another distinct pattern of morning and afternoon rush hours. However, these
peaks have distinctly diﬀerent shapes: the morning one is
more spread out, the evening one more concentrated and
slightly sharper. What we would ideally like to discover is:
1. The main trend in the data repeats at a window (“period”) of approximately 4000 timestamps.
2. A succinct “description” of that main trend that captures most of the recurrent information.
Figure 2a shows the output of our pattern discovery approach, which indeed suggests that the “best” window is
1000 2000 3000 4000
Time (1..4000)
First pattern (basis)
1000 2000 3000 4000
Time (1..4000)
Second pattern (basis)
Time (1..3645)
First pattern (hierarchical, streaming)
Time (1..3645)
Second pattern (hierarchical, streaming)
First basis (DCT)
Time (1..4000)
Second basis (DCT)
Time (1..4000)
Third basis (DCT)
Time (1..4000)
(a) Representative patterns
(b) Representative patterns
(c) Fixed bases (DCT) with highest coeﬃcients,
(batch, non-hierarchical), 5.4% error.
(streaming, hierarchical).
7.7% error (8.4% with two coeﬃcients).
Figure 2: Automobile traﬃc, best selected window (about 1 day) and corresponding representative patterns.
4000 timestamps. Furthermore, the ﬁrst pattern captures
the average and the second pattern correctly captures the
two peaks and also their approximate shape (the ﬁrst one
wide and the second narrower).
For comparison, in Figure 2b we show the output of our fast, streaming computation scheme. In order to reduce the storage and computation
requirements, our fast scheme tries to ﬁlter out some of the
“noise” earlier, while retaining as many of the regularities as
possible. However, which information should be discarded
and which should be retained is once again decided based
on the data itself. Thus, even though we unavoidably discard some information, Figure 2b still correctly captures the
main trends (average level, peaks and their shape).
For comparison, Figure 2c shows the best “local patterns”
we would obtain using ﬁxed bases. For illustration, we chose
the Discrete Cosine Transform (DCT) on the ﬁrst window of
4000 points. First, most ﬁxed-basis schemes cannot be easily
used to capture information at arbitrary time scales (with
the exception of wavelets).
More importantly, any ﬁxedbasis scheme (e.g., wavelets, Fourier, etc) would produce
similar results which are heavily biased towards the shape
of the a priori chosen bases or approximating functions.
Contributions
We introduce a method that can learn the key trends in a
time series. Our main contributions are:
• We introduce the notion of optimal local patterns in
time series, which concisely describe the main trends,
both oscillatory as well as aperiodic, within a ﬁxed
• We show how to extract trends at multiple time scales
(i.e., window sizes).
• We propose a criterion which allows us to choose the
best window sizes from the data.
• We introduce an approach to perform all of the above
incrementally, in a streaming setting.
We evaluate our approach on real data and show that it discovers meaningful patterns. The streaming approach achieves
1-4 orders of magnitude improvement in time and space requirements.
In the rest of this paper we answer the following questions:
1. Given a window, how do we ﬁnd locally optimal patterns?
2. How can we compare the information captured by patterns at diﬀerent window sizes?
3. How can we eﬃciently compute patterns at several different window sizes and quickly zero-in on the “best”
4. How can we do all of the above incrementally, in a
streaming setting?
After reviewing some of the background in Section 2 and
introducing necessary deﬁnitions and notation in Section 3,
we answer the ﬁrst two questions in Section 4. We answer
the third question in Section 5, which introduces an eﬃcient
scheme for pattern discovery at multiple scales. Section 6
answers the fourth question.
Section 7 demonstrates the
eﬀectiveness of our approach on real data. Finally, Section 8
discusses related work and Section 9 concludes.
BACKGROUND
In this section we describe some of the basic background.
For more details, the reader can see, e.g., . We use boldface lowercase letters for column vectors, v ≡[v1 v2 · · · vn]T ∈
Rn, and boldface capital letters for matrices, A ∈Rm×n.
Finally, we adopt the notation aj for the columns of A ≡
[a1 a2 · · · a3] and a(i) for the rows of A ≡[a(1) a(2) · · · a(m)]T .
Note that a(i) are also column vectors, not row vectors—
we always represent vectors as column vectors.
For matrix/vector elements we use either subscripts, aij, or brackets, a[i, j].
The rows of A are points in an (at most) n-dimensional
space, a(i) ∈Rn, which is the row space of A. The actual
dimension of the row space is the rank r of A.
out that we can always ﬁnd a “special” orthonormal basis
for the row space, which deﬁnes a new coordinate system
(see Figure 3). If vj is a unit-length vector deﬁning one of
the axes in the row space then, for each row a(i), it’s j-th
coordinate in the new axes is the dot product aT
(i)vj =: pij
so that, if V := [v1 · · · vr] and we deﬁne P := AV, then
each row of P is the same point as the corresponding row of
A but with respect to the new coordinate system. Therefore
lengths and distances are preserved, i.e. ∥a(i)∥= ∥p(i)∥and
∥a(i) −a(j)∥= ∥p(i) −p(j)∥, for all 1 ≤i, j ≤m.
However, the new coordinate system is “special” in the following sense. Say we keep only the ﬁrst k columns of P (let’s
call this matrix ˜P) thus eﬀectively projecting each point into
a space with lower dimension k. Also, rows of the matrix
˜A := ˜P ˜VT
Figure 3: Illustration of SVD (for dimension n = 2),
with respect to row space. Each point corresponds
to a row of the matrix A and vj, j = 1, 2 are the
left singular vectors of A.
The square is the onedimensional approximation of a(i) by projecting it
onto the ﬁrst singular vector.
are the same points translated back into the original coordinate system of the row space (the square in Figure 3,
if k = 1), where ˜V consists of the ﬁrst k columns of V.
Then ˜P maximizes the sum of squares ∥˜P∥2
or, equivalently, minimizes the sum of squared residual distances (thick dotted line in Figure 3, if k = 1), ∥X −˜X∥2
i=1 ∥x(i) −˜x(i)∥2.
Therefore, from the point of view of the row space, we can
write A = PVT .
We can do the same for the column
space of A and get A = UQT , where U is also columnorthonormal, like V.
It turns out that U and V have a
special signiﬁcance, which is formally stated as follows:
(Singular value decomposition). Every
matrix A ∈Rm×n can be decomposed into
where U ∈Rm×r, V ∈Rn×r and Σ ∈Rr×r, with r ≤
min(m, n) the rank of A. The columns vi of V ≡[v1 · · · vr]
are the right singular vectors of A and they form an orthonormal basis its row space. Similarly, the columns ui of
U ≡[u1 · · · ur] are the left singular vectors and form a basis of the column space of A. Finally Σ ≡diag[σ1 · · · σr] is
a diagonal matrix with positive values σi, called the singular
values of A.
From the above, the matrix of projections P is P = UΣ.
Next, we can formally state the properties of a low-dimensional
approximation using the ﬁrst k singular values (and corresponding singular vectors) of A:
(Low-rank approximation). If we keep
only the singular vectors corresponding to the k highest singular values (k < r), i.e.
if ˜U := [u1 u2 · · · uk], ˜V :=
[v1 v2 · · · vk] and ˜Σ = diag[σ1 σ2 · · · σk], then ˜A = ˜U ˜Σ ˜VT
is the best approximation of A, in the sense that it minimizes
i,j=1 |aij −˜aij|2 = Pr
In Equation (2), note the special signiﬁcance of the singular
values for representing the approximation’s squared error.
Description
Vector A ∈Rn (lowercase bold), always column vectors.
Matrix A ∈Rm×n (uppercase bold).
j-th column of matrix A
i-th row of matrix A, as a column vector.
Euclidean norm of vector y.
Frobenius norm of matrix A,
Time series matrix, with each row corresponding to a timestamp.
The delay coordinates matrix corresponding
to X, for window w.
Right singular vectors of X(w).
Singular values of X(w).
Left singular vectors of X(w).
Dimension of approximating subspace.
Same as V(w), Σ(w), U(w) but only with
with k highest singular values and vectors.
Projection of X(w) onto ﬁrst k right singular
vectors, ˜P(w) := ˜U(w) ˜Σ(w) = X(w) ˜V(w).
Hierarchical singular vectors, values and
projections, for the k highest singular
Hierarchically estimated patterns (bases).
Table 1: Frequently used notation.
Furthermore, since U and V are orthonormal, we have
PRELIMINARIES
In this section we give intuitive deﬁnitions of the problems
we are trying to solve.
We also introduce some concepts
needed later.
Ideally, a pattern discovery approach for arbitrary time series (where we have limited or no prior knowledge) should
satisfy the following requirements:
1. Data-driven: The approximating functions should be
derived directly from the data. A ﬁxed, predetermined
set of bases or approximating functions may (i) miss
some information, and (ii) discovers patterns that are
biased towards the “shape” of those functions.
2. Multi-scale: We do not want to restrict examination to
a ﬁnite, predetermined maximum window size, or we
will miss long range trends that occur at time scales
longer than the window size.
Many approaches assume a ﬁxed-length, sliding window. In
the majority of cases, this restriction cannot be trivially
lifted. For example, short-window Fourier cannot say anything about periods larger than the sliding window length.
Wavelets are by nature multi-scale, but they still use a ﬁxed
set of bases, which is also often hard to choose .
The best way to conceptually summarize the main diﬀerence of our approach is the following: typical methods ﬁrst
project the data onto “all” bases in a given family (e.g.,
Fourier, wavelets, etc) and then choose a few coeﬃcients
that capture the most information. In contrast, among all
possible bases we ﬁrst choose a few bases that are guaranteed
to capture the most information and consequently project
the data only onto those. However, eﬃciently determining
these few bases and incrementally updating them as new
points arrive is a challenging problem.
With respect to the ﬁrst of the above requirements (datadriven), let us assume that someone gives us a window size.
Then, the problem we want to solve (addressed in Section 4)
is the following:
(Fixed-window optimal patterns). Given
a time series xt, t = 1, 2, . . . and a window size w, ﬁnd the
patterns that best summarize the series at this window size.
The patterns are w-dimensional vectors vi ≡[vi,1, . . . , vi,w]T ∈
Rw, chosen so that they capture “most” of the information
in the series (in a way that we will make precise later).
In practice, however, we do not know a priori the right window size. Therefore, with respect to the second requirement
(multi-scale), we want to solve the following problem (addressed in Section 5):
(Optimal local patterns). Given a time
series xt and a set of windows W := {w1, w2, w3, . . .}, ﬁnd
(i) the optimal patterns for each of these, and (ii) the best
window w∗to describe the key patterns in the series.
So how do we go about ﬁnding these patterns? An elementary concept we need to introduce is time-delay coordinates.
We are given a time series xt, t = 1, 2, . . . with m points
seen so far. Intuitively, when looking for patterns of length
w, we divide the series in consecutive, non-overlapping subsequences of length w. Thus, if the original series is a m × 1
matrix (not necessarily materialized), we substitute it with
w × w matrix. Instead of m scalar values we now have a
sequence of m/w vectors with dimension w. It is natural to
look for patterns among these time-delay vectors.
Definition 1
(Delay coordinates). Given a sequence
x ≡[x1, x2, . . . , xt, . . . , xm]T and a delay (or window) w, the
delay coordinates are a ⌈m/w⌉×w matrix with the t′-th row
equal to X(w)
(t′) := [x(t′−1)w+1, x(t′−1)w+2, . . . , xt′w]T .
Of course, neither x nor X(w) need to be fully materialized
at any point in time. In practice, we only need to store the
last row of X(w).
Also, note that we choose non-overlapping windows.
could also use overlapping windows, in which case X(w)
would have m −w + 1 rows, with row t consisting of values xt, xt+1, . . . , xt+w. In this case, there are some subtle
diﬀerences , akin to the diﬀerences between “standard”
wavelets and maximum-overlap or redundant wavelets .
However, in practice non-overlapping windows are equally
eﬀective for pattern discovery and also lend themselves better to incremental, streaming estimation using limited resources.
More generally, the original time series does not have to be
scalar, but can also be vector-valued itself. We still do the
same, only each row of X(w) is now a concatenation of rows
of X (instead of a concatenation of scalar values). More precisely, we construct the general time-delay coordinate matrix
as follows:
Procedure 1 Delay (X ∈Rm×n, w)
m′ ←⌊m/w⌋and n′ ←nw
Output is X(w) ∈Rm′×n′ {not necessarily materialized}
for t = 1 to m′ do
(t) ←concatenation of rows
X((t−1)w+1), X((t−1)w+2), · · · X(tw)
Incremental SVD
Batch SVD algorithms are too costly. For an m × n matrix
A, even ﬁnding only the highest singular value and corresponding singular vector needs time O(n2m), where n < m.
Aside from computational cost, we also need to incrementally update the SVD as new rows are added to A.
SVD update algorithms such as can support both
row additions as well as deletions.
However, besides the
right singular vectors vi, both of these approaches need to
store the left singular vectors ui (whose size is proportional
to the time series length in our case).
Algorithm 1 IncrementalSVD (A, k)
for i = 1 to k do
Initialize vi to unit vectors, vi ←ii
Initialize σ2
i to small positive value, σ2
for all new rows a(t+1) do
Initialize ¯a ←a(t+1)
for i = 1 to k do
i ¯a {projection onto vi}
i {energy ∝singular value}
ei ←¯a −yivi {error, ei ⊥vi}
i yiei {update singular vector estimate}
¯a ←¯a −yivi {repeat with remainder of a(t+1)}
p(t+1) ←VT a(t+1) {ﬁnal low-dim. projection of a(t+1)}
We chose an SVD update algorithm (shown above, for given
number k of singular values and vectors) that has been successfully applied in streams .
Its accuracy is still
very good, while it does not need to store the left singular vectors. Since our goal is to ﬁnd patterns at multiple
scales without an upper bound on the window size, this is a
more suitable choice. Furthermore, if we need to place more
emphasis on recent trends, it is rather straightforward to
incorporate an exponential forgetting scheme, which works
well in practice . For each new row, the algorithm updates k · n numbers, so total space requirements are O(nk)
and the time per update is also O(nk). Finally, the incremental update algorithms need only the observed values and
coordinates
and projections
local patterns
local bases
(patterns)
Figure 4: Illustration of local patterns for a ﬁxed
window (here, w = 4).
can therefore easily handle missing values by imputing them
based on current estimates of the singular vectors .
LOCALLY OPTIMAL PATTERNS
We now have all the pieces in place to answer the ﬁrst question:
for a given window w, how do we ﬁnd the locally
optimal patterns? Figure 4 illustrates the main idea. Starting with the original time series x, we transfer to time-delay
coordinates X(w). The local patterns are the right singular
vectors of X(w), which are optimal in the sense that they
minimize the total squared approximation error of the rows
(i) . The detailed algorithm is shown below.
Algorithm 2 LocalPattern (x ∈Rm, w, k = 3)
Use delay coord. X(w) ←Delay(x, w)
Compute SVD of X(w) = U(w)Σ(w)V(w)
Local patterns are v(w)
, . . . , v(w)
Power is π(w) ←Pw
˜P(w) ←˜U(w) ˜Σ(w) {low-dim. proj. onto local patterns}
return ˜V(w), ˜P(w), ˜Σ(w) and π(w)
For now, the projections ˜P(w) onto the local patterns ˜v(i)
are not needed, but we will use them later, in Section 5.
Also, note that LocalPattern can be applied in general to
n-dimensional vector-valued series. The pseudocode is the
same, since Delay can also operate on matrices X ∈Rm×n.
The reason for this will also become clear in Section 5, but
for now it suﬃces to observe that the ﬁrst argument of LocalPattern may be a matrix, with one row x(t) ∈Rn per
timestamp t = 1, 2, . . . , m.
When computing the SVD, we really need only the highest
k singular values and the corresponding singular vectors,
because we only return ˜V(w) and ˜P(w). Therefore, we can
avoid computing the full SVD and use somewhat more eﬃcient algorithms, just for the quantities we actually need.
Also, note that ˜Σ(w) can be computed from ˜P(w), since by
construction
i = ∥pi∥2 = Pm
However, we return these separately, which avoids duplicate
computation. More importantly, when we later present our
streaming approach, we won’t be materializing ˜P(w). Furthermore, Equation (4) does not hold exactly for the estimates returned by IncrementalSVD and it is better to use
the estimates of the singular values σ2
i computed as part of
IncrementalSVD.
Window size (w = 10..400)
Sine (period 50) − Power profile
Figure 5: Power proﬁle of sine wave xt = sin(2πt/50)+
ǫt, with Gaussian noise ǫt ∼N(5, 0.5).
Number of patterns
We use a default value of k = 3 local patterns, although
we could also employ an energy-based criterion to choose k,
similar to . However, three (or fewer) patterns are suﬃcient, since it can be shown that the ﬁrst pattern captures
the average trend (aperiodic, if present) and the next two
capture the main low-frequency and high-frequency periodic
trends . The periodic trends do not have to be sinusoidal
and in real data they rarely are.
Complexity
For a single window w, batch algorithms for computing the
k highest singular values of a m × n matrix (n < m) are
O(kmn2). Therefore, for window size w the time complexity
If we wish to compute the local
patterns for all windows up to wmax = O(t), then the total
complexity is O(kt3). In Section 5 and 6 we show how we
can reduce this dramatically.
Power proﬁle
Next, let’s assume we have optimal local patterns for a number of diﬀerent window sizes.
Which of these windows is
the best to describe the main trends? Intuitively, the key
idea is that if there is a trend that repeats with a period of
T, then diﬀerent subsequences in the time-delay coordinate
space should be highly correlated when w ≈T. Although
the trends can be arbitrary, we illustrate the intuition with
a sine wave, in Figure 5. The plot shows the squared approximation error per window element, using k = 1 pattern
on a sine wave with period T = 50. As expected, for window
size w = T = 50 the approximation error drops sharply and
essentially corresponds to the Gaussian noise ﬂoor. Naturally, for windows w = iT that are multiples of T the error
also drops. Finally, observe that the error for all windows is
proportional to
w, since it is per window element. Eventually, for window size equal to the length of the entire time
series w = m , we
get π(m) = 0 since ﬁrst pattern is the only singular vector,
which coincides with the series itself, so the residual error is
Formally, the squared approximation error of the time-delay
matrix X(w) is
(t) ∥2 = ∥˜X(w) −X(w)∥2
coordinates
projection onto
local patterns
coordinates
local patterns
local patterns
projection onto
local patterns
coordinates
projection onto
local patterns
"equivalent" pattern for window 8
patterns for wind. 4
Figure 6: Multi-scale pattern discovery (hierarchical, w0 = 4, W = 2, k = 2).
where ˜X(w) := ˜P(w)( ˜V(w))T is the reconstruction (see Equation (1)). From Equation (2) and 3, we have
ǫ(w) = ∥X(w)∥2
F −∥˜P(w)∥2
F ≈∥x∥2 −Pk
Based on this, we deﬁne the power, which is an estimate of
the error per window element.
Definition 2
(Power profile π(w)). For a given number of patterns (k = 2 or 3) and for any window size w, the
power proﬁle is the sequence deﬁned by
π(w) := ǫ(w)
More precisely, this is an estimate of the variance per dimension, assuming that the discarded dimensions correspond to
isotropic Gaussian noise (i.e., uncorrelated with same variance in each dimension) . As explained, this will be much
lower when w = T, where T is the period of an arbitrary
main trend.
The following lemma follows from the above observations.
Note that the conclusion is valid both ways, i.e., perfect
copies imply zero power and vice versa. Also, the conclusion holds regardless of alignment (i.e., the periodic part
does not have to start at the begining of a windowed subsequence). A change in alignment will only aﬀect the phase
of the discovered local patterns, but not their shape or the
reconstruction accuracy.
Observation 1
(Zero power). If x ∈Rt consists of
exact copies of a subsequence of length T then, for every
number of patterns k = 1, 2, . . . and at each multiple of T,
we have π(iT ) = 0, i = 1, 2, . . ., and vice versa.
In general, if the trend does not consist of exact copies, the
power will not be zero, but it will still exhibit a sharp drop.
We exploit precisely this fact to choose the “right” window.
Choosing the window
Next, we state the steps for interpreting the power proﬁle to
choose the appropriate window that best captures the main
1. Compute the power proﬁle π(w) versus w.
2. Look for the ﬁrst window w∗
0 that exhibits a sharp
drop in π(w∗
0 ) and ignore all other drops occurring at
windows w ≈iw∗
0, i = 2, 3, . . . that are approximately
multiples of w∗
3. If there are several sharp drops at windows w∗
i that are
not multiples of each other, then any of these is suitable. We simply choose the smallest one; alternatively,
we could choose based on prior knowledge about the
domain if available, but that is not necessary.
4. If there are no sharp drops, then no strong periodic/cyclic
components are present. However, the local patterns
at any window can still be examined to gain a picture
of the time series behavior.
In Section 7 we illustrate that this window selection criterion works very well in practice and selects windows that
correspond to the true cycles in the data. Beyond this, the
discovered patterns themselves reveal the trends in detail.
MULTIPLE-SCALE PATTERNS
In this section we tackle the second question: how do we
eﬃciently compute the optimal local patterns for multiple
windows (as well as the associated power proﬁles), so as to
quickly zero in to the “best” window size? First, we can
choose a geometric progression of window sizes: rather than
estimating the patterns for windows of length w0, w0 + 1,
w0 + 2, w0 + 3, . . ., we estimate them for windows of w0,
2w0, 4w0, . . . or, more generally, for windows of length wl :=
w0 · W l for l = 0, 1, 2, . . .. Thus, the size of the window set
W we need to examine is dramatically reduced. Still, this is
(i) computationally expensive (for each window we still need
O(ktw) time and, even worse, (ii) still requires buﬀering all
the points (needed for large window sizes, close to the time
series length). Next, we show how we can reduce complexity
even further.
Hierarchical SVD
The main idea of our approach to solve this problem is shown
in Figure 6. Let us assume that we have, say k = 2 local
patterns for a window size of w0 = 100 and we want to compute the patterns for window w(100,1) = 100 · 21 = 200. The
naive approach is to construct X(200) from scratch and compute the SVD. However, we can reuse the patterns found
from X(100).
Using the k = 2 patterns v(100)
and v(100)
we can reduce the ﬁrst w0 = 100 points x1, x2, . . . , x100
into just two points, namely their projections p(100)
onto v(100)
and v(100)
, respectively.
Similarly, we
can reduce the next w0 = 100 points x101, x102, . . . , x200
also into two numbers, p(100)
and p(100)
, and so on. These
projections, by construction, approximate the original series well. Therefore, we can represent the ﬁrst row x(200)
[x1, . . . , x200]T ∈R200 of X(200) with just four numbers,
same for the other rows of X(200), we construct a matrix
X(100,1) with just n = 4 columns, which is a very good approximation of X(200).Consequently, we compute the local
patterns using X(100,1) instead of X(200).
Repeating this
process recursively, we can ﬁnd the local patterns for a window w(100,2) = 100 · 22 = 400 and so on.
Definition 3
(Level-(w0, l) window). The level-(w0, l)
window corresponds to an original window size (or scale)
wl := w0 ·W l. Patterns at each level l are found recursively,
using patterns from the previous level l −1.
In the above example, we have w0 = 100 and l = 0, 1. Since
w0 and W are ﬁxed for a particular sequence of scales wl,
we will simply refer to level-l windows and patterns. The
recursive construction is based on the level-l delay matrix
and corresponding patterns.
Definition 4
(Level-l delay matrix X(w0,l)). Given
a starting window w0 and a scale factor W , the level-l delay matrix is simply X(w0,0) := X(w0) for l = 0 and for
l = 1, 2, . . . it is recursively deﬁned by
X(w0,l) := Delay
`˜P(w0,l−1), W
where ˜P(w0,l) := X(w0,l) ˜V(w0,l) is the projection onto the
level-l patterns ˜V(w0,l) which are found based on X(w0,l).
The level-l delay matrix is an approximation of the delay
matrix X(wl) for window size wl = w0W l.
In our example, the patterns extracted from X(100,1) are
four-dimensional vectors, v(100,1)
∈R4, whereas the patterns for X(200) would be 200-dimensional vectors v(200)
R200. However, we can appropriately combine v(100,1)
to estimate v(200)
Definition 5
(Level-l local pattern v0(w0,l)
level-l pattern v0(w0,l)
, for all i = 1, 2, . . . , k, corresponding
to a window of wl = w0W l is simply v0(w0,0)
l = 0 and for l = 1, 2, . . . it is deﬁned recursively by
[(j −1)wl−1 + 1 : jwl−1] :=
V0(w0,l−1)`
[(j −1)k + 1 : jk]
for j = 1, 2, . . . , W . It is an approximation of the local patterns v(wl)
of the original delay matrix X(wl), for window
size wl = w0W l.
Consider v0(100,1)
in our example.
The ﬁrst k = 2 out
of kW = 4 numbers in v(100,1)
approximate the patterns
among the 2-dimensional vectors p(100,0)
, which in turn capture patterns among the 100-dimensional vectors x(100,0)
of the original time-delay matrix.
Thus, but forming the
appropriate linear combination of the 100-dimensional patterns v(100,0)
≡v0(100,0)
(i.e., the columns of ˜V(100,0) ≡
V0(100,0)), weighted according to v(100,1)
[1 : 2], we can construct the ﬁrst half of the 200-dimensional pattern v0(100,1)
100] (left-slanted entries in Figure 6).
Similarly, a linear
combination of the columns of ˜V(100,0) ≡V0(100,0) weighted
according to v(100,1)
[3 : 4] gives us the second half of the
200-dimensional pattern v0(100,1)
[101 : 200] (right-slanted
entries in Figure 6). For level l = 2 we similarly combine
the columns of V0(100,1) according to v(100,2)
[1 : 2] (for the
ﬁrst half, v0(100,2)
[1 : 200]) and to v(100,2)
3 : 4] (for the
second half, v0(100,2)
[201 : 400]) and so on, for the higher
(Orthonormality of v0(w0,l)
). We have
∥= 1 and, for i ̸= j,
where i, j = 1, 2, . . . , k.
Proof. For level l = 0 they are orthonormal since they
coincide with the original patterns v(w0)
which are by construction orthonormal. We proceed by induction on the level
l ≥1. Without loss of generality, assume that k = 2 and,
for brevity, let B ≡V0(w0,l−1) and bi,1 ≡v(w0,l)
bi,2 ≡v(w0,l)
[k + 1 : k], so that v(w0,l)
= [bi,1, bi,2]. Then
∥2 = [Bbi,1 Bbi,2]2 = ∥Bbi,1∥2 + ∥Bbi,2∥2
= ∥bi,1∥2 + ∥bi,2∥2 = ∥v(w0,l)
= [Bbi,1 Bbi,2]T [Bbj,1 Bbj,2]
i,1BT Bbj,1 + bT
i,2BT Bbj,2
i,1bj,1 + bT
since B preserves dot products as an orthonormal matrix
(by inductive hypothesis) and v(w0,l)
are orthonormal by
construction.
The detailed hierarchical SVD algorithm is shown below. In
practice, the maximum level L is determined based on the
length m of the time series so far, L ≈logW (m/w0).
Algorithm 3 Hierarchical (x ∈Rm, w0, W , L, k = 6)
{Start with level l = 0, corresponding to window w0}
˜V(w0,0), ˜P(w0,0), ˜Σ(w0,0), π(w0,0) ←
LocalPattern(x, w0, k)
{Levels l, corresponding to window wl = w0 · W l}
for level l = 1 to L do
˜V(w0,l), ˜P(w0,l), ˜Σ(w0,l), π(w0,l) ←
LocalPattern(˜P(w0,l−1), W, k)
Compute patterns v0(w0,l)
for window size wl are based
on Equation (6)
Choosing the initial window
The initial window w0 has some impact on the quality of
the approximations. This also depends on the relationship
of k to w0 (the larger k is, the better the approximation
and if k = w0 then ˜P(w0,1) = X(w0), i.e., no information
is discarded at the ﬁrst level). However, we want k to be
relatively small since, as we will see, it determines the buﬀering requirements of the streaming approach. Hence, we ﬁx
We found that this simple choice works well for
real-world sequences, but we could also use energy-based
thresholding , which can be done incrementally.
If w0 is too small, then we discard too much of the variance
too early. If w0 is unnecessarily big, this increases buﬀering
requirements and the beneﬁts of the hierarchical approach
diminish. In practice, a good compromise is a value in the
range 10 ≤w0 ≤20.
Finally, out of the six patterns we keep per level, the ﬁrst
two or three are of interest and reported to the user, as
explained in Section 4. The remaining are kept to ensure
that X(w0,l) is a good approximation of X(wl).
Choosing the scales
As discussed in Section 4.1, if there is a sharp drop of π(T ) at
window w = T, then we will also observe drops at multiples
w = iT, i = 2, 3, . . .. Therefore, we choose a few diﬀerent
starting windows w0 and scale factors W that are relatively
prime to each other. In practice, the following set of three
choices is suﬃcient to quickly zero in on the best windows
and the associated optimal local patterns:
(w0, W ) ∈{(9, 2), (10, 2), (15, 3)}
Complexity
For a total of L ≈logW (t/w0) = O(log t) levels we have to
compute the ﬁrst k singular values and vectors of X(w0,l) ∈
Rt/(w0W l)×W k, for l = 1, 2, . . ..
A batch SVD algorithm
requires time O
k · (W k)2 ·
, which is O
Summing over l = 1, . . . , L, we get O(W 2k2t).
Finally, for l = 0, we need O
= O(kw0t). Thus,
the total complexity is O(W 2k2t + kw0t). Since W and w0
are ﬁxed, we ﬁnally have the following
(Batch hierarchical complexity). The total time for the hierarchical approach is O(k2t), i.e., linear
with respect to the time series length.
This is a big improvement over the O(t3k) time of the nonhierarchical approach. However, we still need to buﬀer all
the points. We address this problem in the next section.
STREAMING COMPUTATION
In this section we explain how to perform the necessary computations in an incremental, streaming fashion. We designed
our models precisely to allow this step. The main idea is that
we recursively invoke only one iteration of each loop in IncrementalSVD (for LocalPattern) and in Hierarchical, as soon as the necessary number of points has arrived.
Subsequently, we can discard these points and proceed with
the next non-overlapping window.
Time (weeks)
(a) Sunspot time series.
20 40 60 80100120
First pattern (basis)
Time (1..134)
20 40 60 80100120
Second pattern (basis)
Time (1..134)
(b) Patterns (batch, non-hierarchical).
80 100 120
Second pattern (hierarchical, streaming)
Time (1..144)
80 100 120
First pattern (hierarchical, streaming)
Time (1..144)
(c) Patterns (streaming, hierarchical).
Figure 7: Sunspot, best selected window (about 11
years) and corresponding representative trends.
Modifying LocalPattern
We buﬀer consecutive points of x (or, in general, rows of X)
until we accumulate w of them, forming one row of X(w). At
that point, we can perform one iteration of the outer loop
in IncrementalSVD to update all k local patterns. Then,
we can discard the w points (or rows) and proceed with the
next w. Also, since on higher levels the number of points
for SVD may be small and close to k, we may choose to
initially buﬀer just the ﬁrst k rows of X(w) and use them to
bootstrap the SVD estimates, which we subsequently update
as described.
Modifying Hierarchical
For level l = 0 we use the modiﬁed LocalPattern on the
original series, as above. However, we also store the k projections onto the level-0 patterns. We buﬀer W consecutive
sets of these projections and as soon as kW values accumulate, we update the k local patterns for level l = 1. Then we
can discard the kW projections from level-0, but we keep
the k level-1 projections. We proceed in the same way for
all other levels l ≥2.
Complexity
Compared to the batch computation, we need O
time to compute the ﬁrst k singular
values and vectors of X(w0,l) for l = 1, 2, . . ..
= O(kt) time. Summing over l =
0, 1, . . . , L we get O(kt).
With respect to space, we need
to buﬀer w0 points for l = 0 and W k points for each of
the remaining L = O(log t) levels, for a total of O(k log t).
Therefore, we have the following
(Streaming, hierarchical complexity).
Amortized cost is O(k) per incoming point and total space
is O(k log t).
Since k = 6, the update time is constant per incoming point
and the space requirements grow logarithmically with respect to the size t of the series.
Table 2 summarizes the
time and space complexity for each approach.
Incremental
O(k log t)
Table 2: Summary of time and space complexity.
EXPERIMENTAL EVALUATION
In this section we demonstrate the eﬀectiveness of our approach on real data. The questions we want to answer are:
1. Do our models for mining locally optimal patterns correctly capture the best window size?
2. Are the patterns themselves accurate and intuitive?
3. How does the quality of the much more eﬃcient, hierarchical streaming approach compare to the exact
computation?
We performed all experiments in Matlab, using the standard functions for batch SVD. Table 3 brieﬂy describes the
datasets we use.
Description
Automobile
Automobile counts on interstate
Sunspot intensities (years 1755–
Light intensity data
Table 3: Description of datasets.
Table 4 shows the best windows identiﬁed from the power
proﬁles. Figure 9 shows the exact power proﬁles computed
using the batch, non-hierarchical approach and Figure 10
shows the power proﬁles estimated using the streaming, hierarchical approach.
Best window
134, 120, 142
156, 166, 314
Table 4: Best windows—see also Figures 9 and 10.
Automobile
The results on this dataset were already presented in Section 1. Our method correctly captures the right window for
the main trend and also an accurate picture of the typical
daily pattern. The patterns discovered both by the batch
and the streaming approach are very close to each other and
equally useful.
(a) Data, beginning and end.
Time (1..156)
Third pattern
Time (1..156)
Second pattern
Time (1..156)
First pattern
(b) Patterns
Figure 8: Mote data and patterns.
This dataset consists of sunspot intensities for a period of
over 200 years, measured at a monthly interval (see Figure 7a). This phenomenon exhibits a cyclic pattern, with
cycles lasting about 11 years (varying slightly between 10
and 12). Indeed, the best window identiﬁed by our model
is 134 months (i.e., 11 years and 2 months), with 120 (i.e.,
10 years) and 142 (i.e., 11 years and 10 months) being close
runner-ups. The fast, streaming approach identiﬁes 144 (or
12 years) as the best window, which is the scale closest to the
average cycle duration. Both the exact patterns (Figure 7b)
as well as those estimated by the fast streaming approach
(Figure 7c) provide an accurate picture of what typically
happens within a cycle.
This dataset consists of light intensity measurements collected in a lab by a wireless sensor device. Like Sunspot, it
exhibits a varying cycle but with much bigger changes. Originally a cycle lasts about 310–320 time ticks, which progressively shortens to a cycle of about half that length (140–160),
as shown in Figure 8a. The power proﬁle drops sharply in
the vicinity of both cycle lengths. If we examine the patterns, they show the same shape as the data. The ﬁrst two
capture the low-frequency component with peaks located
about half a window size apart, while the third pattern captures the higher-frequency component with two peaks (see
Figure 8b). Patterns by the fast, streaming approach convey
the same information.
Quality of streaming approach
In this section we compare the fast, streaming and hierarchical approach against the exact, batch and non-hierarchical.
Even though the fast streaming approach potentially introduces errors via (i) the hierarchical approximation, and (ii)
Y: 0.0007202
Y: 0.001685
Y: 0.0007322
Sunspot − Power profile
Window size
Y: 0.001688
Y: 0.0005688
Window size
Mote − Power profile
Window size
Automobile − Power profile
(a) Sunspot
(c) Automobile
Figure 9: Power proﬁles (non-hierarchical, batch).
the incremental update of the SVD, we show that it still
achieves very good results. The quality should be compared
against the signiﬁcant speedups in Table 5.
Table 5: Wall-clock times (in seconds). The maximum window wmax for the exact approach and the
length of each series are also shown.
First, the streaming approach correctly zeros in to the best
windows, which generally correspond to the scale that is
closest to the “true” best windows identiﬁed by the exact,
batch approach (see Table 4).
Our next goal is to characterize the quality of the patterns
themselves, at the best windows that each approach identiﬁes. To that end, we compare the reconstructions of the
original time series x for the two methods. If v(w∗)
optimal patterns for the best window w∗(or v0
the streaming approach, with corresponding window w∗
0 · W l∗), we ﬁrst zero-pad x into x0 so its length is a
multiple of w∗, then compute the local pattern projections
˜P(w∗) = Delay(x0, w∗) ˜V(w∗), project back to the original
delay coordinates ˜X(w∗) = ˜P(w∗)` ˜V(w∗)´T . Finally, we concatenate the rows of ˜X(w∗) into the reconstruction ˜x of x,
removing the elements corresponding to the zero-padding.
The quality measures we use are
Q := ∥x∥2−∥˜xstream−x∥2
∥x∥2−∥˜xexact−x∥2
stream ˜xexact|
∥˜xstream∥∥˜xexact∥.
The ﬁrst one is a measure of the information retained, with
respect to the total squared error: we compare the fraction of information retained by the streaming approach,
∥x∥2 −∥˜xstream −x∥2´
/∥x∥2, to that retained by the exact
∥x∥2−∥˜xexact−x∥2´
/∥x∥2. The second is simply
the cosine similarity of the two reconstructions, which penalizes small “time shifts” less and more accurately reﬂects
how close are the overall shapes of the reconstructions.
We use the reconstruction ˜xexact as our baseline. For pattern
discovery, using the original series x itself as the baseline is
undesirable, since that contains noise and other irregularities
that should not be part of the discovered patterns.
Finally, we show the cosine similarity of the patterns themselves, for the best window w∗
l identiﬁed by the streaming
approach. This characterizes how close the direction of the
hierarchically and incrementally estimated singular vectors
is to the direction of the “true” singular vectors. Table 6
shows the results.
Figures 2 and 7 allow visual comparison of the patterns (at slightly diﬀerent best windows). In
summary, the streaming approach’s patterns capture all the
essential information, while requiring 1–4 orders of magnitude less time and 1–2 orders of magnitude less space.
Quality ratio
similarity
Table 6: Batch non-hierarchical vs. streaming hierarchical (k = 3, best window from Table 4).
RELATED WORK
Initial work on time series representation uses the
Fourier transform. Even more recent work uses ﬁxed, predetermined bases or approximating functions. APCA and
other similar approaches approximate the time series with
piecewise constant or linear functions. DAWA combines
the DCT and DWT. However, all these approaches focus on
compressing the time series for indexing purposes, and not
on pattern discovery.
AWSOM ﬁrst applies the wavelet transform.
authors observe, just a few wavelet coeﬃcients do not capture all patterns in practice, so AWSOM subsequently captures trends by ﬁtting a linear auto-regressive model at each
time scale. In contrast, our approach learns the best set of
orthogonal bases, while matching the space and time complexity of AWSOM. Finally, it is diﬃcult to apply wavelets
on exponential-size windows that are not powers of two.
The work in proposes a multi-resolution clustering scheme
for time series data.
It uses the average coeﬃcients (low
frequencies) of the wavelet transform to perform k-means
clustering and progressively reﬁnes the clusters by incor-
Y: 0.001437
Y: 0.0006763
Window size
Sunspot − Power profile (Hierarchical, Streaming)
Y: 0.001539
Y: 0.001649
Window size
Mote − Power profile (Hierarchical, Streaming)
Y: 3.123e−005
Window size
Auto − Power profile (Hierarchical, Streaming)
(a) Sunspot
(c) Automobile
Figure 10: Power proﬁles (hierarchical, streaming).
porating higher-level, detail coeﬃcients. This approach requires much less time for convergence, compared to operating directly on the very high dimension of the original series.
However, the focus here is on clustering multiple time series,
rather than discovering local patterns.
The problem of principal components analysis (PCA) and
SVD on streams has been addressed in and . Again,
both of these approaches focus on discovering linear correlations among multiple streams and on applying these correlations for further data processing and anomaly detection
 , rather than discovering optimal local patterns at multiple scales. Also related to these is the work of which uses
a diﬀerent formulation of linear correlations and focuses on
compressing historical data, mainly for power conservation
in sensor networks. Finally, the work in proposes an approach to combine segmentation of multidimensional series
with dimensionality reduction. The reduction is on the segment representatives and it is performed across dimensions
(similar to ), not along time, and the approach is not
applicable to streams.
The seminal work of for rule discovery in time series is
based on sequential patterns extracted after a discretization
Other work has also focused on ﬁnding representative trends . A representative trend is a subsequence of
the time series that has the smallest sum of distances from
all other subsequences of the same length. The proposed
method employs random projections and FFT to quickly
compute the sum of distances. This does not apply directly
to streams and it is not easy to extend, since each section
has to be compared to all others.
Our approach is complementary and could conceivably be used in place of the
FFT in this setting. Related to representative trends are
motifs . Intuitively, these are frequently repeated subsequences, i.e., subsequences of a given length which match
(in terms of some distance and a given distance threshold)
a large number of other subsequences of the same time series. More recently, vector quantization has been used for
time series compression . The ﬁrst focuses on ﬁnding
good-quality and intuitive distance measures for indexing
and similarity search and is not applicable to streams, while
the second focuses on reducing power consumption for wireless sensors and not on pattern discovery.
Finally, other
work on stream mining includes approaches for periodicity
 and periodic pattern discovery.
Approaches for regression on time series and streams include
 and amnesic functions . Both of these estimate the
best ﬁt of a given function (e.g., linear or low-degree polynomial), they work by merging the estimated ﬁt on consecutive
windows and can incorporate exponential-size time windows
placing less emphasis on the past. However, both of these
approaches employ a ﬁxed, given set of approximating functions. Our approach might better be described as agnostic,
rather than amnesic.
A very recent and interesting application of the same principles is on correlation analysis of complex time series through
change-point scores .
Finally, related ideas have been
used in other ﬁelds, such as in image processing for image denoising and physics/climatology for nonlinear
prediction in phase space . However, none of these approaches address incremental computation in streams. More
generally, the potential of this general approach has not received attention in time series and stream processing literature. We demonstrate that its power can be harnessed at
very small cost, no more than that of the widely used wavelet
transform.
CONCLUSION
We introduce a method to that can learn the key trends in
a time series. Our main contributions are:
• We introduce the notion of optimal local patterns in
time series.
• We show how to extract trends at multiple time scales.
• We propose a criterion which allows us to choose the
best window sizes from the data.
• We introduce an approach to perform all of the above
incrementally, in a streaming setting.
Furthermore, our approach can be easily extended to an ensemble of multiple time series, unlike ﬁxed-basis methods
(e.g., FFT, SFT or DWT), and also to deal with missing
values. Besides providing insight about the behavior of the
time series, the discovered patterns can also be used to facilitate further data processing. In fact, our approach can be
used anywhere a ﬁxed-basis orthonormal transform would
Especially in a streaming setting, it makes more sense to
ﬁnd the best bases and project only onto these, rather than
use an a priori determined set of bases and then try to ﬁnd
the “best” coeﬃcients. However, computing these bases incrementally and eﬃciently is a challenging problem.
streaming approach achieves 1-4 orders of magnitude improvement in time and space over the exact, batch approach.
Its time and space requirements are comparable to previous
multi-scale pattern mining approaches on streams ,
while it produces signiﬁcantly more concise and informative
patterns, without any prior knowledge about the data.