IEEE TRANSACTIONS ON XXXX, VOL. X, NO. X, MM YYYY
A Survey on Neural Network Interpretability
Yu Zhang, Peter Tiˇno, Aleˇs Leonardis, and Ke Tang
Abstract—Along with the great success of deep neural networks, there is also growing concern about their black-box
nature. The interpretability issue affects people’s trust on deep
learning systems. It is also related to many ethical problems,
e.g., algorithmic discrimination. Moreover, interpretability is a
desired property for deep networks to become powerful tools
in other research ﬁelds, e.g., drug discovery and genomics. In
this survey, we conduct a comprehensive review of the neural
network interpretability research. We ﬁrst clarify the deﬁnition
of interpretability as it has been used in many different contexts.
Then we elaborate on the importance of interpretability and
propose a novel taxonomy organized along three dimensions:
type of engagement (passive vs. active interpretation approaches),
the type of explanation, and the focus (from local to global
interpretability). This taxonomy provides a meaningful 3D view
of distribution of papers from the relevant literature as two
of the dimensions are not simply categorical but allow ordinal
subcategories. Finally, we summarize the existing interpretability
evaluation methods and suggest possible research directions
inspired by our new taxonomy.
Terms—Machine
interpretability, survey.
I. INTRODUCTION
VER the last few years, deep neural networks (DNNs)
have achieved tremendous success in computer
vision , , speech recognition , natural language processing and other ﬁelds , while the latest applications can
be found in these surveys – . They have not only beaten
many previous machine learning techniques , Project ID: 721463. We also acknowledge
MoD/Dstl and EPSRC for providing the grant to support the UK academics
involvement in a Department of Defense funded MURI project through EPSRC
grant EP/N019415/1.
Y. Zhang and K. Tang are with the Guangdong Key Laboratory of Brain-
Inspired Intelligent Computation, Department of Computer Science and
Engineering, Southern University of Science and Technology, Shenzhen 518055,
P.R.China, and also with the Research Institute of Trust-worthy Autonomous
Systems, Southern University of Science and Technology, Shenzhen 518055,
P.R.China (e-mail: , ).
Y. Zhang, P. Tiˇno and A. Leonardis are with the School of Computer Science,
University of Birmingham, Edgbaston, Birmingham B15 2TT, UK (e-mail:
{p.tino, a.leonardis}@cs.bham.ac.uk).
Manuscript accepted July 09, 2021, IEEE-TETCI. © 2021 IEEE. Personal
use of this material is permitted. Permission from IEEE must be obtained for
all other uses, in any current or future media, including reprinting/republishing
this material for advertising or promotional purposes, creating new collective
works, for resale or redistribution to servers or lists, or reuse of any copyrighted
component of this work in other works.
trees, support vector machines), but also achieved the stateof-the-art performance on certain real-world tasks , .
Products powered by DNNs are now used by billions of people1,
e.g., in facial and voice recognition. DNNs have also become
powerful tools for many scientiﬁc ﬁelds, such as medicine ,
bioinformatics , and astronomy , which usually
involve massive data volumes.
However, deep learning still has some signiﬁcant disadvantages. As a really complicated model with millions of free
parameters (e.g., AlexNet , 62 million), DNNs are often
found to exhibit unexpected behaviours. For instance, even
though a network could get the state-of-the-art performance
and seemed to generalize well on the object recognition task,
Szegedy et al. found a way that could arbitrarily change
the network’s prediction by applying a certain imperceptible
change to the input image. This kind of modiﬁed input is called
“adversarial example”. Nguyen et al. showed another way
to produce completely unrecognizable images (e.g., look like
white noise), which are, however, recognized as certain objects
by DNNs with 99.99% conﬁdence. These observations suggest
that even though DNNs can achieve superior performance on
many tasks, their underlying mechanisms may be very different
from those of humans’ and have not yet been well-understood.
A. An (Extended) Deﬁnition of Interpretability
To open the black-boxes of deep networks, many researchers
started to focus on the model interpretability. Although this
theme has been explored in various papers, no clear consensus
on the deﬁnition of interpretability has been reached. Most
previous works skimmed over the clariﬁcation issue and left it
as “you will know it when you see it”. If we take a closer look,
the suggested deﬁnitions and motivations for interpretability
are often different or even discordant .
One previous deﬁnition of interpretability is the ability to
provide explanations in understandable terms to a human ,
while the term explanation itself is still elusive. After reviewing
previous literature, we make further clariﬁcations of “explanations” and “understandable terms” on the basis of .
Interpretability is the ability to provide explanations1 in understandable terms2 to a human.
1) Explanations, ideally, should be logical decision rules
(if-then rules) or can be transformed to logical rules.
However, people usually do not require explanations to
be explicitly in a rule form (but only some key elements
which can be used to construct explanations).
2) Understandable terms should be from the domain
knowledge related to the task (or common knowledge
according to the task).
1 
 
IEEE TRANSACTIONS ON XXXX, VOL. X, NO. X, MM YYYY
SOME INTERPRETABLE “TERMS” USED IN PRACTICE.
Understandable terms
Computer vision
Images (pixels)
Super pixels (image patches)a
Visual conceptsb
Word embeddings
Bioinformatics
Motifs (position weight matrix)c
a image patches are usually used in attribution methods .
b colours, materials, textures, parts, objects and scenes .
c proposed by and became an essential tool for computational motif
discovery.
Our deﬁnition enables new perspectives on the interpretability research: (1) We highlight the form of explanations rather
than particular explanators. After all, explanations are expressed
in a certain “language”, be it natural language, logic rules
or something else. Recently, a strong preference has been
expressed for the language of explanations to be as close as
possible to logic . In practice, people do not always require
a full “sentence”, which allows various kinds of explanations
(rules, saliency masks etc.). This is an important angle to
categorize the approaches in the existing literature. (2) Domain
knowledge is the basic unit in the construction of explanations.
As deep learning has shown its ability to process data in the raw
form, it becomes harder for people to interpret the model with
its original input representation. With more domain knowledge,
we can get more understandable representations that can be
evaluated by domain experts. Table I lists several commonly
used representations in different tasks.
We note that some studies distinguish between interpretability and explainability (or understandability, comprehensibility,
transparency, human-simulatability etc. , ). In this
paper we do not emphasize the subtle differences among those
terms. As deﬁned above, we see explanations as the core
of interpretability and use interpretability, explainability and
understandability interchangeably. Speciﬁcally, we focus on
the interpretability of (deep) neural networks (rarely recurrent
neural networks), which aims to provide explanations of their
inner workings and input-output mappings. There are also
some interpretability studies about the Generative Adversarial
Networks (GANs). However, as a kind of generative models,
it is slightly different from common neural networks used as
discriminative models. For this topic, we would like to refer
readers to the latest work – , many of which share the
similar ideas with the “hidden semantics” part of this paper (see
Section II), trying to interpret the meaning of hidden neurons
or the latent space.
Under our deﬁnition, the source code of Linux operating
system is interpretable although it might be overwhelming for
a developer. A deep decision tree or a high-dimensional linear
model (on top of interpretable input representations) are also
interpretable. One may argue that they are not simulatable 
(i.e. a human is able to simulate the model’s processing from
input to output in his/her mind in a short time). We claim,
however, they are still interpretable.
Besides above conﬁned scope of interpretability (of a trained
neural network), there is a much broader ﬁeld of understanding
the general neural network methodology, which cannot be
covered by this paper. For example, the empirical success of
DNNs raises many unsolved questions to theoreticians .
What are the merits (or inductive bias) of DNN architectures , ? What are the properties of DNNs’ loss
surface/critical points – ? Why DNNs generalizes so
well with just simple regularization – ? What about
DNNs’ robustness/stability – ? There are also studies
about how to generate adversarial examples , and
detect adversarial inputs .
B. The Importance of Interpretability
The need for interpretability has already been stressed by
many papers , , , emphasizing cases where lack of
interpretability may be harmful. However, a clearly organized
exposition of such argumentation is missing. We summarize
the arguments for the importance of interpretability into three
1) High Reliability Requirement: Although deep networks
have shown great performance on some relatively large test
sets, the real world environment is still much more complex.
As some unexpected failures are inevitable, we need some
means of making sure we are still in control. Deep neural
networks do not provide such an option. In practice, they have
often been observed to have unexpected performance drop in
certain situations, not to mention the potential attacks from the
adversarial examples , .
Interpretability is not always needed but it is important
for some prediction systems that are required to be highly
reliable because an error may cause catastrophic results (e.g.,
human lives, heavy ﬁnancial loss). Interpretability can make
potential failures easier to detect (with the help of domain
knowledge), avoiding severe consequences. Moreover, it can
help engineers pinpoint the root cause and provide a ﬁx
accordingly. Interpretability does not make a model more
reliable or its performance better, but it is an important part
of formulation of a highly reliable system.
2) Ethical and Legal Requirement: A ﬁrst requirement is to
avoid algorithmic discrimination. Due to the nature of machine
learning techniques, a trained deep neural network may inherit
the bias in the training set, which is sometimes hard to notice.
There is a concern of fairness when DNNs are used in our daily
life, for instance, mortgage qualiﬁcation, credit and insurance
risk assessments.
Deep neural networks have also been used for new drug
discovery and design . The computational drug design ﬁeld
was dominated by conventional machine learning methods such
as random forests and generalized additive models, partially
because of their efﬁcient learning algorithms at that time,
and also because a domain chemical interpretation is possible.
Interpretability is also needed for a new drug to get approved by
the regulator, such as the Food and Drug Administration (FDA).
Besides the clinical test results, the biological mechanism
underpinning the results is usually required. The same goes
for medical devices.
Another legal requirement of interpretability is the “right to
explanation” . According to the EU General Data Protection
ZHANG et al.: A SURVEY ON NEURAL NETWORK INTERPRETABILITY
Regulation (GDPR) , Article 22, people have the right not
to be subject to an automated decision which would produce
legal effects or similar signiﬁcant effects concerning him or
her. The data controller shall safeguard the data owner’s right
to obtain human intervention, to express his or her point of
view and to contest the decision. If we have no idea how the
network makes a decision, there is no way to ensure these
3) Scientiﬁc Usage: Deep neural networks are becoming
powerful tools in scientiﬁc research ﬁelds where the data
may have complex intrinsic patterns (e.g., genomics ,
astronomy , physics and even social science ).
The word “science” is derived from the Latin word “scientia”,
which means “knowledge”. When deep networks reach a better
performance than the old models, they must have found some
unknown “knowledge”. Interpretability is a way to reveal it.
C. Related Work and Contributions
There have already been attempts to summarize the techniques for neural network interpretability. However, most of
them only provide basic categorization or enumeration, without
a clear taxonomy. Lipton points out that the term interpretability is not well-deﬁned and often has different meanings
in different studies. He then provides simple categorization
of both the need (e.g., trust, causality, fair decision-making
etc.) and methods (post-hoc explanations) in interpretability
study. Doshi-Velez and Kim provide a discussion on the
deﬁnition and evaluation of interpretability, which inspired
us to formulate a stricter deﬁnition and to categorize the
existing methods based on it. Montavon et al. conﬁne the
deﬁnition of explanation to feature importance (also called
explanation vectors elsewhere) and review the techniques
to interpret learned concepts and individual predictions by
networks. They do not aim to give a comprehensive overview
and only include some representative approaches. Gilpin et
al. divide the approaches into three categories: explaining
data processing, explaining data representation and explanationproducing networks. Under this categorization, the linear proxy
model method and the rule-extraction method are equally
viewed as proxy methods, without noticing many differences
between them (the former is a local method while the latter is
usually global and their produced explanations are different,
we will see it in our taxonomy). Guidotti et al. consider
all black-box models (including tree ensembles, SVMs etc.)
and give a ﬁne-grained classiﬁcation based on four dimensions
(the type of interpretability problem, the type of explanator, the
type of black-box model, and the type of data). However, they
treat decision trees, decision rules, saliency masks, sensitivity
analysis, activation maximization etc. equally, as explanators. In
our view, some of them are certain types of explanations while
some of them are methods used to produce explanations. Zhang
and Zhu review the methods to understand network’s midlayer representations or to learn networks with interpretable
representations in computer vision ﬁeld.
This survey has the following contributions:
• We make a further step towards the deﬁnition of interpretability on the basis of reference . In this deﬁnition,
we emphasize the type (or format) of explanations (e.g,
rule forms, including both decision trees and decision
rule sets). This acts as an important dimension in our
proposed taxonomy. Previous papers usually organize
existing methods into various isolated (to a large extent)
explanators (e.g., decision trees, decision rules, feature
importance, saliency maps etc.).
• We analyse the real needs for interpretability and summarize them into 3 groups: interpretability as an important
component in systems that should be highly-reliable,
ethical or legal requirements, and interpretability providing
tools to enhance knowledge in the relevant science ﬁelds.
In contrast, a previous survey only shows the
importance of interpretability by providing several cases
where black-box models can be dangerous.
• We propose a new taxonomy comprising three dimensions
(passive vs. active approaches, the format of explanations,
and local-semilocal-global interpretability). Note that
although many ingredients of the taxonomy have been
discussed in the previous literature, they were either
mentioned in totally different context, or entangled with
each other. To the best of our knowledge, our taxonomy
provides the most comprehensive and clear categorization
of the existing approaches.
The three degrees of freedom along which our taxonomy
is organized allow for a schematic 3D view illustrating how
diverse attempts at interpretability of deep networks are related.
It also provides suggestions for possible future work by ﬁlling
some of the gaps in the interpretability research (see Figure 2).
D. Organization of the Survey
The rest of the survey is organized as follows. In Section II,
we introduce our proposed taxonomy for network interpretation
methods. The taxonomy consists of three dimensions, passive
vs. active methods, type of explanations and global vs. local
interpretability. Along the ﬁrst dimension, we divide the
methods into two groups, passive methods (Section III) and
active methods (Section IV). Under each section, we traverse
the remaining two dimensions (different kinds of explanations,
and whether they are local, semi-local or global). Section V
gives a brief summary of the evaluation of interpretability.
Finally, we conclude this survey in Section VII.
II. TAXONOMY
We propose a novel taxonomy with three dimensions (see
Figure 1): (1) the passive vs. active approaches dimension,
(2) the type/format of produced explanations, and (3) from
local to global interpretability dimension respectively. The
ﬁrst dimension is categorical and has two possible values,
passive interpretation and active interpretability intervention.
It divides the existing approaches according to whether they
require to change the network architecture or the optimization
process. The passive interpretation process starts from a trained
network, with all the weights already learned from the training
set. Thereafter, the methods try to extract logic rules or extract
some understandable patterns. In contrast, active methods
require some changes before the training, such as introducing
IEEE TRANSACTIONS ON XXXX, VOL. X, NO. X, MM YYYY
Dimension 1 — Passive vs. Active Approaches
Post hoc explain trained neural networks
Actively change the network architecture or training process for better interpretability
Dimension 2 — Type of Explanations (in the order of increasing explanatory power)
To explain a prediction/class by
Attribution
Hidden semantics
Provide example(s) which may be considered similar or as prototype(s)
Assign credit (or blame) to the input features (e.g. feature importance, saliency masks)
Make sense of certain hidden neurons/layers
Extract logic rules (e.g. decision trees, rule sets and other rule formats)
Dimension 3 — Local vs. Global Interpretability (in terms of the input space)
Semi-local
Explain network’s predictions on individual samples (e.g. a saliency mask for an input image)
In between, for example, explain a group of similar inputs together
Explain the network as a whole (e.g. a set of rules/a decision tree)
Fig. 1. The 3 dimensions of our taxonomy.
extra network structures or modifying the training process.
These modiﬁcations encourage the network to become more
interpretable (e.g., more like a decision tree). Most commonly
such active interventions come in the form of regularization
In contrast to previous surveys, the other two dimensions
allow ordinal values. For example, the previously proposed
dimension type of explanator produces subcategories like
decision trees, decision rules, feature importance, sensitivity
analysis etc. However, there is no clear connection among
these pre-recognised explanators (what is the relation between
decision trees and feature importance). Instead, our second
dimension is type/format of explanation. By inspecting various
kinds of explanations produced by different approaches, we can
observe differences in how explicit they are. Logic rules provide
the most clear and explicit explanations while other kinds of
explanations may be implicit. For example, a saliency map
itself is just a mask on top of a certain input. By looking
at the saliency map, people construct an explanation “the
model made this prediction because it focused on this highly
inﬂuential part and that part (of the input)”. Hopefully, these
parts correspond to some domain understandable concepts.
Strictly speaking, implicit explanations by themselves are not
complete explanations and need further human interpretation,
which is usually automatically done when people see them. We
recognize four major types of explanations here, logic rules,
hidden semantics, attribution and explanations by examples,
listed in order of decreasing explanatory power. Similar
discussions can be found in the previous literature, e.g.,
Samek et al. provide a short subsection about “type of
explanations” (including explaining learned representations,
explaining individual predictions etc.). However, it is mixed
up with another independent dimension of the interpretability
research which we will introduce in the following paragraph.
A recent survey follows the same philosophy and treats
saliency maps and concept attribution as different types of
explanations, while we view them as being of the same kind,
but differing in the dimension below.
The last dimension, from local to global interpretability
(w.r.t. the input space), has become very common in recent
papers (e.g., , , , ), where global interpretability
means being able to understand the overall decision logic of a
model and local interpretability focuses on the explanations of
individual predictions. However, in our proposed dimension,
there exists a transition rather than a hard division between
global and local interpretability (i.e. semi-local interpretability).
Local explanations usually make use of the information at the
target input (e.g., its feature values, its gradient). But global
explanations try to generalize to as wide ranges of inputs as
possible (e.g., sequential covering in rule learning, marginal
contribution for feature importance ranking). This view is also
supported by the existence of several semi-local explanation
methods , . There have also been attempts to fuse local
explanations into global ones in a bottom-up fashion , ,
To help understand the latter two dimensions, Table II
lists examples of typical explanations produced by different
subcategories under our taxonomy. (Row 1) When considering
rule as explanation for local interpretability, an example is to
provide rule explanations which only apply to a given input
x(i) (and its associated output ˆy(i)). One of the solutions is to
ﬁnd out (by perturbing the input features and seeing how the
output changes) the minimal set of features xk. . .xl whose
presence supports the prediction ˆy(i). Analogously, features
xm. . .xn can be found which should not be present (larger
values), otherwise ˆy(i) will change. Then an explanation rule
for x(i) can be constructed as “it is because xk. . .xl are
present and xm. . .xn are absent that x(i) is classiﬁed as
ˆy(i)” . If a rule is valid not only for the input x(i), but
also for its “neighbourhood” , we obtain a semi-local
ZHANG et al.: A SURVEY ON NEURAL NETWORK INTERPRETABILITY
interpretability. And if a rule set or decision tree is extracted
from the original network, it explains the general function of
the whole network and thus provides global interpretability.
(Row 2) When it comes to explaining the hidden semantics, a
typical example (global) is to visualize what pattern a hidden
neuron is mostly sensitive to. This can then provide clues about
the inner workings of the network. We can also take a more
pro-active approach to make hidden neurons more interpretable.
As a high-layer hidden neuron may learn a mixture of patterns
that can be hard to interpret, Zhang et al. introduced a
loss term that makes high-layer ﬁlters either produce consistent
activation maps (among different inputs) or keep inactive (when
not seeing a certain pattern). Experiments show that those
ﬁlters are more interpretable (e.g., a ﬁlter may be found to be
activated by the head parts of animals). (Row 3) Attribution as
explanation usually provides local interpretability. Thinking
about an animal classiﬁcation task, input features are all the
pixels of the input image. Attribution allows people to see
which regions (pixels) of the image contribute the most to
the classiﬁcation result. The attribution can be computed e.g.,
by sensitivity analysis in terms of the input features (i.e. all
pixels) or some variants , . For attribution for global
interpretability, deep neural networks usually cannot have as
straightforward attribution as e.g., coefﬁcients w in linear
models y = w⊤x + b, which directly show the importance of
features globally. Instead of concentrating on input features
(pixels), Kim et al. were interested in attribution to a
“concept” (e.g., how sensitive is a prediction of zebra to the
presence of stripes). The concept (stripes) is represented by
the normal vector to the plane which separates having-stripes
and non-stripes training examples in the space of network’s
hidden layer. It is therefore possible to compute how sensitive
the prediction (of zebra) is to the concept (presence of stripes)
and thus have some form of global interpretability. (Row 4)
Sometimes researchers explain network prediction by showing
other known examples providing similar network functionality.
To explain a single input x(i) (local interpretability), we can
ﬁnd an example which is most similar to x(i) in the network’s
hidden layer level. This selection of explanation examples
can also be done by testing how much the prediction of x(i)
will be affected if a certain example is removed from the
training set . To provide global interpretability by showing
examples, a method is adding a (learnable) prototype layer to
a network. The prototype layer forces the network to make
predictions according to the proximity between input and the
learned prototypes. Those learned and interpretable prototypes
can help to explain the network’s overall function.
With the three dimensions introduced above, we can visualize
the distribution of the existing interpretability papers in a 3D
view (Figure 2 only provides a 2D snapshot, we encourage
readers to visit the online interactive version for better presentation). Table III is another representation of all the reviewed
interpretability approaches which is good for quick navigation.
In the following the sections, we will scan through Table III
along each dimension. The ﬁrst dimension results in two
sections, passive methods (Section III) and active methods
(Section IV). We then expand each section to several subsections according to the second dimension (type of explanation).
The distribution of the interpretability papers in the 3D space of
our taxonomy. We can rotate and observe the density of work in certain
areas/planes and ﬁnd the missing parts of interpretability research. (See https:
//yzhang-gh.github.io/tmp-data/index.html)
Under each subsection, we introduce (semi-)local vs. global
interpretability methods respectively.
III. PASSIVE INTERPRETATION OF TRAINED NETWORKS
Most of the existing network interpreting methods are passive
methods. They try to understand the already trained networks.
We now introduce these methods according to their types of
produced explanations (i.e. the second dimension).
A. Passive, Rule as Explanation
Logic rules are commonly acknowledged to be interpretable
and have a long history of research. Thus rule extraction is
an appealing approach to interpret neural networks. In most
cases, rule extraction methods provide global explanations as
they only extract a single rule set or decision tree from the
target model. There are only a few methods producing (semi-
)local rule-form explanations which we will introduce below
(Section III-A1), followed are global methods (Section III-A2).
Another thing to note is that although the rules and decision
trees (and their extraction methods) can be quite different, we
do not explicitly differentiate them here as they provide similar
explanations (a decision tree can be ﬂattened to a decision rule
set). A basic form of a rule is
If P, then Q.
where P is called the antecedent, and Q is called the
consequent, which in our context is the prediction (e.g., class
label) of a network. P is usually a combination of conditions
on several input features. For complex models, the explanation
rules can be of other forms such as the propositional rule,
ﬁrst-order rule or fuzzy rule.
1) Passive, Rule as Explanation, (Semi-)local: According
to our taxonomy, methods in this category focus on a trained
neural network and a certain input (or a small group of inputs),
and produce a logic rule as an explanation. Dhurandhar et
al. construct local rule explanations by ﬁnding out features
IEEE TRANSACTIONS ON XXXX, VOL. X, NO. X, MM YYYY
EXAMPLE EXPLANATIONS OF NETWORKS. Please see Section II for details. Due to lack of space, we do not provide examples for semi-local interpretability
here. (We thank the anonymous reviewer for the idea to improve the clarity of this table.)
Local (and semi-local) interpretability
applies to a certain input x(i) (and its associated output
ˆy(i)), or a small range of inputs-outputs
Global interpretability
w.r.t. the whole input space
Rule as explanation
Explain a certain (x(i), y(i)) with a decision rule:
• The result “x(i) is classiﬁed as ˆy(i)” is because x1, x4, . . .
are present and x3, x5, . . . are absent .
• (Semi-local) For x in the neighbourhood of x(i), if (x1 >
α) ∧(x3 < β) ∧. . ., then y = ˆy(i) .
Explain the whole model y(x) with a decision rule set:
The neural network can be approximated by
If (x2 < α) ∧(x3 > β) ∧. . .
, then y = 1,
If (x1 > γ) ∧(x5 < δ) ∧. . .
, then y = 2,
If (x4 . . . ) ∧(x7 . . . ) ∧. . .
, then y = M
Explaining
hidden semantics
(make sense of certain
hidden neurons/layers)
Explain a hidden neuron/layer h(x(i)):
(*No explicit methods but many local attribution methods
(see below) can be easily modiﬁed to “explain” a hidden
neuron h(x) rather than the ﬁnal output y).
Explain a hidden neuron/layer h(x) instead of y(x):
• An example active method adds a special loss term
that encourages ﬁlters to learn consistent and exclusive
patterns (e.g. head patterns of animals)
animal label
“receptive ﬁelds” :
Attribution
as explanation
Explain a certain (x(i), y(i)) with an attribution a(i):
neural net
ˆy(i): junco bird
The “contribution”1 of each pixel:
a.k.a. saliency map, which can be computed by different
methods like gradients , sensitivity analysis2 etc.
Explain y(x) with attribution to certain features in general:
(Note that for a linear model, the coefﬁcients is the global
attribution to its input features.)
• Kim et al. calculate attribution to a target “concept”
rather than the input pixels of a certain input. For example,
“how sensitive is the output (a prediction of zebra) to a concept
(the presence of stripes)?”
Explanation by
showing examples
Explain a certain (x(i), y(i)) with another x(i)′:
neural net
ˆy(i): ﬁsh
By asking how much the network will change ˆy(i) if
removing a certain training image, we can ﬁnd:
most helpful2 training images:
Explain y(x) collectively with a few prototypes:
• Adds a (learnable) prototype layer to the network.
Every prototype should be similar to at least an encoded
input. Every input should be similar to at least a prototype.
The trained network explains itself by its prototypes. 
1 the contribution to the network prediction of x(i).
2 how sensitive is the classiﬁcation result to the change of pixels.
3 without the training image, the network prediction of x(i) will change a lot. In other words, these images help the network make a decision on x(i).
that should be minimally and sufﬁciently present and features
that should be minimally and necessarily absent. In short,
the explanation takes this form “If an input x is classiﬁed
as class y, it is because features fi, . . . , fk are present and
features fm, . . . , fp are absent”. This is done by ﬁnding
small sparse perturbations that are sufﬁcient to ensure the
same prediction by its own (or will change the prediction
if applied to a target input)2. A similar kind of methods is
counterfactual explanations . Usually, we are asking based
on what features (’s values) the neural network makes the
prediction of class c. However, Goyal et al. try to ﬁnd the
minimum-edit on an input image which can result in a different
predicted class c′. In other words, they ask: “What region in
the input image makes the prediction to be class c, rather
2The authors also extended this method to learn a global interpretable
model, e.g., a decision tree, based on custom features created from above
local explanations .
than c′”. Kanamori et al. introduced distribution-aware
counterfactual explanations, which require above “edit” to
follow the empirical data distribution instead of being arbitrary.
Wang et al. came up with another local interpretability
method, which identiﬁes critical data routing paths (CDRPs)
of the network for each input. In convolutional neural networks,
each kernel produces a feature map that will be fed into the
next layer as a channel. Wang et al. associated every
output channel on each layer with a gate (non-negative weight),
which indicates how critical that channel is. These gate weights
are then optimized such that when they are multiplied with
the corresponding output channels, the network can still make
the same prediction as the original network (on a given input).
Importantly, the weights are encouraged to be sparse (most are
close to zero). CDRPs can then be identiﬁed for each input
by ﬁrst identifying the critical nodes, i.e. the intermediate
kernels associated with positive gates. We can explore and
ZHANG et al.: A SURVEY ON NEURAL NETWORK INTERPRETABILITY
AN OVERVIEW OF THE INTERPRETABILITY PAPERS.
Semi-local
CEM , CDRPs , CVE2 ,
Anchors ,
Interpretable
partial substitution 
KT , MofN , NeuralRule ,
NeuroLinear , GRG ,
GyanFO , •FZ , , Trepan ,
• , DecText , Global model on
Hidden semantics
(*No explicit methods but many in
the below cell can be applied here.)
Visualization , – ,
Network dissection , Net2Vec ,
Linguistic correlation analysis 
Attribution1
LIME ,
MAPLE ,
derivatives , DeconvNet ,
backprop ,
Grad-CAM ,
Shapley values – ,
Sensitivity analysis , , ,
Feature selector ,
Bias attribution 
DeepLIFT ,
LRP ,
Integrated gradients ,
Feature selector ,
Feature selector , TCAV ,
ACE , SpRAy3 , MAME 
DeepConsensus 
By example
Inﬂuence functions ,
Representer point selection 
Regional tree
regularization 
Tree regularization 
Hidden semantics
“One ﬁlter, one concept” 
Attribution
ExpO , DAPr 
Dual-net (feature importance) 
By example
Network with a prototype layer ,
ProtoPNet 
FO First-order rule
FZ Fuzzy rule
1 Some attribution methods (e.g., DeconvNet, Guided Backprop) arguably have certain non-locality because of the rectiﬁcation operation.
2 Short for counterfactual visual explanations
3 SpRAy is ﬂexible to provide semi-local or global explanations by clustering local (individual) attributions.
assign meanings to the critical nodes so that the critical paths
become local explanations. However, as the original paper did
not go further on the CDRPs representation which may not be
human-understandable, it is still more of an activation pattern
than a real explanation.
We can also extract rules that cover a group of inputs rather
than a single one. Ribeiro et al. propose anchors which
are if-then rules that are sufﬁciently precise (semi-)locally. In
other words, if a rule applies to a group of similar examples,
their predictions are (almost) always the same. It is similar
to (actually, on the basis of) an attribution method LIME,
which we will introduce in Section III-C. However, they are
different in terms of the produced explanations (LIME produces
attribution for individual examples). Wang et al. attempted
to ﬁnd an interpretable partial substitution (a rule set) to the
network that covers a certain subset of the input space. This
substitution can be done with no or low cost on the model
accuracy according to the size of the subset.
2) Passive, Rule as Explanation, Global: Most of the time,
we would like to have some form of an overall interpretation
of the network, rather than its local behaviour at a single
point. We again divide these approaches into two groups.
Some rule extraction methods make use of the network-speciﬁc
information such as the network structure, or the learned
weights. These methods are called decompositional approaches
in previous literature . The other methods instead view
the network as a black-box and only use it to generate training
examples for classic rule learning algorithms. They are called
pedagogical approaches.
a) Decompositional approaches: Decompositional approaches generate rules by observing the connections in a
network. As many of the these approaches were developed
before the deep learning era, they are mostly designed for
classic fully-connected feedforward networks. Considering a
single-layer setting of a fully-connected network (only one
output neuron),
where σ is an activation function (usually sigmoid, σ(x) =
1/(1+e−x)), w are the trainable weights, x is the input vector,
and b is the bias term (often referred as a threshold θ is the early
time, and b here can be interpreted as the negation of θ). Lying
at the heart of rule extraction is to search for combinations
of certain values (or ranges) of attributes xi that make y near
1 . This is tractable only when we are dealing with small
networks because the size of the search space will soon grow
to an astronomical number as the number of attributes and
the possible values for each attribute increase. Assuming we
have n Boolean attributes xi as an input, and each attribute
can be true or false or absent in the antecedent, there are
O(3n) combinations to search. We therefore need some search
strategies.
IEEE TRANSACTIONS ON XXXX, VOL. X, NO. X, MM YYYY
One of the earliest methods is the KT algorithm . KT
algorithm ﬁrst divides the input attributes into two groups,
pos-atts (short for positive attributes) and neg-atts, according to
the signs of their corresponding weights. Assuming activation
function is sigmoid, all the neurons are booleanized to true
(if close enough to 1) or false (close to 0). Then, all
combinations of pos-atts are selected if the combination can on
its own make y be true (larger than a pre-deﬁned threshold β
without considering the neg-atts), for instance, a combination
{x1, x3} and σ(P
i∈{1,3} wixi + b) > β. Finally, it takes into
account the neg-atts. For each above pos-atts combination, it
ﬁnds combinations of neg-atts (e.g., {x2, x5}) that when absent
the output calculated from the selected pos-atts and unselected
neg-atts is still true. In other words, σ(P
i∈I wixi + b) > β,
where I = {x1, x3} ∪{neg-atts} \ {x2, x5}. The extracted
rule can then be formed from the combination I and has the
output class 1. In our example, the translated rule is
If x1(is true) ∧x3 ∧¬x2 ∧¬x5, then y = 1.
Similarly, this algorithm can generate rules for class 0 (searching neg-atts ﬁrst and then adding pos-atts). To apply to the
multi-layer network situation, it ﬁrst does layer-by-layer rule
generation and then rewrites them to omit the hidden neurons.
In terms of the complexity, KT algorithm reduces the search
space to O(2n) by distinguishing pos-atts and neg-atts (posatts will be either true or absent, and neg-atts will be either
false or absent). It also limits the number of attributes
in the antecedent, which can further decrease the algorithm
complexity (with the risk of missing some rules).
Towell and Shavlik focus on another kind of rules of
“M-of-N” style. This kind of rule de-emphasizes the individual
importance of input attributes, which has the form
If M of these N expressions are true, then Q.
This algorithm has two salient characteristics. The ﬁrst one
is link (weight) clustering and reassigning them the average
weight within the cluster. Another characteristic is network
simplifying (eliminating unimportant clusters) and re-training.
Comparing with the exponential complexity of subset searching
algorithm, M-of-N method is approximately cubic because of
its special rule form.
NeuroRule introduced a three-step procedure of extracting rules: (1) train the network and prune, (2) discretize
(cluster) the activation values of the hidden neurons, (3) extract
the rules layer-wise and rewrite (similar as previous methods).
NeuroLinear made a little change to the NeuroRule method,
allowing neural networks to have continuous input. Andrews
et al. and Tickle et al. provide a good summary
of the rule extraction techniques before 1998.
b) Pedagogical approaches: By treating the neural network as a black-box, pedagogical methods (or hybrids of
both) directly learn rules from the examples generated by
the network. It is essentially reduced to a traditional rule
learning or decision tree learning problem. For rule set learning,
we have sequential covering framework (i.e. to learn rules
one by one). And for decision tree, there are many classic
algorithms like CART and C4.5 . Example work of
decision tree extraction (from neural networks) can be found
in references – .
Odajima et al. followed the framework of NeuroLinear
but use a greedy form of sequential covering algorithm to
extract rules. It is reported to be able to extract more concise
and precise rules. Gyan method goes further than extracting
propositional rules. After obtaining the propositional rules by
the above methods, Gyan uses the Least General Generalization
(LGG ) method to generate ﬁrst-order logic rules from
them. There are also some approaches attempting to extract
fuzzy logic from trained neural networks , , . The
major difference is the introduction of the membership function
of linguistic terms. An example rule is
If (x1 = high) ∧. . . , then y = class1.
where high is a fuzzy term expressed as a fuzzy set over the
Most of the above “Rule as Explanation, Global” methods
were developed in the early stage of neural network research,
and usually were only applied to relatively small datasets
(e.g., the Iris dataset, the Wine dataset from the UCI Machine
Learning Repository). However, as neural networks get deeper
and deeper in recent applications, it is unlikely for a single
decision tree to faithfully approximate the behaviour of deep
networks. We can see that more recent “Rule as Explanation”
methods turn to local or semi-local interpretability , .
B. Passive, Hidden Semantics as Explanation
The second typical kind of explanations is the meaning
of hidden neurons or layers. Similar to the grandmother cell
hypothesis3 in neuroscience, it is driven by a desire to associate
abstract concepts with the activation of some hidden neurons.
Taking animal classiﬁcation as an example, some neurons may
have high response to the head of an animal while others
neurons may look for bodies, feet or other parts. This kind of
explanations by deﬁnition provides global interpretability.
1) Passive, Hidden Semantics as Explanation, Global:
Existing hidden semantics interpretation methods mainly focus
on the computer vision ﬁeld. The most direct way is to show
what the neuron is “looking for”, i.e. visualization. The key to
visualization is to ﬁnd a representative input that can maximize
the activation of a certain neuron, channel or layer, which
is usually called activation maximization . This is an
optimization problem, whose search space is the potentially
huge input (sample) space. Assuming we have a network
taking as input a 28 × 28 pixels black and white image (as
in the MNIST handwritten digit dataset), there will be 228×28
possible input images, although most of them are probably
nonsensical. In practice, although we can ﬁnd a maximum
activation input image with optimization, it will likely be
unrealistic and uninterpretable. This situation can be helped
with some regularization techniques or priors.
We now give an overview over these techniques. The
framework of activation maximization was introduced by Erhan
et al. (although it was used in the unsupervised deep
3 cell
ZHANG et al.: A SURVEY ON NEURAL NETWORK INTERPRETABILITY
models like Deep Belief Networks). In general, it can be
formulated as
x⋆= arg max
(act(x; θ) −λΩ(x))
where act(·) is the activation of the neuron of interest, θ is the
network parameters (weights and biases) and Ωis an optional
regularizer. (We use the bold upright x to denote an input
matrix in image related tasks, which allows row and column
indices i and j.)
Simonyan et al. for the ﬁrst time applied activation
maximization to a supervised deep convolutional network
(for ImageNet classiﬁcation). It ﬁnds representative images
by maximizing the score of a class (before softmax). And the
Ωis the L2 norm of the image. Later, people realized high
frequency noise is a major nuisance that makes the visualization
unrecognizable , . In order to get natural and useful
visualizations, ﬁnding good priors or regularizers Ωbecomes
the core task in this kind of approaches.
Mahendran and Vedaldi propose a regularizer total
(xi,j+1 −xi,j)2 + (xi+1,j −xi,j)2 β
which encourages neighbouring pixels to have similar values.
This can also be viewed as a low-pass ﬁlter that can reduce
the high frequency noise in the image. This kind of methods
is usually called image blurring. Besides suppressing high
amplitude and high frequency information (with L2 decay
and Gaussian blurring respectively), Yosinski et al. also
include other terms to clip the pixels of small values or little
importance to the activation. Instead of using many handcrafted regularizers (image priors), Nguyen et al. suggest
using natural image prior learned by a generative model.
As Generative Adversarial Networks (GANs) showed
recently great power to generate high-resolution realistic
images , , making use of the generative model of a
GAN appears to be a good choice. For a good summary and
many impressive visualizations, we refer the readers to .
When applied to certain tasks, researchers can get some insights
from these visual interpretations. For example, Minematsu
et al. , inspected the behaviour of the ﬁrst and
last layers of a DNN used for change detection (in video
stream), which may suggest the possibility of a new background
modelling strategy.
Besides visualization, there are also some work trying to
ﬁnd connections between kernels and visual concepts (e.g.,
materials, certain objects). Bau et al. (Network Dissection)
collected a new dataset Broden which provides a pixel-wise
binary mask Lc(x) for every concept c and each input image
x. The activation map of a kernel k is upscaled and converted
(given a threshold) to a binary mask Mk(x) which has the
same size of x. Then the alignment between a kernel k and a
certain concept c (e.g., car) is computed as
P |Mk(x) ∩Lc(x)|
P |Mk(x) ∪Lc(x)|
where | · | is the cardinality of a set and the summation P is
over all the inputs x that contains the concept c. Along the
same lines, Fong and Vedaldi investigate the embeddings
of concepts over multiple kernels by constructing M with a
combination of several kernels. Their experiments show that
multiple kernels are usually required to encode one concept and
kernel embeddings are better representations of the concepts.
Dalvi et al. also analysed the meaning of individual
units/neurons in the networks for NLP tasks. They build a linear
model between the network’s hidden neurons and the output.
The neurons are then ranked according to the signiﬁcance of
the weights of the linear model. For those top-ranking neurons,
their linguistic meanings are investigated by visualizing their
saliency maps on the inputs, or by ﬁnding the top words by
which they get activated.
C. Passive, Attribution as Explanation
Attribution is to assign credit or blame to the input features in
terms of their impact on the output (prediction). The explanation
will be a real-valued vector which indicates feature importance
with the sign and amplitude of the scores . For simple
models (e.g., linear models) with meaningful features, we might
be able to assign each feature a score globally. When it comes
to more complex networks and input, e.g., images, it is hard to
say a certain pixel always has similar contribution to the output.
Thus, many methods do attribution locally. We introduce them
below and at the end of this section we mention a global
attribution method on intermediate representation rather than
the original input features.
1) Passive, Attribution as Explanation, (Semi-)local: Similarly to the decompositional vs. pedagogical division of rule
extraction methods, attribution methods can be also divided
into two groups: gradient-related methods and model agnostic
a) Gradient-related and backpropagation methods: Using
gradients to explain the individual classiﬁcation decisions is
a natural idea as the gradient represents the “direction” and
rate of the fastest increase on the loss function. The gradients
can also be computed with respect to a certain output class,
for example, along which “direction” a perturbation will make
an input more/less likely predicted as a cat/dog. Baehrens et
al. use it to explain the predictions of Gaussian Process
Classiﬁcation (GPC), k-NN and SVM. For a special case, the
coefﬁcients of features in linear models (or general additive
models) are already the partial derivatives, in other words,
the (global) attribution. So people can directly know how
the features affect the prediction and that is an important
reason why linear models are commonly thought interpretable.
While plain gradients, discrete gradients and path-integrated
gradients have been used for attribution, some other methods
do not calculate real gradients with the chain rule but only
backpropagate attribution signals (e.g., do extra normalization
on each layer upon backpropagation). We now introduce these
methods in detail.
In computer vision, the attribution is usually represented as
a saliency map, a mask of the same size of the input image.
In reference , the saliency map is generated from the
IEEE TRANSACTIONS ON XXXX, VOL. X, NO. X, MM YYYY
gradients (speciﬁcally, the maximum absolute values of the
partial derivatives over all channels). This kind of saliency
maps are obtained without effort as they only require a single
backpropagation pass. They also showed that this method is
equivalent to the previously proposed deconvolutional nets
method except for the difference on the calculation
of ReLU layer’s gradients. Guided backpropagation 
combines above two methods. It only takes into account the
gradients (the former method) that have positive error signal
(the latter method) when backpropagating through a ReLU layer.
There is also a variant Guided Grad-CAM (Gradient-weighted
Class Activation Mapping) , which ﬁrst calculates a coarsegrained attribution map (with respect to a certain class) on the
last convolutional layer and then multiply it to the attribution
map obtained from guided backpropagation. (Guided Grad-
CAM is an extension of CAM which requires a special
global average pooling layer.)
However, gradients themselves can be misleading. Considering a piecewise continuous function,
if x1 + x2 < 1 ;
if x1 + x2 ≥1 .
it is saturated when x1+x2 ≥1. At points where x1+x2 > 1,
their gradients are always zeros. DeepLIFT points
out this problem and highlights the importance of having
a reference input besides the target input to be explained.
The reference input is a kind of default or ‘neutral’ input
and will be different in different tasks (e.g., blank images or
zero vectors). Actually, as Sundararajan et al. point out,
DeepLIFT is trying to compute the “discrete gradient” instead
of the (instantaneous) gradient. Another similar “discrete
gradient” method is LRP (choosing a zero vector as
the reference point), differing in how to compute the discrete
gradient. This view is also present in reference that LRP
and DeepLIFT are essentially computing backpropagation for
modiﬁed gradient functions.
However, discrete gradients also have their drawbacks. As
the chain rule does not hold for discrete gradients, DeepLIFT
and LRP adopt modiﬁed forms of backpropagation. This makes
their attributions speciﬁc to the network implementation, in
other words, the attributions can be different even for two
functionally equivalent networks (a concrete example can be
seen in reference appendix B). Integrated gradients 
have been proposed to address this problem. It is deﬁned as the
path integral of all the gradients in the straight line between
input x and the reference input xref. The i-th dimension of the
integrated gradient (IG) is deﬁned as follows,
˜x=(xref+α(x−xref))
where ∂f(x)
is the i-th dimension of the gradient of f(x).
For those attribution methods requiring a reference point,
semi-local interpretability is provided as users can select
different reference points according to what to explain. Table IV
summarizes the above gradient-related attribution methods
(adapted from ). In addition to the “gradient” attribution
FORMULATION OF GRADIENT-RELATED ATTRIBUTION METHODS. Sc is the
output for class c (and it can be any neuron of interest), σ is the nonlinearity
in the network and g is a replacement of σ′ (the derivative of σ) in ∂Sc(x)
in order to rewrite DeepLIFT and LRP with gradient formulation (see 
for more details). xi is the i-th feature (pixel) of x.
Attribution
Gradient , 
Gradient ⊙Input
xi · ∂Sc(x)
xi · ∂gSc(x)
, g = σ(z)
DeepLIFT 
i ) · ∂gSc(x)
, g = σ(z) −σ(zref)
Integrated
Gradient 
˜x=xref+α(x−xref)
discussed above, Wang et al. point out that bias terms can
contain attribution information complementary to the gradients.
They propose a method to recursively assign the bias attribution
back to the input vector.
Those discrete gradient methods (e.g., LRP and DeepLIFT)
provide semi-local explanations as they explain a target input
w.r.t. another reference input. But methods such as DeconvNet
and Guided Backprop, which are only proposed to explain
individual inputs, arguably have certain non-locality because
of the rectiﬁcation operation during the process. Moreover, one
can accumulate multiple local explanations to achieve a certain
degree of global interpretability, which will be introduced in
Section III-C2.
Although we have many approaches to produce plausible
saliency maps, there is still a small gap between saliency
maps and real explanations. There have even been adversarial examples for attribution methods, which can produce
perceptively indistinguishable inputs, leading to the same
predicted labels, but very different attribution maps –
 . Researchers came up with several properties a saliency
map should have to be a valid explanation. Sundararajan et
al. (integrated gradients method) introduced two requirements, sensitivity and implementation invariance. Sensitivity
requirement is proposed mainly because of the (local) gradient
saturation problem (which results in zero gradient/attribution).
Implementation invariance means two functionally equivalent
networks (which can have different learned parameters given
the over-parametrizing setting of DNNs) should have the same
attribution. Kindermans et al. introduced input invariance.
It requires attribution methods to mirror the model’s invariance
with respect to transformations of the input. For example, a
model with a bias term can easily deal with a constant shift of
the input (pixel values). Obviously, (plain) gradient attribution
methods satisfy this kind of input invariance. For discrete
gradient and other methods using reference points, they depend
on the choices of reference. Adebayo et al. took a different
approach. They found that edge detectors can also produce
masks which look similar to saliency masks and highlight
ZHANG et al.: A SURVEY ON NEURAL NETWORK INTERPRETABILITY
some features of the input. But edge detectors have nothing
to do with the network or training data. Thus, they proposed
two tests to verify whether the attribution method will fail (1)
if the network’s weights are replaced with random noise, or
(2) if the labels of training data are shufﬂed. The attribution
methods should fail otherwise it suggests that the method does
not reﬂect the trained network or the training data (in other
words, it is just something like an edge detector).
b) Model agnostic attribution: LIME is a well-known
approach which can provide local attribution explanations (if
choosing linear models as the so-called interpretable components). Let f : Rd →{+1, −1} be a (binary classiﬁcation)
model to be explained. Because the original input x ∈Rd
might be uninterpretable (e.g., a tensor of all the pixels in
an image, or a word embedding ), LIME introduces an
intermediate representation x′ ∈{0, 1}d′ (e.g., the existence
of certain image patches or words). x′ can be recovered to the
original input space Rd. For a given x, LIME tries to ﬁnd a
potentially interpretable model g (such as a linear model or
decision tree) as a local explanation.
gx = arg min
L(f, g, πx) + Ω(g)
where G is the explanation model family, L is the loss function
that measures the ﬁdelity between f and g. L is evaluated on a
set of perturbed samples around x′ (and their recovered input),
which are weighted by a local kernel πx. Ωis the complexity
penalty of g, ensuring g to be interpretable. MAPLE is a
similar method using local linear models as explanations. The
difference is it deﬁnes the locality as how frequently the data
points fall into a same leaf node in a proxy random forest (ﬁt
on the trained network).
In game theory, there is a task to “fairly” assign each player
a payoff from the total gain generated by a coalition of all
players. Formally, let N be a set of n players, v: 2N →R is a
characteristic function, which can be interpreted as the total gain
of the coalition N. Obviously, v(∅) = 0. A coalitional game
can be denoted by the tuple ⟨N, v⟩. Given a coalitional game,
Shapley value is a solution to the payoff assignment
problem. The payoff (attribution) for player i can be computed
as follows,
(v(S ∪{i}) −v(S))
where v(S ∪{i}) −v(S) is the marginal contribution of player
i to coalition S. The rest of the formula can be viewed as a
normalization factor. A well-known alternative form of Shapley
where S(N) is the set of all ordered permutations of N, and
is the set of players in N which are predecessors of player
i in the permutation O. ˇStrumbelj and Kononenko adopted this
form so that v can be approximated in polynomial time 
(also see for another approximation method).
Back to the neural network (denoted by f), let N be all the
input features (attributes), S is an arbitrary feature subset of
interest (S ⊆N). For an input x, the characteristic function
v(S) is the difference between the expected model output when
we know all the features in S, and the expected output when
no feature value is known (i.e. the expected output over all
possible input), denoted by
f(τ(x, y, S)) −
X N and X N\S are respectively the input space containing
feature sets N and N \ S. τ(x, y, S) is a vector composed by
x and y according to whether the feature is in S.
However, a practical problem is the exponential computation
complexity, let alone the cost of the feed-forward computing on
each v(S) call. ˇStrumbelj and Kononenko approximate
Shapley value by sampling from S(N)×X (Cartesian product).
There are other variants such as using different v. More can
be found in reference which proposes a uniﬁed view
including not only the Shapley value methods but also LRP
and DeepLIFT. There is also Shapley value through the lens
of causal graph .
Sensitivity analysis can also be used to evaluate the importance of a feature. Speciﬁcally, the importance of a feature could
be measured as how much the model output will change upon
the change of a feature (or features). There are different kinds
of changes, e.g., perturbation, occlusion , etc. .
Chen et al. propose an instance-wise feature selector E
which maps an input x to a conditional distribution P(S | x),
where S is any subset (of certain size) of the original feature
set. The selected features can be denoted by xS. Then they
aim to maximize the mutual information between the selected
features and the output variable Y ,
subject to
A variational approximation is used to obtain a tractable
solution of the above problem.
2) Passive, Attribution as Explanation, Global: A natural
way to get global attribution is to combine individual ones
obtained from above local/semi-local methods. SpRAy 
clusters on the individual attributions and then summarizes
some groups of prediction strategies. MAME is a similar
method that can generate a multilevel (local to global) explanation tree. Salman et al. provide a different way, which
makes use of multiple neural networks. Each of the network can
provide its own local attributions, on top of which a clustering is
performed. Those clusters, intuitively the consensus of multiple
models, can provide more robust interpretations.
The attribution does not necessarily attribute ‘credits’ or
‘blame’ to the raw input or features. Kim et al. propose a
method TCAV (quantitative Testing with Concept Activation
Vectors) that can compute the model sensitivity of any userinterested concept. By ﬁrst collecting some examples with and
without a target concept (e.g., the presence of stripes in an
animal), the concept can then be represented by a normal vector
to the hyperplane separating those positive/negative examples
IEEE TRANSACTIONS ON XXXX, VOL. X, NO. X, MM YYYY
(pictures of animals with/without stripes) in a hidden layer. The
score of the concept can be computed as the (average) output
sensitivity if the hidden layer representation (of an input x)
moves an inﬁnitesimally small step along the concept vector.
This is a global interpretability method as it explains how a
concept affects the output in general. Besides being manually
picked by a human, these concepts can also be discovered
automatically by clustering input segments .
D. Passive, Explanation by Example
The last kind of explanations we reviewed is explanation
by example. When asked for an explanation for a new input,
these approaches return other example(s) for supporting or
counter example. One basic intuition is to ﬁnd examples that
the model considers to be most similar (in terms of latent
representations) . This is local interpretability but we can
also seek a set of representative samples within a class or for
more classes that provides global interpretability. A general
approach is presented in . There are other methods, such
as measuring how much a training example affects the model
prediction on a target input. Here we only focus on work
related to deep neural networks.
1) Passive, Explanation by Example, Local: Koh and
Liang provide an interesting method to evaluate how much
a training example affects the model prediction on an unseen
test example. The change of model parameters upon a change of
training example is ﬁrst calculated with approximation. Further,
its inﬂuence on the loss at the test point can be computed. By
checking the most inﬂuential (positively or negatively) training
examples (for the test example), we can have some insights
on the model predictions. Yeh et al. show that the logit
(the neuron before softmax) can be decomposed into a linear
combination of training points’ activations in the pre-logit
layer. The coefﬁcients of the training points indicate whether
the similarity to those points is excitatory or inhibitory. The
above two approaches both provide local explanations.
IV. ACTIVE INTERPRETABILITY INTERVENTION DURING
Besides passive looking for human-understandable patterns
from the trained network, researchers also tried to impose
interpretability restrictions during the network training process,
i.e. active interpretation methods in our taxonomy. A popular
idea is to add a special regularization term Ω(θ) to the loss
function, also known as “interpretability loss” (θ collects all
the weights of a network). We now discuss the related papers
according to the forms of explanations they provide.
A. Active, Rule as Explanation (semi-local or global)
Wu et al. propose tree regularization which favours
models that can be well approximated by shallow decision
trees. It requires two steps (1) train a binary decision tree using
data points
x(i), ˆy(i) N, where ˆy = fθ(x) is the network
prediction rather than the true label, (2) calculate the average
path length (from root to leaf node) of this decision tree over
all the data points. However, this tree regularization term Ω(θ)
is not differentiable. Therefore, a surrogate regularization term
ˆΩ(θ) was introduced. Given a dataset
θ(j), Ω(θ(j))
can be trained as a multi-layer perceptron network which
minimizes the squared error loss
θ(j), Ω(θ(j))
j=1 can be assembled during network training.
Also, data augmentation techniques can be used to generate
θ, especially in the early training phase. Tree regularization
enables global interpretability as it forces a network to be
easily approximable by a decision tree. Later, the authors also
proposed regional tree regularization which did this in a semilocal way .
B. Active, Hidden semantics as Explanation (global)
Another method aims to make a convolutional neural network
learn better (disentangled) hidden semantics. Having seen
feature visualization techniques mentioned above and some
empirical studies , , CNNs are believed to have learned
some low-level to high-level representations in the hierarchical
structure. But even if higher-layers have learned some objectlevel concepts (e.g., head, foot), those concepts are usually
entangled with each other. In other words, a high-layer ﬁlter
may contain a mixture of different patterns. Zhang et al. 
propose a loss term which encourages high-layer ﬁlters to
represent a single concept. Speciﬁcally, for a CNN, a feature
map (output of a high-layer ﬁlter, after ReLU) is an n × n
matrix. Zhang et al. predeﬁned a set of n2 ideal feature map
templates (activation patterns) T, each of which is like a
Gaussian kernel only differing on the position of its peak.
During the forward propagation, the feature map is masked
(element-wise product) by a certain template T ∈T according
to the position of the most activated “pixel” in the original
feature map. During the back propagation, an extra loss is
plugged in, which is the mutual information between M (the
feature maps of a ﬁlter calculated on all images) and T∪{T−}
(all the ideal activation patterns plus a negative pattern which
is full of a negative constant). This loss term makes a ﬁlter to
either have a consistent activation pattern or keep inactivated.
Experiments show that ﬁlters in their designed architecture are
more semantically meaningful (e.g., the “receptive ﬁeld” 
of a ﬁlter corresponds to the head of animals).
C. Active, Attribution as Explanation
Similar to tree regularization which helps to achieve better
global interpretability (decision trees), ExpO added an
interpretability regularizer in order to improve the quality of
local attribution. That regularization requires a model to have
ﬁdelitous (high ﬁdelity) and stable local attribution. DAPr 
(deep attribution prior) took into account additional information
(e.g., a rough prior about the feature importance). The prior
will be trained jointly with the main prediction model (as a
regularizer) and biases the model to have similar attribution as
the prior.
ZHANG et al.: A SURVEY ON NEURAL NETWORK INTERPRETABILITY
Besides performing attribution on individual input (locally
in input space), Dual-net was proposed to decide feature
importance population-wise, i.e., ﬁnding an ‘optimal’ feature
subset collectively for an input population. In this method,
a selector network is used to generate an optimal feature
subset, while an operator network makes predictions based
on that feature set. These two networks are trained jointly.
After training, the selector network can be used to rank feature
importance.
D. Active, Explanations by Prototypes (global)
Li et al. incorporated a prototype layer to a network
(speciﬁcally, an autoencoder). The network acts like a prototype classiﬁer, where predictions are made according to
the proximity between (the encoded) inputs and the learned
prototypes. Besides the cross-entropy loss and the (autoencoder)
reconstruction error, they also included two interpretability
regularization terms, encouraging every prototype to be similar
to at least one encoded input, vice versa. After the network is
trained, those prototypes can be naturally used as explanations.
Chen et al. add a prototype layer to a regular CNN rather
than an autoencoder. This prototype layer contains prototypes
that are encouraged to resemble parts of an input. When asked
for explanations, the network can provide several prototypes
for different parts of the input image respectively.
V. EVALUATION OF INTERPRETABILITY
In general, interpretability is hard to evaluate objectively as
the end-tasks can be quite divergent and may require domain
knowledge from experts . Doshi-Velez and Kim 
proposed three evaluation approaches: application-grounded,
human-grounded, and functionally-grounded. The ﬁrst one
measures to what extent interpretability helps the end-task
(e.g., better identiﬁcation of errors or less discrimination).
Human-grounded approaches are, for example, directly letting
people evaluate the quality of explanations with human-subject
experiments (e.g., let a user choose which explanation is of
the highest quality among several explanations). Functionallygrounded methods ﬁnd proxies for the explanation quality (e.g.,
sparsity). The last kind of approaches require no costly human
experiments but how to properly determine the proxy is a
challenge.
In our taxonomy, explanations are divided into different types.
Although the interpretability can hardly be compared between
different types of explanations, there are some measurements
proposed for this purpose. For logic rules and decision trees,
the size of the extracted rule model is often used as a
criterion , , (e.g., the number of rules, the
number of antecedents per rule, the depth of the decision
tree etc.). Strictly speaking, these criteria measure more about
whether the explanations are efﬁciently interpretable. Hidden
semantics approaches produce explanations on certain hidden
units in the network. Network Dissection quantiﬁes the
interpretability of hidden units by calculating their matchiness
to certain concepts. As for the hidden unit visualization
approaches, there is no a good measurement yet. For attribution
approaches, their explanations are saliency maps/masks (or
feature importance etc. according to the speciﬁc task). Samek et
al. evaluate saliency maps by the performance degradation
if the input image is partially masked with noise in an order
from salient to not salient patches. A similar evaluation method
is proposed in and Hooker et al. suggest using
a ﬁxed uninformative value rather than noise as the mask
and evaluating performance degradation on a retrained model.
Samek et al. also use entropy as another measure in the belief
that good saliency maps focus on relevant regions and do not
contain much irrelevant information and noise. Montavon et
al. would like the explanation function (which maps an
input to a saliency map) to be continuous/smooth, which means
the explanations (saliency maps) should not vary too much
when seeing similar inputs.
VI. DISCUSSION
In practice, different interpretation methods have their own
advantages and disadvantages. Passive (post-hoc) methods have
been widely studied because they can be applied in a relatively
straightforward manner to most existing networks. One can
choose methods that make use of a network’s inner information
(such as connection weights, gradients), which are usually
more efﬁcient (e.g., see Paragraph III-C1a). Otherwise there
are also model-agnostic methods that have no requirement of
the model architecture, which usually compute the marginal
effect of a certain input feature. But this generality is also a
downside of passive methods, especially because there is no
easy way to incorporate speciﬁc domain knowledge/priors.
Active (interpretability intervention) methods put forward
ideas about how a network should be optimized to gain
interpretability. The network can be optimized to be easily
ﬁt by a decision tree, or to have preferred feature attribution,
better tailored to a target task. However, the other side of the
coin is that such active intervention requires the compatibility
between networks and interpretation methods.
As for the second dimension, the format of explanations,
logical rules are the most clear (do not need further human
interpretation). However, one should carefully control the
complexity of the explanations (e.g., the depth of a decision
tree), otherwise the explanations will not be useful in practice.
Hidden semantics essentially explain a subpart of a network,
with most work developed in the computer vision ﬁeld.
Attribution is very suitable for explaining individual inputs.
But it is usually hard to get some overall understanding about
the network from attribution (compared to, e.g., logical rules).
Explaining by providing an example has the lowest (the most
implicit) explanatory power.
As for the last dimension, local explanations are more useful
when we care more about every single prediction (e.g., a credit
or insurance risk assessment). For some research ﬁelds, such
as genomics and astronomy, global explanations are more
preferred as they may reveal some general “knowledge”. Note,
however, there is no hard line separating local and global
interpretability. With the help of explanation fusing methods
(e.g., MAME), we can obtain multilevel (from local to global)
explanations.
IEEE TRANSACTIONS ON XXXX, VOL. X, NO. X, MM YYYY
VII. CONCLUSION
In this survey, we have provided a comprehensive review of
neural network interpretability. First, we have discussed the
deﬁnition of interpretability and stressed the importance of the
format of explanations and domain knowledge/representations.
Speciﬁcally, there are four commonly seen types of explanations: logic rules, hidden semantics, attribution and
explanations by examples. Then, by reviewing the previous
literature, we summarized 3 essential reasons why interpretability is important: the requirement of high reliability
systems, ethical/legal requirements and knowledge ﬁnding for
science. After that, we introduced a novel taxonomy for the
existing network interpretation methods. It evolves along three
dimensions: passive vs. active, types of explanations and global
vs. local interpretability. The last two dimensions are not purely
categorical but with ordinal values (e.g., semi-local). This is
the ﬁrst time we have a coherent overview of interpretability
research rather than many isolated problems and approaches.
We can even visualize the distribution of the existing approaches
in the 3D space spanned by our taxonomy.
From the perspective of the new taxonomy, there are still
several possible research directions in the interpretability research. First, the active interpretability intervention approaches
are underexplored. Some analysis of the passive methods also
suggests that the neural network does not necessarily learn
representations which can be easily interpreted by human
beings. Therefore, how to actively make a network interpretable
without harming its performance is still an open problem.
During the survey process, we have seen more and more recent
work ﬁlling this blank.
Another important research direction may be how to better incorporate domain knowledge in the networks. As we
have seen in this paper, interpretability is about providing
explanations. And explanations build on top of understandable
terms (or concepts) which can be speciﬁc to the targeted tasks.
We already have many approaches to construct explanations
of different types, but the domain-related terms used in the
explanations are still very simple (see Table I). If we can make
use of terms that are more domain/task-related, we can get
more informative explanations and better interpretability.