Kernel methods in computational biology
Jean-Philippe Vert (Ecole des Mines de Paris)
Bt~1:!f&J~~=:t:Ht.Q jJ ~*)vit
~?\?$J¥1f! (J?:*1~$mJi:PJf)
~.~~$~mJi:§~~-~~.~.~mJi:~~~~~~h~*.~~-~~M~G.~~$~~~fflti
{hU>t~ Eilh~~:~< t-:/it)~Bt.=f-$~1#l§£9 ~ ~ c -C~ ~o *t-:.
~4m$~:Slt~T~'j:~~~~ti~-~iO!~
~~h~~/it).~hG~~~GT.'5~c~-C~~.$~~m~~~lli9~C~m.ti.e~-~-C~~o
~.~~$~:sltn:·M~cti~~-~~:~'j:.lI~T~~l~-~. 1~$f$j~~-~. lI~T§£m~-~tic
~ ~ -c~'j:.
~h G~~-~~:. '5 ~ c ~PJ~~C9~jJ-*)v$~:~lt~Tm8)j9~o
jJ-*)v$~'j:
~ ~ -t1f.rl'l~~:~~~1!l7tJf~:Slt~TJ£JiG T~~=f-$-C~ ~.
~~$~"BtJ'.~< ~rl=l~e~:.ttffl~h Tlt~~o
2. Mercer fJ - *)v
~~ X ~ramiO' G~.~~. K(x,y) il!.
~t~ltt (K(x,y)=K(y,x)) ~frMJt-: G.
iEJE{[(['tt~frMJt-:9
~~~:. ~. K(.,.)
~'j: Mercer tJ-*)vclJ-¥~;fh~o
K(.,.) il! Mercer jJ-*)v-C~~~~.
~~ 1::)v/\()v t-
~rl'l~<I>.
XiO\G<I>~~~.¢
(X)1J~ffttG. K(x,yP'j:¢ (x) c<!> (y) ~P'lmctiQo
RKHS(reproducing kernel Hilbert space) C: lJ*tfh~ 1:::: )v/\()v J--. ~FI'I~~ ffllt\ ~ z: C: ~::J:: I:J. Mercer jJ -*)v
c RKHS ~~t.tt·:Ht9~ ~ c1J~-C~~o *t-:. RKHS ~m.ti'tt'itc GT. RKHS il!~~R*:lG~rl'l~-c35-:>
35Q~{tf:~-CiEJtrJ1~~ht-:~.~~/J\1~iO~~~R@I~J¢J.~~~~~9Q
~ c-Cfrx~clt~ '5 ~ ciO~~~j
3. fJ-*)v5!
jJ-*)v$~*~tJ.;frJJ¢J.~-~cGT. 1::)v/\()v t- ~FI'l~~~fl9Q ~ c~G ~:~/{~~.iO~frx~
~~;::c~'j:jJ-*)vt- ') '/7clJ-¥~;fh~o
AA!jfti{9ilcGT~'j:1::)v/\()vt-~rl'l~~:S~tQ
2J¢J.rl'l~~Im
l1JliO~jJ-*)v~.~AA!jftitJl~~bit-C*/it)~
~ c1J~-C~ Qo et I:J ~fflti{9ilc GT.
~BtM~~'±.=f-$~
-~-C~ Q'±~7.t7.t~(PCA) 1J~.
jJ-*)v~ ffllt~t-:~~~: 't. 1::)v/\()v t- ~rl'l~~:S~t ~Bt.ti G ~:t-=rX Qo
jJ -*)v~ ffllt\t-:iE$t13~7.t~ (CCA) ~'j:fi!il~{[([fit.rl=l~e~:9m~9 ~ ;:: ciO~-c~.
=~m~~-~ ~ ~~G
t-:M~~t-=r '5 ~~:~ ffl-C~~o
~7f- t- /\( 7 ~-~.~,</ (SVM) ~'j:jJ-*)v$~;:£':J <
~~$~~t-:/it)~=f-$-C.
~4xGh~~.iE~~~~7.tI1JlG.~~.Mm~*-c~ImI1Jl(~-~~)~M*cti~m~w~~.9~o
~llf]H:~'j:.
iE~~{9il~~~~;:7.t11Jl9Q;:: ciO~/f'PJ~~-C~Q~~iO~~lt~~'"t'.
7.tmaJ;'H~cImI1Jl~ t- v- ~::t7
Gt-:~~~~)MHt9~o SVM-C~'j:. 7J-*)vt-1)'/7~;:et~. ~)jti7.tlUm~W1J! (~<~~~~;:~'j:1>tiv~
~-1 A:~) iE~~{9ilO)$7.t~~~:~t9~7J-*)v~m~~bit~;:et
I:J ~m~h~o
~/I'\?~:T-~I;:M9.QfJ-*)v5!
tJ-*)v$~~~$~-~ ~::)jffl9 ~t-:{}).
~ ~ J\7'it~~@9~T-~ ~::~t9 ~jJ-*)v~.iO'q'J!~~
(:~t9~jJ-*)v~.~'j:et<mJi:~hTlt~Qo *~k~$7.tJt~~l~llim
t~JU~~/\(7 t- )v~ffllt)~;:: c ~:~ I:J.
)(~~rjiO' G.::L-71) '/ ~ ~rl'l~~~jJ-*)v~fx:~JE~-C~~1J~.
=f-$~'j: spectrum tJ-*)vclJ-¥~;fhTv) ~o *t;:.
~~lM~~;:J.t< ;frJffl ~hTv' ~~$1::~)v-C~ Q~h~
)v::1 7=C~)v (HMM) tic il, G't¥f¥N~fffitfJ9 ~ ~ c (:~ ~ tJ-*)V~fx:~JE~9Q. Fisher tJ-*)V'tfJ!~
~~UT-~ £)3'i-~:~'j:.
lI~TJ£m~-~. Phylogenetic Profile tic~. '5t-:/it)O)jJ-*)v~. 7·7 7f$j~
(:~9Q diffusion tJ-*)vctJ-*Jv CCA ~tJl~-@Jbit{~~J\Ar).:r:-1
CJ£m~-~~t13~~fffitfJ9Qm
1J~frbhT It, Q
jJ-*Jv~tJl~~bit(:~9 QmJi:'t1-=rbhTS ~.
*iEJE{[([BtTIID$~;:~Q. jJ-*Jv~~W*S~~M)i
{l::;tic1J~;p;fJi:~hTV 1Q 0
Kernel methods in computational biology
Jean-Philippe Vert
Ecole des Mines de Paris
35 rue Saint-Honor
77300 Fontainebleau
 
Introduction
Computational biology in the post-genomics era aims at providing a computational environment for biological research, where huge amounts of data generated by high-throughput
technologies can be integrated, and where biological hypotheses resulting from the automatic analysis of these data can be generated. Besides obvious data management issues,
core problems in computational biology concern the definition of a coherent and useful
mathematical framework to integrate data.
The purpose of this paper is to propose a mathematical framework to model large
sets of objects, such as the set of all genes in a genome or of all chemicals in a cell,
and to represent various types of information about these objects (such as the sequences,
structures and expression of genes) in a unified framework. Moreover, a number of tools
for analysis and inference are provided in this framework. It is based on the theory of
Mercer kernels and reproducing kernel Hilbert spaces, while analysis and inference tools
are derived from the recently developed theory of kernel methods which has attracted a
lot of attention during the last decade in the machine learning community.
This framework should obviously neither be considered a unique nor a final solution to
the problem of building a mathematical framework for post-genomics. There is probably
no single answer to this need, but rather a collection of possible frameworks with different
advantages and drawbacks. The one proposed in this paper is certainly very limited in
terms of its capacity to represent complex relationships among objects, such as evolutionary or regulatory relationships among genes. However, the tools for statistical analysis
and inference in this framework are reasonably powerful and have been shown to be useful
in many applications, such as predicting the functions of genes. Hence we can say that
this formalism is slightly biased toward deriving powerful inference tools, at the expense
of modeling complex biological relationships.
The organization of th~ paper is the following.
We first describe the mathematical
theory of Mercer kernels and review some of their properties, as well as the family of
kernel methods they underlie. We then turn our attention to biological systems such as
the set of all genes of an organism, and argue that various types of information about the
system can be encoded as Mercer kernels and can be considered as various realizations of
a more abstract model of the systems.
The application of kernel methods in computational biology has recently been subject
to much investigation. Even though we mention arbitrary references to illustrate some of
our statements, this short paper is not a review of this field and we apologize for all the
interesting recent contributions which we don't mention here.
Mercer kernels
Let X be a set supposed to represent objects one wants to analyze, e.g., the set of all
genes in a given genome.
We propose to represent any form of information about the
objects by a Hilbert space structure on the set X. Intuitively, the Hilbert product between
two objects will be defined as a mea~ure of similarity of the objects with respect to the
information available.
Obviously, every measure of similarity does not define a valid
Hilbert product.
For a function K : X x X
-t lR to be a valid product, it must be
symmetric (i.e., K(x, y) = K(y, x) for any x, y E X) and positive definite in the ~ense that
for any n EN, any (al,'"
,an) E lRn and any (Xl, ... ,xn) E X n, the following holds:
nL aiajK(xi, Xj) 2: O.
In that case we say that K is a Mercer kernel. It is easy to see that if X is a Hilbert
space with Hilbert product < .,. >, then K(x, y) =< X, Y > is a Mercer kernel on X.
Conversely, if X is a set and K(.,.) a Mercer kernel on X, then there exists a Hilbert
space <I> and a mapping ¢ : X -t <I> such that the kernel between two points be the Hilbert
product between their images: K(x, y) =< ¢(x), ¢(y) >.
An interesting construction of such a mapping ¢ is the reproducing kernel Hilbert space
(RKHS), which we now define. Simply speaking, a (real) RKHS on X is a Hilbert space
1i of real-valued functions on X with the property that, for each X E X, the evaluation
functional Lx which associates any f E 1i with Lxf = f(x), is a bounded linear functional.
The boundedness means that for any x EX, there exists a constant M x E lR such that
ILxfl = If(x)1 :S Mxllfllrt·
We remark that this definition implies that the functions of the RKH8 must be defined
pointwise and thus that the familiar space £2(X), if X is measurable, is not a RKH8.
The link with Mercer ~ernels is given in the following theorem which shows an equivalence
between RKH8 and Mercer kernels :
Theorem 1 Let (1i,< .,. »
be a RKHS on a set X. Then there exists a unique Mercer
kernel K on X with the following properties:
1. K spans 1i, i.e., 1i is the completion of span{K(x, .), x EX}.
2. K has the reproducing property, .i.e.,
V(j, x) E 1i x X,
< f, K(x,.) >= f(x);
in particular,
V(x, x') E X
< K(x, .), K(x' ,.) >= K(x, x').
Conversely, if X is a set and K is a Mercer kernel on X ,then a these equations define
uniquely a RKHS on X.
Equation 1 shows that when X is a set endowed with a Mercer kernel, then the mapping
¢ : x -t ¢(x) = K(x,.) from X to the RKH8 defined by K is a valid mapping from X
to a Hilbert space that satisfies K(x, y) =< ¢(x), ¢(y) >, and justifies the fact that any
Mercer kernel defines a Hilbert space structure on X.
A number of interesting properties make Mercer kernels a very powerful tool in realworld applications. First, Mercer kernels can be designed in such a way that the norm in
the RKHS be of interest for various purposes. As an example, suppose X = JRd , and take
the Gaussian RBF kernel:
K(x, y) = exp
which is a valid Mercer kernel. Then the norm in the associated RKHS is given by:
where j is the Fourier transform of f
(here H is the set of functions with a Fourier
transform such that (2) be finite). In that case Ilfll7-l is a smoothing functional, in the
sense that it is small when f is smooth, and large otherwise. This can typically be useful
as a term for regularization, when one wants to minimize a function 0,(J) on H with
simultaneously imposing some smoothness constraints on f (otherwise the problem might
be ill-posed).
In that case, instead of minimizing 0,(J), one might want to minimize
0,(J) + >'11f 117-l, where >. controls the trade-off between the minimization of n and the
smoothness of f.
A second remarkable propertiy of RKHS is that even though they are typically infinitedimensional, a large class of regularized minimization problems in H can be solved exactly
in a lower-dimensional space. The precise statement is contained in the following representer theorem [KW71, SS02]
Theorem 2 Let w : [0,(0)
---7 JR a strictly monotonic increasing function, X a set with a
Mercer kernel K
and associated RKHS H. Let (Xl, ... , Xn ) E X n be a set of points in X
and n be a function on H. expressed in terms of the Xi and f(Xi) only, i.e., of the form:
Then each minimizer f E H. of the regularized function:
0,(J) +w(llfll7-l)
admits a representation of the form
f(x) = I: CKiK(Xi, .).
The Representer theorem has tremendous consequences on the design of methods and
algorithms, including such techniques as splines [Wah90] and kernel methods [SS02]. It
means that minimizing a regularized form of a functional which only depends on the value
of f on a finite number of points can be done by working in a finite-dimensional space,
and reduces to finding coefficients CKi associated with each point Xi.
A whole variety of
algorithms can be deduced from this theorem by varying the function to be optimized
and the Mercer kernels, ranging from generalized forms of principal component analysis
to support vector machines for classificatiOIl and regression.
Kernel methods
In this section we briefly review a number of algorithms known as kernel methods based on
the Representer theorem. They all take as input a finite number of points in a set endowed
with a Mercer kernels, such as the set of genes where a Mercer kernel has been defined as
a similarity measure with respect to some sort of information, and provide useful tools to
perform various analysis of these points. For each of these methods, two complementary
points of views can help get an intuition of how they work.
First, the direct point of
view consists in viewing each point mapped by a function ¢ to some Hilbert space. The
algorithms consist them in working implicitly in the resulting Hilbert space. Second, from
a dual point of view, the Mercer kernel defines a RKHS. The algorithms consist then
in optimizing a regularized functional in the RKHS. The equivalence between these two
points of view is ensured by the construction of RKHS and the Representer theorem.
Computing a Euclidean distance
Suppose you want to define a Euclidean distance between two points x and x' of a space
X endowed with a Mercer kernel K. Then an obvious solution is to consider the Hilbert
distance between their images in a Hilbert space defined by K, i.e., to consider the following
square distance:
d(x, x')2 = 11¢(x) - ¢(x')112
=< ¢(x) - ¢(x'), ¢(x) - ¢(x') >
=< ¢(x), ¢(x) > + < ¢(x'), ¢(x') > -2 < ¢(x), ¢(x') >
= K(x,x) + K(x', x') - 2K(x,x').
The result is expressed in terms of J( only. This shows than once a kernel is given on a
set X, it defines naturally a Euclidean distance which can be obtained without computing
the images of the points in the Hilbert space associated with K. This is an example of
the kernel trick, which consists in working implicitly in a complex Hilbert space without
ever seeing a point in that space.
Computing the distance between a point and the center of a cloud
Suppose you have a set of points (Xl, . .. ,xn ) in a set X endowed with a Mercer kernel,
and you want to compute how each point is "far" from the "average" of all points. One
way to formalize this intuitive goal is to consider the set of points mapped to a Hilbert
space associated with J( by a mapping ¢. Then a natural definition of "average" of all
points is the center of mass of all all points, i.e.,
m = - L¢(Xi),
and the distance between a point Xi and the average can be quantified by the square
Euclidean distance between ¢(Xi) and m, which can be computed by:
Once again, the kernel trick enables to perform the computation implicitly in the feature
space, and to obtain the result in terms of the function K between the input points only.
Kernel principal component analysis
Suppose you want to analyze the variations among points (Xl, ... ,xn ) of a set X with
a Mercer kernel K.
A classical and useful tool in mathematical statistics is principal
component analysis (PCA), which is defined when X = IRd • It is a powerful technique for
extracting structure form possibly high-dimensional data sets, by projecting the points on
the so-called principal axes of the data, defined as the directions which capture the largest
amounts of variance [Jo196] (see Figure 1).
Figure 1: PCA detects the directions of largest variations among a family of points in a
vector space, called principal directions.
The standard PCA algorithm is readily performed by solving an eigenvalue problem,
i.e., by diagonalizing the covariance matrix:
where the data have been centered first
(L~l Xi = 0).
Suppose now that X is not a
vector space, but simply a set endowed with a Mercer kernel.
Then a useful way to
analyze variations among the points and extract features is to perform a PCA on the
points ¢(Xi) mapped to a Hilbert space associated with K. As shown in [SSM99, SS02],
this can be performed in a dual form by centering the data in the Hilbert space and
diagonalizing the matrix
K~j =< ¢O(Xi)' ¢o(Xj) >,
where ¢O(Xi) represents the point ¢(xiJ after centering. Observe that while the size of the
covariance matrix C is d x d, where d is the dimension of the vector space X in classical
PCA, the dimension of KO is n x n, where n is the number of points to analyze. Hence
it becomes tractable to perform PCA in a possibly infinite-dimensional space, because
the principal components are in fact always in the finite-dimensional space spanned by
the points.
Once again, the centering of the matrix, the computation of the principal
directions and the projections of the points can be computed implicitly by only working
with the function K.
Extracting correlations between two kernels
Suppose that two different Mercer kernels K 1 and K 2 are defined on the same set X. This
can be the case for instance when different notions of similarity are defined on the same
objects, such as sequence similarity and co-expression for genes. The two kernels define two
different mappings ¢ 1 and ¢2 to two Hilbert spaces. In order to compare the two kernels,
and extract correlations betwccn them, a useful tool in statistics is canonical correlation
- 147analysis (CCA). When a set of points is given in simultaneously two finite-dimensional
vector spaces, cla.'3sical CCA consists in extracting pairs of directions, one in each space,
such that the projections of the points on these directions be as correlated as possible
[Hot36] (see Figure 2). Recently it was pointed out that a regularized form of CCA can be
Figure 2: CCA detects directions simultaneously in two vector spaces such that the projections of the points be maximally correlated.
performed implicitly between two sets of points mapped to Hilbert spaces associated with
Mercer kernels [BJ02], and that performing CCA in this context is equivalent to solving
the following generalized eigenvalue problem:
This provides a useful tool to extract correlations between two forms of informations encoded in two kernels, and h&') recently been applied in the context of natural language
processing [AVC03], to extract language-independent semantic representations of texts,
and in computational biology [VK, YVNK03], to compare gene networks and gene expressions or detect operons in prokaryote genomes.
Support vector machines
Support vector machines (SVM) are certainly the best-known kernel methods.
popularity is due to their remarkable performances on many real-world problem, and
they have been a main topic of investigation in the machine learning community during
the last decade.
First introduced by Vapnik and coworkers [BGV92, Vap98], SVM are
a family of algorithms useful for classification and regression.
In the simplest binary
classification case, points (Xl, ... ,xn ) from a set X with a Mercer kernel K are given
together with a binary label associated with each point, denoted by (Yl,"" Yn) where
Yi E {-I, I}. The goal of classification is to find a discrimination between positive and
negative point,s. The solution implemented in SVM, motivated by theoretical results of
statistical learning theory, is to take a linear discrimination between the positive and
negative points mapped to the Hilbert space associated with K, such that the distance
between the separating hyperplane and the closest point (called the margin) be the largest
possible. This problem has a unique solution when the points are linearly separable, i.e.,
when there exists at least one hyperplane which separates positive points from negative
ones. In the general situation, SVM finds an hyperplane that optimizes a trade-off between
the number of misclassified points and the margin (see Figure 3). Once again this problem
can be expressed in a dual formulation and results in the following optimization problem:
1ll""O I:~~j f1; -0\ I:;~j~j f1;f1jY;YjJ((x;, Xj),
'V'l, - 1, ... ,n
O:S O:'I :S C,
2:7=1 O:"iYi = 0,
Figure 3: SVM finds a lineal' discrimination rule between two classes of points, by optimizing a functions which achieves a trade-off between the number of misclassified points
(a small as possible) and the margin of the classification, i.e., the distance between the
separating hyperplane and the closest point (which should be as large as possible).
where C is the parameter which controls the trade-off between large margin and good
classification. This algorithm is one of the most powerful algorithms availablenowadays
on many real-world problems.
In particular, as those equations suggest, if X
then the dimension d does not matter too much on the performance of SVM (thanks to
the choice of the largest margin linear classifier), or at least it seems to matter less for
SVM than for many other classification algorithms which often perform poorly in large
dimensions (this is part of a phenomenon called the curse of dimensionality).
SVM have been applied with success to many real-world problems.
In the field of
computational biology, a non-exhaustive list of applications using SVM include gene functional classification from microarrays [BGL+00, PWCG01], tissue samples classification
from microarrays [MTIVI+9S, FDC+OO, GWBV02], protein family prediction from aminoacid sequences [JDHOO], or protein secondary structure and localization prediction from
amino-acid sequences [HSOlb, HSOla].
Kernelizing the proteome
This quick overview of the theory of Mercer kernels, RKHS and kernel methods aimed at
convincing the reader that they provide a framework with sound theoretical foundations,
and which is associated with a panoply of powerful methods thanks to the simple Representer theorem. In this section, we now turn our attention to the question of the relevance
of this framework to post-genomiGs.
One of the main challenges of computational biology in the post-genomics era is to
integrate and process a variety of data generated by high-throughput technologies about
biological objects, and use them to generate biological hypothesis. In the sequel we focus on problems related to functional genomics and proteomics, where one wants to infer
knowledge about genes such as their functions, their regulation and their interactions
from several different sources of information. Concretely, genes and proteins can be characterized by a number of different points a view: they can be defined by nucleotides or
amino-acid sequences, by the structure of their promoter region in DNA, by their expression measured with DNA microarrays, by the secondary, tertiary and quaternary structure
of the proteins they encode, by their role in metabolic and signaling pathways, by their interactions etc... Each characterization incorporates some partial representation of a more
abstract concept of gene.
One approach to integrate these complementary characteri7;ations in a unified franlc-
- 149work is to encode each type of information into a Mercer kernel defined on the set X of
all genes (which can be considered a finite set when one focuses on the genes of a given
genome, but can be extended to more complex structures such as the set of all finitelengths nucleotide sequences for instance). This suggests a more abstract representation
of a set of genes, as for instance a probability density on the set of kernel functions (e.g.,
the empirical measure corresponding to a set of kernel functions obtained by encoding
different sources of informations). We won't develop further this avenue in this paper, but
further investigations of the shape of this empirical measure is likely to provide interesting
information about the relationships between different types of information. This could
also provide an interesting notion of complexity of a biological system, as the entropy of
the probability measure on the set of Mercer kernels used to represent it.
To illustrate this discussion we review in the following sections several recent attempts
to encode biological information into kernel functions, as well as several attempts to explore
the space of Mercer kernels obtained by different sources of information.
By lack of
space we will conclude with these onsiderations, but believe that exploring this framework
is likely to result in new interesting conceptualizations of biological systems, as well as
powerful tools to handle large amounts of heterogeneous data.
String kernel
Biological databases are full of sequences. A gene can be characterized as a subsequence
of the DNA molecule (hence as a finite string over a four-letter alphabet), or by the
sequence of amino-acids which forms the protein it encodes (hence as a finite string in
over 20-letter alphabet). In both cases, the set of genes is a finite set of sequences, with
a particular hidden structure. Many genes are known to share common ancestors, which
often indicates functional or structural relationships. This evolutionary relationship can be
detected to some extent by comparing the sequences of genes, because two genes sharing a
common ancestor were similar a long time ago, but have evolved in different environments
through mutations or insertion/deletion of letters or subsequences. As a result, measuring
the "similarity" between gene sequences gives some information about the evolutionary,
functional and structural relationships between genes.
It this context, a number of initiatives have been taken to engineer kernel functions for
sequences to transform the sequence similarity relationship into a Hilbert space structure
on the set of genes.
Examples include the spectrum kernel [LEN02] which explicitly
maps any sequence to its k-spectrum (k > 0) i.e., the number of appearance of each kmer in the sequence, and defines the inner product between k-spectra as Hilbert product
between sequences. This is motivated by the idea that even though the global appearance
of a sequence might be strongly modified by several mutations during evolution, its kspectrum might be more stable. This kernel was later improved in the mismatch kernel
[LEWN03] which consists in convolution the k-kernel with a kernel that accepts up to
m < k mismatches, and provides a powerful tool to detect remote homology between
Another general approach for string kernel engineering is the Fisher kernel [JH99] which
is a two-step process.
First, a parametric statistical model for sequences, i.e., a family
of probability densities {Po(.), f) E 8 C lRk } on the set of finite sequences, is built (e.g.,
a hidden Markov models [DEKM98]), and a parameter () E 8 is estimated to fit a set of
sequences of interest. Second, each sequence x is mapped to the score vector of) log Po(x),
i.e., to a vector in the tangent space at the point PiJ of the Riemannian manifold formed by
the statistieallllodel. This manifold has a natlll'al Riemannianllletric [ANOI] and a natural
positive definite quadratic form is therefore available in the tangent space where sequences
- 150are mapped. This ingenious approach is particularly useful because a lot of work has been
devoted in the early days of bioinformatics to the development of parametric statistical
models for biological objects, and the Fisher kernel provides a principled way to derive
a kernel from virtually any such model. We can also mention other approaches to make
kernels from probabilistic models, such as convolution kernels [Hau99, WatOO, SVUA03]
which are based on probabilistic models for pairs of sequences, and can be computed
efficiently using finite-state automata.
Expression profiles
With the development of DNA microarrays, it becomes increasingly simple to characterize
a gene by an expression profile, i.e., a vector of real numbers which indicate its level of
expression in different experiments.
As genes are supposed to be expressed when the
cell needs the proteins they encode, genes with similar profiles are likely to have related
functions. Hence it might be useful to define a Hilbert product from these vectors, which
can easily be achieved as a large number of kernel for real-valued vector have been known
for a long time, such as the simple inner product or the Gaussian RBF kernel.
Other kernels
Here we briefly mention a non-exhaustive list of Mercer kernels which have been developed
recently in the context of computational biology. As the number of sequenced organisms
increases, classical homology detection tools can detect whether a given gene is present
or absent in every fully sequenced organism.
Hence a gene can be characterized by a
vector of 0/1, which indicates its presence or absence in all sequenced organisms. This
vector is called the phylogenetic profile of the gene [PMT+99]. Two genes with similar
phylogenetic profiles are present and absent in the same organisms, hence chances are
high that they have related function or participate to common structural complexes, for
instance. While the simple dot product between vectors of bits provides a valid Mercer
kernel, a more specific kernel for phylogenetic profiles was proposed in [Ver02b] and shown
to be more relevant than the simple dot kernel. This kernel uses prior knowledge about the
evolutionary relationships among sequenced organisms, and is one instance of a general
approach to derive a kernel from probabilistic models when structural knowledge exists
among the random variables [Ver02a].
An other interesting approach is the diffusion kernel [KL02] which is a kernel for the
nodes of a graph. It is a discrete equivalent of the Gaussian RBF kernel, which can be
computed as the matrix exponential 'Of the opposite of the graph Laplacian. Intuitively,
the kernel between two nodes increases when there are many short paths between them;
more precisely, the kernel K(x, y) is the probability of reaching the node y by a random
walk on the graph starting from the node x, and killed with some probability at each step.
This kernel is known as the heat kernel [Chu97] in spectral graph theory, and is particularly
relevant to define a kernel from a network of genes, for example interaction or regulation
network. It was recently applied in combination with kernel canonical correlation analysis
to extract correlations between metabolic pathways (which can be represented as a network
of genes) and gene expression data [VK, VK03].
This short list is far from being exhaustive, and the list of kernels specifically developed
for particular objects is increasing very quickly, providing a panoply of Mercer kernels to
represent various types of information about genes.
Kernel operations
We showed in the previous sections that a number of kernels can be engineered to represent
specific knowledges about genes. As the cla.'3s of Mercer kernel is the class of symmetric
positive definite functions, a variety of new kernels can be computed from a family of basic
kernels using the closure properties of the class of symmetric positive definite functions.
For example, this class is a closed convex cone, i.e., it is stable by addition, multiplication
by a positive constant, and pointwise limit. Some kernels, known as infinitely divisible, can
be taken to an arbitrary non-negative exponent and remain positive definite [Hau99, SS02].
As an example, observe that if 1(1 is a string kernel for protein sequences, and J(2 is
a Gaussian RBF kernel for the expressions of the corresponding genes, then 1(1 + J(2 is
a kernel that incorporates both sequence and expression similarity. Hence the addition of
two kernels is a simple yet powerful way to combine two types of informations, which has
been for example successfully used in the context of gene function prediction [PWCG01].
More generally, if 1(1, ... ,J(p is a family of kernels on the same space, then a valid Mercer
kernel obtained as a linear combination of the basic kernel which optimizes some linear
function can be found by semi-definite programming, which has been explored for instance
in [LCG+02]. Alternatively, one can use information geometric properties of the set of
positive definite matrix to generate new kernels from basic kernels [KT].
Conclusion
In this short overview of recent advances in kernel methods and their applications in
computational biology, we tried to present kernel methods as a relevant framework to
manipulate biological data and perform computations and inference from them.
has been a very active and exciting field or research in the last few years, as it turns
out that a wide range of kernels can be engineered and adapted to particular data, and
that the machinery of kernel methods provide a large family of algorithms to extract
information from kernel representations.
Moreover, mathematical operations on kernels
provide a interesting starting point to develop more satisfactory frameworks for modeling
living systems. As an example, we mentioned the possibility we intend to study in the
future to represellt a biological system as a probability distribution on the set of Mercer
kernels, in such a way that' the Mercer'kernels built from various sources of information
would be different realizations of a more abstract random kernels.
Such developments
would certainly require more research or more integration of theoretical results about
positive definite kernels, and might lead to new families of algorithms to fit the need of
post-genomics.
Acknowledgments
I am grateful to Kenji Veno and Tsuyoshi Kato who offered me the opportunity to participate to the workshop "Mathematical aspects of molecular biology: toward new mathematics" that was held in Nara, Japan, on January 24-27, 2003. This text is based on the
talk I gave there.