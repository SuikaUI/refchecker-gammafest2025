Machine Learning, 52, 199–215, 2003
c⃝2003 Kluwer Academic Publishers. Manufactured in The Netherlands.
Tree Induction for Probability-Based Ranking
FOSTER PROVOST
 
New York University, New York, NY, USA
PEDRO DOMINGOS
 
University of Washington, Seattle, WA, USA
Editor: Douglas Fisher
Tree induction is one of the most effective and widely used methods for building classiﬁcation models.
However, many applications require cases to be ranked by the probability of class membership. Probability estimation trees (PETs) have the same attractive features as classiﬁcation trees (e.g., comprehensibility, accuracy and
efﬁciency in high dimensions and on large data sets). Unfortunately, decision trees have been found to provide poor
probability estimates. Several techniques have been proposed to build more accurate PETs, but, to our knowledge,
there has not been a systematic experimental analysis of which techniques actually improve the probability-based
rankings, and by how much. In this paper we ﬁrst discuss why the decision-tree representation is not intrinsically
inadequate for probability estimation. Inaccurate probabilities are partially the result of decision-tree induction algorithms that focus on maximizing classiﬁcation accuracy and minimizing tree size (for example via reduced-error
pruning). Larger trees can be better for probability estimation, even if the extra size is superﬂuous for accuracy
maximization. We then present the results of a comprehensive set of experiments, testing some straightforward
methods for improving probability-based rankings. We show that using a simple, common smoothing method—the
Laplace correction—uniformly improves probability-based rankings. In addition, bagging substantially improves
the rankings, and is even more effective for this purpose than for improving accuracy. We conclude that PETs,
with these simple modiﬁcations, should be considered when rankings based on class-membership probability are
ranking, probability estimation, classiﬁcation, cost-sensitive learning, decision trees, Laplace
correction, bagging
Introduction
Tree-induction programs have received a great deal of attention over the past ﬁfteen years
in the ﬁelds of machine learning and data mining. Several factors contribute to their popularity. Tree-induction programs are fast and effective . They work
remarkably well with no tweaking of parameters, which has facilitated their wide use in
the comparison of different learning algorithms. Tree induction also works comparatively
well with very large data sets , with large numbers of variables,
and with mixed-type data (continuous, nominal, Boolean, etc.). These qualities result in
part from the simple yet powerful divide-and-conquer algorithm underlying tree learners,
and in part from the high-quality software packages that have been available for learning
decision trees and C4.5 ).
F. PROVOST AND P. DOMINGOS
As they have been used in most research and applications, tree induction programs
produceclassiﬁers(wedonotconsiderregressionhere).Thesearemodelsthatmapinstances
described by a vector of independent variables to one of a set of classes. However, as
describedbelow,inmanyapplicationsthisisnotsufﬁcient;arankingbasedontheprobability
of class membership is needed, for example, so that a person can consider ﬁrst the cases most
likely to belong to the class. As we will show, the model that produces the best classiﬁcations
does not necessarily produce the best probability-based rankings.
Because of the attractive properties of tree induction, probability estimation trees
(PETs)—trees that estimate the probability of class membership—are seeing increasing
use in such applications. Unfortunately, trees have been observed to produce poor estimates
of class probabilities . Several researchers have proposed
techniques to improve the estimates, yet to our knowledge there has not been a systematic
study of their efﬁcacy for ranking.
In this paper, we present a study of how well these techniques improve the quality of rankings based on estimated class-membership probability. We ﬁrst discuss prior work using
and improving probability estimation trees. We then show that the decision tree representation is not inherently doomed to produce poor estimates, and that part of the problem is
that modern decision-tree induction algorithms are biased against building accurate PETs.
We use the results of this analysis and the suggestions of prior work to make a number of
simple modiﬁcations to the popular decision-tree learning program C4.5. We apply the ﬁrst
pair of modiﬁcations to some simple synthetic problems, demonstrating the improvement
in the probability estimates. We then report the results of a comprehensive experiment in
which several modiﬁcations are applied to a wide variety of benchmark data sets. The results
provide strong evidence that it is indeed possible to improve substantially the quality of
probability-based ranking models produced by tree induction.
Prior work
PETs recently have seen increasing use by practitioners and researchers, for example
in speech recognition , as node models in Bayesian networks , in the recently introduced dependency-network representation and
its application to collaborative ﬁltering and other areas , in network
diagnosis , and in cost-sensitive learning research . As described above, tree induction has many
attractive properties. Under what conditions would it be desirable or necessary for a learned
tree to produce effective probability-based rankings?
In many situations, rankings are more appropriate than categorical predictions. For example, a news-story ﬁlter or a web-page recommender may use the probability that an
instance is a member of the class “interesting to user” to rank previously unseen instances
for presentation. A fraud detection system may need to rank accounts by the probability
that they have been compromised.
How are probability estimates typically generated from decision trees? Recall that tree
induction partitions a data set recursively at each node. Each leaf (terminal node) deﬁnes
TREE INDUCTION FOR PROBABILITY-BASED RANKING
the subset of the data corresponding to the conjunction of the conditions along the path
back to the root. The goal of the decision-tree learning program is to make these subsets
be less “impure,” in terms of the mixture of class labels, than the unpartitioned data set.
For example, consider an unpartitioned population with two equally represented classes
(maximally impure). A leaf node deﬁning a subset of the population of which 90% are one
class would be much less impure, and may facilitate accurate classiﬁcation (only 10% error
if this subset were classiﬁed as the majority class).
The previous example illustrates how class-membership probabilities typically are generated from decision trees. If a leaf node deﬁnes a subset of 100 training instances, 90 of
which are one class (call it the “positive” class), then in use, any instance that corresponds
to this leaf is assigned a probability of 0.9 (90/100) that it belongs to the positive class.
Notice a potential problem with this method of probability estimation. What if a leaf
comprises only 5 training instances, all of which are of the positive class? Are you willing
to have your probability estimator give an estimate of 1.0 (5/5) that subsequent instances
matching the leaf’s conditions also will be positive? Perhaps 5 instances is not enough
evidenceforsuchastrongstatement?Therearetwopotentialdirectsolutionstothisproblem.
One is that a statement of conﬁdence in the probability estimation accompany the estimate
itself; then decision making could take the conﬁdence into account . The
second potential solution is to “smooth” the probability estimate, replacing it with a less
extreme value. We consider only the latter in this paper.
Smoothing of probability estimates from small samples is a well-studied statistical problem , and we believe that a thorough study of what are the best methods
(and why) for PETs would be a useful contribution to machine-learning research. In this
paper we focus on the method that has become a de facto standard for practitioners: the socalled Laplace estimate or Laplace correction. Assume there are k examples of the class in
question at a leaf, N total examples, and C total classes. The maximum-likelihood estimate
presented above calculates the estimated probability as k
N . The Laplace estimate calculates
the estimated probability as
N+C . Thus, while the frequency estimate yields a probability
of 1.0 from the k = 5, N = 5 leaf, for a two-class problem the Laplace estimate yields a
probability of 5+1
5+2 = 0.86. The Laplace correction can be viewed as a form of Bayesian
estimation of the expected parameters of a multinomial distribution using a Dirichlet prior
 . It effectively incorporates a prior probability of 1
C for each
class—note that with zero examples the estimated probability of each class is 1
C . This may or
may not be desirable for a speciﬁc problem; however, practitioners have found the Laplace
correction worthwhile. To our knowledge, the Laplace correction was ﬁrst introduced in
machine learning by Niblett . Clark and Boswell incorporated it into the CN2
rule learner, and its use is now widespread. For decision-tree learning the Laplace correction1 has been used by certain researchers and practitioners , but others still use maximum-likelihood estimates.
A more complex method for producing class probability estimates from decision trees
is described by Smyth, Gray, and Fayyad . They do not concentrate on the smaller
leaves, as we have in the discussion so far. Instead they suggest a problem with estimating probabilities from the larger leaves. Speciﬁcally, they note that every example from a
F. PROVOST AND P. DOMINGOS
particular leaf will receive the same probability estimate. They question whether the coarse
granularity of these probability estimates may lead to reduced accuracy. To address this
problem, they make a fundamental change to the representation. Speciﬁcally, at each leaf of
the decision tree they place a kernel-based probability density estimator (just for the subset
of the population deﬁned by the leaf). They show that this method produces substantially
better probability estimates than standard decision-tree programs (CART and C4.5).
This approach seems well founded and quite promising, but it does not address the
question of whether there is a fundamental problem with using decision trees for probability
estimation. If in fact there is, then showing that the new method outperforms the probability
estimates of CART and C4.5 is not particularly informative. Therefore it is important to
investigate whether simple modiﬁcations can improve the probability estimates of standard
tree induction.
Finally, we should note that simply producing a probability estimate may not be enough
for a real-world application. In a recent application of data mining techniques (including
decision trees) to estimate probabilities for discovering insurance risk, Apte et al. 
describe in detail a variety of complications that also must be considered. For this paper,
all we address is the production of probability estimates in order to produce rankings.
Representation versus induction
Viewed as probability estimators, trees consist of piecewise uniform approximations within
regions deﬁned by axis-parallel boundaries. Intuitively this may not seem as appropriate as a
numeric method that estimates class probabilities as smoothly varying continuous outputs.
However, trees in principle can be ﬁne probability estimators. To see this we ﬁrst must
separate trees as a representation from the tree induction algorithm. Here we will consider
the former. In the next section we will see that problems arise with the latter.
First consider nominal attributes. The tree represents the relevant combinations of
features—relevant conditional probabilities. Any discrete conditional probability distribution can be represented by a PET.
For continuous attributes, a sufﬁciently large PET can estimate any class probability
function to arbitrary precision. Consider the simple univariate, two-class problem depicted
in ﬁgure 1: each class is distributed normally about a different mean. These overlapping
probability densities deﬁne a continuous class-membership probability function over the
domain of the variable (call it x). This may be one of the worst possible problems to which to
apply a PET, because piecewise-uniform representations are obviously a poor inductive bias,
and moreover because the problem is easy for other sorts of density estimators. However,
for this and for any such problem a PET can estimate the probability of class membership
to arbitrary precision. For this problem, each split in the tree partitions the x-axis, and each
leaf is a segment of the x-axis. A PET would estimate the probability by looking at the class
distribution for its segment (which in the ﬁgure can be seen by cutting a vertical slice and
looking at the relative heights of the curves of the two classes in the slice). The key is to
note that as the number of leaves increases, the slices become narrower, and the probability
estimates can become more and more precise. In the limit, the tree predicts class probability
perfectly.2
TREE INDUCTION FOR PROBABILITY-BASED RANKING
number cases
A test problem: Overlapping Gaussians.
Of course, learning such PETs is our ultimate interest. In the case of ﬁgure 1, other
methods would learn better using fewer examples. But when the dimensionality of the
problem is even moderately high, and little is known about the form of the underlying
distribution, a piecewise-uniform approximation may well have lower bias or variance or
both than smoother estimators.
Why PETs behave badly
So the question remains: why is it observed repeatedly that the decision trees produced by
standard algorithms do not yield good probability estimates?
The answer is in the tree-building algorithm, not in the representation. For a historical
perspective, it is useful to take a higher-level view of the research focus that (in part) drove
much work on building decision trees. Decision trees have been evaluated, for the most
part, by two criteria: classiﬁcation accuracy and tree size (smaller is better). These have
led to a wide variety of heuristics that have been remarkably successful at building small,
accurate decision trees. However, these very heuristics reduce the quality of the probability
estimates.
Why? Consider again our problem of univariate, overlapping Gaussians. What is the
smallest, accuracy-maximizing decision tree? It is the tree with a single split at x = 1. This
F. PROVOST AND P. DOMINGOS
separates the classes as well as any decision tree, and among the accuracy-maximizing trees
it has minimal size. Thus, a good decision-tree building algorithm should return this simple
tree (or a close approximation thereto). However, this tree’s class probability estimates are
not very accurate. All data points on one side of the split are assigned the same probability,
corresponding to the proportion of the class that falls on the corresponding side of the split.
Above we say that this behavior (pathological from the PET point of view) is due to
the tree-building algorithm, but we can be more speciﬁc. Modern decision-tree building
algorithms ﬁrst grow a (sometimes very) large tree, and then prune it back. The pruning
stage tries to ﬁnd a small, high-accuracy tree. Various pruning strategies are used. One such
strategy is reduced-error pruning: remove sub-trees if they seem not to improve resultant
accuracy on a validation set. In our example above, if the ﬁrst split is correct, no subtree
will improve accuracy. We believe that the details of the growing phase are less critical to
obtaining good PETs than the choice of pruning mechanism. In particular, the commonly
used splitting criteria (e.g., information gain and Gini index) also appear reasonable when
the goal is to obtain good probability-based rankings. This is reinforced by the observations of Breiman et al. and Drummond and Holte that misclassiﬁcation-cost
effectiveness generally is insensitive to the choice of splitting criteria.
Training well-behaved PETs
Our question is whether we can build trees that yield better class probability estimates. The
foregoing analysis suggests that pruning is the culprit. Looking more closely, we see that
pruning removes two types of distinctions made by the tree: (i) false distinctions—those
that were found simply because of “overﬁtting” idiosyncrasies of the training data set, and
(ii) distinctions that indeed generalize (e.g., entropy in fact is reduced), and in fact will
improve class probability estimation, but do not improve accuracy.
To build better PETs we would like not to prune away distinctions of the latter type. The
simplest strategy for keeping type-ii distinctions is simply not to prune at all. We can see
on our overlapping-Gaussians problem that this strategy indeed gives us the desired result.
In particular, we modiﬁed C4.5 by turning off pruning, turning off “collapsing” (a littleknown pruning strategy that C4.5 performs even when growing its “unpruned” tree), and
calculating class probabilities with the Laplace correction. We call this version C4.4.3
On the overlapping Gaussians problem with 100,000 training examples, C4.5 with pruning was used to build a PET (using the Laplace correction at the leaves), as was C4.4 (no
pruning, no collapsing, Laplace correction). Figure 2 shows the performance of the PETs
learned by C4.5 and C4.4. The solid line represents the true class probability boundary of
the overlapping Gaussians problem (from ﬁgure 1). The class probability estimates given by
C4.5 and C4.4 produce a piecewise-constant function, as expected. Note that C4.5 indeed
ﬁnds a high-accuracy split, but the probability estimates (the horizontal segments) do not
track the true class probability boundary well at all. C4.4’s PET tracks the class probability
boundary remarkably well.
TREE INDUCTION FOR PROBABILITY-BASED RANKING
class 0 probability
C4.4 Estimate
C4.5 Estimate
Class probability estimates for C4.5 and C4.4 on the overlapping Gaussians problem.
Of course, one may argue that the boundary still is rather rough,4 and that an estimate
with a better bias (e.g., a sigmoid function of the input) would perform better. As we mentioned earlier, the univariate, overlapping-Gaussians problem is one of the worst possible
applications for a PET, in part because it is easy to propose a better alternative. However,
consider the class probability function shown in ﬁgure 3. This will be more difﬁcult for
most methods than the problem in ﬁgure 2.
Now, consider the performance of C4.5 versus C4.4 on this problem. Note once again
that for this probability function, the optimal decision tree also is a single cut, this time
at a point in the interval (−1,0). Therefore, the following should be viewed simply as a
demonstration of the potential power of PETs over decision trees.
Once again, C4.5 with pruning was used to build a PET (using the Laplace correction
at the leaves), as was C4.4 (no pruning, no collapsing, Laplace correction) from 100,000
training examples. The class probability borders learned by C4.5 and by C4.4 are shown in
ﬁgure 4. As before, and as expected, C4.5 places a single split very near to the point where
error should be minimized. Of course, this gives poor probability estimates for almost all
instances. C4.4, on the other hand, produces class probability estimates that track the actual
class probability border quite well.
Probability-bagging
In the foregoing, we assumed that the goal was to improve the probability estimates resulting
from a single tree. A different strategy for using tree induction for probability estimation
F. PROVOST AND P. DOMINGOS
class 0 probability
Class 0 Probability
A more complex class probability function.
class 0 probability
Class 0 Probability
Learned probability borders: 100,000 training examples.
TREE INDUCTION FOR PROBABILITY-BASED RANKING
has received attention recently. Ensembles of classiﬁers, which learn multiple classiﬁcation
models and then combine their predictions (e.g., having them vote on a classiﬁcation), have
recently been shown often to improve classiﬁcation accuracy when compared to using a
single model. For example, bagging has been shown to outperform single
model techniques with surprising consistency.
Recent results suggest that the improvements from bagging also apply to the use of trees
for probability estimation and ranking, when probability estimates are averaged across the
members of the ensemble . We
should note that averaging multiple trees to produce probability estimates is not a novel
product of the recent interest in multiple models; Buntine studied the technique a decade
ago . However, experiments have led us to the conclusion that bagging and
the Bayesian averaging studied by Buntine are in fact quite different . We
include probability-bagging of PETs in our experimental comparison.
Comparing PETs
For this paper, we are interested in how well the learned models can rank new cases by
the probability of class membership. The standard comparison method in machine learning research, comparing undifferentiated error rates, is not appropriate , because it only assesses to what extent the estimated probabilities are
on the correct side of the classiﬁcation threshold (normally 0.5). One alternative is to
use full-ﬂedged ROC analysis , which compares visually a ranking’s quality across the entire range of possible classiﬁcation thresholds. As described in detail by
Provost and Fawcett , an ROC curve is generated from a ranking model as follows. The examples in the test set are ranked by the scores given by the model. If there
are S different scores, there are S + 1 thresholding ranges, each of which will produce
different classiﬁcation performance (as can be characterized by the true-positive and falsepositive rates) on these test data. Provost and Fawcett describe how for any
two-class problem, precise, objective comparisons can be made with ROC analysis for
various (and even unknown) conditions, such as different misclassiﬁcation costs, different marginal class distributions in the target environment, different target classiﬁcation
thresholds, etc.
However, for the purpose of this study, we want to evaluate the probabilities generally
rather than under speciﬁc conditions or under ranges of conditions. A subtle issue arises
when evaluating the quality of the probabilities in our setting: although the trees are estimating probabilities of class membership, neither for the training data nor for the test data
do we know the true probabilities. All we know is the true class of each example. For this
paper, our task is simpliﬁed because all we address is how well the estimated probabilities
rank cases (by the likelihood of class membership).
Knowing nothing about the task for which they will be used, which probabilities are generally better for ranking cases? The Wilcoxon-Mann-Whitney non-parametric test statistic
(“the Wilcoxon”) is appropriate for this comparison . The Wilcoxon measures, for a particular model, the probability that a randomly chosen class-0 case will
be assigned a higher class-0 probability than a randomly chosen class-1 case. Therefore
F. PROVOST AND P. DOMINGOS
a higher Wilcoxon score indicates that the probability-based ranking is better generally
(there may be speciﬁc conditions under which the classiﬁer with a lower Wilcoxon score
is preferable). Importantly, for the purposes of this paper the calibration of the probabilities is not important, if the estimates rank well.5 Another metric for comparing classi-
ﬁers across a wide range of conditions is the area under the ROC curve (AUC) ; AUC also measures the quality of an estimator’s ranking performance. Interestingly, it has been shown that the AUC is equivalent to the Wilcoxon statistic . .)
Therefore, for this work we will report the AUC when comparing probability-based rankings. provides a thorough treatment of the comparison of class probability estimates both when the true probability distribution is known and when it is
We examine whether, by making the modiﬁcations we make, the probability-based rankings generally improve. We make no claims as to whether one algorithm is “better” than
another for the particular problems from which these data were drawn. The AUC measures
judge the relative quality of the entire rankings.
To our knowledge, there previously has not been a systematic study comparing the
performance of these PET variants for producing probability-based rankings. There do
exist two closely related studies, that partially motivate the current study.
Bauer and Kohavi compare across 14 UCI data sets the quality of the probability
estimates produced by PETs based on MC4 (their implementation of C4.5), a Laplacecorrected version of MC4 (using the m-estimate Laplace correction), and probabilitybagging of MC4. They compare a mean-squared error measure of the quality of the
probability estimates, computed as the square of one6 minus the predicted probability
of the correct class, averaged over the entire test set (we will call this measure 0/1-
MSE). For these experiments they only report averages across the data sets, but their
results are positive. They show a decrease in the average 0/1-MSE from 10.7% for unpruned C4.5 to 10.0% for Laplace-corrected unpruned C4.5 to 7.5% for probability-bagged
Provost, Fawcett, and Kohavi compared the rankings of some of these PET variants. Speciﬁcally, they present the ROC curves of six algorithms evaluated on ten data
sets, including Laplace-corrected PETs and probability-bagged PETs. They do not discuss
which algorithms are better (this was not the purpose of the paper), but one can observe
in their graphs that the ROC curves for probability-bagged PETs have larger areas that the
curves for the PETs. In fact, in all but one case, the probability-bagged PETs completely
dominate the curves of individual PETs. Our results, below, clarify and extend these results
by examining the differences carefully, and by extending the study to a large number of
data sets and to multiple-class problems.
Note that an improvement in 0/1-MSE does not necessarily indicate better probabilitybased rankings. In fact, a perfect ranking can have a worse 0/1-MSE than a ranking with
an error in the ﬁrst position. This is not the case for AUC. Also, we would like to see how
often these techniques lead to improvements. Therefore, we will look individually at a larger
number of domains.
TREE INDUCTION FOR PROBABILITY-BASED RANKING
Experiments and results
Methodology and results
We used the following 25 databases from the UCI repository : audiology, breast cancer (Ljubljana), chess (king-rook vs. king-pawn), credit (Australian), diabetes, echocardiogram, glass, heart disease (Cleveland), hepatitis, hypothyroid, iris, LED,
liver disorders, lung cancer, lymphography, mushroom, primary tumor, promoters, solar
ﬂare, sonar, soybean (small), splice junctions, voting records, wine, and zoology. Each
database was randomly divided 20 times into 2/3 of the examples for training and 1/3 for
testing. The results presented are averages of these 20 runs. For data sets with more than
two classes we computed the expected AUC, which is the weighted average of the AUCs
obtained taking each class as the reference class in turn (i.e., making it class 0 and all other
classes class 1).7 The weight of a class’s AUC is the class’s frequency in the data. The results
obtained are shown in Table 1, and summarized in Table 2. “Sign test” is the signiﬁcance
level of a binomial sign test on the number of wins (with a tie counting as half a win; the
normal approximation to the binomial was used). “Wilcoxon test” is the signiﬁcance level
of a Wilcoxon signed-ranks test. Our observations are summarized below.
Laplace correction and pruning
C4.4 is a marked improvement over C4.5. Most of this improvement is due to the use of the
Laplace correction, which, despite its simplicity, is quite effective in improving the quality
of a tree’s probability estimates. Our results in this respect agree with, but are stronger than,
the results of Bauer and Kohavi . The uniformity of success of the simple Laplace
correction (e.g., 21 wins, 2 ties and 2 losses vs. C4.5) is remarkable.
Not pruning (C4.4) outperforms pruning (C4.5L) in more databases than the reverse, but
the difference is not signiﬁcant. We hypothesize that these inconclusive results are due to
two competing effects: when pruning is disabled, more leaves are produced, which leads to
a ﬁner approximation to the true class probability function, but there are fewer data points
within each leaf, which increases the variance in the approximation. Which of these two
effects will prevail may depend on the size of the database. The limited range of data-set
sizes used in the experiments and the presence of many confounding factors preclude ﬁnding
a clear pattern in our results. We hypothesize that as we move to larger and larger data sets,
as seems to be the trend in data mining, the advantage of C4.4 will become stronger.
Probability-bagging
Bagging also substantially improves the quality of probability estimates in almost all domains, and the improvements are often very large. This also agrees with the results of Bauer
and Kohavi using 0/1-MSE . The present results also show, over the
twenty-ﬁve data sets, not a single case where bagging degrades the probability estimates,
as measured by AUC. This accords with results that can be inferred from the ROC curves
shown by Provost, Fawcett, and Kohavi (as described above).
F. PROVOST AND P. DOMINGOS
Experimental results: Expected AUC (area under the ROC curve, as percentage of maximum possible)
and its standard deviation for C4.5, C4.5 with the Laplace correction (C4.5-L), C4.4, probability bagged C4.5
(C4.5-B) and bagged C4.4 (C4.4-B).
89.4 ± 0.8
91.1 ± 0.9
91.0 ± 0.8
94.7 ± 0.5
95.2 ± 0.6
60.9 ± 1.7
63.1 ± 1.4
60.6 ± 1.2
68.9 ± 1.3
67.4 ± 1.3
99.7 ± 0.1
99.7 ± 0.0
99.9 ± 0.0
99.9 ± 0.0
99.9 ± 0.0
87.9 ± 0.7
89.9 ± 0.5
87.3 ± 0.4
92.6 ± 0.5
92.1 ± 0.4
74.8 ± 0.9
76.9 ± 0.8
77.3 ± 0.7
83.4 ± 0.5
83.2 ± 0.5
Echocardio
54.1 ± 1.3
55.9 ± 1.6
57.7 ± 1.1
67.4 ± 1.5
67.8 ± 1.6
79.2 ± 0.9
81.3 ± 1.0
81.3 ± 0.8
88.9 ± 0.8
88.7 ± 0.8
76.0 ± 1.2
81.1 ± 1.1
83.6 ± 0.8
88.4 ± 0.6
89.1 ± 0.6
64.3 ± 2.5
68.4 ± 2.2
76.7 ± 1.5
83.2 ± 1.4
84.0 ± 1.4
96.0 ± 0.6
96.9 ± 0.3
97.3 ± 0.4
99.0 ± 0.2
99.2 ± 0.2
81.4 ± 0.9
81.9 ± 1.0
84.3 ± 1.0
90.6 ± 0.8
90.6 ± 0.9
62.6 ± 1.2
63.7 ± 1.1
64.8 ± 1.5
74.0 ± 0.7
73.9 ± 0.7
54.6 ± 3.6
51.1 ± 3.5
50.5 ± 3.3
65.3 ± 3.0
62.0 ± 3.4
79.7 ± 1.4
83.0 ± 1.5
84.7 ± 0.8
91.2 ± 0.8
91.3 ± 0.8
100.0 ± 0.0
100.0 ± 0.0
100.0 ± 0.0
100.0 ± 0.0
100.0 ± 0.0
78.4 ± 1.6
82.9 ± 1.5
81.2 ± 1.5
93.0 ± 1.2
93.8 ± 1.0
87.5 ± 0.6
88.9 ± 0.5
88.6 ± 0.5
89.8 ± 0.5
89.7 ± 0.5
70.5 ± 1.3
76.2 ± 1.4
76.5 ± 1.4
85.2 ± 1.4
84.5 ± 1.3
98.2 ± 0.5
97.8 ± 0.7
97.8 ± 0.7
100.0 ± 0.0
100.0 ± 0.0
96.4 ± 0.2
97.7 ± 0.1
97.8 ± 0.1
98.7 ± 0.1
98.9 ± 0.1
94.4 ± 0.9
96.2 ± 0.5
97.0 ± 0.4
97.5 ± 0.4
98.6 ± 0.3
68.8 ± 0.7
71.7 ± 0.7
68.5 ± 0.8
77.0 ± 0.7
76.0 ± 0.6
97.1 ± 0.4
98.2 ± 0.2
94.6 ± 0.7
98.6 ± 0.2
98.9 ± 0.1
94.3 ± 0.6
94.5 ± 0.7
94.4 ± 0.8
99.4 ± 0.1
99.4 ± 0.1
96.4 ± 0.5
98.0 ± 0.4
98.4 ± 0.4
99.4 ± 0.3
99.6 ± 0.1
Summary of experimental results: AUC comparisons.
Avg. diff. (%)
Wilcoxon test
C4.4 vs. C4.5
C4.4 vs. C4.5-L
C4.5-L vs. C4.5
C4.5-B vs. C4.5
C4.4-B vs. C4.4
C4.4-B vs. C4.5-B
TREE INDUCTION FOR PROBABILITY-BASED RANKING
It is noteworthy that bagging’s improvements in AUC are on average much larger than its
improvements in accuracy (7.3% vs. 2.8% for C4.5), indicating that bagging may be even
more effective for improving probability estimators than for improving classiﬁers. The
improvements in AUC are larger on average for C4.5 than for C4.4, presumably because
there is more room for improvement in C4.5. Once bagging is used, whether or not pruning
andtheLaplacecorrectionareusedmakeslittledifference.Despiteitseffectiveness,bagging
has the disadvantage that any comprehensibilityof the single tree is lost. However, individual
PETs can be very large, especially when pruning is not used, so they themselves may or may
not be comprehensible. Bagging also carries greater computational cost. When high-quality
estimation is the sole concern, bagging should clearly be used. When comprehensibility
and/or computational cost are also important, a single C4.4 tree may be preferable, or a
method like CMM (which produces a single-tree approximation of the
ensemble) may be useful.
Conclusions and discussion
The poor performance of PETs built by conventional decision-tree learning programs
can be explained by a combination of factors. First, as shown by the demonstrations
on synthetic data, the heuristics used to build small accurate decision trees are biased
strongly against building accurate PETs. Larger trees can work better for probability
estimation.
The second factor explaining the poor performance of conventional PETs is that, when a
purely frequency-based (unsmoothed) estimate is used, small leaves give poor probability
estimates. This is the probability-estimation counterpart of the well-known “small disjuncts
problem”: in induced disjunctive class descriptions, small disjuncts are more error-prone
 . While this is not surprising statistically, the uniformity
and magnitude of the improvement given by the simple, easy-to-use, Laplace correction
nevertheless is remarkable.
A third factor, which we have not investigated, is the calibration of the probability estimates. Recently, Margineantu and Dietterich have investigated the issue of the
accuracy of the estimates versus the accuracy of the rankings, and show that PETs indeed
produce surprisingly good rankings, even when the probability estimates themselves are
questionable.
Another signiﬁcant observation is that probability-bagged PETs produce excellent
probability-based rankings. As with accuracy, bagging substantially improves PETs. Moreover, over the twenty-ﬁve data sets we tested, bagging never degrades the probability estimates. Furthermore, bagging improves probability estimates (as measured by AUC) even
more than it improves classiﬁcation accuracy. The extent of this is quite remarkable: in 9 of
25 domains bagging gives an absolute AUC improvement of more than 0.10. We strongly
echo the conclusion of Bauer and Kohavi that for problems where probability estimation is required, one should seriously consider using probability-bagged PETs—especially
in ill-deﬁned or high-dimensional domains.
Bagged PETs also have implications for other areas of data mining and machine learning
research. For example, the MetaCost algorithm uses a bagged PET as a
F. PROVOST AND P. DOMINGOS
subprocedure for cost-sensitive learning. The quality of the probability estimates obtained
in this way was an open question; our results partially validate the procedure used.
The purpose of this work was to study how the probability-based rankings obtained by tree
induction could be improved. We believe that the results we have presented have given us a
substantially better understanding. However, what we did not study here is how these PETs
compare with other methods for estimating probabilities. In a working version of this paper
 we hypothesized that as long as there are many examples,
PETs can compete with traditional methods for building class probability estimators. Recent
work shows that indeed this is the case. Perlich, Provost, and Simonoff show that for
large data sets, tree induction often produces probability-based rankings that are superior
to those produced by logistic regression (a standard statistical method for estimating classmembership probability). They also characterize the type of domain for which each method
is preferable. A direction for future work is to study the incorporation of more sophisticated
methods for improving probability estimates ).
Acknowledgments
Ronny Kohavi suggested the use of probability bagging for our 1998 study with Tom
Fawcett. Doug Fisher and our anonymous reviewers made suggestions that improved the
paper considerably. We thank Claudia Perlich, Maytal Saar-Tsechansky, and Jeff Simonoff
for enlightening discussions about probability estimation and ranking, all those who have
pointed us to related work, and the contributors to and the librarians of the UCI repository for
facilitating experimental research in machine learning. This work was partly supported by
IBM Faculty Awards to both authors, and by an NSF CAREER Award to the second author.
1. Including a generalization known as the m-estimate .
2. A similar result for regression trees has been formally demonstrated by Gordon and Olshen .
3. Note that Bradford et al. show that cost-sensitive tree pruning is no better than simply not pruning at
all, as long as the Laplace correction is used.
4. Note that C4.5 uses a minimum description length heuristic to reduce spurious splitting on numeric attributes,
and because of this the leaves remain larger than they would without the heuristic.
5. An inherently good probability estimator can be skewed systematically, so that although the probabilities
are not accurate, they still rank cases equivalently. This would be the case, for example, if the probabilities
were squared. Such an estimator will receive a high Wilcoxon score. A higher Wilcoxon score indicates that,
with proper recalibration, the probabilities of the estimator will be better. Probabilities can be recalibrated
empirically . In addition to describing new
calibration methods, Bennett provides an in-depth discussion of calibration, including additional related work.
6. Recall that for these data we only know the true class of each example, not the true probability of class
membership for the example’s description.
7. This is a minor variant of the method proposed recently by Hand and Till .
TREE INDUCTION FOR PROBABILITY-BASED RANKING