Aggregated Residual Transformations for Deep Neural Networks
Saining Xie1
Ross Girshick2
Piotr Doll´ar2
Zhuowen Tu1
Kaiming He2
1UC San Diego
2Facebook AI Research
{s9xie,ztu}@ucsd.edu
{rbg,pdollar,kaiminghe}@fb.com
We present a simple, highly modularized network architecture for image classiﬁcation. Our network is constructed
by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has
only a few hyper-parameters to set. This strategy exposes a
new dimension, which we call “cardinality” (the size of the
set of transformations), as an essential factor in addition to
the dimensions of depth and width. On the ImageNet-1K
dataset, we empirically show that even under the restricted
condition of maintaining complexity, increasing cardinality
is able to improve classiﬁcation accuracy. Moreover, increasing cardinality is more effective than going deeper or
wider when we increase the capacity. Our models, named
ResNeXt, are the foundations of our entry to the ILSVRC
2016 classiﬁcation task in which we secured 2nd place.
We further investigate ResNeXt on an ImageNet-5K set and
the COCO detection set, also showing better results than
its ResNet counterpart. The code and models are publicly
available online1.
1. Introduction
Research on visual recognition is undergoing a transition from “feature engineering” to “network engineering”
 . In contrast to traditional handdesigned features (e.g., SIFT and HOG ), features
learned by neural networks from large-scale data require minimal human involvement during training, and can
be transferred to a variety of recognition tasks .
Nevertheless, human effort has been shifted to designing
better network architectures for learning representations.
Designing architectures becomes increasingly difﬁcult
with the growing number of hyper-parameters (width2, ﬁlter sizes, strides, etc.), especially when there are many layers. The VGG-nets exhibit a simple yet effective strategy of constructing very deep networks: stacking build-
1 
2Width refers to the number of channels in a layer.
256, 1x1, 4
4, 1x1, 256
256, 1x1, 4
4, 1x1, 256
256, 1x1, 4
4, 1x1, 256
256, 1x1, 64
64, 3x3, 64
64, 1x1, 256
Figure 1. Left: A block of ResNet .
Right: A block of
ResNeXt with cardinality = 32, with roughly the same complexity. A layer is shown as (# in channels, ﬁlter size, # out channels).
ing blocks of the same shape. This strategy is inherited
by ResNets which stack modules of the same topology. This simple rule reduces the free choices of hyperparameters, and depth is exposed as an essential dimension
in neural networks. Moreover, we argue that the simplicity
of this rule may reduce the risk of over-adapting the hyperparameters to a speciﬁc dataset. The robustness of VGGnets and ResNets has been proven by various visual recognition tasks and by non-visual tasks
involving speech and language .
Unlike VGG-nets, the family of Inception models have demonstrated that carefully designed
topologies are able to achieve compelling accuracy with low
theoretical complexity. The Inception models have evolved
over time , but an important common property is
a split-transform-merge strategy. In an Inception module,
the input is split into a few lower-dimensional embeddings
(by 1×1 convolutions), transformed by a set of specialized
ﬁlters (3×3, 5×5, etc.), and merged by concatenation. It
can be shown that the solution space of this architecture is a
strict subspace of the solution space of a single large layer
(e.g., 5×5) operating on a high-dimensional embedding.
The split-transform-merge behavior of Inception modules
is expected to approach the representational power of large
and dense layers, but at a considerably lower computational
complexity.
Despite good accuracy, the realization of Inception models has been accompanied with a series of complicating fac-
 
tors — the ﬁlter numbers and sizes are tailored for each
individual transformation, and the modules are customized
stage-by-stage.
Although careful combinations of these
components yield excellent neural network recipes, it is in
general unclear how to adapt the Inception architectures to
new datasets/tasks, especially when there are many factors
and hyper-parameters to be designed.
In this paper, we present a simple architecture which
adopts VGG/ResNets’ strategy of repeating layers, while
exploiting the split-transform-merge strategy in an easy, extensible way.
A module in our network performs a set
of transformations, each on a low-dimensional embedding,
whose outputs are aggregated by summation. We pursuit a
simple realization of this idea — the transformations to be
aggregated are all of the same topology (e.g., Fig. 1 (right)).
This design allows us to extend to any large number of
transformations without specialized designs.
Interestingly, under this simpliﬁed situation we show that
our model has two other equivalent forms (Fig. 3). The reformulation in Fig. 3(b) appears similar to the Inception-
ResNet module in that it concatenates multiple paths;
but our module differs from all existing Inception modules
in that all our paths share the same topology and thus the
number of paths can be easily isolated as a factor to be investigated. In a more succinct reformulation, our module
can be reshaped by Krizhevsky et al.’s grouped convolutions (Fig. 3(c)), which, however, had been developed
as an engineering compromise.
We empirically demonstrate that our aggregated transformations outperform the original ResNet module, even
under the restricted condition of maintaining computational
complexity and model size — e.g., Fig. 1(right) is designed
to keep the FLOPs complexity and number of parameters of
Fig. 1(left). We emphasize that while it is relatively easy to
increase accuracy by increasing capacity (going deeper or
wider), methods that increase accuracy while maintaining
(or reducing) complexity are rare in the literature.
Our method indicates that cardinality (the size of the
set of transformations) is a concrete, measurable dimension that is of central importance, in addition to the dimensions of width and depth. Experiments demonstrate that increasing cardinality is a more effective way of gaining accuracy than going deeper or wider, especially when depth and
width starts to give diminishing returns for existing models.
Our neural networks, named ResNeXt (suggesting the
next dimension), outperform ResNet-101/152 , ResNet-
200 , Inception-v3 , and Inception-ResNet-v2 
on the ImageNet classiﬁcation dataset.
In particular, a
101-layer ResNeXt is able to achieve better accuracy than
ResNet-200 but has only 50% complexity. Moreover,
ResNeXt exhibits considerably simpler designs than all Inception models. ResNeXt was the foundation of our submission to the ILSVRC 2016 classiﬁcation task, in which
we secured second place.
This paper further evaluates
ResNeXt on a larger ImageNet-5K set and the COCO object
detection dataset , showing consistently better accuracy
than its ResNet counterparts. We expect that ResNeXt will
also generalize well to other visual (and non-visual) recognition tasks.
2. Related Work
Multi-branch convolutional networks.
The Inception
models are successful multi-branch architectures where each branch is carefully customized.
ResNets can be thought of as two-branch networks
where one branch is the identity mapping. Deep neural decision forests are tree-patterned multi-branch networks
with learned splitting functions.
Grouped convolutions. The use of grouped convolutions
dates back to the AlexNet paper , if not earlier. The
motivation given by Krizhevsky et al. is for distributing
the model over two GPUs. Grouped convolutions are supported by Caffe , Torch , and other libraries, mainly
for compatibility of AlexNet. To the best of our knowledge,
there has been little evidence on exploiting grouped convolutions to improve accuracy. A special case of grouped convolutions is channel-wise convolutions in which the number
of groups is equal to the number of channels. Channel-wise
convolutions are part of the separable convolutions in .
Compressing convolutional networks. Decomposition (at
spatial and/or channel level) is a widely
adopted technique to reduce redundancy of deep convolutional networks and accelerate/compress them.
Ioannou et al. present a “root”-patterned network for reducing computation, and branches in the root are realized
by grouped convolutions. These methods 
have shown elegant compromise of accuracy with lower
complexity and smaller model sizes. Instead of compression, our method is an architecture that empirically shows
stronger representational power.
Ensembling. Averaging a set of independently trained networks is an effective solution to improving accuracy ,
widely adopted in recognition competitions . Veit et al.
 interpret a single ResNet as an ensemble of shallower
networks, which results from ResNet’s additive behaviors
 . Our method harnesses additions to aggregate a set of
transformations. But we argue that it is imprecise to view
our method as ensembling, because the members to be aggregated are trained jointly, not independently.
3.1. Template
We adopt a highly modularized design following
VGG/ResNets. Our network consists of a stack of residstage
ResNeXt-50 (32×4d)
conv1 112×112
7×7, 64, stride 2
7×7, 64, stride 2
3×3 max pool, stride 2
3×3 max pool, stride 2
3×3, 128, C=32
3×3, 256, C=32
3×3, 512, C=32
3×3, 1024, C=32
global average pool
global average pool
1000-d fc, softmax
1000-d fc, softmax
Table 1. (Left) ResNet-50. (Right) ResNeXt-50 with a 32×4d
template (using the reformulation in Fig. 3(c)). Inside the brackets
are the shape of a residual block, and outside the brackets is the
number of stacked blocks on a stage. “C=32” suggests grouped
convolutions with 32 groups. The numbers of parameters and
FLOPs are similar between these two models.
ual blocks. These blocks have the same topology, and are
subject to two simple rules inspired by VGG/ResNets: (i)
if producing spatial maps of the same size, the blocks share
the same hyper-parameters (width and ﬁlter sizes), and (ii)
each time when the spatial map is downsampled by a factor of 2, the width of the blocks is multiplied by a factor
of 2. The second rule ensures that the computational complexity, in terms of FLOPs (ﬂoating-point operations, in #
of multiply-adds), is roughly the same for all blocks.
With these two rules, we only need to design a template
module, and all modules in a network can be determined
accordingly. So these two rules greatly narrow down the
design space and allow us to focus on a few key factors.
The networks constructed by these rules are in Table 1.
3.2. Revisiting Simple Neurons
The simplest neurons in artiﬁcial neural networks perform inner product (weighted sum), which is the elementary transformation done by fully-connected and convolutional layers. Inner product can be thought of as a form of
aggregating transformation:
where x = [x1, x2, ..., xD] is a D-channel input vector to
the neuron and wi is a ﬁlter’s weight for the i-th chan-
Figure 2. A simple neuron that performs inner product.
nel. This operation (usually including some output nonlinearity) is referred to as a “neuron”. See Fig. 2.
The above operation can be recast as a combination of
splitting, transforming, and aggregating. (i) Splitting: the
vector x is sliced as a low-dimensional embedding, and
in the above, it is a single-dimension subspace xi.
Transforming: the low-dimensional representation is transformed, and in the above, it is simply scaled: wixi. (iii)
Aggregating: the transformations in all embeddings are aggregated by PD
3.3. Aggregated Transformations
Given the above analysis of a simple neuron, we consider replacing the elementary transformation (wixi) with
a more generic function, which in itself can also be a network. In contrast to “Network-in-Network” that turns
out to increase the dimension of depth, we show that our
“Network-in-Neuron” expands along a new dimension.
Formally, we present aggregated transformations as:
where Ti(x) can be an arbitrary function. Analogous to a
simple neuron, Ti should project x into an (optionally lowdimensional) embedding and then transform it.
In Eqn.(2), C is the size of the set of transformations
to be aggregated.
We refer to C as cardinality .
Eqn.(2) C is in a position similar to D in Eqn.(1), but C
need not equal D and can be an arbitrary number. While
the dimension of width is related to the number of simple
transformations (inner product), we argue that the dimension of cardinality controls the number of more complex
transformations. We show by experiments that cardinality
is an essential dimension and can be more effective than the
dimensions of width and depth.
In this paper, we consider a simple way of designing the
transformation functions: all Ti’s have the same topology.
This extends the VGG-style strategy of repeating layers of
the same shape, which is helpful for isolating a few factors
and extending to any large number of transformations. We
set the individual transformation Ti to be the bottleneckshaped architecture , as illustrated in Fig. 1 (right). In
this case, the ﬁrst 1×1 layer in each Ti produces the lowdimensional embedding.
equivalent
256, 1x1, 4
4, 1x1, 256
256, 1x1, 4
4, 1x1, 256
256, 1x1, 4
4, 1x1, 256
256, 1x1, 4
256, 1x1, 4
256, 1x1, 4
concatenate
128, 1x1, 256
256, 1x1, 128
128, 3x3, 128
group = 32
128, 1x1, 256
Figure 3. Equivalent building blocks of ResNeXt. (a): Aggregated residual transformations, the same as Fig. 1 right. (b): A block equivalent
to (a), implemented as early concatenation. (c): A block equivalent to (a,b), implemented as grouped convolutions . Notations in bold
text highlight the reformulation changes. A layer is denoted as (# input channels, ﬁlter size, # output channels).
The aggregated transformation in Eqn.(2) serves as the
residual function (Fig. 1 right):
where y is the output.
Relation to Inception-ResNet.
Some tensor manipulations show that the module in Fig. 1(right) (also shown in
Fig. 3(a)) is equivalent to Fig. 3(b).3 Fig. 3(b) appears similar to the Inception-ResNet block in that it involves
branching and concatenating in the residual function. But
unlike all Inception or Inception-ResNet modules, we share
the same topology among the multiple paths. Our module
requires minimal extra effort designing each path.
Relation to Grouped Convolutions. The above module becomes more succinct using the notation of grouped convolutions .4 This reformulation is illustrated in Fig. 3(c).
All the low-dimensional embeddings (the ﬁrst 1×1 layers)
can be replaced by a single, wider layer (e.g., 1×1, 128-d
in Fig 3(c)). Splitting is essentially done by the grouped
convolutional layer when it divides its input channels into
groups. The grouped convolutional layer in Fig. 3(c) performs 32 groups of convolutions whose input and output
channels are 4-dimensional.
The grouped convolutional
layer concatenates them as the outputs of the layer. The
block in Fig. 3(c) looks like the original bottleneck residual block in Fig. 1(left), except that Fig. 3(c) is a wider but
sparsely connected module.
3An informal but descriptive proof is as follows. Note the equality:
A1B1 + A2B2 = [A1, A2][B1; B2] where [ , ] is horizontal concatenation and [ ; ] is vertical concatenation. Let Ai be the weight of the last layer
and Bi be the output response of the second-last layer in the block. In the
case of C = 2, the element-wise addition in Fig. 3(a) is A1B1 + A2B2,
the weight of the last layer in Fig. 3(b) is [A1, A2], and the concatenation
of outputs of second-last layers in Fig. 3(b) is [B1; B2].
4In a group conv layer , input and output channels are divided into
C groups, and convolutions are separately performed within each group.
equivalent
64, 3x3, 4
64, 3x3, 4
64, 3x3, 4
4, 3x3, 64
4, 3x3, 64
4, 3x3, 64
64, 3x3, 128
128, 3x3, 64
Figure 4. (Left): Aggregating transformations of depth = 2.
(Right): An equivalent block, which is trivially wider.
We note that the reformulations produce nontrivial
topologies only when the block has depth ≥3. If the block
has depth = 2 (e.g., the basic block in ), the reformulations lead to trivially a wide, dense module. See the illustration in Fig. 4.
Discussion. We note that although we present reformulations that exhibit concatenation (Fig. 3(b)) or grouped convolutions (Fig. 3(c)), such reformulations are not always applicable for the general form of Eqn.(3), e.g., if the transformation Ti takes arbitrary forms and are heterogenous.
We choose to use homogenous forms in this paper because
they are simpler and extensible. Under this simpliﬁed case,
grouped convolutions in the form of Fig. 3(c) are helpful for
easing implementation.
3.4. Model Capacity
Our experiments in the next section will show that
our models improve accuracy when maintaining the model
complexity and number of parameters. This is not only interesting in practice, but more importantly, the complexity
and number of parameters represent inherent capacity of
models and thus are often investigated as fundamental properties of deep networks .
When we evaluate different cardinalities C while preserving complexity, we want to minimize the modiﬁcation
of other hyper-parameters. We choose to adjust the width of
cardinality C
width of bottleneck d
width of group conv.
Table 2. Relations between cardinality and width (for the template
of conv2), with roughly preserved complexity on a residual block.
The number of parameters is ∼70k for the template of conv2. The
number of FLOPs is ∼0.22 billion (# params×56×56 for conv2).
the bottleneck (e.g., 4-d in Fig 1(right)), because it can be
isolated from the input and output of the block. This strategy introduces no change to other hyper-parameters (depth
or input/output width of blocks), so is helpful for us to focus
on the impact of cardinality.
In Fig. 1(left), the original ResNet bottleneck block 
has 256 · 64 + 3 · 3 · 64 · 64 + 64 · 256 ≈70k parameters and
proportional FLOPs (on the same feature map size). With
bottleneck width d, our template in Fig. 1(right) has:
C · (256 · d + 3 · 3 · d · d + d · 256)
parameters and proportional FLOPs. When C = 32 and
d = 4, Eqn.(4) ≈70k. Table 2 shows the relationship between cardinality C and bottleneck width d.
Because we adopt the two rules in Sec. 3.1, the above
approximate equality is valid between a ResNet bottleneck
block and our ResNeXt on all stages (except for the subsampling layers where the feature maps size changes). Table 1 compares the original ResNet-50 and our ResNeXt-50
that is of similar capacity.5 We note that the complexity can
only be preserved approximately, but the difference of the
complexity is minor and does not bias our results.
4. Implementation details
Our implementation follows and the publicly available code of fb.resnet.torch . On the ImageNet
dataset, the input image is 224×224 randomly cropped
from a resized image using the scale and aspect ratio augmentation of implemented by . The shortcuts are
identity connections except for those increasing dimensions
which are projections (type B in ). Downsampling of
conv3, 4, and 5 is done by stride-2 convolutions in the 3×3
layer of the ﬁrst block in each stage, as suggested in .
We use SGD with a mini-batch size of 256 on 8 GPUs (32
per GPU). The weight decay is 0.0001 and the momentum
is 0.9. We start from a learning rate of 0.1, and divide it by
10 for three times using the schedule in . We adopt the
weight initialization of . In all ablation comparisons, we
evaluate the error on the single 224×224 center crop from
an image whose shorter side is 256.
Our models are realized by the form of Fig. 3(c). We
perform batch normalization (BN) right after the con-
5The marginally smaller number of parameters and marginally higher
FLOPs are mainly caused by the blocks where the map sizes change.
volutions in Fig. 3(c).6 ReLU is performed right after each
BN, expect for the output of the block where ReLU is performed after the adding to the shortcut, following .
We note that the three forms in Fig. 3 are strictly equivalent, when BN and ReLU are appropriately addressed as
mentioned above.
We have trained all three forms and
obtained the same results.
We choose to implement by
Fig. 3(c) because it is more succinct and faster than the other
two forms.
5. Experiments
5.1. Experiments on ImageNet-1K
We conduct ablation experiments on the 1000-class ImageNet classiﬁcation task . We follow to construct
50-layer and 101-layer residual networks. We simply replace all blocks in ResNet-50/101 with our blocks.
Notations. Because we adopt the two rules in Sec. 3.1, it is
sufﬁcient for us to refer to an architecture by the template.
For example, Table 1 shows a ResNeXt-50 constructed by a
template with cardinality = 32 and bottleneck width = 4d
(Fig. 3). This network is denoted as ResNeXt-50 (32×4d)
for simplicity. We note that the input/output width of the
template is ﬁxed as 256-d (Fig. 3), and all widths are doubled each time when the feature map is subsampled (see
Cardinality vs. Width. We ﬁrst evaluate the trade-off between cardinality C and bottleneck width, under preserved
complexity as listed in Table 2. Table 3 shows the results
and Fig. 5 shows the curves of error vs. epochs. Comparing with ResNet-50 (Table 3 top and Fig. 5 left), the 32×4d
ResNeXt-50 has a validation error of 22.2%, which is 1.7%
lower than the ResNet baseline’s 23.9%. With cardinality C
increasing from 1 to 32 while keeping complexity, the error
rate keeps reducing. Furthermore, the 32×4d ResNeXt also
has a much lower training error than the ResNet counterpart, suggesting that the gains are not from regularization
but from stronger representations.
Similar trends are observed in the case of ResNet-101
(Fig. 5 right, Table 3 bottom), where the 32×4d ResNeXt-
101 outperforms the ResNet-101 counterpart by 0.8%. Although this improvement of validation error is smaller than
that of the 50-layer case, the improvement of training error is still big (20% for ResNet-101 and 16% for 32×4d
ResNeXt-101, Fig. 5 right).
In fact, more training data
will enlarge the gap of validation error, as we show on an
ImageNet-5K set in the next subsection.
Table 3 also suggests that with complexity preserved, increasing cardinality at the price of reducing width starts
to show saturating accuracy when the bottleneck width is
6With BN, for the equivalent form in Fig. 3(a), BN is employed after
aggregating the transformations and before adding to the shortcut.
top-1 error (%)
ResNet-50 (1 x 64d) train
ResNet-50 (1 x 64d) val
ResNeXt-50 (32 x 4d) train
ResNeXt-50 (32 x 4d) val
top-1 error (%)
ResNet-101 (1 x 64d) train
ResNet-101 (1 x 64d) val
ResNeXt-101 (32 x 4d) train
ResNeXt-101 (32 x 4d) val
Figure 5. Training curves on ImageNet-1K. (Left): ResNet/ResNeXt-50 with preserved complexity (∼4.1 billion FLOPs, ∼25 million
parameters); (Right): ResNet/ResNeXt-101 with preserved complexity (∼7.8 billion FLOPs, ∼44 million parameters).
top-1 error (%)
ResNeXt-50
ResNeXt-50
ResNeXt-50
ResNeXt-50
ResNet-101
ResNeXt-101
ResNeXt-101
ResNeXt-101
ResNeXt-101
Table 3. Ablation experiments on ImageNet-1K. (Top): ResNet-
50 with preserved complexity (∼4.1 billion FLOPs); (Bottom):
ResNet-101 with preserved complexity (∼7.8 billion FLOPs). The
error rate is evaluated on the single crop of 224×224 pixels.
small. We argue that it is not worthwhile to keep reducing
width in such a trade-off. So we adopt a bottleneck width
no smaller than 4d in the following.
Increasing Cardinality vs. Deeper/Wider. Next we investigate increasing complexity by increasing cardinality C
or increasing depth or width. The following comparison
can also be viewed as with reference to 2× FLOPs of the
ResNet-101 baseline. We compare the following variants
that have ∼15 billion FLOPs. (i) Going deeper to 200 layers. We adopt the ResNet-200 implemented in .
(ii) Going wider by increasing the bottleneck width. (iii)
Increasing cardinality by doubling C.
Table 4 shows that increasing complexity by 2× consistently reduces error vs. the ResNet-101 baseline (22.0%).
But the improvement is small when going deeper (ResNet-
200, by 0.3%) or wider (wider ResNet-101, by 0.7%).
On the contrary, increasing cardinality C shows much
top-1 err (%)
top-5 err (%)
1× complexity references:
ResNet-101
ResNeXt-101
2× complexity models follow:
ResNet-200 
ResNet-101, wider
ResNeXt-101
ResNeXt-101
Table 4. Comparisons on ImageNet-1K when the number of
FLOPs is increased to 2× of ResNet-101’s. The error rate is evaluated on the single crop of 224×224 pixels. The highlighted factors
are the factors that increase complexity.
better results than going deeper or wider.
ResNeXt-101 (i.e., doubling C on 1×64d ResNet-101 baseline and keeping the width) reduces the top-1 error by 1.3%
to 20.7%. The 64×4d ResNeXt-101 (i.e., doubling C on
32×4d ResNeXt-101 and keeping the width) reduces the
top-1 error to 20.4%.
We also note that 32×4d ResNet-101 (21.2%) performs
better than the deeper ResNet-200 and the wider ResNet-
101, even though it has only ∼50% complexity. This again
shows that cardinality is a more effective dimension than
the dimensions of depth and width.
Residual connections. The following table shows the effects of the residual (shortcut) connections:
w/ residual
w/o residual
ResNeXt-50
Removing shortcuts from the ResNeXt-50 increases the error by 3.9 points to 26.1%. Removing shortcuts from its
ResNet-50 counterpart is much worse (31.2%). These comparisons suggest that the residual connections are helpful
for optimization, whereas aggregated transformations are
stronger representations, as shown by the fact that they
perform consistently better than their counterparts with or
without residual connections.
Performance.
For simplicity we use Torch’s built-in
grouped convolution implementation, without special optimization. We note that this implementation was brute-force
and not parallelization-friendly. On 8 GPUs of NVIDIA
M40, training 32×4d ResNeXt-101 in Table 3 takes 0.95s
per mini-batch, vs. 0.70s of ResNet-101 baseline that has
similar FLOPs. We argue that this is a reasonable overhead.
We expect carefully engineered lower-level implementation
(e.g., in CUDA) will reduce this overhead. We also expect
that the inference time on CPUs will present less overhead.
Training the 2×complexity model (64×4d ResNeXt-101)
takes 1.7s per mini-batch and 10 days total on 8 GPUs.
Comparisons with state-of-the-art results. Table 5 shows
more results of single-crop testing on the ImageNet validation set.
In addition to testing a 224×224 crop, we
also evaluate a 320×320 crop following .
Our results compare favorably with ResNet, Inception-v3/v4, and
Inception-ResNet-v2, achieving a single-crop top-5 error
rate of 4.4%. In addition, our architecture design is much
simpler than all Inception models, and requires considerably fewer hyper-parameters to be set by hand.
ResNeXt is the foundation of our entries to the ILSVRC
2016 classiﬁcation task, in which we achieved 2nd place.
We note that many models (including ours) start to get saturated on this dataset after using multi-scale and/or multicrop testing. We had a single-model top-1/top-5 error rates
of 17.7%/3.7% using the multi-scale dense testing in ,
on par with Inception-ResNet-v2’s single-model results of
17.8%/3.7% that adopts multi-scale, multi-crop testing. We
had an ensemble result of 3.03% top-5 error on the test set,
on par with the winner’s 2.99% and Inception-v4/Inception-
ResNet-v2’s 3.08% .
320×320 / 299×299
top-1 err top-5 err top-1 err
ResNet-101 
ResNet-200 
Inception-v3 
Inception-v4 
Inception-ResNet-v2 
ResNeXt-101 (64 × 4d)
Table 5. State-of-the-art models on the ImageNet-1K validation
set (single-crop testing).
The test size of ResNet/ResNeXt is
224×224 and 320×320 as in and of the Inception models
is 299×299.
mini-batches
top-1 error (%)
ResNet-101 (1 x 64d) val
ResNeXt-101 (32 x 4d) val
Figure 6. ImageNet-5K experiments. Models are trained on the
5K set and evaluated on the original 1K validation set, plotted as
a 1K-way classiﬁcation task. ResNeXt and its ResNet counterpart
have similar complexity.
5K-way classiﬁcation 1K-way classiﬁcation
ResNeXt-50
ResNet-101
ResNeXt-101 32 × 4d
Table 6. Error (%) on ImageNet-5K. The models are trained on
ImageNet-5K and tested on the ImageNet-1K val set, treated as a
5K-way classiﬁcation task or a 1K-way classiﬁcation task at test
time. ResNeXt and its ResNet counterpart have similar complexity. The error is evaluated on the single crop of 224×224 pixels.
5.2. Experiments on ImageNet-5K
The performance on ImageNet-1K appears to saturate.
But we argue that this is not because of the capability of the
models but because of the complexity of the dataset. Next
we evaluate our models on a larger ImageNet subset that
has 5000 categories.
Our 5K dataset is a subset of the full ImageNet-22K set
 . The 5000 categories consist of the original ImageNet-
1K categories and additional 4000 categories that have the
largest number of images in the full ImageNet set. The 5K
set has 6.8 million images, about 5× of the 1K set. There is
no ofﬁcial train/val split available, so we opt to evaluate on
the original ImageNet-1K validation set. On this 1K-class
val set, the models can be evaluated as a 5K-way classiﬁcation task (all labels predicted to be the other 4K classes are
automatically erroneous) or as a 1K-way classiﬁcation task
(softmax is applied only on the 1K classes) at test time.
The implementation details are the same as in Sec. 4.
The 5K-training models are all trained from scratch, and
Wide ResNet 
ResNeXt-29, 8×64d
ResNeXt-29, 16×64d
Table 7. Test error (%) and model size on CIFAR. Our results are
the average of 10 runs.
are trained for the same number of mini-batches as the 1Ktraining models (so 1/5× epochs). Table 6 and Fig. 6 show
the comparisons under preserved complexity. ResNeXt-50
reduces the 5K-way top-1 error by 3.2% comparing with
ResNet-50, and ResNetXt-101 reduces the 5K-way top-1
error by 2.3% comparing with ResNet-101. Similar gaps
are observed on the 1K-way error. These demonstrate the
stronger representational power of ResNeXt.
Moreover, we ﬁnd that the models trained on the 5K
set (with 1K-way error 22.2%/5.7% in Table 6) perform
competitively comparing with those trained on the 1K set
(21.2%/5.6% in Table 3), evaluated on the same 1K-way
classiﬁcation task on the validation set.
This result is
achieved without increasing the training time (due to the
same number of mini-batches) and without ﬁne-tuning. We
argue that this is a promising result, given that the training
task of classifying 5K categories is a more challenging one.
5.3. Experiments on CIFAR
We conduct more experiments on CIFAR-10 and 100
datasets . We use the architectures as in and replace the basic residual block by the bottleneck template
. Our networks start with a single 3×3 conv
layer, followed by 3 stages each having 3 residual blocks,
and end with average pooling and a fully-connected classi-
ﬁer (total 29-layer deep), following . We adopt the same
translation and ﬂipping data augmentation as . Implementation details are in the appendix.
We compare two cases of increasing complexity based
on the above baseline: (i) increase cardinality and ﬁx all
widths, or (ii) increase width of the bottleneck and ﬁx cardinality = 1. We train and evaluate a series of networks
under these changes. Fig. 7 shows the comparisons of test
error rates vs. model sizes. We ﬁnd that increasing cardinality is more effective than increasing width, consistent to
what we have observed on ImageNet-1K. Table 7 shows the
results and model sizes, comparing with the Wide ResNet
 which is the best published record. Our model with a
similar model size (34.4M) shows results better than Wide
ResNet. Our larger method achieves 3.58% test error (average of 10 runs) on CIFAR-10 and 17.31% on CIFAR-100.
To the best of our knowledge, these are the state-of-the-art
results (with similar data augmentation) in the literature including unpublished technical reports.
# of parameters (M)
test error (%)
ResNet-29 (increase width)
ResNeXt-29 (increase cardinality)
Figure 7. Test error vs. model size on CIFAR-10. The results are
computed with 10 runs, shown with standard error bars. The labels
show the settings of the templates.
ResNeXt-50
ResNet-101
ResNeXt-101
Table 8. Object detection results on the COCO minival set.
ResNeXt and its ResNet counterpart have similar complexity.
5.4. Experiments on COCO object detection
Next we evaluate the generalizability on the COCO object detection set . We train the models on the 80k training set plus a 35k val subset and evaluate on a 5k val subset
(called minival), following . We evaluate the COCOstyle Average Precision (AP) as well as AP@IoU=0.5 .
We adopt the basic Faster R-CNN and follow to
plug ResNet/ResNeXt into it. The models are pre-trained
on ImageNet-1K and ﬁne-tuned on the detection set. Implementation details are in the appendix.
Table 8 shows the comparisons. On the 50-layer baseline, ResNeXt improves by 2.1% and AP by 1.0%,
without increasing complexity. ResNeXt shows smaller improvements on the 101-layer baseline. We conjecture that
more training data will lead to a larger gap, as observed on
the ImageNet-5K set.
It is also worth noting that recently ResNeXt has been
adopted in Mask R-CNN that achieves state-of-the-art
results on COCO instance segmentation and object detection tasks.
Acknowledgment
S.X. and Z.T.’s research was partly supported by NSF
IIS-1618477. The authors would like to thank Tsung-Yi
Lin and Priya Goyal for valuable discussions.
A. Implementation Details: CIFAR
We train the models on the 50k training set and evaluate
on the 10k test set. The input image is 32×32 randomly
cropped from a zero-padded 40×40 image or its ﬂipping,
following . No other data augmentation is used. The
ﬁrst layer is 3×3 conv with 64 ﬁlters. There are 3 stages
each having 3 residual blocks, and the output map size is
32, 16, and 8 for each stage . The network ends with a
global average pooling and a fully-connected layer. Width
is increased by 2× when the stage changes (downsampling),
as in Sec. 3.1. The models are trained on 8 GPUs with a
mini-batch size of 128, with a weight decay of 0.0005 and
a momentum of 0.9. We start with a learning rate of 0.1
and train the models for 300 epochs, reducing the learning
rate at the 150-th and 225-th epoch. Other implementation
details are as in .
B. Implementation Details: Object Detection
We adopt the Faster R-CNN system . For simplicity
we do not share the features between RPN and Fast R-CNN.
In the RPN step, we train on 8 GPUs with each GPU holding
2 images per mini-batch and 256 anchors per image. We
train the RPN step for 120k mini-batches at a learning rate
of 0.02 and next 60k at 0.002. In the Fast R-CNN step, we
train on 8 GPUs with each GPU holding 1 image and 64
regions per mini-batch. We train the Fast R-CNN step for
120k mini-batches at a learning rate of 0.005 and next 60k at
0.0005, We use a weight decay of 0.0001 and a momentum
of 0.9. Other implementation details are as in https://
github.com/rbgirshick/py-faster-rcnn.