Machine Learning, 56, 209–239, 2004
c⃝2004 Kluwer Academic Publishers. Manufactured in The Netherlands.
Semi-Supervised Learning on Riemannian
MIKHAIL BELKIN
 
PARTHA NIYOGI
 
Department of Computer Science, University of Chicago, 1100 E. 58th Street, Chicago, IL 60637, USA
Editors: Nina Mishra and Rajeev Motwani
We consider the general problem of utilizing both labeled and unlabeled data to improve classiﬁcation
accuracy. Under the assumption that the data lie on a submanifold in a high dimensional space, we develop an
algorithmic framework to classify a partially labeled data set in a principled manner. The central idea of our
approach is that classiﬁcation functions are naturally deﬁned only on the submanifold in question rather than the
total ambient space. Using the Laplace-Beltrami operator one produces a basis (the Laplacian Eigenmaps) for a
Hilbert space of square integrable functions on the submanifold. To recover such a basis, only unlabeled examples
are required. Once such a basis is obtained, training can be performed using the labeled data set.
Our algorithm models the manifold using the adjacency graph for the data and approximates the Laplace-
Beltrami operator by the graph Laplacian. We provide details of the algorithm, its theoretical justiﬁcation, and
several practical applications for image, speech, and text classiﬁcation.
semi-supervised learning, manifold learning, graph regularization, laplace operator, graph laplacian
Introduction
In many practical applications of data classiﬁcation and data mining, one ﬁnds a wealth of
easily available unlabeled examples, while collecting labeled examples can be costly and
time-consuming. Classical examples include object recognition in images, speech recognition, classifying news articles by topic and so on. In recent times, genetics has also provided
enormous amounts of readily accessible data. However, classiﬁcation of this data involves
experimentation and can be very resource intensive.
Consequently, it is of interest to develop algorithms that are able to utilize both labeled
and unlabeled data for classiﬁcation and other purposes. Although the area of partially
labeled classiﬁcation is fairly new, a considerable amount of work has been done in that
ﬁeld since the early 90’s . In particular, there has been a lot of recent interest in
semi-supervised learning and graphs, including and closely related graph kernels and especially the diffusion kernel .
In this paper we address the problem of classifying a partially labeled set by developing
the ideas proposed in Belkin and Niyogi for data representation. In particular, we
M. BELKIN AND P. NIYOGI
exploit the intrinsic structure of the data to improve classiﬁcation with unlabeled examples
under the assumption that the data resides on a low-dimensional manifold within a highdimensional representation space. In some cases it seems to be a reasonable assumption
that the data lie on or close to a manifold. For example a handwritten digit 0 can be fairly
accurately represented as an ellipse, which is completely determined by the coordinates of
its foci and the sum of the distances from the foci to any point. Thus the space of ellipses
is a ﬁve-dimensional manifold. An actual handwritten 0 would require more parameters,
but perhaps no more than 15 or 20. On the other hand the dimensionality of the ambient
representation space is the number of pixels which is typically far higher.
For other types of data the question of the manifold structure seems signiﬁcantly more
involved. For example, in text categorization documents are typically represented by vectors
whoseelementsare(sometimesweighted)countsofwords/termsappearinginthedocument.
It is far from clear why the space of documents should be a manifold. However there is
no doubt that it has a complicated intrinsic structure and occupies only a tiny portion of
the representation space, which is typically very high-dimensional, with dimensionality
higher than 1000. We show that even lacking convincing evidence for manifold structure,
we can still use our methods with good results. It is also important to note that while objects
are typically represented by vectors in Rn, the natural distance is often different from the
distance induced by the ambient space Rn.
While there has been recent work on using manifold structure for data representation
 , the only other application
to classiﬁcation, that we are aware of, was in Szummer and Jaakkola , where the
authors use a random walk on the adjacency graph for partially labeled classiﬁcation.
There are two central ideas that come together in this paper. First, we utilize the geometry
of the underlying space of possible patterns to construct representations, invariant maps,
and ultimately learning algorithms. Natural patterns are typically embedded in a very high
dimensional space. However, the intuition of researchers has always been that although
these patterns are ostensibly high dimensional, there is a low dimensional structure that
needs to be discovered. If this low dimensional structure is a linear subspace, then linear
projections to reduce the dimensionality are sufﬁcient. Classical techniques like Principal
Components Analysis and Random Projections may be used and one can then construct
classiﬁers in the low dimensional space. If on the other hand, the patterns lie on a low
dimensional manifold embedded in the higher dimensional space, then one needs to do
something different. This paper presents an approach to this situation. Second, we observe
that in order to estimate the manifold all that is needed are unlabeled data (the x′s). Once the
manifold is estimated, then the Laplace-Beltrami operator may be used to provide a basis
for maps intrinsically deﬁned on this manifold and then the appropriate classiﬁer (map)
is estimated on the basis of the labeled examples. Thus we have a natural framework for
learning with partially labeled examples.
Our framework is fairly general and while we focus in this paper on classiﬁcation problems, it is noteworthy that one may also construct algorithms for dimensionality reduction
and clustering within the same framework (see the discussion in Section 7). The rest of the
paper is organized as follows. In Sections 2, 3, and 4, we gradually present the motivation for
manifolds and an algorithmic framework for exploiting manifold structure. In Section 5 we
SEMI-SUPERVISED LEARNING ON RIEMANNIAN MANIFOLDS
provide the differential geometric underpinnings of the basic framework. Section 6 provides
experimental results on partially labeled classiﬁcation on a number of different data sets
and problems. In Section 7, we provide additional perspectives on issues like regularization
in general, convergence theorems, dimensionality reduction and clustering. We conclude in
Section 8.
Why manifold structure is useful for partially supervised learning
Consider ﬁrst a two-class classiﬁcation problem with classes C1, C2 and the space X, whose
elements are to be classiﬁed. A probabilistic model for that problem would include two
main ingredients, a probability density p(x) on X, and the class densities {p(C1 | x ∈X)},
{p(C2 | x ∈X)}. The unlabeled data alone does not necessarily tell us much about the
conditional distributions as we cannot identify the classes without labels. However, we can
improve our estimate of the probability density p(x) using the unlabeled data.
The simplest example is two disjoint classes on the real line. In that case the Bayes
risk is zero, and given sufﬁciently many unlabeled points, the structure can be recovered
completely with just one labeled example. In general, the unlabeled data provides us with
information about the probability distribution p(x), while labeled points tell us about the
conditional distributions.
In this paper we consider a version of this problem where p(x) puts all its measure
on a compact (low-dimensional) manifold in Rn. Therefore, as we shall see shortly, the
unlabeled examples can be used to estimate the manifold and the labeled examples then
specify a classiﬁer deﬁned on that manifold.
To provide a motivation for using a manifold structure, consider a simple synthetic
example shown in ﬁgure 1. The two classes consist of two parts of the curve shown in the
ﬁrst panel (row 1). We are given a few labeled points and a 500 unlabeled points shown in
panels 2 and 3 respectively. The goal is to establish the identity of the point labeled with
a question mark. There are several observations that may be made in the context of this
1. By observing the picture in panel 2 (row 1) we see that we cannot conﬁdently classify ?
by using the labeled examples alone. On the other hand, the problem seems much more
feasible given the unlabeled data shown in panel 3.
2. Since there is an underlying manifold, it seems clear at the outset that the (geodesic)
distances along the curve are more meaningful than Euclidean distances in the plane.
Many points which happen to be close in the plane are on the opposite sides of the curve.
Therefore rather than building classiﬁers deﬁned on the plane (R2) it seems preferable
to have classiﬁers deﬁned on the curve itself.
3. Even though the data suggests an underlying manifold, the problem is still not quite
trivial since the two different parts of the curve come confusingly close to each other.
There are many possible potential representations of the manifold and the one provided
by the curve itself is unsatisfactory. Ideally, we would like to have a representation of the
data which captures the fact that it is a closed curve. More speciﬁcally, we would like
an embedding of the curve where the coordinates vary as slowly as possible when one
M. BELKIN AND P. NIYOGI
Top row: Panel 1. Two classes on a plane curve. Panel 2. Labeled examples. “?” is a point to be
classiﬁed. Panel 3. 500 random unlabeled examples. Bottom row: Panel 4. Ideal representation of the curve. Panel
5. Positions of labeled points and “?” after applying eigenfunctions of the Laplacian. Panel 6. Positions of all
traverses the curve. Such an ideal representation is shown in the panel 4 (ﬁrst panel of the
second row). Note that both represent the same underlying manifold structure but with
different coordinate functions. It turns out (panel 6) that by taking a two-dimensional
representation of the data with Laplacian Eigenmaps , we get
very close to the desired embedding. Panel 5 shows the locations of labeled points in the
new representation space. We see that “?” now falls squarely in the middle of “+” signs
and can easily be identiﬁed as a “+”.
This artiﬁcial example illustrates that recovering the manifold and developing classiﬁers
on the manifold itself might give us an advantage in classiﬁcation problems. To recover the
manifold, all we need is unlabeled data. The labeled data is then used to develop a classiﬁer
deﬁned on this manifold. Thus given a set of labeled examples ((xi, yi); xi ∈Rk, yi ∈Y)
and a set of unlabeled examples (x j ∈Rk), the normal impulse is to seek a classiﬁer
Since k is very large, this immediately leads to difﬁculties due to the “curse of dimensionality”. Instead, we exploit the fact that all xk ∈M where M is a low dimensional manifold.
SEMI-SUPERVISED LEARNING ON RIEMANNIAN MANIFOLDS
Therefore, we construct classiﬁers of the form
These are the intuitions we formalize in the rest of the paper.
Representing data as a manifold
We hope we provided at least some justiﬁcation for using the manifold structure for classi-
ﬁcation problems. Of course, this structure cannot be utilized unless we have a reasonable
model for the manifold. The model used here is that of a weighted graph whose vertices
are data points. Two data points are connected with an edge if and only if the points are
adjacent, which typically means that either the distance between them is less than some ϵ
or that one of them is in the set of n nearest neighbors of the other.
Toeachedgewecanassociateadistancebetweenthecorrespondingpoints.The“geodesic
distance” between two vertices is the length of the shortest path between them on the adjacency graph. Notice that the geodesic distance can be very different from the distance in the
ambient space. It can be shown that if the points are sampled from a probability distribution supported on the whole manifold the geodesic distance on the graph will converge to
the actual geodesic distance on the manifold as the number of points tends to inﬁnity .
Once we set up an approximation to the manifold, we need a method to exploit the
structure of the model to build a classiﬁer. One possible simple approach would be to use
the “geodesic nearest neighbors”. The geodesic nearest neighbor of an unlabeled point u
is a labeled point l such that “geodesic distance” along the edges of the adjacency graph,
between the points u and l is the shortest. Then as with usual nearest neighbors the label of
l is assigned to u.
However, while simple and well-motivated, this method is potentially unstable. Even
a relatively small amount of noise or a few outliers can change the results dramatically.
A related more sophisticated method based on a random walk on the adjacency graph is
proposed in Szummer and Jaakkola . We also note the approach taken in Blum and
Chawla which uses mincuts of certain graphs for partially labeled classiﬁcations.
Our approach
Our approach is based on the Laplace-Beltrami operator on the manifold. A Riemannian
manifold, i.e. a manifold with a notion of local distance, has a natural operator  on
differentiablefunctions,whichisknownastheLaplace-Beltramioperator,ortheLaplacian.1
In the case of Rn the Laplace-Beltrami operator is simply  = −
i . Note that we
adopt the geometric convention of writing it with the ‘-’ sign.
 is a positive-semideﬁnite self-adjoint (with respect to the L2 inner product) operator on
twice differentiable functions. Remarkably, it turns out when M is a compact manifold, 
M. BELKIN AND P. NIYOGI
has a discrete spectrum and eigenfunctions of  provide an orthogonal basis for the Hilbert
space L2(M). Note that  is only deﬁned on a subspace in L2(M).
Therefore any function f ∈L2(M) can be written as
where ei are eigenfunctions, ei = λiei.
Now assuming that the data lie on a manifold M, we consider the simplest model, where
the class membership is represented by a square integrable function m : M →{−1, 1}.
Equivalently, we can say that the classes are represented by measurable sets S1, S2 with null
intersection. Alternatively, if S1 and S2 do intersect, we can put m(x) = 1−2 Prob(x ∈S1).
The only condition we need is that m(x) is a measurable function.
The classiﬁcation problem can can be interpreted as a problem of interpolating a function
onamanifold.SinceafunctioncanbewrittenintermsoftheeigenfunctionsoftheLaplacian,
we adjust the coefﬁcients of the Laplacian to provide the optimal ﬁt to the data (i.e the labeled
points), just as we might approximate a signal with a Fourier series2 m(x) ≈N
0 aiei(x).
It turns out that not only the eigenfunctions of the Laplacian are a natural basis to consider,
but that they also satisfy a certain optimality condition. In a sense, which we will make
precise later, they provide a maximally smooth approximation, similar to the way splines
are constructed.
Description of the algorithm
Given k points x1, . . . , xk ∈Rl, we assume that the ﬁrst s < k points have labels ci, where
ci ∈{−1, 1} and the rest are unlabeled. The goal is to label the unlabeled points. We also
introduce a straightforward extension of the algorithm when there are more than two classes.
Step 1. (Constructing the adjacency graph with n nearest neighbors). Nodes i and j corresponding to the points xi and x j are connected by an edge if i is among n nearest
neighbors of j or j is among n nearest neighbors of i. The distance can be the standard
Euclidean distance in Rl or some other distance, such as angle, which is natural for text
classiﬁcation problems. For our experiments we take the weights to be one. However,
see Belkin and Niyogi for the discussion about the choice of weights, and its
connection to the heat kernel. Thus wi j = 1 if points xi and x j are close and wi j = 0
otherwise.
Step 2. (Eigenfunctions) Compute p eigenvectors corresponding to the smallest eigenvalues
for the eigenvector problem:
Matrix L = W −D is the graph Laplacian for the adjacency graph. Here W is the
adjacency matrix deﬁned above and D is diagonal matrix of the same size as W, with
row sums of W as entries, Dii = 
j W ji. Laplacian is a symmetric, positive semideﬁnite
SEMI-SUPERVISED LEARNING ON RIEMANNIAN MANIFOLDS
matrix which can be thought of as an operator on functions deﬁned on vertices of the
graph. The eigenfunctions can be interpreted as a generalization of the low frequency
Fourier harmonics on the manifold deﬁned by the data points.
Step 3. (Building the classiﬁer) To approximate the class we minimize the error function
where p is the number of eigenfunctions we wish to employ and the sum is taken over
all labeled points and the minimization is considered over the space of coefﬁcients a =
(a1, . . . , ap)T . The solution is given by
where c = (c1, . . . , cs) and
is the matrix of values of eigenfunctions on the labeled points. For the case of several
classes, we build a one-against-all classiﬁer for each individual class.
Step 4. (Classifying unlabeled points) If xi, i > s is an unlabeled point we put
ei ja j ≥0
ei ja j < 0
This, of course, is just applying a linear classiﬁer constructed in Step 3. If there are several
classes, one-against-all classiﬁers compete using p
j=1 ei ja j as a conﬁdence measure.
M. BELKIN AND P. NIYOGI
Theoretical interpretation
Here we give a brief discussion of the theoretical underpinnings of the algorithm. Let
M ⊂Rk be an n-dimensional compact Riemannian manifold isometrically embedded
in Rk for some k3. Intuitively M can be thought of as a n-dimensional “surface” in Rk.
Riemannian structure on M induces a volume form that allows us to integrate functions
deﬁned on M. The square integrable functions form a Hilbert space L2(M). If by C∞(M)
we denote the space of inﬁnitely differentiable functions on M then we have the Laplace-
Beltrami operator as a second-order differential operator M : C∞(M) →C∞(M).4
There are two important properties of the Laplace-Beltrami operator that are relevant to
our discussion here.
The Laplacian provides a basis on L2(M)
It can be shown that  is a self-adjoint positive semideﬁnite operator
and that its eigenfunctions form a basis for the Hilbert space L2(M). The spectrum of 
is discrete (provided M is compact) , with the smallest eigenvalue 0 corresponding to the
constant eigenfunction. Therefore any f ∈L2(M) can be written as
where ei are eigenfunctions, ei = λiei.
The simplest nontrivial example is a circle S1.
S1 f (φ) = −d2 f (φ)
Therefore the eigenfunctions are given by
where f (φ) is a π-periodic function. It is easy to see that all eigenfunctions of  are
of the form e(φ) = sin(nφ) or e(φ) = cos(nφ) with eigenvalues {12, 22, . . .}. Therefore, as a corollary of these far more general results, we see that the Fourier series for a
π-periodic L2 function f converges to f in L2 (stronger conditions are needed for pointwise convergence):
an sin(nφ) + bn cos(nφ)
SEMI-SUPERVISED LEARNING ON RIEMANNIAN MANIFOLDS
Thus we see that the eigenfunctions of the Laplace-Beltrami operator provide a natural
basis for representing functions on M. However  provides more than just a basis, it also
yields a measure of smoothness for functions on the manifold.
The Laplacian as a smoothness functional
A simple measure of the degree of smoothness for a function f on a unit circle S1 is the “smoothness functional”
S1 | f (φ)′|2dφ
If S( f ) is close to zero, we think of f as being “smooth”.
Naturally, constant functions are the most “smooth”. Integration by parts yields
S1 f ′(φ)d f =
S1 f f dφ = ⟨f, f ⟩L2(S1)
In general, if f : M →R, then
|∇f |2dµ =
f f dµ = ⟨f, f ⟩L2(M)
where ∇f is the gradient vector ﬁeld of f . If the manifold is Rn then ∇f = n
general, for an n-manifold, the expression in a local coordinate chart involves the coefﬁcients
of the metric tensor.
Therefore the smoothness of a unit norm eigenfunction ei of  is controlled by the
corresponding eigenvalue λi since
S(ei) = ⟨ei, ei⟩L2(M) = λi
For an arbitrary f = 
i αiei, we can write S( f ) as
S( f ) = ⟨f, f ⟩=
The linear subspace, where the smoothness functional is ﬁnite is a Reproducing Kernel
Hilbert Space . We develop this point of view further in Section 7.
It is not hard to see that λ1 = 0 is the smallest eigenvalue for which the eigenfunction is
the constant function e1 =
µ(M). It can also be shown that if M is compact and connected
there are no other eigenfunctions with eigenvalue 0.
M. BELKIN AND P. NIYOGI
Therefore approximating a function f (x) ≈p
1 aiei(x) in terms of the ﬁrst p eigenfunctions of  is a way of controlling the smoothness of the approximation. The optimal
approximation is obtained by minimizing the L2 norm of the error:
a = argmin
a=(a1,...,ap)
This approximation is given by a projection in L2 onto the span of the ﬁrst p eigenfunctions
ei(x) f (x)dµ = ⟨ei, f ⟩L2(M)
In practice we only know the values of f at a ﬁnite number of points x1, . . . , xn and
therefore have to solve a discrete version of this problem
¯a = argmin
¯a=(¯a1,...,¯ap)
¯a je j(xi)
The solution to this standard least squares problem is given by
¯aT = (ET E)−1EyT
where Ei j = ei(x j) and y = ( f (x1), . . . , f (xn)).
Connection with the graph Laplacian
As we are approximating a manifold with a graph, we need a suitable measure of smoothness
for functions deﬁned on the graph.
It turns out that many of the concepts in the previous section have parallels in graph theory
 . Let G = (V, E) be a weighted graph on n vertices. We assume that
the vertices are numbered and use the notation i ∼j for adjacent vertices i and j. The graph
Laplacian of G is deﬁned as L = D−W, where W is the weight matrix and D is a diagonal
matrix, Dii = 
j Wi j.5 L can be thought of as an operator on functions deﬁned on vertices
of the graph. It is not hard to see that L is a self-adjoint positive semideﬁnite operator. By
the (ﬁnite dimensional) spectral theorem any function on G can be decomposed as a sum
of eigenfunctions of L.
If we think of G as a model for the manifold M it is reasonable to assume that a function
on G is smooth if it does not change too much between nearby points. If f = ( f1, . . . , fn) is
a function on G, then we can formalize that intuition by deﬁning the smoothness functional
wi j( fi −f j)2
SEMI-SUPERVISED LEARNING ON RIEMANNIAN MANIFOLDS
It is not hard to show that
SG(f) = f L fT = ⟨f, Lf ⟩G =
λi⟨f, ei⟩G
which is the discrete analogue of the integration by parts from the previous section. The
inner product here is the usual Euclidean inner product on the vector space with coordinates
indexed by the vertices of G, ei are normalized eigenvectors of L, Lei = λiei, ∥ei∥=
1. All eigenvalues are non-negative and the eigenfunctions corresponding to the smaller
eigenvalues can be thought as “more smooth”. The smallest eigenvalue λ1 = 0 corresponds
to the constant eigenvector e1.
Experimental results
The experiments with labeled and unlabeled data may be conducted in two different ways.
1. Labeling a partially labeled data set: Given a set L of labeled examples and a set U of
unlabeled data, classify the unlabeled set with maximal accuracy. This setting is often
referred to as “transductive inference”.
2. Labeling a held out test set using a training set consisting of labeled and unlabeled
Note that ultimately (1) and (2) are equivalent in the following sense. First, (2) implies (1)
trivially as we can always use the developed classiﬁer to classify the unlabeled examples.
But also (1) implies (2). If we have an algorithm for solving (1), then we can solve (2), i.e.,
classify a new point x by simply adding x to the unlabeled set and running the algorithm
with this revised unlabeled set U ∪{x}.
In the following sections, we concentrate on experiments conducted in the ﬁrst setting.
We can, of course, use the method (1) for solving problems in the second setting as well.
However, following such protocol literally turns out to be computationally too expensive
as a large eigenvalue problem has to be solved for each new test point. Instead we propose
a simple heuristic and provide some encouraging experimental results for this case.
Handwritten digit recognition
Asanapplicationofourtechniquesweconsidertheproblemofopticalcharacterrecognition.
We use the popular MNIST dataset which contains 28×28 grayscale images of handwritten
digits.6 We use the 60000 image training set for our experiments. For all experiments we
use 8 nearest neighbors to compute the adjacency matrix. Note that the adjacency matrices
are very sparse which makes solving eigenvector problems for matrices as big as 60000 by
60000 possible.
All 60000 images are provided with labels in the original dataset. For a particular trial,
we ﬁx the number of labeled examples we wish to use. A random subset of the 60000
M. BELKIN AND P. NIYOGI
Percentage error rates for different numbers of labeled points for the 60000 point MNIST dataset. The
error rate is calculated on the unlabeled part of the dataset, each number is an average over 20 random splits.The
rightmost column contains the nearest neighbor base line.
Number of eigenvectors
Labeled points
images is used with labels to form L. The rest of the images are used without labels to
form U. The classiﬁcation results (for U) are averaged over 20 different random draws for
L. The results are presented in Table 1. Each row corresponds to a different number of
labeled points (size of L). We compute the error rates when the number of eigenvectors is
smaller than the number of labeled points as no generalization can be expected to take place
otherwise.
The rightmost columns show baseline performances obtained using the best k-nearest
neighbors classiﬁer (k was taken to be 1, 3 or 5) to classify the unlabeled set U using the
labeled set L. We choose the nearest neighbors as a baseline, since the Laplacian based
algorithm presented in this paper makes use only of the nearest neighbor information to
classify the unlabeled data. In addition, nearest neighbors is known to be a good general
purpose classiﬁer. k-NN and its variations are often used in practical applications.
Each row represents a different choice for the number of labeled examples used. The
columns show performance for different choices of the number of eigenvectors of the graph
Laplacian retained by the algorithm.
For a ﬁxed number of eigenvectors, performance improves with the number of labeled
points but saturates after a while. The saturation point is empirically seen to be when the
number of labeled points is roughly ten times the number of eigenvectors.
For a ﬁxed number of labeled points, error rate decreases with the number of eigenvectors
and then begins to increase again. Presumably, if too many eigenvectors are retained, the
algorithm starts to overﬁt. This turning point happens when the number of eigenvectors is
somewhere between 10% and 50% of the number of labeled examples. The 20% percent
ratio seems to work well in a variety of experiments with different data sets and this is what
we recommend for comparison with the base line.
The improvements over the base line are striking, sometimes exceeding 70% depending on the number of labeled and unlabeled examples. With only 100 labeled examples (and 59900 unlabeled examples), the Laplacian classiﬁer does nearly as well as
SEMI-SUPERVISED LEARNING ON RIEMANNIAN MANIFOLDS
MNIST data set. Percentage error rates for different numbers of labeled and unlabeled points compared
to best k-NN base line.
the nearest neighbor classiﬁer with 5000 labeled examples. Similarly, with 500/59500
labeled/unlabeled examples, it does slightly better than the nearest neighbor base line using
20000 labeled examples.
Shown in ﬁgure 2 is a summary plot of classiﬁcation accuracy on the unlabeled set
comparing the nearest neighbors baseline with our algorithm that retains the number of
eigenvectors by following the 20% rule.7 The results for the total 60000 point data set,
and 10000 and 1000 subsets are compared. We see that adding unlabeled data consistently
improves classiﬁcation accuracy. We notice that when almost all of the data is labeled,
the performance of our classiﬁer is close to that of k-NN. It is not particularly surprising
as our method uses the nearest neighbor information. Curiously, it is also the case when
there are very few labeled points (20 labeled points, or just 2 per class on average). Both
observations seem to be applicable across the datasets. Not surprisingly for the same number
of labeled points, using fewer unlabeled points results in a higher error rate. However, yet
again, when the number of labeled examples is very small (20 and 50 labeled examples,
i.e., an average of 2 and 5 examples per class), the number of unlabeled points does not
seem to make much difference. We conjecture this might be due to the small number of
eigenvectors used, which is not sufﬁcient to capture the behavior of the class membership
functions.
M. BELKIN AND P. NIYOGI
Text classiﬁcation
The second application we consider is text classiﬁcation using the popular 20 Newsgroups
data set. This data set contains approximately 1000 postings from each of 20 different
newsgroups. Given an article, the problem is to determine to which newsgroup it was
posted. The problem is fairly difﬁcult as many of the newsgroups deal with similar subjects.
For example, ﬁve newsgroups discuss computer-related subjects, two discuss religion and
three deal with politics. About 4% of the articles are cross-posted. Unlike the handwritten
digit recognition, where human classiﬁcation error rate is very low, there is no reason to
believe that this would be an easy test for humans. There is also no obvious reason why this
data should have manifold structure.
We tokenize the articles using the Rainbow software package written by Andrew
McCallum. We use a standard “stop-list” of 500 most common words to be excluded
and also exclude headers, which among other things contain the correct identiﬁcation of
the newsgroup. No further preprocessing is done. Each document is then represented by the
counts of the most frequent 6000 words normalized to sum to 1. Documents with 0 total
count are removed, thus leaving us with 19935 vectors in a 6000-dimensional space.
The distance is taken to be the angle between the representation vectors. More sophisticated schemes, such as TF-IDF representations, increasing the weights of dimensions
corresponding to more relevant words and treating cross-posted articles properly would be
likely to improve the baseline accuracy.
We follow the same procedure as with the MNIST digit data above. A random subset of
a ﬁxed size is taken with labels to form L. The rest of the dataset is considered to be U.
We average the results over 20 random splits.8 As with the digits, we take the number of
nearest neighbors for the algorithm to be 8.
The results are summarized in the Table 2.
We observe that the general patterns of the data are very similar to those for the MNIST
data set. For a ﬁxed number of labeled points, performance is optimal when the number of
eigenvectors retained is somewhere between 20% and 50% of the number of labeled points.
Percentage error rates for various numbers of labeled points and eigenvectors. The total number of
points is 19935. The error is calculated on the unlabeled part of the dataset.
Number of eigenvectors
Labeled points
SEMI-SUPERVISED LEARNING ON RIEMANNIAN MANIFOLDS
20 Newsgroups data set. Error rates for different numbers of labeled and unlabeled points compared
to best k-NN baseline.
We see that when the ratio of labeled to unlabeled examples is either very small or very
large, the performance of our algorithm is close to that of the nearest neighbor baseline,
with the most signiﬁcant improvements occurring in the midrange, seemingly when the
number of labeled points is between 5% and 30% of the unlabeled examples.
While the decreases in the error rate from the baseline are not quite as good as with
MNIST data set, they are still signiﬁcant, reaching up to 30%.
In ﬁgure 3 we summarize the results by taking 19935, 2000 and 600 total points respectively and calculating the error rate for different numbers of labeled points. The number of
eigenvectors used is always 20% of the number of labeled points. We see that having more
unlabeled points improves the classiﬁcation error in most cases although when there are
very few labeled points, the differences are small.
Phoneme classiﬁcation
Here we consider the problem of phoneme classiﬁcation. More speciﬁcally we are trying to
distinguish between three vowels “aa” (as in “dark”), “iy”(as in “beat”),“eh” (as in “bet”).
The data is taken from the TIMIT data set. The data is presegmented into phonemes. Each
M. BELKIN AND P. NIYOGI
Percentage error rates for various numbers of labeled points and eigenvectors. The total number of
points is 13168. The error is calculated on the unlabeled part of the dataset.
Number of eigenvectors
Labeled points
vowel is represented by the average of the logarithm of the Fourier spectrum of each frame
in the middle third of the phoneme.
We follow the same procedure as before, the number of nearest neighbors is taken to be
10. The total number of phonemes considered is 13168. The results are shown in Table 3
and ﬁgure 4. The results parallel those for the rest of our experiments with one interesting
exception: no signiﬁcant difference is seen between the results for just 2000 total points and
the whole dataset. In fact the corresponding graphs are almost identical. However going
from 600 points to 2000 points yields in a signiﬁcant performance improvement. It seems
that for this particular data set unlabeled the structure is learned with relatively few unlabeled
Comparison with geodesic nearest neighbors
The simplest semi-supervised learning algorithm that utilizes the manifold structure of
the data is arguably the geodesic nearest neighbors (GNN). This is a version of the nearest
neighbor algorithm that uses geodesic distances on the data derived adjacency graph instead
of Euclidean distances in the ambient space. The graph is computed in the same manner as
before. The edge weights are taken to be local Euclidean distances.
In this section, we conduct some experiments to see how our method compares with
GNN. Figure 5 shows the results of the comparison between the Laplacian and the best
k-GNN, where k ∈{1, 3, 5}. The experiments are on a 10000 point subset of the MNIST
hand-written digit dataset. We see that for very small numbers of labeled points (20 total, i.e.
approximately 2 per class, as we did not balance the classes) GNN performs slightly better,
but once the number of labeled reaches exceeds 100, the Laplacian produces a considerably
lower error rate.
It is worth noting that GNN is consistently better than standard nearest neighbors (NN)
suggesting that even a naive utilization of manifold structure provides a beneﬁt. This paper
provides a more sophisticated way to use manifold structure.
SEMI-SUPERVISED LEARNING ON RIEMANNIAN MANIFOLDS
TIMIT dataset. Error rates for different numbers of labeled and unlabeled points compared to best
k-NN baseline.
Improving existing classiﬁers with unlabeled data
Inthispaperweconsideredtheproblemofclassifyingtheunlabeleddata.Whiletheoretically
a classiﬁer can be constructed by simply adding each new point to the unlabeled data and
reclassifying, this method is far too slow to be of much practical use. A somewhat more
practical suggestion would be to accumulate unlabeled data ﬁrst and then classify it in the
“batch” mode.
However another intriguing possibility is to classify the unlabeled data, and then to use
these labels to train a different classiﬁer with the hope that the error rate on the unlabeled
data would be small.
Figure 6 provides an illustration of this technique on the MNIST data set. Using a certain
number of labeled points on the 60000 point training set, we use our algorithm to classify
the remainder of the dataset. We then use the obtained fully labeled training set (with some
incorrect labels, of course) to classify the held out 10000 point test set (which we do not use
in other experiments) using a 3-NN classiﬁer. We see that the performance is only slightly
worse than the Laplacian baseline error rate, which is calculated on the unlabeled portion
of the training set.9 By thus labeling the unlabeled data set and treating it as a fully labeled
training set, we obtained signiﬁcant improvements over the baseline best k-NN (k = 1, 3, 5)
classiﬁer.
M. BELKIN AND P. NIYOGI
MNIST dataset. Error rates for different numbers of labeled and unlabeled points compared to best
k-NN and best k-GNN. The total number of points is 10000.
Perspective
Much of this paper has focused on the problem of utilizing labeled and unlabeled examples
in a coherent fashion. In this section, we take a broader perspective of the manifold learning
framework and make connections to a number of different problems and issues.
Regularization on manifolds and graphs
We see that the Laplace-Beltrami operator might be used to provide a basis for L2(M),
the set of square integrable functions on the manifold. In general, one might take various
classes of functions that are invariantly deﬁned on the manifold and solve a problem of the
following sort
(yi −f (xi))2 + λP( f )
where H : M →R.
SEMI-SUPERVISED LEARNING ON RIEMANNIAN MANIFOLDS
Results on the held out data set. Randomly chosen labeled are used to label the rest of the 60000 point
training set using the Laplacian classiﬁer. Then a 3-NN classiﬁer is applied to the held out 10000 point test set.
The ﬁrst term in the objective function is the empirical risk measured in terms of the least
squared error. The second term P( f ) is a stabilizer that is added within the framework of
Tikhonov regularization . For example, a simple and natural
choice is to take
⟨∇f, ∇f ⟩=
where f = 
i αiei, the ei’s and the λi’s are the eigenfunctions and eigenvalues respectively
of the manifold Laplacian .
In this setting, one may take H to be the following
 P( f ) < ∞
It is easy to check that the optimization problem provided by Eq. (1) reduces to a quadratic
problem in the αi’s.
M. BELKIN AND P. NIYOGI
Various other choices for P( f ) are possible. For example, one may take
where the norm ∥· ∥is the L2(M) norm and i is the iterated Laplacian (iterated i times).
It is easy to check that for f = 
∥i f ∥2 =
Again, the optimization problem reduces to a quadratic problem in the αi ′s.
These problems are best handled within the framework of regularization where H is an
appropriately chosen Reproducing Kernel Hilbert Space (RKHS). A RKHS is a Hilbert
space of functions where the evaluation functionals (functionals that simply evaluate a
function at a point) Ex f = f (x) are bounded, linear functionals. It is possible to show that
for each RKHS there corresponds a kernel K : X × X →R such that
f (x) = ⟨f (·), K(x, ·)⟩H
where the inner product ⟨·, ·⟩H is the one deﬁned naturally in the RKHS , for more details). In our setting, the domain X is the manifold M and
we are therefore interested in kernels K : M×M →R. Let us go through the construction
of an RKHS that is invariantly deﬁned on the manifold.
Fixaninﬁnitesequenceofnon-negativenumbers{ui; i = 1, 2, . . .}suchthat
Now deﬁne the following linear space of continuous functions
Deﬁne an inner product on this space in the following way. For any two functions f =
i αi fi and g = 
j β j f j, the inner product is deﬁned as
It can be veriﬁed that H is a RKHS with the following kernel
µiei(p)ei(q)
SEMI-SUPERVISED LEARNING ON RIEMANNIAN MANIFOLDS
Now one may solve the following optimization problem
(yi −f (xi))2 + λ∥f ∥2
This problem is well posed and has a unique solution given by
ai K(xi, x)
where a = (a1, . . . , an)T is the unique solution of the well posed linear system in Rn given
(λnIn + Kx)a = y
In the above expression In is the n × n identity matrix; Kx is a n × n Gram matrix whose
(i, j) element is equal to K(xi, x j) and y = (y1, . . . , yn)T .
If one considers the domain X to be the graph rather than the manifold, then one obtains
regularization procedures on the graph very naturally. The algorithm presented in this paper
is a special case of this general paradigm.
Various choices of kernels are possible by choosing µi’s differently. For example,
letting µi = e−tλi (λi’s being the eigenvalues of ) one obtains the heat kernel corresponding to diffusion of heat on the manifold. Alternatively, one might let µi = 0 ∀i > N. This
corresponds to solving the optimization problem in a ﬁnite dimensional subspace and is the
basis of the algorithms that were actually used in the experiments reported in the previous
section. Some of these connections have been explored in a different form by Kondor and
Lafferty in the context of developing kernels on graphs. The diffusion kernels in that
work refer to the heat kernel on the graph and its intimate relations with the Laplacian.
Convergence and sample complexity
It is worthwhile to reﬂect on the nature of the convergence theorems that are needed to
characterize the behavior of the manifold learning algorithms as a function of data. We
do this within the context of regularization described previously. A complete technical
treatment is beyond the scope of the current paper but we hope to provide the reader with
a sense of how one might proceed to provides bounds on the performance of such learning
algorithms.
Recall that H : M →R is a RKHS invariantly deﬁned on the manifold M. Then a key
goal is to minimize the regularized true risk as follows:
f ∈H E[(y −f (x))2] + λ∥f ∥2
M. BELKIN AND P. NIYOGI
This is what the learner would do if (i) inﬁnite amounts of labeled examples were available
and (ii) the manifold M were known. Note that E0 = min f ∈H E[(y −f (x))2] is the best
that the learner could possibly do under any circumstances. Thus limλ→0 Eλ = E0 and
the regularization constant λ may be suitably chosen to condition various aspects of the
learning problem at different stages.
In reality, of course, inﬁnite amounts of labeled examples are not available but rather
only a ﬁnite number n of such examples are provided to the learner. On the basis of this, the
learner minimizes the empirical risk and solves the following optimization problem instead
(same as Eq. (2)).
ˆEλ,n = min
(yi −f (xi))2 + λ∥f ∥2
Now one might ask, how far is ˆEλ,n from ˆEλ? In order to get a handle on this question,
one may proceed by building on the techniques described in Cucker and Smale ,
Bousquet and Elisseeff , and Kutin and Niyogi . We only provide a ﬂavor of
the kinds of results that may be obtained.
Let fopt = arg min R( f ) and ˆf n = arg min Remp( f ) where R( f ) = E[(y −f (x))2] +
H and Remp( f ) = 1
i=1(yi −f (xi))2 + λ∥f ∥2
H respectively. Note that for a ﬁxed
function f , Remp( f ) is a random variable whose mean is R( f ). Let labeled examples be
drawn according to a measure µ on M×Y where Y is a compact subset of R. Without loss
of generality, we may take Y = [0, M]. Then the following statements may be made:
Theorem 1.
For the ﬁxed function fopt, the empirical risk Remp( fopt) converges to the true
risk R( fopt) with probability 1. Further, with probability > 1 −δ, the following holds
|Remp( fopt) −R( fopt)| < M
This is simply a statement of the law of large numbers with a Hoeffding bound on the
deviation between the empirical average and the true average of a random variable. One
additionally needs to make use of the fact R( fopt) < R(0) < M2 where 0 denotes the constant
function that takes the value 0 everywhere.
A much harder theorem to prove is
Theorem 2.
Let A = supp∈M K(p, p) where K(p, p) = 
i (p). Then with probability > 1 −δ, we have
R( ˆf n) ≤Remp( ˆf n) + 2M2A2
In order to prove this we make use of the notion of algorithmic stability and the fact that regularization is uniformly hypothesis stable. The above theorem shows that as n becomes larger and larger, we have that
SEMI-SUPERVISED LEARNING ON RIEMANNIAN MANIFOLDS
R( ˆfn) is essentially upper bounded by Remp( ˆf n). Now one may make use of the following
two simple observations:
R( fopt) ≤R( ˆf n)
Remp( ˆf n) ≤Remp( fopt)
Inequality 5 holds because fopt minimizes R( f ) while inequality 6 holds because ˆf n minimizes Remp( f ).
Putting all of this together, we have with high probability (>1 −2δ), the following chain
of inequalities
R( fopt) ≤R( ˆf n) ≤Remp( ˆf n) + B ≤Remp( fopt) + B ≤R( fopt) + B + C
Using this we see that the quantity |Eλ−ˆEλ,n| can be made arbitrarily small and one obtains
rates at which this convergence occurs.
All of the above holds when the manifold M is known. In our situation, the manifold is
unknown and needs to be estimated from the unlabeled data. In particular, as we have seen,
an appropriate basis of functions needs to be estimated. So we solve instead the following
ˆEλ,n,m = min
(yi −f (xi))2 + λ∥f ∥2
where n labeled and m unlabeled examples are collected. Now H ′ : V →R is a collection
of functions deﬁned on the graph G = (V, E) where the locally connected graph G is
constructed in the manner described in earlier sections.
Ideally, one would like to show that ˆEλ,n,m →ˆEλ,n as m →∞. In other words, as the
unlabeled data goes to inﬁnity, regularized solutions on the graph converge to regularized
solutions on the manifold. If this were true, then we would ﬁnd that ˆEλ,n,m →Eλ as both
n (number of labeled examples) and m (number of unlabeled examples) go to inﬁnity.
Preliminary results obtained by the authors suggest that the graph Laplacian converges
to the manifold Laplacian in a certain sense. However, the nature of this convergence is not
strong enough yet for us to be able to formally claim that ˆEλ,n,m →ˆEλ,n as m →∞. We
leave this as a topic of future research.
M. BELKIN AND P. NIYOGI
Dimensionality reduction
Since we are in a setting where the manifold M is low dimensional, it is natural to consider
the question of whether the data may be embedded in a much lower dimensional space than
the original ambient one. As it turns out, the eigenfunctions of the Laplace-Beltrami operator
are arranged in increasing order of smoothness. Consequently, the lower eigenmaps may
be used for dimensionality reduction. It is worthwhile to reﬂect on this for a moment.
Suppose we desire a map f : M →Rm that optimally preserves locality, i.e., points
nearby on the manifold are mapped to nearby points in the lower dimensional space Rm.
The discussion here follows that in Belkin and Niyogi .
Let us ﬁrst consider mapping the manifold to the real line such that points close together
on the manifold get mapped close together on the line. Let f be such a map. Assume that
f : M →R is twice differentiable.
Consider two neighboring points x, z ∈M. They are mapped to f (x) and f (z) respectively. We ﬁrst show that
| f (z) −f (x)| ≤distM(x, z)∥∇f (x)∥+ o(distM(x, z))
The gradient ∇f (x) is a vector in the tangent space T Mx, such that given another vector
v ∈T Mx, d f (v) = ⟨∇f (x), v⟩M.
Let l = distM(x, z). Let c(t) be the geodesic curve parameterized by length connecting
x = c(0) and z = c(l). Then
f (z) = f (x) +
d f (c′(t)) dt = f (x) +
⟨∇f (c(t)), c′(t)⟩dt
Now by Schwartz Inequality,
⟨∇f (c(t)), c′(t)⟩≤∥∇f (c(t))∥∥c′(t)∥= ∥∇f (c(t))∥
Since c(t) is parameterized by length, we have ∥c′(t)∥= 1. We also have ∥∇f (c(t))∥=
∥∇f (x)∥+ O(t) (by Taylor’s approximation). Finally, by integrating we have
| f (z) −f (x)| ≤l ∥∇f (x)∥+ o(l)
where both O and o are used in the inﬁnitesimal sense.
If M is isometrically embedded in Rl then distM(x, z) = ∥x −z∥Rl + o (∥x −z∥Rl) and
| f (z) −f (x)| ≤∥∇f (x)∥∥z −x∥+ o (∥z −x∥)
Thus we see that if ∥∇f ∥provides us with an estimate of how far apart f maps nearby
SEMI-SUPERVISED LEARNING ON RIEMANNIAN MANIFOLDS
We therefore look for a map that best preserves locality on average by trying to ﬁnd
∥f ∥L2(M)=1
where the integral is taken with respect to the standard measure on a Riemannian manifold.
Note that minimizing
M ∥∇f (x)∥2 corresponds to minimizing Lf = 1
i, j( fi −f j)2Wi j
on a graph. Here f is a function on vertices and fi is the value of f on the ith node of the
Recall from the discussion in Section 5, that minimizing the objective function of Eq. (10)
reduces to ﬁnding eigenfunctions of the Laplace-Beltrami operator :
= −div ∇( f )
where div is the divergence of the vector ﬁeld. It follows from the Stokes’ theorem that
−div and ∇are formally adjoint operators, i.e. if f is a function and X is a vector ﬁeld then
M⟨X, ∇f ⟩= −
M div(X) f . Thus
Since  is positive semideﬁnite, f that minimizes
M ∥∇f ∥2 has to be an eigenfunction
of . Let the eigenvalues (in increasing order) be 0 = λ0 ≤λ1 ≤λ2 ≤. . . and let fi be
the eigenfunction corresponding to eigenvalue λi. It is easily seen that f0 is the constant
function that maps the entire manifold to a single point. To avoid this eventuality, we require
that the embedding map f be orthogonal to f0. It immediately follows that f1 is the optimal
embedding map. It is then easy to check that
x →( f1(x), . . . , fm(x))
provides the optimal m-dimensional embedding.
We note that the Laplacian eigenmaps discussed above generally do not provide an isometric embedding of the manifold though they have certain locality preserving
properties on average. Devising an algorithmic procedure to ﬁnd an isometric embedding for a potentially curved manifold from sampled data is an interesting problem for
which no solution is known to the best of our knowledge. However, if the manifold is ﬂat,
Isomap can be shown to converge to the optimal
Of course a variety of classical embedding theorems are known in differential geometry.
Again, we leave this for future investigation.
M. BELKIN AND P. NIYOGI
Clustering a manifold. Above: The manifold M is a three-dimensional “dumbbell”. The boundary
δM1 cuts the manifold in two parts optimally, so that the ratio of the area of the surface δM1 to the volume of the
smallest of the two parts M1 and M −M1 is minimized. Below: e1 is the (hypothetical) second eigenfunction
of . Note that the eigenfunction is almost constant on the two big chunks and changes rapidly along the “neck”
of the dumbbell. Cutting the manifold at the zero locus of f provides an approximation to the optimal clustering.
Clustering
Clustering is a special case of unsupervised learning where one wishes to partition the
data x1, . . . , xn into a ﬁnite number of classes according to some criterion of optimality.
Here we will only consider the case of two clusters, which already seems highly nontrivial.
As it turns out, the Laplacian and its eigenfunctions that have played a central role in our
development here, also provide us with a perspective on clustering.
We ﬁrst consider what a sensible clustering would be, if we already knew the data
manifold. An example of such clustering is given in ﬁgure 7. The manifold M (the socalled Calabi-Cheeger dumbbell) is partitioned in two parts M1 and M −M1 by the
membrane δM1. We use δ to denote the boundary. Intuitively, we want a surface with
small area, which separates M in two large chunks. A formalization of this notion is the
Cheeger constant (also known as the isoperimetric constant), which is essentially a measure
of “goodness” for the best possible clustering:
min (voln(M1), voln(M −M1))
While this quantity and the associated clustering are well-motivated and have a clear
geometrical intuition, they are quite difﬁcult to compute. To approximate them, one uses
an idea of Cheeger .10 The (slightly reformulated) observation made by Cheeger was
that if our manifold is nicely clustered, we can construct a function ˜f , for which
is quite small and such that f is perpendicular to the constant function. To construct this
SEMI-SUPERVISED LEARNING ON RIEMANNIAN MANIFOLDS
function is not hard. We put
vol(M −M1)
appropriately smoothing it at the boundary.
It is clear the
M ˜f = 0 and it can be shown for the details that
M ∥f ∥2 is closely related to the Cheeger constant.
On the other hand, the second (ﬁrst non-constant) eigenfunction of  is equal to
Therefore, at least heuristically, the ﬁrst nontrivial eigenfunction e1 and the clustering
function ˜f are close. We note that several upper and lower bounds for hM in terms of the
smallest nonzero eigenvalue of the Laplacian λ1 are known, e.g., > 0}
and M −M1 = {x|e1(x) ≤0}, cutting the manifold along the zero set of e1. Thus the ﬁrst
nontrivial eigenfunction can be interpreted as a clustering of the manifold.
The situation with graphs is quite similar. A sensible way to cluster the graph, i.e. to cut
the graph in two parts is again given by the Cheeger constant , for a
comprehensive treatment).
Let G be a connected weighted graph with the weight matrix W. By a slight abuse of
notation we will identify G with its set of vertices. Let G1 be a subset of the vertices of G.
We deﬁne the boundary δG1 as the set of vertices connected to both G1 and G −G1. Then
vol(δG1) can be deﬁned as the the amount of the “outward” ﬂow from the boundary11
vol(δG1) =
i∈G1, j∈G−G1
After these deﬁnitions the Cheeger constant is deﬁned in the exactly same way as for the
min (vol(G1), vol(G −G1))
Intuitively, we are trying to minimize the ﬂow between G1 and G −G1.
M. BELKIN AND P. NIYOGI
Suppose now, G1 and G −G1 realize this optimal partition for which vol(δ(G1)) is,
presumably, small.
Consider the following function f on the graph:
vol(G −G1)
It is clear that f ⊥1, where 1 = (1, . . . , 1). Recall that the graph Laplacian of G is deﬁned
as L = D −W, where W is the weight matrix and D is a diagonal matrix, Dii = 
It is not hard to verify that
(xi −x j)2Wi j =
vol(G −G1)
On the other hand,
vol(G −G1)
Noticing that
vol(G −G1)
2 min(vol(G1), vol(G −G1))
we obtain:
vol(G −G1)
vol(δG1) < 2hG
The quantity (
vol(G−G1))−1vol(δG1) was introduced as the Normalized Cut in Shi
and Malik in the context of image segmentation and provides a lower bound (and
an approximation) for the Cheeger constant.
Similarly to the manifold case, while the direct computation of either the Cheeger constant
or Normalized Cut is difﬁcult (NP-hard), the relaxed problem
˜f = argmin
is easily solved by ﬁnding the eigenvector of the normalized Laplacian (see the footnote at the beginning of Section 5.4) D−1/2LD−1/2 corresponding to the second smallest
SEMI-SUPERVISED LEARNING ON RIEMANNIAN MANIFOLDS
(ﬁrst nonzero) eigenvalue. That eigenvalue also provides a lower bound for the Cheeger
As in the manifold case, the graph is then clustered by taking G1 = {i | fi > 0},
G −G1 = {i | fi ≤0}. See Kannan, Vempala, and Vetta for some theoretical
guarantees for the quality of spectral clustering.
From the data, one may construct a locally connected weighted graph following the
procedure outlined in Section 4. The second eigenvector of the normalized Laplacian may
then be used for clustering (bisecting) the data. It is notable, that the resulting algorithm is
very similar to the algorithm proposed in Shi and Malik for image segmentation,
despite the quite different setting and different motivation of the authors.
Conclusions and further directions
We have shown that methods motivated by the geometry of manifolds can yield signiﬁcant
beneﬁts for partially labeled classiﬁcation. We believe that this is just a ﬁrst step towards
more systematically exploiting the geometric structure of the data as many crucial questions
still remain to be answered.
1. In this paper we have provided only a partial glimpse of results relating to the convergence
properties of our algorithm. It seems that under certain conditions convergence can be
demonstrated rigorously, however the precise connection between the parameters of the
manifold such as curvature and the nature of convergence are still unclear. We note that
the heat equation seems to play a crucial role in this context.
2. It would be very interesting to explore different bases for functions on the manifold.
There is no reason to believe that the Laplacian is the only or the most natural choice.
Note that there are a number of different bases for function approximation and regression
3. While the idea that natural data lie on manifolds has recently attracted considerable
attention, there still seems to be no convincing proof that such manifold structures are
actually present. While the results in this paper provide some indirect evidence for this,
it would be extremely interesting to develop methods to look for such structures. Even
the simplest questions such as practical methods for estimating the dimensionality seem
to be unresolved.
Acknowledgments
We are grateful to Yali Amit for a number of conversations and helpful suggestions over the
course of this work. We are also grateful to Dinoj Surendran for preprocessing the TIMIT
Database for our phonemic experiments and to Lehel Csat´o for helping with the ﬁgure 7.
1. There is an extensive literature on the connection between the geometric properties of the manifold and the
Laplace-Beltrami operator. See Rosenberg for an introduction to the subject.
M. BELKIN AND P. NIYOGI
2. In fact when M is a circle, we do get the Fourier series.
3. The assumption that the manifold is isometrically embedded in Rk is not necessary, but will simplify the
discussion.
4. Strictly speaking, the functions do not have to be inﬁnitely differentiable, but we prefer not to worry about
the exact differentiability conditions.
5. The alternative deﬁnition is the so-called normalized Laplacian ˜L = D−1
2 which has many nice properties
and in in some ways closer to the Laplace-Beltrami operator on the manifold. It is taken as the deﬁnition of
the graph Laplacian in Chung . Since we did not see much improvement in the experimental results,
we use L to simplify the exposition.
6. We use the ﬁrst 100 principal components of the set of all images to represent each image as a 100 dimensional
vector. This was done to accelerate ﬁnding the nearest neighbors, but turned out to have a pleasant side effect
of improving the baseline classiﬁcation accuracy, possibly by denoising the data.
7. For 60000 points we were unable to compute more than 1000 eigenvectors due to the memory limitations.
Therefore the actual number of eigenvectors never exceeds 1000. We suspect that computing more eigenvectors
would improve performance even further.
8. In the case of 2000 eigenvectors we take just 10 random splits since the computations are rather timeconsuming.
9. If the test set is included with the training set and is labeled in the “batch mode”, the error rate drops down to
the base line.
10. Interestingly, Cheeger was interested in the opposite problem of estimating the analytical quantity λ1 in terms
of the geometric invariants of the manifold.
11. It is interesting to note that the volume of a hypersurface B on a manifold can be deﬁned as
voln−1(B) = lim
voln(Gt(B))
where Gt is the geodesic ﬂow in the direction normal to B.