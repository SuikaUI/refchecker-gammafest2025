Journal of Statistical Software
June 2011, Volume 42, Issue 7.
 
Multivariate and Propensity Score Matching
Software with Automated Balance Optimization:
The Matching Package for R
Jasjeet S. Sekhon
UC Berkeley
Matching is an R package which provides functions for multivariate and propensity
score matching and for ﬁnding optimal covariate balance based on a genetic search algorithm. A variety of univariate and multivariate metrics to determine if balance actually
has been obtained are provided. The underlying matching algorithm is written in C++,
makes extensive use of system BLAS and scales eﬃciently with dataset size. The genetic
algorithm which ﬁnds optimal balance is parallelized and can make use of multiple CPUs
or a cluster of computers. A large number of options are provided which control exactly
how the matching is conducted and how balance is evaluated.
Keywords: propensity score matching, multivariate matching, genetic optimization, causal
inference, R.
1. Introduction
The R package Matching implements a variety of algorithms for multivariate matching including propensity score, Mahalanobis, inverse variance
and genetic matching (GenMatch). The last of these, genetic matching, is a method which
automatically ﬁnds the set of matches which minimize the discrepancy between the distribution of potential confounders in the treated and control groups—i.e., covariate balance
is maximized. The package enables a wide variety of matching options including matching
with or without replacement, bias adjustment, diﬀerent methods for handling ties, exact and
caliper matching, and a method for the user to ﬁne tune the matches via a general restriction
matrix. Variance estimators include the usual Neyman standard errors (which condition on
the matched data), Abadie and Imbens standard errors which account for the (asymp-
Matching: Multivariate Matching with Automated Balance Optimization in R
totic) variance induced by the matching procedure itself, and robust variances which do not
assume a homogeneous causal eﬀect. The Matching software package is available from the
Comprehensive R Archive Network at 
The package provides a set of functions to do the matching (Match) and to evaluate how good
covariate balance is before and after matching (MatchBalance). The GenMatch function ﬁnds
optimal balance using multivariate matching where a genetic search algorithm determines the
weight each covariate is given. Balance is determined by examining cumulative probability
distribution functions of a variety of standardized statistics. By default, these statistics include paired t tests, univariate and multivariate Kolmogorov-Smirnov (KS) tests. A variety of
descriptive statistics based on empirical-QQ plots are also oﬀered. The statistics are not used
to conduct formal hypothesis tests, because no measure of balance is a monotonic function
of bias in the estimand of interest and because we wish to maximize balance without limit.
GenMatch can maximize balance based on a variety of pre-deﬁned loss functions or any loss
function the user may wish to provide.
In the next section I brieﬂy oﬀer some background material on both the Rubin causal model
and matching methods. Section 3 provides an overview of the Matching package with examples. Section 4 concludes.
2. Background on matching
Matching has become an increasingly popular method of causal inference in many ﬁelds
including statistics , medicine , economics , political
science , sociology 
and even law . There is, however, no consensus on how exactly matching ought
to be done and how to measure the success of the matching procedure. A wide variety of
matching procedures have been proposed, and Matching implements many of them.
When using matching methods to estimate causal eﬀects, a central problem is deciding how
best to perform the matching.
Two common approaches are propensity score matching
 and multivariate matching based on Mahalanobis distance
 . Matching methods based on the propensity
score (estimated by logistic regression), Mahalanobis distance or a combination of the two have
appealing theoretical properties if covariates have ellipsoidal distributions—e.g., distributions
such as the normal or t. If the covariates are so distributed, these methods (more generally
aﬃnely invariant matching methods1) have the property of “equal percent bias reduction”
(EPBR) .2 This property is formally deﬁned in
Appendix A. When this property holds, matching will reduce bias in all linear combinations
of the covariates. If the EPBR property does not hold, then, in general, matching will increase
the bias of some linear functions of the covariates even if all univariate means are closer in
the matched data than the unmatched . Unfortunately, the EPBR property
1Aﬃne invariance means that the matching output is invariant to matching on X or an aﬃne transformation
2The EPBR results of Rubin and Thomas have been extended by Rubin and Stuart to the
case of discriminant mixtures of proportional ellipsoidally symmetric (DMPES) distributions. This extension
is important, but it is restricted to a limited set of mixtures.
Journal of Statistical Software
rarely holds with actual data.
A signiﬁcant shortcoming of common matching methods such as Mahalanobis distance and
propensity score matching is that they may (and in practice, frequently do) make balance
worse across measured potential confounders. These methods may make balance worse, in
practice, even if covariates are distributed ellipsoidally because in a given ﬁnite sample there
may be departures from an ellipsoidal distribution. Moreover, if covariates are neither ellipsoidally symmetric nor are mixtures of discriminant mixtures of proportional ellipsoidally
symmetric (DMPES) distributions, propensity score matching has good theoretical properties
if and only if the true propensity score model is known and the sample size is large.
These limitations often surprise applied researchers. Because of the limited theoretical properties for matching when the propensity score is not known, one approach is to algorithmically
impose additional properties, and this is the approach used by genetic matching.
Diamond and Sekhon and Sekhon and Grieve propose a matching algorithm,
genetic matching (GenMatch), that maximizes the balance of observed covariates between
treated and control groups. GenMatch is a generalization of propensity score and Mahalanobis distance matching, and it has been used by a variety of researchers . The algorithm uses a genetic algorithm
 to optimize balance as much as
possible given the data.
The method is nonparametric and does not depend on knowing
or estimating the propensity score, but the method is improved when a propensity score is
incorporated.
The core motivation for all matching methods is the Rubin causal model which I discuss next
followed by details on Mahalanobis, propensity score and genetic matching.
2.1. Rubin causal model
The Rubin causal model conceptualizes causal inference in terms of potential outcomes under
treatment and control, only one of which is observed for each unit . A causal eﬀect is deﬁned as the diﬀerence between
an observed outcome and its counterfactual.
Let Yi1 denote the potential outcome for unit i if the unit receives treatment, and let Yi0
denote the potential outcome for unit i in the control regime.
The treatment eﬀect for
observation i is deﬁned by τi = Yi1 −Yi0. Causal inference is a missing data problem because
Yi1 and Yi0 are never both observed. Let Ti be a treatment indicator equal to 1 when i is
in the treatment regime and 0 otherwise. The observed outcome for observation i is then
Yi = TiYi1 + (1 −Ti)Yi0.
In principle, if assignment to treatment is randomized, causal inference is straightforward
because the two groups are drawn from the same population by construction, and treatment
assignment is independent of all baseline variables. As the sample size grows, observed and
unobserved baseline variables are balanced across treatment and control groups with arbitrarily high probability, because treatment assignment is independent of Y0 and Y1—i.e., following
the notation of Dawid , {Yi0, Yi1 ⊥⊥Ti}. Hence, for j = 0, 1
E(Yij | Ti = 1) = E(Yij | Ti = 0) = E(Yi | Ti = j)
Matching: Multivariate Matching with Automated Balance Optimization in R
Therefore, the average treatment eﬀect (ATE) can be estimated by:
E(Yi1 | Ti = 1) −E(Yi0 | Ti = 0)
E(Yi | Ti = 1) −E(Yi | Ti = 0)
Equation 1 is estimable in an experimental setting because observations in treatment and
control groups are exchangeable.3 In the simplest experimental setup, individuals in both
groups are equally likely to receive the treatment, and hence assignment to treatment will not
be associated with the outcome. Even in an experimental setup, much can go wrong which
requires statistical correction .
In an observational setting, covariates are almost never balanced across treatment and control
groups because the two groups are not ordinarily drawn from the same population. Thus, a
common quantity of interest is the average treatment eﬀect for the treated (ATT):
τ | (T = 1)
E(Yi1 | Ti = 1) −E(Yi0 | Ti = 1).
Equation 2 cannot be directly estimated because Yi0 is not observed for the treated. Progress
can be made by assuming that selection into treatment depends on observable covariates X.
Following Rosenbaum and Rubin , one can assume that conditional on X, treatment assignment is unconfounded ({Y0, Y1 ⊥⊥T} | X) and that there is overlap: 0 < Pr(T = 1 | X) < 1.
Together, unconfoundedness and overlap constitute a property known as strong ignorability
of treatment assignment which is necessary for identifying the average treatment eﬀect. Heckman, Ichimura, Smith, and Todd show that for ATT, the unconfoundedness assumption
can be weakened to mean independence: E (Yij | Ti, Xi) = E (Yij | Xi).4 The overlap assumption for ATT only requires that the support of X for the treated be a subset of the support
of X for control observations.
Then, following Rubin we obtain
E(Yij | Xi, Ti = 1) = E(Yij | Xi, Ti = 0) = E(Yi | Xi, Ti = j).
By conditioning on observed covariates, Xi, treatment and control groups are exchangeable.
The average treatment eﬀect for the treated is estimated as
τ | (T = 1)
E {E(Yi | Xi, Ti = 1) −E(Yi | Xi, Ti = 0) | Ti = 1} ,
where the outer expectation is taken over the distribution of Xi | (Ti = 1) which is the
distribution of baseline variables in the treated group.
The most straightforward and nonparametric way to condition on X is to exactly match on
the covariates. This is an old approach going back to at least Fechner , the father of
psychophysics. This approach fails in ﬁnite samples if the dimensionality of X is large or if
X contains continuous covariates. Thus, in general, alternative methods must be used.
3It is standard practice to assume the Stable Unit Treatment Value assumption, also known as SUTVA
 . SUTVA requires that the treatment status of any unit be independent of potential
outcomes for all other units, and that treatment is deﬁned identically for all units.
4Also see Abadie and Imbens .
Journal of Statistical Software
2.2. Mahalanobis and propensity score matching
The most common method of multivariate matching is based on Mahalanobis distance . The Mahalanobis distance between any two column vectors is:
md(Xi, Xj)
(Xi −Xj)⊤S−1(Xi −Xj)
where S is the sample covariance matrix of X. To estimate ATT by matching with replacement, one matches each treated unit with the M closest control units, as deﬁned by this
distance measure, md().5 If X consists of more than one continuous variable, multivariate
matching estimates contain a bias term which does not asymptotically go to zero at rate √n
 .
An alternative way to condition on X is to match on the probability of assignment to treatment, known as the propensity score.6 As one’s sample size grows large, matching on the
propensity score produces balance on the vector of covariates X (Rosenbaum and Rubin
Let e(Xi) ≡Pr(Ti = 1 | Xi) = E(Ti | Xi), deﬁning e(Xi) to be the propensity score. Given
0 < Pr(Ti | Xi) < 1 and Pr(T1, T2, · · · TN | X1, X2, · · · XN) = ΠN
i=1e(Xi)Ti(1 −e(Xi))(1−Ti),
Rosenbaum and Rubin prove that
τ | (T = 1)
E {E(Yi | e(Xi), Ti = 1) −E(Yi | e(Xi), Ti = 0) | Ti = 1} ,
where the outer expectation is taken over the distribution of e(Xi) | (Ti = 1). Since the
propensity score is generally unknown, it must be estimated.
Propensity score matching involves matching each treated unit to the nearest control unit on
the unidimensional metric of the propensity score vector. If the propensity score is estimated
by logistic regression, as is typically the case, much is to be gained by matching not on the
predicted probabilities (bounded between zero and one) but on the linear predictor ˆµ ≡X ˆβ.
Matching on the linear predictor avoids compression of propensity scores near zero and one.
Moreover, the linear predictor is often more nearly normally distributed which is of some
importance given the EPBR results if the propensity score is matched on along with other
covariates.
Mahalanobis distance and propensity score matching can be combined in various ways . It is useful to combine the propensity score with
Mahalanobis distance matching because propensity score matching is particularly good at
minimizing the discrepancy along the propensity score and Mahalanobis distance is particularly good at minimizing the distance between individual coordinates of X (orthogonal to the
propensity score) .
5Alternatively one can do optimal full matching instead of the 1-to-N matching with replacement which I focus on in this article. This decision is a
separate one from the choice of a distance metric.
6The ﬁrst estimator of treatment eﬀects to be based on a weighted function of the probability of treatment
was the Horvitz-Thompson statistic .
Matching: Multivariate Matching with Automated Balance Optimization in R
2.3. Genetic matching
The idea underlying the GenMatch algorithm is that if Mahalanobis distance is not optimal for
achieving balance in a given dataset, one should be able to search over the space of distance
metrics and ﬁnd something better. One way of generalizing the Mahalanobis metric is to
include an additional weight matrix:
(Xi −Xj)⊤ S−1/2⊤WS−1/2(Xi −Xj)
where W is a k × k positive deﬁnite weight matrix and S1/2 is the Cholesky decomposition
of S which is the variance-covariance matrix of X.7
Note that if one has a good propensity score model, one should include it as one of the
covariates in GenMatch. If this is done, both propensity score matching and Mahalanobis
matching can be considered special limiting cases of GenMatch. If the propensity score contains
all of the relevant information in a given sample, the other variables will be given zero weight.8
And GenMatch will converge to Mahalanobis distance if that proves to be the appropriate
distance measure.
GenMatch is an aﬃnely invariant matching algorithm that uses the distance measure d(), in
which all elements of W are zero except down the main diagonal. The main diagonal consists
of k parameters which must be chosen. Note that if each of these k parameters are set equal
to 1, d() is the same as Mahalanobis distance.
The choice of setting the non-diagonal elements of W to zero is made for reasons of computational power alone. The optimization problem grows exponentially with the number of free
parameters. It is important that the problem be parameterized so as to limit the number of
parameters which must be estimated.
This leaves the problem of how to choose the free elements of W. Many loss criteria recommend themselves, and GenMatch provides a number the user can choose from via the fit.func
and loss options of GenMatch. By default, cumulative probability distribution functions of a
variety of standardized statistics are used as balance metrics and are optimized without limit.
The default standardized statistics are paired t tests and nonparametric KS tests.
The statistics are not used to conduct formal hypothesis tests, because no measure of balance
is a monotonic function of bias in the estimand of interest and because we wish to maximize
balance without limit. Descriptive measures of discrepancy generally ignore key information
related to bias which is captured by probability distribution functions of standardized test
statistics. For example, using several descriptive metrics, one is unable to recover reliably the
experimental benchmark in a testbed dataset for matching estimators . And these metrics, unlike those based on optimized distribution functions, perform
poorly in a series of Monte Carlo sampling experiments just as one would expect given their
properties. For details see Sekhon .
By default, GenMatch attempts to minimize a measure of the maximum observed discrepancy
between the matched treated and control covariates at every iteration of optimization. For
a given set of matches resulting from a given W, the loss is deﬁned as the minimum p value
7The Cholesky decomposition is parameterized such that S = LL⊤, S1/2 = L. In other words, L is a lower
triangular matrix with positive diagonal elements.
8Technically, the other variables will be given weights just large enough to ensure that the weight matrix is
positive deﬁnite.
Journal of Statistical Software
observed across a series of standardized statistics. The user may specify exactly what tests
are done via the BalanceMatrix option. Examples are oﬀered in Section 3.
Conceptually, the algorithm attempts to minimize the largest observed covariate discrepancy
at every step. This is accomplished by maximizing the smallest p value at each step.9 Because
GenMatch is minimizing the maximum discrepancy observed at each step, it is minimizing the
inﬁnity norm. This property holds even when, because of the distribution of X, the EPBR
property does not hold. Therefore, if an analyst is concerned that matching may increase
the bias in some linear combination of X even if the means are reduced, GenMatch allows
the analyst to put in the loss function all of the linear combinations of X which may be of
concern. Indeed, any nonlinear function of X can also be included in the loss function, which
would ensure that bias in some nonlinear functions of X is not made inordinately large by
The default GenMatch loss function does allow for imbalance in functions of X to worsen as
long as the maximum discrepancy is reduced. Hence, it is important that the maximum discrepancy be small—i.e., that the smallest p value be large. p values conventionally understood
to signal balance (e.g., 0.10), may be too low to produce reliable estimates. After GenMatch
optimization, the p values from these balance tests cannot be interpreted as true probabilities
because of standard pre-test problems, but they remain useful measures of balance. Also,
we are interested in maximizing the balance in the current sample so a hypothesis test for
balance is inappropriate.
The optimization problem described above is diﬃcult and irregular, and the genetic algorithm
implemented in the rgenoud package is used to conduct the
optimization. Details of the algorithm are provided in Sekhon and Mebane .
GenMatch is shown to have better properties than the usual alternative matching methods
both when the EPBR property holds and when it does not . Even when the EPBR property holds and the mapping from X to Y is linear, GenMatch has better eﬃciency—i.e., lower mean square error (MSE)—in ﬁnite samples.
When the EPBR property does not hold as it generally does not, GenMatch retains appealing properties and the diﬀerences in performance between GenMatch and the other matching
methods can become substantial both in terms of bias and MSE reduction. In short, at the
expense of computer time, GenMatch dominates the other matching methods in terms of
MSE when assumptions required for EPBR hold and, even more so, when they do not.
GenMatch is able to retain good properties even when EPBR does not hold because a set of
constraints is imposed by the loss function optimized by the genetic algorithm. The loss function depends on a large number of functions of covariate imbalance across matched treatment
and control groups. Given these measures, GenMatch will optimize covariate balance.
3. Package overview and examples
The three main functions in the package are Match, MatchBalance and GenMatch. The ﬁrst
function, Match, performs multivariate and propensity score matching. It is intended to be
9More precisely lexical optimization will be done: all of the balance statistics will be sorted from the most
discrepant to the least and weights will be picked which minimize the maximum discrepancy. If multiple sets
of weights result in the same maximum discrepancy, then the second largest discrepancy is examined to choose
the best weights. The processes continues iteratively until ties are broken.
Matching: Multivariate Matching with Automated Balance Optimization in R
used in conjunction with the MatchBalance function which checks if the results of Match have
actually achieved balance on a set of covariates. MatchBalance can also be used before any
matching to determine how balanced the raw data is. If one wants to do propensity score
matching, one should estimate the propensity model before calling Match, and then send
Match the propensity score to use. The GenMatch function can be used to automatically ﬁnd
balance by the use of a genetic search algorithm which determines the optimal weight to give
each covariate.
Next, I present a set of propensity score (pscore) models which perform better as adjustments
are made to them after the output of MatchBalance is examined. I then provide an example
using GenMatch.
3.1. Propensity score matching example
In order to do propensity score matching, the work ﬂow is to ﬁrst estimate a propensity score
using, for example, glm if one wants to estimate a propensity score using logistic regression. A
number of alternative methods of estimating the propensity score, such as General Additive
Models (GAMs), are possible. After the propensity score has been estimated, one calls Match
to perform the matching and MatchBalance to examine how well the matching procedure did
in producing balance. If the balance results printed by MatchBalance are not good enough,
one would go back and change either the propensity score model or some parameter of how
the matching is done—e.g., change from 1-to-3 matching to 1-to-1 matching.
The following example is adopted from the documentation of the Match function. The example
uses the LaLonde experimental data which is based on a nationwide job training
experiment. The observations are individuals, and the outcome of interest is real earnings in
1978. There are eight baseline variables age (age), years of education (educ), real earnings in
1974 (re74), real earnings in 1975 (re75), and a series of indicator variables. The indicator
variables are black (black), Hispanic (hisp), married (married) and lack of a high school
diploma (nodegr).
R> library("Matching")
R> data("lalonde")
R> attach(lalonde)
Save the outcome of interest in Y and the treatment indicator in Tr:
R> Y <- lalonde$re78
R> Tr <- lalonde$treat
We now estimate our ﬁrst propensity score model:
R> glm1 <- glm(Tr ~ age + educ + black + hisp + married + nodegr +
re74 + re75, family = binomial, data = lalonde)
Let us do one-to-one matching with replacement using our preliminary propensity score model
where the estimand is the average treatment eﬀect on the treated (ATT):
R> rr1 <- Match(Y = Y, Tr = Tr, X = glm1$fitted)
Journal of Statistical Software
None of the forgoing commands produce output. If we wanted to see the results from the call to
Match which would display the estimate and its standard error we could do summary(rr1), but
it is best to wait until we have achieved satisfactory balance before looking at the estimates.
To this end, Match does not even need to be provided with an outcome variable—i.e., Y—in
order to work. Matches can be found and balance evaluated without knowledge of Y. Indeed,
this is to be preferred so that the design stage of the observational study can be clearly
separated from the estimation stage as is the case with experiments.
In the example above, the call to glm estimates a simple propensity score model and the syntax
of this procedure is covered in the R documentation. Then a call to Match is made which relies
heavily on the function’s default behavior because only three options are explicitly provided:
a vector (Y) containing the outcome variable, a vector (Tr) containing the treatment status of
each observation—i.e., either a zero or one—and a matrix (X) containing the variables to be
matched on, which in this case is simply the propensity score. By default Match does 1-to-1
matching with replacement and estimates ATT. The estimand is chosen via the estimand
option, as in estimand="ATE" to estimate the average treatment eﬀect. The ratio of treated
to control observations is determined by the the M option and this ratio is by default set to 1.
And whether matching should be done with replacement is controlled by the logical argument
replace which defaults to TRUE for matching with replacement.
Ties are by default handled deterministically and this behavior
is controlled by the ties option.
By default ties = TRUE. If, for example, one treated
observation matches more than one control observation, the matched dataset will include the
multiple matched control observations and the matched data will be weighted to reﬂect the
multiple matches. The sum of the weighted observations will still equal the original number
of observations. If ties = FALSE, ties will be randomly broken. This in general is not a good
idea because the variance of Y will be underestimated. But if the dataset is large and there
are many ties between potential matches, setting ties = FALSE often results in signiﬁcantly
faster execution with negligible bias. Whether two potential matches are close enough to be
considered tied, is controlled by the distance.tolerance option.
With these defaults, the command
R> m1 = Match(Y = Y, Tr = Tr, X = glm1$fitted)
is equivalent to
R> m1 = Match(Y = Y, Tr = Tr, X = glm1$fitted, estimand = "ATT",
M = 1, ties = TRUE, replace = TRUE)
We generally want to measure balance for more functions of the data than we include in
our propensity score model. We can do this using the following call to the MatchBalance
function. Note that the function is asked to measure balance for many more functions of the
confounders than we included in the propensity score model.
R> MatchBalance(Tr ~ age + I(age^2) + educ + I(educ^2) + black + hisp +
married + nodegr + re74 + I(re74^2) + re75 + I(re75^2) + u74 + u75 +
I(re74 * re75) + I(age * nodegr) + I(educ * re74) + I(educ * re75),
match.out = rr1, nboots = 1000, data = lalonde)
Matching: Multivariate Matching with Automated Balance Optimization in R
The full output for this call to MatchBalance is presented in Appendix B. The formula used
in the call to MatchBalance does not estimate any model. The formula is simply an eﬃcient
way to use the R modeling language to list the variables we wish to obtain univariate balance
statistics on. The dependent variable in the formula is the treatment indicator.
The propensity score model is diﬀerent from the balance statistics which are requested from
MatchBalance. In general, one does not need to include all of the functions one wants to
test balance on in the propensity score model. Indeed, doing so sometimes results in worse
balance. Generally, one should request balance statistics on more higher-order terms and
interactions than were included in the propensity score used to conduct the matching itself.
Aside from the formula, three additional arguments were given to the MatchBalance call.
The match.out option is used to provide the output object from the previous call to Match.
If this object is provided, MatchBalance will provide balance statistics for both before and
after matching, otherwise balance statistics will only be provided for the unmatched raw
dataset. The nboots option determines the number of bootstrap samples to be run. If zero,
no bootstraps are done.
Bootstrapping is highly recommended because the bootstrapped
Kolmogorov-Smirnov test, unlike the standard test, provides correct coverage even when there
are point masses in the distributions being compared . At least 500 nboots
(preferably 1000) are recommended for publication quality p values. And ﬁnally, the data
argument expects a data frame which contains all of the variables in the formula. If a data
frame is not provided, the variables are obtained via lexical scoping.
For each term included into the modeling equation provided as the ﬁrst argument to
MatchBalance, detailed balance statistics are produced. Let’s ﬁrst consider the output for
the nodegr variable.
One could examine the long output from the call to MatchBalance
above where nodegr is labeled as ‘V8’ because it was the eighth variable listed in the formula
provided to MatchBalance. Alternatively, we could call MatchBalance with just nodegr:
R> MatchBalance(Tr ~ nodegr, match.out = rr1, nboots = 1000, data = lalonde)
***** (V1) nodegr *****
Before Matching
After Matching
mean treatment........
mean control..........
std mean diff.........
mean raw eQQ diff.....
raw eQQ diff.....
raw eQQ diff.....
mean eCDF diff........
eCDF diff........
eCDF diff........
var ratio (Tr/Co).....
T-test p-value........
There are two columns for each variable in the MatchBalance output.
The ﬁrst column
containing the pre-matching balance statistics and the second one the post-matching statistics.
Journal of Statistical Software
nodegr is an indicator variable for whether the individual in the worker training program
has a high school diploma. For such variables, the Kolmogorov-Smirnov test results are not
presented because they are the equivalent to the results from t tests.
Four diﬀerent sets of balance statistics are provided for each variable. The ﬁrst set consists of
the means for the treatment and control groups. The second set contains summary statistics
based on standardized empirical-QQ plots.
The mean, median and maximum diﬀerences
in the standardized empirical-QQ plots are provided. The third set of statistics consists of
summary statistics from the raw empirical-QQ plots so they are on the scale of the variable
in question. And the last set of statistics provides the variance ratio of treatment over control
(which should equal 1 if there is perfect balance), and the t test of diﬀerence of means (the
paired t test is provided post-matching). If they are calculated, the bootstrap Kolmogorov-
Smirnov test results are also provided here.
The balance results make clear that nodegr is poorly balanced both before and after matching.
Seventy-one percent of treatment observations have a high school diploma while seventy-seven
percent of control observations do. And this diﬀerence is highly signiﬁcant.
Next, let’s consider another variable, re74, which is real earnings of participants in 1974:
R> MatchBalance(Tr ~ re74, match.out = rr1, nboots = 1000, data = lalonde)
***** (V1) re74 *****
Before Matching
After Matching
mean treatment........
mean control..........
std mean diff.........
mean raw eQQ diff.....
raw eQQ diff.....
raw eQQ diff.....
mean eCDF diff........
eCDF diff........
eCDF diff........
var ratio (Tr/Co).....
T-test p-value........
KS Bootstrap p-value..
< 2.22e-16
KS Naive p-value......
KS Statistic..........
The balance of the re74 variable has been made worse by matching. Before matching, treatment and control observations were only 11.4 dollars apart and this diﬀerence was not significant as judged by either a t test for diﬀerence of means or by the Kolmogorov-Smirnov test
which tests for a signiﬁcant diﬀerence across the entire distribution. After matching, the mean
diﬀerence increases to almost 100 dollars, but it still not signiﬁcant. Unfortunately, the mean,
median and maximum diﬀerences in the empirical-QQ plots increase sharply. And consistent
with this, the KS tests shows a large and signiﬁcant diﬀerence between the distribution of
control and treatment observations.
Matching: Multivariate Matching with Automated Balance Optimization in R
GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG
Control Observations
Treatment Observations
Before Matching
GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG GGG
GGG GGGGG GG
Control Observations
Treatment Observations
After Matching
Figure 1: Empirical-QQ Plot of re74 before and after pscore matching.
Journal of Statistical Software
Figure 1 plots the empirical-QQ plot of this variable before and after matching, and it shows
that balance has been made worse by matching. The after matching portion of this ﬁgure
(without the captions) was generated by the following code:
R> qqplot(lalonde$re74[rr1$index.control], lalonde$re74[rr1$index.treated])
R> abline(coef = c(0, 1), col = 2)
The index.control and index.treated indices which are in the object returned by Match are
vectors containing the observation numbers from the original dataset for the treated (control)
observations in the matched dataset. Both indices together can be used to construct the
matched dataset. The matched dataset is also returned in the mdata object—see the Match
manual page for details.
This example shows that it is important to not simply look at diﬀerences of means. It is
important to examine more general summaries of the distributions. Both the descriptive eQQ
statistics and the KS test made clear that matching resulted in worse balance for this variable.
When faced with a propensity score which makes balance worse, it is sometimes possible
to learn from the balance output and improve the propensity score. However, because the
covariates are correlated with each other, it is diﬃcult to know exactly how one should change
the propensity score model. For example, the no highschool degree variable has signiﬁcant
imbalance both before and after matching. Should we interact it with other variables or do
something else? It may be the case that we should not change the speciﬁcation of nodegr,
but instead change the speciﬁcation of some other variable with which nodegr is correlated.
In this example, that turns out to work.
Consider the following propensity score model proposed by Dehejia and Wahba to be
used for the LaLonde data:
R> dw.pscore <- glm(Tr ~ age + I(age^2) + educ + I(educ^2) + black + hisp +
married + nodegr + re74 + I(re74^2) + re75 + I(re75^2) + u74 + u75,
family = binomial, data = lalonde)
R> rr.dw <- Match(Y = Y, Tr = Tr, X = dw.pscore$fitted)
This model adds second-order polynomials to the continuous variables we have: age, educ,
re74 and re75. And it adds indicator variables for whether income in 1974 and 1975 were
zero: u74, u75. Note that this pscore model does not do anything diﬀerent with nodegr than
the previous one we used.
The Dehejia and Wahba model does, however, perform signiﬁcantly better. See Appendix C
for the full output for the following call to MatchBalance:
R> MatchBalance(Tr ~ age + I(age^2) + educ + I(educ^2) + black + hisp +
married + nodegr + re74 + I(re74^2) + re75 + I(re75^2) + u74 + u75 +
I(re74 * re75) + I(age * nodegr) + I(educ * re74) + I(educ * re75),
data = lalonde, match.out = rr.dw, nboots = 1000)
To focus, for example, on a few variable, consider the balance of nodegr, re74 and re74^2:
R> MatchBalance(Tr ~ nodegr + re74 + I(re74^2), match.out = rr.dw,
nboots = 1000, data = lalonde)
Matching: Multivariate Matching with Automated Balance Optimization in R
***** (V1) nodegr *****
Before Matching
After Matching
mean treatment........
mean control..........
std mean diff.........
mean raw eQQ diff.....
raw eQQ diff.....
raw eQQ diff.....
mean eCDF diff........
eCDF diff........
eCDF diff........
var ratio (Tr/Co).....
T-test p-value........
***** (V2) re74 *****
Before Matching
After Matching
mean treatment........
mean control..........
std mean diff.........
mean raw eQQ diff.....
raw eQQ diff.....
raw eQQ diff.....
mean eCDF diff........
eCDF diff........
eCDF diff........
var ratio (Tr/Co).....
T-test p-value........
KS Bootstrap p-value..
KS Naive p-value......
KS Statistic..........
***** (V3) I(re74^2) *****
Before Matching
After Matching
mean treatment........
mean control..........
std mean diff.........
mean raw eQQ diff.....
raw eQQ diff.....
Journal of Statistical Software
raw eQQ diff.....
mean eCDF diff........
eCDF diff........
eCDF diff........
var ratio (Tr/Co).....
T-test p-value........
KS Bootstrap p-value..
KS Naive p-value......
KS Statistic..........
Before Matching Minimum p.value: 0.0020368
Variable Name(s): nodegr
Number(s): 1
After Matching Minimum p.value: 0.08604
Variable Name(s): I(re74^2)
Number(s): 3
The balance of the nodegr variable has signiﬁcantly improved from that of the unmatched
dataset. The diﬀerence has been shrunk to the point that the remaining imbalance in this
covariate is probably not a serious concern.
The balance in the income in 1974 is better than that produced by the previous pscore model,
but it is still worse than balance in the unmatched data. The means of the re74 variable across
treatment and control groups and the standardized mean, median and maximum diﬀerence in
the eQQ plots are increased by matching. Although the diﬀerences are not signiﬁcant, they
are if we examine the balance output for re74^2.
Note that the eQQ and KS test results are exactly the same for re74 and re74^2 as is to be
expected because these non-parametric tests depend on the ranks of the observations rather
than their precise values. However, the KS test is less sensitive to mean diﬀerences than the
t test. It is more sensitive than the t test to diﬀerences in the distributions beyond the ﬁrst
two moments. In this case, the t test p value for re74^2 is much lower than it is for re74:
0.086 versus 0.23.
Since the previous outcome is usually the most important confounder we need to worry about,
the remaining imbalance in this variable is of serious concern. And it is further troubling that
matching is making balance worse in this variable than doing nothing at all!
As this example hopefully demonstrates, moving back and forth from balance statistics to
changing the matching model is a tedious process. Fortunately, as described in Section 2.3,
the problem can be clearly posed as an optimization problem that can be algorithmically
3.2. Genetic matching
The GenMatch function can be used for our example problem, and it greatly improves balance
even over the Dehejia and Wahba propensity score model.
GenMatch can be used
with our without a propensity score model. In this example, we will not make use of any
Matching: Multivariate Matching with Automated Balance Optimization in R
propensity score model just to demonstrate that GenMatch can perform well even without a
human providing such a model. However, in general, inclusion of a good propensity score
model helps GenMatch.
R> X <- cbind(age, educ, black, hisp, married, nodegr, re74, re75, u74, u75)
R> BalanceMatrix <- cbind(age, I(age^2), educ, I(educ^2), black, hisp,
married, nodegr, re74, I(re74^2), re75, I(re75^2), u74, u75,
I(re74 * re75), I(age * nodegr), I(educ * re74), I(educ * re75))
R> gen1 <- GenMatch(Tr = Tr, X = X, BalanceMatrix = BalanceMatrix,
pop.size = 1000)
GenMatch takes four key arguments. The ﬁrst two, Tr and X, are just the same as those of the
Match function: the ﬁrst is a vector for the treatment indicator and the second a matrix which
contains the covariates which we wish to match on. The third key argument, BalanceMatrix,
is a matrix containing the variables we wish to achieve balance on. This is by default equal
to X, but it can be a matrix which contains more or less variables than X or variables which
are transformed in various ways. It should generally contain the variables and the function
of these variables that we wish to balance. In this example, I have made BalanceMatrix
contain the same terms we had MatchBalance test balance for, and this, in general, is good
practice. If you care about balance for a given function of the covariates, you should put it
in BalanceMatrix just like how you should put it into the equation in MatchBalance.
The pop.size argument is important and greatly inﬂuences how long the function takes to
run. This argument controls the population size used by the evolutionary algorithm–i.e., it
is the number of individuals genoud uses to solve the optimization problem. This argument
is also the number of random trail solutions which are tried at the beginning of the search
process. The theorems proving that genetic algorithms ﬁnd good solutions are asymptotic in
population size. Therefore, it is important that this value not be small . On the other hand, computational time is ﬁnite so obvious trade-oﬀs must be
GenMatch has a large number of other options which are detailed in its help page. The options
controlling features of the matching itself, such as whether to match with replacement, are the
same as those of the Match function. But many other options are speciﬁc to GenMatch because
they control the optimization process. The most important of these aside from pop.size, are
wait.generations and max.generations.
In order to obtain balance statistics, we can simply do the following with the output object
(gen1) returned by the call to GenMatch above:
R> mgen1 <- Match(Y = Y, Tr = Tr, X = X, Weight.matrix = gen1)
R> MatchBalance(Tr ~ age + I(age^2) + educ + I(educ^2) + black + hisp +
married + nodegr + re74 + I(re74^2) + re75 + I(re75^2) + u74 + u75 +
I(re74 * re75) + I(age * nodegr) + I(educ * re74) + I(educ * re75),
data = lalonde, match.out = mgen1, nboots = 1000)
The balance results from this GenMatch run are excellent. The full output from this call to
MatchBalance is include in Appendix D. Note that GenMatch is a stochastic algorithm so
your results may not be exactly the same.
Journal of Statistical Software
The balance is now excellent for all variables. As shown in Appendix D, the smallest p value
across all of the variables tested in MatchBalance is 0.408 (for I(educ * re74)) compared
with 0.086 for the Dehejia and Wahba propensity score model (for re74^2) and the prematching value of 0.002 (for nodegr).
As for our propensity score examples, the balance output for nodegr, re74 and re74^2 are
presented for close examination:
R> MatchBalance(Tr ~ nodegr + re74 + I(re74^2), match.out = mgen1,
nboots = 1000, data = lalonde)
***** (V1) nodegr *****
Before Matching
After Matching
mean treatment........
mean control..........
std mean diff.........
mean raw eQQ diff.....
raw eQQ diff.....
raw eQQ diff.....
mean eCDF diff........
eCDF diff........
eCDF diff........
var ratio (Tr/Co).....
T-test p-value........
***** (V2) re74 *****
Before Matching
After Matching
mean treatment........
mean control..........
std mean diff.........
mean raw eQQ diff.....
raw eQQ diff.....
raw eQQ diff.....
mean eCDF diff........
eCDF diff........
eCDF diff........
var ratio (Tr/Co).....
T-test p-value........
KS Bootstrap p-value..
KS Naive p-value......
KS Statistic..........
Matching: Multivariate Matching with Automated Balance Optimization in R
***** (V3) I(re74^2) *****
Before Matching
After Matching
mean treatment........
mean control..........
std mean diff.........
mean raw eQQ diff.....
raw eQQ diff.....
raw eQQ diff.....
mean eCDF diff........
eCDF diff........
eCDF diff........
var ratio (Tr/Co).....
T-test p-value........
KS Bootstrap p-value..
KS Naive p-value......
KS Statistic..........
Before Matching Minimum p.value: 0.0020368
Variable Name(s): nodegr
Number(s): 1
After Matching Minimum p.value: 0.4652
Variable Name(s): I(re74^2)
Number(s): 3
The empirical-QQ plot for re74, as shown in Figure 2, now looks good, especially when
compared with Figure 1. Balance is now improved, and not made worse, by matching.
Now that we have achieved excellent balance, we can examine our estimate of the treatment
eﬀect and its standard error.
We can do this by simply running summary on the object
returned by the Match function:
R> summary(mgen1)
Estimate...
AI SE......
T-stat.....
p.val......
Original number of observations..............
Original number of treated obs...............
Matched number of observations...............
Matched number of observations
(unweighted).
Journal of Statistical Software
GGGGGGGGGGGGGG GGGGGG GGGG G
GGGGGGGG G
Control Observations
Treatment Observations
Figure 2: Empirical-QQ plot of re74 using GenMatch.
The estimate of the treatment eﬀect for the treated is $1,671.20 with a standard error of
889.63. By default, the Abadie-Imbens (AI) standard error is printed . In order to also obtain the usual Neyman standard error, one may call the summary
function with the full=TRUE option.
The summary function also provides the number of observations in total (445), the number of
treated observations (185), the number of matched pairs that were produced when the ties are
properly weighted (185), and the number of matched pairs without using the weights which
adjust for ties (268).
3.3. Parallel and cluster processing
GenMatch is a computationally intensive algorithm because it constructs matched datasets
for each trail set of covariate weights.
Fortunately, as with most genetic algorithms, the
algorithm easily parallelizes. This functionality has been built directly in the rgenoud package
and be readily accessed by GenMatch. The parallelization can be used for either multiple
CPU computers or a cluster of computers, and makes use of R’s snow (simple network of
workstations) package . Simulations to estimate how well the
parallel algorithm scales with multiple CPUs are provided below. On a single computer with
multiple CPUs, the proportion of time saved is almost linear in the number of CPUs if the
dataset size is large. For a cluster of separate computers, the algorithm is signiﬁcantly faster
Matching: Multivariate Matching with Automated Balance Optimization in R
for every extra node which is added, but the time savings are signiﬁcantly less than linear.
The exact amount of time saved depends on network latency and a host of other factors.
Two GenMatch options control the parallel processing: cluster and balance. The cluster
option can either be an object of the ‘cluster’ class returned by one of the makeCluster
commands in the snow package or a vector of machine names so that GenMatch can setup
the cluster automatically via secure-shell (SSH). If it is the latter, the vector passed to the
cluster option should look like the following:
R> c("localhost", "localhost", "musil", "musil", "deckard")
This vector would create a cluster with four nodes: two on the localhost another on ‘deckard’
and two on the machine named ‘musil’. Two nodes on a given machine make sense if the
machine has two or more chips/cores.
GenMatch will setup a SOCK cluster by a call to
makeSOCKcluster. This will require the user to type in her password for each node as the
cluster is by default created via SSH. One can add on user names to the machine name if
it diﬀers from the current shell: username@musil. Other cluster types, such as PVM and
MPI, which do not require passwords, can be created by directly calling makeCluster, and
then passing the returned cluster object to GenMatch. For example, one can manually setup
a cluster with a direct call to makeCluster as follows:
R> library("snow")
R> library("Matching")
R> data("lalonde")
R> attach(lalonde)
R> cl <- makeCluster(c("musil", "quetelet", "quetelet"), type = "SOCK")
R> X <- cbind(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74)
R> genout <- GenMatch(Tr = treat, X = X, cluster = cl)
R> stopCluster(cl)
Note the stopCluster(cl) command which is needed because we setup the cluster output
of GenMatch. So, we much manually shut the connections down.
The second GenMatch option which controls the behavior of parallel processing is the balance
This is a logical ﬂag which controls if load balancing is done across the cluster.
Load balancing can result in better cluster utilization; however, increased communication
can reduce performance.
This options is best used if each individual call to Match takes
at least several minutes to calculate or if the nodes in the cluster vary signiﬁcantly in their
performance.
Designing parallel software applications is diﬃcult. A lot of work and trail-and-error has gone
into writing the C++ functions which GenMatch relies upon to ensure that they are reliable
and fast when run either serially or in parallel. Parallel execution is especially tricky because
an algorithm which may be fast in serial mode can cause unexpected bottlenecks when run
in parallel (such as a cache-bottleneck when executing SSE3 instructions via BLAS).
We now explore how well GenMatch scales with additional CPUs by using the following benchmark code:
R> library("Matching")
R> data("lalonde")
Journal of Statistical Software
1780 Observations
run time (seconds)
x CPU/1 CPU run time
1335 Observations
run time (seconds)
x CPU/1 CPU run time
890 Observations
run time (seconds)
x CPU/1 CPU run time
Table 1: Using multiple computer chips to run GenMatch.
R> attach(lalonde)
R> X <- cbind(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74)
R> Xbig <- rbind(X, X, X, X)
R> Ybig <- c(treat, treat, treat, treat)
R> GenMatch(Tr = Ybig, X = Xbig, BalanceMatrix = Xbig, estimand = "ATE",
M = 1, pop.size = 1000, max.generations = 10, wait.generations = 1,
int.seed = 3818, unif.seed = 3527, cluster = c("localhost",
"localhost", "localhost", "localhost"))
This example makes use of four computer chips: note the four calls to localhost.
dataset is replicated four times (e.g., Xbig and Ybig) to obtain 1780 observations. And smaller
datasets are created by not replicating the observations as often. The options int.seed and
unif.seed set the random number seeds in order to ensure replication. These options are
passed onto the genoud function from the rgenoud package.
Please see that package for
Table 1 presents the average run times of this code as it is run on one to four CPUs and on
various dataset sizes.10
GenMatch scales more eﬃciently across computer chips as the dataset size becomes larger.
With 1780 observations, using four computer chips results in a run time which is 29% of
the single chip run time. This is a good increase in performance given that if parallelization
were as eﬃcient as possible, using four chips would result in 25% of the run time as that
of using a single chip. Of course, perfect parallelization is not possible given the overhead
involved in setting up parallel computations.11 Note that that scaling from one to two CPUs
10There is no signiﬁcant diﬀerence in run times across diﬀerent invocations of the same commands so no
variance estimates are presented, just average run times.
A four core Xeon processor (5150 @ 2.66GHz)
computer running 64-bit Linux (Ubuntu Dapper) was used, and all extraneous daemons were shutdown.
11The eﬃciency of this parallelization is more impressive given that the test runs were run on a four core
Xeon processor (5150) which is not really a four core chip.
This chip actually consists of two (dual-core)
Woodcrest chips. The two chips have no way to communicate directly with each other. All communications
between them have to go through a shared front-side bus with the memory controller hub, or north bridge.
And each chip independently accesses the cache coherency scheme.
Matching: Multivariate Matching with Automated Balance Optimization in R
is closer to the theoretical eﬃciency bound (1.08 = 0.54/0.5) then scaling from one to four
chips (1.16 = 0.29/0.25). This may be due to the issue pointed out in Footnote 11.
With 890 observations, using four CPUs takes 36% of the run time as only using one CPU.
This is a signiﬁcantly smaller eﬃciency gain than that achieved with the dataset with 1780
observations.
It is clear from Table 1 that the computational time it takes to execute a matching algorithm
does not increase linearly with sample size. The computational time increases as a polynomial
of the sample size, and the average asymptotic order of the Match function is approximately
O(N2)log(N).12 The run times in Table 1 are generally consistent with this. Although the
Match function increases in polynomial time, the problem which GenMatch attempts to solve
(that of ﬁnding the best distance metric) increases exponentially in sample size, just like the
traveling salesman problem. That is, the set of possible matched datasets grows exponentially
with sample size.
4. Conclusion
The functions in Matching have many more options than can be reviewed in this brief paper.
For additional details see the manual pages for the functions included in the R package. The
Matching package includes four functions in addition to Match, GenMatch, and MatchBalance:
Matchby (for large datasets), qqstats (descriptive eQQ statistics), ks.boot (bootstrap version of ks.test) and balanceUV (univariate balance statistics).
A great deal of eﬀort has been made in order to ensure that the matching functions are as
fast as possible. The computationally intensive functions are written in C++ which make
extensive use of the BLAS libraries, and GenMatch can be used with multiple computers,
CPUs or cores to perform parallel computations. The C++ functions have been written so
that the GNU g++ compiler does a good job of optimizing them. Indeed, compiling the
Matching package with the Intel C++ compiler does not result in faster code. This is unusual
with ﬂoating point code, and is the result of carefully writing code so that the GNU compiler
is able to optimize it aggressively. Moreover, the Matchby function has been tuned to work
well with large datasets.
After intensive benchmarking and instrumenting the code, it was determined that performance
on OS X was seriously limited because the default OS X memory allocator is not as eﬃcient as
Lea ’s malloc given the frequent memory allocations made by the matching code. The
matching algorithm was rewritten in order to be more eﬃcient with memory on all platforms,
and on OS X, Matching is compiled against Lea’s malloc which is something more packages
for R may wish to do. For details see Sekhon .
The literature on matching methods is developing quickly with new innovations being made
by a variety of researchers in ﬁelds ranging from economics, epidemiology and political science
to sociology and statistics. Hence, new options are being added frequently.
12The precise asymptotic order is diﬃcult to calculate because assumptions have to be made about various
features of the data such as the proportion of ties.
Journal of Statistical Software