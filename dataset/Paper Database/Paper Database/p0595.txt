Training Region-based Object Detectors with Online Hard Example Mining
Abhinav Shrivastava1
Abhinav Gupta1
Ross Girshick2
1Carnegie Mellon University
2Facebook AI Research
{ashrivas,abhinavg}@cs.cmu.edu
 
The ﬁeld of object detection has made signiﬁcant advances riding on the wave of region-based ConvNets, but
their training procedure still includes many heuristics and
hyperparameters that are costly to tune. We present a simple yet surprisingly effective online hard example mining
(OHEM) algorithm for training region-based ConvNet detectors. Our motivation is the same as it has always been –
detection datasets contain an overwhelming number of easy
examples and a small number of hard examples. Automatic
selection of these hard examples can make training more
effective and efﬁcient. OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use. But more importantly, it yields consistent and signiﬁcant boosts in detection performance on
benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness increases as datasets become larger and more difﬁcult,
as demonstrated by the results on the MS COCO dataset.
Moreover, combined with complementary advances in the
ﬁeld, OHEM leads to state-of-the-art results of 78.9% and
76.3% mAP on PASCAL VOC 2007 and 2012 respectively.
1. Introduction
Image classiﬁcation and object detection are two fundamental computer vision tasks. Object detectors are often
trained through a reduction that converts object detection
into an image classiﬁcation problem.
This reduction introduces a new challenge that is not found in natural image classiﬁcation tasks: the training set is distinguished by
a large imbalance between the number of annotated objects and the number of background examples (image regions not belonging to any object class of interest). In the
case of sliding-window object detectors, such as the deformable parts model (DPM) , this imbalance may be
as extreme as 100,000 background examples to every one
object. The recent trend towards object-proposal-based detectors mitigates this issue to an extent, but the imbalance ratio may still be high (e.g., 70:1). This challenge
opens space for learning techniques that cope with imbalance and yield faster training, higher accuracy, or both.
Unsurprisingly, this is not a new challenge and a standard solution, originally called bootstrapping (and now often called hard negative mining), has existed for at least 20
years. Bootstrapping was introduced in the work of Sung
and Poggio in the mid-1990’s (if not earlier) for training face detection models. Their key idea was to gradually grow, or bootstrap, the set of background examples by
selecting those examples for which the detector triggers a
false alarm. This strategy leads to an iterative training algorithm that alternates between updating the detection model
given the current set of examples, and then using the updated model to ﬁnd new false positives to add to the bootstrapped training set.
The process typically commences
with a training set consisting of all object examples and a
small, random set of background examples.
Bootstrapping has seen widespread use in the intervening decades of object detection research. Dalal and Triggs
 used it when training SVMs for pedestrian detection.
Felzenszwalb et al. later proved that a form of bootstrapping for SVMs converges to the global optimal solution deﬁned on the entire dataset. Their algorithm is often referred to as hard negative mining and is frequently
used when training SVMs for object detection .
Bootstrapping was also successfully applied to a variety of
other learning models, including shallow neural networks
 and boosted decision trees . Even modern detection methods based on deep convolutional neural networks
(ConvNets) , such as R-CNN and SPPnet ,
still employ SVMs trained with hard negative mining.
It may seem odd then that the current state-of-the-art object detectors, embodied by Fast R-CNN and its descendants , do not use bootstrapping. The underlying
reason is a technical difﬁculty brought on by the shift towards purely online learning algorithms, particularly in the
context of deep ConvNets trained with stochastic gradient
descent (SGD) on millions of examples. Bootstrapping, and
its variants in the literature, rely on the aforementioned alternation template: (a) for some period of time a ﬁxed model
is used to ﬁnd new examples to add to the active training set;
(b) then, for some period of time the model is trained on the
 
ﬁxed active training set. Training deep ConvNet detectors
with SGD typically requires hundreds of thousands of SGD
steps and freezing the model for even a few iterations at a
time would dramatically slow progress. What is needed,
instead, is a purely online form of hard example selection.
In this paper, we propose a novel bootstrapping technique called online hard example mining1 (OHEM) for
training state-of-the-art detection models based on deep
ConvNets. The algorithm is a simple modiﬁcation to SGD
in which training examples are sampled according to a
non-uniform, non-stationary distribution that depends on
the current loss of each example under consideration. The
method takes advantage of detection-speciﬁc problem structure in which each SGD mini-batch consists of only one or
two images, but thousands of candidate examples. The candidate examples are subsampled according to a distribution
that favors diverse, high loss instances. Gradient computation (backpropagation) is still efﬁcient because it only uses a
small subset of all candidates. We apply OHEM to the standard Fast R-CNN detection method and show three beneﬁts
compared to the baseline training algorithm:
• It removes the need for several heuristics and hyperparameters commonly used in region-based ConvNets.
• It yields a consistent and signiﬁcant boosts in mean
average precision.
• Its effectiveness increases as the training set becomes
larger and more difﬁcult, as demonstrated by results on
the MS COCO dataset.
Moreover, the gains from OHEM are complementary
to recent improvements in object detection, such as multiscale testing and iterative bounding-box regression
 . Combined with these tricks, OHEM gives state-ofthe-art results of 78.9% and 76.3% mAP on PASCAL VOC
2007 and 2012, respectively.
2. Related work
Object detection is one of the oldest and most fundamental problems in computer vision. The idea of dataset bootstrapping , typically called hard negative mining in
recent work , appears in the training of most successful
object detectors . Many of
these approaches use SVMs as the detection scoring function, even after training a deep convolutional neural network
(ConvNet) for feature extraction. One notable exception is the Fast R-CNN detector and its descendants,
such as Faster R-CNN . Since these models do not use
SVMs, and are trained purely online with SGD, existing
1We use the term hard example mining, rather than hard negative mining, because our method is applied in a multi-class setting to all classes,
not just a “negative” class.
hard example mining techniques cannot be immediately applied. This work addresses that problem by introducing an
online hard example mining algorithm that improves optimization and detection accuracy. We brieﬂy review hard
example mining, modern ConvNet-based object detection,
and relationships to concurrent works using hard example
selection for training deep networks.
Hard example mining.
There are two hard example mining algorithms in common use. The ﬁrst is used when optimizing SVMs. In this case, the training algorithm maintains
a working set of examples and alternates between training
an SVM to convergence on the working set, and updating
the working set by removing some examples and adding
others according to a speciﬁc rule . The rule removes
examples that are “easy” in the sense that they are correctly
classiﬁed beyond the current model’s margin. Conversely,
the rule adds new examples that are hard in the sense that
they violate the current model’s margin. Applying this rule
leads to the global SVM solution. Importantly, the working
set is usually a small subset of the entire training set.
The second method is used for non-SVMs and has been
applied to a variety of models including shallow neural networks and boosted decision trees . This algorithm
usually starts with a dataset of positive examples and a random set of negative examples. The machine learning model
is then trained to convergence on that dataset and subsequently applied to a larger dataset to harvest false positives.
The false positives are then added to the training set and
then the model is trained again. This process is usually iterated only once and does not have any convergence proofs.
ConvNet-based object detection.
In the last three years
signiﬁcant gains have been made in object detection. These
improvements were made possible by the successful application of deep ConvNets to ImageNet classiﬁcation .
The R-CNN and OverFeat detectors lead this wave
with impressive results on PASCAL VOC and ImageNet detection. OverFeat is based on the sliding-window
detection method, which is perhaps the most intuitive and
oldest search method for detection. R-CNN, in contrast,
uses region proposals , a method
that was made popular by the selective search algorithm
 . Since R-CNN, there has been rapid progress in regionbased ConvNets, including SPPnet , MR-CNN ,
and Fast R-CNN , which our work builds on.
Hard example selection in deep learning.
There is recent work concurrent to our own that selects
hard examples for training deep networks. Similar to our
approach, all these methods base their selection on the current loss for each datapoint. independently selects hard
positive and negative example from a larger set of random
examples based on their loss to learn image descriptors.
Convolution Feature Maps
Last Conv.
Feature Map
Mini-batch
Selective-Search
|Rsel| = Batch Size
RoI Pooling
Classification
For each R𝐬𝐞𝐥
Convolutional Network
RoI Network
Figure 1: Architecture of the Fast R-CNN approach (see Section 3 for details).
Given a positive pair of patches, ﬁnds hard negative
patches from a large set using triplet loss. Akin to our approach, investigates online selection of hard examples
for mini-batch SGD methods. Their selection is also based
on loss, but the focus is on ConvNets for image classiﬁcation. Complementary to , we focus on online hard example selection strategy for region-based object detectors.
3. Overview of Fast R-CNN
We ﬁrst summarize the Fast R-CNN (FRCN) framework. FRCN takes as input an image and a set of object proposal regions of interest (RoIs). The FRCN network itself
can be divided into two sequential parts: a convolutional
(conv) network with several convolution and max-pooling
layers (Figure 1, “Convolutional Network”); and an RoI
network with an RoI-pooling layer, several fully-connected
(fc) layers and two loss layers (Figure 1, “RoI Network”).
During inference, the conv network is applied to the
given image to produce a conv feature map, size of which
depends on the input image dimensions. Then, for each object proposal, the RoI-pooling layer projects the proposal
onto the conv feature map and extracts a ﬁxed-length feature vector. Each feature vector is fed into the fc layers,
which ﬁnally give two outputs: (1) a softmax probability
distribution over the object classes and background; and (2)
regressed coordinates for bounding-box relocalization.
There are several reasons for choosing FRCN as our base
object detector, apart from it being a fast end-to-end system.
Firstly, the basic two network setup (conv and RoI) is also
used by other recent detectors like SPPnet and MR-CNN;
therefore, our proposed algorithm is more broadly applicable. Secondly, though the basic setup is similar, FRCN also
allows for training the entire conv network, as opposed to
both SPPnet and MR-CNN which keep the conv network
ﬁxed. And ﬁnally, both SPPnet and MR-CNN require features from the RoI network to be cached for training a separate SVM classiﬁer (using hard negative mining). FRCN
uses the RoI network itself to train the desired classiﬁers. In
fact, shows that in the uniﬁed system using the SVM
classiﬁers at later stages was unnecessary.
3.1. Training
Like most deep networks, FRCN is trained using
stochastic gradient descent (SGD). The loss per example
RoI is the sum of a classiﬁcation log loss that encourages predicting the correct object (or background) label and
a localization loss that encourages predicting an accurate
bounding box (see for details).
To share conv network computation between RoIs, SGD
mini-batches are created hierarchically.
For each minibatch, N images are ﬁrst sampled from the dataset, and then
B/N RoIs are sampled from each image. Setting N = 2
and B = 128 works well in practice . The RoI sampling procedure uses several heuristics, which we describe
brieﬂy below. One contribution of this paper is to eliminate
some of these heuristics and their hyperparameters.
Foreground RoIs.
For an example RoI to be labeled as
foreground (fg), its intersection over union (IoU) overlap
with a ground-truth bounding box should be at least 0.5.
This is a fairly standard design choice, in part inspired by
the evaluation protocol of the PASCAL VOC object detection benchmark. The same criterion is used in the SVM
hard mining procedures of R-CNN, SPPnet, and MR-CNN.
We use the same setting.
Background RoIs.
A region is labeled background (bg)
if its maximum IoU with ground truth is in the interval
[bg lo, 0.5). A lower threshold of bg lo = 0.1 is used
by both FRCN and SPPnet, and is hypothesized in to
crudely approximate hard negative mining; the assumption
is that regions with some overlap with the ground truth are
more likely to be the confusing or hard ones. We show in
Section 5.4 that although this heuristic helps convergence
and detection accuracy, it is suboptimal because it ignores
some infrequent, but important, difﬁcult background regions. Our method removes the bg lo threshold.
Balancing fg-bg RoIs:
To handle the data imbalance described in Section 1, designed heuristics to rebalance
the foreground-to-background ratio in each mini-batch to a
target of 1 : 3 by undersampling the background patches
at random, thus ensuring that 25% of a mini-batch is fg
RoIs. We found that this is an important design decision
for the training FRCN. Removing this ratio (i.e. randomly
sampling RoIs), or increasing it, decreases accuracy by ∼3
points mAP. With our proposed method, we can remove this
ratio hyperparameter with no ill effect.
4. Our approach
We propose a simple yet effective online hard example
mining algorithm for training Fast R-CNN (or any Fast R-
CNN style object detector). We argue that the current way
of creating mini-batches for SGD (Section 3.1) is inefﬁcient and suboptimal, and we demonstrate that our approach
leads to better training (lower training loss) and higher testing performance (mAP).
4.1. Online hard example mining
Recall the alternating steps that deﬁne a hard example
mining algorithm: (a) for some period of time a ﬁxed model
is used to ﬁnd new examples to add to the active training set;
(b) then, for some period of time the model is trained on the
ﬁxed active training set. In the context of SVM-based object
detectors, such as the SVMs trained in R-CNN or SPPnet,
step (a) inspects a variable number of images (often 10’s or
100’s) until the active training set reaches a threshold size,
and then in step (b) the SVM is trained to convergence on
the active training set. This process repeats until the active
training set contains all support vectors. Applying an analogous strategy to FRCN ConvNet training slows learning
because no model updates are made while selecting examples from the 10’s or 100’s of images.
Our main observation is that these alternating steps can
be combined with how FRCN is trained using online SGD.
The key is that although each SGD iteration samples only a
small number of images, each image contains thousands of
example RoIs from which we can select the hard examples
rather than a heuristically sampled subset. This strategy ﬁts
the alternation template to SGD by “freezing” the model for
only one mini-batch. Thus the model is updated exactly as
frequently as with the baseline SGD approach and therefore
learning is not delayed.
More speciﬁcally, the online hard example mining algorithm (OHEM) proceeds as follows. For an input image at
SGD iteration t, we ﬁrst compute a conv feature map using
the conv network. Then the RoI network uses this feature
map and the all the input RoIs (R), instead of a sampled
mini-batch , to do a forward pass. Recall that this step
only involves RoI pooling, a few fc layers, and loss computation for each RoI. The loss represents how well the current
network performs on each RoI. Hard examples are selected
by sorting the input RoIs by loss and taking the B/N examples for which the current network performs worst. Most
of the forward computation is shared between RoIs via the
conv feature map, so the extra computation needed to forward all RoIs is relatively small. Moreover, because only a
small number of RoIs are selected for updating the model,
the backward pass is no more expensive than before.
However, there is a small caveat: co-located RoIs with
high overlap are likely to have correlated losses. Moreover,
these overlapping RoIs can project onto the same region in
the conv feature map, because of resolution disparity, thus
leading to loss double counting. To deal with these redundant and correlated regions, we use standard non-maximum
suppression (NMS) to perform deduplication (the implementation from ). Given a list of RoIs and their losses,
NMS works by iteratively selecting the RoI with the highest loss, and then removing all lower loss RoIs that have
high overlap with the selected region. We use a relaxed IoU
threshold of 0.7 to suppress only highly overlapping RoIs.
We note that the procedure described above does not
need a fg-bg ratio for data balancing. If any class were
neglected, its loss would increase until it has a high probability of being sampled. There can be images where the fg
RoIs are easy (e.g. canonical view of a car), so the network
is free to use only bg regions in a mini-batch; and viceversa when bg is trivial (e.g. sky, grass etc.), the mini-batch
can be entirely fg regions.
4.2. Implementation details
There are many ways to implement OHEM in the FRCN
detector, each with different trade-offs. An obvious way
is to modify the loss layers to do the hard example selection. The loss layer can compute loss for all RoIs, sort them
based on this loss to select hard RoIs, and ﬁnally set the
loss of all non-hard RoIs to 0. Though straightforward,
this implementation is inefﬁcient as the RoI network still
allocates memory and performs backward pass for all RoIs,
even though most RoIs have 0 loss and hence no gradient
updates (a limitation of current deep learning toolboxes).
To overcome this, we propose the architecture presented
in Figure 2. Our implementation maintains two copies of
the RoI network, one of which is readonly. This implies
that the readonly RoI network (Figure 2(a)) allocates memory only for forward pass of all RoIs as opposed to the standard RoI network, which allocates memory for both forward
and backward passes. For an SGD iteration, given the conv
feature map, the readonly RoI network performs a forward
pass and computes loss for all input RoIs (R) (Figure 2,
green arrows). Then the hard RoI sampling module uses
the procedure described in Section 4.1 to select hard examples (Rhard-sel), which are input to the regular RoI network
(Figure 2(b), red arrows)). This network computes forward
and backward passes only for Rhard-sel, accumulates the gradients and passes them to the conv network. In practice, we
use all RoIs from all N images as R, therefore the effective
batch size for the readonly RoI network is |R| and for the
regular RoI network is the standard B from Section 3.1.
Convolution Feature Maps
Selective-Search
|Rhard−sel| = Batch Size
Last Conv.
Feature Map
RoI Pooling
For each R
Read-only Layer
Order of Computation:
1. Forward for Conv. Network
2. Forward for each R𝑖( )
3. Selection of Rhard−sel
4. Forward-Backward for each Rhard−sel
5. Backward for Conv. Network
Backward Computation for:
1. Each Rhard−sel
2. Gradient Accumulation by
RoI Pooling Layer
3. Conv. Network ( )
Convolutional Network
RoI Network
Shared Weights
Forward for each R𝑖
Forward-Backward each R𝑖
Forward-Backward for each Image
Classification
Classification
Bbox Reg. Loss
For each Rhard−sel
Figure 2: Architecture of the proposed training algorithm. Given an image, and selective search RoIs, the conv network
computes a conv feature map. In (a), the readonly RoI network runs a forward pass on the feature map and all RoIs (shown
in green arrows). Then the Hard RoI module uses these RoI losses to select B examples. In (b), these hard examples are used
by the RoI network to compute forward and backward passes (shown in red arrows).
We implement both options described above using the
Caffe framework (see ). Our implementation uses
gradient accumulation with N forward-backward passes of
single image mini-batches. Following FRCN , we use
N = 2 (which results in |R| ≈4000) and B = 128. Under
these settings, the proposed architecture (Figure 2) has similar memory footprint as the ﬁrst option, but is > 2× faster.
Unless speciﬁed otherwise, the architecture and settings described above will be used throughout this paper.
5. Analyzing online hard example mining
This section compares FRCN training with online hard
example mining (OHEM) to the baseline heuristic sampling
approach. We also compare FRCN with OHEM to a less
efﬁcient approach that uses all available example RoIs in
each mini-batch, not just the B hardest examples.
5.1. Experimental setup
We conduct experiments with two standard ConvNet
architectures:
VGG CNN M 1024 (VGGM, for short)
from , which is a wider version of AlexNet , and
VGG16 from .
All experiments in this section are
performed on the PASCAL VOC07 dataset.
Training is
done on the trainval set and testing on the test set. Unless
speciﬁed otherwise, we will use the default settings from
FRCN . We train all methods with SGD for 80k minibatch iterations, with an initial learning rate of 0.001 and
we decay the learning rate by 0.1 every 30k iterations. The
baseline numbers reported in Table 1 (row 1-2) were reproduced using our training schedule and are slightly higher
than the ones reported in .
5.2. OHEM vs. heuristic sampling
Standard FRCN, reported in Table 1 (rows 1 −2), uses
bg lo = 0.1 as a heuristic for hard mining (Section 3.1).
To test the importance of this heuristic, we ran FRCN with
bg lo = 0. Table 1 (rows 3 −4) shows that for VGGM,
mAP drops by 2.4 points, whereas for VGG16 it remains
roughly the same.
Now compare this to training FRCN
with OHEM (rows 11 −13). OHEM improves mAP by 2.4
points compared to FRCN with the bg lo = 0.1 heuristic
for VGGM, and 4.8 points without the heuristic. This result
demonstrates the sub-optimality of these heuristics and the
effectiveness of our hard mining approach.
5.3. Robust gradient estimates
One concern over using only N = 2 images per batch
is that it may cause unstable gradients and slow convergence because RoIs from an image may be highly correlated . FRCN reports that this was not a practical
issue for their training. But this detail might raise concerns
over our training procedure because we use examples with
high loss from the same image and as a result they may be
more highly correlated. To address this concern, we experiment with N = 1 in order to increase correlation in an effort
to break our method. As seen in Table 1 (rows 5 −6, 11),
performance of the original FRCN drops by ∼1 point with
N = 1, but when using our training procedure, mAP remains approximately the same. This shows that OHEM is
robust in case one needs fewer images per batch in order to
reduce GPU memory usage.
Table 1: Impact of hyperparameters on FRCN training.
Experiment
Fast R-CNN 
Removing hard mining
heuristic (Section 5.2)
Fewer images per batch
(Section 5.3)
Bigger batch, High LR
(Section 5.4)
Our Approach
5.4. Why just hard examples, when you can use all?
Online hard example mining is based on the hypothesis
that it is important to consider all RoIs in an image and then
select hard examples for training. But what if we train with
all the RoIs, not just the hard ones? The easy examples will
have low loss, and won’t contribute much to the gradient;
training will automatically focus on the hard examples. To
compare this option, we ran standard FRCN training with
a large mini-batch size of B = 2048, using bg lo = 0,
N ∈{1, 2} and with other hyperparameters ﬁxed. Because
this experiment uses a large mini-batch, it’s important to
tune the learning rate to adjust for this change. We found
optimal results by increasing it to 0.003 for VGG16 and
0.004 for VGGM. The outcomes are reported in Table 1
(rows 7 −10). Using these settings, mAP of both VGG16
and VGGM increased by ∼1 point compared to B = 128,
but the improvement from our approach is still > 1 points
over using all RoIs. Moreover, because we compute gradients with a smaller mini-batch size training is faster.
5.5. Better optimization
Finally, we analyze the training loss for the various
FRCN training methods discussed above.
It’s important
to measure training loss in a way that does not depend on
the sampling procedure and thus results in a valid comparison between methods. To achieve this goal, we take model
snapshots from each method every 20k steps of optimization and run them over the entire VOC07 trainval set to
compute the average loss over all RoIs. This measures the
training set loss in a way that does not depend on the example sampling scheme.
Figure 3 shows the average loss per RoI for VGG16 with
the various hyperparameter settings discussed above and
presented in Table 1. We see that bg lo = 0 results in the
highest training loss, while using the heuristic bg lo = 0.1
results in a much lower training loss. Increasing the minibatch size to B = 2048 and increasing the learning rate
Iterations −→
Mean loss per RoI −→
FRCN (bg lo=0)
FRCN 
Our Approach (bg lo=0)
Iterations −→
Mean loss per RoI −→
FRCN (N=1)
FRCN (N=1, bg lo=1)
FRCN 
Our Approach (N=1, bg lo=0)
Figure 3: Training loss is computed for various training procedures using VGG16 networks discussed in Section 5. We
report mean loss per RoI. These results indicate that using
hard mining for training leads to lower training loss than
any of the other heuristics.
Table 2: Computational statistics of training FRCN and
FRCN with OHEM (using an Nvidia Titan X GPU).
time (sec/iter)
max. memory (G)
*: uses gradient accumulation over two forward/backward passes
lowers the training loss below the bg lo = 0.1 heuristic.
Our proposed online hard example mining method achieves
the lowest training loss of all methods, validating our claims
that OHEM leads to better training for FRCN.
5.6. Computational cost
OHEM adds reasonable computational and memory
overhead, as reported in Table 2. OHEM costs 0.09s per
training iteration for VGGM network (0.43s for VGG16)
and requires 1G more memory (2.3G for VGG16). Given
that FRCN is a fast detector to train, the increase in
training time is likely acceptable to most users.
6. PASCAL VOC and MS COCO results
In this section, we evaluate our method on VOC 2012
 as well as the more challenging MS COCO 
We demonstrate consistent and signiﬁcant improvement in FRCN performance when using the proposed
OHEM approach. Per-class results are also presented on
VOC 2007 for comparison with prior work.
Experimental setup.
We use VGG16 for all experiments.
When training on VOC07 trainval, we use the SGD parameters as in Section 5 and when using extra data (07+12 and
07++12, see Table 3 and 4), we use 200k mini-batch iterations, with an initial learning rate of 0.001 and decay step
size of 40k. When training on MS COCO , we use 240k
Table 3: VOC 2007 test detection average precision (%). All methods use VGG16. Training set key: 07: VOC07 trainval, 07+12: union
of 07 and VOC12 trainval. All methods use bounding-box regression. Legend: M: using multi-scale for training and testing, B: multi-stage
bbox regression. FRCN⋆refers to FRCN with our training schedule.
mAP aero bike
boat bottle
horse mbike persn plant sheep
74.5 78.3 69.2 53.2
77.3 78.2 82.0
74.6 76.8 67.6 52.9
78.7 78.8 81.6
71.2 78.3 69.2 57.9
81.8 79.1 83.2
77.8 81.3 71.4 60.4
85.0 84.6 86.2
MR-CNN 
78.7 81.8 76.7 66.6
81.7 85.3 82.7
77.7 81.9 76.0 64.9
86.3 86.0 86.8
77.0 78.1 69.3 59.4
81.6 78.6 86.7
77.7 81.2 74.1 64.2
86.2 83.8 88.1
MR-CNN 
80.3 84.1 78.5 70.8
88.0 85.9 87.8
80.6 85.7 79.8 69.9
88.3 87.9 89.6
Table 4: VOC 2012 test detection average precision (%). All methods use VGG16. Training set key: 12: VOC12 trainval, 07++12: union
of VOC07 trainval, VOC07 test, and VOC12 trainval. Legend: M: using multi-scale for training and testing, B: iterative bbox regression.
mAP aero bike
boat bottle
horse mbike persn plant sheep
80.3 74.7 66.9 46.9
73.9 68.6 87.7
81.5 78.9 69.6 52.3
77.4 72.1 88.2
MR-CNN 
85.0 79.6 71.5 55.3
76.0 73.9 84.6
85.8 82.3 74.1 55.8
79.5 77.7 90.4
82.3 78.4 70.8 52.3
77.8 71.6 89.3
83.0 81.3 72.5 55.6
78.9 74.7 89.5
MR-CNN 
85.5 82.9 76.6 57.8
79.4 77.2 86.6
86.3 85.0 77.0 60.9
81.9 81.1 91.9
1 2 
3 4 
mini-batch iterations, with an initial learning rate of 0.001
and decay step size of 160k, owing to a larger epoch size.
6.1. VOC 2007 and 2012 results
Table 3 shows that on VOC07, OHEM improves the
mAP of FRCN from 67.2% to 69.9% (and 70.0% to 74.6%
with extra data).
On VOC12, OHEM leads to an improvement of 4.1 points in mAP (from 65.7% to 69.8%).
With extra data, we achieve an mAP of 71.9% as compared to 68.4% mAP of FRCN, an improvement of 3.5
Interestingly the improvements are not uniform
across categories. Bottle, chair, and tvmonitor show larger
improvements that are consistent across the different PAS-
CAL splits. Why these classes beneﬁt the most is an interesting and open question.
6.2. MS COCO results
To test the beneﬁt of using OHEM on a larger and
more challenging dataset, we conduct experiments on MS
COCO and report numbers from test-dev 2015 evaluation server (Table 5). On the standard COCO evaluation metric, FRCN scores 19.7% AP, and OHEM improves it to 22.6% AP.2 Using the VOC overlap metric of
2COCO AP averages over classes, recall, and IoU levels. See http:
//mscoco.org/dataset/#detections-eval for details.
IoU ≥0.5, OHEM gives a 6.6 points boost in AP50. It is
also interesting to note that OHEM helps improve the AP
of medium sized objects by 4.9 points on the strict COCO
AP evaluation metric, which indicates that the proposed
hard example mining approach is helpful when dealing with
smaller sized objects. Note that FRCN with and without
OHEM were trained on MS COCO train set.
7. Adding bells and whistles
We’ve demonstrated consistent gains in detection accuracy by applying OHEM to FRCN training. In this section,
we show that these improvements are orthogonal to recent
bells and whistles that enhance object detection accuracy.
OHEM with the following two additions yields state-of-theart results on VOC and competitive results on MS COCO.
Multi-scale (M).
We adopt the multi-scale strategy from
SPPnet (and used by both FRCN and MR-
CNN ). Scale is deﬁned as the size of the shortest side
(s) of an image. During training, one scale is chosen at random, whereas at test time inference is run on all scales. For
VGG16 networks, we use s ∈{480, 576, 688, 864, 900} for
training, and s ∈{480, 576, 688, 864, 1000} during testing,
with the max dimension capped at 1000. The scales and
caps were chosen because of GPU memory constraints.
Table 5: MS COCO 2015 test−dev detection average precision (%). All methods use VGG16. Legend: M: using
multi-scale for training and testing.
area FRCN† Ours Ours [+M] Ours* [+M]
[0.50 : 0.95]
[0.50 : 0.95]
[0.50 : 0.95]
[0.50 : 0.95]
†from the leaderboard, *trained on trainval set
Iterative bounding-box regression (B).
We adopt the
iterative localization and bounding-box (bbox) voting
scheme from . The network evaluates each proposal
RoI to get scores and relocalized boxes R1. High-scoring
R1 boxes are the rescored and relocalized, yielding boxes
R2. Union of R1 and R2 is used as the ﬁnal set RF for postprocessing, where RNMS
is obtained using NMS on RF with
an IoU threshold of 0.3 and weighted voting is performed
on each box ri in RNMS
using boxes in RF with an IoU of
≥0.5 with ri (see for details).
7.1. VOC 2007 and 2012 results
We report the results on VOC benchmarks in Table 3
and 4. On VOC07, FRCN with the above mentioned additions achieves 72.4% mAP and OHEM improves it to
75.1%, which is currently the highest reported score under this setting (07 data). When using extra data (07+12),
OHEM achieves 78.9% mAP, surpassing the current stateof-the-art MR-CNN (78.2% mAP). We note that MR-
CNN uses selective search and edge boxes during training,
whereas we only use selective search boxes. Our multiscale implementation is also different, using fewer scales
than MR-CNN. On VOC12 (Table 4), we consistently perform better than MR-CNN. When using extra data, we
achieve state-of-the-art mAP of 76.3% (vs. 73.9% mAP of
Ablation analysis.
We now study in detail the impact of
these two additions and whether OHEM is complementary
to them, and report the analysis in Table 6. Baseline FRCN
mAP improves from 67.2% to 68.6% when using multiscale during both training and testing (we refer to this as M).
However, note that there is only a marginal beneﬁt of using
it at training time. Iterative bbox regression (B) further improves the FRCN mAP to 72.4%. But more importantly,
using OHEM improves it to 75.1% mAP, which is state-ofthe-art for methods trained on VOC07 data (see Table 3). In
fact, using OHEM consistently results in higher mAP for all
variants of these two additions (see Table 6).
Table 6: Impact of multi-scale and iterative bbox reg.
Multi-scale (M)
Iterative bbox
7.2. MS COCO results
MS COCO test-dev 2015 evaluation server results
are reported in Table 5. Using multi-scale improves the
performance of our method to 24.4% AP on the standard
COCO metric and to 44.4% AP50 on the VOC metric. This
again shows the complementary nature of using multi-scale
and OHEM. Finally, we train our method using the entire MS COCO trainval set, which further improves performance to 25.5% AP (and 45.9% AP50). In the 2015 MS
COCO Detection Challenge, a variant of this approach ﬁnished 4th place overall.
8. Conclusion
We presented an online hard example mining (OHEM)
algorithm, a simple and effective method to train regionbased ConvNet detectors. OHEM eliminates several heuristics and hyperparameters in common use by automatically
selecting hard examples, thus simplifying training. We conducted extensive experimental analysis to demonstrate the
effectiveness of the proposed algorithm, which leads to better training convergence and consistent improvements in
detection accuracy on standard benchmarks. We also reported state-of-the-art results on PASCAL VOC 2007 and
2012 when using OHEM with other orthogonal additions.
Though we used Fast R-CNN throughout this paper, OHEM
can be used for training any region-based ConvNet detector.
Our experimental analysis was based on the overall detection accuracy, however it will be an interesting future direction to study the impact of various training methodologies on individual category performance.
Acknowledgment.
This project started as an intern
project at Microsoft Research and continued at CMU. We
thank Larry Zitnick, Ishan Misra and Sean Bell for many
helpful discussions. AS was supported by the Microsoft
Research PhD Fellowship.
This work was also partially
supported by ONR MURI N000141612007.
NVIDIA for donating GPUs.