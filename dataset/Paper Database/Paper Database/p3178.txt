Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction
without Convolutions
Wenhai Wang1, Enze Xie2, Xiang Li3, Deng-Ping Fan4B,
Kaitao Song3, Ding Liang5, Tong Lu1B, Ping Luo2, Ling Shao4
1Nanjing University
2The University of Hong Kong
3Nanjing University of Science and Technology
5SenseTime Research
 
(a) CNNs: VGG , ResNet , etc.
Transformer
(b) Vision Transformer 
Transformer
(c) Pyramid Vision Transformer (ours)
Figure 1: Comparisons of different architectures, where ‚ÄúConv‚Äù and ‚ÄúTF-E‚Äù stand for ‚Äúconvolution‚Äù and ‚ÄúTransformer
encoder‚Äù, respectively. (a) Many CNN backbones use a pyramid structure for dense prediction tasks such as object detection
(DET), instance and semantic segmentation (SEG). (b) The recently proposed Vision Transformer (ViT) is a ‚Äúcolumnar‚Äù
structure speciÔ¨Åcally designed for image classiÔ¨Åcation (CLS). (c) By incorporating the pyramid structure from CNNs, we
present the Pyramid Vision Transformer (PVT), which can be used as a versatile backbone for many computer vision tasks,
broadening the scope and impact of ViT. Moreover, our experiments also show that PVT can easily be combined with
DETR to build an end-to-end object detection system without convolutions.
Although convolutional neural networks (CNNs) have
achieved great success in computer vision, this work investigates a simpler, convolution-free backbone network useful for many dense prediction tasks. Unlike the recentlyproposed Vision Transformer (ViT) that was designed for
image classiÔ¨Åcation speciÔ¨Åcally, we introduce the Pyramid Vision Transformer (PVT), which overcomes the difÔ¨Åculties of porting Transformer to various dense prediction
tasks. PVT has several merits compared to current state
of the arts. (1) Different from ViT that typically yields lowresolution outputs and incurs high computational and memory costs, PVT not only can be trained on dense partitions
of an image to achieve high output resolution, which is important for dense prediction, but also uses a progressive
shrinking pyramid to reduce the computations of large feature maps. (2) PVT inherits the advantages of both CNN
and Transformer, making it a uniÔ¨Åed backbone for vari-
B Corresponding authors: Deng-Ping Fan ( );
Tong Lu ( ).
ous vision tasks without convolutions, where it can be used
as a direct replacement for CNN backbones. (3) We validate PVT through extensive experiments, showing that it
boosts the performance of many downstream tasks, including object detection, instance and semantic segmentation.
For example, with a comparable number of parameters,
PVT+RetinaNet achieves 40.4 AP on the COCO dataset,
surpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute
AP (see Figure 2). We hope that PVT could serve as an
alternative and useful backbone for pixel-level predictions
and facilitate future research.
1. Introduction
Convolutional neural network (CNNs) have achieved remarkable success in computer vision, making them a versatile and dominant approach for almost all tasks . Nevertheless, this work aims to explore an alternative backbone network beyond CNN, which
can be used for dense prediction tasks such as object detec-
 
COCO BBox AP (%)
#Parameter (M)
X101-32x4d
X101-64x4d
#Param (M) AP
PVT-T (ours)
PVT-S (ours)
X101-32x4d 
ViT-S/32 
PVT-M (ours)
X101-64x4d 
PVT-L (ours)
Figure 2: Performance comparison on COCO val2017
of different backbones using RetinaNet for object detection, where ‚ÄúT‚Äù, ‚ÄúS‚Äù, ‚ÄúM‚Äù and ‚ÄúL‚Äù denote our PVT models
with tiny, small, medium and large size. We see that when
the number of parameters among different models are comparable, PVT variants signiÔ¨Åcantly outperform their corresponding counterparts such as ResNets (R) , ResNeXts
(X) , and ViT .
tion , semantic and instance segmentation ,
in addition to image classiÔ¨Åcation .
Inspired by the success of Transformer in natural language processing, many researchers have explored
its application in computer vision.
For example, some
works model the vision task as a dictionary lookup problem with learnable queries, and use the
Transformer decoder as a task-speciÔ¨Åc head on top of the
CNN backbone. Although some prior arts have also incorporated attention modules into CNNs, as far
as we know, exploring a clean and convolution-free Transformer backbone to address dense prediction tasks in computer vision is rarely studied.
Recently, Dosovitskiy et al. introduced the Vision
Transformer (ViT) for image classiÔ¨Åcation. This is an interesting and meaningful attempt to replace the CNN backbone with a convolution-free model. As shown in Figure 1
(b), ViT has a columnar structure with coarse image patches
as input.1
Although ViT is applicable to image classiÔ¨Åcation, it is challenging to directly adapt it to pixel-level
dense predictions such as object detection and segmentation, because (1) its output feature map is single-scale and
low-resolution, and (2) its computational and memory costs
are relatively high even for common input image sizes (e.g.,
1Due to resource constraints, ViT cannot use Ô¨Åne-grained image
patches (e.g., 4√ó4 pixels per patch) as input, instead only receive coarse
patches (e.g., 32√ó32 pixels per patch) as input, which leads to its low output resolution (e.g., 32-stride).
shorter edge of 800 pixels in the COCO benchmark ).
To address the above limitations, this work proposes a
pure Transformer backbone, termed Pyramid Vision Transformer (PVT), which can serve as an alternative to the CNN
backbone in many downstream tasks, including image-level
prediction as well as pixel-level dense predictions. SpeciÔ¨Åcally, as illustrated in Figure 1 (c), our PVT overcomes the
difÔ¨Åculties of the conventional Transformer by (1) taking
Ô¨Åne-grained image patches (i.e., 4√ó4 pixels per patch) as input to learn high-resolution representation, which is essential for dense prediction tasks; (2) introducing a progressive
shrinking pyramid to reduce the sequence length of Transformer as the network deepens, signiÔ¨Åcantly reducing the
computational cost, and (3) adopting a spatial-reduction attention (SRA) layer to further reduce the resource consumption when learning high-resolution features.
Overall, the proposed PVT possesses the following merits.
Firstly, compared to the traditional CNN backbones
(see Figure 1 (a)), which have local receptive Ô¨Åelds that increase with the network depth, our PVT always produces a
global receptive Ô¨Åeld, which is more suitable for detection
and segmentation. Secondly, compared to ViT (see Figure 1 (b)), thanks to its advanced pyramid structure, our
method can more easily be plugged into many representative dense prediction pipelines, e.g., RetinaNet and
Mask R-CNN . Thirdly, we can build a convolutionfree pipeline by combining our PVT with other task-speciÔ¨Åc
Transformer decoders, such as PVT+DETR for object detection. To our knowledge, this is the Ô¨Årst entirely
convolution-free object detection pipeline.
Our main contributions are as follows:
(1) We propose Pyramid Vision Transformer (PVT),
which is the Ô¨Årst pure Transformer backbone designed for
various pixel-level dense prediction tasks. Combining our
PVT and DETR, we can construct an end-to-end object detection system without convolutions and handcrafted components such as dense anchors and non-maximum suppression (NMS).
(2) We overcome many difÔ¨Åculties when porting Transformer to dense predictions, by designing a progressive
shrinking pyramid and a spatial-reduction attention (SRA).
These are able to reduce the resource consumption of Transformer, making PVT Ô¨Çexible to learning multi-scale and
high-resolution features.
(3) We evaluate the proposed PVT on several different tasks, including image classiÔ¨Åcation, object detection,
instance and semantic segmentation, and compare it with
popular ResNets and ResNeXts .
As presented
in Figure 2, our PVT with different parameter scales can
consistently archived improved performance compared to
the prior arts. For example, under a comparable number
of parameters, using RetinaNet for object detection,
PVT-Small achieves 40.4 AP on COCO val2017, outperforming ResNet50 by 4.1 points (40.4 vs. 36.3). Moreover,
PVT-Large achieves 42.6 AP, which is 1.6 points better than
ResNeXt101-64x4d, with 30% less parameters.
2. Related Work
2.1. CNN Backbones
CNNs are the work-horses of deep neural networks in visual recognition. The standard CNN was Ô¨Årst introduced in
 to distinguish handwritten numbers. The model contains convolutional kernels with a certain receptive Ô¨Åeld
that captures favorable visual context. To provide translation equivariance, the weights of convolutional kernels
are shared over the entire image space.
More recently,
with the rapid development of the computational resources
(e.g., GPU), the successful training of stacked convolutional
blocks on large-scale image classiÔ¨Åcation datasets
(e.g., ImageNet ) has become possible. For instance,
GoogLeNet demonstrated that a convolutional operator containing multiple kernel paths can achieve very competitive performance.
The effectiveness of a multi-path
convolutional block was further validated in Inception series , ResNeXt , DPN , MixNet and
SKNet . Further, ResNet introduced skip connections into the convolutional block, making it possible to create/train very deep networks and obtaining impressive results in the Ô¨Åeld of computer vision. DenseNet introduced a densely connected topology, which connects each
convolutional block to all previous blocks. More recent advances can be found in recent survey/review papers .
Unlike the full-blown CNNs, the vision Transformer
backbone is still in its early stage of development. In this
work, we try to extend the scope of Vision Transformer by
designing a new versatile Transformer backbone suitable
for most vision tasks.
2.2. Dense Prediction Tasks
Preliminary. The dense prediction task aims to perform
pixel-level classiÔ¨Åcation or regression on a feature map.
Object detection and semantic segmentation are two representative dense prediction tasks.
Object Detection.
In the era of deep learning,
CNNs have become the dominant framework for object detection, which includes single-stage detectors (e.g.,
SSD , RetinaNet , FCOS , GFL , PolarMask and OneNet ) and multi-stage detectors
(Faster R-CNN , Mask R-CNN , Cascade R-CNN
 and Sparse R-CNN ). Most of these popular object detectors are built on high-resolution or multi-scale feature maps to obtain good detection performance. Recently,
DETR and deformable DETR combined the CNN
backbone and the Transformer decoder to build an endto-end object detector. Likewise, they also require highresolution or multi-scale feature maps for accurate object
detection.
Semantic Segmentation. CNNs also play an important
role in semantic segmentation. In the early stages, FCN
 introduced a fully convolutional architecture to generate a spatial segmentation map for a given image of any
size. After that, the deconvolution operation was introduced
by Noh et al. and achieved impressive performance on
the PASCAL VOC 2012 dataset . Inspired by FCN, U-
Net was proposed for the medical image segmentation
domain speciÔ¨Åcally, bridging the information Ô¨Çow between
corresponding low-level and high-level feature maps of the
same spatial sizes. To explore richer global context representation, Zhao et al. designed a pyramid pooling
module over various pooling scales, and Kirillov et al. 
developed a lightweight segmentation head termed Semantic FPN, based on FPN . Finally, the DeepLab family
 applies dilated convolutions to enlarge the receptive
Ô¨Åeld while maintaining the feature map resolution. Similar
to object detection methods, semantic segmentation models
also rely on high-resolution or multi-scale feature maps.
2.3. Self-Attention and Transformer in Vision
As convolutional Ô¨Ålter weights are usually Ô¨Åxed after
training, they cannot be dynamically adapted to different
inputs. Many methods have been proposed to alleviate this
problem using dynamic Ô¨Ålters or self-attention operations . The non-local block attempts to model
long-range dependencies in both space and time, which
has been shown beneÔ¨Åcial for accurate video classiÔ¨Åcation.
However, despite its success, the non-local operator suffers from the high computational and memory costs.
Criss-cross further reduces the complexity by generating sparse attention maps through a criss-cross path.
Ramachandran et al. proposed the stand-alone selfattention to replace convolutional layers with local selfattention units.
AANet achieves competitive results
when combining the self-attention and convolutional operations. LambdaNetworks uses the lambda layer, an ef-
Ô¨Åcient self-attention to replace the convolution in the CNN.
DETR utilizes the Transformer decoder to model object detection as an end-to-end dictionary lookup problem
with learnable queries, successfully removing the need for
handcrafted processes such as NMS. Based on DETR, deformable DETR further adopts a deformable attention layer to focus on a sparse set of contextual elements,
obtaining faster convergence and better performance. Recently, Vision Transformer (ViT) employs a pure
Transformer model for image classiÔ¨Åcation by treating an image as a sequence of patches. DeiT further
extends ViT using a novel distillation approach. Different
from previous models, this work introduces the pyramid
structure into Transformer to present a pure Transformer
Transformer Encoder (ùêø%√ó)
√ó(ùëÉ!$ùê∂!"#)
Position Embedding
Element-wise Add
Feature Map
Multi-Head
Figure 3: Overall architecture of Pyramid Vision Transformer (PVT). The entire model is divided into four stages, each
of which is comprised of a patch embedding layer and a Li-layer Transformer encoder. Following a pyramid structure, the
output resolution of the four stages progressively shrinks from high (4-stride) to low (32-stride).
backbone for dense prediction tasks, rather than a taskspeciÔ¨Åc head or an image classiÔ¨Åcation model.
3. Pyramid Vision Transformer (PVT)
3.1. Overall Architecture
Our goal is to introduce the pyramid structure into the
Transformer framework, so that it can generate multi-scale
feature maps for dense prediction tasks (e.g., object detection and semantic segmentation). An overview of PVT is
depicted in Figure 3. Similar to CNN backbones , our
method has four stages that generate feature maps of different scales. All stages share a similar architecture, which
consists of a patch embedding layer and Li Transformer encoder layers.
In the Ô¨Årst stage, given an input image of size H√óW√ó3,
we Ô¨Årst divide it into HW
patches,2 each of size 4√ó4√ó3.
Then, we feed the Ô¨Çattened patches to a linear projection
and obtain embedded patches of size HW
42 √óC1. After that,
the embedded patches along with a position embedding are
passed through a Transformer encoder with L1 layers, and
the output is reshaped to a feature map F1 of size H
In the same way, using the feature map from the previous stage as input, we obtain the following feature maps:
F2, F3, and F4, whose strides are 8, 16, and 32 pixels
with respect to the input image. With the feature pyramid
{F1, F2, F3, F4}, our method can be easily applied to most
2As done for ResNet, we keep the highest resolution of our output feature map at 4-stride.
downstream tasks, including image classiÔ¨Åcation, object detection, and semantic segmentation.
3.2. Feature Pyramid for Transformer
Unlike CNN backbone networks , which use
different convolutional strides to obtain multi-scale feature
maps, our PVT uses a progressive shrinking strategy to control the scale of feature maps by patch embedding layers.
Here, we denote the patch size of the i-th stage as Pi. At
the beginning of stage i, we Ô¨Årst evenly divide the input feature map Fi‚àí1 ‚ààRHi‚àí1√óWi‚àí1√óCi‚àí1 into Hi‚àí1Wi‚àí1
patches, and
then each patch is Ô¨Çatten and projected to a Ci-dimensional
embedding. After the linear projection, the shape of the embedded patches can be viewed as Hi‚àí1
Pi √óCi, where
the height and width are Pi times smaller than the input.
In this way, we can Ô¨Çexibly adjust the scale of the feature
map in each stage, making it possible to construct a feature
pyramid for Transformer.
3.3. Transformer Encoder
The Transformer encoder in the stage i has Li encoder
layers, each of which is composed of an attention layer
and a feed-forward layer . Since PVT needs to process
high-resolution (e.g., 4-stride) feature maps, we propose a
spatial-reduction attention (SRA) layer to replace the traditional multi-head attention (MHA) layer in the encoder.
Similar to MHA, our SRA receives a query Q, a key K,
and a value V as input, and outputs a reÔ¨Åned feature. The
difference is that our SRA reduces the spatial scale of K
Multi-Head Attention
Multi-Head
Spatial-Reduction Attention (ours)
Multi-Head
V (ùêª! ùëä!)√óùê∂!
Multi-head attention (MHA) vs. spatialreduction attention (SRA). With the spatial-reduction operation, the computational/memory cost of our SRA is
much lower than that of MHA.
and V before the attention operation (see Figure 4), which
largely reduces the computational/memory overhead. Details of the SRA in the stage i can be formulated as follows:
SRA(Q, K, V ) = Concat(head0, ..., headNi)W O,
headj = Attention(QW Q
j , SR(K)W K
j , SR(V)W V
where Concat(¬∑) is the concatenation operation as in .
j ‚ààRCi√ódhead, W K
‚ààRCi√ódhead, W V
j ‚ààRCi√ódhead, and
W O ‚ààRCi√óCi are linear projection parameters. Ni is the
head number of the attention layer in Stage i. Therefore, the
dimension of each head (i.e., dhead) is equal to Ci
Ni . SR(¬∑) is
the operation for reducing the spatial dimension of the input
sequence (i.e., K or V ), which is written as:
SR(x) = Norm(Reshape(x, Ri)W S).
Here, x ‚ààR(HiWi)√óCi represents a input sequence, and
Ri denotes the reduction ratio of the attention layers in
Stage i. Reshape(x, Ri) is an operation of reshaping the
input sequence x to a sequence of size HiWi
i Ci)√óCi is a linear projection that reduces the dimension of the input sequence to Ci. Norm(¬∑) refers to
layer normalization . As in the original Transformer ,
our attention operation Attention(¬∑) is calculated as:
Attention(q, k, v) = Softmax( qkT
Through these formulas, we can Ô¨Ånd that the computational/memory costs of our attention operation are R2
lower than those of MHA, so our SRA can handle larger
input feature maps/sequences with limited resources.
3.4. Model Details
In summary, the hyper parameters of our method are
listed as follows:
‚Ä¢ Pi: the patch size of Stage i;
‚Ä¢ Ci: the channel number of the output of Stage i;
‚Ä¢ Li: the number of encoder layers in Stage i;
‚Ä¢ Ri: the reduction ratio of the SRA in Stage i;
‚Ä¢ Ni: the head number of the SRA in Stage i;
‚Ä¢ Ei: the expansion ratio of the feed-forward layer 
in Stage i;
Following the design rules of ResNet , we (1) use small
output channel numbers in shallow stages; and (2) concentrate the major computation resource in intermediate stages.
To provide instances for discussion, we describe a series
of PVT models with different scales, namely PVT-Tiny, -
Small, -Medium, and -Large, in Table 1, whose parameter
numbers are comparable to ResNet18, 50, 101, and 152 respectively. More details of employing these models in speciÔ¨Åc downstream tasks will be introduced in Section 4.
3.5. Discussion
The most related work to our model is ViT . Here,
we discuss the relationship and differences between them.
First, both PVT and ViT are pure Transformer models without convolutions.
The primary difference between them
is the pyramid structure. Similar to the traditional Transformer , the length of ViT‚Äôs output sequence is the same
as the input, which means that the output of ViT is singlescale (see Figure 1 (b)). Moreover, due to the limited resource, the input of ViT is coarse-grained (e.g., the patch
size is 16 or 32 pixels), and thus its output resolution is relatively low (e.g., 16-stride or 32-stride). As a result, it is
difÔ¨Åcult to directly apply ViT to dense prediction tasks that
require high-resolution or multi-scale feature maps.
Our PVT breaks the routine of Transformer by introducing a progressive shrinking pyramid.
It can generate multi-scale feature maps like a traditional CNN backbone.
In addition, we also designed a simple but effective attention layer‚ÄîSRA, to process high-resolution feature maps and reduce computational/memory costs. BeneÔ¨Åting from the above designs, our method has the following advantages over ViT: 1) more Ô¨Çexible‚Äîcan generate feature maps of different scales/channels in different stages; 2) more versatile‚Äîcan be easily plugged and
played in most downstream task models; 3) more friendly
to computation/memory‚Äîcan handle higher resolution feature maps or longer sequences.
4. Application to Downstream Tasks
4.1. Image-Level Prediction
Image classiÔ¨Åcation is the most classical task of imagelevel prediction. To provide instances for discussion, we
design a series of PVT models with different scales, namely
PVT-Tiny, -Small, -Medium, and -Large, whose parameter
numbers are similar to ResNet18, 50, 101, and 152, respec-
Output Size
Layer Name
PVT-Medium
Patch Embedding
P1 = 4; C1 = 64
Transformer
Patch Embedding
P2 = 2; C2 = 128
Transformer
Patch Embedding
P3 = 2; C3 = 320
Transformer
Patch Embedding
P4 = 2; C4 =512
Transformer
Table 1: Detailed settings of PVT series. The design follows the two rules of ResNet : (1) with the growth of network
depth, the hidden dimension gradually increases, and the output resolution progressively shrinks; (2) the major computation
resource is concentrated in Stage 3.
tively. Detailed hyper-parameter settings of the PVT series
are provided in the supplementary material (SM).
For image classiÔ¨Åcation, we follow ViT and
DeiT to append a learnable classiÔ¨Åcation token to the
input of the last stage, and then employ a fully connected
(FC) layer to conduct classiÔ¨Åcation on top of the token.
4.2. Pixel-Level Dense Prediction
In addition to image-level prediction, dense prediction
that requires pixel-level classiÔ¨Åcation or regression to be
performed on the feature map, is also often seen in downstream tasks. Here, we discuss two typical tasks, namely
object detection, and semantic segmentation.
We apply our PVT models to three representative dense
prediction methods, namely RetinaNet , Mask R-
CNN , and Semantic FPN . RetinaNet is a widely
used single-stage detector, Mask R-CNN is the most popular two-stage instance segmentation framework, and Semantic FPN is a vanilla semantic segmentation method
without special operations (e.g., dilated convolution). Using these methods as baselines enables us to adequately examine the effectiveness of different backbones.
The implementation details are as follows:
ResNet, we initialize the PVT backbone with the weights
pre-trained on ImageNet; (2) We use the output feature
pyramid {F1, F2, F3, F4} as the input of FPN , and
then the reÔ¨Åned feature maps are fed to the follow-up detection/segmentation head; (3) When training the detection/segmentation model, none of the layers in PVT are
frozen; (4) Since the input for detection/segmentation can
be an arbitrary shape, the position embeddings pre-trained
on ImageNet may no longer be meaningful. Therefore, we
perform bilinear interpolation on the pre-trained position
embeddings according to the input resolution.
5. Experiments
We compare PVT with the two most representative CNN
backbones, i.e., ResNet and ResNeXt , which are
widely used in the benchmarks of many downstream tasks.
5.1. Image ClassiÔ¨Åcation
Settings. Image classiÔ¨Åcation experiments are performed
on the ImageNet 2012 dataset , which comprises 1.28
million training images and 50K validation images from
1,000 categories.
For fair comparison, all models are
trained on the training set, and report the top-1 error on the
validation set. We follow DeiT and apply random cropping, random horizontal Ô¨Çipping , label-smoothing regularization , mixup , CutMix , and random erasing as data augmentations. During training, we employ
AdamW with a momentum of 0.9, a mini-batch size of
128, and a weight decay of 5 √ó 10‚àí2 to optimize models.
The initial learning rate is set to 1√ó10‚àí3 and decreases following the cosine schedule . All models are trained for
300 epochs from scratch on 8 V100 GPUs. To benchmark,
we apply a center crop on the validation set, where a 224√ó
224 patch is cropped to evaluate the classiÔ¨Åcation accuracy.
Results. In Table 2, we see that our PVT models are superior to conventional CNN backbones under similar parameter numbers and computational budgets. For example, when
#Param (M)
Top-1 Err (%)
ResNet18* 
ResNet18 
DeiT-Tiny/16 
PVT-Tiny (ours)
ResNet50* 
ResNet50 
ResNeXt50-32x4d* 
ResNeXt50-32x4d 
T2T-ViTt-14 
TNT-S 
DeiT-Small/16 
PVT-Small (ours)
ResNet101* 
ResNet101 
ResNeXt101-32x4d* 
ResNeXt101-32x4d 
T2T-ViTt-19 
ViT-Small/16 
PVT-Medium (ours)
ResNeXt101-64x4d* 
ResNeXt101-64x4d 
ViT-Base/16 
T2T-ViTt-24 
TNT-B 
DeiT-Base/16 
PVT-Large (ours)
Table 2: Image classiÔ¨Åcation performance on the ImageNet validation set. ‚Äú#Param‚Äù refers to the number of
parameters. ‚ÄúGFLOPs‚Äù is calculated under the input scale
of 224 √ó 224. ‚Äú*‚Äù indicates the performance of the method
trained under the strategy of its original paper.
the GFLOPs are roughly similar, the top-1 error of PVT-
Small reaches 20.2, which is 1.3 points higher than that of
ResNet50 (20.2 vs. 21.5). Meanwhile, under similar or
lower complexity, PVT models archive performances comparable to the recently proposed Transformer-based models, such as ViT and DeiT (PVT-Large: 18.3 vs.
ViT(DeiT)-Base/16: 18.3). Here, we clarify that these results are within our expectations, because the pyramid structure is beneÔ¨Åcial to dense prediction tasks, but brings little
improvements to image classiÔ¨Åcation.
Note that ViT and DeiT have limitations as they are
speciÔ¨Åcally designed for classiÔ¨Åcation tasks, and thus are
not suitable for dense prediction tasks, which usually require effective feature pyramids.
5.2. Object Detection
Settings. Object detection experiments are conducted on
the challenging COCO benchmark .
All models are
trained on COCO train2017 (118k images) and evaluated on val2017 (5k images). We verify the effectiveness
of PVT backbones on top of two standard detectors, namely
RetinaNet and Mask R-CNN . Before training, we
use the weights pre-trained on ImageNet to initialize the
backbone and Xavier to initialize the newly added layers. Our models are trained with a batch size of 16 on 8
V100 GPUs and optimized by AdamW with an initial learning rate of 1 √ó 10‚àí4. Following common practices , we adopt 1√ó or 3√ó training schedule (i.e.,
12 or 36 epochs) to train all detection models. The training
image is resized to have a shorter side of 800 pixels, while
the longer side does not exceed 1,333 pixels. When using
the 3√ó training schedule, we randomly resize the shorter
side of the input image within the range of . In
the testing phase, the shorter side of the input image is Ô¨Åxed
to 800 pixels.
Results. As shown in Table 3, when using RetinaNet for
object detection, we Ô¨Ånd that under comparable number of
parameters, the PVT-based models signiÔ¨Åcantly surpasses
their counterparts. For example, with the 1√ó training schedule, the AP of PVT-Tiny is 4.9 points better than that of
ResNet18 (36.7 vs. 31.8). Moreover, with the 3√ó training
schedule and multi-scale training, PVT-Large archive the
best AP of 43.4, surpassing ResNeXt101-64x4d (43.4 vs.
41.8), while our parameter number is 30% fewer. These results indicate that our PVT can be a good alternative to the
CNN backbone for object detection.
Similar results are found in instance segmentation experiments based on Mask R-CNN, as shown in Table 4. With
the 1√ó training schedule, PVT-Tiny achieves 35.1 mask AP
(APm), which is 3.9 points better than ResNet18 (35.1 vs.
31.2) and even 0.7 points higher than ResNet50 (35.1 vs.
34.4). The best APm obtained by PVT-Large is 40.7, which
is 1.0 points higher than ResNeXt101-64x4d (40.7 vs. 39.7),
with 20% fewer parameters.
5.3. Semantic Segmentation
Settings. We choose ADE20K , a challenging scene
parsing dataset, to benchmark the performance of semantic
segmentation. ADE20K contains 150 Ô¨Åne-grained semantic
categories, with 20,210, 2,000, and 3,352 images for training, validation, and testing, respectively. We evaluate our
PVT backbones on the basis of Semantic FPN , a simple segmentation method without dilated convolutions .
In the training phase, the backbone is initialized with the
weights pre-trained on ImageNet , and other newly
added layers are initialized with Xavier . We optimize
our models using AdamW with an initial learning rate
of 1e-4. Following common practices , we train our
models for 80k iterations with a batch size of 16 on 4 V100
GPUs. The learning rate is decayed following the polynomial decay schedule with a power of 0.9. We randomly
resize and crop the image to 512 √ó 512 for training, and
rescale to have a shorter side of 512 pixels during testing.
As shown in Table 5, when using Semantic FPN for semantic segmentation,
models consistently outperforms the models based on
ResNet or ResNeXt .
For example, with al-
RetinaNet 1x
RetinaNet 3x + MS
ResNet18 
PVT-Tiny (ours)
36.7(+4.9)
39.4(+4.0)
ResNet50 
PVT-Small (ours)
40.4(+4.1)
42.2(+3.2)
ResNet101 
ResNeXt101-32x4d 
39.9(+1.4)
41.4(+0.5)
PVT-Medium (ours)
41.9(+3.4)
43.2(+2.3)
ResNeXt101-64x4d 
PVT-Large (ours)
42.6(+1.6)
43.4(+1.6)
Table 3: Object detection performance on COCO val2017. ‚ÄúMS‚Äù means that multi-scale training is used.
Mask R-CNN 1x
Mask R-CNN 3x + MS
ResNet18 
PVT-Tiny (ours)
36.7(+2.7)
35.1(+3.9)
39.8(+2.9)
37.4(+3.8)
ResNet50 
PVT-Small (ours)
40.4(+2.4)
37.8(+3.4)
43.0(+2.0)
39.9(+2.8)
ResNet101 
ResNeXt101-32x4d 
41.9(+1.5)
37.5(+1.1)
44.0(+1.2)
39.2(+0.7)
PVT-Medium (ours)
42.0(+1.6)
39.0(+2.6)
44.2(+1.4)
40.5(+2.0)
ResNeXt101-64x4d 
PVT-Large (ours)
42.9(+0.1)
39.5(+1.1)
44.5(+0.1)
40.7(+1.0)
Table 4: Object detection and instance segmentation performance on COCO val2017. APb and APm denote bounding
box AP and mask AP, respectively.
Semantic FPN
#Param (M)
ResNet18 
PVT-Tiny (ours)
35.7(+2.8)
ResNet50 
PVT-Small (ours)
39.8(+3.1)
ResNet101 
ResNeXt101-32x4d 
39.7(+0.9)
PVT-Medium (ours)
41.6(+2.8)
ResNeXt101-64x4d 
PVT-Large (ours)
42.1(+1.9)
PVT-Large* (ours)
Table 5: Semantic segmentation performance of different backbones on the ADE20K validation set. ‚ÄúGFLOPs‚Äù
is calculated under the input scale of 512 √ó 512. ‚Äú*‚Äù indicates 320K iterations training and multi-scale Ô¨Çip testing.
most the same number of parameters and GFLOPs, our
PVT-Tiny/Small/Medium are at least 2.8 points higher than
ResNet-18/50/101.
In addition, although the parameter
number and GFLOPs of our PVT-Large are 20% lower than
those of ResNeXt101-64x4d, the mIoU is still 1.9 points
higher (42.1 vs. 40.2). With a longer training schedule and
multi-scale testing, PVT-Large+Semantic FPN archives the
best mIoU of 44.8, which is very close to the state-of-the-art
performance of the ADE20K benchmark. Note that Semantic FPN is just a simple segmentation head. These results
demonstrate that our PVT backbones can extract better features for semantic segmentation than the CNN backbone,
DETR (50 Epochs)
ResNet50 
PVT-Small (ours)
34.7(+2.4)
Table 6: Performance of the pure Transformer object
detection pipeline. We build a pure Transformer detector
by combining PVT and DETR , whose AP is 2.4 points
higher than the original DETR based on ResNet50 .
beneÔ¨Åting from the global attention mechanism.
5.4. Pure Transformer Detection & Segmentation
PVT+DETR. To reach the limit of no convolution, we build
a pure Transformer pipeline for object detection by simply combining our PVT with a Transformer-based detection
head‚ÄîDETR . We train models on COCO train2017
for 50 epochs with an initial learning rate of 1 √ó 10‚àí4.
The learning rate is divided by 10 at the 33rd epoch. We
use random Ô¨Çipping and multi-scale training as data augmentation. All other experimental settings is the same as
those in Sec. 5.2. As reported in Table 6, PVT-based DETR
archieves 34.7 AP on COCO val2017, outperforming
the original ResNet50-based DETR by 2.4 points (34.7 vs.
32.3). These results prove that a pure Transformer detector
can also works well in the object detection task.
PVT+Trans2Seg.We build a pure Transformer model
for semantic segmentation by combining our PVT with
#Param (M)
ResNet50-d8+DeeplabV3+ 
ResNet50-d16+DeeplabV3+ 
ResNet50-d16+Trans2Seg 
PVT-Small+Trans2Seg
42.6(+2.9)
Table 7: Performance of the pure Transformer semantic
segmentation pipeline. We build a pure Transformer detector by combining PVT and Trans2Seg . It is 2.9%
higher than ResNet50-d16+Trans2Seg and 1.1% higher
than ResNet50-d8+DeeplabV3+ with lower GFlops. ‚Äúd8‚Äù
and ‚Äúd16‚Äù means dilation 8 and 16, respectively.
RetinaNet 1x
ViT-Small/4 
Out of Memory
ViT-Small/32 
PVT-Small (ours)
Table 8: Performance comparison between ViT and our
PVT using RetinaNet for object detection. ViT-Small/4
runs out of GPU memory due to small patch size (i.e.,
4√ó4 per patch). ViT-Small/32 obtains 31.7 AP on COCO
val2017, which is 8.7 points lower than our PVT-Small.
Trans2Seg , a Transformer-based segmentation head.
According to the experimental settings in Sec. 5.3, we
perform experiments on ADE20K with 40k iterations training,
single scale testing,
and compare it
with ResNet50+Trans2Seg and DeeplabV3+ with
ResNet50-d8 (dilation 8) and -d16(dilation 8) in Table
7. We Ô¨Ånd that our PVT-Small+Trans2Seg achieves 42.6
mIoU, outperforming ResNet50-d8+DeeplabV3+ (41.5).
Note that, ResNet50-d8+DeeplabV3+ has 120.5 GFLOPs
due to the high computation cost of dilated convolution, and
our method has only 31.6 GFLOPs, which is 4 times fewer.
In addition, our PVT-Small+Trans2Seg performs better than
ResNet50-d16+Trans2Seg (mIoU: 42.6 vs. 39.7, GFlops:
31.6 vs. 79.3). These results prove that a pure Transformer
segmentation network is workable.
5.5. Ablation Study
Settings. We conduct ablation studies on ImageNet 
and COCO datasets. The experimental settings on ImageNet are the same as the settings in Sec. 5.1. For COCO,
all models are trained with a 1√ó training schedule (i.e., 12
epochs) and without multi-scale training, and other settings
follow those in Sec. 5.2.
Pyramid Structure. A Pyramid structure is crucial when
applying Transformer to dense prediction tasks. ViT (see
Figure 1 (b)) is a columnar framework, whose output is
single-scale. This results in a low-resolution output feature map when using coarse image patches (e.g., 32√ó32
pixels per patch) as input, leading to poor detection perfor-
COCO BBox AP (%)
PVT-Small w pre-train 1x
PVT-Small w/ pre-train 3x
PVT-Small w/o pre-train 1x
PVT-Small w/o pre-train 3x
COCO BBox AP (%)
PVT-Small 1x
PVT-Small 3x
ResNet50 1x
ResNet50 3x
Figure 5: AP curves of RetinaNet on COCO val2017
under different backbone settings. Top: using weights
pre-trained on ImageNet vs. random initialization. Bottom:
PVT-S vs. R50 .
RetinaNet 1x
Wider PVT-Small
Deeper PVT-Small
Table 9: Deeper vs. Wider. ‚ÄúTop-1‚Äù denotes the top-1 error
on the ImageNet validation set. ‚ÄúAP‚Äù denotes the bounding
box AP on COCO val2017. The deep model (i.e., PVT-
Medium) obtains better performance than the wide model
(i.e., PVT-Small-Wide ) under comparable parameter number.
mance (31.7 AP on COCO val2017),3 as shown in Table
8. When using Ô¨Åne-grained image patches (e.g., 4√ó4 pixels
per patch) as input like our PVT, ViT will exhaust the GPU
memory (32G). Our method avoids this problem through a
progressive shrinking pyramid. SpeciÔ¨Åcally, our model can
process high-resolution feature maps in shallow stages and
low-resolution feature maps in deep stages. Thus, it obtains
a promising AP of 40.4 on COCO val2017, 8.7 points
higher than ViT-Small/32 (40.4 vs. 31.7).
Deeper vs. Wider. The problem of whether the CNN backbone should go deeper or wider has been extensively discussed in previous work . Here, we explore this
3For adapting ViT to RetinaNet, we extract the features from the layer
2, 4, 6, and 8 of ViT-Small/32, and interpolate them to different scales.
Mask R-CNN 1x
ResNet50+GC r4 
PVT-Small (ours)
Table 10: PVT vs. CNN w/ non-local.
APm denotes
mask AP. Under similar parameter nubmer and GFLOPs,
our PVT outperform the CNN backbone w/ Non-Local
(ResNet50+GC r4) by 1.6 APm (37.8 vs. 36.2).
problem in our PVT. For fair comparisons, we multiply
the hidden dimensions {C1, C2, C3, C4} of PVT-Small by
a scale factor 1.4 to make it have an equivalent parameter
number to the deep model (i.e., PVT-Medium). As shown
in Table 9, the deep model (i.e., PVT-Medium) consistently
works better than the wide model (i.e., PVT-Small-Wide) on
both ImageNet and COCO. Therefore, going deeper is more
effective than going wider in the design of PVT. Based on
this observation, in Table 1, we develop PVT models with
different scales by increasing the model depth.
Pre-trained Weights. Most dense prediction models (e.g.,
RetinaNet ) rely on the backbone whose weights are
pre-trained on ImageNet. We also discuss this problem in
our PVT. In the top of Figure 5, we plot the validation AP
curves of RetinaNet-PVT-Small w/ (red curves) and w/o
(blue curves) pre-trained weights. We Ô¨Ånd that the model
w/ pre-trained weights converges better than the one w/o
pre-trained weights, and the gap between their Ô¨Ånal AP
reaches 13.8 under the 1√ó training schedule and 8.4 under
the 3√ó training schedule and multi-scale training. Therefore, like CNN-based models, pre-training weights can also
help PVT-based models converge faster and better. Moreover, in the bottom of Figure 5, we also see that the convergence speed of PVT-based models (red curves) is faster
than that of ResNet-based models (green curves).
PVT vs. ‚ÄúCNN w/ Non-Local‚Äù To obtain a global receptive Ô¨Åeld, some well-engineered CNN backbones, such as
GCNet , integrate the non-local block in the CNN framework. Here, we compare the performance of our PVT (pure
Transformer) and GCNet (CNN w/ non-local), using Mask
R-CNN for instance segmentation. As reported in Table
10, we Ô¨Ånd that our PVT-Small outperforms ResNet50+GC
r4 by 1.6 points in APm (37.8 vs. 36.2), and 2.0 points in
75 (38.3 vs. 40.3), under comparable parameter number
and GFLOPs. There are two possible reasons for this result:
(1) Although a single global attention layer (e.g., nonlocal or multi-head attention (MHA) ) can acquire global-receptive-Ô¨Åeld features, the model performance keeps improving as the model deepens. This indicates that stacking multiple MHAs can further enhance the
representation capabilities of features. Therefore, as a pure
Transformer backbone with more global attention layers,
our PVT tends to perform better than the CNN backbone
RetinaNet 1x
ResNet50 
PVT-Small (ours)
Table 11: Latency and AP under different input scales.
‚ÄúScale‚Äù and ‚ÄúTime‚Äù denote the input scale and time cost
per image. When the shorter side is 640 pixels, the PVT-
Small+RetinaNet has a lower GFLOPs and time cost (on a
V100 GPU) than ResNet50+RetinaNet, while obtaining 2.4
points better AP (38.7 vs. 36.3).
equipped with non-local blocks (e.g., GCNet).
(2) Regular convolutions can be deemed as special instantiations of spatial attention mechanisms . In other
words, the format of MHA is more Ô¨Çexible than the regular
convolution. For example, for different inputs, the weights
of the convolution are Ô¨Åxed, but the attention weights of
MHA change dynamically with the input. Thus, the features
learned by the pure Transformer backbone full of MHA layers, could be more Ô¨Çexible and expressive.
Computation Overhead. With increasing input scale, the
growth rate of the GFLOPs of our PVT is greater than
ResNet , but lower than ViT , as shown in Figure
6. However, when the input scale does not exceed 640√ó640
pixels, the GFLOPs of PVT-Small and ResNet50 are similar. This means that our PVT is more suitable for tasks with
medium-resolution input.
On COCO, the shorter side of the input image is 800
pixels. Under this condition, the inference speed of RetinaNet based on PVT-Small is slower than the ResNet50based model, as reported in Table 11. (1) A direct solution
for this problem is to reduce the input scale. When reducing the shorter side of the input image to 640 pixels, the
model based on PVT-Small runs faster than the ResNet50based model (51.7ms vs., 55.9ms), with 2.4 higher AP
(38.7 vs. 36.3). 2) Another solution is to develop a selfattention layer with lower computational complexity. This
is a worth exploring direction, we recently propose a solution PVTv2 .
Detection & Segmentation Results. In Figure 7, we also
present some qualitative object detection and instance segmentation results on COCO val2017 , and semantic
segmentation results on ADE20K . These results indicate that a pure Transformer backbone (i.e., PVT) without
convolutions can also be easily plugged in dense prediction
models (e.g., RetinaNet , Mask R-CNN , and Semantic FPN ), and obtain high-quality results.
6. Conclusions and Future Work
We introduce PVT, a pure Transformer backbone for
dense prediction tasks, such as object detection and seman-
Input Scale
ViT-Small/16
ViT-Small/32
PVT-Small (ours)
Figure 6: Models‚Äô GFLOPs under different input scales.
The growth rate of GFLOPs:
ViT-Small/16 >ViT-
Small/32 >PVT-Small (ours)>ResNet50 . When
the input scale is less than 640 √ó 640, the GFLOPs of PVT-
Small and ResNet50 are similar.
tic segmentation. We develop a progressive shrinking pyramid and a spatial-reduction attention layer to obtain highresolution and multi-scale feature maps under limited computation/memory resources. Extensive experiments on object detection and semantic segmentation benchmarks verify that our PVT is stronger than well-designed CNN backbones under comparable numbers of parameters.
Although PVT can serve as an alternative to CNN backbones (e.g., ResNet, ResNeXt), there are still some speciÔ¨Åc
modules and operations designed for CNNs and not considered in this work, such as SE , SK , dilated convolution , model pruning , and NAS . Moreover,
with years of rapid developments, there have been many
well-engineered CNN backbones such as Res2Net ,
EfÔ¨ÅcientNet , and ResNeSt .
In contrast, the
Transformer-based model in computer vision is still in
its early stage of development.
Therefore, we believe
there are many potential technologies and applications (e.g.,
OCR , 3D and medical 
image analysis) to be explored in the future, and hope that
PVT could serve as a good starting point.
Acknowledgments
This work was supported by the Natural Science Foundation of China under Grant 61672273 and Grant 61832008,
the Science Foundation for Distinguished Young Scholars of Jiangsu under Grant BK20160021, Postdoctoral Innovative Talent Support Program of China under Grant
BX20200168, 2020M681608, the General Research Fund
of Hong Kong No. 27208720.