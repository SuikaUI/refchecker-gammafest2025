CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image
Classiﬁcation
Chun-Fu (Richard) Chen, Quanfu Fan, Rameswar Panda
MIT-IBM Watson AI Lab
 , , 
The recently developed vision transformer (ViT) has
achieved promising results on image classiﬁcation compared to convolutional neural networks. Inspired by this,
in this paper, we study how to learn multi-scale feature representations in transformer models for image classiﬁcation.
To this end, we propose a dual-branch transformer to combine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with
two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce
computation, we develop a simple yet effective token fusion
module based on cross attention, which uses a single token
for each branch as a query to exchange information with
other branches. Our proposed cross-attention only requires
linear time for both computational and memory complexity
instead of quadratic time otherwise. Extensive experiments
demonstrate that our approach performs better than or on
par with several concurrent works on vision transformer,
in addition to efﬁcient CNN models. For example, on the
ImageNet1K dataset, with some architectural changes, our
approach outperforms the recent DeiT by a large margin of
2% with a small to moderate increase in FLOPs and model
parameters. Our source codes and models are available at
 
1. Introduction
The novel transformer architecture has led to a big
leap forward in capabilities for sequence-to-sequence modeling in NLP tasks . The great success of transformers in NLP has sparked particular interest from the vision
community in understanding whether transformers can be a
strong competitor against the dominant Convolutional Neural Network based architectures (CNNs) in vision tasks
such as ResNet and EfﬁcientNet . Previous re-
FLOPs (109)
Top-1 Acc. (%)
10M 50M 100M 200M
Figure 1: Improvement of our proposed approach over
DeiT and ViT . The circle size is proportional to
the model size. All models are trained on ImageNet1K from
scratch. The results of ViT are referenced from .
search efforts on transformers in vision have, until very recently, been largely focused on combining CNNs with selfattention . While these hybrid approaches
achieve promising performance, they have limited scalability in computation compared to purely attention-based
transformers. Vision Transformer (ViT) , which uses a
sequence of embedded image patches as input to a standard
transformer, is the ﬁrst kind of convolution-free transformers that demonstrate comparable performance to CNN models. However, ViT requires very large datasets such as ImageNet21K and JFT300M for training. DeiT 
subsequently shows that data augmentation and model regularization can enable training of high-performance ViT
models with fewer data. Since then, ViT has instantly inspired several attempts to improve its efﬁciency and effectiveness from different aspects .
Along the same line of research on building stronger
vision transformers, in this work, we study how to learn
multi-scale feature representations in transformer models
for image recognition. Multi-scale feature representations
have proven beneﬁcial for many vision tasks , but such potential beneﬁt for vision transformers remains to be validated. Motivated by the effectiveness of multi-branch CNN architectures such as Big-Little
Net and Octave convolutions , we propose a dualbranch transformer to combine image patches (i.e. tokens in
a transformer) of different sizes to produce stronger visual
features for image classiﬁcation. Our approach processes
small and large patch tokens with two separate branches of
different computational complexities and these tokens are
fused together multiple times to complement each other.
Our main focus of this work is to develop feature fusion
methods that are appropriate for vision transformers, which
has not been addressed to the best of our knowledge. We
do so by an efﬁcient cross-attention module, in which each
transformer branch creates a non-patch token as an agent
to exchange information with the other branch by attention.
This allows for linear-time generation of the attention map
in fusion instead of quadratic time otherwise. With some
proper architectural adjustments in computational loads of
each branch, our proposed approach outperforms DeiT 
by a large margin of 2% with a small to moderate increase
in FLOPs and model parameters (See Figure 1).
The main contributions of our work are as follows:
• We propose a novel dual-branch vision transformer to
extract multi-scale feature representations for image
classiﬁcation. Moreover, we develop a simple yet effective token fusion scheme based on cross-attention,
which is linear in both computation and memory to
combine features at different scales.
• Our approach performs better than or on par with several concurrent works based on ViT , and demonstrates comparable results with EfﬁcientNet with
regards to accuracy, throughput and model parameters.
2. Related Works
Our work relates to three major research directions:
convolutional neural networks with attention, vision transformer and multi-scale CNNs. Here, we focus on some representative methods closely related to our work.
CNN with Attention. Attention has been widely used in
many different forms to enhance feature representations,
e.g., SENet uses channel-attention, CBAM adds
the spatial attention and ECANet proposes an efﬁcient channel attention to further improve SENet. There
has also been a lot of interest in combining CNNs with
different forms of self-attention .
SASA and SAN deploy a local-attention layer
to replace convolutional layer. Despite promising results,
prior approaches limited the attention scope to local region due to its complexity. LambdaNetwork recently
introduces an efﬁcient global attention to model both content and position-based interactions that considerably improves the speed-accuracy tradeoff of image classiﬁcation
models. BoTNet replaced the spatial convolutions with
global self-attention in the ﬁnal three bottleneck blocks of
a ResNet resulting in models that achieve a strong performance for image classiﬁcation on ImageNet benchmark.
In contrast to these approaches that mix convolution with
self-attention, our work is built on top of pure self-attention
network like Vision Transformer which has recently
shown great promise in several vision applications.
Vision Transformer. Inspired by the success of Transformers in machine translation, convolution-free models
that only rely on transformer layers have gone viral in computer vision. In particular, Vision Transformer (ViT) 
is the ﬁrst such example of a transformer-based method to
match or even surpass CNNs for image classiﬁcation. Many
variants of vision transformers have also been recently proposed that uses distillation for data-efﬁcient training of vision transformer , pyramid structure like CNNs ,
or self-attention to improve the efﬁciency via learning an
abstract representation instead of performing all-to-all selfattention . Perceiver leverages an asymmetric attention mechanism to iteratively distill inputs into a tight
latent bottleneck, allowing it to scale to handle very large
inputs. T2T-ViT introduces a layer-wise Tokens-to-
Token (T2T) transformation to encode the important local
structure for each token instead of the naive tokenization
used in ViT . Unlike these approaches, we propose a
dual-path architecture to extract multi-scale features for better visual representation with vision transformers.
Multi-Scale CNNs.
Multi-scale feature representations
have a long history in computer vision (e.g., image pyramids , scale-space representation , and coarse-to-ﬁne
approaches ). In the context of CNNs, multi-scale feature representations have been used for detection and recognition of objects at multiple scales , as well
as to speed up neural networks in Big-Little Net and
OctNet . bLVNet-TAM uses a two-branch multiresolution architecture while learning temporal dependencies across frames. SlowFast Networks rely on a similar two-branch model, but each branch encodes different
frame rates, as opposed to frames with different spatial resolutions. While multi-scale features have shown to beneﬁt
CNNs, it’s applicability for vision transformer still remains
as a novel and largely under-addressed problem.
Our method is built on top of vision transformer , so
we ﬁrst present a brief overview of ViT and then describe
our proposed method (CrossViT) for learning multi-scale
features for image classiﬁcation.
Transformer Encoder ⨉N
Transformer Encoder ⨉M
Cross-Attention ⨉L
: CLS token
Linear projection
Linear projection
Small patch size Ps
Large patch size Pl
MLP header
MLP header
Multi-Scale Transformer Encoder ⨉K
: Image patch token
Figure 2: An illustration of our proposed transformer
architecture for learning multi-scale features with crossattention (CrossViT). Our architecture consists of a stack
of K multi-scale transformer encoders. Each multi-scale
transformer encoder uses two different branches to process
image tokens of different sizes (Ps and Pl, Ps < Pl) and
fuse the tokens at the end by an efﬁcient module based on
cross attention of the CLS tokens. Our design includes different numbers of regular transformer encoders in the two
branches (i.e. N and M) to balance computational costs.
3.1. Overview of Vision Transformer
Vision Transformer (ViT) ﬁrst converts an image
into a sequence of patch tokens by dividing it with a certain patch size and then linearly projecting each patch into
tokens. An additional classiﬁcation token (CLS) is added
to the sequence, as in the original BERT . Moreover,
since self-attention in the transformer encoder is positionagnostic and vision applications highly need position information, ViT adds position embedding into each token, including the CLS token. Afterwards, all tokens are passed
through stacked transformer encoders and ﬁnally the CLS
token is used for classiﬁcation. A transformer encoder is
composed of a sequence of blocks where each block contains multiheaded self-attention (MSA) with a feed-forward
network (FFN). FFN contains two-layer multilayer perceptron with expanding ratio r at the hidden layer, and one
GELU non-linearity is applied after the ﬁrst linear layer.
Layer normalization (LN) is applied before every block, and
residual shortcuts after every block. The input of ViT, x0,
and the processing of the k-th block can be expressed as
x0 = [xcls||xpatch] + xpos
yk = xk−1 + MSA(LN(xk−1))
xk = yk + FFN(LN(yk)),
where xcls ∈R1×C and xpatch ∈RN×C are the CLS
and patch tokens respectively and xpos ∈R(1+N)×C is the
position embedding. N and C are the number of patch tokens and dimension of the embedding, respectively.
It is worth noting that one very different design of ViT
from CNNs is the CLS token. In CNNs, the ﬁnal embedding
is usually obtained by averaging the features over all spatial
locations while ViT uses the CLS that interacts with patch
tokens at every transformer encoder as the ﬁnal embedding.
Thus, we consider CLS as an agent that summarizes all the
patch tokens and hence the proposed module is designed
based on CLS to form a dual-path multi-scale ViT.
3.2. Proposed Multi-Scale Vision Transformer
The granularity of the patch size affects the accuracy
and complexity of ViT; with ﬁne-grained patch size, ViT
can perform better but results in higher FLOPs and memory
consumption. For example, the ViT with a patch size of 16
outperforms the ViT with a patch size of 32 by 6% but the
former needs 4× more FLOPs. Motivated by this, our proposed approach is trying to leverage the advantages from
more ﬁne-grained patch sizes while balancing the complexity. More speciﬁcally, we ﬁrst introduce a dual-branch ViT
where each branch operates at a different scale (or patch
size in the patch embedding) and then propose a simple yet
effective module to fuse information between the branches.
Figure 2 illustrates the network architecture of our
proposed Cross-Attention Multi-Scale Vision Transformer
(CrossViT). Our model is primarily composed of K multiscale transformer encoders where each encoder consists of
two branches: (1) L-Branch: a large (primary) branch
that utilizes coarse-grained patch size (Pl) with more transformer encoders and wider embedding dimensions, (2) S-
Branch: a small (complementary) branch that operates
at ﬁne-grained patch size (Ps) with fewer encoders and
smaller embedding dimensions. Both branches are fused
together L times and the CLS tokens of the two branches
at the end are used for prediction. Note that for each token
of both branches, we also add a learnable position embedding before the multi-scale transformer encoder for learning
position information as in ViT .
Effective feature fusion is the key for learning multiscale feature representations. We explore four different fu-
(a) All-attention
(d) Cross-attention
(c) Pairwise
(b) Class token
: CLS token
: Image patch token
Figure 3: Multi-scale fusion. (a) All-attention fusion where all tokens are bundled together without considering any characteristic of tokens. (b) Class token fusion, where only CLS tokens are fused as it can be considered as global representation
of one branch. (c) Pairwise fusion, where tokens at the corresponding spatial locations are fused together and CLS are fused
separately. (d) Cross-attention, where CLS token from one branch and patch tokens from another branch are fused together.
sion strategies: three simple heuristic approaches and the
proposed cross-attention module as shown in Figure 3. Below we provide the details on these fusion schemes.
3.3. Multi-Scale Feature Fusion
Let xi be the token sequence (both patch and CLS tokens) at branch i, where i can be l or s for the large (primary) or small (complementary) branch. xi
cls and xi
represent CLS and patch tokens of branch i, respectively.
All-Attention Fusion.
A straightforward approach is to
simply concatenate all the tokens from both branches without considering the property of each token and then fuse
information via the self-attention module, as shown in Figure 3(a).
This approach requires quadratic computation
time since all tokens are passed through the self-attention
module. The output zi of the all-attention fusion scheme
can be expressed as
f l(xl) || f s(xs)
, o = y + MSA(LN(y)),
, zi = gi(oi),
where f i(·) and gi(·) are the projection and back-projection
functions to align the dimension.
Class Token Fusion. The CLS token can be considered as
an abstract global feature representation of a branch since
it is used as the ﬁnal embedding for prediction. Thus, a
simple approach is to sum the CLS tokens of two branches,
as shown in Figure 3(b). This approach is very efﬁcient as
only one token needs to be processed. Once CLS tokens are
fused, the information will be passed back to patch tokens
at the later transformer encoder. More formally, the output
zi of this fusion module can be represented as
cls)) || xi
where f i(·) and gi(·) play the same role as Eq. 2.
Pairwise Fusion. Figure 3(c) shows how both branches are
fused in pairwise fusion. Since patch tokens are located at
its own spatial location of an image, a simple heuristic way
for fusion is to combine them based on their spatial location. However, the two branches process patches of different sizes, thus having different number of patch tokens. We
ﬁrst perform an interpolation to align the spatial size, and
then fuse the patch tokens of both branches in a pair-wise
manner. On the other hand, the two CLS are fused separately. The output zi of pairwise fusion of branch l and s
can be expressed as
cls)) || gi(
where f i(·) and gi(·) play the same role as Eq. 2.
Cross-Attention Fusion. Figure 3(d) shows the basic idea
of our proposed cross-attention, where the fusion involves
the CLS token of one branch and patch tokens of the other
branch. Speciﬁcally, in order to fuse multi-scale features
more efﬁciently and effectively, we ﬁrst utilize the CLS token at each branch as an agent to exchange information
among the patch tokens from the other branch and then back
project it to its own branch. Since the CLS token already
learns abstract information among all patch tokens in its
own branch, interacting with the patch tokens at the other
branch helps to include information at a different scale. After the fusion with other branch tokens, the CLS token interacts with its own patch tokens again at the next transformer
encoder, where it is able to pass the learned information
from the other branch to its own patch tokens, to enrich the
representation of each patch token. In the following, we
describe the cross-attention module for the large branch (Lbranch), and the same procedure is performed for the small
branch (S-branch) by simply swapping the index l and s.
An illustration of the cross-attention module for the large
branch is shown in Figure 4. Speciﬁcally, for branch l, it
ﬁrst collects the patch tokens from the S-Branch and concatenates its own CLS tokens to them, as shown in Eq. 5.
Small Branch
Large Branch
Figure 4: Cross-attention module for Large branch. The
CLS token of the large branch (circle) serves as a query token to interact with the patch tokens from the small branch
through attention. f l(·) and gl(·) are projections to align
dimensions. The small branch follows the same procedure
but swaps CLS and patch tokens from another branch.
cls) || xs
where f l(·) is the projection function for dimension alignment. The module then performs cross-attention (CA) between xl
cls and x′l, where CLS token is the only query as
the information of patch tokens are fused into CLS token.
Mathematically, the CA can be expressed as
k = x′lWk,
v = x′lWv,
A = softmax(qkT /
CA(x′l) = Av,
where Wq, Wk, Wv ∈RC×(C/h) are learnable parameters, C and h are the embedding dimension and number
of heads. Note that since we only use CLS in the query,
the computation and memory complexity of generating the
attention map (A) in cross-attention are linear rather than
quadratic as in all-attention, making the entire process more
efﬁcient. Moreover, as in self-attention, we also use multiple heads in the CA and represent it as (MCA). However, we
do not apply a feed-forward network FFN after the crossattention. Speciﬁcally, the output zl of a cross-attention
module of a given xl with layer normalization and residual
shortcut is deﬁned as follows.
cls =f l  xl
cls) || xs
where f l(·) and gl(·) are the projection and back-projection
function for dimension alignment, respectively. We empirically show in Section 4.3 that cross-attention achieves the
best accuracy compared to other three simple heuristic approaches while being efﬁcient for mult-scale feature fusion.
4. Experiments
In this section, we conduct extensive experiments to
show the effectiveness of our proposed CrossViT over existing methods. First, we check the advantages of our proposed model over the baseline DeiT in Table 2, and then we
compare with serveral concurrent ViT variants and CNNbased models in Table 3 and Table 4, respectively. Moreover, we also test the transferability of CrossViT on 5 downstream tasks (Table 5). Finally, we perform ablation studies
on different fusion schemes in Table 6 and discuss the effect
of different parameters of CrossViT in Table 7.
4.1. Experimental Setup
Dataset. We validate the effectiveness of our proposed approach on the ImageNet1K dataset , and use the top-1
accuracy on the validation set as the metrics to evaluate
the performance of a model. ImageNet1K contains 1,000
classes and the number of training and validation images
are 1.28 millions and 50,000, respectively. We also test
the transferability of our approach using several smaller
datasets, such as CIFAR10 and CIFAR100 .
Training and Evaluation. The original ViT achieves
competitive results compared to some of the best CNN
models but only when trained on very large-scale datasets
ImageNet21K and JFT300M ).
Nevertheless, DeiT shows that with the help of a rich set of
data augmentation techniques, ViT can be trained from ImageNet alone to produce comparable results to CNN models.
Therefore, in our experiments, we build our models based
on DeiT , and apply their default hyper-parameters for
training. These data augmentation methods include rand
augmentation , mixup and cutmix as well as
random erasing .
We also apply drop path for
model regularization but instance repetition is only enabled for CrossViT-18 as it does not improve small models.
We train all our models for 300 epochs (30 warm-up
epochs) on 32 GPUs with a batch size of 4,096. Other setup
includes a cosine linear-rate scheduler with linear warm-up,
an initial learning rate of 0.004 and a weight decay of 0.05.
During evaluation, we resize the shorter side of an image to
256 and take the center crop 224×224 as the input. Moreover, we also ﬁne-tuned our models with a larger resolution
(384×384) for fair comparison in some cases. Bicubic interpolation was applied to adjust the size of the learnt position embedding, and the ﬁnetuning took 30 epochs. More
details can be found in supplementary material.
Patch size
# of heads
CrossViT-Ti
CrossViT-S
CrossViT-B
CrossViT-9
CrossViT-15
CrossViT-18
CrossViT-9†
CrossViT-15†
CrossViT-18†
Table 1: Model architectures of CrossViT. K = 3, N =
1, L = 1 for all models, and number of heads are same
for both branches. K denotes the number of multi-scale
transformer encoders. M, N and L denote the number of
transformer encoders of the small and large branches and
the cross-attention modules in one multi-scale transformer
encoder. r is the expanding ratio of feed-forward network
(FFN) in the transformer encoder. See Figure 2 for details.
Models. Table 1 speciﬁes the architectural conﬁgurations
of the CrossViT models used in our evaluation. Among
these models, CrossViT-Ti, CrossViT-S and CrossViT-B
set their large (primary) branches identical to the tiny (DeiT-
Ti), small (DeiT-S) and base (DeiT-B) models introduced in
DeiT , respectively. The other models vary by different
expanding ratios in FFN (r), depths and embedding dimensions. In particular, the ending number in a model name
tells the total number of transformer encoders in the large
branch used. For example, CrossViT-15 has 3 multi-scale
encoders, each of which includes 5 regular transformers, resulting in a total of 15 transformer encoders.
The original ViT paper shows that a hybrid approach
that generates patch tokens from a CNN model such as
ResNet-50 can improve the performance of ViT on the ImageNet1K dataset. Here we experiment with a similar idea
by substituting the linear patch embedding in ViT by three
convolutional layers as the patch tokenizer. These models
are differentiated from others by a sufﬁx † in Table 1.
4.2. Main Results
Comparisons with DeiT. DeiT is a better trained version of ViT, we thus compare our approach with three baseline models introduced in DeiT, i.e., DeiT-Ti,DeiT-S and
DeiT-B. It can be seen from Table 2 that CrossViT improves DeiT-Ti, DeiT-S and DeiT-B by 1.2%, 1.2% and
0.4% points respectively when they are used as the primary branch of CrossViT. This clearly demonstrates that
our proposed cross-attention is effective in learning multiscale transformer features for image recognition. By making a few architectural changes (see Table 1), CrossViT
further raises the accuracy of the baselines by another 0.3-
0.5% point, with only a small increase in FLOPs and model
parameters.
Surprisingly, the convolution-based embedding provides a signiﬁcant performance boost to CrossViT-
Top-1 Acc.
Throughput
(images/s)
CrossViT-Ti
73.4 (+1.2)
CrossViT-9
73.9 (+0.5)
CrossViT-9†
77.1 (+3.2)
CrossViT-S
81.0 (+1.2)
CrossViT-15
81.5 (+0.5)
CrossViT-15†
82.3 (+0.8)
CrossViT-B
82.2 (+0.4)
CrossViT-18
82.5 (+0.3)
CrossViT-18†
82.8 (+0.3)
Comparisons with DeiT baseline on ImageNet1K. The numbers in the bracket show the improvement from each change. See Table 1 for model details.
9 (+3.2%) and CrossViT-15 (+0.8%). As the number of
transformer encoders increases, the effectiveness of convolution layers seems to become weaker, but CrossViT-
18† still gains another 0.3% improvement over CrossViT-
18. We would like to point out that the work of T2T 
concurrently proposes a different approach based on tokento-token transformation to address the limitation of linear
patch embedding in vision transformer.
Despite the design of CrossViT is intended for accuracy,
the efﬁciency is also considered. E.g., CrossViT-9† and
CrossViT-15† incur 30-50% more FLOPs and parameters
than the baselines. However, their accuracy is considerably
improved by ∼2.5-5%. On the other hand, CrossViT-18†
reduces the FLOPs and parameters almost by half compared
to DeiT-B while still being 1.0% more accurate.
Comparisons with SOTA Transformers. We further compare our proposed approach with some very recent concurrent works on vision transformers. They all improve the
original ViT with respect to efﬁciency, accuracy or
both. As shown in Table 3, CrossViT-15† outperforms the
small models of all the other approaches with comparable
FLOPs and parameters. Interestingly when compared with
ViT-B, CrossViT-18† signiﬁcantly outperforms it by 4.9%
(77.9% vs 82.8%) in accuracy while requiring 50% less
FLOPs and parameters. Furthermore, CrossViT-18† performs as well as TNT-B and better than the others, but also
has fewer FLOPs and parameters. Our approach is consistently better than T2T-ViT and PVT in terms of
accuracy and FLOPs, showing the efﬁcacy of multi-scale
features in vision transformers.
Comparisons with CNN-based Models.
models are dominant in computer vision applications. In
this experiment, we compare our proposed approach with
Top-1 Acc. (%)
Params (M)
Peceiver 
DeiT-S 
CentroidViT-S 
PVT-S 
PVT-M 
T2T-ViT-14 
TNT-S 
CrossViT-15 (Ours)
CrossViT-15† (Ours)
ViT-B@384 
DeiT-B 
PVT-L 
T2T-ViT-19 
T2T-ViT-24 
TNT-B 
CrossViT-18 (Ours)
CrossViT-18† (Ours)
∗: We recompute the ﬂops by using our tools.
Table 3: Comparisons with recent transformer-based
models on ImageNet1K. All models are trained using only
ImageNet1K dataset. Numbers are referenced from their
recent version as of the submission date.
Top-1 Acc.
Throughput
(images/s)
ResNet-101 
ResNet-152 
ResNeXt-101-32×4d 
ResNeXt-101-64×4d 
SEResNet-101 
SEResNet-152 
SENet-154 
ECA-Net101 
ECA-Net152 
RegNetY-8GF 
RegNetY-12GF 
RegNetY-16GF 
RegNetY-32GF 
EfﬁcienetNet-B4@380 
EfﬁcienetNet-B5@456 
EfﬁcienetNet-B6@528 
EfﬁcienetNet-B7@600 
CrossViT-15
CrossViT-15†
CrossViT-15†@384
CrossViT-18
CrossViT-18†
CrossViT-18†@384
CrossViT-18†@480
Comparisons with CNN models on ImageNet1K. Models are evaluated under 224×224 if not speciﬁed. The inference throughput is measured under a batch
size of 64 on a Nvidia Tesla V100 GPU with cudnn 8.0. We
report the averaged speed over 100 iterations.
some of the best CNN models including both hand-crafted
(e.g., ResNet ) and search based ones (e.g., Efﬁcient-
Net ). In addition to accuracy, FLOPs and parameters,
run-time speed is measured for all the models and shown as
CropDiseases
ChestXRay8
DeiT-S 
DeiT-B 
CrossViT-15
CrossViT-18
∗: numbers reported in the original paper.
Table 5: Transfer learning performance. Our CrossViT
models are very competitive with the recent DeiT models on all the downstream classiﬁcation tasks.
inference throughput (images/second) in Table 4. We follow prior work to report accuracy from the original papers. First, when compared to the ResNet family, including
ResNet , ResNeXt , SENet , ECA-ResNet 
and RegNet , CrossViT-15 outperforms all of them in
accuracy while being smaller and running more efﬁciently
(except ResNet-101, which is slightly faster). In addition,
our best models such as CrossViT-15† and CrossViT-18†,
when evaluated at higher image resolution, are encouragingly competitive against EfﬁcientNet with regard to
accuracy, throughput and parameters. We expect neural architecture search (NAS) to close the performance gap
between our approach and EfﬁcientNet.
Transfer Learning.
Despite our model achieves better accuracy on ImageNet1K compared to the baselines
(Table 2), it is crucial to check generalization of the
models by evaluating transfer performance on tasks with
fewer samples.
We validate this by performing transfer learning on 5 image classiﬁcation tasks, including CI-
FAR10 , CIFAR100 , Pet , CropDisease ,
and ChestXRay8 . While the ﬁrst four datasets contains natural images, ChestXRay8 consists of medical images. We ﬁnetune the whole pretrained models with 1,000
epochs, batch size 768, learning rate 0.01, SGD optimizer,
weight decay 0.0001, and using the same data augmentation in training on ImageNet1K. Table 5 shows the results. While being better in ImageNet1K, our model is on
par with DeiT models on all the downstream classiﬁcation
tasks. This result assures that our models still have good
generalization ability rather than only ﬁt to ImageNet1K.
4.3. Ablation Studies
In this section, we ﬁrst compare the different fusion approaches (Section 3.3), and then analyze the effects of different parameters of our architecture design, including the
patch sizes, the channel width and depth of the small branch
and number of cross-attention modules. At the end, we also
validate that the proposed can cooperate with other concurrent works for better accuracy.
Comparison of Different Fusion Schemes. Table 6 shows
the performance of different fusions schemes, including (I)
no fusion, (II) all-attention, (III) class token fusion, (IV)
Single Branch Acc. (%)
All-Attention
Class Token
Cross-Attention
Table 6: Ablation study with different fusions on ImageNet1K. All models are based on CrossViT-S. Single
branch Acc. is computed using CLS from one branch only.
pairwise fusion, and (V) the proposed cross-attention fusion.
Among all the compared strategies, the proposed
cross-attention fusion achieves the best accuracy with minor
increase in FLOPs and parameters. Surprisingly, despite the
use of additional self-attention to combine information between two branches, all-attention fails to achieve better performance compared to the simple class token fusion. While
the primary L-branch dominates in accuracy by diminishing
the effect of complementary S-branch in other fusion strategies, both of the branches in our proposed cross-attention
fusion scheme achieve certain accuracy and their ensemble
becomes the best, suggesting that these two branches learn
different features for different images.
Effect of Patch Sizes. We perform experiments to understand the effect of patch sizes in our CrossViT by testing
two pairs of patch sizes such as (8, 16) and (12, 16), and
observe that the one with (12, 16) achieves better accuracy
with fewer FLOPs as shown in Table 7 (A). Intuitively, (8,
16) should get better results as patch size of 8 provides more
ﬁne-grained features; however, it is not good as (12, 16) because of the large difference in granularity between the two
branches, which makes it difﬁcult for smooth learning of
the features. For the pair (8, 16), the number of patch tokens are 4× difference while the ratio of patch tokens are
only 2× for the model with (12, 16).
Channel Width and Depth in S-branch.
Despite our
cross-attention is designed to be light-weight, we check the
performance by using a more complex S-branch, as shown
in Table 7 (B and C). Both models increase FLOPs and parameters without any improvement in accuracy, which we
think is due to the fact that L-branch has the main role to
extract features while S-branch only provides additional information; thus, a light-weight branch is enough.
Depth of Cross-Attention and Number of Multi-Scale
Transformer Encoders.
To increase frequency of fusion across two branches, we can either stack more crossattention modules (L) or stack more multi-scale transformer
encoders (K) (by reducing M to keep the same total depth
of a model). Results are shown in Table 7 (D and E). With
CrossViT-S as baseline, too frequent fusion of branches
does not provide any performance improvement but intro-
Patch size
CrossViT-S
Ablation study with different architecture
parameters on ImageNet1K. The blue color indicates
changes from CrossViT-S.
duces more FLOPs and parameters. This is because patch
token from the other branch is untouched, and the advantages from stacking more than one cross-attention is small
as cross-attention is a linear operation without any nonlinearity function.
Likewise, using more multi-scale transformer encoders also does not help in performance which
is the similar case to increase the capacity of S-branch.
Importance of CLS Tokens.
We experiment with one
model based on CrossViT-S without CLS tokens, where
the model averages the patch tokens of one branch as the
CLS token for cross attention with the other branch. This
model achieved 80.0% accuracy which is is 1% worse than
CrossViT-S (81.0%) on ImageNet1K, showing effectiveness of CLS token in summarizing information of current
branch for passing to another one through cross-attention.
Cooperation with Concurrent Works.
Our proposed
cross-attention is also capable of cooperating with other
concurrent ViT variants. We consider T2T-ViT as a
case study and use the T2T module to replace linear projection of patch embedding in both branches on CrossViT-18.
CrossViT-18+T2T achieves an top-1 accuracy of 83.0% on
ImageNet1K, additional 0.5% improvement over CrossViT-
18. This shows that our proposed cross-attention is also capable of learning multi-scale features for other ViT variants.
5. Conclusion
In this paper, we present CrossViT, a dual-branch vision
transformer for learning multi-scale features, to improve the
recognition accuracy for image classiﬁcation. To effectively
combine image patch tokens of different scales, we further
develop a fusion method based on cross-attention to exchange information between two branches efﬁciently in linear time. With extensive experiments, we demonstrate that
our proposed model performs better than or on par with several concurrent works on vision transformer, in addition to
efﬁcient CNN models. While our current work scratches the
surface on multi-scale vision transformers for image classiﬁcation, we anticipate that in future there will be more
works in developing efﬁcient multi-scale transformers for
other vision applications, including object detection, semantic segmentation, and video action recognition.