The Creation and Detection of Deepfakes: A Survey
YISROEL MIRSKYâˆ—, Georgia Institute of Technology and Ben-Gurion University
WENKE LEE, Georgia Institute of Technology
Generative deep learning algorithms have progressed to a point where it is diï¬ƒcult to tell the diï¬€erence between
what is real and what is fake. In 2018, it was discovered how easy it is to use this technology for unethical
and malicious applications, such as the spread of misinformation, impersonation of political leaders, and the
defamation of innocent individuals. Since then, these â€˜deepfakesâ€™ have advanced signiï¬cantly.
In this paper, we explore the creation and detection of deepfakes an provide an in-depth view how these
architectures work. Te purpose of this survey is to provide the reader with a deeper understanding of (1) how
deepfakes are created and detected, (2) the current trends and advancements in this domain, (3) the shortcomings
of the current defense solutions, and (4) the areas which require further research and atention.
CCS Concepts: â€¢Security and privacy â†’Social engineering attacks; Human and societal aspects of security
and privacy; â€¢Computing methodologies â†’Machine learning;
Additional Key Words and Phrases: Deepfake, Deep fake, reenactment, replacement, face swap, generative AI,
social engineering, impersonation
ACM Reference format:
Yisroel Mirsky and Wenke Lee. 2020. Te Creation and Detection of Deepfakes: A Survey. ACM Comput. Surv.
1, 1, Article 1 , 38 pages.
DOI: XX.XXXX/XXXXXXX.XXXXXXX
INTRODUCTION
A deepfake is content, generated by an artiï¬cial intelligence, that is authentic in the eyes of a human
being. Te word deepfake is a combination of the words â€˜deep learningâ€™ and â€˜fakeâ€™ and primarily
relates to content generated by an artiï¬cial neural network, a branch of machine learning.
Te most common form of deepfakes involve the generation and manipulation of human imagery.
Tis technology has creative and productive applications. For example, realistic video dubbing of
foreign ï¬lms,1 education though the reanimation of historical ï¬gures , and virtually trying on
clothes while shopping.2 Tere are also numerous online communities devoted to creating deepfake
memes for entertainment,3 such as music videos portraying the face of actor Nicolas Cage.
However, despite the positive applications of deepfakes, the technology is infamous for its unethical and malicious aspects. At the end of 2017, a Reddit user by the name of â€˜deepfakesâ€™ was using
âˆ—Corresponding Author
1htps://variety.com/2019/biz/news/
ai-dubbing-david-beckham-multilingual-1203309213/
2htps://www.forbes.com/sites/forbestechcouncil/2019/05/21/gans-and-deepfakes-could-revolutionize-the-fashion-industry/
3htps://www.reddit.com/r/SFWdeepfakes/
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for proï¬t or commercial advantage and that copies bear this notice and
the full citation on the ï¬rst page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permited. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior speciï¬c permission and/or a fee. Request permissions from .
Â© 2020 ACM. 0360-0300/2020/1-ART1 $15.00
DOI: XX.XXXX/XXXXXXX.XXXXXXX
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
 
Mirsky, et al.
III. Entertainment
IV. Trusted
II. Propaganda
Intention to mislead
Scams & Fraud:
Trickery via spoofing, falsifying audit
records, generating artwork, â€¦
Tampering of Evidence:
Medical, forensic, court, â€¦
Harming Credibility:
Revenge porn, political sabotage via
generated videos or articles, â€¦
Altering Published Movies:
Comedy, satire, â€¦
Editing & Special Effects:
Generating actors in movies, â€¦
Art & Demonstration:
Animating dead characters, generated
portraits, technology demos, â€¦
Authentic Content:
Credible Multimedia / Data
Misdirection
Generated discourse to amplify
events / facts, â€¦
Political Warfare:
Tone change of articles, content
loosely based on facts, conspiracyâ€¦
Corruption:
Increased xenophobia, â€¦
Fig. 1. A deepfake information trust chart.
Samples created by machines
Adversarial
â€¦humans: entertainment, impersonation, art fraud.
â€¦machines: hiding a stop sign, evading face recog.
â€¦both: tampering medical scans, malware evasion.
Fig. 2. The diï¬€erence between adversarial
machine learning and deepfakes.
deep learning to swap faces of celebrities into pornographic videos, and was posting them online4.
Te discovery caused a media frenzy and a large number of new deepfake videos began to emerge
thereafer. In 2018, BuzzFeed released a deepfake video of former president Barak Obama giving
a talk on the subject. Te video was made using the Reddit userâ€™s sofware (FakeApp), and raised
concerns over identity thef, impersonation, and the spread of misinformation on social media. Fig.
presents an information trust chart for deepfakes, inspired by .
Following these events, the subject of deepfakes gained traction in the academic community, and
the technology has been rapidly advancing over the last few years. Since 2017, the number of papers
published on the subject rose from 3 to over 250 .
To understand where the threats are moving and how to mitigate them, we need a clear view of the
technologyâ€™s, challenges, limitations, capabilities, and trajectory. Unfortunately, to the best of our
knowledge, there are no other works which present the techniques, advancements, and challenges,
in a technical and encompassing way. Terefore, the goals of this paper are (1) to provide the reader
with an understanding of how modern deepfakes are created and detected, (2) to inform the reader of
the recent advances, trends, and challenges in deepfake research, (3) to serve as a guide to the design
of deepfake architectures, and (4) to identify the current status of the atacker-defender game, the
atackerâ€™s next move, and future work that may help give the defender a leading edge.
We achieve these goals through an overview of human visual deepfakes (Section 2), followed by a
technical background which identiï¬es technologyâ€™s basic building blocks and challenges (Section 3).
We then provide a chronological and systematic review for each category of deepfake, and provide the
networksâ€™ schematics to give the reader a deeper understanding of the various approaches (Sections
4 and 5). Finally, afer reviewing the countermeasures (Section 6), we discuss their weaknesses, note
the current limitations of deepfakes, suggest alternative research, consider the adversaryâ€™s next steps,
and raise awareness to the spread of deepfakes to other domains (Section 7).
Scope. In this survey we will focus on deepfakes pertaining to the human face and body. We will
not be discussing the synthesis of new faces or the editing of facial features because they do not
have a clear atack goal associated with them. In Section 7.3 we will discuss deepfakes with a much
4htps://www.vice.com/en us/article/gydydm/gal-gadot-fake-ai-porn
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
broader scope, note the future trends, and exemplify how deepfakes have spread to other domains
and media such as forensics, ï¬nance, and healthcare.
We note to the reader that deepfakes should not be confused with adversarial machine learning,
which is the subject of fooling machine learning algorithms with maliciously crafed inputs (Fig. 2).
Te diï¬€erence being that for deepfakes, the objective of the generated content is to fool a human
and not a machine.
OVERVIEW & ATTACK MODELS
We deï¬ne a deepfake as
â€œBelievable media generated by a deep neural networkâ€
In the context of human visuals, we identify four categories: reenactment, replacement, editing, and
synthesis. Fig. 3 illustrates some examples facial deepfakes in each of these categories and their
sub-types. Troughout this paper we denote s and t as the source and the target identities. We also
denote xs and xt as images of these identities and xĞ´ as the deepfake generated from s and t.
Reenactment
A reenactment deepfake is where xs is used to drive the expression, mouth, gaze, pose, or body of xt:
Expression reenactment is where xs drives the expression of xt. It is the most common form of
reenactment since these technologies ofen drive targetâ€™s mouth and pose as well, providing a
wide range of ï¬‚exibility. Benign uses are found in the movie and video game industry where
the performances of actors are tweaked in post, and in educational media where historical
ï¬gures are reenacted.
Mouth reenactment, also known as â€˜dubbingâ€™, is where the mouth of xt is driven by that of xs, or
an audio input as containing speech. Benign uses of the technology includes realistic voice
dubbing into another language and editing.
Gaze reenactment is where direction ofxtâ€™s eyes, and the position of the eyelids, are driven by those
of xs. Tis is used to improve photographs or to automatically maintain eye contact during
video interviews .
Pose reenactment is where the head position of xt is driven by xs. Tis technology has primarily been used for face frontalization of individuals in security footage, and as a means for
improving facial recognition sofware .
Body reenactment, a.k.a. pose transfer and human pose synthesis, is similar to the facial reenactments listed above except thatâ€™s its the pose of xtâ€™s body being driven.
Expression
Face Replacement
Facial Reenactment
â—‹: Sometimes
Face Editing
Expression
Face Synthesis
Transfers:
Fig. 3. Examples of reenactment, replacement, editing, and synthesis deepfakes of the human face.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
Te Attack Model. Reenactment deep fakes give atackers the ability to impersonate an identity,
controlling what he or she says or does. Tis enables an atacker to perform acts of defamation, cause
discredability, spread misinformation, and tamper with evidence. For example, an atacker can impersonate t to gain trust the of a colleague, friend, or family member as a means to gain access to money,
network infrastructure, or some other asset. An atacker can also generate embarrassing content of t
forblackmailingpurposesorgeneratecontenttoaï¬€ectthepublicâ€™sopinionofanindividualorpolitical
leader. Tetechnologycanalsobeusedtotampersurveillancefootageorsomeotherarchivalimagery
in an atempt to plant false evidence in a trial. Finally, the atack can either take place online (e.g.,
impersonating someone in a real-time conversation) or oï¬„ine (e.g., fake media spread on the Internet).
Replacement
A replacement deepfake is where the content of xt is replaced with that of xs, preserving the identity
Transfer is where the content of xt is replaced with that of xs. A common type of transfer is facial
transfer, used in the fashion industry to visualize an individual in diï¬€erent outï¬ts.
Swap is where the content transferred to xt from xs is driven by xt. Te most popular type of swap
replacement is â€˜face swapâ€™, ofen used to generate memes or satirical content by swapping the
identity of an actor with that of a famous individual. Another benign use for face swapping includes the anonymization of oneâ€™s identity in public content in-place of blurring or pixelation.
Te Attack Model. Replacement deepfakes are well-known for their harmful applications. For
example, revenge porn is where an atacker swaps a victimâ€™s face onto the body of a porn actress
to humiliate, defame, and blackmail the victim. Face replacement can also be used as a short-cut to
fully reenacting t by transferring tâ€™s face onto the body of a look-alike. Tis approach has been used
as a tool for disseminating political opinions in the past .
Editing & Synthesis
An enchantment deepfake is where the atributes ofxt are added, altered, or removed. Some examples
include the changing a targetâ€™s clothes, facial hair, age, weight, beauty, and ethnicity. Apps such as
FaceApp enable users to alter their appearance for entertainment and easy editing of multimedia.
Te same process can be used by and atacker to build a false persona for misleading others. For
example, a sick leader can be made to look healthy , and child or sex predators can change their
age and gender to build dynamic proï¬les online. A known unethical use of editing deepfakes is the
removal of a victimâ€™s clothes for humiliation or entertainment .
Synthesis is where the deepfake xĞ´ is created with no target as a basis. Human face and body
synthesis techniques such as (used in Fig. 3) can create royalty free stock footage or generate
characters for movies and games. However, similar to editing deepfakes, it can also be used to create
fake personas online.
Although human image editing and synthesis are active research topics, reenactment and replacement deepfakes are the greatest concern because they give an atacker control over oneâ€™s identity . Terefore, in this survey we will be focusing on reenactment and replacement deepfakes.
TECHNICAL BACKGROUND
Although there are a wide variety of neural networks, most deepfakes are created using variations
or combinations of generative networks and encoder decoder networks. In this section we provide a
brief introduction to these networks, how they are trained, and the notations which we will be using
throughout the paper.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
Neural Networks
Neural networks are non-linear models for predicting or generating content based on an input. Tey
are made up of layers of neurons, where each layer is connected sequentially via synapses. Te
synapses have associated weights which collectively deï¬ne the concepts learned by the model. To
execute a network on ann-dimensional inputx, a process known as forward-propagation is performed
where x propagated through each layer and an activation function is used to summarize a neuronâ€™s
output (e.g., the Sigmoid or ReLU function).
Concretely,letl(i) denotethei-thlayerinthenetworkM,andlet âˆ¥l(i)âˆ¥denotethenumberofneurons
in l(i). Finally, let the total number of layers in M be denoted as L. Te weights which connect l(i) to
l(i+1) are denoted as the âˆ¥l(i)âˆ¥-by-âˆ¥l(i+1)âˆ¥matrixW (i) and âˆ¥l(i+1)âˆ¥dimensional bias vector Â®b(i). Finally,
we denote the collection of all parameters Î¸ as the tuple Î¸ â‰¡(W ,b), whereW andb are the weights of
each layer respectively. Let a(i+1) denote the output (activation) of layer l(i) obtained by computing
W (i)Â· Â®a(i)+ Â®
where f is ofen the Sigmoid or ReLU function. To execute a network on an ndimensional inputx, a process known as forward-propagation is performed wherex is used to activate
l(1) which activates l(2) and so on until the activation of l(L) produces them-dimensional outputy.
To summarize this process, we consider M a black box and denote its execution as M(x)=y. To
train M in a supervised seting, a dataset of paired samples with the form (xi,yi) is obtained and an
objective loss function L is deï¬ned. Te loss function is used to generate a signal at the output of
M which is back-propagated through M to ï¬nd the errors of each weight. An optimization algorithm,
such as gradient descent (GD), is then used to update the weights for a number of epochs. Te
function L is ofen a measure of error between the input x and predicted outputyâ€². As a result the
network learns the function M(xi)â‰ˆyi and can be used to make predictions on unseen data.
Some deepfake networks use a technique called one-shot or few-shot learning which enables
a pre-trained network to adapt to a new dataset X â€² similar to X on which it was trained. Two
common approaches for this are to (1) pass information on x â€² âˆˆX â€² to the inner layers of M during the
feed-forward process, and (2) perform a few additional training iterations on a few samples from X â€².
Loss Functions
In order to update the weights with an optimization algorithm, such as GD, the loss function must
be diï¬€erentiable. Tere are various types of loss functions which can be applied in diï¬€erent ways
depending on the learning objective. For example, when training a M as an n-class classiï¬er, the
output of M would be the probability vector y âˆˆRn. To train M, we perform forward-propagation
to obtainy
â€² =M(x), compute the cross-entropy loss (LCE) by comparingyâ€² to the ground truth label
y, and then perform back-propagation and to update the weights with the training signal. Te loss
LCE over the entire training set X is calculated as
yi[c]log(yâ€²
whereyâ€²[c] is the predicted probability of xi belonging to the c-th class.
Other popular loss functions used in deepfake networks include the L1 and L2 norms L1 = |xâˆ’xĞ´|1
and L2 = |xâˆ’xĞ´|2. However, L1 and L2 require paired images (e.g., of s and t with same expression)
and perform poorly when there are large oï¬€sets between the images such as diï¬€erent poses or facial
features. Tis ofen occurs in reenactment when xt has a diï¬€erent pose than xs which is reï¬‚ected
in xĞ´, and ultimately weâ€™d like xĞ´ to match the appearance of xt.
One approach to compare two unaligned images is to pass them through another network (a
perceptual model) and measure the diï¬€erence between the layerâ€™s activations (feature maps). Tis
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
Encoder Decoder
Vanilla GAN
Generative
Discriminator
Fig. 4. Five basic neural network architectures used to create deepfakes. The lines indicate dataflows used
during deployment (black) and training (grey).
loss is called the perceptual loss (Lperc) and is described in for image generation tasks. In the
creation of deepfakes, Lperc is ofen computed using a face recognition network such as VGGFace.
Te intuition behind Lperc is that the feature maps (inner layer activations) of the perceptual model
act as a normalized representation of x in the context of how the model was trained. Terefore,
by measuring the distance between the feature maps of two diï¬€erent images, we are essentially
measuring their semantic diï¬€erence (e.g., how similar the noses are to each other and other ï¬ner
details.) Similar to Lperc, there is a feature matching loss (LF M) which uses the last output
of a network. Te idea behind LF M is to consider the high level semantics captured by the last layer
of the perceptual model (e.g., the general shape and textures of the head).
Another common loss is a type of content loss (LC) which is used to help the generator create
realistic features, based on the perspective of a perceptual model. In LC, only xĞ´ is passed through
the perceptual model and the diï¬€erence between the networkâ€™s feature maps are measured.
Generative Neural Networks (for deepfakes)
Deep fakes are ofen created using combinations or variations of six diï¬€erent networks, ï¬ve of which
are illustrated in Fig. 4.
Encoder-Decoder Networks (ED). An ED consists of at least two networks, an encoder En and
decoder De. Te ED has narrower layers towards its center so that when itâ€™s trained as
De(En(x))=xĞ´, the network is forced to summarize the observed concepts. Te summary
of x, given its distribution X, is En(x) = e, ofen referred to as an encoding or embedding
and E =En(X) is referred to as the â€˜latent spaceâ€™. Deepfake technologies ofen use multiple
encoders or decoders and manipulate the encodings to inï¬‚uence the output xĞ´. If an encoder
and decoder are symmetrical, and the network is trained with the objective De(En(x))=x,
then the network is called an autoencoder and the output is the reconstruction of x denoted
Ë†x. Another special kind of ED is the variational autorencoder (VAE) where the encoder learns
the posterior distribution of the decoder given X. VAEs are beter at generating content than
autoencoders because the concepts in the latent space are disentangled, and thus encodings
respond beter to interpolation and modiï¬cation.
Convolutional Neural Network (CNN). In contrast to a fully connected (dense) network, a CNN
learnspaternhierarchiesinthedataandisthereforemuchmoreeï¬ƒcientathandlingimagery.
A convolutional layer in a CNN learns ï¬lters which are shifed over the input forming an
abstract feature map as the output. Pooling layers are used to reduce the dimensionality as
the network gets deeper and up-sampling layers are used to increase it. With convolutional,
pooling, and upsampling layers, it is possible to build an ED CNNs for imagery.
Generative Adversarial Networks (GAN) Te GAN was ï¬rst proposed in 2014 by Goodfellow
et al. in . A GANs consist of two neural networks which work against each other: the
generator G and the discriminator D. G creates fake samples xĞ´ with the aim of fooling D,
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
and D learns to diï¬€erentiate between real samples (x âˆˆX) and fake samples (xĞ´ =G(z) where
z âˆ¼N). Concretely, there is an adversarial loss used to train D andG respectively:
Ladv(D)=maxlogD(x)+log(1âˆ’D(G(z)))
Ladv(G)=minlog(1âˆ’D(G(z)))
Tis zero-sum game leads toG learning how to generate samples that are indistinguishable
from the original distribution. Afer training, D is discarded andG is used to generate content.
When applied to imagery, this approach produces photo realistic images.
Numerous of variations and improvements of GANs have been proposed over the years.
In the creation of deepfakes, there are two popular image translation frameworks which use
the fundamental principles of GANs:
Image-to-Image Translation (pix2pix). Te pix2pix framework enables paired translations from one image domain to another . In pix2pix, G tries to generate the image
xĞ´ given a visual context xc as an input, and D discriminates between (x,xc) and (xĞ´,xc).
Moreover,G is a an ED CNN with skip connections from En to De (called a U-Net) which
enables G to produce high ï¬delity imagery by bypassing the compression layers when
needed. Later, pix2pixHD was proposed for generating high resolution imagery
with beter ï¬delity.
CycleGAN. An improvement of pix2pix which enables image translation through unpaired
training . Te network forms a cycle consisting of two GANs used to convert
images from one domain to another, and then back again to ensure consistency with
a cycle consistency loss (Lcyc).
Recurrent Neural Networks (RNN) An RNN is type of neural network that can handle sequential
and variable length data. Te network remembers is internal state afer processing x(iâˆ’1) and
can use it to processx(i) and so on. In deepfake creation, RNNs are ofen used to handle audio
and sometimes video. More advanced versions of RNNs include long short-term memory
(LSTM) and gate reccurent units (GRU).
Feature Representations
Most deep fake architectures use some form of intermediate representation to capture and sometimes
manipulate s and tâ€™s facial structure, pose, and expression. One way is to use the facial action coding
system (FACS) and measure each of the faceâ€™s taxonomized action units (AU) . Another way is
to use monocular reconstruction to obtain a 3D morphable model (3DMM) of the head from a 2D
image, where the pose and expression are parameterized by a set of vectors and matrices. Ten use
the parameters or a 3D rendering of the head itself. Some use a UV map of the head or body to give
the network a beter understanding of the shapeâ€™s orientation.
Another approach is to use image segmentation to help the network separate the diï¬€erent concepts
(face, hair, etc). Te most common representation is landmarks (a.k.a. key-points) which are a set
of deï¬ned positions on the face or body which can be eï¬ƒciently tracked using open source CV
libraries. Te landmarks are ofen presented to the networks as a 2D image with Gaussian points
at each landmark. Some works separate the landmarks by channel to make it easier for the network
to identity and associate them. Similarly, facial boundaries and body skeletons can also used.
For audio (speech), the most common approach is to split the audio into segments, and for each segment, measure the Mel-Cepstral Coeï¬ƒcients (MCC) which captures the dominant voice frequencies.
Deepfake Creation Basics
To generatexĞ´, reenactment and face swap networks follow some variation of this process (illustrated
in Fig. 5): Pass x through a pipeline that (1) detects and crops the face, (2) extracts intermediate
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
ğ‘¥ğ‘ and/or ğ‘¥ğ‘¡
Detect & Crop
Intermediate Representation
Preprocessing
Generation
Postprocessing
Driver and/or
Landmarks/
key points
Boundaries/
Parameters
Fig. 5. The processing pipeline for making reenactment and face swap deepfakes. Usually only a subset of
these steps are performed.
representations, (3) generates a new face based on some driving signal (e.g., another face), and then
(4) blends the generated face back into the target frame.
In general there are six approaches to driving an image:
(1) Let a network work directly on the image and perform the mapping itself.
(2) Train an ED network to disentangle the identity from the expression, and then modify/swap
the encodings of the target the before passing it through the decoder.
(3) Add an additional encoding (e.g., AU or embedding) before passing it to the decoder.
(4) Convert the intermediate face/body representation to the desired identity/expression before
generation (e.g., transform the boundaries with a secondary network or render a 3D model
of the target with the desired expression).
(5) Use the optical ï¬‚ow ï¬eld from subsequent frames in a source video to drive the generator.
(6) Create composite of the original content (hair, scene, etc) with a combination of the 3D
rendering, warped image, or generated content, and pass the composite through another
network (such as pix2pix) to reï¬ne the realism.
Generalization
A deepfake network may be trained or designed to work with only a speciï¬c set of target and source
identities. An identity agnostic model is sometimes hard to achieve due to correlations learned by
the model between s and t during training.
Let E be some model or process for representing or extracting features from x, and let M be a
trained model for performing replacement or reenactment. We identify three primary categories
in regard to generalization:
one-to-one: A model that uses a speciï¬c identity to drive a speciï¬c identity: xĞ´ =Mt(Es(xs))
many-to-one: A model that uses any identity to drive a speciï¬c identity: xĞ´ =Mt(E(xs))
many-to-many: A model that uses any identity to drive any identity: xĞ´ =M(E1(xs),E2(xt))
Challenges
Te following are some challenges in creating realistic deepfakes:
Generalization. Generativenetworksaredatadriven, andthereforereï¬‚ectthetrainingdataintheir
outputs. Tis means that high quality images of a speciï¬c identity requires a large number
of samples of that identity. Moreover, access to a large dataset of the driver is typically much
easier to obtain than the victim. As a result, over the last few years, researchers have worked
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
hard to minimize the amount of training data required, and to enable the execution of a
trained model on new target and source identities (unseen during training).
Paired Training. One way to train a neural network is to present the desired output to the model for
each given input. Tis process of data pairing is a laborious an sometimes impractical when
training on multiple identities and actions. To avoid this issue, many deepfake networks
either (1) train in a self-supervised manner by using frames selected from the same video oft,
(2) use unpaired networks such as Cycle-GAN, or (3) utilize the encodings of an ED network.
Identity Leakage. Sometimes the identity of the driver (e.g., s in reenactment) is partially transferred to xĞ´. Tis occurs when training on a single input identity, or when the network is
trained on many identities but data pairing is done with the same identity. Some solutions
proposed by researchers include atention mechanisms, few-shot learning, disentanglement,
boundary conversions, and AdaIN or skip connections to carry the relevant information to
the generator.
Occlusions. Occlusions are where part of xs or xt is obstructed with a hand, hair, glasses, or any
other item. Another type of obstruction is the eyes and mouth region that may be hidden or
dynamically changing. As a result, artifacts appear such as cropped imagery or inconsistent
facial features. To mitigate this, works such as perform segmentation and
in-painting on the obstructed areas.
Temporal Coherence. Deepfake videos ofen produce more obvious artifacts such as ï¬‚ickering
and jiter . Tis is because most deepfake networks process each frame individually
with no context of the preceding frames. To mitigate this, some researchers either provide
this context to G and D, implement temporal coherence losses, use RNNs, or perform a
combination thereof.
REENACTMENT
In this section we present a chronological review of deep learning based reenactment, organized
according to their class of identity generalization. Table 1 provides a summary and systematization
of all the works mentioned in this section. Later, in Section 7, we contrast the various methods and
identify the most signiï¬cant approaches.
Expression Reenactment
Expression reenactment turns an identity into a puppet, giving atackers the most ï¬‚exibility to
achieve their desired impact. Before we review the subject, we note that expression reenactment
has been around long before deepfakes were popularized. In 2003, researchers morphed models of
3D scanned heads . In 2005, it was shown how this can be done without a 3D model , and
through warping with matching similar textures . Later, between 2015 and 2018, Ties et al.
demonstrated how 3D parametric models can be used to achieve high quality and real-time results
with depth sensing and ordinary cameras ( and ).
Regardless, today deep learning approaches are recognized as the simplest way to generate believable content. To help the reader understand the networks and follow the text, we provide the
modelâ€™s network schematics and loss functions in ï¬gures 6-8.
One-to-One (Identity to Identity). In 2017, the authors of proposed using a CycleGAN
for facial reenactment, without the need for data pairing. Te two domains where video frames of
s and t. However, to avoid artifacts in xĞ´, the authors note that both domains must share a similar
distributions (e.g., poses and expressions).
In 2018, Bansal et al. proposed a generic translation network based on CycleGAN called Recycle-
GAN . Teir framework improves temporal coherence and mitigates artifacts by including
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
Table 1. Summary of Deep Learning Reenactment Models (Body and Face)
Reenactment
Retraining for newâ€¦
Representation
Model Training
Model Execution
Model Output
Source (s)
Target (t)
Identity Agnostic
Discriminators
Other Netw.
3DMM/Rendering
UV Mapping
Segmentation
Landmark / Keypoint
Boundary / Skeleton
Labeling of: ID
Labeling of: Action
No Pairing
Paring within Same Video
Paring ID to Same ID
Paring ID to Diï¬€r. Actions
Paring Action to Diï¬€r. IDs
Requires Video
Source (xs â€¦)
Target (xt â€¦)
Image/Frame
Resolution
 2017
>20 min. video >20 min. video
Recycle-GAN
5-10 min. video 5-10 min. video
DeepFaceLab
1-3 hr. video
1-3 hr. video
portrait video
One-to-One
 2019
Liu et al. 2019
1-3 hr. video
1-3 hr. video
upperbody video
 2017
Syth. Obama
17 hr. video
portiat video
â€¢ 2048x1024
17 hr. video
 2018 Deep Video Portr. â€¢ â€¢ â€¢ â€¢
1-3 min. video
portrait video
neural texture
 2018
ReenactGAN
30 min. video
 2018
â€¢ â€¢ â€¢ â—¦or â€¢
3-8 min. video
portrait video
 2018
1 min. video
expression label
identity label
2 hr. video
 2019
3-10 images
3-10 eye images
1 hr. video
portiat video
 2019
N.V. Puppetry
2-3 min. video
portiat video
 2019
8 min. video
body image
background
 2019 Deep Video P.C.
2 min. video
body image
 2019 Everybody D. N.
20 min. video
body image
 2019 D. D. Generation
3 min. video
body video
 2019 N. Talking Heads â€¢ â€¢ â€¢
1-3 portraits
â€¢ portrait/landmarks
1-3 portraits
Many-to-One
 2019 Few-shot Vid2Vid â€¢ â€¢ â€¢ â—¦or â€¢
1-10 portraits
â€¢ portrait/body video 1-10 portr./bodies â€¢
 2015
Shimba et al.
â€¢ 0 0 0 1 â€¢
face database
latent variables
 2017
 2017
â€¢ 1 1 2 0 â€¢
 2018
portrait - neutral
 2018
1-3 portraits
 2018
GANnotation
â€¢ portrait/landmarks
 2018
â€¢ 1 1 1 2 â€¢
portrait/AUs
 2018
FaceID-GAN
 2018
FaceFeat-GAN
latent variables
 2018
1+ portraits
 2018 Deformable GAN
body image/landm.
body image
body image
body image/pose
body image
body image
 2018
Dense Pose Tr.
â€¢ 25 25 1 2
body image
body image
 2018
Song et al.
 2019
â€¢ portrait/landmarks
 2019
GANimation
â€¢ 2 2 1 1 â€¢
portrait/AUs
 2019
â€¢ 2 2 1 2 â€¢
portrait/AUs
 2019
FaceSwapNet
portrait/landmarks portrait/landmarks â€¢
 2019
Monkey-Net
portrait/body
portrait/body
 2019 First-Order-Model â€¢ â€¢ â€¢
portrait/body
portrait/body
 2019
expression label
portrait/boundaries
Fu et al. 2019
portrait/label
 2019
portriat/landmarks
 2019 Speech D. Anm. 1 â€¢ â—¦
 2019 Speech D. Anm. 2 â€¢ â—¦
 2019 Speech D. Anm. 3 â€¢ â—¦
 2019
â€¢ audio/portrait video
portiat video
Speech2Vid
portiat video
 2019
body video
body image
body image
body image
body image
body/pose image
 2019
body image
body/pose image
 2020
ImaGINator
expression label
MarioNETte
1-8 portraits
Many-to-Many
16 portraits
next-frame predictor networks for each domain. For facial reenactment, the authors train their
network to translate the facial landmarks of xs into portraits of xt.
Many-to-One (Multiple Identities to a Single Identity). In 2017, the authors of proposed
CVAE-GAN, a conditional VAE-GAN where the generator is conditioned on an atribute vector
or class label. However, reenactment with CVAE-GAN requires manual atribute morphing by
interpolating the latent variables (e.g., between target poses).
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
Later, in 2018, a large number of source-identity agnostic models were published, each proposing
a diï¬€erent method to decoupling s from t:5
Facial Boundary Conversion. One approach was to ï¬rst convert the structure of sourceâ€™s facial
boundaries tothatofthe targetâ€™sbeforepassing themthroughthe generator . Intheir framework
â€˜ReenactGANâ€™, the authors use a CycleGAN to transform the boundarybs to the targetâ€™s face shape
asbt before generating xĞ´ with a pix2pix-like generator.
Temporal GANs. To improve the temporal coherence of deepfake videos, the authors of 
proposed MoCoGAN: a temporal GAN which generates videos while disentangling the motion and
content (objects) in the process. Each frame is generated using a target expression label zc, and a
motion embedding z(i)
M for the i-th frame, obtained from a noise seeded RNN. MoCoGAN uses two
discriminators, one for realism (per frame) and one for temporal coherence (on the lastT frames).
In , the authors proposed a framework called Vid2Vid, which is similar to pix2pix but for
videos. Vid2Vid considers the temporal aspect by generating each frame based on the last L source
and generated frames. Te model also considers optical ï¬‚ow to perform next-frame occlusion prediction (due to moving objects). Similar to pix2pixHD, a progressive training strategy is to generate
high resolution imagery. In their evaluations, the authors demonstrate facial reenactment using
the sourceâ€™s facial boundaries. In comparison to MoCoGAN, Vid2Vid is more practical since it the
deepfake is driven by xs (e.g., an actor) instead of crafed labels.
Teauthorsof tooktemporaldeepfakesonestepfurtherachievingcompletefacialreenactment
(gaze, blinking, pose, mouth, etc.) with only one minute of training video. Teir approach was to
extract the source and targetâ€™s 3D facial models from 2D images using monocular reconstruction,
and then for each frame, (1) transfer the facial pose and expression of the sourceâ€™s 3D model to the
targetâ€™s, and (2) produce xĞ´ with a modiï¬ed pix2pix framework, using the last 11 frames of rendered
heads, UV maps, and gaze masks as the input.
Many-to-Many (Multiple IDs to Multiple IDs).
Label Driven Reenactment. Te ï¬rst atempts at identity agnostic models were made in 2017,
where the authors of used a conditional GAN (CGAN) for the task. Teir approach was to (1)
extract the inner-face regions as (xt,xs), and then (2) pass them to an ED to produce xĞ´ subjected
to L1 and Ladv losses. Te challenge of using a CGAN was that the training data had to be paired
(images of diï¬€erent identities with the same expression).
Going one step further, in the authors reenacted full portraits at low resolutions. Teir
approach was to decoupling the identities was to use a conditional adversarial autoencoder to disentangle the identity from the expression in the latent space. However, their approach is limited to
drivingxt with discreet AU expression labels (ï¬xed expressions) that capturexs. A similar label based
reenactment was presented in the evaluation of StarGAN ; an architecture similar to CycleGAN
but for N domains (poses, expressions, etc).
Later, in 2018, the authors of proposed GATH which can drive xt using continuous action
units (AU) as an input, extracted from xs. Using continuous AUs enables smoother reenactments
over previous approaches . Teir generator is ED network trained on the loss signals
from using three other networks: (1) a discriminator, (2) an identity classiï¬er, and (3) a pretrained
AU estimator. Te classiï¬er shares the same hidden weights as the discriminator to disentangle the
identity from the expressions.
5Although works such as and achieved fully agnostic models (many-to-many) in 2017, their works were on low
resolution or partial faces.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
ğ‘¥ğ‘ , ğ‘¥ğ‘¡, ğ‘¥ğ‘”: The source, target, and generated images (e.g., portraits)
ğ‘¦: A label (e.g., fake vs real, one-hot encoding, â€¦)
ğ‘¥â€²: Another sample from the same distribution, à·œğ‘¥: reconstructed
ğ‘š: Binary mask, ğ‘ : Segmentation map, ğ‘™: Landmark or Keypoint, ğ‘§: Noise
: Concatenate, : Subtract, :Multiply : Add : Paste content
: Crop out region ğ‘from image where ğ‘âˆˆ{f:face, e:eye, m:mouth}
: Create mask using region ğ‘of the image where ğ‘âˆˆ{f:face, e:eye, m:mouth}
ğ‘¥ğ‘: Image ğ‘¥cropped to the region of ğ‘âˆˆ{ğ‘“:face, ğ‘’:eye, ğ‘š:mouth}
: Spatial replication of a vector (channel-wise or dim-wise)
: Scale image down by factor of X
ğ¿ğ¸, ğµğ¸, ğ´ğ¸, 3ğ·ğ¸: Landmark, Boundary, Action Unit (AU),
and 3DMM facial model Extractors (open source CV library)
ğ¿ğ‘‡, 3ğ·ğ‘‡: Landmark and 3D model transformers, from ğ‘ to ğ‘¡
ğ‘€ğ¸: MFCC audio feature extractor
Losses: L1 : L1, L2 : L2, LCE : Cross Entropy, Ladv : Adversarial, LF M :
Feature Matching, Lperc : Perceptual, Lcyc : Cycle Consistency, Latt : Attention, Ltrip : Triplet, Ltv : Total Variance, LKL : KL Divergence
 Reenact GAN:
Generic Boundary Encoder
ğ‘: facial boundaries, ğ‘ğ‘¥ğ‘”: a boundary translated to domain ğ‘¥
Target Specific
Source Generic
Target Specific
â„’1: ğ‘¥ğ‘¡, ğ‘¥ğ‘”
â„’ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘›ğ‘¡: ğ¼â„“2 ğ‘¥ğ‘”, ğ¼â„“3 ğ‘¥ğ‘”
â„’ğ‘ğ‘¦ğ‘ğ‘™ğ‘’: à· ğ‘ğ‘¡, ğ‘ğ‘¡
â„’ğ‘ğ‘¦ğ‘ğ‘™ğ‘’: à· ğ‘ğ‘ , ğ‘ğ‘ 
â„’ğ‘ğ‘‘ğ‘£(âˆ™): ğ‘¦âˆ™,
â„’ğ‘ğ‘‘ğ‘£(ğ‘¡): ğ‘¦ğ‘¡,
 MocoGAN:
ğ‘¦ğ‘ : source expression label, ğ‘’ğ‘¡: one-hot encoding of target identity,
ğ‘’ğ‘¥: temporal expression embedding, ğºğ‘…ğ‘ˆ: Gated Recurrent Unit of an RNN
Image Disc.
Video Disc.
â„’ğ‘ğ‘‘ğ‘£(ğ¼): ğ‘¦ğ¼,
â„’ğ‘ğ‘‘ğ‘£(ğ‘‰): ğ‘¦ğ‘‰,
 Vid2Vid:
(ğ‘–âˆ’ğ¿), â€¦ ğ‘™ğ‘ 
(ğ‘–âˆ’ğ¿), â€¦ ğ‘¥ğ‘”
(ğ‘–âˆ’ğ¿), â€¦ ğ‘¥ğ‘ 
Intermediate Synth.
Occlusion Masking
Warp Field Pred.
Image Disc.
Video Disc.
ğ‘–âˆ’ğ¾âˆ’1 :(ğ‘–)
ğ‘‡: frames in the video clip, ğ¿, ğ¾: system parameters
â„’ğ¹ğ‘€(ğ¼): ğ‘¥ğ‘¡
â„’ğ¹ğ‘€(ğ·ğ¼): ğ‘¥ğ‘¡
â„’ğ‘ğ‘‘ğ‘£ğ·ğ¼: ğ‘¦ğ¼, ğ‘™ğ‘¡
â„’ğ‘ğ‘‘ğ‘£ğ‘‰ğ·ğ‘‰: ğ‘¦ğ‘‰, ğ‘¤ğ‘¡
ğ‘–âˆ’ğ¾âˆ’1 : ğ‘–,
 Deep Video Portrait:
ğ‘šğ‘’ğ‘¦ğ‘’: mask of eye region (gaze), ğ‘¥ğ‘ˆğ‘‰: UV correspondence map, ğ‘¥ğ‘ğ‘”ğ‘–: 3D rendered
image of ğ‘¥
(ğ‘–âˆ’ğ‘), â€¦ ğ‘¥ğ‘ğ‘”ğ‘–
(ğ‘–âˆ’ğ‘), â€¦ ğ‘¥ğ‘ˆğ‘‰
(ğ‘–âˆ’ğ‘), â€¦ ğ‘šğ‘’ğ‘¦ğ‘’
(ğ‘–âˆ’ğ‘), â€¦ ğ‘¥ğ‘¡
â„’ğ‘ğ‘‘ğ‘£: ğ‘¦, ğ‘¥ğ‘¡
(ğ‘–âˆ’ğ‘), â€¦ ğ‘¥ğ‘¡
 GATH:
shared weights
â„’ğ´ğ‘ˆğ´: â„’2 ğ‘ğ‘”â€² , ğ‘ğ‘ â€²
â„’1: ğ‘¥ğ‘¡, ğ‘¥ğ‘”
 GANimation:
ğ‘šğ‘: attention mask, ğ‘šğ‘: color mask
â„’1: ğºğºğ‘¥ğ‘¡, ğ‘ğ‘ , ğ‘ğ‘¡, ğ‘¥ğ‘¡
â„’ğ‘ğ‘¡ğ‘¡ğ‘›: ğ‘šğ‘-see paper
â„’ğ‘’ğ‘¥ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘›: ğ‘ğ‘¡,
â€² -see paper â„’ğ‘ğ‘‘ğ‘£: ğ‘¦,
 GANotation:
ğ‘šğ‘: attention mask, ğ‘šğ‘: color map, training: ğ‘ and ğ‘¡have same ID
â„’ğ‘¡ğ‘£: ğ‘¥ğ‘ , ğ‘¥ğ‘”
â„’2: ğ‘¥ğ‘ , ğ‘¥ğ‘”(same ID) â„’ğ‘ğ‘œğ‘›ğ‘ ğ‘–ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¦: ğ»ğ‘¥ğ‘”, ğ‘™ğ‘¡, ğ‘¥ğ‘¡
â„’ğ‘ğ‘‘ğ‘£: ğ‘¦, ğ‘¥ğ‘”, ğ‘¥ğ‘¡
â„’3Ã—ğ‘ğ‘œğ‘›ğ‘ ğ‘–ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¦: ğ»ğ‘¥ğ‘”, ğ‘™ğ‘ â€² , ğ»ğ‘¥ğ‘¡, ğ‘™ğ‘ â€²
â„’ğ‘ğ‘’ğ‘Ÿğ‘: ğ¼â„“ğ‘˜,â„“ğ‘˜âˆ’1 ğ‘¥ğ‘”, ğ¼â„“ğ‘˜,â„“ğ‘˜âˆ’1 ğ‘¥ğ‘¡
â„’ğ‘ğ‘’ğ‘Ÿğ‘: ğ‘ƒâ„“ğ‘˜,â„“ğ‘˜âˆ’1 ğ‘¥ğ‘”, ğ‘ƒâ„“ğ‘˜,â„“ğ‘˜âˆ’1 ğ‘¥ğ‘¡
 FaceID-GAN:
3DMM predictor
Identity encoder
â„’ğ‘ğ‘‘ğ‘£: â„’1 ğ‘¥ğ‘¡, à·œğ‘¥ğ‘¡âˆ’ğ›¼â„’1(ğ‘¥ğ‘”, à·œğ‘¥ğ‘”)
â„’ğ¶ğ¸ğ¼: ğ‘’ğ‘”, ğ‘¦ğ‘–ğ‘‘
â„’ğ¶ğ¸ğ¼: ğ‘’ğ‘¡, ğ‘¦ğ‘–ğ‘‘
â„’1 ğ·ğ‘’: ğ‘¥ğ‘”, à·œğ‘¥ğ‘”
â„’2 ğ·ğ‘’: ğ‘ğ‘¡ğ‘ , ğ‘ğ‘”
â„’ğ‘ğ‘œğ‘ ğ·ğ‘’: ğ‘’ğ‘”, ğ‘’ğ‘¡
â„’ğ‘ğ‘œğ‘ ğ‘’ğ‘ƒ: ğ‘ğ‘¡, 3ğ·ğ‘€ğ‘€ğ‘¥ğ‘¡
 FaceFeat-GAN:
3DMM pred.
ID predictor
ğ‘§: Sample of random noise later mapped to ğ‘¥ğ‘ 
â„’ğ‘ğ‘‘ğ‘£(ğ·): ğ‘¦,
â„’ğ‘ğ‘‘ğ‘£ğ·ğ‘: ğ‘¦ğ‘, Î¤
â„’ğ‘ğ‘‘ğ‘£ğ·ğ‘: ğ‘¦ğ‘, Î¤
â„’ğ¶ğ¸ğ¼: ğ‘¦ğ‘¡, ğ‘¦ğ‘¡
â„’ğ‘ğ‘œğ‘ ğ‘’ğ‘ƒ: ğ‘, 3ğ·ğ‘€ğ‘€(ğ‘¥ğ‘¡)
â„’1 ğ‘„: ğ‘¥ğ‘¡, ğºğ‘¦ğ‘¡, ğ‘, ğ‘„ğ‘¥ğ‘¡
â„’1 ğº: ğ‘¥ğ‘¡, à·œğ‘¥ğ‘¡
â„’2 ğº: ğ¼à·œğ‘¥ğ‘¡, ğ‘¦ğ‘¡& ğ¼ğ‘¥ğ‘”, ğ‘¦ğ‘¡
Fig. 6. Architectural schematics of reenactment networks. Black lines indicate prediction flows used during
deployment, dashed gray lines indicate dataflows performed during training. Zoom in for more detail.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
 paGAN:
ğ‘¥ğ‘ˆğ‘‰: UV correspondence map, ğ‘¥ğ‘ğ‘”ğ‘–: 3D rendered image of ğ‘¥, ğ‘¥ğ‘‘ğ‘’ğ‘ğ‘¡â„: image of depth
map of model ğ‘¥
â„’ğ‘ğ‘’ğ‘Ÿğ‘(ğ¼): ğ‘¥ğ‘¡
 X2Face:
ğ‘£: vector map of pixel deltas (changes), ğ‘¥ğœ‚: a face with a neutral expression/pose,
ğ‘: some other modality (e.g., audio)
Encoding Network
interpolate
Driving Network
interpolate
â„’1: ğ‘¥ğ‘ , ğ‘¥ğ‘”
â„’ğ‘ğ‘’ğ‘Ÿğ‘(ğ¼): ğ‘¥ğ‘¡, ğ‘¥ğ‘”
 FaceSwapNet:
Landmark Converter
Landmark Guided
â„’1: ğ‘¥ğ‘”, ğ‘¥ğ‘¡
â€² (with same expr.) â„’2: ğ‘™ğ‘”, ğ‘™ğ‘¡
â€² (with same expr.)
â„’ğ‘ğ‘‘ğ‘£: ğ‘¦, ğ‘¥ğ‘”, ğ‘¥ğ‘¡
â„’ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ğ‘’ğ‘¡: â„’ğ‘ğ‘’ğ‘Ÿğ‘(ğ‘¥ğ‘¡1, ğ‘¥ğ‘¡2), â„’ğ‘ğ‘’ğ‘Ÿğ‘(ğ‘¥ğ‘¡1, ğ‘¥ğ‘ )
Same ID, different expression
Different IDs
 FSGAN:
ğ‘š: segmentation mask (face, hair, other), ğ‘™: 3D facial landmarks, ğ»ğ‘›: ğ‘›passes through
ğ»while interpolating ğ‘™ğ‘ to ğ‘™ğ‘¡
Reenactment
â„’1: ğ‘¥ğ‘”, ğ‘¥ğ‘¡
â„’1: ğ»ğ‘›(ğ‘¥ğ‘”, ğ‘™ğ‘ ), ğ‘¥ğ‘¡
â„’ğ‘ğ‘’ğ‘Ÿğ‘(ğ»): ğ¼â„“âˆ—(ğ‘¥ğ‘¡), ğ¼â„“âˆ—(ğ‘¥ğ‘¡)
â„’ğ‘ğ‘’ğ‘Ÿğ‘(ğ»): ğ¼â„“âˆ—(ğ»ğ‘›(ğ‘¥ğ‘”, ğ‘™ğ‘ )), ğ¼â„“âˆ—(ğ‘¥ğ‘¡)
â„’ğ‘ğ‘‘ğ‘£: ğ‘¦, ğ‘¥ğ‘”, ğ‘¥ğ‘¡
 Fu et al. 2019:
Boundary encoder
ğ·ğ‘’ğ‘1 ğ‘ğ‘”ğ¸ğ‘›ğ‘2
Texture encoder
Pose predictor
Expr. predictor
Boundary encoder
â„’ğ‘Ÿğ‘’ğ‘”ğ·ğ‘’ğ‘: ğ·ğ‘’ğ‘ğ‘ğ‘ â€², ğ‘’ğ‘ â€², ğ‘£ğ‘¡, ğ‘ƒğ‘ğ‘ â€² , ğ¸(ğ‘’ğ‘ â€²)
â„’1 ğ·ğ‘’ğ‘: ğ‘ğ‘¡, ğ‘ğ‘”
â„’ğ‘¡â„ğ‘Ÿ: ğ¼1 ğ‘¥ğ‘¡, ğ¸ğ‘›ğ‘¡(ğ‘¥ğ‘¡)
â„’ğ‘ğ‘‘ğ‘£1,2,3:
â„’ğ¹ğ‘€ğ·ğ‘’ğ‘”: ğ¼2,â„“âˆ™ğ‘¥ğ‘ , ğ¼2,â„“âˆ™ğ‘¥ğ‘”
 ICFace:
To neutral Gen.
To expression Gen.
ğ‘: Action Units (AU), ğœ‚: neutral expression
â„’1: ğ‘¥ğ‘¡, ğ‘¥ğ‘”
â„’ğ‘ğ‘‘ğ‘£(ğ‘›): ğ‘¦,
â„’ğ‘ğ‘‘ğ‘£(ğ‘’): ğ‘¦,
â„’2: ğ´ğ‘¥ğ‘¡, ğ‘ğ‘ 
â„’2: ğ´ğ‘¥ğ‘”, ğ‘ğ‘ 
â„’ğ‘ğ‘’ğ‘Ÿğ‘(ğ¼): ğ‘¥ğ‘¡, ğ‘¥ğ‘¡
ğœ‚â„’ğ‘ğ‘’ğ‘Ÿğ‘(ğ¼): ğ‘¥ğ‘”, ğ‘¥ğ‘¡
 AF-VAE:
ğ´ğ‘€ğ¸: Additive Memory Encoder â€“ models ğ‘’ğ‘as a Gaussian mixture of clustered
facial boundaries.
Appearance encoder
Identity encoder
â„’ğ¾ğ¿: ğ‘¥ğ‘”, ğ‘’ğ‘, ğ‘’ğ‘¥, ğ´ğ‘€ğ¸
â„’ğ¹ğ‘€ğ¼: ğ‘¥ğ‘”, ğ‘¥ğ‘¡
 wg-GAN:
ğ‘£ğ‘ ğ‘¡: vector map of the warp from ğ‘¥ğ‘¡to ğ‘¥ğ‘ , ğ‘¤ğ‘ ğ‘¡: ğ‘¥ğ‘¡warped according to ğ‘£ğ‘ ğ‘¡
Training: for each ğ‘¥ğ‘¡
(ğ‘–), ğ‘¥ğ‘ = ğ‘¥ğ‘¡
(ğ‘–âˆ’10) taken from the same video clip
Refinement
Inpainting
ğ¿ğ‘ğ‘‘ğ‘£2: ğ‘¦2,
ğ¿ğ‘ğ‘‘ğ‘£3: ğ‘¦3,
 Motion&Texture-GAN:
ğ‘¥ğœ‚: cropped neutral expression face, ğ‘¦ğ‘ : face expression label of source, ğ‘ : an SRVF
point on a spherical manifold, ğ¿ğ‘…: landmark reconstruction from ğ‘ , ğ‘™: facial landmarks
Texture-GAN
Motion-GAN
â„’ğ‘ğ‘‘ğ‘£(ğ‘‡): ğ‘¦ğ‘‡,
â„’ğ‘ğ‘’ğ‘Ÿğ‘(ğ¼): ğ‘¥ğ‘¡
â„’1: ğ‘ ğ‘ , ğ‘ ğ‘¡
â„’ğ‘ğ‘‘ğ‘£(ğ‘€): ğ‘¦ğ‘†, ğ‘¦ğ‘š, àµ—
 ImaGINator:
ğ‘™: One-hot label encoding of expression, ğ‘§: Random value ğ‘§~ğ‘(0,1)
â€²(1), â€¦ ğ‘¥ğ‘¡
Image Disc.
Video Disc.
â€²(1), â€¦ ğ‘¥ğ‘¡
â„’ğ‘ğ‘‘ğ‘£(ğ‘‰): ğ‘¦ğ‘£, ğ‘™ğ‘ , ğ‘¥ğ‘¡
â€²(1), â€¦ , ğ‘¥ğ‘”
â„’ğ‘ğ‘‘ğ‘£(ğ¼): ğ‘¦ğ¼, ğ‘¥ğ‘¡
Fig. 7. Architectural schematics of reenactment networks. Black lines indicate prediction flows used during
deployment, dashed gray lines indicate dataflows performed during training. Zoom in for more detail.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
 Monkey-NET:
Keypoint Detector
Motion Transfer Gen.
Motion Network
ğ‘˜: 2D matrix of keypoints, ğ‘£: vector field, ğ‘£ğ‘Ÿğ‘’ğ‘ , ğ‘£ğ‘ğ‘œğ‘Ÿ: residual and coarse motion fields,
ğ‘š: estimated motion mask
â„’ğ‘ğ‘‘ğ‘£: ğ‘¦, ğ‘˜ğ‘¡,
â„’ğ¹ğ‘€: ğ‘¥ğ‘”âˆ™ğ‘£ğ‘ ğ‘¡, ğ‘¥ğ‘¡
 Neural Talking Heads:
â„’ğ‘ğ‘’ğ‘Ÿğ‘ğ¼: ğ‘¥ğ‘ , ğ‘¥ğ‘”
â„’ğ¹ğ‘€: ğ·â„“âˆ—ğ‘¥ğ‘¡
â€², ğ‘™ğ‘¡, ğ·â„“âˆ—ğ‘¥ğ‘”, ğ‘™ğ‘”
â„’â„ğ‘–ğ‘›ğ‘”ğ‘’: ğ·, ğ‘¥ğ‘”, ğ‘¥ğ‘¡
 MarioNETte:
Source encoder
Target encoder
ğ‘™ğ‘ ğ‘¡: ğ‘¡â€™s landmarks with ğ‘ â€™s expression, ğ‘£: feature maps, ğ‘¤: warped feature maps
ğ¼ğ´ğµ: Image Attention Block
â„’ğ‘ğ‘‘ğ‘£: ğ‘™ğ‘ , ğ‘¦ğ‘¡, ğ‘¥ğ‘”, ğ‘¥ğ‘ 
â„’ğ‘ğ‘’ğ‘Ÿğ‘: ğ¼1,â„“âˆ™ğ‘¥ğ‘”, ğ¼1,â„“âˆ™ğ‘¥ğ‘ 
â„’ğ‘ğ‘’ğ‘Ÿğ‘: ğ¼2,â„“âˆ™ğ‘¥ğ‘”, ğ¼2,â„“âˆ™ğ‘¥ğ‘ 
â„’ğ¹ğ‘€: ğ·â„“ğ‘¥ğ‘”, ğ·â„“ğ‘¥ğ‘ 
 Liu et al. 2019:
ğ‘ˆğµğ¾ğ¸: Upper-body Key point Extractor
(ğ‘¡âˆ’ğ¿) , ğ¶ğ‘”= ğ‘ğ‘”
(ğ‘¡âˆ’ğ¿) , ğ‘‹ğ‘¡= ğ‘¥ğ‘¡
Face Boundary Pred.
Image Generator
â„’ğ¹ğ‘€ğ·2 : ğ¶ğ‘”, ğ‘‹ğ‘”, ğ¶ğ‘”, ğ‘‹ğ‘¡
â„’ğ‘ğ‘’ğ‘Ÿğ‘(ğ¼): ğ‘‹ğ‘”, ğ‘‹ğ‘¡
â„’ğ‘ğ‘‘ğ‘£1: ğ‘¦1,
â„’ğ‘ğ‘‘ğ‘£2: ğ‘¦2, ğ¶ğ‘”, ğ‘‹ğ‘”, ğ¶ğ‘”, ğ‘‹ğ‘¡
â„’1: ğ‘ğ‘”, ğ‘ğ‘¡
Fig. 8. Architectural schematics of the reenactment networks. Black lines indicate prediction flows used
during deployment, dashed gray lines indicate dataflows performed during training. Zoom in for more detail.
Self-Attention Modeling. Similar to , another work called GANimation reenacts faces
through AU value inputs estimated from xs. Teir architecture uses an AU based generator that
uses a self atention model to handle occlusions, and mitigate other artifacts. Furthermore, another
network penalizesG with an expression prediction loss, and shares its weights with the discriminator
to encourage realistic expressions. Similar to CycleGAN, GANimation uses a cycle consistency loss
which eliminates the need for image pairing.
Instead of relying on AU estimations, the authors of propose GANnotation which uses facial
landmark images. Doing so enables the network to learn facial structure directly from the input
but is more susceptible to identity leakage compared to AUs which are normalized. GANotation
generates xĞ´ based on (xt,ls), where ls is the facial landmarks of xs. Te model uses the same self
atention model as GANimation, but proposes a novel â€œtriple consistency lossâ€ to minimize artifacts
in xĞ´. Te loss teaches the network how to deal with intermediate poses/expressions not found in
the training set. Given ls,lt and lz sampled randomly from the same video, the loss is computed as
Ltrip = âˆ¥G(xt,ls)âˆ’G(G(xt,lz),ls)âˆ¥2
3D Parametric Approaches. Concurrent to the work of , other works also leveraged 3D parametric facial models to prevent identity leakage in the generation process. In , the authors
propose FaceID-GAN which can reenacts t at oblique poses and high resolution. Teir ED generator
is trained in tandem with a 3DMM face model predictor, where the model parameters of xt are used
to transform xs before being joined with the encoderâ€™s embedding. Furthermore, to prevent identity
leakage from xs to xĞ´, FaceID-GAN incorporates an identiï¬cation classiï¬er within the adversarial
game. Teclassiï¬erhas2N outputswheretheï¬rstN outputs(correspondingtotrainingsetidentities)
are activated if the input is real and the rest are activated if itâ€™s fake.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
Later, the authors of proposed FaceFeat-GAN which improves the diversty of the faces while
preserving the identity . Te approach is to use a set of GANs to learn facial feature distributions
as encodings, and then use these generators to create new content with a decoder. Concretely, three
encoder/predictor neural networks P, Q, and I, are trained on real images to extract feature vectors
from portraits. P predicts 3DMM parameters p, Q encodes the image as q capturing general facial
features using feedback from I, and I is an identity classiï¬er trained to predict label yi. Next two
GANs, seeded with noise vectors, producepâ€² and qâ€² while a third GAN is trained to reconstruct xt
from (p,q,yi) and xĞ´ from (pâ€²,qâ€²,yi). To reenact xt, (1)yt is predicted using I (even if the identity was
previously unseen), (2) zp and zq are selected empirically to ï¬t xs, and (3) the third GANâ€™s generator
uses (pâ€²,qâ€²,yt) to create xĞ´. Although FaceFeat-GAN improves image diversty, it is less practical than
FaceID-GAN since the GANâ€™s input seed z be selected empirically to ï¬t xs.
In , the authors present paGAN, a method for complete facial reenactment of a 3D avatar,
using a single image of the target as input. An expression neutral image of xt is used to generate a 3D
model which is then driven byxs. Te driven model is used to create inputs for a U-Net generator: the
rendered head, its UV map, its depth map, a masked image of xt for texture, and a 2D mask indicating
the gaze of xs. Although paGAN is very eï¬ƒcient, the ï¬nal deepfake is 3D rendered which detracts
from the realism.
Using Multi-Modal Sources. In the authors propose X2Face which can reenact xt with xs
or some other modality such as audio or a pose vector. X2Face uses two ED networks: an embedding
network and a driving network. First the embedding network encodes 1-3 examples of the targetâ€™s
face tovt: the optical ï¬‚ow ï¬eld required to transform xt to a neutral pose and expression. Next, xt
is interpolated according tomt producing x
t. Finally, the driving network maps xs to the vector map
vs, crafed to interpolate x
t to xĞ´, having the pose and expression of xs. During training, ï¬rst L1 loss
is used between xt and xĞ´, and then an identity loss is used between xs and xĞ´ using a pre-trained
identity model trained on the VGG-Face Dataset. All interpolation is performed with a tensorï¬‚ow
interpolation layer to enable back propagation using x
t and xĞ´. Te authors also show how the
embedding of driving network can be mapped to other modalities such as audio and pose.
In 2019, nearly all works pursued identity agnostic models:
Facial Landmark & Boundary Conversion. In , the authors propose FaceSwapNet which
tries to mitigate the issue of identity leakage from facial landmarks. First two encoders and a decoder
are used to transfer the expression in landmark ls to the face structure of lt, denoted lĞ´. Ten a
generator network is used to convert xt to xĞ´ wherelĞ´ is injected into the network with AdaIn layers
like a Style-GAN. Te authors found that it is crucial to use triplet perceptual loss with an external
VGG network.
In , the authors propose a method for high resolution reenactment and at oblique angles. A
set of networks encode the sourceâ€™s pose, expression, and the targetâ€™s facial boundary for a decoder
that generates the reenacted boundarybĞ´. Finally, an ED network generates xĞ´ using an encoding
of xtâ€™s texture in its embedding. A multi-scale loss is used to improve quality and the authors utilize
a small labeled dataset by training their model in a semi-supervised way.
In , the authors present FSGAN: a face swapping and facial reenactment model which can
handle occlusions. For reenactment a pix2pixHD generator receives xt and the sourceâ€™s 3D facial
landmarks ls, represented as a 256x256x70 image (one channel for each of the 70 landmarks). Te
output is xĞ´ and its segmentation map mĞ´ with three channels (background, face, and hair). Te
generator is trained recurrently where each output is passed back as input for several iterations while
ls is interpolated incrementally from ls to lt. To improve results further, delaunay Triangulation and
barycentric coordinate interpolation are used to generate content similar to the targetâ€™s pose. In
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
contrast to other facial conversion methods , FSGAN uses fewer neural networks enabling
real time reenactment at 30fps.
Latent Space Manipulation. In , the authors present a model called ICFace where the expression, pose, mouth, eye, andeyebrowsofxt canbedrivenindependently. Teirarchitectureissimilarto
aCycleGANinthatonegeneratortranslatesxt intoaneutralexpressiondomainasxÎ·
t andanothergenerator translatesxÎ·
t into an expression domain asxĞ´. Both generators ar conditioned on the target AU.
In the authors propose an Additive Focal Variational Auto-encoder (AF-VAE) for high quality
reenactment. Tis is accomplished by separating a C-VAEâ€™s latent code into an appearance encoding
ea and identity-agnostic expression coding ex. To capture a wide variety of factors in ea (e.g., age,
illumination, complexion, â€¦), the authors use an additive memory module during training which
conditionsthelatentvariablesonaGaussianmixturemodel, ï¬tedtoclusteredsetoffacialboundaries.
Subpixel convolutions were used in the decoder to mitigate artifacts and improve ï¬delity.
Warp-based Approaches. In the past, facial reenactment was done by warping the image xt to the
landmarks ls . In , the authors propose wgGAN which uses the same approach but creates
high-ï¬delity facial expressions by reï¬ning the image though a series of GANs: one for reï¬ning the
warped face and another for in-painting the occlusions (eyes and mouth). A challenge with wgGAN
is that the warping process is sensitive to head motion (change in pose).
In , the authors propose a system which can also control the gaze: a decoder generates xĞ´
with an encoding ofxt as the input and a segmentation map ofxs as reenactment guidance via SPADE
residual blocks. Te authors blend xĞ´ with a warped version, guided by the segmentation, to mitigate
artifacts in the background.
To overcome issue of occlusions in the eyes and mouth, the authors of use multiple images oft
as a reference, in contrast to and which only use one. In their approach (FLNet), the model
is provided with N samples of t (Xt) having various mouth expressions, along with the landmark
deltas between Xt and xs (Lt). Teir model is an ED (conï¬gured like GANimation ) which
produces (1) N encodings for a warped xĞ´, (2) an appearance encoding, and (3) a selection (weight)
encoding. Te encodings are then coverted into images using seperate CNN layers and merged
together through masked multiplication. Te entire model is trained end-to-end in a self supervised
manner using frames of t taken from diï¬€erent videos.
Motion-Content Disentanglement. In the authors propose a GAN to reenact neutral expression faces with smooth animations. Te authors describe the animations as temporal curves in 2D
space, summarizedaspointsonasphericalmanifoldbycalculatingtheirsquare-rootvelocityfunction
(SRVF). A WGAN is used to complete this distribution given target expression labels, and a pix2pix
GAN is used to convert the sequences of reconstructed landmarks into a video frames of the target.
In contrast to MoCoGAN , the authors of propose ImaGINator: a conditional GAN
which fuses both motion and content and uses with transposed 3D convolutions to capture the
distinct spatio-temporal relationships. Te GAN also uses a temporal discriminator, and to increase
diversity, the authors train the temporal discriminator with some videos using the wrong label.
A challenge with works such as and is that they are label driven and produce videos
with a set number of frames. Tis makes the deepfake creation process manual and less practical. In
contrast, the authors of propose Monkey-Net: a self supervised network for driving an image
with an arbitrary video sequence. Similar to MoCoGAN , the authors decouple the sourceâ€™s
content and motion. First a series of networks produce a motion heat map (optical ï¬‚ow) using the
source and targetâ€™s key-points, and then an ED generator produces xĞ´ using xs and the optical ï¬‚ow
(in its embedding).
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
Later in , the authors extend Monkey-Net by improving the object appearance when large
pose transformations occur. Tey accomplish this by (1) modeling motion around the keypoints
using aï¬ƒne transformations, (2) updating the key-point loss function accordingly, and (3) having the
motion generator predict an occlusion mask on the preceding frame for in-painting inference. Teir
work has been implemented as a free real-time reenactment tool for video chats, called Avitarify.6
Few-Shot Learning. Towards the end of 2019 and into the beginning of 2020, researchers
beganlookingintominimizingtheamountoftrainingdatafurtherviaone-shotandfew-shotlearning.
In , the authors propose a few-shot model which works well at oblique angles. To accomplish
this, the authors perform meta-transfer learning, where the network is ï¬rst trained on many diï¬€erent
identities and then ï¬ne-tuned on the targetâ€™s identity. Ten, an identity encoding of xt is obtained
by averaging the encodings of k sets of (xt,lt). Ten a pix2pix GAN is used to generate xĞ´ using ls
as an input, and the identity encoding via AdaIN layers. Unfortunately, the authors note that their
method is sensitive to identity leakage.
In the authors of Vid2Vid (Section 4.1.2) extend their work with few-shot learning. Tey
use a network weight generation module which utilizes an atention mechanism. Te module learns
to extract appearance paterns from a few samples of xt which are injected into the video synthesis
layers. In contrast to FLNet , , and which merge the multiple representations of t
before passing it through the generator. Tis approach is more eï¬ƒcient because it involves fewer
passes through the modelâ€™s networks.
In , the authors propose MarioNETte which alleviates identity leakage when the pose of xs
is diï¬€erent than xt. In contrast to other works which encode the identity separately or use of AdaIN
layers, the authors use an image atention block and target feature alignment. Tis enables the model
to beter handle the diï¬€erences between face structures. Finally, the identity is also preserved using
a novel landmark transformer inspired by .
Mouth Reenactment (Dubbing)
In contrast to expression reenactment, mouth reenactment (a.k.a., video or image dubbing) is concerned with driving a targetâ€™s mouth with a segment of audio. Fig. 9 presents the relevant schematics
for this section.
Many-to-One (Multiple Identities to a Single Identity).
Obama Puppetry. In 2017, the authors of created a realistic reenactment of former president
Obama. Tis was accomplished by (1) using a time delayed RNN over MFCC audio segments to generate a sequence of mouth landmarks (shapes), (2) generating the mouth textures (nose and mouth) by
applyingaweightedmediantoimageswithsimilarmouthshapesviaPCA-spacesimilarity,(3)reï¬ning
the teeth by transferring the high frequency details other frames in the target video, and (4) by using
dynamic programming to re-time the target video to match the source audio and blend the texture in.
Later that year, the authors of presented ObamaNet: a network that reenacts an individualâ€™s
mouth and voice using text as input instead of audio like . Te process is to (1) convert the
source text to audio using Char2Wav , (2) generate a sequence of mouth-keypoints using a
time-delayed LSTM on the audio, and (3) use a U-Net CNN to perform in-painting on a composite
of the target video frame with a masked mouth and overlayed keypoints.
Later in 2018, Jalalifar et al. proposed a network that synthesizes the entire head portrait of
Obama, and therefore does not require pose re-timing and can trained end-to-end, unlike and
 . First,abidirectionalLSTMcovertsMFCCaudiosegmentsintosequenceofmouthlandmarks,and
6htps://github.com/alievk/avatarify
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
 Synthesizing Obama:
ğ‘ğ‘–: the ğ‘–-th 25ms segment of audio with a stride of 10ms. ğ‘€ğ‘…: mouth retrieval and
enhancement based on 3DMM reconstructions. ğ‘‚ğ¹: optical flow extractor
Step delay=20
Audio to Mouth Landmark
Mouth Landmark to Face
 TETH:
ğ‘¡ğ‘ : text to be inserted into speech. ğ‘‡23ğ·: A 3DMM video renderer based on ğ‘¡ğ‘ using a
viseme lookup on ğ‘¡. *Audio gen not shown (TTS is done procedurally) .
â„’ğ‘ğ‘‘ğ‘£ğ‘†: ğ‘¦ğ‘ , ğ‘¥ğ‘
â„’ğ‘ğ‘‘ğ‘£ğ‘‡: ğ‘¦ğ‘‡, ğ‘¥ğ‘¡
â„’ğ‘â„ğ‘œğ‘¡ğ‘œğ‘šğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘: ğ‘¥ğ‘œ
ğ‘–:ğ‘–âˆ’2 , ğ‘¥ğ‘¡
ğ‘–:ğ‘–âˆ’2 , ğ‘šğ‘š
 SD-CGAN:
Audio to Landmark
Landmark to Face
ğ‘(ğ‘–): the ğ‘–-th 33ms segment of audio. ğ‘: lip landmarks.
â„’ğ‘ğ‘‘ğ‘£: ğ‘¦, ğ‘’ğ¼,
ğ‘ğ‘¡, ğ‘¥ğ‘¡ğ‘ğ‘”, ğ‘¥ğ‘”
â„’ğ‘€ğ‘†ğ¸ğ¿ğ‘†ğ‘‡ğ‘€: ğ‘ğ‘¡, ğ‘¢ğ‘¡
 Neural Voice Puppetry:
ğ‘ğ‘–: the ğ‘–-th 300ms audio segment with stride 20ms. ğ¶: content aware filter network
ğ‘ğ‘‡ğ¸: Neural Texture Extractor
DeepSpeech
Expression
Stabilization
Deferred Renderer
Compositor
ğ‘–âˆ’1:ğ‘–+1 , ğ‘ğ‘”
â„’ğ‘ğ‘‘ğ‘£2: ğ‘¦2, ğ‘¥ğ‘¡
â„’1 ğ»1 : ğ‘¥ğ‘”
ğ‘š(ğ‘–) â„’ğ‘ğ‘’ğ‘Ÿğ‘âˆ’ğ‘ ğ‘¡ğ‘¦ğ‘™ğ‘’(ğ»2): ğ‘¥ğ‘”
â„’1(ğ»2): ğ‘¥ğ‘”
(ğ‘–) â„’ğ‘ğ‘‘ğ‘£1: ğ‘¦1, ğ‘¢ğ‘”
 ATVGnet:
Audio to Landmark
Landmark to Face
ğ‘: landmarks compressed with PCA. ğ‘(ğ‘–): 10ms of audio around the ğ‘–-th frame.
ğ‘šğ‘: attention map. ğ‘šğ‘¤: motion map.
â„’ğ‘ğ‘–ğ‘¥ğµ, ğ¶: ğ‘¥ğ‘”, ğ‘¥ğ‘¡, ğ‘šğ‘2
â„’ğ‘ğ‘‘ğ‘£: ğ‘¦, ğ‘’ğ¼, ğ‘‹ğ‘¡/ğ‘‹ğ‘”
â„’ğ‘€ğ‘†ğ¸ğ¿ğ‘†ğ‘‡ğ‘€: ğ‘ğ‘”
 DAVS:
Temporal D.
Identity Enc.
Word (video) Enc.
Word (audio) Enc.
Word Classif.
ID Classif.
ğ‘(ğ‘–): the ğ‘–-th segment of audio containing a word. ğ‘¦ğ¼: identity label
ğ‘¦ğ‘¤: word label (one-hot encoding)
â„’1 ğ·ğ‘’: ğ‘‹ğ‘”, ğ‘‹ğ‘¡
â„’ğ‘ğ‘‘ğ‘£: ğ‘‹ğ‘”/ğ‘‹ğ‘¡, ğ‘¦
â„’ğ‘ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘ğ‘ ğ‘¡ğ‘–ğ‘£ğ‘’: ğ‘’ğ‘¤ğ‘£, ğ‘’ğ‘¤ğ‘, ğ‘¦ğ‘¤
â„’ğ‘ğ‘‘ğ‘£ğ¶1, ğ¶2 : ğ‘’ğ¼, ğ‘’ğ‘¤ğ‘£, ğ‘¦ğ‘¤
â„’ğ¶ğ¸ğ¶ğ¼: ğ‘’ğ¼, ğ‘¦ğ¼
â„’ğ¶ğ¸ğ¶ğ¼: ğ‘’ğ‘¤ğ‘£/ğ‘’ğ‘¤ğ‘, ğ‘¦ğ¼
 Speech2Vid:
Context Enc.
Audio Enc.
Identity Enc.
Skip Connections
Face Recog.
ğ‘(ğ‘–): the ğ‘–-th 350ms segment of audio with stride 40ms
â„’ğ‘ğ‘’ğ‘Ÿğ‘: ğ¼â„“âˆ—ğ‘¥ğ‘”
â„’2 ğµ: ğ‘ğ‘™ğ‘¢ğ‘Ÿğ‘¥ğ‘¡
 Speech Driven Animation:
ğ‘(ğ‘–): a 160ms audio segment, shifted according to the frames.
Audio Enc.
Identity Enc.
Frame Disc.
Sync Disc. ğ·ğ‘ 
Sequence Disc. ğ·ğ‘
â„’ğ‘ğ‘‘ğ‘£ğ¹: ğ‘¦ğ‘“, ğ‘¥ğ‘¡
â„’ğ‘ğ‘‘ğ‘£ğ‘„: ğ‘¦ğ‘, ğ‘¥ğ‘¡
â„’ğ‘ğ‘‘ğ‘£ğ‘†: ğ‘¦ğ‘ ,
Fig. 9. Architectural schematics for some mouth reenactment networks. Black lines indicate prediction
flows used during deployment, dashed gray lines indicate dataflows performed during training.
then a pix2pix like network generates frames using the landmarks and a noise signal. Afer training,
the pix2pix network is ï¬ne-tuned using a single video of the target to ensure consistent textures.
3D Parametric Approaches. Later on in 2019, the authors of proposed a method for editing
a transcript of a talking heads which, in turn, modiï¬es the targetâ€™s mouth and speech accordingly.
Te approach is to (1) align phenomes to as, (2) ï¬t a 3D parametric head model to each frame of Xt
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
like , (3) blend matching phenomes to create any new audio content, (4) animate the head model
with the respective frames used during the blending process, and (5) generate XĞ´ with a CGAN RNN
using composites as inputs (rendered mouths placed over the original frame).
Te authors of had a diï¬€erent approach: (1) animate a the reconstructed 3D head with the
predicted blend shape parameters from as using a DeepSpeech model for feature extraction, (2)
use Deferred Neural Rendering to generate the mouth region, and then (3) use a network to
blend the mouth into the original frame. Compared to previous works, the authors found that their
approach only requires 2-3 minutes of video while producing very realistic results. Tis is because
neural rendering can summarize textures with a high ï¬delity and operate on UV maps â€“mitigating
artifacts in how the textures are mapped to the face.
Many-to-Many (Multiple IDs to Multiple IDs). One of the ï¬rst works to perform identity
agnostic video dubbing was . Tere the authors used an LSTM to map MFCC audio segments
to the face shape. Te face shapes were represented as the coeï¬ƒcients of an active appearance model
(AAM), which were then used to retrieve the correct face shape of the target.
Improvements in Lip-sync. Noting a humanâ€™s sensitivity to temporal coherence, the authors of
 use a GAN with three discriminators: on the frames, video, and lip-sync. Frames are generated by (1) encoding each MFCC audio segment a(i)
s and xt with separate encoders, (2) passing the
encodings through an RNN, and (3) decoding the outputs as x(i)
Ğ´ using a decoder.
In the authors try to improve the lipsyncing with a textual context. A time-delayed LSTM is
used to predict mouth landmarks given MFCC segments and the spoken text using a text-to-speech
model. Te target frames are then converted into sketches using an edge ï¬lter and the predicted
mouth shapes are composited into them. Finally, a pix2pix like GAN with self-atention is used to
generate the frames with both video and image conditional discriminators.
Compared to direct models such as direct models , the authors of improve the
lip-syncing by preventing the model from learning irrelevant correlations between the audiovisual
signal and the speech content. Tis was accomplished with LSTM audio-to-landmark network and
a landmark-to-identity CNN-RNN used in sequence. Tere, the facial landmarks are compressed
with PCA and the atention mechanism from is used to help focus the model on the relevant
paterns. To improve synchronization further, the authors proposed a regression based discriminator
which considers both sequence and content information.
EDs for Preventing Identity Leakage. Te authors in mitigate identity leakage by disentangling the speech and identity latent spaces using adversarial classiï¬ers. Since their speech encoder is
trained to project audio and video into the same latent space, the authors show how xĞ´ can be driven
using xs or as.
In , the authors propose Speech2Vid which also uses separate encoders for audio and identity.
However, to capture the identity beter, the identity encoder EnI uses a concatenation of ï¬ve images
of the target, and there are skip connections from the EnI to the decoder. To blend the mouth in
beter, a third â€˜contextâ€™ encoder is used to encourage in-painting. Finally, a VDSR CNN is applied
to xĞ´ to sharpen the image.
A disadvantage with and is that they cannot control facial expressions and blinking.
To resolve this, the authors in generate frames with a stride transposed CNN decoder on
GRU-generated noise, in addition to the audio and identity encodings. Teir video discriminator
uses two RNNs for both the audio and video. When applying the L1 loss, the authors focus on the
lower half of the face to encourage beter lip sync quality over facial expressions.
Later in , the same authors improve the temporal coherence by spliting the video discriminator into two: (1) for temporal realism in mouth to audio synchronization, and (2) for temporal
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
realism in overall facial expressions. Ten in , the authors tune their approach further by fusing
the encodings (audio, identity, and noise) with a polynomial fusion layer as opposed to simply
concatenating the encodings together. Doing so makes the network less sensitive to large facial
motions compared to and .
Pose Reenactment
Most deep learning works in this domain focus on the problem of face frontalization. However, there
are some works which focus on facial pose reenactment.
In the authors use a U-Net to convert (xt,lt,ls) into xĞ´ using a GAN with two discriminators:
one conditioned with the neutral pose image, and the other conditioned with the landmarks. In ,
theauthorsproposeDR-GANforpose-invariantfacerecognition. Toadjusttheposeofxt, theauthors
use an ED GAN which encodesxt aset, and then decodes (et,ps,z) asxĞ´, whereps is the sourceâ€™s pose
vectorandz isanoisevector. Comparedto , hastheï¬‚exibilityofmanipulatingtheencodings
for diï¬€erent tasks and the authors improve the quality of xĞ´ by averaging multiple examples of the
identity encoding before passing it through the decoder (similar to ). In , the authors
suggest using two GANs: Te ï¬rst frontalizes the face and produces a UV map, and second rotates
the face, given the target angle as an injected embedding. Te result is that each model performs a
less complex operation and can therefore the models collectively can produce a higher quality image.
Gaze Reenactment
Tereareonlyafewdeeplearningworkswhichhavefocusedongazereenactment. In theauthors
convert a cropped eye xt, its landmarks, and the source angle, to a ï¬‚ow (vector) ï¬eld using a 2-scale
CNN.xĞ´ is then generated by applying a ï¬‚ow ï¬eld toxt to warping it to the source angle. Te authors
then correct the illumination of xĞ´ with a second CNN. A challenge with is that the head must
be frontal to avoid inconsistencies due to pose and perspective. To mitigate this issue, the authors of
 proposedtheGazeRedirectionNetwork(GRN).InGRN,thetargetâ€™scroppedeye, headpose, and
source angle are encoded separately and then passed though an ED network to generate an optical
ï¬‚ow ï¬eld. Te ï¬eld is used to warpxt intoxĞ´. To overcome the lack of training data and the challenge
of data pairing, the authors (1) pre-train their network on 3D synthesized examples, (2) further tune
their network on real images, and then (3) ï¬ne tune their network on 3-10 examples of the target.
Body Reenactment
Several facial reenactment papers from Section 4.1 discuss body reenactment too. For example,
Vid2Vid , MocoGAN , and others . In this section, we focus on methods
which speciï¬cally target body reenactment. Schematics for some of these architectures can be found
in Fig. 10.
One-to-One (Identity to Identity). In the work , the authors perform facial reenactment with the upper-body as well (arms and hands). Te approach is to (1) use a pix2pixHD GAN
to convert the sourceâ€™s facial boundaries to the targets, (2) and then paste them onto a captured pose
skeleton of the source, and (3) use a pix2pixHD GAN to generate xĞ´ from the composite.
Many-to-One (Multiple Identities to a Single Identity).
Dance Reenactment. In the authors make people dance using a target speciï¬c pix2pixHD
GAN with a custom loss function. Te generator receives an image of the captured pose skeleton and
the discriminator receives the current and last image conditioned on their poses. Te quality of face
is then improved with a residual predicted by an additional pix2pixHD GAN, given the face region of
the pose. A many-to-one relationship is achieved by normalizing the input pose to that of the targetâ€™s.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
 Everybody Dance Now:
Pose Predictor
Body Generator
Face Refiner
ğ‘Ÿ: residual
â„’ğ¹ğ‘€ğº: ğ·ğµâˆ™ğ‘ğ‘¡
â„’ğ‘ğ‘‘ğ‘£ğµâˆ™: ğ‘¦ğµ, ğ‘ğ‘¡
â„’ğ‘ğ‘‘ğ‘£ğ¹: ğ‘¦ğ¹, ğ‘ğ‘™
â„’ğ‘ğ‘’ğ‘Ÿğ‘ğºğµ: ğ¼â„“ğ‘¥ğ‘”
â„’ğ‘ğ‘’ğ‘Ÿğ‘ğºğµ: ğ¼â„“ğ‘¥ğ‘”
ğ‘–+1 â€² , ğ¼â„“ğ‘¥ğ‘¡
 NRR-HAV:
3ğ·ğ¶: produces a textures 3D body mesh using photogrammetry software.
3ğ·ğ‘€: Motion capture. ğ‘¥ğ‘: target background image. : Hadamard product
â„’ğ‘ğ‘‘ğ‘£ğ‘ğ‘¡ğ‘¡: ğ‘¦,
â„’1: ğ‘¥ğ‘”, ğ‘¥ğ‘¡
 Deep Vid. Perf. Cloning:
Pose Predictor
shared weights
ğ‘¥ğ‘œ: an image with an identity not ğ‘ or ğ‘¡. ğ‘‚ğ¸: optical flow field extractor (between frames)
â„’ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™âˆ’ğ‘ğ‘œâ„ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’: ğ‘¥ğ‘œ,ğ‘”
â„’ğ‘ğ‘’ğ‘Ÿğ‘: ğ¼â„“1 à·œğ‘¥ğ‘¡, ğ¼â„“âˆ™ğ‘¥ğ‘¡
â„’ğ‘ğ‘‘ğ‘£1: ğ‘¦1, à·œğ‘¥ğ‘¡/ğ‘¥ğ‘¡
â„’ğ‘ğ‘‘ğ‘£2: ğ‘¦2, ğ‘¥ğ‘¡/ğ‘¥ğ‘”
 DwNet:
ğ‘¢: UV map to the body, segmented by limb. ğ‘Šğºğ¸: Warp Guided Extractor. ğ‘”: warp grid
Appearance Enc.ğ¸ğ‘›ğ´
Warp Module (ğ‘Š)
â„’ğ¹ğ‘€: ğ·â„“ğ‘˜ğ‘¥ğ‘”
â„’ğ‘ğ‘’ğ‘Ÿğ‘: ğ¼â„“ğ‘¥ğ‘”
Fig. 10. Architectural schematics for some body reenactment networks. Black lines indicate prediction
flows used during deployment, dashed gray lines indicate dataflows performed during training.
Te authors of then tried to overcome artifacts which occur in such stretched limbs due
to incorrectly detected pose skeletons. Tey used photogrammetry sofware on hundreds of images of
the target, and then reenacted the 3D rendering of the targetâ€™s body. Te rendering, partitioned depth
map, and background are then passed to a pix2pix model for image generation, using an atention loss.
Another artifact in was that the model could not generalize well to unseen poses. To improve
the generalization, the authors of trained their network on many identities other than s and t.
First they trained the GAN on paired data (the same identity doing diï¬€erent poses) and then later
added another discriminator to evaluate the temporal coherence given (1) x(i)
Ğ´ driven by another
video, and (2) the optical ï¬‚ow predicted version.
Achallengewiththepreviousworkswasthattheyrequiredalotsoftrainingdata. Tiswasreduced
from about an hour of video footage to only 3 minutes in by segmenting and orienting the
limbs of xt according to xs before the generation step. Ten a pix2pixHD GAN uses this composition
and the last k framesâ€™ poses to generate the body. Finally, another pix2pixHD GAN is used to blend
the body into the background.
Many-to-Many (Multiple IDs to Multiple IDs).
Pose Alignment. In the authors try to resolve the issue of misalignment when using pix2pix
like architectures. Tey propose â€˜deformable skip connectionsâ€™ which help orient the shutled feature
maps according to the source pose. Te authors also propose a novel nearest neighbor loss instead
of using L1 or L2 losses. To modify unseen identities at test time, an encoding of xt is passed to the
decoderâ€™s inner layers.
Although the work of helps align the general images, artifacts can still occur when xs and
xt have very diï¬€erent poses. To resolve this, the authors of use novel Pose-Atentional Transfer blocks (PATB) inside their GAN-based generator. Te architecture passes xt and the poses ps
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
concatenated with pt through separate encoders which are passed though a series of PATBs before
being decoded. Te PATBs progressively transfer regional information of the poses to regions of
the image to ultimately create a body that has beter shape and appearance consistency.
Pose Warping. In the authors use a pre-trained DensePose network to reï¬ne a predicted
pose with a warped and in-painted DensePose UV spatial map of the target. Since the spatial map
covers all surfaces of the body, the generated image has improved texture consistency. In contrast to
 which uses feature mappings to alleviate misalignment, the authors of use warping
which reduces the complexity of the networkâ€™s task. Teir model, called DwNet, uses a â€˜warp moduleâ€™
in an ED network to encode x(iâˆ’1)
warped to p(i)
s , where p is a UV body map of a pose obtained a
DensePose network.
A challenge with the alignment techniques of the previous works is that the bodyâ€™s 3D shape and
limb scales are not considered by the network resulting in identity leakage from xs. In , the
authors counter this issue with their Liquid Warping GAN. Tis is accomplished by predicting target
and sourceâ€™s 3D bodies with the model in and then by translating the two through a novel liquid
warping block (LWB) in their generator. Speciï¬cally, the estimated UV maps of xs and xt, along with
their calculated transformation ï¬‚ow, are passed through a three stream generator which produces
(1) the background via in-painting, (2) a reconstruction of the xs and its mask for feature mapping,
and (3) the reenacted foreground and its mask. Te later two streams use a shared LWB to help
the networks address multiple sources (appearance, pose, and identity). Te ï¬nal image is obtained
through masked multiplication and the system is trained end-to-end.
Background Foreground Compositing. In , the authors break the process down into three
stages, trained end-to-end: (1) use a U-Net to segmentxtâ€™s body parts and then orient them according
to the source pose ps, (2) use a second U-Net to generate the body xĞ´ from the composite, and (3) use
a third U-Net to perform in-painting on the background and paste xĞ´ into it. Te authors of then
streamlinedthisprocess byusingasingleEDGAN network todisentangletheforegroundappearance
(body), background appearance, and pose. Furthermore, by using an ED network, the user gains
control over each of these aspects. Tis is accomplished by segmenting each of these aspects before
passing them through encoders. To improve the control over the compositing, the authors of 
used a CVAE-GAN. Tis enabled the authors to change the pose and appearance of bodies individually.
Te approach was to condition the network on heatmaps of the predicted pose and skeleton.
Few-Shot Learning. In , the authors demonstrate the few-shot learning technique
of on a pix2pixHD network and the network of . Using just a few sample images, they were
able to transfer the resemblance of a target to new videos in the wild.
REPLACEMENT
Te network schematics and summary of works for replacement deepfakes can be found in Fig. 12
and Table 2 respectively.
At ï¬rst, face swapping was a manual process accomplished using tools such as Photoshop. More
automated systems ï¬rst appeared between 2004-08 in and . Later, fully automated methods
were proposed in and using methods such as warping and reconstructed 3D
morphable face models.
One-to-One (Identity to Identity).
Online Communities. Afer the Reddit user â€˜deepfakesâ€™ was exposed in the media, researchers
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
Basic Losses:
â„’1(ğ¸ğ‘›, ğ·ğ‘’ğ‘ ): à·œğ‘¥ğ‘ 
â„’1 ğ¸ğ‘›, ğ·ğ‘’ğ‘¡: à·œğ‘¥ğ‘¡
DeepFaceLab :
â„’2 ğ¸ğ‘›, ğ·ğ‘’ğ‘ : ğµğ‘™ğ‘¢ğ‘Ÿğ‘šğ‘ 
â„’2 ğ¸ğ‘›, ğ·ğ‘’ğ‘¡: ğµğ‘™ğ‘¢ğ‘Ÿğ‘šğ‘ 
Fig. 11. The basic schematic for the Reddit â€˜deepfakesâ€™ model and its variants .
and online communities began ï¬nding improved ways to perform face swapping with deep neural
networks. Te original deepfake network, published by the Reddit user, is an ED network (visualized
in Fig. 11). Te architecture consists of one encoder En and two decoders Des and Det. Te components are trained concurrently as two autoencoders: Des(En(xs))= Ë†xs and Det(En(xt))= Ë†xt, where
x is a cropped face image. As a result, En learns to map s and t to a shared latent space, such that
Des(En(xt))=xĞ´
Currently, there are a number of open source face swapping tools on GitHub based on the original
network. One of the most popular is DeepFaceLab . Teir current version oï¬€ers a wide variety
of model conï¬gurations, including adversarial training, residual blocks, a style transfer loss, and
masked loss to improve the quality of the face and eyes. To help the network map the targetâ€™s identity
into arbitrary face shapes, the training set is augmented with random face warps.
Another tool called FaceSwap-GAN follows a similar architecture, but uses a denoising
autoencoder with a self-atention mechanisms, and oï¬€ers cycle-consistency loss which can reduce
theidentityleakageandincreasetheimageï¬delity. TedecodersinFaceSwap-GANalsogeneratesegmentation masks which helps the model handle occlusions and is used to blendxĞ´ back into the target
frame. Finally, is another open source tool that provides a GUI. Teir sofware comes with 10 popular implementations, including that of , and multiple variations of the original Redit userâ€™s code.
One-to-Many (Single Identity to Multiple Identities).
In , the authors use a modiï¬ed style transfer with CNN, where the content is xt and the style
is the identity of xs. Te process is (1) align xt to a reference xs, (2) transfer the identity of s to the
image using a multi scale CNN, trained with style loss on images of s, and (3) align the output to xt
and blend the face back in with a segmentation mask.
Many-to-Many (Multiple IDs to Multiple IDs).
One of the ï¬rst identity agnostic methods was , mentioned in Section 4.1.3. However, to train
this CGAN, one needs a dataset of paired faces with diï¬€erent identities having the same expression.
Disentanglement with EDs. However, To provide more control over the In the authors us
an ED to disentangle the identity from the atributes (pose, hair, background, and lighting) during
the training process. Te identity encodings are the last pooling layer of a face classiï¬er, and the
atribute encoder is trained using a weighted L2 lossand a KL divergence loss to mitigate identity
leakage. Te authors also show that they can adjust atributes, expression, and pose via interpolation
of the encodings. Instead of swapping identities, the authors of wanted to variably obfuscate
the targetâ€™s identity. To accomplish this, the authors used an ED to predict the 3D head parameters
which where either modiï¬ed or replaced with the sourceâ€™s. Finally a GAN was used to in-paint the
face of xt given the modiï¬ed head model parameters.
Disentanglement with VAEs. In , the authors propose RSGAN: a VAE-GAN consisting of
two VAEs and a decoder. One VAE encodes the hair region and the other encodes the face region,
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
Table 2. Summary of Deep Learning Replacement Models
Replacement Retraining for newâ€¦
Model Training Model Execution Model Outp.
Source (s)
Target (t)
Identity Agnostic
Discriminators
Other Netw.
3DMM/Rendering
Segmentation
Landmark / Keypoint
Labeling of: ID
Labeling of: Other
No Pairing
Paring within Same Video
Paring ID to Diï¬€r. Actions
Requires Video
Source (xs â€¦)
Target (xt â€¦)
Resolution
 2017 Deepfakes for All
2k-5k portraits
 2018 FaceSwap-GAN
2k-5k portraits
One-to-One
DeepFaceLab
2k-5k portraits
One-to-Many
 2017 Fast Face Swap
â€¢ 60 portraits None
portrait portrait
 2018
portrait portrait
 2018
portrait portrait
portrait portrait
 2018
None â€¢ 3 2
portrait portrait
 2018
None â€¢ 4 4
â€¢ â€¢ portrait portrait
 2018
None â€¢ 1 1
portrait portrait
 2019
FS Face Trans.
None â€¢ 1 1
portraits portrait
 2019
None â€¢ 2 1
â€¢ cropped cropped
Many-to-Many
FaceShifer
None â€¢ 3 3
portrait portrait
where both are conditioned on a predicted atribute vector c describing x. Since VAEs are used, the
facial atributes can be edited through c.
In contrast to , the authors of use a VAE to prepare the content for the generator, and
use a network to perform the blending via in-painting. A single VAE-ED network is run on xs and
then xt producing encodings for the face of xs and the landmarks of xt. To perform a face swap,
a generator receives the masked portrait of xt and performs in-painting on the masked face. Te
generator uses the landmark encodings in its embedding layer. During training, randomly generated
faces are used with triplet loss on the encodings to preserve identities.
Face Occlusions. FSGAN , mentioned Section 4.1.3, is also capable of face swapping and can
handle occlusions. Afer the face reenactment generator produces xr, a second network predicts
the targetâ€™s segmentation mask mt. Ten (x âŸ¨f âŸ©
,mt) is passed to a third network that performs
in-painting for occlusion correction. Finally a fourth network blends the corrected face into xt while
considering ethnicity and lighting. Instead of using interpolation like , the authors of 
propose FaceShifer which uses novel Adaptive Atentional Denormalization layers (AAD) to transfer
localized feature maps between the faces. In contrast to , FaceShifer reduces the number of
operations by handling the occlusions through a reï¬nement network trained to consider the delta
between the original xt and a reconstructed Ë†xt.
Few-Shot Learning. Te same author of FaceSwap-GAN also hosts few-shot approach online dubbed â€œOne Model to Swap Tem Allâ€ . In this version the generator receives
,mt) where its encoder is conditioned on VGGFace2 features of xt using FC-AdaIN layers, and its decoder is conditioned on xt and the face structure mt via layer concatenations and
SPADE-ResBlocks respectively. Two discriminators are used: one on image quality given the face
segmentation and the other on the identities.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
 Face Swap GAN:
ğ‘š: segmentation mask (face)
â„’ğ‘¡ğ‘£(ğ‘ ): ğ‘¥ğ‘ 
â„’ğ‘¡ğ‘£(ğ‘¡): ğ‘¥ğ‘¡
â„’ğ‘ğ‘‘ğ‘£(ğ‘ ): ğ‘¦ğ‘ , ğ‘¥ğ‘ â€²/à·œğ‘¥ğ‘ 
â„’ğ‘ğ‘‘ğ‘£(ğ‘¡): ğ‘¦ğ‘¡, ğ‘¥ğ‘¡
â„’1(ğ‘ ): à·œğ‘¥ğ‘ 
â„’1(ğ‘¡): à·œğ‘¥ğ‘¡
â„’ğ‘ğ‘’ğ‘Ÿğ‘(ğ‘ ): ğ¼â„“âˆ™
â„’ğ‘ğ‘’ğ‘Ÿğ‘(ğ‘¡): ğ¼â„“âˆ™
â„’ğ‘ğ‘¦ğ‘(ğ‘ ): ğºğ‘¡ğ‘ ğºğ‘ ğ‘¡ğ‘¥ğ‘ 
â„’ğ‘ğ‘¦ğ‘(ğ‘¡): ğºğ‘ ğ‘¡ğºğ‘¡ğ‘ ğ‘¥ğ‘¡
 Fast Face Swap:
Style Transfer Network
Light Measure
Siamese Network
â„’ğ‘ğ‘’ğ‘Ÿğ‘: ğ¿ğ‘¥ğ‘”
â„’ğ‘ ğ‘¡ğ‘¦ğ‘™ğ‘’: ğ‘¥ğ‘”
â„’ğ¹ğ‘€: ğ¿â„“âˆ™ğ‘¥ğ‘”
 OSIP-FS:
Identity Enc.
Attribute Enc.
Discriminator
â„’ğ¶ğ¸ğ¸ğ‘›ğ¼: ğ‘¦ğ¼, ğ‘¥
â„’2 ğ¸ğ‘›ğ´: ğ‘¥ğ‘”, ğ‘¥ğ‘¡-weighted â„’ğ¾ğ¿: ğ‘’ğ´
â„’ğ‘ğ‘‘ğ‘£ğ·: ğ‘¦, ğ‘¥ğ‘¡/ğ‘¥ğ‘”
â„’ğ¹ğ‘€ğ·ğ‘’: ğ·â„“âˆ™ğ‘¥ğ‘”, ğ·â„“âˆ™(ğ‘¥ğ‘¡)
â„’ğ¶ğ¸(ğ·ğ¼): ğ‘¦ğ¼, ğ‘¥â€²/ğ‘¥ğ‘”
â„’ğ¹ğ‘€ğ·ğ‘’: ğ·ğ¼â„“âˆ™ğ‘¥ğ‘”, ğ·ğ¼â„“âˆ™(ğ‘¥â€²)
 RSGAN:
ğ‘¥â„: Hair region
â„’1: ğ‘¥ğ‘“, à·œğ‘¥ğ‘“
â„’1: ğ‘¥â„, à·œğ‘¥â„
â„’ğ‘ğ‘‘ğ‘£1: ğ‘¦, Î¤
â„’ğ‘ğ‘‘ğ‘£2: ğ‘¦, Î¤
â„’ğ¶ğ¸ğ¶: ğ‘, ğ‘¦ğ‘
 FSNet:
ED Network (EN)
Face Encoder
Landmark Enc.
Patch disc.
VAE Objectives: â„’ğ¶ğ¸ğ¸ğ·:ğ‘šğ‘“, à·ğ‘šğ‘“
â„’ğ¶ğ¸ğ¸ğ·: ğ‘™, áˆ˜ğ‘™â„’1 ğ¸ğ·: ğ‘¥ğ‘“, à·œğ‘¥ğ‘“
â„’1 ğ¸ğ·: ğ‘¥ğ‘“âˆ™ğ‘šğ‘“, à·œğ‘¥ğ‘“âˆ™ğ‘šğ‘“
â„’ğ‘ğ‘‘ğ‘£ğ¸ğ·: ğ‘¦ğ‘š, ğ‘šğ‘“/ğ‘šâ€² ğ‘“
â„’ğ‘ğ‘‘ğ‘£ğ¸ğ·: ğ‘¦ğ‘“, ğ‘¥ğ‘“/ğ‘¥â€² ğ‘“
â„’ğ‘ğ‘‘ğ‘£ğ¸ğ·: ğ‘¦ğ‘™, ğ‘™/ğ‘™â€² ğ‘“
â„’ğ‘ğ‘‘ğ‘£: ğ‘¦, ğ‘¥â€²/ğ‘¥ğ‘”
â„’ğ‘ğ‘‘ğ‘£,ğ‘ƒ: ğ‘¦ğ‘ƒ, ğ‘¥â€²/ğ‘¥ğ‘”
â„’ğ‘¡ğ‘Ÿğ‘–ğ‘ğ¸ğ·: (ğ‘¥ğ‘¡1, ğ‘¥ğ‘¡2, ğ‘¥ğ‘ ), (ğ‘¥ğ‘”1, ğ‘¥ğ‘¡1, ğ‘¥ğ‘ ), (ğ‘¥ğ‘”2, ğ‘¥ğ‘¡2, ğ‘¥ğ‘ )
 FaceShifer:
Identity Enc.
Attribute Encoder
Recog. Net
Occlusion Correction
â„’ğ‘ğ‘‘ğ‘£1: ğ‘¦1, àµ—
â„’ğ‘ğ‘‘ğ‘£2: ğ‘¦2,
â„’ğ‘ğ‘‘ğ‘£3: ğ‘¦3,
â„’ğ‘ğ‘œğ‘ ğ‘–ğ‘›ğ‘’ğ·ğ‘’, ğ»1 : ğ‘’ğ‘ ğ¼, ğ‘’ğ‘”ğ¼
â„’ğ‘ğ‘’ğ‘Ÿğ‘ğ·ğ‘’, ğ»1 : ğ»1 â„“âˆ™ğ‘¥ğ‘¡, ğ»1 â„“âˆ™ğ‘¥ğ‘”âˆ—
â„’2 ğ·ğ‘’, ğ»1 : ğ‘¥ğ‘”, ğ‘¥ğ‘¡-if s=t
â„’ğ‘ğ‘œğ‘ ğ‘–ğ‘›ğ‘’ğ»2 : ğ‘’ğ‘ ğ¼, ğ‘’ğ‘”ğ¼
â„’1 ğ»2 : ğ‘¥ğ‘”, ğ‘¥ğ‘¡âˆ’ğ‘¥ğ‘”âˆ—
â„’2 ğ»2 : ğ‘¥ğ‘”, ğ‘¥ğ‘¡-if s=t
 Few-Shot Face Translation:
Face Segmentation
Losses unknown: Training code was not released as of writing
 Depth Nets:
Blend Repair
ğ‘ğ‘ ğ‘¡: affine transformation parameters and depth measures
â„’ğ‘ğ‘¢ğ‘ ğ‘¡ğ‘œğ‘š(ğ¸ğ‘›, ğ‘ƒ): ğ‘ğ‘ ğ‘¡
â„’ğ‘ğ‘¦ğ‘ğ»ğ‘”ğ‘¥, ğ»ğ‘¥ğ‘”: ğ‘¥ğ‘”âˆ—, ğ‘¥
â„’ğ‘ğ‘‘ğ‘£ğ·ğ‘¡: ğ‘¦ğ‘¡, Î¤
â„’ğ‘ğ‘‘ğ‘£ğ·âˆ™: ğ‘¦ğ‘¡,
 IHPT:
Identity Enc.
Video ID Classifier
Realism D.
Identity D.
â„’1: ğ‘¥ğ‘”, ğ‘¥ğ‘¡
â„’1: ğ‘’ğ¼, Æ¸ğ‘’ğ¼
â„’1: ğ‘’ğ‘ƒ, Æ¸ğ‘’ğ‘ƒ
â„’ğ‘ğ‘–ğ‘¥ğ‘’ğ‘™âˆ’ğ‘£ğ‘’ğ‘Ÿğ‘–ğ‘“: ğ‘¥ğ‘”, ğ‘¥ğ‘¡
â„’ğ¶ğ¸ğ·ğ‘’, ğ¶: ğ‘¦, à·œğ‘¥
â„’ğ‘ğ‘‘ğ‘£1: ğ‘¦1, Î¤
â„’ğ‘ğ‘‘ğ‘£2: ğ‘¦2,
â„’ğ‘ğ‘‘ğ‘£ğ¼1: ğ‘¦ğ¼1, Î¤
â„’ğ‘ğ‘‘ğ‘£ğ¼2: ğ‘¦ğ¼2,
 FSGAN:
ğ‘š: segmentation mask (face, hair, other), ğ‘™: 3D facial landmarks,
ğ»ğ‘›: ğ‘›passes through ğ»while interpolating ğ‘™ğ‘ to ğ‘™ğ‘¡
Reenactment
Segmentation
in-painting
â„’1 ğ»1 : ğ‘¥ğ‘¡
â„’ğ‘ğ‘’ğ‘Ÿğ‘ğ»1 : ğ¼â„“âˆ™ğ‘¥ğ‘¡
â„’ğ‘ğ‘‘ğ‘£1: ğ‘¦1, ğ‘¥ğ‘¡
ğ‘“/ğ‘¥ğ‘Ÿâ„’1 ğ»1 : ğ‘šğ‘Ÿ, ğ‘šğ‘¡
â„’1 ğ»2 : ğ‘šğ‘Ÿ, ğ»1,ğ‘šğ‘¥ğ‘¡
â„’1 ğ»3 : ğ‘¥ğ‘Ÿâ€², ğ‘¥ğ‘¡
â„’ğ‘ğ‘’ğ‘Ÿğ‘(ğ»3): ğ¼â„“âˆ™ğ‘¥ğ‘Ÿâ€² , ğ¼â„“âˆ™ğ‘¥ğ‘¡
â„’ğ‘ğ‘‘ğ‘£3: ğ‘¦3, ğ‘¥ğ‘ â€²/ğ‘¥ğ‘¡
â„’ğ‘ğ‘‘ğ‘£4: ğ‘¦4, ğ‘¥ğ‘”/ğ‘¥ğ‘¡
â„’ğ‘ğ‘œğ‘–ğ‘ ğ‘ ğ‘œğ‘›âˆ’ğ‘ğ‘’ğ‘Ÿğ‘ğ»4 : ğ‘¥ğ‘¡, ğ‘šğ‘¡, ğ‘ğ‘ğ‘ ğ‘¡ğ‘’(ğ‘¥ğ‘Ÿâ€², ğ‘¥ğ‘¡)
Fig. 12. Architectural schematics of the replacement networks with their generation and training dataflows.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
Although face transfers precede face swaps, today there are very few works that use deep learning
for this task. However, we note that face a transfer is equivalent to performing self-reenactment on
a face swapped portrait. Terefore, high quality face a transfers can be achieved by combining a
method from Section 4.1 and Section 5.1.
In 2018, the authors of proposed DepthNets: an unsupervised network for capturing facial
landmarks and translating the pose from one identity to another. Te authors use a Siamese network
to predict a transformation matrix that maps the xsâ€™s 3D facial landmarks to the corresponding 2D
landmarksofxt. A3Drenderer(OpenGL)isthenusedtowarpx âŸ¨f âŸ©
tothesourceposelt,andthecomposition is reï¬ned using a CycleGAN. Since warping is involved, the approach is sensitive to occlusions.
Later in 2019, the authors of proposed a self-supervised network which can change the
identity of an object within an image. Teir ED disentangles the identity from an objects pose using
a novel disentanglement loss. Furthermore to handle misaligned poses, an L1 loss is computed using
a pixel mapped version of xĞ´ to xs (using the weights of the identity encoder). Similarly, the authors
of proposed a method disentangled identity transfer. However neither or were
explicitly performed on faces.
COUNTERMEASURES
In general, countermeasures to malicious deepfakes can be categorized as either detection or prevention. We will now brieï¬‚y discuss each accordingly. A summary and systematization of the deepfake
detection methods can be found in Table 3.
Te subject of image forgery detection is a well researched subject . In our review of detection
methods, we will focus on works which speciï¬cally deal with detecting deepfakes of humans.
Artifact-Specific. Deepfakes ofen generate artifacts which may be subtle to humans, but can
be easily detected using machine learning and forensic analysis. Some works identify deepfakes by
searching for speciï¬c artifacts. We identify seven types of artifacts: Spatial artifacts in blending, environments, and forensics; temporal artifacts in behavior, physiology, synchronization, and coherence.
Blending (spatial). Some artifacts appear where the generated content is blended back into the
frame. To help emphasize these artifacts to a learner, researchers have proposed edge detectors,
quality measures, and frequency analysis . In the authors follow a more explicit
approach to detecting the boundary. Tey trained a CNN network to predict an imageâ€™s blending
boundary and a label (real or fake). Instead of using a deepfake dataset, the authors trained their
network on a dataset of face swaps generated by splicing similar faces found through facial landmark
similarity. By doing so, the model has the advantage that is focuses on the blending boundary and
not other artifacts caused by the generative model.
Environment (spatial). Te content of a fake face can be anomalous in context to the rest of the
frame. For example, residuals from face warping processes , lighting , and varying
ï¬delity can indicate the presence of generated content. In , the authors follow a diï¬€erent
approach by contrasting the generated foreground to the (untampered) background using a patch and
pair CNN. Te authors of also contrast the fore/background but enable a network to identify
the distinguishing features automatically. Tey accomplish this by (1) encoding the face and context
(hair and background) with an ED and (2) passing the diï¬€erence between the encodings with the
complete image (encoded) to a classiï¬er.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
Forensics (spatial). Several works detect deepfakes by analyzing subtle features and paterns lef
by the model. In and , the authors found that GANs leave unique ï¬ngerprints and show
how it is possible to classify the generator given the content, even in the presence of compression and
noise. In the authors analyze a cameraâ€™s unique sensor noise (PRNU) to detect pasted content.
To focus on the residuals, the authors of use a two stream ED to encode the color image and
a frequency enhanced version using â€œLaplacian of Gaussian layersâ€ (LoG). Te two encodings are
then fed through an LSTM which then classiï¬es the video based on a sequence of frames.
Instead of searching for residuals, the authors of search for imperfections and found that
deepfakes tend to have inconsistent head poses. Terefore, they detect deepfakes by predicting and
monitoring facial landmarks. Te authors of had a diï¬€erent approach by training classiï¬er
to focus on the imperfections instead of the residuals. Tis was accomplished by using a dataset
generated using a ProGAN instead of other GANs since the ProGANâ€™s images contain the least
amount of frequency artifacts. In contrast to , the authors in use a network to emphasize the
residuals and suppress the imperfections in a preprocessing step for a classiï¬er. Teir network uses
adaptive convolutional layers that predict residuals to maximize the artifactsâ€™ inï¬‚uence. Although
this approach may help the network identify artifacts beter, it may not generalize as well to new
types of artifacts.
Behavior (temporal). With large amounts of data on the target, mannerisms and other behaviors
can be monitored for anomalies. For example, in the authors protect world leaders from a wide
variety of deepfake atacks by modeling their recorded stock footage. Recently, the authors of 
showed how behavior can be used with no reference footage of the target. Te approach is to detect
discrepancies in the perceived emotion extracted from the clipâ€™s audio and video content. Te authors
use a custom Siamese network to consider the audio and video emotions when contrasted to real
and fake videos.
Physiology (temporal). In 2014, researchers hypothesized that generated content will lack physiological signals and identiï¬ed computer generated faces by monitoring their heart rate . Regarding
deepfakes, monitored blood volume paterns (pulse) under the skin, and took a more robust
approach by monitoring irregular eye blinking paterns. Instead of detecting deepfakes, the authors
of use the pulse signal to help determine the model used to create the deepfake.
Synchronization (temporal). Inconsistencies are also a revealing factor. In and , the
authors noticed that video dubbing atacks can be detected my correlating the speech to landmarks
around the mouth. Later, in , the authors reï¬ned the approach by detecting when visemes
(mouth shapes) are inconsistent with the spoken phonemes (uternaces). In particular, they focus
on phonemes where the mouth is fully closed (B, P, M) since deepfakes in the wild tend to fail in
generating these visemes.
Coherence (temporal). As noted in Section 4.1, realistic temporal coherence is challenging to generate, and some authors capitalize on the resulting artifacts to detect the fake content. For example,
 uses an RNN to detect artifacts such as ï¬‚ickers and jiter, and uses an LSTM on the face
region only. In a classiï¬er is trained pairs of sequential frames and in the authors reï¬ne
the networkâ€™s focus by monitoring the framesâ€™ optical ï¬‚ow. Later the same authors use an LSTM
to predict the next frame, and expose deepfakes when the reconstruction error is high .
Undirected Approaches. Instead of focusing on a speciï¬c artifact, some authors train deep
neural networks as generic classiï¬ers, and let the network decide which features to analyze. In
general, researchers have taken one of two approaches: classiï¬cation or anomaly detection.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
Classiï¬cation. In , it was shown that deep neural networks tend to perform beter
than traditional image forensic tools on compressed imagery. Various authors then demonstrated
how standard CNN architectures can eï¬€ectively detect deepfake videos . In , the
authors train the CNN as a Siamese network using contrasting examples of real and fake images.
In , the authors were concerned that a CNN can only detect the atacks on which they trained. To
close this gap, the authors propose using Hierarchical Memory Network (HMN) architecture which
considers the contents of the face and previously seen faces. Te network encodes the face region
which is then processed using a bidirectional GRU while applying an atention mechanism. Te ï¬nal
encoding is then passed to a memory module, which compares it to recently seen encodings and
makesaprediction. Later, in , theauthorsuseanensembleapproachandleveragethepredictions
of seven deepfake CNNs by passing their predicitons to a meta classifer. Doing so produces results
which are more robust (fewer false positives) than using any single model. In , the authors tried a
variety of diï¬€erent classic spatio-temproal networks and feature extractors as a baseline for temporal
deepfake detection. Tey found that a 3D CNN, which looks at multiple frames at once, out performs
both recurrent networks and a the state of the art ID3 architecture.
To localize the tampered areas, some works train networks to predicting masks learned from a
ground truth dataset, or by mapping the neural activations back to the raw image .
In general, we note that the use of classiï¬ers to detect deepfakes is problematic since an atacker
can evade detection via adversarial machine learning. We will discuss this issue further in Section 7.2.
Anomaly Detection. In contrast to classiï¬cation, anomaly detection models are trained on the
normal data and then detect outliers during deployment. By doing so, these methods do not make
assumptions on how the atacks look and thus generalize beter to unknown creation methods.
Te authors of follow this approach by measuring the neural activation (coverage) of a face
recognition network. By doing so, the model is able to overcome noise and other distortions, by
obtaining a stronger signal from than just using the raw pixels. Similarly, in a one-class VAE
is trained to used to reconstruct real images. Ten, for new images, an anomaly score is computed
by taking the MSE between mean component of the encoded image and the mean component of the
reconstructed image. Alternatively, the authors of measure an inputâ€™s embedding distance to
real samples using an EDâ€™s latent space. Te diï¬€erence between these works is that and rely
on a modelâ€™s inability to process unknown paterns while contrasts the modelâ€™s representations.
Instead of using a neural network directly, the authors of use a state of the art atribution
based conï¬dence metric (ABC). To detect a fake image, the ABC is used to determine if the image
ï¬ts the training distribution of a pretrained face recognition network (e.g., VGG).
Prevention & Mitigation
Data Provenance. To prevent deepfakes, some have suggested that data provenance of multimedia
should be tracked through distributed ledgers and blockchain networks . In the authors
suggest that the content should be ranked by participants and AI. In contrast, proposes that the
content should authenticated and managed as a global ï¬le system over Etherium smart contracts.
Counter Attacks. To combat deepfakes, the authors of show how adversarial machine learning can be used to disrupt and corrupt deepfake networks. Te authors perform adversarial machine
learning to add crafed noise perturbations to x, which prevents deepfake technologies from locating
a proper face in x. In a diï¬€erent approach, the authors of use adversarial noise to change the
identity of the face so that web crawlers will not be able ï¬nd the image of t to train their model.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
Table 3. Summary of Deepfake Detection Models
Type Modality Content
Eval. Dataset
Performance*
Reenactment
Replacement
Indicates Aï¬€ected Area
Input Resolution
DeepfakeTIMIT 
DFFD 
FaceForensics 
FaceForensics++ 
Celeb-DF 
Other Deepfake DB
 2017
 2018 â€¢ â€¢ â€¢ â€¢
 2018 â€¢ â€¢
â€¢ SVM, Kmeansâ€¦
 2019 â€¢ â€¢ â€¢ â€¢
Classic ML
 2019 â€¢ â€¢
 2018 â€¢ â€¢ â€¢ â€¢
 2018 â€¢ â€¢
 2018 â€¢ â€¢ â€¢ â€¢
Capsule-CNN
 2018 â€¢ â€¢ â€¢
 2018 â€¢ â€¢
 2018 â€¢ â€¢ â€¢
 2018 â€¢ â€¢ â€¢ â€¢
 2018 â€¢ â€¢
 2019 â€¢ â€¢
 2019 â€¢ â€¢ â€¢ â€¢
 2019 â€¢ â€¢ â€¢ â€¢
CNN AE GAN â€¢
 2019 â€¢ â€¢ â€¢ â€¢
CNN+Atention â€¢
 2019 â€¢ â€¢ â€¢ â€¢
 2019
 2019 â€¢ â€¢ â€¢ â€¢
 2019 â€¢ â€¢ â€¢ â€¢
 2019 â€¢ â€¢ â€¢ â€¢
 2019 â€¢ â€¢ â€¢ â€¢
 2019 â€¢ â€¢ â€¢
 2019 â€¢ â€¢
 2019 â€¢ â€¢ â€¢ â€¢
 2019 â€¢ â€¢
[? ] 2019 â€¢
 2019 â€¢
 2019 â€¢
Deep Learning
 2019 â€¢ â€¢ â€¢
 2019 â€¢ â€¢ â€¢ â€¢
SVM+VGGnet
 2019 â€¢ â€¢ â€¢ â€¢
 2020 â—¦â€¢ â€¢
20.86 0.86
 2020 â€¢ â€¢ â€¢
 2020 â€¢ â€¢ â€¢
 2020 â€¢ â€¢
 2020 â€¢ â€¢ â€¢
CNN ResNet
Prec.= 0.93
 2020 â€¢ â€¢ â€¢
 2020 â€¢ â€¢
 2020 â€¢ â€¢
 2020 â€¢ â€¢
 2020 â€¢ â€¢ â€¢
Siamese CNN
â€¢ TPR=0.91
 2020 â€¢ â€¢ â€¢
 2020 â€¢ â€¢
 2020 â€¢ â€¢ â€¢
 2020 â€¢ â€¢ â€¢
ABC-ResNet
 2019
Statistics
Statistics & Steganalysis
 2019 â€¢ â€¢ â€¢
*Only the best reported performance, averaged over the test datasets, is displayed to capture the â€˜best-caseâ€™ scenario.
DISCUSSION
The Creation of Deepfakes
Trade-oï¬€s Between the Methodologies. In general, there is a diï¬€erent cost and payoï¬€for
each deepfake creation method. However, the most eï¬€ective and threatening deepfakes are those
which are (1) the most practical to implement [Training Data, Execution Speed, and Accessibility]
and (2) are the most believable to the victim [Qality]:
Data vs Qality. Models trained on numerous samples of the target ofen yield beter results (e.g.,
 ). For example, in 2017, produced an extremely believable
reenactment of Obama which exceeds the quality of recent works. However, these models require
many hours footage for training, and are therefore are only suitable for exposed targets such as
actors, CEOs, and political leaders. An atacker who wants to commit defamation, impersonation,
or a scam on an arbitrary individual will need to use a many-to-many or few-shot approach. On
the other hand, most of these methods rely on a single reference of t and are therefore prone to
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
generating artifacts. Tis is because the model must â€˜imagineâ€™ missing information (e.g., diï¬€erent
poses and occlusions). Terefore, approaches which provide the model with a limited number of
reference samples strike the best balance between data and quality.
Speed vs Qality. Te trade-oï¬€between these aspects depends on whether the atack is online
(interactive) or oï¬„ine (stored media). Social engineering atacks involving deepfakes are likely
to be online and thus require real-time speeds. However, high resolution models have many
parameters and sometimes use several networks (e.g., ) and some process multiple frames to
provide temporal coherence (e.g., ). Other methods may be slowed down due to their
pre/post-processing steps, such as warping , UV mapping or segmentation prediction
 , and the use of reï¬nement networks . To the best of
our knowledge, and are the only papers which claim to generate real time
deepfakes, yet they subjectively tend to be blurry or distort the face. Regardless, a victim is likely
fall for an imperfect deepfake in a social engineering atack when placed under pressure in a false
pretext . Moreover, it is likely that an atacker will implement a complex method at a lower
resolution to speed up the frame rate. In which case, methods that have texture artifacts would be
preferredoverthosewhichproduceshapeoridentityï¬‚aws(e.g., vs ). Foratacksthatare
not real-time (e.g, fake news), resolution and ï¬delity is critical. In these cases, works that produce
high quality images and videos with temporal coherence are the best candidates (e.g., ).
Availability vs Qality. We also note that availability and reproducibility are key factors in the
proliferation of new technologies. Works that publish their code and datasets online (e.g.,
 ) are more likely to be used by researchers and criminals compared
to those which are unavailable or require highly speciï¬c
or private datasets . Tis is because the payoï¬€in implementing a paper is minor
compared to using a functional and eï¬€ective method available online. Of course, this does not
include state-actors who have plenty of time and funding.
We have also observed that approaches which augment a networkâ€™s inputs with synthetic ones
produce beter results in terms of quality and stability. For example, by rotating limbs ,
reï¬ning rendered heads , providing warped imagery 
and UV maps . Tis is because the provided contextual information reduces the
problemâ€™s complexity for the neural network.
Given these considerations, in our opinion, the most signiï¬cant and available deepfake technologies today are for facial reenactment because of itâ€™s eï¬ƒciency and practicality; for mouth
reenactment because of its quality; and for face replacement because its high ï¬delity and wide
spread use. However, this is a subjective opinion based on the samples provided online and in the respectivepapers. Acomparativeresearchstudy, wherethemethodsaretrainedonthesamedatasetand
evaluated by a number of people is necessary to determine the best quality deepfake in each category.
ResearchTrends. Overthelastfewyearstherehasbeenashiftowardsidentityagnosticmodels and high resolution deepfakes. Some notable advancements include (1) unpaired self-supervised
training techniques to reduce the amount of initial training data, (2) one/few-shot learning which enables identity thef with a single proï¬le picture, (3) improvements of face quality and identity through
AdaIN layers, disentanglement, and pix2pixHD network components, (4) ï¬‚uid and realistic videos
through temporal discriminators and optical ï¬‚ow prediction, and (5) the mitigation of boundary
artifacts by using secondary networks to blend composites into seamless imagery (e.g., ).
Another large advancement in this domain was the use of perceptual loss on a pre-trained VGG
Face recognition network. Te approach boosts the facial quality signiï¬cantly, and as a result, has
been adopted in popular online deepfake tools . Another advancement being adopted is the
use of a network pipeline. Instead of enforcing a set of global losses on a single network, a pipeline of
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
networks is used where each network is tasked with a diï¬€erent responsibility (conversion, generation,
occlusions, blending, etc.) Tis give more control over the ï¬nal output and has been able to mitigate
most of the challenges mention in Section 3.7.
Current Limitations. Aside from quality, there are a few limitations with the current deepfake technologies. First, for reenactment, content is always driven and generated with a frontal pose.
Tis limits the reenactment to a very static performance. Today, this is avoided by face swapping the
identity onto a lookalikeâ€™s body, but a good match is not always possible and this approach has limited
ï¬‚exibility. Second, reenactments and replacements depend on the driverâ€™s performance to deliver the
identityâ€™s personality. We believe that next generation deepfakes will utilize videos of the target to
stylize the generated content with the expected expressions and mannerisms. Tis will enable a much
more automatic process of creating believable deepfakes. Finally, a new trend is real-time deepfakes.
Works such as have achieved real-time deepfakes at 30fps. Although real-time deepfakes
are an enabler for phishing atacks, the realism is not quite there yet. Other limitations include the
coherent rendering of hair, teeth, tongues, shadows, and the ability to render the targetâ€™s hands
(especially when touching the face). Regardless, deepfakes are already very convincing and
are improving at a rapid rate. Terefore, it is important that we focus on eï¬€ective countermeasures.
The Deepfake Arms Race
Likeanybatleincybersecurity,thereisanarmsracebetweentheatackeranddefender. Inoursurvey,
we observed that the majority deepfake detection algorithms assume a static game with the adversary:
Tey are either focused on identifying a speciï¬c artifact, or do not generalize well to new distributions
and unseen atacks . Moreover, based on the recent benchmark of , we observe that the performance of state-of-the-art detectors are decreasing rapidly as the quality of the deepfakes improve.
Concretely, the three most recent benchmark datasets (DFD by Google , DFDC by Facebook ,
and Celeb-DF by ) were released within one month of each other at the end of 2019. However, the
deepfake detectors only achieved an AUC of 0.86,0.76, and 0.66 on each of them respectively. Even
a false alarm rate of 0.001 is far too low considering the millions of images published online daily.
EvadingArtifact-basedDetectors. Toevadeanartifact-baseddetector,theadversaryonlyneedsto
mitigateasingleï¬‚awtoevadedetection. Forexample,G cangeneratethebiologicalsignalsmonitored
by by adding a discriminator which monitors these signals. To avoid anomalies in extensive
the neuron activation , the adversary can add a loss which minimizes neuron coverage. Methods
which detect abnormal poses and mannerisms can be evaded by reenacting the entire head and by
learning the mannerisms from the same databases. Models which identify blurred content are
aï¬€ected by noise and sharpening GANs , and models which search for the boundary where the
face was blended in do not work on deepfakes passed through reï¬ner networks,
which use in-painting, or those which output full frames (e.g., ).
Finally, solutions which search for forensic evidence can be evaded (or at least raise the
false alarm rate) by passing xĞ´ through ï¬lters, or by performing physical replication or compression.
Evading Deep Learning Classiï¬ers. Tere are a number of detection methods which apply deep
learning directly to the task of deepfake detection (e.g., ). However, an adversary can
use adversarial machine learning to evade detection by adding small perturbations toxĞ´. Advances in
adversarial machine learning has shown that these atacks transfer across multiple models regardless
of the training data used . Recent works have shown how these atacks not only work on
deepfakes classiï¬ers but also work with no knowledge of the classiï¬er or itâ€™s training set .
Moving Forward. Nevertheless, deepfakes are still imperfect, and these methods oï¬€er a modest
defense for the time being. Furthermore, these works play an important role in understanding the
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
currentlimitationsofdeepfakes,andraisethediï¬ƒcultythresholdformalicioususers. Atsomepoint,it
may become too time-consuming and resource-intensive a common atacker to create a good-enough
fake to evade detection. However, we argue that solely relying on the development of content-based
countermeasures is not sustainable and may lead to a reactive arms-race. Terefore, we advocate for
more out-of-band approaches for detecting a preventing deepfakes. For example, the establishment
of content provenance and authenticity frameworks for online videos , and proactive
defenses such as the use of adversarial machine learning to protect content from tampering .
Deepfakes in other Domains
In this survey, we put a focus on human reenactment and replacement atacks; the type of deepfakes
which has made the largest impact so far . However, deepfakes extend beyond human visuals,
and have spread many other domains. In healthcare, the authors of showed how deepfakes can
be used to inject tor remove medical evidence in CT and MRI scan for insurance fraud, disruption, and
physical harm. In it was shown how oneâ€™s voice can be cloned with only ï¬ve seconds of audio,
and in Sept. 2019 a CEO was scammed out of $250K via a voice clone deepfake . Te authors
of have shown how deep learning can generate realistic human ï¬ngerprints that can unlock
multiple usersâ€™ devices. In it was shown how deepfakes can be applied to ï¬nancial records to
evade the detection of auditors. Finally, it has been shown how deepfakes of news articles can be
generated and that deepfake tweets exist as well .
Teseexamplesdemonstratethatdeepfakesarenotjustatacktoolsformisinformation,defamation,
and propaganda, but also sabotage, fraud, scams, obstruction of justice, and potentially many more.
Whatâ€™s on the Horizon
We believe that in the coming years, we will see more deepfakes being weaponized for monetization.
Te technology has proven itself in humiliation, misinformation, and defamtion atacks. Moreover,
the tools are becoming more practical and eï¬ƒcient . Terefore, is seems natural that malicious
users will ï¬nd ways to use the technology for a proï¬t. As a result, we expect to see an increase in
deepfake phishing atacks and scams targeting both companies and individuals.
As the technology matures, real-time deepfakes will become increasingly realistic. Terefore, we
can expect that the technology will be used by hacking groups to perform reconnaissance as part
of an APT, and by state actors to perform espionage and sabotage by reenacting of oï¬ƒcials or family
To keep ahead of the game, we must be proactive and consider the adversaryâ€™s next step, not
just the weaknesses of the current atacks. We suggest that more work be done on evaluating the
theoretical limits of these atacks. For example, by ï¬nding a bound on a modelâ€™s delay can help detect
real-time atacks such as , and determining the limits of GANs like can help us devise the
appropriate strategies. As mentioned earlier, we recommend further research on solutions which
do not require analyzing the content itself. Moreover, we believe it would be beneï¬cial for future
works to explore the weaknesses and limitations of current deepfakes detectors. By identifying and
understanding these vulnerabilities, researchers will be able to develop stronger countermeasures.
CONCLUSION
Notalldeepfakesaremalicious. However, becausethetechnologymakesitsoeasytocreatebelievable
media, malicious users are exploiting it to perform atacks. Tese atacks are targeting individuals
and causing psychological, political, monetary, and physical harm. As time goes on, we expect to
see these malicious deepfakes spread to many other modalities and industries.
In this survey we focused on reenactment and replacement deepfakes of humans. We provided
a deep review of how these technologies work, the diï¬€erences between their architectures, and
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
what is being done to detect them. We hope this information will be helpful to the community in
understanding and preventing malicious deepfakes.