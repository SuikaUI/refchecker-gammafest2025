The Creation and Detection of Deepfakes: A Survey
YISROEL MIRSKY∗, Georgia Institute of Technology and Ben-Gurion University
WENKE LEE, Georgia Institute of Technology
Generative deep learning algorithms have progressed to a point where it is diﬃcult to tell the diﬀerence between
what is real and what is fake. In 2018, it was discovered how easy it is to use this technology for unethical
and malicious applications, such as the spread of misinformation, impersonation of political leaders, and the
defamation of innocent individuals. Since then, these ‘deepfakes’ have advanced signiﬁcantly.
In this paper, we explore the creation and detection of deepfakes an provide an in-depth view how these
architectures work. Te purpose of this survey is to provide the reader with a deeper understanding of (1) how
deepfakes are created and detected, (2) the current trends and advancements in this domain, (3) the shortcomings
of the current defense solutions, and (4) the areas which require further research and atention.
CCS Concepts: •Security and privacy →Social engineering attacks; Human and societal aspects of security
and privacy; •Computing methodologies →Machine learning;
Additional Key Words and Phrases: Deepfake, Deep fake, reenactment, replacement, face swap, generative AI,
social engineering, impersonation
ACM Reference format:
Yisroel Mirsky and Wenke Lee. 2020. Te Creation and Detection of Deepfakes: A Survey. ACM Comput. Surv.
1, 1, Article 1 , 38 pages.
DOI: XX.XXXX/XXXXXXX.XXXXXXX
INTRODUCTION
A deepfake is content, generated by an artiﬁcial intelligence, that is authentic in the eyes of a human
being. Te word deepfake is a combination of the words ‘deep learning’ and ‘fake’ and primarily
relates to content generated by an artiﬁcial neural network, a branch of machine learning.
Te most common form of deepfakes involve the generation and manipulation of human imagery.
Tis technology has creative and productive applications. For example, realistic video dubbing of
foreign ﬁlms,1 education though the reanimation of historical ﬁgures , and virtually trying on
clothes while shopping.2 Tere are also numerous online communities devoted to creating deepfake
memes for entertainment,3 such as music videos portraying the face of actor Nicolas Cage.
However, despite the positive applications of deepfakes, the technology is infamous for its unethical and malicious aspects. At the end of 2017, a Reddit user by the name of ‘deepfakes’ was using
∗Corresponding Author
1htps://variety.com/2019/biz/news/
ai-dubbing-david-beckham-multilingual-1203309213/
2htps://www.forbes.com/sites/forbestechcouncil/2019/05/21/gans-and-deepfakes-could-revolutionize-the-fashion-industry/
3htps://www.reddit.com/r/SFWdeepfakes/
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and
the full citation on the ﬁrst page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permited. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior speciﬁc permission and/or a fee. Request permissions from .
© 2020 ACM. 0360-0300/2020/1-ART1 $15.00
DOI: XX.XXXX/XXXXXXX.XXXXXXX
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
 
Mirsky, et al.
III. Entertainment
IV. Trusted
II. Propaganda
Intention to mislead
Scams & Fraud:
Trickery via spoofing, falsifying audit
records, generating artwork, …
Tampering of Evidence:
Medical, forensic, court, …
Harming Credibility:
Revenge porn, political sabotage via
generated videos or articles, …
Altering Published Movies:
Comedy, satire, …
Editing & Special Effects:
Generating actors in movies, …
Art & Demonstration:
Animating dead characters, generated
portraits, technology demos, …
Authentic Content:
Credible Multimedia / Data
Misdirection
Generated discourse to amplify
events / facts, …
Political Warfare:
Tone change of articles, content
loosely based on facts, conspiracy…
Corruption:
Increased xenophobia, …
Fig. 1. A deepfake information trust chart.
Samples created by machines
Adversarial
…humans: entertainment, impersonation, art fraud.
…machines: hiding a stop sign, evading face recog.
…both: tampering medical scans, malware evasion.
Fig. 2. The diﬀerence between adversarial
machine learning and deepfakes.
deep learning to swap faces of celebrities into pornographic videos, and was posting them online4.
Te discovery caused a media frenzy and a large number of new deepfake videos began to emerge
thereafer. In 2018, BuzzFeed released a deepfake video of former president Barak Obama giving
a talk on the subject. Te video was made using the Reddit user’s sofware (FakeApp), and raised
concerns over identity thef, impersonation, and the spread of misinformation on social media. Fig.
presents an information trust chart for deepfakes, inspired by .
Following these events, the subject of deepfakes gained traction in the academic community, and
the technology has been rapidly advancing over the last few years. Since 2017, the number of papers
published on the subject rose from 3 to over 250 .
To understand where the threats are moving and how to mitigate them, we need a clear view of the
technology’s, challenges, limitations, capabilities, and trajectory. Unfortunately, to the best of our
knowledge, there are no other works which present the techniques, advancements, and challenges,
in a technical and encompassing way. Terefore, the goals of this paper are (1) to provide the reader
with an understanding of how modern deepfakes are created and detected, (2) to inform the reader of
the recent advances, trends, and challenges in deepfake research, (3) to serve as a guide to the design
of deepfake architectures, and (4) to identify the current status of the atacker-defender game, the
atacker’s next move, and future work that may help give the defender a leading edge.
We achieve these goals through an overview of human visual deepfakes (Section 2), followed by a
technical background which identiﬁes technology’s basic building blocks and challenges (Section 3).
We then provide a chronological and systematic review for each category of deepfake, and provide the
networks’ schematics to give the reader a deeper understanding of the various approaches (Sections
4 and 5). Finally, afer reviewing the countermeasures (Section 6), we discuss their weaknesses, note
the current limitations of deepfakes, suggest alternative research, consider the adversary’s next steps,
and raise awareness to the spread of deepfakes to other domains (Section 7).
Scope. In this survey we will focus on deepfakes pertaining to the human face and body. We will
not be discussing the synthesis of new faces or the editing of facial features because they do not
have a clear atack goal associated with them. In Section 7.3 we will discuss deepfakes with a much
4htps://www.vice.com/en us/article/gydydm/gal-gadot-fake-ai-porn
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
broader scope, note the future trends, and exemplify how deepfakes have spread to other domains
and media such as forensics, ﬁnance, and healthcare.
We note to the reader that deepfakes should not be confused with adversarial machine learning,
which is the subject of fooling machine learning algorithms with maliciously crafed inputs (Fig. 2).
Te diﬀerence being that for deepfakes, the objective of the generated content is to fool a human
and not a machine.
OVERVIEW & ATTACK MODELS
We deﬁne a deepfake as
“Believable media generated by a deep neural network”
In the context of human visuals, we identify four categories: reenactment, replacement, editing, and
synthesis. Fig. 3 illustrates some examples facial deepfakes in each of these categories and their
sub-types. Troughout this paper we denote s and t as the source and the target identities. We also
denote xs and xt as images of these identities and xд as the deepfake generated from s and t.
Reenactment
A reenactment deepfake is where xs is used to drive the expression, mouth, gaze, pose, or body of xt:
Expression reenactment is where xs drives the expression of xt. It is the most common form of
reenactment since these technologies ofen drive target’s mouth and pose as well, providing a
wide range of ﬂexibility. Benign uses are found in the movie and video game industry where
the performances of actors are tweaked in post, and in educational media where historical
ﬁgures are reenacted.
Mouth reenactment, also known as ‘dubbing’, is where the mouth of xt is driven by that of xs, or
an audio input as containing speech. Benign uses of the technology includes realistic voice
dubbing into another language and editing.
Gaze reenactment is where direction ofxt’s eyes, and the position of the eyelids, are driven by those
of xs. Tis is used to improve photographs or to automatically maintain eye contact during
video interviews .
Pose reenactment is where the head position of xt is driven by xs. Tis technology has primarily been used for face frontalization of individuals in security footage, and as a means for
improving facial recognition sofware .
Body reenactment, a.k.a. pose transfer and human pose synthesis, is similar to the facial reenactments listed above except that’s its the pose of xt’s body being driven.
Expression
Face Replacement
Facial Reenactment
○: Sometimes
Face Editing
Expression
Face Synthesis
Transfers:
Fig. 3. Examples of reenactment, replacement, editing, and synthesis deepfakes of the human face.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
Te Attack Model. Reenactment deep fakes give atackers the ability to impersonate an identity,
controlling what he or she says or does. Tis enables an atacker to perform acts of defamation, cause
discredability, spread misinformation, and tamper with evidence. For example, an atacker can impersonate t to gain trust the of a colleague, friend, or family member as a means to gain access to money,
network infrastructure, or some other asset. An atacker can also generate embarrassing content of t
forblackmailingpurposesorgeneratecontenttoaﬀectthepublic’sopinionofanindividualorpolitical
leader. Tetechnologycanalsobeusedtotampersurveillancefootageorsomeotherarchivalimagery
in an atempt to plant false evidence in a trial. Finally, the atack can either take place online (e.g.,
impersonating someone in a real-time conversation) or oﬄine (e.g., fake media spread on the Internet).
Replacement
A replacement deepfake is where the content of xt is replaced with that of xs, preserving the identity
Transfer is where the content of xt is replaced with that of xs. A common type of transfer is facial
transfer, used in the fashion industry to visualize an individual in diﬀerent outﬁts.
Swap is where the content transferred to xt from xs is driven by xt. Te most popular type of swap
replacement is ‘face swap’, ofen used to generate memes or satirical content by swapping the
identity of an actor with that of a famous individual. Another benign use for face swapping includes the anonymization of one’s identity in public content in-place of blurring or pixelation.
Te Attack Model. Replacement deepfakes are well-known for their harmful applications. For
example, revenge porn is where an atacker swaps a victim’s face onto the body of a porn actress
to humiliate, defame, and blackmail the victim. Face replacement can also be used as a short-cut to
fully reenacting t by transferring t’s face onto the body of a look-alike. Tis approach has been used
as a tool for disseminating political opinions in the past .
Editing & Synthesis
An enchantment deepfake is where the atributes ofxt are added, altered, or removed. Some examples
include the changing a target’s clothes, facial hair, age, weight, beauty, and ethnicity. Apps such as
FaceApp enable users to alter their appearance for entertainment and easy editing of multimedia.
Te same process can be used by and atacker to build a false persona for misleading others. For
example, a sick leader can be made to look healthy , and child or sex predators can change their
age and gender to build dynamic proﬁles online. A known unethical use of editing deepfakes is the
removal of a victim’s clothes for humiliation or entertainment .
Synthesis is where the deepfake xд is created with no target as a basis. Human face and body
synthesis techniques such as (used in Fig. 3) can create royalty free stock footage or generate
characters for movies and games. However, similar to editing deepfakes, it can also be used to create
fake personas online.
Although human image editing and synthesis are active research topics, reenactment and replacement deepfakes are the greatest concern because they give an atacker control over one’s identity . Terefore, in this survey we will be focusing on reenactment and replacement deepfakes.
TECHNICAL BACKGROUND
Although there are a wide variety of neural networks, most deepfakes are created using variations
or combinations of generative networks and encoder decoder networks. In this section we provide a
brief introduction to these networks, how they are trained, and the notations which we will be using
throughout the paper.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
Neural Networks
Neural networks are non-linear models for predicting or generating content based on an input. Tey
are made up of layers of neurons, where each layer is connected sequentially via synapses. Te
synapses have associated weights which collectively deﬁne the concepts learned by the model. To
execute a network on ann-dimensional inputx, a process known as forward-propagation is performed
where x propagated through each layer and an activation function is used to summarize a neuron’s
output (e.g., the Sigmoid or ReLU function).
Concretely,letl(i) denotethei-thlayerinthenetworkM,andlet ∥l(i)∥denotethenumberofneurons
in l(i). Finally, let the total number of layers in M be denoted as L. Te weights which connect l(i) to
l(i+1) are denoted as the ∥l(i)∥-by-∥l(i+1)∥matrixW (i) and ∥l(i+1)∥dimensional bias vector ®b(i). Finally,
we denote the collection of all parameters θ as the tuple θ ≡(W ,b), whereW andb are the weights of
each layer respectively. Let a(i+1) denote the output (activation) of layer l(i) obtained by computing
W (i)· ®a(i)+ ®
where f is ofen the Sigmoid or ReLU function. To execute a network on an ndimensional inputx, a process known as forward-propagation is performed wherex is used to activate
l(1) which activates l(2) and so on until the activation of l(L) produces them-dimensional outputy.
To summarize this process, we consider M a black box and denote its execution as M(x)=y. To
train M in a supervised seting, a dataset of paired samples with the form (xi,yi) is obtained and an
objective loss function L is deﬁned. Te loss function is used to generate a signal at the output of
M which is back-propagated through M to ﬁnd the errors of each weight. An optimization algorithm,
such as gradient descent (GD), is then used to update the weights for a number of epochs. Te
function L is ofen a measure of error between the input x and predicted outputy′. As a result the
network learns the function M(xi)≈yi and can be used to make predictions on unseen data.
Some deepfake networks use a technique called one-shot or few-shot learning which enables
a pre-trained network to adapt to a new dataset X ′ similar to X on which it was trained. Two
common approaches for this are to (1) pass information on x ′ ∈X ′ to the inner layers of M during the
feed-forward process, and (2) perform a few additional training iterations on a few samples from X ′.
Loss Functions
In order to update the weights with an optimization algorithm, such as GD, the loss function must
be diﬀerentiable. Tere are various types of loss functions which can be applied in diﬀerent ways
depending on the learning objective. For example, when training a M as an n-class classiﬁer, the
output of M would be the probability vector y ∈Rn. To train M, we perform forward-propagation
to obtainy
′ =M(x), compute the cross-entropy loss (LCE) by comparingy′ to the ground truth label
y, and then perform back-propagation and to update the weights with the training signal. Te loss
LCE over the entire training set X is calculated as
yi[c]log(y′
wherey′[c] is the predicted probability of xi belonging to the c-th class.
Other popular loss functions used in deepfake networks include the L1 and L2 norms L1 = |x−xд|1
and L2 = |x−xд|2. However, L1 and L2 require paired images (e.g., of s and t with same expression)
and perform poorly when there are large oﬀsets between the images such as diﬀerent poses or facial
features. Tis ofen occurs in reenactment when xt has a diﬀerent pose than xs which is reﬂected
in xд, and ultimately we’d like xд to match the appearance of xt.
One approach to compare two unaligned images is to pass them through another network (a
perceptual model) and measure the diﬀerence between the layer’s activations (feature maps). Tis
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
Encoder Decoder
Vanilla GAN
Generative
Discriminator
Fig. 4. Five basic neural network architectures used to create deepfakes. The lines indicate dataflows used
during deployment (black) and training (grey).
loss is called the perceptual loss (Lperc) and is described in for image generation tasks. In the
creation of deepfakes, Lperc is ofen computed using a face recognition network such as VGGFace.
Te intuition behind Lperc is that the feature maps (inner layer activations) of the perceptual model
act as a normalized representation of x in the context of how the model was trained. Terefore,
by measuring the distance between the feature maps of two diﬀerent images, we are essentially
measuring their semantic diﬀerence (e.g., how similar the noses are to each other and other ﬁner
details.) Similar to Lperc, there is a feature matching loss (LF M) which uses the last output
of a network. Te idea behind LF M is to consider the high level semantics captured by the last layer
of the perceptual model (e.g., the general shape and textures of the head).
Another common loss is a type of content loss (LC) which is used to help the generator create
realistic features, based on the perspective of a perceptual model. In LC, only xд is passed through
the perceptual model and the diﬀerence between the network’s feature maps are measured.
Generative Neural Networks (for deepfakes)
Deep fakes are ofen created using combinations or variations of six diﬀerent networks, ﬁve of which
are illustrated in Fig. 4.
Encoder-Decoder Networks (ED). An ED consists of at least two networks, an encoder En and
decoder De. Te ED has narrower layers towards its center so that when it’s trained as
De(En(x))=xд, the network is forced to summarize the observed concepts. Te summary
of x, given its distribution X, is En(x) = e, ofen referred to as an encoding or embedding
and E =En(X) is referred to as the ‘latent space’. Deepfake technologies ofen use multiple
encoders or decoders and manipulate the encodings to inﬂuence the output xд. If an encoder
and decoder are symmetrical, and the network is trained with the objective De(En(x))=x,
then the network is called an autoencoder and the output is the reconstruction of x denoted
ˆx. Another special kind of ED is the variational autorencoder (VAE) where the encoder learns
the posterior distribution of the decoder given X. VAEs are beter at generating content than
autoencoders because the concepts in the latent space are disentangled, and thus encodings
respond beter to interpolation and modiﬁcation.
Convolutional Neural Network (CNN). In contrast to a fully connected (dense) network, a CNN
learnspaternhierarchiesinthedataandisthereforemuchmoreeﬃcientathandlingimagery.
A convolutional layer in a CNN learns ﬁlters which are shifed over the input forming an
abstract feature map as the output. Pooling layers are used to reduce the dimensionality as
the network gets deeper and up-sampling layers are used to increase it. With convolutional,
pooling, and upsampling layers, it is possible to build an ED CNNs for imagery.
Generative Adversarial Networks (GAN) Te GAN was ﬁrst proposed in 2014 by Goodfellow
et al. in . A GANs consist of two neural networks which work against each other: the
generator G and the discriminator D. G creates fake samples xд with the aim of fooling D,
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
and D learns to diﬀerentiate between real samples (x ∈X) and fake samples (xд =G(z) where
z ∼N). Concretely, there is an adversarial loss used to train D andG respectively:
Ladv(D)=maxlogD(x)+log(1−D(G(z)))
Ladv(G)=minlog(1−D(G(z)))
Tis zero-sum game leads toG learning how to generate samples that are indistinguishable
from the original distribution. Afer training, D is discarded andG is used to generate content.
When applied to imagery, this approach produces photo realistic images.
Numerous of variations and improvements of GANs have been proposed over the years.
In the creation of deepfakes, there are two popular image translation frameworks which use
the fundamental principles of GANs:
Image-to-Image Translation (pix2pix). Te pix2pix framework enables paired translations from one image domain to another . In pix2pix, G tries to generate the image
xд given a visual context xc as an input, and D discriminates between (x,xc) and (xд,xc).
Moreover,G is a an ED CNN with skip connections from En to De (called a U-Net) which
enables G to produce high ﬁdelity imagery by bypassing the compression layers when
needed. Later, pix2pixHD was proposed for generating high resolution imagery
with beter ﬁdelity.
CycleGAN. An improvement of pix2pix which enables image translation through unpaired
training . Te network forms a cycle consisting of two GANs used to convert
images from one domain to another, and then back again to ensure consistency with
a cycle consistency loss (Lcyc).
Recurrent Neural Networks (RNN) An RNN is type of neural network that can handle sequential
and variable length data. Te network remembers is internal state afer processing x(i−1) and
can use it to processx(i) and so on. In deepfake creation, RNNs are ofen used to handle audio
and sometimes video. More advanced versions of RNNs include long short-term memory
(LSTM) and gate reccurent units (GRU).
Feature Representations
Most deep fake architectures use some form of intermediate representation to capture and sometimes
manipulate s and t’s facial structure, pose, and expression. One way is to use the facial action coding
system (FACS) and measure each of the face’s taxonomized action units (AU) . Another way is
to use monocular reconstruction to obtain a 3D morphable model (3DMM) of the head from a 2D
image, where the pose and expression are parameterized by a set of vectors and matrices. Ten use
the parameters or a 3D rendering of the head itself. Some use a UV map of the head or body to give
the network a beter understanding of the shape’s orientation.
Another approach is to use image segmentation to help the network separate the diﬀerent concepts
(face, hair, etc). Te most common representation is landmarks (a.k.a. key-points) which are a set
of deﬁned positions on the face or body which can be eﬃciently tracked using open source CV
libraries. Te landmarks are ofen presented to the networks as a 2D image with Gaussian points
at each landmark. Some works separate the landmarks by channel to make it easier for the network
to identity and associate them. Similarly, facial boundaries and body skeletons can also used.
For audio (speech), the most common approach is to split the audio into segments, and for each segment, measure the Mel-Cepstral Coeﬃcients (MCC) which captures the dominant voice frequencies.
Deepfake Creation Basics
To generatexд, reenactment and face swap networks follow some variation of this process (illustrated
in Fig. 5): Pass x through a pipeline that (1) detects and crops the face, (2) extracts intermediate
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
𝑥𝑠and/or 𝑥𝑡
Detect & Crop
Intermediate Representation
Preprocessing
Generation
Postprocessing
Driver and/or
Landmarks/
key points
Boundaries/
Parameters
Fig. 5. The processing pipeline for making reenactment and face swap deepfakes. Usually only a subset of
these steps are performed.
representations, (3) generates a new face based on some driving signal (e.g., another face), and then
(4) blends the generated face back into the target frame.
In general there are six approaches to driving an image:
(1) Let a network work directly on the image and perform the mapping itself.
(2) Train an ED network to disentangle the identity from the expression, and then modify/swap
the encodings of the target the before passing it through the decoder.
(3) Add an additional encoding (e.g., AU or embedding) before passing it to the decoder.
(4) Convert the intermediate face/body representation to the desired identity/expression before
generation (e.g., transform the boundaries with a secondary network or render a 3D model
of the target with the desired expression).
(5) Use the optical ﬂow ﬁeld from subsequent frames in a source video to drive the generator.
(6) Create composite of the original content (hair, scene, etc) with a combination of the 3D
rendering, warped image, or generated content, and pass the composite through another
network (such as pix2pix) to reﬁne the realism.
Generalization
A deepfake network may be trained or designed to work with only a speciﬁc set of target and source
identities. An identity agnostic model is sometimes hard to achieve due to correlations learned by
the model between s and t during training.
Let E be some model or process for representing or extracting features from x, and let M be a
trained model for performing replacement or reenactment. We identify three primary categories
in regard to generalization:
one-to-one: A model that uses a speciﬁc identity to drive a speciﬁc identity: xд =Mt(Es(xs))
many-to-one: A model that uses any identity to drive a speciﬁc identity: xд =Mt(E(xs))
many-to-many: A model that uses any identity to drive any identity: xд =M(E1(xs),E2(xt))
Challenges
Te following are some challenges in creating realistic deepfakes:
Generalization. Generativenetworksaredatadriven, andthereforereﬂectthetrainingdataintheir
outputs. Tis means that high quality images of a speciﬁc identity requires a large number
of samples of that identity. Moreover, access to a large dataset of the driver is typically much
easier to obtain than the victim. As a result, over the last few years, researchers have worked
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
hard to minimize the amount of training data required, and to enable the execution of a
trained model on new target and source identities (unseen during training).
Paired Training. One way to train a neural network is to present the desired output to the model for
each given input. Tis process of data pairing is a laborious an sometimes impractical when
training on multiple identities and actions. To avoid this issue, many deepfake networks
either (1) train in a self-supervised manner by using frames selected from the same video oft,
(2) use unpaired networks such as Cycle-GAN, or (3) utilize the encodings of an ED network.
Identity Leakage. Sometimes the identity of the driver (e.g., s in reenactment) is partially transferred to xд. Tis occurs when training on a single input identity, or when the network is
trained on many identities but data pairing is done with the same identity. Some solutions
proposed by researchers include atention mechanisms, few-shot learning, disentanglement,
boundary conversions, and AdaIN or skip connections to carry the relevant information to
the generator.
Occlusions. Occlusions are where part of xs or xt is obstructed with a hand, hair, glasses, or any
other item. Another type of obstruction is the eyes and mouth region that may be hidden or
dynamically changing. As a result, artifacts appear such as cropped imagery or inconsistent
facial features. To mitigate this, works such as perform segmentation and
in-painting on the obstructed areas.
Temporal Coherence. Deepfake videos ofen produce more obvious artifacts such as ﬂickering
and jiter . Tis is because most deepfake networks process each frame individually
with no context of the preceding frames. To mitigate this, some researchers either provide
this context to G and D, implement temporal coherence losses, use RNNs, or perform a
combination thereof.
REENACTMENT
In this section we present a chronological review of deep learning based reenactment, organized
according to their class of identity generalization. Table 1 provides a summary and systematization
of all the works mentioned in this section. Later, in Section 7, we contrast the various methods and
identify the most signiﬁcant approaches.
Expression Reenactment
Expression reenactment turns an identity into a puppet, giving atackers the most ﬂexibility to
achieve their desired impact. Before we review the subject, we note that expression reenactment
has been around long before deepfakes were popularized. In 2003, researchers morphed models of
3D scanned heads . In 2005, it was shown how this can be done without a 3D model , and
through warping with matching similar textures . Later, between 2015 and 2018, Ties et al.
demonstrated how 3D parametric models can be used to achieve high quality and real-time results
with depth sensing and ordinary cameras ( and ).
Regardless, today deep learning approaches are recognized as the simplest way to generate believable content. To help the reader understand the networks and follow the text, we provide the
model’s network schematics and loss functions in ﬁgures 6-8.
One-to-One (Identity to Identity). In 2017, the authors of proposed using a CycleGAN
for facial reenactment, without the need for data pairing. Te two domains where video frames of
s and t. However, to avoid artifacts in xд, the authors note that both domains must share a similar
distributions (e.g., poses and expressions).
In 2018, Bansal et al. proposed a generic translation network based on CycleGAN called Recycle-
GAN . Teir framework improves temporal coherence and mitigates artifacts by including
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
Table 1. Summary of Deep Learning Reenactment Models (Body and Face)
Reenactment
Retraining for new…
Representation
Model Training
Model Execution
Model Output
Source (s)
Target (t)
Identity Agnostic
Discriminators
Other Netw.
3DMM/Rendering
UV Mapping
Segmentation
Landmark / Keypoint
Boundary / Skeleton
Labeling of: ID
Labeling of: Action
No Pairing
Paring within Same Video
Paring ID to Same ID
Paring ID to Diﬀr. Actions
Paring Action to Diﬀr. IDs
Requires Video
Source (xs …)
Target (xt …)
Image/Frame
Resolution
 2017
>20 min. video >20 min. video
Recycle-GAN
5-10 min. video 5-10 min. video
DeepFaceLab
1-3 hr. video
1-3 hr. video
portrait video
One-to-One
 2019
Liu et al. 2019
1-3 hr. video
1-3 hr. video
upperbody video
 2017
Syth. Obama
17 hr. video
portiat video
• 2048x1024
17 hr. video
 2018 Deep Video Portr. • • • •
1-3 min. video
portrait video
neural texture
 2018
ReenactGAN
30 min. video
 2018
• • • ◦or •
3-8 min. video
portrait video
 2018
1 min. video
expression label
identity label
2 hr. video
 2019
3-10 images
3-10 eye images
1 hr. video
portiat video
 2019
N.V. Puppetry
2-3 min. video
portiat video
 2019
8 min. video
body image
background
 2019 Deep Video P.C.
2 min. video
body image
 2019 Everybody D. N.
20 min. video
body image
 2019 D. D. Generation
3 min. video
body video
 2019 N. Talking Heads • • •
1-3 portraits
• portrait/landmarks
1-3 portraits
Many-to-One
 2019 Few-shot Vid2Vid • • • ◦or •
1-10 portraits
• portrait/body video 1-10 portr./bodies •
 2015
Shimba et al.
• 0 0 0 1 •
face database
latent variables
 2017
 2017
• 1 1 2 0 •
 2018
portrait - neutral
 2018
1-3 portraits
 2018
GANnotation
• portrait/landmarks
 2018
• 1 1 1 2 •
portrait/AUs
 2018
FaceID-GAN
 2018
FaceFeat-GAN
latent variables
 2018
1+ portraits
 2018 Deformable GAN
body image/landm.
body image
body image
body image/pose
body image
body image
 2018
Dense Pose Tr.
• 25 25 1 2
body image
body image
 2018
Song et al.
 2019
• portrait/landmarks
 2019
GANimation
• 2 2 1 1 •
portrait/AUs
 2019
• 2 2 1 2 •
portrait/AUs
 2019
FaceSwapNet
portrait/landmarks portrait/landmarks •
 2019
Monkey-Net
portrait/body
portrait/body
 2019 First-Order-Model • • •
portrait/body
portrait/body
 2019
expression label
portrait/boundaries
Fu et al. 2019
portrait/label
 2019
portriat/landmarks
 2019 Speech D. Anm. 1 • ◦
 2019 Speech D. Anm. 2 • ◦
 2019 Speech D. Anm. 3 • ◦
 2019
• audio/portrait video
portiat video
Speech2Vid
portiat video
 2019
body video
body image
body image
body image
body image
body/pose image
 2019
body image
body/pose image
 2020
ImaGINator
expression label
MarioNETte
1-8 portraits
Many-to-Many
16 portraits
next-frame predictor networks for each domain. For facial reenactment, the authors train their
network to translate the facial landmarks of xs into portraits of xt.
Many-to-One (Multiple Identities to a Single Identity). In 2017, the authors of proposed
CVAE-GAN, a conditional VAE-GAN where the generator is conditioned on an atribute vector
or class label. However, reenactment with CVAE-GAN requires manual atribute morphing by
interpolating the latent variables (e.g., between target poses).
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
Later, in 2018, a large number of source-identity agnostic models were published, each proposing
a diﬀerent method to decoupling s from t:5
Facial Boundary Conversion. One approach was to ﬁrst convert the structure of source’s facial
boundaries tothatofthe target’sbeforepassing themthroughthe generator . Intheir framework
‘ReenactGAN’, the authors use a CycleGAN to transform the boundarybs to the target’s face shape
asbt before generating xд with a pix2pix-like generator.
Temporal GANs. To improve the temporal coherence of deepfake videos, the authors of 
proposed MoCoGAN: a temporal GAN which generates videos while disentangling the motion and
content (objects) in the process. Each frame is generated using a target expression label zc, and a
motion embedding z(i)
M for the i-th frame, obtained from a noise seeded RNN. MoCoGAN uses two
discriminators, one for realism (per frame) and one for temporal coherence (on the lastT frames).
In , the authors proposed a framework called Vid2Vid, which is similar to pix2pix but for
videos. Vid2Vid considers the temporal aspect by generating each frame based on the last L source
and generated frames. Te model also considers optical ﬂow to perform next-frame occlusion prediction (due to moving objects). Similar to pix2pixHD, a progressive training strategy is to generate
high resolution imagery. In their evaluations, the authors demonstrate facial reenactment using
the source’s facial boundaries. In comparison to MoCoGAN, Vid2Vid is more practical since it the
deepfake is driven by xs (e.g., an actor) instead of crafed labels.
Teauthorsof tooktemporaldeepfakesonestepfurtherachievingcompletefacialreenactment
(gaze, blinking, pose, mouth, etc.) with only one minute of training video. Teir approach was to
extract the source and target’s 3D facial models from 2D images using monocular reconstruction,
and then for each frame, (1) transfer the facial pose and expression of the source’s 3D model to the
target’s, and (2) produce xд with a modiﬁed pix2pix framework, using the last 11 frames of rendered
heads, UV maps, and gaze masks as the input.
Many-to-Many (Multiple IDs to Multiple IDs).
Label Driven Reenactment. Te ﬁrst atempts at identity agnostic models were made in 2017,
where the authors of used a conditional GAN (CGAN) for the task. Teir approach was to (1)
extract the inner-face regions as (xt,xs), and then (2) pass them to an ED to produce xд subjected
to L1 and Ladv losses. Te challenge of using a CGAN was that the training data had to be paired
(images of diﬀerent identities with the same expression).
Going one step further, in the authors reenacted full portraits at low resolutions. Teir
approach was to decoupling the identities was to use a conditional adversarial autoencoder to disentangle the identity from the expression in the latent space. However, their approach is limited to
drivingxt with discreet AU expression labels (ﬁxed expressions) that capturexs. A similar label based
reenactment was presented in the evaluation of StarGAN ; an architecture similar to CycleGAN
but for N domains (poses, expressions, etc).
Later, in 2018, the authors of proposed GATH which can drive xt using continuous action
units (AU) as an input, extracted from xs. Using continuous AUs enables smoother reenactments
over previous approaches . Teir generator is ED network trained on the loss signals
from using three other networks: (1) a discriminator, (2) an identity classiﬁer, and (3) a pretrained
AU estimator. Te classiﬁer shares the same hidden weights as the discriminator to disentangle the
identity from the expressions.
5Although works such as and achieved fully agnostic models (many-to-many) in 2017, their works were on low
resolution or partial faces.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
𝑥𝑠, 𝑥𝑡, 𝑥𝑔: The source, target, and generated images (e.g., portraits)
𝑦: A label (e.g., fake vs real, one-hot encoding, …)
𝑥′: Another sample from the same distribution, ො𝑥: reconstructed
𝑚: Binary mask, 𝑠: Segmentation map, 𝑙: Landmark or Keypoint, 𝑧: Noise
: Concatenate, : Subtract, :Multiply : Add : Paste content
: Crop out region 𝑎from image where 𝑎∈{f:face, e:eye, m:mouth}
: Create mask using region 𝑎of the image where 𝑎∈{f:face, e:eye, m:mouth}
𝑥𝑎: Image 𝑥cropped to the region of 𝑎∈{𝑓:face, 𝑒:eye, 𝑚:mouth}
: Spatial replication of a vector (channel-wise or dim-wise)
: Scale image down by factor of X
𝐿𝐸, 𝐵𝐸, 𝐴𝐸, 3𝐷𝐸: Landmark, Boundary, Action Unit (AU),
and 3DMM facial model Extractors (open source CV library)
𝐿𝑇, 3𝐷𝑇: Landmark and 3D model transformers, from 𝑠to 𝑡
𝑀𝐸: MFCC audio feature extractor
Losses: L1 : L1, L2 : L2, LCE : Cross Entropy, Ladv : Adversarial, LF M :
Feature Matching, Lperc : Perceptual, Lcyc : Cycle Consistency, Latt : Attention, Ltrip : Triplet, Ltv : Total Variance, LKL : KL Divergence
 Reenact GAN:
Generic Boundary Encoder
𝑏: facial boundaries, 𝑏𝑥𝑔: a boundary translated to domain 𝑥
Target Specific
Source Generic
Target Specific
ℒ1: 𝑥𝑡, 𝑥𝑔
ℒ𝑐𝑜𝑛𝑡𝑒𝑛𝑡: 𝐼ℓ2 𝑥𝑔, 𝐼ℓ3 𝑥𝑔
ℒ𝑐𝑦𝑐𝑙𝑒: ෠𝑏𝑡, 𝑏𝑡
ℒ𝑐𝑦𝑐𝑙𝑒: ෠𝑏𝑠, 𝑏𝑠
ℒ𝑎𝑑𝑣(∙): 𝑦∙,
ℒ𝑎𝑑𝑣(𝑡): 𝑦𝑡,
 MocoGAN:
𝑦𝑠: source expression label, 𝑒𝑡: one-hot encoding of target identity,
𝑒𝑥: temporal expression embedding, 𝐺𝑅𝑈: Gated Recurrent Unit of an RNN
Image Disc.
Video Disc.
ℒ𝑎𝑑𝑣(𝐼): 𝑦𝐼,
ℒ𝑎𝑑𝑣(𝑉): 𝑦𝑉,
 Vid2Vid:
(𝑖−𝐿), … 𝑙𝑠
(𝑖−𝐿), … 𝑥𝑔
(𝑖−𝐿), … 𝑥𝑠
Intermediate Synth.
Occlusion Masking
Warp Field Pred.
Image Disc.
Video Disc.
𝑖−𝐾−1 :(𝑖)
𝑇: frames in the video clip, 𝐿, 𝐾: system parameters
ℒ𝐹𝑀(𝐼): 𝑥𝑡
ℒ𝐹𝑀(𝐷𝐼): 𝑥𝑡
ℒ𝑎𝑑𝑣𝐷𝐼: 𝑦𝐼, 𝑙𝑡
ℒ𝑎𝑑𝑣𝑉𝐷𝑉: 𝑦𝑉, 𝑤𝑡
𝑖−𝐾−1 : 𝑖,
 Deep Video Portrait:
𝑚𝑒𝑦𝑒: mask of eye region (gaze), 𝑥𝑈𝑉: UV correspondence map, 𝑥𝑐𝑔𝑖: 3D rendered
image of 𝑥
(𝑖−𝑁), … 𝑥𝑐𝑔𝑖
(𝑖−𝑁), … 𝑥𝑈𝑉
(𝑖−𝑁), … 𝑚𝑒𝑦𝑒
(𝑖−𝑁), … 𝑥𝑡
ℒ𝑎𝑑𝑣: 𝑦, 𝑥𝑡
(𝑖−𝑁), … 𝑥𝑡
 GATH:
shared weights
ℒ𝐴𝑈𝐴: ℒ2 𝑎𝑔′ , 𝑎𝑠′
ℒ1: 𝑥𝑡, 𝑥𝑔
 GANimation:
𝑚𝑎: attention mask, 𝑚𝑐: color mask
ℒ1: 𝐺𝐺𝑥𝑡, 𝑎𝑠, 𝑎𝑡, 𝑥𝑡
ℒ𝑎𝑡𝑡𝑛: 𝑚𝑎-see paper
ℒ𝑒𝑥𝑝𝑟𝑒𝑠𝑠𝑖𝑜𝑛: 𝑎𝑡,
′ -see paper ℒ𝑎𝑑𝑣: 𝑦,
 GANotation:
𝑚𝑎: attention mask, 𝑚𝑐: color map, training: 𝑠and 𝑡have same ID
ℒ𝑡𝑣: 𝑥𝑠, 𝑥𝑔
ℒ2: 𝑥𝑠, 𝑥𝑔(same ID) ℒ𝑐𝑜𝑛𝑠𝑖𝑡𝑒𝑛𝑐𝑦: 𝐻𝑥𝑔, 𝑙𝑡, 𝑥𝑡
ℒ𝑎𝑑𝑣: 𝑦, 𝑥𝑔, 𝑥𝑡
ℒ3×𝑐𝑜𝑛𝑠𝑖𝑡𝑒𝑛𝑐𝑦: 𝐻𝑥𝑔, 𝑙𝑠′ , 𝐻𝑥𝑡, 𝑙𝑠′
ℒ𝑝𝑒𝑟𝑐: 𝐼ℓ𝑘,ℓ𝑘−1 𝑥𝑔, 𝐼ℓ𝑘,ℓ𝑘−1 𝑥𝑡
ℒ𝑝𝑒𝑟𝑐: 𝑃ℓ𝑘,ℓ𝑘−1 𝑥𝑔, 𝑃ℓ𝑘,ℓ𝑘−1 𝑥𝑡
 FaceID-GAN:
3DMM predictor
Identity encoder
ℒ𝑎𝑑𝑣: ℒ1 𝑥𝑡, ො𝑥𝑡−𝛼ℒ1(𝑥𝑔, ො𝑥𝑔)
ℒ𝐶𝐸𝐼: 𝑒𝑔, 𝑦𝑖𝑑
ℒ𝐶𝐸𝐼: 𝑒𝑡, 𝑦𝑖𝑑
ℒ1 𝐷𝑒: 𝑥𝑔, ො𝑥𝑔
ℒ2 𝐷𝑒: 𝑝𝑡𝑠, 𝑝𝑔
ℒ𝑐𝑜𝑠𝐷𝑒: 𝑒𝑔, 𝑒𝑡
ℒ𝑝𝑜𝑠𝑒𝑃: 𝑝𝑡, 3𝐷𝑀𝑀𝑥𝑡
 FaceFeat-GAN:
3DMM pred.
ID predictor
𝑧: Sample of random noise later mapped to 𝑥𝑠
ℒ𝑎𝑑𝑣(𝐷): 𝑦,
ℒ𝑎𝑑𝑣𝐷𝑝: 𝑦𝑝, Τ
ℒ𝑎𝑑𝑣𝐷𝑞: 𝑦𝑞, Τ
ℒ𝐶𝐸𝐼: 𝑦𝑡, 𝑦𝑡
ℒ𝑝𝑜𝑠𝑒𝑃: 𝑝, 3𝐷𝑀𝑀(𝑥𝑡)
ℒ1 𝑄: 𝑥𝑡, 𝐺𝑦𝑡, 𝑝, 𝑄𝑥𝑡
ℒ1 𝐺: 𝑥𝑡, ො𝑥𝑡
ℒ2 𝐺: 𝐼ො𝑥𝑡, 𝑦𝑡& 𝐼𝑥𝑔, 𝑦𝑡
Fig. 6. Architectural schematics of reenactment networks. Black lines indicate prediction flows used during
deployment, dashed gray lines indicate dataflows performed during training. Zoom in for more detail.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
 paGAN:
𝑥𝑈𝑉: UV correspondence map, 𝑥𝑐𝑔𝑖: 3D rendered image of 𝑥, 𝑥𝑑𝑒𝑝𝑡ℎ: image of depth
map of model 𝑥
ℒ𝑝𝑒𝑟𝑐(𝐼): 𝑥𝑡
 X2Face:
𝑣: vector map of pixel deltas (changes), 𝑥𝜂: a face with a neutral expression/pose,
𝑎: some other modality (e.g., audio)
Encoding Network
interpolate
Driving Network
interpolate
ℒ1: 𝑥𝑠, 𝑥𝑔
ℒ𝑝𝑒𝑟𝑐(𝐼): 𝑥𝑡, 𝑥𝑔
 FaceSwapNet:
Landmark Converter
Landmark Guided
ℒ1: 𝑥𝑔, 𝑥𝑡
′ (with same expr.) ℒ2: 𝑙𝑔, 𝑙𝑡
′ (with same expr.)
ℒ𝑎𝑑𝑣: 𝑦, 𝑥𝑔, 𝑥𝑡
ℒ𝑡𝑟𝑖𝑝𝑙𝑒𝑡: ℒ𝑝𝑒𝑟𝑐(𝑥𝑡1, 𝑥𝑡2), ℒ𝑝𝑒𝑟𝑐(𝑥𝑡1, 𝑥𝑠)
Same ID, different expression
Different IDs
 FSGAN:
𝑚: segmentation mask (face, hair, other), 𝑙: 3D facial landmarks, 𝐻𝑛: 𝑛passes through
𝐻while interpolating 𝑙𝑠to 𝑙𝑡
Reenactment
ℒ1: 𝑥𝑔, 𝑥𝑡
ℒ1: 𝐻𝑛(𝑥𝑔, 𝑙𝑠), 𝑥𝑡
ℒ𝑝𝑒𝑟𝑐(𝐻): 𝐼ℓ∗(𝑥𝑡), 𝐼ℓ∗(𝑥𝑡)
ℒ𝑝𝑒𝑟𝑐(𝐻): 𝐼ℓ∗(𝐻𝑛(𝑥𝑔, 𝑙𝑠)), 𝐼ℓ∗(𝑥𝑡)
ℒ𝑎𝑑𝑣: 𝑦, 𝑥𝑔, 𝑥𝑡
 Fu et al. 2019:
Boundary encoder
𝐷𝑒𝑏1 𝑏𝑔𝐸𝑛𝑏2
Texture encoder
Pose predictor
Expr. predictor
Boundary encoder
ℒ𝑟𝑒𝑔𝐷𝑒𝑏: 𝐷𝑒𝑏𝑝𝑠′, 𝑒𝑠′, 𝑣𝑡, 𝑃𝑝𝑠′ , 𝐸(𝑒𝑠′)
ℒ1 𝐷𝑒𝑏: 𝑏𝑡, 𝑏𝑔
ℒ𝑡ℎ𝑟: 𝐼1 𝑥𝑡, 𝐸𝑛𝑡(𝑥𝑡)
ℒ𝑎𝑑𝑣1,2,3:
ℒ𝐹𝑀𝐷𝑒𝑔: 𝐼2,ℓ∙𝑥𝑠, 𝐼2,ℓ∙𝑥𝑔
 ICFace:
To neutral Gen.
To expression Gen.
𝑎: Action Units (AU), 𝜂: neutral expression
ℒ1: 𝑥𝑡, 𝑥𝑔
ℒ𝑎𝑑𝑣(𝑛): 𝑦,
ℒ𝑎𝑑𝑣(𝑒): 𝑦,
ℒ2: 𝐴𝑥𝑡, 𝑎𝑠
ℒ2: 𝐴𝑥𝑔, 𝑎𝑠
ℒ𝑝𝑒𝑟𝑐(𝐼): 𝑥𝑡, 𝑥𝑡
𝜂ℒ𝑝𝑒𝑟𝑐(𝐼): 𝑥𝑔, 𝑥𝑡
 AF-VAE:
𝐴𝑀𝐸: Additive Memory Encoder – models 𝑒𝑎as a Gaussian mixture of clustered
facial boundaries.
Appearance encoder
Identity encoder
ℒ𝐾𝐿: 𝑥𝑔, 𝑒𝑎, 𝑒𝑥, 𝐴𝑀𝐸
ℒ𝐹𝑀𝐼: 𝑥𝑔, 𝑥𝑡
 wg-GAN:
𝑣𝑠𝑡: vector map of the warp from 𝑥𝑡to 𝑥𝑠, 𝑤𝑠𝑡: 𝑥𝑡warped according to 𝑣𝑠𝑡
Training: for each 𝑥𝑡
(𝑖), 𝑥𝑠= 𝑥𝑡
(𝑖−10) taken from the same video clip
Refinement
Inpainting
𝐿𝑎𝑑𝑣2: 𝑦2,
𝐿𝑎𝑑𝑣3: 𝑦3,
 Motion&Texture-GAN:
𝑥𝜂: cropped neutral expression face, 𝑦𝑠: face expression label of source, 𝑠: an SRVF
point on a spherical manifold, 𝐿𝑅: landmark reconstruction from 𝑠, 𝑙: facial landmarks
Texture-GAN
Motion-GAN
ℒ𝑎𝑑𝑣(𝑇): 𝑦𝑇,
ℒ𝑝𝑒𝑟𝑐(𝐼): 𝑥𝑡
ℒ1: 𝑠𝑠, 𝑠𝑡
ℒ𝑎𝑑𝑣(𝑀): 𝑦𝑆, 𝑦𝑚, ൗ
 ImaGINator:
𝑙: One-hot label encoding of expression, 𝑧: Random value 𝑧~𝑁(0,1)
′(1), … 𝑥𝑡
Image Disc.
Video Disc.
′(1), … 𝑥𝑡
ℒ𝑎𝑑𝑣(𝑉): 𝑦𝑣, 𝑙𝑠, 𝑥𝑡
′(1), … , 𝑥𝑔
ℒ𝑎𝑑𝑣(𝐼): 𝑦𝐼, 𝑥𝑡
Fig. 7. Architectural schematics of reenactment networks. Black lines indicate prediction flows used during
deployment, dashed gray lines indicate dataflows performed during training. Zoom in for more detail.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
 Monkey-NET:
Keypoint Detector
Motion Transfer Gen.
Motion Network
𝑘: 2D matrix of keypoints, 𝑣: vector field, 𝑣𝑟𝑒𝑠, 𝑣𝑐𝑜𝑟: residual and coarse motion fields,
𝑚: estimated motion mask
ℒ𝑎𝑑𝑣: 𝑦, 𝑘𝑡,
ℒ𝐹𝑀: 𝑥𝑔∙𝑣𝑠𝑡, 𝑥𝑡
 Neural Talking Heads:
ℒ𝑝𝑒𝑟𝑐𝐼: 𝑥𝑠, 𝑥𝑔
ℒ𝐹𝑀: 𝐷ℓ∗𝑥𝑡
′, 𝑙𝑡, 𝐷ℓ∗𝑥𝑔, 𝑙𝑔
ℒℎ𝑖𝑛𝑔𝑒: 𝐷, 𝑥𝑔, 𝑥𝑡
 MarioNETte:
Source encoder
Target encoder
𝑙𝑠𝑡: 𝑡’s landmarks with 𝑠’s expression, 𝑣: feature maps, 𝑤: warped feature maps
𝐼𝐴𝐵: Image Attention Block
ℒ𝑎𝑑𝑣: 𝑙𝑠, 𝑦𝑡, 𝑥𝑔, 𝑥𝑠
ℒ𝑝𝑒𝑟𝑐: 𝐼1,ℓ∙𝑥𝑔, 𝐼1,ℓ∙𝑥𝑠
ℒ𝑝𝑒𝑟𝑐: 𝐼2,ℓ∙𝑥𝑔, 𝐼2,ℓ∙𝑥𝑠
ℒ𝐹𝑀: 𝐷ℓ𝑥𝑔, 𝐷ℓ𝑥𝑠
 Liu et al. 2019:
𝑈𝐵𝐾𝐸: Upper-body Key point Extractor
(𝑡−𝐿) , 𝐶𝑔= 𝑐𝑔
(𝑡−𝐿) , 𝑋𝑡= 𝑥𝑡
Face Boundary Pred.
Image Generator
ℒ𝐹𝑀𝐷2 : 𝐶𝑔, 𝑋𝑔, 𝐶𝑔, 𝑋𝑡
ℒ𝑝𝑒𝑟𝑐(𝐼): 𝑋𝑔, 𝑋𝑡
ℒ𝑎𝑑𝑣1: 𝑦1,
ℒ𝑎𝑑𝑣2: 𝑦2, 𝐶𝑔, 𝑋𝑔, 𝐶𝑔, 𝑋𝑡
ℒ1: 𝑏𝑔, 𝑏𝑡
Fig. 8. Architectural schematics of the reenactment networks. Black lines indicate prediction flows used
during deployment, dashed gray lines indicate dataflows performed during training. Zoom in for more detail.
Self-Attention Modeling. Similar to , another work called GANimation reenacts faces
through AU value inputs estimated from xs. Teir architecture uses an AU based generator that
uses a self atention model to handle occlusions, and mitigate other artifacts. Furthermore, another
network penalizesG with an expression prediction loss, and shares its weights with the discriminator
to encourage realistic expressions. Similar to CycleGAN, GANimation uses a cycle consistency loss
which eliminates the need for image pairing.
Instead of relying on AU estimations, the authors of propose GANnotation which uses facial
landmark images. Doing so enables the network to learn facial structure directly from the input
but is more susceptible to identity leakage compared to AUs which are normalized. GANotation
generates xд based on (xt,ls), where ls is the facial landmarks of xs. Te model uses the same self
atention model as GANimation, but proposes a novel “triple consistency loss” to minimize artifacts
in xд. Te loss teaches the network how to deal with intermediate poses/expressions not found in
the training set. Given ls,lt and lz sampled randomly from the same video, the loss is computed as
Ltrip = ∥G(xt,ls)−G(G(xt,lz),ls)∥2
3D Parametric Approaches. Concurrent to the work of , other works also leveraged 3D parametric facial models to prevent identity leakage in the generation process. In , the authors
propose FaceID-GAN which can reenacts t at oblique poses and high resolution. Teir ED generator
is trained in tandem with a 3DMM face model predictor, where the model parameters of xt are used
to transform xs before being joined with the encoder’s embedding. Furthermore, to prevent identity
leakage from xs to xд, FaceID-GAN incorporates an identiﬁcation classiﬁer within the adversarial
game. Teclassiﬁerhas2N outputswheretheﬁrstN outputs(correspondingtotrainingsetidentities)
are activated if the input is real and the rest are activated if it’s fake.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
Later, the authors of proposed FaceFeat-GAN which improves the diversty of the faces while
preserving the identity . Te approach is to use a set of GANs to learn facial feature distributions
as encodings, and then use these generators to create new content with a decoder. Concretely, three
encoder/predictor neural networks P, Q, and I, are trained on real images to extract feature vectors
from portraits. P predicts 3DMM parameters p, Q encodes the image as q capturing general facial
features using feedback from I, and I is an identity classiﬁer trained to predict label yi. Next two
GANs, seeded with noise vectors, producep′ and q′ while a third GAN is trained to reconstruct xt
from (p,q,yi) and xд from (p′,q′,yi). To reenact xt, (1)yt is predicted using I (even if the identity was
previously unseen), (2) zp and zq are selected empirically to ﬁt xs, and (3) the third GAN’s generator
uses (p′,q′,yt) to create xд. Although FaceFeat-GAN improves image diversty, it is less practical than
FaceID-GAN since the GAN’s input seed z be selected empirically to ﬁt xs.
In , the authors present paGAN, a method for complete facial reenactment of a 3D avatar,
using a single image of the target as input. An expression neutral image of xt is used to generate a 3D
model which is then driven byxs. Te driven model is used to create inputs for a U-Net generator: the
rendered head, its UV map, its depth map, a masked image of xt for texture, and a 2D mask indicating
the gaze of xs. Although paGAN is very eﬃcient, the ﬁnal deepfake is 3D rendered which detracts
from the realism.
Using Multi-Modal Sources. In the authors propose X2Face which can reenact xt with xs
or some other modality such as audio or a pose vector. X2Face uses two ED networks: an embedding
network and a driving network. First the embedding network encodes 1-3 examples of the target’s
face tovt: the optical ﬂow ﬁeld required to transform xt to a neutral pose and expression. Next, xt
is interpolated according tomt producing x
t. Finally, the driving network maps xs to the vector map
vs, crafed to interpolate x
t to xд, having the pose and expression of xs. During training, ﬁrst L1 loss
is used between xt and xд, and then an identity loss is used between xs and xд using a pre-trained
identity model trained on the VGG-Face Dataset. All interpolation is performed with a tensorﬂow
interpolation layer to enable back propagation using x
t and xд. Te authors also show how the
embedding of driving network can be mapped to other modalities such as audio and pose.
In 2019, nearly all works pursued identity agnostic models:
Facial Landmark & Boundary Conversion. In , the authors propose FaceSwapNet which
tries to mitigate the issue of identity leakage from facial landmarks. First two encoders and a decoder
are used to transfer the expression in landmark ls to the face structure of lt, denoted lд. Ten a
generator network is used to convert xt to xд wherelд is injected into the network with AdaIn layers
like a Style-GAN. Te authors found that it is crucial to use triplet perceptual loss with an external
VGG network.
In , the authors propose a method for high resolution reenactment and at oblique angles. A
set of networks encode the source’s pose, expression, and the target’s facial boundary for a decoder
that generates the reenacted boundarybд. Finally, an ED network generates xд using an encoding
of xt’s texture in its embedding. A multi-scale loss is used to improve quality and the authors utilize
a small labeled dataset by training their model in a semi-supervised way.
In , the authors present FSGAN: a face swapping and facial reenactment model which can
handle occlusions. For reenactment a pix2pixHD generator receives xt and the source’s 3D facial
landmarks ls, represented as a 256x256x70 image (one channel for each of the 70 landmarks). Te
output is xд and its segmentation map mд with three channels (background, face, and hair). Te
generator is trained recurrently where each output is passed back as input for several iterations while
ls is interpolated incrementally from ls to lt. To improve results further, delaunay Triangulation and
barycentric coordinate interpolation are used to generate content similar to the target’s pose. In
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
contrast to other facial conversion methods , FSGAN uses fewer neural networks enabling
real time reenactment at 30fps.
Latent Space Manipulation. In , the authors present a model called ICFace where the expression, pose, mouth, eye, andeyebrowsofxt canbedrivenindependently. Teirarchitectureissimilarto
aCycleGANinthatonegeneratortranslatesxt intoaneutralexpressiondomainasxη
t andanothergenerator translatesxη
t into an expression domain asxд. Both generators ar conditioned on the target AU.
In the authors propose an Additive Focal Variational Auto-encoder (AF-VAE) for high quality
reenactment. Tis is accomplished by separating a C-VAE’s latent code into an appearance encoding
ea and identity-agnostic expression coding ex. To capture a wide variety of factors in ea (e.g., age,
illumination, complexion, …), the authors use an additive memory module during training which
conditionsthelatentvariablesonaGaussianmixturemodel, ﬁtedtoclusteredsetoffacialboundaries.
Subpixel convolutions were used in the decoder to mitigate artifacts and improve ﬁdelity.
Warp-based Approaches. In the past, facial reenactment was done by warping the image xt to the
landmarks ls . In , the authors propose wgGAN which uses the same approach but creates
high-ﬁdelity facial expressions by reﬁning the image though a series of GANs: one for reﬁning the
warped face and another for in-painting the occlusions (eyes and mouth). A challenge with wgGAN
is that the warping process is sensitive to head motion (change in pose).
In , the authors propose a system which can also control the gaze: a decoder generates xд
with an encoding ofxt as the input and a segmentation map ofxs as reenactment guidance via SPADE
residual blocks. Te authors blend xд with a warped version, guided by the segmentation, to mitigate
artifacts in the background.
To overcome issue of occlusions in the eyes and mouth, the authors of use multiple images oft
as a reference, in contrast to and which only use one. In their approach (FLNet), the model
is provided with N samples of t (Xt) having various mouth expressions, along with the landmark
deltas between Xt and xs (Lt). Teir model is an ED (conﬁgured like GANimation ) which
produces (1) N encodings for a warped xд, (2) an appearance encoding, and (3) a selection (weight)
encoding. Te encodings are then coverted into images using seperate CNN layers and merged
together through masked multiplication. Te entire model is trained end-to-end in a self supervised
manner using frames of t taken from diﬀerent videos.
Motion-Content Disentanglement. In the authors propose a GAN to reenact neutral expression faces with smooth animations. Te authors describe the animations as temporal curves in 2D
space, summarizedaspointsonasphericalmanifoldbycalculatingtheirsquare-rootvelocityfunction
(SRVF). A WGAN is used to complete this distribution given target expression labels, and a pix2pix
GAN is used to convert the sequences of reconstructed landmarks into a video frames of the target.
In contrast to MoCoGAN , the authors of propose ImaGINator: a conditional GAN
which fuses both motion and content and uses with transposed 3D convolutions to capture the
distinct spatio-temporal relationships. Te GAN also uses a temporal discriminator, and to increase
diversity, the authors train the temporal discriminator with some videos using the wrong label.
A challenge with works such as and is that they are label driven and produce videos
with a set number of frames. Tis makes the deepfake creation process manual and less practical. In
contrast, the authors of propose Monkey-Net: a self supervised network for driving an image
with an arbitrary video sequence. Similar to MoCoGAN , the authors decouple the source’s
content and motion. First a series of networks produce a motion heat map (optical ﬂow) using the
source and target’s key-points, and then an ED generator produces xд using xs and the optical ﬂow
(in its embedding).
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
Later in , the authors extend Monkey-Net by improving the object appearance when large
pose transformations occur. Tey accomplish this by (1) modeling motion around the keypoints
using aﬃne transformations, (2) updating the key-point loss function accordingly, and (3) having the
motion generator predict an occlusion mask on the preceding frame for in-painting inference. Teir
work has been implemented as a free real-time reenactment tool for video chats, called Avitarify.6
Few-Shot Learning. Towards the end of 2019 and into the beginning of 2020, researchers
beganlookingintominimizingtheamountoftrainingdatafurtherviaone-shotandfew-shotlearning.
In , the authors propose a few-shot model which works well at oblique angles. To accomplish
this, the authors perform meta-transfer learning, where the network is ﬁrst trained on many diﬀerent
identities and then ﬁne-tuned on the target’s identity. Ten, an identity encoding of xt is obtained
by averaging the encodings of k sets of (xt,lt). Ten a pix2pix GAN is used to generate xд using ls
as an input, and the identity encoding via AdaIN layers. Unfortunately, the authors note that their
method is sensitive to identity leakage.
In the authors of Vid2Vid (Section 4.1.2) extend their work with few-shot learning. Tey
use a network weight generation module which utilizes an atention mechanism. Te module learns
to extract appearance paterns from a few samples of xt which are injected into the video synthesis
layers. In contrast to FLNet , , and which merge the multiple representations of t
before passing it through the generator. Tis approach is more eﬃcient because it involves fewer
passes through the model’s networks.
In , the authors propose MarioNETte which alleviates identity leakage when the pose of xs
is diﬀerent than xt. In contrast to other works which encode the identity separately or use of AdaIN
layers, the authors use an image atention block and target feature alignment. Tis enables the model
to beter handle the diﬀerences between face structures. Finally, the identity is also preserved using
a novel landmark transformer inspired by .
Mouth Reenactment (Dubbing)
In contrast to expression reenactment, mouth reenactment (a.k.a., video or image dubbing) is concerned with driving a target’s mouth with a segment of audio. Fig. 9 presents the relevant schematics
for this section.
Many-to-One (Multiple Identities to a Single Identity).
Obama Puppetry. In 2017, the authors of created a realistic reenactment of former president
Obama. Tis was accomplished by (1) using a time delayed RNN over MFCC audio segments to generate a sequence of mouth landmarks (shapes), (2) generating the mouth textures (nose and mouth) by
applyingaweightedmediantoimageswithsimilarmouthshapesviaPCA-spacesimilarity,(3)reﬁning
the teeth by transferring the high frequency details other frames in the target video, and (4) by using
dynamic programming to re-time the target video to match the source audio and blend the texture in.
Later that year, the authors of presented ObamaNet: a network that reenacts an individual’s
mouth and voice using text as input instead of audio like . Te process is to (1) convert the
source text to audio using Char2Wav , (2) generate a sequence of mouth-keypoints using a
time-delayed LSTM on the audio, and (3) use a U-Net CNN to perform in-painting on a composite
of the target video frame with a masked mouth and overlayed keypoints.
Later in 2018, Jalalifar et al. proposed a network that synthesizes the entire head portrait of
Obama, and therefore does not require pose re-timing and can trained end-to-end, unlike and
 . First,abidirectionalLSTMcovertsMFCCaudiosegmentsintosequenceofmouthlandmarks,and
6htps://github.com/alievk/avatarify
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
 Synthesizing Obama:
𝑎𝑖: the 𝑖-th 25ms segment of audio with a stride of 10ms. 𝑀𝑅: mouth retrieval and
enhancement based on 3DMM reconstructions. 𝑂𝐹: optical flow extractor
Step delay=20
Audio to Mouth Landmark
Mouth Landmark to Face
 TETH:
𝑡𝑠: text to be inserted into speech. 𝑇23𝐷: A 3DMM video renderer based on 𝑡𝑠using a
viseme lookup on 𝑡. *Audio gen not shown (TTS is done procedurally) .
ℒ𝑎𝑑𝑣𝑆: 𝑦𝑠, 𝑥𝑐
ℒ𝑎𝑑𝑣𝑇: 𝑦𝑇, 𝑥𝑡
ℒ𝑝ℎ𝑜𝑡𝑜𝑚𝑒𝑡𝑟𝑖𝑐: 𝑥𝑜
𝑖:𝑖−2 , 𝑥𝑡
𝑖:𝑖−2 , 𝑚𝑚
 SD-CGAN:
Audio to Landmark
Landmark to Face
𝑎(𝑖): the 𝑖-th 33ms segment of audio. 𝑝: lip landmarks.
ℒ𝑎𝑑𝑣: 𝑦, 𝑒𝐼,
𝑝𝑡, 𝑥𝑡𝑝𝑔, 𝑥𝑔
ℒ𝑀𝑆𝐸𝐿𝑆𝑇𝑀: 𝑝𝑡, 𝑢𝑡
 Neural Voice Puppetry:
𝑎𝑖: the 𝑖-th 300ms audio segment with stride 20ms. 𝐶: content aware filter network
𝑁𝑇𝐸: Neural Texture Extractor
DeepSpeech
Expression
Stabilization
Deferred Renderer
Compositor
𝑖−1:𝑖+1 , 𝑝𝑔
ℒ𝑎𝑑𝑣2: 𝑦2, 𝑥𝑡
ℒ1 𝐻1 : 𝑥𝑔
𝑚(𝑖) ℒ𝑝𝑒𝑟𝑐−𝑠𝑡𝑦𝑙𝑒(𝐻2): 𝑥𝑔
ℒ1(𝐻2): 𝑥𝑔
(𝑖) ℒ𝑎𝑑𝑣1: 𝑦1, 𝑢𝑔
 ATVGnet:
Audio to Landmark
Landmark to Face
𝑝: landmarks compressed with PCA. 𝑎(𝑖): 10ms of audio around the 𝑖-th frame.
𝑚𝑎: attention map. 𝑚𝑤: motion map.
ℒ𝑝𝑖𝑥𝐵, 𝐶: 𝑥𝑔, 𝑥𝑡, 𝑚𝑎2
ℒ𝑎𝑑𝑣: 𝑦, 𝑒𝐼, 𝑋𝑡/𝑋𝑔
ℒ𝑀𝑆𝐸𝐿𝑆𝑇𝑀: 𝑝𝑔
 DAVS:
Temporal D.
Identity Enc.
Word (video) Enc.
Word (audio) Enc.
Word Classif.
ID Classif.
𝑎(𝑖): the 𝑖-th segment of audio containing a word. 𝑦𝐼: identity label
𝑦𝑤: word label (one-hot encoding)
ℒ1 𝐷𝑒: 𝑋𝑔, 𝑋𝑡
ℒ𝑎𝑑𝑣: 𝑋𝑔/𝑋𝑡, 𝑦
ℒ𝑐𝑜𝑛𝑡𝑟𝑎𝑠𝑡𝑖𝑣𝑒: 𝑒𝑤𝑣, 𝑒𝑤𝑎, 𝑦𝑤
ℒ𝑎𝑑𝑣𝐶1, 𝐶2 : 𝑒𝐼, 𝑒𝑤𝑣, 𝑦𝑤
ℒ𝐶𝐸𝐶𝐼: 𝑒𝐼, 𝑦𝐼
ℒ𝐶𝐸𝐶𝐼: 𝑒𝑤𝑣/𝑒𝑤𝑎, 𝑦𝐼
 Speech2Vid:
Context Enc.
Audio Enc.
Identity Enc.
Skip Connections
Face Recog.
𝑎(𝑖): the 𝑖-th 350ms segment of audio with stride 40ms
ℒ𝑝𝑒𝑟𝑐: 𝐼ℓ∗𝑥𝑔
ℒ2 𝐵: 𝑏𝑙𝑢𝑟𝑥𝑡
 Speech Driven Animation:
𝑎(𝑖): a 160ms audio segment, shifted according to the frames.
Audio Enc.
Identity Enc.
Frame Disc.
Sync Disc. 𝐷𝑠
Sequence Disc. 𝐷𝑞
ℒ𝑎𝑑𝑣𝐹: 𝑦𝑓, 𝑥𝑡
ℒ𝑎𝑑𝑣𝑄: 𝑦𝑞, 𝑥𝑡
ℒ𝑎𝑑𝑣𝑆: 𝑦𝑠,
Fig. 9. Architectural schematics for some mouth reenactment networks. Black lines indicate prediction
flows used during deployment, dashed gray lines indicate dataflows performed during training.
then a pix2pix like network generates frames using the landmarks and a noise signal. Afer training,
the pix2pix network is ﬁne-tuned using a single video of the target to ensure consistent textures.
3D Parametric Approaches. Later on in 2019, the authors of proposed a method for editing
a transcript of a talking heads which, in turn, modiﬁes the target’s mouth and speech accordingly.
Te approach is to (1) align phenomes to as, (2) ﬁt a 3D parametric head model to each frame of Xt
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
like , (3) blend matching phenomes to create any new audio content, (4) animate the head model
with the respective frames used during the blending process, and (5) generate Xд with a CGAN RNN
using composites as inputs (rendered mouths placed over the original frame).
Te authors of had a diﬀerent approach: (1) animate a the reconstructed 3D head with the
predicted blend shape parameters from as using a DeepSpeech model for feature extraction, (2)
use Deferred Neural Rendering to generate the mouth region, and then (3) use a network to
blend the mouth into the original frame. Compared to previous works, the authors found that their
approach only requires 2-3 minutes of video while producing very realistic results. Tis is because
neural rendering can summarize textures with a high ﬁdelity and operate on UV maps –mitigating
artifacts in how the textures are mapped to the face.
Many-to-Many (Multiple IDs to Multiple IDs). One of the ﬁrst works to perform identity
agnostic video dubbing was . Tere the authors used an LSTM to map MFCC audio segments
to the face shape. Te face shapes were represented as the coeﬃcients of an active appearance model
(AAM), which were then used to retrieve the correct face shape of the target.
Improvements in Lip-sync. Noting a human’s sensitivity to temporal coherence, the authors of
 use a GAN with three discriminators: on the frames, video, and lip-sync. Frames are generated by (1) encoding each MFCC audio segment a(i)
s and xt with separate encoders, (2) passing the
encodings through an RNN, and (3) decoding the outputs as x(i)
д using a decoder.
In the authors try to improve the lipsyncing with a textual context. A time-delayed LSTM is
used to predict mouth landmarks given MFCC segments and the spoken text using a text-to-speech
model. Te target frames are then converted into sketches using an edge ﬁlter and the predicted
mouth shapes are composited into them. Finally, a pix2pix like GAN with self-atention is used to
generate the frames with both video and image conditional discriminators.
Compared to direct models such as direct models , the authors of improve the
lip-syncing by preventing the model from learning irrelevant correlations between the audiovisual
signal and the speech content. Tis was accomplished with LSTM audio-to-landmark network and
a landmark-to-identity CNN-RNN used in sequence. Tere, the facial landmarks are compressed
with PCA and the atention mechanism from is used to help focus the model on the relevant
paterns. To improve synchronization further, the authors proposed a regression based discriminator
which considers both sequence and content information.
EDs for Preventing Identity Leakage. Te authors in mitigate identity leakage by disentangling the speech and identity latent spaces using adversarial classiﬁers. Since their speech encoder is
trained to project audio and video into the same latent space, the authors show how xд can be driven
using xs or as.
In , the authors propose Speech2Vid which also uses separate encoders for audio and identity.
However, to capture the identity beter, the identity encoder EnI uses a concatenation of ﬁve images
of the target, and there are skip connections from the EnI to the decoder. To blend the mouth in
beter, a third ‘context’ encoder is used to encourage in-painting. Finally, a VDSR CNN is applied
to xд to sharpen the image.
A disadvantage with and is that they cannot control facial expressions and blinking.
To resolve this, the authors in generate frames with a stride transposed CNN decoder on
GRU-generated noise, in addition to the audio and identity encodings. Teir video discriminator
uses two RNNs for both the audio and video. When applying the L1 loss, the authors focus on the
lower half of the face to encourage beter lip sync quality over facial expressions.
Later in , the same authors improve the temporal coherence by spliting the video discriminator into two: (1) for temporal realism in mouth to audio synchronization, and (2) for temporal
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
realism in overall facial expressions. Ten in , the authors tune their approach further by fusing
the encodings (audio, identity, and noise) with a polynomial fusion layer as opposed to simply
concatenating the encodings together. Doing so makes the network less sensitive to large facial
motions compared to and .
Pose Reenactment
Most deep learning works in this domain focus on the problem of face frontalization. However, there
are some works which focus on facial pose reenactment.
In the authors use a U-Net to convert (xt,lt,ls) into xд using a GAN with two discriminators:
one conditioned with the neutral pose image, and the other conditioned with the landmarks. In ,
theauthorsproposeDR-GANforpose-invariantfacerecognition. Toadjusttheposeofxt, theauthors
use an ED GAN which encodesxt aset, and then decodes (et,ps,z) asxд, whereps is the source’s pose
vectorandz isanoisevector. Comparedto , hastheﬂexibilityofmanipulatingtheencodings
for diﬀerent tasks and the authors improve the quality of xд by averaging multiple examples of the
identity encoding before passing it through the decoder (similar to ). In , the authors
suggest using two GANs: Te ﬁrst frontalizes the face and produces a UV map, and second rotates
the face, given the target angle as an injected embedding. Te result is that each model performs a
less complex operation and can therefore the models collectively can produce a higher quality image.
Gaze Reenactment
Tereareonlyafewdeeplearningworkswhichhavefocusedongazereenactment. In theauthors
convert a cropped eye xt, its landmarks, and the source angle, to a ﬂow (vector) ﬁeld using a 2-scale
CNN.xд is then generated by applying a ﬂow ﬁeld toxt to warping it to the source angle. Te authors
then correct the illumination of xд with a second CNN. A challenge with is that the head must
be frontal to avoid inconsistencies due to pose and perspective. To mitigate this issue, the authors of
 proposedtheGazeRedirectionNetwork(GRN).InGRN,thetarget’scroppedeye, headpose, and
source angle are encoded separately and then passed though an ED network to generate an optical
ﬂow ﬁeld. Te ﬁeld is used to warpxt intoxд. To overcome the lack of training data and the challenge
of data pairing, the authors (1) pre-train their network on 3D synthesized examples, (2) further tune
their network on real images, and then (3) ﬁne tune their network on 3-10 examples of the target.
Body Reenactment
Several facial reenactment papers from Section 4.1 discuss body reenactment too. For example,
Vid2Vid , MocoGAN , and others . In this section, we focus on methods
which speciﬁcally target body reenactment. Schematics for some of these architectures can be found
in Fig. 10.
One-to-One (Identity to Identity). In the work , the authors perform facial reenactment with the upper-body as well (arms and hands). Te approach is to (1) use a pix2pixHD GAN
to convert the source’s facial boundaries to the targets, (2) and then paste them onto a captured pose
skeleton of the source, and (3) use a pix2pixHD GAN to generate xд from the composite.
Many-to-One (Multiple Identities to a Single Identity).
Dance Reenactment. In the authors make people dance using a target speciﬁc pix2pixHD
GAN with a custom loss function. Te generator receives an image of the captured pose skeleton and
the discriminator receives the current and last image conditioned on their poses. Te quality of face
is then improved with a residual predicted by an additional pix2pixHD GAN, given the face region of
the pose. A many-to-one relationship is achieved by normalizing the input pose to that of the target’s.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
 Everybody Dance Now:
Pose Predictor
Body Generator
Face Refiner
𝑟: residual
ℒ𝐹𝑀𝐺: 𝐷𝐵∙𝑐𝑡
ℒ𝑎𝑑𝑣𝐵∙: 𝑦𝐵, 𝑐𝑡
ℒ𝑎𝑑𝑣𝐹: 𝑦𝐹, 𝑐𝑙
ℒ𝑝𝑒𝑟𝑐𝐺𝐵: 𝐼ℓ𝑥𝑔
ℒ𝑝𝑒𝑟𝑐𝐺𝐵: 𝐼ℓ𝑥𝑔
𝑖+1 ′ , 𝐼ℓ𝑥𝑡
 NRR-HAV:
3𝐷𝐶: produces a textures 3D body mesh using photogrammetry software.
3𝐷𝑀: Motion capture. 𝑥𝑏: target background image. : Hadamard product
ℒ𝑎𝑑𝑣𝑎𝑡𝑡: 𝑦,
ℒ1: 𝑥𝑔, 𝑥𝑡
 Deep Vid. Perf. Cloning:
Pose Predictor
shared weights
𝑥𝑜: an image with an identity not 𝑠or 𝑡. 𝑂𝐸: optical flow field extractor (between frames)
ℒ𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙−𝑐𝑜ℎ𝑒𝑟𝑒𝑛𝑐𝑒: 𝑥𝑜,𝑔
ℒ𝑝𝑒𝑟𝑐: 𝐼ℓ1 ො𝑥𝑡, 𝐼ℓ∙𝑥𝑡
ℒ𝑎𝑑𝑣1: 𝑦1, ො𝑥𝑡/𝑥𝑡
ℒ𝑎𝑑𝑣2: 𝑦2, 𝑥𝑡/𝑥𝑔
 DwNet:
𝑢: UV map to the body, segmented by limb. 𝑊𝐺𝐸: Warp Guided Extractor. 𝑔: warp grid
Appearance Enc.𝐸𝑛𝐴
Warp Module (𝑊)
ℒ𝐹𝑀: 𝐷ℓ𝑘𝑥𝑔
ℒ𝑝𝑒𝑟𝑐: 𝐼ℓ𝑥𝑔
Fig. 10. Architectural schematics for some body reenactment networks. Black lines indicate prediction
flows used during deployment, dashed gray lines indicate dataflows performed during training.
Te authors of then tried to overcome artifacts which occur in such stretched limbs due
to incorrectly detected pose skeletons. Tey used photogrammetry sofware on hundreds of images of
the target, and then reenacted the 3D rendering of the target’s body. Te rendering, partitioned depth
map, and background are then passed to a pix2pix model for image generation, using an atention loss.
Another artifact in was that the model could not generalize well to unseen poses. To improve
the generalization, the authors of trained their network on many identities other than s and t.
First they trained the GAN on paired data (the same identity doing diﬀerent poses) and then later
added another discriminator to evaluate the temporal coherence given (1) x(i)
д driven by another
video, and (2) the optical ﬂow predicted version.
Achallengewiththepreviousworkswasthattheyrequiredalotsoftrainingdata. Tiswasreduced
from about an hour of video footage to only 3 minutes in by segmenting and orienting the
limbs of xt according to xs before the generation step. Ten a pix2pixHD GAN uses this composition
and the last k frames’ poses to generate the body. Finally, another pix2pixHD GAN is used to blend
the body into the background.
Many-to-Many (Multiple IDs to Multiple IDs).
Pose Alignment. In the authors try to resolve the issue of misalignment when using pix2pix
like architectures. Tey propose ‘deformable skip connections’ which help orient the shutled feature
maps according to the source pose. Te authors also propose a novel nearest neighbor loss instead
of using L1 or L2 losses. To modify unseen identities at test time, an encoding of xt is passed to the
decoder’s inner layers.
Although the work of helps align the general images, artifacts can still occur when xs and
xt have very diﬀerent poses. To resolve this, the authors of use novel Pose-Atentional Transfer blocks (PATB) inside their GAN-based generator. Te architecture passes xt and the poses ps
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
concatenated with pt through separate encoders which are passed though a series of PATBs before
being decoded. Te PATBs progressively transfer regional information of the poses to regions of
the image to ultimately create a body that has beter shape and appearance consistency.
Pose Warping. In the authors use a pre-trained DensePose network to reﬁne a predicted
pose with a warped and in-painted DensePose UV spatial map of the target. Since the spatial map
covers all surfaces of the body, the generated image has improved texture consistency. In contrast to
 which uses feature mappings to alleviate misalignment, the authors of use warping
which reduces the complexity of the network’s task. Teir model, called DwNet, uses a ‘warp module’
in an ED network to encode x(i−1)
warped to p(i)
s , where p is a UV body map of a pose obtained a
DensePose network.
A challenge with the alignment techniques of the previous works is that the body’s 3D shape and
limb scales are not considered by the network resulting in identity leakage from xs. In , the
authors counter this issue with their Liquid Warping GAN. Tis is accomplished by predicting target
and source’s 3D bodies with the model in and then by translating the two through a novel liquid
warping block (LWB) in their generator. Speciﬁcally, the estimated UV maps of xs and xt, along with
their calculated transformation ﬂow, are passed through a three stream generator which produces
(1) the background via in-painting, (2) a reconstruction of the xs and its mask for feature mapping,
and (3) the reenacted foreground and its mask. Te later two streams use a shared LWB to help
the networks address multiple sources (appearance, pose, and identity). Te ﬁnal image is obtained
through masked multiplication and the system is trained end-to-end.
Background Foreground Compositing. In , the authors break the process down into three
stages, trained end-to-end: (1) use a U-Net to segmentxt’s body parts and then orient them according
to the source pose ps, (2) use a second U-Net to generate the body xд from the composite, and (3) use
a third U-Net to perform in-painting on the background and paste xд into it. Te authors of then
streamlinedthisprocess byusingasingleEDGAN network todisentangletheforegroundappearance
(body), background appearance, and pose. Furthermore, by using an ED network, the user gains
control over each of these aspects. Tis is accomplished by segmenting each of these aspects before
passing them through encoders. To improve the control over the compositing, the authors of 
used a CVAE-GAN. Tis enabled the authors to change the pose and appearance of bodies individually.
Te approach was to condition the network on heatmaps of the predicted pose and skeleton.
Few-Shot Learning. In , the authors demonstrate the few-shot learning technique
of on a pix2pixHD network and the network of . Using just a few sample images, they were
able to transfer the resemblance of a target to new videos in the wild.
REPLACEMENT
Te network schematics and summary of works for replacement deepfakes can be found in Fig. 12
and Table 2 respectively.
At ﬁrst, face swapping was a manual process accomplished using tools such as Photoshop. More
automated systems ﬁrst appeared between 2004-08 in and . Later, fully automated methods
were proposed in and using methods such as warping and reconstructed 3D
morphable face models.
One-to-One (Identity to Identity).
Online Communities. Afer the Reddit user ‘deepfakes’ was exposed in the media, researchers
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
Basic Losses:
ℒ1(𝐸𝑛, 𝐷𝑒𝑠): ො𝑥𝑠
ℒ1 𝐸𝑛, 𝐷𝑒𝑡: ො𝑥𝑡
DeepFaceLab :
ℒ2 𝐸𝑛, 𝐷𝑒𝑠: 𝐵𝑙𝑢𝑟𝑚𝑠
ℒ2 𝐸𝑛, 𝐷𝑒𝑡: 𝐵𝑙𝑢𝑟𝑚𝑠
Fig. 11. The basic schematic for the Reddit ‘deepfakes’ model and its variants .
and online communities began ﬁnding improved ways to perform face swapping with deep neural
networks. Te original deepfake network, published by the Reddit user, is an ED network (visualized
in Fig. 11). Te architecture consists of one encoder En and two decoders Des and Det. Te components are trained concurrently as two autoencoders: Des(En(xs))= ˆxs and Det(En(xt))= ˆxt, where
x is a cropped face image. As a result, En learns to map s and t to a shared latent space, such that
Des(En(xt))=xд
Currently, there are a number of open source face swapping tools on GitHub based on the original
network. One of the most popular is DeepFaceLab . Teir current version oﬀers a wide variety
of model conﬁgurations, including adversarial training, residual blocks, a style transfer loss, and
masked loss to improve the quality of the face and eyes. To help the network map the target’s identity
into arbitrary face shapes, the training set is augmented with random face warps.
Another tool called FaceSwap-GAN follows a similar architecture, but uses a denoising
autoencoder with a self-atention mechanisms, and oﬀers cycle-consistency loss which can reduce
theidentityleakageandincreasetheimageﬁdelity. TedecodersinFaceSwap-GANalsogeneratesegmentation masks which helps the model handle occlusions and is used to blendxд back into the target
frame. Finally, is another open source tool that provides a GUI. Teir sofware comes with 10 popular implementations, including that of , and multiple variations of the original Redit user’s code.
One-to-Many (Single Identity to Multiple Identities).
In , the authors use a modiﬁed style transfer with CNN, where the content is xt and the style
is the identity of xs. Te process is (1) align xt to a reference xs, (2) transfer the identity of s to the
image using a multi scale CNN, trained with style loss on images of s, and (3) align the output to xt
and blend the face back in with a segmentation mask.
Many-to-Many (Multiple IDs to Multiple IDs).
One of the ﬁrst identity agnostic methods was , mentioned in Section 4.1.3. However, to train
this CGAN, one needs a dataset of paired faces with diﬀerent identities having the same expression.
Disentanglement with EDs. However, To provide more control over the In the authors us
an ED to disentangle the identity from the atributes (pose, hair, background, and lighting) during
the training process. Te identity encodings are the last pooling layer of a face classiﬁer, and the
atribute encoder is trained using a weighted L2 lossand a KL divergence loss to mitigate identity
leakage. Te authors also show that they can adjust atributes, expression, and pose via interpolation
of the encodings. Instead of swapping identities, the authors of wanted to variably obfuscate
the target’s identity. To accomplish this, the authors used an ED to predict the 3D head parameters
which where either modiﬁed or replaced with the source’s. Finally a GAN was used to in-paint the
face of xt given the modiﬁed head model parameters.
Disentanglement with VAEs. In , the authors propose RSGAN: a VAE-GAN consisting of
two VAEs and a decoder. One VAE encodes the hair region and the other encodes the face region,
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
Table 2. Summary of Deep Learning Replacement Models
Replacement Retraining for new…
Model Training Model Execution Model Outp.
Source (s)
Target (t)
Identity Agnostic
Discriminators
Other Netw.
3DMM/Rendering
Segmentation
Landmark / Keypoint
Labeling of: ID
Labeling of: Other
No Pairing
Paring within Same Video
Paring ID to Diﬀr. Actions
Requires Video
Source (xs …)
Target (xt …)
Resolution
 2017 Deepfakes for All
2k-5k portraits
 2018 FaceSwap-GAN
2k-5k portraits
One-to-One
DeepFaceLab
2k-5k portraits
One-to-Many
 2017 Fast Face Swap
• 60 portraits None
portrait portrait
 2018
portrait portrait
 2018
portrait portrait
portrait portrait
 2018
None • 3 2
portrait portrait
 2018
None • 4 4
• • portrait portrait
 2018
None • 1 1
portrait portrait
 2019
FS Face Trans.
None • 1 1
portraits portrait
 2019
None • 2 1
• cropped cropped
Many-to-Many
FaceShifer
None • 3 3
portrait portrait
where both are conditioned on a predicted atribute vector c describing x. Since VAEs are used, the
facial atributes can be edited through c.
In contrast to , the authors of use a VAE to prepare the content for the generator, and
use a network to perform the blending via in-painting. A single VAE-ED network is run on xs and
then xt producing encodings for the face of xs and the landmarks of xt. To perform a face swap,
a generator receives the masked portrait of xt and performs in-painting on the masked face. Te
generator uses the landmark encodings in its embedding layer. During training, randomly generated
faces are used with triplet loss on the encodings to preserve identities.
Face Occlusions. FSGAN , mentioned Section 4.1.3, is also capable of face swapping and can
handle occlusions. Afer the face reenactment generator produces xr, a second network predicts
the target’s segmentation mask mt. Ten (x ⟨f ⟩
,mt) is passed to a third network that performs
in-painting for occlusion correction. Finally a fourth network blends the corrected face into xt while
considering ethnicity and lighting. Instead of using interpolation like , the authors of 
propose FaceShifer which uses novel Adaptive Atentional Denormalization layers (AAD) to transfer
localized feature maps between the faces. In contrast to , FaceShifer reduces the number of
operations by handling the occlusions through a reﬁnement network trained to consider the delta
between the original xt and a reconstructed ˆxt.
Few-Shot Learning. Te same author of FaceSwap-GAN also hosts few-shot approach online dubbed “One Model to Swap Tem All” . In this version the generator receives
,mt) where its encoder is conditioned on VGGFace2 features of xt using FC-AdaIN layers, and its decoder is conditioned on xt and the face structure mt via layer concatenations and
SPADE-ResBlocks respectively. Two discriminators are used: one on image quality given the face
segmentation and the other on the identities.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
 Face Swap GAN:
𝑚: segmentation mask (face)
ℒ𝑡𝑣(𝑠): 𝑥𝑠
ℒ𝑡𝑣(𝑡): 𝑥𝑡
ℒ𝑎𝑑𝑣(𝑠): 𝑦𝑠, 𝑥𝑠′/ො𝑥𝑠
ℒ𝑎𝑑𝑣(𝑡): 𝑦𝑡, 𝑥𝑡
ℒ1(𝑠): ො𝑥𝑠
ℒ1(𝑡): ො𝑥𝑡
ℒ𝑝𝑒𝑟𝑐(𝑠): 𝐼ℓ∙
ℒ𝑝𝑒𝑟𝑐(𝑡): 𝐼ℓ∙
ℒ𝑐𝑦𝑐(𝑠): 𝐺𝑡𝑠𝐺𝑠𝑡𝑥𝑠
ℒ𝑐𝑦𝑐(𝑡): 𝐺𝑠𝑡𝐺𝑡𝑠𝑥𝑡
 Fast Face Swap:
Style Transfer Network
Light Measure
Siamese Network
ℒ𝑝𝑒𝑟𝑐: 𝐿𝑥𝑔
ℒ𝑠𝑡𝑦𝑙𝑒: 𝑥𝑔
ℒ𝐹𝑀: 𝐿ℓ∙𝑥𝑔
 OSIP-FS:
Identity Enc.
Attribute Enc.
Discriminator
ℒ𝐶𝐸𝐸𝑛𝐼: 𝑦𝐼, 𝑥
ℒ2 𝐸𝑛𝐴: 𝑥𝑔, 𝑥𝑡-weighted ℒ𝐾𝐿: 𝑒𝐴
ℒ𝑎𝑑𝑣𝐷: 𝑦, 𝑥𝑡/𝑥𝑔
ℒ𝐹𝑀𝐷𝑒: 𝐷ℓ∙𝑥𝑔, 𝐷ℓ∙(𝑥𝑡)
ℒ𝐶𝐸(𝐷𝐼): 𝑦𝐼, 𝑥′/𝑥𝑔
ℒ𝐹𝑀𝐷𝑒: 𝐷𝐼ℓ∙𝑥𝑔, 𝐷𝐼ℓ∙(𝑥′)
 RSGAN:
𝑥ℎ: Hair region
ℒ1: 𝑥𝑓, ො𝑥𝑓
ℒ1: 𝑥ℎ, ො𝑥ℎ
ℒ𝑎𝑑𝑣1: 𝑦, Τ
ℒ𝑎𝑑𝑣2: 𝑦, Τ
ℒ𝐶𝐸𝐶: 𝑐, 𝑦𝑐
 FSNet:
ED Network (EN)
Face Encoder
Landmark Enc.
Patch disc.
VAE Objectives: ℒ𝐶𝐸𝐸𝐷:𝑚𝑓, ෝ𝑚𝑓
ℒ𝐶𝐸𝐸𝐷: 𝑙, መ𝑙ℒ1 𝐸𝐷: 𝑥𝑓, ො𝑥𝑓
ℒ1 𝐸𝐷: 𝑥𝑓∙𝑚𝑓, ො𝑥𝑓∙𝑚𝑓
ℒ𝑎𝑑𝑣𝐸𝐷: 𝑦𝑚, 𝑚𝑓/𝑚′ 𝑓
ℒ𝑎𝑑𝑣𝐸𝐷: 𝑦𝑓, 𝑥𝑓/𝑥′ 𝑓
ℒ𝑎𝑑𝑣𝐸𝐷: 𝑦𝑙, 𝑙/𝑙′ 𝑓
ℒ𝑎𝑑𝑣: 𝑦, 𝑥′/𝑥𝑔
ℒ𝑎𝑑𝑣,𝑃: 𝑦𝑃, 𝑥′/𝑥𝑔
ℒ𝑡𝑟𝑖𝑝𝐸𝐷: (𝑥𝑡1, 𝑥𝑡2, 𝑥𝑠), (𝑥𝑔1, 𝑥𝑡1, 𝑥𝑠), (𝑥𝑔2, 𝑥𝑡2, 𝑥𝑠)
 FaceShifer:
Identity Enc.
Attribute Encoder
Recog. Net
Occlusion Correction
ℒ𝑎𝑑𝑣1: 𝑦1, ൗ
ℒ𝑎𝑑𝑣2: 𝑦2,
ℒ𝑎𝑑𝑣3: 𝑦3,
ℒ𝑐𝑜𝑠𝑖𝑛𝑒𝐷𝑒, 𝐻1 : 𝑒𝑠𝐼, 𝑒𝑔𝐼
ℒ𝑝𝑒𝑟𝑐𝐷𝑒, 𝐻1 : 𝐻1 ℓ∙𝑥𝑡, 𝐻1 ℓ∙𝑥𝑔∗
ℒ2 𝐷𝑒, 𝐻1 : 𝑥𝑔, 𝑥𝑡-if s=t
ℒ𝑐𝑜𝑠𝑖𝑛𝑒𝐻2 : 𝑒𝑠𝐼, 𝑒𝑔𝐼
ℒ1 𝐻2 : 𝑥𝑔, 𝑥𝑡−𝑥𝑔∗
ℒ2 𝐻2 : 𝑥𝑔, 𝑥𝑡-if s=t
 Few-Shot Face Translation:
Face Segmentation
Losses unknown: Training code was not released as of writing
 Depth Nets:
Blend Repair
𝑝𝑠𝑡: affine transformation parameters and depth measures
ℒ𝑐𝑢𝑠𝑡𝑜𝑚(𝐸𝑛, 𝑃): 𝑝𝑠𝑡
ℒ𝑐𝑦𝑐𝐻𝑔𝑥, 𝐻𝑥𝑔: 𝑥𝑔∗, 𝑥
ℒ𝑎𝑑𝑣𝐷𝑡: 𝑦𝑡, Τ
ℒ𝑎𝑑𝑣𝐷∙: 𝑦𝑡,
 IHPT:
Identity Enc.
Video ID Classifier
Realism D.
Identity D.
ℒ1: 𝑥𝑔, 𝑥𝑡
ℒ1: 𝑒𝐼, Ƹ𝑒𝐼
ℒ1: 𝑒𝑃, Ƹ𝑒𝑃
ℒ𝑝𝑖𝑥𝑒𝑙−𝑣𝑒𝑟𝑖𝑓: 𝑥𝑔, 𝑥𝑡
ℒ𝐶𝐸𝐷𝑒, 𝐶: 𝑦, ො𝑥
ℒ𝑎𝑑𝑣1: 𝑦1, Τ
ℒ𝑎𝑑𝑣2: 𝑦2,
ℒ𝑎𝑑𝑣𝐼1: 𝑦𝐼1, Τ
ℒ𝑎𝑑𝑣𝐼2: 𝑦𝐼2,
 FSGAN:
𝑚: segmentation mask (face, hair, other), 𝑙: 3D facial landmarks,
𝐻𝑛: 𝑛passes through 𝐻while interpolating 𝑙𝑠to 𝑙𝑡
Reenactment
Segmentation
in-painting
ℒ1 𝐻1 : 𝑥𝑡
ℒ𝑝𝑒𝑟𝑐𝐻1 : 𝐼ℓ∙𝑥𝑡
ℒ𝑎𝑑𝑣1: 𝑦1, 𝑥𝑡
𝑓/𝑥𝑟ℒ1 𝐻1 : 𝑚𝑟, 𝑚𝑡
ℒ1 𝐻2 : 𝑚𝑟, 𝐻1,𝑚𝑥𝑡
ℒ1 𝐻3 : 𝑥𝑟′, 𝑥𝑡
ℒ𝑝𝑒𝑟𝑐(𝐻3): 𝐼ℓ∙𝑥𝑟′ , 𝐼ℓ∙𝑥𝑡
ℒ𝑎𝑑𝑣3: 𝑦3, 𝑥𝑠′/𝑥𝑡
ℒ𝑎𝑑𝑣4: 𝑦4, 𝑥𝑔/𝑥𝑡
ℒ𝑝𝑜𝑖𝑠𝑠𝑜𝑛−𝑝𝑒𝑟𝑐𝐻4 : 𝑥𝑡, 𝑚𝑡, 𝑝𝑎𝑠𝑡𝑒(𝑥𝑟′, 𝑥𝑡)
Fig. 12. Architectural schematics of the replacement networks with their generation and training dataflows.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
Although face transfers precede face swaps, today there are very few works that use deep learning
for this task. However, we note that face a transfer is equivalent to performing self-reenactment on
a face swapped portrait. Terefore, high quality face a transfers can be achieved by combining a
method from Section 4.1 and Section 5.1.
In 2018, the authors of proposed DepthNets: an unsupervised network for capturing facial
landmarks and translating the pose from one identity to another. Te authors use a Siamese network
to predict a transformation matrix that maps the xs’s 3D facial landmarks to the corresponding 2D
landmarksofxt. A3Drenderer(OpenGL)isthenusedtowarpx ⟨f ⟩
tothesourceposelt,andthecomposition is reﬁned using a CycleGAN. Since warping is involved, the approach is sensitive to occlusions.
Later in 2019, the authors of proposed a self-supervised network which can change the
identity of an object within an image. Teir ED disentangles the identity from an objects pose using
a novel disentanglement loss. Furthermore to handle misaligned poses, an L1 loss is computed using
a pixel mapped version of xд to xs (using the weights of the identity encoder). Similarly, the authors
of proposed a method disentangled identity transfer. However neither or were
explicitly performed on faces.
COUNTERMEASURES
In general, countermeasures to malicious deepfakes can be categorized as either detection or prevention. We will now brieﬂy discuss each accordingly. A summary and systematization of the deepfake
detection methods can be found in Table 3.
Te subject of image forgery detection is a well researched subject . In our review of detection
methods, we will focus on works which speciﬁcally deal with detecting deepfakes of humans.
Artifact-Specific. Deepfakes ofen generate artifacts which may be subtle to humans, but can
be easily detected using machine learning and forensic analysis. Some works identify deepfakes by
searching for speciﬁc artifacts. We identify seven types of artifacts: Spatial artifacts in blending, environments, and forensics; temporal artifacts in behavior, physiology, synchronization, and coherence.
Blending (spatial). Some artifacts appear where the generated content is blended back into the
frame. To help emphasize these artifacts to a learner, researchers have proposed edge detectors,
quality measures, and frequency analysis . In the authors follow a more explicit
approach to detecting the boundary. Tey trained a CNN network to predict an image’s blending
boundary and a label (real or fake). Instead of using a deepfake dataset, the authors trained their
network on a dataset of face swaps generated by splicing similar faces found through facial landmark
similarity. By doing so, the model has the advantage that is focuses on the blending boundary and
not other artifacts caused by the generative model.
Environment (spatial). Te content of a fake face can be anomalous in context to the rest of the
frame. For example, residuals from face warping processes , lighting , and varying
ﬁdelity can indicate the presence of generated content. In , the authors follow a diﬀerent
approach by contrasting the generated foreground to the (untampered) background using a patch and
pair CNN. Te authors of also contrast the fore/background but enable a network to identify
the distinguishing features automatically. Tey accomplish this by (1) encoding the face and context
(hair and background) with an ED and (2) passing the diﬀerence between the encodings with the
complete image (encoded) to a classiﬁer.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
Forensics (spatial). Several works detect deepfakes by analyzing subtle features and paterns lef
by the model. In and , the authors found that GANs leave unique ﬁngerprints and show
how it is possible to classify the generator given the content, even in the presence of compression and
noise. In the authors analyze a camera’s unique sensor noise (PRNU) to detect pasted content.
To focus on the residuals, the authors of use a two stream ED to encode the color image and
a frequency enhanced version using “Laplacian of Gaussian layers” (LoG). Te two encodings are
then fed through an LSTM which then classiﬁes the video based on a sequence of frames.
Instead of searching for residuals, the authors of search for imperfections and found that
deepfakes tend to have inconsistent head poses. Terefore, they detect deepfakes by predicting and
monitoring facial landmarks. Te authors of had a diﬀerent approach by training classiﬁer
to focus on the imperfections instead of the residuals. Tis was accomplished by using a dataset
generated using a ProGAN instead of other GANs since the ProGAN’s images contain the least
amount of frequency artifacts. In contrast to , the authors in use a network to emphasize the
residuals and suppress the imperfections in a preprocessing step for a classiﬁer. Teir network uses
adaptive convolutional layers that predict residuals to maximize the artifacts’ inﬂuence. Although
this approach may help the network identify artifacts beter, it may not generalize as well to new
types of artifacts.
Behavior (temporal). With large amounts of data on the target, mannerisms and other behaviors
can be monitored for anomalies. For example, in the authors protect world leaders from a wide
variety of deepfake atacks by modeling their recorded stock footage. Recently, the authors of 
showed how behavior can be used with no reference footage of the target. Te approach is to detect
discrepancies in the perceived emotion extracted from the clip’s audio and video content. Te authors
use a custom Siamese network to consider the audio and video emotions when contrasted to real
and fake videos.
Physiology (temporal). In 2014, researchers hypothesized that generated content will lack physiological signals and identiﬁed computer generated faces by monitoring their heart rate . Regarding
deepfakes, monitored blood volume paterns (pulse) under the skin, and took a more robust
approach by monitoring irregular eye blinking paterns. Instead of detecting deepfakes, the authors
of use the pulse signal to help determine the model used to create the deepfake.
Synchronization (temporal). Inconsistencies are also a revealing factor. In and , the
authors noticed that video dubbing atacks can be detected my correlating the speech to landmarks
around the mouth. Later, in , the authors reﬁned the approach by detecting when visemes
(mouth shapes) are inconsistent with the spoken phonemes (uternaces). In particular, they focus
on phonemes where the mouth is fully closed (B, P, M) since deepfakes in the wild tend to fail in
generating these visemes.
Coherence (temporal). As noted in Section 4.1, realistic temporal coherence is challenging to generate, and some authors capitalize on the resulting artifacts to detect the fake content. For example,
 uses an RNN to detect artifacts such as ﬂickers and jiter, and uses an LSTM on the face
region only. In a classiﬁer is trained pairs of sequential frames and in the authors reﬁne
the network’s focus by monitoring the frames’ optical ﬂow. Later the same authors use an LSTM
to predict the next frame, and expose deepfakes when the reconstruction error is high .
Undirected Approaches. Instead of focusing on a speciﬁc artifact, some authors train deep
neural networks as generic classiﬁers, and let the network decide which features to analyze. In
general, researchers have taken one of two approaches: classiﬁcation or anomaly detection.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
Classiﬁcation. In , it was shown that deep neural networks tend to perform beter
than traditional image forensic tools on compressed imagery. Various authors then demonstrated
how standard CNN architectures can eﬀectively detect deepfake videos . In , the
authors train the CNN as a Siamese network using contrasting examples of real and fake images.
In , the authors were concerned that a CNN can only detect the atacks on which they trained. To
close this gap, the authors propose using Hierarchical Memory Network (HMN) architecture which
considers the contents of the face and previously seen faces. Te network encodes the face region
which is then processed using a bidirectional GRU while applying an atention mechanism. Te ﬁnal
encoding is then passed to a memory module, which compares it to recently seen encodings and
makesaprediction. Later, in , theauthorsuseanensembleapproachandleveragethepredictions
of seven deepfake CNNs by passing their predicitons to a meta classifer. Doing so produces results
which are more robust (fewer false positives) than using any single model. In , the authors tried a
variety of diﬀerent classic spatio-temproal networks and feature extractors as a baseline for temporal
deepfake detection. Tey found that a 3D CNN, which looks at multiple frames at once, out performs
both recurrent networks and a the state of the art ID3 architecture.
To localize the tampered areas, some works train networks to predicting masks learned from a
ground truth dataset, or by mapping the neural activations back to the raw image .
In general, we note that the use of classiﬁers to detect deepfakes is problematic since an atacker
can evade detection via adversarial machine learning. We will discuss this issue further in Section 7.2.
Anomaly Detection. In contrast to classiﬁcation, anomaly detection models are trained on the
normal data and then detect outliers during deployment. By doing so, these methods do not make
assumptions on how the atacks look and thus generalize beter to unknown creation methods.
Te authors of follow this approach by measuring the neural activation (coverage) of a face
recognition network. By doing so, the model is able to overcome noise and other distortions, by
obtaining a stronger signal from than just using the raw pixels. Similarly, in a one-class VAE
is trained to used to reconstruct real images. Ten, for new images, an anomaly score is computed
by taking the MSE between mean component of the encoded image and the mean component of the
reconstructed image. Alternatively, the authors of measure an input’s embedding distance to
real samples using an ED’s latent space. Te diﬀerence between these works is that and rely
on a model’s inability to process unknown paterns while contrasts the model’s representations.
Instead of using a neural network directly, the authors of use a state of the art atribution
based conﬁdence metric (ABC). To detect a fake image, the ABC is used to determine if the image
ﬁts the training distribution of a pretrained face recognition network (e.g., VGG).
Prevention & Mitigation
Data Provenance. To prevent deepfakes, some have suggested that data provenance of multimedia
should be tracked through distributed ledgers and blockchain networks . In the authors
suggest that the content should be ranked by participants and AI. In contrast, proposes that the
content should authenticated and managed as a global ﬁle system over Etherium smart contracts.
Counter Attacks. To combat deepfakes, the authors of show how adversarial machine learning can be used to disrupt and corrupt deepfake networks. Te authors perform adversarial machine
learning to add crafed noise perturbations to x, which prevents deepfake technologies from locating
a proper face in x. In a diﬀerent approach, the authors of use adversarial noise to change the
identity of the face so that web crawlers will not be able ﬁnd the image of t to train their model.
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
Table 3. Summary of Deepfake Detection Models
Type Modality Content
Eval. Dataset
Performance*
Reenactment
Replacement
Indicates Aﬀected Area
Input Resolution
DeepfakeTIMIT 
DFFD 
FaceForensics 
FaceForensics++ 
Celeb-DF 
Other Deepfake DB
 2017
 2018 • • • •
 2018 • •
• SVM, Kmeans…
 2019 • • • •
Classic ML
 2019 • •
 2018 • • • •
 2018 • •
 2018 • • • •
Capsule-CNN
 2018 • • •
 2018 • •
 2018 • • •
 2018 • • • •
 2018 • •
 2019 • •
 2019 • • • •
 2019 • • • •
CNN AE GAN •
 2019 • • • •
CNN+Atention •
 2019 • • • •
 2019
 2019 • • • •
 2019 • • • •
 2019 • • • •
 2019 • • • •
 2019 • • •
 2019 • •
 2019 • • • •
 2019 • •
[? ] 2019 •
 2019 •
 2019 •
Deep Learning
 2019 • • •
 2019 • • • •
SVM+VGGnet
 2019 • • • •
 2020 ◦• •
20.86 0.86
 2020 • • •
 2020 • • •
 2020 • •
 2020 • • •
CNN ResNet
Prec.= 0.93
 2020 • • •
 2020 • •
 2020 • •
 2020 • •
 2020 • • •
Siamese CNN
• TPR=0.91
 2020 • • •
 2020 • •
 2020 • • •
 2020 • • •
ABC-ResNet
 2019
Statistics
Statistics & Steganalysis
 2019 • • •
*Only the best reported performance, averaged over the test datasets, is displayed to capture the ‘best-case’ scenario.
DISCUSSION
The Creation of Deepfakes
Trade-oﬀs Between the Methodologies. In general, there is a diﬀerent cost and payoﬀfor
each deepfake creation method. However, the most eﬀective and threatening deepfakes are those
which are (1) the most practical to implement [Training Data, Execution Speed, and Accessibility]
and (2) are the most believable to the victim [Qality]:
Data vs Qality. Models trained on numerous samples of the target ofen yield beter results (e.g.,
 ). For example, in 2017, produced an extremely believable
reenactment of Obama which exceeds the quality of recent works. However, these models require
many hours footage for training, and are therefore are only suitable for exposed targets such as
actors, CEOs, and political leaders. An atacker who wants to commit defamation, impersonation,
or a scam on an arbitrary individual will need to use a many-to-many or few-shot approach. On
the other hand, most of these methods rely on a single reference of t and are therefore prone to
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
generating artifacts. Tis is because the model must ‘imagine’ missing information (e.g., diﬀerent
poses and occlusions). Terefore, approaches which provide the model with a limited number of
reference samples strike the best balance between data and quality.
Speed vs Qality. Te trade-oﬀbetween these aspects depends on whether the atack is online
(interactive) or oﬄine (stored media). Social engineering atacks involving deepfakes are likely
to be online and thus require real-time speeds. However, high resolution models have many
parameters and sometimes use several networks (e.g., ) and some process multiple frames to
provide temporal coherence (e.g., ). Other methods may be slowed down due to their
pre/post-processing steps, such as warping , UV mapping or segmentation prediction
 , and the use of reﬁnement networks . To the best of
our knowledge, and are the only papers which claim to generate real time
deepfakes, yet they subjectively tend to be blurry or distort the face. Regardless, a victim is likely
fall for an imperfect deepfake in a social engineering atack when placed under pressure in a false
pretext . Moreover, it is likely that an atacker will implement a complex method at a lower
resolution to speed up the frame rate. In which case, methods that have texture artifacts would be
preferredoverthosewhichproduceshapeoridentityﬂaws(e.g., vs ). Foratacksthatare
not real-time (e.g, fake news), resolution and ﬁdelity is critical. In these cases, works that produce
high quality images and videos with temporal coherence are the best candidates (e.g., ).
Availability vs Qality. We also note that availability and reproducibility are key factors in the
proliferation of new technologies. Works that publish their code and datasets online (e.g.,
 ) are more likely to be used by researchers and criminals compared
to those which are unavailable or require highly speciﬁc
or private datasets . Tis is because the payoﬀin implementing a paper is minor
compared to using a functional and eﬀective method available online. Of course, this does not
include state-actors who have plenty of time and funding.
We have also observed that approaches which augment a network’s inputs with synthetic ones
produce beter results in terms of quality and stability. For example, by rotating limbs ,
reﬁning rendered heads , providing warped imagery 
and UV maps . Tis is because the provided contextual information reduces the
problem’s complexity for the neural network.
Given these considerations, in our opinion, the most signiﬁcant and available deepfake technologies today are for facial reenactment because of it’s eﬃciency and practicality; for mouth
reenactment because of its quality; and for face replacement because its high ﬁdelity and wide
spread use. However, this is a subjective opinion based on the samples provided online and in the respectivepapers. Acomparativeresearchstudy, wherethemethodsaretrainedonthesamedatasetand
evaluated by a number of people is necessary to determine the best quality deepfake in each category.
ResearchTrends. Overthelastfewyearstherehasbeenashiftowardsidentityagnosticmodels and high resolution deepfakes. Some notable advancements include (1) unpaired self-supervised
training techniques to reduce the amount of initial training data, (2) one/few-shot learning which enables identity thef with a single proﬁle picture, (3) improvements of face quality and identity through
AdaIN layers, disentanglement, and pix2pixHD network components, (4) ﬂuid and realistic videos
through temporal discriminators and optical ﬂow prediction, and (5) the mitigation of boundary
artifacts by using secondary networks to blend composites into seamless imagery (e.g., ).
Another large advancement in this domain was the use of perceptual loss on a pre-trained VGG
Face recognition network. Te approach boosts the facial quality signiﬁcantly, and as a result, has
been adopted in popular online deepfake tools . Another advancement being adopted is the
use of a network pipeline. Instead of enforcing a set of global losses on a single network, a pipeline of
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
networks is used where each network is tasked with a diﬀerent responsibility (conversion, generation,
occlusions, blending, etc.) Tis give more control over the ﬁnal output and has been able to mitigate
most of the challenges mention in Section 3.7.
Current Limitations. Aside from quality, there are a few limitations with the current deepfake technologies. First, for reenactment, content is always driven and generated with a frontal pose.
Tis limits the reenactment to a very static performance. Today, this is avoided by face swapping the
identity onto a lookalike’s body, but a good match is not always possible and this approach has limited
ﬂexibility. Second, reenactments and replacements depend on the driver’s performance to deliver the
identity’s personality. We believe that next generation deepfakes will utilize videos of the target to
stylize the generated content with the expected expressions and mannerisms. Tis will enable a much
more automatic process of creating believable deepfakes. Finally, a new trend is real-time deepfakes.
Works such as have achieved real-time deepfakes at 30fps. Although real-time deepfakes
are an enabler for phishing atacks, the realism is not quite there yet. Other limitations include the
coherent rendering of hair, teeth, tongues, shadows, and the ability to render the target’s hands
(especially when touching the face). Regardless, deepfakes are already very convincing and
are improving at a rapid rate. Terefore, it is important that we focus on eﬀective countermeasures.
The Deepfake Arms Race
Likeanybatleincybersecurity,thereisanarmsracebetweentheatackeranddefender. Inoursurvey,
we observed that the majority deepfake detection algorithms assume a static game with the adversary:
Tey are either focused on identifying a speciﬁc artifact, or do not generalize well to new distributions
and unseen atacks . Moreover, based on the recent benchmark of , we observe that the performance of state-of-the-art detectors are decreasing rapidly as the quality of the deepfakes improve.
Concretely, the three most recent benchmark datasets (DFD by Google , DFDC by Facebook ,
and Celeb-DF by ) were released within one month of each other at the end of 2019. However, the
deepfake detectors only achieved an AUC of 0.86,0.76, and 0.66 on each of them respectively. Even
a false alarm rate of 0.001 is far too low considering the millions of images published online daily.
EvadingArtifact-basedDetectors. Toevadeanartifact-baseddetector,theadversaryonlyneedsto
mitigateasingleﬂawtoevadedetection. Forexample,G cangeneratethebiologicalsignalsmonitored
by by adding a discriminator which monitors these signals. To avoid anomalies in extensive
the neuron activation , the adversary can add a loss which minimizes neuron coverage. Methods
which detect abnormal poses and mannerisms can be evaded by reenacting the entire head and by
learning the mannerisms from the same databases. Models which identify blurred content are
aﬀected by noise and sharpening GANs , and models which search for the boundary where the
face was blended in do not work on deepfakes passed through reﬁner networks,
which use in-painting, or those which output full frames (e.g., ).
Finally, solutions which search for forensic evidence can be evaded (or at least raise the
false alarm rate) by passing xд through ﬁlters, or by performing physical replication or compression.
Evading Deep Learning Classiﬁers. Tere are a number of detection methods which apply deep
learning directly to the task of deepfake detection (e.g., ). However, an adversary can
use adversarial machine learning to evade detection by adding small perturbations toxд. Advances in
adversarial machine learning has shown that these atacks transfer across multiple models regardless
of the training data used . Recent works have shown how these atacks not only work on
deepfakes classiﬁers but also work with no knowledge of the classiﬁer or it’s training set .
Moving Forward. Nevertheless, deepfakes are still imperfect, and these methods oﬀer a modest
defense for the time being. Furthermore, these works play an important role in understanding the
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mirsky, et al.
currentlimitationsofdeepfakes,andraisethediﬃcultythresholdformalicioususers. Atsomepoint,it
may become too time-consuming and resource-intensive a common atacker to create a good-enough
fake to evade detection. However, we argue that solely relying on the development of content-based
countermeasures is not sustainable and may lead to a reactive arms-race. Terefore, we advocate for
more out-of-band approaches for detecting a preventing deepfakes. For example, the establishment
of content provenance and authenticity frameworks for online videos , and proactive
defenses such as the use of adversarial machine learning to protect content from tampering .
Deepfakes in other Domains
In this survey, we put a focus on human reenactment and replacement atacks; the type of deepfakes
which has made the largest impact so far . However, deepfakes extend beyond human visuals,
and have spread many other domains. In healthcare, the authors of showed how deepfakes can
be used to inject tor remove medical evidence in CT and MRI scan for insurance fraud, disruption, and
physical harm. In it was shown how one’s voice can be cloned with only ﬁve seconds of audio,
and in Sept. 2019 a CEO was scammed out of $250K via a voice clone deepfake . Te authors
of have shown how deep learning can generate realistic human ﬁngerprints that can unlock
multiple users’ devices. In it was shown how deepfakes can be applied to ﬁnancial records to
evade the detection of auditors. Finally, it has been shown how deepfakes of news articles can be
generated and that deepfake tweets exist as well .
Teseexamplesdemonstratethatdeepfakesarenotjustatacktoolsformisinformation,defamation,
and propaganda, but also sabotage, fraud, scams, obstruction of justice, and potentially many more.
What’s on the Horizon
We believe that in the coming years, we will see more deepfakes being weaponized for monetization.
Te technology has proven itself in humiliation, misinformation, and defamtion atacks. Moreover,
the tools are becoming more practical and eﬃcient . Terefore, is seems natural that malicious
users will ﬁnd ways to use the technology for a proﬁt. As a result, we expect to see an increase in
deepfake phishing atacks and scams targeting both companies and individuals.
As the technology matures, real-time deepfakes will become increasingly realistic. Terefore, we
can expect that the technology will be used by hacking groups to perform reconnaissance as part
of an APT, and by state actors to perform espionage and sabotage by reenacting of oﬃcials or family
To keep ahead of the game, we must be proactive and consider the adversary’s next step, not
just the weaknesses of the current atacks. We suggest that more work be done on evaluating the
theoretical limits of these atacks. For example, by ﬁnding a bound on a model’s delay can help detect
real-time atacks such as , and determining the limits of GANs like can help us devise the
appropriate strategies. As mentioned earlier, we recommend further research on solutions which
do not require analyzing the content itself. Moreover, we believe it would be beneﬁcial for future
works to explore the weaknesses and limitations of current deepfakes detectors. By identifying and
understanding these vulnerabilities, researchers will be able to develop stronger countermeasures.
CONCLUSION
Notalldeepfakesaremalicious. However, becausethetechnologymakesitsoeasytocreatebelievable
media, malicious users are exploiting it to perform atacks. Tese atacks are targeting individuals
and causing psychological, political, monetary, and physical harm. As time goes on, we expect to
see these malicious deepfakes spread to many other modalities and industries.
In this survey we focused on reenactment and replacement deepfakes of humans. We provided
a deep review of how these technologies work, the diﬀerences between their architectures, and
ACM Computing Surveys, Vol. 1, No. 1, Article 1. Publication date: January 2020.
The Creation and Detection of Deepfakes: A Survey
what is being done to detect them. We hope this information will be helpful to the community in
understanding and preventing malicious deepfakes.