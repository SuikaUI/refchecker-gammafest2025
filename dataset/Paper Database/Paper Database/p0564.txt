The Annals of Statistics
2001, Vol. 29, No. 5, 1189–1232
1999 REITZ LECTURE
GREEDY FUNCTION APPROXIMATION:
A GRADIENT BOOSTING MACHINE1
By Jerome H. Friedman
Stanford University
Function estimation/approximation is viewed from the perspective of
numerical optimization in function space, rather than parameter space. A
connection is made between stagewise additive expansions and steepestdescent minimization. A general gradient descent “boosting” paradigm is
developed for additive expansions based on any ﬁtting criterion. Speciﬁc
algorithms are presented for least-squares, least absolute deviation, and
Huber-M loss functions for regression, and multiclass logistic likelihood
for classiﬁcation. Special enhancements are derived for the particular case
where the individual additive components are regression trees, and tools
for interpreting such “TreeBoost” models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable
procedures for both regression and classiﬁcation, especially appropriate for
mining less than clean data. Connections between this approach and the
boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.
1. Function estimation.
In the function estimation or “predictive learning” problem, one has a system consisting of a random “output” or “response”
variable y and a set of random “input” or “explanatory” variables x = x1    
xn. Using a “training” sample yi xiN
1 of known y x-values, the goal is to
obtain an estimate or approximation 
Fx, of the function F∗x mapping
x to y, that minimizes the expected value of some speciﬁed loss function
Ly Fx over the joint distribution of all y x-values,
F∗= arg min
F Ey xLy Fx = arg min
F ExEyLy Fx  x 
Frequently employed loss functions Ly F include squared-error y −F2
and absolute error y −F for y ∈R1 (regression) and negative binomial loglikelihood, log1 + e−2yF, when y ∈−1 1 (classiﬁcation).
A common procedure is to restrict Fx to be a member of a parameterized
class of functions Fx
P, where P = P1 P2    is a ﬁnite set of parameters
whose joint values identify individual class members. In this article we focus
Received May 1999; revised April 2001.
1Supported in part by CSIRO Mathematical and Information Science, Australia; Department
of Energy Contract DE-AC03-76SF00515; and NSF Grant DMS-97-64431.
AMS 2000 subject classiﬁcations. 62-02, 62-07, 62-08, 62G08, 62H30, 68T10.
Key words and phrases. Function estimation, boosting, decision trees, robust nonparametric
regression.
J. H. FRIEDMAN
on “additive” expansions of the form
The (generic) function hx
a in (2) is usually a simple parameterized function of the input variables x, characterized by parameters a = a1 a2   .
The individual terms differ in the joint values am chosen for these parameters.
Such expansions (2) are at the heart of many function approximation methods such as neural networks [Rumelhart, Hinton, and Williams ], radial
basis functions [Powell ], MARS [Friedman ], wavelets [Donoho
 ] and support vector machines [Vapnik ]. Of special interest here
is the case where each of the functions hx
am is a small regression tree,
such as those produced by CART TM [Breiman, Friedman, Olshen and Stone
 ]. For a regression tree the parameters am are the splitting variables,
split locations and the terminal node means of the individual trees.
1.1. Numerical optimization.
In general, choosing a parameterized model
P changes the function optimization problem to one of parameter optimization,
P∗= arg min
P = Ey xLy Fx
F∗x = Fx
For most Fx
P and L, numerical optimization methods must be applied to
solve (3). This often involves expressing the solution for the parameters in the
where p0 is an initial guess and pmM
1 are successive increments (“steps” or
“boosts”), each based on the sequence of preceding steps. The prescription for
computing each step pm is deﬁned by the optimization method.
1.2. Steepest-descent.
Steepest-descent is one of the simplest of the frequently used numerical minimization methods. It deﬁnes the increments pmM
(4) as follows. First the current gradient gm is computed:
gm = gjm =
GREEDY FUNCTION APPROXIMATION
The step is taken to be
pm = −ρmgm
ρm = arg min
Pm−1 −ρgm
The negative gradient −gm is said to deﬁne the “steepest-descent” direction
and (5) is called the “line search” along that direction.
2. Numerical optimization in function space.
Here we take a “nonparametric” approach and apply numerical optimization in function space.
That is, we consider Fx evaluated at each point x to be a “parameter” and
seek to minimize
F = Ey xLy Fx = ExEyLy Fx  x 
or equivalently,
φFx = EyLy Fx  x
at each individual x, directly with respect to Fx. In function space there are
an inﬁnite number of such parameters, but in data sets (discussed below) only
a ﬁnite number FxiN
1 are involved. Following the numerical optimization
paradigm we take the solution to be
where f0x is an initial guess, and fmxM
are incremental functions
(“steps” or “boosts”) deﬁned by the optimization method.
For steepest-descent,
fmx = −ρmgmx
Fx=Fm−1x
∂EyLy Fx  x
Fx=Fm−1x
J. H. FRIEDMAN
Assuming sufﬁcient regularity that one can interchange differentiation and
integration, this becomes
gmx = Ey
∂Ly Fx
Fx=Fm−1x
The multiplier ρm in (6) is given by the line search
ρm = arg min
Ey xLy Fm−1x −ρgmx
3. Finite data.
This nonparametric approach breaks down when the joint
distribution of y x is estimated by a ﬁnite data sample yi xiN
1 . In this
case Ey·  x cannot be estimated accurately by its data value at each xi, and
even if it could, one would like to estimate F∗x at x values other than the
training sample points. Strength must be borrowed from nearby data points
by imposing smoothness on the solution. One way to do this is to assume a
parameterized form such as (2) and do parameter optimization as discussed in
Section 1.1 to minimize the corresponding data based estimate of expected loss,
β′m a′mM
In situations where this is infeasible one can try a “greedy stagewise”
approach. For m = 1 2     M,
βm am = arg min
Lyi Fm−1xi + βhxi
Fmx = Fm−1x + βmhx
Note that this stagewise strategy is different from stepwise approaches that
readjust previously entered terms when new ones are added.
In signal processing this stagewise strategy is called “matching pursuit”
[Mallat and Zhang ] where Ly F is squared-error loss and the
are called basis functions, usually taken from an overcomplete
waveletlike dictionary. In machine learning, (9), (10) is called “boosting” where
y ∈−1 1 and Ly F is either an exponential loss criterion e−yF [Freund
and Schapire , Schapire and Singer ] or negative binomial loglikelihood [Friedman, Hastie and Tibshirani (here after reffered to as
FHT00)]. The function hx
a is called a “weak learner” or “base learner” and
is usually a classiﬁcation tree.
Suppose that for a particular loss Ly F and/or base learner hx
solution to (9) is difﬁcult to obtain. Given any approximator Fm−1x, the
function βmhx
am (9), (10) can be viewed as the best greedy step toward
the data-based estimate of F∗x (1), under the constraint that the step “direction” hx
am be a member of the parameterized class of functions hx
can thus be regarded as a steepest descent step (6) under that constraint. By
GREEDY FUNCTION APPROXIMATION
construction, the data-based analogue of the unconstrained negative gradient (7),
−gmxi = −
∂Lyi Fxi
Fx=Fm−1x
gives the best steepest-descent step direction −gm = −gmxiN
1 in the Ndimensional data space at Fm−1x. However, this gradient is deﬁned only at
the data points xiN
1 and cannot be generalized to other x-values. One possibility for generalization is to choose that member of the parameterized class
am that produces hm = hxi
1 most parallel to −gm ∈RN. This is
a most highly correlated with −gmx over the data distribution. It
can be obtained from the solution
am = arg min
−gmxi −βhxi
This constrained negative gradient hx
am is used in place of the unconstrained one −gmx (7) in the steepest-descent strategy. Speciﬁcally, the line
search (8) is performed
ρm = arg min
Lyi Fm−1xi + ρhxi
and the approximation updated,
Fmx = Fm−1x + ρmhx
Basically, instead of obtaining the solution under a smoothness constraint
(9), the constraint is applied to the unconstrained (rough) solution by ﬁtting hx
a to the “pseudoresponses”  ˜yi = −gmxiN
i=1 (7). This permits the
replacement of the difﬁcult function minimization problem (9) by least-squares
function minimization (11), followed by only a single parameter optimization
based on the original criterion (12). Thus, for any hx
a for which a feasible
least-squares algorithm exists for solving (11), one can use this approach to
minimize any differentiable loss Ly F in conjunction with forward stagewise additive modeling. This leads to the following (generic) algorithm using
steepest-descent.
Algorithm 1 (Gradient Boost).
1. F0x = arg minρ
i=1 Lyi ρ
2. For m = 1 to M do:
3. ˜yi = −
∂LyiFxi
Fx=Fm−1x i = 1 N
4. am = arg mina β
i=1 ˜yi −βhxi
5. ρm = arg minρ
i=1 Lyi Fm−1xi + ρhxi
6. Fmx = Fm−1x + ρmhx
J. H. FRIEDMAN
end Algorithm
Note that any ﬁtting criterion that estimates conditional expectation (given
x) could in principle be used to estimate the (smoothed) negative gradient (7)
at line 4 of Algorithm 1. Least-squares (11) is a natural choice owing to the
superior computational properties of many least-squares algorithms.
In the special case where y ∈−1 1 and the loss function Ly F depends
on y and F only through their product Ly F = LyF, the analogy of boosting (9), (10) to steepest-descent minimization has been noted in the machine
learning literature [Ratsch, Onoda and Muller , Breiman ]. Duffy
and Helmbold elegantly exploit this analogy to motivate their GeoLev
and GeoArc procedures. The quantity yF is called the “margin” and the
steepest-descent is performed in the space of margin values, rather than the
space of function values F. The latter approach permits application to more
general loss functions where the notion of margins is not apparent. Drucker
 employs a different strategy of casting regression into the framework of
classiﬁcation in the context of the AdaBoost algorithm [Freund and Schapire
4. Applications: additive modeling.
In this section the gradient boosting strategy is applied to several popular loss criteria: least-squares (LS), least
absolute deviation (LAD), Huber (M), and logistic binomial log-likelihood (L).
The ﬁrst serves as a “reality check”, whereas the others lead to new boosting
algorithms.
4.1. Least-squares regression.
Here Ly F = y −F2/2. The pseudoresponse in line 3 of Algorithm 1 is ˜yi = yi −Fm−1xi. Thus, line 4 simply ﬁts
the current residuals and the line search (line 5) produces the result ρm =
βm, where βm is the minimizing β of line 4. Therefore, gradient boosting on
squared-error loss produces the usual stagewise approach of iteratively ﬁtting
the current residuals.
Algorithm 2 (LS Boost).
F0x = ¯y
For m = 1 to M do:
˜yi = yi −Fm−1xi
ρm am = arg minaρ
i=1 ˜yi −ρhxi
Fmx = Fm−1x + ρmhx
end Algorithm
4.2. Least absolute deviation (LAD) regression.
For the loss function
Ly F = y −F, one has
∂Lyi Fxi
Fx=Fm−1x
= signyi −Fm−1xi
GREEDY FUNCTION APPROXIMATION
This implies that hx
a is ﬁt (by least-squares) to the sign of the current
residuals in line 4 of Algorithm 1. The line search (line 5) becomes
ρm = arg min
yi −Fm−1xi −ρhxi
yi −Fm−1xi
yi −Fm−1xi
wi = hxi
Here medianW· is the weighted median with weights wi. Inserting these
results [(13), (14)] into Algorithm 1 yields an algorithm for least absolute
deviation boosting, using any base learner hx
4.3. Regression trees.
Here we consider the special case where each base
learner is an J-terminal node regression tree [Breiman, Friedman, Olshen
and Stone ]. Each regression tree model itself has the additive form
bj1x ∈Rj
Here RjJ
1 are disjoint regions that collectively cover the space of all joint
values of the predictor variables x. These regions are represented by the terminal nodes of the corresponding tree. The indicator function 1· has the
value 1 if its argument is true, and zero otherwise. The “parameters” of this
base learner (15) are the coefﬁcients bjJ
1 , and the quantities that deﬁne the
boundaries of the regions RjJ
1 . These are the splitting variables and the
values of those variables that represent the splits at the nonterminal nodes of
the tree. Because the regions are disjoint, (15) is equivalent to the prediction
rule: if x ∈Rj then hx = bj.
For a regression tree, the update at line 6 of Algorithm 1 becomes
Fmx = Fm−1x + ρm
bjm1x ∈Rjm
Here RjmJ
1 are the regions deﬁned by the terminal nodes of the tree at
the mth iteration. They are constructed to predict the pseudoresponses  ˜yiN
(line 3) by least-squares (line 4). The bjm are the corresponding least-squares
coefﬁcients,
bjm = avexi∈Rjm ˜yi
The scaling factor ρm is the solution to the “line search” at line 5.
J. H. FRIEDMAN
The update (16) can be alternatively expressed as
Fmx = Fm−1x +
γjm1x ∈Rjm
with γjm = ρmbjm. One can view (17) as adding J separate basis functions
at each step 1x ∈RjmJ
1 , instead of a single additive one as in (16). Thus,
in this case one can further improve the quality of the ﬁt by using the optimal coefﬁcients for each of these separate basis functions (17). These optimal
coefﬁcients are the solution to
1 = arg min
yi Fm−1xi +
γj1x ∈Rjm
Owing to the disjoint nature of the regions produced by regression trees, this
reduces to
γjm = arg min
Lyi Fm−1xi + γ
This is just the optimal constant update in each terminal node region, based
on the loss function L, given the current approximation Fm−1x.
For the case of LAD regression (18) becomes
γjm = medianxi∈Rjmyi −Fm−1xi
which is simply the median of the current residuals in the jth terminal node at
the mth iteration. At each iteration a regression tree is built to best predict the
sign of the current residuals yi −Fm−1xi, based on a least-squares criterion.
Then the approximation is updated by adding the median of the residuals in
each of the derived terminal nodes.
Algorithm 3 (LAD TreeBoost).
F0x = medianyiN
For m = 1 to M do:
˜yi = signyi −Fm−1xi i = 1 N
1 = J-terminal node tree ˜yi xiN
γjm = medianxi∈Rjmyi −Fm−1xi j = 1 J
Fmx = Fm−1x +
j=1 γjm1x ∈Rjm
end Algorithm
This algorithm is highly robust. The trees use only order information on
the individual input variables xj, and the pseudoresponses ˜yi (13) have only
two values, ˜yi ∈−1 1. The terminal node updates are based on medians.
GREEDY FUNCTION APPROXIMATION
An alternative approach would be to build a tree to directly minimize the loss
criterion,
treemx = arg
J-node tree
yi −Fm−1xi −treexi
Fmx = Fm−1xi + treemx
However, Algorithm 3 is much faster since it uses least-squares to induce the
trees. Squared-error loss is much more rapidly updated than mean absolute
deviation when searching for splits during the tree building process.
4.4. M-Regression.
M-regression techniques attempt resistance to longtailed error distributions and outliers while maintaining high efﬁciency for
normally distributed errors. We consider the Huber loss function [Huber
y −F ≤δ
δy −F −δ/2
y −F > δ.
Here the pseudoresponse is
∂Lyi Fxi
Fx=Fm−1x
 yi −Fm−1xi
yi −Fm−1xi ≤δ,
δ · signyi −Fm−1xi
yi −Fm−1xi > δ,
and the line search becomes
ρm = arg min
Lyi Fm−1xi + ρhxi
with L given by (19). The solution to (19), (20) can be obtained by standard
iterative methods [see Huber ].
The value of the transition point δ deﬁnes those residual values that are
considered to be “outliers,” subject to absolute rather than squared-error loss.
An optimal value will depend on the distribution of y −F∗x, where F∗is
the true target function (1). A common practice is to choose the value of δ to
be the α-quantile of the distribution of y −F∗x, where 1 −α controls the
breakdown point of the procedure. The “breakdown point” is the fraction of
observations that can be arbitrarily modiﬁed without seriously degrading the
quality of the result. Since F∗x is unknown one uses the current estimate
Fm−1x as an approximation at the mth iteration. The distribution of y −
Fm−1x is estimated by the current residuals, leading to
δm = quantileαyi −Fm−1xiN
With regression trees as base learners we use the strategy of Section 4.3,
that is, a separate update (18) in each terminal node Rjm. For the Huber loss
J. H. FRIEDMAN
(19) the solution to (18) can be approximated by a single step of the standard
iterative procedure [Huber ] starting at the median
˜rjm = medianxi∈Rjmrm−1xi
where rm−1xiN
1 are the current residuals
rm−1xi = yi −Fm−1xi
The approximation is
γjm = ˜rjm+
signrm−1xi−˜rjm·minδmabsrm−1xi−˜rjm
where Njm is the number of observations in the jth terminal node. This gives
the following algorithm for boosting regression trees based on Huber loss (19).
Algorithm 4 M TreeBoost.
F0x = medianyiN
For m = 1 to M do:
rm−1xi = yi −Fm−1xi i = 1 N
δm =quantileα
rm−1xi
 rm−1xi
rm−1xi ≤δm
δm · signrm−1xi
rm−1xi > δm
 i = 1 N
1 = J-terminal node tree ˜yi xiN
˜rjm = medianxi∈Rjmrm−1xi, j = 1 J
γjm = ˜rjm +
xi∈Rjmsign rm−1xi −˜rjm · minδm absrm−1xi −˜rjm,
Fmx = Fm−1x +
j=1 γjm1x ∈Rjm
end Algorithm
According to the motivations underlying robust regression, this algorithm
should have properties similar to that of least-squares boosting (Algorithm 2)
for normally distributed errors, and similar to that of least absolute deviation
regression (Algorithm 3) with very long-tailed distributions. For error distributions with only moderately long tails it can have performance superior to
both (see Section 6.2).
4.5. Two-class logistic regression and classiﬁcation.
Here the loss function
is negative binomial log-likelihood (FHT00)
Ly F = log1 + exp−2yF
y ∈−1 1
 Pry = 1  x
Pry = −1  x
GREEDY FUNCTION APPROXIMATION
The pseudoresponse is
∂Lyi Fxi
Fx=Fm−1x
= 2yi/1 + exp2yiFm−1xi
The line search becomes
ρm = arg min
log1 + exp−2yiFm−1xi + ρhxi
With regression trees as base learners we again use the strategy (Section 4.3)
of separate updates in each terminal node Rjm:
γjm = arg min
log1 + exp−2yiFm−1xi + γ
There is no closed-form solution to (23). Following FHT00, we approximate it
by a single Newton–Raphson step. This turns out to be
 ˜yi 2 − ˜yi
with ˜yi given by (22). This gives the following algorithm for likelihood gradient
boosting with regression trees.
Algorithm 5 (LK TreeBoost).
2 log 1+ ¯y
For m = 1 to M do:
˜yi = 2yi/1 + exp2yiFm−1xi, i = 1 N
1 = J-terminal node tree ˜yi xiN
xi∈Rjm ˜yi/
xi∈Rjm  ˜yi2 − ˜yi, j = 1 J
Fmx = Fm−1x +
j=1 γjm1x ∈Rjm
end Algorithm
The ﬁnal approximation FMx is related to log-odds through (21). This can
be inverted to yield probability estimates
Pry = 1  x = 1/1 + e−2FMx
Pry = −1  x = 1/1 + e2FMx
These in turn can be used for classiﬁcation,
ˆyx = 2 · 1c−1 1p+x > c1 −1p−x −1
where c ˆy y is the cost associated with predicting ˆy when the truth is y.
J. H. FRIEDMAN
4.5.1. Inﬂuence trimming.
The empirical loss function for the two-class
logistic regression problem at the mth iteration is
φmρ a =
log1 + exp−2yiFm−1xi · exp−2yiρhxi
If yiFm−1xi is very large, then (24) has almost no dependence on ρhxi
for small to moderate values near zero. This implies that the ith observation
yi xi has almost no inﬂuence on the loss function, and therefore on its
ρm am = arg min
ρ a φmρ a
This suggests that all observations yi xi for which yiFm−1xi is relatively
very large can be deleted from all computations of the mth iteration without
having a substantial effect on the result. Thus,
wi = exp−2yiFm−1xi
can be viewed as a measure of the “inﬂuence” or weight of the ith observation
on the estimate ρmhx
More generally, from the nonparametric function space perspective of
Section 2, the parameters are the observation function values FxiN
inﬂuence on an estimate to changes in a “parameter” value Fxi (holding all
the other parameters ﬁxed) can be gauged by the second derivative of the loss
function with respect to that parameter. Here this second derivative at the
mth iteration is  ˜yi2 − ˜yi with ˜yi given by (22). Thus, another measure of
the inﬂuence or “weight” of the ith observation on the estimate ρmhx
the mth iteration is
wi =  ˜yi2 − ˜yi
Inﬂuence trimming deletes all observations with wi-values less than wlα,
where lα is the solution to
Here wiN
are the weights wiN
arranged in ascending order. Typical
values are α ∈005 02 . Note that inﬂuence trimming based on (25), (27)
is identical to the “weight trimming” strategy employed with Real AdaBoost,
whereas (26), (27) is equivalent to that used with LogitBoost, in FHT00. There
it was seen that 90% to 95% of the observations were often deleted without
sacriﬁcing accuracy of the estimates, using either inﬂuence measure. This
results in a corresponding reduction in computation by factors of 10 to 20.
GREEDY FUNCTION APPROXIMATION
4.6. Multiclass logistic regression and classiﬁcation.
Here we develop a
gradient-descent boosting algorithm for the K-class problem. The loss function is
Lyk FkxK
yk log pkx
where yk = 1(class = k) ∈0 1, and pkx = Pryk = 1  x. Following
FHT00, we use the symmetric multiple logistic transform
Fkx = log pkx −1
or equivalently
pkx = expFkx
expFlx
Substituting (30) into (28) and taking ﬁrst derivatives one has
∂Lyil FlxiK
Flx=Fl m−1xK
= yik −pk m−1xi
where pk m−1x is derived from Fk m−1x through (30). Thus, K-trees are
induced at each iteration m to predict the corresponding current residuals for
each class on the probability scale. Each of these trees has J-terminal nodes,
with corresponding regions RjkmJ
j=1. The model updates γjkm corresponding
to these regions are the solution to
γjkm = arg min
yik Fk m−1xi +
γjk1xi ∈Rjm
where φyk Fk = −yk log pk from (28), with Fk related to pk through (30).
This has no closed form solution. Moreover, the regions corresponding to the
different class trees overlap, so that the solution does not reduce to a separate
calculation within each region of each tree in analogy with (18). Following
FHT00, we approximate the solution with a single Newton–Raphson step,
using a diagonal approximation to the Hessian. This decomposes the problem
into a separate calculation for each terminal node of each tree. The result is
γjkm = K −1
xi∈Rjkm ˜yik
xi∈Rjkm  ˜yik1 − ˜yik
This leads to the following algorithm for K-class logistic gradient boosting.
Algorithm 6 (LK TreeBoost).
Fk0x = 0, k = 1 K
For m = 1 to M do:
pkx = expFkx/
l=1 expFlx, k = 1 K
J. H. FRIEDMAN
For k = 1 to K do:
˜yik = yik −pkxi, i = 1 N
j=1 = J-terminal node tree ˜yik xiN
γjkm = K−1
xi∈Rjkm ˜yik
xi∈Rjkm  ˜yik1− ˜yik, j = 1 J
Fkmx = Fk m−1x +
j=1 γjkm1x ∈Rjkm
end Algorithm
The ﬁnal estimates FkMxK
1 can be used to obtain corresponding probability estimates pkMxK
through (30). These in turn can be used for
classiﬁcation
ˆkx = arg min
ck k′pk′Mx
where ck k′ is the cost associated with predicting the kth class when the
truth is k′. Note that for K = 2, Algorithm 6 is equivalent to Algorithm 5.
Algorithm 6 bears a close similarity to the K-class LogitBoost procedure
of FHT00, which is based on Newton–Raphson rather than gradient descent
in function space. In that algorithm K trees were induced, each using corresponding pseudoresponses
˜yik = K −1
yik −pkxi
pkxi1 −pkxi
and a weight
wkxi = pkxi1 −pkxi
applied to each observation  ˜yik xi. The terminal node updates were
xi∈Rjkm wkxi ˜yik
xi∈Rjkm wkxi
which is equivalent to (32). The difference between the two algorithms is the
splitting criterion used to induce the trees and thereby the terminal regions
The least-squares improvement criterion used to evaluate potential splits
of a currently terminal region R into two subregions Rl Rr is
i2Rl Rr =
 ¯yl −¯yr2
where ¯yl, ¯yr are the left and right daughter response means respectively,
and wl, wr are the corresponding sums of the weights. For a given split,
using (31) with unit weights, or (33) with weights (34), give the same values for ¯yl, ¯yr. However, the weight sums wl, wr are different. Unit weights
(LK TeeBoost) favor splits that are symmetric in the number of observations in
GREEDY FUNCTION APPROXIMATION
each daughter node, whereas (34) (LogitBoost) favors splits for which the sums
of the currently estimated response variances varyik = pkxi1 −pkxi
are more equal.
LK TreeBoost has an implementation advantage in numerical stability.
LogitBoost becomes numerically unstable whenever the value of (34) is close
to zero for any observation xi, which happens quite frequently. This is a consequence of the difﬁculty that Newton–Raphson has with vanishing second
derivatives. Its performance is strongly affected by the way this problem is
handled (see FHT00, page 352). LK TreeBoost has such difﬁculties only when
(34) is close to zero for all observations in a terminal node. This happens much
less frequently and is easier to deal with when it does happen.
Inﬂuence trimming for the multiclass procedure is implemented in the
same way as that for the two-class case outlined in Section 4.5.1. Associated
with each “observation” yik xi is an inﬂuence wik =  ˜yik1 − ˜yik which is
used for deleting observations (27) when inducing the kth tree at the current
iteration m.
5. Regularization.
In prediction problems, ﬁtting the training data too
closely can be counterproductive. Reducing the expected loss on the training
data beyond some point causes the population-expected loss to stop decreasing and often to start increasing. Regularization methods attempt to prevent
such “overﬁtting” by constraining the ﬁtting procedure. For additive expansions (2) a natural regularization parameter is the number of components M.
This is analogous to stepwise regression where the hx
are considered explanatory variables that are sequentially entered. Controlling the value
of M regulates the degree to which expected loss on the training data can be
minimized. The best value for M can be estimated by some model selection
method, such as using an independent “test” set, or cross-validation.
Regularizing by controlling the number of terms in the expansion places an
implicit prior belief that “sparse” approximations involving fewer terms are
likely to provide better prediction. However, it has often been found that regularization through shrinkage provides superior results to that obtained by
restricting the number of components [Copas ]. In the context of additive models (2) constructed in a forward stagewise manner (9), (10), a simple
shrinkage strategy is to replace line 6 of the generic algorithm (Algorithm 1)
Fmx = Fm−1x + ν · ρmhx
and making the corresponding equivalent changes in all of the speciﬁc algorithms (Algorithms 2–6). Each update is simply scaled by the value of the
“learning rate” parameter ν.
Introducing shrinkage into gradient boosting (36) in this manner provides
two regularization parameters, the learning rate ν and the number of components M. Each one can control the degree of ﬁt and thus affect the best
value for the other one. Decreasing the value of ν increases the best value
for M. Ideally one should estimate optimal values for both by minimizing a
J. H. FRIEDMAN
model selection criterion jointly with respect to the values of the two parameters. There are also computational considerations; increasing the size of M
produces a proportionate increase in computation.
We illustrate this ν–M trade-off through a simulation study. The training
sample consists of 5000 observations yi xi with
yi = F∗xi + εi
The target function F∗x, x ∈R10, is randomly generated as described in
Section 6.1. The noise ε was generated from a normal distribution with zero
mean, and variance adjusted so that
2ExF∗x −medianxF∗x
giving a signal-to-noise ratio of 2/1. For this illustration the base learner
a is taken to be an 11-terminal node regression tree induced in a best-
ﬁrst manner (FHT00). A general discussion of tree size choice appears in
Section 7.
Figure 1 shows the lack of ﬁt (LOF) of LS TreeBoost, LAD TreeBoost, and
L2 TreeBoost as a function of number of terms (iterations) M, for several
values of the shrinkage parameter ν ∈10 025 0125 006. For the ﬁrst
two methods, LOF is measured by the average absolute error of the estimate
FMx relative to that of the optimal constant solution
ExF∗x −
ExF∗x −medianxF∗x
For logistic regression the y-values were obtained by thresholding at the
median of F∗x over the distribution of x-values; F∗xi values greater than
the median were assigned yi = 1; those below the median were assigned
yi = −1. The Bayes error rate is thus zero, but the decision boundary is
fairly complicated. There are two LOF measures for L2 TreeBoost; minus
twice log-likelihood (“deviance”) and the misclassiﬁcation error rate Ex1y ̸=
FMx . The values of all LOF measures were computed by using an
independent validation data set of 10,000 observations.
As seen in Figure 1, smaller values of the shrinkage parameter ν (more
shrinkage) are seen to result in better performance, although there is a diminishing return for the smallest values. For the larger values, behavior characteristic of overﬁtting is observed; performance reaches an optimum at some
value of M and thereafter diminishes as M increases beyond that point. This
effect is much less pronounced with LAD TreeBoost, and with the error rate
criterion of L2 TreeBoost. For smaller values of ν there is less overﬁtting, as
would be expected.
Although difﬁcult to see except for ν = 1, the misclassiﬁcation error rate
(lower right panel) continues to decrease well after the logistic likelihood has
reached its optimum (lower left panel). Thus, degrading the likelihood by
overﬁtting actually improves misclassiﬁcation error rate. Although perhaps
GREEDY FUNCTION APPROXIMATION
Performance of three gradient boosting algorithms as a function of number of iterations
M. The four curves correspond to shrinkage parameter values of ν ∈10 025 0125 006 and
are in that order (top to bottom) at the extreme right of each plot.
counterintuitive, this is not a contradiction; likelihood and error rate measure
different aspects of ﬁt quality. Error rate depends only on the sign of 
whereas likelihood is affected by both its sign and magnitude. Apparently,
overﬁtting degrades the quality of the magnitude estimate without affecting
(and sometimes improving) the sign. Thus, misclassiﬁcation error is much less
sensitive to overﬁtting.
Table 1 summarizes the simulation results for several values of ν including
those shown in Figure 1. Shown for each ν-value (row) are the iteration number
at which the minimum LOF was achieved and the corresponding minimizing
value (pairs of columns).
J. H. FRIEDMAN
Iteration number giving the best ﬁt and the best ﬁt value for several shrinkage parameter
ν-values, with three boosting methods
LS AFMx
LAD AFMx
L2 −2 log like
L2 error rate
The ν–M trade-off is clearly evident; smaller values of ν give rise to larger
optimal M-values. They also provide higher accuracy, with a diminishing
return for ν < 0125. The misclassiﬁcation error rate is very ﬂat for M  200,
so that optimal M-values for it are unstable.
Although illustrated here for just one target function and base learner (11terminal node tree), the qualitative nature of these results is fairly universal.
Other target functions and tree sizes (not shown) give rise to the same behavior. This suggests that the best value for ν depends on the number of iterations
M. The latter should be made as large as is computationally convenient or
feasible. The value of ν should then be adjusted so that LOF achieves its minimum close to the value chosen for M. If LOF is still decreasing at the last
iteration, the value of ν or the number of iterations M should be increased,
preferably the latter. Given the sequential nature of the algorithm, it can easily be restarted where it ﬁnished previously, so that no computation need be
repeated. LOF as a function of iteration number is most conveniently estimated using a left-out test sample.
As illustrated here, decreasing the learning rate clearly improves performance, usually dramatically. The reason for this is less clear. Shrinking the
model update (36) at each iteration produces a more complex effect than direct
proportional shrinkage of the entire model
Fνx = ¯y + ν ·  
FMx −¯y
FMx is the model induced without shrinkage. The update ρmhx
at each iteration depends on the speciﬁc sequence of updates at the previous
iterations. Incremental shrinkage (36) produces very different models than
global shrinkage (38). Empirical evidence (not shown) indicates that global
shrinkage (38) provides at best marginal improvement over no shrinkage, far
from the dramatic effect of incremental shrinkage. The mystery underlying
the success of incremental shrinkage is currently under investigation.
6. Simulation studies.
The performance of any function estimation
method depends on the particular problem to which it is applied. Important
characteristics of problems that affect performance include training sample
size N, true underlying “target” function F∗x (1), and the distribution of
GREEDY FUNCTION APPROXIMATION
the departures, ε, of y  x from F∗x. For any given problem, N is always
known and sometimes the distribution of ε is also known, for example when y
is binary (Bernoulli). When y is a general real-valued variable the distribution
of ε is seldom known. In nearly all cases, the nature of F∗x is unknown.
In order to gauge the value of any estimation method it is necessary to accurately evaluate its performance over many different situations. This is most
conveniently accomplished through Monte Carlo simulation where data can
be generated according to a wide variety of prescriptions and resulting performance accurately calculated. In this section several such studies are presented
in an attempt to understand the properties of the various Gradient TreeBoost
procedures developed in the previous sections. Although such a study is far
more thorough than evaluating the methods on just a few selected examples,
real or simulated, the results of even a large study can only be regarded as
suggestive.
6.1. Random function generator.
One of the most important characteristics of any problem affecting performance is the true underlying target
function F∗x (1). Every method has particular targets for which it is most
appropriate and others for which it is not. Since the nature of the target function can vary greatly over different problems, and is seldom known, we compare the merits of regression tree gradient boosting algorithms on a variety
of different randomly generated targets. Each one takes the form
The coefﬁcients al20
are randomly generated from a uniform distribution
al  U−1 1 . Each glzl is a function of a randomly selected subset, of size nl,
of the n-input variables x. Speciﬁcally,
zl = xPlj
where each Pl is a separate random permutation of the integers 1 2     n.
The size of each subset nl is itself taken to be random, nl = 15 + r, with
r being drawn from an exponential distribution with mean λ = 2. Thus, the
expected number of input variables for each glzl is between three and four.
However, most often there will be fewer than that, and somewhat less often,
more. This reﬂects a bias against strong very high-order interaction effects.
However, for any realized F∗x there is a good chance that at least a few
of the 20 functions glzl will involve higher-order interactions. In any case,
F∗x will be a function of all, or nearly all, of the input variables.
Each glzl is an nl-dimensional Gaussian function
glzl = exp
2zl −µlTVlzl −µl
where each of the mean vectors µl20
1 is randomly generated from the same
distribution as that of the input variables x. The nl × nl covariance matrix Vl
J. H. FRIEDMAN
is also randomly generated. Speciﬁcally,
Vl = UlDlUT
where Ul is a random orthonormal matrix (uniform on Haar measure) and
Dl = diag d1l · · · dnll. The square roots of the eigenvalues are randomly generated from a uniform distribution
djl  Ua b , where the limits a b depend
on the distribution of the input variables x.
For all of the studies presented here, the number of input variables was
taken to be n = 10, and their joint distribution was taken to be standard normal x  N0 I. The eigenvalue limits were a = 01 and b = 20. Although the
tails of the normal distribution are often shorter than that of data encountered
in practice, they are still more realistic than uniformly distributed inputs often
used in simulation studies. Also, regression trees are immune to the effects of
long-tailed input variable distributions, so shorter tails gives a relative advantage to competitors in the comparisons.
In the simulation studies below, 100 target functions F∗x were randomly
generated according to the above prescription (39), (40). Performance is evaluated in terms of the distribution of approximation inaccuracy [relative approximation error (37) or misclassiﬁcation risk] over these different targets. This
approach allows a wide variety of quite different target functions to be generated in terms of the shapes of their contours in the ten-dimensional input
space. Although lower order interactions are favored, these functions are not
especially well suited to additive regression trees. Decision trees produce tensor product basis functions, and the components glzl of the targets F∗x
are not tensor product functions. Using the techniques described in Section 8,
visualizations of the dependencies of the ﬁrst randomly generated function on
some of its more important arguments are shown in Section 8.3.
Although there are only ten input variables, each target is a function of
all of them. In many data mining applications there are many more than ten
inputs. However, the relevant dimensionalities are the intrinsic dimensionality of the input space, and the number of inputs that actually inﬂuence the
output response variable y. In problems with many input variables there are
usually high degrees of collinearity among many of them, and the number of
roughly independent variables (approximate intrinsic dimensionality) is much
smaller. Also, target functions often strongly depend only on a small subset of
all of the inputs.
6.2. Error distribution.
In this section, LS TreeBoost, LAD TreeBoost,
and M TreeBoost are compared in terms of their performance over the 100
target functions for two different error distributions. Best-ﬁrst regression trees
with 11 terminal nodes were used with all algorithms. The breakdown parameter for the M TreeBoost was set to its default value α = 09. The learning
rate parameter (36) was set to ν = 01 for all TreeBoost procedures in all of
the simulation studies.
GREEDY FUNCTION APPROXIMATION
One hundred data sets yi xiN
1 were generated according to
yi = F∗xi + εi
where F∗x represents each of the 100 target functions randomly generated
as described in Section 6.1. For the ﬁrst study, the errors εi were generated
from a normal distribution with zero mean, and variance adjusted so that
Eε = ExF∗x −medianxF∗x
giving a 1/1 signal-to-noise ratio. For the second study the errors were generated from a “slash” distribution, εi = s · u/v, where u  N0 1 and v 
U0 1 . The scale factor s is adjusted to give a 1/1 signal-to-noise ratio (41).
The slash distribution has very thick tails and is often used as an extreme to
test robustness. The training sample size was taken to be N = 7500, with 5000
used for training, and 2500 left out as a test sample to estimate the optimal
number of components M. For each of the 100 trials an additional validation
sample of 5000 observations was generated (without error) to evaluate the
approximation inaccuracy (37) for that trial.
The left panels of Figure 2 show boxplots of the distribution of approximation inaccuracy (37) over the 100 targets for the two error distributions for each
of the three methods. The shaded area of each boxplot shows the interquartile range of the distribution with the enclosed white bar being the median.
Distribution of absolute approximation error (left panels) and error relative to the best
(right panels) for LS TreeBoost, LAD TreeBoost and M TreeBoost for normal and slash error distributions. LS TreeBoost, performs best with the normal error distribution. LAD TreeBoost and
M TreeBoost both perform well with slash errors. M TreeBoost is very close to the best for both
error distributions. Note the use of logarithmic scale in the lower right panel.
J. H. FRIEDMAN
The outer hinges represent the points closest to (plus/minus) 1.5 interquartile range units from the (upper/lower) quartiles. The isolated bars represent
individual points outside this range (outliers).
These plots allow the comparison of the overall distributions, but give no
information concerning relative performance for individual target functions.
The right two panels of Figure 2 attempt to provide such a summary. They
show distributions of error ratios, rather than the errors themselves. For each
target function and method, the error for the method on that target is divided
by the smallest error obtained on that target, over all of the methods (here
three) being compared. Thus, for each of the 100 trials, the best method
receives a value of 1.0 and the others receive a larger value. If a particular method was best (smallest error) for all 100 target functions, its resulting
distribution (boxplot) would be a point mass at the value 1.0. Note that the
logarithm of this ratio is plotted in the lower right panel.
From the left panels of Figure 2 one sees that the 100 targets represent a
fairly wide spectrum of difﬁculty for all three methods; approximation errors
vary by over a factor of two. For normally distributed errors LS TreeBoost
is the superior performer, as might be expected. It had the smallest error
in 73 of the trials, with M TreeBoost best the other 27 times. On average
LS TreeBoost was 0.2% worse than the best, M TreeBoost 0.9% worse, and
LAD TreeBoost was 7.4% worse than the best.
With slash-distributed errors, things are reversed. On average the approximation error for LS TreeBoost was 0.95, thereby explaining only 5% target
variation. On individual trials however, it could be much better or much
worse. The performance of both LAD TreeBoost and M TreeBoost was much
better and comparable to each other. LAD TreeBoost was best 32 times and
M TreeBoost 68 times. On average LAD TreeBoost was 4.1% worse than the
best, M TreeBoost 1.0% worse, and LS TreeBoost was 364.6% worse that the
best, over the 100 targets.
The results suggest that of these three, M TreeBoost is the method of
choice. In both the extreme cases of very well-behaved (normal) and very
badly behaved (slash) errors, its performance was very close to that of the
best. By comparison, LAD TreeBoost suffered somewhat with normal errors,
and LS TreeBoost was disastrous with slash errors.
6.3. LS TreeBoost versus MARS.
All Gradient TreeBoost algorithms produce piecewise constant approximations. Although the number of such pieces
is generally much larger than that produced by a single tree, this aspect of the
approximating function 
FMx might be expected to represent a disadvantage
with respect to methods that provide continuous approximations, especially
when the true underlying target F∗x (1) is continuous and fairly smooth.
All of the randomly generated target functions (39), (40) are continuous and
very smooth. In this section we investigate the extent of the piecewise constant
disadvantage by comparing the accuracy of Gradient TreeBoost with that of
MARS [Friedman ] over these 100 targets. Like TreeBoost, MARS produces a tensor product based approximation. However, it uses continuous func-
GREEDY FUNCTION APPROXIMATION
tions as the product factors, thereby producing a continuous approximation.
It also uses a more involved (stepwise) strategy to induce the tensor products.
Since MARS is based on least-squares ﬁtting, we compare it to LS Tree-
Boost using normally distributed errors, again with a 1/1 signal-to-noise ratio
(41). The experimental setup is the same as that in Section 6.2. It is interesting
to note that here the performance of MARS was considerably enhanced by
using the 2500 observation test set for model selection, rather than its default
generalized cross-validation (GCV) criterion [Friedman ].
The top left panel of Figure 3 compares the distribution of MARS average
absolute approximation errors, over the 100 randomly generated target functions (39), (40), to that of LS TreeBoost from Figure 2. The MARS distribution
is seen to be much broader, varying by almost a factor of three. There were
many targets for which MARS did considerably better than LS TreeBoost,
and many for which it was substantially worse. This further illustrates the
fact that the nature of the target function strongly inﬂuences the relative
performance of different methods. The top right panel of Figure 3 shows the
distribution of errors, relative to the best for each target. The two methods
exhibit similar performance based on average absolute error. There were a
number of targets where each one substantially outperformed the other.
The bottom two panels of Figure 3 show corresponding plots based on root
mean squared error. This gives proportionally more weight to larger errors
in assessing lack of performance. For LS TreeBoost the two error measures
have close to the same values for all of the 100 targets. However with MARS,
root mean squared error is typically 30% higher than average absolute error.
This indicates that MARS predictions tend to be either very close to, or far
from, the target. The errors from LS TreeBoost are more evenly distributed.
It tends to have fewer very large errors or very small errors. The latter may
be a consequence of the piecewise constant nature of the approximation which
makes it difﬁcult to get arbitrarily close to very smoothly varying targets with
approximations of ﬁnite size. As Figure 3 illustrates, relative performance can
be quite sensitive to the criterion used to measure it.
These results indicate that the piecewise constant aspect of TreeBoost
approximations is not a serious disadvantage. In the rather pristine environment of normal errors and normal input variable distributions, it is competitive with MARS. The advantage of the piecewise constant approach is robustness; speciﬁcally, it provides immunity to the adverse effects of wide tails and
outliers in the distribution of the input variables x. Methods that produce
continuous approximations, such as MARS, can be extremely sensitive to such
problems. Also, as shown in Section 6.2, M TreeBoost (Algorithm 4) is nearly
as accurate as LS TreeBoost for normal errors while, in addition, being highly
resistant to output y-outliers. Therefore in data mining applications where
the cleanliness of the data is not assured and x- and/or y-outliers may be
present, the relatively high accuracy, consistent performance and robustness
of M TreeBoost may represent a substantial advantage.
J. H. FRIEDMAN
Distribution of approximation error (left panels) and error relative to the best (right panels)
for LS TreeBoost and MARS. The top panels are based on average absolute error, whereas the
bottom ones use root mean squared error. For absolute error the MARS distribution is wider,
indicating more frequent better and worse performance than LS TreeBoost. MARS performance as
measured by root mean squared error is much worse, indicating that it tends to more frequently
make both larger and smaller errors than LS TreeBoost.
6.4. LK TreeBoost versus K-class LogitBoost and AdaBoost.MH.
section the performance of LK TreeBoost is compared to that of K-class LogitBoost (FHT00) and AdaBoost.MH [Schapire and Singer ] over the 100
randomly generated targets (Section 6.1). Here K = 5 classes are generated
by thresholding each target at its 0.2, 0.4, 0.6 and 0.8 quantiles over the distribution of input x-values. There are N = 7500 training observations for each
trial (1500 per class) divided into 5000 for training and 2500 for model selection (number of iterations, M). An independently generated validation sample
of 5000 observations was used to estimate the error rate for each target. The
GREEDY FUNCTION APPROXIMATION
Bayes error rate is zero for all targets, but the induced decision boundaries can
become quite complicated, depending on the nature of each individual target
function F∗x. Regression trees with 11 terminal nodes were used for each
Figure 4 shows the distribution of error rate (left panel), and its ratio to
the smallest (right panel), over the 100 target functions, for each of the three
methods. The error rate of all three methods is seen to vary substantially over
these targets. LK TreeBoost is seen to be the generally superior performer. It
had the smallest error for 78 of the trials and on average its error rate was
0.6% higher than the best for each trial. LogitBoost was best on 21 of the
targets and there was one tie. Its error rate was 3.5% higher than the best on
average. AdaBoost.MH was never the best performer, and on average it was
15% worse than the best.
Figure 5 shows a corresponding comparison, with the LogitBoost and
AdaBoost.MH procedures modiﬁed to incorporate incremental shrinkage (36),
with the shrinkage parameter set to the same (default) value ν = 01 used with
LK TreeBoost. Here one sees a somewhat different picture. Both LogitBoost
and AdaBoost.MH beneﬁt substantially from shrinkage. The performance of
all three procedures is now nearly the same, with LogitBoost perhaps having a slight advantage. On average its error rate was 0.5% worse that the
best; the corresponding values for LK TreeBoost and AdaBoost.MH were 2.3%
and 3.9%, respectively. These results suggest that the relative performance of
these methods is more dependent on their aggressiveness, as parameterized
by learning rate, than on their structural differences. LogitBoost has an addi-
Distribution of error rate on a ﬁve-class problem (left panel) and error rate relative to the
best (right panel) for LK TreeBoost, LogitBoost, and AdaBoost.MH. LK TreeBoost exhibits superior
performance.
J. H. FRIEDMAN
Distribution of error rate on a ﬁve-class problem (left panel), and error rate relative to the
best (right panel), for LK TreeBoost, and with proportional shrinkage applied to LogitBoost and
RealAdaBoost. Here the performance of all three methods is similar.
tional internal shrinkage associated with stabilizing its pseudoresponse (33)
when the denominator is close to zero (FHT00, page 352). This may account
for its slight superiority in this comparison. In fact, when increased shrinkage is applied to LK TreeBoost (ν = 005) its performance improves, becoming
identical to that of LogitBoost shown in Figure 5. It is likely that when the
shrinkage parameter is carefully tuned for each of the three methods, there
would be little performance differential between them.
7. Tree boosting.
The GradientBoost procedure (Algorithm 1) has two
primary metaparameters, the number of iterations M and the learning rate
parameter ν (36). These are discussed in Section 5. In addition to these, there
are the metaparameters associated with the procedure used to estimate the
base learner hx
a. The primary focus of this paper has been on the use of
best-ﬁrst induced regression trees with a ﬁxed number of terminal nodes, J.
Thus, J is the primary metaparameter of this base learner. The best choice for
its value depends most strongly on the nature of the target function, namely
the highest order of the dominant interactions among the variables.
Consider an ANOVA expansion of a function
fjkxj xk +
fjklxj xk xl + · · ·
The ﬁrst sum is called the “main effects” component of Fx. It consists of a
sum of functions that each depend on only one input variable. The particular
functions fjxjN
1 are those that provide the closest approximation to Fx
GREEDY FUNCTION APPROXIMATION
under this additive constraint. This is sometimes referred to as an “additive”
model because the contributions of each xj, fjxj, add to the contributions
of the others. This is a different and more restrictive deﬁnition of “additive”
than (2). The second sum consists of functions of pairs of input variables.
They are called the two-variable “interaction effects.” They are chosen so that
along with the main effects they provide the closest approximation to Fx
under the limitation of no more than two-variable interactions. The third sum
represents three-variable interaction effects, and so on.
The highest interaction order possible is limited by the number of input
variables n. However, especially for large n, many target functions F∗x
encountered in practice can be closely approximated by ANOVA decompositions of much lower order. Only the ﬁrst few terms in (42) are required to
capture the dominant variation in F∗x. In fact, considerable success is often
achieved with the additive component alone [Hastie and Tibshirani ].
Purely additive approximations are also produced by the “naive” -Bayes
method [Warner, Toronto, Veasey and Stephenson ], which is often
highly successful in classiﬁcation. These considerations motivated the bias
toward lower-order interactions in the randomly generated target functions
(Section 6.1) used for the simulation studies.
The goal of function estimation is to produce an approximation 
closely matches the target F∗x. This usually requires that the dominant
interaction order of 
Fx be similar to that of F∗x. In boosting regression
trees, the interaction order can be controlled by limiting the size of the individual trees induced at each iteration. A tree with J terminal nodes produces
a function with interaction order at most minJ −1 n. The boosting process is additive, so the interaction order of the entire approximation can be
no larger than the largest among its individual components. Therefore, with
any of the TreeBoost procedures, the best tree size J is governed by the effective interaction order of the target F∗x. This is usually unknown so that
J becomes a metaparameter of the procedure to be estimated using a model
selection criterion such as cross-validation or on a left-out subsample not used
in training. However, as discussed above, it is unlikely that large trees would
ever be necessary or desirable.
Figure 6 illustrates the effect of tree size on approximation accuracy for the
100 randomly generated functions (Section 6.1) used in the simulation studies.
The experimental set-up is the same as that used in Section 6.2. Shown is the
distribution of absolute errors (37) (left panel), and errors relative to the lowest
for each target (right panel), for J ∈2 3 6 11 21. The ﬁrst value J = 2
produces additive main effects components only; J = 3 produces additive and
two-variable interaction terms, and so on. A J terminal node tree can produce
interaction levels up to a maximum of minJ−1 n, with typical values being
less than that, especially when J −1  n.
As seen in Figure 6 the smallest trees J ∈2 3 produce lower accuracy on
average, but their distributions are considerably wider than the others. This
means that they produce more very accurate, and even more very inaccurate,
J. H. FRIEDMAN
Distribution of absolute approximation error (left panel) and error relative to the best
(right panel) for LS TreeBoost with different sized trees, as measured by number of terminal nodes
J. The distribution using the smallest trees J ∈2 3 is wider, indicating more frequent better
and worse performance than with the larger trees, all of which have similar performance.
approximations. The smaller trees, being restricted to low-order interactions,
are better able to take advantage of targets that happen to be of low interaction
level. However, they do quite badly when trying to approximate the highorder interaction targets. The larger trees J ∈6 11 21 are more consistent.
They sacriﬁce some accuracy on low-order interaction targets, but do much
better on the higher-order functions. There is little performance difference
among the larger trees, with perhaps some slight deterioration for J = 21.
The J = 2 trees produced the most accurate approximation eight times; the
corresponding numbers for J ∈3 6 11 21 were 2, 30, 31, 29, respectively.
On average the J = 2 trees had errors 23.2% larger than the lowest for
each target, while the others had corresponding values of 16.4%, 2.4%, 2.2%
and 3.7%, respectively. Higher accuracy should be obtained when the best
tree size J is individually estimated for each target. In practice this can be
accomplished by evaluating the use of different tree sizes with an independent
test data set, as illustrated in Section 9.
8. Interpretation.
In many applications it is useful to be able to interpret the derived approximation 
Fx. This involves gaining an understanding
of those particular input variables that are most inﬂuential in contributing to
its variation, and the nature of the dependence of 
Fx on those inﬂuential
inputs. To the extent that 
Fx at least qualitatively reﬂects the nature of the
target function F∗x (1), such tools can provide information concerning the
underlying relationship between the inputs x and the output variable y. In
this section, several tools are presented for interpreting TreeBoost approxima-
GREEDY FUNCTION APPROXIMATION
tions. Although they can be used for interpreting single decision trees, they
tend to be more effective in the context of boosting (especially small) trees.
These interpretative tools are illustrated on real data examples in Section 9.
8.1. Relative importance of input variables.
Among the most useful descriptions of an approximation 
Fx are the relative inﬂuences Ij, of the
individual inputs xj, on the variation of 
Fx over the joint input variable
distribution. One such measure is
For piecewise constant approximations produced by decision trees, (43) does
not strictly exist and it must be approximated by a surrogate measure that
reﬂects its properties. Breiman, Friedman, Olshen and Stone proposed
t 1vt = j
where the summation is over the nonterminal nodes t of the J-terminal node
tree T, vt is the splitting variable associated with node t, and ˆı2
t is the corresponding empirical improvement in squared error (35) as a result of the
split. The right-hand side of (44) is associated with squared inﬂuence so that
its units correspond to those of (43). Breiman, Friedman, Olshen and Stone
 used (44) directly as a measure of inﬂuence, rather than squared inﬂuence. For a collection of decision trees TmM
1 , obtained through boosting, (44)
can be generalized by its average over all of the trees,
in the sequence.
The motivation for (44), (45) is based purely on heuristic arguments. As a
partial justiﬁcation we show that it produces expected results when applied
in the simplest context. Consider a linear target function
F∗x = a0 +
where the covariance matrix of the inputs is a multiple of the identity
x −¯xx −¯xT
In this case the inﬂuence measure (43) produces
Ij = aj
J. H. FRIEDMAN
Table 2 shows the results of a small simulation study similar to those in
Section 6, but with F∗x taken to be linear (46) with coefﬁcients
aj = −1jj
and a signal-to-noise ratio of 1/1 (41). Shown are the mean and standard
deviation of the values of (44), (45) over ten random samples, all with F∗x
given by (46), (48). The inﬂuence of the estimated most inﬂuential variable
xj∗is arbitrarily assigned the value Ij∗= 100, and the estimated values of
the others scaled accordingly. The estimated importance ranking of the input
variables was correct on every one of the ten trials. As can be seen in Table 2,
the estimated relative inﬂuence values are consistent with those given by (47)
In Breiman, Friedman, Olshen and Stone 1983, the inﬂuence measure (44)
is augmented by a strategy involving surrogate splits intended to uncover the
masking of inﬂuential variables by others highly associated with them. This
strategy is most helpful with single decision trees where the opportunity for
variables to participate in splitting is limited by the size J of the tree in (44). In
the context of boosting, however, the number of splitting opportunities is vastly
increased (45), and surrogate unmasking is correspondingly less essential.
In K-class logistic regression and classiﬁcation (Section 4.6) there are K
(logistic) regression functions FkMxK
k=1, each described by a sequence of
M trees. In this case (45) generalizes to
where Tkm is the tree induced for the kth class at iteration m. The quantity
Ijk can be interpreted as the relevance of predictor variable xj in separating
class k from the other classes. The overall relevance of xj can be obtained by
Estimated mean and standard deviation of input variable
relative inﬂuence for a linear target function
GREEDY FUNCTION APPROXIMATION
averaging over all classes
However, the individual Ijk themselves can be quite useful. It is often the case
that different subsets of variables are highly relevant to different subsets of
classes. This more detailed knowledge can lead to insights not obtainable by
examining only overall relevance.
8.2. Partial dependence plots.
Visualization is one of the most powerful
interpretational tools. Graphical renderings of the value of 
Fx as a function of its arguments provides a comprehensive summary of its dependence
on the joint values of the input variables. Unfortunately, such visualization is
limited to low-dimensional arguments. Functions of a single real-valued variable x 
Fx can be plotted as a graph of the values of 
Fx against each
corresponding value of x. Functions of a single categorical variable can be
represented by a bar plot, each bar representing one of its values, and the bar
height the value of the function. Functions of two real-valued variables can be
pictured using contour or perspective mesh plots. Functions of a categorical
variable and another variable (real or categorical) are best summarized by
a sequence of (“trellis”) plots, each one showing the dependence of 
the second variable, conditioned on the respective values of the ﬁrst variable
[Becker and Cleveland ].
Viewing functions of higher-dimensional arguments is more difﬁcult. It is
therefore useful to be able to view the partial dependence of the approximation
Fx on selected small subsets of the input variables. Although a collection of
such plots can seldom provide a comprehensive depiction of the approximation,
it can often produce helpful clues, especially when 
Fx is dominated by loworder interactions (Section 7).
Let zl be a chosen “target” subset, of size l, of the input variables x,
zl = z1     zl ⊂x1     xn
and z\l be the complement subset
z\l ∪zl = x
The approximation 
Fx in principle depends on variables in both subsets
Fzl  z\l
If one conditions on speciﬁc values for the variables in z\l, then 
Fx can be
considered as a function only of the variables in the chosen subset zl,
Fz\lzl = 
Fzl  z\l
J. H. FRIEDMAN
In general, the functional form of 
Fz\lzl will depend on the particular values
chosen for z\l. If, however, this dependence is not too strong then the average
Flzl = Ez\l 
Fzl  z\l p\lz\l dz\l
can represent a useful summary of the partial dependence of 
Fx on the
chosen variable subset zl. Here p\lz\l is the marginal probability density
p\lz\l =
where px is the joint density of all of the inputs x. This complement marginal
density (52) can be estimated from the training data, so that (51) becomes
Flzl = 1
Fzl  zi\l
In the special cases where the dependence of 
Fx on zl is additive,
Flzl + 
or multiplicative,
Flzl · 
the form of 
Fz\lzl (50) does not depend on the joint values of the complement
variables z\l. Then
Flzl (51) provides a complete description of the nature
of the variation of 
Fx on the chosen input variable subset zl.
An alternative way of summarizing the dependence of 
Fx on a subset zl
is to directly model 
Fx as a function of zl on the training data
Flzl = Ex 
Fx  zl =
Fx pz\l  zl dz\l
However, averaging over the conditional density in (56), rather than the
marginal density in (51), causes
Flzl to reﬂect not only the dependence
Fx on the selected variable subset zl, but in addition, apparent dependencies induced solely by the associations between them and the complement
variables z\l. For example, if the contribution of zl happens to be additive
(54) or multiplicative (55),
Flzl (56) would not evaluate to the corresponding term or factor 
Flzl, unless the joint density px happened to be the
px = plzl · p\lz\l
Partial dependence functions (51) can be used to help interpret models produced by any “black box” prediction method, such as neural networks, support
vector machines, nearest neighbors, radial basis functions, etc. When there are
a large number of predictor variables, it is very useful to have a measure of
GREEDY FUNCTION APPROXIMATION
relevance (Section 8.1) to reduce the potentially large number variables and
variable combinations to be considered. Also, a pass over the data (53) is
required to evaluate each
Flzl for each set of joint values zl of its argument.
This can be time-consuming for large data sets, although subsampling could
help somewhat.
For regression trees based on single-variable splits, however, the partial
dependence of 
Fx on a speciﬁed target variable subset zl (51) is straightforward to evaluate given only the tree, without reference to the data itself
(53). For a speciﬁc set of values for the variables zl, a weighted traversal of
the tree is performed. At the root of the tree, a weight value of 1 is assigned.
For each nonterminal node visited, if its split variable is in the target subset
zl, the appropriate left or right daughter node is visited and the weight is not
modiﬁed. If the node’s split variable is a member of the complement subset
z\l, then both daughters are visited and the current weight is multiplied by
the fraction of training observations that went left or right, respectively, at
that node.
Each terminal node visited during the traversal is assigned the current
value of the weight. When the tree traversal is complete, the value of
is the corresponding weighted average of the 
Fx values over those terminal nodes visited during the tree traversal. For a collection of M regression
trees, obtained through boosting, the results for the individual trees are simply
For purposes of interpretation through graphical displays, input variable
subsets of low cardinality (l ≤2) are most useful. The most informative of
such subsets would likely be comprised of the input variables deemed to be
among the most inﬂuential (44), (45) in contributing to the variation of 
Illustrations are provided in Sections 8.3 and 9.
The closer the dependence of 
Fx on the subset zl is to being additive (54)
or multiplicative (55), the more completely the partial dependence function
Flzl (51) captures the nature of the inﬂuence of the variables in zl on the
derived approximation 
Fx. Therefore, subsets zl that group together those
inﬂuential inputs that have complex [nonfactorable (55)] interactions between
them will provide the most revealing partial dependence plots. As a diagnostic,
Flzl and
Flz\l can be separately computed for candidate subsets. The
value of the multiple correlation over the training data between 
F\lz\l and/or
F\lz\l can be used to gauge the degree of
additivity and/or factorability of 
Fx with respect to a chosen subset zl. As
an additional diagnostic, 
Fz\lzl (50) can be computed for a small number of
z\l-values randomly selected from the training data. The resulting functions of
zl can be compared to
Flzl to judge the variability of the partial dependence
Fx on zl, with respect to changing values of z\l.
In K-class logistic regression and classiﬁcation (Section 4.6) there are K
(logistic) regression functions FkxK
k=1. Each is logarithmically related to
pkx = Pry = k  x through (29). Larger values of Fkx imply higher
J. H. FRIEDMAN
probability of observing class k at x. Partial dependence plots of each Fkx
on variable subsets zl most relevant to that class (49) provide information on
how the input variables inﬂuence the respective individual class probabilities.
8.3. Randomly generated function.
In this section the interpretational
tools described in the preceding two sections are applied to the ﬁrst (of the
100) randomly generated functions (Section 6.1) used for the Monte Carlo
studies of Section 6.
Figure 7 shows the estimated relative importance (44), (45) of the 10 input
predictor variables. Some are seen to be more inﬂuential than others, but no
small subset appears to dominate. This is consistent with the mechanism used
to generate these functions.
Figure 8 displays single variable (l = 1) partial dependence plots (53)
on the six most inﬂuential variables. The hash marks at the base of each
plot represent the deciles of the corresponding predictor variable distribution.
The piecewise constant nature of the approximation is evident. Unlike most
approximation methods, there is no explicit smoothness constraint imposed
upon TreeBoost models. Arbitrarily sharp discontinuities can be accommodated. The generally smooth trends exhibited in these plots suggest that a
smooth approximation best describes this target. This is again consistent with
the way these functions were generated.
Relative importance of the input predictor variables for the ﬁrst randomly generated
function used in the Monte Carlo studies.
GREEDY FUNCTION APPROXIMATION
Single-variable partial dependence plots for the six most inﬂuential predictor variables
for the ﬁrst randomly generated function used in the simulation studies.
Figure 9 displays two-variable (l = 2) partial dependence plots on some
of the more inﬂuential variables. Interaction effects of varying degrees are
indicated among these variable pairs. This is in accordance with the way in
which these target functions were actually generated (39), (40).
Given the general complexity of these generated targets as a function of
their arguments, it is unlikely that one would ever be able to uncover their
complete detailed functional form through a series of such partial dependence
plots. The goal is to obtain an understandable description of some of the important aspects of the functional relationship. In this example the target function
was generated from a known prescription, so that at least qualitatively we can
verify that this is the case here.
9. Real data.
In this section the TreeBoost regression algorithms are
illustrated on two moderate-sized data sets. The results in Section 6.4 suggest
that the properties of the classiﬁcation algorithm LK TreeBoost are very similar to those of LogitBoost, which was extensively applied to data in FHT00.
The ﬁrst (scientiﬁc) data set consists of chemical concentration measurements
on rock samples, and the second (demographic) is sample survey questionnaire
data. Both data sets were partitioned into a learning sample consisting of twothirds of the data, with the remaining data being used as a test sample for
choosing the model size (number of iterations M). The shrinkage parameter
(36) was set to ν = 01.
J. H. FRIEDMAN
Two-variable partial dependence plots on a few of the important predictor variables for
the ﬁrst randomly generated function used in the simulation studies.
9.1. Garnet data.
This data set consists of a sample of N = 13317 garnets
collected from around the world [Grifﬁn, Fisher, Friedman, Ryan and O’ Reilly
 ]. A garnet is a complex Ca–Mg–Fe–Cr silicate that commonly occurs as
a minor phase in rocks making up the earth’s mantle. The variables associated
with each garnet are the concentrations of various chemicals and the tectonic
plate setting where the rock was collected:
TiO2 Cr2O3 FeO MnO MgO CaO Zn Ga Sr Y Zr tec
The ﬁrst eleven variables representing concentrations are real-valued. The
last variable (tec) takes on three categorical values: “ancient stable shields,”
“Proterozoic shield areas,” and “young orogenic belts.” There are no missing
values in these data, but the distribution of many of the variables tend to be
highly skewed toward larger values, with many outliers.
The purpose of this exercise is to estimate the concentration of titanium
(TiO2) as a function of the joint concentrations of the other chemicals and the
tectonic plate index.
GREEDY FUNCTION APPROXIMATION
Average absolute error of LS TreeBoost, LAD TreeBoost, and M TreeBoost on the
garnet data for varying numbers of terminal nodes in the individual trees
Terminal nodes
Table 3 shows the average absolute error in predicting the output y-variable,
relative to the optimal constant prediction,
y −mediany
based on the test sample, for LS TreeBoost, LAD TreeBoost, and M TreeBoost
for several values of the size (number of terminal nodes) J of the constituent
trees. Note that this prediction error measure (58) includes the additive irreducible error associated with the (unknown) underlying target function F∗x
(1). This irreducible error adds same amount to all entries in Table 3. Thus,
differences in those entries reﬂect a proportionally greater improvement in
approximation error (37) on the target function itself.
For all three methods the additive (J = 2) approximation is distinctly inferior to that using larger trees, indicating the presence of interaction effects
(Section 7) among the input variables. Six terminal node trees are seen to be
adequate and using only three terminal node trees is seen to provide accuracy
within 10% of the best. The errors of LAD TreeBoost and M TreeBoost are
smaller than those of LS TreeBoost and similar to each other, with perhaps
M TreeBoost having a slight edge. These results are consistent with those
obtained in the simulation studies as shown in Figures 2 and 6.
Figure 10 shows the relative importance (44), (45) of the 11 input variables
in predicting TiO2 concentration based on the M TreeBoost approximation
using six terminal node trees. Results are very similar for the other models in
Table 3 with similar errors. Ga and Zr are seen to be the most inﬂuential with
MnO being somewhat less important. The top three panels of Figure 11 show
the partial dependence (51) of the approximation 
Fx on these three most
inﬂuential variables. The bottom three panels show the partial dependence
Fx on the three pairings of these variables. A strong interaction effect
between Ga and Zr is clearly evident. 
Fx has very little dependence on
either variable when the other takes on its smallest values. As the value of one
of them is increased, the dependence of 
Fx on the other is correspondingly
ampliﬁed. A somewhat smaller interaction effect is seen between MnO and Zr.
J. H. FRIEDMAN
Relative inﬂuence of the eleven input variables on the target variation for the garnet
data. Ga and Zr are much more inﬂuential that the others.
Partial dependence plots for the three most inﬂuential input variables in the garnet data.
Note the different vertical scales for each plot. There is a strong interaction effect between Zr and
Ga, and a somewhat weaker one between Zr and MnO.
GREEDY FUNCTION APPROXIMATION
Variables for the demographic data
Demographic
Number values
martial status
occupation
years in Bay Area
dual incomes
number in household
number in household<18
householder status
type of home
ethnic classiﬁcation
language in home
9.2. Demographic data.
This data set consists of N = 9409 questionnaires
ﬁlled out by shopping mall customers in the San Francisco Bay Area [Impact
Resources, Inc, Columbus, Ohio ]. Here we use answers to the ﬁrst 14
questions, relating to demographics, for illustration. These questions are listed
in Table 4. The data are seen to consist of a mixture of real and categorical
variables, each with a small numbers of distinct values. There are many missing values.
We illustrate TreeBoost on these data by modeling income as a function of
the other 13 variables. Table 5 shows the average absolute error in predicting
income, relative to the best constant predictor (58), for the three regression
TreeBoost algorithms.
There is little difference in performance among the three methods. Owing
to the highly discrete nature of these data, there are no outliers or long-tailed
distributions among the real-valued inputs or the output y. There is also very
little reduction in error as the constituent tree size J is increased, indicating
Average absolute error of LS TreeBoost, LAD TreeBoost, and M TreeBoost on the
demographic data for varying numbers of terminal nodes in the individual trees
Terminal nodes
J. H. FRIEDMAN
Relative inﬂuence of the 13 input variables on the target variation for the demographic
data. No small group of variables dominate.
lack of interactions among the input variables; an approximation additive in
the individual input variables (J = 2) seems to be adequate.
Figure 12 shows the relative importance of the input variables in predicting
income, based on the (J = 2) LS TreeBoost approximation. There is no small
subset of them that dominates. Figure 13 shows partial dependence plots on
the six most inﬂuential variables. Those for the categorical variables are represented as bar plots, and all plots are centered to have zero mean over the
data. Since the approximation consists of main effects only [ﬁrst sum in (42)],
these plots completely describe the corresponding contributions fjxj of each
of these inputs.
There do not appear to be any surprising results in Figure 13. The dependencies for the most part conﬁrm prior suspicions and suggest that the approximation is intuitively reasonable.
10. Data mining.
As “off the shelf” tools for predictive data mining, the
TreeBoost procedures have some attractive properties. They inherit the favorable characteristics of trees while mitigating many of the unfavorable ones.
Among the most favorable is robustness. All TreeBoost procedures are invariant under all (strictly) monotone transformations of the individual input variables. For example, using xj log xj exj, or xa
j as the jth input variable yields
the same result. Thus, the need for considering input variable transformations
is eliminated. As a consequence of this invariance, sensitivity to long-tailed
GREEDY FUNCTION APPROXIMATION
Partial dependence plots for the six most inﬂuential input variables in the demographic
data. Note the different vertical scales for each plot. The abscissa values for age and education are
codes representing consecutive equal intervals. The dependence of income on age is nonmonotonic
reaching a maximum at the value 5, representing the interval 45–54 years old.
distributions and outliers is also eliminated. In addition, LAD TreeBoost is
completely robust against outliers in the output variable y as well. M Tree-
Boost also enjoys a fair measure of robustness against output outliers.
Another advantage of decision tree induction is internal feature selection.
Trees tend to be quite robust against the addition of irrelevant input variables.
Also, tree-based models handle missing values in a uniﬁed and elegant manner
[Breiman, Friedman, Olshen and Stone ]. There is no need to consider
external imputation schemes. TreeBoost clearly inherits these properties as
The principal disadvantage of single tree models is inaccuracy. This is a
consequence of the coarse nature of their piecewise constant approximations,
especially for smaller trees, and instability, especially for larger trees, and
the fact that they involve predominately high-order interactions. All of these
are mitigated by boosting. TreeBoost procedures produce piecewise constant
approximations, but the granularity is much ﬁner. TreeBoost enhances stability by using small trees and by the effect of averaging over many of them.
The interaction level of TreeBoost approximations is effectively controlled by
limiting the size of the individual constituent trees.
Among the purported biggest advantages of single tree models is interpretability, whereas boosted trees are thought to lack this feature. Small trees
J. H. FRIEDMAN
can be easily interpreted, but due to instability such interpretations should
be treated with caution. The interpretability of larger trees is questionable
[Ripley ]. TreeBoost approximations can be interpreted using partial
dependence plots in conjunction with the input variable relative importance
measure, as illustrated in Sections 8.3 and 9. While not providing a complete
description, they at least offer some insight into the nature of the input–
output relationship. Although these tools can be used with any approximation method, the special characteristics of tree-based models allow their rapid
calculation. Partial dependence plots can also be used with single regression
trees, but as noted above, more caution is required owing to greater instability.
After sorting of the input variables, the computation of the regression Tree-
Boost procedures (LS , LAD , and M TreeBoost) scales linearly with the number of observations N, the number of input variables n and the number of
iterations M. It scales roughly as the logarithm of the size of the constituent
trees J. In addition, the classiﬁcation algorithm LK TreeBoost scales linearly
with the number of classes K; but it scales highly sublinearly with the number of iterations M, if inﬂuence trimming (Section 4.5.1) is employed. As a
point of reference, applying M TreeBoost to the garnet data of Section 9.1
(N = 13317 n = 11 J = 6 M = 500) required 20 seconds on a 933Mh Pentium III computer.
As seen in Section 5, many boosting iterations (M ≃500) can be required
to obtain optimal TreeBoost approximations, based on small values of the
shrinkage parameter ν (36). This is somewhat mitigated by the very small
size of the trees induced at each iteration. However, as illustrated in Figure
1, improvement tends to be very rapid initially and then levels off to slower
increments. Thus, nearly optimal approximations can be achieved quite early
(M ≃100) with correspondingly much less computation. These near-optimal
approximations can be used for initial exploration and to provide an indication
of whether the ﬁnal approximation will be of sufﬁcient accuracy to warrant
continuation. If lack of ﬁt improves very little in the ﬁrst few iterations (say
100), it is unlikely that there will be dramatic improvement later on. If continuation is judged to be warranted, the procedure can be restarted where it left
off previously, so that no computational investment is lost. Also, one can use
larger values of the shrinkage parameter to speed initial improvement for this
purpose. As seen in Figure 1, using ν = 025 provided accuracy within 10% of
the optimal (ν = 01) solution after only 20 iterations. In this case however,
boosting would have to be restarted from the beginning if a smaller shrinkage
parameter value were to be subsequently employed.
The ability of TreeBoost procedures to give a quick indication of potential
predictability, coupled with their extreme robustness, makes them a useful
preprocessing tool that can be applied to imperfect data. If sufﬁcient predictability is indicated, further data cleaning can be invested to render it
suitable for more sophisticated, less robust, modeling procedures.
If more data become available after modeling is complete, boosting can be
continued on the new data starting from the previous solution. This usually
improves accuracy provided an independent test data set is used to monitor
GREEDY FUNCTION APPROXIMATION
improvement to prevent overﬁtting on the new data. Although the accuracy
increase is generally less than would be obtained by redoing the entire analysis
on the combined data, considerable computation is saved.
Boosting on successive subsets of data can also be used when there is insuf-
ﬁcient random access main memory to store the entire data set. Boosting can
be applied to “arcbites” of data [Breiman ] sequentially read into main
memory, each time starting at the current solution, recycling over previous
subsets as time permits. Again, it is crucial to use an independent test set
to stop training on each individual subset at that point where the estimated
accuracy of the combined approximation starts to diminish.
Acknowledgments.
Helpful discussions with Trevor Hastie, Bogdan
Popescu and Robert Tibshirani are gratefully acknowledged.