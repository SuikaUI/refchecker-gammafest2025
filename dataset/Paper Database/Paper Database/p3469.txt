Deep Hashing Network for Efﬁcient Similarity Retrieval∗
Han Zhu, Mingsheng Long, Jianmin Wang and Yue Cao
School of Software, Tsinghua University, Beijing, China
Tsinghua National Laboratory for Information Science and Technology
{zhuhan10,caoyue10}@gmail.com
{mingsheng,jimwang}@tsinghua.edu.cn
Due to the storage and retrieval efﬁciency, hashing has been
widely deployed to approximate nearest neighbor search for
large-scale multimedia retrieval. Supervised hashing, which
improves the quality of hash coding by exploiting the semantic similarity on data pairs, has received increasing attention
recently. For most existing supervised hashing methods for
image retrieval, an image is ﬁrst represented as a vector of
hand-crafted or machine-learned features, followed by another separate quantization step that generates binary codes.
However, suboptimal hash coding may be produced, because
the quantization error is not statistically minimized and the
feature representation is not optimally compatible with the
binary coding. In this paper, we propose a novel Deep Hashing Network (DHN) architecture for supervised hashing, in
which we jointly learn good image representation tailored
to hash coding and formally control the quantization error.
The DHN model constitutes four key components: (1) a subnetwork with multiple convolution-pooling layers to capture
image representations; (2) a fully-connected hashing layer to
generate compact binary hash codes; (3) a pairwise crossentropy loss layer for similarity-preserving learning; and (4)
a pairwise quantization loss for controlling hashing quality.
Extensive experiments on standard image retrieval datasets
show the proposed DHN model yields substantial boosts over
latest state-of-the-art hashing methods.
Introduction
While image big data with large volume and high dimension
are pervasive in search engines and social networks, it has
attracted increasing attention to enable approximate nearest
neighbors (ANN) retrieval of images with both computation
efﬁciency and search quality. An advantageous solution is
hashing methods , which transform highdimensional data into compact binary codes and generate
similar binary codes for similar data items. In this paper, we
focus on learning to hash methods that build data-dependent
hash coding for efﬁcient image retrieval, which have shown
better performance than data-independent hashing methods,
e.g. Locality-Sensitive Hashing (LSH) .
Many learning to hash methods have been proposed to enable efﬁcient ANN search using Hamming distance . All rights reserved.
and Darrell 2009; Gong and Lazebnik 2011; Norouzi and
Blei 2011; Fleet, Punjani, and Norouzi 2012; Liu et al. 2012;
Wang, Kumar, and Chang 2012; Liu et al. 2013; Gong et al.
2013; Yu et al. 2014; Xia et al. 2014; Zhang et al. 2014;
Shen et al. 2015; Lai et al. 2015; Erin Liong et al. 2015).
Hash learning can be divided into unsupervised methods and
supervised methods. While unsupervised methods are more
general and can be trained without semantic labels or relevances, they are restricted by the semantic gap dilemma
 that high-level semantic description
of an object often differs from low-level feature descriptors.
Supervised methods can incorporate semantic labels or relevances to mitigate the semantic gap and improve the hashing
quality, i.e. achieve accurate search with fewer bits of codes.
Recently, deep learning to hash methods have shown that both feature representation
and hash coding can be learned more effectively using deep
neural networks , which can naturally encode any
nonlinear hashing functions. These deep hashing methods
have created state-of-the-art results on many benchmarks.
However, a crucial disadvantage of these deep learning to
hash methods is that the quantization error is not statistically
minimized hence the feature representation is not optimally
compatible with binary hash coding. The continuous relaxation , i.e. solving the discrete optimization of hash codes by more viable continuous optimization,
may give rise to two important issues widely ignored by previous learning to hash work: (1) uncontrollable quantization
error by binarizing continuous embeddings to binary codes,
and (2) large approximation error by adopting ordinary distance between continuous embeddings as the surrogate of
Hamming distance between binary codes. Another potential
limitation is that they do not adopt principled pairwise loss
function to link the pairwise Hamming distances with the
pairwise similarity labels, i.e. to classify whether a data pair
is similar or dissimilar (pairwise classiﬁcation) based on the
pairwise Hamming distances. Therefore, suboptimal hash
coding may be produced by existing deep hashing methods.
In this paper, we simultaneously control the quantization
error and close the gap between Hamming distance and its
approximate distance for learning high-quality hash codes.
To approach this goal, we propose a novel Deep Hashing
Network (DHN) architecture for supervised hashing using a
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)
Bayesian framework. We jointly learn good image representation for hash coding and formally control the quantization
error. The DHN model constitutes four key components: (1)
a sub-network with multiple convolution-pooling layers to
capture image representations; (2) a fully-connected hashing
layer to generate compact binary hash codes; (3) a pairwise
cross-entropy loss layer for similarity-preserving learning;
and (4) a pairwise quantization loss for controlling hashing
quality. Extensive experiments on standard image retrieval
datasets show the proposed DHN model yields substantial
improvements over current state-of-the-art hashing methods.
Related Work
Existing learning to hash methods can be categorized in two
categories: unsupervised hashing and supervised hashing.
Refer to for a comprehensive survey.
Unsupervised hashing methods learn hash functions that
can encode input data points to binary codes only using the
unlabeled training data. Typical learning criteria include reconstruction error minimization , neighborhood preserving as graph-based hashing , and quantization error minimization
as Iterative Quantization (ITQ) .
Supervised hashing explores supervised information (e.g.,
class labels, relative similarity, or relevance feedback) to
learn compact hash coding. Binary Reconstruction Embedding (BRE) pursues hash functions
by minimizing the squared errors between the distances of
data points and the distances of corresponding hash codes.
Minimal Loss Hashing (MLH) and
Hamming Distance Metric Learning learn hash codes by minimizing hingelike loss functions based on relative similarity of data points.
Supervised Hashing with Kernels (KSH) 
is a kernel-based method that builds compact binary codes
by minimizing the Hamming distances on similar pairs and
maximizing the Hamming distances on dissimilar pairs.
Recent revolution in deep learning shows that deep convolutional neural network (CNN) can automatically learn effective image
representations that yield breakthrough performance on general computer vision tasks. Xia et al. proposed CNNH that decomposes the hash learning process into
a stage of learning approximate hash codes, followed by a
deep-network-based stage of simultaneously ﬁne-tuning the
image features and hash functions. Lai et al. improved the
two-stage CNNH by proposing DNNH , a simultaneous feature learning and hash coding deep network
such that image representations and hash codes can improve
each other in the joint learning process. DNNH has created
the latest state-of-the-art results on many benchmarks.
In this work, we further improve DNNH by exploiting two
key problems: (1) control the quantization error in a principled way, and (2) devise a more principled pairwise crossentropy loss to link the pairwise Hamming distances with the
pairwise similarity labels. These two improvements constitute the proposed Deep Hashing Network (DHN) approach.
Deep Hashing Network
In similarity retrieval, we are given a training set of N points
i=1, each represented as D-dimensional feature vector
x ∈RD. Some pairs of points are associated with similarity
labels sij, where sij = 1 implies xi and xj are similar and
sij = 0 indicates xi and xj are dissimilar. Our goal is to
learn nonlinear hashing function f : x →h ∈{−1, 1}K to
encode each point x in compact K-bit hash code h = f(x)
such that the similarity between given pairs is preserved. In
supervised hashing, S = {sij} is usually constructed from
the semantic labels within the data points or the relevance
feedback from click-through data in image retrieval systems.
In this paper, we propose a deep hashing network (DHN)
architecture for hash learning, shown in Figure 1. This architecture accepts input images in a pairwise form (xi, xj, sij)
and processes them through the deep hashing pipeline: (1)
a sub-network with multiple convolution-pooling layers to
extract image representations; (2) a fully-connected hashing
layer to generate compact hash codes; (3) a pairwise crossentropy loss layer for similarity-preserving learning; and (4)
a pairwise quantization loss for controlling hashing quality.
Model Formulation
We start with AlexNet , the deep convolutional neural network (CNN) comprised of ﬁve convolutional layers (conv1–conv5) and three
fully connected layers (fc6–fc8). Each fc layer ℓlearns
a nonlinear mapping zℓ
, where zℓ
is the ℓ-th layer hidden representation of point xi, W ℓand
bℓare the weight and bias parameters of the ℓ-th layer, and
aℓis the activation function, taken as rectiﬁer units (ReLU)
aℓ(x) = max(0, x) for all hidden layers conv1–fc7. For
hash function learning, we replace the fc8 layer of the softmax classiﬁer in the original AlexNet with a new fch layer
of K hidden units, which transforms the fc7 representation
to K-dimensional hash coding by hi = zl
i, where l = 8 is
the total number of layers and zl
i is the hidden representation
of the fch layer. To encourage the fch layer representation
i to be binary codes, we ﬁrst squash its output to be within
[−1, 1] by utilizing the hyperbolic tangent (tanh) activation
al(x) = tanh(x). To guarantee that the fch representation
i will be good hash coding, we must preserve the similarity
between given pairs in S and control the quantization error
of binarizing the hidden representation into binary codes.
In this work, we jointly preserve the pairwise similarity
and control the quantization error in a Bayesian framework.
For a pair of binary codes hi and hj, there exists a nice linear relationship between their Hamming distance distH(·, ·)
and inner product ⟨·, ·⟩: distH (hi, hj) = 1
2 (K −⟨hi, hj⟩).
Hence in the sequel, we will use the inner product as a good
surrogate of the Hamming distance to quantify the pairwise
similarity. Given the pairwise similarity labels S = {sij},
the logarithm Maximum a Posteriori (MAP) estimation of
hash codes H = [h1, . . . , hN] can be derived as follows,
log p (H|S) ∝log p (S|H) p (H)
log p (sij|hi, hj) p (hi) p (hj), (1)
Figure 1: Deep Hashing Network (DHN) with a hash layer fch, a pairwise cross-entropy loss, and a pairwise quantization loss.
where p(S|H) is the likelihood function, and p(H) is the
prior distribution. For each pair, p(sij|hi, hj) is the conditional probability of similarity label sij given hash codes hi
and hj, which is deﬁned as the pairwise logistic function,
p (sij|hi, hj) =
where σ (x) =
1+e−x is the sigmoid function and note that
i. Similar to logistic regression, we can see that the
smaller the Hamming distance distH (hi, hj) is, the larger
the inner product ⟨hi, hj⟩will be, and the larger p (1|hi, hj)
will be, implying that pair hi and hj should be classiﬁed as
“similar”; otherwise, the larger p (0|hi, hj) will be, implying that pair hi and hj should be classiﬁed as “dissimilar”.
Hence, Equation (2) is a reasonable extension of the logistic
regression classiﬁer to the pairwise classiﬁcation scenario,
which is optimal for binary similarity labels sij ∈{0, 1}.
Figure 2: The bimodal Laplacian prior for quantization.
Since discrete optimization of Equation (1) with binary
constraints hi ∈{−1, 1}K is very challenging, for ease of
optimization, continuous relaxation is applied to the binary
constraints, as widely adopted by existing hashing methods
 . However, the continuous relaxation will
give rise to two important issues widely ignored by previous
learning to hash work: (1) uncontrollable quantization error
by binarizing continuous embeddings to binary codes, and
(2) large approximation error by adopting inner product between continuous embeddings as the surrogate of Hamming
distance between binary codes. To control the quantization
error and close the gap between Hamming distance and its
surrogate for learning high-quality hash codes, in this paper,
we propose a novel bimodal Laplacian prior (unnormalized)
for the continuous representations {hi}, which is deﬁned as
p (hi) = 1
−∥|hi| −1∥1
where ϵ is the diversity parameter, and an illustration of the
proposed prior is shown in Figure 2. We can observe that the
prior puts the largest density on the discrete values {−1, 1},
which enforces that the learned Hamming embeddings {hi}
are assigned to {−1, 1} with the largest probability.
By taking Equations (2) and (3) into the MAP estimation
in Equation (1), we achieve the DHN optimization problem:
Θ C = L + λQ,
where λ = 1/ϵ is trade-off parameter between the pairwise
cross-entropy loss L and the pairwise quantization loss Q,
denotes the set of network parameters.
Speciﬁcally, the pairwise cross-entropy loss L is deﬁned as
Similarly, the pairwise quantization loss can be derived as
where 1 ∈RK is the vector of ones. As Q is a non-smooth
function whose derivative is difﬁcult to compute, we adopt
a smooth surrogate of
the absolute function |x| ≈log cosh x, which reduces (6) to
+ log cosh
By optimizing the MAP estimation in Equation (4), we
can achieve statistically optimal learning of the hash codes,
by jointly preserving the pairwise similarity in training data
and controlling the quantization error of binarizing continuous embeddings to binary codes. Finally, we can obtain Kbit binary codes by simple quantization h ←sgn(zl), where
sgn(zl) is the sign function on vectors that for i = 1, . . . , K,
i) = 1 if zl
i > 0, otherwise sgn(zl
i) = −1. It is worth
noting that, since we have minimized the quantization error
in (4) during training, this ﬁnal binarization step will incur
very small loss of retrieval quality as validated empirically.
Learning Algorithm
We derive the learning algorithms for the DHN model in
Equation (4), and show rigorously that both pairwise crossentropy loss and pairwise quantization loss can be optimized
efﬁciently through the standard back-propagation (BP) procedure. For notation brevity, we deﬁne the pairwise cost as
Cij ≜Lij + λQij
+ log cosh
Then we derive the gradient of point-wise cost Ci w.r.t. W ℓ
the network parameter of the k-th unit in the ℓ-th layer as
i = W ℓzℓ−1
+ bℓis the output of the ℓ-th layer before activation aℓ(·), and δℓ
ik + λ ∂Qij
the point-wise residual term that measures how much the kth unit in the ℓ-th layer is responsible for the error of point xi
in the network output. For an output unit k, we can directly
measure the difference between the network’s activation and
the true target value, and use it to deﬁne the residual δl
where l = 8 denotes the index of the output layer, and ˙al(·)
is the derivative of the l-th layer activation function. For a
hidden unit k in the (ℓ−1)-th layer, we compute the residual
based on a weighted average of the errors of all the
units k′ = 1, . . . , uℓin the ℓ-th layer that involve zℓ−1
input, which is just consistent with standard BP procedure,
where uℓis the number of hidden units in the ℓ-th layer. The
residuals in all layers can be computed by back-propagation.
An important property of the proposed algorithm is that,
only computing the residual of the output layer involves the
pairwise summation as in Equation (10). For all hidden layers, all the residuals can be simply computed recursively by
Equation (11), which does not involve pairwise summation.
Hence we do not need to modify the implementation of BP
in all hidden layers 1 ≤ℓ≤l−1. We only need modify standard BP by replacing the output residual with Equation (10).
Since the only difference between standard BP and our
algorithm is Equation (10), we analyze the computational
complexity based on Equation (10). Denote the number of
similarity pairs S available for training as |S|, then it is easy
to verify that the computational complexity is linear O(|S|).
In practice, the similarity set S is very sparse, i.e. |S| ≪N 2,
hence we only need to optimize pairwise loss (5) by processing input pairs that are provided with similarity information.
Theoretical Analysis
We elaborate the connection between our work and Iterative
Quantization (ITQ) , the seminal
work that considers the quantization error in an iterative Procrustean procedure. In ITQ, point-wise quantization error is
QITQ = ∥hi −sgn (hi)∥2.
We only consider point-wise quantization error for brevity,
while taking the summation for all training points (pairs)
yields the overall quantization error similar to Equation (6).
Theorem 1 (Upper Bound). The proposed quantization loss
(6) is an upper bound of the ITQ quantization error (12),
∥hi −sgn (hi)∥2 ≤∥|hi| −1∥1.
Proof. As hi and sgn (hi) have the same sign, it yields that
∥hi −sgn (hi)∥2 = ∥|hi| −|sgn (hi)|∥2
= ∥|hi| −1∥2
≤∥|hi| −1∥1. (norm inequality)
Theorem 1 reveals that the proposed quantization loss in
Equation (6) is a reasonable criterion for hash learning. The
loss is easier to optimize in the back-propagation algorithm,
since it does not involve a computation-intensive alternating
optimization procedure as ITQ .
Different from all existing methods, the proposed quantization loss is jointly optimized with the pairwise cross-entropy
loss in a deep network, which yields better hashing schemes.
An important advantage of the proposed loss is that the L1loss (6) may encourage sparsity, that is, more hash bits may
be enforced to be {−1, 1} compared with the L2-loss (12),
giving rise to more compact and discriminative hash codes.
Experiments
We conduct extensive experiments to evaluate the efﬁcacy
of the proposed DHN model against several state-of-the-art
hashing methods on three widely-used benchmark datasets.
The codes and conﬁgurations will be made available online.
Evaluation Setup
We conduct extensive empirical evaluation on three public
benchmark datasets, NUS-WIDE, CIFAR-10, and Flickr.
• NUS-WIDE1 is a public web image dataset. We follow
the settings in and use the
subset of 195,834 images that are associated with the 21
most frequent concepts, where each concept consists of at
least 5,000 images. We resize all images into 256×256.
1 
Table 1: Mean Average Precision (MAP) of Hamming Ranking for Different Number of Bits on Three Image Datasets
NUS-WIDE (MAP)
CIFAR-10 (MAP)
Flickr (MAP)
• CIFAR-102 is a dataset containing 60,000 color images in
10 classes, and each class has 6,000 images in size 32×32.
• Flickr3 consists of 25,000 images collected from Flickr,
where each image is labeled with one of the 38 semantic
concepts. We resize images of this subset into 256×256.
We follow the experimental protocols in .
In NUS-WIDE and CIFAR-10, we randomly select 100 images per class as the test query set, and 500 images per class
as the training set. In Flickr, we randomly select 1000 images as the test query set, and 4000 images as the training set. The similarity pairs for training are randomly constructed using image labels: each pair is considered similar
(dissimilar) if they share at least one (none) semantic label.
We follow to evaluate the retrieval quality
based on four evaluation metrics: Mean Average Precision
(MAP), Precision-Recall curves, Precision curves within
Hamming distance 2, and Precision curves with respect to
different numbers of top returned samples. For fair comparison, all of the methods use identical training and test sets.
We evaluate and compare, in several metrics, the retrieval
quality of the proposed DHN approach with ten state-of-theart hashing methods, including three unsupervised methods
LSH , SH and ITQ , and seven supervised methods DNNH , CNNH and its variant CNNH⋆ , KSH , MLH , BRE and ITQ-CCA .
For the deep learning based methods, including CNNH,
CNNH⋆, DNNH and DHN, we directly use the image pixels
as input. For the shallow learning based methods, we follow
 to represent each image in NUS-WIDE by a
500-dimensional bag-of-words vector, to represent each image in CIFAR-10 by a 512-dimensional GIST vector, and to
represent each image in Flickr by a 3,857-dimensional vector concatenated by local SIFT feature, global GIST feature,
etc. All image features are available at the datasets’ website.
2 
3 
To guarantee that our results directly comparable to most
published results, the results of LSH, BRE, ITQ, ITQ-CCA,
KSH, MLH and SH on both the NUS-WIDE and CIFAR-10
datasets are directly reported from the latest work , while the results on the Flickr dataset are obtained
by the implementations provided by their authors, following
standard cross-validation procedures for model selection.
We implement the DHN model based on the open-source
Caffe framework . We employ the AlexNet
architecture , ﬁnetune convolutional layers conv1–conv5 and fully-connected
layers fc6–fc7 that were copied from the pre-trained model,
and train hashing layer fch, all via back-propagation. As the
fch layer is trained from scratch, we set its learning rate to
be 10 times that of the lower layers. We use the mini-batch
stochastic gradient descent (SGD) with 0.9 momentum and
the learning rate annealing strategy implemented in Caffe,
and cross-validate the learning rate from 10−5 to 10−2 with
a multiplicative step-size 10. We choose the quantization
penalty parameter λ by cross-validation from 10−5 to 102
with a multiplicative step-size 10. We ﬁx the mini-batch size
of images as 64 and the weight decay parameter as 0.0005.
Results and Discussion
The MAP results of all methods are listed in Table 1, which
show the proposed DHN method substantially outperforms
all the comparison methods. Speciﬁcally, compared to the
best baseline using traditional hand-crafted visual features,
KSH, we achieve absolute increases of 16.3%, 25.8%, and
12.7% in average MAP for different bits on NUS-WIDE,
CIFAR-10, and Flickr respectively. Compared to the stateof-the-art deep-network-based hashing method, DNNH, we
achieve absolute increments of 3.8%, 2.9%, 3.6% in average MAP for different bits on the three datasets respectively.
DNNH uses a ﬁxed-margin loss and piecewise-linear activation function to train deep networks, which may cause information loss and objective oscillations in back propagation.
Figures 3 and 4 show the results of precision curves within
Hamming radius 2, precision-recall curves with 48 bits, and
precision curves with 48 bits w.r.t.
different numbers of
top returned images on NUS-WIDE and CIFAR-10, respectively. The proposed DHN approach generally outperforms
Number of bits
Precision (Hamming dist. <= 2)
(a) Precision within Hamming radius 2
(b) Precision-recall curve @ 48 bits
Number of top returned images
(c) Precisou curve w.r.t. top-n @ 48 bits
Figure 3: The results of comparison methods on the NUS-WIDE dataset: (a) precision curves within Hamming radius 2; (b)
precision-recall curves of Hamming ranking with 48 bits; (c) precision curves w.r.t different numbers of top returned images.
Number of bits
Precision (Hamming dist. <= 2)
(a) Precision within Hamming radius 2
(b) Precision-recall curve @ 48 bits
Number of top returned images
(c) Precisou curve w.r.t. top-n @ 48 bits
Figure 4: The results of comparison methods on the CIFAR-10 dataset: (a) precision curves within Hamming radius 2; (b)
precision-recall curves of Hamming ranking with 48 bits; (c) precision curves w.r.t different numbers of top returned images.
all comparison methods by large margins in the metrics of
precision-recall curves of Hamming ranking (Figures 3(b)
and 4(b)) and the precision curves w.r.t. different numbers of
top returned images (Figures 3(c) and 4(c)). DHN achieves
particularly good results at lower recall levels, which is very
desirable for precision-oriented image retrieval systems.
The retrieval performance using Hamming ranking within
Hamming radius 2 is particularly important for retrieval with
binary hash codes, because such Hamming ranking only requires constant time cost. As shown in Figures 3(a) and 4(a),
the proposed DHN approach achieves the highest precision
within Hamming radius 2 with 24 bits in both datasets, and
outperforms the state-of-the-art method, DNNH, when hash
bits is no more than 32. In particular, the precision of DHN
with 24 bits is better than that of DNNH with 48 bits. These
results testify that DHN learns more compact binary codes
such that shorter codes can already establish accurate results.
It is worth noting that, when using longer hash codes, the
Hamming space becomes increasingly sparse and very few
data points fall within the Hamming ball with radius 2. This
explains why DHN achieves the best performance with relatively shorter hash codes. DNNH mitigates the sparsity issue
using a complicated divide-and-encode module to generate
several pieces of independent hash codes and combine them.
It is interesting to extend the DHN model using this module.
Empirical Analysis
We investigate several variants of DHN: DHN-B is the DHN
variant without binarization (h ←sgn(zl) not performed),
which may serve as an upper bound of performance. DHN-
Q is the DHN variant without the quantization loss (λ = 0).
DHN-E is the DHN variant using widely-adopted pairwise
squared loss L = 
2 instead of the pairwise cross-entropy
loss (5). We compare the results of DHN variants in Table 2.
We can observe that, by optimizing the quantization loss
(6), DHN incurs small average MAP decreases of 4.18%,
0.01%, and 2.29% when binarizing continuous embeddings
of DHN-B. In contrast, without optimizing the quantization
loss (6), DHN-Q suffers from very large MAP decreases of
10.8%, 4.20%, and 5.33%, and substantially underperforms
DHN. These results validate that the proposed quantization
loss (6) can effectively reduce the binarization error and lead
to nearly lossless hash coding for high retrieval quality.
Another crucial observation is that, by using the pairwise cross-entropy loss (5), DHN outperforms DHN-E using
the pairwise squared loss by very large margins of 9.07%,
8.01%, and 6.64% in average MAP for different bits. The
Table 2: Mean Average Precision (MAP) Results of DHN and Its Variants, DHN-B, DHN-Q, DHN-E on Three Datasets
NUS-WIDE (MAP)
CIFAR-10 (MAP)
Flickr (MAP)
pairwise squared loss has been widely adopted in previous
work . However, this loss
does not link well the pairwise Hamming distances (taking
values in (−∞, +∞) when using continuous relaxation) to
the pairwise similarity labels (taking binary values {-1,1}).
This is a misspeciﬁed use of pairwise regression for pairwise
classiﬁcation problems, which gives rise to large similarity
search errors. The proposed pairwise cross-entropy loss (5)
is derived rigorously from Bayesian learning framework (1)
and is well-speciﬁed to the pairwise classiﬁcation problem.
Conclusion
In this paper, we have formally approached the problem of
supervised deep hashing in a Bayesian learning framework.
The proposed Deep Hashing Network (DHN) architecture
simultaneously optimizes the pairwise cross-entropy loss on
semantic similarity pairs and the pairwise quantization loss
on compact hash codes. Extensive experiments on standard
image retrieval datasets show the DHN architecture yields
substantial boosts over the state-of-the-art hashing methods.
Acknowledgments
This work was supported by National Natural Science Foundation of China (No. 61502265), National Natural Science
Funds for Distinguished Young Scholars (No. 613250154),
China Postdoctoral Science Foundation (No. 2015T80088),
and Tsinghua National Laboratory (TNList) Special Funds
for Big Data Science and Technology.