Modeling wine preferences by data mining
from physicochemical properties
Paulo Cortez a,∗Ant´onio Cerdeira b Fernando Almeida b
Telmo Matos b Jos´e Reis a,b
aDepartment of Information Systems/R&D Centre Algoritmi, University of
Minho, 4800-058 Guimar˜aes, Portugal
bViticulture Commission of the Vinho Verde region (CVRVV), 4050-501 Porto,
We propose a data mining approach to predict human wine taste preferences that
is based on easily available analytical tests at the certiﬁcation step. A large dataset
(when compared to other studies in this domain) is considered, with white and red
vinho verde samples (from Portugal). Three regression techniques were applied, under a computationally eﬃcient procedure that performs simultaneous variable and
model selection. The support vector machine achieved promising results, outperforming the multiple regression and neural network methods. Such model is useful
to support the oenologist wine tasting evaluations and improve wine production.
Furthermore, similar techniques can help in target marketing by modeling consumer
tastes from niche markets.
Key words: Sensory preferences, Regression, Variable selection, Model selection,
Support vector machines, Neural networks
 
22 May 2009
Introduction
Once viewed as a luxury good, nowadays wine is increasingly enjoyed by a
wider range of consumers. Portugal is a top ten wine exporting country with
3.17% of the market share in 2005 . Exports of its vinho verde wine (from
the northwest region) have increased by 36% from 1997 to 2007 . To support
its growth, the wine industry is investing in new technologies for both wine
making and selling processes. Wine certiﬁcation and quality assessment are
key elements within this context. Certiﬁcation prevents the illegal adulteration
of wines (to safeguard human health) and assures quality for the wine market.
Quality evaluation is often part of the certiﬁcation process and can be used
to improve wine making (by identifying the most inﬂuential factors) and to
stratify wines such as premium brands (useful for setting prices).
Wine certiﬁcation is generally assessed by physicochemical and sensory tests
 . Physicochemical laboratory tests routinely used to characterize wine include determination of density, alcohol or pH values, while sensory tests rely
mainly on human experts. It should be stressed that taste is the least understood of the human senses , thus wine classiﬁcation is a diﬃcult task.
Moreover, the relationships between the physicochemical and sensory analysis
are complex and still not fully understood .
Advances in information technologies have made it possible to collect, store
and process massive, often highly complex datasets. All this data hold valuable information such as trends and patterns, which can be used to improve
∗Corresponding author. E-mail ; tel.: +351 253510313; fax:
+351 253510300.
decision making and optimize chances of success . Data mining (DM) techniques aim at extracting high-level knowledge from raw data. There are
several DM algorithms, each one with its own advantages. When modeling continuous data, the linear/multiple regression (MR) is the classic approach. The
backpropagation algorithm was ﬁrst introduced in 1974 and later popularized in 1986 . Since then, neural networks (NNs) have become increasingly
used. More recently, support vector machines (SVMs) have also been proposed
 . Due to their higher ﬂexibility and nonlinear learning capabilities, both
NNs and SVMs are gaining an attention within the DM ﬁeld, often attaining
high predictive performances . SVMs present theoretical advantages
over NNs, such as the absence of local minima in the learning phase. In eﬀect,
the SVM was recently considered one of the most inﬂuential DM algorithms
 . While the MR model is easier to interpret, it is still possible to extract
knowledge from NNs and SVMs, given in terms of input variable importance
When applying these DM methods, variable and model selection are critical
issues. Variable selection is useful to discard irrelevant inputs, leading
to simpler models that are easier to interpret and that usually give better
performances. Complex models may overﬁt the data, losing the capability
to generalize, while a model that is too simple will present limited learning
capabilities. Indeed, both NN and SVM have hyperparameters that need to
be adjusted , such as the number of NN hidden nodes or the SVM kernel
parameter, in order to get good predictive accuracy (see Section 2.3).
The use of decision support systems by the wine industry is mainly focused
on the wine production phase . Despite the potential of DM techniques to
predict wine quality based on physicochemical data, their use is rather scarce
and mostly considers small datasets. For example, in 1991 the “Wine” dataset
was donated into the UCI repository . The data contain 178 examples with
measurements of 13 chemical constituents (e.g. alcohol, Mg) and the goal is
to classify three cultivars from Italy. This dataset is very easy to discriminate
and has been mainly used as a benchmark for new DM classiﬁers. In 1997 ,
a NN fed with 15 input variables (e.g. Zn and Mg levels) was used to predict
six geographic wine origins. The data included 170 samples from Germany
and a 100% predictive rate was reported. In 2001 , NNs were used to
classify three sensory attributes (e.g. sweetness) of Californian wine, based
on grape maturity levels and chemical analysis (e.g. titrable acidity). Only
36 examples were used and a 6% error was achieved. Several physicochemical
parameters (e.g. alcohol, density) were used in to characterize 56 samples
of Italian wine. Yet, the authors argued that mapping these parameters with a
sensory taste panel is a very diﬃcult task and instead they used a NN fed with
data taken from an electronic tongue. More recently, mineral characterization
(e.g. Zn and Mg) was used to discriminate 54 samples into two red wine
classes . A probabilistic NN was adopted, attaining 95% accuracy. As a
powerful learning tool, SVM has outperformed NN in several applications,
such as predicting meat preferences . Yet, in the ﬁeld of wine quality only
one application has been reported, where spectral measurements from 147
bottles were successfully used to predict 3 categories of rice wine age .
In this paper, we present a case study for modeling taste preferences based on
analytical data that are easily available at the wine certiﬁcation step. Building such model is valuable not only for certiﬁcation entities but also wine
producers and even consumers. It can be used to support the oenologist wine
evaluations, potentially improving the quality and speed of their decisions.
Moreover, measuring the impact of the physicochemical tests in the ﬁnal wine
quality is useful for improving the production process. Furthermore, it can
help in target marketing , i.e. by applying similar techniques to model the
consumers preferences of niche and/or proﬁtable markets.
The main contributions of this work are:
• We present a novel method that performs simultaneous variable and model
selection for NN and SVM techniques. The variable selection is based on
sensitivity analysis , which is a computationally eﬃcient method that
measures input relevance and guides the variable selection process. Also, we
propose a parsimony search method to select the best SVM kernel parameter
with a low computational eﬀort.
• We test such approach in a real-world application, the prediction of vinho
verde wine (from the Minho region of Portugal) taste preferences, showing
its impact in this domain. In contrast with previous studies, a large dataset
is considered, with a total of 4898 white and 1599 red samples. Wine preferences are modeled under a regression approach, which preserves the order
of the grades, and we show how the deﬁnition of the tolerance concept is
useful for accessing diﬀerent performance levels. We believe that this integrated approach is valuable to support applications where ranked sensory
preferences are required, for example in wine or meat quality assurance.
The paper is organized as follows: Section 2 presents the wine data, DM models and variable selection approach; in Section 3, the experimental design is
described and the obtained results are analyzed; ﬁnally, conclusions are drawn
in Section 4.
Materials and methods
This study will consider vinho verde, a unique product from the Minho (northwest) region of Portugal. Medium in alcohol, is it particularly appreciated due
to its freshness (specially in the summer). This wine accounts for 15% of the
total Portuguese production , and around 10% is exported, mostly white
wine. In this work, we will analyze the two most common variants, white and
red (ros´e is also produced), from the demarcated region of vinho verde. The
data were collected from May/2004 to February/2007 using only protected
designation of origin samples that were tested at the oﬃcial certiﬁcation entity (CVRVV). The CVRVV is an inter-professional organization with the
goal of improving the quality and marketing of vinho verde. The data were
recorded by a computerized system (iLab), which automatically manages the
process of wine sample testing from producer requests to laboratory and sensory analysis. Each entry denotes a given test (analytical or sensory) and the
ﬁnal database was exported into a single sheet (.csv).
During the preprocessing stage, the database was transformed in order to
include a distinct wine sample (with all tests) per row. To avoid discarding
examples, only the most common physicochemical tests were selected. Since
the red and white tastes are quite diﬀerent, the analysis will be performed
separately, thus two datasets 1 were built with 1599 red and 4898 white examples. Table 1 presents the physicochemical statistics per dataset. Regarding
the preferences, each sample was evaluated by a minimum of three sensory
1 The datasets are available at: 
assessors (using blind tastes), which graded the wine in a scale that ranges
from 0 (very bad) to 10 (excellent). The ﬁnal sensory score is given by the median of these evaluations. Fig. 1 plots the histograms of the target variables,
denoting a typical normal shape distribution (i.e. with more normal grades
that extreme ones).
[ insert Table 1 and Fig. 1 around here ]
Data mining approach and evaluation
We will adopt a regression approach, which preserves the order of the preferences. For instance, if the true grade is 3, then a model that predicts 4 is better
than one that predicts 7. A regression dataset D is made up of k ∈{1, ..., N}
examples, each mapping an input vector with I input variables (xk
1, . . . , xk
a given target yk. The regression performance is commonly measured by an
error metric, such as the mean absolute deviation (MAD) :
i=1 |yi −byi|/N
where byk is the predicted value for the k input pattern. The regression error
characteristic (REC) curve is also used to compare regression models, with
the ideal model presenting an area of 1.0. The curve plots the absolute error
tolerance T (x-axis), versus the percentage of points correctly predicted (the
accuracy) within the tolerance (y-axis).
The confusion matrix is often used for classiﬁcation analysis, where a C × C
matrix (C is the number of classes) is created by matching the predicted
values (in columns) with the desired classes (in rows). For an ordered output,
the predicted class is given by pi = yi, if |yi −byi| ≤T, else pi = y′
i, where y′
denotes the closest class to byi, given that y′
i ̸= yi. From the matrix, several
metrics can be used to access the overall classiﬁcation performance, such as
the accuracy and precision (i.e. the predicted column accuracies) .
The holdout validation is commonly used to estimate the generalization capability of a model . This method randomly partitions the data into training
and test subsets. The former subset is used to ﬁt the model (typically with 2/3
of the data), while the latter (with the remaining 1/3) is used to compute the
estimate. A more robust estimation procedure is the k-fold cross-validation
 , where the data is divided into k partitions of equal size. One subset is
tested each time and the remaining data are used for ﬁtting the model. The
process is repeated sequentially until all subsets have been tested. Therefore,
under this scheme, all data are used for training and testing. However, this
method requires around k times more computation, since k models are ﬁtted.
Data mining methods
We will adopt the most common NN type, the multilayer perceptron, where
neurons are grouped into layers and connected by feedforward links . For
regression tasks, this NN architecture is often based on one hidden layer of
H hidden nodes with a logistic activation and one output node with a linear
function :
by = wo,0 +
1 + exp(−PI
i=1 xiwj,i −wj,0) · wo,i
where wi,j denotes the weight of the connection from node j to i and o the
output node. The performance is sensitive to the topology choice (H). A NN
with H = 0 is equivalent to the MR model. By increasing H, more complex
mappings can be performed, yet an excess value of H will overﬁt the data,
leading to generalization loss. A computationally eﬃcient method to set H is
to search through the range {0, 1, 2, 3, . . . , Hmax} (i.e. from the simplest NN to
more complex ones). For each H value, a NN is trained and its generalization
estimate is measured (e.g. over a validation sample). The process is stopped
when the generalization decreases or when H reaches the maximum value
In SVM regression , the input x ∈ℜI is transformed into a high mdimensional feature space, by using a nonlinear mapping (φ) that does not
need to be explicitly known but that depends of a kernel function (K). The
aim of a SVM is to ﬁnd the best linear separating hyperplane, tolerating a
small error (ϵ) when ﬁtting the data, in the feature space:
The ϵ-insensitive loss function sets an insensitive tube around the residuals
and the tiny errors within the tube are discarded (Fig. 2).
[ insert Fig. 2 around here ]
We will adopt the popular gaussian kernel, which presents less parameters than
other kernels (e.g. polynomial) : K(x, x′) = exp(−γ||x−x′||2), γ > 0. Under
this setup, the SVM performance is aﬀected by three parameters: γ, ϵ and C (a
trade-oﬀbetween ﬁtting the errors and the ﬂatness of the mapping). To reduce
the search space, the ﬁrst two values will be set using the heuristics : C = 3
(for a standardized output) and ϵ = bσ/
N, where bσ = 1.5/N × PN
i=1(yi −byi)2
and by is the value predicted by a 3-nearest neighbor algorithm. The kernel
parameter (γ) produces the highest impact in the SVM performance, with
values that are too large or too small leading to poor predictions. A practical
method to set γ is to start the search from one of the extremes and then search
towards the middle of the range while the predictive estimate increases .
Variable and Model Selection
Sensitivity analysis is a simple procedure that is applied after the training phase and analyzes the model responses when the inputs are changed.
Originally proposed for NNs, this sensitivity method can also be applied to
other algorithms, such as SVM . Let byaj denote the output obtained by
holding all input variables at their average values except xa, which varies
through its entire range with j ∈{1, . . . , L} levels. If a given input variable
(xa ∈{x1, . . . , xI}) is relevant then it should produce a high variance (Va).
Thus, its relative importance (Ra) can be given by:
j=1 (byaj −byaj)2/(L −1)
Ra = Va/ PI
i=1 Vi × 100 (%)
In this work, the Ra values will be used to measure the importance of the inputs
and also to discard irrelevant inputs, guiding the variable selection algorithm.
We will adopt the popular backward selection, which starts with all variables
and iteratively deletes one input until a stopping criterion is met . Yet,
we guide the variable deletion (at each step) by the sensitivity analysis, in a
variant that allows a reduction of the computational eﬀort by a factor of I
(when compared to the standard backward procedure) and that in has
outperformed other methods (e.g. backward and genetic algorithms). Similarly
to , the variable and model selection will be performed simultaneously, i.e.
in each backward iteration several models are searched, with the one that
presents the best generalization estimate selected. For a given DM method,
the overall procedure is depicted bellow:
(1) Start with all F = {x1, . . . , xI} input variables.
(2) If there is a hyperparameter P ∈{P1, . . . , Pk} to tune (e.g. NN or SVM),
start with P1 and go through the remaining range until the generalization
estimate decreases. Compute the generalization estimate of the model by
using an internal validation method. For instance, if the holdout method
is used, the available data are further split into training (to ﬁt the model)
and validation sets (to get the predictive estimate).
(3) After ﬁtting the model, compute the relative importances (Ri) of all xi ∈
F variables and delete from F the least relevant input. Go to step 4 if
the stopping criterion is met, otherwise return to step 2.
(4) Select the best F (and P in case of NN or SVM) values, i.e., the input
variables and model that provide the best predictive estimates. Finally,
retrain this conﬁguration with all available data.
Empirical results
The R environment is an open source, multiple platform (e.g. Windows,
Linux) and high-level matrix programming language for statistical and data
analysis. All experiments reported in this work were written in R and conducted in a Linux server, with an Intel dual core processor. In particular, we
adopted the RMiner , a library for the R tool that facilitates the use of
DM techniques in classiﬁcation and regression tasks.
Before ﬁtting the models, the data was ﬁrst standardized to a zero mean and
one standard deviation . RMiner uses the eﬃcient BFGS algorithm to
train the NNs (nnet R package), while the SVM ﬁt is based on the Sequential
Minimal Optimization implementation provided by LIBSVM (kernlab package). We adopted the default R suggestions . The only exception are the
hyperparameters (H and γ), which will be set using the procedure described
in the previous section and with the search ranges of H ∈{0, 1, . . . , 11} 
and γ ∈{23, 21, . . . , 2−15} . While the maximum number of searches is
12/10, in practice the parsimony approach (step 2 of Section 2.4) will reduce
this number substantially.
Regarding the variable selection, we set the estimation metric to the MAD
value (Equation 1), as advised in . To reduce the computational eﬀort,
we adopted the simpler 2/3 and 1/3 holdout split as the internal validation method. The sensitivity analysis parameter was set to L = 5, i.e. xa ∈
{−1.0, −0.5, . . . , 1.0} for a standardized input. As a reasonable balance between the pressure towards simpler models and the increase of computational
search, the stopping criterion was set to 2 iterations without any improvement
or when only one input is available.
To evaluate the selected models, we adopted 20 runs of the more robust 5-fold
cross-validation, in a total of 20×5=100 experiments for each tested conﬁguration. Statistical conﬁdence will be given by the t-student test at the 95%
conﬁdence level . The results are summarized in Table 2. The test set
errors are shown in terms of the mean and conﬁdence intervals. Three metrics are present: MAD, the classiﬁcation accuracy for diﬀerent tolerances (i.e.
T = 0.25, 0.5 and 1.0) and Kappa (T = 0.5). The selected models are described
in terms of the average number of inputs (I) and hyperparameter value (H or
γ). The last row shows the total computational time required in seconds.
[ insert Table 2 and Fig. 3 around here ]
For both tasks and all error metrics, the SVM is the best choice. The diﬀerences
are higher for small tolerances and in particular for the white wine (e.g. for
T = 0.25, the SVM accuracy is almost two times better when compared to
other methods). This eﬀect is clearly visible when plotting the full REC curves
(Fig. 3). The Kappa statistic measures the accuracy when compared with
a random classiﬁer (which presents a Kappa value of 0%). The higher the
statistic, the more accurate the result. The most practical tolerance values are
T = 0.5 and T = 1.0. The former tolerance rounds the regression response
into the nearest class, while the latter accepts a response that is correct within
one of the two closest classes (e.g. a 3.1 value can be interpreted as grade 3
or 4 but not 2 or 5). For T = 0.5, the SVM accuracy improvement is 3.3
pp for red wine (6.2 pp for Kappa), a value that increases to 12.0 pp for the
white task (20.4 pp for Kappa). The NN is quite similar to MR in the red wine
modeling, thus similar performances were achieved. For the white data, a more
complex NN model (H = 2.1) was selected, slightly outperforming the MR
results. Regarding the variable selection, the average number of deleted inputs
ranges from 0.9 to 1.8, showing that most of the physicochemical tests used
are relevant. In terms of computational eﬀort, the SVM is the most expensive
method, particularly for the larger white dataset.
A detailed analysis of the SVM classiﬁcation results is presented by the average
confusion matrixes for T = 0.5 (Table 3). To simplify the visualization, the 3
and 9 grade predictions were omitted, since these were always empty. Most of
the values are close to the diagonals (in bold), denoting a good ﬁt by the model.
The true predictive accuracy for each class is given by the precision metric
(e.g. for the grade 4 and white wine, precisionT=0.5=19/(19+7+4)=63.3%).
This statistic is important in practice, since in a real deployment setting the
actual values are unknown and all predictions within a given column would
be treated the same. For a tolerance of 0.5, the SVM red wine accuracies
are around 57.7 to 67.5% in the intermediate grades (5 to 7) and very low
(0%/20%) for the extreme classes (3, 8 and 4), which are less frequent (Fig.
1). In general, the white data results are better: 60.3/63.3% for classes 6 and
4, 67.8/72.6% for grades 7 and 5, and a surprising 85.5% for the class 8 (the
exception are the 3 and 9 extremes with 0%, not shown in the table). When
the tolerance is increased (T = 1.0), high accuracies ranging from 81.9 to
100% are attained for both wine types and classes 4 to 8.
[ insert Table 3 and Fig. 4 around here ]
The average SVM relative importance plots (Ra values) of the analytical tests
are shown in Fig. 4. It should be noted that the whole 11 inputs are shown,
since in each simulation diﬀerent sets of variables can be selected. In several
cases, the obtained results conﬁrm the oenological theory. For instance, an
increase in the alcohol (4th and 2nd most relevant factor) tends to result in
a higher quality wine. Also, the rankings are diﬀerent within each wine type.
For instance, the citric acid and residual sugar levels are more important in
white wine, where the equilibrium between the freshness and sweet taste is
more appreciated. Moreover, the volatile acidity has a negative impact, since
acetic acid is the key ingredient in vinegar. The most intriguing result is the
high importance of sulphates, ranked ﬁrst for both cases. Oenologically this
result could be very interesting. An increase in sulphates might be related to
the fermenting nutrition, which is very important to improve the wine aroma.
Conclusions and implications
In recent years, the interest in wine has increased, leading to growth of the
wine industry. As a consequence, companies are investing in new technologies to improve wine production and selling. Quality certiﬁcation is a crucial
step for both processes and is currently largely dependent on wine tasting by
human experts. This work aims at the prediction of wine preferences from
objective analytical tests that are available at the certiﬁcation step. A large
dataset (with 4898 white and 1599 red entries) was considered, including vinho
verde samples from the northwest region of Portugal. This case study was addressed by two regression tasks, where each wine type preference is modeled
in a continuous scale, from 0 (very bad) to 10 (excellent). This approach preserves the order of the classes, allowing the evaluation of distinct accuracies,
according to the degree of error tolerance (T) that is accepted.
Due to advances in the data mining (DM) ﬁeld, it is possible to extract knowledge from raw data. Indeed, powerful techniques such as neural networks
(NNs) and more recently support vector machines (SVMs) are emerging. While
being more ﬂexible models (i.e. no a priori restriction is imposed), the performance depends on a correct setting of hyperparameters (e.g. number of
hidden nodes of the NN architecture or SVM kernel parameter). On the other
hand, the multiple regression (MR) is easier to interpret than NN/SVM, with
most of the NN/SVM applications considering their models as black boxes.
Another relevant aspect is variable selection, which leads to simpler models
while often improving the predictive performance. In this study, we present an
integrated and computationally eﬃcient approach to deal with these issues.
Sensitivity analysis is used to extract knowledge from the NN/SVM models,
given in terms of relative importance of the inputs. Simultaneous variable and
model selection scheme is also proposed, where the variable selection is guided
by sensitivity analysis and the model selection is based on parsimony search
that starts from a reasonable value and is stopped when the generalization
estimate decreases.
Encouraging results were achieved, with the SVM model providing the best
performances, outperforming the NN and MR techniques, particularly for
white vinho verde wine, which is the most common type. When admitting
only the correct classiﬁed classes (T = 0.5), the overall accuracies are 62.4%
(red) and 64.6% (white). It should be noted that the datasets contain six/seven
classes (from 3 to 8/9). These accuracies are much better than the ones expected by a random classiﬁer. The performance is substantially improved when
the tolerance is set to accept responses that are correct within the one of the
two nearest classes (T = 1.0), obtaining a global accuracy of 89.0% (red) and
86.8% (white). In particular, for both tasks the majority of the classes present
an individual accuracy (precision) higher than 90%.
The superiority of SVM over NN is probably due to the diﬀerences in the training phase. The SVM algorithm guarantees an optimum ﬁt, while NN training
may fall into a local minimum. Also, the SVM cost function (Fig. 2) gives a
linear penalty to large errors. In contrast, the NN algorithm minimizes the sum
of squared errors. Thus, the SVM is expected to be less sensitive to outliers
and this eﬀect results in a higher accuracy for low error tolerances. As argued
in , it is diﬃcult to compare DM methods in a fair way, with data analysts
tending to favor models that they know better. We adopted the default suggestions of the R tool , except for the hyperparameters (which were set
using a grid search). Since the default settings are more commonly used, this
seems a reasonable assumption for the comparison. Nevertheless, diﬀerent NN
results could be achieved if diﬀerent hidden node and/or minimization cost
functions were used. Under the tested setup, the SVM algorithm provided the
best results while requiring more computation. Yet, the SVM ﬁtting can still
be achieved within a reasonable time with current processors. For example,
one run of the 5-fold cross-validation testing takes around 26 minutes for the
larger white dataset, which covers a three-year collection period.
The result of this work is important for the wine industry. At the certiﬁcation
phase and by Portuguese law, the sensory analysis has to be performed by human tasters. Yet, the evaluations are based in the experience and knowledge of
the experts, which are prone to subjective factors. The proposed data-driven
approach is based on objective tests and thus it can be integrated into a
decision support system, aiding the speed and quality of the oenologist performance. For instance, the expert could repeat the tasting only if her/his
grade is far from the one predicted by the DM model. In eﬀect, within this
domain the T = 1.0 distance is accepted as a good quality control process and,
as shown in this study, high accuracies were achieved for this tolerance. The
model could also be used to improve the training of oenology students. Furthermore, the relative importance of the inputs brought interesting insights
regarding the impact of the analytical tests. Since some variables can be controlled in the production process this information can be used to improve the
wine quality. For instance, alcohol concentration can be increased or decreased
by monitoring the grape sugar concentration prior to the harvest. Also, the
residual sugar in wine could be raised by suspending the sugar fermentation
carried out by yeasts. Moreover, the volatile acidity produced during the malolactic fermentation in red wine depends on the lactic bacteria control activity.
Another interesting application is target marketing . Speciﬁc consumer
preferences from niche and/or proﬁtable markets (e.g. for a particular country) could be measured during promotion campaigns (e.g. free wine tastings
at supermarkets) and modeled using similar DM techniques, aiming at the
design of brands that match these market needs.
Acknowledgments
We would like to thank Cristina Lagido and the anonymous reviewers for their
helpful comments. The work of P. Cortez is supported by the FCT project
PTDC/EIA/64541/2006.