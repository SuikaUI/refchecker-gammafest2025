The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)
Bayesian Graph Convolutional Neural
Networks for Semi-Supervised Classiﬁcation
Yingxue Zhang∗
Huawei Noah’s Ark Lab
Montreal Research Centre
7101 Avenue du Parc, H3N 1X9
Montreal, QC Canada
Soumyasundar Pal,∗Mark Coates
Dept. Electrical and Computer Engineering
McGill University
3480 University St, H3A 0E9
Montreal, QC, Canada
Deniz ¨Ustebay
Huawei Noah’s Ark Lab
Montreal Research Centre
7101 Avenue du Parc, H3N 1X9
Montreal, QC Canada
Recently, techniques for applying convolutional neural networks to graph-structured data have emerged. Graph convolutional neural networks (GCNNs) have been used to address node and graph classiﬁcation and matrix completion.
Although the performance has been impressive, the current
implementations have limited capability to incorporate uncertainty in the graph structure. Almost all GCNNs process
a graph as though it is a ground-truth depiction of the relationship between nodes, but often the graphs employed in
applications are themselves derived from noisy data or modelling assumptions. Spurious edges may be included; other
edges may be missing between nodes that have very strong
relationships. In this paper we adopt a Bayesian approach,
viewing the observed graph as a realization from a parametric family of random graphs. We then target inference of the
joint posterior of the random graph parameters and the node
(or graph) labels. We present the Bayesian GCNN framework
and develop an iterative learning procedure for the case of
assortative mixed-membership stochastic block models. We
present the results of experiments that demonstrate that the
Bayesian formulation can provide better performance when
there are very few labels available during the training process.
Introduction
Novel approaches for applying convolutional neural networks to graph-structured data have emerged in recent
years. Commencing with the work in , there have been numerous developments and improvements. Although these graph
convolutional neural networks (GCNNs) are promising, the
current implementations have limited capability to handle
uncertainty in the graph structure, and treat the graph topology as ground-truth information. This in turn leads to an inability to adequately characterize the uncertainty in the predictions made by the neural network.
In contrast to this past work, we employ a Bayesian
framework and view the observed graph as a realization
from a parametric random graph family. The observed adjacency matrix is then used in conjunction with features and
labels to perform joint inference. The results reported in this
∗These authors contributed equally to this work.
Copyright c⃝2019, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
paper suggest that this formulation, although computationally more demanding, can lead to an ability to learn more
from less data, a better capacity to represent uncertainty,
and better robustness and resilience to noise or adversarial
In this paper, we present the novel Bayesian GCNN
framework and discuss how inference can be performed. To
provide a concrete example of the approach, we focus on
a speciﬁc random graph model, the assortative mixed membership block model. We address the task of semi-supervised
classiﬁcation of nodes and examine the resilience of the
derived architecture to random perturbations of the graph
Related work
A signiﬁcant body of research focuses on using neural networks to analyze structured data when there is an underlying
graph describing the relationship between data items. Early
work led to the development of the graph neural network
(GNN) . The GNN approaches
rely on recursive processing and propagation of information across the graph. Training can often take a long time to
converge and the required time scales undesirably with respect to the number of nodes in the graph, although recently
an approach to mitigate this has been proposed by .
Graph convolutional neural networks (GCNNs) have
emerged more recently, with the ﬁrst proposals in . A spectral ﬁltering approach
was introduced in and this method was simpliﬁed or improved in . Spatial ﬁltering or aggregation strategies were adopted in . A general framework for training neural networks on graphs and manifolds was presented
by and the authors explain how several of the other methods can be interpreted as
special cases.
The performance of the GCNNs can be improved by incorporating attention nodes , leading to the graph attention network (GAT). Experiments
have also demonstrated that gates, edge conditioning, and
skip connections can prove beneﬁcial . In some problem settings it is also bene-
ﬁcial to consider an ensemble of graphs , multiple adjacency matrices or the dual graph . Compared to this past work, the primary methodological novelty in our proposed approach involves the adoption
of a Bayesian framework and the treatment of the observed
graph as additional data to be used during inference.
There is a rich literature on Bayesian neural networks,
commencing with pioneering work 
and extending to more recent contributions . To the
best of our knowledge, Bayesian neural networks have not
yet been developed for the analysis of data on graphs.
Background
Graph convolutional neural networks (GCNNs)
Although graph convolutional neural networks can be applied to a variety of inference tasks, in order to make the
description more concrete we consider the task of identifying the labels of nodes in a graph. Suppose that we observe a
graph Gobs = (V, E), comprised of a set of N nodes V and a
set of edges E. For each node we measure data (or derive features), denoted xi for node i. For some subset of the nodes
L ⊂V, we can also measure labels YL = {yi : i ∈L}.
In a classiﬁcation context, the label yi identiﬁes a category;
in a regression context yi can be real-valued. Our task is to
use the features x and the observed graph structure Gobs to
estimate the labels of the unlabelled nodes.
A GCNN performs this task by performing graph convolution operations within a neural network architecture. Collecting the feature vectors as the rows of a matrix X, the
layers of a GCNN are of the form:
H(1) = σ(AGXW(0))
H(l+1) = σ(AGH(l)W(l))
Here W(l) are the weights of the neural network at layer
l, H(l) are the output features from layer l −1, and σ is
a non-linear activation function. The matrix AG is derived
from the observed graph and determines how the output
features are mixed across the graph at each layer. The ﬁnal output for an L-layer network is Z = H(L). Training
of the weights of the neural network is performed by backpropagation with the goal of minimizing an error metric between the observed labels Y and the network predictions
Z. Performance improvements can be achieved by enhancing the architecture with components that have proved useful
for standard CNNs, including attention nodes , and skip connections and gates .
Although there are many different ﬂavours of GCNNs, all
current versions process the graph as though it is a groundtruth depiction of the relationship between nodes. This is
despite the fact that in many cases the graphs employed
in applications are themselves derived from noisy data or
modelling assumptions. Spurious edges may be included;
other edges may be missing between nodes that have very
strong relationships. Incorporating attention mechanisms as
in addresses this to some extent; attention nodes can learn that some edges are not representative of a meaningful relationship and reduce the impact
that the nodes have on one another. But the attention mechanisms, for computational expediency, are limited to processing existing edges — they cannot create an edge where one
should probably exist. This is also a limitation of the ensemble approach of , where
learning is performed on multiple graphs derived by erasing
some edges in the graph.
Bayesian neural networks
We consider the case where we have training inputs X =
{x1, ..., xn} and corresponding outputs Y = {y1, ..., yn}.
Our goal is to learn a function y = f(x) via a neural network with ﬁxed conﬁguration (number of layers, activation
function, etc., so that the weights are sufﬁcient statistics for
f) that provides a likely explanation for the relationship between x and y. The weights W are modelled as random variables in a Bayesian approach and we introduce a prior distribution over them. Since W is not deterministic, the output
of the neural network is also a random variable. Prediction
for a new input x can be formed by integrating with respect
to the posterior distribution of W as follows:
p(y|x, X, Y) =
p(y|x, W)p(W|X, Y) dW .
The term p(y|x, W) can be viewed as a likelihood; in a classiﬁcation task it is modelled using a categorical distribution
by applying a softmax function to the output of the neural
network; in a regression task a Gaussian likelihood is often
an appropriate choice. The integral in eq. (3) is in general
intractable. Various techniques for inference of p(W|X, Y)
have been proposed in the literature, including expectation
propagation , variational inference , and Markov Chain
Monte Carlo methods . In particular, in ,
it was shown that with suitable variational approximation
for the posterior of W, Monte Carlo dropout is equivalent to
drawing samples of W from the approximate posterior and
eq. (3) can be approximated by a Monte Carlo integral as
p(y|x, X, Y) ≈1
p(y|x, Wi) ,
where S weights Wi are obtained via dropout.
Methodology
We consider a Bayesian approach, viewing the observed
graph as a realization from a parametric family of random
graphs. We then target inference of the joint posterior of
the random graph parameters, weights in the GCNN and
the node (or graph) labels. Since we are usually not directly
interested in inferring the graph parameters, posterior estimates of the labels are obtained by marginalization. The goal
is to compute the posterior probability of labels, which can
be written as:
p(Z|YL, X, Gobs) =
p(Z|W, G, X)p(W|YL, X, G)
p(G|λ)p(λ|Gobs) dW dG dλ .
Here W is a random variable representing the weights of a
Bayesian GCNN over graph G, and λ denotes the parameters that characterize a family of random graphs. The term
p(Z|W, G, X) can be modelled using a categorical distribution by applying a softmax function to the output of the
GCNN, as discussed above.
This integral in eq. (5) is intractable. We can adopt a
number of strategies to approximate it, including variational
methods and Markov Chain Monte Carlo (MCMC). For
example, in order to approximate the posterior of weights
p(W|YL, X, G), we could use variational inference or MCMC . Various parametric random graph
generation models can be used to model p(λ|Gobs), for example a stochastic block model , a mixed
membership stochastic block model , or
a degree corrected block model .
For inference of p(λ|Gobs), we can use MCMC or variational inference .
A Monte Carlo approximation of eq. (5) is:
p(Z|YL, X, Gobs) ≈
p(Z|Ws,i,v, Gi,v, X) .
In this approximation, V
samples λv are drawn from
p(λ|Gobs); the precise method for generating these samples
from the posterior varies depending on the nature of the
graph model. The NG graphs Gi,v are sampled from p(G|λv)
using the adopted random graph model. S weight matrices Ws,i,v are sampled from p(W|YL, X, Gi,v) from the
Bayesian GCN corresponding to the graph Gi,v.
Example: Assortative mixed membership
stochastic block model
For the Bayesian GCNNs derived in this paper, we use an
assortative mixed membership stochastic block model (a-
MMSBM) for the graph and learn its parameters λ =
{π, β} using a stochastic optimization approach. The assortative MMSBM, described in the following section, is a
good choice to model a graph that has relatively strong community structure (such as the citation networks we study in
the experiments section). It generalizes the stochastic block
model by allowing nodes to belong to more than one community and to exhibit assortative behaviour, in the sense that
a node can be connected to one neighbour because of a relationship through community A and to another neighbour
because of a relationship through community B.
Since Gobs is often noisy and may not ﬁt the adopted
parametric block model well, sampling πv and βv from
p(π, β|Gobs) can lead to high variance. This can lead to the
sampled graphs Gi,v being very different from Gobs. Instead,
we replace the integration over π and β with a maximum a
posteriori estimate . We approximately compute
{ˆπ, ˆβ} = arg max
β,π p(β, π|Gobs)
by incorporating suitable priors over β and π and use the
approximation:
p(Z|YL, X, Gobs) ≈
p(Z|Ws,i, Gi, X) . (8)
In this approximation, Ws,i are approximately sampled
from p(W|YL, X, Gi) using Monte Carlo dropout over the
Bayesian GCNN corresponding to Gi. The Gi are sampled
from p(G|ˆπ, ˆβ).
Posterior inference for the MMSBM
For the undirected observed graph Gobs = {yab ∈{0, 1} :
1 ≤a < b ≤N}, yab = 0 or 1 indicates absence or
presence of a link between node a and node b. In MMSBM,
each node a has a K dimensional community membership
probability distribution πa = [πa1, ...πaK]T , where K is
the number of categories/communities of the nodes. For
any two nodes a and b, if both of them belong to the same
community, then the probability of a link between them
is signiﬁcantly higher than the case where the two nodes
belong to different communities . The
generative model is described as:
For any two nodes a and b,
• Sample zab ∼πa and zba ∼πb.
• If zab = zba = k, sample a link yab ∼Bernoulli(βk).
Otherwise, yab ∼Bernoulli(δ).
Here, 0 ≤βk ≤1 is termed community strength of the
k-th community and δ is the cross community link probability, usually set to a small value. The joint posterior of the
parameters π and β is given as:
p(π, β|Gobs) ∝p(β)p(π)p(Gobs|π, β)
p(yab, zab, zba|πa, πb, β) .
We use a Beta(η) distribution for the prior of βk and a
Dirichlet distribution, Dir(α), for the prior of πa, where η
and α are hyper-parameters.
Expanded mean parameterisation
Maximizing the posterior of eq. (9) is a constrained optimization problem with βk, πak ∈(0, 1) and
Employing a standard iterative algorithm with a gradient
based update rule does not guarantee that the constraints
will be satisﬁed. Hence we consider an expanded mean parameterisation as follows. We introduce the alternative parameters θk0, θk1 ≥0 and adopt
as the prior for these parameters a product of independent
Gamma(η, ρ) distributions. These parameters are related to
the original parameter βk through the relationship βk =
. This results in a Beta(η) prior for βk. Similarly,
we introduce a new parameter φa ∈RK
+ and adopt as its
prior a product of independent Gamma(α, ρ) distributions.
We deﬁne πak =
, which results in a Dirichlet
prior, Dir(α), for πa. The boundary conditions θki, φak ≥0
can be handled by simply taking the absolute value of the
Stochastic optimization and mini-batch sampling
We use preconditioned gradient ascent to maximize the joint
posterior in eq. (9) over θ and φ. In many graphs that are
appropriately modelled by a stochastic block model, most
of the nodes belong strongly to only one of the K communities, so the MAP estimate for many πa lies near one of
the corners of the probability simplex. This suggests that
the scaling of different dimensions of φa can be very different. Similarly, as Gobs is typically sparse, the community
strengths βk are very low, indicating that the scales of θk0
and θk1 are very different. We use preconditioning matrices
G(θ) = diag(θ)−1 and G(φ) = diag(φ)−1 as in , to obtain the following update rules:
η −1 −ρθ(t)
α −1 −ρφ(t)
ϵ0(t + τ)−κ is a decreasing step-size,
and gab(θki) and gab(φak) are the partial derivatives of
log p(yab|πa, πb, β) w.r.t. θki and φak, respectively. Detailed
expressions for these derivatives are provided in eqs. (9) and
(14) of .
Implementation of (10) and (11) is O(N 2K) per iteration, where N is the number of nodes in the graph and K
the number of communities. This can be prohibitively expensive for large graphs. We instead employ a stochastic
gradient based strategy as follows. For update of θki’s in
eq. (10), we split the O(N 2) sum over all edges and nonedges, PN
b=a+1, into two separate terms. One of these
is a sum over all observed edges and the other is a sum over
all non-edges. We calculate the term corresponding to observed edges exactly (in the sparse graphs of interest, the
number of edges is closer to O(N) than O(N 2)). For the
other term we consider a mini-batch of 1 percent of randomly sampled non-edges and scale the sum by a factor of
At any single iteration, we update the φak values for only
n randomly sampled nodes (n < N), keeping the rest of
them ﬁxed. For the update of φak values of any of the randomly selected n nodes, we split the sum in eq. (11) into two
terms. One involves all of the neighbours (the set of neighbours of node a is denoted by N(a)) and the other involves
all the non-neighbours of node a. We calculate the ﬁrst term
exactly. For the second term, we use n −|N(a)| randomly
sampled non-neighbours and scale the sum by a factor of
N −1 −|N(a)|
to maintain unbiasedness of the stochastic
gradient. Overall the update of the φ values involve O(n2K)
operations instead of O(N 2K) complexity for a full batch
Since the posterior in the MMSBM is very highdimensional, random initialization often does not work well.
We train a GCNN on Gobs and use
its softmax output to initialize π and then initialize β based
on the block structure imposed by π. The resulting algorithm
is given in Algorithm 1.
Algorithm 1 Bayesian-GCNN
Input: Gobs, X, YL
Output: p(Z|YL, X, Gobs)
1: Initialization: train a GCNN to initialize the inference in
MMSBM and the weights in the Bayesian GCNN.
2: Perform Nb iterations of MMSBM inference to obtain
3: for i = 1 : NG do
Sample graph Gi ∼p(G|ˆπ, ˆβ).
for s = 1 : S do
Sample weights Ws,i via MC dropout by training a
GCNN over the graph Gi.
8: end for
9: Approximate p(Z|YL, X, Gobs) using eq. (8).
Experimental Results
We explore the performance of the proposed Bayesian
GCNN on three well-known citation datasets : Cora, CiteSeer, and Pubmed. In these
datasets each node represents a document and has a sparse
bag-of-words feature vector associated with it. Edges are
formed whenever one document cites another. The direction of the citation is ignored and an undirected graph with a
symmetric adjacency matrix is constructed. Each node label
represents the topic that is associated with the document.
We assume that we have access to several labels per class
and the goal is to predict the unknown document labels. The
statistics of these datasets are represented in Table 1.
The hyperparameters of GCNN are the same for all of the
experiments and are based on . The
GCNN has two layers where the number of hidden units is
Features per node
Table 1: Summary of the datasets used in the experiments.
16, the learning rate is 0.01, the L2 regularization parameter
is 0.0005, and the dropout rate is 50% at each layer. These
hyperparameters are also used in the Bayesian GCNN. In
addition, the hyperparameters associated with MMSBM inference are set as follows: η = 1, α = 1, ρ = 0.001, n =
500, ϵ0 = 1, τ = 1024 and κ = 0.5.
Semi-supervised node classiﬁcation
We ﬁrst evaluate the performance of the proposed Bayesian
GCNN algorithm and compare it to the state-of-the-art
methods on the semi-supervised node classiﬁcation problem. In addition to the 20 labels per class training setting explored in previous work , we also evaluate the performance of
these algorithms under more severely limited data scenarios
where only 10 or 5 labels per class are available.
The data is split into train and test datasets in two different
ways. The ﬁrst is the ﬁxed data split originating from . In 5 and 10 training labels
per class cases, we construct the ﬁxed split of the data by using the ﬁrst 5 and 10 labels in the original partition of . The second type of split is
random where the training and test sets are created at random for each run. This provides a more robust comparison
of the model performance as the speciﬁc split of data can
have a signiﬁcant impact in the limited training labels case.
We compare ChebyNet , GCNN , and
GAT to the Bayesian GCNN proposed in this paper. Tables 2, 3, 4 show the summary of results on Cora, Citeseer and Pubmed datasets respectively.
The results are from 50 runs with random weight initializations. The standard errors in the ﬁxed split scenarios are
due to the random initialization of weights whereas the random split scenarios have higher variance due to the additional randomness induced by the split of data. We conducted Wilcoxon signed rank tests to evaluate the signiﬁcance of the difference between the best-performing algorithm and the second-best. The asterisks in the table indicate
the scenarios where the score differentials were statistically
signiﬁcant for a p-value threshold of 0.05.
Note that the implementation of the GAT method as provided by the authors employs a validation set of 500 examples which is used to monitor validation accuracy. The
model that yields the minimum validation error is selected
as ﬁnal model. We report results without this validation set
monitoring as large validation sets are not always available
and the other methods examined here do not require one.
The results of our experiments illustrate the improvement
Random split
Bayesian GCN
Fixed split
Bayesian GCN
Table 2: Prediction accuracy (percentage of correctly predicted labels) for Cora dataset. Asterisks denote scenarios
where a Wilcoxon signed rank test indicates a statistically
signiﬁcant difference between the scores of the best and
second-best algorithms.
Random split
Bayesian GCN
Fixed split
Bayesian GCN
Table 3: Prediction accuracy (percentage of correctly predicted labels) for Citeseer dataset. Asterisks denote scenarios where a Wilcoxon signed rank test indicates a statistically signiﬁcant difference between the scores of the best
and second-best algorithms.
in classiﬁcation accuracy provided by Bayesian GCNN for
Cora and Citeseer datasets in the random split scenarios.
The improvement is more pronounced when the number
of available labels is limited to 10 or 5. In addition to increased accuracy, Bayesian GCNN provides lower variance
results in most tested scenarios. For the Pubmed dataset, the
Bayesian GCNN provides the best performance for the 5label case, but is outperformed by other techniques for the
10- and 20-label cases. The Pubmed dataset has a much
lower intra-community density than the other datasets and
a heavy-tailed degree distribution. The assortative MMSBM
is thus a relatively poor choice for the observed graph, and
this prevents the Bayesian approach from improving the prediction accuracy.
In order to provide some insight into the information
available from the posterior of the MMSBM, we examined
the 50 observed edges with lowest average posterior probability for both the Cora and Citeseer graphs. In the majority of cases the identiﬁed edges were inter-community (connecting edges with different labels) or had one node with
very low degree (lower than 2). This accounted for 39 of the
50 edges for Cora and 42 of the 50 edges for Citeseer. For
Random split
Bayesian GCNN
Fixed split
Bayesian GCNN
Table 4: Prediction accuracy (percentage of correctly predicted labels) for Pubmed dataset. Asterisks denote scenarios where a Wilcoxon signed rank test indicates a statistically signiﬁcant difference between the scores of the best
and second-best algorithms.
the unobserved edges, we analyzed the most probable edges
from the posterior. Most of these are intra-community edges
(connecting nodes with the same label). For Cora 177 of the
200 edges identiﬁed as most probable are intra-community,
and for Citeseer 197 of 200.
Classiﬁcation under node attacks
Several studies have shown the vulnerability of deep neural networks to adversarial examples . For graph convolutional neural networks, recently introduced a method to create adversarial attacks that
involve limited perturbation of the input graph. The aim of
the study was to demonstrate the vulnerability of the graphbased learning algorithms. Motivated by this study we use
a random attack mechanism to compare the robustness of
GCNN and Bayesian GCNN algorithms in the presence of
noisy edges.
Random node attack mechanism: In each experiment,
we target one node to attack. We choose a ﬁxed number of
perturbations ∆= dv0 + 2, where v0 is the node we want
to attack, and dv0 is the degree of this target node. The random attack involves removing (dv0 + 2)/2 nodes from the
target node’s set of neighbors, and sampling (dv0 + 2)/2
cross-community edges (randomly adding neighbors that
have different labels than the target node) to the target node.
For each target node, this procedure is repeated ﬁve times
so that ﬁve perturbed graphs are generated. There are two
types of adversarial mechanisms in . In the ﬁrst type, called an evasion
attack, data is modiﬁed to fool an already trained classiﬁer,
and in the second, called a poisoning attack, the perturbation
occurs before the model training. All of our experiments are
performed in the poisoning attack fashion.
Selection of target node: Similar to the setup in , we choose 40 nodes
from the test set that are correctly classiﬁed and simulate
attacks on these nodes. The margin of classiﬁcation for node
v is deﬁned as:
marginv = scorev(ctrue) −max
c̸=ctruescorev(c) ,
where ctrue is the true class of node v and scorev denotes
the classiﬁcation score vector reported by the classiﬁer for
node v. A correct classiﬁcation leads to a positive margin;
incorrect classiﬁcations are associated with negative margins. For each algorithm we choose the 10 nodes with the
highest margin of classiﬁcation and 10 nodes with the lowest positive margin of classiﬁcation. The remaining 20 nodes
are selected at random from the set of nodes correctly classiﬁed by both algorithms. Thus, among the 40 target nodes,
the two algorithms are sharing at least 20 common nodes.
Evaluation: For each targeted node, we run the algorithm
for 5 trials. The results of this experiment are summarized in
Tables 5 and 6. These results illustrate average performance
over the target nodes and the trials. Note that the accuracy
ﬁgures in these tables are different from Table 2 and 3 as
here we are reporting the accuracy for the 40 selected target
nodes instead of the entire test set.
Random attack
Bayesian GCNN
Classiﬁer margin
Bayesian GCNN
Table 5: Comparison of accuracy and classiﬁer margins
for the no attack and random attack scenarios on the Cora
dataset. The results are for 40 selected target nodes and 5
runs of the algorithms for each target.
Random attack
Bayesian GCNN
Classiﬁer margin
Bayesian GCNN
Table 6: Comparison of accuracy and classiﬁer margins for
the no attack and random attack scenarios on the Citeseer
dataset. The results are for 40 selected target nodes and 5
runs of the algorithms for each target.
Overall the attacks affect both algorithms severely. GCNN
loses 30% in prediction accuracy for the Cora dataset and
44.5% for Citeseer whereas the drop in prediction accuracy
is more limited for Bayesian GCNN with 17% for Cora and
20.5% for the Citeseer dataset. The Bayesian GCNN is able
to maintain the classiﬁer margin much better compared to
GCNN. For the Citeseer dataset the random attacks almost
eliminate the GCNN margin whereas Bayesian GCNN suffers a 34% decrease, but retains a positive margin on average.
Figure 1 provides further insight concerning the impact
of the attack on the two algorithms. The ﬁgure depicts the
distribution of average classiﬁer margins over the targeted
nodes before and after the random attacks. Each circle in the
ﬁgure shows the margin for one target node averaged over
the 5 random perturbations of the graph. Note that some of
the nodes have a negative margin prior to the random attack
because we select the correctly classiﬁed nodes with lowest
average margin based on 10 random trials and then perform
another 5 random trials to generate the depicted graph. We
see that for GCNN the attacks cause nearly half of the target
nodes to be wrongly classiﬁed whereas there are considerably fewer prediction changes for the Bayesian GCNN.
Classification Margin
Classification Margin
Figure 1: Boxplots of the average classiﬁcation margin
for 40 nodes before and after random attacks for GCNN
and Bayesian GCNN on (a) Cora dataset and (b) Citeseer
dataset. The box indicates 25-75 percentiles; the triangle
represents the mean value; and the median is shown by a
horizontal line. Whiskers extend to the minimum and maximum of data points.
Conclusions and Future Work
In this paper we have presented Bayesian graph convolutional neural networks, which provide an approach for incorporating uncertain graph information through a parametric random graph model. We provided an example of the
framework for the case of an assortative mixed membership
stochastic block model and explained how approximate inference can be performed using a combination of stochastic
optimization (to obtain maximum a posteriori estimates of
the random graph parameters) and approximate variational
inference through Monte Carlo dropout (to sample weights
from the Bayesian GCNN). We explored the performance of
the Bayesian GCNN for the task of semi-supervised node
classiﬁcation and observed that the methodology improved
upon state-of-the-art techniques, particularly for the case
where the number of training labels is small. We also compared the robustness of Bayesian GCNNs and standard GC-
NNs under an adversarial attack involving randomly changing a subset of the edges of node. The Bayesian GCNN appears to be considerably more resilient to attack.
This paper represents a preliminary investigation into
Bayesian graph convolutional neural networks and focuses
on one type of graph model and one graph learning problem.
In future work, we will expand the approach to other graph
models and explore the suitability of the Bayesian framework for other learning tasks.