Learning to Quantize Deep Networks by
Optimizing Quantization Intervals with Task Loss
Sangil Jung1∗
Changyong Son1∗
Seohyung Lee1
Jinwoo Son1
Jae-Joon Han1
Youngjun Kwak1
Sung Ju Hwang2
Changkyu Choi1
1Samsung Advanced Institute of Technology (SAIT), South Korea
2Korea Advanced Institute of Science and Technology (KAIST), South Korea
Reducing bit-widths of activations and weights of deep
networks makes it efﬁcient to compute and store them in
memory, which is crucial in their deployments to resourcelimited devices, such as mobile phones. However, decreasing bit-widths with quantization generally yields drastically
degraded accuracy. To tackle this problem, we propose to
learn to quantize activations and weights via a trainable
quantizer that transforms and discretizes them. Speciﬁcally,
we parameterize the quantization intervals and obtain their
optimal values by directly minimizing the task loss of the
network. This quantization-interval-learning (QIL) allows
the quantized networks to maintain the accuracy of the fullprecision (32-bit) networks with bit-width as low as 4-bit
and minimize the accuracy degeneration with further bitwidth reduction (i.e., 3 and 2-bit). Moreover, our quantizer
can be trained on a heterogeneous dataset, and thus can
be used to quantize pretrained networks without access to
their training data. We demonstrate the effectiveness of our
trainable quantizer on ImageNet dataset with various network architectures such as ResNet-18, -34 and AlexNet, on
which it outperforms existing methods to achieve the stateof-the-art accuracy.
1. Introduction
Increasing the depth and width of a convolutional neural network generally improves its accuracy in exchange for the increased memory and the computational
cost. Such a memory- and computation- heavy network is
difﬁcult to be deployed to resource-limited devices such as
mobile phones. Thus, many prior work have sought various
means to reduce the model size and computational cost, including the use of separable ﬁlters , weight pruning and bit-width reduction of weights or activations
∗These two authors contributed equally.
 . Our work aims to reduce bit-widths
of deep networks both for weights and activations, while
preserving the accuracy of the full-precision networks.
Reducing bit-width inherently includes a quantization
process which maps continuous real values to discrete integers. Decrease in bit-width of deep networks naturally
increases the quantization error, which in turn causes accuracy degeneration, and to preserve the accuracy of a fullprecision network, we need to reduce the quantization error.
For example, Cai et al. optimize the activation quantizer by minimizing mean-squared quantization error using
Lloyd’s algorithm with the assumption of half-wave Gaussian distribution of the response map, and some other work
approximate the layerwise convolutional outputs .
However, while such quantization approaches may accurately approximate the original distribution of the weights
and activations, there is no guarantee that they will be
beneﬁcial toward suppressing the prediction error from increasing. To overcome this limitation, our trainable quantizer approximates neither the weight/activation values nor
layerwise convolutional outputs. Instead, we quantize the
weights and activations for each layer by directly minimizing the task loss of the networks, which helps it to preserve
the accuracy of the full-precision counterpart.
Our quantizer can be viewed as a composition of a transformer and a discretizer. The transformer is a (non-linear)
function from the unbounded real values to the normalized
real values (i.e., R(−∞,∞) →R[−1,1]), and the discretizer
maps the normalized values to integers (i.e., R[−1,1] →I).
Here, we propose to parameterize quantization intervals for
the transformer, which allows our quantizer to focus on appropriate interval for quantization by pruning (less important) small values and clipping (rarely appeared) large values.
Since reducing bit-widths decreases the number of discrete values, this will generally cause the quantization error
to increase. To maintain or increase the quantization reso-
 
lution while reducing the bit-width, the interval for quantization should be compact. On the other hand, if the quantization interval is too compact, it may remove valid values outside the interval which can degrade the performance
of the network. Thus, the quantization interval should be
selected as compact as possible according to the given bitwidth, such that the values inﬂuencing the network accuracy
are within the interval. Our trainable quantizer adaptively
ﬁnds the optimal intervals for quantization that minimize
the task loss.
Note that our quantizer is applied to both weights and
activations for each layer.
Thus, the convolution operation can be computed efﬁciently by utilizing bit-wise operations which is composed of logical operations (i.e., ’AND’
or ’XNOR’) and bitcount if the bit-widths of weights
and activations become low enough. The weight and activation quantizers are jointly trained with full-precision model
weights. Note that the weight quantizers and full-precision
model weights are kept and updated only in the training
stage; at the inference time, we drop them and use only the
quantized weights.
We demonstrate our method on the ImageNet classiﬁcation dataset with various network architectures such
as ResNet-18, -34 and AlexNet. Compared to the existing
methods on weight and activation quantization, our method
achieves signiﬁcantly higher accuracy, achieving the stateof-the-art results. Our quantizers are trained in end-to-end
fashion without any layerwise optimization .
In summary, our contributions are threefold:
• We propose a trainable quantizer with parameterized
intervals for quantization, which simultaneously performs both pruning and clipping.
• We apply our trainable quantizer to both the weights
and activations of a deep network, and optimize it
along with the target network weights for the taskspeciﬁc loss in an end-to-end manner.
• We experimentally show that our quantizer achieves
the state-of-the-art classiﬁcation accuracies on ImageNet with extremely low bit-width (2, 3, and 4-bit)
networks, and achieves high performance even when
trained with a heterogeneous dataset and applied to a
pretrained network.
2. Related Work
Our quantization method aims to obtain low-precision
Low-precision networks have two beneﬁts:
model compression and operation acceleration. Some work
compresses the network by reducing bit-width of model
weights, such as BinaryConnect (BC) , Binary-Weight-
Network (BWN) , Ternary-Weight-Network (TWN)
 and Trained-Ternary-Quantization (TTQ) .
uses either deterministic or stochastic methods for binarizing the weights which are {−1, +1}. BWN approximates
the full-precision weights as the scaled bipolar({−1, +1})
weights, and ﬁnds the scale in a closed form solution. TWN
utilizes the ternary({−1, 0, +1}) weights with scaling factor which is trained in training phase. TTQ uses the same
quantization method but the different scaling factors are
learned on the positive and negative sides. These methods
solely considers quantization of network weights, mostly
for binary or ternary cases only, and do not consider quantization of the activations.
In order to maximally utilize the bit-wise operations for
convolution, we should quantize both weights and activations. Binarized-Neural-Network(BNN) binarizes the
weights and activations to {−1, +1} in the same way as
BC, and uses these binarized values for computing gradients. XNOR-Net further conducts activation binarization with a scaling factor where the scaling factor is obtained in a closed form solution.
DoReFa-Net performs a bit-wise operation for convolution by quantizing
both weights and activations with multiple-bits rather than
performing bipolar quantization. They adopt various activation functions to bound the activation values. The weights
are transformed by the hyperbolic tangent function and then
normalized with respect to the maximum values before
quantization. Half-Wave-Gaussian-Quantization (HWGQ)
 exploits the statistics of activations and proposes variants of ReLU activation which constrain the unbounded values. Both DoReFa-Net and HWGQ use upper bounds for
the activation but they are ﬁxed during training and their
quantizations are not learned as done in our model.
Several recent work proposed highly accurate low bit-width models by considering both weight and
activation quantization. LQ-Nets allows ﬂoating-point
values to represent the basis of K-bit quantized value instead of the standard basis [1, 2, ..., 2K−1], and learn the basis for weights and activations of each layer or channel by
minimizing the quantization error. On the other hand, our
trainable quantizer estimates the optimal quantization interval, which is learned in terms of minimizing the output task
loss rather than minimizing the quantization error. Furthermore, the LQ-Nets has to use a few ﬂoating-point multiplication for computing convolution due to the ﬂoating-point
basis, but our convolution can use shift operation instead of
multiplication because all of our quantized values are integers.
Wang et al. proposes a two-step quantization (TSQ)
that decomposes activation and weight quantization steps,
which achieves good performance with AlexNet and VG-
GNet architectures. However, TSQ adopts layerwise optimization for the weight quantization step, and thus is not
applicable to ResNet architecture which includes skip connections.
Contrarily, our quantizer is applicable to any
quantization interval
Weight Quantization
Activation Quantization
𝑐𝑐𝑋𝑋𝑙𝑙, 𝑑𝑑𝑋𝑋𝑙𝑙
𝑙𝑙𝑡𝑡𝑡 Conv.
𝑐𝑐𝑊𝑊𝑙𝑙, 𝑑𝑑𝑊𝑊𝑙𝑙
: Parameterized interval
: back-propagation
(a) Quantization Interval
(b) A convolutional layer of our low bit-width network
Figure 1. Illustration of our trainable quantizer. (a) Our trainable quantization interval, which performs pruning and clipping simultaneously.
(b) The lth convolution layer of our low-precision network. Given bit-width, the quantized weights ¯
Wl and activations ¯
Xl are acquired
using the parameterized intervals. The interval parameters (cWl, dWl, cXl, dXl) are trained jointly with the full-precision weights Wl
during backpropagation.
types of network architectures regardless of skip connection.
Zhuang et al. proposes a two-stage and progressive optimization method to obtain good initialization
to avoid the network to get trapped in a poor local minima. We adopt their progressive strategy, which actually improves the accuracy especially for extremely low bit-width
network, i.e., 2-bit. PACT proposes a parameterized
clipping activation function where the clipping parameter
is obtained during training. However, it does not consider
pruning and the weight quantization is ﬁxed as in DoReFa-
Net. A couple of recent quantization methods use Bayesian
approches; Louizos et al. propose Bayesian compression, which determines optimal number of bit precision per
layer via the variance of the estimated posterior, but does
not cluster weights. Achterhold et al. propose a variational inference framework to learn networks that quantize well, using multi-modal quantizing priors with peaks
at quantization target values. Yet neither approaches prune
activations or learn the quantizer itself as done in our work.
We propose the trainable quantization interval which
performs pruning and clipping simultaneously during training, and by applying this parameterization both for weight
and activation quantization we keep the classiﬁcation accuracy on par with the one of the full-precision network while
signiﬁcantly reducing the bit-width.
The trainable quantization interval has quantization
range within the interval, and prunes and clips ranges out of
the interval. We apply the trainable quantization intervals to
both activation and weight quantization and optimize them
with respect to the task loss (Fig. 1). In this section, we
ﬁrst review and interpret the quantization process for low
bit-width networks, and then present our trainable quantizer
with quantization intervals.
3.1. Quantization in low bit-width network
For the l-th layer of a full-precision convolutional neural
network (CNN), the weight Wl is convolved with the input activation Xl where Wl and Xl are real-valued tensors.
We denote the elements of Wl and Xl by wl and xl, respectively. For notational simplicity, we drop the subscript l in
the following. Reducing bit-widths inherently involves a
quantization process, where we obtain the quantized weight
W and the quantized input activation ¯x ∈¯X via quantizers,
A quantizer Q∆(∆∈{W, X}) is a composition of a transformer T∆and a discretizer D. The transformer maps the
weight and activation values to [−1, 1] or . The simplest example of the transformer is the normalization that
divides the values by their maximum absolute values. Another example is a function of tanh(·) for weights and clipping for activations . The discretizer maps a real-value
ˆv in the range [−1, 1] (or ) to some discrete value ¯v as
¯v = ⌈ˆv · qD⌋
where ⌈·⌋is the round operation and qD is the discretization
In this paper, we parameterize the quantizer (transformer) to make it trainable rather than ﬁxed.
quantizers can be jointly optimized together with the neural network model weights. We can obtain the optimal ¯
Figure 2. A quantizer as a combination of a transformer and a discretizer with various γ where (a) γ = 1, (b) γ < 1, and (c) γ > 1. The
blue dotted lines indicate the transformers, and the red solid lines are their corresponding quantizers. The thp
∆and the thc
∆represent the
pruning and clipping thresholds, respectively.
and ¯X that directly minimize the task loss (i.e., classiﬁcation loss) of the entire network (Fig. 1 (b)) rather than
simply approximating the full-precision weight/activation
W, X ≈¯X) or the convolutional outputs
W ∗¯X) where ∗denotes convolution
operation.
3.2. Trainable quantization interval
To design the quantizers, we consider two operations:
clipping and pruning (Fig. 1 (a)). The underlying idea of
clipping is to limit the upper bound for quantization .
Decreasing the upper bound increases the quantization resolution within the bound so that the accuracy of the low
bit-width network can increase. On the other hand, if the
upper bound is set too low, accuracy may decrease because
too many values will be clipped. Thus, setting a proper clipping threshold is crucial for maintaining the performance of
the networks. Pruning removes low-valued weight parameters . Increasing pruning threshold helps to increase the
quantization resolution and reduce the model complexity,
while setting pruning threshold too high can cause the performance degradation due to the same reason as the clipping
scheme does.
We deﬁne the quantization interval to consider both
pruning and clipping. To estimate the interval automatically, the intervals are parameterized by c∆and d∆(∆∈
{W, X}) for each layer where c∆and d∆indicate the center of the interval and the distance from the center, respectively. Note that this is simply a design choice and other
types of parameterization, such as parameterization with
lower and upper bound, are also possible.
Let us ﬁrst consider the weight quantization. Because
weights contain both positive and negative values, the quantizer has symmetry on both positive and negative sides.
Given the interval parameters cW and dW , we deﬁne the
transformer TW as follows:
|w| < cW −dW
|w| > cW + dW
(αW |w| + βW )γ · sign(w)
otherwise,
where αW = 0.5/dW , βW = −0.5cW /dW + 0.5 and the
γ is another trainable parameter of the transformer. That is,
the quantizer is designed by the interval parameters cW , dW
and γ which are trainable. The non-linear function with γ
considers the distribution inside the interval. The graphs in
Fig. 2 show the transformers (dotted blue lines) and their
corresponding quantizers (solid red lines) with various γ. If
γ = 1, then the transformer is a piecewise linear function
where the inside values of the interval are uniformly quantized (Fig. 2 (a)). The inside values can be non-uniformly
quantized by adjusting γ (Fig. 2 (b, c)). γ could be either
set to a ﬁxed value or trained. We demonstrate the effects
of γ in the experiment. If γ ̸= 1, the function is complex
to be calculated. However, the weight quantizers are removed after training and we use only the quantized weights
for inference. For this reason, this complex non-linear function does not decrease the inference speed at all. The actual
pruning threshold thp
∆and clipping threshold thc
∆vary according to the parameters cW , dW and γ as shown in Fig.
2. For example, the thp
∆in the case of γ = 1 are
derived as follows:
∆= c∆+ d∆+ 0.5d∆/q∆
∆= c∆−d∆−0.5d∆/q∆.
Note that the number of quantization levels qW for the
weight can be computed as qW = 2NW −1 −1 (one-side,
except 0), given the bit-width NW . Thus, the 2-bit weights
are actually ternary {−1, 0, 1}.
The activations fed into a convolutional layer are nonnegative due to ReLU operation. For activation quantization, a value larger than cX + dX is clipped and mapped to
1 and we prune a value smaller than cX −dX to 0. The values in between are linearly mapped to , which means
that the values are uniformly quantized in the quantization
Algorithm 1 Training low bit-width network using parameterized quantizers
Input: Training data
Output: A low bit-width model with quantized weights
l=1 and activation quantizers {cXl, dXl}L
1: procedure TRAINING
Initialize the parameter set {Pl}L
l=1 where Pl =
{wl, cWl, dWl, γl, cXl, dXl}
for l = 1, ..., L do
Compute ¯wl from wl using Eq. 3 and Eq. 2
Compute ¯xl from xl using Eq. 5 and Eq. 2
Compute ¯wl ∗¯xl
Compute the loss ℓ
Compute the gradient w.r.t. the output ∂ℓ/∂xL+1
for l = L, ..., 1 do
Given ∂ℓ/∂xl+1,
Compute the gradient of the parameters in Pl
Update the parameters in Pl
Compute ∂ℓ/∂xl
13: procedure DEPLOYMENT
for l = 1, ..., L do
Compute ¯wl from wl using Eq. 3 and Eq. 2
Deploy the low bit-width model {wl, cXl, dXl}L
interval. Unlike the weight quantization, activation quantizaiton should be conducted on-line during inference, thus
we ﬁx the γ to 1 for fast computation. Then, the transformer
TX for activation is deﬁned as follows (Fig. 2 (a)):
x < cX −dX
x > cX + dX
otherwise,
where αX = 0.5/dX and βX = −0.5cX/dX + 0.5. Given
the bit-width NX of activation, the number of quantization
levels qX (except 0) can be computed as qX = 2NX −1; i.e.,
the quantized values are {0, 1, 2, 3} for 2-bit activations.
We use stochastic gradient descent for optimizing the parameters of both the weights and the quantizers. The transformers are piece-wise differentiable, and thus we can compute the gradient with respect to the interval parameters c∆,
d∆and γ. We use straight-through-estimator for the
gradient of the discretizers.
Basically, our method trains the weight parameters
jointly with quantizers. However, it is also possible to train
the quantizers on a pre-trained network with full-precision
weights. Surprisingly, training only the quantizer without
updating weights also yields reasonably good accuracy, although its accuracy is lower than that of joint training (See
We describe the pseudo-code for training and deploying
our low bit-width model in Algorithm 1.
4. Experiment results
To demonstrate the effectiveness of our trainable quantizer, we evaluated it on the ImageNet and the CIFAR-
100 datasets.
4.1. ImageNet
The ImageNet classiﬁcation dataset consists of
1.2M training images from 1000 general object classes and
50,000 validation images. We used various network architectures such as ResNet-18, -34 and AlexNet for evaluation.
Implementation details
We implement our method using PyTorch with multiple GPUs. We use original ResNet
architecture without any structural change for ResNet-
18 and -34. For AlexNet , we use batch-normalization
layer after each convolutional layer and remove the dropout
layers and the LRN layers while preserving the other factors such as the number and the sizes of ﬁlters. In all the
experiments, the training images are randomly cropped and
resized to 224 × 224, and horizontally ﬂipped at random.
We use no further data augmentation. For testing, we use
the single center-crop of 224×224. We used stochastic gradient descent with the batch size of 1024 (8 GPUs), the momentum of 0.9, and the weight decay of 0.0001 for ResNet
(0.0005 for AlexNet). We trained each full-precision network up to 120 epochs where the learning rate was initially
set to 0.4 for ResNet-18 and -34 (0.04 for AlexNet), and is
decayed by a factor of 10 at 30, 60, 85, 95, 105 epochs. We
ﬁnetune low bit-width networks for up to 90 epochs where
the learning rate is set to 0.04 for ResNet-18 and -34 (0.004
for AlexNet) and is divided by 10 at 20, 40, 60, 80 epochs.
We set the learning rates of interval parameters to be 100×
smaller than those of weight parameters. We did not quantize the ﬁrst and the last layers as was done in .
Comparison with existing methods
We evaluate our
learnable quantization method with existing methods, by
quoting the reported top-1 accuracies from the original papers (Table 1). Our 5/5 and 4/4-bit models preserve the
accuracy of full-precision model for all three network architectures (ResNet-18, -34 and AlexNet). For 3/3-bit, the
accuracy drops only by 1% for ResNet-18 and by 0.6% for
ResNet-34. If we further reduced the bit-width to 2/2-bit,
the accuracy drops by 4.5% (ResNet-18) and 3.1% compared to the full-precision. Compared to the second best
method (LQ-Nets ), our 3/3 and 2/2-bit models are
around 1% more accurate for ResNet architectures.
AlexNet, our 3/3-bit model drops the top-1 accuracy only
by 0.5% with respect to full-precision which beats the second best by large margin of 5.7%. For 2/2-bit, accuracy
drops by 3.7% which is almost same accuracy with TSQ
 . Note that TSQ used layerwise optimization, which
Table 1. Top-1 accuracy (%) on ImageNet. Comparion with the existing methods on ResNet-18, -34 and AlexNet. The ‘FP’ represents the
full-precision (32/32-bit) accuracy in our implementation.
ResNet-18 (FP: 70.2)
ResNet-34 (FP: 73.7)
AlexNet (FP: 61.8)
Bit-width (A/W)
QIL (Ours)†
LQ-Nets 
DoReFa-Net 
ABC-Net 
BalancedQ 
Zhuang et al. 
Table 2. The top-1 accuracy (%) of low bit-width networks on
ResNet-18 with direct and progressive ﬁnetuning. The 5/5-bit network was ﬁnetuned only from full-precision network.
Initialization
Bit-width (A/W)
Progressive
Table 3. Joint training vs. Quantizer only. The top-1 accuracy (%)
with ResNet-18.
Initialization
Bit-width (A/W)
Joint training
Quantizer only
makes it difﬁcult to apply to the ResNet architecture with
skip connections. However, our method is general and is
applicable any types of network architecture.
Initialization
Good initialization of parameters improves
the accuracy of trained low bit-width networks . We
adopt ﬁnetuning approach for good initialization. In ,
they progressively conduct the quantization from higher bitwidths to lower bit-widths for better initialization. We compare the results of direct ﬁnetuning of full-precision network with progressively ﬁnetuning from 1-bit higher bitwidth network (Table 2). For progressive ﬁnetuning, we sequentially train the 4/4, 3/3, and 2/2-bit networks (i.e, FP →
5/5 →4/4 →3/3 →2/2 for 2/2-bit network). Generally, the
accuracies of progressive ﬁnetuning are higher than those
of direct ﬁnetuning. The progressive ﬁnetuning is crucial
for 2/2-bit network (9.7% point accuracy improvement), but
has marginal impact on 4/4 and 3/3-bit networks (only 0.2%
Pruning ratio
Bit-width (A/W)
AlexNet, Weight
AlexNet, Activation
ResNet-18, Weight
ResNet-18, Activation
Figure 3. Average pruning ratio of weights and activations on
AlexNet and ResNet-18 with various bit-widths
and 0.5% point improvements, respectively).
Joint training vs. Quantizer only
The weight parameters can be optimized jointly with quantization parameters
or we can optimize only the quantizers while keeping the
weight parameters ﬁxed.
Table 3 shows the top-1 accuracy with ResNet-18 network on the both cases. Both the
cases utilize the progressive ﬁnetuning.
The joint training of quantizer and weights works better than training
the quantizer only, which is consistent with our intuition.
The joint training shows graceful performance degradation
while reducing bit-width, compared to training of only the
quantizers.
Nevertheless, the accuracy of the quantizers
is quite promising with 4-bit or higher bit-width. For example, the accuracy drop with 4/4-bit model is only 2.1%
(70.1%→68.0%).
Pruning ratio
In order to see the pruning effect of our
quantizer, we compute the pruning ratio which is the number of zeros over the total number of weights or activations.
† With this mark in Table 1 and 6, the 2-bit of weights is ternary
{−1, 0, 1}, otherwise it is 4-level.
Pruning ratio
# Parameters
Pruning ratio
# Activations
(a) Weight
(b) Activation
Figure 4. Blockwise pruning ratio of (a) weight and (b) activation for each ResBlock with ResNet-18. The bar graph shows the number of
weights or activations for each ResBlock on a log scale.
(a) weight, initial
(b) weight, epoch 11
(c) weight, epoch 90
(d) activation, epoch 90
Figure 5. Weight and activation distributions at the 3rd layer of the 3/3-bit AlexNet. The 3/3-bit network is ﬁnetuned from the pretrained
full-precision network. The ﬁgures show the distributions of (a) initial weights, (b) weights at epoch 11, (c) weight at epoch 90 and (d)
activation at epoch 90. The thp
∆and the thc
∆represent the pruning and the clipping thresholds, respectively.
Fig. 3 shows the average pruning ratios of the entire network for ResNet-18 and AlexNet. As expected, the pruning
ratio increases as the bit-width decreases. If the bit-width
is high, the quantization interval can be relaxed due to the
high quantization resolution. However, as the bit-width decreases, more compact interval should be found to maintain
the accuracy, and as a result, the pruning ratio increases.
For 2/2-bit network, 91% and 81% of weights are pruned
on average for AlexNet and ResNet-18, respectively (Fig.
3). The AlexNet is more pruned than the ResNet-18 because the AlexNet has fully-connected layers which have
18∼64 times larger parameters than the convolutional layers and the fully-connected layers are more likely pruned.
The activations are less affected by the bit-width. Fig. 4
shows the blockwise pruning ratio of ResNet-18. We compute the pruning ratio for each ResBlock which consists of
two convolutional layers and a skip connection. For activations, the upper layers are more likely to be pruned, which
may be because more abstraction occurs at higher layers.
Training γ
For weight quantization, we designed the
transformer with γ (Eq. 3) which considers the distribution
inside the interval. We investigated the effects of training γ.
Table 4 shows the top-1 accuracies according to the various
Table 4. The top-1 accuray with various γ on AlexNet
Table 5. Trainable γ with various network architecture for 2/2-bit
Fixed γ = 1
Trainable γ
58.1 (+0.9)
66.1 (+0.4)
70.6 (+0.0)
γ for 3/3 and 2/2-bit AlexNet. We report both the trainable
γ and ﬁxed γ. For 3/3-bit model, the trainable γ does not
affect the model accuracy; i.e., 61.4% with trainable γ and
61.3% with γ = 1. However, the trainable γ is effective for
2/2-bit model which improves the top-1 accuracy by 0.9%
compared with γ = 1 while the accuracies of the ﬁxed γ of
0.5 and 0.75 are similar with γ = 1. The ﬁxed γ of 1.5 degrades the performance. We also evaluated trainable γ with
various network models for 2/2-bit (Table 5). The trainable
γ is less effective for ResNet-18 and -34.
Table 6. Accuracy with ResNet-18 on ImageNet. The weights are
quantized to low bits and the activations remain at 32 bits. The
TTQ-B and TWN-B used the ResNet-18B where the number
of ﬁlters in each block is 1.5× of the ResNet-18.
Accuracy (%)
QIL (Ours)
LQ-Nets 
QIL (Ours)
LQ-Nets 
QIL†(Ours)
LQ-Nets 
TTQ-B† 
TWN-B† 
Weight quantization
To demonstrate the effectiveness of
our method on weight quantization, we only quantize the
weights and leave the activations as full-precision. We measure the accuracy on ResNet-18 with various bit-widths for
weights and compared the other existing methods (Table 6).
The accuracies of the networks quantized with our method
are slightly higher than those obtained using the second-best
method (LQ-Nets ).
Distributions of weights and activations
Fig. 5 shows
the distributions of weights and activations with different
epochs. Since we train both weights and quantizers, the distributions of weights and quantization intervals are changing during training. Note that the peaks of the weight distribution appear at the transition of each quantized values. If
the objective is to minimize the quantization errors, this distribution is not desirable. Since our objective is to minimize
the task loss, during training the loss become different only
when the quantized level of weights are different. Therefore
the weight values move toward the transition boundaries.
We also plot the activation distribution with the pruning and
the clipping thresholds. (Fig. 5 (d)).
4.2. CIFAR-100
In this experiment, we train a low bit-width network
from a pre-trained full-precision network without the original training dataset. The motivation of this experiment is
to validate whether it is possible to train the low bit-width
model when the original training dataset is not given. To
demonstrate this scenario, we used CIFAR-100 , which
consists of 100 classes containing 500 training and 100 validation images for each class where 100 classes are grouped
into 20 superclasses (4 classes for each superclass). We di-
Accuracy (%) on dataset A
Training dataset B
Full-Precision
Ours-Joint
Ours-QuantOnly
DoReFa-Net
Figure 6. The accuracy on CIFAR-100 for low bit-width model
training with heterogeneous dataset.
vide the dataset into two disjoint groups A (4 superclasses,
20 classes) and B (16 superclasses, 80 classes) where A is
the original training set and B is used as a heterogeneous
dataset for training low bit-width model (4/4-bit model in
this experiment). The B is further divided into four subsets B10, B20, B40 and B80 (B10 ⊂B20 ⊂B40 ⊂
B80 = B) with 10, 20, 40 and 80 classes, respectively.
First, we train the full-precision networks with A, then
ﬁnetune the low bit-width networks with B by minimizing the mean-squared-errors between the outputs of the fullprecision model and the low bit-width model. We report the
testing accuracy of A for evaluation (Fig. 6). We compare
our method with other existing methods such as DoReFa-
Net and PACT .
We carefully re-implemented
these methods with pyTorch. Our joint training of weights
and quantizers preserve the full-precision accuracy with all
types of B. If we train the quantizer only with ﬁxed weights,
then the accuracies are lower than those of the joint training.
Our method achieves better accuracy compared to DoReFa-
Net and PACT. As expected, as the size of dataset increases
(B10 →B20 →B40 →B80), the accuracy improves.
These are impressive results, since they show that we can
quantize any given low bit-width networks can without access to the original data they are trained on.
5. Conclusion
We proposed a novel trainable quantizer with parameterized quantization intervals for training low bit-width
Our trainable quantizer performs simultaneous pruning and clipping for both weights and activations,
while maintaining the accuracy of the full-precision network by learning appropriate quantization intervals. Instead
of minimizing the quantization error with respect to the
weights/activations of the full-precision networks as done in
previous work, we train the quantization parameters jointly
with the weights by directly minimizing the task loss. As
a result, we achieved very promising results on the large
scale ImageNet classiﬁcation dataset. The 4-bit networks
obtained using our method preserve the accuracies of the
full-precision networks with various architectures, 3-bit networks yield comparable accuracy to the full-precision networks, and the 2-bit networks suffers from minimal accuracy loss. Our quantizer also achieves good quantization
performance that outperforms the existing methods even
when trained on a heterogeneous dataset, which makes it
highly practical in situations where we have pretrained networks without access to the original training data. Future
work may include more accurate parameterization of the
quantization intervals with piecewise linear functions and
use of Bayesian approaches.