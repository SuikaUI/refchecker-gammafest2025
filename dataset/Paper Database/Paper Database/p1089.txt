Deep Networks with Stochastic Depth
Gao Huang*, Yu Sun*, Zhuang Liu†, Daniel Sedra, Kilian Q. Weinberger
* Authors contribute equally
{gh349, ys646, dms422, kqw4}@cornell.edu, Cornell University
† , Tsinghua University
Abstract. Very deep convolutional networks with hundreds of layers
have led to signiﬁcant reductions in error on competitive benchmarks.
Although the unmatched expressiveness of the many layers can be highly
desirable at test time, training very deep networks comes with its own
set of challenges. The gradients can vanish, the forward ﬂow often diminishes, and the training time can be painfully slow. To address these
problems, we propose stochastic depth, a training procedure that enables
the seemingly contradictory setup to train short networks and use deep
networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass
them with the identity function. This simple approach complements the
recent success of residual networks. It reduces training time substantially
and improves the test error signiﬁcantly on almost all data sets that we
used for evaluation. With stochastic depth we can increase the depth
of residual networks even beyond 1200 layers and still yield meaningful
improvements in test error (4.91% on CIFAR-10).
Introduction
Convolutional Neural Networks (CNNs) were arguably popularized within the
vision community in 2009 through AlexNet and its celebrated victory at the
ImageNet competition . Since then there has been a notable shift towards
CNNs in many areas of computer vision . As this shift unfolds, a
second trend emerges; deeper and deeper CNN architectures are being developed
and trained. Whereas AlexNet had 5 convolutional layers , the VGG network
and GoogLeNet in 2014 had 19 and 22 layers respectively , and most recently
the ResNet architecture featured 152 layers .
Network depth is a major determinant of model expressiveness, both in theory and in practice . However, very deep models also introduce
new challenges: vanishing gradients in backward propagation, diminishing feature reuse in forward propagation, and long training time.
Vanishing Gradients is a well known nuisance in neural networks with many
layers . As the gradient information is back-propagated, repeated multiplication or convolution with small weights renders the gradient information ineffectively small in earlier layers. Several approaches exist to reduce this eﬀect in
practice, for example through careful initialization , hidden layer supervision
 , or, recently, Batch Normalization .
 
Gao Huang*, Yu Sun*, Zhuang Liu, Daniel Sedra, Kilian Q. Weinberger
Diminishing feature reuse during forward propagation (also known as loss in
information ﬂow ) refers to the analogous problem to vanishing gradients in
the forward direction. The features of the input instance, or those computed by
earlier layers, are “washed out” through repeated multiplication or convolution
with (randomly initialized) weight matrices, making it hard for later layers to
identify and learn “meaningful” gradient directions. Recently, several new architectures attempt to circumvent this problem through direct identity mappings
between layers, which allow the network to pass on features unimpededly from
earlier layers to later layers .
Long training time is a serious concern as networks become very deep. The
forward and backward passes scale linearly with the depth of the network. Even
on modern computers with multiple state-of-the-art GPUs, architectures like the
152-layer ResNet require several weeks to converge on the ImageNet dataset .
The researcher is faced with an inherent dilemma: shorter networks have
the advantage that information ﬂows eﬃciently forward and backward, and can
therefore be trained eﬀectively and within a reasonable amount of time. However, they are not expressive enough to represent the complex concepts that are
commonplace in computer vision applications. Very deep networks have much
greather model complexity, but are very diﬃcult to train in practice and require
a lot of time and patience.
In this paper, we propose deep networks with stochastic depth, a novel training algorithm that is based on the seemingly contradictory insight that ideally
we would like to have a deep network during testing but a short network during
training. We resolve this conﬂict by creating deep Residual Network architectures (with hundreds or even thousands of layers) with suﬃcient modeling
capacity; however, during training we shorten the network signiﬁcantly by randomly removing a substantial fraction of layers independently for each sample or
mini-batch. The eﬀect is a network with a small expected depth during training,
but a large depth during testing. Although seemingly simple, this approach is
surprisingly eﬀective in practice.
In extensive experiments we observe that training with stochastic depth substantially reduces training time and test error (resulting in multiple new records
to the best of our knowledge at the time of initial submission to ECCV). The reduction in training time can be attributed to the shorter forward and backward
propagation, so the training time no longer scales with the full depth, but the
shorter expected depth of the network. We attribute the reduction in test error
to two factors: 1) shortening the (expected) depth during training reduces the
chain of forward propagation steps and gradient computations, which strengthens the gradients especially in earlier layers during backward propagation; 2)
networks trained with stochastic depth can be interpreted as an implicit ensemble of networks of diﬀerent depths, mimicking the record breaking ensemble of
depth varying ResNets trained by He et al. .
We also observe that similar to Dropout , training with stochastic depth
acts as a regularizer, even in the presence of Batch Normalization . On ex-
Deep Networks with Stochastic Depth
periments with CIFAR-10, we increase the depth of a ResNet beyond 1000 layers
and still obtain signiﬁcant improvements in test error.
Background
Many attempts have been made to improve the training of very deep networks.
Earlier works adopted greedy layer-wise training or better initialization schemes
to alleviate the vanishing gradients and diminishing feature reuse problems . A notable recent contribution towards training of very deep networks is
Batch Normalization , which standardizes the mean and variance of hidden
layers with respect to each mini-batch. This approach reduces the vanishing
gradients problem and yields a strong regularizing eﬀect.
Recently, several authors introduced extra skip connections to improve the
information ﬂow during forward and backward propagation. Highway Networks
 allow earlier representations to ﬂow unimpededly to later layers through
parameterized skip connections known as “information highways”, which can
cross several layers at once. The skip connection parameters, learned during
training, control the amount of information allowed on these “highways”.
Residual networks (ResNets) simplify Highway Networks by shortcutting
(mostly) with identity functions. This simpliﬁcation greatly improves training
eﬃciency, and enables more direct feature reuse. ResNets are motivated by the
observation that neural networks tend to obtain higher training error as the
depth increases to very large values. This is counterintuitive, as the network gains
more parameters and therefore better function approximation capabilities. The
authors conjecture that the networks become worse at function approximation
because the gradients and training signals vanish when they are propagated
through many layers. As a ﬁx, they propose to add skip connections to the
network. Formally, if Hℓdenotes the output of the ℓth layer (or sequence of
layers) and fℓ(·) represents a typical convolutional transformation from layer
ℓ−1 to ℓ, we obtain
Hℓ= ReLU(fℓ(Hℓ−1) + id(Hℓ−1)),
where id(·) denotes the identity transformation and we assume a ReLU transition function . Fig. 1 illustrates an example of a function fℓ, which consists
of multiple convolutional and Batch Normalization layers. When the output dimensions of fℓdo not match those of Hℓ−1, the authors redeﬁne id(·) as a linear
projection to reduce the dimensions of id(Hℓ−1) to match those of fℓ(Hℓ−1). The
propagation rule in (1) allows the network to pass gradients and features (from
the input or those learned in earlier layers) back and forth between the layers
via the identity transformation id(·).
Dropout. Stochastically dropping hidden nodes or connections has been a
popular regularization method for neural networks. The most notable example is Dropout , which multiplies each hidden activation by an independent
Gao Huang*, Yu Sun*, Zhuang Liu, Daniel Sedra, Kilian Q. Weinberger
Residual Block
Convolution
Batch Norm.
Convolution
Batch Norm.
Fig. 1. A close look at the ℓth ResBlock in a ResNet.
Bernoulli random variable. Intuitively, Dropout reduces the eﬀect known as “coadaptation” of hidden nodes collaborating in groups instead of independently
producing useful features; it also makes an analogy with training an ensemble of
exponentially many small networks. Many follow up works have been empirically
successful, such as DropConnect , Maxout and DropIn .
Similar to Dropout, stochastic depth can be interpreted as training an ensemble of networks, but with diﬀerent depths, possibly achieving higher diversity
among ensemble members than ensembling those with the same depth. Diﬀerent
from Dropout, we make the network shorter instead of thinner, and are motivated by a diﬀerent problem. Anecdotally, Dropout loses eﬀectiveness when used
in combination with Batch Normalization . Our own experiments with
various Dropout rates (on CIFAR-10) show that Dropout gives practically no
improvement when used on 110-layer ResNets with Batch Normalization.
We view all of these previous approaches to be extremely valuable and consider our proposed training with stochastic depth complimentary to these eﬀorts.
In fact, in our experiments we show that training with stochastic depth is indeed
very eﬀective on ResNets with Batch Normalization.
Deep Networks with Stochastic Depth
Learning with stochastic depth is based on a simple intuition. To reduce the
eﬀective length of a neural network during training, we randomly skip layers
entirely. We achieve this by introducing skip connections in the same fashion
as ResNets, however the connection pattern is randomly altered for each minibatch. For each mini-batch we randomly select sets of layers and remove their
corresponding transformation functions, only keeping the identity skip connection. Throughout, we use the architecture described by He et al. . Because the
architecture already contains skip connections, it is straightforward to modify,
and isolates the beneﬁts of stochastic depth from that of the ResNet identity
connections. Next we describe this network architecture and then explain the
stochastic depth training procedure in detail.
ResNet architecture. Following He et al. , we construct our network as
the functional composition of L residual blocks (ResBlocks), each encoding the
update rule (1). Fig. 1 shows a schematic illustration of the ℓth ResBlock. In
this example, fℓconsists of a sequence of layers: Conv-BN-ReLU-Conv-BN, where
Deep Networks with Stochastic Depth
Fig. 2. The linear decay of pℓillustrated on a ResNet with stochastic depth for p0 =1
and pL = 0.5. Conceptually, we treat the input to the ﬁrst ResBlock as H0, which is
always active.
Conv and BN stand for Convolution and Batch Normalization respectively. This
construction scheme is adopted in all our experiments except ImageNet, for
which we use the bottleneck block detailed in He et al. . Typically, there are
64, 32, or 16 ﬁlters in the convolutional layers (see Section 4 for experimental
Stochastic depth aims to shrink the depth of a network during training, while
keeping it unchanged during testing. We can achieve this goal by randomly
dropping entire ResBlocks during training and bypassing their transformations
through skip connections. Let bℓ∈{0, 1} denote a Bernoulli random variable,
which indicates whether the ℓth ResBlock is active (bℓ= 1) or inactive (bℓ= 0).
Further, let us denote the “survival” probability of ResBlock ℓas pℓ= Pr(bℓ= 1).
With this deﬁnition we can bypass the ℓth ResBlock by multiplying its function fℓwith bℓand we extend the update rule from (1) to
Hℓ= ReLU(bℓfℓ(Hℓ−1) + id(Hℓ−1)).
If bℓ= 1, eq. (2) reduces to the original ResNet update (1) and this ResBlock
remains unchanged. If bℓ=0, the ResBlock reduces to the identity function,
Hℓ= id(Hℓ−1).
This reduction follows from the fact that the input Hℓ−1 is always non-negative,
at least for the architectures we use. For ℓ≥2, it is the output of the previous
ResBlock, which is non-negative because of the ﬁnal ReLU transition function
(see Fig. 1). For ℓ=1, its input is the output of a Conv-BN-ReLU sequence that
begins the architecture before the ﬁrst ResBlock. For non-negative inputs the
ReLU transition function acts as an identity.
The survival probabilities pℓare new hyper-parameters of our training procedure. Intuitively, they should take on similar values for neighboring ResBlocks.
One option is to set pℓ= pL uniformly for all ℓto obtain a single hyper-parameter
Gao Huang*, Yu Sun*, Zhuang Liu, Daniel Sedra, Kilian Q. Weinberger
pL. Another possibility is to set them according to a smooth function of ℓ. We
propose a simple linear decay rule from p0 = 1 for the input, to pL for the last
See Fig. 2 for a schematic illustration. The linearly decaying survival probability
originates from our intuition that the earlier layers extract low-level features
that will be used by later layers and should therefore be more reliably present. In
Section 4 we perform a more detailed empirical comparison between the uniform
and decaying assignments for pℓ. We conclude that the linear decay rule (4) is
preferred and, as training with stochastic depth is surprisingly stable with respect
to pL, we set pL = 0.5 throughout (see Fig. 8).
Expected network depth. During the forward-backward pass the transformation fℓis bypassed with probability (1 −pℓ), leading to a network with reduced depth. With stochastic depth, the number of eﬀective ResBlocks during
training, denoted as ˜L, becomes a random variable. Its expectation is given by:
E(˜L) = PL
Under the linear decay rule with pL = 0.5, the expected number of ResBlocks
during training reduces to E(˜L) = (3L −1)/4, or E(˜L) ≈3L/4 when L is large.
For the 110-layer network with L = 54 commonly used in our experiments, we
have E(˜L) ≈40. In other words, with stochastic depth, we train ResNets with
an average number of 40 ResBlocks, but recover a ResNet with 54 blocks at test
time. This reduction in depth signiﬁcantly alleviates the vanishing gradients and
the information loss problem in deep ResNets. Note that because the connectivity
is random, there will be updates with signiﬁcantly shorter networks and more
direct paths to individual layers. We provide an empirical demonstration of this
eﬀect in Section 5.
Training time savings. When a ResBlock is bypassed for a speciﬁc iteration,
there is no need to perform forward-backward computation or gradient updates.
As the forward-backward computation dominates the training time, stochastic
depth signiﬁcantly speeds up the training process. Following the calculations
above, approximately 25% of training time could be saved under the linear decay rule with pL = 0.5. The timings in practice using our implementation are
consistent with this analysis (see the last paragraph of Section 4). More computational savings can be obtained by switching to a uniform probability for
pℓor lowering pL accordingly. In fact, Fig. 8 shows that with pL = 0.2, the
ResNet with stochastic depth obtains the same test error as its constant depth
counterpart on CIFAR-10 but gives a 40% speedup.
Implicit model ensemble. In addition to the predicted speedups, we also
observe signiﬁcantly lower testing errors in our experiments, in comparison with
ResNets of constant depth. One explanation for our performance improvements
is that training with stochastic depth can be viewed as training an ensemble
of ResNets implicitly. Each of the L layers is either active or inactive, resulting
Deep Networks with Stochastic Depth
Table 1. Test error (%) of ResNets trained with stochastic depth compared to other
most competitive methods previously published (whenever available). A ”+” in the
name denotes standard data augmentation. ResNet with constant depth refers to our
reproduction of the experiments by He et al.
CIFAR10+ CIFAR100+ SVHN ImageNet
Maxout 
DropConnect 
Net in Net 
Deeply Supervised 
Frac. Pool 
All-CNN 
Learning Activation 
R-CNN 
Scalable BO 
Highway Network 
Gen. Pool 
ResNet with constant depth
ResNet with stochastic depth
in 2L possible network combinations. For each training mini-batch one of the
2L networks (with shared weights) is sampled and updated. During testing all
networks are averaged using the approach in the next paragraph.
Stochastic depth during testing requires small modiﬁcations to the network.
We keep all functions fℓactive throughout testing in order to utilize the fulllength network with all its model capacity. However, during training, functions
fℓare only active for a fraction pℓof all updates, and the corresponding weights
of the next layer are calibrated for this survival probability. We therefore need to
re-calibrate the outputs of any given function fℓby the expected number of times
it participates in training, pℓ. The forward propagation update rule becomes:
= ReLU(pℓfℓ(HTest
ℓ−1 ; Wℓ) + HTest
From the model ensemble perspective, the update rule (5) can be interpreted
as combining all possible networks into a single test architecture, in which each
layer is weighted by its survival probability.
We empirically demonstrate the eﬀectiveness of stochastic depth on a series of
benchmark data sets: CIFAR-10, CIFAR-100 , SVHN , and ImageNet .
Implementation details. For all data sets we compare the results of ResNets
with our proposed stochastic depth and the original constant depth, and other
most competitive benchmarks. We set pℓwith the linear decay rule of p0 = 1
and pL = 0.5 throughout. In all experiments we report the test error from the
epoch with the lowest validation error. For best comparisons we use the same
Gao Huang*, Yu Sun*, Zhuang Liu, Daniel Sedra, Kilian Q. Weinberger
110−layer ResNet on CIFAR−10
test error (%)
training loss
Test Error with Constant Depth
Test Error with Stochastic Depth
Training Loss with Constant Depth
Training Loss with Stochastic Depth
110−layer ResNet on CIFAR−100
test error (%)
training loss
Test Error with Constant Depth
Test Error with Stochastic Depth
Training Loss with Constant Depth
Training Loss with Stochastic Depth
Fig. 3. Test error on CIFAR-10 (left) and CIFAR-100 (right) during training, with
data augmentation, corresponding to results in the ﬁrst two columns of Table 1.
construction scheme (for constant and stochastic depth) as described by He et
al. . In the case of CIFAR-100 we use the same 110-layer ResNet used by He
et al. for CIFAR-10, except that the network has a 100-way softmax output.
Each model contains three groups of residual blocks that diﬀer in number of
ﬁlters and feature map size, and each group is a stack of 18 residual blocks. The
numbers of ﬁlters in the three groups are 16, 32 and 64, respectively. For the
transitional residual blocks, i.e. the ﬁrst residual block in the second and third
group, the output dimension is larger than the input dimension. Following He et
al. , we replace the identity connections in these blocks by an average pooling
layer followed by zero paddings to match the dimensions. Our implementations
are in Torch 7 . The code to reproduce the results is publicly available on
GitHub at 
CIFAR-10. CIFAR-10 is a dataset of 32-by-32 color images, representing
10 classes of natural scene objects. The training set and test set contain 50,000
and 10,000 images, respectively. We hold out 5,000 images as validation set, and
use the remaining 45,000 as training samples. Horizontal ﬂipping and translation
by 4 pixels are the two standard data augmentation techniques adopted in our
experiments, following the common practice .
The baseline ResNet is trained with SGD for 500 epochs, with a mini-batch
size 128. The initial learning rate is 0.1, and is divided by a factor of 10 after
epochs 250 and 375. We use a weight decay of 1e-4, momentum of 0.9, and
Nesterov momentum with 0 dampening, as suggested by . For stochastic
depth, the network structure and all optimization settings are exactly the same
as the baseline. All settings were chosen to match the setup of He et al. .
The results are shown in Table 1. ResNets with constant depth result in a
competitive 6.41% error on the test set. ResNets trained with stochastic depth
yield a further relative improvement of 18% and result in 5.25% test error. To
our knowledge this is signiﬁcantly lower than the best existing single model performance (6.05%) on CIFAR-10 prior to our submission, without resorting
Deep Networks with Stochastic Depth
152−layer ResNet on SVHN
test error (%)
training loss
Test Error with Constant Depth
Test Error with Stochastic Depth
Training Loss with Constant Depth
Training Loss with Stochastic Depth
1202−layer ResNet on CIFAR−10
test error (%)
training loss
Test Error with Constant Depth
Test Error with Stochastic Depth
Training Loss with Constant Depth
Training Loss with Stochastic Depth
Fig. 4. Left: Test error on SVHN, corresponding to results on column three in Table 1.
right: Test error on CIFAR-10 using 1202-layer ResNets. The points of lowest validation
errors are highlighted in each case.
to massive data augmentation .1 Fig. 3 (left) shows the test error as a
function of epochs. The point selected by the lowest validation error is circled
for both approaches. We observe that ResNets with stochastic depth yield lower
test error but also slightly higher ﬂuctuations (presumably due to the random
depth during training).
CIFAR-100. Similar to CIFAR-10, CIFAR-100 contains 32-by-32 color images with the same train-test split, but from 100 classes. For both the baseline
and our method, the experimental settings are exactly the same as those of
CIFAR-10. The constant depth ResNet yields a test error of 27.22%, which is
already the state-of-the-art in CIFAR-100 with standard data augmentation.
Adding stochastic depth drastically reduces the error to 24.98%, and is again
the best published single model performance to our knowledge (see Table 1 and
Fig. 3 right).
We also experiment with CIFAR-10 and CIFAR-100 without data augmentation. ResNets with constant depth obtain 13.63% and 44.74% on CIFAR-10
and CIFAR-100 respectively. Adding stochastic depth yields consistent improvements of about 15% on both datasets, resulting in test errors of 11.66% and
37.8% respectively.
SVHN. The format of the Street View House Number (SVHN) dataset
that we use contains 32-by-32 colored images of cropped out house numbers
from Google Street View. The task is to classify the digit at the center. There
are 73,257 digits in the training set, 26,032 in the test set and 531,131 easier
samples for additional training. Following the common practice, we use all the
training samples but do not perform data augmentation. For each of the ten
classes, we randomly select 400 samples from the training set and 200 from the
additional set, forming a validation set with 6,000 samples in total. We preprocess
1 The only model that performs even better is the 1202-layer ResNet with stochastic
depth, discussed later in this section.
Gao Huang*, Yu Sun*, Zhuang Liu, Daniel Sedra, Kilian Q. Weinberger
Table 2. Training time comparison on benchmark datasets.
CIFAR10+ CIFAR100+ SVHN
Constant Depth
Stochastic Depth
the data by subtracting the mean and dividing the standard deviation. Batch
size is set to 128, and validation error is calculated every 200 iterations.
Our baseline network has 152 layers. It is trained for 50 epochs with a beginning learning rate of 0.1, divided by 10 after epochs 30 and 35. The depth
and learning rate schedule are selected by optimizing for the validation error of
the baseline through many trials. This baseline obtains a competitive result of
1.80%. However, as seen in Fig. 4, it starts to overﬁt at the beginning of the
second phase with learning rate 0.01, and continues to overﬁt until the end of
training. With stochastic depth, the error improves to 1.75%, the second-best
published result on SVHN to our knowledge after .
Training time comparison. We compare the training eﬃciency of the constant depth and stochastic depth ResNets used to produce the previous results.
Table 2 shows the training (clock) time under both settings with the linear decay rule pL = 0.5. Stochastic depth consistently gives a 25% speedup, which
conﬁrms our analysis in Section 3. See Fig. 8 and the corresponding section on
hyper-parameter sensitivity for more empirical analysis.
110 layers
1202 layers
test error (%)
Constant Depth
Stochastic Depth
Fig. 5. With stochastic depth, the 1202layer ResNet still signiﬁcantly improves
over the 110-layer one.
Training with a 1202-layer ResNet.
He et al. tried to learn CIFAR-
10 using an aggressively deep ResNet
with 1202 layers. As expected, this extremely deep network overﬁtted to the
training set: it ended up with a test
error of 7.93%, worse than their 110layer network. We repeat their experiment on the same 1202-layer network,
with constant and stochastic depth.
We train for 300 epochs, and set the
learning rate to 0.01 for the ﬁrst 10
epochs to “warm-up” the network and
facilitate initial convergence, then restore it to 0.1, and divide it by 10 at
epochs 150 and 225.
The results are summarized in Fig. 4 (right) and Fig. 5. Similar to He et
al. , the ResNets with constant depth of 1202 layers yields a test error of
6.67%, which is worse than the 110-layer constant depth ResNet. In contrast, if
trained with stochastic depth, this extremely deep ResNet performs remarkably
well. We want to highlight two trends: 1) Comparing the two 1202-layer nets
shows that training with stochastic depth leads to a 27% relative improvement; 2)
Deep Networks with Stochastic Depth
Comparing the two networks trained with stochastic depth shows that increasing
the architecture from 110 layers to 1202 yields a further improvement on the
previous record-low 5.25%, to a 4.91% test error without sign of overﬁtting, as
shown in Fig. 4 (right) 2.
To the best of our knowledge, this is the lowest known test error on CIFAR-10
with moderate image augmentation and the ﬁrst time that a network with more
than 1000 layers has been shown to further reduce the test error 3. We consider
these ﬁndings highly encouraging and hope that training with stochastic depth
will enable researchers to leverage extremely deep architectures in the future.
152−layer ResNet on ImageNet
validation error (%)
Constant Depth
Stochastic Depth
Fig. 6. Validation error on ILSVRC 2012
classiﬁcation.
ImageNet. The ILSVRC 2012 classiﬁcation dataset consists of 1000
classes of images, in total 1.2 million for training, 50,000 for validation, and 100,000 for testing. Following the common practice, we only report the validation errors. We follow He et al. to build a 152-layer
ResNet with 50 bottleneck residual
blocks. When input and output dimensions do not match, the skip connection uses a learned linear projection for the mismatching dimensions,
and an identity transformation for
the other dimensions. Our implementation is based on the github repository
fb.resnet.torch4 , and the optimization settings are the same as theirs,
except that we use a batch size of 128 instead of 256 because we can only spread
a batch among 4 GPUs (instead of 8 as they did).
We train the constant depth baseline for 90 epochs (following He et al. and
the default setting in the repository) and obtain a ﬁnal error of 23.06%. With
stochastic depth, we obtain an error of 23.38% at epoch 90, which is slightly
higher. We observe from Fig.6 that the downward trend of the validation error
with stochastic depth is still strong, and from our previous experience, could
beneﬁt from further training. Due to the 25% computational saving, we can add
30 epochs (giving 120 in total, after decreasing the learning rate to 1e-4 at epoch
90), and still ﬁnish in almost the same total time as 90 epochs of the baseline.
This reaches a ﬁnal error of 21.98%. We have also kept the baseline running for
30 more epochs. This reaches a ﬁnal error of 21.78%.
Because ImageNet is a very complicated and large dataset, the model complexity required could potentially be much more than that of the 152-layer
2 We do not include this result in Table 1 since this architecture was only trained on
one of the datasets.
3 This is, until early March, 2016, when this paper was submitted to ECCV. Many
new developments have further decreased the error on CIFAR-10 since then (and
some are based on this work).
4 
Gao Huang*, Yu Sun*, Zhuang Liu, Daniel Sedra, Kilian Q. Weinberger
mean gradient magnitude
Constant Depth
Stochastic Depth
Fig. 7. The ﬁrst convolutional layer’s mean gradient magnitude for each epoch during
training. The vertical dotted lines indicate scheduled reductions in learning rate by a
factor of 10, which cause gradients to shrink.
ResNet . In the words of an anonymous reviewer, the current generation
of models for ImageNet are still in a diﬀerent regime from those of CIFAR. Although there seems to be no immediate beneﬁt from applying stochastic depth
on this particular architecture, it is possible that stochastic depth will lead to improvements on ImageNet with larger models, which the community might soon
be able to train as GPU capacities increase.
Analytic Experiments
In this section, we provide more insights into stochastic depth by presenting a
series of analytical results. We perform experiments to support the hypothesis
that stochastic depth eﬀectively addresses the problem of vanishing gradients in
backward propagation. Moreover, we demonstrate the robustness of stochastic
depth with respect to its hyper-parameter.
Improved gradient strength. Stochastically dropping layers during training
reduces the eﬀective depth on which gradient back-propagation is performed,
while keeping the test-time model depth unmodiﬁed. As a result we expect
training with stochastic depth to reduce the vanishing gradient problem in the
backward step. To empirically support this, we compare the magnitude of gradients to the ﬁrst convolutional layer of the ﬁrst ResBlock (ℓ=1) with and without
stochastic depth on the CIFAR-10 data set.
Fig. 7 shows the mean absolute values of the gradients. The two large drops
indicated by vertical dotted lines are due to scheduled learning rate division. It
can be observed that the magnitude of gradients in the network trained with
stochastic depth is always larger, especially after the learning rate drops. This
seems to support out claim that stochastic depth indeed signiﬁcantly reduces
the vanishing gradient problem, and enables the network to be trained more
eﬀectively. Another indication of the eﬀect is in the left panel of Fig. 3, where
one can observe that the test error of the ResNets with constant depth approximately plateaus after the ﬁrst drop of learning rate, while stochastic depth still
Deep Networks with Stochastic Depth
survival probability pL
test error (%)
110-layer ResNet on CIFAR-10 with Varying Survival Probabilities
Stochastic Depth (linear decay)
Stochastic Depth (uniform)
Constant Depth
network depth (in layers)
Fig. 8. Left: Test error (%) on CIFAR-10 with respect to the pL with uniform and
decaying assignments of pℓ. Right: Test error (%) heatmap on CIFAR-10 varyied over
pL and network depth.
improves the performance even after the learning rate drops for the second time.
This further supports that stochastic depth combines the beneﬁts of shortened
network during training with those of deep models at test time.
Hyper-parameter sensitivity. The survival probability pL is the only hyperparameter of our method. Although we used pL = 0.5 throughout all our experiments, it is still worth investigating the sensitivity of stochastic depth with
respect to its hyper-parameter. To this end, we compare the test error of the
110-layer ResNet under varying values of pL (L = 54) for both linear decay and
uniform assignment rules on the CIFAR-10 data set in Fig. 8 (left). We make
the following observations: 1) both assignment rules yield better results than
the baseline when pL is set properly; 2) the linear decay rule outperforms the
uniform rule consistently; 3) the linear decay rule is relatively robust to ﬂuctuations in pL and obtains competitive results when pL ranges from 0.4 to 0.8; 4)
even with a rather small survival probability e.g. pL = 0.2, stochastic depth with
linear decay still performs well, while giving a 40% reduction in training time.
This shows that stochastic depth can save training time substantially without
compromising accuracy.
The heatmap on the right shows the test error varied over both pL and
network depth. Not surprisingly, deeper networks (at least in the range of our
experiments) do better with a pL = 0.5. The ”valley” of the heatmap is along the
diagonal. A deep enough model is necessary for stochastic depth to signiﬁcantly
outperform the baseline (an observation we also make with the ImageNet data
set), although shorter networks can still beneﬁt from less aggressive skipping.
Conclusion
In this paper we introduced deep networks with stochastic depth, a procedure
to train very deep neural networks eﬀectively and eﬃciently. Stochastic depth
Gao Huang*, Yu Sun*, Zhuang Liu, Daniel Sedra, Kilian Q. Weinberger
reduces the network depth during training in expectation while maintaining the
full depth at testing time. Training with stochastic depth allows one to increase
the depth of a network well beyond 1000 layers, and still obtain a reduction in
test error. Because of its simplicity and practicality we hope that training with
stochastic depth may become a new tool in the deep learning “toolbox”, and
will help researchers scale their models to previously unattainable depths and
capabilities.
Acknowledgements. We thank the anonymous reviewers for their kind suggestions. Kilian Weinberger is supported by NFS grants IIS-1550179, IIS-1525919
and EFRI-1137211. Gao Huang is supported by the International Postdoctoral
Exchange Fellowship Program of China Postdoctoral Council (No.20150015). Yu
Sun is supported by the Cornell University Oﬃce of Undergraduate Research.
We also thank our lab mates, Matthew Kusner and Shuang Li for useful and
interesting discussions.