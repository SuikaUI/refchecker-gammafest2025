HAL Id: hal-01224806
 
Submitted on 5 Nov 2015
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
BLSTM-RNN based 3D Gesture Classification
Grégoire Lefebvre, Samuel Berlemont, Franck Mamalet, Christophe Garcia
To cite this version:
Grégoire Lefebvre, Samuel Berlemont, Franck Mamalet, Christophe Garcia.
BLSTM-RNN based
3D Gesture Classification.
Artificial Neural Networks and Machine Learning, ICANN 2013, 23rd
International Conference on Artificial Neural Networks, Sep 2013, Sofia, Bulgaria. ￿hal-01224806￿
BLSTM-RNN based 3D Gesture Classiﬁcation
Gr´egoire Lefebvre1, Samuel Berlemont1,2,
Franck Mamalet1, and Christophe Garcia2
1Orange Labs, R&D, France
{firstname.surname}@orange.com
2LIRIS, UMR 5205 CNRS, INSA-Lyon, F-69621, France.
{firstname.surname}@liris.cnrs.fr
Abstract. This paper presents a new robust method for inertial MEM
(MicroElectroMechanical systems) 3D gesture recognition. The linear acceleration and the angular velocity, respectively provided by the accelerometer and the gyrometer, are sampled in time resulting in 6D values
at each time step which are used as inputs for the gesture recognition
system. We propose to build a system based on Bidirectional Long Short-
Term Memory Recurrent Neural Networks (BLSTM-RNN) for gesture
classiﬁcation from raw MEM data. We also compare this system to a geometric approach using DTW (Dynamic Time Warping) and a statistical
method based on HMM (Hidden Markov Model) from ﬁltered and denoised MEM data. Experimental results on 22 individuals producing 14
gestures in the air show that the proposed approach outperforms classical classiﬁcation methods with a classiﬁcation mean rate of 95.57% and
a standard deviation of 0.50 for 616 test gestures. Furthermore, these
experiments underline that combining accelerometer and gyrometer information gives better results that using a single inertial description.
Keywords: LSTM-RNN, DTW, HMM, MEM, hand gesture recognition
Introduction
Accelerometers and gyrometers are nowadays present in our everyday Smartphones. These sensors capture hand movements when users grasp their devices.
We can consider two main issues: posture recognition and symbolic gesture recognition. In the ﬁrst case, the user maintains a posture during a certain period
of time, describing for instance the fact that the device is upside down. In the
second situation, the user may produce a gesture to execute a system command,
like drawing a heart symbol in 3D space to call its favorite phone number. Dynamic gesture recognition based on inertial sensors is a very challenging task.
Algorithms are confronted to numerous factors causing errors in the recognition
process: dynamical diﬀerences (intensive versus phlegmatic gestures), temporal
diﬀerences (slow versus fast movements), physical constraints (device weight,
human body elasticity, left or right-handed, seated or standing up, on the move,
etc.), classiﬁcation constraints (mono versus multi users, open or closed world
paradigm, etc.). Classically, several steps operate from signal data preprocessing
G. Lefebvre, S. Berlemont, F. Mamalet, C. Garcia
to gesture classiﬁcation with some intermediate steps like data clustering and
gesture model learning. The preprocessing steps aim at reducing the input signals that characterize the corresponding gestures. Diﬀerent methods can then
be applied: calibration, ﬁltering, normalization or vectorization. Data clustering
is often applied to reduce the input space dimension and ﬁnd class referent gesture vectors. A learning phase of a gesture model follows this clustering step and
ﬁnally a decision rule or a speciﬁc classiﬁer is built to label the input data as a
recognized gesture or an unknown gesture. In this article, we propose to learn
an eﬃcient gesture classiﬁer without any preprocessing method (i.e. from raw
MEM data) using a BLSTM-RNN model.
This paper is organized as follows. In Section 2, sensor-based gesture recognition is described with a survey. Section 3 presents our recognition method.
Section 4 describes the experimental results. Finally, conclusions are drawn.
Accelerometer based 3D Gesture Recognition
3D gesture recognition using accelerometers has been studied in recent years,
and for gesture classiﬁcation three main strategies stand out which are based on
statistics, on geometry or on boosting classiﬁer approaches.
The ﬁrst strategy has been deeply studied in the last decade with two main
approaches: discrete versus continuous HMM . Hofmann et al. proposed to use discrete HMM (dHMM) for recognizing dynamic gestures thanks
to their velocity proﬁle. This approach consists of two levels and stages of recognition: a low-level stage essentially dividing the input data space into diﬀerent
regions and assigning each of them (i.e. creation of a vector codebook), and a
high-level stage taking the sequences of vector indexes from the ﬁrst stage and
classifying them with discrete HMM. The experiments are built using a training
set with 10 samples per gesture, each sample representing hand orientation, acceleration data and ﬁnger joint angle. A vector codebook is obtained by an input
space clustering method (i.e. K-means algorithm). Clustering essentially serves
as an unsupervised learning procedure to model the shape of the feature vector
distribution in the input data space. Here, the number of HMM states vary from
1 to 10 and the observation alphabet size equals to 120. The comparison between
ergodic HMM and left-to-right HMM shows similar results with 95.6% correct
recognition rate for 100 gestures. Similar results are presented in . Kallio et
al. use 5 HMM states and a codebook size of 8 for 16 gestures. The authors
highlight that the performances decrease when using 4 sequences for training the
system compared to 20 sequences. The recognition rate falls from 95% to 75%
even for this mono-user case study. In , a 37 multi-user case is studied with
8 gestures, evaluating the eﬀect of vector quantization and sampling. A rate of
96.1% of correct classiﬁcation is obtained with 5 HMM states and a codebook
size of 8. However, this study can be seen as biased since the K-means clustering
is performed from all the available data set and not only the training database.
In opposition to the previous studies, and to take into consideration that gesture
data are correlated in time, Pylv¨an¨ainen proposes in to build a system based
3D Gesture Classiﬁcation with BLSTM-RNN
on continuous HMM (cHMM). Again, the results are convincing, with 96.76%
on a dataset providing 20 samples for 10 gestures realized by 7 persons.
The second strategy for recognizing 3D gestures is based on geometric models
with distance computation. The goal is to provide a gallery of some gesture references to model each gesture class and design a decision rule for a test gesture
regarding the respective distance to these referent instances. On the contrary to
the HMM strategy, no learning phase is needed but computational time is required for a test gesture to be compared to all referent instances. Consequently,
the main drawback of this approach is the necessity to ﬁnd the most relevant
samples to represent a gesture class while keeping the number of these referents
low in order to minimize the ﬁnal evaluation processing time. Wilson et al. in
 compare Linear Time Warping (LTW) and Dynamic Time Warping (DTW)
to the HMM based strategy. Their experiment with 7 types of gesture from 6
users shows an advantage for HMM with 90% in opposition to the score of LTW
and DTW of respectively 40% and 71%. Liu et al. experiment with more success
the DTW strategy in . Gesture recognition and user identiﬁcation are performed with good recognition rates of respectively 93.5% and 88%. The authors
introduce an averaging window of 50 ms for reducing noise and erratic moves.
The gesture data, performed over multiple days, consists of 30 samples of 8
gestures for 8 individuals and the user recognition results are obtained from 25
participants. Likewise, in , Akl et al. use DTW and aﬃnity propagation for dimension reduction for recognizing 3D gestures. 7 subjects participated producing
3700 gesture traces for a good classiﬁcation rate of 90%.
The third strategy for recognizing 3D gestures is to learn a speciﬁc classiﬁer.
Hoﬀman et al. (see ) improve 3D gesture recognition with a linear classiﬁer
and Adaboost, inspired by the method proposed in for 2D symbol writer
recognition. The experiments show an accuracy of 98% for 13 gestures made by
17 participants. Other studies focus on SVM (i.e. Support Vector Machine) like
in . This study uses frame-based descriptors. Each gesture is divided into
segments where are computed to form descriptors: mean, energy, entropy, standard deviation and correlation. These descriptors constitute the feature vector
to be classiﬁed by a multi-class SVM. The obtained results are 95.21% of good
recognition for 12 gestures made by 10 individuals.
Consequently, many strategies are explored with diﬀerent paradigms and
speciﬁc data processing methods on diﬀerent databases. Nevertheless, theses
approaches suﬀer from ﬁnding automatically the relevant parameters (e.g. signal
processing, etc.) to deal with gesture variabilities. We develop hereafter our 3D
gesture recognition method based on BLSTM-RNN from raw input data and
compare it with classical methods on a common database.
The proposed 3D Gesture Recognition Method
Bidirectional Long Short-Term Memory RNNs
Classical RNNs are a common learning technique for temporal analysis of data
since they are able to take into consideration the temporal context. This is
G. Lefebvre, S. Berlemont, F. Mamalet, C. Garcia
achieved by using recurrent connections within the hidden layer which allow the
network to remember a state representing the previous input values. However,
Hochreiter and Schmidhuber in have shown that if RNNs can handle shorttime lags between inputs, the problem of exponential error decay prevent them
from tackling real-life long-term dependencies. They introduced thus the Long
Short Term Memory RNNs, that allows a constant error signal propagation
through time using a special node called constant error carousel (CEC) and
multiplicative gates (Fig 1.a). These gates are neurons that can set (input gate),
reset (forget gate) or hide (output gate) the internal value of the CEC according
to neuron input values and context.
LSTM-RNNs have proven their great ability to deal with temporal data in
many applications (e.g. phoneme classiﬁcation , action classiﬁcation ). In
this paper we consider gesture data using 6D input vectors through sampling
timestep. These data are correlated during the user gestural production, and time
lags between the beginning and the end of gesture can be long. For these reasons,
LSTM-RNN is chosen to classify the input MEM data sequence. Furthermore,
since gesture recognition, at a given timestep, may depend on past and future
context, we use Bidirectional LSTM-RNN (BLSTM-RNN), introduced in ,
that consists in two separate hidden layers, the forward (resp. backward) layer
able to deal with past (resp. future) context. The output layer is connected to
both hidden layers in order to fuse past and future contexts.
Fig. 1. (a) LSTM neuron. (b) BLSTM-RNN Architecture.
BLSTM-RNN Architecture, Training and Decision Rule
The proposed gesture classiﬁcation scheme based on BLSTM-RNN is described
in Figure 1.b. First, the input layer consists in the concatenation of accelerometer and gyrometer information synchronized in time (i.e. 6 input values per
timestep). Notice that our system relies only on the raw MEMs data, without
any preprocessing in opposition to most of state-of-the-art methods. These data
3D Gesture Classiﬁcation with BLSTM-RNN
are linearly normalized between -1 and +1 according to the maximum value
that sensors can provide. The forward and backward LSTM hidden layers are
fully connected to the input layer and consist in 100 LSTM neurons each with
full recurrent connections. The output layer has a size equals to the number of
gesture to classify. The SoftMax activation function is used for this layer to give
network responses between 0 and 1 at every timestep. Classically, these outputs
can be considered as posterior probabilities of the input sequence to belong to
a speciﬁc category at a given timestep. This network is learned using classical
on-line backpropagation through time with momentum (i.e. learning rate 5e−4,
momentum 0.2), as described in , on a training set, by targeting the same
corresponding gesture class at each time step for each input example. For evaluation of a new gesture sequence, we use a majority voting rule over the outputs
along the sequence (i.e. keeping only the most probable class at each time step)
to determine the ﬁnal gesture class.
Experimental Results
There is no public dataset for comparison of 3D gesture recognition. Therefore,
we have collected our 3D gesture dataset to compare classiﬁcation methods.
Our dataset has been captured on an Android Nexus S Samsung device. 22
participants, from 20 to 55 years old, all right-handed, performed 5 times each of
the 14 symbolic gestures. This corresponds to 1540 temporal segmented gestures.
The sampling time for accelerometer and gyroscope capture is 40 ms. The 14
symbolic gestures are divided into 2 families: linear gestures (e.g. north, south,
east and west ﬂicks, and up, down, pick and throw gestures) and curvilinear
gestures (e.g. alpha, heart, letter N, letter Z, clockwise and counter-clockwise).
These choices make the dataset diﬃcult. There are classically confusions between
ﬂick gestures and letter N and Z. Likewise, the clockwise movement is often
confused with alpha or heart symbols. Hereafter, we use temporal segmented
gestures where only useful data are eﬃcient to classify the inputs.
We use 3 diﬀerent conﬁgurations to compare our solution based on BLSTM-
RNN to 3 state-of-the-art solutions: DTW, dHMM and cHMM based methods.
The DTW solution uses a 5 nearest neighbor classiﬁcation and the HMM
solution uses the maximum of likelihood as a decision rule . In all experiments,
we use a ﬁltered and vectorized gestural information for these methods and raw
MEM information for LSTM solution. In the following, we use a 3-fold cross
validation.
The ﬁrst conﬁguration (DB1) corresponds to the personalization paradigm,
where only one user is considered with few learning examples. For this conﬁguration we have used the 70 gestures of a single participant in the learning phase,
and ask him to process 16 more instances of each gesture for test (i.e. 224 gestures). The second conﬁguration (DB2) uses 3 instances of each gesture per user
for the learning phase: 924 gestures (i.e. 60% of all data) are used for the learning phase and 616 gestures (i.e. 40%) for the test phase. This case corresponds
to a multi-user system and a closed world paradigm. The third conﬁguration
G. Lefebvre, S. Berlemont, F. Mamalet, C. Garcia
(DB3) is composed of all samples from 17 users (i.e. 1190 gestures) and the test
data uses the other available gestures (i.e. 350 gestures from unknown users).
This case is close to a real system trained with a few examples and having to
generalize to new users who want to use it without any personalization phase.
Here, the conﬁguration represents the open world paradigm.
Table 1. Good classiﬁcation rates on DB1, DB2 and DB3.
Mean & Standard Deviation
99.40% ± 0.21%
92.59% ± 0.20%
90.29% ± 2.07%
95.39% ± 0.56%
80.63% ± 2.39%
79.81% ± 1.72%
DTW acc+gyro
99.70% ± 0.42%
94.04% ± 0.15%
91.71% ± 1.46%
77.14% ± 5.18%
64.09% ± 1.60%
63.81% ± 0.58%
57.50% ± 3.24%
43.13% ± 2.35%
49.05% ± 1.15%
dHMM acc+gyro
81.02% ± 3.72%
69.46% ± 2.11%
66.95% ± 1.87%
99.02% ± 0.81%
83.99% ± 1.09%
80.09% ± 2.82%
95.05% ± 2.62%
70.92% ± 0.74%
70.76% ± 0.58%
cHMM acc+gyro
99.86% ± 0.02%
85.79% ± 0.67%
82.76% ± 1.41%
BLSTM-RNN acc
84.15% ± 0.67%
94.86% ± 1.23%
89.42% ± 2.45%
BLSTM-RNN gyro
68.90% ± 4.85%
83.39% ± 0.65%
74.19% ± 1.55%
BLSTM-RNN acc+gyro
86.75% ± 0.75%
95.57% ± 0.50% 92.57% ± 2.85%
Classiﬁcation Results Table 1 outlines the global performances of each classiﬁer for conﬁgurations DB1, DB2 and DB3 coupling or not accelerometer and
gyrometer data. Considering coupled input data (accelerometer+gyroscope), this
table shows that our BLSTM-RNN based classiﬁer gives the best results on DB2
and DB3, with respectively 95.57 ± 0.50% and 92.57 ± 2.85%.
In the three conﬁgurations, the dHMM solution provides lower performances
which is mainly due to the input data variability and the complexity to determine
an automatic discriminant codebook.
On two conﬁgurations (DB2 and DB3), the DTW solution achieves the second best performance in mean recognition rate before the cHMM based one.
On DB1 conﬁguration, DTW and cHMM achieve equivalent performances
while our BLSTM-RNN approach is less eﬃcient. This is mainly due to the lack
of learning data which leads to the classical over-ﬁtting issue. The attempts made
with smaller LSTM networks did not allow any improvement on generalization.
When comparing these methods using a single input MEM sensor (accelerometer or gyroscope), we can see that using only gyroscope data is less eﬃcient
than using single accelerometer data. Moreover, when these two information are
combined, the performances increase with respectively 99.70 ± 0.42%, 94.04 ±
0.15% and 91.71 ± 1.46%, for instance, for the DTW based method on DB1,
DB2 and DB3 conﬁgurations.
Main conclusions of a deep analysis of confusion matrices (not provided here
due to lack of space) are the following. The main drawback for the cHMM based
3D Gesture Classiﬁcation with BLSTM-RNN
method in this context is the incorrect classiﬁcation of the N gestures with only
0.95% of correct classiﬁcation. 62.86% of the N gestures are confused with the
pick gestures. A strong confusion appears with opposite gestures as pick and
throw or down and up gestures. Opposite gestures may be mis-classiﬁed when
some user anticipate a north ﬂick gesture by slightly moving back the device
in the beginning of the production. On the contrary, the DTW based method
provide a good solution to classify linear gestures except for the throw gesture
which is often recognized as east and north ﬂicks, which can be explained by
the similar nature of production of these three gesture types. Our BLSTM-RNN
approach have some issue to distinguish the east ﬂick gesture from the letter Z
and the up gesture from the letter N, both sharing the same initial movement.
This may be due to the uniform learning target chosen (same class at each time
step), or the majority voting scheme in recognition phase.
Table 2. Computing time (in ms) to classify one unknown gesture.
Leaning samples
Test samples
DTW accgyro
11.93 ±0.02 34.57 ±0.47 44.58±0.38
dHMM accgyro
18.31 ±0.17 24.84 ±0.32 16.18±0.32
cHMM accgyro
42.53 ±1.97 23.89±2.74 30.19±1.65
BLSTM-RNN accgyro 30.47± 0.23 31.12±0.57 29.56±0.48
Computing Times Table 2 presents the computing times for all methods for
the 3 conﬁgurations in recognition phase executed on an Intel Core i5 CPU at
2.67 GHz with 3.42 Go of RAM. These experimental results show that the computing time for the BLSTM-RNN and HMM based solutions is quite constant
regarding the tasks on the diﬀerent database (i.e. around 30 ms for BLSTM-
RNN and 18 ms for dHMM to classify one input gesture for DB1). The learning process is built indeed oﬀ-line and consequently the recognition process is
fast. On the contrary, the DTW solution requires to compare the input gesture
with all learning reference samples. That is why the computing time increases
in mean from 11.93 ms for 70 learning samples to 44.58 ms for 1190 learning
samples. The DTW solution requires a small number of reference gestures and
which makes it hard to cover all user gesture variations. Consequently, the proposed system, based on BLSTM-RNN, achieving the best result performances
in multi-user conﬁguration with a recognition computing time independent of
training dataset size is a very challenging solution.
Conclusion and Perspectives
In this paper, we have presented a contribution based on BLSTM-RNN and a
comparison for inertial MEM based gesture recognition. This study about sym-
G. Lefebvre, S. Berlemont, F. Mamalet, C. Garcia
bolic gesture recognition compares our contribution to 3 classical pattern recognition methods: the geometric approach using DTW and the statistical method
based on dHMM and cHMM. We have shown that on multi-user conﬁguration
our approach achieves the best mean classiﬁcation rates, up to 95.57%, in a
closed world conﬁguration. Main remaining confusions with the proposed solution are when two 3D trajectories are similar or share some initial movements,
as an east ﬂick and a Z letter. New approach using a modiﬁed objective function,
such as Connectionist Temporal Classiﬁcation , that permits to jointly learn
to localize and classify events in input sequences, might be used to overcome
this issue or to classify non segmented gestures.