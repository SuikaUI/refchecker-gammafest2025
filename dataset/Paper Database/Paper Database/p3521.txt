Where are the Blobs:
Counting by Localization with Point Supervision
Issam H. Laradji1,2, Negar Rostamzadeh1, Pedro O. Pinheiro1, David
Vazquez1, Mark Schmidt2,1
1Element AI, Montreal, Canada
{negar,pedro,dvazquez}@elementai.com
2Dept. of Computer Science, University of British Columbia, Vancouver, Canada
{issamou,schmidtm}@cs.ubc.ca
Abstract. Object counting is an important task in computer vision
due to its growing demand in applications such as surveillance, traf-
ﬁc monitoring, and counting everyday objects. State-of-the-art methods
use regression-based optimization where they explicitly learn to count
the objects of interest. These often perform better than detection-based
methods that need to learn the more diﬃcult task of predicting the location, size, and shape of each object. However, we propose a detectionbased method that does not need to estimate the size and shape of the
objects and that outperforms regression-based methods. Our contributions are three-fold: (1) we propose a novel loss function that encourages
the network to output a single blob per object instance using pointlevel annotations only; (2) we design two methods for splitting large predicted blobs between object instances; and (3) we show that our method
achieves new state-of-the-art results on several challenging datasets including the Pascal VOC and the Penguins dataset. Our method even
outperforms those that use stronger supervision such as depth features,
multi-point annotations, and bounding-box labels.
Introduction
Object counting is an important task in computer vision with many applications
in surveillance systems , traﬃc monitoring , ecological surveys , and
cell counting . In traﬃc monitoring, counting methods can be used to track
the number of moving cars, pedestrians, and parked cars. They can also be used
to monitor the count of diﬀerent species such as penguins, which is important for
animal conservation. Furthermore, it has been used for counting objects present
in everyday scenes in challenging datasets where the objects of interest come
from a large number of classes such as the Pascal VOC dataset .
Many challenges are associated with object counting. Models need to learn
the variability of the objects in terms of shape, size, pose, and appearance.
Moreover, objects may appear at diﬀerent angles and resolutions, and may be
partially occluded (see Fig. 1). Also, the background, weather conditions, and
illuminations can vary widely across the scenes. Therefore, the model needs to
 
Laradji, Rostamzadeh, Pinheiro, Vazquez, Schmidt
Penguins: 10
Penguins: 28
Persons: 5
Fig. 1. Qualitative results on the Penguins and PASCAL VOC datasets . Our
method explicitly learns to localize object instances using only point-level annotations.
The trained model then outputs blobs where each unique color represents a predicted
object of interest. Note that the predicted count is simply the number of predicted
be robust enough to recognize objects in the presence of these variations in order
to perform eﬃcient object counting.
Due to these challenges, regression-based models such as “glance” and object density estimators have consistently deﬁned state-of-the-art results in object
counting . This is because their loss functions are directly optimized for predicting the object count. In contrast, detection-based methods need to optimize
for the more diﬃcult task of estimating the location, shape, and size of the object
instances. Indeed, perfect detection implies perfect count as the count is simply
the number of detected objects. However, models that learn to detect objects
often lead to worse results for object counting . For this reason, we look at
an easier task than detection by focusing on the task of simply localizing object
instances in the scene. Predicting the exact size and shape of the object instances
is not necessary and usually poses a much more diﬃcult optimization problem.
Therefore, we propose a novel loss function that encourages the model to output
instance regions such that each region contains a single object instance (i.e. a
single point-level annotation). Similar to detection, the predicted count is the
number of predicted instance regions (see Fig. 1). Our model only requires point
supervision which is a weaker supervision than bounding-box, and per-pixel annotations used by most detection-based methods . Consequently, we
can train our model for most counting datasets as they often have point-level
annotations.
This type of annotation is cheap to acquire as it requires lower human effort than bounding box and per-pixel annotations . Point-level annotations
provide a rough estimate of the object locations, but not their sizes nor shapes.
Our counting method uses the provided point annotations to guide its attention to the object instances in the scenes in order to learn to localize them. As
a result, our model has the ﬂexibility to predict diﬀerent sized regions for different object instances, which makes it suitable for counting objects that vary
in size and shape. In contrast, state-of-the-art density-based estimators often
assume a ﬁxed object size (deﬁned by the Gaussian kernel) or a constrained
environment which makes it diﬃcult to count objects with diﬀerent sizes and
Where are the Blobs: Counting by Localization with Point Supervision
Given only point-level annotations, our model uses a novel loss function that
(i) enforces it to predict the semantic segmentation labels for each pixel in the
image (similar to ) and (ii) encourages it to output a segmentation blob for
each object instance. During the training phase, the model learns to split the
blobs that contain more than one point annotation and to remove the blobs that
contain no point-level annotations.
Our experiments show that our method achieves superior object counting
results compared to state-of-the-art counting methods including those that use
stronger supervision such as per-pixel labels. Our benchmark uses datasets representing diﬀerent settings for object counting: Mall , UCSD , and ShanghaiTech B as crowd datasets; MIT Traﬃc , and Park lot as surveillance datasets; Trancos as a traﬃc monitoring dataset; and Penguins as a
population monitoring dataset. We also show counting results for the PASCAL
VOC dataset which consists of objects present in natural, ‘everyday’ images.
We also study the eﬀect of using diﬀerent parts of the proposed loss function
against counting and localization performance.
We summarize our contributions as follows: (1) we propose a novel loss function that encourages the network to output a single blob per object instance
using point-level annotations only; (2) we design two methods for splitting large
predicted blobs between object instances; and (3) we show that our method
achieves new state-of-the-art results on several challenging datasets including
the Pascal VOC and the Penguins dataset.
The rest of the paper is organized as follows: Section 2 presents related works
on object counting; Section 3 describes the proposed approach; and Section 4
describes our experiments and results. Finally, we present the conclusion in Section 5.
Related Work
Object counting has received signiﬁcant attention over the past years . It
can be roughly divided into three categories : (1) counting by clustering, (2)
counting by regression, and (3) counting by detection.
Early work in object counting use clustering-based methods. They are unsupervised approaches where objects are clustered based on features such as
appearance and motion cues . Rabaud and Belongie proposed to use
feature points which are detected by motion and appearance cues and are tracked
through time using KLT . The objects are then clustered based on similar
features. Sebastian et al. used an expectation-maximization method that
cluster individuals in crowds based on head and shoulder features. These methods use basic features and often perform poorly for counting compared to deep
learning approaches. Another drawback is that these methods only work for
video sequences, rather than still images.
Counting by regression methods have deﬁned state-of-the-art results in many
benchmarks. They were shown to be faster and more accurate than other groups
such as counting by detection. These methods include glance and density-based
Laradji, Rostamzadeh, Pinheiro, Vazquez, Schmidt
methods that explicitly learn how to count rather than optimize for a localizationbased objective. Lempitsky et al. proposed the ﬁrst method that used object
density to count people. They transform the point-level annotation matrix into
a density map using a Gaussian kernel. Then, they train their model using a
least-squares objective to predict the density map. One major challenge is determining the optimal size of the Gaussian kernel which highly depends on the
object sizes. As a result, Zhang et al. proposed a deep learning method that
adjusted the kernel size using a perspective map. This assumes ﬁxed camera
images such as those used in surveillance applications. Onoro-Rubio et al. 
extended this method by proposing a perspective-free multi-scale deep learning
approach. However, this method cannot be used for counting everyday objects
as their sizes vary widely across the scenes as it is highly sensitive to the kernel
A straight-forward method for counting by regression is ‘glance’ , which
explicitly learns to count using image-level labels only. Glance methods are ef-
ﬁcient if the object count is small . Consequently, the authors proposed a
grid-based counting method, denoted as “subitizing”, in order to count a large
number of objects in the image. This method uses glance to count objects at
diﬀerent non-overlapping regions of the image, independently. While glance is
easy to train and only requires image-level annotation, the “subitizing” method
requires a more complicated training procedure that needs full per-pixel annotation ground-truth.
Counting by detection methods ﬁrst detect the objects of interest and then
simply count the number of instances. Successful object detection methods rely
on bounding boxes and per-pixel labels ground-truth. Perfect object detection implies perfect count. However, Chattopadhyay et al. 
showed that Fast RCNN , a state-of-the-art object detection method, performs worse than glance and subitizing-based methods. This is because the detection task is challenging in that the model needs to learn the location, size,
and shape of object instances that are possibly heavily occluded. While several
works suggest that counting by detection is infeasible for surveillance
scenes where objects are often occluded, we show that learning a notion of localization can help the model improve counting.
Similar to our method is the line of work proposed by Arteta et al. .
They proposed a method that detects overlapping instances based on optimizing
a tree-structured discrete graphical model. While their method showed promising detection results using point-level annotations only, it performed worse for
counting than regression-based methods such as .
Our method is also similar to segmentation methods such as U-net which
learns to segment objects using a fully-convolutional neural network. Unlike our
method, U-net requires the full per-pixel instance segmentation labels, whereas
we use point-level annotations only.
Where are the Blobs: Counting by Localization with Point Supervision
Localization-based Counting FCN
Our model is based on the fully convolutional neural network (FCN) proposed
by Long et al. . We extend their semantic segmentation loss to perform
object counting and localization with point supervision. We denote the novel loss
function as localization-based counting loss (LC) and, we refer to the proposed
model as LC-FCN. Next, we describe the proposed loss function, the architecture
of our model, and the prediction procedure.
The Proposed Loss Function
LC-FCN uses a novel loss function that consists of four distinct terms. The ﬁrst
two terms, the image-level and the point-level loss, enforces the model to predict
the semantic segmentation labels for each pixel in the image. This is based on
the weakly supervised semantic segmentation algorithm proposed by Bearman
et al. . These two terms alone are not suitable for object counting as the predicted blobs often group many object instances together (see the ablation studies
in Section 4). The last two terms encourage the model to output a unique blob
for each object instance and remove blobs that have no object instances. Note
that LC-FCN only requires point-level annotations that indicate the locations
of the objects rather than their sizes, and shapes.
Let T represent the point annotation ground-truth matrix which has label c
at the location of each object (where c is the object class) and zero elsewhere.
Our model uses a softmax function to output a matrix S where each entry Sic
is the probability that pixel i belongs to category c. The proposed loss function
can be written as:
Image-level loss
Point-level loss
+ LS(S, T)
Split-level loss
False positive loss
which we describe in detail next.
Image-level loss. Let Ce be the set of classes present in the image. For each
class c ∈Ce, LI increases the probability that the model labels at least one pixel
as class c. Also, let C¬e be the set of classes not present in the image. For each
class c ∈C¬e, the loss decreases the probability that the model labels any pixel as
class c. Ce and C¬e can be obtained from the provided ground-truth point-level
annotations. More formally, the image level loss is computed as follows:
LI(S, T) = −1
log(Stcc) −
log(1 −Stcc) ,
where tc = argmaxi∈ISic. For each category present in the image, at least one
pixel should be labeled as that class. For classes that do not exist in the image,
none of the pixels should belong to that class. Note that we assume that each
image has at least one background pixel; therefore, the background class belongs
Laradji, Rostamzadeh, Pinheiro, Vazquez, Schmidt
Point-level loss. This term encourages the model to correctly label the small
set of supervised pixels Is contained in the ground-truth. Is represents the locations of the object instances. This is formally deﬁned as,
LP (S, T) = −
log(SiTi) ,
where Ti represents the true label of pixel i. Note that this loss ignores all the
pixels that are not annotated.
Split-level loss. LS discourages the model from predicting blobs that have two
or more point-annotations. Therefore, if a blob has n point annotations, this loss
enforces it to be split into n blobs, each corresponding to a unique object. These
splits are made by ﬁrst ﬁnding boundaries between object pairs. The model then
learns to predict these boundaries as the background class. The model outputs a
binary matrix F where pixel i is foreground if argmaxkSik > 0, and background,
otherwise.
We apply the connected components algorithm proposed by to ﬁnd the
blobs B in the foreground mask F. We only consider the blobs with two or more
ground truth point annotations ¯B. We propose two methods for splitting blobs
(see Fig. 2),
1. Line split method. For each blob b in ¯B we pair each point with its closest
point resulting in a set of pairs bP . For each pair (pi, pj) ∈bP we use a
scoring function to determine the best segment E that is perpendicular to
the line between pi and pj. The segment lines are within the predicted blob
and they intersect the blob boundaries. The scoring function z(·) for segment
E is computed as,
which is the mean of the background probabilities belonging to segment E
(where 0 is the background class). The best edge Ebest is deﬁned as the set of
pixels representing the edge with the highest probability of being background
among all the perpendicular lines. This determines the ‘most likely’ edge of
separation between the two objects. Then we set Tb as the set of pixels
representing the best edges generated by the line split method.
2. Watershed split method. This consists of global and local segmentation procedures. For the global segmentation, we apply the watershed segmentation
algorithm globally on the input image where we set the ground-truth
point-annotations as the seeds. The segmentation is applied on the distance
transform of the foreground probabilities, which results in k segments where
k is the number of point-annotations in the image.
For the local segmentation procedure, we apply the watershed segmentation
only within each blob b in ¯B where we use the point-annotation ground-truth
inside them as seeds. This adds more importance to splitting big blobs when
computing the loss function. Finally, we deﬁne Tb as the set of pixels representing the boundaries determined by the local and global segmentation.
Where are the Blobs: Counting by Localization with Point Supervision
Predicted blobs
Line splits
Watershed splits
Fig. 2. Split methods. Comparison between the line split, and the watershed split.
The loss function identiﬁes the boundary splits (shown as yellow lines). Yellow blobs
represent those with more than one object instance, and red blobs represent those
that have no object instance. Green blobs are true positives. The squares represent the
ground-truth point annotations.
Fig. 2 shows the split boundaries using the line split and the watershed split
methods (as yellow lines). Given Tb, we compute the split loss as follows,
LS(S, T) = −
αi log(Si0),
where Si0 is the probability that pixel i belongs to the background class and αi is
the number of point-annotations in the blob in which pixel i lies. This encourages
the model to focus on splitting blobs that have the most point-level annotations.
The intuition behind this method is that learning to predict the boundaries
between the object instances allows the model to distinguish between them. As
a result, the penalty term encourages the model to output a single blob per
object instance.
We emphasize that it is not necessary to get the right edges in order to
accurately count. It is only necessary to make sure we have a positive region on
each object and a negative region between objects. Other heuristics are possible
to construct a negative region which could still be used in our framework. For
example, fast label propagation methods proposed in can be used to
determine the boundaries between the objects in the image. Note that these 4
loss functions are only used during training. The framework does not split or
remove false positive blobs at test time. The predictions are based purely on the
blobs obtained from the probability matrix S.
False Positive loss. LF discourages the model from predicting a blob with no
point annotations, in order to reduce the number of false positive predictions.
The loss function is deﬁned as
LF (S, T) = −
Laradji, Rostamzadeh, Pinheiro, Vazquez, Schmidt
Fig. 3. Given an input image, our model ﬁrst extracts features using a backbone architecture such as ResNet. The extracted features are then upsampled through the
upsampling path to obtain blobs for the objects. In this example, the model predicts
the blobs for persons and bikes for an image in the PASCAL VOC 2007 dataset.
where Bfp is the set of pixels constituting the blobs predicted for each class
(except the background class) that contain no ground-truth point annotations
(note that Si0 is the probability that pixel i belongs to the background class). All
the predictions within Bfp are considered false positives (see the red blobs in Fig.
5). Therefore, optimizing this loss term results in less false positive predictions
as shown in the qualitative results in Fig. 5. The experiments show that this loss
term is extremely important for accurate object counting.
LC-FCN Architecture and Inference
LC-FCN can be any FCN architecture such as FCN8 architecture , Deeplab ,
Tiramisu , and PSPnet . LC-FCN consists of a backbone that extracts
the image features. The backbone is an Imagenet pretrained network such as
VGG16 or ResNet-50 . The image features are then upscaled using an
upsampling path to output a score for each pixel i indicating the probability
that it belongs to class c (see Fig. 3).
We predict the number of objects for class c through the following three
steps: (i) the upsampling path outputs a matrix Z where each entry Zic is the
probability that pixel i belongs to class c; then (ii) we generate a binary mask
F, where pixel Fic = 1 if arg maxk Zik = c, and 0 otherwise; lastly (iii) we apply
the connected components algorithm on F to get the blobs for each class c.
The count is the number of predicted blobs (see Fig. 3).
Experiments
In this section we describe the evaluation metrics, the training procedure, and
present the experimental results and discussion.
Evaluation Metric. For datasets with single-class objects, we report the mean
absolute error (MAE) which measures the deviation of the predicted count pi
Where are the Blobs: Counting by Localization with Point Supervision
Density-only 
With seg. and depth 
With seg and no depth 
Table 1. Penguins datasets. Evaluation of our method against previous state-of-theart methods. The evaluation is made across the four setups explained in the dataset
description.
from the true count ci, computed as
i |pi −ci|. MAE is a commonly used
metric for evaluating object counting methods . For datasets with multiclass objects, we report the mean root mean square error (mRMSE) as used
in for the PASCAL VOC 2007 dataset. We measure the localization performance using the average mean absolute error (GAME) as in . Since our
model predicts blobs instead of a density map, GAME might not be an accurate
localization measure. Therefore, in section 4.3 we use the F-Score metric to assess the localization performance of the predicted blobs against the point-level
annotation ground-truth.
Training Procedure. We use the Adam optimizer with a learning rate of
10−5 and weight decay of 5 × 10−5. We use the provided validation set for early
stopping only. During training, the model uses a batch size of 1 which can be an
image of any size. We double our training set by applying the horizontal ﬂip augmentation method on each image. Finally, we report the prediction results on the
test set. We compare between three architectures: FCN8 ; ResFCN which is
FCN8 that uses ResNet-50 as the backbone instead of VGG16; and PSPNet 
with ResNet-101 as the backbone. We use the watershed split procedure in all
our experiments.
Results and Discussion
Penguins Dataset . The Penguins dataset comprises images of penguin
colonies located in Antarctica. We use the two dataset splits as in . In the
‘separated’ dataset split, the images in the training set come from diﬀerent cameras than those in the test set. In the ‘mixed’ dataset split, the images in the
training set come from the same cameras as those in the test set. In Table 1, the
MAE is computed with respect to the Max and Median count (as there are multiple annotators). Our methods signiﬁcantly outperform theirs in all of the four
settings, although their methods use depth features and the multiple annotations
Laradji, Rostamzadeh, Pinheiro, Vazquez, Schmidt
Lemptisky+SIFT 
Hydra CCNN 
FCN-MT 
FCN-HA 
CSRNet 
Table 2. Trancos dataset. Evaluation of our method against previous state-of-theart methods, comparing the mean absolute error (MAE) and the grid average mean
absolute error (GAME) as described in .
provided for each penguin. This suggests that LC-FCN can learn to distinguish
between individual penguins despite the heavy occlusions and crowding.
Trancos Dataset . The Trancos dataset comprises images taken from traf-
ﬁc surveillance cameras located along diﬀerent roads. The task is to count the
vehicles present in the regions of interest of the traﬃc scenes. Each vehicle is
labeled with a single point annotation that represents its location in the image.
We observe in Table 2 that our method achieves new state-of-the-art results for
counting and localization. Note that GAME(L) subdivides the image using a
grid of 4L non-overlapping regions, and the error is computed as the sum of
the mean absolute errors in each of these subregions. For our method, the predicted count of a region is the number of predicted blob centers in that region.
This provides a rough assessment of the localization performance. Compared
to the methods in Table 2, LC-FCN does not require a perspective map nor
a multi-scale approach to learn objects of diﬀerent sizes. These results suggest
that LC-FCN can accurately localize and count extremely overlapping vehicles.
Parking Lot . The dataset comprises surveillance images taken at a parking
lot in Curitiba, Brazil. We used the PUCPR subset of the dataset where the ﬁrst
50% of the images was set as the training set and the last 50% as the test set. The
last 20% of the training set was set as the validation set for early stopping. The
ground truth consists of a bounding box for each parked car since this dataset
is primarily used for the detection task. Therefore, we convert them into pointlevel annotations by taking the center of each bounding box. Table 5 shows that
LC-FCN signiﬁcantly outperforms Glance in MAE. LC-FCN8 achieves only 0.21
average miscount per image although many images contain more than 20 parked
cars. This suggests that explicitly learning to localize parked cars can perform
Where are the Blobs: Counting by Localization with Point Supervision
mRMSE mRMSE-nz m-relRMSE m-relRMSE-nz
Glance-noft-2L 
Aso-sub-ft-3 × 3 
Faster-RCNN 
Table 3. PASCAL VOC. We compare against the methods proposed in
model evaluates on the full test set, whereas the other methods take the mean of ten
random samples of the test set evaluation.
UCSD Mall ShanghaiTech B
FCN-rLSTM 
MoCNN 
CNN-boosting 
M-CNN 
CP-CNN 
CSRNet 
Table 4. Crowd datasets MAE results.
Fig. 4. Predicted blobs on a ShanghaiTech
B test image.
better in counting than methods that explicitly learn to count from image-level
labels (see Fig. 5 for qualitative results). Note that this is the ﬁrst counting
method being applied on this dataset.
MIT Traﬃc . This dataset consists of surveillance videos taken from a single
ﬁxed camera. It has 20 videos, which are split into a training set (Videos 1-8), a
validation set (Videos 0-10), and a test set (Videos 11-20). Each video frame is
provided with a bounding box indicating each pedestrian. We convert them into
point-level annotations by taking the center of each bounding box. Table 5 shows
that our method signiﬁcantly outperforms Glance, suggesting that learning a
localization-based objective allows the model to ignore the background regions
that do not contribute to the object count. As a result, LC-FCN is less likely to
overﬁt on irrelevant features from the background. To the best of our knowledge,
this is the ﬁrst counting method being applied on this dataset.
Pascal VOC 2007 . We use the standard training, validation, and test split
as speciﬁed in . We use the point-level annotation ground-truth provided by
Bearman et al. to train our LC-FCN methods. We evaluated against the
count of the non-diﬃcult instances of the Pascal VOC 2007 test set.
Table 3 compares the performance of LC-FCN with diﬀerent methods proposed by . We point the reader to for a description of the evaluation metrics
used in the table. We show that LC-FCN achieves new state-of-the-art results
Laradji, Rostamzadeh, Pinheiro, Vazquez, Schmidt
LI + LP + LS
LI + LP + LF
Table 5. Quantitative results. Comparison of diﬀerent parts of the proposed loss
function for counting and localization performance.
with respect to mRMSE. We see that LC-FCN outperforms methods that explicitly learn to count although learning to localize objects of this dataset is a
very challenging task. Further, LC-FCN uses weaker supervision than Aso-sub
and Seq-sub as they require the full per-pixel labels to estimate the object count
for diﬀerent image regions.
Crowd Counting Datasets. Table 4 reports the MAE score of our method
on 3 crowd datasets using the setup described in the survey paper . For this
experiment, we show our results using ResFCN as the backbone with the Watershed split method. We see that our method achieves competitive performance
for crowd counting. Fig. 4 shows the predicted blobs of our model on a test
image of the ShanghaiTech B dataset. We see that our model predicts a blob on
the face of each individual. This is expected since the ground-truth point-level
annotations are marked on each person’s face.
Ablation Studies
Localization Benchmark. Since robust localization is useful in many computer vision applications, we use the F-Score measure to directly assess the
localization performance of our model. F-Score is a standard measure for detection as it considers both precision and recall, F-Score =
2TP+FP+FN, where
the number of true positives (TP) is the number of blobs that contain at least
one point annotation; the number of false positives (FP) is the number of blobs
that contain no point annotation; and the number of false negatives (FN) is the
number of point annotations minus the number of true positives. Table 5 shows
the localization results of our method on several datasets.
Loss Function Analysis. We assess the eﬀect of each term of the loss function
on counting and localization results. We start by looking at the results of a model
Where are the Blobs: Counting by Localization with Point Supervision
Fig. 5. Qualitative results of LC-FCN trained with diﬀerent terms of the proposed
loss function. (a) Test images obtained from MIT Traﬃc, Parking Lot, Trancos, and
Penguins. (b) Prediction results using only image-level and point-level loss terms. (c)
Prediction results using image-level, point-level, and split-level loss terms. (d) Prediction results trained with the full proposed loss function. The green blobs and red blobs
indicate true positive and false positive predictions, respectively. Yellow blobs represent
those that contain more than one object instance.
trained with the image-level loss LI and the point-level loss LP only. These two
terms were used for semantic segmentation using point annotations . We observe in Fig. 5(b) that a model using these two terms results in a single blob that
groups many object instances together. Consequently, this performs poorly in
terms of the mean absolute error and the F-Score (see Table 5). As a result, we
introduced the split-level loss function LS that encourages the model to predict
blobs that do not contain more than one point-annotation. We see in Fig. 5(c)
that a model using this additional loss term predicts several blobs as object instances rather than one large single blob. However, since LI + LP + LS does
not penalize the model from predicting blobs with no point annotations, it can
often lead to many false positives. Therefore, we introduce the false positive loss
LF that discourages the model from predicting blobs with no point annotations.
By adding this loss term to the optimization, LC-FCN achieves signiﬁcant improvement as seen in the qualitative and quantitative results (see Fig. 5(d) and
Table 5). Further, including only the split-level loss leads to predicting a huge
number of small blobs, leading to many false positives which makes performance
Laradji, Rostamzadeh, Pinheiro, Vazquez, Schmidt
Split method
Trancos Penguins
LC-ResFCN (L)
LC-ResFCN (W)
Fig. 6. Split Heuristics Analysis. Comparison between the watershed split method
and the line split method against the validation MAE score.
MAE on Trancos Val Set
Line Split
Watershed Split
MAE on Penguins Val Set
Line Split
Watershed Split
worse. Combining it with the false-positive loss avoids this issue which leads
to a net improvement in performance. On the other hand, using only the false
positive loss it tends to predict one huge blob.
Split Heuristics Analysis. In Fig. 6 we show that the watershed split achieves
better MAE on Trancos and Penguins validation sets. Further, using the watershed split achieves much faster improvement on the validation set with respect
to the number of epochs. This suggests that using proper heuristics to identify
the negative regions is important, which leaves an open area for future work.
Conclusion
We propose LC-FCN, a fully-convolutional neural network, to address the problem of object counting using point-level annotations only. We propose a novel
loss function that encourages the model to output a single blob for each object
instance. Experimental results show that LC-FCN outperforms current stateof-the-art models on the PASCAL VOC 2007, Trancos, and Penguins datasets
which contain objects that are heavily occluded. For future work, we plan to
explore diﬀerent FCN architectures and splitting methods that LC-FCN can use
to eﬃciently split between overlapping objects that have complicated shapes and
appearances.
Acknowledgements
We would like to thank the anonymous referees for their useful comments that
signiﬁcantly improved the paper. Issam Laradji is funded by the UBC Four-Year
Doctoral Fellowships (4YF).
Where are the Blobs: Counting by Localization with Point Supervision