HAL Id: inria-00541855
 
Submitted on 6 Feb 2011
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Wavelets on graphs via spectral graph theory
David K. Hammond, Pierre Vandergheynst, Rémi Gribonval
To cite this version:
David K. Hammond, Pierre Vandergheynst, Rémi Gribonval.
Wavelets on graphs via spectral graph theory.
Applied and Computational Harmonic Analysis, 2011, 30 (2), pp.129–150.
￿10.1016/j.acha.2010.04.005￿. ￿inria-00541855￿
Wavelets on graphs via spectral graph theory
David K. Hammond∗,a,1, Pierre Vandergheynstb,2, R´emi Gribonvalc
aNeuroInformatics Center, University of Oregon, Eugene, USA
bEcole Polytechnique F´ed´erale de Lausanne, Lausanne, Switzerland
cINRIA, Rennes, France
We propose a novel method for constructing wavelet transforms of functions deﬁned on
the vertices of an arbitrary ﬁnite weighted graph. Our approach is based on deﬁning scaling using the graph analogue of the Fourier domain, namely the spectral decomposition
of the discrete graph Laplacian L. Given a wavelet generating kernel g and a scale parameter t, we deﬁne the scaled wavelet operator T t
g = g(tL). The spectral graph wavelets are
then formed by localizing this operator by applying it to an indicator function. Subject
to an admissibility condition on g, this procedure deﬁnes an invertible transform. We
explore the localization properties of the wavelets in the limit of ﬁne scales. Additionally, we present a fast Chebyshev polynomial approximation algorithm for computing the
transform that avoids the need for diagonalizing L. We highlight potential applications
of the transform through examples of wavelets on graphs corresponding to a variety of
diﬀerent problem domains.
1. Introduction
Many interesting scientiﬁc problems involve analyzing and manipulating structured
data. Such data often consist of sampled real-valued functions deﬁned on domain sets
themselves having some structure.
The simplest such examples can be described by
scalar functions on regular Euclidean spaces, such as time series data, images or videos.
However, many interesting applications involve data deﬁned on more topologically complicated domains. Examples include data deﬁned on network-like structures, data deﬁned
on manifolds or irregularly shaped domains, and data consisting of “point clouds”, such
as collections of feature vectors with associated labels. As many traditional methods
for signal processing are designed for data deﬁned on regular Euclidean spaces, the development of methods that are able to accommodate complicated data domains is an
important problem.
∗Principal Corresponding Author
Email addresses: (David K. Hammond ), 
(Pierre Vandergheynst ), (R´emi Gribonval)
1This work was performed while DKH was at EPFL
2This work was supported in part by the EU Framework 7 FET-Open project FP7-ICT-225913-
SMALL : Sparse Models, Algorithms and Learning for Large-Scale Data
 
April 19, 2010
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
Many signal processing techniques are based on transform methods, where the input
data is represented in a new basis before analysis or processing. One of the most successful
types of transforms in use is wavelet analysis. Wavelets have proved over the past 25
years to be an exceptionally useful tool for signal processing.
Much of the power of
wavelet methods comes from their ability to simultaneously localize signal content in
both space and frequency. For signals whose primary information content lies in localized
singularities, such as step discontinuities in time series signals or edges in images, wavelets
can provide a much more compact representation than either the original domain or a
transform with global basis elements such as the Fourier transform. An enormous body
of literature exists for describing and exploiting this wavelet sparsity. We include a few
representative references for applications to signal compression , denoising
 , and inverse problems including deconvolution . As
the individual waveforms comprising the wavelet transform are self similar, wavelets
are also useful for constructing scale invariant descriptions of signals.
This property
can be exploited for pattern recognition problems where the signals to be recognized or
classiﬁed may occur at diﬀerent levels of zoom . In a similar vein, wavelets can be
used to characterize fractal self-similar processes .
The demonstrated eﬀectiveness of wavelet transforms for signal processing problems
on regular domains motivates the study of extensions to irregular, non-euclidean spaces.
In this paper, we describe a ﬂexible construction for deﬁning wavelet transforms for data
deﬁned on the vertices of a weighted graph. Our approach uses only the connectivity
information encoded in the edge weights, and does not rely on any other attributes of
the vertices (such as their positions as embedded in 3d space, for example). As such, the
transform can be deﬁned and calculated for any domain where the underlying relations
between data locations can be represented by a weighted graph. This is important as
weighted graphs provide an extremely ﬂexible model for approximating the data domains
of a large class of problems.
Some data sets can naturally be modeled as scalar functions deﬁned on the vertices of
graphs. For example, computer networks, transportation (road, rail, airplane) networks
or social networks can all be described by weighted graphs, with the vertices corresponding to individual computers, cities or people respectively. The graph wavelet transform
could be useful for analyzing data deﬁned on these vertices, where the data is expected
to be inﬂuenced by the underlying topology of the network. As a mock example problem, consider rates of infection of a particular disease among diﬀerent population centers.
As the disease may be expected to spread by people traveling between diﬀerent areas,
the graph wavelet transform based on a weighted graph representing the transportation
network may be helpful for this type of data analysis.
Weighted graphs also provide a ﬂexible generalization of regular grid domains. By
identifying the grid points with vertices and connecting adjacent grid points with edges
with weights inversely proportional to the square of the distance between neighbors, a
regular lattice can be represented with weighted graph. A general weighted graph, however, has no restriction on the regularity of vertices. For example points on the original
lattice may be removed, yielding a “damaged grid”, or placed at arbitrary locations corresponding to irregular sampling. In both of these cases, a weighted graph can still be
constructed that represents the local connectivity of the underlying data points. Wavelet
transforms that rely upon regular spaced samples will fail in these cases, however transforms based on weighted graphs may still be deﬁned.
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
Similarly, weighted graphs can be inferred from mesh descriptions for geometrical
domains. An enormous literature exists on techniques for generating and manipulating
meshes; such structures are widely used in applications for computer graphics and numerical solution of partial diﬀerential equations. The transform methods we will describe
thus allow the deﬁnition of a wavelet transform for data deﬁned on any geometrical shape
that can be described by meshes.
Weighted graphs can also be used to describe the similarity relationships between
“point clouds” of vectors. Many approaches for machine learning or pattern recognition
problems involve associating each data instance with a collection of feature vectors that
hopefully encapsulate suﬃcient information about the data point to solve the problem at
hand. For example, for machine vision problems dealing with object recognition, a common preprocessing step involves extracting keypoints and calculating the Scale Invariant
Feature Transform (SIFT) features . In many automated systems for classifying or
retrieving text, word frequencies counts are used as feature vectors for each document
 . After such feature extraction, each data point may be associated to a feature vector vm ∈RN, where N may be very large depending on the application.
problems, the local distance relationships between data points are crucial for successful
learning or classiﬁcation. These relationships can be encoded in a weighted graph by
considering the data points as vertices and setting the edge weights equal to a distance
metric Am,n = d(vm, vn) for some function d : RN × RN →R.
The spectral graph
wavelets in this setting could ﬁnd a number of uses for analysis of data deﬁned on such
point clouds. They may be useful for regularization of noisy or corrupted data on a point
cloud, or could serve as a building blocks for building a hypothesis function for learning
Classical wavelets are constructed by translating and scaling a single “mother” wavelet.
The transform coeﬃcients are then given by the inner products of the input function with
these translated and scaled waveforms. Directly extending this construction to arbitrary
weighted graphs is problematic, as it is unclear how to deﬁne scaling and translation on
an irregular graph.
We approach this problem by working in the spectral graph domain, i.e. using the
basis consisting of the eigenfunctions of the graph Laplacian L. This tool from spectral
graph theory , provides an analogue of the Fourier transform for functions on weighted
graphs. In our construction, the wavelet operator at unit scale is given as an operator
valued function Tg = g(L) for a generating kernel g.
Scaling is then deﬁned in the
spectral domain, i.e. the operator T t
g at scale t is given by g(tL). Applying this operator
to an input signal f gives the wavelet coeﬃcients of f at scale t.
These coeﬃcients
are equivalent to inner products of the signal f with the individual graph wavelets.
These wavelets can be calculated by applying this operator to a delta impulse at a single
vertex, i.e. ψt,m = T t
gδm. We show that this construction is analogous to the 1-d wavelet
transform for a symmetric wavelet, where the transform is viewed as a Fourier multiplier
operator at each wavelet scale.
In this paper we introduce this spectral graph wavelet transform and study several
of its properties.
We show that in the ﬁne scale limit, for suﬃciently regular g, the
wavelets exhibit good localization properties. With continuously deﬁned spatial scales,
the transform is analogous to the continuous wavelet transform, and we show that it is
formally invertible subject to an admissibility condition on the kernel g. Sampling the
spatial scales at a ﬁnite number of values yields a redundant, invertible transform with
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
overcompleteness equal to the number of spatial scales chosen. We show that in this
case the transform deﬁnes a frame, and give a condition for computing the frame bounds
depending on the selection of spatial scales.
While we deﬁne our transform in the spectral graph domain, directly computing it via
fully diagonalizing the Laplacian operator is infeasible for problems with size exceeding a
few thousand vertices. We introduce a method for approximately computing the forward
transform through operations performed directly in the vertex domain that avoids the
need to diagonalize the Laplacian. By approximating the kernel g with a low dimensional
Chebyshev polynomial, we may compute an approximate forward transform in a manner
which accesses the Laplacian only through matrix-vector multiplication. This approach
is computationally eﬃcient if the Laplacian is sparse, as is the case for many practically
relevant graphs.
We show that computation of the pseudoinverse of the overcomplete spectral graph
wavelet transform is compatible with the Chebyshev polynomial approximation scheme.
Speciﬁcally, the pseudoinverse may be calculated by an iterative conjugate gradient
method that requires only application of the forward transform and its adjoint, both
of which may be computed using the Chebyshev polynomial approximation methods.
Our paper is structured as follows. Related work is discussed in Section 1.1. We review
the classical wavelet transform in Section 2, and highlight the interpretation of the wavelet
acting as a Fourier multiplier operator. We then set our notation for weighted graphs and
introduce spectral graph theory in Section 4. The spectral graph wavelet transform is
deﬁned in Section 4. In Section 5 we discuss and prove some properties of the transform.
Section 6 is dedicated to the polynomial approximation and fast computation of the
transform. The inverse transform is discussed in section 7. Finally, several examples of
the transform on domains relevant for diﬀerent problems are shown in Section 8.
1.1. Related Work
Since the original introduction of wavelet theory for square integrable functions de-
ﬁned on the real line, numerous authors have introduced extensions and related transforms for signals on the plane and higher dimensional spaces. By taking separable products of one dimensional wavelets, one can construct orthogonal families of wavelets in any
dimension . However, this yields wavelets with often undesirable bias for coordinate
axis directions. Additionally, this approach yields a number of wavelets that is exponential in the dimension of the space, and is unsuitable for data embedded in spaces of large
dimensionality. A large family of alternative multiscale transforms has been developed
and used extensively for image processing, including Laplacian pyramids , steerable
wavelets , complex dual-tree wavelets , curvelets , and bandlets . Wavelet
transforms have also been deﬁned for certain non-Euclidean manifolds, most notably the
sphere and other conic sections .
Previous authors have explored wavelet transforms on graphs, albeit via diﬀerent
approaches to those employed in this paper. Crovella and Kolaczyk deﬁned wavelets
on unweighted graphs for analyzing computer network traﬃc. Their construction was
based on the n-hop distance, such that the value of a wavelet centered at a vertex n on
vertex m depended only on the shortest-path distance between m and n. The wavelet
values were such that the sum over each n-hop annulus equaled the integral over an
interval of a given zero-mean function, thus ensuring that the graph wavelets had zero
mean. Their results diﬀer from ours in that their construction made no use of graph
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
weights and no study of the invertibility or frame properties of the transform was done.
Smalter et.
 used the graph wavelets of Crovella and Kolaczyk as part of a
larger method for measuring structural diﬀerences between graphs representing chemical
structures, for machine learning of chemical activities for virtual drug screening.
Jansen et. al. develop a multiscale scheme for data on graphs based on lifting.
Their scheme requires distances to be assigned to each edge, which for their examples
are inferred from Euclidean distances when the graph vertices correspond to irregularly
sampled points of Euclidean space.
The lifting procedure is generated by using the
weighted average of the graph neighbors of each vertex for the lifting prediction step.
The main diﬀerence with our method is that this lifting is performed directly in the
vertex domain, as opposed to our spectral domain approach for deﬁning scaling.
Other authors have considered analogues of the wavelet transform for data deﬁned
on tree structures. Murtagh developed a Haar wavelet transform for rooted binary
trees, known as dendrograms. This concept was expanded upon by Lee et. al. who
developed the treelet transform, incorporating automatic construction of hierarchcal trees
for multivariate data.
Maggioni and Coifman introduced “diﬀusion wavelets”, a general theory for
wavelet decompositions based on compressed representations of powers of a diﬀusion
operator. The diﬀusion wavelets were described with a framework that can apply on
smooth manifolds as well as graphs. Their construction interacts with the underlying
graph or manifold space through repeated applications of a diﬀusion operator T, analogously to how our construction is parametrized by the choice of the graph Laplacian
L. The largest diﬀerence between their work and ours is that the diﬀusion wavelets are
designed to be orthonormal. This is achieved by running a localized orthogonalization
procedure after applying dyadic powers of T at each scale to yield nested approximation
spaces, wavelets are then produced by locally orthogonalizing vectors spanning the difference of these approximation spaces. While an orthogonal transform is desirable for
many applications, notably operator and signal compression, the use of the orthogonalization procedure complicates the construction of the transform, and somewhat obscures
the relation between the diﬀusion operator T and the resulting wavelets. In contrast our
approach is conceptually simpler, gives a highly redundant transform, and aﬀords ﬁner
control over the selection of wavelet scales.
Maggioni and Mhaskar have developed a theory of “diﬀusion polynomial frames”
 that is closely related to the present work, in a more general quasi-metric measure
space setting. While some of their localization results are similar to ours, they do not
provide any algorithms for eﬃcient computation of the frames. Geller and Mayeli 
studied a construction for wavelets on compact diﬀerentiable manifolds that is formally
similar to our approach on weighted graphs. In particular, they deﬁne scaling using a
pseudodiﬀerential operator tLe−tL, where L is the manifold Laplace-Beltrami operator
and t is a scale parameter, and obtain wavelets by applying this to a delta impulse. They
also study the localization of the resulting wavelets, however the methods and theoretical
results in their paper are diﬀerent as they are in the setting of smooth manifolds.
2. Classical Wavelet Transform
We ﬁrst give an overview of the classical Continuous Wavelet Transform (CWT) for
L2(R), the set of square integrable real valued functions. We will describe the forward
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
transform and its formal inverse, and then show how scaling may be expressed in the
Fourier domain. These expressions will provide an analogue that we will later use to
deﬁne the Spectral Graph Wavelet Transform.
In general, the CWT will be generated by the choice of a single “mother” wavelet ψ.
Wavelets at diﬀerent locations and spatial scales are formed by translating and scaling
the mother wavelet. We write this by
ψs,a(x) = 1
This scaling convention preserves the L1 norm of the wavelets. Other scaling conventions are common, especially those preserving the L2 norm, however in our case the L1
convention will be more convenient. We restrict ourselves to positive scales s > 0.
For a given signal f, the wavelet coeﬃcient at scale s and location a is given by the
inner product of f with the wavelet ψs,a, i.e.
Wf(s, a) =
The CWT may be inverted provided that the wavelet ψ satisﬁes the admissibility
dω = Cψ < ∞
This condition implies, for continuously diﬀerentiable ψ, that ˆψ(0) =
ψ(x)dx = 0, so
ψ must be zero mean.
Inversion of the CWT is given by the following relation 
Wf(s, a)ψs,a(x)dads
This method of constructing the wavelet transform proceeds by producing the wavelets
directly in the signal domain, through scaling and translation. However, applying this
construction directly to graphs is problematic. For a given function ψ(x) deﬁned on the
vertices of a weighted graph, it is not obvious how to deﬁne ψ(sx), as if x is a vertex of
the graph there is no interpretation of sx for a real scalar s. Our approach to this obstacle is to appeal to the Fourier domain. We will ﬁrst show that for the classical wavelet
transform, scaling can be deﬁned in the Fourier domain. The resulting expression will
give us a basis to deﬁne an analogous transform on graphs.
For the moment, we consider the case where the scale parameter is discretized while
the translation parameter is left continuous. While this type of transform is not widely
used, it will provide us with the closest analogy to the spectral graph wavelet transform.
For a ﬁxed scale s, the wavelet transform may be interpreted as an operator taking the
function f and returning the function T sf(a) = Wf(s, a). In other words, we consider
the translation parameter as the independent variable of the function returned by the
operator T s. Setting
¯ψs(x) = 1
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
we see that this operator is given by convolution, i.e.
(T sf)(a) =
¯ψs(a −x)f(x)dx
= ( ¯ψs ￿f)(a)
Taking the Fourier transform and applying the convolution theorem yields
T sf(ω) = ˆ¯ψs(ω) ˆf(ω)
Using the scaling properties of the Fourier transform and the deﬁnition (5) gives
ˆ¯ψs(ω) = ˆ
Combining these and inverting the transform we may write
(T sf)(x) = 1
eiωx ˆψ∗(sω) ˆf(ω)dω
In the above expression, the scaling s appears only in the argument of ˆψ∗(sω), showing
that the scaling operation can be completely transferred to the Fourier domain. The
above expression makes it clear that the wavelet transform at each scale s can be viewed
as a Fourier multiplier operator, determined by ﬁlters that are derived from scaling
a single ﬁlter ˆψ∗(ω).
This can be understood as a bandpass ﬁlter, as ˆψ(0) = 0 for
admissible wavelets. Expression (9) is the analogue that we will use to later deﬁne the
Spectral Graph Wavelet Transform.
Translation of the wavelets may be deﬁned through “localizing” the wavelet operator
by applying it to an impulse. Writing δa(x) = δ(x −a), one has
(T sδa)(x) = 1
For real valued and even ψ this reduces to (T sδa)(x) = ψa,s(x).
3. Weighted Graphs and Spectral Graph Theory
The previous section showed that the classical wavelet transform could be deﬁned
without the need to express scaling in the original signal domain. This relied on expressing the wavelet operator in the Fourier domain. Our approach to deﬁning wavelets on
graphs relies on generalizing this to graphs; doing so requires the analogue of the Fourier
transform for signals deﬁned on the vertices of a weighted graph. This tool is provided
by Spectral Graph Theory. In this section we ﬁx our notation for weighted graphs, and
motivate and deﬁne the Graph Fourier transform.
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
3.1. Notation for Weighted Graphs
A weighted graph G = {E, V, w} consists of a set of vertices V , a set of edges E, and
a weight function w : E →R+ which assigns a positive weight to each edge. We consider
here only ﬁnite graphs where |V | = N < ∞. The adjacency matrix A for a weighted
graph G is the N × N matrix with entries am,n where
w(e) if e ∈E connects vertices m and n
0 otherwise
In the present work we consider only undirected graphs, which correspond to symmetric
adjacency matrices. We do not consider the possibility of negative weights.
A graph is said to have loops if it contain edges that connect a single vertex to itself.
Loops imply the presence of nonzero diagonal entries in the adjacency matrix. As the
existence of loops presents no signiﬁcant problems for the theory we describe in this
paper, we do not speciﬁcally disallow them.
For a weighted graph, the degree of each vertex m, written as d(m), is deﬁned as the
sum of the weights of all the edges incident to it. This implies d(m) = ￿
n am,n. We
deﬁne the matrix D to have diagonal elements equal to the degrees, and zeros elsewhere.
Every real valued function f : V →R on the vertices of the graph G can be viewed
as a vector in RN, where the value of f on each vertex deﬁnes each coordinate. This
implies an implicit numbering of the vertices. We adopt this identiﬁcation, and will write
f ∈RN for functions on the vertices of the graph, and f(m) for the value on the mth
Of key importance for our theory is the graph Laplacian operator L.
The nonnormalized Laplacian is deﬁned as L = D −A. It can be veriﬁed that for any f ∈RN,
L satisﬁes
am,n · (f(m) −f(n))
where the sum over m ∼n indicates summation over all vertices n that are connected to
the vertex m, and am,n denotes the weight of the edge connecting m and n.
For a graph arising from a regular mesh, the graph Laplacian corresponds to the
standard stencil approximation of the continuous Laplacian (with a diﬀerence in sign).
Consider the graph deﬁned by taking vertices vm,n as points on a regular two dimensional
grid, with each point connected to its four neighbors with weight 1/(δx)2, where δx is
the distance between adjacent grid points. Abusing the index notation, for a function
f = fm,n deﬁned on the vertices, applying the graph Laplacian to f yields
(Lf)m,n = (4fm,n −fm+1,n −fm−1,n −fm,n+1 −fm,n−1)/(δx)2
which is the standard 5-point stencil for approximating −∇2f.
Some authors deﬁne and use an alternative, normalized form of the Laplacian, deﬁned
Lnorm = D−1/2LD−1/2 = I −D−1/2AD−1/2
It should be noted that L and Lnorm are not similar matrices, in particular their eigenvectors are diﬀerent. As we shall see in detail later, both operators may be used to deﬁne
spectral graph wavelet transforms, however the resulting transforms will not be equivalent. Unless noted otherwise we will use the non-normalized form of the Laplacian,
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
however much of the theory presented in this paper is identical for either choice. We
consider that the selection of the appropriate Laplacian for a particular problem should
depend on the application at hand.
For completeness, we note the following. The graph Laplacian can be deﬁned for
graphs arising from sampling points on a diﬀerentiable manifold.
The regular mesh
example described previously is a simple example of such a sampling process.
increasing sampling density, by choosing the weights appropriately the normalized graph
Laplacian operator will converge to a diﬀerential operator on the manifold. In the case
of sampling from a uniform distribution, the limit will be the intrinsic Laplace-Beltrami
operator; otherwise the limit will depend on the underlying distribution. Several authors
have studied this limiting process in detail, notably .
3.2. Graph Fourier Transform
On the real line, the complex exponentials eiωx deﬁning the Fourier transform are
eigenfunctions of the one-dimensional Laplacian operator
dx2 . The inverse Fourier transform
ˆf(ω)eiωxdω
can thus be seen as the expansion of f in terms of the eigenfunctions of the Laplacian
The graph Fourier transform is deﬁned in precise analogy to the previous statement.
As the graph Laplacian L is a real symmetric matrix, it has a complete set of orthonormal
eigenvectors. We denote these by χ￿for ￿= 0, . . . , N −1, with associated eigenvalues λ￿
As L is symmetric, each of the λ￿are real. For the graph Laplacian, it can be shown that
the eigenvalues are all non-negative, and that 0 appears as an eigenvalue with multiplicity
equal to the number of connected components of the graph . Henceforth, we assume
the graph G to be connected, we may thus order the eigenvalues such that
0 = λ0 < λ1 ≤λ2... ≤λN−1
For any function f ∈RN deﬁned on the vertices of G, its graph Fourier transform ˆf
is deﬁned by
ˆf(￿) = ￿χ￿, f￿=
where we adopt the convention that the inner product be conjugate-linear in the ﬁrst
argument. The inverse transform reads as
ˆf(￿)χ￿(n)
The Parseval relation holds for the graph Fourier transform, in particular for any
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
4. Spectral Graph Wavelet Transform
Having deﬁned the analogue of the Fourier transform for functions deﬁned on the
vertices of weighted graphs, we are now ready to deﬁne the spectral graph wavelet transform (SGWT). The transform will be determined by the choice of a kernel function
g : R+ →R+, which is analogous to Fourier domain wavelet ˆψ∗in equation 9. This kernel g should behave as a band-pass ﬁlter, i.e. it satisﬁes g(0) = 0 and limx→∞g(x) = 0.
We will defer the exact speciﬁcation of the kernel g that we use until later.
4.1. Wavelets
The spectral graph wavelet transform is generated by wavelet operators that are
operator-valued functions of the Laplacian. One may deﬁne a measurable function of a
bounded self-adjoint linear operator on a Hilbert space using the continuous functional
calculus . This is achieved using the Spectral representation of the operator, which in
our setting is equivalent to the graph Fourier transform deﬁned in the previous section.
In particular, for our spectral graph wavelet kernel g, the wavelet operator Tg = g(L)
acts on a given function f by modulating each Fourier mode as
Tgf(￿) = g(λ￿) ˆf(￿)
Employing the inverse Fourier transform yields
(Tgf)(m) =
g(λ￿) ˆf(￿)χ￿(m)
The wavelet operators at scale t is then deﬁned by T t
g = g(tL). It should be emphasized that even though the “spatial domain” for the graph is discrete, the domain of the
kernel g is continuous and thus the scaling may be deﬁned for any positive real number
The spectral graph wavelets are then realized through localizing these operators by
applying them to the impulse on a single vertex, i.e.
ψt,n = T t
Expanding this explicitly in the graph domain shows
Formally, the wavelet coeﬃcients of a given function f are produced by taking the
inner product with these wavelets, as
Wf(t, n) = ￿ψt,n, f￿
Using the orthonormality of the {χ￿}, it can be seen that the wavelet coeﬃcients can
also be achieved directly from the wavelet operators, as
Wf(t, n) =
g(tλ￿) ˆf(￿)χ￿(n)
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
Figure 1: Scaling function h(λ) (blue curve), wavelet generating kernels g(tjλ), and sum of squares G
(black curve), for J = 5 scales, λmax = 10, K = 20. Details in Section 8.1.
Note that from equation 24, it can be seen that the wavelets ψt,n depend on the values
of g(tx) only for x in the spectrum of L. This implies that selection of scales appropriate
for a particular problem requires some knowledge of the spectrum. As we shall see later
in section 8.1, this will be done using an upper bound on the largest eigenvalue of L.
4.2. Scaling functions
By construction, the spectral graph wavelets ψt,n are all orthogonal to the null eigenvector χ0, and nearly orthogonal to χ￿for λ￿near zero. In order to stably represent the
low frequency content of f deﬁned on the vertices of the graph, it is convenient to introduce a second class of waveforms, analogous to the lowpass residual scaling functions
from classical wavelet analysis. These spectral graph scaling functions have an analogous
construction to the spectral graph wavelets. They will be determined by a single real
valued function h : R+ →R, which acts as a lowpass ﬁlter, and satisﬁes h(0) > 0 and
h(x) →0 as x →∞. The scaling functions are then given by φn = Thδn = h(L)δn, and
the coeﬃcients by Sf(n) = ￿φn, f￿.
Introducing the scaling functions helps ensure stable recovery of the original signal f
from the wavelet coeﬃcients when the scale parameter t is sampled at a discrete number
of values tj, i.e. so that small perturbations in the wavelet coeﬃcients cannot lead to
large changes in recovered f. As we shall see in detail in Section 5.3, stable recovery will
be assured if the quantity G(λ) = h(λ)2 + ￿J
j=1 g(tjλ)2 is bounded away from zero on
the spectrum of L. Representative choices for h and g are shown in ﬁgure 1; the exact
speciﬁcation of h and g is deferred to Section 8.1.
Note that the scaling functions deﬁned in this way are present merely to smoothly
represent the low frequency content on the graph. They do not generate the wavelets
ψ through the two-scale relation as for traditional orthogonal wavelets. The design of
the scaling function generator h is thus uncoupled from the choice of wavelet kernel g,
provided reasonable tiling for G is achieved.
5. Transform properties
In this section we detail several properties of the spectral graph wavelet transform.
We ﬁrst show an inverse formula for the transform analogous to that for the continuous
wavelet transform. We examine the small-scale and large-scale limits, and show that the
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
wavelets are localized in the limit of small scales. Finally we discuss discretization of the
scale parameter and the resulting wavelet frames.
5.1. Continuous SGWT Inverse
In order for a particular transform to be useful for signal processing, and not simply
signal analysis, it must be possible to reconstruct a signal corresponding to a given set
of transform coeﬃcients. We will show that the spectral graph wavelet transform admits
an inverse formula analogous to (4) for the continuous wavelet transform.
Intuitively, the wavelet coeﬃcient Wf(t, n) provides a measure of “how much of” the
wavelet ψt,n is present in the signal f. This suggests that the original signal may be
recovered by summing the wavelets ψt,n multiplied by each wavelet coeﬃcient Wf(t, n).
The reconstruction formula below shows that this is indeed the case, subject to a nonconstant weight dt/t.
Lemma 5.1. If the SGWT kernel g satisﬁes the admissibility condition
dx = Cg < ∞,
and g(0) = 0, then
Wf(t, n)ψt,n(m)dt
t = f #(m)
where f # = f −￿χ0, f￿χ0. In particular, the complete reconstruction is then given by
f = f # + ˆf(0)χ0.
Proof. Using (24) and (26) to express ψt,n and Wf(t, n) in the graph Fourier basis, the
l.h.s. of the above becomes
g(tλ￿)χ￿(n) ˆf(￿)
￿￿(n)χ￿￿(m)
g(tλ￿￿)g(tλ￿) ˆf(￿)χ￿￿(m)
￿￿(n)χ￿(n)
The orthonormality of the χ￿implies ￿
￿￿(n)χ￿(n) = δ￿,￿￿, inserting this above and
summing over ￿￿gives
ˆf(￿)χ￿(m)
If g satisﬁes the admissibility condition, then the substitution u = tλ￿shows that
dt = Cg independent of ￿, except for when λ￿= 0 at ￿= 0 when the integral is zero. The expression (31) can be seen as the inverse Fourier transform evaluated
at vertex m, where the ￿= 0 term is omitted. This omitted term is exactly equal to
￿χ0, f￿χ0 = ˆf(0)χ0, which proves the desired result.
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
Note that for the non-normalized Laplacian, χ0 is constant on every vertex and f #
above corresponds to removing the mean of f. Formula (28) shows that the mean of f
may not be recovered from the zero-mean wavelets. The situation is diﬀerent from the
analogous reconstruction formula (4) for the CWT, which shows the somewhat counterintuitive result that it is possible to recover a non zero-mean function by summing
zero-mean wavelets. This is possible on the real line as the Fourier frequencies are continuous; the fact that it is not possible for the SGWT should be considered a consequence
of the discrete nature of the graph domain.
While it is of theoretical interest, we note that this continuous scale reconstruction
formula may not provide a practical reconstruction in the case when the wavelet coef-
ﬁcients may only be computed at a discrete number of scales, as is the case for ﬁnite
computation on a digital computer. We shall revisit this and discuss other reconstruction
methods in sections 5.3 and 7.
5.2. Localization in small scale limit
One of the primary motivations for the use of wavelets is that they provide simultaneous localization in both frequency and time (or space). It is clear by construction that
if the kernel g is localized in the spectral domain, as is loosely implied by our use of the
term bandpass ﬁlter to describe it, then the associated spectral graph wavelets will all be
localized in frequency. In order to be able to claim that the spectral graph wavelets can
yield localization in both frequency and space, however, we must analyze their behaviour
in the space domain more carefully.
For the classical wavelets on the real line, the space localization is readily apparent :
if the mother wavelet ψ(x) is well localized in the interval [−￿, ￿], then the wavelet ψt,a(x)
will be well localized within [a−￿t, a+￿t]. In particular, in the limit as t →0, ψt,a(x) →0
for x ￿= a. The situation for the spectral graph wavelets is less straightforward to analyze
because the scaling is deﬁned implicitly in the Fourier domain. We will nonetheless show
that, for g suﬃciently regular near 0, the normalized spectral graph wavelet ψt,j/ ||ψt,j||
will vanish on vertices suﬃciently far from j in the limit of ﬁne scales, i.e. as t →0. This
result will provide a quantitative statement of the localization properties of the spectral
graph wavelets.
One simple notion of localization for ψt,n is given by its value on a distant vertex m,
e.g. we should expect ψt,n(m) to be small if n and m are separated, and t is small. Note
that ψt,n(m) = ￿ψt,n, δm￿=
. The operator T t
g = g(tL) is self-adjoint as L is
self adjoint. This shows that ψt,n(m) =
, i.e. a matrix element of the operator
Our approach is based on approximating g(tL) by a low order polynomial in L as
t →0. As is readily apparent by inspecting equation (22), the operator T t
only on the values of gt(λ) restricted to the spectrum {λ￿}N−1
￿=0 of L. In particular, it is
insensitive to the values of gt(λ) for λ > λN−1. If g(λ) is smooth in a neighborhood of the
origin, then as t approaches 0 the zoomed in gt(λ) can be approximated over the entire
interval [0, λN−1] by the Taylor polynomial of g at the origin. In order to transfer the
study of the localization property from g to an approximating polynomial, we will need
to examine the stability of the wavelets under perturbations of the generating kernel.
This, together with the Taylor approximation will allow us to examine the localization
properties for integer powers of the Laplacian L.
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
In order to formulate the desired localization result, we must specify a notion of
distance between points m and n on a weighted graph. We will use the shortest-path
distance, i.e. the minimum number of edges for any paths connecting m and n :
dG(m, n) = argmin
{k1, k2, ..., ks}
s.t. m = k1, n = ks, and akr,kr+1 > 0 for 1 ≤r < s.
Note that as we have deﬁned it, dG disregards the values of the edge weights. In particular
it deﬁnes the same distance function on G as on the binarized graph where all of the
nonzero edge weights are set to unit weight.
We now state the localization result for integer powers of the Laplacian.
Lemma 5.2. Let G be a weighted graph, L the graph Laplacian (normalized or nonnormalized) and s > 0 an integer. For any two vertices m and n, if dG(m, n) > s then
(Ls)m,n = 0.
Proof. First note that Li,j = 0 if i and j are distinct vertices that are not connected by
a nonzero edge. By repeatedly expressing matrix multiplication with explicit sums, we
Lm,k1Lk1,k2...Lks−1,n
where the sum is taken over all s −1 length sequences k1, k2...ks−1 with 1 ≤kr ≤N.
Assume for contradiction that (Ls)m,n ￿= 0. This is only possible if at least one of the
terms in the above sum is nonzero, i.e. there exists k1, k2, ...ks−1 such that Lm,k1 ￿= 0,
Lk1,k2 ￿= 0, ... , Lks−1 ￿= 0. After removing possibly repeated values of the kr’s, this
implies the existence of a path of length less than or equal to s from m to n, so that
d(m, n) ≤s, which contradicts the hypothesis.
We now proceed to examining how perturbations in the kernel g aﬀect the wavelets
in the vertex domain. If two kernels g and ˜g are close to each other in some sense, then
the resulting wavelets should be close to each other. More precisely, we have
Lemma 5.3. Let ψt,n = T t
gδn and ˜ψt,n = T t
˜gδn be the wavelets at scale t generated by
the kernels g and ˜g. If |g(tλ) −˜g(tλ)| ≤M(t) for all λ ∈[0, λN−1], then |ψt,n(m) −
˜ψt,n(m)| ≤M(t) for each vertex m. Additionally,
￿￿￿ψt,n −˜ψt,n
Proof. First recall that ψt,n(m) = ￿δm, g(tL)δn￿. Thus,
|ψt,n(m) −˜ψt,n(m)| = | ￿δm, (g(tL) −˜g(tL)) δn￿|
χ￿(m)(g(tλ￿) −˜g(tλ￿))χ∗
|χ￿(m)χ￿(n)∗|
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
where we have used the Parseval relation (20) on the second line. By Cauchy-Schwartz,
the above sum over ￿is bounded by 1 as
￿|χ￿(m)|2 = 1 for all m, as the χ￿form a complete orthonormal basis.3 Using this
bound in (35) proves the ﬁrst statement.
The second statement follows immediately as
￿￿￿ψt,n −˜ψt,n
ψt,n(m) −˜ψt,n(m)
M(t)2 = NM(t)2
We will prove the ﬁnal localization result for kernels g which have a zero of integer
multiplicity at the origin. Such kernels can be approximated by a single monomial for
small scales.
Lemma 5.4. Let g be K + 1 times continuously diﬀerentiable, satisfying g(0) = 0,
g(r)(0) = 0 for all r < K, and g(K)(0) = C ￿= 0. Assume that there is some t￿> 0 such
that |g(K+1)(λ)| ≤B for all λ ∈[0, t￿λN−1]. Then, for ˜g(tλ) = (C/K!)(tλ)K we have
λ∈[0,λN−1]
|g(tλ) −˜g(tλ)| ≤tK+1
for all t < t￿.
Proof. As the ﬁrst K−1 derivatives of g are zero, Taylor’s formula with remainder shows,
for any values of t and λ,
g(tλ) = C (tλ)K
+ g(K+1)(x∗) (tλ)K+1
for some x∗∈[0, tλ]. Now ﬁx t < t￿. For any λ ∈[0, λN−1], we have tλ < t￿λN−1, and so
the corresponding x∗∈[0, t￿λN−1], and so |g(K+1)(x∗) ≤B. This implies
|g(tλ) −˜g(tλ)| ≤B tK+1λK+1
≤B tK+1λK+1
As this holds for all λ ∈[0, λN−1], taking the sup over λ gives the desired result.
We are now ready to state the complete localization result. Note that due to the
normalization chosen for the wavelets, in general ψt,n(m) →0 as t →0 for all m and n.
Thus a non vacuous statement of localization must include a renormalization factor in
the limit of small scales.
3Orthonormality typically reads as P
￿(m)χ￿￿(m) = δ￿,￿￿. To see the desired statement with the
sum over ￿, set the matrix Ui,j = χj(i). Orthonormality implies U∗U = I. As matrices commute with
their inverses, also UU∗= I which implies P
l (n) = δm,n
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
Theorem 5.5. Let G be a weighted graph with Laplacian L. Let g be a kernel satisfying
the hypothesis of Lemma 5.4, with constants t￿and B. Let m and n be vertices of G such
that dG(m, n) > K. Then there exist constants D and t￿￿, such that
||ψt,n|| ≤Dt
for all t < min(t￿, t￿￿).
Proof. Set ˜g(λ) = g(K)(0)
λK and ˜ψt,n = T t
˜gδn. We have
˜ψt,n(m) = g(K)(0)
by Lemma 5.2, as dG(m, n) > K. By the results of Lemmas 5.3 and 5.4, we have
|ψt,n(m) −˜ψt,n(m)| = |ψt,n(m)| ≤tK+1C￿
(K+1)!B. Writing ψt,n = ˜ψt,n+(ψt,n−˜ψt,n) and applying the triangle inequality
￿￿￿ψt,n −˜ψt,n
￿￿￿≤||ψt,n||
We may directly calculate
￿￿￿= tK g(K)(0)
￿￿￿￿, and we have
￿￿￿ψt,n −˜ψt,n
NtK+1 λK+1
(K+1)!B from Lemma 5.4. These imply together that the l.h.s. of (44) is greater
than or equal to tK
. Together with (43), this shows
||ψt,n|| ≤
￿￿￿￿and b =
An elementary calculation shows
a t if t ≤
This implies the desired result with D =
g(K)(0)||LKδn|| and
g(K)(0)||LKδn||(K+1)
As this localization result uses the shortest path distance deﬁned without
using the edge weights, it is only directly useful for sparse weighted graphs where a
signiﬁcant number of edge weights are exactly zero. Many large-scale graphs which arise
in practice are sparse, however, so the class of sparse weighted graphs is of practical
signiﬁcance.
5.3. Spectral Graph Wavelet Frames
The spectral graph wavelets depend on the continuous scale parameter t. For any
practical computation, t must be sampled to a ﬁnite number of scales. Choosing J scales
j=1 will yield a collection of NJ wavelets ψtj,n, along with the N scaling functions
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
It is a natural question to ask how well behaved this set of vectors will be for representing functions on the vertices of the graph. We will address this by considering the
wavelets at discretized scales as a frame, and examining the resulting frame bounds.
We will review the basic deﬁnition of a frame. A more complete discussion of frame
theory may be found in and . Given a Hilbert space H, a set of vectors Γk ∈H
form a frame with frame bounds A and B if the inequality
A ||f||2 ≤
| ￿f, Γk￿|2 ≤B ||f||2
holds for all f ∈H.
The frame bounds A and B provide information about the numerical stability of
recovering the vector f from inner product measurements ￿f, Γk￿. These correspond to
the scaling function coeﬃcients Sf(n) and wavelet coeﬃcients Wf(tj, n) for the frame
consisting of the scaling functions and the spectral graph wavelets with sampled scales.
As we shall see later in section 7, the speed of convergence of algorithms used to invert
the spectral graph wavelet transform will depend on the frame bounds.
Theorem 5.6. Given a set of scales {tj}J
j=1, the set F = {φn}N
n=1 ∪{ψtj,n}J
forms a frame with bounds A, B given by
λ∈[0,λN−1] G(λ)
λ∈[0,λN−1] G(λ),
where G(λ) = h2(λ) + ￿
j g(tjλ)2.
Proof. Fix f. Using expression (26), we see
|Wf(t, n)|2 =
g(tλ￿)χ￿(n) ˆf(￿)
g(tλ￿￿)χ￿￿(n) ˆf(￿￿)
|g(tλ￿)|2| ˆf(￿)|2
upon rearrangement and using ￿
￿￿(n) = δ￿,￿￿. Similarly,
|Sf(n)|2 =
|h(λ￿)|2| ˆf(￿)|2
Denote by Q the sum of squares of inner products of f with vectors in the collection F.
Using (48) and (49), we have
|h(λ￿)|2 +
|g(tjλ￿)|2
| ˆf(￿)|2 =
G(λ￿)| ˆf(λ￿)|2
Then by the deﬁnition of A and B, we have
| ˆf(￿)|2 ≤Q ≤B
Using the Parseval relation ||f||2 = ￿
￿| ˆf(￿)|2 then gives the desired result.
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
6. Polynomial Approximation and Fast SGWT
We have deﬁned the SGWT explicitly in the space of eigenfunctions of the graph
Laplacian. The naive way of computing the transform, by directly using equation (26),
requires explicit computation of the entire set of eigenvectors and eigenvalues of L. This
approach scales poorly for large graphs. General purpose eigenvalue routines such as the
QR algorithm have computational complexity of O(N 3) and require O(N 2) memory .
Direct computation of the SGWT through diagonalizing L is feasible only for graphs with
fewer than a few thousand vertices. In contrast, problems in signal and image processing
routinely involve data with hundreds of thousands or millions of dimensions. Clearly, a
fast transform that avoids the need for computing the complete spectrum of L is needed
for the SGWT to be a useful tool for practical computational problems.
We present a fast algorithm for computing the SGWT that is based on approximating
the scaled generating kernels g by low order polynomials. Given this approximation, the
wavelet coeﬃcients at each scale can then be computed as a polynomial of L applied to
the input data. These can be calculated in a way that accesses L only through repeated
matrix-vector multiplication. This results in an eﬃcient algorithm in the important case
when the graph is sparse, i.e. contains a small number of edges.
We ﬁrst show that the polynomial approximation may be taken over a ﬁnite range
containing the spectrum of L.
Lemma 6.1. Let λmax ≥λN−1 be any upper bound on the spectrum of L. For ﬁxed t > 0,
let p(x) be a polynomial approximant of g(tx) with L∞error B = supx∈[0,λmax] |g(tx) −
p(x)|. Then the approximate wavelet coeﬃcients ˜Wf(t, n) = (p(L)f)n satisfy
|Wf(t, n) −˜Wf(t, n)| ≤B ||f||
Proof. Using equation (26) we have
|Wf(t, n) −˜Wf(t, n)| =
g(tλ￿) ˆf(￿)χ￿(n) −
p(λ￿) ˆf(￿)χ￿(n)
|g(tλ￿) −p(λ￿)|| ˆf(￿)χ￿(n)|
The last step follows from using Cauchy-Schwartz and the orthonormality of the χ￿’s.
Remark : The results of the lemma hold for any λmax ≥λN−1. Computing extremal
eigenvalues of a self-adjoint operator is a well studied problem, and eﬃcient algorithms
exist that access L only through matrix-vector multiplication, notably Arnoldi iteration
or the Jacobi-Davidson method . In particular, good estimates for λN−1 may be
computed at far smaller cost than that of computing the entire spectrum of L.
For ﬁxed polynomial degree M, the upper bound on the approximation error from
Lemma 6.1 will be minimized if p is the minimax polynomial of degree M on the interval [0, λmax]. Minimax polynomial approximations are well known, in particular it has
been shown that they exist and are unique . Several algorithms exist for computing
minimax polynomials, most notably the Remez exchange algorithm .
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
Figure 2: (a) Wavelet kernel g(λ) (black), truncated Chebyshev expansion (blue) and minimax polynomial approximation (red) for degree m = 20. Approximation errors shown in (b), truncated Chebyshev
expansion has maximum error 0.206, minimax polynomial has maximum error 0.107 .
In this work, however, we will instead use a polynomial approximation given by the
truncated Chebyshev polynomial expansion of g(tx). It has been shown that for analytic
functions in an ellipse containing the approximation interval, the truncated Chebyshev
expansions gives an approximate minimax polynomial .
Minimax polynomials of
order m are distinguished by having their approximation error reach the same extremal
value at m + 2 points in their domain. As such, they distribute their approximation
error across the entire interval. We have observed that for the wavelet kernels we use
in this work, truncated Chebyshev expansions result in a maximum error only slightly
higher than the true minimax polynomials, and have a much lower approximation error
where the wavelet kernel to be approximated is smoothly varying.
A representative
example of this is shown in Figure 2. We have observed that for small weighted graphs
where the wavelets may be computed directly in the spectral domain, the truncated
Chebyshev expansion approximations give slightly lower approximation error than the
minimax polynomial approximations computed with the Remez algorithm.
For these reasons, we use approximating polynomials given by truncated Chebyshev
expansions.
In addition, we will exploit the recurrence properties of the Chebyshev
polynomials for eﬃcient evaluation of the approximate wavelet coeﬃcients. An overview
of Chebyshev polynomial approximation may be found in , we recall here brieﬂy a
few of their key properties.
The Chebyshev polynomials Tk(y) may be generated by the stable recurrence relation
Tk(y) = 2yTk−1(y) −Tk−2(y), with T0 = 1 and T1 = y. For y ∈[−1, 1], they satisfy
the trigonometric expression Tk(y) = cos (k arccos(y)), which shows that each Tk(y)
is bounded between -1 and 1 for y ∈[−1, 1].
The Chebyshev polynomials form an
orthogonal basis for L2([−1, 1],
1−y2 ), the Hilbert space of square integrable functions
with respect to the measure dy/
1 −y2. In particular they satisfy
Tl(y)Tm(y)
1 −y2 dy =
if m, l > 0
if m = l = 0
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
Every h ∈L2([−1, 1],
1−y2 ) has a convergent (in L2 norm) Chebyshev series
with Chebyshev coeﬃcients
1 −y2 dy = 2
cos(kθ)h(cos(θ))dθ
We now assume a ﬁxed set of wavelet scales tn. For each n, approximating g(tnx) for
x ∈[0, λmax] can be done by shifting the domain using the transformation x = a(y + 1),
with a = λmax/2. Denote the shifted Chebyshev polynomials T k(x) = Tk( x−a
may then write
g(tnx) = 1
cn,kT k(x),
valid for x ∈[0, λmax], with
cos(kθ)g(tn(a(cos(θ) + 1)))dθ.
For each scale tj, the approximating polynomial pj is achieved by truncating the
Chebyshev expansion (58) to Mj terms. We may use exactly the same scheme to approximate the scaling function kernel h by the polynomial p0.
Selection of the values of Mj may be considered a design problem, posing a trade-oﬀ
between accuracy and computational cost. The fast SGWT approximate wavelet and
scaling function coeﬃcients are then given by
˜Wf(tj, n) =
cj,kT k(L)f
c0,kT k(L)f
The utility of this approach relies on the eﬃcient computation of T k(L)f. Crucially,
we may use the Chebyshev recurrence to compute this for each k < Mj accessing L
only through matrix-vector multiplication. As the shifted Chebyshev polynomials satisfy
T k(x) = 2
a(x −1)T k−1(x) −T k−2(x), we have for any f ∈RN,
T k(L)f = 2
−T k−2(L)f
Treating each vector T k(L)f as a single symbol, this relation shows that the vector
T k(L)f can be computed from the vectors T k−1(L)f and T k−2(L)f with computational
cost dominated by a single matrix-vector multiplication by L.
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
Many weighted graphs of interest are sparse, i.e. they have a small number of nonzero
edges. Using a sparse matrix representation, the computational cost of applying L to a
vector is proportional to |E|, the number of nonzero edges in the graph. The computational complexity of computing all of the Chebyshev polynomials Tk(L)f for k ≤M
is thus O(M|E|). The scaling function and wavelet coeﬃcients at diﬀerent scales are
formed from the same set of Tk(L)f, but by combining them with diﬀerent coeﬃcients
cj,k. The computation of the Chebyshev polynomials thus need not be repeated, instead
the coeﬃcients for each scale may be computed by accumulating each term of the form
cj,kTk(L)f as Tk(L)f is computed for each k ≤M. This requires O(N) operations at
scale j for each k ≤Mj, giving an overall computational complexity for the fast SGWT
of O(M|E| + N ￿J
j=0 Mj), where J is the number of wavelet scales. In particular, for
classes of graphs where |E| scales linearly with N, such as graphs of bounded maximal
degree, the fast SGWT has computational complexity O(N). Note that if the complexity
is dominated by the computation of the Tk(L)f, there is little beneﬁt to choosing Mj to
vary with j.
Applying the recurrence (61) requires memory of size 3N. The total memory requirement for a straightforward implementation of the fast SGWT would then be N(J + 1) +
6.1. Fast computation of Adjoint
Given a ﬁxed set of wavelet scales {tj}J
j=1, and including the scaling functions φn,
one may consider the overall wavelet transform as a linear map W : RN →RN(J+1)
deﬁned by Wf =
(Thf)T , (T t1
g f)T , · · · , (T tJ
g f)T ￿T . Let ˜W be the corresponding approximate wavelet transform deﬁned by using the fast SGWT approximation, i.e. ˜Wf =
(p0(L)f)T , (p1(L)f)T , · · · , (pJ(L)f)T ￿T . We show that both the adjoint ˜W ∗: RN(J+1) →
RN, and the composition W ∗W : RN →RN can be computed eﬃciently using Chebyshev polynomial approximation. This is important as several methods for inverting the
wavelet transform or using the spectral graph wavelets for regularization can be formulated using the adjoint operator, as we shall see in detail later in Section 7.
For any η ∈RN(J+1), we consider η as the concatenation η = (ηT
1 , · · · , ηT
each ηj ∈RN for 0 ≤j ≤J.
Each ηj for j ≥1 may be thought of as a subband
corresponding to the scale tj, with η0 representing the scaling function coeﬃcients. We
￿η, Wf￿N(J+1) = ￿η0, Thf￿+
as Th and each T tj
are self adjoint.
As (62) holds for all f ∈RN, it follows that
W ∗η = Thη0 + ￿J
g ηn, i.e. the adjoint is given by re-applying the corresponding
wavelet or scaling function operator on each subband, and summing over all scales.
This can be computed using the same fast Chebyshev polynomial approximation
scheme in equation (60) as for the forward transform, e.g. as ˜W ∗η = ￿J
j=0 pj(L)ηj.
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
Note that this scheme computes the exact adjoint of the approximate forward transform,
as may be veriﬁed by replacing Th by p0(L) and T tj
by pj(L) in (62).
We may also develop a polynomial scheme for computing ˜W ∗˜W. Naively computing
this by ﬁrst applying ˜W, then ˜W ∗by the fast SGWT would involve computing 2J
Chebyshev polynomial expansions.
By precomputing the addition of squares of the
approximating polynomials, this may be reduced to application of a single Chebyshev
polynomial with twice the degree, reducing the computational cost by a factor J. Note
pj(L) (pj(L)f) =
Set P(x) = ￿J
j=0(pj(x))2, which has degree M ∗= 2 max{Mj}. We seek to express P in
the shifted Chebyshev basis as P(x) = 1
k=1 dkT k(x). The Chebyshev polynomials
satisfy the product formula
Tk(x)Tl(x) = 1
Tk+l(x) + T|k−l|(x)
which we will use to compute the Chebyshev coeﬃcients dk in terms of the Chebyshev
coeﬃcients cj,k for the individual pj’s.
Expressing this explicitly is slightly complicated by the convention that the k = 0
Chebyshev coeﬃcient is divided by 2 in the Chebyshev expansion (58). For convenience
in the following, set c￿
j,k = cj,k for k ≥1 and c￿
2cj,0, so that pj(x) = ￿Mn
j,kT k(x).
Writing (pj(x))2 = ￿2∗Mn
j,kT k(x), and applying (64), we compute
j,k−i + ￿Mj−k
j,k+i + ￿Mj
if 0 < k ≤Mj
if Mj < k ≤2Mj
Finally, setting dn,0 = 2d￿
j,0 and dj,k = d￿
j,k for k ≥1, and setting dk = ￿J
gives the Chebyshev coeﬃcients for P(x). We may then compute
˜W ∗˜Wf = P(L)f = 1
following (60).
7. Reconstruction
For most interesting signal processing applications, merely calculating the wavelet
coeﬃcients is not suﬃcient.
A wide class of signal processing applications are based
on manipulating the coeﬃcients of a signal in a certain transform, and later inverting
the transform. For the SGWT to be useful for more than simply signal analysis, it is
important to be able to recover a signal corresponding to a given set of coeﬃcients.
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
The SGWT is an overcomplete transform as there are more wavelets ψtj,n than original vertices of the graph. Including the scaling functions φn in the wavelet frame, the
SGWT maps an input vector f of size N to the N(J + 1) coeﬃcients c = Wf. As
is well known, this means that W will have an inﬁnite number of left-inverses M s.t.
MWf = f. A natural choice among the possible inverses is to use the pseudoinverse
L = (W ∗W)−1W ∗. The pseudoinverse satisﬁes the minimum-norm property
Lc = argmin
f∈RN ||c −Wf||2
For applications which involve manipulation of the wavelet coeﬃcients, it is very likely to
need to apply the inverse to a a set of coeﬃcients which no longer lie directly in the image
of W. The above property indicates that, in this case, the pseudoinverse corresponds to
orthogonal projection onto the image of W, followed by inversion on the image of W.
Given a set of coeﬃcients c, the pseudoinverse will be given by solving the square
matrix equation (W ∗W)f = W ∗c. This system is too large to invert directly. Solving
it may be performed using any of a number of iterative methods, including the classical
frame algorithm , and the faster conjugate gradients method . These methods
have the property that each step of the computation is dominated by application of
W ∗W to a single vector. We use the conjugate gradients method, employing the fast
polynomial approximation (66) for computing application of ˜W ∗˜W.
8. Implementation and examples
In this section we ﬁrst give the explicit details of the wavelet and scaling function
kernels used, and how we select the scales. We then show examples of the spectral graph
wavelets on several diﬀerent real and synthetic data sets.
8.1. SGWT design details
Our choice for the wavelet generating kernel g is motivated by the desire to achieve
localization in the limit of ﬁne scales. According to Theorem 5.5, localization can be
ensured if g behaves as a monic power of x near the origin. We choose g to be exactly a
monic power near the origin, and to have power law decay for large x. In between, we
set g to be a cubic spline such that g and g￿are continuous. Our g is parametrized by
the integers α and β, and x1 and x2 determining the transition regions :
g(x; α, β, x1, x2) =
for x < x1
for x1 ≤x ≤x2
for x > x2
Note that g is normalized such that g(x1) = g(x2) = 1. The coeﬃcients of the cubic
polynomial s(x) are determined by the continuity constraints s(x1) = s(x2) = 1 , s￿(x1) =
α/x1 and s￿(x2) = −β/x2.
All of the examples in this paper were produced using
α = β = 1, x1 = 1 and x2 = 2; in this case s(x) = −5 + 11x −6x2 + x3.
The wavelet scales tj are selected to be logarithmically equispaced between the minimum and maximum scales tJ and t1. These are themselves adapted to the upper bound
λmax of the spectrum of L. The placement of the maximum scale t1 as well as the scaling
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
Figure 3: Spectral graph wavelets on Swiss Roll data cloud, with J = 4 wavelet scales. (a) vertex at
which wavelets are centered (b) scaling function (c)-(f) wavelets, scales 1-4.
function kernel h will be determined by the selection of λmin = λmax/K, where K is a
design parameter of the transform. We then set t1 so that g(t1x) has power-law decay
for x > λmin, and set tJ so that g(tJx) has monic polynomial behaviour for x < λmax.
This is achieved by t1 = x2/λmin and tJ = x2/λmax.
For the scaling function kernel we take h(x) = γ exp(−(
0.6λmin )4), where γ is set such
that h(0) has the same value as the maximum value of g.
This set of scaling function and wavelet generating kernels, for parameters λmax = 10,
K = 20, α = β = 2, x1 = 1, x2 = 2, and J = 4, are shown in Figure 1.
8.2. Illustrative examples : spectral graph wavelet gallery
As a ﬁrst example of building wavelets in a point cloud domain, we consider the
spectral graph wavelets constructed on the “Swiss roll”. This example data set consists
of points randomly sampled on a 2-d manifold that is embedded in R3. The manifold is
described parametrically by ￿x(s, t) = (t cos(t)/4π, s, t sin(t)/4π) for −1 ≤s ≤1, π ≤t ≤
4π. For our example we take 500 points sampled uniformly on the manifold.
Given a collection xi of points, we build a weighted graph by setting edge weights
ai,j = exp(−||xj −xj||2 /2σ2). For larger data sets this graph could be sparsiﬁed by
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
Figure 4: Spectral graph wavelets on Minnesota road graph, with K = 100, J = 4 scales. (a) vertex at
which wavelets are centered (b) scaling function (c)-(f) wavelets, scales 1-4.
thresholding the edge weights, however we do not perform this here. In Figure 3 we
show the Swiss roll data set, and the spectral graph wavelets at four diﬀerent scales
localized at the same location. We used σ = 0.1 for computing the underlying weighted
graph, and J = 4 scales with K = 20 for computing the spectral graph wavelets. In many
examples relevant for machine learning, data are given in a high dimensional space that
intrinsically lie on some underlying lower dimensional manifold. This ﬁgure shows how
the spectral graph wavelets can implicitly adapt to the underlying manifold structure of
the data, in particular notice that the support of the coarse scale wavelets diﬀuse locally
along the manifold and do not “jump” to the upper portion of the roll.
A second example is provided by a transportation network. In Figure 4 we consider a
graph describing the road network for Minnesota. In this dataset, edges represent major
roads and vertices their intersection points, which often but not always correspond to
towns or cities. For this example the graph is unweighted, i.e. the edge weights are all
equal to unity independent of the physical length of the road segment represented. In
particular, the spatial coordinates of each vertex are used only for displaying the graph
and the corresponding wavelets, but do not aﬀect the edge weights. We show wavelets
constructed with K = 100 and J = 4 scales.
Graph wavelets on transportation networks could prove useful for analyzing data
measured at geographical locations where one would expect the underlying phenomena
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
Figure 5: Spectral graph wavelets on cerebral cortex, with K = 50, J = 4 scales. (a) ROI at which
wavelets are centered (b) scaling function (c)-(f) wavelets, scales 1-4.
to be inﬂuenced by movement of people or goods along the transportation infrastructure. Possible example applications of this type include analysis of epidemiological data
describing the spread of disease, analysis of inventory of trade goods (e.g. gasoline or
grain stocks) relevant for logistics problems, or analysis of census data describing human
migration patterns.
Another promising potential application of the spectral graph wavelet transform is for
use in data analysis for brain imaging. Many brain imaging modalities, notably functional
MRI, produce static or time series maps of activity on the cortical surface. Functional
MRI imaging attempts to measure the diﬀerence between “resting” and “active” cortical
states, typically by measuring MRI signal correlated with changes in cortical blood ﬂow.
Due to both constraints on imaging time and the very indirect nature of the measurement,
functional MRI images typically have a low signal-to-noise ratio. There is thus a need for
techniques for dealing with high levels of noise in functional MRI images, either through
direct denoising in the image domain or at the level of statistical hypothesis testing for
deﬁning active regions.
Classical wavelet methods have been studied for use in fMRI processing, both for
denoising in the image domain and for constructing statistical hypothesis testing
 . The power of these methods relies on the assumption that the underlying cortical
activity signal is spatially localized, and thus can be eﬃciently represented with localized
wavelet waveforms. However, such use of wavelets ignores the anatomical connectivity
of the cortex.
A common view of the cerebral cortex is that it is organized into distinct functional
regions which are interconnected by tracts of axonal ﬁbers. Recent advances in diﬀusion
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
MRI imaging, notable diﬀusion tensor imaging (DTI) and diﬀusion spectrum imaging
(DSI), have enabled measuring the directionality of ﬁber tracts in the brain. By tracing the ﬁber tracts, it is possible to non-invasively infer the anatomical connectivity of
cortical regions. This raises an interesting question of whether knowledge of anatomical
connectivity can be exploited for processing of image data on the cortical surface.
We 4 have begun to address this issue by implementing the spectral graph wavelets
on a weighted graph which captures the connectivity of the cortex. Details of measuring
the cortical connection matrix are described in . Very brieﬂy, the cortical surface
is ﬁrst subdivided into 998 Regions of Interest (ROI’s). A large number of ﬁber tracts
are traced, then the connectivity of each pair of ROI’s is proportional to the number of
ﬁber tracts connecting them, with a correction term depending on the measured ﬁber
length. The resulting symmetric matrix can be viewed as a weighted graph where the
vertices are the ROI’s. Figure 5 shows example spectral graph wavelets computed on
the cortical connection graph, visualized by mapping the ROI’s back onto a 3d model
of the cortex. Only the right hemisphere is shown, although the wavelets are deﬁned on
both hemispheres. For future work we plan to investigate the use of these cortical graph
wavelets for use in regularization and denoising of functional MRI data.
A ﬁnal interesting application for the spectral graph wavelet transform is the construction of wavelets on irregularly shaped domains. As a representative example, consider
that for some problems in physical oceanography one may need to manipulate scalar
data, such as water temperature or salinity, that is only deﬁned on the surface of a given
body of water. In order to apply wavelet analysis for such data, one must adapt the
transform to the potentially very complicated boundary between land and water. The
spectral wavelets handle the boundary implicitly and gracefully. As an illustration we
examine the spectral graph wavelets where the domain is determined by the surface of a
For this example the lake domain is given as a mask deﬁned on a regular grid. We
construct the corresponding weighted graph having vertices that are grid points inside
the lake, and retaining only edges connecting neighboring grid points inside the lake.
We set all edge weights to unity. The corresponding graph Laplacian is thus exactly the
5-point stencil (13) for approximating the continuous operator −∇2 on the interior of
the domain; while at boundary points the graph Laplacian is modiﬁed by the deletion
of edges leaving the domain. We show an example wavelet on Lake Geneva in Figure
Shoreline data was taken from the GSHHS database and the lake mask was
created on a 256 x 153 pixel grid using an azimuthal equidistant projection, with a scale
of 232 meters/pixel. The wavelet displayed is from the coarsest wavelet scale, using the
generating kernel described in 8.1 with parameters K = 100 and J = 5 scales.
For this type of domain derived by masking a regular grid, one may compare the
wavelets with those obtained by simply truncating the wavelets derived from a large
regular grid. As the wavelets have compact support, the true and truncated wavelets will
coincide for wavelets located far from the irregular boundary. As can be seen in Figure 6,
however, they are quite diﬀerent for wavelets located near the irregular boundary. This
comparison gives direct evidence for the ability of the spectral graph wavelets to adapt
gracefully and automatically to the arbitrarily shaped domain.
4In collaboration with Dr Leila Cammoun and Prof. Jean-Philippe Thiran, EPFL, Lausanne, Dr
Patric Hagmann and Prof. Reto Meuli, CHUV, Lausanne
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
Figure 6: Spectral graph wavelets on lake Geneva domain, (spatial map (a), contour plot (c)); compared
with truncated wavelets from graph corresponding to complete mesh (spatial map (b), contour plot (d)).
Note that the graph wavelets adapt to the geometry of the domain.
We remark that the regular sampling of data within the domain may be unrealistic
for problems where data are collected at irregularly placed sensor locations. The spectral
graph wavelet transform could also be used in this case by constructing a graph with
vertices at the sensor locations, however we have not considered such an example here.
9. Conclusions and Future Work
We have presented a framework for constructing wavelets on arbitrary weighted
By analogy with classical wavelet operators in the Fourier domain, we have
shown that scaling may be implemented in the spectral domain of the graph Laplacian.
We have shown that the resulting spectral graph wavelets are localized in the small scale
limit, and form a frame with easily calculable frame bounds. We have detailed an algorithm for computing the wavelets based on Chebyshev polynomial approximation that
avoids the need for explicit diagonalization of the graph Laplacian, and allows the application of the transform to large graphs. Finally we have shown examples of the wavelets
on graphs arising from several diﬀerent potential application domains.
There are many possible directions for future research for improving or extending the
SGWT. One property of the transform presented here is that, unlike classical orthogonal
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
wavelet transforms, we do not subsample the transform at coarser spatial scales. As a
result the SGWT is overcomplete by a factor of J+1 where J is the number of wavelet
scales. Subsampling of the SGWT can be determined by selecting a mask of vertices
at each scale corresponding to the centers of the wavelets to preserve. This is a more
diﬃcult problem on an arbitrary weighted graph than on a regular mesh, where one
may exploit the regular geometry of the mesh to perform dyadic subsampling at each
scale. An interesting question for future research would be to investigate an appropriate
criterion for determining a good selection of wavelets to preserve after subsampling. As
an example, one may consider preserving the frame bounds as much as possible under
the constraint that the overall overcompleteness should not exceed a speciﬁed factor.
A related question is to consider how the SGWT would interact with graph contraction. A weighted graph may be contracted by partitioning its vertices into disjoint sets;
the resulting contracted graph has vertices equal to the number of partitions and edge
weights determined by summing the weights of the edges connecting any two partitions.
Repeatedly contracting a given weighted graph could deﬁne a multiscale representation
of the weighted graph. Calculating a single scale of the spectral graph wavelet transform
for each of these contracted graphs would then yield a multiscale wavelet analysis. This
proposed scheme is inspired conceptually by the fast wavelet transform for classical orthogonal wavelets, based on recursive ﬁltering and subsampling. The question of how
to automatically deﬁne the contraction at each scale on an arbitrary irregular graph is
itself a diﬃcult research problem.
The spectral graph wavelets presented here are not directional. In particular when
constructed on regular meshes they yield radially symmetric waveforms. This can be
understood as in this case the graph Laplacian is the discretization of the isotropic continuous Laplacian. In the ﬁeld of image processing, however, it has long been recognized
that directionally selective ﬁlters are more eﬃcient at representing image structure. This
raises the interesting question of how, and when, graph wavelets can be constructed which
have some directionality. Intuitively, this will require some notion of local directionality,
i.e. some way of deﬁning directions of all of the neighbors of a given vertex. As this would
require the deﬁnition of additional structure beyond the raw connectivity information,
it may not be appropriate for completely arbitrary graphs. For graphs which arise from
sampling a known orientable manifold, such as the meshes with irregular boundary used
in Figure 6, one may infer such local directionality from the original manifold.
For some problems it may be useful to construct graphs that mix both local and
non-local connectivity information. As a concrete example consider the cortical graph
wavelets shown in Figure 5. As the vertices of the graph correspond to sets of MRI voxels
grouped into ROI’s, the wavelets are deﬁned on the ROI’s and thus cannot be used to
analyze data deﬁned on the scale of individual voxels. Analyzing voxel scale data with
the SGWT would require constructing a graph with vertices corresponding to individual
voxels. However, the nonlocal connectivity is deﬁned only on the scale of the ROI’s. One
way of deﬁning the connectivity for the ﬁner graph would be as a sum Anonlocal + Alocal,
where Anonlocal
is the weight of the connection between the ROI containing vertex m and
the ROI containing vertex n, and Alocal
m,n indexes whether m and n are spatial neighbors.
Under this scheme we consider Alocal as implementing a “default” local connectivity not
arising from any particular measurement. Considering this raises interesting questions
of how to balance the relative contributions of the local and nonlocal connectivities, as
c⃝2010 Elsevier. This is the author version of an article published in an Elsevier journal. The original publication
is available at www.sciencedirect.com with DOI: 10.1016/j.acha.2010.04.005
well as how the special structure of the hybrid connectivity matrix could be exploited
for eﬃcient computation.
The particular form of the wavelet generating kernel g used in the examples illustrating this work was chosen in a somewhat ad-hoc manner. Aside from localization in
the small-scale limit which required polynomial behaviour of g at the origin, we have
avoided detailed analysis of how the choice of g aﬀects the wavelets. In particular, we
have not chosen g and the choice of spatial scales to optimize the resulting frame bounds.
More detailed investigation is called for regarding optimizing the design of g for diﬀerent
applications.
The fast Chebyshev polynomial approximation scheme we describe here could itself
be useful independent of its application for computing the wavelet transform. One application could be for ﬁltering of data on irregularly shaped domains, such as described in
Figure 6. For example, smoothing data on such a domain by convolving with a Gaussian
kernel is confounded by the problem that near the edges the kernel would extend oﬀof
the domain. As an alternative, one could express the convolution as multiplication by
a function in the Fourier domain, approximate this function with a Chebyshev polynomial, and then apply the algorithm described in this paper. This could also be used for
band-pass or high-pass ﬁltering of data on irregular domains, by designing appropriate
ﬁlters in the spectral domain induced by the graph Laplacian.
The Chebyshev approximation scheme may also be useful for machine learning problems on graphs. Some recent work has studied using the “diﬀusion kernel” Kt = e−tL
for use with kernel-based machine learning algorithms . The Chebyshev polynomial
scheme provides a fast way to approximate this exponential that may be useful for large
problems on unstructured yet sparse graphs.
Code implementing the SGWT (in MATLAB) is available at wiki.epfl.ch/sgwt .