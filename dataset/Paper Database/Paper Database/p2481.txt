Dynamic Atention Deep Model for Article Recommendation by
Learning Human Editors’ Demonstration
Xuejian Wang†, Lantao Yu†, Kan Ren†, Guanyu Tao‡, Weinan Zhang†, Yong Yu†, Jun Wang♯∗
†Shanghai Jiao Tong University, ‡ULU Technologies Inc., ♯University College London
{xjwang,yulantao,kren,wnzhang,yyu}@apex.sjtu.edu.cn, , 
As aggregators, online news portals face great challenges in continuously selecting a pool of candidate articles to be shown to
their users. Typically, those candidate articles are recommended
manually by platform editors from a much larger pool of articles
aggregated from multiple sources. Such a hand-pick process is labor
intensive and time-consuming. In this paper, we study the editor
article selection behavior and propose a learning by demonstration
system to automatically select a subset of articles from the large
pool. Our data analysis shows that (i) editors’ selection criteria are
non-explicit, which are less based only on the keywords or topics,
but more depend on the quality and atractiveness of the writing
from the candidate article, which is hard to capture based on traditional bag-of-words article representation. And (ii) editors’ article
selection behaviors are dynamic: articles with diﬀerent data distribution come into the pool everyday and the editors’ preference
varies, which are driven by some underlying periodic or occasional
paterns. To address such problems, we propose a meta-atention
model across multiple deep neural nets to (i) automatically catch
the editors’ underlying selection criteria via the automatic representation learning of each article and its interaction with the meta data
and (ii) adaptively capture the change of such criteria via a hybrid
atention model. Te atention model strategically incorporates
multiple prediction models, which are trained in previous days. Te
system has been deployed in a commercial article feed platform. A
9-day A/B testing has demonstrated the consistent superiority of
our proposed model over several strong baselines.
Recommendation; Learning by Demonstration; Atention Models;
Convolutional Neural Network
∗X. Wang and L. Yu contribute equally and share the co-ﬁrst authorship. W. Zhang is
the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permited. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from .
KDD ’17, August 13-17, 2017, Halifax, NS, Canada
© 2017 ACM. 978-1-4503-4887-4/17/08...$15.00
DOI: 10.1145/3097983.3098096
INTRODUCTION
As the wide adoption of high bandwidth mobile networks such
as 4G, mobile news portals or news feed services including social
media posts and news articles have gained signiﬁcant
atention. Such textual content feed services or news portals are
commonly presented in a cascade-form of user interface (UI) and
interactively learn each user’s interest and further provide personalized content for them . Notable examples include BuzzFeeds
in United States, which serves more than 200 million unique users
monthly in 2016 , and Toutiao in China, which has 600 million
users in total and 66 million daily active users in 2016 .
Typically, there are two stages of news article ﬁltering in those
systems. In the ﬁrst stage, professionally trained editors select
articles manually, that they think are of high quality, from a huge
amount of crawled or submited articles and in the second stage,
user personalized recommender systems deliver relevant articles
to each end user with machine learning models based on user
data collection . So far, extensive researches have been
conducted in the second user-oriented stage . However,
litle atention has been made on how these articles are gathered as
candidates within the platform ﬁrst. For example, in the article feed
platform that we studied in this paper, each of the editors needs
to read more than 1,000 articles per day, which is a labor intensive
and time-consuming work. In this paper, we aim to alleviate the
platform editors’ working load by automating the manual article
selection process and recommending a subset of articles that ﬁts
the human editors’ taste and interest as illustrated in Figure 1.
We do this by learning through the limited demonstration from
the human editors1. Speciﬁcally, each editor tries to be objective
to perform a judgement on whether an article should be passed to
today’s candidate set, which will be further picked by the personalized recommender system to deliver to diﬀerent end users. Tus
it is feasible to regard the editor team as a whole article ﬁlter and
learn a model to select articles to ﬁt their hidden criteria. Such a
judgement process seems easy to be automated by training a binary
classiﬁer based on the text content. However, the underlying criteria for the editors’ selection is non-explicit and is hardly just based
on the keywords or topics of the article. Instead, it highly depends
on the writing styles, such as atractiveness and stringency, which
is hidden and hard to capture from the traditional bag-of-words
article representation or unsupervised topic models .
For example, in our test, a well-tuned naive Bayes text classiﬁer
1In commercial production, the proposed automation of article selection will not
completely replace the editors’ work but alleviate their working load. Te editors’
demonstration will constantly guide the learning system.
Huge numbers of candidate articles daily
Editors’ selected candidate articles
ready for recommendation
Quality articles selected
for news feed to end users
Editors manually select
quality articles
Personalized
Recommender System
Design automatic article
selection systems to alleviate
human editors’ working load
Training Data
User Historic
Figure 1: An illustration of the process that human editors select quality articles from a huge pool and then the recommender
systems will do the personalized recommendation for the end users on the selected candidate pool. Our work aims to automate
the quality article selection process to alleviate the editors’ working load.
 can only atain a fair 60% AUC performance in our deployed
commercial platform for the binary prediction of whether an editor
will select an article or not.
Te second challenge lies in that the crawled or submited article
data distribution and the editors’ article selection behavior on the
data are non-stationary. Articles with diﬀerent data distributions
come into the pool everyday and the editors’ preference also varies
signiﬁcantly, which could be driven by some underlying periodic or
occasional paterns. As we will see in Figures 4 and 5, the incoming
article volume, editors’ overall selection ratio, the distribution of
the source organization of the total and selected articles all vary
over time.
In this paper, we propose a Dynamic Atention Deep Model
(DADM) to address above two problems for the editor article recommendation task. Speciﬁcally, DADM uses character-level text
modeling and convolutional neural networks (CNNs) to
eﬀectively learn the representation of each article, which captures
words interaction paterns that are helpful to predict the editors’
selection behavior. More importantly, to handle diverse variance
of editors’ selection behavior, we introduce atention-based network architecture that would dynamically assign inﬂuence factors
on recent models based on the current article and these models’
timeliness, aiming to improve the performance of the current recommender system. Speciﬁcally, we propose to incorporate two
kinds of factors into the atention model:
• Model proﬁle is a latent factor associated with each day’s individual model, depicting which types of articles the model is
capable of predicting the editors’ selection behavior. For example, the Monday model would be more helpful on predicting a
ﬁnancial article, while the Friday model would be more leveraged to predict an entertainment article.
• Time proﬁle is a latent factor associated with each relative
day, which tells how fast the model prediction on some type of
articles would get out-of-date. For example, the editors’ selection
criteria on ﬁnancial articles varies dramatically while that on
academic articles varies slightly across time.
With such two types of factors, the hybrid atention model is capable
of adaptively allocating atention to previously trained individual
models and make a prediction with a lower variance.
Our proposed model has been deployed in ULU Technologies
article ﬁltering service rendered by application program interfaces
(APIs). A 9-day online A/B testing has been conducted, where at
the end of each day the editors return their article selection to
demonstrate the learning of the system, and the system learns to
predict the editors’ article selection during the next day. DADM
signiﬁcantly reduces training eﬀorts by dynamically reusing recent
learned models. Te results demonstrate higher than 90% AUC
performance of our proposed DADM and its consistent performance
gains over several strong baselines.
In the rest of the paper, we will discuss related work in Section 2
and present our model in Section 3. Te experiments and the corresponding results will be given in Section 4. Finally, we will conclude
this paper and discuss the future work in Section 5.
RELATED WORK
Recommender Systems
Tere are two main solutions to recommender systems, namely,
collaborative ﬁltering (CF) based and content based (CB) recommendation . CF methods focus on revealing the
paterns in the historic user-item interaction data and make
recommendations based on the assumption that users with similar
behavior like similar items, without considering any atributes or
content of the items. However, these methods face the cold-start
problem and may perform poor on sparse history logs . On
the other hand, content-based recommendation fully
depends on the item atributes or content thus has no cold-start
problem but may not provide personalized recommendation.
By contrast, the goal in this paper is to recommend articles for
professional editors by learning from textual contents and their side
information, e.g., author, original media, author city etc., which
needs to combine these two types of data to derive general models
for capturing non-trivial useful paterns. In addition, most articles
are news, which have very short time span, with very sparse user interaction data. As such, content based recommendation techniques
are more suitable for our work than CF ones.
In terms of tasks, our recommendation problem is diﬀerent compared to the conventional content-based recommendation or collaborative ﬁltering based recommender systems. In our case, the
‘users’ are a group of professional editors facing abundant articles
to manually classify or select. Tus, it is an aggregated interest
rather than individual, personalized interest studied by the most
mainstream recommender systems.
Deep Learning for Text Representation
Due to its adequate model capability and the support of big data,
deep neural network (DNN) has achieved a huge success in computer vision , speech recognition and natural language
processing during the recent ﬁve years. Neural network
language models (NNLM) provide a general solution for text
distributed representation learning . Ranging from character
level to word level , the embedding models trained by
back-propagation from the higher-layer neural work makes the
text representation of high ﬂexibility and eﬀectiveness. Further
techniques are adopted for constructing high level representations
of sentences and documents . Tese NNLM methods have
shown very promising performance for capturing semantic and
morphological relatedness . In this paper, we adopt characterlevel NNLM for low-level textual feature learning.
For textual classiﬁcation and recommendation tasks, deep learning also delivers convincing performance . In
 , the authors proposed a hierarchical Bayesian model to jointly
perform deep representation learning for the textual information
and collaborative ﬁltering for the rating matrix. Deep recurrent
neural networks are utilized in to encode the text sequence into
a latent vector and trained end-to-end on the collaborative ﬁltering
task. However, these methods require a large number of user-item
samples which are not guaranteed in our task and the variance of
editors’ selection criteria is an issue within these models. To our
knowledge, there has litle work in leveraging CNNs as the
representation learner for articles and based on that performing
article recommendation.
Attention-based Models
Atention is a mechanism to ﬂexibly selecting the reference part
of context information, which can facilitate global learning .
Atention model was originally proposed in machine translation
tasks to deal with the issue for encoder-decoder approaches that all
the necessary information should be compressed into the ﬁx-length
encoding vector . Soon afer the use on language, atention
model is leveraged on image caption task where the salient
part of an image is automatically detect and based on that the model
could generate high-quality description of the image. Ten, the
atention model is leveraged in various tasks. Te authors in 
utilized atention to capture hierarchical paterns of documents
from word to sentence and ﬁnally to the whole document. Te
authors in took atention on question text and extracted the
semantically related parts between question-answer pairs. Other
atention-based work includes natural language parsing and
text classiﬁcation . In these work, atention has been used as a
textual level mechanism for modeling interactions within diﬀerent
parts of the contents. In order to capture the dynamics of editors’
selection behavior, we build our new atention-based architectures,
which will dynamically take the eﬀects from recent knowledge to
the current model. To our knowledge, it is the ﬁrst work utilizing
atention to model varying user preferences.
METHODOLOGY
Problem Deﬁnition
We formally deﬁne the problem as below. We have a set of articles gathered from multiple sources at a given time t as Dt =
Nt }, where t ∈{1, ...,T } and Nt = |Dt | is the number of articles gathered at time t. In the past, we have observed
that the human editors have selected a subset of articles as relevant
ones: St ⊂Dt at each timestamp from t −1, t −2, …1, where
Mt = |St | and Mt ≪Nt . Now the task is, at the current time
t, to automatically obtain a new subset St from the new pool Dt
gathered. Te objective is to make the predicted set St as close as
possible to the one if we had asked the human editors to select.
To make the problem manageable, we assume the decision whether
a document is recommended or not is independent on the other
documents decisions in the same day although the documents could
be correlated. Te rationale behind the idea is that like text classiﬁcation, the model is trained to capture the underlying paterns
among documents and then is used to predict the label of each
document independently. Tus we could simplify the problem by
predicting the selection probability Pr(yi |dt
i ; Dt ) of the professional
editor taking the speciﬁc action yt of selecting for public readers
the given article dt
i ; and we then choose top-K documents as the
chosen set. In the dataset, each document’s features dt
consist of textual content x and categorical meta-data i, e.g. source
website, article categories, authors, etc.
Te above problem is a unique one. On one hand, at a speciﬁc
time t, it is an unsupervised problem as the document pool Dt is
disjoint from the previous and we don’t have any label at that time
timestamp. But, on the other hand, it is also a supervised binary
prediction task as we have human editors’ labels for previous time
t −1, ..., 1. Tus, we need to transfer or combine the knowledge of
recently learned models from the previous timestamps and improve
the overall performance of the current model.
Tere are three challenges for the modeling and learning of the
above problem. (i) Te editors’ selection criteria for rich textual
content is non-explicit, such as the atractiveness or stringency of
the writing, which could be hidden within some deep interaction
among word sequences. One needs a general model to capture such
underlying paterns, which is an open research topic in natural
language processing and content-based recommendation . (ii) Te meta-data is another issue since its sparse property
and categorical data type . Learning this hybrid style of
data remains unsolved in the community of data mining .
Moreover, the editors’ preference may vary over time, which implies
some trends and periodic paterns among daily dataset. So that the
third challenge is (iii) to dynamically capture varying preferences
for beter model generalization. Speciﬁcally, the ﬁnal objective is
to combine the knowledge of recent learned models and perform a
robust prediction.
To solve these problems, we propose our DADM model constituted by three main parts: (i) We take CNN-based method to extract
a general representation of the document; (ii) Motivated by wide &
deep model , we combine the linear model and our CNN part
together to jointly model the sparse categorical meta-information
and the sequential categorical data (textual content of the document). (iii) To adaptively capture the editors’ dynamic behavior,
we propose an atention model over multiple deep networks which
jointly considers the speciality and the timeliness of each model
trained in previous days.
Text Representation Learning
To model the textual content of the document, traditional methods including bag-of-words features , e.g. TF-IDF feature
or naive Bayes and unsupervised learning objective , e.g.
topic models, are based on counting statistics which ignore word orders and suﬀer from sparsity and poor generalization performance.
Considering the semantic relatedness between diﬀerent words, we
implement a convolutional neural network architecture to model
the text. Moreover, in order to generalize for diﬀerent languages,
we construct the CNN based on character level since not all the
languages have explicit “word” speciﬁcation. For example, Chinese
needs word segmentation which requires much speciﬁc domain
knowledge .
Figure 2 illustrates the architecture of our CNN component
which is motivated by . We will introduce the architecture
with a botom-up approach.
Te raw textual input of the document is represented as a sequence of characters x = {c1,c2, . . . ,cl } of length l, where the i-th
entry ci is one of the elements in the character set C. We deﬁne
E ∈Rd×|C| as the set of character embeddings where d is the
predeﬁned dimension of each embedding vector.
First of all, by concatenating the corresponding character embedding ei ∈Rd, provided by the embedding function Π : c →e ∈E,
we build the document matrix D ∈Rd×l.
Secondly, we apply a convolution operation on D with a kernel
Kj ∈Rd×w, j ∈[1, J] among the total J kernels of width w , to
obtain the feature map mj as
mj[i] = f (D[∗,i : i + w −1] ⊙Kj + bj) ,
wherei ∈[1,l−w+1] is the iteration index and mj ∈Rl−w+1, while
f is the non-linear transformation function such as the hyperbolic
tangent (tanh) f (z) = (exp(z) −exp(−z))/(exp(z) + exp(−z)) and
operation ⊙is the Frobenius inner product between two matrices.
Now we obtain a feature map matrix M = [m1, m2, . . . , mk] ∈
R(l−w+1)×k.
Tirdly, the max-over-time pooling is used on the column
of the feature map matrix such that
x = max M[∗,p] , p ∈[1,k] ,
where the pooling output x ∈Rk is the learned representation of
the textual content D.
Character Embedding
Input Text
Google released a new product yesterday
r e l e a s e
Feature Map
Max over Time
Highway Network
Single Model Prediction
CNN Output & Cat. Data
Figure 2: CNN architecture for text prediction.
Te ﬁnal part of the CNN model is a highway network deﬁned
q · xq + bH
q ) ,q ∈[1,n] ,
xq+1 = η · д(Wq · xq + bq) + (1 −η)xq ,
oD = σ(wD · xn + bD) ,
where σ(z) = 1/(1 + exp(−z)) is the sigmoid function and the ﬁnal
output oD, as the extracted features of the textual content, will
be fed into later prediction. д(·) represents the operation of each
highway net layer in the CNN model. Here η and (1 −η) plays
the role of “transform gate” and “carry gate” respectively, which
controls the information carried by the highway layer from input
to the output .
Te reason for the adoption of CNN model with one convolutional layer and a total of 1050 kernels on the textual content is
that convolutional operation and max-pooling technique can be
leveraged to capture the underlying semantic paterns within the
word sequence, which are helpful for the prediction but may not be
explicit to be speciﬁed. Recent literatures have shown
that CNN-based model can achieve promising performance comparative with or even more competitive than other deep models,
e.g. Long Short Term Memory (LSTM) in many NLP tasks.
Multi-view Categorical Data Modeling
As previously described, each document is combined with two parts
of information: textual content and categorical meta-information.
We apply a wide & deep infrastructure to jointly model these
two types of data as illustrated in the top layer of the architecture in Figure 2. Te diﬀerence is that we adopt a base model
with CNN architecture to represent the textual content, which
has been presented above, while only reuses the categorical
meta-information to both the deep part and the linear part.
We use one-hot representation for the categorical meta-information
of each article. Speciﬁcally, the ﬁeld set S contains three ﬁelds:
authors, source organizations and original websites. For the s-th
ﬁeld in S, we take binary vector os ∈Rls , where only the value of
the column corresponding to the presented category is 1 and the
value of other columns is 0. ls is the total number of the possible
category values taken in the s-th ﬁeld.
Tus we obtain a hybrid feature representation o, which combines the one-hot categorical vectors and numerical CNN output
2 ⊕, . . . ⊕o⊤
D]⊤∈Rl1+l2+...+l|S|+1 ,
where ⊕is the concatenation operator between vectors.
Afer all these operations, we utilize a logistic regression to
predict the ﬁnal probability over the model as
ˆy = Pr(yi |dt
i ; Dt ) = σ(w · o + b) .
Dynamic Attention Deep Model
Here we present our dynamic atention deep model (DADM) over
multiple networks, which plays a key role for capturing editors’
dynamic behavior paterns to make the ﬁnal prediction. Te basic
idea is from twofold considerations:
• Model Speciality: as the data distribution of the incoming
candidate articles is diﬀerent across days, the correspondent
trained model has diﬀerent speciality. For example, there would
be a higher portion of news articles on ﬁnance on Monday than
that on Saturday, thus for an incoming article about ﬁnance, it
is likely that the Monday model is more helpful to predict the
editors’ preference than the Saturday model.
• Timeliness: the editors’ behavior may vary over time since
overabundance may cause disturbing and it might also repeatedly present preference over daily fed corpus from recent experiences. More importantly, for diﬀerent types of articles, their
timeliness could be highly diﬀerent. For example, the news articles on the latest event would be easily out-of-date while a
research article on a scientiﬁc ﬁnding would be atractive for a
longer time.
In order to incorporate these two aspects, we consider a twofold
atention solution. Speciﬁcally, we deal with speciality of each
recent model i using a factor vector wM
∈Rl1+l2+...+l|S|+1 and
the article timeliness for the model trained on day t using a factor
t ∈Rl1+l2+...+l|S|+1. Based on such two factors, we build
the model of allocating atention over recently trained prediction
Te atention model (DADM) architecture is shown in Figure 3.
We formulate the DADM in a sofmax form as below. Speciﬁcally,
Document with Textual and Categorical Information
Prediction
Overall Prediction
Figure 3: Dynamic attention over multiple deep nets.
denote the model trained on day t as mt , we have the assigned
atention to the model mt as
mt · o + bM
t · o + b T
ρt = sofmax(αλM
mt + (1 −α)λT
exp(α · λM
mt + (1 −α) · λT
τ ∈[0,K] exp(α · λM
mτ + (1 −α) · λTτ )
In the above equations, the model speciality is formulated as
mt , where bM
mt is the overall eﬀectiveness term and the inner
product term wM
mt · o further captures the model’s capability on
predicting the speciﬁc article representation o. Similarly, the article
timeliness is formulated as λT
t , where b T
t is the overall timeliness
of the paterns in day t to the current prediction day and the inner
product term wT
t · o further models the relative timeliness of the
speciﬁc article representation o.
Overall, {wM
t }t=t0−K+1,...,t0 is the set of parameters to train for our atention model. α is the hyperparameter to
control the impact of the two factors, and K is the atention day
parameter representing maximal distance to the current date.
Finally, we obtain the atention-based probability estimation as
ˆy = Pr(yi |dt0
i ; Dt0) =
ρτ · ˆyτ .
Te additional advantage of our atention model lies in saving
training eﬀorts since DADM leverages the learned knowledge from
previous models while traditional method needs to train on the full
past history to atain competitive performance .
EXPERIMENTS
In this section, we present our experiments and the corresponding
results, including data analysis about the dynamics of the article
distribution and the editors’ selection behavior. We also make some
discussions about the hyperparameter tuning in the ablation study.
Experimental Setup
We have conducted experiments based on ULU Technologies article
ﬁltering API platform. ULU Technologies is a startup team based
Figure 4: Dynamic characteristics of the dataset. Above: Te
change of the number of total submitted articles over time.
Below: Te change of the editors’ selection ratio over time.
in Beijing, working on artiﬁcial intelligence based Platform-as-a-
Service (PaaS) for media partners, including the API services of
article recommendation in web pages and mobile news feeds, text
mining, sentiment analysis, article ﬁltering and native advertising.
By Jun. 2017, ULU platform serves more than 30 medias with
37 million daily page views and 20 million daily active users. Te
platform API links to the article data and editors’ selection interfaces
of an anonymized large Chinese ﬁnance article feeds platform. Te
model was deployed in a 3-node cluster with Tensorﬂow (TF) 
based on CUDA 7.5 using single GPU, GeForce GTX 1080 with 8GB
Te dataset is a large collection of quality article selection demonstration with average length of 900 characters over six months,
manually created by professional editors. As is shown in Figure
4, both of the total number of given articles and selection ratio on
each day vary over time, which indicates that the data volume and
editors’ selection board line vary signiﬁcantly. Furthermore, as
is shown in Figure 5, we also keep track of the total volume and
the selection ratio of the top three organizations consuming the
largest proportion of the dataset, which underlines the drastic drif
in the distribution of the given articles and the variability of editors’
selection criteria every day.
Each data instance of the dataset can be represented as a tuple
(d,y), where y is the binary feedback of editors’ selection and d
is the combination of the article with textual content x and categorical meta-information i including author, source organization
and original website. For exploiting meta-information eﬀectively,
we discard the authors with low frequency(less than 3 times). For
preprocessing the textual content in the tensor form using GPU
acceleration, we pad (clip) the shorter(longer) articles to the same
length of 100.
Figure 5: Dynamic characteristics of 3 main source organizations. Each of the 3 areas in the ﬁgures represents the proportion of their total volume (above) and selected articles
Compared Settings
To investigate the selection performance of quality candidate articles, we compare our proposed DADM with three models in our
experiments.
LR-meta is the logistic regression model, which is based on the
categorical meta-information formulated with one-hot encoding.
CNN-text is the powerful convolutional neural network for text
representation and classiﬁcation, which focuses on the
textual information.
W&D is the widely-used wide&deep model (discussed in Sections 3.2 and 3.3) which leverages both of the two aspects
of information, where the deep neural network learns the
text representation and a logistic regression component
takes the learned text representation and categorical metainformation as inputs and performs the ﬁnal prediction
task. Note that, our W&D model takes one more step than
traditional concept since our model consumes diﬀerent structure of data into diﬀerent components rather than
feeding the same data source into both the wide part and
the deep part.
DADM is our proposed dynamic atention deep model as discussed
in Section 3.4.
To evaluate the candidate pool recommendation quality of the
models, we use the widely-used measurements of Precision, Recall,
F1 score and Area under the Curve of ROC (AUC) as the evaluation
metrics. For the threshold selection of Precision and Recall measurements, we choose the one to maximize the F1 score, which can
be interpreted as a weighted average of the precision and recall,
since it is more reasonable to take both into consideration. Tus
Table 1: Te data statistics over 9 tested days.
Table 2: Qality articles recommendation performance comparison.
0.777±0.052
0.186±0.079
0.170±0.077
0.253±0.143
0.807±0.055
0.255±0.107
0.221±0.118
0.376±0.148
0.833±0.049
0.284±0.091
0.220±0.094
0.484±0.187
0.853±0.036
0.317±0.079
0.258±0.059
0.451±0.202
we mainly compare the models’ performance on AUC and F1 while
the precision and recall serve as the auxiliary evaluation metrics.
Results and Discussions
First, Table 1 shows the data statistics over the 9 tested days, i.e.,
Oct. 1-9, 2016. Since Oct. 1-7 is Chinese national holiday, the
data volume and distribution in such a period is diﬀerent with the
later two days, which makes the data even more dynamic than
other period. In addition, the number of article source organization
(Orgs.) each day is more than 50% of the number of authors, which
means the authors are distributed in a variety of organizations and
thus the text writing style could be much diverse. Due to business
constraints, we could only perform a 9-day A/B testing, which
is considered as a suﬃciently long period for a full-volume A/B
testing for the compared algorithms on a commercial platform.
In Table 2, we report the overall performance of recommending
quality articles over a time period of 9 days. We can observe that the
proposed wide&deep model successfully utilizes both the textual
content and categorical meta-information and achieves beter classiﬁcation performance over both AUC and F1 metrics than LR-meta
and CNN-text, which only utilize one aspect of the information.
Such a result also indicates the eﬀectiveness of using categorical
meta-information which contains ﬁelds of authors, organizations,
etc. Furthermore, we can see the obvious impact of DADM over
the strong W&D due to the dynamic atention mechanism which
adaptively and smartly takes previous knowledge into consideration to capture the dynamics of the editors’ preference. Moreover,
as the F1 score is a weighted average of both precision and recall
and thus provides more comprehensive evaluation of the model,
which the selection threshold tries to maximize, DADM tends to
emphasize the precision more and W&D is just the opposite.
Figure 6: AUC performance of quality article recommendation over 9 days .
Figure 7: AUC performance against the number of attention
days of DADM.
It should be noted that in our problem, with the same F1 score,
precision is usually more meaningful than recall because the article
volume is always too large for the editor team to check even the
recommended subset of articles. For example there are 106 articles
coming into the large pool, among which 105 articles are qualiﬁed
to be selected into the small pool for recommending to end users.
But the news feed platform requires that every delivered article
should be checked by the editor team, which can check at most 104
articles per day. In such a case, the recommended 104 scale articles
should be with a high precision, no mater the recall could be as
low as 10%.
Figure 6 presents the AUC performance of recommending the
quality candidate articles for each of the 9 tested days. As can be
observed clearly, the DADM consistently outperforms all compared
methods over 9 days, which indicates the feasibility of using atention mechanism to capture the dynamics of editor’s selection behavior and leveraging recent knowledge to improve current model.
Furthermore, as the proposed hybrid atention model adaptively
allocates atention to previously well-trained individual models,
DADM is capable of making more accurate decisions with lower
variance. Tis is because the atention mechanism can be viewed
as a smart and adaptive ensemble of the several well-trained models for each speciﬁc data entry. For example, when the current
model is incapable of performing accurate classiﬁcation for some
speciﬁc kind of data entry, atention mechanism will strategically
combine the potentially useful recent knowledge from previously
well-trained model to perform beter prediction, which increases
the robustness and stability signiﬁcantly.
Figure 8: AUC performance against α in Eq. (6).
Furthermore, we have an ablation study about the hyperparameters of DADM. Figure 7 shows the AUC performance of DADM
against diﬀerence numbers of atention days. We can observe that
the empirical optimal atention day number is 7, which is intuitive since the weekly paterns are obvious in our scenario, as
shown in Figures 4 and 5. Te prediction on this Tuesday’s articles
should make use of the editors’ demonstration data on last Tuesday.
Further previous model may not be that helpful because of the
timeliness of the learned paterns.
Figure 8 shows the AUC performance against diﬀerent values
of α in Eq. (6). As can be observed, the empirically optimal α is
around 0.8, which means model specialty plays a more important
role in the atention allocation than the timeliness. Neither of the
extreme cases of α = 0 or α = 1 is the best, which means both types
of atention factors should be consider in such a hybrid atention
CONCLUSION
In this paper we have proposed a dynamic atention deep model
to deal with the problems of non-explicit selection criteria and
non-stationary data in the editors’ article selection stage of content recommendation pipeline. For each single model, we leverage
the CNNs and wide model to automatically learn the editors’ underlying selection criteria; for atention assignment over multiple
models trained in previous days, we incorporate both the model specialty factor and model timeline factor into the atention network
to strategically assign atentions given each speciﬁc article. Te
experiments were conducted over a commercial API platform linking to a Chinese ﬁnance article feeds platform. A 9-day online A/B
testing has shown that our proposed dynamic atention deep model
performs the best in terms of both prediction AUC and F1 score as
well as the low variance in handling the dynamic data and editors’
behavior. For the future work, we will consider the inﬂuence of the
article images on the editors’ selection behavior, which could be an
eﬀective feature . On the modeling aspect, we plan to further
investigate the learning scheme of the whole hierarchical network
since we found the learning behaviors of the CNN (deep net) and
the logistic regression (shallow net) are diﬀerent . Also we will
study how our recommendations inﬂuence the editors’ further actions since their observed data is ‘biased’ due to our provided article
ranking. It is likely that the exploitation-exploration techniques
would be leveraged to handle such a problem .
ACKNOWLEDGEMENT
Te work done by Shanghai Jiao Tong University is ﬁnancially
supported by Shanghai Sailing Program (17YF1428200) and National
Natural Science Foundation of China (61632017).