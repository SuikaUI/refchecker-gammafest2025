Domain Generalization by Solving Jigsaw Puzzles
Fabio M. Carlucci1∗
Antonio D’Innocente2,3
Silvia Bucci3
Barbara Caputo3,4
Tatiana Tommasi4
1Huawei, London
2University of Rome Sapienza, Italy
3Italian Institute of Technology
4Politecnico di Torino, Italy
 
{antonio.dinnocente, silvia.bucci}@iit.it
{barbara.caputo, tatiana.tommasi}@polito.it
Human adaptability relies crucially on the ability to
learn and merge knowledge both from supervised and unsupervised learning: the parents point out few important
concepts, but then the children ﬁll in the gaps on their own.
This is particularly effective, because supervised learning
can never be exhaustive and thus learning autonomously
allows to discover invariances and regularities that help
to generalize. In this paper we propose to apply a similar
approach to the task of object recognition across domains:
our model learns the semantic labels in a supervised fashion, and broadens its understanding of the data by learning
from self-supervised signals how to solve a jigsaw puzzle on
the same images. This secondary task helps the network to
learn the concepts of spatial correlation while acting as a
regularizer for the classiﬁcation task. Multiple experiments
on the PACS, VLCS, Ofﬁce-Home and digits datasets con-
ﬁrm our intuition and show that this simple method outperforms previous domain generalization and adaptation solutions. An ablation study further illustrates the inner workings of our approach.
1. Introduction
In the current gold rush towards artiﬁcial intelligent systems it is becoming more and more evident that there is
little intelligence without the ability to transfer knowledge
and generalize across tasks, domains and categories .
A large portion of computer vision research is dedicated
to supervised methods that show remarkable results with
convolutional neural networks in well deﬁned settings, but
still struggle when attempting these types of generalizations. Focusing on the ability to generalize across domains,
∗This work was done while at University of Rome Sapienza, Italy
this object?
Figure 1. Recognizing objects across visual domains is a challenging task that requires high generalization abilities. Other tasks,
based on intrinsic self-supervisory image signals, allow to capture
natural invariances and regularities that can help to bridge across
large style gaps. With JiGen we learn jointly to classify objects and
solve jigsaw puzzles, showing that this supports generalization to
new domains.
the community has attacked this issue so far mainly by supervised learning processes that search for semantic spaces
able to capture basic data knowledge regardless of the speciﬁc appearance of input images. Existing methods range
from decoupling image style from the shared object content , to pulling data of different domains together and
imposing adversarial conditions , up to generating
new samples to better cover the space spanned by any future
target . With the analogous aim of getting general
purpose feature embeddings, an alternative research direction has been recently pursued in the area of unsupervised
learning. The main techniques are based on the deﬁnition of
tasks useful to learn visual invariances and regularities captured by spatial co-location of patches , counting
primitives , image coloring , video frame ordering
 and other self-supervised signals.
Since unlabeled data are largely available and by their
very nature are less prone to bias (no labeling bias issue
 
 ), they seem the perfect candidate to provide visual information independent from speciﬁc domain styles. Despite
their large potential, the existing unsupervised approaches
often come with tailored architectures that need dedicated
ﬁnetuning strategies to re-engineer the acquired knowledge
and make it usable as input for a standard supervised training process . Moreover, this knowledge is generally applied on real-world photos and has not been challenged before across large domain gaps with images of other nature
like paintings or sketches.
This clear separation between learning intrinsic regularities from images and robust classiﬁcation across domains is
in contrast with the visual learning strategies of biological
systems, and in particular of the human visual system. Indeed, numerous studies highlight that infants and toddlers
learn both to categorize objects and about regularities at the
same time . For instance, popular toys for infants teach
to recognize different categories by ﬁtting them into shape
sorters; jigsaw puzzles of animals or vehicles to encourage learning of object parts’ spatial relations are equally
widespread among 12-18 months old. This type of joint
learning is certainly a key ingredient in the ability of humans to reach sophisticated visual generalization abilities
at an early age .
Inspired by this, we propose the ﬁrst end-to-end architecture that learns simultaneously how to generalize across
domains and about spatial co-location of image parts (Figure 1, 2). In this work we focus on the unsupervised task
of recovering an original image from its shufﬂed parts, also
known as solving jigsaw puzzles. We show how this popular
game can be re-purposed as a side objective to be optimized
jointly with object classiﬁcation over different source domains and improve generalization with a simple multi-task
process . We name our Jigsaw puzzle based Generalization method JiGen. Differently from previous approaches
that deal with separate image patches and recombine their
features towards the end of the learning process ,
we move the patch re-assembly at the image level and we
formalize the jigsaw task as a classiﬁcation problem over
recomposed images with the same dimension of the original one. In this way object recognition and patch reordering
can share the same network backbone and we can seamlessly leverage over any convolutional learning structure as
well as several pretrained models without the need of speciﬁc architectural changes.
We demonstrate that JiGen allows to better capture the
shared knowledge among multiple sources and acts as a regularization tool for a single source. In the case unlabeled
samples of the target data are available at training time, running the unsupervised jigsaw task on them contributes to
the feature adaptation process and shows competing results
with respect to state of the art unsupervised domain adaptation methods.
2. Related Work
Solving Jigsaw Puzzles
The task of recovering an original image from its shufﬂed parts is a basic pattern recognition problem that is commonly identiﬁed with the jigsaw
puzzle game. In the area of computer science and artiﬁcial intelligence it was ﬁrst introduced by , which proposed a 9-piece puzzle solver based only on shape information and ignoring the image content. Later, started to
make use of both shape and appearance information. The
problem has been mainly cast as predicting the permutations of a set of squared patches with all the challenges related to number and dimension of the patches, their completeness (if all tiles are/aren’t available) and homogeneity
(presence/absence of extra tiles from other images). The
application ﬁeld for algorithms solving jigsaw puzzles is
wide, from computer graphics and image editing 
to re-compose relics in archaeology , from modeling in biology to unsupervised learning of visual representations . Existing assembly strategies can be
broadly classiﬁed into two main categories: greedy methods
and global methods. The ﬁrst ones are based on sequential
pairwise matches, while the second ones search for solutions that directly minimize a global compatibility measure
over all the patches. Among the greedy methods, proposed a minimum spanning tree algorithm which progressively merges components while respecting the geometric
consistent constraint. To eliminate matching outliers, 
introduced loop constraints among the patches. The problem can be also formulated as a classiﬁcation task to predict
the relative position of a patch with respect to another as
in . Recently, expressed the patch reordering as
the shortest path problem on a graph whose structure depends on the puzzle completeness and homogeneity. The
global methods consider all the patches together and use
Markov Random Field formulations , or exploit genetic
algorithms . A condition on the consensus agreement
among neighbors is used in , while focuses on a
subset of possible permutations involving all the image tiles
and solves a classiﬁcation problem. The whole set of permutations is instead considered in by approximating
the permutation matrix and solving a bi-level optimization
problem to recover the right ordering.
Regardless of the speciﬁc approach and application, all
the most recent deep-learning jigsaw-puzzle solvers tackle
the problem by dealing with the separate tiles and then ﬁnding a way to recombine them. This implies designing tilededicated network architectures then followed by some speciﬁc process to transfer the collected knowledge in more
standard settings that manage whole image samples.
Domain Generalization and Adaptation
The goal of
domain generalization (DG) is that of learning a system that
can perform uniformly well across multiple data distribu-
Object Classiﬁer
(object label)
Jigsaw Classiﬁer
(permutation index)
index p = P
permutation:
1,9,5,6,3,2,8,4,7
index p = 2
permutation:
9,2,3,4,5,6,7,8,1
index: p = 1
permutation:
1,2,3,4,5,6,7,8,9
Figure 2. Illustration of the proposed method JiGen. We start from images of multiple domains and use a 3 × 3 grid to decompose them
in 9 patches which are then randomly shufﬂed and used to form images of the same dimension of the original ones. By using the maximal
Hamming distance algorithm in we deﬁne a set of P patch permutations and assign an index to each of them. Both the original ordered
and the shufﬂed images are fed to a convolutional network that is optimized to satisfy two objectives: object classiﬁcation on the ordered
images and jigsaw classiﬁcation, meaning permutation index recognition, on the shufﬂed images.
tions. The main challenge is being able to distill the most
useful and transferrable general knowledge from samples
belonging to a limited number of population sources. Several works have reduced the problem to the domain adaptation (DA) setting where a fully labeled source dataset and
an unlabeled set of examples from a different target domain
are available . In this case the provided target data is
used to guide the source training procedure, that however
has to run again when changing the application target. To
get closer to real world conditions, recent work has started
to focus on cases where the source data are drawn from
multiple distributions and the target covers only a
part of the source classes . For the more challenging
DG setting with no target data available at training time, a
large part of the previous literature presented model-based
strategies to neglect domain speciﬁc signatures from multiple sources. They are both shallow and deep learning methods that build over multi-task learning , low-rank network parameter decomposition or domain speciﬁc aggregation layers . Alternative solutions are based on
source model weighting , or on minimizing a validation
measure on virtual tests deﬁned from the available sources
 . Other feature-level approaches search for a data representation able to capture information shared among multiple domains. This was formalized with the use of deep
learning autoencoders in , while proposed to
learn an embedding space where images of same classes but
different sources are projected nearby. The recent work of
 adversarially exploits class-speciﬁc domain classiﬁcation modules to cover the cases where the covariate shift
assumption does not hold and the sources have different
class conditional distributions. Data-level methods propose
to augment the source domain cardinality with the aim of
covering a larger part of the data space and possibly get
closer to the target. This solution was at ﬁrst presented with
the name of domain randomization for samples from
simulated environments whose variety was extended with
random renderings. In the augmentation is obtained
with domain-guided perturbations of the original source instances. Even when dealing with a single source domain,
 showed that it is still possible to add adversarially
perturbed samples by deﬁning ﬁctitious target distributions
within a certain Wasserstein distance from the source.
Our work stands in this DG framework, but proposes an
orthogonal solution with respect to previous literature by investigating the importance of jointly exploiting supervised
and unsupervised inherent signals from the images.
3. The JiGen Approach
Starting from the samples of multiple source domains,
we wish to learn a model that can perform well on any
new target data population covering the same set of categories. Let us assume to observe S domains, with the ith domain containing Ni labeled instances {(xi
j indicates the j-th image and yi
j ∈{1, . . . , C}
is its class label. The ﬁrst basic objective of JiGen is to
minimize the loss Lc(h(x|θf, θc), y) that measures the error between the true label y and the label predicted by the
deep model function h, parametrized by θf and θc. These
parameters deﬁne the feature embedding space and the ﬁnal classiﬁer, respectively for the convolutional and fully
connected parts of the network. Together with this objective, we ask the network to satisfy a second condition related to solving jigsaw puzzles. We start by decomposing
the source images using a regular n × n grid of patches,
which are then shufﬂed and re-assigned to one of the n2
grid positions. Out of the n2! possible permutations we select a set of P elements by following the Hamming distance
based algorithm in , and we assign an index to each entry. In this way we deﬁne a second classiﬁcation task on
Ki labeled instances {(zi
k=1, where zi
k indicates the
recomposed samples and pi
k ∈{1, . . . , P} the related permutation index, for which we need to minimize the jigsaw
loss Lp(h(z|θf, θp), p). Here the deep model function h has
the same structure used for object classiﬁcation and shares
with that the parameters θf. The ﬁnal fully connected layer
dedicated to permutation recognition is parametrized by θp.
Overall we train the network to obtain the optimal model
j|θf, θc), yi
k|θf, θp), pi
where both Lc and Lp are standard cross-entropy losses. We
underline that the jigsaw loss is also calculated on the ordered images. Indeed, the correct patch sorting corresponds
to one of the possible permutations and we always include
it in the considered subset P. On the other way round, the
classiﬁcation loss is not inﬂuenced by the shufﬂed images,
since this would make object recognition tougher. At test
time we use only the object classiﬁer to predict on the new
target images.
Unsupervised
Adaptation
Thanks to the unsupervised nature of the jigsaw puzzle task, we can always extend JiGen to the unlabeled
samples of target domain when available at training
This allows us to exploit the jigsaw task for unsupervised domain adaptation.
In this setting, for the
target ordered images we minimize the classiﬁer prediction uncertainty through the empirical entropy loss
LE(xt) = P
y∈Y h(xt|θf, θc)log{h(xt|θf, θc)}, while for
the shufﬂed target images we keep optimizing the jigsaw
loss Lp(h(zt|θf, θp), pt).
Implementation Details
Overall JiGen1 has two parameters related to how we deﬁne the jigsaw task, and three related to the learning process. The ﬁrst two are respectively
the grid size n × n used to deﬁne the image patches and the
cardinality of the patch permutation subset P. As we will
detail in the following section, JiGen is robust to these values and for all our experiments we kept them ﬁxed, using
3 × 3 patch grids and P = 30. The remaining parameters
are the weights α of the jigsaw loss, and η assigned to the
entropy loss when included in the optimization process for
unsupervised domain adaptation. The ﬁnal third parameter
regulates the data input process: the shufﬂed images enter
the network together with the original ordered ones, hence
each image batch contains both of them. We deﬁne a data
bias parameter β to specify their relative ratio. For instance
1Code available at 
β = 0.6 means that for each batch, 60% of the images are
ordered, while the remaining 40% are shufﬂed. These last
three parameters were chosen by cross validation on a 10%
subset of the source images for each experimental setting.
We designed the JiGen network making it able to leverage over many possible convolutional deep architectures.
Indeed it is sufﬁcient to remove the existing last fully connected layer of a network and substitute it with the new object and jigsaw classiﬁcation layers. JiGen is trained with
SGD solver, 30 epochs, batch size 128, learning rate set to
0.001 and stepped down to 0.0001 after 80% of the training
epochs. We used a simple data augmentation protocol by
randomly cropping the images to retain between 80−100%
and randomly applied horizontal ﬂipping. Following 
we randomly (10% probability) convert an image tile to
grayscale.
4. Experiments
To evaluate the performance of JiGen when
training over multiple sources we considered three domain
generalization datasets. PACS covers 7 object categories and 4 domains (Photo, Art Paintings, Cartoon and
Sketches). We followed the experimental protocol in 
and trained our model considering three domains as source
datasets and the remaining one as target. VLCS aggregates images of 5 object categories shared by the PASCAL
VOC 2007, LabelMe, Caltech and Sun datasets which are
considered as 4 separated domains. We followed the standard protocol of dividing each domain into a training
set (70%) and a test set (30%) by random selection from
the overall dataset. The Ofﬁce-Home dataset contains
65 categories of daily objects from 4 domains: Art, Clipart,
Product and Real-World. In particular Product images are
from vendor websites and show a white background, while
Real-World represents object images collected with a regular camera. For this dataset we used the same experimental
protocol of . Note that Ofﬁce-Home and PACS are related in terms of domain types and it is useful to consider
both as test-beds to check if JiGen scales when the number
of categories changes from 7 to 65. Instead VLCS offers
different challenges because it combines object categories
from Caltech with scene images of the other domains.
To understand if solving jigsaw puzzles supports generalization even when dealing with a single source, we extended our analysis to digit classiﬁcation as in . We
trained a model on 10k digit samples of the MNIST dataset
 and evaluated on the respective test sets of MNIST-
M and SVHN . To work with comparable datasets,
all the images were resized to 32 × 32 treated as RGB.
Patch-Based Convolutional Models for Jigsaw Puzzles
We start our experimental analysis by evaluating the application of existing jigsaw related patch-based convolu-
art paint. cartoon sketches photo
CFN - Alexnet
J-CFN-Finetune
J-CFN-Finetune++
C-CFN-Deep All
C-CFN-JiGen
 Deep All
 Deep All
 Deep All
 Deep All
Table 1. Domain Generalization results on PACS. The results of
JiGen are average over three repetitions of each run. Each column
title indicates the name of the domain used as target. We use the
bold font to highlight the best results of the generalization methods, while we underline a result when it is higher than all the others
despite produced by the na¨ıve Deep All baseline. Top: comparison
with previous methods that use the jigsaw task as a pretext to learn
transferable features using a context-free siamese-ennead network
(CFN). Center and Bottom: comparison of JiGen with several domain generalization methods when using respectively Alexnet and
Resnet-18 architectures.
Figure 3. Confusion matrices on Alexnet-PACS DG setting, when
sketches is used as target domain.
tional architectures and models to the domain generalization task. We considered two recent works that proposed
a jigsaw puzzle solver for 9 shufﬂed patches from images
decomposed by a regular 3 × 3 grid. Both and 
use a Context-Free Network (CFN) with 9 siamese branches
that extract features separately from each image patch and
then recompose them before entering the ﬁnal classiﬁcation layer.
Speciﬁcally, each CFN branch is an Alexnet
 up to the ﬁrst fully connected layer (fc6) and all the
branches share their weights. Finally, the branches’ outputs
are concatenated and given as input to the following fully
connected layer (fc7). The jigsaw puzzle task is formalized as a classiﬁcation problem on a subset of patch permutations and, once the network is trained on a shufﬂed
version of Imagenet , the learned weights can be used
to initialize the conv layers of a standard Alexnet while the
rest of the network is trained from scratch for a new target
task. Indeed, according to the original works, the learned
representation is able to capture semantically relevant content from the images regardless of the object labels. We
followed the instructions in and started from the pretrained Jigsaw CFN (J-CFN) model provided by the authors
to run ﬁnetuning for classiﬁcation on the PACS dataset with
all the source domain samples aggregated together. In the
top part of Table 1 we indicate with J-CFN-Finetune the
results of this experiment using the jigsaw model proposed
in , while with J-CFN-Finetune++ the results from the
advanced model proposed in . In both cases the average
classiﬁcation accuracy on the domains is lower than what
can be obtained with a standard Alexnet model pre-trained
for object classiﬁcation on Imagenet and ﬁnetuned on all the
source data aggregated together. We indicate this baseline
approach with Deep All and we can use as reference the corresponding values in the following central part of Table 1.
We can conclude that, despite its power as an unsupervised
pretext task, completely disregarding the object labels when
solving jigsaw puzzles induces a loss of semantic information that may be crucial for generalization across domains.
To demonstrate the potentialities of the CFN architecture, the authors of used it also to train a supervised object Classiﬁcation model on Imagenet (C-CFN) and demonstrated that it can produce results analogous to the standard
Alexnet. With the aim of further testing this network to
understand if and how much its peculiar siamese-ennead
structure can be useful to distill shared knowledge across
domains, we considered it as the main convolutional backbone for JiGen. Starting from the C-CFN model provided
by the authors, we ran the obtained C-CFN-JiGen on PACS
data, as well as its plain object classiﬁcation version with
the jigsaw loss disabled (α = 0) that we indicate as C-CFN-
Deep All. From the obtained recognition accuracy we can
state that combining the jigsaw puzzle with the classiﬁcation task provides an average improvement in performance,
which is the ﬁrst result to conﬁrm our intuition. However,
C-CFN-Deep All is still lower than the reference results obtained with standard Alexnet.
For all the following experiments we consider the convolutional architecture of JiGen built with the same main
structure of Alexnet or Resnet, using always the image as
a whole (ordered or shufﬂed) instead of relying on separate
patch-based network branches. A detailed comparison of
per-class results on the challenging sketches domain for J-
CFN-Finetune++ and JiGen based on Alexnet reveals that
for four out of seven categories, J-CFN-Finetune++ is actually doing a good job, better than Deep All. With JiGen we
improve over Deep All for the same categories by solving
jigsaw puzzles at image level and we keep the advantage of
Deep All for the remaining categories.
Multi-Source Domain Generalization
We compare the
performance of JiGen against several recent domain generalization methods. TF is the low-rank parametrized network that was presented together with the dataset PACS
in . CIDDG is the conditional invariant deep domain
generalization method presented in that trains for image classiﬁcation with two adversarial constraints: one that
maximizes the overall domain confusion following and
a second one that does the same per-class. In the DeepC
variant, only this second condition is enabled.
 is a meta-learning approach that simulates train/test domain shift during training and exploit them to optimize the
learning model. CCSA learns an embedding subspace
where mapped visual domains are semantically aligned
and yet maximally separated. MMD-AAE is a deep
method based on adversarial autoencoders that learns an invariant feature representation by aligning the data distributions to an arbitrary prior through the Maximum Mean Discrepancy (MMD). SLRC is based on a single domain
invariant network and multiple domain speciﬁc ones and it
applies a low rank constraint among them. D-SAM is
a method based on the use of domain-speciﬁc aggregation
modules combined to improve model generalization: it provides the current sota results on PACS and Ofﬁce-Home.
For each of these methods, the Deep All baseline indicates
the performance of the corresponding network when all the
introduced domain adaptive conditions are disabled.
The central and bottom parts of Table 1 show the results of JiGen on the dataset PACS when using as backbone architecture Alexnet and Resnet-182. On average Ji-
Gen produces the best result when using Alexnet and it is
just slightly worse than the D-SAM reference for Resnet-18.
Note however, that in this last case, JiGen outperforms D-
SAM in three out of four target cases and the average advantage of D-SAM originate only from its result on sketches.
On average, JiGen outperforms also the competing methods
on the VLCS and on the Ofﬁce-Home datasets (see respectively Table 2 and 3). In particular we remark that VLCS
is a tough setting where the most recent works have only
2With Resnet18, to put JiGen on equal footing with D-SAM we follow
the same data augmentation protocol in and enabled color jittering.
Caltech Labelme Pascal
 MMD-AAE
Table 2. Domain Generalization results on VLCS. For details about number of runs, meaning of columns and use of
bold/underline fonts, see Table 1.
Ofﬁce-Home
Clipart Product Real-World
 Deep All 55.59
Deep All 52.15
Table 3. Domain Generalization results on Ofﬁce-Home.
details about number of runs, meaning of columns and use of
bold/underline fonts, see Table 1.
presented small gain in accuracy with respect to the corresponding Deep All baseline (e.g. TF). Since did not
present the results of D-SAM on the VLCS dataset, we used
the code provided by the authors to run these experiments.
The obtained results show that, although generally able to
close large domain gaps across images of different styles
as in PACS and Ofﬁce-Home, when dealing with domains
all coming from real-world images, the use of aggregative
modules does not support generalization.
We focus on the Alexnet-PACS DG setting for
an ablation analysis on the respective roles of the jigsaw
and of the object classiﬁcation task in the learning model.
For these experiments we kept the jigsaw hyperparameters
ﬁxed with a 3 × 3 patch grid and P = 30 jigsaw classes.
{α = 0, β = 1} means that the jigsaw task is off, and the
data batches contain only original ordered images, which
corresponds to Deep All. The value assigned to the data
bias β drives the overall training: it moves the focus from
jigsaw when using low values (β < 0.5) to object classiﬁcation when using high values (β ≥0.5). By setting
the data bias to β = 0.6 we feed the network with more
ordered than shufﬂed images, thus keeping the classiﬁcation as the primary goal of the network. In this case, when
changing the jigsaw loss weight α in {0.1, 1}, we observe
results which are always either statistically equal or better
than the Deep All baseline as shown in the ﬁrst plot of Fig-
jigsaw loss weight
=0.6, P=30
Figure 4. Ablation results on the Alexnet-PACS DG setting. The reported accuracy is the global average over all the target domains with
three repetitions for each run. The red line represents our Deep All average from Table 1.
=0.9, P=30
Object Classification
Jigsaw Classification
jigsaw classes P
Jigsaw Classification
Figure 5. Analysis of the behaviour of the jigsaw classiﬁer on the
Alexnet-PACS DG setting. For the plot on the left each axes refers
to the color matching curve in the graph.
ure 4. The second plot indicates that, for high values of
α, tuning β has a signiﬁcant effect on the overall performance. Indeed {α ∼1, β = 1} means that jigsaw task is
on and highly relevant in the learning process, but we are
feeding the network only with ordered images: in this case
the jigsaw task is trivial and forces the network to recognize
always the same permutation class which, instead of regularizing the learning process, may increase the risk of data
memorization and overﬁtting. Further experiments conﬁrm
that, for β = 1 but lower α values, JiGen and Deep All
perform equally well. Setting β = 0 means feeding the network only with shufﬂed images. For each image we have
P variants, only one of which has the patches in the correct
order and is allowed to enter the object classiﬁer, resulting
in a drastic reduction of the real batch size. In this condition the object classiﬁer is unable to converge, regardless
of whether the jigsaw classiﬁer is active (α > 0) or not
(α = 0). In those cases the accuracy is very low (< 20%),
so we do not show it in the plots to ease the visualization.
Jigsaw hyperparameter tuning
By using the same experimental setting of the previous paragraph, the third plot
in Figure 4 shows the change in performance when the number of jigsaw classes P varies between 5 and 1000. We
started from a low number, with the same order of magnitude of the number of object classes in PACS, and we
grew till 1000 which is the number used for the experiments
in . We observe an overall variation of 1.5 percentage
points in the accuracy which still remains (almost always)
higher than the Deep All baseline. Finally, we ran a test to
check the accuracy when changing the grid size and consequently the patch number. Even in this case, the range of
variation is limited when passing from a 2 × 2 to a 4 × 4
grid, conﬁrming the conclusions of robustness already obtained for this parameter in and . Moreover all the
results are better than the Deep All reference.
It is also interesting to check whether the jigsaw classiﬁer is producing meaningful results per-se, besides supporting generalization for the object classiﬁer. We show its
recognition accuracy when testing on the same images used
to evaluate the object classiﬁer but with shufﬂed patches. In
Figure 5, the ﬁrst plot shows the accuracy over the learning epochs for the object and jigsaw classiﬁers indicating
that both grows simultaneously (on different scales). The
second plot shows the jigsaw recognition accuracy when
changing the number of permutation classes P: of course
the performance decreases when the task becomes more
difﬁcult, but overall the obtained results indicate that the
jigsaw model is always effective in reordering the shufﬂed
Single Source Domain Generalization
The generalization ability of a model depends both on the chosen learning
process and on the used training data. To investigate the former and better evaluate the regularization effect provided by
the jigsaw task, we consider the case of training data from
a single source domain. For these experiments we compare against the generalization method based on adversarial data augmentation (Adv.DA) recently presented in .
This work proposes an iterative procedure that perturbs the
samples to make them hard to recognize under the current
model and then combine them with the original ones while
solving the classiﬁcation task. We reproduced the experimental setting used in and adopt a similar result display
style with bar plots for experiments on the MNIST-M and
SVHN target datasets when training on MNIST. In Figure
6 we show the performance of JiGen when varying the data
bias β and the jigsaw weight α. With the red background
shadow we indicate the overall range covered by Adv.DA
results when changing its parameters3, while the horizontal line is the reference Adv.DA results around which the
authors of ran their parameter ablation analysis. The
ﬁgure indicates that, although Adv.DA can reach high peak
3The whole set of results is provided as supplementary material of .
jigsaw weight
jigsaw weight
jigsaw weight
jigsaw weight
Figure 6. Single Source Domain Generalization experiments. We analyze the performance of JiGen in comparison with the method
Adv.DA . The shaded background area covers the overall range of results of Adv.DA obtained when changing the hyper-parameters
of the method. The reference result of Adv.DA (γ = 1, K = 2) together with its standard deviation is indicated here by the horizontal red
line. The blue histogram bars show the performance of JiGen when changing the jigsaw weight α and data bias β.
art paint. cartoon sketches photo
DDiscovery
JiGen αs=αt=0.7
JiGen αt=0.1
JiGen αt=0.3
JiGen αt=0.5
JiGen αt=0.9
Table 4. Multi-source Domain Adaptation results on PACS obtained as average over three repetitions for each run. Besides considering the same jigsaw loss weight for source and target samples
αs = αt, we also tuned the target jigsaw loss weight while keeping αs = 0.7 showing that we can get even higher results.
values, it is also very sensitive to the chosen hyperparameters. On the other hand, JiGen is much more stable and it
is always better than the lower accuracy value of Adv.DA
with a single exception for SVHN and data bias 0.5, but we
know from the ablation analysis, that this corresponds to a
limit case for the proper combination of object and jigsaw
classiﬁcation. Moreover, JiGen gets close to Adv.DA reference results for MNIST-M and signiﬁcantly outperform it
Unsupervised Domain Adaptation
When unlabeled target samples are available at training time we can let the
jigsaw puzzle task involve these data.
Indeed patch reordering does not need image labels and running the jigsaw optimization process on both source and target data
may positively inﬂuence the source classiﬁcation model for
adaptation.
To verify this intuition we considered again
the PACS dataset and used it in the same unsupervised domain adaptation setting of . This previous work proposed a method to ﬁrst discover the existence of multiple
latent domains in the source data and then differently adapt
their knowledge to the target depending on their respective
similarity. It has been shown that this domain discovery
(DDiscovery) technique outperforms other powerful adaptive approaches as Dial when the source actually includes multiple domains. Both these methods exploit the
minimization of the entropy loss as an extra domain alignment condition: in this way the source model when predicting on the target samples is encouraged to assign maximum
prediction probability to a single label rather than distributing it over multiple class options. For a fair comparison
we also turned on the entropy loss for JiGen with weight
η = 0.1. Moreover, we considered two cases for the jigsaw loss: either keeping the weight α already used for the
PACS-Resnet-18 DG experiments for both the source and
target data (α = αs = αt = 0.7), or treating the domain
separately with a dedicated weight for the jigsaw target loss
(αs = 0.7, αt = [0.1, 0.3, 0.5, 0.9]). The results for this
setting are summarized in Table 4. The obtained accuracy
indicates that JiGen outperforms the competing methods on
average and in particular on the difﬁcult task of recognizing
sketches. Furthermore, the advantage remains true regardless of the speciﬁc choice of the target jigsaw loss weight.
5. Conclusions
In this paper we showed for the ﬁrst time that generalization across visual domains can be achieved effectively
by learning to classify and learning intrinsic image invariances at the same time. We focused on learning spatial colocation of image parts, and proposed a simple yet powerful
framework that can accommodate a wide spectrum of pretrained convolutional architectures. Our method JiGen can
be seamlessly and effectively used for domain adaptation
and generalization as shown by the experimental results.
We see this paper as opening the door to a new research
thread in domain adaptation and generalization. While here
we focused on a speciﬁc type of invariance, several other
regularities could be learned possibly leading to an even
stronger beneﬁt. Also, the simplicity of our approach calls
for testing its effectiveness in applications different from
object categorization, like semantic segmentation and person re-identiﬁcation, where the domain shift effect strongly
impact the deployment of methods in the wild.
Acknowledgments
This work was supported by the ERC
grant 637076 RoboExNovo and a NVIDIA Academic Hardware Grant.
Deep All 
Deep All 
Deep All 
Deep All 
Figure 7. CAM activation maps: yellow corresponds to high values, while dark blue corresponds to low values. JiGen is able to
localize the most informative part of the image, useful for object
class prediction regardless of the visual domain.
A. Appendix
We provide here some further analysis and experimental results on using jigsaw puzzle and other self-supervised
tasks as auxiliary objectives to improve generalization
across visual domains.
Visual explanation and Failure cases
The relative position of each image patch with respect to the others captures visual regularities which are at the same time shared
among domains and discriminative with respect to the object classes. Thus, by solving jigsaw puzzles we encourage the network to localize and re-join relevant object subparts regardless of the visual domain. This helps to focus
on the most informative image areas. For an in-depth analysis of the learned model we adopted the Class Activation
Mapping (CAM, ) method on ResNet-18, with which
we produced the activation maps in Figure 7 for the PACS
dataset. The ﬁrst two rows show that JiGen is better at localizing the object class with respect to Deep All. The last
row indicates that the mistakes are related to some ﬂaw in
data interpretation, while the localization remains correct.
Self-supervision by predicting image rotations
Reordering image patches to solve jigsaw puzzle is not the
only self-supervised approach that can be combined with
supervised learning for domain generalization. We ran experiments by using as auxiliary self-supervised task the
rotation classiﬁer (four classes [0◦, 90◦, 180◦, 270◦]) proposed in .
We focused on the PACS dataset with
the Alexnet-based architecture, following the same protocol used for JiGen.
The obtained accuracy (Table 5) is
higher than the Deep All baseline, but still lower than what
obtained with our method.
Indeed object 2d orientation
provides useful semantic information when dealing with
real photos, but it becomes less critical for cartoons and
art paint. cartoon sketches photo
Table 5. Top: results obtained by using Rotation recognition as
auxiliary self-supervised task. Bottom: three cartoons and three
sketches that show objects with odd orientations.