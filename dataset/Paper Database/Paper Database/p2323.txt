Received July 10, 2020, accepted July 20, 2020, date of publication July 22, 2020, date of current version July 31, 2020.
Digital Object Identifier 10.1109/ACCESS.2020.3011186
STGAT: Spatial-Temporal Graph Attention
Networks for Traffic Flow Forecasting
XIANGYUAN KONG
, WEIWEI XING
, (Member, IEEE), XIANG WEI
, (Member, IEEE),
, (Member, IEEE), JIAN ZHANG, AND WEI LU
School of Software Engineering, Beijing Jiaotong University, Beijing 100044, China
Corresponding author: Wei Lu ( )
This work was supported by the National Natural Science Foundation of China under Grant 61876017, Grant 61876018, Grant 61906014,
and Grant 61702031.
ABSTRACT Trafﬁc ﬂow forecasting is a critical task for urban trafﬁc control and dispatch in the ﬁeld of
transportation, which is characterized by the high nonlinearity and complexity. In this paper, we propose
an end-to-end deep learning based dual path framework, i.e., Spatial-Temporal Graph Attention Network
(STGAT), for trafﬁc ﬂow forecasting. Speciﬁcally, different from previous structure-based approaches,
STGAT can be directly generalized to the graph with arbitrary structure. Furthermore, STGAT is capable of
handling long temporal sequence by stacking gated temporal convolution layer. The dual path architectures
is proposed for taking both potential and existing spatial dependencies into account. By capturing potential
spatial dependencies will naturally catch more useful information for forecasting. We design a gated fusion
mechanism to combine the outputs from each path. The proposed model can be directly applicable to
inductive learning tasks by introducing a graph attention mechanism into spatial-temporal framework, which
means our model can be generalized to completely unseen graphs. Moreover, experimental results on two
public real-world trafﬁc network datasets, METR-LA and PEMS-BAY, show that our STGAT outperforms
the state-of-the-art baselines. Additionally, we demonstrate the proposed model is competent for efﬁcient
migration between graphs with different structures.
INDEX TERMS Trafﬁc ﬂow forecasting, spatial-temporal graph neural networks, intelligent transportation
I. INTRODUCTION
With the rapid development of machine learning and the
emergence of new data resources, examining and predicting trafﬁc conditions in smart cities is becoming more and
more accurate . Through the above techniques, the design
and management of transport services in smart cities can
be ﬂexibly optimized. Trafﬁc ﬂow forecasting is a typical
task in the transportation domain, the job of which if to
predict the future trafﬁc ﬂow speeds of a sensor network
with given historical speeds data and underlying road networks. In practice, according to the length of prediction time,
trafﬁc ﬂow forecasting is divided into short-term (5-30 minutes), medium-term (30-60 minutes) and long-term (over
an hour) . Most prevalent approaches can perform well
on short-term forecast. However, due to the uncertainty and
The associate editor coordinating the review of this manuscript and
approving it for publication was Tony Thomas.
complexity of trafﬁc ﬂow, those methods are less effective for
relatively long-term predictions . In this paper, we study
trafﬁc ﬂow forecasting problems on road networks for the
full-time interval.
To reach more effective forecasting for trafﬁc ﬂow in
long-term and complex spatial conditions, spatial-temporal
graph modeling has received increasing attention with the
advance of graph neural networks . Trafﬁc data are
recorded by sensors in the city at ﬁxed position. Those sensors construct a graph where the edge weights are determined
by distance (Euclidean, road network distance, etc.) between
two nodes. Apparently, trafﬁc data at different time of the
same node is not only affected by its historical data but also
by neighborhood nodes. Therefore, the key to solve such
problem is to effectively extract the spatial-temporal dependence of data. Meanwhile, spatial-temporal graph modeling
is a powerful tool to deal with complex spatial-temporal
dependencies.
VOLUME 8, 2020
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see 
X. Kong et al.: STGAT: Spatial-Temporal Graph Attention Networks for Traffic Flow Forecasting
Recent studies on spatial-temporal graph modeling mainly
follow two aspects. They either introduce graph convolution networks (GCN) into recurrent neural networks
(RNN) , or use convolution neural networks (CNN)
directly , . Despite showing the effectiveness by introducing the graph structure of data into a model, these
approaches still face some shortcomings.
Speciﬁcally, most of these studies are structure-based ,
 , – , which have good performance on transductive
learning tasks. Moreover, current studies for spatial-temporal
graph modeling are effective to learn short-term or mid-term
temporal dependencies, however, long-term temporal forecasting is still a problem. For instance, RNN-based approaches for sequence learning require iterative training, which
introduce error accumulation by steps and spends extra
training time. Additionally, RNN-based approaches suffer
from gradient exploding or vanishing while capturing the
long-term temporal dependency , , . CNN-based
approaches take the advantages of parallel computing, stable gradients, and low memory requirement. However, these
methods need to use many layers to capture long-term
temporal dependency, which will inevitably loss of some
critical information. Moreover, GMAN proposes an
attention-based encoder-decoder framework, calculate spatial
attention score from all vertices, which the time and memory
consumption is heavy.
Inspired by the studies above, in this paper, we represent
an end-to-end deep learning based dual path framework,
i.e., Spatial-Temporal Graph Attention Network (STGAT),
The main contributions of this paper are as follows:
• The proposed model is designed as a dual path networks with gating mechanisms and residual architecture,
so that it can take both potential and existing spatial
dependency into account. We design a Spatial-Temporal
Block, which contains gated temporal convolution and
graph attention layer to handle spatial and temporal
dependency at the same time.
• The proposed model can be directly applicable to
inductive learning tasks by introducing a graph attention mechanism into spatial-temporal framework, which
means our model can be generalized to completely
unseen graphs. When the training data in a certain area
is lacking, we can directly use the model trained on other
related datasets to make prediction.
• The proposed model outperforms all our competitors on
two public real-world trafﬁc datasets, to the best our
knowledge. The source codes of STGAT are publicly
available from github.
II. RELATED WORK
Graph data contains rich relation information among elements. Graph neural networks (GNNs) are connectionist
models that capture the dependence on graphs via message
passing between the nodes of graphs. In recent years, GNNs
have a wide variety of applications. In this section, we ﬁrst
present the overview of trafﬁc ﬂow forecasting, then we
present practical applications of Graph Spatial-temporal Networks which combine time series model and GNN model.
A. TRAFFIC FLOW FORECASTING
The literature often refers to trafﬁc as a ﬂow, because it has
similar properties to ﬂuids . Trafﬁc ﬂow forecasting has
been long regarded as a key functional component in ITSs.
As early as 1970s, the autoregressive integrated moving average (ARIMA) model was used to predict short-term freeway
trafﬁc ﬂow . Most early trafﬁc (i.e., – ) forecasting approaches use shallow trafﬁc models and are still somewhat unsatisfying. Compared with traditional approachs,
recent advances in deep learning show more superior performance, such as – .
B. GRAPH NEURAL NETWORKS
There have been several attempts in the literature to extend
neural networks to deal with arbitrarily structured graphs.
Graph Neural Networks is ﬁrst introduced in and ,
which can directly deal with a more general class of graphs,
e.g., cyclic, directed and undirected graphs .
Nevertheless, there is an increasing interest in generalizing convolutions to the graph domain. Some researchers
categorize graph neural networks as graph convolution networks, graph attention networks and graph autoencoders.
Graph Convolution Networks play a core role in capturing structural dependencies. Spectral CNN proposes the
ﬁrst spectral convolution neural network. Chebyshev Spectral
CNN is proposed to approximate the ﬁlters by means
of a Chebyshev expansion of the graph Laplacian, removing
the need to compute the eigenvectors of the Laplacian and
yielding spatially localized ﬁlters. ChebNet simpliﬁes
the previous method by restricting the ﬁlters to operate in
a 1-step neighborhood around each node. Graph Attention
Networks (GAT) also plays an important role in capturing structural dependencies. The idea of GAT is to compute the hidden representations of each node in the graph
by attending over its neighbors following a self-attention
strategy. Graph Autoencoder is an unsupervised learning
framework on graph-structured data, which is a kind of
popular approaches to learning the graph embedding for
plain graphs without attributed information , . These
approaches focus on spatial dependencies and ignore information on temporal dependencies, thus they are unstable for
spatial-temporal graph modeling.
C. SPATIAL-TEMPORAL GRAPH NETWORKS
Graph Spatial-Temporal Networks aims at learning unseen
patterns from spatial-temporal graphs, which is increasingly
important in many applications such as trafﬁc forecasting and
human activity prediction. Graph Spatial-Temporal Networks
combines time series model and GNN model. The key idea of
graph spatial-temporal networks is to consider spatial dependency and temporal dependency at the same time.
VOLUME 8, 2020
X. Kong et al.: STGAT: Spatial-Temporal Graph Attention Networks for Traffic Flow Forecasting
Recently, Graph Spatial-temporal Networks is being
applied to various ﬁelds by researchers. Surgical Tool Graph
Convolutional Networks utilizes both spatial and temporal information from surgical videos for surgical tool
presence detection. Spatial-Temporal Graph Convolutional
Networks takes advantage of the dynamics of human
body skeletons convey signiﬁcant information for human
action recognition. In addition, most researchers are interested in trafﬁc forecasting. By developing effective graph
spatial-temporal network models, researchers can accurately
predict the trafﬁc status of the whole trafﬁc system. Many
current methods in trafﬁc forecasting apply GCNs to capture
the dependency together with some RNN , , , or
CNN , , to model the temporal dependency.
OGCRNN (optimized graph convolution recurrent neural
network) is a RNN based approach for trafﬁc prediction.
Additionally, OGCRNN learns an optimized graph in the
training phase and reveals the latent relationship among the
road segments from the trafﬁc data.
ST-MGCN is a RNN based approach, which represents their data in the form of graphs from three perspectives
(spatial geographic relationship, regional functional similarity and regional trafﬁc connectivity), and deﬁnes the problem
as a multi-modal machine learning problem on multi-graph
convolution networks. This requires additional trafﬁc information to help make predictions. ST-MetaNet is RNN
based approach, which employs a sequence-to-sequence
architecture consisting of an encoder to learn historical
information and a decoder to make predictions step by
ASTGCN is a CNN based approach, by using three
independent components to respectively model three temporal properties of trafﬁc ﬂow (recent, daily-periodic, and
weekly-periodic) dependencies, this approach requires more
data for training and prediction. Such methods have achieved
great performance in trafﬁc prediction. However, in all of the
aforementioned approaches with applying GCNs to capture
the spatial dependence, the learned ﬁlters depend on the
Laplacian eigenbasis. Thus, a model trained as a speciﬁc
structure can not be directly applied to a graph with a different
structure.
Graph WaveNet develops a novel adaptive dependency
matrix to capture the hidden spatial dependency in the data,
however, it still can not be directly applied to a graph with
a different structure. GMAN adapts an encoder-decoder
architecture, where both the encoder and the decoder consist
of multiple spatio-temporal attention blocks to model the
impact of the spatio-temporal factors on trafﬁc conditions.
As an attention based approach, GMAN simply compute the
sum of spatial attention scores from all vertices. STGRAT 
also proposes an attention-based encoder-decoder architecture, use node attention to capture the spatial correlation
among roads and temporal attention to capture the temporal
dynamics of the road network by directly attending to features
in long sequences.
III. PRELIMINARIES
In this section, we start by offering a deﬁnition of Trafﬁc
Networks, then give the formal deﬁnition of Trafﬁc Flow
Forecasting problem explored in this paper.
A. TRAFFIC NETWORKS
In our model, we deﬁne a trafﬁc network as a directed graph
G = (V, E), where V is the set of nodes with the scale
N and E is the set of edges. The adjacency matrix of G is
denoted by W ∈RN×N. We extract the whole time period
(e.g., one hour) into m continuous time steps accroding to
the same sampling frequency. At each time step t, the graph
G has a dynamic feature matrix X ∈RN×F, which is used
interchangeably with graph signals. In other words, each node
on the trafﬁc network G detects F measurements at time
step t. The Graph-structured trafﬁc data is shown as Figure 1.
FIGURE 1. The Graph-structured traffic data. Each slices indicates a frame
of current traffic status at time step t, which is recorded in a
graph-structured data matrix.
B. PROBLEM DEFINITION
Trafﬁc ﬂow forecasting is a typical time-series prediction
problem, i.e., predicting the most likely trafﬁc ﬂow in the next
p time steps given the previous h steps trafﬁc observations.
Suppose we have h historical time step, each time step t has
a dynamic feature matrix Xt. The historical data H is denoted
as H = {Xt−p+1, Xt−p+2, . . . , Xt}.
a: TRAFFIC FLOW FORECASTING
Given the historical data H, the trafﬁc ﬂow Forecasting problem aims to predict P = {Xt+1, Xt+2, . . . , Xt+p} the next p
time step graph signals. In practice, according to the size of p,
we follow the most studied dividing principle: where p ≤30
as short-term, where 30<p ≤60 as long-term, where 60<p as
long-term.
IV. METHODOLOGY
In this section, we proposed the framework of Spatial-
Temporal Graph Attention Networks. We design a dual path
architecture by stacked Spatial-Temporal Blocks.
A. THE DUAL PATH ARCHITECTURE
Figure 2. We consider that the existing and potential spatial dependency which are both important for learning
good representations. To enjoy the beneﬁts from both path
VOLUME 8, 2020
X. Kong et al.: STGAT: Spatial-Temporal Graph Attention Networks for Traffic Flow Forecasting
FIGURE 2. The framework of Spatial-Temporal Graph Attention Networks.
topologies, each path in our proposed STGAT shares common structure while maintaining the spatial dependencies
to explore new features through dual path architecture. One
path takes adjacency matrix as input and the other takes
the self-adaptive adjacency matrix. In addition, self-adaptive
adjacency matrix is initially as a directed complete graph and
the ﬁnal values are obtained by training. Each path consists
of stacked spatial-temporal convolutional blocks(ST-Block).
The spatial-temporal convolutional blocks are composed
of three GTCN with different dilation factors (the factors
are 1, 2, 1) and one graph attention layer. By stacking multiple
spatial-temporal layers, STGAT is capable of handling spatial
and temporal dependencies at the same time. In the end,
we design a gated fusion mechanism to combine the nodes
features get from each path. The inputs of STGAT is the
trafﬁc history data H = {Xt−p+1, Xt−p+2, . . . , Xt} as we
deﬁned. Instead generating Xp recursively through p steps,
STGAT predict P = {Xt+1, Xt+2, . . . , Xt+p} as a whole.
B. SELF-ADAPTIVE ADJACENCY MATRIX
In order to take potential spatial dependencies into account
and catch more useful information from the historical traf-
ﬁc data for better forecasting performance, we propose a
self-adaptive adjacency matrix Aadj. Aadj is a parameterized
module consists of N × N (N is the number of nodes)
learnable parameters, where aadj(i,j) determines the weight
between node i and node j. We initialize Aadj = 1, which
means we ﬁrst compute attention scores from all vertices and
adjust Aadj during the training.
C. SPATIAL-TEMPORAL BLOCK
In order to capture features of both spatial and temporal
domains, the spatio-temporal block (ST-Block) is constructed
to jointly process graph-structured time series. The block
itself can be stacked or extended based on the scale and complexity of particular cases. Each block is composed of three
Gated Temporal Convolution Layer (GTCN) with different
dilation factors (the factors are 1, 2, 1) and one graph attention
1) GATED TEMPORAL CONVOLUTIONAL LAYER
In the temporal dimension, to capture the complex temporal
dependencies, we adopt a Gated Temporal Convolutional
Layer (GTCN). GTCN is designed as residual architecture to
let more information ﬂow through, we stacked three GTCN
layer and design a residual connect from input X to the output
of last GTCN layer, as shown in Figure 2. The structure
of each GTCN layer is shown in Figure 3. In our GTCN,
each layer contains two convolutional layers with the same
parameters number. For ensuring a suitable temporal context
window to take advantage of the long-term dependence of
the trafﬁc signal, we set different dilation factors of different
layers, respectively. Given the input X ∈RN×F×K, where K
is the number of kernels, the temporal gated convolution can
VOLUME 8, 2020
X. Kong et al.: STGAT: Spatial-Temporal Graph Attention Networks for Traffic Flow Forecasting
FIGURE 3. The structure of Gated Temporal Convolutional Layer.
be deﬁned as equation 1, 2.
gated = σ(X × M + b)
GTCN(X) = gated ⊗(X × V + c)
+ (1 −gated) ⊗(X × U + d)
where M, V, U, b, c, d are learnable parameters in convolution, ⊗is the element-wise product, and σ is the sigmoid
function which determines the ratio of information passed to
the next layer.
2) GRAPH ATTENTION LAYER
The trafﬁc network is usually described by a graph structure.
It is natural to treat the road networks as graphs mathematically. Graph Attention Layer is a novel neural network approach that operates on graph-structured data. Graph
Attention Layer is not structure-dependent like Graph Convolutional Network (GCN) and the model with Graph Attention Layer can be directly applicable to inductive learning
problems. The implementation of GAT is as same as .
The input to the layer is a set of node features, h
{⃗h1, ⃗h2, . . . , , ⃗hN}, ⃗hi ∈RF, where N is the number of nodes
in the graph and F is the number of nodes features. Here we
formulate the GAT from equation 3 to 5 as follows.
eij = LeakyReLU( ⃗aT
i (W ⃗hi ∥W ⃗hj))
In equation 3, ∥denotes concate operation, ⃗ai ∈R2F′ is a
learnable weight vector.
aij = softmaxj(eij) =
k∈N(i) exp(eik)
In equation 4 and 5, N(i) denotes a neighbor nodes set of
node i, σ a nonlinearity function, σ, and h′
i is the new features
of node i.
To further stabilize the learning process, a multi-head
attention is designed. The K independent attention heads have
their parameters respectively, and their outputs can be merged
into two ways, concatenation (equation 6) and average (equation 7). In this work, we use concatenation for hidden layers
and average for the ﬁnal layer.
Instead of simple linear combination, we design a gated
fusion mechanism. The input is the output of each path ⃗t ∈
RN∗P and ⃗s ∈RN∗P, where N is the number of nodes, P are
the features size of output. The formulation of Gated Fusion
as follows,
gate = σ(W1 × (⃗t + ⃗s))
Fusion(⃗s,⃗t) = tanh
 W2 × ⃗t × gate + (I −gate) × W3 × ⃗s
where matrix I has the same shape with gate and each element
in I is 1, W1, W2 and W3 are learnable parameters in network.
V. EXPERIMENTS
In this section, we compare the proposed STGAT with other
state-of-the-art baselines on two public trafﬁc (network)
datasets for trafﬁc ﬂow forecasting.
A. DATASETS
We design the same procedures as DCRNN , verifying STGAT on two public real-world large-scale trafﬁc
network datasets: (1) METR-LA including trafﬁc speed
data with 207 sensors collected from Mar 1st, 2012 to
Jun 30th, 2012 for 4 months on the highways of Los Angeles County . (2) PEMS-BAY is collected by California
Transportation Agencies (CalTrans) Performance Measurement System (PeMS). We select 325 sensors data in the Bay
Area and collect 6 months of data ranging from Jan 1st,
2017 to May 31th, 2017 as DCRNN for the experiment.
Detailed dataset information are provided in Table 1. For
both datasets, the readings of the sensors are aggregated into
5-minutes windows and apply Z-Score normalization.
TABLE 1. Detailed dataset information of METR-LA and PEMS-BAY.
B. PREPROCESSING
To have a fair comparison, we design exactely the same
preprosessing procedures as discribed in . The adjacency
VOLUME 8, 2020
X. Kong et al.: STGAT: Spatial-Temporal Graph Attention Networks for Traffic Flow Forecasting
matrix of the sensor is constructed by road network distance with a threshold Gaussian kernel . As shown in
equation 10, Wij represents the weight between sensor vi
and sensor vj, dist(vi, vj) denotes the road network distance
(not spatial distance) from sensor vi to sensor vj, the Floyd
algorithm is applied to ﬁnd the shortest distance for each
sensor (As data is insurﬁcient, we only adopt Floyd in Los
Angeles). σ is the standard deviation of distances and k is
predeﬁned threshold.
−dist(vi, vj)2
dist(vi, vj) > k
The datasets are splited into three parts, where 70% is for
training, 20% is for testing while the remaining 10% for
validation.
C. BASELINES
In order to evaluate the performance of our proposed STGAT,
we conduct the comparison experiments with the state-ofthe-art baselines listed as follows: ARIMA. Auto-Regressive
Integrated Moving Average model with Kalman ﬁlter .
FC-LSTM. Recurrent neural network with fully connected
LSTM hidden units . DCRNN. Diffusion convolution
recurrent neural network , which captures the spatial
dependency using bidirectional random walks on the graph,
and the temporal dependency using the encoder-decoder
architecture with scheduled sampling. GGRU. Graph gated
recurrent unit network . A convolutional sub-network
is used to control each attention head’s importance. Design
the Graph Gated Recurrent Unit (GGRU) to address the
trafﬁc speed forecasting problem. STGCN. Spatial-temporal
graph convolution network , which formulate the problem on graphs and build the model with complete convolutional structures, instead of applying regular convolutional and recurrent units, to get much faster training speed
with fewer parameters. Graph WaveNet. A framework for
Deep Spatial-Temporal Graph Modeling . Use a learnable adaptive dependency matrix to capture the hidden spatial dependency in the data. With a stacked dilated 1D
convolution component with different receptive ﬁelds to
handle very long sequences. ST-MetaNet employs a
sequence-to-sequence architecture, consisting of an encoder
to learn historical information and a decoder to make predictions step by step.STGRAT propose an attention-based
encoder-decoder architecture, use node attention captures the
spatial correlation among roads and temporal attention captures the temporal dynamics of the road network by directly
attending to features in long sequences. GMAN adapts
an encoder-decoder architecture (similar with transformer),
where both the encoder and the decoder consist of multiple
spatio-temporal attention blocks to model the impact of the
spatio-temporal factors on trafﬁc conditions.
Since ST-MGCN requires additional trafﬁc information to construct graphs and ASTGCN needs longer time
spans data for training and forecasting, we not include these
two methods ST-MGCN and ASTGCN in our comparison
experiments.
D. EXPERIMENTAL DETAILS
The experiments are compiled and tested on Linux system, with the environment of Intel(R) Core(TM) i9-9900K
 and NVIDIA GeForce GTX 2080 Ti. We
adopt a grid search strategy to ﬁnd the best hyper-parameters
on validations. All the tests take 60 minutes (with 12 observed
data points) with the historical time window. The historical
time window is used to forecast trafﬁc conditions in the
following 15, 30, and 60 minutes (with 3, 6, 12 observed data
points respectively), where 15, 30, 60 minutes correspond to
short-term, medium-term and long-term respectively.
Each path of daul path architecture has 4 ST-Blocks and
one output layer to construct STGAT. Equtaion 6 is used as
the ﬁrst three ST-Block’s graph attention layers. Both of the
ﬁrst three layers consist of K = 4 attention heads computing
F′ = 64 features. Equation 7 is used as the last ﬂoor graph
attention layers. The last layer consists of K = 6 attention
heads computing F′ = 64 features. Each block is stacked by
three Gated Temporal Convolution Layer (GTCN) with dilation factors 1, 2, 1. We apply dropout, batch normalization
and residual connection inside each block.
STGAT is trained using Adam optimizer with an initial
learning rate of 0.0003, dropout with p = 0.6 is applied
to the outputs of the graph attention layer. All models are
initialized using Glorot initialization . The evaluation
metrics follows Graph WaveNet’s work , including mean
absolute error (MAE), root mean squared error (RMSE), and
mean absolute percentage error (MAPE). Missing values are
excluded in both training and testing.
TABLE 2. Performance comparison of STGAT and baseline models.
E. RESULTS
The overall performance of all competing methods across
datasets is illustrated in Table 2. Our framework achieves
superior performance in all three evaluation metrics (MAE,
RMSE, MAPE) on both dataset. The results demonstrate our
VOLUME 8, 2020
X. Kong et al.: STGAT: Spatial-Temporal Graph Attention Networks for Traffic Flow Forecasting
proposed framework STGAT can be applied to a variety of
prediction scenarios.
Our STGAT performs much better than temporal methods (including ARIMA, FC-LSTM, and WaveNet) and
spatial-temporal
(including
convolution-based
approach STGCN and recurrent-based approaches DCRNN
and GGRU). The explanation is that STGAT considers not
only temporal but also spatial dependencies, especially in
the spatial dimension, while considering both potential and
existing dependencies.
A phenomenon worth considering is that STGAT achieves
small improvement over the latest method Graph WaveNet
and ST-MetaNet in 15 minutes ahead of prediction and outperforms by a large margin in 30 minutes and 60 minutes
ahead of prediction. Graph WaveNet also uses gating mechanisms in temporal dimension, but without residual architecture. STGRAT uses temporal attention attends to important
time steps of each node, but without gating mechanisms
may contain redundant information for prediction. Moreover, to further understand the effect of GTCN, we design a
variant STGAT-TCN of our STGAT, which only uses TCN
as describe in without gated mechanism and residual
architecture. For experimental result is shown in Table 2,
STGAT-TCN has worse performance in long-term forecast
than other variants. The possible explanation is that in our
STGAT the residual architecture allows more information
pass to the next layer, while the dilation factor is used to
ensure a suitable temporal context window to take advantage
of the long-term dependence of the trafﬁc data, which results
a good performance in long-term prediction.
Another interesting phenomenon is that all the methods perform better in dataset PEMS-BAY than METR-LA.
Moreover, each method has similar performance in dataset
PEMS-BAY even if for the traditional method ARIMA. The
reason is that Los Angeles is known for its complicated
trafﬁc conditions, so trafﬁc forecasting in METR-LA dataset
is more challenging than that in the PEMS-BAY (Bay Area)
dataset. What’s more, our STGAT outperforms all the baseline methods with a signiﬁcant reduction of prediction error
on dataset METR-LA under three evaluation metrics, which
means that compared with baseline methods, our STGAT can
better handle more complex situations.
1) ABLATION STUDY
To understand the impact of different paths in our dual path
network architecture, we split our dual path network STGAT
into two subnetworks STGAT-Adj and STGAT-Adp, where
STGAT-Adj with adjacency matrix as input and STGAT-Adp
with self-adaptive adjacency matrix as input. As shown
in Table 2, comparing to our full version STGAT, the two
simpliﬁed versions of STGAT all lead to certain degradation
of performance. This empirically proves the effectiveness of
the two important components of STGAT.
Both subnetworks can use stacked ST-Block to capture the
spatial-temporal dependencies effectively. The subnetwork
with adjacency matrix is more capable of detecting spatial
dependencies at each temporal stage. Since trafﬁc ﬂows can
be affected by many complex external factors, the subnetwork
with self-adaptive adjacency can bring extra useful information and capture the hidden spatial dependence in the trafﬁc
data. We believe that the trafﬁc data contains many implicit
spatial dependencies that are not represented in the adjacency
matrix, future trafﬁc ﬂow is not only inﬂuenced by neighboring areas but also by other further nodes. Thus, through the
joint efforts of both component, STGAT can better explore
that implicit information in the data. It demonstrates that
adopting a component with learnable self-adaptive adjacency
matrix to capture the hidden spatial dependence in the data
can improve the performance of trafﬁc ﬂow forecasting.
2) GENERALIZATION PERFORMANCE OF STGAT
In order to verify the generalization ability of the framework, we trained STGAT-BAY with dataset PEMS-BAY
while test it on the other dataset METR-LA (not similar
with work , we do not use transfer learning.). Since
the two road networks may have different hidden dependencies, the self-adaptive adjacency matrix learned on dataset
PEMS-BAY can not be directly applied. To tackle this problem, we use the adjacency matrix of METR-LA to replace the
self-adaptive adjacency matrix. For both path in our STGAT,
we use adjacency matrix as input in generalization experiments. The hyper-parameter settings and experimental setup
is identical as described previously. Table 3 demonstrates the
performance of STGAT-BAY and other GCN-based model
for 15 minutes, 30 minutes and 60 minutes ahead prediction
on the datasets PEMS-BAY.
TABLE 3. Generalization performance of the STGAT on different graph
structure. We training STGAT with dataset PEMS-BAY, and testing it on
dataset METR-LA.
Since the learned ﬁlters depend on the Laplacian eigenbasis which requires the graph structure, all GCN-based models
trained on a speciﬁc structure can not be directly applied to
another graph. This suggests that except for our STGAT, all
GCN-based models cannot be trained with PEMS-BAY and
test on METR-LA. STGAT trained on a speciﬁc structure
can be directly applied to another graph without additional
training step. When the training data in a certain area is
lacking, we can directly use the model trained in other places
to make predictions. These experimental results demonstrate
that our framework is fairly ﬂexible to be directly applied to
another graph without any prior knowledge.
However, the generalization performance of our model are
not ideal. We also design another experiment to improve the
generalization performance of our STGAT which uses both
datasets for training. We set self-adaptive adjacency matrix
as N × N, where N = 325 for dataset PEMS-BAY, and use
VOLUME 8, 2020
X. Kong et al.: STGAT: Spatial-Temporal Graph Attention Networks for Traffic Flow Forecasting
FIGURE 4. The speed curves of nodes on dataset METR-LA.
the ﬁrst 207 × 207 elements for dataset METR-LA. The
hyper-parameter settings and experimental setup is also identical described previously. The results are shown in Table 4.
Our STGAT has better performance over baseline (STGCN,
WaveNet, FC-LSTM and ARIMA) on long-term prediction.
Moreover, our STGAT has better performance than baselines
(WaveNet, FC-LSTM and ARIMA) on both short-term and
mid-term prediction. This demonstrates that our model has
the potential for further improvement on generalization performance.
TABLE 4. Generalization performance of the STGAT on different graph
structure. We training STGAT with both dataset PEMS-BAY and METR-LA,
testing it on dataset METR-LA.
FIGURE 5. The subnetwork STGAT-Adp with self-adaptive adjacency
F. EFFECT OF SELF-ADAPTIVE ADJACENCY MATRIX
In order to better understand our model, we further discuss about the hidden dependencies captured by subnetwork
STGAT-Adp as shown in Figure 5. According to Figure 5a,
some columns in row 3 have higher value than other rows,
such as columns 15, 17 and 19. It suggests that each node
has different inﬂuence on the other nodes in graph. Figure 5b
conﬁrms our observation, that is the strong hidden inﬂuence
between nodes pretends to be able to match up with a real
geographic routine that links each other. To further study the
correlation between node 3, 15, 17 and 19, we plot the speed
curves of these nodes during one day in Figure 4. We ﬁnd
an interesting phenomenon that the time series sequence of
all nodes have the same trend. The possible explanation is
that our self-adaptive adjacency matrix can connect the nodes
which have the same temporal patterns at close distance
VI. DISCUSSION AND CONCLUSION
In this paper, we propose a novel spatial-temporal deep learning framework STGAT for trafﬁc ﬂow prediction. STGAT
is a general framework to process spatial-temporal forecasting tasks. To take into account both potential and existing
spatial dependencies, we design a dual path networks architecture and each subnetwork share the same structure. Integrating the graph attention mechanism and gated temporal
convolution through spatial-temporal blocks. By adopting
different dilation factor of different layer in gated temporal convolution, STGAT is able to handle long temporal
sequence. Through adopting graph attention mechanism into
our spatial-temporal framework, the model can be directly
applied to inductive learning problems. Finally, our STGAT
achieves state-of-the-art results on two public real-world traf-
ﬁc network datasets. In the future, we plan to investigate the
proposed model on other spatial-temporal prediction problems. What’s more, we will consider more external factors,
such as weather and event, and use all of them to improve the
performance of trafﬁc ﬂow predictions.