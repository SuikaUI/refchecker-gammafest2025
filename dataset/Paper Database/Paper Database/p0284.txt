HAL Id: hal-02869787
 
Submitted on 25 Feb 2021
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Bootstrap Your Own Latent: A new approach to
self-supervised learning
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H
Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan
Daniel Guo, Mohammad Gheshlaghi Azar, et al.
To cite this version:
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, et al.. Bootstrap Your Own Latent: A new approach to self-supervised learning. Neural Information Processing
Systems, 2020, Montréal, Canada. ￿hal-02869787v2￿
Bootstrap Your Own Latent
A New Approach to Self-Supervised Learning
Jean-Bastien Grill∗,1
Florian Strub∗,1
Florent Altché∗,1
Corentin Tallec∗,1
Pierre H. Richemond∗,1,2
Elena Buchatskaya1
Carl Doersch1
Bernardo Avila Pires1
Zhaohan Daniel Guo1
Mohammad Gheshlaghi Azar1
Bilal Piot1
Koray Kavukcuoglu1
Rémi Munos1
Michal Valko1
2Imperial College
[jbgrill,fstrub,altche,corentint,richemond]@google.com
We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image
representation learning. BYOL relies on two neural networks, referred to as online and target
networks, that interact and learn from each other. From an augmented view of an image, we train
the online network to predict the target network representation of the same image under a different
augmented view. At the same time, we update the target network with a slow-moving average
of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a
new state of the art without them. BYOL reaches 74.3% top-1 classiﬁcation accuracy on ImageNet
using a linear evaluation with a ResNet-50 architecture and 79.6% with a larger ResNet. We
show that BYOL performs on par or better than the current state of the art on both transfer and
semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.3
Introduction
Number of parameters
ImageNet top-1 accuracy (%)
SimCLR (2×)
SimCLR (4×)
BYOL (200-2×)
Sup.(200-2×)
Figure 1: Performance of BYOL on ImageNet (linear evaluation) using ResNet-50 and our best architecture ResNet-
200 (2×), compared to other unsupervised and supervised
(Sup.) baselines .
Learning good image representations is a key challenge
in computer vision as it allows for efﬁcient
training on downstream tasks . Many different training approaches have been proposed to learn
such representations, usually relying on visual pretext
tasks. Among them, state-of-the-art contrastive methods are trained by reducing the distance between representations of different augmented
views of the same image (‘positive pairs’), and increasing the distance between representations of augmented
views from different images (‘negative pairs’). These
methods need careful treatment of negative pairs 
by either relying on large batch sizes , memory
banks or customized mining strategies to retrieve the negative pairs. In addition, their performance
critically depends on the choice of image augmentations .
In this paper, we introduce Bootstrap Your Own
Latent (BYOL), a new algorithm for self-supervised
learning of image representations. BYOL achieves higher
∗Equal contribution; the order of ﬁrst authors was randomly selected.
3 
performance than state-of-the-art contrastive methods
without using negative pairs. It iteratively bootstraps4 the outputs of a network to serve as targets for an enhanced
representation. Moreover, BYOL is more robust to the choice of image augmentations than contrastive methods; we
suspect that not relying on negative pairs is one of the leading reasons for its improved robustness. While previous
methods based on bootstrapping have used pseudo-labels , cluster indices or a handful of labels ,
we propose to directly bootstrap the representations. In particular, BYOL uses two neural networks, referred to
as online and target networks, that interact and learn from each other. Starting from an augmented view of an
image, BYOL trains its online network to predict the target network’s representation of another augmented view of
the same image. While this objective admits collapsed solutions, e.g., outputting the same vector for all images,
we empirically show that BYOL does not converge to such solutions. We hypothesize (see Section 3.2) that the
combination of (i) the addition of a predictor to the online network and (ii) the use of a slow-moving average of
the online parameters as the target network encourages encoding more and more information within the online
projection and avoids collapsed solutions.
We evaluate the representation learned by BYOL on ImageNet and other vision benchmarks using ResNet
architectures . Under the linear evaluation protocol on ImageNet, consisting in training a linear classiﬁer on
top of the frozen representation, BYOL reaches 74.3% top-1 accuracy with a standard ResNet-50 and 79.6% top-1
accuracy with a larger ResNet (Figure 1). In the semi-supervised and transfer settings on ImageNet, we obtain results
on par or superior to the current state of the art. Our contributions are: (i) We introduce BYOL, a self-supervised
representation learning method (Section 3) which achieves state-of-the-art results under the linear evaluation
protocol on ImageNet without using negative pairs. (ii) We show that our learned representation outperforms the
state of the art on semi-supervised and transfer benchmarks (Section 4). (iii) We show that BYOL is more resilient to
changes in the batch size and in the set of image augmentations compared to its contrastive counterparts (Section 5).
In particular, BYOL suffers a much smaller performance drop than SimCLR, a strong contrastive baseline, when only
using random crops as image augmentations.
Related work
Most unsupervised methods for representation learning can be categorized as either generative or discriminative . Generative approaches to representation learning build a distribution over data and latent embedding and
use the learned embeddings as image representations. Many of these approaches rely either on auto-encoding
of images or on adversarial learning , jointly modelling data and representation .
Generative methods typically operate directly in pixel space. This however is computationally expensive, and the
high level of detail required for image generation may not be necessary for representation learning.
Among discriminative methods, contrastive methods currently achieve state-of-the-art
performance in self-supervised learning . Contrastive approaches avoid a costly generation step in
pixel space by bringing representation of different views of the same image closer (‘positive pairs’), and spreading
representations of views from different images (‘negative pairs’) apart . Contrastive methods often require
comparing each example with many other examples to work well prompting the question of whether using
negative pairs is necessary.
DeepCluster partially answers this question. It uses bootstrapping on previous versions of its representation
to produce targets for the next representation; it clusters data points using the prior representation, and uses the
cluster index of each sample as a classiﬁcation target for the new representation. While avoiding the use of negative
pairs, this requires a costly clustering phase and speciﬁc precautions to avoid collapsing to trivial solutions.
Some self-supervised methods are not contrastive but rely on using auxiliary handcrafted prediction tasks to learn
their representation. In particular, relative patch prediction , colorizing gray-scale images , image
inpainting , image jigsaw puzzle , image super-resolution , and geometric transformations 
have been shown to be useful. Yet, even with suitable architectures , these methods are being outperformed by
contrastive methods .
Our approach has some similarities with Predictions of Bootstrapped Latents (PBL, ), a self-supervised representation learning technique for reinforcement learning (RL). PBL jointly trains the agent’s history representation and
an encoding of future observations. The observation encoding is used as a target to train the agent’s representation,
and the agent’s representation as a target to train the observation encoding. Unlike PBL, BYOL uses a slow-moving
average of its representation to provide its targets, and does not require a second network.
4Throughout this paper, the term bootstrap is used in its idiomatic sense rather than the statistical sense.
The idea of using a slow-moving average target network to produce stable targets for the online network was inspired
by deep RL . Target networks stabilize the bootstrapping updates provided by the Bellman equation,
making them appealing to stabilize the bootstrap mechanism in BYOL. While most RL methods use ﬁxed target
networks, BYOL uses a weighted moving average of previous networks (as in ) in order to provide smoother
changes in the target representation.
In the semi-supervised setting , an unsupervised loss is combined with a classiﬁcation loss over a handful of
labels to ground the training . Among these methods, mean teacher (MT) also
uses a slow-moving average network, called teacher, to produce targets for an online network, called student. An
ℓ2 consistency loss between the softmax predictions of the teacher and the student is added to the classiﬁcation
loss. While demonstrates the effectiveness of MT in the semi-supervised learning case, in Section 5 we show
that a similar approach collapses when removing the classiﬁcation loss. In contrast, BYOL introduces an additional
predictor on top of the online network, which prevents collapse.
Finally, in self-supervised learning, MoCo uses a slow-moving average network (momentum encoder) to maintain
consistent representations of negative pairs drawn from a memory bank. Instead, BYOL uses a moving average
network to produce prediction targets as a means of stabilizing the bootstrap step. We show in Section 5 that this
mere stabilizing effect can also improve existing contrastive methods.
We start by motivating our method before explaining its details in Section 3.1. Many successful self-supervised
learning approaches build upon the cross-view prediction framework introduced in . Typically, these approaches
learn representations by predicting different views (e.g., different random crops) of the same image from one
another. Many such approaches cast the prediction problem directly in representation space: the representation of
an augmented view of an image should be predictive of the representation of another augmented view of the same
image. However, predicting directly in representation space can lead to collapsed representations: for instance, a
representation that is constant across views is always fully predictive of itself. Contrastive methods circumvent
this problem by reformulating the prediction problem into one of discrimination: from the representation of an
augmented view, they learn to discriminate between the representation of another augmented view of the same
image, and the representations of augmented views of different images. In the vast majority of cases, this prevents
the training from ﬁnding collapsed representations. Yet, this discriminative approach typically requires comparing
each representation of an augmented view with many negative examples, to ﬁnd ones sufﬁciently close to make the
discrimination task challenging. In this work, we thus tasked ourselves to ﬁnd out whether these negative examples
are indispensable to prevent collapsing while preserving high performance.
To prevent collapse, a straightforward solution is to use a ﬁxed randomly initialized network to produce the targets
for our predictions. While avoiding collapse, it empirically does not result in very good representations. Nonetheless,
it is interesting to note that the representation obtained using this procedure can already be much better than the
initial ﬁxed representation. In our ablation study (Section 5), we apply this procedure by predicting a ﬁxed randomly
initialized network and achieve 18.8% top-1 accuracy (Table 5a) on the linear evaluation protocol on ImageNet,
whereas the randomly initialized network only achieves 1.4% by itself. This experimental ﬁnding is the core
motivation for BYOL: from a given representation, referred to as target, we can train a new, potentially enhanced
representation, referred to as online, by predicting the target representation. From there, we can expect to build a
sequence of representations of increasing quality by iterating this procedure, using subsequent online networks as
new target networks for further training. In practice, BYOL generalizes this bootstrapping procedure by iteratively
reﬁning its representation, but using a slowly moving exponential average of the online network as the target network
instead of ﬁxed checkpoints.
Description of BYOL
BYOL’s goal is to learn a representation yθ which can then be used for downstream tasks. As described previously,
BYOL uses two neural networks to learn: the online and target networks. The online network is deﬁned by a set
of weights θ and is comprised of three stages: an encoder fθ, a projector gθ and a predictor qθ, as shown in
Figure 2 and Figure 8. The target network has the same architecture as the online network, but uses a different set of
weights ξ. The target network provides the regression targets to train the online network, and its parameters ξ are an
exponential moving average of the online parameters θ . More precisely, given a target decay rate τ ∈ ,
after each training step we perform the following update,
ξ ←τξ + (1 −τ)θ.
representation
projection
prediction
Figure 2: BYOL’s architecture. BYOL minimizes a similarity loss between qθ(zθ) and sg(z′
ξ), where θ are the trained
weights, ξ are an exponential moving average of θ and sg means stop-gradient. At the end of training, everything
but fθ is discarded, and yθ is used as the image representation.
Given a set of images D, an image x ∼D sampled uniformly from D, and two distributions of image augmentations
T and T ′, BYOL produces two augmented views v =
∆t(x) and v′ =
∆t′(x) from x by applying respectively image
augmentations t ∼T and t′ ∼T ′. From the ﬁrst augmented view v, the online network outputs a representation
∆fθ(v) and a projection zθ =
∆gθ(y). The target network outputs y′
∆fξ(v′) and the target projection
∆gξ(y′) from the second augmented view v′. We then output a prediction qθ(zθ) of z′
ξ and ℓ2-normalize both
qθ(zθ) and z′
ξ to qθ(zθ) =
∆qθ(zθ)/∥qθ(zθ)∥2 and z′
ξ∥2. Note that this predictor is only applied to the
online branch, making the architecture asymmetric between the online and target pipeline. Finally we deﬁne the
following mean squared error between the normalized predictions and target projections,5
qθ(zθ) −z′
2 = 2 −2 ·
⟨qθ(zθ), z′
We symmetrize the loss Lθ,ξ in Eq. 2 by separately feeding v′ to the online network and v to the target network to
compute eLθ,ξ. At each training step, we perform a stochastic optimization step to minimize LBYOL
= Lθ,ξ + eLθ,ξ
with respect to θ only, but not ξ, as depicted by the stop-gradient in Figure 2. BYOL’s dynamics are summarized as
θ ←optimizer
 θ, ∇θLBYOL
ξ ←τξ + (1 −τ)θ,
where optimizer is an optimizer and η is a learning rate.
At the end of training, we only keep the encoder fθ; as in . When comparing to other methods, we consider the
number of inference-time weights only in the ﬁnal representation fθ. The full training procedure is summarized in
Appendix A, and python pseudo-code based on the libraries JAX and Haiku is provided in in Appendix J.
Intuitions on BYOL’s behavior
As BYOL does not use an explicit term to prevent collapse (such as negative examples ) while minimizing
with respect to θ, it may seem that BYOL should converge to a minimum of this loss with respect to (θ, ξ)
(e.g., a collapsed constant representation). However BYOL’s target parameters ξ updates are not in the direction of
θ,ξ . More generally, we hypothesize that there is no loss Lθ,ξ such that BYOL’s dynamics is a gradient descent
on L jointly over θ, ξ. This is similar to GANs , where there is no loss that is jointly minimized w.r.t. both
the discriminator and generator parameters. There is therefore no a priori reason why BYOL’s parameters would
converge to a minimum of LBYOL
While BYOL’s dynamics still admit undesirable equilibria, we did not observe convergence to such equilibria in our
experiments. In addition, when assuming BYOL’s predictor to be optimal6 i.e., qθ = q⋆with
q⋆(zθ) = E
5While we could directly predict the representation y and not a projection z, previous work have empirically shown that
using this projection improves performance.
6For simplicity we also consider BYOL without normalization (which performs reasonably close to BYOL, see Appendix F.6)
nor symmetrization
we hypothesize that the undesirable equilibria are unstable. Indeed, in this optimal predictor case, BYOL’s updates
on θ follow in expectation the gradient of the expected conditional variance (see Appendix H for details),
q⋆(zθ) −z′
ξ,i is the i-th feature of z′
Note that for any random variables X, Y, and Z, Var(X|Y, Z) ≤Var(X|Y ). Let X be the target projection, Y the
current online projection, and Z an additional variability on top of the online projection induced by stochasticities
in the training dynamics: purely discarding information from the online projection cannot decrease the conditional
In particular, BYOL avoids constant features in zθ as, for any constant c and random variables zθ and z′
ξ|c); hence our hypothesis on these collapsed constant equilibria being unstable. Interestingly, if we were
to minimize E[P
ξ,i|zθ)] with respect to ξ, we would get a collapsed z′
ξ as the variance is minimized for a
constant z′
ξ. Instead, BYOL makes ξ closer to θ, incorporating sources of variability captured by the online projection
into the target projection.
Furthemore, notice that performing a hard-copy of the online parameters θ into the target parameters ξ would be
enough to propagate new sources of variability. However, sudden changes in the target network might break the
assumption of an optimal predictor, in which case BYOL’s loss is not guaranteed to be close to the conditional variance.
We hypothesize that the main role of BYOL’s moving-averaged target network is to ensure the near-optimality of the
predictor over training; Section 5 and Appendix I provide some empirical support of this interpretation.
Implementation details
Image augmentations
BYOL uses the same set of image augmentations as in SimCLR . First, a random patch
of the image is selected and resized to 224 × 224 with a random horizontal ﬂip, followed by a color distortion,
consisting of a random sequence of brightness, contrast, saturation, hue adjustments, and an optional grayscale
conversion. Finally Gaussian blur and solarization are applied to the patches. Additional details on the image
augmentations are in Appendix B.
Architecture
We use a convolutional residual network with 50 layers and post-activation (ResNet-50(1×)
v1) as our base parametric encoders fθ and fξ. We also use deeper (50, 101, 152 and 200 layers) and wider (from
1× to 4×) ResNets, as in . Speciﬁcally, the representation y corresponds to the output of the ﬁnal
average pooling layer, which has a feature dimension of 2048 (for a width multiplier of 1×). As in SimCLR ,
the representation y is projected to a smaller space by a multi-layer perceptron (MLP) gθ, and similarly for the
target projection gξ. This MLP consists in a linear layer with output size 4096 followed by batch normalization ,
rectiﬁed linear units (ReLU) , and a ﬁnal linear layer with output dimension 256. Contrary to SimCLR, the
output of this MLP is not batch normalized. The predictor qθ uses the same architecture as gθ.
Optimization
We use the LARS optimizer with a cosine decay learning rate schedule , without restarts,
over 1000 epochs, with a warm-up period of 10 epochs. We set the base learning rate to 0.2, scaled linearly 
with the batch size (LearningRate = 0.2 × BatchSize/256). In addition, we use a global weight decay parameter of
1.5 · 10−6 while excluding the biases and batch normalization parameters from both LARS adaptation and weight
decay. For the target network, the exponential moving average parameter τ starts from τbase = 0.996 and is increased
to one during training. Speciﬁcally, we set τ ≜1 −(1 −τbase) · (cos(πk/K) + 1)/2 with k the current training
step and K the maximum number of training steps. We use a batch size of 4096 split over 512 Cloud TPU v3 cores.
With this setup, training takes approximately 8 hours for a ResNet-50(×1). All hyperparameters are summarized in
Appendix J; an additional set of hyperparameters for a smaller batch size of 512 is provided in Appendix G.
Experimental evaluation
We assess the performance of BYOL’s representation after self-supervised pretraining on the training set of the
ImageNet ILSVRC-2012 dataset . We ﬁrst evaluate it on ImageNet (IN) in both linear evaluation and semisupervised setups. We then measure its transfer capabilities on other datasets and tasks, including classiﬁcation,
segmentation, object detection and depth estimation. For comparison, we also report scores for a representation
trained using labels from the train ImageNet subset, referred to as Supervised-IN. In Appendix E, we assess the
generality of BYOL by pretraining a representation on the Places365-Standard dataset before reproducing this
evaluation protocol.
Linear evaluation on ImageNet
We ﬁrst evaluate BYOL’s representation by training a linear classiﬁer on top of
the frozen representation, following the procedure described in , and appendix C.1; we report top-1
and top-5 accuracies in % on the test set in Table 1. With a standard ResNet-50 (×1) BYOL obtains 74.3% top-1
accuracy (91.6% top-5 accuracy), which is a 1.3% (resp. 0.5%) improvement over the previous self-supervised state
of the art . This tightens the gap with respect to the supervised baseline of , 76.5%, but is still signiﬁcantly
below the stronger supervised baseline of , 78.9%. With deeper and wider architectures, BYOL consistently
outperforms the previous state of the art (Appendix C.2), and obtains a best performance of 79.6% top-1 accuracy,
ranking higher than previous self-supervised approaches. On a ResNet-50 (4×) BYOL achieves 78.6%, similar to
the 78.9% of the best supervised baseline in for the same architecture.
Local Agg.
CPC v2 
SimCLR 
MoCo v2 
InfoMin Aug. 
BYOL (ours)
(a) ResNet-50 encoder.
Architecture
SimCLR 
ResNet-50 (2×)
ResNet-50 (2×)
BYOL (ours)
ResNet-50 (2×)
CPC v2 
ResNet-161
ResNet-50 (4×)
SimCLR 
ResNet-50 (4×)
BYOL (ours)
ResNet-50 (4×)
BYOL (ours)
ResNet-200 (2×)
(b) Other ResNet encoder architectures.
Table 1: Top-1 and top-5 accuracies (in %) under linear evaluation on ImageNet.
Semi-supervised training on ImageNet
Next, we evaluate the performance obtained when ﬁne-tuning BYOL’s
representation on a classiﬁcation task with a small subset of ImageNet’s train set, this time using label information.
We follow the semi-supervised protocol of detailed in Appendix C.1, and use the same ﬁxed splits of
respectively 1% and 10% of ImageNet labeled training data as in . We report both top-1 and top-5 accuracies on
the test set in Table 2. BYOL consistently outperforms previous approaches across a wide range of architectures.
Additionally, as detailed in Appendix C.1, BYOL reaches 77.7% top-1 accuracy with ResNet-50 when ﬁne-tuning
over 100% of ImageNet labels.
Supervised 
SimCLR 
BYOL (ours)
(a) ResNet-50 encoder.
Architecture
CPC v2 ResNet-161
SimCLR 
ResNet-50 (2×)
BYOL (ours)
ResNet-50 (2×)
SimCLR 
ResNet-50 (4×)
BYOL (ours)
ResNet-50 (4×)
BYOL (ours)
ResNet-200 (2×)
(b) Other ResNet encoder architectures.
Table 2: Semi-supervised training with a fraction of ImageNet labels.
Transfer to other classiﬁcation tasks
We evaluate our representation on other classiﬁcation datasets to assess
whether the features learned on ImageNet (IN) are generic and thus useful across image domains, or if they are
ImageNet-speciﬁc. We perform linear evaluation and ﬁne-tuning on the same set of classiﬁcation tasks used
in , and carefully follow their evaluation protocol, as detailed in Appendix D. Performance is reported using
standard metrics for each benchmark, and results are provided on a held-out test set after hyperparameter selection
on a validation set. We report results in Table 3, both for linear evaluation and ﬁne-tuning. BYOL outperforms
SimCLR on all benchmarks and the Supervised-IN baseline on 7 of the 12 benchmarks, providing only slightly
worse performance on the 5 remaining benchmarks. BYOL’s representation can be transferred over to small images,
e.g., CIFAR , landscapes, e.g., SUN397 or VOC2007 , and textures, e.g., DTD .
Caltech-101
Linear evaluation:
BYOL (ours)
SimCLR (repro)
SimCLR 
Supervised-IN 
Fine-tuned:
BYOL (ours)
SimCLR (repro)
SimCLR 
Supervised-IN 
Random init 
Table 3: Transfer learning results from ImageNet (IN) with the standard ResNet-50 architecture.
Transfer to other vision tasks
We evaluate our representation on different tasks relevant to computer vision
practitioners, namely semantic segmentation, object detection and depth estimation. With this evaluation, we assess
whether BYOL’s representation generalizes beyond classiﬁcation tasks.
We ﬁrst evaluate BYOL on the VOC2012 semantic segmentation task as detailed in Appendix D.4, where the
goal is to classify each pixel in the image . We report the results in Table 4a. BYOL outperforms both the
Supervised-IN baseline (+1.9 mIoU) and SimCLR (+1.1 mIoU).
Similarly, we evaluate on object detection by reproducing the setup in using a Faster R-CNN architecture ,
as detailed in Appendix D.5. We ﬁne-tune on trainval2007 and report results on test2007 using the standard
AP50 metric; BYOL is signiﬁcantly better than the Supervised-IN baseline (+3.1 AP50) and SimCLR (+2.3 AP50).
Finally, we evaluate on depth estimation on the NYU v2 dataset, where the depth map of a scene is estimated
given a single RGB image. Depth prediction measures how well a network represents geometry, and how well
that information can be localized to pixel accuracy . The setup is based on and detailed in Appendix D.6.
We evaluate on the commonly used test subset of 654 images and report results using several common metrics
in Table 4b: relative (rel) error, root mean squared (rms) error, and the percent of pixels (pct) where the error,
max(dgt/dp, dp/dgt), is below 1.25n thresholds where dp is the predicted depth and dgt is the ground truth
depth . BYOL is better or on par with other methods for each metric. For instance, the challenging pct.<1.25
measure is respectively improved by +3.5 points and +1.3 points compared to supervised and SimCLR baselines.
Supervised-IN 
SimCLR (repro)
BYOL (ours)
(a) Transfer results in semantic
segmentation and object detection.
Higher better
Lower better
pct.<1.252
pct.<1.253
Supervised-IN 
SimCLR (repro)
BYOL (ours)
(b) Transfer results on NYU v2 depth estimation.
Table 4: Results on transferring BYOL’s representation to other vision tasks.
Building intuitions with ablations
We present ablations on BYOL to give an intuition of its behavior and performance. For reproducibility, we run each
conﬁguration of parameters over three seeds, and report the average performance. We also report the half difference
between the best and worst runs when it is larger than 0.25. Although previous works perform ablations at 100
epochs , we notice that relative improvements at 100 epochs do not always hold over longer training. For
this reason, we run ablations over 300 epochs on 64 TPU v3 cores, which yields consistent results compared to
our baseline training of 1000 epochs. For all the experiments in this section, we set the initial learning rate to 0.3
with batch size 4096, the weight decay to 10−6 as in SimCLR and the base target decay rate τbase to 0.99. In this
section we report results in top-1 accuracy on ImageNet under the linear evaluation protocol as in Appendix C.1.
Batch size
Among contrastive methods, the ones that draw negative examples from the minibatch suffer performance drops when their batch size is reduced. BYOL does not use negative examples and we expect it to be more
robust to smaller batch sizes. To empirically verify this hypothesis, we train both BYOL and SimCLR using different
batch sizes from 128 to 4096. To avoid re-tuning other hyperparameters, we average gradients over N consecutive
steps before updating the online network when reducing the batch size by a factor N. The target network is updated
once every N steps, after the update of the online network; we accumulate the N-steps in parallel in our runs.
As shown in Figure 3a, the performance of SimCLR rapidly deteriorates with batch size, likely due to the decrease in
the number of negative examples. In contrast, the performance of BYOL remains stable over a wide range of batch
sizes from 256 to 4096, and only drops for smaller values due to batch normalization layers in the encoder.7
Batch size
Decrease of accuracy from baseline
SimCLR (repro)
(a) Impact of batch size
Transformations set
Decrease of accuracy from baseline
SimCLR (repro)
(b) Impact of progressively removing transformations
Figure 3: Decrease in top-1 accuracy (in % points) of BYOL and our own reproduction of SimCLR at 300
epochs, under linear evaluation on ImageNet.
Image augmentations
Contrastive methods are sensitive to the choice of image augmentations. For instance,
SimCLR does not work well when removing color distortion from its image augmentations. As an explanation,
SimCLR shows that crops of the same image mostly share their color histograms. At the same time, color histograms
vary across images. Therefore, when a contrastive task only relies on random crops as image augmentations, it
can be mostly solved by focusing on color histograms alone. As a result the representation is not incentivized
to retain information beyond color histograms. To prevent that, SimCLR adds color distortion to its set of image
augmentations. Instead, BYOL is incentivized to keep any information captured by the target representation into its
online network, to improve its predictions. Therefore, even if augmented views of a same image share the same
color histogram, BYOL is still incentivized to retain additional features in its representation. For that reason, we
believe that BYOL is more robust to the choice of image augmentations than contrastive methods.
Results presented in Figure 3b support this hypothesis: the performance of BYOL is much less affected than the
performance of SimCLR when removing color distortions from the set of image augmentations (−9.1 accuracy
points for BYOL, −22.2 accuracy points for SimCLR). When image augmentations are reduced to mere random crops,
BYOL still displays good performance (59.4%, i.e. −13.1 points from 72.5% ), while SimCLR loses more than a
third of its performance (40.3%, i.e. −27.6 points from 67.9%). We report additional ablations in Appendix F.3.
Bootstrapping
BYOL uses the projected representation of a target network, whose weights are an exponential
moving average of the weights of the online network, as target for its predictions. This way, the weights of
the target network represent a delayed and more stable version of the weights of the online network. When the
target decay rate is 1, the target network is never updated, and remains at a constant value corresponding to its
initialization. When the target decay rate is 0, the target network is instantaneously updated to the online network at
each step. There is a trade-off between updating the targets too often and updating them too slowly, as illustrated in
Table 5a. Instantaneously updating the target network (τ = 0) destabilizes training, yielding very poor performance
while never updating the target (τ = 1) makes the training stable but prevents iterative improvement, ending with
low-quality ﬁnal representation. All values of the decay rate between 0.9 and 0.999 yield performance above 68.4%
top-1 accuracy at 300 epochs.
7The only dependency on batch size in our training pipeline sits within the batch normalization layers.
Constant random network
Moving average of online
Moving average of online
Moving average of online
Stop gradient of online†
(a) Results for different target modes. †In the stop gradient of
online, τ = τbase = 0 is kept constant throughout training.
Target network
(b) Intermediate variants between BYOL and SimCLR.
Table 5: Ablations with top-1 accuracy (in %) at 300 epochs under linear evaluation on ImageNet.
Ablation to contrastive methods
In this subsection, we recast SimCLR and BYOL using the same formalism to
better understand where the improvement of BYOL over SimCLR comes from. Let us consider the following objective
that extends the InfoNCE objective (see Appendix F.4),
InfoNCEα,β
exp Sθ(vi, vj)
exp Sθ(vi, v′
where α > 0 is a ﬁxed temperature, β ∈ a weighting coefﬁcient, B the batch size, v and v′ are batches
of augmented views where for any batch index i, vi and v′
i are augmented views from the same image; the realvalued function Sθ quantiﬁes pairwise similarity between augmented views. For any augmented view u we denote
zθ(u) ≜fθ(gθ(u)) and zξ(u) ≜fξ(gξ(u)). For given φ and ψ, we consider the normalized dot product
Sθ(u1, u2) =
⟨φ(u1), ψ(u2)⟩
∥φ(u1)∥2 · ∥ψ(u2)∥2
Up to minor details (cf. Appendix F.5), we recover the SimCLR loss with φ(u1) = zθ(u1) (no predictor), ψ(u2) =
zθ(u2) (no target network) and β = 1. We recover the BYOL loss when using a predictor and a target network, i.e.,
φ(u1) = pθ(zθ(u1)) and ψ(u2) = zξ(u2) with β = 0. To evaluate the inﬂuence of the target network, the predictor
and the coefﬁcient β, we perform an ablation over them. Results are presented in Table 5b and more details are
given in Appendix F.4.
The only variant that performs well without negative examples (i.e., with β = 0) is BYOL, using both a bootstrap
target network and a predictor. Adding the negative pairs to BYOL’s loss without re-tuning the temperature parameter
hurts its performance. In Appendix F.4, we show that we can add back negative pairs and still match the performance
of BYOL with proper tuning of the temperature.
Simply adding a target network to SimCLR already improves performance (+1.6 points). This sheds new light on
the use of the target network in MoCo , where the target network is used to provide more negative examples. Here,
we show that by mere stabilization effect, even when using the same number of negative examples, using a target
network is beneﬁcial. Finally, we observe that modifying the architecture of Sθ to include a predictor only mildly
affects the performance of SimCLR.
Network hyperparameters
In Appendix F, we explore how other network parameters may impact BYOL’s
performance. We iterate over multiple weight decays, learning rates, and projector/encoder architectures to observe
that small hyperparameter changes do not drastically alter the ﬁnal score. We note that removing the weight decay
in either BYOL or SimCLR leads to network divergence, emphasizing the need for weight regularization in the
self-supervised setting. Furthermore, we observe that changing the scaling factor in the network initialization 
did not impact the performance (higher than 72% top-1 accuracy).
Relationship with Mean Teacher
Another semi-supervised approach, Mean Teacher (MT) , complements
a supervised loss on few labels with an additional consistency loss. In , this consistency loss is the ℓ2 distance
between the logits from a student network, and those of a temporally averaged version of the student network, called
teacher. Removing the predictor in BYOL results in an unsupervised version of MT with no classiﬁcation loss that
uses image augmentations instead of the original architectural noise (e.g., dropout). This variant of BYOL collapses
(Row 7 of Table 5) which suggests that the additional predictor is critical to prevent collapse in an unsupervised
Importance of a near-optimal predictor
Table 5b already shows the importance of combining a predictor and
a target network: the representation does collapse when either is removed. We further found that we can remove
the target network without collapse by making the predictor near-optimal, either by (i) using an optimal linear
predictor (obtained by linear regression on the current batch) before back-propagating the error through the network
(52.5% top-1 accuracy), or (ii) increasing the learning rate of the predictor (66.5% top-1). By contrast, increasing
the learning rates of both projector and predictor (without target network) yields poor results (≈25% top-1). See
Appendix I for more details. This seems to indicate that keeping the predictor near-optimal at all times is important
to preventing collapse, which may be one of the roles of BYOL’s target network.
Conclusion
We introduced BYOL, a new algorithm for self-supervised learning of image representations. BYOL learns its
representation by predicting previous versions of its outputs, without using negative pairs. We show that BYOL
achieves state-of-the-art results on various benchmarks. In particular, under the linear evaluation protocol on
ImageNet with a ResNet-50 (1×), BYOL achieves a new state of the art and bridges most of the remaining gap
between self-supervised methods and the supervised learning baseline of . Using a ResNet-200 (2×), BYOL
reaches a top-1 accuracy of 79.6% which improves over the previous state of the art (76.8%) while using 30% fewer
parameters.
Nevertheless, BYOL remains dependent on existing sets of augmentations that are speciﬁc to vision applications.
To generalize BYOL to other modalities (e.g., audio, video, text, ...) it is necessary to obtain similarly suitable
augmentations for each of them. Designing such augmentations may require signiﬁcant effort and expertise.
Therefore, automating the search for these augmentations would be an important next step to generalize BYOL to
other modalities.
Broader impact
The presented research should be categorized as research in the ﬁeld of unsupervised learning. This work may
inspire new algorithms, theoretical, and experimental investigation. The algorithm presented here can be used for
many different vision applications and a particular use may have both positive or negative impacts, which is known
as the dual use problem. Besides, as vision datasets could be biased, the representation learned by BYOL could be
susceptible to replicate these biases.
Acknowledgements
The authors would like to thank the following people for their help throughout the process of writing this paper, in
alphabetical order: Aaron van den Oord, Andrew Brock, Jason Ramapuram, Jeffrey De Fauw, Karen Simonyan,
Katrina McKinney, Nathalie Beauguerlange, Olivier Henaff, Oriol Vinyals, Pauline Luc, Razvan Pascanu, Sander
Dieleman, and the DeepMind team. We especially thank Jason Ramapuram and Jeffrey De Fauw, who provided the
JAX SimCLR reproduction used throughout the paper.