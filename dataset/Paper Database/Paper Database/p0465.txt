Problems in Current Text Simpliﬁcation Research:
New Data Can Help
Wei Xu1 and Chris Callison-Burch1 and Courtney Napoles2
1 Computer and Information Science Department
University of Pennsylvania
{xwe, ccb}@seas.upenn.edu
2 Department of Computer Science
Johns Hopkins University
 
Simple Wikipedia has dominated simpliﬁcation research in the past 5 years.
opinion paper, we argue that focusing on
Wikipedia limits simpliﬁcation research. We
back up our arguments with corpus analysis and by highlighting statements that other
researchers have made in the simpliﬁcation
literature.
We introduce a new simpliﬁcation dataset that is a signiﬁcant improvement
over Simple Wikipedia, and present a novel
quantitative-comparative approach to study
the quality of simpliﬁcation data resources.
Introduction
The goal of text simpliﬁcation is to rewrite complex text into simpler language that is easier to understand. Research into this topic has many potential practical applications. For instance, it can provide reading aids for people with disabilities ,
low-literacy , non-native backgrounds or non-expert
knowledge . Text simpliﬁcation may also help
improve the performance of many natural language
processing (NLP) tasks, such as parsing , summarization , semantic role
labeling , information extraction and machine translation , by
transforming long, complex sentences into ones that
are more easily processed.
The Parallel Wikipedia Simpliﬁcation (PWKP)
corpus prepared by Zhu et al. , has become
the benchmark dataset for training and evaluating
automatic text simpliﬁcation systems. An associated
test set of 100 sentences from Wikipedia has been
used for comparing the state-of-the-art approaches.
The collection of simple-complex parallel sentences
sparked a major advance for machine translationbased approaches to simpliﬁcation.
However, we
will show that this dataset is deﬁcient and should be
considered obsolete.
In this opinion paper, we argue that Wikipedia as a
simpliﬁcation data resource is suboptimal for several
reasons: 1) It is prone to automatic sentence alignment errors; 2) It contains a large proportion of inadequate simpliﬁcations; 3) It generalizes poorly to
other text genres. These problems are largely due
to the fact that Simple Wikipedia is an encyclopedia
spontaneously and collaboratively created for “children and adults who are learning English language”
without more speciﬁc guidelines. We quantitatively
illustrate the seriousness of these problems through
manual inspection and statistical analysis.
Our manual inspection reveals that about 50% of
the sentence pairs in the PWKP corpus are not simpliﬁcations. We also introduce a new comparative
approach to simpliﬁcation corpus analysis. In particular, we assemble a new simpliﬁcation corpus of
news articles,1 re-written by professional editors to
meet the readability standards for children at multi-
1This Newsela corpus can be requested following the instructions at: 
Transactions of the Association for Computational Linguistics, vol. 3, pp. 283–297, 2015. Action Editor: Rada Mihalcea.
Submission batch: 12/2014; Revision batch 4/2015; Published 5/2015.
c⃝2015 Association for Computational Linguistics. Distributed under a CC-BY-NC-SA 4.0 license.
Downloaded from by guest on 26 March 2025
Not Aligned
[NORM] The soprano ranges are also written from middle C to A an octave higher,
but sound one octave higher than written.
[SIMP] The xylophone is usually played so that the music sounds an octave higher
than written.
Not Simpler
[NORM] Chile is the longest north-south country in the world, and also claims of
Antarctica as part of its territory.
[SIMP] Chile, which claims a part of the Antarctic continent, is the longest country
[NORM] Death On 1 October 1988, Strauss collapsed while hunting with the Prince
of Thurn and Taxis in the Thurn and Taxis forests, east of Regensburg.
[SIMP] Death On October 1, 1988, Strauß collapsed while hunting with the Prince
of Thurn and Taxis in the Thurn and Taxis forests, east of Regensburg.
[NORM] This article is a list of the 50 U.S. states and the District of Columbia
ordered by population density.
[SIMP] This is a list of the 50 U.S. states, ordered by population density.
Paraphrase
[NORM] In 2002, both Russia and China also had prison populations in excess of 1
[SIMP] In 2002, both Russia and China also had over 1 million people in prison.
Paraphrase
[NORM] All adult Muslims, with exceptions for the inﬁrm, are required to offer
Salat prayers ﬁve times daily.
[SIMP] All adult Muslims should do Salat prayers ﬁve times a day.
Table 1: Example sentence pairs (NORM-SIMP) aligned between English Wikipedia and Simple English
Wikipedia. The breakdown in percentages is obtained through manual examination of 200 randomly sampled sentence pairs in the Parallel Wikipedia Simpliﬁcation (PWKP) corpus.
ple grade levels. This parallel corpus is higher quality and its size is comparable to the PWKP dataset.
It helps us to showcase the limitations of Wikipedia
data in comparison and it provides potential remedies that may improve simpliﬁcation research.
We are not the only researchers to notice problems with Simple Wikipedia. There are many hints
in past publications that reﬂect the inadequacy of
this resource, which we piece together in this paper to support our arguments.
Several different
simpliﬁcation datasets have been proposed ,
but most of these are derived from Wikipedia and
not thoroughly analyzed. Siddharthan ’s excellent survey of text simpliﬁcation research states
that one of the most important questions that needs
to be addressed is “how good is the quality of Simple
English Wikipedia”. To the best of our knowledge,
we are the ﬁrst to systematically quantify the quality of Simple English Wikipedia and directly answer
this question.
We make our argument not as a criticism of others
or ourselves, but as an effort to refocus research directions in the future . We hope to
inspire the creation of higher quality simpliﬁcation
datasets, and to encourage researchers to think critically about existing resources and evaluation methods. We believe this will lead to breakthroughs in
text simpliﬁcation research.
Simple Wikipedia is not that simple
The Parallel Wikipedia Simpliﬁcation (PWKP) corpus contains approximately
108,000 automatically aligned sentence pairs from
cross-linked articles between Simple and Normal
English Wikipedia.
It has become a benchmark
dataset for simpliﬁcation largely because of its
size and availability, and because follow-up papers
 often compare with Zhu et al.’s system
outputs to demonstrate further improvements.
The large quantity of parallel text from Wikipedia
made it possible to build simpliﬁcation systems using statistical machine translation (SMT) technology.
But after the initial success of these ﬁrstgeneration systems, we started to suffer from the
Downloaded from by guest on 26 March 2025
inadequacy of the parallel Wikipedia simpliﬁcation
datasets. There is scattered evidence in the literature.
Bach et al. mentioned they have attempted to use parallel Wikipedia data, but opted to
construct their own corpus of 854 sentences (25%
from New York Times and 75% are from Wikipedia)
with one manual simpliﬁcation per sentence. Woodsend and Lapata showed that rewriting
rules learned from Simple Wikipedia revision histories produce better output compared to the “unavoidably noisy” aligned sentences from Simple-
Normal Wikipedia.
The Woodsend and Lapata
 model, that used quasi-synchronous grammars learned from Wikipedia revision history, left
22% sentences unchanged in the test set. Wubben
et al. found that a phrase-based machine
translation model trained on the PWKP dataset often left the input unchanged, since “much of training data consists of partially equal input and output strings”. Coster and Kauchak constructed
another parallel Wikipedia dataset using a more sophisticated sentence alignment algorithm with an
additional step that ﬁrst aligns paragraphs. They noticed that 27% aligned sentences are identical between simple and normal, and retained them in the
dataset “since not all sentences need to be simpliﬁed
and it is important for any simpliﬁcation algorithm
to be able to handle this case”. However, we will
show that many sentences that need to be simpliﬁed
are not simpliﬁed in the Simple Wikipedia.
We manually examined the Parallel Wikipedia
Simpliﬁcation (PWKP) corpus and found that it is
noisy and half of its sentence pairs are not simpliﬁcations (Table 1). We randomly sampled 200 one-toone sentence pairs from the PWKP dataset (one-tomany sentence splitting cases consist of only 6.1%
of the dataset), and classify each sentence pair into
one of the three categories:
Not Aligned (17%) -
Two sentences have different meanings, or only
have partial content overlap.
Not Simpler (33%)-
The SIMP sentence has the same meaning as
the NORM sentence, but is not simpler.
Real Simpliﬁcation (50%)-
The SIMP sentence has the same meaning as
the NORM sentence, and is simpler. We further breakdown into whether the simpliﬁcation
is due to deletion or paraphrasing.
Table 1 shows a detailed breakdown and representative examples for each category. Although Zhu
et al. and Coster and Kauchak have
provided a simple analysis on the accuracy of sentence alignment, there are some important facts that
cannot be revealed without in-depth manual inspection. The “non-simpliﬁcation” noise in the parallel
Simple-Normal Wikipedia data is a much more serious problem than we all thought. The quality of
“real simpliﬁcations” also varies: some sentences
are simpler by only one word while the rest of sentence is still complex.
The main causes of non-simpliﬁcations and
partial-simpliﬁcations in the parallel Wikipedia corpus include: 1) The Simple Wikipedia was created
by volunteer contributors with no speciﬁc objective;
2) Very rarely are the simple articles complete
re-writes of the regular articles in Wikipedia ,
which makes automatic
sentence alignment errors worse; 3) As an encyclopedia, Wikipedia contains many difﬁcult sentences
with complex terminology. The difﬁculty of sentence alignment between Normal-Simple Wikipedia
is highlighted by a recent study by Hwang et al.
 that achieves state-of-the-art performance
of 0.712 maximum F1 score (over the precisionrecall curve) by combining Wiktionary-based and
dependency-parse-based sentence similarities. And
in fact, even the simple side of the PWKP corpus
contains an extensive English vocabulary of 78,009
unique words. 6,669 of these words do not exist in
the normal side (Table 2). Below is a sentence from
an article entitled “Photolithography" in Simple
Wikipedia:
Microphototolithography is the use of photolithography to transfer geometric shapes on a
photomask to the surface of a semiconductor wafer
for making integrated circuits.
We should use the PWKP corpus with caution and
consider other alternative parallel simpliﬁcation corpora. Alternatives could come from Wikipedia (but
better aligned and selected) or from manual simpli-
ﬁcation of other domains, such as newswire. In the
Downloaded from by guest on 26 March 2025
#words (avg. freq)
95,111 (23.91)
78,009 (23.88)
6,669(1.31)
23,771 (1.42)
The vocabulary size of the Parallel
Wikipedia Simpliﬁcation (PWKP) corpus and the
vocabulary difference between its normal and simple sides (as a 2×2 matrix). Only words consisting
of the 26 English letters are counted.
next section, we will present a corpus of news articles simpliﬁed by professional editors, called the
Newsela corpus. We perform a comparative corpus
analysis of the Newsela corpus versus the PWKP
corpus to further illustrate concerns about PWKP’s
What the Newsela corpus teaches us
To study how professional editors conduct text simpliﬁcation, we have assembled a new simpliﬁcation
dataset that consists of 1,130 news articles. Each article has been re-written 4 times for children at different grade levels by editors at Newsela2, a company that produces reading materials for pre-college
classroom use. We use Simp-4 to denote the most
simpliﬁed level and Simp-1 to denote the least simpliﬁed level.
This data forms a parallel corpus,
where we can align sentences at different reading
levels, as shown in Table 3.
Unlike Simple Wikipedia, which was created
without a well-deﬁned objective, Newsela is meant
to help teachers prepare curricula that match the English language skills required at each grade level. It
is motivated by the Common Core Standards in the United States. All the Newsela articles are grounded in the Lexile3 readability score,
which is widely used to measure text complexity and
assess students’ reading ability.
Manual examination of Newsela corpus
We conducted a manual examination of the Newsela
data similar to the one for Wikipedia data in Table 1.
The breakdown of aligned sentence pairs between
different versions in Newsela is shown in Figure 1.
2 
3 
Figure 1: Manual classiﬁcation of aligned sentence
pairs from the Newsela corpus. We categorize randomly sampled 50 sentence pairs drawn from the
Original-Simp2 and 50 sentences from the Original-
It is based on 50 randomly selected sentence pairs
and shows much more reliable simpliﬁcation than
the Wikipedia data.
We designed a sentence alignment algorithm for
the Newsela corpus based on Jaccard similarity . We ﬁrst align each sentence in the simpler version (e.g. s1 in Simp-3) to the sentence in the
immediate more complex version (e.g. s2 in Simp-
2) of the highest similarity score. We compute the
similarity based on overlapping word lemmas:4
Sim(s1, s2) = |Lemmas(s1) ∩Lemmas(s2)|
|Lemmas(s1) ∪Lemmas(s2)|
We then align sentences into groups across all 5 versions for each article. For cases where no sentence
splitting is involved, we discard any sentence pairs
with a similarity smaller than 0.40. If splitting occurs, we set the similarity threshold to 0.20 instead.
Newsela’s professional editors produce simpliﬁcations with noticeably higher quality than
Wikipedia’s simpliﬁcations. Compared to sentence
alignment for Normal-Simple Wikipedia, automatically aligning Newsela is more straightforward and
The better correspondence between the
simpliﬁed and complex articles and the availability
of multiple simpliﬁed versions in the Newsela data
also contribute to the accuracy of sentence alignment.
4We use the WordNet lemmatization in the NLTK package:
 
Downloaded from by guest on 26 March 2025
Grade Level
Lexile Score
Slightly more fourth-graders nationwide are reading proﬁciently compared
with a decade ago, but only a third of them are now reading well, according to
a new report.
Fourth-graders in most states are better readers than they were a decade
ago. But only a third of them actually are able to read well, according to a
new report.
Fourth-graders in most states are better readers than they were a decade ago.
But only a third of them actually are able to read well, according to a new
Most fourth-graders are better readers than they were 10 years ago. But few of
them can actually read well.
Fourth-graders are better readers than 10 years ago. But few of them read well.
Table 3: Example of sentences written at multiple levels of text complexity from the Newsela data set. The
Lexile readability score and grade level apply to the whole article rather than individual sentences, so the
same sentences may receive different scores, e.g. the above sentences for the 6th and 7th grades. The bold
font highlights the parts of sentence that are different from the adjacent version(s).
Total #sents
Total #tokens
Avg #sents per doc
Avg #words per doc
Avg #words per sent
Avg #chars per word
Basic statistics of the Newsela Simpliﬁcation corpus vs. the Parallel Wikipedia Simpliﬁcation
(PWKP) corpus. The Newsela corpus consists of 1130 articles with original and 4 simpliﬁed versions each.
Simp-1 is of the least simpliﬁed level, while Simp-4 is the most simpliﬁed. The numbers marked by * are
slightly different from previously reported, because of the use of different tokenizers.
#words (avg. freq)
**39,046 (28.31)
33,272 (28.64)
29,569 (30.09)
24,468 (31.17)
20,432 (31.45)
724 (1.19)
815 (1.25)
720 (1.32)
*583 (1.33)
6,498 (1.38)
618 (1.08)
604 (1.15)
521 (1.21)
10,292 (1.67)
4,321 (1.32)
536 (1.13)
475 (1.16)
15,298 (2.14)
9,408 (1.79)
5,637 (1.46)
533 (1.14)
**19,197 (2.60)
13,361 (2.24)
9,612 (1.87)
4,569 (1.40)
Table 5: This table shows the vocabulary changes between different levels of simpliﬁcation in the Newsela
corpus (as a 5×5 matrix). Each cell shows the number of unique word types that appear in the corpus listed
in the column but do not appear in the corpus listed in the row. We also list the average frequency of those
vocabulary items. For example, in the cell marked *, the Simp-4 version contains 583 unique words that
do not appear in the Original version. By comparing the cells marked **, we see about half of the words
(19,197 out of 39,046) in the Original version are not in the Simp-4 version. Most of the vocabulary that is
removed consists of low-frequency words (with an average frequency of 2.6 in the Original).
Downloaded from by guest on 26 March 2025
Vocabulary statistics
Table 4 shows the basic statistics of the Newsela corpus and the PWKP corpus. They are clearly different. Compared to the Newsela data, the Wikipedia
corpus contains remarkably longer (more complex)
words and the difference of sentence length before
and after simpliﬁcation is much smaller. We use the
Penn Treebank tokenizer in the Moses package.5
Tables 2 and 5 show the vocabulary statistics
and the vocabulary difference matrix of the PWKP
and Newsela corpus.
While the vocabulary size
of the PWKP corpus drops only 18% from 95,111
unique words to 78,009, the vocabulary size of the
Newsela corpus is reduced dramatically by 50.8%
from 39,046 to 19,197 words at its most simpliﬁed
level (Simp-4). Moreover, in the Newsela data, only
several hundred words that occur in the simpler version do not occur in the more complex version. The
words introduced are often abbreviations (“National
Hurricane Center” →“NHC”), less formal words
(“unscrupulous” →“crooked”) and shortened words
(“chimpanzee” →“chimp”). This implies a more
complete and precise degree of simpliﬁcation in the
Newsela than the PWKP dataset.
Log-odds-ratio analysis of words
In this section, we visualize the differences in the
topics and degree of simpliﬁcation between the Simple Wikipedia and the Newsela corpus. To do this,
we employ the log-odds-ratio informative Dirichlet
prior method of Monroe et al. to ﬁnd words
and punctuation marks that are statistically overrepresented in the simpliﬁed text compared to the original text. The method measures each token by the
z-score of its log-odds-ratio as:
It uses a background corpus when calculating the
log-odds-ratio δt for token t, and controls for its variance σ2. Therefore it is capable of detecting differences even in very frequent tokens. Other methods used to discover word associations, such as mu-
5 
mosesdecoder/blob/master/scripts/
tokenizer/tokenizer.perl
tual information, log likelihood ratio, t-test and chisquare, often have problems with frequent words
 . We choose the Monroe et al.
 method because many function words and
punctuations are very frequent and play important
roles in text simpliﬁcation.
The log-odds-ratio δ(i−j)
for token t estimates the
difference of the frequency of token t between two
text sets i and j as:
ni + α0 − - Normal
Punctuation
, "– ; ’ ( )
Determiner/Pronoun
which we an such who i that a whose
which whom
Contraction
Conjunction
and while although
and although while
Prepositions
of as including with according by
among in despite
as with following to of within upon
currently approximately initially
primarily subsequently typically thus
percent director data research decades
industry policy development state
decade status university residents
ﬁlm commune footballer
pays-de-la-loire walloon links
midﬁelder defender goalkeeper
federal potential recent executive
northern northwestern southwestern
external due numerous undated various
advocates based access
referred derived established situated
considered consists regarded having
Table 6: Top 50 tokens associated with the complex text, computed using the Monroe et al. method.
Bold words are shared by the complex version of Newsela and the complex version of Wikipedia.
Linguistic class
Newsela - Simp4
Wikipedia (PWKP) - Simple
Punctuation
Determiner/Pronoun
they it he she them lot
it he they lot this she
Conjunction
also not there too about very now then
about very there
people money scientists government
things countries rules problems group
movie people northwest north region
loire player websites southwest movies
football things
many important big new used
big biggest famous different important
is are can will make get were wants
was called help hurt be made like stop
want works do live
found is made called started pays said
was got are like get can means says has
went comes make put used
Table 7: Top 50 tokens associated with the simpliﬁed text.
odds-ratio
odds-ratio
approximately
Table 8: Frequency of example words from Table 6. These complex words are reduced at a much greater
rate in the simpliﬁed Newsela than they are in the Simple English Wikipedia. A smaller odds ratio indicates
greater reduction.
Downloaded from by guest on 26 March 2025
Newsela - Original
Wikipedia (PWKP) - Normal
Newsela - Simp4
Wikipedia (PWKP) - Simple
PP(of) →IN NP
PP(as) →IN NP
S(is) →NP VP .
NP(it) →PRP
WHNP(which) →WDT
PP(of) →IN NP
NP(they) →PRP
S(is) →NP VP .
SBAR(which) →WHNP S
VP(born) →VBN NP NP PP
S(are) →NP VP .
S(was) →NP VP .
PP(to) →TO NP
WHNP(which) →WDT
S(was) →NP VP .
NP(he) →PRP
NP(percent) →CD NN
PP(to) →TO NP
NP(people) →NNS
NP(they) →PRP
WHNP(that) →WDT
NP(municipality) →DT JJ NN
VP(is) →VBZ NP
NP(player) →DT JJ JJ NN NN
SBAR(that) →WHNP S
FRAG(-) →ADJP :
NP(he) →PRP
S(are) →NP VP .
PP(with) →IN NP
FRAG(-) →FRAG : FRAG
S(were) →NP VP .
NP(movie) →DT NN
PP(according) →VBG PP
NP()) →NNP NNP NNP
NP(it) →PRP
S(has) →NP VP .
NP(percent) →NP PP
NP(ﬁlm) →DT NN
S(can) →NP VP .
VP(called) →VBN NP
NP(we) →PRP
NP(footballer) →DT JJ JJ NN
S(will) →NP VP .
VP(is) →VBZ PP
PP(including) →VBG NP
NP(footballer) →NP SBAR
ADVP(also) →RB
VP(made) →VBN PP
SBAR(who) →WHNP S
ADVP(currently) →RB
S(have) →NP VP .
VP(said) →VBD SBAR
SBAR(as) →IN S
VP(born) →VBN NP NP
S(could) →NP VP .
VP(has) →VBZ NP
WHNP(who) →WP
ADVP(initially) →RB
S(said) →NP VP .
VP(is) →VBZ NP
PP(with) →IN NP
S(has) →NP VP .
NP(this) →DT
PP(as) →IN NP
WHPP(of) →IN WHNP
NP(people) →JJ NNS
VP(was) →VBD NP
NP(director) →NP PP
SBAR(although) →IN S
NP(money) →NN
NP(people) →NNS
PP(by) →IN NP
ADVP(primarily) →RB
NP(government) →DT NN
NP(lot) →DT NN
S(has) →VP
S(links) →NP VP .
S(do) →NP VP .
NP(season) →NN CD
PP(in) →IN NP
VP(links) →VBZ NP
NP(scientists) →NNS
S(can) →NP VP .
SBAR(while) →IN S
PP(following) →VBG NP
VP(called) →VBN NP
VP(is) →VBZ VP
PP(as) →JJ IN NP
ADVP(subsequently) →RB
S(had) →NP VP .
SBAR(because) →IN S
PRN(–) →: NP :
SBAR(which) →WHNP S
S(says) →NP VP .
VP(are) →VBP NP
S(’s) →NP VP
SBAR(while) →IN S
S(would) →NP VP .
NP(player) →DT JJ NN NN
S(said) →” S , ” NP VP .
S(plays) →ADVP VP
S(say) →NP VP .
NP(there) →EX
PP(at) →IN NP
PP(within) →IN NP
S(works) →NP VP .
NP(lot) →NP PP
PP(among) →IN NP
PP(by) →IN NP
S(may) →NP VP .
NP(websites) →JJ NNS
SBAR(although) →IN S
SBAR(of) →WHNP S
S(did) →NP VP .
PP(like) →IN NP
VP(said) →VBD NP
S(is) →S : S .
S(think) →NP VP .
S(started) →NP VP .
Table 9: Top 30 syntax patterns associated with the complex text (left) and simpliﬁed text (right). Bold
patterns are the top patterns shared by Newsela and Wikipedia.
that are retained in Simple Wikipedia indicates
the incompleteness of simpliﬁcation in the Simple Wikipedia.
The dramatic frequency decrease
of words like “which” and “advocates” in Newsela
shows the consistent quality from professional simpliﬁcations. Wikipedia has good coverage on certain
words, such as “approximately”, because of its large
Log-odds-ratio analysis of syntax patterns
We can also reveal the syntax patterns that are most
strongly associated with simple text versus complex text using the log-odds-ratio technique.
Table 9 shows syntax patterns that represent “parent
node (head word) →children node(s)" structures
from a constituency parse tree.
To extract theses
patterns we parsed our corpus with the Stanford
Parser and applied its
built-in head word identiﬁer from Collins .
Both the Newsela and Wikipedia corpora exhibit
syntactic differences that are intuitive and interesting. However, as with word frequency (Table 8),
complex syntactic patterns are retained more often
in Wikipedia’s simpliﬁcations than in Newsela’s.
In order to show interesting syntax patterns
in the Wikipedia parallel data for Table 9, we
ﬁrst had to discard 3613 sentences in PWKP that
contain both "is a commune" and "France". As the
word-level analysis in Tables 6 and 7 hints, there is
an exceeding number of sentences about communes
in France in the PWKP corpus, such as the sentence
pair below:
La Couture
Pas-de-Calais
department
Nord-Pas-de-Calais region of France .
[SIMP] La Couture, Pas-de-Calais is a commune.
It is found in the region Nord-Pas-de-Calais in the
Pas-de-Calais department in the north of France.
This is a template sentence from a stub geographic article and its deterministic simpliﬁcation.
The inﬂuence of this template sentence is more over-
Downloaded from by guest on 26 March 2025
whelming in the syntax-level analysis than in the
word-level analysis —- about 1/3 of the top 30 syntax patterns would be related to these sentence pairs
if they were not discarded.
Document-level compression
There are few publicly accessible document-level
parallel simpliﬁcation corpora . The Newsela corpus will enable more
research on document-level simpliﬁcation, such
as anaphora choice , content selection , and discourse relation preservation .
document-level simpliﬁcation. Woodsend and Lapata developed a model that simpliﬁes
Wikipedia articles while selecting their most important content. However, they could only use Simple
Wikipedia in very limited ways. They noted that
Simple Wikipedia is “less mature” with many articles that are just “stubs, comprising a single paragraph of just one or two sentences”. We quantify
their observation in Figure 2, plotting the documentlevel compression ratio of Simple vs.
Wikipedia articles.
The compression ratio is the
ratio of the number of characters between each
simple-complex article pair. In the plot, we use all
60 thousand article pairs from the Simple-Normal
Wikipedia collected by Kauchak in May
2011. The overall compression ratio is skewed towards almost 0. For comparison, we also plot the
ratio between the simplest version (Simp-4) and the
original version (Original) of the news articles in the
Newsela corpus. The Newsela corpus has a much
more reasonable compression ratio and is therefore
likely to be more suitable for studying documentlevel simpliﬁcation.
Analysis of discourse connectives
Although discourse is known to affect readability,
the relation between discourse and text simpliﬁcation is still under-studied with the use of statistical
methods . Text simpliﬁcation
often involves splitting one sentence into multiple
sentences, which is likely to require discourse-level
changes such as introducing explicit rhetorical relations. However, previous research that uses Simple-
Normal Wikipedia largely focuses on sentence-level
transformation, without taking large discourse structure into account.
Figure 3: A radar chart that visualizes the odds ratio (radius axis) of discourse connectives in simple side vs.
complex side.
An odds ratio larger
than 1 indicates the word is more likely to occur
in the simpliﬁed text than in the complex text, and
vice versa.
Simple cue words (in the shaded region), except “hence”, are more likely to be added
during Newsela’s simpliﬁcation process than in
Wikipedia’s. Complex conjunction connectives (in
the unshaded region) are more likely to be retained
in Wikipedia’s simpliﬁcations than in Newsela’s.
To preserve the rhetorical structure, Siddharthan
 proposed to introduce cue words when
simplifying various conjoined clauses. We perform
an analysis on discourse connectives that are relevant to readability as suggested by Siddharthan
 . Figure 3 presents the odds ratios of simple cue words and complex conjunction connectives.
The odds radios are computed for Newsela between
the Original and Simp-4 versions, and for Wikipedia
between Normal and Simple documents collected
by Kauchak . It suggests that Newsela exhibits a more complete degree of simpliﬁcation than
Wikipedia, and that it may be able to enable more
computational studies of the role of discourse in text
simpliﬁcation in the future.
Downloaded from by guest on 26 March 2025
Compression Ratio
Compression Ratio
Figure 2: Distribution of document-level compression ratio, displayed as a histogram smoothed by kernel
density estimation. The Newsela corpus is more normally distributed, suggesting more consistent quality.
Newsela’s quality is better than Wikipedia
Overall, we have shown that the professional simpliﬁcation of Newsela is more rigorous and more
consistent than Simple English Wikipedia. The language and content also differ between the encyclopedia and news domains. They are not exchangeable
in developing nor in evaluating simpliﬁcation systems. In the next section, we will review the evaluation methodology used in recent research, discuss its
shortcomings and propose alternative evaluations.
Evaluation of simpliﬁcation systems
With the popularity of parallel Wikipedia data
in simpliﬁcation research,
most state-of-the-art
systems evaluate on simplifying sentences from
Wikipedia. All simpliﬁcation systems published in
the ACL, NAACL, EACL, COLING and EMNLP
main conferences since Zhu’s 2010 work compared
solely on the same test set that consists of only
100 sentences from Wikipedia, except one paper
that additionally experimented with 5 short news
summaries. The most widely practiced evaluation
methodology is to have human judges rate on grammaticality (or ﬂuency), simplicity, and adequacy (or
meaning preservation) on a 5-point Likert scale.
Such evaluation is insufﬁcient to measure 1) the
practical value of a system to a speciﬁc target reader
population and 2) the performance of individual
simpliﬁcation components: sentence splitting, deletion and paraphrasing. Although the inadequacy of
text simpliﬁcation evaluations has been discussed
before , we focus on these two
common deﬁciencies and suggest two future directions.
Targeting speciﬁc audiences
Simpliﬁcation has many subtleties, since what constitutes simpliﬁcation for one type of user may not
be appropriate for another. Many researchers have
studied simpliﬁcation in the context of different audiences. However, most recent automatic simpliﬁcation systems are developed and evaluated with little
consideration of target reader population. There is
one attempt by Angrosh et al. who evaluate
their system by asking non-native speakers comprehension questions. They conducted an English vocabulary size test to categorize the users into different levels of language skills.
The Newsela corpus allows us to target children at
different grade levels. From the application point of
view, making knowledge accessible to all children is
an important yet challenging part of education . From the technical point of view, reading grade level is a clearly
deﬁned objective for both simpliﬁcation systems and
human annotators. Once there is a well-deﬁned objective, with constraints such as vocabulary size and
sentence length, it is easier to fairly compare different systems. Newsela provides human simpliﬁcation
Downloaded from by guest on 26 March 2025
at different grade levels and reading comprehension
quizzes alongside each article.
In addition, readability is widely studied and can
be automatically estimated . Although existing readability metrics assume
text is well-formed, they can potentially be used in
combination with text quality metrics to evaluate simpliﬁcations. They can also be used to aid humans in the
creation of reference simpliﬁcations.
Evaluating sub-tasks separately
It is widely accepted that sentence simpliﬁcation involves three different elements: splitting, deletion
and paraphrasing . Splitting breaks a long sentence into
a few short sentences to achieve better readability.
Deletion reduces the complexity by removing unimportant parts of a sentence. Paraphrasing rewrites
text into a simpler version via reordering, substitution and occasionally expansion.
Most state-of-the-art systems consist of all or a
subset of these three components. However, the popular human evaluation criteria (grammaticality, simplicity and adequacy) do not explain which components in a system are good or bad. More importantly,
deletion may be unfairly penalized since shorter output tends to result in lower adequacy judgements
 .
We therefore advocate for a more informative
evaluation that separates out each sub-task. We believe this will lead to more easily quantiﬁable metrics and possibly the development of automatic metrics. For example, early work shows potential use
of precision and recall to evaluate splitting and deletion
 .
Several studies also have investigated various metrics for evaluating sentence paraphrasing . We introduced a new, high-quality corpus of
professionally simpliﬁed news articles, Newsela, as
an alternative resource, that allowed us to demonstrate Simple Wikipedia’s inadequacies in comparison. We further discussed problems with current
simpliﬁcation evaluation methodology and proposed
potential improvements.
Our goal for this opinion paper is to stimulate
progress in text simpliﬁcation research. Simple English Wikipedia played a vital role in inspiring simpliﬁcation approaches based on statistical machine
translation.
However, it has so many drawbacks
that we recommend the community to drop it as the
standard benchmark set for simpliﬁcation. Other resources like the Newsela corpus are superior, since
they provide a more consistent level of quality, target a particular audience, and approach the size of
parallel Simple-Normal English Wikipedia. We believe that simpliﬁcation is an important area of research that has the potential for broader impact beyond NLP research. But we must ﬁrst adopt appropriate data sets and research methodologies.
Researchers can request the Newsela data following the instructions at: 
Acknowledgments
The authors would like to thank Dan Cogan-Drew,
Jennifer Coogan, and Kieran Sobel from Newsela
for creating their data and generously sharing it with
We also thank action editor Rada Mihalcea
and three anonymous reviewers for their thoughtful
comments, and Ani Nenkova, Alan Ritter and Maxine Eskenazi for valuable discussions.
This material is based on research sponsored by
the NSF under grant IIS-1430651. The views and
conclusions contained in this publication are those
of the authors and should not be interpreted as representing ofﬁcial policies or endorsements of the NSF
or the U.S. Government.
Downloaded from by guest on 26 March 2025