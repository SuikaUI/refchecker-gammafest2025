AttnGAN: Fine-Grained Text to Image Generation
with Attentional Generative Adversarial Networks
Tao Xu∗1, Pengchuan Zhang2, Qiuyuan Huang2,
Han Zhang3, Zhe Gan4, Xiaolei Huang1, Xiaodong He2
1Lehigh University
2Microsoft Research
3Rutgers University
4Duke University
{tax313, xih206}@lehigh.edu, {penzhan, qihua, xiaohe}@microsoft.com
 , 
In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven,
multi-stage reﬁnement for ﬁne-grained text-to-image generation. With a novel attentional generative network, the AttnGAN can synthesize ﬁne-grained details at different subregions of the image by paying attentions to the relevant
words in the natural language description. In addition, a
deep attentional multimodal similarity model is proposed to
compute a ﬁne-grained image-text matching loss for training the generator. The proposed AttnGAN signiﬁcantly outperforms the previous state of the art, boosting the best reported inception score by 14.14% on the CUB dataset and
170.25% on the more challenging COCO dataset. A detailed analysis is also performed by visualizing the attention layers of the AttnGAN. It for the ﬁrst time shows that
the layered attentional GAN is able to automatically select
the condition at the word level for generating different parts
of the image.
1. Introduction
Automatically generating images according to natural
language descriptions is a fundamental problem in many
applications, such as art generation and computer-aided design. It also drives research progress in multimodal learning
and inference across vision and language, which is one of
the most active research areas in recent years 
Most recently proposed text-to-image synthesis methods
are based on Generative Adversarial Networks (GANs) .
A commonly used approach is to encode the whole text description into a global sentence vector as the condition for
GAN-based image generation . Although
∗work was performed when was an intern with Microsoft Research
this bird is red with white and has a very short beak
Figure 1. Example results of the proposed AttnGAN. The ﬁrst row
gives the low-to-high resolution images generated by G0, G1 and
G2 of the AttnGAN; the second and third row shows the top-5
most attended words by F attn
and F attn
of the AttnGAN, respectively. Here, images of G0 and G1 are bilinearly upsampled
to have the same size as that of G2 for better visualization.
impressive results have been presented, conditioning GAN
only on the global sentence vector lacks important ﬁnegrained information at the word level, and prevents the generation of high quality images. This problem becomes even
more severe when generating complex scenes such as those
in the COCO dataset .
To address this issue, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attentiondriven, multi-stage reﬁnement for ﬁne-grained text-toimage generation. The overall architecture of the AttnGAN
is illustrated in Figure 2. The model consists of two novel
components. The ﬁrst component is an attentional gener-
 
ative network, in which an attention mechanism is developed for the generator to draw different sub-regions of the
image by focusing on words that are most relevant to the
sub-region being drawn (see Figure 1). More speciﬁcally,
besides encoding the natural language description into a
global sentence vector, each word in the sentence is also
encoded into a word vector. The generative network utilizes the global sentence vector to generate a low-resolution
image in the ﬁrst stage. In the following stages, it uses
the image vector in each sub-region to query word vectors
by using an attention layer to form a word-context vector.
It then combines the regional image vector and the corresponding word-context vector to form a multimodal context
vector, based on which the model generates new image features in the surrounding sub-regions. This effectively yields
a higher resolution picture with more details at each stage.
The other component in the AttnGAN is a Deep Attentional
Multimodal Similarity Model (DAMSM). With an attention
mechanism, the DAMSM is able to compute the similarity
between the generated image and the sentence using both
the global sentence level information and the ﬁne-grained
word level information. Thus, the DAMSM provides an additional ﬁne-grained image-text matching loss for training
the generator.
The contribution of our method is threefold.
Attentional Generative Adversarial Network is proposed
for synthesizing images from text descriptions.
Specifically, two novel components are proposed in the AttnGAN, including the attentional generative network and
the DAMSM. (ii) Comprehensive study is carried out to empirically evaluate the proposed AttnGAN. Experimental results show that the AttnGAN signiﬁcantly outperforms previous state-of-the-art GAN models. (iii) A detailed analysis is performed through visualizing the attention layers of
the AttnGAN. For the ﬁrst time, it is demonstrated that the
layered conditional GAN is able to automatically attend to
relevant words to form the condition for image generation.
2. Related Work
Generating high resolution images from text descriptions, though very challenging, is important for many practical applications such as art generation and computeraided design. Recently, great progress has been achieved
in this direction with the emergence of deep generative
models . Mansimov et al. built the align-
DRAW model, extending the Deep Recurrent Attention
Writer (DRAW) to iteratively draw image patches while
attending to the relevant words in the caption.
et al. proposed an approximate Langevin approach
to generate images from captions. Reed et al. used
conditional PixelCNN to synthesize images from text
with a multi-scale model structure. Compared with other
deep generative models, Generative Adversarial Networks
(GANs) have shown great performance for generating
sharper samples . Reed et al. ﬁrst
showed that the conditional GAN was capable of synthesizing plausible images from text descriptions. Their followup work also demonstrated that GAN was able to generate better samples by incorporating additional conditions
(e.g., object locations). Zhang et al. stacked several GANs for text-to-image synthesis and used different
GANs to generate images of different sizes. However, all
of their GANs are conditioned on the global sentence vector, missing ﬁne-grained word level information for image
generation.
The attention mechanism has recently become an integral part of sequence transduction models. It has been successfully used in modeling multi-level dependencies in image captioning , image question answering and
machine translation . Vaswani et al. also demonstrated that machine translation models could achieve stateof-the-art results by solely using an attention model.
spite of these progress, the attention mechanism has not
been explored in GANs for text-to-image synthesis yet. It is
worth mentioning that the alignDRAW also used LAP-
GAN to scale the image to a higher resolution. However, the GAN in their framework was only utilized as a
post-processing step without attention. To our knowledge,
the proposed AttnGAN for the ﬁrst time develops an attention mechanism that enables GANs to generate ﬁne-grained
high quality images via multi-level (e.g., word level and
sentence level) conditioning.
3. Attentional Generative Adversarial Network
As shown in Figure 2, the proposed Attentional Generative Adversarial Network (AttnGAN) has two novel components: the attentional generative network and the deep
attentional multimodal similarity model. We will elaborate
each of them in the rest of this section.
3.1. Attentional Generative Network
Current GAN-based models for text-to-image generation typically encode the whole-sentence
text description into a single vector as the condition for image generation, but lack ﬁne-grained word level information. In this section, we propose a novel attention model
that enables the generative network to draw different subregions of the image conditioned on words that are most
relevant to those sub-regions.
As shown in Figure 2, the proposed attentional generative network has m generators (G0, G1, ..., Gm−1), which
take the hidden states (h0, h1, ..., hm−1) as input and generate images of small-to-large scales (ˆx0, ˆx1, ..., ˆxm−1).
Attentional Generative Network
Attention models
Local image
Deep Attentional Multimodal Similarity Model (DAMSM)
Upsampling
FC with reshape
this bird is red with
white and has a
very short beak
Figure 2. The architecture of the proposed AttnGAN. Each attention model automatically retrieves the conditions (i.e., the most relevant
word vectors) for generating different sub-regions of the image; the DAMSM provides the ﬁne-grained image-text matching loss for the
generative network.
Speciﬁcally,
h0 = F0(z, F ca(e));
hi = Fi(hi−1, F attn
(e, hi−1)) for i = 1, 2, ..., m −1;
ˆxi = Gi(hi).
Here, z is a noise vector usually sampled from a standard
normal distribution. e is a global sentence vector, and e is
the matrix of word vectors. F ca represents the Conditioning
Augmentation that converts the sentence vector e to the
conditioning vector. F attn
is the proposed attention model
at the ith stage of the AttnGAN. F ca, F attn
, Fi, and Gi are
modeled as neural networks.
The attention model F attn(e, h) has two inputs: the
word features e ∈RD×T and the image features from the
previous hidden layer h ∈R ˆ
D×N. The word features are
ﬁrst converted into the common semantic space of the image features by adding a new perceptron layer, i.e., e′ = Ue,
where U ∈R ˆ
D×D. Then, a word-context vector is computed for each sub-region of the image based on its hidden
features h (query). Each column of h is a feature vector of
a sub-region of the image. For the jth sub-region, its wordcontext vector is a dynamic representation of word vectors
relevant to hj, which is calculated by
i, where βj,i =
k=0 exp(s′
i, and βj,i indicates the weight the model attends
to the ith word when generating the jth sub-region of the
image. We then donate the word-context matrix for image
feature set h by F attn(e, h) = (c0, c1, ..., cN−1) ∈R ˆ
Finally, image features and the corresponding word-context
features are combined to generate images at the next stage.
To generate realistic images with multiple levels (i.e.,
sentence level and word level) of conditions, the ﬁnal objective function of the attentional generative network is deﬁned
L = LG + λLDAMSM, where LG =
Here, λ is a hyperparameter to balance the two terms of
Eq. (3). The ﬁrst term is the GAN loss that jointly approximates conditional and unconditional distributions . At
the ith stage of the AttnGAN, the generator Gi has a corresponding discriminator Di. The adversarial loss for Gi is
2 Eˆxi∼pGi [log(Di(ˆxi)]
unconditional loss
2 Eˆxi∼pGi [log(Di(ˆxi, e)]
conditional loss
where the unconditional loss determines whether the image
is real or fake while the conditional loss determines whether
the image and the sentence match or not.
Alternately to the training of Gi, each discriminator Di
is trained to classify the input into the class of real or fake
by minimizing the cross-entropy loss deﬁned by
2 Exi∼pdatai [log Di(xi)] −1
2 Eˆxi∼pGi [log(1 −Di(ˆxi)]
unconditional loss
2 Exi∼pdatai [log Di(xi, e)] −1
2 Eˆxi∼pGi [log(1 −Di(ˆxi, e)]
conditional loss
where xi is from the true image distribution pdatai at the
ith scale, and ˆxi is from the model distribution pGi at the
same scale. Discriminators of the AttnGAN are structurally
disjoint, so they can be trained in parallel and each of them
focuses on a single image scale.
The second term of Eq. (3), LDAMSM, is a word level
ﬁne-grained image-text matching loss computed by the
DAMSM, which will be elaborated in Subsection 3.2.
3.2. Deep Attentional Multimodal Similarity Model
The DAMSM learns two neural networks that map subregions of the image and words of the sentence to a common
semantic space, thus measures the image-text similarity at
the word level to compute a ﬁne-grained loss for image generation.
The text encoder is a bi-directional Long Short-Term
Memory (LSTM) that extracts semantic vectors from
the text description. In the bi-directional LSTM, each word
corresponds to two hidden states, one for each direction.
Thus, we concatenate its two hidden states to represent the
semantic meaning of a word.
The feature matrix of all
words is indicated by e ∈RD×T . Its ith column ei is the
feature vector for the ith word. D is the dimension of the
word vector and T is the number of words. Meanwhile, the
last hidden states of the bi-directional LSTM are concatenated to be the global sentence vector, denoted by e ∈RD.
The image encoder is a Convolutional Neural Network
(CNN) that maps images to semantic vectors. The intermediate layers of the CNN learn local features of different
sub-regions of the image, while the later layers learn global
features of the image.
More speciﬁcally, our image encoder is built upon the Inception-v3 model pretrained
on ImageNet . We ﬁrst rescale the input image to be
299×299 pixels. And then, we extract the local feature matrix f ∈R768×289 (reshaped from 768×17×17) from the
“mixed 6e” layer of Inception-v3. Each column of f is the
feature vector of a sub-region of the image. 768 is the dimension of the local feature vector, and 289 is the number
of sub-regions in the image. Meanwhile, the global feature
vector f ∈R2048 is extracted from the last average pooling
layer of Inception-v3. Finally, we convert the image features to a common semantic space of text features by adding
a perceptron layer:
where v ∈RD×289 and its ith column vi is the visual feature vector for the ith sub-region of the image; and v ∈RD
is the global vector for the whole image. D is the dimension
of the multimodal (i.e., image and text modalities) feature
space. For efﬁciency, all parameters in layers built from the
Inception-v3 model are ﬁxed, and the parameters in newly
added layers are jointly learned with the rest of the network.
The attention-driven image-text matching score
designed to measure the matching of an image-sentence pair
based on an attention model between the image and the text.
We ﬁrst calculate the similarity matrix for all possible
pairs of words in the sentence and sub-regions in the image
where s ∈RT ×289 and si,j is the dot-product similarity
between the ith word of the sentence and the jth sub-region
of the image. We ﬁnd that it is beneﬁcial to normalize the
similarity matrix as follows
k=0 exp(sk,j)
Then, we build an attention model to compute a regioncontext vector for each word (query). The region-context
vector ci is a dynamic representation of the image’s subregions related to the ith word of the sentence. It is computed as the weighted sum over all regional visual vectors,
αjvj, where αj =
exp(γ1si,j)
k=0 exp(γ1si,k)
Here, γ1 is a factor that determines how much attention is
paid to features of its relevant sub-regions when computing
the region-context vector for a word.
Finally, we deﬁne the relevance between the ith word
and the image using the cosine similarity between ci and ei,
i.e., R(ci, ei) = (cT
i ei)/(||ci||||ei||). Inspired by the minimum classiﬁcation error formulation in speech recognition
(see, e.g., ), the attention-driven image-text matching score between the entire image (Q) and the whole text
description (D) is deﬁned as
R(Q, D) = log
exp(γ2R(ci, ei))
where γ2 is a factor that determines how much to magnify the importance of the most relevant word-to-regioncontext pair. When γ2 →∞, R(Q, D) approximates to
i=1 R(ci, ei).
The DAMSM loss
is designed to learn the attention
model in a semi-supervised manner, in which the only supervision is the matching between entire images and whole
sentences (a sequence of words). Similar to , for a
batch of image-sentence pairs {(Qi, Di)}M
i=1, the posterior
probability of sentence Di being matching with image Qi
is computed as
P(Di|Qi) =
exp(γ3R(Qi, Di))
j=1 exp(γ3R(Qi, Dj))
where γ3 is a smoothing factor determined by experiments.
In this batch of sentences, only Di matches the image Qi,
and treat all other M −1 sentences as mismatching descriptions. Following , we deﬁne the loss function as
the negative log posterior probability that the images are
matched with their corresponding text descriptions (ground
truth), i.e.,
log P(Di|Qi),
where ‘w’ stands for “word”.
Symmetrically, we also minimize
log P(Qi|Di),
where P(Qi|Di) =
exp(γ3R(Qi,Di))
j=1 exp(γ3R(Qj,Di)) is the posterior
probability that sentence Di is matched with its corresponding image Qi.
If we redeﬁne Eq. (10) by R(Q, D) =
 ||v||||e||
and substitute it to Eq. (11), (12) and
(13), we can obtain loss functions Ls
2 (where ‘s’
stands for “sentence”) using the sentence vector e and the
global image vector v.
Finally, the DAMSM loss is deﬁned as
LDAMSM = Lw
Based on experiments on a held-out validation set, we set
the hyperparameters in this section as: γ1 = 5, γ2 = 5,
γ3 = 10 and M = 50. Our DAMSM is pretrained 1 by
minimizing LDAMSM using real image-text pairs. Since
the size of images for pretraining DAMSM is not limited
by the size of images that can be generated, real images of
size 299×299 are utilized. In addition, the pretrained textencoder in the DAMSM provides visually-discriminative
word vectors learned from image-text paired data for the
attentional generative network.
In comparison, conventional word vectors pretrained on pure text data are often
not visually-discriminative, e.g., word vectors of different
colors, such as red, blue, yellow, etc., are often clustered
together in the vector space, due to the lack of grounding
them to the actual visual signals.
In sum, we propose two novel attention models, the attentional generative network and the DAMSM, which play
different roles in the AttnGAN. (i) The attention mechanism
in the generative network (see Eq. 2) enables the AttnGAN
to automatically select word level condition for generating
different sub-regions of the image. (ii) With an attention
mechanism (see Eq. 9), the DAMSM is able to compute
the ﬁne-grained text-image matching loss LDAMSM. It is
worth mentioning that, LDAMSM is applied only on the
output of the last generator Gm−1, because the eventual
goal of the AttnGAN is to generate large images by the last
1We also ﬁnetuned the DAMSM with the whole network, however the
performance was not improved.
caption/image
Table 1. Statistics of datasets.
generator. We tried to apply LDAMSM on images of all
resolutions generated by (G0, G1, ..., Gm−1). However, the
performance was not improved but the computational cost
was increased.
4. Experiments
Extensive experimentation is carried out to evaluate the
proposed AttnGAN. We ﬁrst study the important components of the AttnGAN, including the attentional generative network and the DAMSM. Then, we compare our AttnGAN with previous state-of-the-art GAN models for textto-image synthesis .
Same as previous text-to-image methods , our method is evaluated on CUB 
and COCO datasets. We preprocess the CUB dataset
according to the method in . Table 1 lists the statistics
of datasets.
Evaluation.
Following Zhang et al. , we use the
inception score as the quantitative evaluation measure.
Since the inception score cannot reﬂect whether the generated image is well conditioned on the given text description,
we propose to use R-precision, a common evaluation metric for ranking retrieval results, as a complementary evaluation metric for the text-to-image synthesis task. If there
are R relevant documents for a query, we examine the top
R ranked retrieval results of a system, and ﬁnd that r are
relevant, and then by deﬁnition, the R-precision is r/R.
More speciﬁcally, we conduct a retrieval experiment, i.e.,
we use generated images to query their corresponding text
descriptions. First, the image and text encoders learned in
our pretrained DAMSM are utilized to extract global feature
vectors of the generated images and the given text descriptions. And then, we compute cosine similarities between the
global image vectors and the global text vectors. Finally,
we rank candidate text descriptions for each image in descending similarity and ﬁnd the top r relevant descriptions
for computing the R-precision. To compute the inception
score and the R-precision, each model generates 30,000 images from randomly selected unseen text descriptions. The
candidate text descriptions for each query image consist of
one ground truth (i.e., R = 1) and 99 randomly selected
mismatching descriptions.
Besides quantitative evaluation, we also qualitatively
examine the samples generated by our models.
Speciﬁcally, we visualize the intermediate results with attention
learned by the attention models F attn.
As deﬁned in
Eq. (2), weights βj,i indicates which words the model at-
Inception score
AttnGAN1,no DAMSM
AttnGAN1,𝞴=0.1
AttnGAN1,𝞴=1
AttnGAN1,𝞴=5
AttnGAN1,𝞴=10
AttnGAN2,𝞴=5
R-precision (%)
AttnGAN1,no DAMSM
AttnGAN1,𝞴=0.1
AttnGAN1,𝞴=1
AttnGAN1,𝞴=5
AttnGAN1,𝞴=10
AttnGAN2,𝞴=5
Inception score
AttnGAN1,𝞴=0.1
AttnGAN1,𝞴=1
AttnGAN1,𝞴=10
AttnGAN1,𝞴=50
AttnGAN1,𝞴=100
AttnGAN2,𝞴=50
R-precision(%)
AttnGAN1,𝞴=0.1
AttnGAN1,𝞴=1
AttnGAN1,𝞴=10
AttnGAN1,𝞴=50
AttnGAN1,𝞴=100
AttnGAN2,𝞴=50
Figure 3. Inception scores and R-precision rates by our AttnGAN
and its variants at different epochs on CUB (top) and COCO (bottom) test sets. For the text-to-image synthesis task, R = 1.
inception score
R-precision(%)
AttnGAN1, no DAMSM
3.98 ± .04
10.37± 5.88
AttnGAN1, λ = 0.1
4.19 ± .06
16.55± 4.83
AttnGAN1, λ = 1
4.35 ± .05
34.96± 4.02
AttnGAN1, λ = 5
4.35 ± .04
58.65± 5.41
AttnGAN1, λ = 10
4.29 ± .05
63.87± 4.85
AttnGAN2, λ = 5
4.36 ± .03
67.82 ± 4.43
AttnGAN2, λ = 50
25.89 ± .47
85.47 ± 3.69
Table 2. The best inception score and the corresponding Rprecision rate of each AttnGAN model on CUB (top six rows) and
COCO (the last row) test sets. More results in Figure 3.
tends to when generating a sub-region of the image, and
i=0 βj,i = 1. We suppress the less-relevant words for an
image’s sub-region via
if βj,i > 1/T,
otherwise.
For better visualization, we ﬁx the word and compute its attention weights with N different sub-regions of an image,
ˆβ0,i, ˆβ1,i, ..., ˆβN−1,i. We reshape the N attention weights
N pixels, which are then upsampled with Gaussian ﬁlters to have the same size as the generated images.
Limited by the length of the paper, we only visualize the
top-5 most attended words (i.e., words with top-5 highest
j=0 ˆβj,i values) for each attention model.
4.1. Component analysis
In this section, we ﬁrst quantitatively evaluate the AttnGAN and its variants. The results are shown in Table 2
and Figure 3. Our “AttnGAN1” architecture has one attention model and two generators, while the “AttnGAN2” architecture has two attention models stacked with three generators (see Figure 2). In addition, as illustrated in Figure 4,
Figure 5, Figure 6, and Figure 7, we qualitatively examine
the images generated by our AttnGAN.
The DAMSM loss.
To test the proposed LDAMSM,
we adjust the value of λ (see Eq. (3)). As shown in Figure 3, a larger λ leads to a signiﬁcantly higher R-precision
rate on both CUB and COCO datasets. On the CUB dataset,
when the value of λ is increased from 0.1 to 5, the inception score of the AttnGAN1 is improved from 4.19 to 4.35
and the corresponding R-precision rate is increased from
16.55% to 58.65% (see Table 2). On the COCO dataset,
by increasing the value of λ from 0.1 to 50, the AttnGAN1
achieves both high inception score and R-precision rate (see
Figure 3). This comparison demonstrates that properly increasing the weight of LDAMSM helps to generate higher
quality images that are better conditioned on given text descriptions.
The reason is that the proposed ﬁne-grained
image-text matching loss LDAMSM provides additional supervision (i.e., word level matching information) for training the generator. Moreover, in our experiments, we do not
observe any collapsed nonsensical mode in the visualization
of AttnGAN-generated images. It indicates that, with extra
supervision, the ﬁne-grained image-text matching loss also
helps to stabilize the training process of the AttnGAN. In
addition, if we replace the proposed DAMSM sub-network
with the text encoder used in , on the CUB dataset, the
inception score and R-precision drops to 3.98 and 10.37%,
respectively (i.e., the “AttnGAN1, no DAMSM” entry in table 2), which further demonstrates the effectiveness of the
proposed LDAMSM.
The attentional generative network. As shown in Table 2 and Figure 3, stacking two attention models in the
generative networks not only generates images of a higher
resolution (from 128×128 to 256×256 resolution), but also
yields higher inception scores on both CUB and COCO
datasets. In order to guarantee the image quality, we ﬁnd
the best value of λ for each dataset by increasing the value
of λ until the overall inception score is starting to drop on
a held-out validation set. “AttnGAN1” models are built for
searching the best λ, based on which a “AttnGAN2” model
is built to generate higher resolution images. Due to GPU
memory constraints, we did not try the AttnGAN with three
attention models. As the result, our ﬁnal model for CUB
and COCO is “AttnGAN2, λ=5” and “AttnGAN2, λ=50”,
respectively. The ﬁnal λ of the COCO dataset turns out to
be much larger than that of the CUB dataset, indicating that
the proposed LDAMSM is especially important for generatthe bird has a yellow crown and a black eyering that is round
this bird has a green crown black primaries and a white belly
a photo of a homemade swirly pasta with broccoli carrots and onions
a fruit stand display with bananas and kiwi
8:broccoli
8:broccoli
Figure 4. Intermediate results of our AttnGAN on CUB (top) and COCO (bottom) test sets. In each block, the ﬁrst row gives 64×64 images
by G0, 128×128 images by G1 and 256×256 images by G2 of the AttnGAN; the second and third row shows the top-5 most attended
words by F attn
and F attn
of the AttnGAN, respectively. Refer to the supplementary material for more examples.
GAN-INT-CLS 
GAWWN 
StackGAN 
StackGAN-v2 
Our AttnGAN
2.88 ± .04
3.62 ± .07
3.70 ± .04
3.82 ± .06
4.36 ± .03
7.88 ± .07
8.45 ± .03
9.58 ± .21
25.89 ± .47
Table 3. Inception scores by state-of-the-art GAN models and our AttnGAN on CUB and COCO test sets.
ing complex scenarios like those in the COCO dataset.
To better understand what has been learned by the AttnGAN, we visualize its intermediate results with attention.
As shown in Figure 4, the ﬁrst stage of the AttnGAN (G0)
just sketches the primitive shape and colors of objects and
generates low resolution images. Since only the global sentence vectors are utilized in this stage, the generated images
lack details described by exact words, e.g., the beak and
eyes of a bird. Based on word vectors, the following stages
(G1 and G2) learn to rectify defects in results of the previous stage and add more details to generate higher-resolution
images. Some sub-regions/pixels of G1 or G2 images can
be inferred directly from images generated by the previous
stage. For those sub-regions, the attention is equally allothis bird has wings that are black and has a white belly
this bird has wings that are red and has a yellow belly
this bird has wings that are blue and has a red belly
Figure 5. Example results of our AttnGAN model trained on CUB
while changing some most attended words in the text descriptions.
a ﬂuffy black
cat ﬂoating on
top of a lake
a red double
decker bus
is ﬂoating on
top of a lake
a stop sign
is ﬂoating on
top of a lake
a stop sign
is ﬂying in
the blue sky
Figure 6. 256×256 images generated from descriptions of novel
scenarios using the AttnGAN model trained on COCO. (Intermediate results are given in the supplementary material.)
Figure 7. Novel images by our AttnGAN on the CUB test set.
cated to all words and shown to be black in the attention
map (see Figure 4). For other sub-regions, which usually
have semantic meaning expressed in the text description
such as the attributes of objects, the attention is allocated to
their most relevant words (bright regions in Figure 4). Thus,
those regions are inferred from both word-context features
and previous image features of those regions. As shown in
Figure 4, on the CUB dataset, the words the, this, bird are
usually attended by the F attn models for locating the object; the words describing object attributes, such as colors
and parts of birds, are also attended for correcting defects
and drawing details. On the COCO dataset, we have similar
observations. Since there are usually more than one object in each COCO image, it is more visible that the words
describing different objects are attended by different subregions of the image, e.g., bananas, kiwi in the bottom-right
block of Figure 4. Those observations demonstrate that the
AttnGAN learns to understand the detailed semantic meaning expressed in the text description of an image. Another
observation is that our second attention model F attn
to attend to some new words that were omitted by the ﬁrst
attention model F attn
(see Figure 4). It demonstrates that,
to provide richer information for generating higher resolution images at latter stages of the AttnGAN, the corresponding attention models learn to recover objects and attributes
omitted at previous stages.
Generalization ability. Our experimental results above
have quantitatively and qualitatively shown the generalization ability of the AttnGAN by generating images from
unseen text descriptions. Here we further test how sensitive the outputs are to changes in the input sentences by
changing some most attended words in the text descriptions.
Some examples are shown in Figure 5. It illustrates that the
generated images are modiﬁed according to the changes in
the input sentences, showing that the model can catch subtle semantic differences in the text description. Moreover,
as shown in Figure 6, our AttnGAN can generate images to
reﬂect the semantic meaning of descriptions of novel scenarios that are not likely to happen in the real world, e.g.,
a stop sign is ﬂoating on top of a lake. On the other hand,
we also observe that the AttnGAN sometimes generates images which are sharp and detailed, but are not likely realistic. As examples shown in Figure 7, the AttnGAN creates
birds with multiple heads, eyes or tails, which only exist in
fairy tales. This indicates that our current method is still
not perfect in capturing global coherent structures, which
leaves room to improve. To sum up, observations shown
in Figure 5, Figure 6 and Figure 7 further demonstrate the
generalization ability of the AttnGAN.
4.2. Comparison with previous methods
We compare our AttnGAN with previous state-of-theart GAN models for text-to-image generation on CUB and
COCO test sets. As shown in Table 3, on the CUB dataset,
our AttnGAN achieves 4.36 inception score, which significantly outperforms the previous best inception score of
More impressively, our AttnGAN boosts the best
reported inception score on the COCO dataset from 9.58
to 25.89, a 170.25% improvement relatively. The COCO
dataset is known to be much more challenging than the
CUB dataset because it consists of images with more complex scenarios. Existing methods struggle in generating realistic high-resolution images on this dataset.
in Figure 4 and Figure 6 illustrate that our AttnGAN succeeds in generating 256×256 images for various scenarios
on the COCO dataset, although those generated images of
the COCO dataset are not as photo-realistic as that of the
CUB dataset.
The experimental results show that, compared to previous state-of-the-art approaches, the AttnGAN
is more effective for generating complex scenes due to its
novel attention mechanism that catches ﬁne-grained word
level and sub-region level information in text-to-image generation.
5. Conclusions
In this paper, we propose an Attentional Generative Adversarial Network, named AttnGAN, for ﬁne-grained textto-image synthesis. First, we build a novel attentional generative network for the AttnGAN to generate high quality image through a multi-stage process. Second, we propose a deep attentional multimodal similarity model to compute the ﬁne-grained image-text matching loss for training the generator of the AttnGAN. Our AttnGAN significantly outperforms previous state-of-the-art GAN models,
boosting the best reported inception score by 14.14% on the
CUB dataset and 170.25% on the more challenging COCO
dataset. Extensive experimental results clearly demonstrate
the effectiveness of the proposed attention mechanism in
the AttnGAN, which is especially critical for text-to-image
generation for complex scenes.