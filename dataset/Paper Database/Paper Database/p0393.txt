Published as a conference paper at ICLR 2017
ADVERSARIAL FEATURE LEARNING
Jeff Donahue
 
Computer Science Division
University of California, Berkeley
Philipp Krähenbühl
 
Department of Computer Science
University of Texas, Austin
Trevor Darrell
 
Computer Science Division
University of California, Berkeley
The ability of the Generative Adversarial Networks (GANs) framework to learn
generative models mapping from simple latent distributions to arbitrarily complex
data distributions has been demonstrated empirically, with compelling results
showing that the latent space of such generators captures semantic variation in
the data distribution. Intuitively, models trained to predict these semantic latent
representations given data may serve as useful feature representations for auxiliary
problems where semantics are relevant. However, in their existing form, GANs
have no means of learning the inverse mapping – projecting data back into the
latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs)
as a means of learning this inverse mapping, and demonstrate that the resulting
learned feature representation is useful for auxiliary supervised discrimination tasks,
competitive with contemporary approaches to unsupervised and self-supervised
feature learning.
INTRODUCTION
Deep convolutional networks (convnets) have become a staple of the modern computer vision pipeline.
After training these models on a massive database of image-label pairs like ImageNet , the network easily adapts to a variety of similar visual tasks, achieving impressive
results on image classiﬁcation 
or localization tasks. In other perceptual domains such as
natural language processing or speech recognition, deep networks have proven highly effective as
well . However,
all of these recent results rely on a supervisory signal from large-scale databases of hand-labeled data,
ignoring much of the useful information present in the structure of the data itself.
Meanwhile, Generative Adversarial Networks (GANs) have emerged as a
powerful framework for learning generative models of arbitrarily complex data distributions. The
GAN framework learns a generator mapping samples from an arbitrary latent distribution to data, as
well as an adversarial discriminator which tries to distinguish between real and generated samples
as accurately as possible. The generator’s goal is to “fool” the discriminator by producing samples
which are as close to real data as possible. When trained on databases of natural images, GANs
produce impressive results .
Interpolations in the latent space of the generator produce smooth and plausible semantic variations,
and certain directions in this space correspond to particular semantic attributes along which the data
distribution varies. For example, Radford et al. showed that a GAN trained on a database of
human faces learns to associate particular latent directions with gender and the presence of eyeglasses.
A natural question arises from this ostensible “semantic juice” ﬂowing through the weights of
generators learned using the GAN framework: can GANs be used for unsupervised learning of rich
feature representations for arbitrary data distributions? An obvious issue with doing so is that the
 
Published as a conference paper at ICLR 2017
Figure 1: The structure of Bidirectional Generative Adversarial Networks (BiGAN).
generator maps latent samples to generated data, but the framework does not include an inverse
mapping from data to latent representation.
Hence, we propose a novel unsupervised feature learning framework, Bidirectional Generative
Adversarial Networks (BiGAN). The overall model is depicted in Figure 1. In short, in addition to
the generator G from the standard GAN framework , BiGAN includes an
encoder E which maps data x to latent representations z. The BiGAN discriminator D discriminates
not only in data space (x versus G(z)), but jointly in data and latent space (tuples (x, E(x)) versus
(G(z), z)), where the latent component is either an encoder output E(x) or a generator input z.
It may not be obvious from this description that the BiGAN encoder E should learn to invert the
generator G. The two modules cannot directly “communicate” with one another: the encoder never
“sees” generator outputs (E(G(z)) is not computed), and vice versa. Yet, in Section 3, we will both
argue intuitively and formally prove that the encoder and generator must learn to invert one another
in order to fool the BiGAN discriminator.
Because the BiGAN encoder learns to predict features z given data x, and prior work on GANs has
demonstrated that these features capture semantic attributes of the data, we hypothesize that a trained
BiGAN encoder may serve as a useful feature representation for related semantic tasks, in the same
way that fully supervised visual models trained to predict semantic “labels” given images serve as
powerful feature representations for related visual tasks. In this context, a latent representation z may
be thought of as a “label” for x, but one which came for “free,” without the need for supervision.
An alternative approach to learning the inverse mapping from data to latent representation is to
directly model p(z|G(z)), predicting generator input z given generated data G(z). We’ll refer to
this alternative as a latent regressor, later arguing (Section 4.1) that the BiGAN encoder may be
preferable in a feature learning context, as well as comparing the approaches empirically.
BiGANs are a robust and highly generic approach to unsupervised feature learning, making no
assumptions about the structure or type of data to which they are applied, as our theoretical results will
demonstrate. Our empirical studies will show that despite their generality, BiGANs are competitive
with contemporary approaches to self-supervised and weakly supervised feature learning designed
speciﬁcally for a notoriously complex data distribution – natural images.
Dumoulin et al. independently proposed an identical model in their concurrent work, exploring
the case of a stochastic encoder E and the ability of such models to learn in a semi-supervised setting.
PRELIMINARIES
Let pX(x) be the distribution of our data for x ∈ΩX (e.g. natural images). The goal of generative
modeling is capture this data distribution using a probabilistic model. Unfortunately, exact modeling
of this probability density function is computationally intractable for all but the most trivial models. Generative Adversarial Networks (GANs) instead model the data distribution as a transformation of a ﬁxed latent distribution
pZ(z) for z ∈ΩZ. This transformation, called a generator, is expressed as a deterministic feed
forward network G : ΩZ →ΩX with pG(x|z) = δ (x −G(z)) and pG(x) = Ez∼pZ [pG(x|z)]. The
goal is to train a generator such that pG(x) ≈pX(x).
The GAN framework trains a generator, such that no discriminative model D : ΩX 7→ can
distinguish samples of the data distribution from samples of the generative distribution. Both generator
and discriminator are learned using the adversarial (minimax) objective min
D V (D, G), where
V (D, G) := Ex∼pX [log D(x)] + Ex∼pG [log (1 −D(x))]
Ez∼pZ[log(1−D(G(z)))]
Goodfellow et al. showed that for an ideal discriminator the objective C(G)
maxD V (D, G) is equivalent to the Jensen-Shannon divergence between the two distributions pG
The adversarial objective 1 does not directly lend itself to an efﬁcient optimization, as each step in
the generator G requires a full discriminator D to be learned. Furthermore, a perfect discriminator
no longer provides any gradient information to the generator, as the gradient of any global or local
maximum of V (D, G) is 0. To provide a strong gradient signal nonetheless, Goodfellow et al. 
slightly alter the objective between generator and discriminator updates, while keeping the same ﬁxed
point characteristics. They also propose to optimize (1) using an alternating optimization switching
between updates to the generator and discriminator. While this optimization is not guaranteed to
converge, empirically it works well if the discriminator and generator are well balanced.
Despite the empirical strength of GANs as generative models of arbitrary data distributions, it is not
clear how they can be applied as an unsupervised feature representation. One possibility for learning
such representations is to learn an inverse mapping regressing from generated data G(z) back to
the latent input z. However, unless the generator perfectly models the data distribution pX, a nearly
impossible objective for a complex data distribution such as that of high-resolution natural images,
this idea may prove insufﬁcient.
BIDIRECTIONAL GENERATIVE ADVERSARIAL NETWORKS
In Bidirectional Generative Adversarial Networks (BiGANs) we not only train a generator, but
additionally train an encoder E : ΩX →ΩZ. The encoder induces a distribution pE(z|x) =
δ(z −E(x)) mapping data points x into the latent feature space of the generative model. The
discriminator is also modiﬁed to take input from the latent space, predicting PD(Y |x, z), where
Y = 1 if x is real (sampled from the real data distribution pX), and Y = 0 if x is generated (the
output of G(z), z ∼pZ).
The BiGAN training objective is deﬁned as a minimax objective
D V (D, E, G)
V (D, E, G) := Ex∼pX
Ez∼pE(·|x) [log D(x, z)]
log D(x,E(x))
Ex∼pG(·|z) [log (1 −D(x, z))]
log(1−D(G(z),z))
We optimize this minimax objective using the same alternating gradient based optimization as
Goodfellow et al. . See Section 3.4 for details.
BiGANs share many of the theoretical properties of GANs , while additionally guaranteeing that at the global optimum, G and E are each other’s inverse. BiGANs are also
closely related to autoencoders with an ℓ0 loss function. In the following sections we highlight some
of the appealing theoretical properties of BiGANs.
Deﬁnitions
Let pGZ(x, z) := pG(x|z)pZ(z) and pEX(x, z) := pE(z|x)pX(x) be the joint distributions modeled by the generator and encoder respectively. Ω:= ΩX × ΩZ is the joint latent and
Published as a conference paper at ICLR 2017
data space. For a region R ⊆Ω,
ΩpEX(x, z)1[(x,z)∈R] d(x, z) =
ΩZ pE(z|x)1[(x,z)∈R] dz dx
ΩpGZ(x, z)1[(x,z)∈R] d(x, z) =
ΩX pG(x|z)1[(x,z)∈R] dx dz
are probability measures over that region. We also deﬁne
ΩX pX(x)1[x∈RX] dx
ΩZ pZ(z)1[z∈RZ] dz
as measures over regions RX ⊆ΩX and RZ ⊆ΩZ. We refer to the set of features and data samples
in the support of PX and PZ as ˆΩX := supp(PX) and ˆΩZ := supp(PZ) respectively. DKL (P || Q)
and DJS (P || Q) respectively denote the Kullback-Leibler (KL) and Jensen-Shannon divergences
between probability measures P and Q. By deﬁnition,
DKL (P || Q) := Ex∼P [log fP Q(x)]
DJS (P || Q) := 1
where fP Q := dP
dQ is the Radon-Nikodym (RN) derivative of measure P with respect to measure Q,
with the deﬁning property that P(R) =
R fP Q dQ. The RN derivative fP Q : Ω7→R≥0 is deﬁned
for any measures P and Q on space Ωsuch that P is absolutely continuous with respect to Q: i.e.,
for any R ⊆Ω, P(R) > 0 =⇒Q(R) > 0.
OPTIMAL DISCRIMINATOR, GENERATOR, & ENCODER
We start by characterizing the optimal discriminator for any generator and encoder, following Goodfellow et al. . This optimal discriminator then allows us to reformulate objective (3), and show
that it reduces to the Jensen-Shannon divergence between the joint distributions PEX and PGZ.
Proposition 1 For any E and G, the optimal discriminator D∗
EG := arg maxD V (D, E, G) is the
Radon-Nikodym derivative fEG :=
d(PEX+PGZ) : Ω7→ of measure PEX with respect to
measure PEX + PGZ.
Proof. Given in Appendix A.1.
This optimal discriminator now allows us to characterize the optimal generator and encoder.
Proposition 2 The encoder and generator’s objective for an optimal discriminator C(E, G) :=
maxD V (D, E, G) = V (D∗
EG, E, G) can be rewritten in terms of the Jensen-Shannon divergence
between measures PEX and PGZ as C(E, G) = 2 DJS (PEX || PGZ ) −log 4.
Proof. Given in Appendix A.2.
Theorem 1 The global minimum of C(E, G) is achieved if and only if PEX = PGZ. At that point,
C(E, G) = −log 4 and D∗
Proof. From Proposition 2, we have that C(E, G) = 2 DJS (PEX || PGZ ) −log 4. The Jensen-
Shannon divergence DJS (P || Q) ≥0 for any P and Q, and DJS (P || Q) = 0 if and only if P = Q.
Therefore, the global minimum of C(E, G) occurs if and only if PEX = PGZ, and at this point the
value is C(E, G) = −log 4. Finally, PEX = PGZ implies that the optimal discriminator is chance:
d(PEX+PGZ) =
2 dPEX = 1
The optimal discriminator, encoder, and generator of BiGAN are similar to the optimal discriminator
and generator of the GAN framework . However, an important difference is
that BiGAN optimizes a Jensen-Shannon divergence between a joint distribution over both data X
and latent features Z. This joint divergence allows us to further characterize properties of G and E,
as shown below.
OPTIMAL GENERATOR & ENCODER ARE INVERSES
We ﬁrst present an intuitive argument that, in order to “fool” a perfect discriminator, a deterministic
BiGAN encoder and generator must invert each other. Consider a BiGAN discriminator input pair (x, z). Due to the sampling procedure, (x, z)
must satisfy at least one of the following two properties:
(a) x ∈ˆΩX ∧E(x) = z
(b) z ∈ˆΩZ ∧G(z) = x
If only one of these properties is satisﬁed, a perfect discriminator can infer the source of (x, z) with
certainty: if only (a) is satisﬁed, (x, z) must be an encoder pair (x, E(x)) and D∗
EG(x, z) = 1; if
only (b) is satisﬁed, (x, z) must be a generator pair (G(z), z) and D∗
EG(x, z) = 0.
Therefore, in order to fool a perfect discriminator at (x, z) (so that 0 < D∗
EG(x, z) < 1), E and
G must satisfy both (a) and (b). In this case, we can substitute the equality E(x) = z required
by (a) into the equality G(z) = x required by (b), and vice versa, giving the inversion properties
x = G(E(x)) and z = E(G(z)).
Formally, we show in Theorem 2 that the optimal generator and encoder invert one another almost
everywhere on the support ˆΩX and ˆΩZ of PX and PZ.
Theorem 2 If E and G are an optimal encoder and generator, then E = G−1 almost everywhere;
that is, G(E(x)) = x for PX-almost every x ∈ΩX, and E(G(z)) = z for PZ-almost every z ∈ΩZ.
Proof. Given in Appendix A.4.
While Theorem 2 characterizes the encoder and decoder at their optimum, due to the non-convex
nature of the optimization, this optimum might never be reached. Experimentally, Section 4 shows
that on standard datasets, the two are approximate inverses; however, they are rarely exact inverses. It
is thus also interesting to show what objective BiGAN optimizes in terms of E and G. Next we show
that BiGANs are closely related to autoencoders with an ℓ0 loss function.
RELATIONSHIP TO AUTOENCODERS
As argued in Section 1, a model trained to predict features z given data x should learn useful semantic
representations. Here we show that the BiGAN objective forces the encoder E to do exactly this: in
order to fool the discriminator at a particular z, the encoder must invert the generator at that z, such
that E(G(z)) = z.
Theorem 3 The encoder and generator objective given an optimal discriminator C(E, G) :=
maxD V (D, E, G) can be rewritten as an ℓ0 autoencoder loss function
C(E, G) = Ex∼pX
1[E(x)∈ˆΩZ∧G(E(x))=x] log fEG(x, E(x))
1[G(z)∈ˆΩX∧E(G(z))=z] log (1 −fEG(G(z), z))
with log fEG ∈(−∞, 0) and log (1 −fEG) ∈(−∞, 0) PEX-almost and PGZ-almost everywhere.
Proof. Given in Appendix A.5.
Here the indicator function 1[G(E(x))=x] in the ﬁrst term is equivalent to an autoencoder with ℓ0 loss,
while the indicator 1[E(G(z))=z] in the second term shows that the BiGAN encoder must invert the
generator, the desired property for feature learning. The objective further encourages the functions
E(x) and G(z) to produce valid outputs in the support of PZ and PX respectively. Unlike regular
autoencoders, the ℓ0 loss function does not make any assumptions about the structure or distribution
of the data itself; in fact, all the structural properties of BiGAN are learned as part of the discriminator.
In practice, as in the GAN framework , each BiGAN module D, G, and E
is a parametric function (with parameters θD, θG, and θE, respectively). As a whole, BiGAN can be
optimized using alternating stochastic gradient steps. In one iteration, the discriminator parameters
θD are updated by taking one or more steps in the positive gradient direction ∇θDV (D, E, G),
then the encoder parameters θE and generator parameters θG are together updated by taking a step
in the negative gradient direction −∇θE,θGV (D, E, G). In both cases, the expectation terms of
Published as a conference paper at ICLR 2017
V (D, E, G) are estimated using mini-batches of n samples {x(i) ∼pX}n
i=1 and {z(i) ∼pZ}n
drawn independently for each update step.
Goodfellow et al. found that an objective in which the real and generated labels Y are swapped
provides stronger gradient signal to G. We similarly observed in BiGAN training that an “inverse”
objective provides stronger gradient signal to G and E. For efﬁciency, we also update all modules
D, G, and E simultaneously at each iteration, rather than alternating between D updates and G, E
updates. See Appendix B for details.
GENERALIZED BIGAN
It is often useful to parametrize the output of the generator G and encoder E in a different, usually
smaller, space Ω′
Z rather than the original ΩX and ΩZ. For example, for visual feature
learning, the images input to the encoder should be of similar resolution to images used in the
evaluation. On the other hand, generating high resolution images remains difﬁcult for current
generative models. In this situation, the encoder may take higher resolution input while the generator
output and discriminator input remain low resolution.
We generalize the BiGAN objective V (D, G, E) (3) with functions gX : ΩX 7→Ω′
X and gZ : ΩZ 7→
Z, and encoder E : ΩX 7→Ω′
Z, generator G : ΩZ 7→Ω′
X, and discriminator D : Ω′
Z 7→ :
Ez′∼pE(·|x) [log D(gX(x), z′)]
log D(gX(x),E(x))
Ex′∼pG(·|z) [log (1 −D(x′, gZ(z)))]
log(1−D(G(z),gZ(z)))
An identity gX(x) = x and gZ(z) = z (and Ω′
X = ΩX, Ω′
Z = ΩZ) yields the original objective. For
visual feature learning with higher resolution encoder inputs, gX is an image resizing function that
downsamples a high resolution image x ∈ΩX to a lower resolution image x′ ∈Ω′
X, as output by the
generator. (gZ is identity.)
In this case, the encoder and generator respectively induce probability measures PEX′ and
PGZ′ over regions R
Ω′ of the joint space Ω′
Z, with PEX′(R)
Z pEX(x, z′)1[(x′,z′)∈R]δ(gX(x) −x′) dz′ dx′ dx =
ΩX pX(x)1[(gX(x),E(x))∈R] dx,
and PGZ′ deﬁned analogously. For optimal E and G, we can show PEX′ = PGZ′: a generalization
of Theorem 1. When E and G are deterministic and optimal, Theorem 2 – that E and G invert one
another – can also be generalized: ∃z∈ˆΩZ{E(x) = gZ(z) ∧G(z) = gX(x)} for PX-almost every
x ∈ΩX, and ∃x∈ˆΩX{E(x) = gZ(z) ∧G(z) = gX(x)} for PZ-almost every z ∈ΩZ.
EVALUATION
We evaluate the feature learning capabilities of BiGANs by ﬁrst training them unsupervised as
described in Section 3.4, then transferring the encoder’s learned feature representations for use in
auxiliary supervised learning tasks. To demonstrate that BiGANs are able to learn meaningful feature
representations both on arbitrary data vectors, where the model is agnostic to any underlying structure,
as well as very high-dimensional and complex distributions, we evaluate on both permutation-invariant
MNIST and on the high-resolution natural images of ImageNet .
In all experiments, each module D, G, and E is a parametric deep (multi-layer) network. The BiGAN
discriminator D(x, z) takes data x as its initial input, and at each linear layer thereafter, the latent
representation z is transformed using a learned linear transformation to the hidden layer dimension
and added to the non-linearity input.
BASELINE METHODS
Besides the BiGAN framework presented above, we considered alternative approaches to learning
feature representations using different GAN variants.
Discriminator
The discriminator D in a standard GAN takes data samples x ∼pX as input, making
its learned intermediate representations natural candidates as feature representations for related tasks.
Published as a conference paper at ICLR 2017
Table 1: One Nearest Neighbors (1NN) classiﬁcation accuracy (%) on the permutation-invariant
MNIST test set in the feature space learned by BiGAN, Latent Regressor (LR),
Joint Latent Regressor (JLR), and an autoencoder (AE) using an ℓ1 or ℓ2 distance.
Figure 2: Qualitative results for permutation-invariant MNIST BiGAN training, including generator
samples G(z), real data x, and corresponding reconstructions G(E(x)).
This alternative is appealing as it requires no additional machinery, and is the approach used for
unsupervised feature learning in Radford et al. . On the other hand, it is not clear that the task of
distinguishing between real and generated data requires or beneﬁts from intermediate representations
that are useful as semantic feature representations. In fact, if G successfully generates the true data
distribution pX(x), D may ignore the input data entirely and predict P(Y = 1) = P(Y = 1|x) = 1
unconditionally, not learning any meaningful intermediate representations.
Latent regressor
We consider an alternative encoder training by minimizing a reconstruction loss
L(z, E(G(z))), after or jointly during a regular GAN training, called latent regressor or joint latent
regressor respectively. We use a sigmoid cross entropy loss L as it naturally maps to a uniformly
distributed output space. Intuitively, a drawback of this approach is that, unlike the encoder in a
BiGAN, the latent regressor encoder E is trained only on generated samples G(z), and never “sees”
real data x ∼pX. While this may not be an issue in the theoretical optimum where pG(x) = pX(x)
exactly – i.e., G perfectly generates the data distribution pX – in practice, for highly complex data
distributions pX, such as the distribution of natural images, the generator will almost never achieve
this perfect result. The fact that the real data x are never input to this type of encoder limits its utility
as a feature representation for related tasks, as shown later in this section.
PERMUTATION-INVARIANT MNIST
We ﬁrst present results on permutation-invariant MNIST . In the permutationinvariant setting, each 28×28 digit image must be treated as an unstructured 784D vector . In our case, this condition is met by designing each module as a multi-layer perceptron
(MLP), agnostic to the underlying spatial structure in the data (as opposed to a convnet, for example).
See Appendix C.1 for more architectural and training details. We set the latent distribution pZ =
[U(−1, 1)]50 – a 50D continuous uniform distribution.
Table 1 compares the encoding learned by a BiGAN-trained encoder E with the baselines described
in Section 4.1, as well as autoencoders trained directly to minimize
either ℓ2 or ℓ1 reconstruction error. The same architecture and optimization algorithm is used across
all methods. All methods, including BiGAN, perform at roughly the same level. This result is not
overly surprising given the relative simplicity of MNIST digits. For example, digits generated by
G in a GAN nearly perfectly match the data distribution (qualitatively), making the latent regressor
(LR) baseline method a reasonable choice, as argued in Section 4.1. Qualitative results are presented
in Figure 2.
Next, we present results from training BiGANs on ImageNet LSVRC ,
a large-scale database of natural images. GANs trained on ImageNet cannot perfectly reconstruct
Published as a conference paper at ICLR 2017
Noroozi & Favaro 
AlexNet-based D
Krizhevsky et al. 
Figure 3: The convolutional ﬁlters learned by the three modules (D, G, and E) of a BiGAN (left,
top-middle) trained on the ImageNet database. We compare with the
ﬁlters learned by a discriminator D trained with the same architecture (bottom-middle), as well as
the ﬁlters reported by Noroozi & Favaro , and by Krizhevsky et al. for fully supervised
ImageNet training (right).
Figure 4: Qualitative results for ImageNet BiGAN training, including generator samples G(z), real
data x, and corresponding reconstructions G(E(x)).
the data, but often capture some interesting aspects. Here, each of D, G, and E is a convnet. In all
experiments, the encoder E architecture follows AlexNet through the ﬁfth
and last convolution layer (conv5). We also experiment with an AlexNet-based discriminator D as
a baseline feature learning approach. We set the latent distribution pZ = [U(−1, 1)]200 – a 200D
continuous uniform distribution. Additionally, we experiment with higher resolution encoder input
images – 112 × 112 rather than the 64 × 64 used elsewhere – using the generalization described in
Section 3.5. See Appendix C.2 for more architectural and training details.
Qualitative results
The convolutional ﬁlters learned by each of the three modules are shown in
Figure 3. We see that the ﬁlters learned by the encoder E have clear Gabor-like structure, similar to
those originally reported for the fully supervised AlexNet model . The ﬁlters
also have similar “grouping” structure where one half (the bottom half, in this case) is more color
sensitive, and the other half is more edge sensitive. (This separation of the ﬁlters occurs due to the
AlexNet architecture maintaining two separate ﬁlter paths for computational efﬁciency.)
In Figure 4 we present sample generations G(z), as well as real data samples x and their BiGAN reconstructions G(E(x)). The reconstructions, while certainly imperfect, demonstrate empirically that
Published as a conference paper at ICLR 2017
Random 
Wang & Gupta 
Doersch et al. 
Noroozi & Favaro *
BiGAN (ours)
BiGAN, 112 × 112 E (ours)
Table 2: Classiﬁcation accuracy (%) for the ImageNet LSVRC validation
set with various portions of the network frozen, or reinitialized and trained from scratch, following
the evaluation from Noroozi & Favaro . In, e.g., the conv3 column, the ﬁrst three layers
– conv1 through conv3 – are transferred and frozen, and the last layers – conv4, conv5, and fully
connected layers – are reinitialized and trained fully supervised for ImageNet classiﬁcation. BiGAN is
competitive with these contemporary visual feature learning methods, despite its generality. are not directly comparable to those of the other methods as a different
base convnet architecture with larger intermediate feature maps is used.)
the BiGAN encoder E and generator G learn approximate inverse mappings, as shown theoretically
in Theorem 2. In Appendix C.2, we present nearest neighbors in the BiGAN learned feature space.
ImageNet classiﬁcation
Following Noroozi & Favaro , we evaluate by freezing the ﬁrst
N layers of our pretrained network and randomly reinitializing and training the remainder fully
supervised for ImageNet classiﬁcation. Results are reported in Table 2.
VOC classiﬁcation, detection, and segmentation
We evaluate the transferability of BiGAN representations to the PASCAL VOC computer vision benchmark tasks,
including classiﬁcation, object detection, and semantic segmentation. The classiﬁcation task involves
simple binary prediction of presence or absence in a given image for each of 20 object categories.
The object detection and semantic segmentation tasks go a step further by requiring the objects to
be localized, with semantic segmentation requiring this at the ﬁnest scale: pixelwise prediction of
object identity. For detection, the pretrained model is used as the initialization for Fast R-CNN (FRCN) training; and for semantic segmentation, the model is used as the initialization
for Fully Convolutional Network (FCN) training, in each case replacing the
AlexNet model trained fully supervised for ImageNet classiﬁcation. We
report results on each of these tasks in Table 3, comparing BiGANs with contemporary approaches
to unsupervised and self-supervised feature learning in the visual domain, as well as the
baselines discussed in Section 4.1.
DISCUSSION
Despite making no assumptions about the underlying structure of the data, the BiGAN unsupervised
feature learning framework offers a representation competitive with existing self-supervised and even
weakly supervised feature learning approaches for visual feature learning, while still being a purely
generative model with the ability to sample data x and predict latent representation z. Furthermore,
BiGANs outperform the discriminator (D) and latent regressor (LR) baselines discussed in Section 4.1,
conﬁrming our intuition that these approaches may not perform well in the regime of highly complex
data distributions such as that of natural images. The version in which the encoder takes a higher
resolution image than output by the generator (BiGAN 112 × 112 E) performs better still, and this
strategy is not possible under the LR and D baselines as each of those modules take generator outputs
as their input.
Although existing self-supervised approaches have shown impressive performance and thus far tended
to outshine purely unsupervised approaches in the complex domain of high-resolution images, purely
unsupervised approaches to feature learning or pre-training have several potential beneﬁts.
Published as a conference paper at ICLR 2017
Classiﬁcation
Segmentation
trained layers
ImageNet 
Agrawal et al. 
Pathak et al. 
Wang & Gupta 
Doersch et al. 
k-means 
Discriminator (D)
Latent Regressor (LR)
Autoencoder (ℓ2)
BiGAN (ours)
BiGAN, 112 × 112 E (ours)
Table 3: Classiﬁcation and Fast R-CNN detection results for the PASCAL VOC
2007 test set, and FCN segmentation results on the
PASCAL VOC 2012 validation set, under the standard mean average precision (mAP) or mean
intersection over union (mIU) metrics for each task. Classiﬁcation models are trained with various
portions of the AlexNet model frozen. In the fc8 column, only the linear
classiﬁer (a multinomial logistic regression) is learned – in the case of BiGAN, on top of randomly
initialized fully connected (FC) layers fc6 and fc7. In the fc6-8 column, all three FC layers are trained
fully supervised with all convolution layers frozen. Finally, in the all column, the entire network is
“ﬁne-tuned”. BiGAN outperforms other unsupervised (unsup.) feature learning approaches, including
the GAN-based baselines described in Section 4.1, and despite its generality, is competitive with
contemporary self-supervised (self-sup.) feature learning approaches speciﬁc to the visual domain.
BiGAN and other unsupervised learning approaches are agnostic to the domain of the data. The
self-supervised approaches are speciﬁc to the visual domain, in some cases requiring weak supervision from video unavailable in images alone. For example, the methods are not applicable in the
permutation-invariant MNIST setting explored in Section 4.2, as the data are treated as ﬂat vectors
rather than 2D images.
Furthermore, BiGAN and other unsupervised approaches needn’t suffer from domain shift between
the pre-training task and the transfer task, unlike self-supervised methods in which some aspect of the
data is normally removed or corrupted in order to create a non-trivial prediction task. In the context
prediction task , the network sees only small image patches – the global image
structure is unobserved. In the context encoder or inpainting task , each image
is corrupted by removing large areas to be ﬁlled in by the prediction network, creating inputs with
dramatically different appearance from the uncorrupted natural images seen in the transfer tasks.
Other approaches rely on auxiliary information unavailable in the static image domain, such as video, egomotion, or tracking. Unlike BiGAN, such
approaches cannot learn feature representations from unlabeled static images.
We ﬁnally note that the results presented here constitute only a preliminary exploration of the space
of model architectures possible under the BiGAN framework, and we expect results to improve signiﬁcantly with advancements in generative image models and discriminative convolutional networks
ACKNOWLEDGMENTS
The authors thank Evan Shelhamer, Jonathan Long, and other Berkeley Vision labmates for helpful
discussions throughout this work. This work was supported by DARPA, AFRL, DoD MURI award
N000141110688, NSF awards IIS-1427425 and IIS-1212798, and the Berkeley Artiﬁcial Intelligence
Research laboratory. The GPUs used for this work were donated by NVIDIA.
Published as a conference paper at ICLR 2017