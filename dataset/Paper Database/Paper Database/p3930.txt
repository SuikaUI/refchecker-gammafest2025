AutoML: A Survey of the State-of-the-Art
Xin He, Kaiyong Zhao, Xiaowen Chu∗
Department of Computer Science, Hong Kong Baptist University
Deep learning (DL) techniques have obtained remarkable achievements on various tasks, such as image recognition,
object detection, and language modeling. However, building a high-quality DL system for a speciﬁc task highly relies
on human expertise, hindering its wide application. Meanwhile, automated machine learning (AutoML) is a promising
solution for building a DL system without human assistance and is being extensively studied. This paper presents a
comprehensive and up-to-date review of the state-of-the-art (SOTA) in AutoML. According to the DL pipeline, we
introduce AutoML methods –– covering data preparation, feature engineering, hyperparameter optimization, and neural
architecture search (NAS) –– with a particular focus on NAS, as it is currently a hot sub-topic of AutoML. We summarize
the representative NAS algorithms’ performance on the CIFAR-10 and ImageNet datasets and further discuss the following
subjects of NAS methods: one/two-stage NAS, one-shot NAS, joint hyperparameter and architecture optimization, and
resource-aware NAS. Finally, we discuss some open problems related to the existing AutoML methods for future research.
deep learning, automated machine learning (AutoML), neural architecture search (NAS), hyperparameter
optimization (HPO)
1. Introduction
In recent years, deep learning has been applied in various ﬁelds and used to solve many challenging AI tasks, in
areas such as image classiﬁcation , object detection ,
and language modeling . Speciﬁcally, since AlexNet 
outperformed all other traditional manual methods in the
2012 ImageNet Large Scale Visual Recognition Challenge
(ILSVRC) , increasingly complex and deep neural networks have been proposed. For example, VGG-16 has
more than 130 million parameters, occupies nearly 500 MB
of memory space, and requires 15.3 billion ﬂoating-point
operations to process an image of size 224 × 224. Notably,
however, these models were all manually designed by experts by a trial-and-error process, which means that even
experts require substantial resources and time to create
well-performing models.
To reduce these onerous development costs, a novel idea
of automating the entire pipeline of machine learning (ML)
has emerged, i.e., automated machine learning (AutoML).
There are various deﬁnitions of AutoML. For example, according to , AutoML is designed to reduce the demand
for data scientists and enable domain experts to automatically build ML applications without much requirement for
statistical and ML knowledge. In , AutoML is deﬁned as
a combination of automation and ML. In a word, AutoML
∗Corresponding author
Email addresses: (Xin He),
 (Kaiyong Zhao), 
(Xiaowen Chu)
can be understood to involve the automated construction
of an ML pipeline on the limited computational budget.
With the exponential growth of computing power, AutoML
has become a hot topic in both industry and academia. A
complete AutoML system can make a dynamic combination
of various techniques to form an easy-to-use end-to-end
ML pipeline system (as shown in Figure 1). Many AI companies have created and publicly shared such systems (e.g.,
Cloud AutoML 1 by Google) to help people with little or
no ML knowledge to build high-quality custom models.
As Figure 1 shows, the AutoML pipeline consists of
several processes: data preparation, feature engineering,
model generation, and model evaluation. Model generation
can be further divided into search space and optimization
methods. The search space deﬁnes the design principles of
ML models, which can be divided into two categories: the
traditional ML models (e.g., SVM and KNN), and neural
architectures.
The optimization methods are classiﬁed
into hyperparameter optimization (HPO) and architecture
optimization (AO), where the former indicates the trainingrelated parameters (e.g., the learning rate and batch size),
and the latter indicates the model-related parameters (e.g.,
the number of layer for neural architectures and the number
of neighbors for KNN). NAS consists of three important
components: the search space of neural architectures, AO
methods, and model evaluation methods. AO methods may
also refer to search strategy or search policy . Zoph
et al. were one of the ﬁrst to propose NAS, where a
1 
 
April 19, 2021
 
Extraction
Construction
Feature Engineering
Model Generation
Traditional
(SVM, KNN)
Search Space
Optimization Methods
Hyperparameter
Optimization
Model Estimation
Collection
Data Cleaning
Data Preparation
Augmentation
Deep Neural
(CNN, RNN)
Architecture
Optimization
Low-fidelity
Early-stopping
Surrogate Model
Weight-sharing
Neural Architecture Search (NAS)
Figure 1: An overview of AutoML pipeline covering data preparation (Section 2), feature engineering (Section 3), model generation (Section 4)
and model evaluation (Section 5).
recurrent network is trained by reinforcement learning to
automatically search for the best-performing architecture.
Since successfully discovered a neural network achieving
comparable results to human-designed models, there has
been an explosion of research interest in AutoML, with most
focusing on NAS. NAS aims to search for a robust and wellperforming neural architecture by selecting and combining
diﬀerent basic operations from a predeﬁned search space.
By reviewing NAS methods, we classify the commonly used
search space into entire-structured , cell-based
 , hierarchical and morphism-based
 search space. The commonly used AO methods
contain reinforcement learning (RL) ,
evolution-based algorithm (EA) ,
and gradient descent (GD) , Surrogate Model-
Based Optimization (SMBO) ,
and hybrid AO methods .
Although there are already several excellent AutoMLrelated surveys , to the best of our knowledge, our survey covers a broader range of AutoML methods. As summarized in Table 1, only focus
on NAS, while cover little of NAS technique. In
this paper, we summarize the AutoML-related methods
according to the complete AutoML pipeline (Figure 1),
providing beginners with a comprehensive introduction to
the ﬁeld. Notably, many sub-topics of AutoML are large
enough to have their own surveys. However, our goal is
not to conduct a thorough investigation of all AutoML
sub-topics. Instead, we focus on the breadth of research
in the ﬁeld of AutoML. Therefore, we will summarize and
discuss some representative methods of each process in the
The rest of this paper is organized as follows. The
NAS Survey 
A Survey on NAS 
NAS Challenges 
A Survey on AutoML 
AutoML Challenges 
AutoML Benchmark 
Table 1: Comparison between diﬀerent AutoML surveys. The “Survey”
column gives each survey a label based on their title for increasing the
readability. DP, FE, HPO, NAS indicate data preparation, feature
engineering, hyperparameter optimization and neural architecture
search, respectively. “-”, “✓”, and “†” indicate the content is 1)
not mentioned; 2) mentioned detailed; 3) mentioned brieﬂy, in the
original paper, respectively.
processes of data preparation, feature engineering, model
generation, and model evaluation are presented in Sections
2, 3, 4, 5, respectively.
In Section 6, we compare the
performance of NAS algorithms on the CIFAR-10 and
ImageNet dataset, and discuss several subtopics of great
concern in NAS community: one/two-stage NAS, one-shot
NAS, joint hyperparameter and architecture optimization,
and resource-aware NAS. In Section 7, we describe several
open problems in AutoML. We conclude our survey in
Section 8.
2. Data Preparation
The ﬁrst step in the ML pipeline is data preparation.
Figure 2 presents the workﬂow of data preparation, which
can be introduced in three aspects: data collection, data
cleaning, and data augmentation.
Data collection is a
necessary step to build a new dataset or extend the existing dataset. The process of data cleaning is used to
ﬁlter noisy data so that downstream model training is not
compromised. Data augmentation plays an important role
in enhancing model robustness and improving model performance. The following subsections will cover the three
aspects in more detail.
Data Collection
Enough data?
Data quality
Any exsiting
Augmentation
Data Cleaning
Figure 2: The ﬂow chart for data preparation.
2.1. Data Collection
ML’s deepening study has led to a consensus that highquality datasets are of critical importance for ML; as a
result, numerous open datasets have emerged. In the early
stages of ML study, a handwritten digital dataset, i.e.,
MNIST , was developed.
After that, several larger
datasets like CIFAR-10 and CIFAR-100 and ImageNet
 were developed. A variety of datasets can also be
retrieved by entering the keywords into these websites:
Kaggle 2, Google Dataset Search (GOODS) 3, and Elsevier
Data Search 4.
However, it is usually challenging to ﬁnd a proper
dataset through the above approaches for some particular tasks, such as those related to medical care or other
privacy matters. Two types of methods are proposed to
solve this problem: data searching and data synthesis.
2.1.1. Data Searching
As the Internet is an inexhaustible data source, searching for Web data is an intuitive way to collect a dataset
 . However, there are some problems with
using Web data.
First, the search results may not exactly match the
Thus, unrelated data must be ﬁltered.
2 
3 
4 
example, Krause et al. separate inaccurate results
as cross-domain or cross-category noise, and remove any
images that appear in search results for more than one
category. Vo et al. re-rank relevant results and provide
search results linearly, according to keywords.
Second, Web data may be incorrectly labeled or even
unlabeled. A learning-based self-labeling method is often
used to solve this problem. For example, the active learning method selects the most “uncertain” unlabeled
individual examples for labeling by a human, and then iteratively labels the remaining data. Roh et al. provided
a review of semi-supervised learning self-labeling methods,
which can help take the human out of the loop of labeling
to improve eﬃciency, and can be divided into the following
categories: self-training , co-training , and
co-learning . Moreover, due to the complexity of Web
images content, a single label cannot adequately describe
an image. Consequently, Yang et al. assigned multiple
labels to a Web image, i.e., if the conﬁdence scores of these
labels are very close or the label with the highest score is
the same as the original label of the image, then this image
will be set as a new training sample.
However, the distribution of Web data can be extremely
diﬀerent from that of the target dataset, which will increase
the diﬃculty of training the model. A common solution is to
ﬁne-tune these Web data . Yang et al. proposed
an iterative algorithm for model training and Web data-
ﬁltering. Dataset imbalance is another common problem, as
some special classes have a very limited number of Web data.
To solve this problem, the synthetic minority over-sampling
technique (SMOTE) is used to synthesize new minority
samples between existing real minority samples, instead
of simply up-sampling minority samples or down-sampling
the majority samples. In another approach, Guo et al. 
combined the boosting method with data generation to
enhance the generalizability and robustness of the model
against imbalanced data sets.
2.1.2. Data Synthesis
Data simulator is one of the most commonly used methods to generate data. For some particular tasks, such as
autonomous driving, it is not possible to test and adjust a
model in the real world during the research phase, due to
safety hazards. Therefore, a practical approach to generating data is to use a data simulator that matches the real
world as closely as possible. OpenAI Gym is a popular
toolkit that provides various simulation environments, in
which developers can concentrate on designing their algorithms, instead of struggling to generate data. Wang
et al. used a popular game engine, Unreal Engine
4, to build a large synthetic indoor robotics stereo (IRS)
dataset, which provides the information for disparity and
surface normal estimation. Furthermore, a reinforcement
learning-based method is applied in for optimizing the
parameters of a data simulator to control the distribution
of the synthesized data.
Another novel technique for deriving synthetic data
is Generative Adversarial Networks (GANs) , which
can be used to generate images , tabular
 and text data. Karras et al. applied GAN
technique to generate realistic human face images. Oh and
Jaroensri et al. built a synthetic dataset, which captures small motion for video-motion magniﬁcation. Bowles
et al. demonstrated the feasibility of using GAN to
generate medical images for brain segmentation tasks. In
the case of textual data, applying GAN to text has proved
diﬃcult because the commonly used method is to use reinforcement learning to update the gradient of the generator,
but the text is discrete, and thus the gradient cannot propagate from discriminator to generator. To solve this problem,
Donahue et al. used an autoencoder to encode sentences into a smooth sentence representation to remove the
barrier of reinforcement learning. Park et al. applied
GAN to synthesize fake tables that are statistically similar
to the original table but do not cause information leakage.
Similarly, in , GAN is applied to generate tabular data
like medical or educational records.
2.2. Data Cleaning
The collected data inevitably have noise, but the noise
can negatively aﬀect the training of the model. Therefore,
the process of data cleaning must be carried out if
necessary. Across the literature, the eﬀort of data cleaning
is shifting from crowdsourcing to automation. Traditionally, data cleaning requires specialist knowledge, but access
to specialists is limited and generally expensive. Hence,
Chu et al. proposed Katara, a knowledge-based and
crowd-powered data cleaning system. To improve eﬃciency,
some studies proposed only to clean a small subset
of the data and maintain comparable results to the case of
cleaning the full dataset. However, these methods require
a data scientist to design what data cleaning operations
are applied to the dataset. BoostClean attempts to
automate this process by treating it as a boosting problem. Each data cleaning operation eﬀectively adds a new
cleaning operation to the input of the downstream ML
model, and through a combination of Boosting and feature
selection, a good series of cleaning operations, which can
well improve the performance of the ML model, can be
generated. AlphaClean transforms data cleaning into
a hyperparameter optimization problem, which further increases automation. Speciﬁcally, the ﬁnal data cleaning
combinatorial operation in AlphaClean is composed of several pipelined cleaning operations that need to be searched
from a predeﬁned search space. Gemp et al. attempted
to use meta-learning technique to automate the process of
data cleaning.
The data cleaning methods mentioned above are applied
to a ﬁxed dataset.
However, the real world generates
vast amounts of data every day.
In other words, how
to clean data in a continuous process becomes a worth
studying problem, especially for enterprises. Ilyas et al.
 proposed an eﬀective way of evaluating the algorithms
Figure 3: A classiﬁcation of data augmentation techniques.
of continuously cleaning data. Mahdavi et al. built
a cleaning workﬂow orchestrator, which can learn from
previous cleaning tasks, and proposed promising cleaning
workﬂows for new datasets.
2.3. Data Augmentation
To some degree, data augmentation (DA) can also be
regarded as a tool for data collection, as it can generate new
data based on the existing data. However, DA also serves as
a regularizer to avoid over-ﬁtting of model training and has
received more and more attention. Therefore, we introduce
DA as a separate part of data preparation in detail. Figure
3 classiﬁes DA techniques from the perspective of data type
(image, audio, and text), and incorporates automatic DA
techniques that have recently received much attention.
For image data, the aﬃne transformations include rotation, scaling, random cropping, and reﬂection; the elastic
transformations contain the operations like contrast shift,
brightness shift, blurring, and channel shuﬄe; the advanced
transformations involve random erasing, image blending,
cutout , and mixup , etc.
These three types of
common transformations are available in some open source
libraries, like torchvision 5, ImageAug , and Albumentations . In terms of neural-based transformations, it
5 
can be divided into three categories: adversarial noise ,
neural style transfer , and GAN technique . For
textual data, Wong et al. proposed two approaches for
creating additional training examples: data warping and
synthetic over-sampling. The former generates additional
samples by applying transformations to data-space, and
the latter creates additional samples in feature-space. Textual data can be augmented by synonym insertion or by
ﬁrst translating the text into a foreign language and then
translating it back to the original language. In a recent
study, Xie et al. proposed a non-domain-speciﬁc DA
policy that uses noising in RNNs, and this approach works
well for the tasks of language modeling and machine translation. Yu et al. proposed a back-translation method
for DA to improve reading comprehension. NLPAug 
is an open-source library that integrates many types of
augmentation operations for both textual and audio data.
The above augmentation techniques still require human to select augmentation operations and then form a
speciﬁc DA policy for speciﬁc tasks, which requires much
expertise and time. Recently, there are many methods
 proposed to search for augmentation policy for diﬀerent tasks.
AutoAugment is a pioneering work to automate the
search for optimal DA policies using reinforcement learning. However, AutoAugment is not eﬃcient as it takes
almost 500 GPU hours for one augmentation search. In
order to improve search eﬃciency, a number of improved
algorithms have subsequently been proposed using diﬀerent
search strategies, such as gradient descent-based ,
Bayesian-based optimization , online hyperparameter
learning , greedy-based search and random search
 . Besides, LingChen et al. proposed a searchfree DA method, namely UniformAugment, by assuming
that the augmentation space is approximately distribution
invariant.
3. Feature Engineering
It is generally accepted that data and features determine the upper bound of ML, and that models and algorithms can only approximate this limit. In this context,
feature engineering aims to maximize the extraction of
features from raw data for use by algorithms and models.
Feature engineering consists of three sub-topics: feature
selection, feature extraction, and feature construction. Feature extraction and construction are variants of feature
transformation, by which a new set of features is created
 . In most cases, feature extraction aims to reduce the
dimensionality of features by applying speciﬁc mapping
functions, while feature construction is used to expand
original feature spaces, and the purpose of feature selection
is to reduce feature redundancy by selecting important features. Thus, the essence of automatic feature engineering
is, to some degree, a dynamic combination of these three
processes.
3.1. Feature Selection
Feature selection builds a feature subset based on the
original feature set by reducing irrelevant or redundant
features. This tends to simplify the model, hence avoiding
overﬁtting and improving model performance. The selected
features are usually divergent and highly correlated with
object values. According to , there are four basic steps
in a typical process of feature selection (see Figure 4), as
Original feature set
Generation
(Search Strategy)
Subset Evaluation
validation
criterion?
Figure 4: The iterative process of feature selection.
A subset of
features is selected, based on a search strategy, and then evaluated.
Then, a validation procedure is implemented to determine whether
the subset is valid.
The above steps are repeated until the stop
criterion is satisﬁed.
The search strategy for feature selection involves three
types of algorithms: complete search, heuristic search, and
random search. Complete search comprises exhaustive and
non-exhaustive searching; the latter can be further split
into four methods: breadth-ﬁrst search, branch and bound
search, beam search, and best-ﬁrst search. Heuristic search
comprises sequential forward selection (SFS), sequential
backward selection (SBS), and bidirectional search (BS).
In SFS and SBS, the features are added from an empty set
or removed from a full set, respectively, whereas BS uses
both SFS and SBS to search until these two algorithms
obtain the same subset. The most commonly used random
search methods are simulated annealing (SA) and genetic
algorithms (GAs).
Methods of subset evaluation can be divided into three
diﬀerent categories. The ﬁrst is the ﬁlter method, which
scores each feature according to its divergence or correlation and then selects features according to a threshold.
Commonly used scoring criteria for each feature are variance, the correlation coeﬃcient, the chi-square test, and
mutual information. The second is the wrapper method,
which classiﬁes the sample set with the selected feature
subset, after which the classiﬁcation accuracy is used as the
criterion to measure the quality of the feature subset. The
third method is the embedded method, in which variable
selection is performed as part of the learning procedure.
Regularization, decision tree, and deep learning are all
embedded methods.
3.2. Feature Construction
Feature construction is a process that constructs new
features from the basic feature space or raw data to enhance
the robustness and generalizability of the model. Essentially, this is done to increase the representative ability of
the original features. This process is traditionally highly
dependent on human expertise, and one of the most commonly used methods is preprocessing transformation, such
as standardization, normalization, or feature discretization.
In addition, the transformation operations for diﬀerent
types of features may vary. For example, operations such
as conjunctions, disjunctions and negation are typically
used for Boolean features; operations such as minimum,
maximum, addition, subtraction, mean are typically used
for numerical features, and operations such as Cartesian
product and M-of-N are commonly used for
nominal features.
It is impossible to manually explore all possibilities.
Hence, to further improve eﬃciency, some automatic feature construction methods have been
proposed to automate the process of searching and evaluating the operation combination, and shown to achieve results
as good as or superior to those achieved by human expertise. Besides, some feature construction methods, such as
decision tree-based methods and genetic algorithms , require a predeﬁned operation space, while
the annotation-based approaches do not, as they can
use domain knowledge (in the form of annotation) and
the training examples, and hence, can be traced back to
the interactive feature-space construction protocol introduced by . Using this protocol, the learner identiﬁes
inadequate regions of feature space and, in coordination
with a domain expert, adds descriptiveness using existing
semantic resources. After selecting possible operations and
constructing a new feature, feature-selection techniques are
applied to evaluate the new feature.
3.3. Feature Extraction
Feature extraction is a dimensionality-reduction process
performed via some mapping functions. It extracts informative and non-redundant features according to certain
metrics. Unlike feature selection, feature extraction alters
the original features. The kernel of feature extraction is a
mapping function, which can be implemented in many ways.
The most prominent approaches are principal component
analysis (PCA), independent component analysis, isomap,
nonlinear dimensionality reduction, and linear discriminant
analysis (LDA). Recently, the feed-forward neural network
approach has become popular; this uses the hidden units
of a pretrained model as extracted features. Furthermore,
many autoencoder-based algorithms are proposed; for example, Zeng et al. proposed a relation autoencoder
model that considers data features and their relationships,
Model Generation
Entire-structured
Search Space
Architecture Optimization
Random search
Reinforcement
Evolutionary
Gradient Descent
Optimization
Model Estimation
Low-fidelity
Early-stopping
Surrogate Model
Weight-sharing
Cell-based
Hierarchical
Morphism-based
Figure 5: An overview of neural architecture search pipeline.
while an unsupervised feature-extraction method using
autoencoder trees is proposed by .
4. Model Generation
Model generation is divided into two parts––search
space and optimization methods––as shown in Figure 1.
The search space deﬁnes the model structures that can be
designed and optimized in principle. The types of models
can be broadly divided into two categories: traditional
ML models, such as support vector machine (SVM) 
and k-nearest neighbors algorithm (KNN) , and deep
neural network (DNN). There are two types of parameters
for the optimization methods: hyperparameters used for
training, such as the learning rate, and those used for model
design, such as the ﬁlter size and the number of layers
for DNN. Neural architecture search (NAS) has recently
attracted considerable attention; therefore, in this section,
we introduce the search space and optimization methods of
NAS technique. Readers who are interested in traditional
models (e.g., SVM) can refer to other reviews .
Figure 5 presents an overview of the NAS pipeline,
which is categorized into the following three dimensions
 : search space, architecture optimization (AO)
method6, and model evaluation method.
• Search Space. The search space deﬁnes the design
principles of neural architectures. Diﬀerent scenarios
require diﬀerent search spaces. Here, we summarize
four types of commonly used search spaces: entirestructured, cell-based, hierarchical, and morphismbased.
6It can also be referred to as the “search strategy ”,
“search policy ”, or “optimization method ”.
• Architecture Optimization Method. The architecture
optimization (AO) method deﬁnes how to guide the
search to eﬃciently ﬁnd the model architecture with
high performance after the search space is deﬁned.
• Model Evaluation Method. Once a model is generated, its performance needs to be evaluated. The
simplest approach of doing this is to train the model
to converge on the training set, and then estimate
model performance on the validation set; however,
this method is time-consuming and resource-intensive.
Some advanced methods can accelerate the evaluation process but lose ﬁdelity in the process. Thus,
how to balance the eﬃciency and eﬀectiveness of an
evaluation is a problem worth studying.
The search space and AO methods are presented in
this section, while the methods of model evaluation are
presented in the next section.
4.1. Search Space
A neural architecture can be represented as a direct
acyclic graph (DAG) comprising B ordered nodes. In DAG,
each node and directed edge indicate a feature tensor and
an operation, respectively. Eq. 1 presents a formula for
computation at any node Zk, k ∈{1, 2, ..., B}.
Figure 6: Two simpliﬁed examples of entire-structured neural architectures. Each layer is speciﬁed with a diﬀerent operation, such as
convolution and max-pooling operations. The edge indicates the information ﬂow. The skip-connection operation used in the right example
can help explore deeper and more complex neural architectures.
oi(Ii), oi ∈O
where Nk indicates the indegree of node Zk, Ii and oi
represent i-th input tensor and its associated operation,
respectively, and O is a set of candidate operations, such
as convolution, pooling, activation functions, skip connection, concatenation, and addition. To further enhance the
model performance, many NAS methods use certain advanced human-designed modules as primitive operations,
such as depth-wise separable convolution , dilated
convolution , and squeeze-and-excitation (SE) blocks
 . The selection and combination of these operations
vary with the design of search space. In other words, the
search space deﬁnes the structural paradigm that AO methods can explore; thus, designing a good search space is a
vital but challenging problem. In general, a good search
space is expected to exclude human bias and be ﬂexible
enough to cover a wider variety of model architectures.
Based on the existing NAS studies, we detail the commonly used search spaces as follows.
4.1.1. Entire-structured Search Space
The space of entire-structured neural networks 
is one of the most intuitive and straightforward search
spaces. Figure 6 presents two simpliﬁed examples of entirestructured models, which are built by stacking a predeﬁned
number of nodes, where each node represents a layer and
performs a speciﬁed operation. The left model shown in
Figure 6 indicates the simplest structure, while the right
model is relatively complex, as it permits arbitrary skip
connections to exist between the ordered nodes; these
connections have been proven eﬀective in practice . Although an entire structure is easy to implement, it has
several disadvantages. For example, it is widely accepted
that the deeper is the model, the better is its generalization ability; however, searching for such a deep network
is onerous and computationally expensive. Furthermore,
the generated architecture lacks transferability; that is, a
model generated on a small dataset may not ﬁt a larger
dataset, which necessitates the generation of a new model
for a larger dataset.
4.1.2. Cell-based Search Space
Motivation. To enable the transferability of the generated model, the cell-based search space has been proposed
 , in which the neural architecture is composed
of a ﬁxed number of repeating cell structures. This design approach is based on the observation that many wellperforming human-designed models are also built
by stacking a ﬁxed number of modules. For example, the
ResNet family builds many variants, such as ResNet50,
ResNet101, and ResNet152, by stacking several BottleNeck
modules . Throughout the literature, this repeated module is referred to as a motif, cell, or block, while in this
paper, we call it a cell.
Design. Figure 7 (left) presents an example of a ﬁnal
cell-based neural network, which comprises two types of
cells: normal and reduction cells. Thus, the problem of
searching for a full neural architecture is simpliﬁed into
normal cell
reduction cell
normal cell
reduction cell
normal cell
reduction cell
Figure 7: (Left) Example of a cell-based model comprising three
motifs, each with n normal cells and one reduction cell.
Example of a normal cell, which contains two blocks, each having two
nodes. Each node is speciﬁed with a diﬀerent operation and input.
searching for an optimal cell structure in the context of
cell-based search space. Besides, the output of the normal
cell retains the same spatial dimension as the input, and
the number of normal cell repeats is usually set manually
based on the actual demand. The reduction cell follows
behind a normal cell and has a similar structure to that of
the normal cell, with the diﬀerences being that the width
and height of the output feature maps of the reduction
cell are half the input, and the number of channels is
twice the input. This design approach follows the common
practice of manually designing neural networks. Unlike
the entire-structured search space, the model built on cellbased search space can be expanded to form a larger model
by simply adding more cells without re-searching for the
cell structure. Meanwhile, many approaches 
have experimentally demonstrated the transferability of
the model generated in cell-based search space, such as the
model built on CIFAR-10, which can also achieve comparable results to those achieved by SOTA human-designed
models on ImageNet.
The design paradigm of the internal cell structure of
most NAS studies refers to Zoph et al. , who were
among the ﬁrst to propose the exploration of cell-based
search space. Figure 7 (right) shows an example of a normal
cell structure. Each cell contains B blocks (here B = 2),
and each block has two nodes. Each node in a block can be
assigned diﬀerent operations and receive diﬀerent inputs.
The output of two nodes in the block can be combined
through addition or concatenation operation. Therefore,
each block can be represented by a ﬁve-element tuple,
(I1, I2, O1, O2, C), where I1, I2 ∈Ib indicate the inputs to
the block, while O1, O2 ∈O indicate the operations applied
to inputs, and C ∈C describes how to combine O1 and
O2. As the blocks are ordered, the set of candidate inputs
Ib for the nodes in block bk, which contains the output of
the previous two cells and the output set of all previous
blocks {bi, i < k} of the same cell. The ﬁrst two inputs of
the ﬁrst cell of the whole model are set to the image data
by default.
In the actual implementation, certain essential details
need to be noted. First, the number of channels may diﬀer
for diﬀerent inputs. A commonly used solution is to apply a
calibration operation on each node’s input tensor to ensure
that all inputs have the same number of channels. The
calibration operation generally uses 1×1 convolution ﬁlters,
such that it will not change the width and height of the
input tensor, but keep the channel number of all input
tensors consistent. Second, as mentioned above, the input
of a node in a block can be obtained from the previous two
cells or the previous blocks within the same cell; hence,
the cell’s output must have the same spatial resolution. To
this end, if the input/output resolutions are diﬀerent, the
calibration operation has stride 2; otherwise, it has stride
1. Besides, all blocks have stride 1.
Complexity. Searching for a cell structure is more
eﬃcient than searching for an entire structure. To illustrate
this, let us assume that there are M predeﬁned candidate
operations, the number of layers for both entire and the
cell-based structures is L, and the number of blocks in a
cell is B. Then, the number of possible entire structures
can be expressed as:
Nentire = M L × 2
The number of possible cells is (M B × (B + 2)!)2. However,
as there are two types of cells (i.e., normal and reduction cells), the ﬁnal size of the cell-based search space is
calculated as
Ncell = (M B × (B + 2)!)4
Evidently, the complexity of searching for the entire structure grows exponentially with the number of layers. For
an intuitive comparison, we assign the variables in the
2 and 3 the typical value in the literature, i.e.,
M = 5, L = 10, B = 3; then Nentire = 3.44 × 1020 is
much larger than Ncell = 5.06 × 1016.
Two-stage Gap.
The NAS methods of cell-based
search space usually comprise two phases: search and evaluation. First, in the search phase, the best-performing
model is selected, and then, in the evaluation phase, it is
trained from scratch or ﬁne-tuned. However, there exists a
large gap in the model depth between the two phases. As
Figure 8 (left) shows, for DARTS , the generated model
in the search phase only comprises eight cells for reducing
the GPU memory consumption, while in the evaluation
phase, the number of cells is extended to 20. Although the
Estimation
Estimation
Figure 8: Diﬀerence between DARTS and P-DARTS . Both
methods search and evaluate networks on the CIFAR-10 dataset. As
the number of cell structures increases from 5 to 11 and then 17, the
number of candidate operations is gradually reduced accordingly.
search phase ﬁnds the best cell structure for the shallow
model, this does not mean that it is still suitable for the
deeper model in the evaluation phase. In other words,
simply adding more cells may deteriorate the model performance. To bridge this gap, Chen et al. proposed an
improved method based on DARTS, namely progressive-
DARTS (P-DARTS), which divides the search phase into
multiple stages and gradually increases the depth of the
searched networks at the end of each stage, hence bridging
the gap between search and evaluation. However, increasing the number of cells in the search phase may result in
heavier computational overhead. Thus, for reducing the
computational consumption, P-DARTS gradually reduces
the number of candidate operations from 5 to 3, and then
2, through search space approximation methods, as shown
in Figure 8. Experimentally, P-DARTS obtains a 2.50%
error rate on the CIFAR-10 test dataset, outperforming
the 2.83% error rate achieved by DARTS.
4.1.3. Hierarchical Search Space
Figure 9: Network-level search space proposed by . The blue
point (top-left) indicates the ﬁxed “stem” structure, the remaining gray and orange points are cell structure, as described above.
The black arrows along the orange points indicate the ﬁnal selected
network-level structure. “d” and “L” indicate the down sampling
rate and layer, respectively.
The cell-based search space enables the transferability
of the generated model, and most of the cell-based methods
 follow a two-level hierarchy: the inner
is the cell level, which selects the operation and connection
for each node in the cell, and the outer is the network level,
which controls the spatial-resolution changes. However,
these approaches focus on the cell level and ignore the
network level. As shown in Figure 7, whenever a ﬁxed
number of normal cells are stacked, the spatial dimension
of the feature maps is halved by adding a reduction cell.
To jointly learn a suitable combination of repeatable cell
and network structures, Liu et al. deﬁned a general
formulation for a network-level structure, depicted in Figure
9, from which many existing good network designs can be
reproduced. In this way, we can fully explore the diﬀerent
number of channels and sizes of feature maps of each layer
in the network.
max-pooling
level-three
Figure 10: Example of a three-level hierarchical architecture representation. The level-one primitive operations are assembled into
level-two cells. The level-two cells are viewed as primitive operations
and assembled into level-three cell.
In terms of the cell level, the number of blocks (B) in a
cell is still manually predeﬁned and ﬁxed in the search stage.
In other words, B is a new hyperparameter that requires
tuning by human input. To address this problem, Liu et al.
 proposed a novel hierarchical genetic representation
scheme, namely HierNAS, in which a higher-level cell is
generated by iteratively incorporating lower-level cells. As
shown in Figure 10, level-one cells can be some primitive
operations, such as 1 × 1 and 3 × 3 convolution and 3 × 3
max-pooling, and are the basic components of level-two
cells. Then, level-two cells are used as primitive operations
to generate level-three cells. The highest-level cell is a single
motif corresponding to the full architecture. Besides, a
higher-level cell is deﬁned by a learnable adjacency uppertriangular matrix G, where Gij = k indicates that the
k-th operation 0k is implemented between nodes i and j.
For example, the level-two cell shown in Figure 10(a) is
deﬁned by a matrix G, where G01 = 2, G02 = 1, G12 = 0
(the index starts from 0). This method can identify more
types of cell structures with more complex and ﬂexible
topologies. Similarly, Liu et al. proposed progressive
NAS (PNAS) to search for the cell progressively, starting
from the simplest cell structure, which is composed of only
one block, and then expanding to a higher-level cell by
adding more possible block structures. Moreover, PNAS
improves the search eﬃciency by using a surrogate model
to predict the top-k promising blocks from the search space
at each stage of cell construction.
For both HierNAS and PNAS, once a cell structure is
searched, it is used in all network layers, which limits the
layer diversity. Besides, for achieving both high accuracy
and low latency, some studies proposed to search
for complex and fragmented cell structures. For example,
Tan et al. proposed MnasNet, which uses a novel
factorized hierarchical search space to generate diﬀerent
cell structures, namely MBConv, for diﬀerent layers of the
ﬁnal network. Figure 11 presents the factorized hierarchical
search space of MnasNet, which comprises a predeﬁned
number of cell structures. Each cell has a diﬀerent structure and contains a variable number of blocks––whereas all
blocks in the same cell exhibit the same structure, those
in other cells exhibit diﬀerent structures. As this design
method can achieve a suitable balance between model performance and latency, many subsequent studies 
have referred to it.
Owing to the large computational
consumption, most of the diﬀerentiable NAS (DNAS) techniques (e.g., DARTS) ﬁrst search for a suitable cell structure on a proxy dataset (e.g., CIFAR10), and then transfer
it to a larger target dataset (e.g., ImageNet). Han et al.
 proposed ProxylessNAS, which can directly search
for neural networks on the targeted dataset and hardware
platforms by using BinaryConnect , which addresses
the high memory consumption issue.
Block 3-B3
Block 1-B1
Figure 11: Factorized hierarchical search space in MnasNet .
The ﬁnal network comprises diﬀerent cells. Each cell is composed of
a variable number of repeated blocks, where the block in the same
cell shares the same structure but diﬀers from that in the other cells.
4.1.4. Morphism-based Search Space
Isaac Newton is reported to have said that “If I have
seen further, it is by standing on the shoulders of giants.”
Similarly, several training tricks have been proposed, such
as knowledge distillation and transfer learning .
However, these methods do not directly modify the model
structure.
To this end, Chen et al.
 proposed the
Net2Net technique for designing new neural networks based
on an existing network by inserting identity morphism
(IdMorph) transformations between the neural network
layers. An IdMorph transformation is function-preserving
and can be classiﬁed into two types – depth and width
IdMorph (shown in Figure 12) – which makes it possible to
replace the original model with an equivalent model that
is deeper or wider.
However, IdMorph is limited to width and depth changes,
and can only modify them separately; moreover, the sparsity of its identity layer can create problems . Therefore, an improved method is proposed, namely network
morphism , which allows a child network to inherit
all knowledge from its well-trained parent network and
continue to grow into a more robust network within a
shortened training time. Compared with Net2Net, network morphism exhibits the following advantages: 1) it can
embed nonidentity layers and handle arbitrary nonlinear
activation functions, and 2) it can simultaneously perform
depth, width, and kernel size-morphing in a single operation, whereas Net2Net has to separately consider depth
and width changes. The experimental results in show
that network morphism can substantially accelerate the
training process, as it uses one-ﬁfteenth of the training
time and achieves better results than the original VGG16.
Deeper Net
Initial Net
Depth IdMorph
Width IdMorph
Figure 12: Net2DeeperNet and Net2WiderNet transformations in
 . “IdMorph” refers to identity morphism operation. The value on
each edge indicates the weight.
Several subsequent studies are based on network morphism. For instance,
Jin et al. proposed a framework that enables Bayesian
optimization to guide the network morphism for an eﬃcient neural architecture search. Wei et al. further
improved network morphism at a higher level, i.e., by morphing a convolutional layer into the arbitrary module of a
neural network. Additionally, Tan and Le proposed
EﬃcientNet, which re-examines the eﬀect of model scaling
on convolutional neural networks, and proved that carefully
balancing the network depth, width, and resolution can
lead to better performance.
4.2. Architecture Optimization
After deﬁning the search space, we need to search for
the best-performing architecture, a process we call architecture optimization (AO). Traditionally, the architecture of a
neural network is regarded as a set of static hyperparameters that are tuned based on the performance observed on
the validation set. However, this process highly depends
on human experts and requires considerable time and resources for trial and error. Therefore, many AO methods
have been proposed to free humans from this tedious procedure and to search for novel architectures automatically.
Below, we detail the commonly used AO methods.
4.2.1. Evolutionary Algorithm
The evolutionary algorithm (EA) is a generic populationbased metaheuristic optimization algorithm that takes inspiration from biological evolution. Compared with traditional optimization algorithms such as exhaustive methods,
EA is a mature global optimization method with high
robustness and broad applicability. It can eﬀectively address the complex problems that traditional optimization
algorithms struggle to solve, without being limited by the
problem’s nature.
Encoding Scheme.
Diﬀerent EAs may use diﬀerent types of encoding schemes for network representation.
There are two types of encoding schemes: direct and indirect.
Direct encoding is a widely used method that explicitly
speciﬁes the phenotype. For example, genetic CNN 
encodes the network structure into a ﬁxed-length binary
string, e.g., 1 indicates that two nodes are connected, and
vice versa. Although binary encoding can be performed
easily, its computational space is the square of the number
of nodes, which is ﬁxed-length, i.e., predeﬁned manually.
For representing variable-length neural networks, DAG encoding is a promising solution . For example,
Suganuma et al. used the Cartesian genetic programming (CGP) encoding scheme to represent a
neural network built by a list of sub-modules that are de-
ﬁned as DAG. Similarly, in , the neural architecture
is also encoded as a graph, whose vertices indicate rank-3
tensors or activations (with batch normalization performed
with rectiﬁed linear units (ReLUs) or plain linear units)
and edges indicate identity connections or convolutions.
Neuro evolution of augmenting topologies (NEAT) 
also uses a direct encoding scheme, where each node and
connection is stored.
Indirect encoding speciﬁes a generation rule to build
the network and allows for a more compact representation.
Cellular encoding (CE) is an example of a system
that utilizes indirect encoding of network structures. It
encodes a family of neural networks into a set of labeled
trees and is based on a simple graph grammar. Some recent
studies have described the use of indirect
encoding schemes to represent a network. For example,
the network in can be encoded by a function, and
Initialization
Termination
Figure 13: Overview of the evolutionary algorithm.
each network can be modiﬁed using function-preserving
network morphism operators. Hence, the child network has
increased capacity and is guaranteed to perform at least
as well as the parent networks.
Four Steps. A typical EA comprises the following
steps: selection, crossover, mutation, and update (Figure
• Selection This step involves selecting a portion of
the networks from all generated networks for the
crossover, which aims to maintain well-performing
neural architectures while eliminating the weak ones.
The following three strategies are adopted for network
selection. The ﬁrst is ﬁtness selection, in which the
probability of a network being selected is proportional
to its ﬁtness value, i.e., P(hi) =
F itness(hi)
j=1 F itness(hj),
where hi indicates the i-th network. The second is
rank selection, which is similar to ﬁtness selection,
but with the network’s selection probability being
proportional to its relative ﬁtness rather than its
absolute ﬁtness. The third method is tournament
selection . Here, in each iteration, k
(tournament size) networks are randomly selected
from the population and sorted according to their
performance; then, the best network is selected with
a probability of p, the second-best network has a
probability of p × (1 −p), and so on.
• Crossover After selection, every two networks are selected to generate a new oﬀspring network, inheriting
half of the genetic information of each of its parents.
This process is analogous to the genetic recombination, which occurs during biological reproduction and
crossover. The particular manner of crossover varies
and depends on the encoding scheme. In binary encoding, networks are encoded as a linear string of
bits, where each bit represents a unit, such that two
parent networks can be combined through one- or
multiple-point crossover. However, the crossover of
the data arranged in such a fashion can sometimes
damage the data. Thus, Xie et al. denoted the
basic unit in a crossover as a stage rather than a
bit, which is a higher-level structure constructed by
a binary string. For cellular encoding, a randomly selected sub-tree is cut from one parent tree to replace
a sub-tree cut from the other parent tree. In another
approach, NEAT performs an artiﬁcial synapsis based
on historical markings, adding a new structure without losing track of the gene present throughout the
simulation.
• Mutation As the genetic information of the parents
is copied and inherited by the next generation, gene
mutation also occurs. A point mutation is
one of the most widely used operations and involves
randomly and independently ﬂipping each bit. Two
types of mutations have been described in : one
enables or disables a connection between two layers, and the other adds or removes skip connections
between two nodes or layers. Meanwhile, Real and
Moore et al. predeﬁned a set of mutation operators, such as altering the learning rate and removing
skip connections between the nodes. By analogy with
the biological process, although a mutation may appear as a mistake that causes damage to the network
structure and leads to a loss of functionality, it also
enables the exploration of more novel structures and
ensures diversity.
• Update Many new networks are generated by completing the above steps, and considering the limitations on computational resources, some of these must
be removed. In , the worst-performing network
of two randomly selected networks is immediately
removed from the population. Alternatively, in ,
the oldest networks are removed. Other methods
 discard all models at regular intervals.
However, Liu et al. did not remove any network
from the population, and instead, allowed the network number to grow with time. Zhu et al. 
regulated the population number through a variable
λ, i.e., removed the worst model with probability λ
and the oldest model with 1 −λ.
4.2.2. Reinforcement Learning
Zoph et al. were among the ﬁrst to apply reinforcement learning (RL) to neural architecture search. Figure 14
presents an overview of an RL-based NAS algorithm. Here,
the controller is usually a recurrent neural network (RNN)
that executes an action At at each step t to sample a new architecture from the search space and receives an observation
of the state St together with a reward scalar Rt from the
environment to update the controller’s sampling strategy.
Environment refers to the use of a standard neural network training procedure to train and evaluate the network
generated by the controller, after which the corresponding
Controller
Environment
action At: sample an architecture
Figure 14: Overview of neural architecture search using reinforcement
results (such as accuracy) are returned. Many follow-up
approaches have used this framework, but
with diﬀerent controller policies and neural-architecture
encoding. Zoph et al. ﬁrst used the policy gradient
algorithm to train the controller, and sequentially
sampled a string to encode the entire neural architecture.
In a subsequent study , they used the proximal policy
optimization (PPO) algorithm to update the controller, and proposed the method shown in Figure 15 to
build a cell-based neural architecture. MetaQNN is a
meta-modeling algorithm using Q-learning with an ϵ-greedy
exploration strategy and experience replay to sequentially
search for neural architectures.
Block 1 of cell k
prediction
Block 2 of cell k
Figure 15: Example of a controller generating a cell structure. Each
block in the cell comprises two nodes that are speciﬁed with diﬀerent
operations and inputs. The indices −2 and −1 indicate the inputs
are derived from prev-previous and previous cell, respectively.
Although the above RL-based algorithms have achieved
SOTA results on the CIFAR-10 and Penn Treebank (PTB)
 datasets, they incur considerable time and computational resources. For instance, the authors in took 28
days and 800 K40 GPUs to search for the best-performing
architecture, and MetaQNN also took 10 days and 10
GPUs to complete its search. To this end, some improved
RL-based algorithms have been proposed. BlockQNN 
uses a distributed asynchronous framework and an earlystop strategy to complete searching on only one GPU
within 20 hours. The eﬃcient neural architecture search
(ENAS) is even better, as it adopts a parameter-sharing
strategy in which all child architectures are regarded as
sub-graphs of a supernet; this enables these architectures
to share parameters, obviating the need to train each child
model from scratch. Thus, ENAS took only approximately
10 hours using one GPU to search for the best architecture
on the CIFAR-10 dataset, which is nearly 1000× faster
than .
4.2.3. Gradient Descent
The above-mentioned search strategies sample neural
architectures from a discrete search space. A pioneering algorithm, namely DARTS , was among the ﬁrst gradient
descent (GD)-based method to search for neural architectures over a continuous and diﬀerentiable search space by
using a softmax function to relax the discrete space, as
outlined below:
where o(x) indicates the operation performed on input
i,j indicates the weight assigned to the operation ok
between a pair of nodes (i, j), and K is the number of
predeﬁned candidate operations. After the relaxation, the
task of searching for architectures is transformed into a
joint optimization of neural architecture α and the weights
of this neural architecture θ. These two types of parameters
are optimized alternately, indicating a bilevel optimization
Speciﬁcally, α and θ are optimized with the
validation and the training sets, respectively. The training
and the validation losses are denoted by Ltrain and Lval,
respectively. Hence, the total loss function can be derived
as follows:
Lval (θ∗, α)
θ∗= argminθ Ltrain(θ, α)
Figure 16 presents an overview of DARTS, where a cell
is composed of N (here N = 4) ordered nodes and the
node zk (k starts from 0) is connected to the node zi, i ∈
{k + 1, ..., N}. The operation on each edge ei,j is initially
a mixture of candidate operations, each being of equal
weight. Therefore, the neural architecture α is a supernet
that contains all possible child neural architectures. At
the end of the search, the ﬁnal architecture is derived by
retaining only the maximum-weight operation among all
mixed operations.
Although DARTS substantially reduces the search time,
it incurs several problems. First, as Eq. 5 shows, DARTS
describes a joint optimization of the neural architecture
and weights as a bilevel optimization problem. However,
this problem is diﬃcult to solve directly, because both architecture α and weights θ are high dimensional parameters.
Another solution is single-level optimization, which can be
formalized as
θ,α Ltrain(θ, α)
which optimizes both neural architecture and weights together. Although the single-level optimization problem
can be eﬃciently solved as a regular training, the searched
architecture α commonly overﬁts the training set and its
performance on the validation set cannot be guaranteed.
The authors in proposed mixed-level optimization:
α,θ [Ltrain (θ∗, α) + λLval (θ∗, α)]
where α indicates the neural architecture, θ is the weight assigned to it, and λ is a non-negative regularization variable
to control the weights of the training loss and validation
loss. When λ = 0, Eq. 7 reduces to a single-level optimization (Eq. 6); in contrast, Eq. 7 becomes a bilevel
optimization (Eq. 5). The experimental results presented
in showed that mixed-level optimization not only overcomes the overﬁtting issue of single-level optimization but
also avoids the gradient error of bilevel optimization.
Second, in DARTS, the output of each edge is the
weighted sum of all candidate operations (shown in Eq.
4) during the whole search stage, which leads to a linear
increase in the requirements of GPU memory with the
number of candidate operations. To reduce resource consumption, many subsequent studies 
have developed a diﬀerentiable sampler to sample a child
architecture from the supernet by using a reparameterization trick, namely Gumbel Softmax . The neural
architecture is fully factorized and modeled with a concrete
distribution , which provides an eﬃcient approach to
sampling a child architecture and allows gradient backpropagation. Therefore, Eq. 4 is re-formulated as
i,j = −log(−log(uk
i,j)) is the k-th Gumbel sample,
i,j is a uniform random variable, and τ is the Softmax
temperature. When τ →∞, the possibility distribution of
all operations between each node pair approximates to onehot distribution. In GDAS , only the operation with
the maximum possibility for each edge is selected during
the forward pass, while the gradient is backpropagated
according to Eq. 8. In other words, only one path of the
supernet is selected for training, thereby reducing the GPU
memory usage.
Besides, ProxylessNAS alleviates
the huge resource consumption through path binarization.
Speciﬁcally, it transforms the real-valued path weights 
to binary gates, which activates only one path of the mixed
operations, and hence, solves the memory issue.
Another problem is the optimization of diﬀerent operations together, as they may compete with each other,
leading to a negative inﬂuence. For example, several studies
 have found that skip-connect operation dominates at a later search stage in DARTS, which causes the
network to be shallower and leads to a marked deterioration
in performance. To solve this problem, DARTS+ uses
an additional early-stop criterion, such that when two or
Figure 16: Overview of DARTS. (a) The data can only ﬂow from lower-level nodes to higher-level nodes, and the operations on edges are
initially unknown. (b) The initial operation on each edge is a mixture of candidate operations, each having equal weight. (c) The weight of
each operation is learnable and ranges from 0 to 1, but for previous discrete sampling methods, the weight could only be 0 or 1. (d) The ﬁnal
neural architecture is constructed by preserving the maximum weight-value operation on each edge.
more skip-connects occur in a normal cell, the search process stops. In another example, P-DARTS regularizes
the search space by executing operation-level dropout to
control the proportion of skip-connect operations occurring
during training and evaluation.
4.2.4. Surrogate Model-based Optimization
Another group of architecture optimization methods is
surrogate model-based optimization (SMBO) algorithms
 . The
core concept of SMBO is that it builds a surrogate model
of the objective function by iteratively keeping a record
of past evaluation results, and uses the surrogate model
to predict the most promising architecture. Thus, these
methods can substantially shorten the search time and
improve eﬃciency.
SMBO algorithms diﬀer from the surrogate models,
which can be broadly divided into Bayesian optimization
(BO) methods (including Gaussian process (GP) ,
random forest (RF) , tree-structured Parzen estimator
(TPE) ), and neural networks .
BO is one of the most popular methods for
hyperparameter optimization. Many recent studies have attempted to apply
these SOTA BO methods to AO. For example, in , the validation results of the generated
neural architectures were modeled as a Gaussian process,
which guides the search for the optimal neural architectures.
However, in GP-based BO methods, the inference time
scales cubically in the number of observations, and they
cannot eﬀectively handle variable-length neural networks.
Camero et al. proposed three ﬁxed-length encoding
schemes to cope with variable-length problems by using
RF as the surrogate model. Similarly, both and 
used RF as a surrogate model, and showed that it
works better in setting high dimensionality than GP-based
Instead of using BO, some studies have used a neural
network as the surrogate model. For example, in PNAS
 and EPNAS , an LSTM is derived as the surrogate
model to progressively predict variable-sized architectures.
Meanwhile, NAO uses a simpler surrogate model, i.e.,
multilayer perceptron (MLP), and NAO is more eﬃcient
and achieves better results on CIFAR-10 than does PNAS
 . White et al. trained an ensemble of neural
networks to predict the mean and variance of the validation
results for candidate neural architectures.
4.2.5. Grid and Random Search
Both grid search (GS) and random search (RS) are simple optimization methods applied to several NAS studies
 . For instance, Geifman et al. proposed a modular architecture search space (A = {A(B, i, j)|i ∈
{1, 2, ..., Ncells}, j ∈{1, 2, ..., Nblocks}}) that is spanned
by the grid deﬁned by the two corners A(B, 1, 1) and
A(B, Ncells, Nblocks), where B is a searched block structure. Evidently, a larger value Ncells × Nblocks leads to the
exploration of a larger space, but requires more resources.
The authors in conducted an eﬀectiveness comparison between SOTA NAS methods and RS. The results
showed that RS is a competitive NAS baseline. Speciﬁcally,
RS with an early-stopping strategy performs as well as
ENAS , which is an RL-based leading NAS method.
Besides, Yu et al. demonstrated that the SOTA NAS
techniques are not signiﬁcantly better than random search.
4.2.6. Hybrid Optimization Method
The abovementioned architecture optimization methods
have their own advantages and disadvantages. 1) EA is a
mature global optimization method with high robustness.
However, it requires considerable computational resources
 , and its evolution operations (such as crossover and
mutations) are performed randomly. 2) Although RL-based
methods (e.g., ENAS ) can learn complex architectural
patterns, the searching eﬃciency and stability of the RL
controller are not guaranteed because it may take several
actions to obtain a positive reward. 3) The GD-based methods (e.g., DARTS ) substantially improve the searching
eﬃciency by relaxing the categorical candidate operations
to continuous variables. Nevertheless, in essence, they all
search for a child network from a supernet, which limits the
diversity of neural architectures. Therefore, some methods
have been proposed to incorporate diﬀerent optimization
methods to capture the best of their advantages; these
methods are summarized as follows
EA+RL. Chen et al. integrated reinforced mutations into an EA, which avoids the randomness of evolution
and improves the searching eﬃciency.
Another similar
method developed in parallel is the evolutionary-neural
hybrid controller (Evo-NAS) , which also captures the
merits of both RL-based methods and EA. The Evo-NAS
controller’s mutations are guided by an RL-trained neural
network, which can explore a vast search space and sample
architectures eﬃciently.
EA+GD. Yang et al. combined the EA and GDbased method. The architectures share parameters within
one supernet and are tuned on the training set with a few
epochs. Then, the populations and the supernet are directly inherited in the next generation, which substantially
accelerates the evolution. The authors in only took 0.4
GPU days for searching, which is more eﬃcient than early
EA methods (e.g., AmoebaNet took 3150 GPU days
and 450 GPUs for searching).
EA+SMBO. The authors in used RF as a surrogate to predict model performance, which accelerates the
ﬁtness evaluation in EA.
GD+SMBO. Unlike DARTS, which learns weights
for candidate operations, NAO proposes a variational
autoencoder to generate neural architectures and further
build a regression model as a surrogate to predict the
performance of the generated architecture. The encoder
maps the representations of the neural architecture to
continuous space, and then a predictor network takes the
continuous representations of the neural architecture as
input and predicts the corresponding accuracy. Finally,
the decoder is used to derive the ﬁnal architecture from a
continuous network representation.
4.3. Hyperparameter Optimization
Most NAS methods use the same set of hyperparameters
for all candidate architectures during the whole search stage;
thus, after ﬁnding the most promising neural architecture,
it is necessary to redesign a hyperparameter set and use
it to retrain or ﬁne-tune the architecture. As some HPO
methods (such as BO and RS) have also been applied in
NAS, we will only brieﬂy introduce these methods here.
4.3.1. Grid and Random Search
Figure 17 shows the diﬀerence between grid search (GS)
and random search (RS): GS divides the search space into
regular intervals and selects the best-performing point after
evaluating all points; while RS selects the best point from
a set of randomly drawn points.
Important parameter
Unimportant parameter
Important parameter
Unimportant parameter
Figure 17: Examples of grid search (left) and random search (right) in
nine trials for optimizing a two-dimensional space function f(x, y) =
g(x) + h(y) ≈g(x) . The parameter in g(x) (light-blue part)
is relatively important, while that in h(y) (light-yellow part) is not
important. In a grid search, nine trials cover only three important
parameter values; however, random search can explore nine distinct
values of g.
Therefore, random search is more likely to ﬁnd the
optimal combination of parameters than grid search (the ﬁgure is
adopted from ).
GS is very simple and naturally supports parallel implementation; however, it is computationally expensive and
ineﬃcient when the hyperparameter space is very large, as
the number of trials grows exponentially with the dimensionality of hyperparameters. To alleviate this problem,
Hsu et al. proposed a coarse-to-ﬁne grid search, in
which a coarse grid is ﬁrst inspected to locate a good region, and then a ﬁner grid search is implemented on the
identiﬁed region. Similarly, Hesterman et al. proposed a contracting GS algorithm, which ﬁrst computes
the likelihood of each point in the grid, and then generates
a new grid centered on the maximum-likelihood value. The
point separation in the new grid is reduced to half that
on the old grid. The above procedure is iterated until the
results converge to a local minimum.
Although the authors in empirically and theoretically showed that RS is more practical and eﬃcient than
GS, RS does not promise an optimum value. This means
that although a longer search increases the probability
of ﬁnding optimal hyperparameters, it consumes more resources. Li and Jamieson et al. proposed a hyperband
algorithm to create a tradeoﬀbetween the performance
of the hyperparameters and resource budgets. The hyperband algorithm allocates limited resources (such as time
or CPUs) to only the most promising hyperparameters, by
successively discarding the worst half of the conﬁguration
settings long before the training process is ﬁnished.
4.3.2. Bayesian Optimization
Bayesian optimization (BO) is an eﬃcient method for
the global optimization of expensive blackbox functions.
In this section, we brieﬂy introduce BO. For an in-depth
discussion on BO, we recommend readers to refer to the
excellent surveys conducted in .
BO is an SMBO method that builds a probabilistic
model mapping from the hyperparameters to the objective
metrics evaluated on the validation set. It well balances
exploration (evaluating as many hyperparameter sets as
possible) and exploitation (allocating more resources to
promising hyperparameters).
Algorithm 1 Sequential Model-Based Optimization
INPUT: f, Θ, S, M
D ←INITSAMPLES (f, Θ)
for i in [1, 2, .., T] do
p(y|θ, D) ←FITMODEL (M, D)
θi ←arg maxθ∈Θ S(θ, p(y|θ, D))
yi ←f (θi)
▷Expensive step
D ←D ∪(θi, yi)
The steps of SMBO are expressed in Algorithm 1 (adopted
from ). Here, several inputs need to be predeﬁned initially, including an evaluation function f, search space Θ,
acquisition function S, probabilistic model M, and record
dataset D. Speciﬁcally, D is a dataset that records many
sample pairs (θi, yi), where θi ∈Θ indicates a sampled
neural architecture and yi indicates its evaluation result.
After the initialization, the SMBO steps are described as
1. The ﬁrst step is to tune the probabilistic model M
to ﬁt the record dataset D.
2. The acquisition function S is used to select the next
promising neural architecture from the probabilistic
3. The performance of the selected neural architecture
is evaluated by f, which is an expensive step as it
involves training the neural network on the training
set and evaluating it on the validation set.
4. The record dataset D is updated by appending a new
pair of results (θi, yi).
The above four steps are repeated T times, where T
needs to be speciﬁed according to the total time or resources
available. The commonly used surrogate models for the
BO method are GP, RF, and TPE. Table 2 summarizes
the existing open-source BO methods, where GP is one of
the most popular surrogate models. However, GP scales
cubically with the number of data samples, while RF can
natively handle large spaces and scales better to many data
samples. Besides, Falkner and Klein et al. proposed the
BO-based hyperband (BOHB) algorithm, which combines
the strengths of TPE-based BO and hyperband, and hence,
performs much better than standard BO methods. Furthermore, FABOLAS is a faster BO procedure, which
maps the validation loss and training time as functions of
dataset size, i.e., trains a generative model on a sub-dataset
that gradually increases in size. Here, FABOLAS is 10-100
times faster than other SOTA BO algorithms and identiﬁes
the most promising hyperparameters.
 
 
 
 
 
 
 
 
 
Table 2: Open-source Bayesian optimization libraries. GP, RF, and
TPE represent Gaussian process , random forest , and treestructured Parzen estimator , respectively.
4.3.3. Gradient-based Optimization
Another group of HPO methods are gradient-based optimization (GO) algorithms .
Unlike the above blackbox HPO methods (e.g., GS, RS,
and BO), GO methods use the gradient information to
optimize the hyperparameters and substantially improve
the eﬃciency of HPO. Maclaurin et al. proposed a
reversible-dynamics memory-tape approach to handle thousands of hyperparameters eﬃciently through the gradient
information. However, optimizing many hyperparameters
is computationally challenging. To alleviate this issue, the
authors in used approximate gradient information
rather than the true gradient to optimize continuous hyperparameters, where the hyperparameters can be updated
before the model is trained to converge. Franceschi et al.
 studied both reverse- and forward-mode GO methods. The reverse-mode method diﬀers from the method
proposed in and does not require reversible dynamics;
however, it needs to store the entire training history for
computing the gradient with respect to the hyperparameters. The forward-mode method overcomes this problem by
using real-time updating hyperparameters, and is demonstrated to signiﬁcantly improve the eﬃciency of HPO on
large datasets. Chandra proposed a gradient-based
ultimate optimizer, which can optimize not only the regular
hyperparameters (e.g., learning rate) but also those of the
optimizer (e.g., Adam optimizer ’s moment coeﬃcient
5. Model Evaluation
Once a new neural network has been generated, its
performance must be evaluated. An intuitive method is
to train the network to convergence and then evaluate its
performance. However, this method requires extensive time
and computing resources. For example, took 800 K40
GPUs and 28 days in total to search. Additionally, NASNet
 and AmoebaNet required 500 P100 GPUs and 450
K40 GPUs, respectively. In this section, we summarize
several algorithms for accelerating the process of model
evaluation.
5.1. Low ﬁdelity
As model training time is highly related to the dataset
and model size, model evaluation can be accelerated in different ways. First, the number of images or the resolution
of images (in terms of image-classiﬁcation tasks) can be
decreased. For example, FABOLAS trains the model
on a subset of the training set to accelerate model evaluation. In , ImageNet64×64 and its variants 32×32,
16×16 are provided, while these lower resolution datasets
can retain characteristics similar to those of the original
ImageNet dataset. Second, low-ﬁdelity model evaluation
can be realized by reducing the model size, such as by
training with fewer ﬁlters per layer . By analogy
to ensemble learning, proposes the Transfer Series
Expansion (TSE), which constructs an ensemble estimator
by linearly combining a series of basic low-ﬁdelity estimators, hence avoiding the bias that can derive from using
a single low-ﬁdelity estimator. Furthermore, Zela et al.
 empirically demonstrated that there is a weak correlation between performance after short or long training
times, thus conﬁrming that a prolonged search for network
conﬁgurations is unnecessary.
5.2. Weight sharing
In , once a network has been evaluated, it is dropped.
Hence, the technique of weight sharing is used to accelerate the process of NAS. For example, Wong and Lu et al.
 proposed transfer neural AutoML, which uses knowledge from prior tasks to accelerate network design. ENAS
 shares parameters among child networks, leading to
a thousand-fold faster network design than . Network
morphism based algorithms can also inherit the
weights of previous architectures, and single-path NAS
 uses a single-path over-parameterized ConvNet to
encode all architectural decisions with shared convolutional
kernel parameters.
5.3. Surrogate
The surrogate-based method is another powerful tool that approximates the black-box function. In general, once a good approximation has been obtained, it is trivial to ﬁnd the conﬁgurations that directly
optimize the original expensive objective. For example,
Progressive Neural Architecture Search (PNAS) introduces a surrogate model to control the method of searching.
Although ENAS has been proven to be very eﬃcient, PNAS
is even more eﬃcient, as the number of models evaluated
by PNAS is over ﬁve times that evaluated by ENAS, and
PNAS is eight times faster in terms of total computational
speed. A well-performing surrogate usually requires large
amounts of labeled architectures, while the optimization
space is too large and hard to quantify, and the evaluation of each conﬁguration is extremely expensive . To
alleviate this issue, Luo et al. proposed SemiNAS,
a semi-supervised NAS method, which leverages amounts
of unlabeled architectures to train the surrogate, a controller that is used to predict the accuracy of architectures
without evaluation. Initially, the surrogate is only trained
with a small number of labeled data pairs (architectures,
accuracy), then the generated data pairs will be gradually
added to the original data to further improve the surrogate.
5.4. Early stopping
Early stopping was ﬁrst used to prevent overﬁtting in
classical ML, and it has been used in several recent studies
 to accelerate model evaluation by stopping
evaluations that are predicted to perform poorly on the
validation set. For example, proposes a learning-curve
model that is a weighted combination of a set of parametric
curve models selected from the literature, thereby enabling
the performance of the network to be predicted. Furthermore, presents a novel approach for early stopping
based on fast-to-compute local statistics of the computed
gradients, which no longer relies on the validation set and
allows the optimizer to make full use of all of the training
6. NAS Discussion
In Section 4, we reviewed the various search space and
architecture optimization methods, and in Section 5, we
summarized commonly used model evaluation methods.
These two sections introduced many NAS studies, which
may cause the readers to get lost in details. Therefore, in
this section, we summarize and compare these NAS algorithms’ performance from a global perspective to provide
readers a clearer and more comprehensive understanding of
NAS methods’ development. Then, we discuss some major
topics of the NAS technique.
6.1. NAS Performance Comparison
Many NAS studies have proposed several neural architecture variants, where each variant is designed for diﬀerent
scenarios. For instance, some architecture variants perform
better but are larger, while some are lightweight for a mobile device but with a performance penalty. Therefore, we
only report the representative results of each study. Besides,
to ensure a valid comparison, we consider the accuracy and
algorithm eﬃciency as comparison indices. As the number
(Millions)
ResNet-110 
PyramidNet 
DenseNet 
GeNet#2 (G-50) 
Large-scale ensemble 
Hierarchical-EAS 
CGP-ResSet 
AmoebaNet-B (N=6, F=128)+c/o 
AmoebaNet-B (N=6, F=36)+c/o 
Lemonade 
EENA 
1 Titan Xp
EENA (more channels) 
1 Titan Xp
NASv3+more ﬁlters 
MetaQNN 
NASNet-A (7 @ 2304)+c/o 
NASNet-A (6 @ 768)+c/o 
Block-QNN-Connection more ﬁlter 
Block-QNN-Depthwise, N=3 
ENAS+macro 
ENAS+micro+c/o 
Path-level EAS 
Path-level EAS+c/o 
ProxylessNAS-RL+c/o 
FPNAS 
DARTS(ﬁrst order)+c/o 
DARTS(second order)+c/o 
sharpDARTS 
P-DARTS+c/o 
P-DARTS(large)+c/o 
GDAS+c/o 
SNAS+moderate constraint+c/o 
BayesNAS 
ProxylessNAS-GD+c/o 
PC-DARTS+c/o 
MiLeNAS 
GDAS-NSAS 
NASBOT 
EPNAS 
NAO+random+c/o 
SMASH 
Hierarchical-random 
RandomNAS 
DARTS - random+c/o 
RandomNAS-NSAS 
NAO+weight sharing+c/o 
RENASNet+c/o 
Table 3: Performance of diﬀerent NAS algorithms on CIFAR-10. The “AO” column indicates the architecture optimization method. The
dash (-) indicates that the corresponding information is not provided in the original paper. “c/o” indicates the use of Cutout . RL, EA,
GD, RS, and SMBO indicate reinforcement learning, evolution-based algorithm, gradient descent, random search, and surrogate model-based
optimization, respectively.
(Millions)
ResNet-152 
70.62/95.51
PyramidNet 
SENet-154 
71.32/95.53
DenseNet-201 
78.54/94.46
MobileNetV2 
GeNet#2 
72.13/90.26
AmoebaNet-C(N=4,F=50) 
Hierarchical-EAS 
AmoebaNet-C(N=6,F=228) 
GreedyNAS 
NASNet-A(4@1056)
NASNet-A(6@4032)
Block-QNN 
81.0/95.42
Path-level EAS 
ProxylessNAS(GPU) 
ProxylessNAS-RL(mobile) 
MnasNet 
EﬃcientNet-B0 
EﬃcientNet-B7 
FPNAS 
DARTS (searched on CIFAR-10) 
sharpDARTS 
P-DARTS 
GDAS 
ProxylessNAS-G 
BayesNAS 
FBNet 
AtomNAS 
MiLeNAS 
DSNAS 
74.4/91.54
PC-DARTS 
DenseNAS 
FBNetV2-L1 
PNAS-5(N=3,F=54) 
PNAS-5(N=4,F=216) 
SemiNAS 
Hierarchical-random 
OFA-random 
RENASNet 
Evo-NAS 
Table 4: Performance of diﬀerent NAS algorithms on ImageNet. The “AO” column indicates the architecture optimization method. The dash
(-) indicates that the corresponding information is not provided in the original paper. RL, EA, GD, RS, and SMBO indicate reinforcement
learning, evolution-based algorithm, gradient descent, random search, and surrogate model-based optimization, respectively.
and types of GPUs used vary for diﬀerent studies, we use
GPU Days to approximate the eﬃciency, which is deﬁned
GPU Days = N × D
where N represents the number of GPUs, and D represents
the actual number of days spent searching.
Tables 3 and 4 present the performances of diﬀerent
NAS methods on CIFAR-10 and ImageNet, respectively.
Besides, as most NAS methods ﬁrst search for the neural
architecture based on a small dataset (CIFAR-10), and then
transfer the architecture to a larger dataset (ImageNet),
the search time for both datasets is the same. The tables
show that the early studies on EA- and RL-based NAS
methods focused more on high performance, regardless of
the resource consumption. For example, although AmoebaNet achieved excellent results for both CIFAR-10
and ImageNet, the searching took 3,150 GPU days and 450
GPUs. The subsequent NAS studies attempted to improve
the searching eﬃciency while ensuring the searched model’s
high performance. For instance, EENA elaborately
designs the mutation and crossover operations, which can
reuse the learned information to guide the evolution process, and hence, substantially improve the eﬃciency of
EA-based NAS methods. ENAS is one of the ﬁrst
RL-based NAS methods to adopt the parameter-sharing
strategy, which reduces the number of GPU budgets to
1 and shortens the searching time to less than one day.
We also observe that gradient descent-based architecture
optimization methods can substantially reduce the computational resource consumption for searching, and achieve
SOTA results. Several follow-up studies have been conducted to achieve further improvement and optimization
in this direction. Interestingly, RS-based methods can also
obtain comparable results. The authors in demonstrated that RS with weight-sharing could outperform a
series of powerful methods, such as ENAS and DARTS
6.1.1. Kendall Tau Metric
As RS is comparable to more sophisticated methods
(e.g., DARTS and ENAS), a natural question is, what are
the advantages and signiﬁcance of the other AO algorithms
compared with RS? Researchers have tried to use other
metrics to answer this question, rather than simply considering the model’s ﬁnal accuracy. Most NAS methods
comprise two stages: 1) search for a best-performing architecture on the training set and 2) expand it to a deeper
architecture and estimate it on the validation set. However,
there usually exists a large gap between the two stages. In
other words, the architecture that achieves the best result
in the training set is not necessarily the best one for the
validation set. Therefore, instead of merely considering
the ﬁnal accuracy and search time cost, many NAS studies
 have used Kendall Tau (τ) metric
 to evaluate the correlation of the model performance
between the search and evaluation stages. The parameter
τ is deﬁned as
τ = NC −ND
where NC and ND indicate the numbers of concordant and
discordant pairs. τ is a number in the range [-1,1] with the
Retraining the
best-performing
model of the
searching stage
Evaluation Stage
Architecture
Optimization
Searching Stage
(a) Two-stage NAS comprises the searching stage and evaluation
stage. The best-performing model of the searching stage is further
retrained in the evaluation stage.
Architecture
Optimization
Parameter Training
(b) One-stage NAS can directly deploy a well-performing model
without extra retraining or ﬁne-tuning. The two-way arrow indicates
that the processes of architecture optimization and parameter training
run simultaneously.
Figure 18: Illustration of two- and one-stage neural architecture
search ﬂow.
following properties:
• τ = 1: two rankings are identical
• τ = −1: two rankings are completely opposite.
• τ = 0: there is no relationship between two rankings.
6.1.2. NAS-Bench Dataset
Although Tables 3 and 4 present a clear comparison
between diﬀerent NAS methods, the results of diﬀerent
methods are obtained under diﬀerent settings, such as
training-related hyperparameters (e.g., batch size and training epochs) and data augmentation (e.g., Cutout ). In
other words, the comparison is not quite fair. In this context, NAS-Bench-101 is a pioneering work for improving the reproducibility. It provides a tabular dataset containing 423,624 unique neural networks generated and evaluated from a ﬁxed graph-based search space and mapped
to their trained and evaluated performance on CIFAR-10.
Meanwhile, Dong et al. further built NAS-Bench-201,
which is an extension to NAS-Bench-101 and has a diﬀerent search space, results on multiple datasets (CIFAR-10,
CIFAR-100, and ImageNet-16-120 ), and more diagnostic information. Similarly, Klyuchnikov et al. 
proposed a NAS-Bench for the NLP task. These datasets
enable NAS researchers to focus solely on verifying the effectiveness and eﬃciency of their AO algorithms, avoiding
repetitive training for selected architectures and substantially helping the NAS community to develop.
6.2. One-stage vs. Two-stage
The NAS methods can be roughly divided into two
classes according to the ﬂow ––two-stage and one-stage––
as shown in Figure 18.
Two-stage NAS comprises the searching stage and
evaluation stage. The searching stage involves two processes: architecture optimization, which aims to ﬁnd the
optimal architecture, and parameter training, which is to
train the found architecture’s parameter. The simplest
idea is to train all possible architectures’ parameters from
scratch and then choose the optimal architecture. However,
it is resource-consuming (e.g., NAS-RL took 22,400
GPU days with 800 K40 GPUs for searching) ), which is infeasible for most companies and institutes. Therefore, most
NAS methods (such as ENAS and DARTS ) sample
and train many candidate architectures in the searching
stage, and then further retrain the best-performing architecture in the evaluation stage.
One-stage NAS refers to a class of NAS methods
that can export a well-designed and well-trained neural
architecture without extra retraining, by running AO and
parameter training simultaneously. In this way, the ef-
ﬁciency can be substantially improved. However, model
architecture and its weight parameters are highly coupled; it
is diﬃcult to optimize them simultaneously. Several recent
studies have attempted to overcome this
challenge. For instance, the authors in proposed the
progressive shrinking algorithm to post-process the weights
after the training was completed. They ﬁrst pretrained the
entire neural network, and then progressively ﬁne-tuned
the smaller networks that shared weights with the complete
network. Based on well-designed constraints, the performance of all subnetworks was guaranteed. Thus, given a
target deployment device, a specialized subnetwork can be
directly exported without ﬁne-tuning. However, was
still computational resource-intensive, as the whole process
took 1,200 GPU hours with V100 GPUs. BigNAS revisited the conventional training techniques of stand-alone
networks, and empirically proposed several techniques to
handle a wider set of models, ranging in size from 200M to
1G FLOPs, whereas only handled models under 600M
FLOPs. Both AtomNAS and DSNAS proposed
an end-to-end one-stage NAS framework to further boost
the performance and simplify the ﬂow.
6.3. One-shot/Weight-sharing
One-shot̸=one-stage. Note that one shot is not exactly equivalent to one stage. As mentioned above, we
divide the NAS studies into one- and two-stage methods according to the ﬂow (Figure 18), whereas whether a NAS algorithm belongs to a one-shot method depends on whether
the candidate architectures share the same weights (Figure 19). However, we observe that most one-stage NAS
methods are based on the one-shot paradigm.
Search Space
Search Space
Figure 19: (Left) One-shot models. (Right) Non-one-shot models.
Each circle indicates a diﬀerent model, and its area indicates the
model’s size. We use concentric circles to represent one-shot models,
as they share the weights with each other.
What is One-shot NAS? One-shot NAS methods
embed the search space into an overparameterized supernet,
and thus, all possible architectures can be derived from
the supernet. Figure 18 shows the diﬀerence between the
search spaces of one-shot and non-one-shot NAS. Each
circle indicates a diﬀerent architecture, where the architectures of one-shot NAS methods share the same weights.
One-shot NAS methods can be divided into two categories
according to how to handle AO and parameter training:
coupled and decoupled optimization .
Coupled optimization. The ﬁrst category of oneshot NAS methods optimizes the architecture and weights
in a coupled manner . For instance,
ENAS uses an LSTM network to discretely sample
a new architecture, and then uses a few batches of the
training data to optimize the weight of this architecture.
After repeating the above steps many times, a collection
of architectures and their corresponding performances are
recorded. Finally, the best-performing architecture is selected for further retraining. DARTS uses a similar
weight sharing strategy, but has a continuously parameterized architecture distribution. The supernet contains
all candidate operations, each with learnable parameters.
The best architecture can be directly derived from the
distribution. However, as DARTS directly optimizes
the supernet weights and the architecture distribution, it
suﬀers from vast GPU memory consumption. Although
DARTS-like methods have adopted diﬀerent
approaches to reduce the resource requirements, coupled
optimization inevitably introduces a bias in both architecture distribution and supernet weights , as they
treat all subnetworks unequally. The rapidly converged
architectures can easily obtain more opportunities to be
optimized , and are only a small portion of all
candidates; therefore, it is challenging to ﬁnd the best
architecture.
Another disadvantage of coupling optimization is that
when new architectures are sampled and trained continuously, the weights of previous architectures are negatively
impacted, leading to performance degradation. The authors
in deﬁned this phenomenon as multimodel forgetting.
To overcome this problem, Zhang et al. modeled
supernet training as a constrained optimization problem
of continual learning and proposed novel search-based architecture selection (NSAS) loss function. They applied
the proposed method to RandomNAS and GDAS
 , where the experimental result demonstrated that
the method eﬀectively reduces the multimodel forgetting
and boosting the predictive ability of the supernet as an
evaluator.
Decoupled optimization. The second category of
one-shot NAS methods decouples the
optimization of architecture and weights into two sequential
phases: 1) training the supernet and 2) using the trained
supernet as a predictive performance estimator of diﬀerent
architectures to select the most promising architecture.
In terms of the supernet training phase, the supernet
cannot be directly trained as a regular neural network because its weights are also deeply coupled . Yu et al.
 experimentally showed that the weight-sharing strategy degrades the individual architecture’s performance and
negatively impacts the real performance ranking of the
candidate architectures. To reduce the weight coupling,
many one-shot NAS methods adopt the
random sampling policy, which randomly samples an architecture from the supernet, activating and optimizing only
the weights of this architecture. Meanwhile, RandomNAS
 demonstrates that a random search policy is a competitive baseline method. Although some one-shot approaches
 have adopted the strategy that
samples and trains only one path of the supernet at a time,
they sample the path according to the RL controller ,
Gumbel Softmax , or the BinaryConnect network , which instead highly couples the architecture
and supernet weights. SMASH adopts an auxiliary
hypernetwork to generate weights for randomly sampled
architectures. Similarly, Zhang et al. proposed a
computation graph representation, and used the graph hypernetwork (GHN) to predict the weights for all possible
architectures faster and more accurately than regular hypernetworks . However, through a careful experimental
analysis conducted to understand the weight-sharing strategy’s mechanism, Bender et al. showed that neither a
hypernetwork nor an RL controller is required to ﬁnd the
optimal architecture. They proposed a path dropout strategy to alleviate the problem of weight coupling. During
supernet training, each path of the supernet is randomly
dropped with gradually increasing probability. GreedyNAS
 adopts a multipath sampling strategy to train the
greedy supernet. This strategy focuses on more potentially
suitable paths, and is demonstrated to eﬀectively achieve
a fairly high rank correlation of candidate architectures
compared with RS.
The second phase involves the selection of the most
promising architecture from the trained supernet, which
is the primary purpose of most NAS tasks. Both SMASH
 and randomly selected a set of architectures from
the supernet, and ranked them according to their performance. SMASH can obtain the validation performance of
all selected architectures at the cost of a single training run
for each architecture, as these architectures are assigned
the weights generated by the hypernetwork. Besides, the
authors in observed that the architectures with a
smaller symmetrized KL divergence value are more likely
to perform better. This can be expressed as follows:
DSKL = DKL(p∥q) + DKL(q∥p)
s.t. DKL(p∥q) =
where (p1, ..., pn) and (q1, ..., qn) indicate the predictions of
the sampled architecture and one-shot model, respectively,
and n indicates the number of classes. The cost of calculating the KL value is very small; in , only 64 random
training data examples were used. Meanwhile, EA is also
a promising search solution . For instance, SPOS
 uses EA to search for architectures from the supernet.
It is more eﬃcient than the EA methods introduced in
Section 4, because each sampled architecture only performs
inference. The self-evaluated template network (SETN)
 proposes an estimator to predict the probability of
each architecture having a lower validation loss. The experimental results show that SETN can potentially ﬁnd
an architecture with better performance than RS-based
methods .
6.4. Joint Hyperparameter and Architecture Optimization
Most NAS methods ﬁx the same setting of trainingrelated hyperparameters during the whole search stage. After the search, the hyperparameters of the best-performing
architecture are further optimized. However, this paradigm
may result in suboptimal results as diﬀerent architectures
tend to ﬁt diﬀerent hyperparameters, making the model
ranking unfair . Therefore, a promising solution is the
joint hyperparameter and architecture optimization (HAO)
 . We summary the existing joint HAO
methods as follows.
Zela et al. cast NAS as a hyperparameter optimization problem, where the search spaces of NAS and
standard hyperparameters are combined. They applied
BOHB , an eﬃcient HPO method, to optimize the architecture and hyperparameters jointly. Similarly, Dong
et al. proposed a diﬀerentiable method, namely AutoHAS, which builds a Cartesian product of the search
spaces of both NAS and HPO by unifying the representation of all candidate choices for the architecture (e.g.,
number of layers) and hyperparameters (e.g., learning rate).
However, a challenge here is that the candidate choices for
the architecture search space are usually categorical, while
hyperparameters choices can be categorical (e.g., the type
of optimizer) and continuous (e.g., learning rate). To overcome this challenge, AutoHAS discretizes the continuous
hyperparameters into a linear combination of multiple categorical bases. For example, the categorical bases for the
learning rate are {0.1, 0.2, 0.3}, and then, the ﬁnal learning
rate is deﬁned as lr = w1 ×0.1+w2 ×0.2+w3 ×0.3. Meanwhile, FBNetv3 jointly searches both architectures
and the corresponding training recipes (i.e., hyperparameters).
The architectures are represented with one-hot
categorical variables and integral (min-max normalized)
range variables, and the representation is fed to an encoder
network to generate the architecture embedding. Then, the
concatenation of architecture embedding and the training
hyperparameters is used to train the accuracy predictor,
which will be applied to search for promising architectures
and hyperparameters at a later stage.
6.5. Resource-aware NAS
Early NAS studies pay more attention to
searching for neural architectures that achieve higher performance (e.g., classiﬁcation accuracy), regardless of the
associated resource consumption (i.e., the number of GPUs
and time required). Therefore, many follow-up studies
investigate resource-aware algorithms to trade oﬀperformance against the resource budget. To do so, these algorithms add computational cost to the loss function as a
resource constraint. These algorithms diﬀer in the type
of computational cost, which may be 1) the parameter
size; 2) the number of Multiply-ACcumulate (MAC) operations; 3) the number of ﬂoat-point operations (FLOPs); or
4) the real latency. For example, MONAS considers
MAC as the constraint, and as MONAS uses a policy-based
reinforcement-learning algorithm to search, the constraint
can be directly added to the reward function. MnasNet
 proposes a customized weighted product to approximate a Pareto optimal solution:
where LAT(m) denotes measured inference latency of the
model m on the target device, T is the target latency, and
w is the weight variable deﬁned as:
if LAT(m) ≤T
where the recommended value for both α and β is −0.07.
In terms of a diﬀerentiable neural architecture search
(DNAS) framework, the constraint (i.e., loss function)
should be diﬀerentiable. For this purpose, FBNet 
uses a latency lookup table model to estimate the overall
latency of a network based on the runtime of each operator.
The loss function is deﬁned as
L (a, θa) = CE (a, θa) · α log(LAT(a))β
where CE(a, θa) indicates the cross-entropy loss of architecture a with weights θa. Similar to MnasNet , this loss
function also comprises two hyperparameters that need to
be set manually: α and β control the magnitude of the loss
function and the latency term, respectively. In SNAS ,
the cost of time for the generated child network is linear
to the one-hot random variables, such that the resource
constraint’s diﬀerentiability is ensured.
7. Open Problems and Future Directions
This section discusses several open problems of the existing AutoML methods and proposes some future research
directions.
7.1. Flexible Search Space
As summarized in Section 4, there are various search
spaces where the primitive operations can be roughly classiﬁed into pooling and convolution.
Some spaces even
use a more complex module (e.g., MBConv ) as the
primitive operation. Although these search spaces have
been proven eﬀective for generating well-performing neural
architectures, all of them are based on human knowledge
and experience, which inevitably introduce human bias,
and hence, still do not break away from the human design
paradigm. AutoML-Zero uses very simple mathematical operations (e.g., cos, sin, mean,std) as the primitive
operations of the search space to minimize the human
bias, and applies EA to discover complete machine learning
algorithms. AutoML-Zero successfully designs two-layer
neural networks based on these basic mathematical operations. Although the network searched by AutoML-Zero is
much simpler than both human-designed and NAS-designed
networks, the experimental results show the potential to
discover a new model design paradigm with minimal human
design. Therefore, the design of a more general, ﬂexible,
and free of human bias search space and the discovery of
novel neural architectures based on this search space would
be challenging and advantageous.
7.2. Exploring More Areas
As described in Section 6, the models designed by NAS
algorithms have achieved comparable results in image classiﬁcation tasks (CIFAR-10 and ImageNet) to those of manually designed models. Additionally, many recent studies
have applied NAS to other CV tasks (Table 5).
However, in terms of the NLP task, most NAS studies
have only conducted experiments on the PTB dataset.
Besides, some NAS studies have attempted to apply NAS
to other NLP tasks (shown in Table 5). However, Figure
20 shows that, even on the PTB dataset, there is still a
big gap in performance between the NAS-designed models
( ) and human-designed models (GPT-2 ,
FRAGE AWD-LSTM-Mos , adversarial AWD-LSTM-
Mos and Transformer-XL ). Therefore, the NAS
community still has a long way to achieve comparable
results to those of the models designed by experts on NLP
Besides the CV and NLP tasks, Table 5 also shows that
AutoML technique has been applied to other tasks, such
as network compression, federate learning, image caption,
Application
References
Computer Vision
Medical Image Recognition
 
Object Detection
 
Semantic Segmentation
 
Person Re-identiﬁcation
Super-Resolution
 
Image Restoration
Generative Adversarial Network (GAN)
 
Disparity Estimation
Video Task
 
Natural Language Processing
Translation
Language Modeling
Entity Recognition
Text Classiﬁcation
Sequential Labeling
Keyword Spotting
Network Compression
 
Graph Neural Network (GNN)
Federate Learning
 
Loss Function Search
 
Activation Function Search
Image Caption
 
Text to Speech (TTS)
Recommendation System
 
Table 5: Summary of the existing automated machine learning applications.
adversarial+AWD-LSTM-MoS
FRAGE + AWD-LSTM-MoS
Transformer-XL
Perplexity
Figure 20: State-of-the-art models on the PTB dataset. The lower the
perplexity, the better is the performance. The green bar represents
the automatically generated model, and the yellow bar represents the
model designed by human experts. Best viewed in color.
recommendation system, and searching for loss and activation functions. Therefore, these interesting studies have
indicated the potential of AutoML to be applied in more
7.3. Interpretability
Although AutoML algorithms can ﬁnd promising con-
ﬁguration settings more eﬃciently than humans, there is a
lack of scientiﬁc evidence for illustrating why the found settings perform better. For example, in BlockQNN , it is
unclear why the NAS algorithm tends to select the concatenation operation to process the output of each block in the
cell, instead of the element-wise addition operation. Some
recent studies have shown that the explanation for these occurrences is usually hindsight and lacks
rigorous mathematical proof. Therefore, increasing the
mathematical interpretability of AutoML is an important
future research direction.
7.4. Reproducibility
A major challenge with ML is reproducibility. AutoML
is no exception, especially for NAS, because most of the
existing NAS algorithms still have many parameters that
need to be set manually at the implementation level; however, the original papers do not cover much detail. For
instance, Yang et al. experimentally demonstrated
that the seed plays an important role in NAS experiments;
however, most NAS studies do not mention the seed set in
the experiments. Besides, considerable resource consumption is another obstacle to reproduction. In this context,
several NAS-Bench datasets have been proposed, such as
NAS-Bench-101 , NAS-Bench-201 , and NAS-
Bench-NLP . These datasets allow NAS researchers
to focus on the design of optimization algorithms without
wasting much time on the model evaluation.
7.5. Robustness
NAS has been proven eﬀective in searching promising
architectures on many open datasets (e.g., CIFAR-10 and
ImageNet). These datasets are generally used for research;
therefore, most of the images are well-labeled. However,
in real-world situations, the data inevitably contain noise
(e.g., mislabeling and inadequate information). Even worse,
the data might be modiﬁed to be adversarial with carefully
designed noises. Deep learning models can be easily fooled
by adversarial data, and so can NAS.
So far, there are a few studies have
attempted to boost the robustness of NAS against adversarial data. Guo et al. experimentally explored the
intrinsic impact of network architectures on network robustness against adversarial attacks, and observed that
densely connected architectures tend to be more robust.
They also found that the ﬂow of solution procedure (FSP)
matrix is a good indicator of network robustness, i.e.,
the lower is the FSP matrix loss, the more robust is the network. Chen et al. proposed a robust loss function for
eﬀectively alleviating the performance degradation under
symmetric label noise. The authors in adopted EA
to search for robust architectures from a well-designed and
vast search space, where various adversarial attacks are
used as the ﬁtness function for evaluating the robustness
of neural architectures.
7.6. Joint Hyperparameter and Architecture Optimization
Most NAS studies have considered HPO and AO as two
separate processes. However, as already noted in Section
4, there is a tremendous overlap between the methods
used in HPO and AO, e.g., both of them apply RS, BO,
and GO methods. In other words, it is feasible to jointly
optimize both hyperparameters and architectures, which is
experimentally conﬁrmed by several studies .
Thus, how to solve the problem of joint hyperparameter
and architecture optimization (HAO) elegantly is a worthy
studying issue.
7.7. Complete AutoML Pipeline
So far, many AutoML pipeline libraries have been proposed, but most of them only focus on some parts of the
AutoML pipeline (Figure 1). For instance, TPOT ,
Auto-WEAK , and Auto-Sklearn are built on
top of scikit-learn for building classiﬁcation and regression pipelines, but they only search for the traditional
ML models (such as SVM and KNN). Although TPOT
involves neural networks (using Pytorch backend), it
only supports an MLP network. Besides, Auto-Keras 
is an open-source library developed based on Keras ,
which focuses more on searching for deep learning models
and supports multi-modal and multi-task. NNI is
a more powerful and lightweight toolkit of AutoML, as
its built-in capability contains automated feature engineering, hyperparameter optimization, and neural architecture
search. Additionally, the NAS module in NNI supports
both Pytorch and Tensorﬂow and reproduces
many SOTA NAS methods ,
which is very friendly for NAS researchers and developers. Besides, NNI also integrates scikit-learn features ,
which is one step closer to achieving a complete pipeline.
Similarly, Vega is another AutoML algorithm tool that
constructs a complete pipeline covering a set of highly decoupled functions: data augmentation, HPO, NAS, model
compression, and full training. In summary, designing an
easy-to-use and complete AutoML pipeline system is a
promising research direction.
7.8. Lifelong Learning
Finally, most AutoML algorithms focus only on solving
a speciﬁc task on some ﬁxed datasets, e.g., image classiﬁcation on CIFAR-10 and ImageNet. However, a high-quality
AutoML system should have the capability of lifelong learning, i.e., it should be able to 1) eﬃciently learn new data
and 2) remember old knowledge.
7.8.1. Learn New Data
First, the system should be able to reuse prior knowledge to solve new tasks (i.e., learning to learn). For example,
a child can quickly identify tigers, rabbits, and elephants
after seeing several pictures of these animals. However, the
current DL models must be trained on considerable data
before they can correctly identify images. A hot topic in
this area is meta-learning, which aims to design models for
new tasks using previous experience.
Meta-learning. Most of the existing NAS methods
can search a well-performing architecture for a single task.
However, they have to search for a new architecture on
a new task; otherwise, the old architecture might not be
optimal. Several studies have combined
meta-learning and NAS to solve this problem. Recently,
Lian et al. proposed a novel and meta-learning-based
transferable neural architecture search method to generate a
meta-architecture, which can adapt to new tasks easily and
quickly through a few gradient steps. Another challenge
of learning new data is few-shot learning scenarios, where
there are only limited data for the new tasks. To overcome this challenge, the authors in and applied
NAS to few-shot learning, where they only searched for the
most promising architecture and optimized it to work on
multiple few-shot learning tasks. Elsken et al. proposed a gradient-based meta-learning NAS method, namely
METANAS, which can generate task-speciﬁc architectures
more eﬃciently as it does not require meta-retraining.
Unsupervised learning. Meta-learning-based NAS
methods focus more on labeled data, while in some cases,
only a portion of the data may have labels or even none
at all. Liu et al. proposed a general problem setup,
namely unsupervised neural architecture search (UnNAS),
to explore whether labels are necessary for NAS. They experimentally demonstrated that the architectures searched
without labels are competitive with those searched with labels; therefore, labels are not necessary for NAS, which has
provoked some reﬂection among researchers about which
factors do aﬀect NAS.
7.8.2. Remember Old Knowledge
An AutoML system must be able to constantly learn
from new data, without forgetting the knowledge from
old data. However, when we use new datasets to train a
pretrained model, the model’s performance on the previous
datasets is substantially reduced. Incremental learning can
alleviate this problem. For example, Li and Hoiem 
proposed the learning without forgetting (LwF) method,
which trains a model using only new data while preserving
its original capabilities. In addition, iCaRL makes
progress based on LwF. It only uses a small proportion of
old data for pretraining, and then gradually increases the
proportion of a new class of data used to train the model.
8. Conclusions
This paper provides a detailed and systematic review
of AutoML studies according to the DL pipeline (Figure
1), ranging from data preparation to model evaluation.
Additionally, we compare the performance and eﬃciency of
existing NAS algorithms on the CIFAR-10 and ImageNet
datasets, and provide an in-depth discussion of diﬀerent
research directions on NAS: one/two-stage NAS, one-shot
NAS, and joint HAO. We also describe several interesting
open problems and discuss some important future research
directions. Although research on AutoML is in its infancy,
we believe that future researchers will eﬀectively solve
these problems. In this context, this review provides a
comprehensive and clear understanding of AutoML for the
beneﬁt of those new to this area, and will thus assist with
their future research endeavors.