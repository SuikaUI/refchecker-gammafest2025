END-TO-END ATTENTION-BASED LARGE VOCABULARY SPEECH RECOGNITION
Dzmitry Bahdanau∗, Jan Chorowski†, Dmitriy Serdyuk‡, Phil´emon Brakel‡ and Yoshua Bengio‡1
∗Jacobs University Bremen
†University of Wrocław
‡ Universit´e de Montr´eal
1 CIFAR Fellow
Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids
of neural networks and Hidden Markov Models (HMMs).
Most of these systems contain separate components that deal
with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in
which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the
character level. Alignment between the input features and
the desired character sequence is learned automatically by an
attention mechanism built into the RNN. For each predicted
character, the attention mechanism scans the input sequence
and chooses relevant frames. We propose two methods to
speed up this operation: limiting the scan to a subset of
most promising frames and pooling over time the information
contained in neighboring frames, thereby reducing source
sequence length. Integrating an n-gram language model into
the decoding process yields recognition accuracies similar to
other HMM-free RNN-based approaches.
Index Terms—
neural networks, LVCSR, attention,
speech recognition, ASR
1. INTRODUCTION
Deep neural networks have become popular acoustic models for state-of-the-art large vocabulary speech recognition
systems . However, in these systems
most of the other components, such as Hidden Markov Models (HMMs), Gaussian Mixture Models (GMMs) and n-gram
language models, are the same as in their predecessors. These
combinations of neural networks and statistical models are
often referred to as hybrid systems. In a typical hybrid system, a deep neural network is trained to replace the Gaussian
Mixture Model (GMM) emission distribution of an HMM by
predicting for each input frame the most likely HMM state.
These state labels are obtained from a trained GMM-HMM
system that has been used to perform forced alignment. In
other words, a two-stage training process is required, in which
the older GMM approach is still used as a starting point. An
obvious downside of this hybrid approach is that the acoustic
model is not directly trained to minimize the ﬁnal objective
of interest. Our aim was to investigate neural LVCSR models
that can be trained with a more direct approach by replacing
the HMMs with a Attention-based Recurrent Sequence Generators (ARSG) such that they can be trained end-to-end for
sequence prediction.
Recently, some work on end-to-end neural network
LVCSR systems has shown promising results.
network model trained with Connectionist Temporal Classiﬁcation (CTC) achieved promising
results on the Wall Street Journal corpus . A similar setup was used to
obtain state-of-the-art results on the Switchboard task as well
 . Both of these models were trained to
predict sequences of characters and were later combined with
a word level language model. Furthermore, when the language model was implemented as a CTC-speciﬁc Weighted
Finite State Transducer, decoding accuracies competitive
with DNN-HMM hybrids were obtained .
At the same time, a new direction of neural network research has emerged that deals with models that learn to focus
their “attention” to speciﬁc parts of their input. Systems of
this type have shown very promising results on a variety of
tasks including machine translation ,
caption generation , handwriting synthesis
 , visual object classiﬁcation 
and phoneme recognition .
In this work, we investigate the application of an Attentionbased Recurrent Sequence Generator (ARSG) as a part of an
end-to-end LVCSR system. We start from the system proposed in and make the following
contributions:
1. We show how training on long sequences can be made
feasible by limiting the area explored by the attention
to a range of most promising locations. This reduces
the total training complexity from quadratic to linear,
largely solving the scalability issue of the approach.
This has already been proposed under the name “windowing”, but was used only
 
at the decoding stage in that work.
2. In the spirit of he Clockwork RNN and hierarchical gating RNN , we introduce a recurrent architecture that successively reduces source sequence length by pooling
frames neighboring in time. 1
3. We show how a character-level ARSG and n−gram
word-level language model can be combined into a
complete system using the Weighted Finite State Transducers (WFST) framework.
2. ATTENTION-BASED RECURRENT SEQUENCE
GENERATORS FOR SPEECH
The system we propose is a neural network that can map sequences of speech frames to sequences of characters. While
the whole system is differentiable and can be trained directly
to perform the task at hand, it can still be divided into different functional parts that work together to learn how to encode the speech signal into a suitable feature representation
and to decode this representation into a sequence of characters. We used RNNs for both the encoder and decoder2 parts
of the system. The decoder combines an RNN and an attention mechanism into an Attention-based Recurrent Sequence
Generator that is able to learn the alignment between its input and its output. Therefore, we will ﬁrst discuss RNNs,
and subsequently, how they can be combined with attention
mechanisms to perform sequence alignment.
2.1. Recurrent Neural Networks
There has been quite some research into Recurrent Neural
Networks (RNNs) for speech recognition and this can probably be explained
to a large extent by the elegant way in which they can deal
with sequences of variable length.
Given a sequence of feature vectors (x1, · · · , xT ), a standard RNN computes a corresponding sequence of hidden state
vectors (h1, · · · , hT ) using
ht = g(Wxhxt + Whhht−1 + bh),
where Wxh and Whh are matrices of trainable parameters
that represent the connection weights of the network and bh
is a vector of trainable bias parameters. The function g(·) is
often a non-linear squashing function like the hyperbolic tangent and applied element-wise to its input. The hidden states
can be used as features that serve as inputs to a layer that performs a task like classiﬁcation or regression. Given that this
1This mechanism has been recently independently proposed in .
2The word “decoder” refers to a network in this context, not to the ﬁnal
recognition algorithm.
output layer and the objective to optimize are differentiable,
the gradient of this objective with respect to the parameters of
the network can be computed with backpropagation through
time. Like feed-forward networks, RNNs can process discrete
input data by representing it as 1-hot-coding feature vectors.
An RNN can be used as a statistical model over sequences
of labels. For that, it is trained it to predict the probability of
the next label conditioned on the part of the sequence it has
already processed. If (y1, · · · , yT ) is a sequence of labels, an
RNN can be trained to provide the conditional distribution the
next label using
p(yt|y1, · · · , yt−1) = p(yt|ht)
= softmax(Whlht + bl),
where Whl is a matrix of trainable connection weights, bl is
a vector of bias parameters and softmaxi(a) =
j exp(aj).
The likelihood of the complete sequence is now given by
t=2 p(yt|y1, · · · , yt−1). This distribution can be used
to generate sequences by either sampling from the distribution p(yt|y1, · · · , yt−1) or choosing the most likely labels iteratively.
Equation 1 deﬁnes the simplest RNN, however in practice usually more advanced equations deﬁne the dependency
of ht on ht−1. Famous examples of these so-called recurrent transitions are Long Short Term Memory and Gated Recurrent Units (GRU) , which are both designed to better handle longterm dependencies. In this work we use GRU for it has a simpler architecture and is easier to implement efﬁciently. The
hidden states ht are computed using the following equations:
zt = σ(Wxzxt + Uhzht−1),
rt = σ (Wxrxt + Uhrht−1) ,
˜ht = tanh (Wxhxt + Urh(rt ⊗ht−1)) ,
ht = (1 −zt)ht−1 + zt˜ht,
where ˜ht are candidate activations, zt and rt are update and
reset gates respectively. The symbol ⊗signiﬁes element-wise
multiplication.
To obtain a model that uses information from both future
frames and past frames, one can pass the input data through
two recurrent neural networks that run in opposite directions
and concatenate their hidden state vectors. Recurrent neural network of this type are often referred to as bidirectional
Finally, it has been shown that better results for speech
recognition tasks can be obtained by stacking multiple layers of recurrent neural networks on top of each other . This can simply be done by treating the sequence of state vectors (h1, · · · , hT ) as the input sequence
for the next RNN in the pile. Figure 1 shows an example of
two bidirectional RNNs that have been stacked on top of each
other to construct a deep architecture.
Fig. 1. Two Bidirectional Recurrent Neural Networks stacked
on top of each other.
Fig. 2. A pooling over time BiRNN: the upper layer runs
twice slower then the lower one. It can average, or subsample
(as shown in the ﬁgure) the hidden states of the layer below
2.2. Encoder-Decoder Architecture
Many challenging tasks involve inputs and outputs which may
have variable length. Examples are machine translation and
speech recognition, where both input and output have variable
length; and image caption generation, where the captions may
have variable lengths.
Encoder-decoder networks are often used to deal with
variable length input and output sequences . The encoder is a network that transforms the input into an intermediate representation.
decoder is typically an RNN that uses this representation in
order to generate the outputs sequences as described in 2.1.
In this work, we use a deep BiRNN as an encoder. Thus,
the representation is a sequence of BiRNN state vectors
(h1, . . . , hL).
For a standard deep BiRNN, the sequence
(h1, . . . , hL) is as long as the input of the bottom-most layer,
which in the context of speech recongnition means one hi for
every 10ms of the recordings. We found that for our decoder
(see 2.3) such representation is overly precise and contains
much redundant information.
This led us to add pooling
between BiRNN layers as illustrated by Figure 2.
2.3. Attention-equipped RNNs
The decoder network in our system is an Attention-based Recurrent Sequence Generator (ARSG). This subsection introduces ARSGs and explains the motivation behind our choice
of an ARSG for this study.
While RNNs can process and generate sequential data, the
length of the sequence of hidden state vectors is always equal
to the length of the input sequence. One can aim to learn
the alignment between these two sequences to model a distribution p(y1, · · · , yT |h1, · · · , hL) for which there is no clear
functional dependency between T and L.
An ARSG produces an output sequence (y1, · · · , yT ) one
element at a time, simultaneously aligning each generated element to the encoded input sequence (h1, · · · , hL). It is composed of an RNN and an additional subnetwork called ‘attention mechanism’. The attention selects the temporal locations
over the input sequence that should be used to update the hidden state of the RNN and to make a prediction for the next
output value. Typically, the selection of elements from the input sequence is a weighted sum ct = P
l αtlhl, where αtl are
called the attention weights and we require that αtl ≥0 and
l αtl = 1. See Figure 3 for a schematic representation
of an ARSG.
The attention mechanism used in this work is an improved
version of the hybrid attention with convolutional features
from , which is described by the following equations:
F = Q ∗αt−1
etl = w⊤tanh(Wst−1 + Vhl + Ufl + b)
αtl = exp(etl)
exp(etl) .
where W, V, U, Q are parameter matrices, w and b are parameter vectors, ∗denotes convolution, st−1 stands for the
previous state of the RNN component of the ARSG. We explain how it works starting from the end: (4) shows how the
weights αtl are obtained by normalizing the scores etl. As illustrated by (3), the score depends on the previous state st−1,
the content in the respective location hl and the vector of socalled convolutional features fl. The name “convolutional”
comes from the convolution along the time axis used in (2) to
compute the matrix F that comprises all feature vectors fl.
Simply put, the attention mechanism described above
combines information from three sources to decide where to
focus at the step t: the decoding history contained in st−1,
the content in the candidate location hl and the focus from
the previous step described by attention weights αt−1. It is
shown in that making the attention
location-aware, that is using αt−1 in the equations deﬁning
αt, is crucial for reliable behaviour on long input sequences.
A disadvantage of the approach from is the complexity of the training procedure, which is
O(LT) since weights αtl have to be computed for all pairs
of input and output positions. The same paper showcases a
windowing approach that reduces the complexity of decoding to O(L + T).
In this work we apply the windowing
at the training stage as well. Namely, we constrain the attention mechanism to only consider positions from the range
(mt−1 −wl, . . . , mt−1 + wr), where mt−1 is the median of
αt−1, interpreted in this context as a distribution. The values
wl and wr deﬁne how much the window expands to the left
and to the right respectively. This modiﬁcation makes training
signiﬁcantly faster.
Apart from the speedup it brings, windowing can be also
very helpful for starting the training procedure. From our experience, it becomes increasingly harder to train an ARSG
completely from scratch on longer input sequences. We found
that providing a very rough estimate of the desired alignment at the early training stage is an effective way to quickly
bring network parameters in a good range. Speciﬁcally, we
forced the network to only choose from positions in the range
Rt = (smin+tvmin, . . . , smax+tvmax). The numbers smin,
smax, vmin, vmax were roughly estimated from the training
set so that the number of leading silent frames for training utterances were between smin and smax and so that the speaker
speed, i.e. the ratio between the transcript and the encoded
input lengths, were between vmin and vmax. We aimed to
make the windows Rt as narrow as possible, while keeping
the invariant that the character yt was pronounced within the
window Rt. The resulting sequence of windows is quickly
expanding, but still it was sufﬁcient to quickly move the network out of the random initial mode, in which it had often
aligned all characters to a single location in the audio data.
We note, that the median-centered windowing could not be
used for this purpose, since it relies on the quality of the previous alignment to deﬁne the window for the new one.
3. INTEGRATION WITH A LANGUAGE MODEL
Although an ARSG by construction implicitly learns how an
output symbol depends on the previous ones, the transcriptions of the training utterances are typically insufﬁcient to
learn a high-quality language model.
For this reason, we
investigate how an ARSG can be integrated with a language
The main challenge is that in speech recognition
word-based language models are used, whereas our ARSG
models a distribution over character sequences.
We use the Weighted Finite State Transducer (WFST)
framework to build
a character-level language model from a word-level one. A
WFST is a ﬁnite automaton, whose transitions have weight
and input and output labels. It deﬁnes a cost of transducing
Fig. 3. Schematic representation of the Attention-based Recurrent Sequence Generator. At each time step t, an MLP
combines the hidden state st−1 with all the input vectors hl
to compute the attention weights αtl. Subsequently, the new
hidden state st and prediction for output label yt can be computed.
an input sequence into an output sequence by considering
all pathes with corresponding sequences of input and output
labels. The composition operation can be used to combine
FSTs that deﬁne different levels of representation, such as
characters and words in our case.
We compose the language model Finite State Transducer
(FST) G with a lexicon FST L that simply spells out the
letters of each word.
More speciﬁcally, we build an FST
T = min(det(L ◦G)) to deﬁne the log-probability for character sequences. We push the weights of this FST towards the
starting state to help hypothesis pruning during decoding.
For decoding we look for a transcript y that minimizes
the cost L which combines the encoder-decoder (ED) and the
language model (LM) outputs as follows:
L = −log pED(y|x) −β log pLM(y) −γT
where β and γ are tunable parameters. The last term γT is
important, because without it the LM component dominates
and the cost L is minimized by too short sequences. We note
that the same criterion for decoding was proposed in for a CTC network.
Integrating an FST and an ARSG in a beam-search decoding is easy because they share the property that the current
state depends only on the previous one and the input symbol.
Therefore one can use a simple left-to-right beam search algorithm similar to the one described in 
to approximate the value of y that minimizes L.
The determinization of the FST becomes impractical for
moderately large FSTs, such as the trigram model shipped
with the Wall Street Journal corpus (see Subsection 5.1). To
handle non-deterministic FSTs we assume that its weights
are in the logarithmic semiring and compute the total logprobability of all FST paths corresponding to a character pre-
ﬁx from the beam. This probability can be quickly recomputed when a new character is added to the preﬁx.
4. RELATED WORK
A popular method to train networks to perform sequence
prediction is Connectionist Temporal Classiﬁcation . It has been used with great success for both
phoneme recognition and characterbased LVCSR .
CTC allows recurrent neural
networks to predict sequences that are shorter than the input
sequence by summing over all possible alignments between
the output sequence and the input of the CTC module. This
summation is done using dynamic programming in a way
that is similar to the forward and backward passes that are
used to do inference in an HMM. In the CTC approach, output labels are conditionally independent given the alignment
and the output sequences. In the context of speech recognition, this means that a CTC network lacks a language model,
which greatly boosts the system performance when added to
a trained CTC network . One network is
similar to a CTC network and runs at the same time-scale as
the input sequence, while a separate RNN models the probability of the next label output label conditioned on the previous ones. Like in CTC, inference is done with a dynamic
programming method similar to the backward-forward algorithm for HMMs, but taking into account the constraints de-
ﬁned by both of the RNNs. Unlike CTC, RNN transduction
systems can also generate output sequences that are longer
than the input. RNN Transducers have led to state-of-the-art
results in phoneme recognition on the TIMIT dataset which were recently matched by an ASRG network .
The RNN Transducer and ARSG approaches are roughly
equivalent in their capabilities. In both approaches an implicit
language model is learnt jointly with the rest of the network.
The main difference between the approaches is that in ARSG
the alignment is explicitly computed by the network, as opposed to dealing with a distribution of alignments in the RNN
Transducer. We hypothesize that this difference might have a
major impact on the further development of these methods.
Finally, we must mention two very recently published
works that partially overlap with the content of this paper.
In Encoder-Decoder for character-based
recognition, with the model being quite similar to ours. In
particular, in this work pooling between the BiRNN layers
is also proposed. Also, in using FSTs
to build a character-level model from an n-gram model is
advocated. We note, that the research described in this paper
was carried independently and without communication with
the authors of both aforementioned works.
5. EXPERIMENTS
We trained and evaluated our models 3 on the Wall Street
Journal (WSJ) corpus (available at the Linguistic Data Consortium as LDC93S6B and LDC94S13B). Training was done
on the 81 hour long SI-284 set of about 37K sentences. As
input features, we used 40 mel-scale ﬁlterbank coefﬁcients
together with the energy. These 41 dimensional features were
extended with their ﬁrst and second order temporal derivatives to obtain a total of 123 feature values per frame. Evaluation was done on the “eval92” evaluation set. Hyperparameter selection was performed on the “dev93” set. For language
model integration we have used the 20K closed vocabulary
setup and the bigram and trigram language model that were
provided with the data set. We use the same text preprocessing as in , leaving only 32 distinct labels: 26 characters, apostrophe, period, dash, space, noise and
end-of-sequence tokens.
5.2. Training
Our model used 4 layers of 250 forward + 250 backward GRU
units in the encoder, with the top two layers reading every
second of hidden states of the network below it (see Figure
2). Therefore, the encoder reduced the utterance length by
the factor of 4. A centered convolution ﬁlter of width 200
was used in the attention mechanism to extract a single feature
from the previous step alignment as described in (4).
The AdaDelta algorithm with gradient clipping was used for optimization. We initialized all the weights
randomly from an isotropic Gaussian distribution with variance 0.1.
We used a rough estimate of the proper alignment for
the ﬁrst training epoch as described in Section 2.3.
that the training was restarted with the windowing described
in the same section. The window parameters were wL =
wR = 100, which corresponds to considering a large 8 second long span of audio data at each step, taking into account
the pooling done between layers. Training with the AdaDelta
hyperparameters ρ = 0.95, ϵ = 10−8 was continued until
log-likelihood stopped improving. Finally, we annealed the
best model in terms of CER by restarting the training with
ϵ = 10−10.
We found regularization necessary for the best performance. The column norm constraint 1 was imposed on all
weight matrices . This corresponds to
3Our code is available at 
constraining the norm of the weights of all the connections
incoming to a unit.
5.3. Decoding and Evaluation
As explained in Section 3, we used beam search to minimize
the combined cost L deﬁned by (5). We ﬁnished when k terminated sequences cheaper than any non-terminated sequence
in the beam were found. A sequence was considered terminated when it ended with the special end-of-sequence token,
which the network was trained to generate in the end of each
transcript.
To measure the best performance we used the beam size
200, however this brought us only ≈10% relative improvement over beam size 10. We used parameter settings α = 0.5
and γ = 1 with a language model and γ = 0.1 without one.
It was necessary to use an asymmetric window for the attention when decoding with large γ. More speciﬁcally, we reduced wL to 10. Without this trick, the cost L could be in-
ﬁnitely minimized by looping across the input utterance, for
the penalty for jumping back in time included in log p(y|x)
was not high enough.
5.4. Results
Results of our experiments are gathered in Table 5.4. Our
model shows performance superior to that of CTC systems
when no external language model is used. The improvement
from adding an external language model is however much
larger for CTC-based systems. The ﬁnal peformance of our
model is better than the one reported in 
(13.0% vs 14.1%), but worse than the the one from (11.3% vs 9.0%) when the same language models are used.
6. DISCUSSION
A major difference between the CTC and ARSG approaches
is that a language model is implicitly learnt in the latter. Indeed, one can see that an RNN sequence model as explained
in 2.1 is literally contained in an ARSG as a subnetwork. We
believe that this is the reason for the greater performance of
the ARSG-based system when no external LM is used. However, this implicit language model was trained on a relatively
small corpus of WSJ transcripts containing less than 4 million
characters. It has been reported that RNNs overﬁt on corpora
of such size and in our experiments we had to
combat overﬁtting as well. Using the weight clipping brought
a consistent performance gain but did not change the big picture. For these reasons, we hypothesize that overﬁtting of the
internal RNN language model was one of the main reasons
why our model did not reach the performance level reported
in , where a CTC network is used.
Table 1. Character Error Rate (CER) and Word Error Rate
(WER) scores for our setup on the Wall Street Journal Corpus
in comparison with other results from the literature. Note that
our results are not directly comparable with those of networks
predicting phonemes instead of characters, since phonemes
are easier targets.
Encoder-Decoder
Encoder-Decoder + bigram LM
Encoder-Decoder + trigram LM
Encoder-Decoder + extended trigram LM
Graves and Jaitly 
CTC, expected transcription loss
Hannun et al. 
CTC + bigram LM
Miao et al. ,
CTC for phonemes + lexicon
CTC for phonemes + trigram LM
CTC + trigram LM
That being said, we treat it as an advantage of the ARSG
that it supports joint training of a language model with the
rest of the network. For one, WSJ contains approximately
only 80 hours of training data, and overﬁtting might be less
of an issue for corpora containing hundreds or even thousands
hours of annotated speech. For two, an RNN language model
trained on a large text corpus could be integrated in an ARSG
from the beginning of training by using the states of this language model as an additional input of the ARSG. We suppose
that this would block the incentive of memorizing the training
utterances, and thereby reduce the overﬁtting. In addition, no
extra n-gram model would be required. We note that a similar
idea has been already proposed in for
machine translation.
Finally, trainable integration with an n-gram language
model could also be investigated.
6.1. Conclusion
In this work we showed how an Encoder-Decoder network
with an attention mechanism can be used to build a LVCSR
system. The resulting approach is signiﬁcantly simpler than
the dominating HMM-DNN one, with fewer training stages,
much less auxiliary data and less domain expertise involved.
Combined with a trigram language model our system shows
decent, although not yet state-of-the-art performance.
We present two methods to improve the computational
complexity of the investigated model. First, we propose pooling over time between BiRNN layers to reduce the length of
the encoded input sequence. Second, we propose to use windowing during training to ensure that the decoder network
performs a constant number of operations for each output
character. Together these two methods facilitate application
of attention-based models to large-scale speech recognition.
Unlike CTC networks, our model has an intrinsic languagemodeling capability.
Furthermore, it has a potential to be
trained jointly with an external language model. Investigations in this direction are likely to be a part of our future
Acknowledgments
The experiments were conducted using Theano , Blocks and Fuel libraries.
The authors would like to acknowledge the support of
the following agencies for research funding and computing
support: National Science Center (Poland), NSERC, Calcul Qu´ebec, Compute Canada, the Canada Research Chairs
and CIFAR. Bahdanau also thanks Planet Intelligent Systems
GmbH and Yandex.