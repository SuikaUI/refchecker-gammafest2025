BIROn - Birkbeck Institutional Research Online
Aminanto, M.E. and Choi, R. and Tanuwidjaja, H.C. and Yoo, Paul D. and
Kim, K. Deep abstraction and weighted feature selection for Wi-Fi
impersonation detection. IEEE Transactions on Information Forensics and
Security 13 (3), pp. 621-636. ISSN 1556-6013.
Downloaded from: 
Usage Guidelines:
Please refer to usage guidelines at or alternatively
contact .
IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. XX, NO. X, XXXXX 2017
Deep Abstraction and Weighted Feature Selection
for Wi-Fi Impersonation Detection
Muhamad Erza Aminanto, Rakyong Choi, Harry Chandra Tanuwidjaja,
Paul D. Yoo, Senior Member, IEEE and Kwangjo Kim, Member, IEEE
Abstract—The recent advances in mobile technologies have
resulted in IoT-enabled devices becoming more pervasive and
integrated into our daily lives. The security challenges that need
to be overcome mainly stem from the open nature of a wireless
medium such as a Wi-Fi network. An impersonation attack is
an attack in which an adversary is disguised as a legitimate
party in a system or communications protocol. The connected
devices are pervasive, generating high-dimensional data on a
large scale, which complicates simultaneous detections. Feature
learning, however, can circumvent the potential problems that
could be caused by the large-volume nature of network data.
This study thus proposes a novel Deep-Feature Extraction and
Selection (D-FES), which combines stacked feature extraction
and weighted feature selection. The stacked autoencoding is
capable of providing representations that are more meaningful
by reconstructing the relevant information from its raw inputs.
We then combine this with modiﬁed weighted feature selection
inspired by an existing shallow-structured machine learner. We
ﬁnally demonstrate the ability of the condensed set of features
to reduce the bias of a machine learner model as well as
the computational complexity. Our experimental results on a
well-referenced Wi-Fi network benchmark dataset, namely, the
Aegean Wi-Fi Intrusion Dataset (AWID), prove the usefulness
and the utility of the proposed D-FES by achieving a detection
accuracy of 99.918% and a false alarm rate of 0.012%, which
is the most accurate detection of impersonation attacks reported
in the literature.
Index Terms—Intrusion detection system, impersonation attack, deep learning, feature extraction, stacked autoencoder,
large-scale Wi-Fi networks.
Manuscript received March 16, 2017; revised July 17, 2017 and September
xx, 2017; accepted xxxxx xxxxxx; date of publication xxx xxxx; date of
current version xxxx xxxxxx. The associate editor coordinating the review
process of this manuscript and approving it for publication was Prof. Tansu
Tanuwidjaja
with the School of Computing (SoC), Korea Advanced Institute of Science and Technology (KAIST), Daejeon 34141, Republic of Korea. (email: ; ; ;
 )
P. D. Yoo is with Centre for Electronic Warfare, Information and
Cyber (EWIC), Cranﬁeld Defence and Security, Defence Academy of
the United Kingdom, Shrivenham, SN6 8LA, United Kingdom. (email:
 )
The work of M. E. Aminanto, R. Choi and K. Kim was supported by
Institute for Information & communications Technology Promotion(IITP)
grant funded by the Korea government(MSIT) and by National Research Foundation
of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-
2015R1A-2A2A01006812)
Color versions of one or more of the ﬁgures in this paper are available
online at 
Digital Object Identiﬁer 10.xxxx/TIFS.2017.xxxxxxxx
I. INTRODUCTION
HE rapid growth of the Internet has led to a signiﬁcant increase in wireless network trafﬁc in recent years.
According to a worldwide telecommunication consortium ,
proliferation of 5G and Wi-Fi networks is expected to occur
in the next decades. By 2020 1 wireless network trafﬁc is
anticipated to account for two thirds of total Internet trafﬁc
— with 66% of IP trafﬁc expected to be generated by Wi-Fi
and cellular devices only. Although wireless networks such as
IEEE 802.11 have been widely deployed to provide users with
mobility and ﬂexibility in the form of high-speed local area
connectivity, other issues such as privacy and security have
raised. The rapid spread of Internet of Things (IoT)-enabled
devices has resulted in wireless networks becoming to both
passive and active attacks, the number of which has grown
dramatically . Examples of these attacks are impersonation,
ﬂooding, and injection attacks.
An Intrusion Detection System (IDS) is one of the most
common components of every network security infrastructure
 including wireless networks . Machine-learning techniques have been well adopted as the main detection algorithm
in IDS owing to their model-free properties and learnability
 . Leveraging the recent development of machine-learning
techniques such as deep learning can be expected to
bring signiﬁcant beneﬁts in terms of improving existing IDSs
particularly for detecting impersonation attacks in large-scale
networks. Based on the detection method, IDS can be classiﬁed into three types; misuse, anomaly, and speciﬁcationbased IDS. A misuse-based IDS also known as a signaturebased IDS detects any attack by checking whether the
attack characteristics match previously stored signatures or
patterns of attacks. This type of IDS is suitable for detecting
known attacks; however, new or unknown attacks are difﬁcult
to detect.
An anomaly-based IDS identiﬁes malicious activities by
proﬁling normal behavior and then measuring any deviation
from it. The advantage of this detection type is its ability
for detecting unknown attacks. However, misuse-based IDS
achieved higher performance for detecting known attacks than
anomaly-based IDS. An IDS that leverages machine-learning
method is an example of an anomaly-based IDS , whereas
1Cisco Visual Networking Index: Forecast and Methodology 2015-
www.cisco.com/c/en/us/solutions/collateral/serviceprovider/visual-networking-index-vni/complete-white-paper-c11-481360.html
Author’s Copy
IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. XX, NO. X, XXXXX 2017
a speciﬁcation-based IDS manually deﬁnes a set of rules and
constraints to express the normal operations. Any violation
of the rules and constraints during execution is ﬂagged as an
attack as discussed by Mitchell et al. . In this study, we
consider anomaly-based IDS rather than misuse-based IDS to
enable us to include new attacks. We adopt anomaly-based IDS
instead of speciﬁcation-based IDS in order to avoid manual
intervention by the network administrator when deﬁning the
set of new rules and constraints. From the perspective of
human error, relying on human intelligence is undesirable ,
(e.g., experts may misunderstand the alarm while a network
is being monitored).
The wide and rapid spread of computing devices using
Wi-Fi networks creates complex, large, and high-dimensional
data, which cause confusion when capturing attack properties.
Feature learning acts as an important tool for improving the
learning process of a machine-learning model. It consists of
feature construction, extraction, and selection. Feature construction expands the original features to enhance their expressiveness, whereas feature extraction transforms the original
features into a new form and feature selection eliminates
unnecessary features . Feature learning is a key to improve
the performance of existing machine-learning-based IDSs.
One of the key contributions of this study is the introduction
of novel Deep-Feature Extraction and Selection (D-FES),
which improves its feature learning process by combining
stacked feature extraction with weighted feature selection. The
feature extraction of Stacked Auto Encoder (SAE) is capable
of transforming the original features into a more meaningful
representation by reconstructing its input and providing a
way to check that the relevant information in the data has
been captured. SAE can be efﬁciently used for unsupervised
learning on a complex dataset. Unlike supervised learning,
unsupervised learning does not require a labeled dataset for
training. Unsupervised learning capability is of critical importance as it allows a model to be built to detect new attacks
without creating costly labels or dependent variables. SAE
is built by stacking additional unsupervised feature learning
layers and can be trained by using greedy methods for each
additional layer. We train a new hidden layer by training a
standard supervised neural network with one hidden layer. The
output of the previously trained layer is taken as pre-training
initialization for the next layer and as the extracted features.
We then propose a modiﬁed feature-selection-based method
by considering the weights of each feature obtained from
lightweight machine-learning models. Supervised machinelearning algorithms are capable of extracting the relevant
necessary features from classiﬁcation results. Support Vector
Machine (SVM) was suggested to be embedded as a feature
selection since it ranks the input features based on weights
 . Weights computed by Artiﬁcial Neural Network (ANN)
classiﬁcation are able to determine the reliability of detecting
a certain attack . C4.5 also claimed to be inherently
includes feature selection functions . In those algorithms,
the weights of nodes represent the strength of connections
among nodes. The weight values from a trained model indicate
the importance of the corresponding inputs. We select the
most suitable features according to the weights provided SVM,
ANN and C4.5. The small set of selected features obtained
from the combination of the extracted features and the large
number of original features is not only essential for realtime processing but also suitable for the large-scale nature
of Wi-Fi networks. The ﬁnal step of the proposed approach
entails utilizing an ANN as a classiﬁer to build an IDS using
signiﬁcantly condensed and extracted features only.
We evaluate the proposed approach on the well-referenced
AWID dataset, a benchmark dataset built by Kolias et al.
 for Wi-Fi networks. They tested a number of existing machine-learning models on the dataset in a heuristic
manner. The lowest detection rate with an accuracy of 22%
was observed speciﬁcally for an impersonation attack. The
proposed approach outperforms their model in that particular
category to achieve an accuracy of 99.91%. Clearly, the
novelty of combining a deep-learning feature extractor and
weighted feature selection with an ANN classiﬁer improves
the ability to detect impersonation attacks. Furthermore, the
proposed approach can be further generalized for different
attack types, both known and unknown large-scale attacks on
Wi-Fi networks. In summary, the main contributions of this
study are three-fold:
• Design of an impersonation attack detector using a condensed set of features without modifying any protocol
and without using any additional measurement.
• Abstraction of raw features using a deep learning technique. The extracted features are measured once more
using weighted feature selection techniques such that a
measure-in-depth technique is proposed.
• Design of the proposed D-FES, which can be implemented in a real wireless network setup because an
unbalanced dataset is used for testing purposes.
The remainder of the paper is organized as follows: Section
II introduces previous work in which feature learning was
adopted. A review of studies relating to an impersonation
attack is provided in Section III. Section IV describes the
proposed methodology. Section V discusses the experimental
results. Section VI compares the experimental results of the
proposed methodology with those of other well-referenced
models. Section VII concludes the paper with an overview
of future work.
II. RELATED WORK
An IDS has been studied for decades especially on anomalybased IDSs. Fragkiadakis et al. proposed an anomaly-based
IDS using Dempster-Shafer’s rule for measurement. Shah et
al. also developed an evidence theory for combining
anomaly-based and misuse-based IDSs. Bostani et al. 
proposed an anomaly-based IDS by modifying an optimum
path forest combined with k-means clustering. A distributed
anomaly-based IDS, called TermID, proposed by Kolias et
al. incorporating ant colony optimization and rule induction in a reduced data operation to achieve data parallelism
and reduce privacy risks.
Feature selection techniques are useful for reducing model
complexity, which leads to faster learning and real-time processes. Kayacik et al. investigated the relevance of each
AMINANTO et al.: DEEP ABSTRACTION AND WEIGHTED FEATURE SELECTION FOR WI-FI IMPERSONATION DETECTION
feature in the KDD’99 Dataset with a list of the most relevant
features for each class label and provided useful discussions on
the roles of information gain theories. Their work conﬁrmed
the importance and the role of feature selection for building
an accurate IDS model. Puthran et al. also worked on
relevant features in the KDD’99 Dataset and improved the decision tree by using binary and quad splits. Almusallam et al.
 leveraged a ﬁlter-based feature selection method. Zaman
and Karray categorized IDSs based on the Transmission
Control Protocol/Internet Protocol (TCP/IP) network model
using a feature selection method known as the Enhanced
Support Vector Decision Function (ESVDF). Louvieris et al.
 proposed an effect-based feature identiﬁcation IDS using
na¨ıve Bayes as a feature selection method. Zhu et al. also
proposed a feature selection method using a multi-objective
On the other hand, Manekar and Waghmare leveraged
Particle Swarm Optimization (PSO) and SVM. PSO performs
feature optimization to obtain an optimized feature, after which
SVM performs the classiﬁcation task. A similar approach was
introduced by Saxena and Richariya , although the concept
of weighted feature selection was introduced by Schaffernicht
and Gross . Exploiting SVM-based algorithms as a feature
selection method was introduced by Guyon et al. . This
method leveraged the weights adjusted during support vector
learning and resulted in ranking the importance of input
features. Another related approach was proposed by Wang
 who ranked input features based on weights learned by an
ANN. This method showed the ability of deep neural networks
to ﬁnd useful features among the raw data. Aljawarneh et
al. proposed a hybrid model of feature selection and an
ensemble of classiﬁers which are tend to be computationally
demanding.
We have examined several feature selection methods for
IDS. Huseynov et al. inspected ant colony clustering
method to ﬁnd feature clusters of botnet trafﬁc. The selected
features in are independent from trafﬁc payload and represent the communication patterns of botnet trafﬁc. However,
this botnet detection does not scale for enormous and noisy
dataset due to the absence of control mechanism for clustering
threshold. Kim et al. tested artiﬁcial immune system and
swarm intelligence-based clustering to detect unknown attacks.
Furthermore, Aminanto et al. discussed the utility of ant
clustering algorithm and fuzzy inference system for IDS. We
can claim that their bio-inspired clustering methods need to
be scrutinized further.
Not only feature selection, but also feature extraction has
been proposed to improve classiﬁcation performance. Shin et
al. leveraged SAE for unsupervised feature learning in
the ﬁeld of medical imaging. This method showed that SAE,
which is a type of deep learning techniques, can be effectively
used for unsupervised feature learning on a complex dataset.
Unsupervised learning by SAE can be used to learn hierarchical features that are useful for limited instances on a dataset.
Owing to the scale and complexity of recent data, building
a machine-learning-based IDS has become a daunting task.
As we aim to detect impersonation attacks in large-scale
Wi-Fi networks, a large AWID dataset was chosen for this
study. Kolias et al. created a comprehensive 802.11
networks dataset that is publicly available and evaluated
various machine-learning algorithms to validate their dataset
in a heuristic manner. The performance of the IDS was
unsatisfactory, indicating that conventional machine-learning
methods are incapable of detecting attacks on large-scale Wi-
Fi networks. Moreover, among all the classiﬁcation results
obtained, an impersonation attack detection was the most
unsatisfactory. One of the main goals of our study thus is
to improve the detection of impersonation attacks by leveraging the advancement of recent machine-learning techniques.
Recently, Usha and Kavitha proposed a combination of
modiﬁed normalized gain and PSO to select optimal features
and successfully improved the attack detection rate tested on
the AWID dataset. However, they focus on detecting all attack
classes rather than impersonation attacks only, which is the
problem raised by Kolias et al. in . We leveraged SAE for
classiﬁcation purposes and then improved our impersonation
detector using weighted feature learning from shallow machine
learners . Thus, this study extends our previous work
 to a novel IDS, which combines deep learning abstraction
and a weighted feature selection technique.
III. IMPERSONATION ATTACK
Impersonation attacks are one of the most common Wi-Fi
network attacks, where an adversary impersonates a legitimate
object on the network, including users or base stations, to
obtain unauthorized access to a system or a wireless network.
Impersonation attacks are performed in different ways such
as gaining unauthorized access, cloning device, creating a
rogue Access Point (AP), spooﬁng address, and performing
a replay attack . Based on the aim of the attackers, an
impersonation attack can be categorized as an impersonation
of any device in the network, i.e., the AP, either case (i)
to crack the key or case (ii) to act as a man-in-the-middle
(MITM) to attract a client to connect to the disguised AP
In case (i), adversaries retrieve the keystream of a network
protected by Wired Equivalent Privacy (WEP) and take an
attack to the next level. Ahmad and Ramachandran 
demonstrated that WEP keys can be cracked remotely, without
requiring the attacker to be in the coverage area of the target
AP. As its name, Caffe-Latte, suggests the attack could be
completed from a remote location and within a considerably
short amount of time. This attack captures a client’s proberequests of previously associated APs, which might be out
of the client’s range. It then disguises itself as one of those
APs in which case the client would connect to the disguised
AP. Similarly, a Hirte attack also takes advantage of retrieving
the WEP key by forging Address Resolution Protocol (ARP)
requests. It, however, uses a different approach by leveraging
a fragmentation attack. Both Caffe-Latte and Hirte attacks
could be executed using aircrack-ng, a Wi-Fi network security
assessment tool .
In case (ii), adversaries adopt an MITM approach to perform
an impersonation attack such as malicious honeypot, Evil Twin,
and rogue APs . A malicious honeypot attack is achieved
IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. XX, NO. X, XXXXX 2017
Fig. 1. Stepwise procedure of D-FES with two target classes: normal and impersonation attack
by providing an AP with an attractive name such as Free Wi-Fi
or Public Wi-Fi. Once victims are connected to the malicious
honeypot, adversaries can easily gain access to the victim’s
system. An Evil Twin AP uses a Service Set Identiﬁer (SSID)
identical to that of a legitimate AP. The victim would thus
connect to the Evil Twin AP instead of the legitimate one.
This attack requires two important assumptions. First, one area
may have two identical SSIDs. Second, any legitimate client
prefers an AP with higher signal strength. A rogue AP attack
using a bait AP, is achieved by the illegitimate installation of
an AP on a secure network by an adversary. The rogue AP
confuses the legitimate client who sends or receives packets
through what they believe to be a legitimate base station or
An impersonation attack might cause a serious breach of
network security as it allows unauthorized or malicious users
to access the network . Some researchers proposed new
detectors particularly for an impersonation attack , ,
 , , and . Malisa et al. proposed a
mobile application, which can detect both whole and partial
user interface impersonations. We agree that the concept of
extracting features captures important information to detect
an attack. However, our method builds one general method
that can be generalized for other attack models. Goga et al.
 released a social network impersonation dataset. They
focused on identifying doppelganger attacks by deﬁning a set
of features manually. We leverage a deep learning technique
to extract and select a set of features automatically without
human intervention. The model of Beyah et al. is designed
to only detect rogue APs. Shang and Gui proposed a
novel strategy considering a Differential Flag Byte (DFB)
to detect impersonation attacks at the bottom of a protocol
stack with low computational complexity. Yilmaz and Arslan
 developed an impersonation attack detector by measuring
power delay proﬁle differences of transmitters located in
different places. Lakshmi et al. demonstrated a novel way
of detecting impersonation attacks by leveraging the properties
of each node and operating independently on any cryptographic protocol. This method leveraged a special correlation
of Received Signal Strength (RSS) transmitted from wireless
nodes and Efﬁcient Probabilistic Packet Marking (EPPM) to
detect the impersonation attacks. It then used a cluster-based
measurement developed to count the number of attackers and
an SVM learner to improve the accuracy of counting the
attackers. This method ends with an integrated detection and
localization system that would localize the positions of the
attackers. However, unlike the above-mentioned detectors, we
need a general model that can speciﬁcally detect an impersonation attack. A machine-learning-based IDS is believed to be
a general countermeasure for monitoring network trafﬁc.
IV. METHODOLOGY
Feature learning could be deﬁned as a technique that models
the behavior of data from a subset of attributes only. It could
also show the correlation between detection performance and
trafﬁc model quality effectively . However, feature extraction and feature selection are different. Feature extraction
algorithms derive new features from the original features to: (i)
reduce the cost of feature measurement, (ii) increase classiﬁer
efﬁciency, and (iii) improve classiﬁcation accuracy, whereas
feature selection algorithms select no more than m features
from a total of M input features, where m is smaller than M.
Thus, the newly generated features are simply selected from
the original features without any transformation. However,
their goal is to derive or select a characteristic feature vector
with a lower dimensionality which is used for the classiﬁcation
We adopt both feature extraction and selection techniques
in D-FES. Fig.1 shows the stepwise procedure of D-FES
with two target classes. A pre-processing procedure, which
comprises the normalization and balancing steps, is necessary.
The procedure is explained in Section V in detail. As illustrated in Algorithm 1, we start D-FES by constructing SAEbased feature extractor with two consecutive hidden layers in
order to optimize the learning capabality and the execution
time . The SAE outputs 50 extracted features, which are
then combined with the 154 original features existing in the
AWID dataset . Weighted feature selection methods are
then utilized using well-referenced machine learners including
SVM, ANN, and C4.5 in order to construct the candidate
models, namely D-FES-SVM, D-FES-ANN, and D-FES-C4.5,
respectively. SVM separates the classes using a support vector
(hyperplane). Then, ANN optimizes the parameters related
to hidden layers that minimize the classifying error with
AMINANTO et al.: DEEP ABSTRACTION AND WEIGHTED FEATURE SELECTION FOR WI-FI IMPERSONATION DETECTION
Algorithm 1 Pseudocode of D-FES
1: procedure D-FES
function DATASET PRE-PROCESSING(Raw Dataset)
function (Dataset Normalization)Raw Dataset
return NormalizedDataset
end function
function (Dataset Balancing)Normalized Dataset
return BalancedDataset
end function
return InputDataset
end function
function DEEP ABSTRACTION(InputDataset)
for i=1 to h do
▷h=2; number of hidden layers
for each data instance do
Compute yi (Eq. (1))
Compute zi (Eq. (2))
Minimize Ei (Eq. (6))
θi = {Wi, Vi, bfi, bgi}
▷2nd layer, 50 extracted features
InputFeatures ←W + InputDataset
return InputFeatures
end function
function FEATURE SELECTION(InputFeatures)
switch D-FES do
case D-FES-ANN(InputFeatures)
return SelectedFeatures
case D-FES-SVM(InputFeatures)
return SelectedFeatures
case D-FES-C4.5(InputFeatures)
return SelectedFeatures
end function
procedure CLASSIFICATION(SelectedFeatures)
Training ANN
Minimize E (Eq. (7))
end procedure
37: end procedure
respect to the training data, whereas C4.5 adopts a hierarchical
decision scheme such as a tree to distinguish each feature .
The ﬁnal step of the detection task involves learning an ANN
classiﬁer with 12–22 trained features only.
A. Feature Extraction
1) (Sparse) Auto Encoder: An Auto Encoder (AE) is a
symmetric neural network model, which uses an unsupervised
approach to build a model with non-labeled data, as shown in
Fig. 2. AE extracts new features by using an encoder-decoder
paradigm by running from inputs through the hidden layer
only. This paradigm enhances its computational performance
and validates that the code has captured the relevant information from the data. The encoder is a function that maps an
input x to a hidden representation as expressed by Eq. (1).
y = sf (W · x + bf) ,
Fig. 2. AE network with symmetric input-output layers and three neurons in
one hidden layer
where sf is a nonlinear activation function which is a decisionmaking function to determine the necessity of any feature.
Mostly, a logistic sigmoid, sig(t) =
1 + e−t is used as an
activation function because of its continuity and differentiability properties . The decoder function expressed in Eq.
(2) maps hidden representation y back to a reconstruction.
z = sg (V · y + bg) ,
where sg is the activation function of the decoder which
commonly uses either the identity function, sg(t) = t, or a
sigmoid function such as an encoder. We use W and V acts
as a weight matrix for the features. bf and bg acts as a bias
vector for encoding and decoding, respectively. Its training
phase ﬁnds optimal parameters θ = {W, V, bf, bg} which
minimize the reconstruction error between the input data and
its reconstruction output on a training set.
This study uses a modiﬁed form of AE, i.e. sparse AE
 . This is based on the experiments of Eskin et al. , in
which anomalies usually form small clusters in sparse areas
of a feature space. Moreover, dense and large clusters usually
contain benign data . For the sparsity of AE, we ﬁrst
observe the average output activation value of a neuron i, as
expressed by Eq. (3).
i xj + bf,i
where N is the total number of training data, xj is the jth training data, wT
is the i-th row of a weight matrix W,
and bf,i is the i-th row of a bias vector for encoding bf. By
lowering the value of ˆρi, a neuron i in the hidden layer shows
the speciﬁc feature presented in a smaller number of training
The task of machine-learning is to ﬁt a model to the given
training data. However, the model often ﬁts the particular
training data but is incapable of classifying other data and this
IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. XX, NO. X, XXXXX 2017
Fig. 3. SAE network with two hidden layers and two target classes
is known as the overﬁtting problem. In this case, we can use
a regularization technique to reduce the overﬁtting problem.
The sparsity regularization Ωsparsity evaluates how close the
average output activation value ˆρi and the desired value ρ are,
typically with Kullback-Leibler (KL) divergence to determine
the difference between the two distributions, as expressed in
Ωsparsity =
+ (1 −ρ) log
where h is the number of neurons in the hidden layer.
We may increase the value of the entries of the weight
matrix W to reduce the value of sparsity regularization. To
avoid this situation, we also add regularization for the weight
matrix, known as L2 regularization as stated in Eq. (5).
Ωweights = 1
where N and K are the number of training data and the
number of variables for each data, respectively.
The goal of training sparse AE is to ﬁnd the optimal
parameters, θ = {W, V, bf, bg}, to minimize the cost function
shown in Eq. (6).
(zkn −xkn)2 + λ · Ωweights + β · Ωsparsity,
which is a regulated mean square error with L2 regularization
and sparsity regularization. The coefﬁcient of the L2 regularization term λ and the coefﬁcient of sparsity regularization
term β are speciﬁed while training the AE.
2) Stacked (Sparse) AE: (Sparse) AE can be used as deep
learning technique by an unsupervised greedy layer-wise pretraining algorithm known as Stacked (Sparse) Auto Encoder
(SAE). Here, pre-training refers to the training of a single
AE using a single hidden layer. Each AE is trained separately
before being cascaded afterwards. This pre-training phase is
required to construct a stacked AE. In this algorithm, all layers
except the output layer are initialized in a multi-layer neural
network. Each layer is then trained in an unsupervised manner
as an AE, which constructs new representations of the input.
The performance of the unsupervised greedy layer-wise pretraining algorithm can be signiﬁcantly more accurate than the
supervised one. This is because the greedy supervised procedure may behave too greedy as it extracts less information
and considers one layer only . A neural network
containing only one hidden layer, it may discard some of the
information about the input data as more information could be
exploited by composing additional hidden layers.
Fig. 3 shows the SAE network with two hidden layers and
two target classes. The ﬁnal layer implements the softmax
function for the classiﬁcation in the deep neural network.
Softmax function is a generalized term of the logistic function
that suppresses the K-dimensional vector v ∈RK into Kdimensional vector v∗∈(0, 1)K, which adds up to 1. In this
function, it is deﬁned T and C as the number of training
instances and the number of classes, respectively. The softmax
layer minimizes the loss function, which is either the crossentropy function as in Eq. (7),
[zij log yij + (1 −zij) log (1 −yij)] ,
or the mean-squared error. However, the cross-entropy function is used in this study.
The features from the pre-training phase, which is greedy
layer wise, can be used either as an input to a standard
supervised machine-learning algorithm or as an initialization
for a deep supervised neural network.
B. Feature Selection
The supervised feature selection block in Fig.1 consists of
three different feature selection techniques. These techniques
are similar in that they consider their resulting weights to select
the subset of important features. The following subsections
contain further details of each feature selection technique.
1) D-FES-ANN: ANN is used as a weighted feature selection method. The ANN is trained with two target classes
only (normal and impersonation attack classes). Fig. 4 shows
an ANN network with one hidden layer only where b1 and
b2 represent the bias values for the corresponding hidden and
output layer, respectively.
In order to select the important features, we consider the
weight values between the ﬁrst two layers. The weight represents the contribution from the input features to the ﬁrst hidden
layer. A wij value close to zero means that the corresponding
input feature xj is meaningless for further propagation, thus
having one hidden layer is sufﬁcient for this particular task. We
AMINANTO et al.: DEEP ABSTRACTION AND WEIGHTED FEATURE SELECTION FOR WI-FI IMPERSONATION DETECTION
Fig. 4. ANN network with one hidden layer only
Algorithm 2 D-FES-ANN Function
1: function D-FES-ANN(InputFeatures)
Training ANN
for each input feature do
Compute Vj (Eq. (8))
Sort descending
SelectedFeatures ←Vj > threshold
return SelectedFeatures
10: end function
deﬁne the important value of each input feature as expressed
by Eq. (8).
where h is the number of neurons in the ﬁrst hidden layer.
As described in Algorithm 2, the feature selection process
involves selecting the features of which the Vj values are
greater than the threshold value after the input features are
sorted according to their Vj values in a descending order.
Following the weighted feature selection, ANN is also used
as a classiﬁer. When learning with ANN, a minimum global
error function is executed. It has two learning approaches,
supervised and unsupervised. This study uses a supervised
approach since knowing the class label may increase the
classiﬁer performance . In addition, a scaled conjugate
gradient optimizer, which is suitable for a large scale problem,
is used .
2) D-FES-SVM: A supervised SVM is usually used for
classiﬁcation or regression tasks. If n is the number of input
features, the SVM plots each feature value as a coordinate
point in n-dimensional space. Subsequently, a classiﬁcation
process is executed by ﬁnding the hyperplane that distinguishes two classes. Although SVM can handle a nonlinear
decision border of arbitrary complexity, we use a linear
SVM since the nature of the dataset can be investigated by
linear discriminant classiﬁers. The decision boundary for linear
SVM is a straight line in two-dimensional space. The main
computational property of SVM is the support vectors which
are the data points that lie closest to the decision boundary.
The decision function of input vector x as expressed by Eq.
(9), heavily depends on the support vectors.
D(x) = w⃗x + b
b = (yk −wxk)
Eqs. (10) and (11) show the corresponding value of w
and b, respectively. From Eq. (9), we can see that decision
function D(x) of input vector ⃗x is deﬁned as the sum of
the multiplication of a weight vector and input vector ⃗x and
a bias value. A weight vector w is a linear combination of
training patterns. The training patterns with non-zero weights
are support vectors. The bias value is the average of the
marginal support vectors.
SVM-Recursive Feature Elimination (SVM-RFE) is an application of RFE using the magnitude of the weight to perform
rank clustering . The RFE ranks the feature set and
eliminates the low-ranked features which contribute less than
the other features for classiﬁcation task . We use the SVM-
RFE by using the linear case described in Algorithm 3.
The inputs are training instances and class labels. First, we
initialize a feature ranked list that is ﬁlled by a subset of
important features that is used for selecting training instances.
We then train the classiﬁer and compute the weight vector of
the dimension length. After the value of the weight vector is
obtained, we compute the ranking criteria and ﬁnd the feature
with the smallest ranking criterion. Using that feature, the
feature ranking list is updated and the feature with the smallest
ranking criterion is eliminated. A feature ranked list is ﬁnally
created as its output.
Algorithm 3 D-FES-SVM Function
1: function D-FES-SVM(InputFeatures)
Training SVM
Compute w (Eq. (10))
Compute the ranking criteria
Find the smallest ranking criterion
f = argmin(c)
Update feature ranked list
r = [s(f), r]
Eliminate the smallest ranking criterion
s = s(1 : f −1, f + 1 : length(s))
SelectedFeatures ←s
return SelectedFeatures
14: end function
3) Decision Tree: C4.5 is robust to noise data and able to
learn disjunctive expressions . It has a k-ary tree structure,
which can represent a test of attributes from the input data by
each node. Every branch of the tree shows potentially selected
IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. XX, NO. X, XXXXX 2017
important features as the values of nodes and different test
results. C4.5 uses a greedy algorithm to construct a tree in
a top-down recursive divide-and-conquer approach . The
algorithm begins by selecting the attributes that yield the best
classiﬁcation result. This is followed by generating a test node
for the corresponding attributes. The data are then divided
based on the information gain value of the nodes according to
the test attributes that reside in the parent node. The algorithm
terminates when all data are grouped in the same class, or the
process of adding additional separations produces a similar
classiﬁcation result, based on its predeﬁned threshold. The
feature selection process begins by selecting the top-three level
nodes as explained in Algorithm 4. It then removes the equal
nodes and updates the list of selected features.
Algorithm 4 D-FES-C4.5 Function
1: function D-FES-C4.5(InputFeatures)
Training C4.5
SelectedFeatures ←top-three level nodes
for i=1 to n do
▷n = size of SelectedFeatures
for j=1 to n do
if SelectedFeatures[i]=SelectedFeatures[j]
then Remove SelectedFeatures[j]
return SelectedFeatures
11: end function
V. EVALUATION
A set of experiments was conducted to evaluate the performance of the proposed D-FES method in Wi-Fi impersonation
detection. Choosing a proper dataset is an important step
in the IDS research ﬁeld . We employed the AWID
Dataset which comprises the largest amount of Wi-Fi
network data collected from real network environments. We
achieved fair model comparison and evaluation by performing
the experiments on the same testing sets as in . We
then implement the proposed methodology using MATLAB
R2016a and Java code extracted and modiﬁed from WEKA
packages running on an Intel Xeon E-3-1230v3 CPU
@3.30 GHz with 32 GB RAM.
A. Dataset Pre-processing
There are two types of AWID dataset. The ﬁrst type, named
“CLS”, has four target classes, whereas the second, named
“ATK”, has 16 target classes. The 16 classes of the “ATK”
dataset belong to the four attack categories in the “CLS”
dataset. As an example, the Caffe-Latte, Hirte, Honeypot
and EvilTwin attack types listed in the “ATK” dataset, are
categorized as an impersonation attack in the “CLS” dataset.
Based on the size of the data instances included, the AWID
dataset comprises both full and reduced versions. In this study,
we use the reduced “CLS” for simplicity.
The data contained in the AWID dataset are generally
diverse in value, discrete, continuous, and symbolic, with a
DISTRIBUTION OF EACH CLASS FOR BOTH BALANCED AND UNBALANCED
Unbalanced
Impersonation
AWID dataset mimics the natural unbalanced network distribution between normal and
attack instances. ”Balanced” means to make equal distribution between the number of
normal instances (163,319) and thereof total attack instances (162,385). 15% of training
data were withdrawn for validation data.
Algorithm 5 Dataset Pre-processing Function
1: function DATASET PRE-PROCESSING(Raw Dataset)
function DATASET NORMALIZATION(Raw Dataset)
for each data instance do
cast into integer value
normalize (Eq. (12))
NormalizedDataset
end function
ING(NormalizedDataset)
Pick 10% of normal instances randomly
BalancedDataset
end function
InputDataset ←BalancedDataset
return InputDataset
15: end function
ﬂexible value range. These data characteristics could make it
difﬁcult for the classiﬁers to learn the underlying patterns correctly . The pre-processing phase thus includes mapping
symbolic valued attributes to numeric values, according to the
normalization steps and dataset-balancing process described
in Algorithm 5. The target classes are mapped to one of
these integer valued classes: 1 for normal instances, 2 for an
impersonation, 3 for a ﬂooding, and 4 for an injection attack.
Meanwhile, symbolic attributes such as receiver, destination,
transmitter, and source address are mapped to integer values
with a minimum value of 1 and a maximum value, which is
the number of all symbols. Some dataset attributes such as
the WEP Initialization Vector (IV) and Integrity Check Value
(ICV) are hexadecimal data, which need to be transformed
into integer values as well. The continuous data such as the
timestamps were also left for the normalization step. Some of
the attributes have question marks, ?, to indicate unavailable
values. We use one alternative in which the question mark
is assigned to a constant zero value . After all data are
transformed into numerical values, attribute normalization is
needed . Data normalization is a process; hence, all value
ranges of each attribute are equal. We adopt the mean range
method in which each data item is linearly normalized
between zero and one in order to avoid the undue inﬂuence of
AMINANTO et al.: DEEP ABSTRACTION AND WEIGHTED FEATURE SELECTION FOR WI-FI IMPERSONATION DETECTION
different scales . Eq. (12) shows the normalizing formula.
xi −min(x)
max(x) −min(x),
where, zi denotes the normalized value, xi refers to the
corresponding attribute value and min(x) and max(x) are the
minimum and maximum values of the attribute, respectively.
The reduced “CLS” data are a good representation of a real
network, in which normal instances signiﬁcantly outnumber
attack instances. The ratio between the normal and attack
instances is 10:1 for both unbalanced training and the test
dataset as shown in Table I. This property might be biased
to the training model and affect the model performance 
 . To alleviate this, we balance the dataset by selecting
10% of the normal instances randomly. However, we set a
certain value as the seed of the random number generator for
reproducibility purposes. The ratio between normal and attack
instances became 1:1, which is an appropriate proportion for
the training phase . D-FES is trained using the balanced
dataset and then veriﬁed on the unbalanced dataset.
B. Evaluation Metrics
We ensured that the evaluation of the performance of D-
FES was fair by adopting the most well-referenced modelperformance measures : accuracy (Acc), Detection Rate
(DR), False Alarm Rate (FAR), Mcc, Precision, F1 score,
CPU time to build model (TBM), and CPU time to test
the model (TT). Acc shows the overall effectiveness of an
algorithm . DR, also known as Recall, refers to the
number of impersonation attacks detected divided by the
total number of impersonation attack instances in the test
dataset. Unlike Recall, Precision counts the number of
impersonation attacks detected among the total number of
instances classiﬁed as an attack. The F1 score measures the
harmonic mean of Precision and Recall. FAR is the number
of normal instances classiﬁed as an attack divided by the total
number of normal instances in the test dataset while FNR
shows the number of attack instances that are unable to be
detected. Mcc represents the correlation coefﬁcient between
the detected and observed data . Intuitively, our goal is to
achieve a high Acc, DR, Precision, Mcc, and F1 score and
at the same time, maintaining low FAR, TBM, and TT. The
above measures can be deﬁned by Eqs. (13), (14), (15), (16),
(17), (18), and (19):
TP + TN + FP + FN ,
DR(Recall) =
Precision =
2TP + FP + FN ,
THE EVALUATION OF SAE’S SCHEMES
SAE Scheme
Imbalance 40 (154:40:10:4)
Imbalance 100 (154:100:50:4)
Balance 40 (154:40:10:4)
Balance 100 (154:100:50:4)
Each model uses either balanced or unbalanced data for the SAE algorithm with
following parameters: (input features: number of features in 1st hidden layer: number
of features in 2nd hidden layer: target classes).
(TP × TN) −(FP × FN)
(TP + FP)(TP + FN)(TN + FP)(TN + FN)
where True Positive (TP) is the number of intrusions correctly
classiﬁed as an attack, True Negative (TN) is the number of
normal instances correctly classiﬁed as a benign packet, False
Negative (FN) is the number of intrusions incorrectly classiﬁed
as a benign packet, and False Positive (FP) is the number of
normal instances incorrectly classiﬁed as an attack.
C. Experimental Result
The proposed D-FES is evaluated on a set of experiments.
First, we implement and verify different architectures of the
feature extractor, SAE. Second, we verify two feature selection approaches: ﬁlter-based and wrapper-based methods. We
ﬁnally validate the usefulness and the utility of D-FES on a
realistic unbalanced test dataset.
1) Feature Extraction: We vary the SAE architectures in
order to optimize the SAEs implementation with two hidden
layers. The features generated from the ﬁrst encoder layer
are employed as the training data in the second encoder
layer. Meanwhile, the size of each hidden layer is decreased
accordingly such that the encoder in the second encoder
layer learns an even smaller representation of the input data.
The regression layer with the softmax activation function is
then implemented in the ﬁnal step. The four schemes are
examined to determine the SAE learning characteristics. The
ﬁrst scheme, Imbalance 40, has two hidden layers with 40
and 10 hidden neurons in each layer. The second scheme, Imbalance 100, also has two hidden layers; however, it employs
100 and 50 hidden neurons in each layer. Although there is
no strict rule for determining the number of hidden neurons,
we consider a common rule of thumb , which ranges from
70% to 90% from inputs. The third and fourth schemes, named
Balance 40 and Balance 100, have the same hidden layer
architecture with the ﬁrst and second schemes, respectively;
however, in this case we use the balanced dataset, because the
common assumption is that a classiﬁer model built by a highly
unbalanced data distribution performs poorly on minority class
detection . For our testing purpose, we leverage all four
classes contained in the AWID dataset.
Table II shows the evaluation of the SAE schemes. The
SAE architectures with 100 hidden neurons have higher DR
than those with 40 hidden neurons. On the other hand, the
SAE architectures with 40 hidden neurons have lower FAR
than those with 100 hidden neurons. In order to draw a proper
conclusion, other performance metrics that consider whole
IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. XX, NO. X, XXXXX 2017
Evaluation of SAE’s Scheme on Acc and F1 Score. The red bar
represents F1 score while the blue bar represents Acc rate.
classes are needed as the DR checks for the attack class only
and the FAR measures for the normal class only. The Acc
metric would be affected by the distribution of data, for which
different balanced and unbalanced distributions may result in
an incorrect conclusion. If we consider the Acc metric only as
in Fig. 5, we may incorrectly select the Imbalance 100 with
97.03% accuracy, whereas the Balance 100 only achieved
87.63% accuracy. In fact, the Imbalance 100 achieved the
highest accuracy rate because of the unbalanced proportion
of normal class to attack class. We obtain the best performance by checking F1 score, for which the Balance 100 has
achieved the highest F1 score among all schemes with 87.59%.
Therefore, we choose the SAE architecture with 154:100:50:4
2) Feature Selection: In order to show the effectiveness of
D-FES, we compare the following feature selection methods:
• CfsSubsetEval (CFS) considers the predictive ability
of each feature individually and the degree of redundancy
between them, in order to evaluate the importance of
a subset of features. This approach selects subsets of
features that are highly correlated with the class, while
having low inter-correlation.
• Correlation (Corr) measures the correlation between the
feature and the class in order to evaluate the importance
of a subset of features.
• The weight from a trained ANN model mimics the
importance of the correspondence input. By selecting
the important features only, the training process becomes
lighter and faster than before .
• SVM measures the importance of each feature based on
the weight of the SVM classiﬁcation results.
• C4.5 is one of the decision tree approaches. It can select a
subset of features that is not highly correlated. Correlated
features should be in the same split; hence, features that
belong to different splits are not highly correlated .
A ﬁlter-based method usually measures the correlation
and redundancy of each attribute without executing a learning algorithm. Therefore, the ﬁlter-based method is usually
lightweight and fast. On the other hand, the wrapper-based
method examines the results of any learning algorithm that
outputs a subset of features . CFS and Corr belong to
Cross entropy error of ANN. The best validation performance is
achieved at the epoch of 163.
the ﬁlter-based methods, whereas ANN, SVM, and C4.5 are
wrapper-based methods.
We select a subset of features using the wrapper-based
method for considering each feature weight. For ANN, we
ﬁrst set a threshold weight value and if the weight of a feature
is greater than the threshold then the feature is selected. The
SVM attribute selection function ranks the features based on
their weight values. The subset of features with a higher weight
value than the predeﬁned threshold value is then selected.
Similarly, C4.5 produces a deep binary tree. We select the
features that belong to the top-three levels in the tree. CFS
produces a ﬁxed number of selected features and Corr provides
a correlated feature list.
During ANN training for both feature selection and classiﬁcation, we optimize the trained model using a separate
validation dataset; that is, we separate the dataset into three
parts: training data, validation data and testing data in the
following proportions: 70%, 15% and 15%, respectively. The
training data are used as input into the ANN during training
and the weights of neurons are adjusted during the training according to its classiﬁcation error. The validation data are used
to measure model generalization providing useful information
on when to terminate the training process. The testing data is
used for an independent measure of the model performance
after training. The model is said to be optimized when it
reaches the smallest average square error on the validation
dataset. Fig. 6 shows an example of ANN performance with
respect to the cross-entropy error during ANN training. At
the epoch of 163, the cross-entropy error, a logarithmic-based
error measurement comparing the output values and desired
values, starts increasing, meaning that at the epoch of 163,
the model is optimized. Although the training data outputs
decreasing error values after the epoch point of 163, the
performance of the model no longer continues to improve, as
the decreasing cross-entropy error may indicate the possibility
of overﬁtting.
Table III contains all the feature lists selected from the
various feature selection methods. Some features are essential
for detecting an impersonation attack. These are the 4th and
the 7th, which are selected by the ANN and SVM and the 71st,
AMINANTO et al.: DEEP ABSTRACTION AND WEIGHTED FEATURE SELECTION FOR WI-FI IMPERSONATION DETECTION
FEATURE SET COMPARISONS BETWEEN FEATURE SELECTION AND D-FES
Selected Features
5, 38, 70, 71, 154
38, 71, 154, 197
47, 50, 51, 67, 68, 71, 73, 82
71, 155, 156, 159, 161, 165, 166, 179, 181, 191, 193, 197
4, 7, 38, 77, 82, 94, 107, 118
4, 7, 38, 67, 73, 82, 94, 107, 108, 111, 112, 122, 138, 140, 142, 154, 161, 166, 192, 193, 201, 204
47, 64, 82, 94, 107, 108, 122, 154
4, 7, 47, 64, 68, 70, 73, 78, 82, 90, 94, 98, 107, 108, 111, 112, 122, 130, 141, 154, 159
11, 38, 61, 66, 68, 71, 76, 77, 107, 119, 140
61, 76, 77, 82, 107, 108, 109, 111, 112, 119, 158, 160
Characteristics of (a) 38th and (b) 166th features. The blue line
represents normal instances while the red line represents attack instances.
MODEL COMPARISONS ON SELECTED FEATURES
which is selected by CFS and Corr. The characteristics of the
selected features are shown in Figs. 7(a) and 7(b). The blue
line indicates normal instances and at the same time, the red
line depicts the characteristics of an impersonation attack. We
can distinguish between normal and attack instances based on
the attribute value of data instances. For example, once a data
instance has an attribute value of 0.33 in the 166th feature,
the data instance has high probability of being classiﬁed as an
attack. This could be applied to the 38th and other features as
MODEL COMPARISONS ON D-FES FEATURE SET
FEATURE SET SELECTED BY D-FES-SVM
Feature Name
Description
radiotap.datarate
Data rate (Mb/s)
wlan.fc.type subtype
Type or Subtype
Sequence number
wlan mgt.ﬁxed.capabilities.preamble
Short Preamble
wlan mgt.ﬁxed.timestamp
wlan mgt.ﬁxed.beacon
Beacon Interval
wlan mgt.tim.dtim period
DTIM period
Table IV lists the performance of each algorithm on the
selected feature set only. SVM achieved the highest DR
(99.86%) and Mcc (99.07%). However, it requires CPU time
of 10,789s to build a model, the longest time among the
models observed. As expected, the ﬁlter-based methods (CFS
and Corr) built their models quickly; however, they attained
the lowest Mcc for CFS (89.67%).
Table V compares the performances of the candidate models
on the feature sets that are produced by D-FES. SVM again
achieved the highest DR (99.92%) and Mcc (99.92%). It
also achieved the highest FAR with a value of only 0.01%.
Similarly, the lowest Mcc is achieved by Corr (95.05%). This
enables us to draw the conclusion that wrapper-based feature
selections outperform ﬁlter-based feature selections. As SVM
showed the best performance, we may consider the properties
of selected features by SVM as described in Table VI.
We observe the following patterns from Tables IV and V:
Only two out of ﬁve methods (Corr anf C4.5) showed lower
FAR without D-FES, which we expect to minimize the FAR
value of the proposed IDS. This phenomenon might exist
because the original and extracted features are not correlated
because Corr and C4.5 measure the correlation between each
feature. Filter-based feature selection methods require much
shorter CPU time compared to the CPU time taken by D-FES.
However, D-FES improves the ﬁlter-based feature selections
performance signiﬁcantly.
IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. XX, NO. X, XXXXX 2017
Fig. 8. Models performance comparisons in terms of (a) Acc, (b) Precision
and (c) F1 score. The blue bar represents performances by feature selection
only while the red bar represents performances by D-FES.
Similar patterns are captured by Figs. 8(a), 8(b) and 8(c),
which depict the performance of different models in terms
of Acc, Precision and F1 score, respectively. D-FES-SVM
achieved the highest Acc, Precision and F1 score of 99.97%,
99.96% and 99.94%, respectively. By D-FES, all methods
achieve Precision of more than 96%, shows that D-FES can
reduce number of incorrect classiﬁcation of normal instances
as an attack. We can also observe that D-FES improves the
Acc of ﬁlter-based feature selections signiﬁcantly. Except the
C4.5, all feature selection methods are improved both the Acc
and F1 score by using D-FES. This makes the proposed D-
FES a good candidate for an intrusion detector.
Fig. 9. Model performance comparisons between D-FES and random method
in terms of: (a) DR (b) FAR (c) FNR (d) TT
AMINANTO et al.: DEEP ABSTRACTION AND WEIGHTED FEATURE SELECTION FOR WI-FI IMPERSONATION DETECTION
VI. COMPARISONS WITH STATE-OF-THE-ART METHODS
For a fair testing and comparison of the candidate models,
we leverage the unbalanced test dataset, as it mimics the real
situation of Wi-Fi networks. The unbalanced dataset contains
530,785 normal instances and 20,079 impersonation attack
instances as shown in Table I. We compare D-FES-SVM,
as the highest F1 score and randomly selected features with
respect to the number of features involved during training
as depicted in Figs. 9(a), 9(b), 9(c) and 9(d). The blue line
with blue squares shows the performance of the proposed D-
FES-SVM method, whereas the red line with red triangles
represents selected features randomly. Fig. 9(a) shows that
the model learned by D-FES-SVM has the ability to classify
the impersonation attack instances with selected features only.
The DR of D-FES-SVM has increased insigniﬁcantly as
the number of features added since the previously selected
features are more informative. D-FES-SVM also maintains
low FAR on ﬁve features only; even more, FAR of D-FES-
SVM further decreased as the number of features added as
shown in Fig. 9(b). Although the random method achieved
almost perfect FAR of 0%, this result does not necessarily
mean it is good because the random method classiﬁed all
data instances as normal instances, thus none of the normal
instances are misclassiﬁed as an attack and all attack instances
are incorrectly detected as a normal instance as shown in 100%
of FNR in Fig. 9(c). We can see that both D-FES and random
method are taking the same amount of time during the testing
with 25 features as shown in Fig. 9(d). However, the random
method took longer time at 70 features because the 5 initial
features selected were inappropriate to distinguish between
normal and attack instances as indicated as 0% of DR. Thus,
the random method requires less amount of time as it is not
capable enough to distinguish attack instances. The CPU time
increases as the number of features increases. D-FES-SVM
takes longer time than the random method and it increases
DR signiﬁcantly. However, the random method cannot even
classify a single impersonation attack.
We also compare the performance of D-FES against our previous work and that of Kolias et al. . The experimental
results are provided in Table VII. D-FES-SVM classiﬁes impersonation attack instances with a detection rate of 99.918%,
whereas it maintains a low FAR of only 0.012%. However, as a
trade-off, D-FES takes longer time to build the model as seen
in Tables IV and V. This computational burden may affect
the feasibility to implement D-FES in resource-constrained
devices. Nevertheless, we may minimize this by implementing
over the clouds or adopting distributed approach using
parallel computation for SAE and ANN. The other two methods, D-FES-ANN and D-FES-C4.5 performed comparably to
D-FES-SVM, demonstrating that weighted feature selection is
useful for improving feature-based learning. Kolias et al. 
tested various classiﬁcation algorithms such as random tree,
random forest, J48, and na¨ıve Bayes, on their own dataset. In
terms of an impersonation attack, the na¨ıve Bayes algorithm
showed the best performance by correctly classifying 4,419 out
of 20,079 impersonation instances. However, it only achieved
approximately 22% DR, which is unsatisfactory. Our previous
COMPARISONS WITH OTHER WORK
D-FES-C4.5
ANN+SAE 
Kolias et al. 
approach, combining ANN with SAE, successfully improved
the model performance on the impersonation attack detection
task . We achieved a DR of 84.829% and a FAR of
2.364%. Unlike our previous work , this study leverages
SAE as a feature extractor and considers the extracted features
in the feature selection process in order to achieve a condensed
subset of features. We observe the advantage of SAE for
abstracting complex and high-dimensional data, as shown by
the near-perfect DR and FAR achieved by D-FES.
VII. CONCLUSION
In this study, we presented a novel method, D-FES, which
combines stacked feature extraction and weighted feature
selection techniques in order to detect impersonation attacks
in Wi-Fi networks. SAE is implemented to achieve high-level
abstraction of complex and large amounts of Wi-Fi network
data. The model-free properties in SAE and its learnability
on complex and large-scale data take into account the open
nature of Wi-Fi networks, where an adversary can easily inject
false data or modify data forwarded in the network. Extracted
features combined with original features were examined using
weighted feature selection in order to eliminate redundant and
unimportant features. We tested ANN, SVM, and C4.5 for
weighted feature selection and observed that a few important
features are sufﬁcient to detect impersonation attack instances
in large-scale Wi-Fi networks. The proposed methodology
achieved a detection rate of 99.918% and a false alarm rate
of 0.012%. Clearly, these are the best results on impersonation attacks reported in the literature. In future, we plan to
extend D-FES to i) detect any attack classes not limited to
impersonation attack only, ii) have the capability to identify
an unknown attack that exploits zero-day vulnerability and
iii) ﬁt the distributed nature of the IoT environment, which is
characterized by limited computing power, memory, and power
Table VIII summarizes the abbreviations used in this study.
ACKNOWLEDGMENT
The authors would like to thank the anonymous reviewers
for their helpful and constructive comments that greatly contributed to improving the ﬁnal version of this study.
IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. XX, NO. X, XXXXX 2017
TABLE VIII
LIST OF ABBREVIATIONS
Abbreviation
Abbreviation
Intrusion Detection System
Auto Encoder
Internet of Things
Artiﬁcial Neural Network
Initialization Vector
Access Point
Matthews correlation
coefﬁcient
Address Resolution Protocol
Kullback-Leibler
Aegean Wi-Fi Intrusion
Man in the Middle
CfsSubsetEval 
Particle Swarm Optimization
Correlation
Received Signal Strength
Central Processing Unit
Stacked Auto Encoder
Differential Flag Byte
Service Set Identiﬁer
Deep Feature Extraction and
Support Vector Machine
Detection Rate
SVM-Recursive Feature
Elimination
False Alarm Rate
Time to Build Model
False Negative
Transmission Control
Protocol/Internet Protocol
False Negative Rate
True Negative
False Positive
Time to Test Model
Efﬁcient Probabilistic
Packet Marking
True Positive
Enhanced Support Vector
Decision Function
Wired Equivalent Privacy
Integrity Check Value