Noname manuscript No.
(will be inserted by the editor)
Pedestrian Alignment Network for
Large-scale Person Re-identiﬁcation
Zhedong Zheng · Liang Zheng · Yi Yang
Received: date / Accepted: date
Abstract Person re-identiﬁcation (person re-ID) is mostly
viewed as an image retrieval problem. This task aims
to search a query person in a large image pool. In practice, person re-ID usually adopts automatic detectors to
obtain cropped pedestrian images. However, this process suﬀers from two types of detector errors: excessive background and part missing. Both errors deteriorate the quality of pedestrian alignment and may
compromise pedestrian matching due to the position
and scale variances. To address the misalignment problem, we propose that alignment can be learned from an
identiﬁcation procedure. We introduce the pedestrian
alignment network (PAN) which allows discriminative
embedding learning and pedestrian alignment without
extra annotations. Our key observation is that when
the convolutional neural network (CNN) learns to discriminate between diﬀerent identities, the learned feature maps usually exhibit strong activations on the human body rather than the background. The proposed
network thus takes advantage of this attention mechanism to adaptively locate and align pedestrians within
a bounding box. Visual examples show that pedestrians are better aligned with PAN. Experiments on three
large-scale re-ID datasets conﬁrm that PAN improves
the discriminative ability of the feature embeddings and
yields competitive accuracy with the state-of-the-art
methods. 1
Keywords Computer vision · Person re-identiﬁcation ·
Person search · Person alignment · Image retrieval ·
Deep learning
Zhedong Zheng · Liang Zheng · Yi Yang
University of Technology Sydney, Australia
E-mail: 
The project website of this paper is 
com/layumi/Pedestrian_Alignment.
Fig. 1 Sample images inﬂuenced by detector errors (the ﬁrst
row) which are aligned by the proposed method (the second
row). Two types of errors are shown: excessive background
and part missing. We show that the pedestrian alignment
network (PAN) corrects the misalignment problem by 1) removing extra background or 2) padding zeros to the image
borders. PAN reduces the scale and position variance, and the
aligned output thus beneﬁt the subsequent matching step.
1 Introduction
Person re-identiﬁcation (person re-ID) aims at spotting
the target person in diﬀerent cameras, and is mostly
viewed as an image retrieval problem, i.e., searching
for the query person in a large image pool (gallery).
Recent progress mainly consists in the discriminatively
learned embeddings using the convolutional neural network (CNN) on large-scale datasets. The learned embeddings extracted from the ﬁne-tuned CNNs are shown
to outperform the hand-crafted features [Zheng et al.,
2016b,Xiao et al., 2016,Zhong et al., 2017].
Among the many inﬂuencing factors, misalignment
is a critical one in person re-ID. This problem arises
due to the usage of pedestrian detectors. In realistic setarXiv:1707.00408v1 [cs.CV] 3 Jul 2017
Zhedong Zheng et al.
tings, the hand-drawn bounding boxes, existing in some
previous datasets such as VIPER [Gray et al., 2007],
CUHK01 [Li et al., 2012] and CUHK02 [Li and Wang,
2013], are infeasible to acquire when millions of bounding boxes are to be generated. So recent large-scale
benchmarks such as CUHK03 [Li et al., 2014], Market1501 [Zheng et al., 2015] and MARS [Zheng et al.,
2016a] adopt the Deformable Part Model (DPM) [Felzenszwalb et al., 2008] to automatically detect pedestrians.
This pipeline largely saves the amount of labeling eﬀort
and is closer to realistic settings. However, when detectors are used, detection errors are inevitable, which may
lead to two common noisy factors: excessive background
and part missing. For the former, the background may
take up a large proportion of a detected image. For the
latter, a detected image may contain only part of the
human body, i.e., with missing parts (see Fig. 1).
Pedestrian alignment and re-identiﬁcation are two
connected problems. When we have the identity labels
of the pedestrian bounding boxes, we might be able
to ﬁnd the optimal aﬃne transformation that contains
the most informative visual cues to discriminate between diﬀerent identities. With the aﬃne transformation, pedestrians can be better aligned. Furthermore,
with superior alignment, more discriminative features
can be learned, and the pedestrian matching accuracy
can, in turn, be improved.
Motivated by the above-mentioned aspects, we propose to incorporate pedestrian alignment into an identi-
ﬁcation re-ID architecture, yielding the pedestrian alignment network (PAN). Given a pedestrian detected image, this network simultaneously learns to re-localize
the person and categorize the person into pre-deﬁned
identities. Therefore, PAN takes advantage of the complementary nature of person alignment and re-identiﬁcation.
In a nutshell, the training process of PAN is composed of the following components: 1) a network to predict the identity of an input image, 2) an aﬃne transformation to be estimated which re-localizes the input
image, and 3) another network to predict the identity
of the re-localized image. For components 1) and 3), we
use two convolutional branches called the base branch
and alignment branch, to respectively predict the identity of the original image and the aligned image. Internally, they share the low-level features and during testing are concatenated at the fully-connected (FC) layer
to generate the pedestrian descriptor. In component 2),
the aﬃne parameters are estimated using the feature
maps from the high-level convolutional layer of the base
branch. The aﬃne transformation is later applied on
the lower-level feature maps of the base branch. In this
step, we deploy a diﬀerentiable localization network:
spatial transformer network (STN) [Jaderberg et al.,
2015]. With STN, we can 1) crop the detected images
which may contain too much background or 2) pad zeros to the borders of images with missing parts. As a
result, we reduce the impact of scale and position variances caused by misdetection and thus make pedestrian
matching more precise.
Note that our method addresses the misalignment
problem caused by detection errors, while the commonly
used patch matching strategy aims to discover matched
local structures in well-aligned images. For methods
that use patch matching, it is assumed that the matched
local structures locate in the same horizontal stripe [Li
et al., 2014, Yi et al., 2014, Zhao et al., 2013a, Zhao
et al., 2014, Liao et al., 2015, Cheng et al., 2016] or
square neighborhood [Ahmed et al., 2015]. Therefore,
these algorithms are robust to some small spatial variance, e.g., position and scale. However, when misdetection happens, due to the limitation of the search scope,
this type of methods may fail to discover the matched
structures, and the risk of part mismatching may be
high. Therefore, regarding the problem to be solved,
the proposed method is signiﬁcantly diﬀerent from this
line of works [Li et al., 2014, Yi et al., 2014, Ahmed
et al., 2015, Zhao et al., 2013a, Zhao et al., 2014, Liao
et al., 2015,Cheng et al., 2016]. We speculate that our
method is a good complementary step for those using
part matching.
Our contributions are summarized as follows:
– We propose the pedestrian alignment network (PAN),
which simultaneously aligns pedestrians within images and learns pedestrian descriptors. Except for
the identity label, we do not need any extra annotation;
– We observe that the manually cropped images are
not as perfect as preassumed to be. We show that
our network also improves the re-ID performance
on the hand-drawn datasets which are considered
to have decent person alignment.
– We achieve competitive accuracy compared to the
state-of-the-art methods on three large-scale person
re-ID datasets .
The rest of this paper is organized as follows. Section 2 reviews and discusses related works. Section 3
illustrates the proposed method in detail. Experimental results and comparisons on three large-scale person
re-ID datasets are discussed in Section 4, followed by
conclusions in Section 5.
Pedestrian Alignment Network for Large-scale Person Re-identiﬁcation
2 Related work
Our work aims to address two tasks: person re-identiﬁcation
(person re-ID) and person alignment jointly. In this section, we review the relevant works in these two domains.
2.1 Hand-crafted Systems for Re-ID
Person re-ID needs to ﬁnd the robust and discriminative
features among diﬀerent cameras. Several pioneering
approaches have explored person re-ID by extracting
local hand-crafted features such as LBP [Mignon and
Jurie, 2012], Gabor [Prosser et al., 2010] and LOMO
[Liao et al., 2015]. In a series of works by [Zhao et al.,
2014,Zhao et al., 2013a,Zhao et al., 2013b], the 32-dim
LAB color histogram and the 128-dim SIFT descriptor
are extracted from each 10 × 10 patches. [Zheng et al.,
2015] use color name descriptor for each local patch and
aggregate them into a global vector through the Bagof-Words model. Approximate nearest neighbor search
[Wang and Li, 2012] is employed for fast retrieval but
accuracy compromise. [Chen et al., 2017] also deploy
several diﬀerent hand-crafted features extracting from
overlapped body patches. Diﬀerently, [Cheng et al., 2011]
localize the parts ﬁrst and calculate color histograms for
part-to-part correspondences. This line of works is beneﬁcial from the local invariance in diﬀerent viewpoints.
Besides ﬁnding robust feature, metric learning is
nontrivial for person re-ID. [K¨ostinger et al., 2012] propose “KISSME” based on Mahalanobis distance and
formulate the pair comparison as a log-likelihood ratio
test. Further, [Liao et al., 2015] extend the Bayesian
face and KISSME to learn a discriminant subspace with
a metric. Aside from the methods using Mahalanobis
distance, Prosser et al. apply a set of weak RankSVMs
to assemble a strong ranker [Prosser et al., 2010]. Gray
and Tao propose using the AdaBoost algorithm to fuse
diﬀerent features into a single similarity function [Gray
and Tao, 2008]. [Loy et al., 2010] propose a cross canonical correlation analysis for the video-based person re-
2.2 Deeply-learned Models for Re-ID
CNN-based deep learning models have been popular
since [Krizhevsky et al., 2012] won ILSVRC’12 by a
large margin. It extracts features and learns a classiﬁer
in an end-to-end system. More recent approaches based
on CNN apply spatial constraints by splitting images
or adding new patch-matching layers. [Yi et al., 2014]
split a pedestrian image into three horizontal parts and
respectively trained three part-CNNs to extract features. Similarly, [Cheng et al., 2016] split the convolutional map into four parts and fuse the part features
with the global feature. [Li et al., 2014] add a new
layer that multiplies the activation of two images in
diﬀerent horizontal stripes. They use this layer to allow patch matching in CNN explicitly. Later, [Ahmed
et al., 2015] improve the performance by proposing a
new part-matching layer that compares the activation
of two images in neighboring pixels. Besides, [Varior
et al., 2016a] combine CNN with some gate functions,
similar to long-short-term memory in spirit, which aims to focus
on the similar parts of input image pairs adaptively.
But it is limited by the computational ineﬃciency because the input should be in pairs. Similarly, [Liu et al.,
2016a] propose a soft attention-based model to focus on
parts and combine CNN with LSTM components selectively; its limitation also consists of the computation
ineﬃciency.
Moreover, a convolutional network has the high discriminative ability by itself without explicit patch-matching.
For person re-ID, [Zheng et al., 2016c] directly use a
conventional ﬁne-tuning approach on Market-1501 [Zheng
et al., 2015] and their performance outperform other
recent results. [Wu et al., 2016c] combine the CNN embedding with hand-crafted features. [Xiao et al., 2016]
jointly train a classiﬁcation model with multiple datasets
and propose a new dropout function to deal with the
hundreds of identity classes. [Wu et al., 2016b] deepen
the network and use ﬁlters of smaller size. [Lin et al.,
2017] use person attributes as auxiliary tasks to learn
more information. [Zheng et al., 2016d] propose combining the identiﬁcation model with the veriﬁcation model
and improve the ﬁne-tuned CNN performance. [Ding
et al., 2015] and [Hermans et al., 2017] use triplet samples for training the network which considers the images
from the same people and the diﬀerent people at the
same time. Recent work by Zheng et al. combined original training dataset with GAN-generated images and
regularized the model [Zheng et al., 2017b]. In this paper, we adopt the similar convolutional branches without explicit part-matching layers. It is noted that we
focus on a diﬀerent goal on ﬁnding robust pedestrian
embedding for person re-identiﬁcation, and thus our
method can be potentially combined with the previous
methods to further improve the performance.
2.3 Objective Alignment
Face alignment (here refer to the rectiﬁcation of face
misdetection) has been widely studied. [Huang et al.,
2007] propose an unsupervised method called funneled
Zhedong Zheng et al.
Fig. 2 Architecture of the pedestrian alignment network (PAN). It consists of two identiﬁcation networks (blue) and an aﬃne
estimation network (orange). The base branch predicts the identities from the original image. We use the high-level feature
maps of the base branch (Res4 Feature Maps) to predict the grid. Then the grid is applied to the low-level feature maps
(Res2 Feature Maps) to re-localize the pedestrian (red star). The alignment stream then receives the aligned feature maps to
identify the person again. Note that we do not perform alignment on the original images (dotted arrow) as previously done
in [Jaderberg et al., 2015] but directly on the feature maps. In the training phase, the model minimizes two identiﬁcation losses.
In the test phase, we concatenate two 1 × 1 × 2048 FC embeddings to form a 4096-dim pedestrian descriptor for retrieval.
image to align faces according to the distribution of
other images and improve this method with convolutional RBM descriptor later [Huang et al., 2012]. However, it is not trained in an end-to-end manner, and
thus following tasks i.e., face recognition take limited
beneﬁts from the alignment. On the other hand, several works introduce attention models for task-driven
object localization. Jadeburg et al. [Jaderberg et al.,
2015] deploy the spatial transformer network (STN) to
ﬁne-grained bird recognition and house number recognition. [Johnson et al., 2015] combine faster-RCNN [Girshick, 2015], RNN and STN to address the localization and description in image caption. Aside from using STN, Liu et al. use reinforcement learning to detect parts and assemble a strong model for ﬁne-grained
recognition [Liu et al., 2016c].
In person re-ID, [Baltieri et al., 2015] exploits 3D
body models to the well-detected images to align the
pose but does not handle the misdetection problem.
Besides, the work that inspires us the most is “Pose-
Box” proposed by [Zheng et al., 2017a]. The PoseBox
is a strengthened version of the Pictorial Structures
proposed in [Cheng et al., 2011]. PoseBox is similar to
our work in that 1) both works aim to solve the misalignment problem, and that 2) the networks have two
convolutional streams. Nevertheless, our work diﬀers
signiﬁcantly from PoseBox in two aspects. First, Pose-
Box employs the convolutional pose machines (CPM) to
generate body parts for alignment in advance, while this
work learns pedestrian alignment in an end-to-end manner without extra steps. Second, PoseBox can tackle the
problem of excessive background but may be less eﬀective when some parts are missing, because CPM fails
to detect body joints when the body part is absent.
However, our method automatically provides solutions
to both problems, i.e., excessive background and part
3 Pedestrian Alignment Network
3.1 Overview of PAN
Our goal is to design an architecture that jointly aligns
the images and identiﬁes the person. The primary challenge is to develop a model that supports end-to-end
training and beneﬁts from the two inter-connected tasks.
The proposed architecture draws on two convolutional
branches and one aﬃne estimation branch to simultaneously address these design constraints. Fig. 2 brieﬂy
illustrates our model.
To illustrate our method, we use the ResNet-50 model
[He et al., 2016] as the base model which is applied
on the Market-1501 dataset [Zheng et al., 2015]. Each
Res i, i = 1, 2, 3, 4, 5 block in Fig. 2 denotes several convolutional layers with batch normalization, ReLU, and
optionally max pooling. After each block, the feature
Pedestrian Alignment Network for Large-scale Person Re-identiﬁcation
Fig. 3 We visualize the Res4 Feature Maps in the base
branch. We observe that high responses are mostly concentrated on the pedestrian body. So we use the Res4 Feature
Maps to estimate the aﬃne parameters.
maps are down-sampled to be half of the size of the
feature maps in the previous block. For example, Res 1
down-samples the width and height of an image from
224 × 224 to 112 × 112. In Section 3.2 and Section 3.3,
we ﬁrst describe the convolutional branches and aﬃne
estimation branches of our model. Then in Section 3.4
we address the details of a pedestrian descriptor. When
testing, we use the descriptor to retrieve the query person. Further, we discuss the re-ranking method as a
subsequent processing in Section 3.5.
3.2 Base and Alignment Branches
Recent progress in person re-ID datasets allows the
CNN model to learn more discriminative visual representations. There are two main convolutional branches
exist in our model, called the base branch and the alignment branch. Both branches are classiﬁcation networks
that predict the identity of the training images. Given
an originally detected image, the base branch not only
learns to distinguish its identity from the others but
also encodes the appearance of the detected image and
provides the clues for the spatial localization (see Fig.
3). The alignment branch shares a similar convolutional
network but processes the aligned feature maps produced by the aﬃne estimation branch.
In the base branch, we train the ResNet-50 model
[He et al., 2016], which consists of ﬁve down-sampling
blocks and one global average pooling. We deploy the
model pre-trained on ImageNet [Deng et al., 2009] and
remove the ﬁnal fully-connected (FC) layer. There are
K = 751 identities in the Market-1501 training set, so
we add an FC layer to map the CNN embedding of size
1 × 1 × 2048 to 751 unnormalized probabilities. The
alignment branch, on the other hand, is comprised of
three ResBlocks and one average pooling layer. We also
add an FC layer to predict the multi-class probabilities.
The two branches do not share weight. We use W1 and
W2 to denote the parameters of the two convolutional
branches, respectively.
More formally, given an input image x, we use p(k|x)
to denote the probability that the image x belongs to
the class k ∈{1...K}. Speciﬁcally, p(k|x) =
k=1 exp(zi).
Here zi is the outputted probability from the CNN
model. For the two branches, the cross-entropy losses
are formulated as:
lbase(W1, x, y) = −
(log(p(k|x))q(k|x)),
lalign(W2, xa, y) = −
(log(p(k|xa))q(k|xa)),
where xa denotes the aligned input. It can be derived
from the original input xa = T(x). Given the label y,
the ground-truth distribution q(y|x) = 1 and q(k|x) = 0
for all k ̸= y. If we discard the 0 term in Eq. 1 and Eq.
2, the losses are equivalent to:
lbase(W1, x, y) = −log(p(y|x)),
lalign(W2, xa, y) = −log(p(y|xa)).
Thus, at each iteration, we wish to minimize the total
entropy, which equals to maximizing the possibility of
the correct prediction.
3.3 Aﬃne Estimation Branch
To address the problems of excessive background and
part missing, the key idea is to predict the position of
the pedestrian and do the corresponding spatial transform. When excessive background exists, a cropping
strategy should be used; under part missing, we need
to pad zeros to the corresponding image borders. Both
strategies need to ﬁnd the parameters for the aﬃne
transformation. In this paper, this function is implemented by the aﬃne estimation branch.
The aﬃne estimation branch receives two input tensors of activations 14×14×1024 and 56×56×256 from
the base branch. We name the two tensors the Res2
Feature Maps and the Res4 Feature Maps, respectively.
The Res4 Feature Maps contain shallow feature maps
of the original image and reﬂects the local pattern information. On the other hand, since the Res2 Feature
Maps are closer to the classiﬁcation layer, it encodes
the attention on the pedestrian and semantic cues for
aiding identiﬁcation. The aﬃne estimation branch contains one bilinear sampler and one small network called
Zhedong Zheng et al.
Grid Network. The Grid Network contains one Res-
Block and one average pooling layer. We pass the Res4
Feature Maps through Grid Network to regress a set
of 6-dimensional transformer parameters. The learned
transforming parameters θ are used to produce the image grid. The aﬃne transformation process can be written as below,
θ11 θ12 θ13
θ21 θ22 θ23
i) are the target coordinates on the output feature map, and (xs
i ) are the source coordinates on the input feature maps (Res2 Feature Maps).
θ11, θ12, θ21 and θ22 deal with the scale and rotation
transformation, while θ13 and θ23 deal with the oﬀset. In this paper, we set the coordinates as: (-1,-1)
refer to the pixel on the top left of the image, while
(1,1) refer to the bottom right pixel. For example, if
0.8 0 −0.1
, the pixel value of point (−1, −1)
on the output image is equal to that of (−0.9, −0.7) on
the original map. We use a bilinear sampler to make
up the missing pixels, and we assign zeros to the pixels
located out of the original range. So we obtain an injective function from the original feature map V to the
aligned output U. More formally, we can formulate the
(xs,ys)max(0, 1−|xt−m|)max(0, 1−|yt−n|).
(m,n) is the output feature map at location (m, n) in
channel c, and V c
(xs,ys) is the input feature map at location (xs, ys) in channel c. If (xt, yt) is close to (m, n),
we add the pixel at (xs, ys) according to the bilinear
In this work, we do not perform pedestrian alignment on the original image; instead, we choose to achieve
an equivalent function on the shallow feature maps. By
using the feature maps, we reduce the running time and
parameters of the model. This explains why we apply
re-localization grid on the feature maps. The bilinear
sampler receives the grid, and the feature maps to produce the aligned output xa. We provide some visualization examples in Fig. 3. Res4 Feature maps are shown.
We can observe that through ID supervision, we are
able to re-localize the pedestrian and correct misdetections to some extent.
3.4 Pedestrian Descriptor
Given the ﬁne-tuned PAN model and an input image
xi, the pedestrian descriptor is the weighted fusion of
the FC features of the base branch and the alignment
branch. That is, we are able to capture the pedestrian
characteristic from the original image and the aligned
image. In the Section 4.3, the experiment shows that
the two features are complementary to each other and
improve the person re-ID performance.
In this paper, we adopt a straightforward late fusion
strategy, i.e., fi = g(f 1
i ). Here f 1
i are the FC
descriptors from two types of images, respectively. We
reshape the tensor after the ﬁnal average pooling to a 1dim vector as the pedestrian descriptor of each branch.
The pedestrian descriptor is then written as:
i |T, (1 −α)|f 2
The | · | operator denotes an l2-normalization step. We
concatenate the aligned descriptor with the original descriptor, both after l2-normalization. α is the weight
for the two descriptors. If not speciﬁed, we simply use
α = 0.5 in our experiments.
3.5 Re-ranking for re-ID
In this work, we ﬁrst obtain the rank list N(q, n) =
[x1, x2, ...xn] by sorting the Euclidean distance of gallery
images to the query. Distance is calculated as Di,j =
(fi −fj)2, where fi and fj are l2-normalized features
of image i and j, respectively. We then perform reranking to obtain better retrieval results. Several reranking methods can be applied in person re-ID [Ye
et al., 2015,Qin et al., 2011,Zhong et al., 2017]. Specifically, we follow a state-of-the-art re-ranking method
proposed in [Zhong et al., 2017].
Apart from the Euclidean distance, we additionally
consider the Jaccard similarity. To introduce this distance, we ﬁrst deﬁne a robust retrieval set for each
image. The k-reciprocal nearest neighbors R(p, k) are
comprised of such element that appears in the top-k
retrieval rank of the query p while the query is in the
top-k rank of the element as well.
R(p, k) = {x|x ∈N(p, k), p ∈N(x, k)}
According to [Zhong et al., 2017], we extend the set
R to R∗to include more positive samples. Taking the
advantage of set R∗, we use the Jaccard similarity for
re-ranking. When we use the correctly matched images
to conduct the retrieval, we should retrieve a similar
Pedestrian Alignment Network for Large-scale Person Re-identiﬁcation
Fig. 4 Sample images from Market-1501 [Zheng et al., 2015],
CUHK03 [Li et al., 2014] and DukeMTMC-reID [Zheng et al.,
2017b]. The three datasets are collected in diﬀerent scenes
with diﬀerent detection bias.
rank list as we use the original query. The Jaccard distance can be simply formulated as:
Dsimilarity = 1 −|R∗(q, k) ∩R∗(xi, k)|
|R∗(q, k) ∪R∗(xi, k)|,
where | · | denotes the cardinality of the set. If R∗(q, k)
and R∗(xi, k) share more elements, xi is more likely to
be a true match. This usually helps us to distinguish
some hard negative samples from the correct matches.
During testing, this similarity distance is added to the
Euclidean distance to re-rank the result. In the experiment, we show that re-ranking further improves our
4 Experiments
In this section, we report the results on three large-scale
datasets: Market-1501 [Zheng et al., 2015], CUHK03 [Li
et al., 2014] and DukeMTMC-reID [Zheng et al., 2017b].
As for the detector, Market-1501 and CUHK03 (detected) datasets are automatically detected by DPM
and face the misdetection problem. It is unknown if
the manually annotated images after slight alignment
would bring any extra beneﬁt. So we also evaluate the
proposed method on the manually annotated images of
CUHK03 (labeled) and DukeMTMC-reID, which consist of hand-drawn bounding boxes. As shown in Fig. 4,
the three datasets have diﬀerent characteristics, i.e., scene
variances, and detection bias.
4.1 Datasets
Market1501 is a large-scale person re-ID dataset collected in a university campus. It contains 19,732 gallery
images, 3,368 query images and 12,936 training images
collected from six cameras. There are 751 identities in
the training set and 750 identities in the testing set
without overlapping. Every identity in the training set
has 17.2 photos on average. All images are automatically detected by the DPM detector [Felzenszwalb et al.,
2008]. The misalignment problem is common, and the
dataset is close to the realistic settings. We use all the
12,936 detected images to train the network and follow
the evaluation protocol in the original dataset. There
are two evaluation settings. A single query is to use one
image of one person as query, and multiple query means
to use several images of one person under one camera
as a query.
CUHK03 contains 14,097 images of 1,467 identities [Li et al., 2014]. Each identity contains 9.6 images
on average. We follow the new training/testing protocol
proposed in [Zhong et al., 2017] to evaluate the multishot re-ID performance. This setting uses a larger testing gallery and is diﬀerent from the papers published
earlier than [Zhong et al., 2017], such as [Liu et al.,
2016a] and [Varior et al., 2016b]. There are 767 identities in the training set and 700 identities in the testing set (The former setting uses 1,367 IDs for training
and the other 100 IDs for testing). Since we usually
face a large-scale searching image pool cropped from
surveillance videos, a larger testing pool is closer to the
realistic image retrieval setting. In the “detected” set,
all the bounding boxes are produced by DPM; in the
“labeled” set, the images are all hand-drawn. In this
paper, we evaluate our method on “detected” and “labeled” sets, respectively. If not speciﬁed, “CUHK03”
denotes the detected set, which is more challenging.
DukeMTMC-reID is a subset of the DukeMTMC
[Ristani et al., 2016] and contains 36,411 images of 1,812
identities shot by eight high-resolution cameras. It is
one of the largest pedestrian image datasets. The pedestrian images are cropped from hand-drawn bounding
boxes. The dataset consists 16,522 training images of
702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images. It is challenging because many pedestrians are in the similar clothes, and
may be occluded by cars or trees. We follow the evaluation protocol in [Zheng et al., 2017b].
Evaluation Metrics. We evaluate our method with
rank-1, 5, 20 accuracy and mean average precision (mAP).
Here, rank-i accuracy denotes the probability whether
one or more correctly matched images appear in topi. If no correctly matched images appear in the top-i
Zhedong Zheng et al.
Market-1501
CUHK03 (detected)
CUHK03 (labeled)
DukeMTMC-reID
2,048 80.17 91.69 96.59 59.14 30.50 51.07 71.64 29.04 31.14 52.00 74.21 29.80 65.22 79.13 87.75 44.99
Alignment 2,048 79.01 90.86 96.14 58.27 34.14 54.50 72.71 31.71 35.29 53.64 72.43 32.90 68.36 81.37 88.64 47.14
4,096 82.81 93.53 97.06 63.35 36.29 55.50 75.07 34.00 36.86 56.86 75.14 35.03 71.59 83.89 90.62 51.51
Table 1 Comparison of diﬀerent methods on Market-1501, CUHK03 (detected), CUHK03 (labeled) and DukeMTMC-reID.
Rank-1, 5, 20 accuracy (%) and mAP (%) are shown. Note that the base branch is the same as the classiﬁcation baseline [Zheng
et al., 2016b]. We observe consistent improvement of our method over the individual branches on the three datasets.
of the retrieval list, rank-i = 0, otherwise rank-i = 1.
We report the mean rank-i accuracy for query images.
On the other hand, for each query, we calculate the
area under the Precision-Recall curve, which is known
as the average precision (AP). The mean of the average
precision (mAP) then is calculated, which reﬂects the
precision and recall rate of the performance.
4.2 Implementation Details
ConvNet. In this work, we ﬁrst ﬁne-tune the base
branch on the person re-ID datasets. Then, the base
branch is ﬁxed while we ﬁne-tune the whole network.
Speciﬁcally, when ﬁne-tuning the base branch, the learning rate decrease from 10−3 to 10−4 after 30 epochs.
We stop training at the 40th epoch. Similarly, when we
train the whole model, the learning rate decrease from
10−3 to 10−4 after 30 epochs. We stop training at the
40th epoch. We use mini-batch stochastic gradient descent with a Nesterov momentum ﬁxed to 0.9 to update
the weights. Our implementation is based on the Matconvnet [Vedaldi and Lenc, 2015] package. The input
images are uniformly resized to the size of 224×224. In
addition, we perform simple data augmentation such as
cropping and horizontal ﬂipping following [Zheng et al.,
STN. For the aﬃne estimation branch, the network
may fall into a local minimum in early iterations. To
stabilize training, we ﬁnd that a small learning rate is
useful. We, therefore, use a learning rate of 1 × 10−5
for the ﬁnal convolutional layer in the aﬃne estimation
branch. In addition, we set the all θ = 0 except that
θ11, θ22 = 0.8 and thus, the alignment branch starts
training from looking at the center part of the Res2
Feature Maps.
4.3 Evaluation
Evaluation of the ResNet baseline. We implement
the baseline according to the routine proposed in [Zheng
et al., 2016b], with the details speciﬁed in Section 4.2.
Fig. 5 Sensitivity of person re-ID accuracy to parameter
α. Rank-1 accuracy(%) and mAP(%) on three datasets are
We report our baseline results in Table 1. The rank-
1 accuracy is 80.17%, 30.50%, 31.14% and 65.22% on
Market1501, CUHK03 (detected), CUHK03 (labeled)
and DukeMTMC-reID respectively. The baseline model
is on par with the network in [Zheng et al., 2016d,Zheng
et al., 2016b]. In our recent implementation, we use a
smaller batch size of 16 and a dropout rate of 0.75. We
have therefore obtained a higher baseline rank-1 accuracy 80.17% on Market-1501 than 73.69% in the [Zheng
et al., 2016d, Zheng et al., 2016b]. For a fair comparison, we will present the results of our methods built
on this new baseline. Note that this baseline result itself is higher than many previous works [Barbosa et al.,
2017, Varior et al., 2016a, Zheng et al., 2017a, Zheng
et al., 2016d].
Base branch. vs. alignment branch To investigate how alignment helps to learn discriminative pedestrian representations, we evaluate the Pedestrian descriptors of the base branch and the alignment branch,
respectively. Two conclusions can be inferred.
First, as shown in Table 1, the alignment branch
yields higher performance i.e., +3.64% / +4.15% on
the two dataset settings (CUHK03 detected/labeled)
and +3.14% on DukeMTMC-reID, and achieves a very
similar result with the base branch on Market-1501.
We speculate that Market-1501 contains more intensive
detection errors than the other three datasets and thus,
the eﬀect of alignment is limited.
Second, although the CUHK (labeled) dataset and
the DukeMTMC-reID dataset are manually annotated,
Pedestrian Alignment Network for Large-scale Person Re-identiﬁcation
Single Query
Multi. Query
DADM [Su et al., 2016]
BoW+kissme [Zheng et al., 2015]
MR-CNN [Ustinova et al., 2015]*
MST-CNN [Liu et al., 2016b]
FisherNet [Wu et al., 2016a]*
CAN [Liu et al., 2016a]*
SL [Chen et al., 2016]
S-LSTM [Varior et al., 2016b]
DNS [Zhang et al., 2016]
Gate Reid [Varior et al., 2016a]
SOMAnet [Barbosa et al., 2017]*
PIE [Zheng et al., 2017a]*
Verif.-Identif. [Zheng et al., 2016d]*
SVDNet [Sun et al., 2017]*
DeepTransfer [Geng et al., 2016]*
GAN [Zheng et al., 2017b]*
APR [Lin et al., 2017]*
Triplet [Hermans et al., 2017]*
Triplet+re-rank [Hermans et al., 2017]*
Ours+re-rank
Ours (GAN)
Ours (GAN)+re-rank
Table 2 Rank-1 precision (%) and mAP (%) on Market-1501. We also provide results of the ﬁne-tuned ResNet50 baseline
which has the same accuracy with the base branch. * the respective paper is on ArXiv but not published.
the alignment branch still improves the performance of
the base branch. This observation demonstrates that
the manual annotations may not be good enough for
the machine to learn a good descriptor. In this scenario,
alignment is non-trivial and makes the pedestrian representation more discriminative.
The complementary of the two branches. As
mentioned, the two descriptors capture the diﬀerent
pedestrian characteristic from the original image and
the aligned image. We follow the setting in Section
3.4 and simply combine the two features to form a
stronger pedestrian descriptor. The results are summarized in Table 1. We observe a constant improvement on the three datasets when we concatenate the
two branch descriptors. The fused descriptor improves
+2.64%, +2.15%, +1.63% and 3.23% on Market-1501,
CUHK03(detected), CUHK03(labeled) and DukeMTMCreID, respectively. The two branches are complementary and thus, contain more meaningful information
than a separate branch. Aside from the improvement
of the accuracy, this simple fusion is eﬃcient sine it
does not introduce additional computation.
Parameter sensitivity. We evaluate the sensitivity of the person re-ID accuracy to the parameter α.
As shown in Fig. 5, we report the rank-1 accuracy and
mAP when tuning the α from 0 to 1. We observe that
the change of rank-1 accuracy and mAP are relatively
BoW+kissme [Zheng et al., 2015]
LOMO+XQDA [Liao et al., 2015]
Gan [Zheng et al., 2017b]
OIM [Xiao et al., 2017]
APR [Lin et al., 2017]
SVDNet [Sun et al., 2017]
Basel. [Zheng et al., 2017b]
Ours + re-rank
DukeMTMC-reID. We follow the evaluation protocol in
[Zheng et al., 2017b]. We also provide the result of the ﬁnetuned ResNet50 baseline for fair comparison.
small corresponding to the α. Our reported result simply use α = 0.5. α = 0.5 may not be the best choice
for a particular dataset. But if we do not foreknow the
distribution of the dataset, it is a simple and straightforward choice.
Comparison with the state-of-the-art methods. We compare our method with the state-of-the-art
methods on Market-1501, CUHK03 and DukeMTMCreID in Table 2, Table 4 and Table 3, respectively. On
Market-1501, we achieve rank-1 accuracy = 85.78%,
mAP = 76.56% after re-ranking, which is the best result compared to the published paper, and the second
Zhedong Zheng et al.
BoW+XQDA [Zheng et al., 2015]
LOMO+XQDA [Liao et al., 2015]
ResNet50+XQDA [Zhong et al., 2017]
ResNet50+XQDA+re-rank [Zhong et al., 2017]
Ours+re-rank
Table 4 Rank-1 accuracy (%) and mAP (%) on CUHK03 using the new evaluation protocol in [Zhong et al., 2017]. This
setting uses a larger testing gallery and is diﬀerent from the papers published earlier than [Zhong et al., 2017], such as [Liu et al.,
2016a] and [Varior et al., 2016b]. There are 767 identities in the training set and 700 identities in the testing set (The former
setting uses 1,367 IDs for training and the other 100 IDs for testing). Since we usually face a large-scale searching image pool
cropped from surveillance videos, a larger testing pool is more challenging and closer to the realistic image retrieval setting.
So we evaluate the proposed method on the “detected” and “labeled” subsets according to this new multi-shot protocol. We
also provide the result of our ﬁne-tuned ResNet50 baseline for fair comparison.
Fig. 6 Sample retrieval results on
the three datasets. The images in
the ﬁrst column are queries. The
retrieved images are sorted according to the similarity score from left
to right. For each query, the ﬁrst
row shows the result of baseline
[Zheng et al., 2016b], and the second row denotes the results of PAN.
The correct and false matches are
in the blue and red rectangles, respectively. Images in the rank lists
obtained by PAN demonstrate amelioration in alignment. Best viewed
when zoomed in.
best among all the available results including the arXiv
paper. Our model is also adaptive to previous models. One of the previous best results is based on the
model regularized by GAN [Zheng et al., 2017b]. We
combine the model trained on GAN generated images
and thus, achieve the state-of-the-art result rank-1 accuracy = 88.57%, mAP = 81.53% on Market-1501.
On CUHK03, we arrive at a competitive result rank-1
accuracy = 36.3%, mAP=34.0% on the detected
dataset and rank-1 accuracy = 36.9%, mAP =
35.0% on the labeled dataset. After re-ranking, we further achieve a state-of-the art result rank-1 accuracy
= 41.9%, mAP=43.8% on the detected dataset and
rank-1 accuracy = 43.9%, mAP = 45.8% on the
labeled dataset. On DukeMTMC-reID, we also observe
a state-of-the-art result rank-1 accuracy = 75.94%
and mAP = 66.74% after re-ranking. Despite the
visual disparities among the three datasets, i.e., scene
variance, and detection bias, we show that our method
consistently improves the re-ID performance.
As shown in Fig. 6, we visualize some retrieval results on the three datasets. Images in the rank lists
obtained by PAN demonstrate amelioration in alignment. Comparing to the baseline, true matches which
are misaligned originally receive higher ranks, while
false matches have lower ranks
Visualization of the alignment. We further visualize the aligned images in Fig. 7. As aforementioned,
the proposed network does not process the alignment on
the original image. To visualize the aligned images, we
extract the predicted aﬃne parameters and then apply
the aﬃne transformation on the originally detected images manually. We observe that the network does
not perform perfect alignment as the human,
Pedestrian Alignment Network for Large-scale Person Re-identiﬁcation
Fig. 7 Examples of pedestrian images before and after alignment on three datasets (Market-1501, DukeMTMC-reID and
CUHK03). Pairs of input images and aligned images are shown. By removing excessive background or padding zeros to image
borders, we observe that PAN reduces the scale and location variance.
but it more or less reduces the scale and position variance, which is critical for the network to
learn the representations. So the proposed network
improves the performance of the person re-ID.
5 Conclusion
Pedestrian alignment and re-identiﬁcation are two innerconnected problems, which inspires us to develop an
attention-based system. In this work, we propose the
pedestrian alignment network (PAN), which simultaneously aligns the pedestrians within bounding boxes and
learns the pedestrian descriptors. Taking advantage of
the attention of CNN feature maps to the human body,
PAN addresses the misalignment problem and person
re-ID together and thus, improves the person re-ID accuracy. Except for the identity label, we do not need
any extra annotation. We also observe that the manually cropped images are not as perfect as preassumed to
be. Our network also improves the re-ID performance
on the datasets with hand-drawn bounding boxes. Experiments on three diﬀerent datasets indicate that our
method is competitive with the state-of-the-art methods. In the future, we will continue to investigate the
attention-based model and apply our model to other
ﬁelds i.e., car recognition.