Real-Time Grasp Detection Using Convolutional Neural Networks
Joseph Redmon1, Anelia Angelova2
Abstract— We present an accurate, real-time approach to
robotic grasp detection based on convolutional neural networks.
Our network performs single-stage regression to graspable
bounding boxes without using standard sliding window or
region proposal techniques. The model outperforms state-ofthe-art approaches by 14 percentage points and runs at 13
frames per second on a GPU. Our network can simultaneously
perform classiﬁcation so that in a single step it recognizes the
object and ﬁnds a good grasp rectangle. A modiﬁcation to this
model predicts multiple grasps per object by using a locally
constrained prediction mechanism. The locally constrained
model performs signiﬁcantly better, especially on objects that
can be grasped in a variety of ways.
I. INTRODUCTION
Perception—using the senses (or sensors if you are a
robot) to understand your environment—is hard. Visual perception involves mapping pixel values and light information
onto a model of the universe to infer your surroundings. General scene understanding requires complex visual tasks such
as segmenting a scene into component parts, recognizing
what those parts are, and disambiguating between visually
similar objects. Due to these complexities, visual perception
is a large bottleneck in real robotic systems.
General purpose robots need the ability to interact with
and manipulate objects in the physical world. Humans see
novel objects and know immediately, almost instinctively,
how they would grab them to pick them up. Robotic grasp
detection lags far behind human performance. We focus on
the problem of ﬁnding a good grasp given an RGB-D view
of the object.
We evaluate on the Cornell Grasp Detection Dataset, an
extensive dataset with numerous objects and ground-truth
labelled grasps (see Figure 1). Recent work on this dataset
runs at 13.5 seconds per frame with an accuracy of 75 percent
 . This translates to a 13.5 second delay between a robot
viewing a scene and ﬁnding where to move its grasper.
The most common approach to grasp detection is a sliding
window detection framework. The sliding window approach
uses a classiﬁer to determine whether small patches of an
image constitute good grasps for an object in that image. This
type of system requires applying the classiﬁer to numerous
places on the image. Patches that score highly are considered
good potential grasps.
We take a different approach; we apply a single network
once to an image and predict grasp coordinates directly. Our
network is comparatively large but because we only apply
it once to an image we get a massive performance boost.
1University of Washington
2Google Research
The Cornell Grasping Dataset contains a variety of objects, each
with multiple labelled grasps. Grasps are given as oriented rectangles in
Instead of looking only at local patches our network uses
global information in the image to inform its grasp predictions, making it signiﬁcantly more accurate. Our network
achieves 88 percent accuracy and runs at real-time speeds
(13 frames per second). This redeﬁnes the state-of-the-art
for RGB-D grasp detection.
II. RELATED WORK
Signiﬁcant past work uses 3-D simulations to ﬁnd good
grasps . These approaches are powerful but
rely on a full 3-D model and other physical information about
an object to ﬁnd an appropriate grasp. Full object models are
often not known a priori. General purpose robots may need
to grasp novel objects without ﬁrst building complex 3-D
models of the object.
Robotic systems increasingly leverage RGB-D sensors and
data for tasks like object recognition , detection ,
and mapping . RGB-D sensors like the Kinect are
cheap, and the extra depth information is invaluable for
robots that interact with a 3-D environment.
Recent work on grasp detection focusses on the problem
 
of ﬁnding grasps solely from RGB-D data . These
techniques rely on machine learning to ﬁnd the features of
a good grasp from data. Visual models of grasps generalize
well to novel objects and only require a single view of the
object, not a full physical model .
Convolutional networks are a powerful model for learning
feature extractors and visual models . Lenz et al.
successfully use convolutional networks for grasp detection
as a classiﬁer in a sliding window detection pipeline . We
address the same problem as Lenz et al. but use a different
network architecture and processing pipeline that is capable
of higher accuracy at much faster speeds.
III. PROBLEM DESCRIPTION
Given an image of an object we want to ﬁnd a way
to safely pick up and hold that object. We use the ﬁvedimensional representation for robotic grasps proposed by
Lenz et al. . This representation gives the location and
orientation of a parallel plate gripper before it closes on an
object. Ground truth grasps are rectangles with a position,
size, and orientation:
g = {x, y, θ, h, w}
where (x, y) is the center of the rectangle, θ is the orientation
of the rectangle relative to the horizontal axis, h is the height,
and w is the width. Figure 2 shows an example of this grasp
representation.
A ﬁve-dimensional grasp representation, with terms for location,
size, and orientation. The blue lines demark the size and orientation of the
gripper plates. The red lines show the approximate distance between the
plates before the grasp is executed.
This is a simpliﬁcation of Jiang et al.’s seven-dimensional
representation . Instead of ﬁnding the full 3-D grasp
location and orientation, we implicitly assume that a good
2-D grasp can be projected back to 3-D and executed by a
robot viewing the scene. Lenz et al. describe a process to
do this and while they don’t evaluate it directly it appears to
work well in their experiments .
Using a ﬁve-dimensional representation makes the problem of grasp detection analogous to object detection in
computer vision with the only difference being an added
term for gripper orientation.
IV. GRASP DETECTION WITH NEURAL
Convolutional neural networks (CNNs) currently outperform other techniques by a large margin in computer vision
problems such as classiﬁcation and detection .
CNNs already perform well on grasp detection when applied
as a classiﬁer in a sliding-window approach .
We want to avoid the computational costs of running a
small classiﬁer numerous times on small patches of an image.
We harness the extensive capacity of a large convolutional
network to make global grasp predictions on the full image
of an object.
A. Architecture
When building our grasp detection system we want to start
from a strong foundation. We derive our model from a version of the widely adopted convolutional network proposed
by Krizhevsky et al. for object recognition tasks (AlexNet)
Our network has ﬁve convolutional layers followed by
three fully connected layers. The convolutional layers are
interspersed with normalization and maxpooling layers at
various stages. A full description of the architecture can be
found in Figure 3.
B. Direct Regression To Grasps
The simplest model we explore is a direct regression from
the raw RGB-D image to grasp coordinates. The raw image is
given to the model which uses convolutional layers to extract
features from the image. The fully connected layers terminate
in an output layer with six output neurons corresponding to
the coordinates of a grasp. Four of the neurons correspond
to location and height. Grasp angles are two-fold rotationally
symmetric so we parameterize by using the two additional
coordinates: the sine and cosine of twice the angle.
This model assumes the strong prior that every image
contains a single graspable object and it only needs to predict
a one grasp for that object. This strong assumption may
not hold outside of experimental conditions. In practice this
model would have to come in a pipeline that ﬁrst segments
the image into pieces that only contain individual objects.
The beneﬁt of enforcing this assumption is that instead of
classifying many of small patches in a sliding window type
approach, we only need to look at a single image and make
a global prediction.
During training our model picks a random ground truth
grasp every time it sees an object to treat as the single ground
truth grasp. Because the grasp changes often, the model does
not overﬁt to a single grasp on an object. We minimize the
squared error of the predicted grasp. The end effect is that
our model ﬁts to the average of the possible grasps for an
C. Regression + Classiﬁcation
In order to use a grasped object the robot must ﬁrst
recognize the object. By extending our model we show that
The full architecture of our direct regression grasp model.
recognition and grasp detection can be combined into a
single, efﬁcient pipeline.
We modify our architecture from the previous section by
adding extra neurons to the output layer that correspond
to object categories. We keep the rest of the architecture
the same thus our model uses common features from the
convolutional layers for both recognition and detection.
This combined model processes an image in a single pass
and predicts both the category of the object in the image and
a good grasp for that object. It runs just as fast as the direct
regression model because the architecture remains largely
unchanged.
D. MultiGrasp Detection
Our third model is a generalization of the ﬁrst model, we
call it MultiGrasp. The preceeding models assume that there
is only a single correct grasp per image and try to predict
that grasp. MultiGrasp divides the image into an NxN grid
and assumes that there is at most one grasp per grid cell. It
predicts one grasp per cell and also the likelihood that the
predicted grasp would be feasible on the object. For a cell
to predict a grasp the center of that grasp must fall within
The output of this model is an NxNx7 prediction. The ﬁrst
channel is a heatmap of how likely a region is to contain a
correct grasp. The other six channels contain the predicted
grasp coordinates for that region. For experiments on the
Cornell dataset we used a 7x7 grid, making the actual output
layer 7x7x7 or 343 neurons. Our ﬁrst model can be seen as a
speciﬁc case of this model with a grid size of 1x1 where the
probability of the grasp existing in the single cell is implicitly
Training MultiGrasp requires some special considerations.
Every time MultiGrasp sees an image it randomly picks up
to ﬁve grasps to treat as ground truth. It constructs a heatmap
with up to ﬁve cells marked with ones and the rest ﬁlled with
zeros. It also calculates which cells those grasps fall into and
ﬁlls in the appropriate columns of the ground truth with the
grasp coordinates. During training we do not backpropagate
error for the entire 7x7x7 grid because many of the column
entries are blank (if there is no grasp in that cell). Instead
we backpropagate error for the entire heatmap channel and
also for the speciﬁc cells that contain ground truth grasps.
This model has several precursors in object detection
literature but is novel in important aspects. Szegedy et al.
use deep neural networks to predict binary object masks on
images and use the predicted masks to generate bounding
boxes . The heatmap that we predict is similar to this
object mask but we also predict full bounding boxes and
only use the heatmap for weighting our predictions. Our
system does not rely on post-processing or heuristics to
extract bounding boxes but rather predicts them directly.
Erhan et al. predict multiple bounding boxes and con-
ﬁdence scores associated with those bounding boxes .
This approach is most similar to our own, we also predict
multiple bounding boxes and weight them by a conﬁdence
score. The key difference is the we enforce structure on our
predictions so that each cell can only make local predictions
for its region of the image.
V. EXPERIMENTS AND EVALUATION
The Cornell Grasping Dataset contains 885 images of
240 distinct objects and labelled ground truth grasps. Each
image has multiple labelled grasps corresponding to different
possible ways to grab the object. The dataset is speciﬁcally
designed for parallel plate grippers. The labels are comprehensive and varied in terms of orientation, location, and scale
but they are by no means exhaustive of every possible grasp.
Instead they are meant to be diverse examples of particularly
good grasps.
A visualization of the MultiGrasp model running on a test image of a ﬂying disc. The MultiGrasp model splits the image into an NxN grid.
For each cell in the grid, the model predicts a bounding box centered at that cell and a probability that this grasp is a true grasp for the object in the
image. The predicted bounding boxes are weighted by this probability. The model can predict multiple good grasps for an object, as in this instance. For
experiments on the Cornell dataset we pick the bounding box with the highest weight as the ﬁnal prediction.
A. Grasp Detection
Previous work uses two different metrics when evaluating
grasps on the Cornell dataset. The point metric looks at the
distance from the center of the predicted grasp to the center
of each of the ground truth grasps. If any of these distances
is less than some threshold, the grasp is considered a success.
There are a number of issues with this metric, most notably
that it does not consider grasp angle or size. Furthermore,
past work does not disclose what values they use for the
threshold which makes it impossible to compare new results
to old ones. For these reasons we do not evaluate on the
point metric.
The second metric considers full grasp rectangles during
evaluation. The rectangle metric considers a grasp to be
correct if both:
1) The grasp angle is within 30◦of the ground truth grasp.
2) The Jaccard index of the predicted grasp and the
ground truth is greater than 25 percent.
Where the Jaccard index is given by:
J(A, B) = |A ∩B|
The rectangle metric discriminates between good and bad
grasps better than the point metric. It is similar to the metrics
used in object detection although the threshold on the Jaccard
index is lower (25 percent instead of a more standard 50
percent in computer vision) because the ground truth grasps
are not exhaustive. A rectangle with the correct orientation
that only overlaps by 25 percent with one of the ground
truth grasps is still often a good grasp. We perform all of
our experiments using the rectangle metric.
Like prior work we use ﬁve-fold cross validation for our
experimental results. We do two different splits of the data:
1) Image-wise splitting splits images randomly.
2) Object-wise splitting splits object instances randomly,
putting all images of the same object into the same
cross-validation split.
Image-wise splitting tests how well the model can generalize to new positions for objects it has seen previously. Objectwise splitting goes further, testing how well the network
can generalize to novel objects. In practice, both splitting
techniques give comparable performance. This may be due
to the similarity between different objects in the dataset (e.g.
there are multiple sunglasses of slightly different shapes and
B. Object Classiﬁcation
We manually classify the images in the Cornell Grasping
Dataset into 16 distinct categories, with categories like
“bottle”, “shoe”, and “sporting equipment”. The dataset is
not evenly distributed between categories but every category
has enough examples in the dataset to be meaningful. The
least represented category has 20 images in the dataset while
the most represented has 156.
We train and test our combined regression + classiﬁcation
model using these class labels. At test time the combined
model simultaneously predicts the best grasp and the object
category. We report classiﬁcation accuracy on the same
cross-validation splits as above.
C. Pretraining
Before training our network on grasps we pretrain on the
ImageNet classiﬁcation task . Our experience backed by
current literature suggests that pretraining large convolutional
neural networks greatly improves training time and helps
avoid overﬁtting .
Krizevsky et al. designed AlexNet for standard RGB
images. Low-cost stereo vision systems like the Kinect make
RGB-D data increasingly ubiquitous in robotic systems. To
use AlexNet with RGB-D data we simply replace the blue
channel in the image with the depth information. We could
instead modify the architecture to have another input channel
but then we would not be able to pretrain the full network.
RECTANGLE METRIC DETECTION ACCURACY ON THE CORNELL DATASET
Detection accuracy
Time / image
Image-wise split
Object-wise split
Chance 
Jiang et al. 
Lenz et al. 
Direct Regression
Regression + Classiﬁcation
MultiGrasp Detection
Pretraining is crucial when there is limited domain-speciﬁc
data (like labeled RGB-D grasps). Through pretraining the
network ﬁnds useful, generalizable ﬁlters that often translate
well to the speciﬁc application . Even in this case
where the data format actually changes we still ﬁnd that the
pretrained ﬁlters perform well. This may be because good
visual ﬁlters (like oriented edges) are also good ﬁlters in
depth space.
D. Training
We undertake a similar training regimen for each of the
models we tested. For each fold of cross-validation, we train
each model for 25 epochs. We use a learning rate of 0.0005
across all layers and a weight decay of 0.001. In the hidden
layers between fully connected layers we use dropout with
a probability of 0.5 as an added form of regularization.
cuda-convnet2 package running on an nVidia Tesla K20
GPU. GPUs offer great beneﬁts in terms of computational
power and our timing results depend on using a GPU as
part of our pipeline. While GPUs are far from a mainstay
in robotic platforms, they are becoming increasingly popular
due to their utility in vision tasks.
E. Data Preprocessing
We perform a minimal amount of preprocessing on the
data before feeding it to the network. As discussed previously, the depth information is substituted into the blue
channel of the image. The depth information is normalized to
fall between 0 and 255. Some pixels lack depth information
because they are occluded in the stereo image; we substitute
0 for these pixel values. We then approximately mean-center
the image by globally subtracting 144.
When preparing data for training we perform extensive
data augmentation by randomly translating and rotating the
image. We take a center crop of 320x320 pixels, randomly
translate it by up to 50 pixels in both the x and y direction,
and rotate it by a random amount. This image is then resized
to 224x224 to ﬁt the input layer of our architecture. We
generate 3000 training examples per original image. For test
images we simply take the center 320x320 crop and resize
it without translation or rotation.
VI. RESULTS
Across the board our models outperform the current stateof-the-art both in terms of accuracy and speed. In Table I
Examples of correct (top) and incorrect (bottom) grasps from the
direct regression model. Some incorrect grasps (e.g. the can opener) may
actually be viable while others (e.g. the bowl) are clearly not.
we compare our results to previous work using their selfreported scores for the rectangle metric accuracy.
The direct regression model sets a new baseline for
performance in grasp detection. It achieves around 85 percent
accuracy in both image-wise and object-wise splits, ten
percentage points higher than the previous best. At test
time the direct regression model runs in 76 milliseconds per
batch, with a batch size of 128 images. While this amounts
to processing more than 1,600 images per second, latency
matters more than throughput in grasp detection so we report
the per batch number as 13 fps. The main source of this
speedup is the transition from a scanning window classiﬁer
based approach to our single-pass model and our usage of
GPU hardware to accelerate computation. 76 milliseconds
per frame is certainly achievable on a CPU because it would
require only 1/128th of the ﬂoating point operations required
for processing a full batch on a GPU.
The direct regression model is trained using a different
random ground truth grasp every time it sees an image.
Due to this it learns to predict the average ground truth
grasp for a given object. Predicting average grasps works
well with certain types of objects, such as long, thin objects
like markers or rolling pins. This model fails mainly in
cases where average grasps do not translate to viable grasps
on the object, for instance with circular objects like ﬂying
discs. Figure 5 shows some examples of correct and incorrect
grasps that the direct regression model predicts.
The combined regression + classiﬁcation model shows that
we can extend our base detection model to simultaneously
perform classiﬁcation without sacriﬁcing detection accuracy;
see Table II for classiﬁcation results. Our model can correctly
predict the category of an object it has previously seen 9 out
of 10 times. When shown novel objects our model predicts
the correct category more than 60 percent of the time. By
comparison, predicting the most common class would give
an accuracy of 17.7 percent.
CLASSIFICATION ACCURACY ON THE CORNELL DATASET
Image-wise split
Object-wise split
Most Common Class
Regression + Classiﬁcation
Even with the added classiﬁcation task the combined
model maintains high detection accuracy. It has identical
performance on the object-wise split and actually performs
slightly better on the image-wise split. This model establishes
a strong baseline for combined grasp detection and object
classiﬁcation on the Cornell dataset.
The MultiGrasp model outperforms our baseline direct
regression model by a signiﬁcant margin. For most objects
MultiGrasp gives very similar results to the direct regression
model. However, MultiGrasp does not have the same problem with bad average grasps that the direct regression model
has which accounts for most of the error reduction. Figure
6 shows examples of MultiGrasp outperforming the direct
regression model and examples where both models fail.
MultiGrasp has a very similar architecture to the direct
regression model and operates at the same real-time speeds.
With a grasp detection accuracy of 88 percent and a processing rate of 13 frames per second, MultiGrasp redeﬁnes the
state-of-the-art in robotic grasp detection.
VII. DISCUSSION
We show that robot perception can be both fast and
highly accurate. GPUs provide a large speed boost for visual
systems, especially systems based on convolutional neural
networks. CNNs continue to dominate other techniques in
visual tasks, making GPUs an important component in any
high performance robotic system. However, GPUs are most
vital during model training and are optimized for throughput,
not latency. At test time a CPU could run our model in far
less than a second per image, making it viable in real-time
robotics applications.
The comparative performance of the direct regression model and
MultiGrasp. The top two rows show examples where direct regression model
fails due to averaging effects but MultiGrasp predicts a viable grasp. The
bottom two rows show examples where both models fail to predict good
grasps. The ground truth grasps are shown in blue and red on the direct
regression model images.
Model consideration is important for achieving high performance. We take advantage of a strong constraint on the
data so that our model only needs a single pass over an image
to make an accurate grasp prediction.
Our direct regression model uses global information about
the image to make its prediction, unlike sliding-window
approaches. Sliding window classiﬁers only see small, local
patches thus they can not effectively decide between good
grasps and are more easily fooled by false positives. Lenz et
al. report very high recognition accuracy for their classiﬁer
(94%) yet it still falls victim to this false positive paradox
and its detection accuracy is much lower as a result. In this
respect, global models have a large advantage over models
that only see local information.
Global models also have their downside. Notably our
direct regression model often tries to split the difference
between a few good grasps and ends up with a bad grasp.
A sliding window approach would never make the mistake
of predicting a grasp in the center of a circular object like a
ﬂying disc.
Our MultiGrasp model combines the strongest aspects of
global and local models. It sees the entire image and can
effectively ﬁnd the best grasp and ignore false positives.
However, because each cell can only make a local prediction,
it avoids the trap of predicting a bad grasp that falls between
several good ones.
The local prediction model also has the ability to predict
multiple grasps per image. We are unable to quantitatively
evaluate the model in this respect because no current dataset
has an appropriate evaluation for multiple grasps in an image.
In the future we hope to evaluate this model in a full
detection task, either for multiple grasps in an image or on
a more standard object detection dataset.
One further consideration is the importance of pretraining
when building large convolutional neural networks. Without
pretraining on ImageNet, our models quickly overﬁt to the
training data without learning meaningful representations of
good grasps. Interestingly, pretraining worked even across
domains and across feature types. We use features tuned for
the blue channel of an image on depth information instead
and still get good results. Importantly, we get much better
results using these features on the depth channel than using
them on the original RGB images.
VIII. CONCLUSION
We present a fast, accurate system for predicting robotic
grasps of objects in RGB-D images. Our models improve
the state-of-the-art and run more than 150 times faster than
previous methods. We show that grasp detection and object
classiﬁcation can be combined without sacriﬁcing accuracy
or performance. Our MultiGrasp model gets the best known
performance on the Cornell Grasping Dataset by combining
global information with a local prediction procedure.
IX. ACKNOWLEDGEMENTS
We would like to thank Alex Krizevsky for helping us
with model construction and pretraining, and for helping us
customize his cuda-convnet2 code. We would also like
to thank Vincent Vanhoucke for his insights on model design
and for his feedback throughout the experimental process.