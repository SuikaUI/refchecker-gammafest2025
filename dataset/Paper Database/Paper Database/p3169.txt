Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics , pages 96–120
August 5–6, 2021. ©2021 Association for Computational Linguistics
GEM Benchmark:
Natural Language Generation, its Evaluation and Metrics
Sebastian Gehrmann,9,* Tosin Adewumi,20,21 Karmanya Aggarwal,14
Pawan Sasanka Ammanamanchi,15 Aremu Anuoluwapo,21,38 Antoine Bosselut,28
Khyathi Raghavi Chandu,2 Miruna Clinciu,7,11,35 Dipanjan Das,9 Kaustubh D. Dhole,1
Wanyu Du,42 Esin Durmus,5 Ondˇrej Dušek,3 Chris Emezue,21,30 Varun Gangal,2
Cristina Garbacea,39 Tatsunori Hashimoto,28 Yufang Hou,13 Yacine Jernite,12 Harsh Jhamtani,2
Yangfeng Ji,42 Shailza Jolly,6,29 Mihir Kale,9 Dhruv Kumar,44 Faisal Ladhak,4 Aman Madaan,2
Mounica Maddela,8 Khyati Mahajan,34 Saad Mahamood,32 Bodhisattwa Prasad Majumder,37
Pedro Henrique Martins,16 Angelina McMillan-Major,43 Simon Mille,26 Emiel van Miltenburg,31
Moin Nadeem,22 Shashi Narayan,9 Vitaly Nikolaev,9 Rubungo Andre Niyongabo,21,36
Salomey Osei,19,21 Ankur Parikh,9 Laura Perez-Beltrachini,35 Niranjan Ramesh Rao,24
Vikas Raunak,23 Juan Diego Rodriguez,41 Sashank Santhanam,34 João Sedoc,25
Thibault Sellam,9 Samira Shaikh,34 Anastasia Shimorina,33 Marco
Antonio Sobrevilla Cabezudo,40 Hendrik Strobelt,13 Nishant Subramani,17,21 Wei Xu,8
Diyi Yang,8 Akhila Yerukola,27 Jiawei Zhou10
1Amelia R&D, New York, 2Carnegie Mellon University, 3Charles University, Prague, 4Columbia University, 5Cornell
University, 6DFKI, Germany 7Edinburgh Centre for Robotics, 8Georgia Tech, 9Google Research, 10Harvard University,
11Heriot-Watt University, 12Hugging Face, 13IBM Research, 14IIIT Delhi, 15IIIT Hyderabad, 16Instituto de Telecomunicações,
17Intelligent Systems Lab, Intel, 18Johns-Hopkins University, 19Kwame Nkrumah University of Science and Technology
20Luleå University of Technology, 21Masakhane, Africa, 22Massachusetts Institute of Technology, 23Microsoft, 24National
Institute of Technology Karnataka India, 25New York University, 26Pompeu Fabra University, 27Samsung Research, 28Stanford
University, 29Technical University of Kaiserslautern, 30Technical University Munich, 31Tilburg University, 32trivago,
33Université de Lorraine, 34University of North Carolina Charlotte, 35University of Edinburgh, 36University of Electronic
Science and Technology of China, 37University of California San Diego, 38University of Lagos, 39University of Michigan Ann
Arbor, 40University of São Paulo, 41University of Texas at Austin, 42University of Virginia, 43University of Washington,
44University of Waterloo
We introduce GEM, a living benchmark for
natural language Generation (NLG), its Evaluation, and Metrics.
Measuring progress in
NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate
on divergent anglo-centric corpora with wellestablished, but ﬂawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities
for progress. Addressing this limitation, GEM
provides an environment in which models can
easily be applied to a wide set of tasks and in
which evaluation strategies can be tested. Regular updates to the benchmark will help NLG
research become more multilingual and evolve
the challenge alongside models.
This paper
serves as the description of the data for which
we are organizing a shared task at our ACL
2021 Workshop and to which we invite the entire NLG community to participate.
* Correspondence to 
Introduction
Natural language generation is the task to automatically generate understandable texts, typically using
a non-linguistic or textual representation of information as input . These
texts aim to fulﬁll an underlying communicative
goal (e.g., to produce a summary of an article)
while remaining faithful to the input information,
ﬂuent, grammatical, and natural-looking. An NLG
system needs to be robust to shifts in the data distribution and be able to produce text in many different
languages. Finally, it is often desired that repeated
interactions with the model produce diverse outputs, for example, to explain concepts in multiple
ways or to become a more interesting conversational agent. These optimization objectives can
often be conﬂicting and,
as a result, evaluations that focus only on a single
aspect may fail to recognize the drawbacks of a
particular method. To demonstrate this trade-off,
consider an improvement on the CNN-DM summarization dataset measured by the ROUGE-L met-
ric . Since ROUGE only tests the extent
to which a generated summary has a lexical overlap with a reference summary, it can erroneously
produce high scores for ﬂuent, yet meaningless
and unfaithful outputs as long as many of the same
words are used . Moreover, ROUGE tends to favor systems
that produce longer summaries .
It is thus crucial to carefully assess the progress
of NLG toward all of its goals at the same time
in ways that evolve alongside the models. This is
currently not the case; new models are evaluated
on different datasets, most of which focus only
on the English language , and using these ﬂawed metrics. Moreover, while human
evaluations of generated texts can provide complementary insights to automatic evaluation , it can also lead to contradicting results
since studies often omit crucial replication details
and assume different deﬁnitions of the measured
quantities .
We propose a living benchmark called GEM
(Generation, Evaluation, and Metrics) that aims
to enable research on a wide range of NLG challenges. To avoid the fallacy of encouraging hill
climbing on a leaderboard , GEM
focuses on an in-depth evaluation of model outputs across human and automatic evaluation that
aims to uncover shortcomings and opportunities
for progress. As datasets, metrics, and models improve, the benchmark environment will improve as
well, replacing “solved” tasks with more challenging ones, incorporating newly developed metrics,
and addressing discovered ﬂaws in the experimental setup, as demonstrated in Figure 1. Making all
model outputs available under an open-source license will support evaluation research and integrating new metrics will, in turn, help their adoption
and increase the robustness of model evaluations.
The initial set of eleven included datasets is presented in Table 1. They measure speciﬁc generation
challenges, such as the content selection and planning (What to say?), and the surface realization
(How to say it?) . Models need to be capable of
paraphrasing, simpliﬁcation, and others. In addition to those challenges, GEM datasets also differ
in their communicative goals, languages, the noisiness of data, and resource availability, to evaluate
the consistency of evaluation schemes. About half
of the datasets have multiple references and more
Consistent
Evaluation on
“solved” data
Evaluation with
gameable metrics
experimental
Non-repeatable
human evaluation
Figure 1: The opportunities of living benchmarks and
pitfalls of evaluation. As models improve, we need consistent evaluations such that models can be compared
to each other. This can only happen if we develop robust human evaluation standards and improve our automated metrics. Otherwise, results are challenging to
interpret and compare to each other. Finally, as models
improve and metrics saturate, we need to evaluate them
on more challenging datasets instead of continuing to
move sideways on old ones. GEM aims to provide this
environment for natural language generation.
than half were post-processed to improve data quality. The sizes range from 5k to 500k data points.
GEM features 18 languages across all tasks and
two of the datasets do not include English at all.
To be able to properly assess the performance of
models in a way robust to the shortcuts a model
can take, we additionally introduce ten types of
challenging test sets that probe for speciﬁc modeling aspects . To ensure that research with
GEM is conducted responsibly, all the datasets are
documented in an NLG-speciﬁc version of data
cards we developed and for which we release a
template and guide. Moreover, all submitted models will have an associated data card 
are common for natural language understanding
Communicative Goal
Language(s)
Input Type
 
Produce a likely sentence which mentions
all of the source concepts.
Concept Set
Czech Restaurant
 
Produce a text expressing the given intent
and covering the speciﬁed attributes.
Representation
 
Describe cells in a table, covering all information provided in triples.
Triple Set
 
 
Describe a restaurant, given all and only
the attributes speciﬁed on the input.
Representation
 
Summarize relevant points within a news
Schema-Guided Dialog
 
Provide the surface realization for a virtual assistant
Dialog Act
 
Produce an English sentence that describes the highlighted cells in the context
of the given table.
Highlighted
 
Highlight relevant points in a news article
 
Produce a text that verbalises the input
triples in a grammatical and natural way.
RDF triple
WikiAuto + Turk/ASSET
 
 
 
Communicate the same information as
the source sentence using simpler words
and grammar.
WikiLingua
 
Produce high quality summaries of an
instructional article.
*ar/cs/de/en
es/fr/hi/id/it
ja/ko/nl/pt/ru
th/tr/vi/zh
Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and
input type. * indicates changes from the originally published dataset made for GEM.
(NLU) tasks. They aggregate multiple tasks under
a uniﬁed evaluation framework, which enables researchers to fairly compare their models to others.
Due to the improved model comparability, benchmarks are critical in measuring modeling progress.
However, they also pose a risk that progress is
reduced to the single number shown in a benchmark’s leaderboard and thus may encourage blindly
optimizing it without regard to other considerations like model size or fairness . This is especially challenging for
benchmarks in NLG since, as discussed above, the
performance cannot be described through a single metric and it is often not clear what metric to
optimize for. This shortfall can be seen in benchmarks like DecaNLP and
GLGE which include NLG tasks
but focus only on a single metric and, as a result,
may mischaracterize a system’s performance.
Moreover, an easy-to-use data infrastructure also
disincentivizes researchers from interacting with
and conducting in-depth analyses of the data sets
that models are trained on. The limited analysis delegates the responsibility to ensure that all included
datasets have been collected fairly to the creators of
the benchmark . The dataset
and benchmark creators thus must provide in-depth
statements that describe the data characteristics and
surface potential issues and consider these issues
when selecting datasets for a benchmark .
These dangers emphasize selecting datasets for
a benchmark needs to be carefully done, that the
setup has to remain ﬂexible to be able to address
newly found limitations, and that the benchmark
should focus on climbing a leaderboard. Instead,
a living benchmark that can adjust its datasets and
speciﬁc evaluation metrics can be much more powerful and long-lived. This can, for example, be
seen in Dynabench,1 which has
a static evaluation, but interactively adds more test
1 
data through a human-in-the-loop approach.
Increasing multilingualism of NLG research.
Another potentially harmful choice by benchmark
creators is the choice of the languages of the included datasets. It is often assumed that work on
English transfers to other languages .
However, this assumption does not consider differences between the languages that lead to higher
modeling complexity, for example, a richer morphology or a ﬂexible word-order. Still, the majority
of work in NLP and almost all benchmarks exclusively focus on English . Even if
multiple languages are considered, the availability
of data in a language often does not represent the
number of speakers of a language. This means that
work on languages with little available data can
potentially impact many more people than work on
highly resourced languages .
As a result, many recent benchmarking and
dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or
which explore cross-lingual transfer. For example,
XTREME introduces a benchmark covering 40 languages across multiple NLU
and retrieval tasks, XCOPA 
is a commonsense reasoning dataset for eleven
languages, and MLQA is a
dataset for extractive question answering across
seven languages. We can observe a similar recent
trend in natural language generation, where ML-
Sum and WikiLingua were created as multilingual summarization datasets. There also have been ﬁrst
steps toward including NLG tasks in multilingual
NLU benchmarks. For example, XGLUE includes
Question and News Title Generation . Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG .
There have also been multiple shared tasks in
NLG that focus on multilingualism, for instance,
the shared task on multilingual surface realization
which includes eleven languages . The shared task on document-level
generation and translation featured German and English generation challenges .
The WebNLG+ shared task asked participants to
contribute models that can realize text in Russian
and English .
A benchmark that focuses only on NLG can enable much richer evaluation (as described in the
next sections), and promote non-English datasets.
In addition, it can ensure that the datasets created
for those shared tasks continue being evaluated.
Providing a testbed for automated evaluation.
Most traditional automated metrics, such as
ROUGE and BLEU , measure the n-gram overlap between a reference and the generated text. However, in most
cases, there is more than one correct way to generate a text, especially in tasks with a latent content
planning or selection step .
That means that a correct solution may score low
on a metric. While multiple references alleviate the
issue somewhat, these metrics still have a low correlation with human judgments . To address the issue, the machine
translation community has been organizing yearly
metrics shared tasks which produce metrics that
achieve a high correlation . The latest metrics focus on
semantic equivalence instead of lexical similarity,
which improves the correlations drastically. However, recent work by Fabbri et al. demonstrates that this may not hold in summarization,
where the automated metric BERTScore does not improve upon the correlation of ROUGE. Moreover, Mathur et al. 
and Freitag et al. ﬁnd that when comparing
two high-quality systems, differences according to
a metric may also stem from how references are
written or ﬂaws in the metric itself.2
Given that automated metrics perform differently
across tasks, setups, and languages, a multi-task
NLG benchmark has the opportunity to act as a
testbed to evaluate how the latest advances in automated metrics perform on these different tasks.
The benchmark can facilitate this research through
the release of system outputs and associated human
annotations, which is what we are planning to do
with GEM. Moreover, we allow the integration of
additional metrics into our living benchmark system, which enables a much faster adoption.
Developing reproducible human evaluation
standards.
In recent work, Howcroft et al.
 investigated NLG papers from the last
2For a more complete description of recent developments
in NLG evaluation, we refer to the survey by Celikyilmaz et al.
twenty years and the evaluation methodologies differ drastically across papers. Moreover, in most
cases, it is not even mentioned what the human
evaluation aims to measure and that deﬁnitions
of measures like “accuracy” or “ﬂuency” are inconsistent. They thus suggest reporting standards
for criteria and methods, following a classiﬁcation
system proposed by Belz et al. . In addition, regularly scheduled shared tasks like WMT
have lead to standardization of human evaluation
setups and enabled controlled experimentation with
them. GEM has the opportunity to develop reproducible standards for how human evaluation for
NLG tasks beyond translation should be conducted
while at the same time incorporating lessons from
related work. Acting on the same need, the recently
proposed GENIE system
aims to automate and standardize the human evaluation of different NLG systems, however with the
contrasting goal of reducing the evaluating to a
leaderboard-like score. To avoid further fragmentation of the ﬁeld, GEM is developing its own human
evaluation approaches, but uses the infrastructure
provided by GENIE to run its human evaluation.
In addition to GENIE, multiple other related efforts exist that work toward the goal of reproducible
and robust in-depth human and automatic evaluation for NLG tasks, and which focus on speciﬁc
modeling- or task-aspects that are different from
those in GEM. Among those are KILT which focuses on knowledge-intensive
tasks and retrieval-based models, Storium which focuses on open-ended story
generation, and BIG bench3 which focuses on measuring few-shot and zero-shot capabilities of language models.
Dataset Selection
As highlighted in Figure 1, the selection of included
datasets is an integral part of a benchmark. They
should be challenging for models, but it should
still be possible to evaluate models trained on them.
Moreover, the datasets should cover a wide range
of relevant generation challenges that allow for
ﬁndings to be as general as possible. Finally, the
datasets should cover tasks that are interesting for
contributors to work on to facilitate the wide adoption of the benchmark.
To collect datasets with those desired properties,
the selection methodology for GEM is composed
3 
of three steps. First, we elicited a set of proposals
from everyone involved in the effort. Second, we
identiﬁed criteria for the selection. Third, all GEM
members voted on individual dataset and criteria
utilities. The ﬁnal selection maximizes the utility
under constrained resources, similar to a knapsack
solver.4 This can be seen as an extension of the selection process of SuperGLUE 
that had similar ﬁrst and second steps but made the
ﬁnal decision based on which were harder for a
baseline model to solve after identifying a ﬁnal set
of candidate datasets. Since we are going to introduce challenge sets, the baseline performance of
models on a dataset matters less.
Dataset Elicitation.
In the ﬁrst step, all GEM
participants were asked to suggest datasets following the schema provided in Appendix A. The categories included multiple brief categorizations, such
as a description of the challenge that this dataset
provides, its high-level task, and the communicative goal of an agent trained on the data. Following
our goal to focus on non-English languages, we further asked for the languages included in the dataset,
as well as the language locale. This step yielded 35
proposed datasets, listed in Appendix B.
Estimating Task+Criterion Utility.
The second
step focused on the selection of criteria to inform
the selection. The initial set of criteria was selected through open discussion involving all members. We split criteria into “hard” and “soft” ones
– hard criteria would lead to the deﬁnite inclusion/exclusion of a task if (not) satisﬁed. Soft
criteria inform the utility of the remaining tasks.
All GEM members ﬁlled out a survey asking them
to rate, on a 5-point Likert scale, how much they
wanted to see a task included in GEM. Additionally, we posed yes/no questions for all considered
hard criteria and various questions about the soft
criteria (e.g., “what percentage of the tasks should
feature non-English language?”, or “do we prefer
noisy or clean datasets?”). Finally, the survey included open text ﬁelds that asked for (1) comments
on any of the tasks, (2) comments or suggestions on
hard exclusion criteria, and (3) suggestions of additional criterion/criteria. The full list of questions is
4Consider the criterion “We need equal representation of
large and small datasets” under the constraint that only two
datasets can be selected. If we have two large datasets with
utility 10, and one small one with utility 5, we may want to
include the smaller dataset over the second large dataset to
satisfy the criterion.
shown in Appendix C.
The survey received 28 responses, revealing that
the initial version of GEM should include a median
of 10 tasks or an average of 12. Of those tasks,
about a third should feature non-English language.
Selected Criteria.
For the hard criteria, there
was an agreement to focus only on open-access
datasets and that concurrent or past shared tasks
for the same datasets are not an issue. Overall,
the sentiment determined the following selection
principles:
• We focus on diverse high-level tasks over
a single high-level task evaluated in-depth.
However, each high-level task should include
multiple datasets.
• We focus on clean datasets to avoid conﬂating
model mistakes and learned noise.
• We include a mix of high- and low-resource
• We focus on data with interesting test sets.
• We should not focus on the quality of current
evaluation strategies for a given dataset.
• We prefer multi-reference datasets since those
have been shown to lead to more robust automatic evaluation.
High-Level Tasks.
Since these principles dictate that we should focus on a small set of
high-level tasks, we used the free-text replies to
evaluate the interest in different high-level tasks.
Grouping the proposed tasks yielded the following candidates: Summarization, Dialog, Simpliﬁcation/Compression, Question Answering, Creative
Writing, Data-to-Text, and Question Generation.5
There was a preference to exclude image inputs and
question answering because those tasks add complexity to the evaluation beyond the generated text.
Moreover, since creative generation tasks like story
generation and poetry generation suffer even more
from inadequate evaluation approaches, there was
a consensus to not include them. There was, however, a strong preference for the high-level tasks
Summarization, Data-to-text, and Dialog.6
5For a full overview of potential future expansions and
challenges, we refer to the survey by Gatt and Krahmer .
6One may question the absence of Translation from this list.
While it is a generation task, we excluded it since Translation
already has regular benchmarking efforts with WMT.
Speciﬁc Datasets.
The ﬁnal selection is shown
in Table 1. To arrive at the selection, we ﬁrst
ranked all datasets by their average rating. For
this, we treated positive ratings as 1, negative ratings as -1, and neutral ratings as 0. The highestranked datasets were E2E with 0.577, XSum with
0.538, and ToTTo with 0.461. Unfortunately, non-
English datasets were ranked lower, with only
WebNLG and MLSum among the top 15 datasets.
We grouped all datasets by their high-level tasks
and selected a group that would not violate the selection principles (e.g., only high-resource tasks).
If two datasets ﬁt, we picked the one with a higher
interest rating. Among the 11 datasets, we have
18different languages, and the dataset sizes range
from 5,000 examples to 1.5M, with most datasets
between 50-150k examples. Two of them do not include English at all, which we hope reduces the dependence of the modeling approaches on anglocentric pretraining .
The high-level tasks include Dialog, Summarization, Data-to-Text, and Simpliﬁcation. About half
of the datasets have multiple references and more
than half had post-processing steps applied to them
to ensure high data quality.
GEMifying the data
We produce data cards for all data sets in GEM,
for which we developed an NLG-speciﬁc template.7 In addition to describing the data itself,
the cards acknowledge potential limitations of a
dataset regarding its creation process and describe
its real-world use cases to ensure that the research
is conducted responsibly.
These datasets are the base selection, and as part
of GEM, we may change datasets and how they are
used. For example, we may improve the training
sets, make the test sets more challenging, or probe
for speciﬁc skills a model must exhibit with testonly datasets . We may also ask to evaluate a single model
on multiple test sets, following the design by Dua
et al. .
We are including modiﬁcations to several of the
datasets: (1) MLSum: We excluded all languages
besides Spanish and German since the sources for
other languages disallow scraping content. Addi-
7Our template extends and restructures that from Hugging
Face Datasets and along with a guide can be found at https:
//gem-benchmark.com/data_cards.
Challenge Set Type
Numerical Variation
Attribute Order
English Cheap ->Cheap English All data-to-text tasks
Typographical Errors
English Cheap ->Enlish Chesp
Schema-Guided, WikiAuto, XSum
No Punctuation
... the dog. ->... the dog
Schema-Guided, WikiAuto, XSum
Backtranslation
fantastic ->toll ->great
Schema-Guided, WikiAuto, XSum
Train & Validation Samples
Gender, Ethnicity, Nationality
Input Shape
Syntactic Complexity
Covid Summaries
MLSUM (es+de), XSum
Table 2: An overview of the types of challenge sets for GEM. The ﬁrst category are modiﬁcations to inputs of a
model, the second category identiﬁes contrast sets which are subsets of the original test set, and the third describes
newly collected data.
tionally, we removed all duplicate items (i.e., items
with the same input text) and we used langdetect8
to ﬁlter out examples that were in the wrong language. In total, 147 examples were removed from
the German portion (0.06%) and 7417 examples
were removed from the Spanish portion (2.5%). (2)
XSum: Summaries in this dataset often have divergence issues between the source and target texts
since gold summaries are introductory sentences
prefacing each article. Models agnostic to such
noises are vulnerable to hallucinations . To combat this,
we ﬁne-tuned a BERT-based 
classiﬁer on 500 document and gold summary pairs,
manually annotated for faithfulness and excluded all document-summary pairs
from the original XSum dataset where the classiﬁer
was not conﬁdent (p(faithful) > 0.8) whether the
summary is faithful to the document or not. (3)
Schema-Guided Dialog: We are focusing on the
response-generation part of the dataset and thus
reformatted the dataset to treat the service agent
utterances as the targets to be generated and the
previous customer utterance and the agent’s dialog
act as the input. We additionally reformat the dialog acts to directly conform to the format described
in the paper . (4) WikiLingua: We focus on the same ﬁve languages that
were benchmarked in its original release (en, es, ru,
tr, vi) in a cross-lingual setup in which the inputs
are in the respective language and the outputs are
in English. However, we re-split the original data
to avoid train-test overlaps between languages and
provide training data in 13 additional languages (as
shown in Table 1). For GEM, we allow submis-
8 
sions trained on any of the languages in isolation
or as part of a multilingual model.
Challenge Sets
In addition to applying consistent metrics to existing test sets, understanding speciﬁc model behavior,
such as model generalization capabilities or performance under targeted cases, is also key for improvement. This is difﬁcult to assess through evaluations on i.i.d. test splits. We thus release challenge
sets to evaluate data-to-text and text-to-text models
(overview in Table 2). In addition to enabling a
more speciﬁc breakdown of how a model performs
in the presence of challenging inputs, the set of
system outputs on these test sets also constitutes a
rich corpus that enables further error analysis and
research. We apply multiple strategies to create the
special test sets, in particular (I) alteration of the
existing test sets (e.g., the introduction of distractors), (II) breaking down of the existing sets into
subsets with certain properties (e.g., subsets with
different complexity), and (III) the compilation of
new test sets (e.g., out-of-vocabulary inputs). We
restrict the size of each challenge set to about 500
examples to minimize computational overhead. On
the WebNLG challenge sets, all subset items are
selected proportionally from each category to ensure a similar distribution to the original set; on all
other datasets the subset items are selected from
the whole set. The results of the different systems
on these subsets will be compared to the results
obtained by the same systems on the same subsets
of the original test data.
For case (I), altering existing test sets, the
ﬁrst challenge set adds numerical variation in
WebNLG. This variation attempts to respect the
format of the current cardinal value (e.g. alpha,
integer, or ﬂoating-point) and replaces the existing value with a new random value as a means to
challenge existing trained models. The generated
number is lower-bounded between zero and upper
bounded to be within to the highest power of 10
unit for the given value (e.g. replacing a value of
54 would result in a random value between 0-100).
Floating values are also bounded to have the same
degree of precision as the input value. For structureto-text and dialog datasets, we produce a version
of the test sets in which the order of the components of the input structures (triples, concepts, dialog acts, table rows, etc.) is randomly changed. For
text-to-text datasets and Schema-guided Dialog, we
introduce several types of perturbations: (a) typographical errors, using butter-ﬁngers 9 with two
thresholds 0.02 and 0.05, which respectively correspond to lower and higher error frequencies; (b)
removal of the ﬁnal punctuation sign (if any); (c)
substitution of the input text by a backtranslated
version, using the backtranslation implementation
by Xie et al. . We rejected backtranslation
outputs based on a character length to ensure that
the difference in character length between original
and backtranslation does not exceed 35% of the
original source character length. For XSum 99.8%
of the backtranslations were accepted, for Wiki-
Auto 94.42% (ASSET) and 87.18% (TURK), and
for Schema-Guided Dialog 78%.
In case (II), the breaking down existing sets, we
ﬁrst provide for each dataset random samples of
training and validation data, in order to assess
to what extent the scores of the different systems
drop when run on the test data. Then, speciﬁc
splits are created for particular datasets, in order
to assess possible biases of the models, and their
robustness across inputs with different speciﬁcations. For ToTTo, test set splits are built according
to several aspects that can be identiﬁed using Wiki-
Data: gender, ethnicity and nationality grouped
by continent. For gender, we compare the performance between male and female people, but
cannot compare other genders due to a lack of original data - only seven people in the original test
set are marked as having a different gender. We
compare across the continent of the underlying
nationality to address the issue that data for each
country can be very sparse – i.e., only 19 coun-
9 
butter-fingers
tries are represented by more than ten people and
only one of these is located in Africa (Kenya). In
case a person has citizenships across multiple continents, we may include the person in any of the
included continents. Finally, we compare African
Americans vs. all Americans. Ethnicity is very
sparsely annotated in WikiData with fewer than
150 annotated test examples in total and 128 of
these are African Americans. We thus are unable
to compare the performance on, e.g., Yoruba or
Punjabi people, both of which have fewer than ﬁve
instances. Another caveat here is that only 21 of
the 128 people are female. Our contrast subset that
can include any US citizens matches these counts.
Across all three challenge subsets, we additionally
match the fraction of the existing non-overlap and
overlap properties. For WebNLG, we propose subsets based on the shape of the inputs (number of
triples, number of common subjects and/or objects,
depth, etc.) For Turk/ASSET, splits are created
in terms of the syntactic complexity of the sentences to be simpliﬁed. To characterise sentence
complexity we use the developmental level scale
proposed by Covington et al. .10 Although
Turk and ASSET contain similar input sentences,
the human references in Turk were created without
allowing sentence splits and ASSET was created
by encouraging annotators to split long sentences.
For all datasets, we propose splits based on the
frequency of the parts that compose the input in
the training data; the resulting test sets range from
being made of very common components to being
made only from components unseen in the training
data. For case (III), we collect time-shifted test
data for news summarization in the form of articles
with Covid19-related keywords. Since MLSum and
XSum were collected before the pandemic, we can
measure how a model responds to context not seen
in the training data (outside of potential pretraining). The new set of articles covers existing article
topics (economy, sports, etc.) but all in relation
to the Covid19 pandemic. In addition, some new
topics appear in the collected data derived from
outlet sections that were not part of the original
data collection.11
10We use the implementation provided by Lu .
11To collect this data we use the scripts provided for the
re-creation of MLSum and XSum datasets.
Experimental Setup
Since the GEM test sets and ﬁnal metrics selection have not been released yet, we describe an
experimental setup that will ensure that participating models are trained correctly and evaluated on
publicly available data with available metrics that
will give a sufﬁcient indication of a model’s performance. To do this, we are reporting the results of
the baseline models on the validation sets.
Modeling Baselines
Much of the recent modeling progress in NLP can
be attributed to the rise of the pretrain-then-ﬁnetune
paradigm which has led to consistently better results. This ﬁnding is consistent with human judgments for summarization, as shown by Fabbri et al.
 , among others. However, many of the tasks
included in GEM may not beneﬁt from a language
model encoder since their input is not natural language. We thus apply a variety of different architectures that vary in size, complexity, and training schema. Our main baselines are T5 with 60M
parameters and BART with
139M parameters . For non-
English datasets, we use their multilingual counterparts mT5 in various sizes and
mBART . We additionally train
the following baselines on a subset of tasks: TGen
(with added language model and lemma tags denoted as TGen+/++) ,
an architecture for generation from dialog acts, an
LSTM-based Sequence-to-sequence model with attention , DialoGPT , a pretraining approach for conversational models, and PEGASUS , which uses a summarization-speciﬁc pretraining schema that masks and predicts entire sentences.For WikiLingua, we additionally report results on a setup proposed by Ladhak et al. 
which includes ﬁrst training a monolingual model
followed by ﬁnetuning with the correct source
language, coupled with synthetic data generated
through translation (mBART+).
Almost all baselines can be reproduced on a GPUbased colaboratory notebook within 2-3 hours.
Automated Evaluation
As mentioned above, GEM provides a testbed for
automated metrics and can be used to popularize
newly developed ones. Thus, models are evaluated
via a constantly expanding list of metrics and, to
avoid overﬁtting to known metrics, we will use metrics on the test submissions that are not included in
this initial writeup. Consequentially, the baseline
results are an incomplete list which will be expanded upon the announcement of the test metrics.
The set of metrics can be computed via the framework described at 
com/shared_task which comprises metrics in
the following categories:
Lexical Similarity.
We include multiple “traditional” metrics as baseline metrics, notably
BLEU , ROUGE-1/2/L , and METEOR .
These metrics can often be gamed, for example,
ROUGE can be improved by increased the output length of the model . Moreover, the reliability of these metrics depends on
the quality and number of the references . However, on a
system-level, they still correlate well with human
judgments for some tasks .
Semantic Equivalence.
More recently, metrics
that rely on pretrained language models have
shown improved correlations with human judgments on the segment-level.
We thus include
BERTScore , a metric based
on the similarity of sentence embeddings, and
BLEURT , a metric that is
ﬁne-tuned on human ratings. The reported baseline
results use RoBERTa-large and
mBERT for BERTScore and
the English-only BLEURT-base-128 for BLEURT.
Probing for Faithfulness.
Another approach
that has shown promise in summarization. The
approach relies on the insight that a reader of a
reference and generated summary should be able
to answer the same question, regardless of how the
summary is phrased. There has been much development toward these QA-based approaches and they
can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches . While
most related work on these metrics is limited to
summarization, we are evaluating systems using a
QA-based method called QuestEval that supports all of our tasks.
In addition to QA-based evaluation, there have
also been related efforts to develop more ﬁne-
Metrics (Lexical Similarity and Semantic Equivalence)
Czech Restaurant
MLSum (de)
MLSum (es)
Schema-Guided
WebNLG (en)
WebNLG (ru)
WikiLingua (es→en)
WikiLingua (ru→en)
WikiLingua (tr→en)
WikiLingua (vi→en)
Table 3: The set of baseline results we release alongside GEM with a focus on reference-based evaluation.
Metrics (Diversity and System Characterization)
Output Len.
Czech Restaurant
MLSum (de)
MLSum (es)
Schema-Guided
WebNLG (en)
WebNLG (ru)
WikiLingua (es→en)
WikiLingua (ru→en)
WikiLingua (tr→en)
WikiLingua (vi→en)
Table 4: Results of the baseline results we release with GEM, focusing on diversity of the outputs and neutral
system characterizations.
Figure 2: A screenshot of the interactive result exploration tool. [Top Left] The selection of tasks, task-groups, or
individual submissions. [Top Right] The selection of metric-groups or metrics [Bottom] The parallel coordinates
visualization of the selection. The selection here can be ﬁltered by brushing over a section of an individual metric,
as is shown here for BLEURT. Hovering over a line presents detailed information of the particular submission.
grained and interpretable evaluation metrics, for example to measure consistency in data-to-text problems .
We are using one such metric called NUBIA , the NeUral Based Interchangeability
Assessor, which combines multiple measures such
as entailment and similarity into a decomposable
and interpretable score.
Diversity.
As argued by Hashimoto et al. 
among many others, NLG models intrinsically
trade off diversity and quality. A model can produce more diverse outputs through sampling but at
the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge 
and by van Miltenburg et al. . These include
the Shannon Entropy 
over unigrams and bigrams (H1, H2), the mean
segmented type token ratio over segment lengths
of 100 , the ratio of distinct n-grams over the total number of n-grams
(Distinct1,2), and the count of n-grams that only appear once across the entire test output .
System Characterization.
The ﬁnal section of
metrics will characterize the systems. While the
focus of this section will be on qualitative descriptions through model cards, we also gather quantitative information that is not necessarily associated
with a judgment. As part of this, we collect the
number of parameters of a system, as suggested
by Ethayarajh and Jurafsky . For each task,
we additionally report the vocabulary size over the
output (|V|) and the mean output length of a system .
One of the central aims of GEM is to measure
the progress in NLG without misrepresenting the
complex interactions between the sometimes contradicting measures. We thus will not distill the
complex interplay of the data, metrics, and model
outputs into a single number or statement, and
we do not present results in a traditional leaderboard. Instead, we developed an interactive result
exploration system that allows analyses of model
results, and which we describe in this section. To
further motivate this change, consider the following conclusion someone may draw from looking at
a leaderboard:
System Foo performs the best.
Our interactive system aims to enable more nuanced statements such as:
System Foo leads to consistent performance increases in Bar-type metrics on
challenges that measure Baz while maintaining equal performance on most metrics of type Qux.
A screenshot of our system is presented in Figure 2.12 In addition, our baseline results are presented in a tabular view in Tables 3 and 4. Our
interactive system is centered around a parallel coordinates plot which shows all
results as lines through parallel axes. Every line
intersects the axes at the corresponding mapped
value. For instance, see the red line representing
the results for task “ToTTo” of baseline “t5-small”.
Filters can be applied along axes (see BLEURT
axis in Figure 2) and the ﬁltered selection is highlighted through bold lines. A selection can be a set
of metrics, systems, or tasks. This style of presentation has not been used before for a benchmark. The
closest prior work is by Fu et al. for namedentity recognition which allows similar ﬁltering
and sorting, but presents the results in a table.
However, the parallel coordinates approach can
scale to a much greater number of metrics than a
table. Moreover, by using a parallel coordinates
plot instead of a table, it is easy to spot patterns that
span multiple metrics, systems, or tasks. For example, the highlighted line in Figure 2 uncovers that,
for the T5 baseline on ToTTo, the diversity metrics
score higher than other systems while scoring lower
on reference-based metrics. Since we only have
a single baseline for ToTTo, it is unclear whether
this difference can be attributed to the dataset or
the system but this relationship will be uncovered
once we receive submissions.
The ﬁnal system will additionally be able to
display the model cards and other related metainformation associated with submissions. It will
also be able to show (and compare) exemplary outputs for each test set. Those two features will improve the transparency of the results and systems
to those who are not familiar with a task and provide necessary information to those who consider
using a particular system. The combination of all
components will enable analysis on quantitative,
individual, and qualitative level which can support
formulating new research hypotheses and gather
in-depth insights about system performance. For
example, the functionality to compare human anno-
12An initial version showcasing our baseline results is deployed on our website.
tation and automatic measures could lead to a better
understanding how ﬂuency affect BERTScore.
In addition to the interactive self-directed result
exploration, our shared task features an evaluation
and analysis part. Instead of dictating the interpretation of the modeling shared task results, we will
release all system outputs and metrics in this second part and participants of this part may run their
own evaluation and conduct interesting analyses.
Submitting to the benchmark
While we ask submitters to try to cover as many
tasks as possible, we acknowledge potential restrictions on computation resources. We thus do not
require that a submissions to GEM has to include
predictions on every included test and challenge
sets. All predictions from a model should be formatted and added into a single ﬁle as outlined on
our website.
In addition, we require every submitter to answer
a series of questions that we will use to construct a
model card and externalize
potential concerns regarding the social impact of
a model and its use, or its training data. The card
will additionally display information to replicate
the experiments. While we require responses to
these questions at submission time, we allow the
information about a model to remain anonymous
during required anonymization periods should a
paper describing the model be under submission
elsewhere. All submitted model outputs will be
made publicly available for download.
After a submission, we will run the evaluation
suite on the submitted outputs and additionally collect human annotations.
Human Evaluation
GEM will be used to develop reproducible and consistent human evaluation strategies for generated text. This task involves
selecting and deﬁning which quantities of the generated text should be measured, developing annotation schemes and rater guidelines to capture these
quantities accurately, and infrastructure to annotate
system outputs.
We aim to develop these setups for all task setups
such as summarization, dialogue, simpliﬁcation,
and data-to-text. To approach this task, we will follow the recently proposed taxonomy of human evaluation measures by Belz et al. and follow
the reporting strategies proposed by Howcroft et al.
 . The detailed setups will be described in a
evaluation datasheet .
All shared task participants will be asked to provide gold annotations on system outputs, which we
will then use to evaluate the consistency of crowdsourced annotations.13
Next Steps
This section lists the currently active developments
and the long-term steps we will take to ensure that
GEM will continue to evolve and improve.
Collecting more multilingual data
Many of the initial datasets in GEM are focused on
(American or British) English; we see this release
as a starting point for the collection of new datasets
to improve the inclusiveness of other languages and
cultures. From the task point of view, to ensure the
longevity of the dataset, we want it to be practical
and socially beneﬁcial. Through GEM, we have
developed a set of desired criteria for NLG datasets
and we aim to apply this knowledge to data collection and actively work toward reducing the disparity in data availability between languages . To this end, we are focusing on a
task that requires content selection, planning, and
surface realization along in a grounded scenario.
The idea is in the prototyping stage with prospects
broadly towards dialog response generation and
topic summarization in multiple languages. We
plan to do so by collaborating with speakers of
low-resourced languages through a participatory
research approach, as suggested by .
Toward this goal, GEM welcomes anyone interested in collaborating on this effort.
Personalizing and Controlling NLG
GEM currently focuses on tasks that deterministically transform an input into an output. With the
increasing use of NLG models in real-world applications, how to enable and evaluate personalized
NLG systems (e.g., in dialect or formality) remains
challenging. Several related tasks have been proposed, for example, the transfer of writing style
from informal to formal ,
personalization of machine translation systems to
align with particular personal traits , or persona-guided response generation of dialogue systems .
We envision our framework to be extended (e.g.,
13This approach has been successfully used by WMT
for many years. See, e.g., 
wmt20/translation-task.html.
dataset, evaluation) to incorporate this line of userfocused NLG.
Regular updates to the living benchmark
To activate the beneﬁts of a living benchmark that
is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form
of model outputs, analyses, and metrics at any time
and will automatically update the results presented
on our website to incorporate them. For the updates
to the dataset selection, we want to consider the
input of the wider NLG research community. To do
so, we will set up a yearly selection process similar
to the one described in Section 3. The ﬁrst update
process will be run after the GEM workshop at
ACL 2021. To be able to have a robust comparison
between different versions of GEM, we will only
replace a small subset of datasets at a time.
Conclusion
In this paper, we have introduced GEM, a living
natural language generation benchmark with a focus on evaluation. While GEM does not claim to
instantly solve all issues of benchmarks in NLG,
we aim to provide an environment in which systems
can be tested in a principled manner and which can
elevate the prominence of interesting evaluation approaches. By providing a testbed to easily conduct
experiments across many datasets and evaluate in a
repeatable, consistent, and more interpretable way,
we will be able to track progress toward the goals
in NLG research much more clearly. Moreover, we
will be able to extend and shape GEM in the future
to include more multilingual datasets, which will
assist in their adoption across the wider research
community.
Contribution Statements
GEM is a large effort with a decentralized organization that is split into different task-speciﬁc subgroups. To acknowledge everyone’s contribution,
we list the contribution statements below for all
Steering Committee.
Antoine Bosselut, Esin
Varun Prashant Gangal,
Perez-Beltrachini,
Shaikh, and Wei Xu make up the steering committee. Sebastian Gehrmann coordinates and leads
the GEM effort. All others provide feedback and
discuss larger decisions regarding the direction of
GEM and act as conference organizers for the ACL
2021 workshop.
Summarization.
summarization
members are Chris Emezue, Esin Durmus, Faisal
Ladhak, Jiawei Zhou, Juan Diego Rodriguez,
Kaustubh Dhole, Khyathi Chandu, Laura Perez,
Pawan Sasanka Ammanamanchi, Pedro Henrique
Martins, Rubungo Andre Niyongabo, Shashi
Narayan, Vikas Raunak, and Yufang Hou. Pedro
Henrique Martins organized the group and wrote
the data statement for the MLSum dataset. Pawan
Sasanka Ammanamanchi was responsible for
the XSum data statement, while Vikas Raunak
worked on the Wikilingua statement.
Narayan prepared the GEM version of the XSum
dataset and trained its baseline models.
Diego Rodriguez was responsible for cleaning the
MLSum dataset and trained its baseline models.
Faisal Ladhak was responsible for the Wikilingua
baseline models.
Rubungo Andre Niyongabo
participated in the discussions and added related
papers to the planning document.
Sashank Santhanam, Samira Shaikh,
Bodhisattwa Prasad Majumder, Harsh Jhamtani,
Yangfeng Ji, Tosin Adewumi, and Wanyu Du are
part of this group. Tosin Adewumi contributed
code for DialoGPT, and Wanyu Du trained baselines for Schema-Guided Dialog. Harsh Jhamtani
wrote the data card for Wizards of Wikipedia.
Data2Text.
Ondrej Dusek wrote the data cards
for E2E NLG and Czech Restaurants data and a
TF loader for Czech Restaurants. He also supplied baseline outputs for E2E, Czech Restaurants,
and WebNLG. Sebastian Gehrmann supplied baseline outputs for E2E, WebNLG, and CommonGen.
Yacine Jernite wrote the data card for CommonGen
and the Hugging Face loaders for Czech Restaurants and WebNLG. Teven Le Scao wrote the Hugging Face loader for E2E. Simon Mille and Anastasia Shimorina wrote the data card for WebNLG.
Table2Text.
Varun Gangal and Miruna Clinciu
are part of this group. Miruna Clinciu was responsible primarily for DART and Varun Gangal for
ToTTo while maintaining a close correspondence
and understanding between them to ensure all steps,
such as code structure, preprocessing primitives,
baselines were as uniform as possible.
Simpliﬁcation.
Dhruv Kumar, Mounica Maddela, and Wei Xu contributed to the GEM Simpli-
ﬁcation task. Dhruv Kumar created the data cards
for the datasets, added Wiki-Auto and Turk/ASSET
datasets to TFDS, and integrated the SARI metric
 into the GEM evaluation framework. Mounica Maddela created baselines for the
task and added the Turk benchmark corpus to Hugging Face and TFDS. Wei Xu helped in the organization and planning of the task setup.
Automated Evaluation.
Ondrej Dusek wrote
the base code and included BLEU, Meteor,
ROUGE, and referenceless metrics (the latter based
on code supplied by Emiel van Miltenburg). He
also prepared reference sets for E2E, Czech Restaurants and WebNLG. Sebastian Gehrman included
BLEURT and BERTScore and prepared the reference sets. Dhruv Kumar included SARI and
adapted the code for source-based metrics. Nishant
Subramani helped with code refactoring. Miruna
Clinciu , Emiel van Miltenburg and Thibault Sellam provided feedback and participated in discussions.
Human Evaluation.
Samira Shaikh was the
point of contact for this working group. She led the
discussions to make progress on the group goals.
She also worked with the group to select the general evaluation criteria as well as the criteria for
dialogue and simpliﬁcation tasks. Khyathi Chandu
and Miruna Clinciu worked on selecting evaluation
criteria for the summarization task and participated
in the group discussions. Simon Mille provided
support on using the criteria taxonomy and the annotated evaluation sheets for selecting and deﬁning
the criteria to use; worked on selecting the D2T
criteria. Vitaly Nikolaev and Sashank Santhanam
worked on selecting evaluation criteria for dialog
and simpliﬁcation tasks. João Sedoc worked with
the group to select the evaluation criteria in general
as well as the speciﬁc ones for dialog and simpliﬁcation. He also helped to select among annotation
interfaces. Anastasia Shimorina worked with the
group to select the evaluation criteria and participated in the discussions. Chris Emezue, Sebastian
Gehrmann, Khyati Mahajan, and Yufang Hou participated in discussions.
Submission
Madaan, Moin Nadeem, Hendrik Strobelt, and Sebastian Gehrmann are part of this group. Sebastian
Gehrmann developed the website. Aman Madaan
wrote the initial version of the result presentation.
Hendrik Strobelt leads the visualization effort for
interactive exploration of results.
Model Infrastructure.
Yacine Jernite wrote the
initial script template for evaluating and ﬁne-tuning
Hugging Face models with the CommonGen example. Sebastian Gehrmann generalized the script to
work with other datasets. Tosin Adewumi wrote a
script for ﬁne-tuning the DialoGPT model for dialogue datasets. Juan Diego Rodriguez worked on
the infrastructure to ﬁne-tune mBART on MLSum.
Mihir Kale trained all mT5 baselines.
Data and Model Sheets and Statements.
Salomey Osei, Pawan Sasanka Ammanamanchi, Juan
Diego Rodriguez, Sebastian Gehrmann, Yacine Jernite, and Angelina McMillan-Major are part of this
group. The Data Sheet structure was adapted from
a combination of designs created for the Hugging
Face Datasets library by Angelina McMillan-Major
and Yacine Jernite and one written by Sebastian
Gehrmann. Juan Diego Rodriguez and Yacine Jernite wrote initial statements for ASSET and CommonGen respectively. The feedback on those was
used to improve the structure of the ﬁnal template.
Everyone contributed to the model card template.
Challenge Sets.
Simon Mille, Emiel van Miltenburg, Kaustubh Dhole, Varun Prashant Gangal, Saad Mahamood, and Laura Perez-Beltrachini
proposed and discussed ideas of interest for the
data-to-text and the text-to-text tasks. Simon Mille
coordinated the group.
Emiel van Miltenburg,
Saad Mahamood, and Simon Mille worked on the
creation of the data-to-text datasets, while Varun
Prashant Gangal, Kaustubh Dhole and Laura Perez-
Beltrachini worked on the text-to-text datasets. Sebastian Gehrmann contributed the ToTTo challenge
Crowdsourcing
Rubungo Andre Niyongabo, Aremu Anuoluwapo,
Khyathi Chandu, Yufang Hou, Samira Shaikh,
Varun Prashant Gangal, and Dimitra Gkatzia are
members of this group. Khyathi Chandu worked on
identifying where the current datasets fall short to
motivate the crowdsourcing of data for a new task.
Based on the suggestions from collaborators, she
wrote two task proposals in the domains of longform text, conversations, and data-to-text that address an array of challenges in generation and easily
scale to multiple languages. Samira Shaikh participated in the discussions and gave feedback on the
task proposals in the pilot study phase. Dimitra
Gkatzia looked into potential resources for crowdsourcing. Chris Emezue and Rubungo Andre Niyongabo explored potential low-resource African
languages for crowdsourcing. We are in the process of piloting the tasks internally.
The authors of this paper not named in the groups
participated in initial discussions, participated in
the surveys, and provided regular feedback and
guidance. Many participants commented on and
helped write this paper. We additionally thank all
participants of INLG 2019, the Generation Birdsof-a-Feather meeting at ACL 2020, the EvalNL-
GEval Workshop at INLG 2020, and members of
the generation challenge mailing list of SIGGEN
for their participation in the discussions that inspired and inﬂuenced the creation of GEM.