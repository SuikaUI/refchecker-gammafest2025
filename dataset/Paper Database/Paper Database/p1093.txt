Unsupervised Salience Learning for Person Re-identiﬁcation
Wanli Ouyang
Xiaogang Wang
Department of Electronic Engineering, The Chinese University of Hong Kong
{rzhao, wlouyang, xgwang}@ee.cuhk.edu.hk
Human eyes can recognize person identities based on
some small salient regions. However, such valuable salient
information is often hidden when computing similarities of
images with existing approaches. Moreover, many existing approaches learn discriminative features and handle
drastic viewpoint change in a supervised way and require
labeling new training data for a different pair of camera
views. In this paper, we propose a novel perspective for person re-identiﬁcation based on unsupervised salience learning.
Distinctive features are extracted without requiring
identity labels in the training procedure. First, we apply
adjacency constrained patch matching to build dense correspondence between image pairs, which shows effectiveness in handling misalignment caused by large viewpoint
and pose variations. Second, we learn human salience in
an unsupervised manner. To improve the performance of
person re-identiﬁcation, human salience is incorporated in
patch matching to ﬁnd reliable and discriminative matched
patches. The effectiveness of our approach is validated on
the widely used VIPeR dataset and ETHZ dataset.
1. Introduction
Person re-identiﬁcation handles pedestrian matching and
ranking across non-overlapping camera views. It has many
important applications in video surveillance by saving a lot
of human efforts on exhaustively searching for a person
from large amounts of video sequences. However, this is
also a very challenging task. A surveillance camera may
observe hundreds of pedestrians in a public area within one
day, and some of them have similar appearance. The same
person observed in different camera views often undergoes signiﬁcant variation in viewpoints, poses, camera settings, illumination, occlusions and background, which usually make intra-personal variations even larger than interpersonal variations as shown in Figure 1.
Our work is mainly motivated in three aspects. Most
existing works handle the problem
of cross-view variations and extract discriminative features
Figure 1. Examples of human image matching and salience
maps. Images on the left of the vertical dashed black line are from
camera view A and those on the right are from camera view B.
Upper part of the ﬁgure shows an example of matching based on
dense correspondence and weighting with salience values, and the
lower part shows some pairs of images with their salience maps.
by employing supervised models, which require training
data with identity labels. Also, most of them require labeling new training data when camera settings change, since
the cross-view transforms are different for different pairs of
camera views. This is impractical in many applications especially for large-scale camera networks. In this paper, we
propose a new approach of learning discriminative and reliable descriptions of pedestrians through unsupervised learning. Therefore, it has much better adaptability to generel
camera view settings.
In person re-identiﬁcation, viewpoint change and pose
variation cause uncontrolled misalignment between images.
For example in Figure 1, the central region of image (a1)
is a backpack in camera view A, while it becomes an arm
2013 IEEE Conference on Computer Vision and Pattern Recognition
1063-6919/13 $26.00 © 2013 IEEE
DOI 10.1109/CVPR.2013.460
2013 IEEE Conference on Computer Vision and Pattern Recognition
1063-6919/13 $26.00 © 2013 IEEE
DOI 10.1109/CVPR.2013.460
2013 IEEE Conference on Computer Vision and Pattern Recognition
1063-6919/13 $26.00 © 2013 IEEE
DOI 10.1109/CVPR.2013.460
in image (b1) in camera view B. Thus spatially misaligned
feature vectors cannot be directly compared. In our method,
patch matching is applied to tackle the misalignment problem. In addition, based on prior knowledge on pedestrian
structures, some constraints are added in patch matching in
order to enhance the matching accuracy. With patch matching, we are able to align the blue tilted stripe on the handbag
of the lady in the dashed black boxes in Figure 1.
Salient regions in pedestrian images provide valuable information in identiﬁcation. However, if they are small in
size, salience information is often hidden when computing
similarities of images. In this paper, salience means distinct
features that 1) are discriminative in making a person standing out from their companions, and 2) are reliable in ﬁnding
the same person across different views. For example, in Figure 1, if most persons in the dataset wear similar clothes and
trousers, it is hard to identify them. However, human eyes
are easy to identify the matching pairs because they have
distinct features, e.g. person (a1 −b1) has a backpack with
tilted blue stripes, person (a2 −b2) has a red folder under
her arms, and person (a3 −b3) has a red bottle in his hand.
These distinct features are discriminative in distinguishing
one from others and robust in matching themselves across
different camera views. Intuitively, if a body part is salient
in one camera view, it is usually also salient in another camera view. Moreover, our computation of salience is based
on the comparison with images from a large scale reference
dataset rather than a small group of persons. Therefore, it is
quite stable in most circumstances. However, these distinct
features may be considered by existing approaches as outliers to be removed, since some of they (such as baggages or
folders) do not belong to body parts. Clothes and trousers
are generally considered as the most important regions for
person re-identiﬁcation. Aided by patch matching, these
discriminative and reliable features are employed in this paper for person re-identiﬁcation.
The contributions of this paper can be summarized in
three-folds. First, an unsupervised framework is proposed
to extract distinctive features for person re-identiﬁcation
without requiring manually labeled person identities in the
training procedure. Second, patch matching is utilized with
adjacency constraint for handling the misalignment problem caused by viewpoint change, pose variation and articulation. We show that the constrained patch matching
greatly improves person re-identiﬁcation accuracy because
of its ﬂexibility in handling large viewpoint change. Third,
human salience is learned in an unsupervised way. Different from general image salience detection methods , our
salience is especially designed for human matching, and has
the following properties. 1) It is robust to viewpoint change,
pose variation and articulation. 2) Distinct patches are considered as salient only when they are matched and distinct
in both camera views. 3) Human salience itself is a useful
descriptor for pedestrian matching. For example, a person
only with salient upper body and a person only with salient
lower body must have different identities.
2. Related Work
Discriminative models like SVM and boosting are widely used for feature learning. Prosser et al. 
formulated person re-identiﬁcation as a ranking problem,
and used ensembled RankSVMs to learn pairwise similarity. Gray et al. combined spatial and color information in an ensmeble of local features by boosting. Schwartz
et al. extracted high-dimensional features including
color, gradient, and texture, and then utilized the partial
least square (PLS) for dimension reduction. Another direction is to learn task-speciﬁc distance functions with metric
learning algorithms . Li and Wang partitioned the image spaces of two camera views into different
conﬁgurations and learned different metrics for different locally aligned common feature spaces. Li et al. proposed a transferred metric learning framework for learning
speciﬁc metric for individual query-candidate settings. In
all these supervised methods, training samples with identity
labels are required.
Some unsupervised methods have also been developed
for person re-identiﬁcation . Farenzena et al.
 proposed the Symmetry-Driven Accumulation of Local
Features (SDALF). They exploited the property of symmetry in pedestrian images and obtained good view invariance.
Ma et al. developed the BiCov descriptor, which combined the Gabor ﬁlters and the covariance descriptor to handle illumination change and background variations. Malocal et al. employed Fisher Vector to encode higher
order statistics of local features. All these methods focused
on feature design, but rich information from the distribution of samples in the dataset has not been fully exploited.
Our approach exploit the salience information among person images, and it can be generalized to take use of these
Several appoaches were developed to handle pose variations . Wang et al. proposed shape and
appearance context to model the spatial distributions of appearance relative to body parts in order to extract discriminative features robust to misalignment. Gheissari et al. 
ﬁt a triangluar graph model. Bak et al. and Cheng et
al. adopted part-based models to handle pose variation.
However, these appoaches are not ﬂexible enough and only
applicable when the pose estimators work accurately. Our
approach differs from them in that patch matching is employed to handle spatial misalignment.
Contextual visual knowledge coming from surrounding
people was used to enrich human signature . Liu et
al. used an attribute-based weighting scheme, which
shared similar spirit with our salience in ﬁnding the unique
and inherent appearance property. They clustered prototypes in an unsupervised manner, and learned attributebased feature importance for feature weighting. Their approach was based on global features. They weighted different types of features instead of local patches. Therefore
they could not pick up salient regions as shown in Figure 1.
Experimental results show that our deﬁned salience is much
more effective.
3. Dense Correspondence
Dense correpondence has been applied to face and scene
alignment . Inheriting the characteristics of partbased and region-based approaches, ﬁne-grained methods including optical ﬂow in pixel-level, keypoint feature
matching and local patch matching are often better choices
for more robust alignment. In our approach, considering
moderate resolution of human images captured by far-ﬁeld
surveillance cameras, we adopt the mid-level local patches
for matching persons. To ensure the robustness in matching,
local patches are densely sampled in each image. Different
than general patch matching approaches, a simple but effective horizontal constraint is imposed on searching matched
patches, which makes patch matching more adaptive in person re-identiﬁcation.
3.1. Feature Extraction
Dense Color Histogram. Each human image is densely
segmented into a grid of local patches. A LAB color histogram is extracted from each patch. To robustly capture
color information, LAB color histograms are also computed
on downsampled scales. For the purpose of combination
with other features, all the histograms are L2 normalized.
Dense SIFT. To handle viewpoint and illumination
change, SIFT descriptor is used as a complementary feature
to color histograms. The same as the setting of extracting
dense color histograms, a dense grid of patches are sampled
on each human image. We divide each patch into 4×4 cells,
quantize the orientations of local gradients into 8 bins, and
obtain a 4 × 4 × 8 = 128 dimentional SIFT feature. SIFT
features are also L2 normalized.
Dense color histograms and dense SIFT features are concatenated as the ﬁnal multi-dimensional descriptor vector
for each patch. In our experiment, the parameters of feature extraction are as follows: patches of size 10×10 pixels
are sampled on a dense grid with a grid step size 4; 32bin color histograms are computed in L, A, B channels respectively, and in each channel, 3 levels of downsampling
are used with scaling factors 0.5, 0.75 and 1; SIFT features
are also extracted in 3 color channels and thus produces a
128 × 3 feature vector for each patch. In a summary, each
patch is ﬁnally represented by a discriminative descriptor
vector with length 32 × 3 × 3 + 128 × 3 = 672. We denote
the combined feature vector as dColorSIFT.
3.2. Adjacency Constrained Search
In order to deal with misalignment, we conduct adjacency constrained search. dColorSIFT features in human
image are represented as xA,p
m,n, where (A, p) denotes the
p-th image in camera A, and (m, n) denotes the patch centered at the m-th row and the n-th column of image p. The
m-th row T of image p from camera A are represented as:
T A,p(m) = {xA,p
m,n|n = 1, 2, ..., N}.
All patches in T A,p(m) have the same search set S for patch
matching in image q from camera B:
m,n, xB,q) = T B,q(m), ∀xA,p
m,n ∈T A,p(m),
where xB,q represent the collection of all patch features in
image q from camera B. The S restricts the search set in
image q within the m-th row. However, bounding boxes
produced by a human detector are not always well aligned,
and also uncontrolled human pose variations exist in some
conditions. To cope with the spatial variations, we relax the
strict horizontal constraint to have a larger search range.
m,n, xB,q) = {T B,q(b)|b ∈N(m)},
m,n ∈T A,p(m),
where N(m) = {m −l, ..., m, ...m + l}, m −l ≥0 and
m+l ≤M. l deﬁnes the size of the relaxed adjacent vertical
space. If l is very small, a patch may not ﬁnd correct match
due to vertical misalignment. When l is set to be very large,
a patch in the upper body would ﬁnd a matched patch on the
legs. Thus less relaxed search space cannot well tolerate the
spatial variation while more relaxed search space increases
the chance of matching different body parts. l = 2 is chosen
in our setting.
Adjacency Searching.
Generalized patch matching is a
very mature technique in computer vision. Many off-theshelf methods are available to boost the performance
and efﬁciency.
In this work, we simply do a k-nearest
neighbor search for each xA,p
m,n in search set ˆS(xA,p
m,n, xB,q)
of every image in the reference set.
The search returns
the nearest neighbor for each image according to the Euclidean distance. As suggested in , aggregaing similarity scores is much more effective than minimizing accumulated distances, especially for those misaligned or background patches which could generate very large distances
during matching. By converting to similarity, their effect
could be reduced. We convert distance value to similarity
score with the Gaussian function:
s(x, y) = exp(−d(x, y)2
where d(x, y) = ∥x −y∥2 is the Euclidean distance between patch features x and y, and σ is the bandwidth of
Figure 2. Examples of adjacency search. (a) A test image from
the VIPeR dataset. Local patches are densely sampled, and ﬁve
exemplar patches on different body parts are shown in red boxes.
(b) One nearest neighbor from each reference image is returned
by adjacency search for each patch on the left, and then N nearest
neighbors from N reference images are sorted. The top ten nearest
neighbor patches are shown. Note that the ten nearest neighbors
are from ten different images.
the Gaussian function. Figure 2 shows some visually similar patches returned by the discriminative adjacency constrained search.
4. Unsupervised Salience Learning
With dense correpondence, we learn human salience
with unsupervised methods.
In this paper, we propose
two methods for learning human salience: the K-Nearest
Neighbor (KNN) and One-Class SVM (OCSVM).
4.1. K-Nearest Neighbor Salience
Byers et al. found the KNN distances can be used
for clutter removal. To apply the KNN distance to person
re-identiﬁcation, we search for the K-nearest neighbors
of a test patch in the output set of the dense correspondence.
With this strategy, salience is better adapted to
re-identiﬁcation problem.
Following the shared goal of
abnormality detection and salience detection, we redeﬁne
the salient patch in our task as follows:
Salience for person re-identiﬁcation: salient patches are
those possess uniqueness property among a speciﬁc set.
Denote the number of images in the reference set by Nr.
After building the dense correspondeces between a test image and images in reference set, the most similar patch in
every image of the reference set is returned for each test
patch, i.e., each test patch xA,p
m,n have Nr neighbors in set
m,n) ={x| argmax
m,n, ˆx), q = 1, 2, ..., Nr},
where ˆSp,q = ˆS(xA,p
m,n, xB,q) is the search set in Eq. (3),
and s is the similarity score function in Eq. (4).
Figure 3. Illustration of salient patch distribution.
patches are distributed far way from other pathes.
We apply a similar scheme in to Xnn(xA,p
m,n) of each
test patch, and the KNN distance is utilized to deﬁne the
salience score:
scoreknn(xA,p
m,n) = Dk(Xnn(xA,p
where Dk denotes the distance of the k-th nearest neighbor.
If the distribution of the reference set well relects the test
scenario, the salient patches can only ﬁnd limited number
(k = αNr) of visually similar neighbors, as shown in Figure 3(a), and then scoreknn(xA,p
m,n) is expected to be large.
0 < α < 1 is a proportion parameter relecting our expectation on the statistical distribution of salient patches. Since k
depends on the size of the reference set, the deﬁned salience
score works well even if the reference size is very large.
Choosing the Value of k.
The goal of salience detection for person re-identiﬁcatioin is to identify persons with
unique appearance. We assume that if a person has such
unique appearance, more than half of the people in the reference set are dissimilar with him/her. With this assumption, k = Nr/2 is used in our experiment. For seeking a
more principled method to compute human salience, oneclass SVM salience is discussion in Section 4.2.
To qualitatively compare with sophiscated supervised
learning methods, Figure 4(a) shows the feature weighting
map estimated by partial least square (PLS) . PLS is
used to reduce the dimensionality and the weights of the
ﬁrst projection vector are shown as the average of the feature weights in each block. Our results of unsupervised
KNN salience are show in Figure 4(b) on the ETHZ dataset
and 4(c) on the VIPeR dataset. Salience scores are assigned
to the center of patches, and the salience map is upsampled
for better visualization. Our unsupervised learning method
better captures the salient regions.
4.2. One-class SVM Salience
One-class SVM has been widely used for outlier
detection. Only positive samples are used in training. The
basic idea of one-class SVM is to use a hypersphere to describe data in the feature space and put most of the data into
the hypersphere. The problem is formulated into an objective function as follows:
R∈R,ξ∈Rl,c∈F R2 + 1
s.t.∥Φ(Xi) −c∥2 ≤R2 + ξi, ∀i ∈{1, ...l} : ξi ≥0,
where Φ(Xi) is the multi-dimensional feature vector of
training sample Xi, l is the number of training samples,
R and c are the radius and center of the hypersphere, and
v ∈ is a trade-off parameter. The goal of optimizing
the objective function is to keep the hypersphere as small
as possible and include most of the training data. The optimization problem can be solved in a dual form by QP optimization methods , and the decision function is:
f(X) = R2 −∥Φ(X) −c∥2,
∥Φ(X) −c∥2 = k(X, X) −2
αik(Xi, X)
αiαjk(Xi, Xj),
where αi and αj are the parameters for each constraint in
the dual problem. In our task, we use the radius basis function (RBF) K(X, Y ) = exp{−∥X−Y ∥2/2σ2} as kernel in
one-class SVM to deal with high-dimensional, non-linear,
multi-mode distributions.
As shown in , the decision
function of kernel one-class SVM can well capture the density and modality of feature distribution. To approximate
the KNN salience algorithm (Section 4.1) in a nonparametric form, the sailence score is re-deﬁned in terms of kernel
one-class SVM decision function:
scoreocsvm(xA,p
m,n) = d(xA,p
x∈Xnn(xA,p
where d is the Euclidean distance between patch features.
Our experiments show very similar results in person
re-identiﬁcation with the two salience detection methods.
scoreocsvm performs slightly better than scoreknn in some
circumstances.
5. Matching for re-identiﬁcation
Dense correspondence and salience described in Section
3 and 4 are used for person re-identiﬁcation.
5.1. Bi-directional Weighted Matching
A bi-directional weighted matching mechanism is designed to incorporate salience information into dense correspondence matching. First, we consider matching between
a pair of images. As mentioned in Section 4.1, patch xA,p
Figure 4. Qualitative comparison on salience. (a) shows the feature weighting maps estimated by partial least square . (b)
shows our KNN salience estimation. Red indicates large weights.
matched to xB,q within search range ˆSp,q = ˆS(xA,p
m,n, xB,q).
Denote the nearest neighbor produced by dense correspondence algorithm as
i,j = argmax
Then searching for the best matched image in the gallery
can be formulated as ﬁnding the maximal similarity score.
q∗= argmax
Sim(xA,p, xB,q),
where xA,p and xB,q are collection of patch features in
two images, i.e. xA,p = {xA,p
m,n}m∈M,n∈N , and xB,q =
i,j }m∈M,n∈N , and the similarity between two image
is computed with a bi-directional weighting mechanism illustrated in Figure 5. Intuitively, images of the same person would be more likely to have similar salience distributions than those of different persons. Thus, the difference in salience score can be used as a penalty to the similarity score. In another aspect, large salience scores are
used to enhance the similarity score of matched patches. Finally, we formulate the bi-directional weighting mechanism
as follows:
Sim(xA,p, xB,q) =
scoreknn(xA,p
m,n) · s(xA,p
i,j ) · scoreknn(xB,q
α + |scoreknn(xA,p
m,n) −scoreknn(xB,q
Figure 5. Illustration of bi-directional weighting for patch
matching. Patches in red boxes are matched in dense correspondence with the guidence of corresponding salience scores in dark
blue boxes.
where α is a parameter controlling the penalty of salience
difference.
One can also change the salience score to
scoreocsvm in a more principled framework without choosing the parameter k in Eq. (5).
5.2. Combination with existing approaches
Our approach is complementary to existing approaches.
In order to combine the similarity scores of existing approaches with the similarity score in Eq. (11), the distance
between two images can be computed as follows:
βi · di(fi(IA
p ), fi(IB
−βSDC · Sim(xA,p, xB,q),
where βi(> 0) is the weight for the ith distance measure
and βSDC(> 0) the weight for our approach. di and fi
correspond to the distance measures and features (wHSV
and MSCR) in . In the experiment, {βi} are chosen the
same as in . βSDC is ﬁxed as 1.
6. Experiments
We evaluated our approach on two publicly available
datasets, the VIPeR dataset , and the ETHZ dataset
These two datasets are the most widely used for
evaluation and reﬂect most of the challenges in real-world
person re-identiﬁcation applications, e.g., viewpoint, pose,
and illumination variation, low resolution, background
clutter, and occlusions. The results are show in standard
Cumulated Matching Characteristics (CMC) curve .
Comparisons to the state-of-the-art feature based methods
are provided, and we also show the comparison with some
classical metric learning algorithms.
VIPeR Dataset . The VIPeR dataset1 is captured by
two cameras in outdoor academic environment with two
images for each persons seen from different viewpoints.
1The VIPeR dataset is available to download at the website http:
//vision.soe.ucsc.edu/?q=node/178
It is one of the most challenging person re-identiﬁcation
datasets, which suffers from signiﬁcant viewpoint change,
pose variation, and illumination difference between two
camera views. It contains 632 pedestrian pairs, each pair
contains two images of the same individual seen from different viewpoints, one from CAM A and another from
CAM B. All images are normalized to 128 × 48 for experiments. CAM A captured images mainly from 0 degree
to 90 degree while CAM B mostly from 90 degree to 180
degree, and most of the image pairs show viewpoint change
larger than 90 degree.
Following the evaluation protocol in , we randomly
sample half of the dataset, i.e., 316 image pairs, for training
(however, the identity information is not used), and the remaining for test. In the ﬁrst round, images from CAM A are
used as probe and those from CAM B as gallery. Each probe
image is matched with every gallery image, and the correctly matched rank is obtained. Rank-k recognition rate is
the expectation of the matches at rank k, and the CMC curve
is the cumulated values of recognition rate at all ranks. After this round, the probe and gallery are switched. We take
the average of the two rounds of CMC curves as the result
of one trial. 10 trials of evaluation are repeated to achieve
stable statistics, and the average result is reported.
Since ELF ,
SDALF ,
and LDFV have
published their results on the VIPeR dataset, they are
used for comparison. The splitting assignments 2 in these
approaches are used in our experiments. Figure 6 report
the comparison results. It is observed that our two salience
detection based methods (SDC knn and SDC ocsvm)
outperform all the three benchmarking approaches. In particular, rank 1 matching rate is around 24% for SDC knn
and 25% for SDC ocsvm, versus 20% for SDALF, 15%
for LDFV, and 12% for ELF. The matching rate at rank 10
is around 52% for SDC knn, and 56% for SDC ocsvm,
versus 49% for SDALF, 48% for LDFV, and 44% for ELF.
The improvement is due to two aspects of our approach.
First, the dense correspondece matching can tolerate larger
extent of pose and appearance variations.
Second, we
incorporate human salience information to guide dense
correspondence. By combining with other descriptors, the
rank 1 matching rate of eSDC knn goes to 26.31% and
eSDC ocsvm goes to 26.74%.
This shows the complementarity of our SDC approach to other features.
comparison results are show in Table 1.
The compared
methods includes the classical metric learning approaches,
such as LMNN , and ITML , and their variants
modiﬁed for person re-identiﬁcation, such as PRDC ,
attribute PRDC (denoted as aPRDC) , and PCCA .
assignment
code at 
sdalf-descriptor/
SDALF 
eBiCov 
eLDFV 
eSDC ocsvm
VIPeR dataset: top ranked matching rates in [%] with
316 persons.
Cumulative Matching Characteristic (CMC)
Matching Rate (%)
eSDC_ocsvm
Figure 6. Performance on the VIPeR dataset.
Our approach:
SDC knn and SDC ocsvm. Our approach combined with wHSV
and MSCR : eSDC knn and eSDC ocsvm.
ETHZ Dataset . This dataset3 contains three video sequences captured from moving cameras. It contains a large
number of different people in uncontrolled conditions. With
these videos sequences, Schwartz, et al. extracted a set
of images for each people to test their Partial Least Square
method. Since the original video sequences are captured
from moving cameras, images have a range of variations in
human appearance and illumination, and some even suffer
from heavy occlusions. Following the settings in , all
image samples are normalized to 64 × 32 pixels, and the
dataset is structured as follows: SEQ.#1 contains 83 persons (4,857 images); SEQ.#2 contains 35 persons (1,936
images); SEQ.#3 contains 28 persons (1,762 images).
The same settings of experiments in are reproduced to make fair comparisons. Similar to them, we use
a single-shot evaluation strategy. For each person, one im-
3The ETHZ dataset is available to download at the website http://
homepages.dcc.ufmg.br/˜william/datasets.html
age is randomly selected to build gallery set while the rest
images form the probe set. Each image in probe is matched
to every gallery image and the correct matched rank is obtained. The whole procedure is repeated for 10 times, and
the average CMC curves are plotted in Figure 7.
As shown in Figure 7, our approach outperforms
the three benchmarking methods,
PLS, SDALF and
eBiCov on all three sequences. Comparisons with supervised learning methods PLS and RPLM are reported
in Table 2. On SEQ.#2 and SEQ.#3, our eSDC knn and
eSDC ocsvm outperforms all other methods. On SEQ.#1,
our SDC approach has better results than supervised methods, PLS and RPLM, and has comparable performance with
the recently proposed eLDFV .
7. Conclusion
In this work, we propose an unsupervised framework
with salience detection for person re-identiﬁcation. Patch
matching is utilized with adjacency constraint for handling
the viewpoint and pose variation. It shows great ﬂexibility
in matching across large viewpoint change. Human salience
is unsupervisedly learned to seek for discriminative and reliable patch matching. Experiments show that our unsupervised salience learning approach greatly improve the performance of person re-identiﬁcation.
8. Acknowledgement
This work is supported by the General Research Fund
sponsored by the Research Grants Council of Hong Kong
(Project No. CUHK 417110 and CUHK 417011) and National Natural Science Foundation of China (Project No.
61005057).