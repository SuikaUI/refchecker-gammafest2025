ORIGINAL RESEARCH ARTICLE
published: 07 June 2013
doi: 10.3389/fpsyg.2013.00313
PowerPlay: training an increasingly general problem
solver by continually searching for the simplest still
unsolvable problem
Jürgen Schmidhuber*
The Swiss AI Lab IDSIA, University of Lugano, SUPSI, Lugano, Switzerland
Edited by:
Gianluca Baldassarre, Italian National
Research Council, Italy
Reviewed by:
Georg Martius, Max Planck Institute
for Mathematics in the Sciences,
Vieri G. Santucci, Istituto di Scienze e
Tecnologie della
Cognizione – Consiglio Nazionale
delle Ricerche, Italy
*Correspondence:
Jürgen Schmidhuber, The Swiss AI
Lab IDSIA, University of Lugano and
SUPSI, Galleria 2, 6928 Manno,
Switzerland
e-mail: 
Most of computer science focuses on automatically solving given computational problems. I focus on automatically inventing or discovering problems in a way inspired by
the playful behavior of animals and humans, to train a more and more general problem
solver from scratch in an unsupervised fashion. Consider the inﬁnite set of all computable
descriptions of tasks with possibly computable solutions. Given a general problem-solving
architecture, at any given time, the novel algorithmic framework PowerPlay searches the space of possible pairs of new tasks and modiﬁcations of the
current problem solver, until it ﬁnds a more powerful problem solver that provably solves
all previously learned tasks plus the new one, while the unmodiﬁed predecessor does not.
Newly invented tasks may require to achieve a wow-effect by making previously learned
skills more efﬁcient such that they require less time and space. New skills may (partially)
re-use previously learned skills.The greedy search of typical PowerPlay variants uses timeoptimal program search to order candidate pairs of tasks and solver modiﬁcations by their
conditional computational (time and space) complexity, given the stored experience so far.
The new task and its corresponding task-solving skill are those ﬁrst found and validated.
This biases the search toward pairs that can be described compactly and validated quickly.
The computational costs of validating new tasks need not grow with task repertoire size.
Standard problem solver architectures of personal computers or neural networks tend to
generalize by solving numerous tasks outside the self-invented training set; PowerPlay’s
ongoing search for novelty keeps breaking the generalization abilities of its present solver.
This is related to Gödel’s sequence of increasingly powerful formal theories based on adding
formerly unprovable statements to the axioms without affecting previously provable theorems.The continually increasing repertoire of problem-solving procedures can be exploited
by a parallel search for solutions to additional externally posed tasks. PowerPlay may be
viewed as a greedy but practical implementation of basic principles of creativity . A ﬁrst experimental analysis can be found in separate papers .
Keywords: problem discovery, task invention, skill learning, general problem solver, intrinsic motivation, curiosity,
creativity
INTRODUCTION
Given a realistic piece of computational hardware with speciﬁc
resource limitations, how can one devise software for it that will
solve all, or at least many, of the a priori unknown tasks that
are in principle easily solvable on this architecture? In other
words, how to build a practical general problem solver, given
the computational restrictions? It does not need to be universal
and asymptotically optimal like the recent (not necessarily practically
feasible) general problem solvers discussed in Section 7.2; instead
it should take into account all constant architecture-speciﬁc slowdowns ignored in the asymptotic optimality notation of theoretical computer science, and be generally useful for real-world
applications.
Let us draw inspiration from biology. How do initially helpless human babies become rather general problem solvers over
time? Apparently by playing. For example, even in the absence of
external reward or hunger they are curious about what happens
if they move their eyes or ﬁngers in particular ways, creating little experiments which lead to initially novel and surprising but
eventually predictable sensory inputs, while also learning motor
skills to reproduce these outcomes. Infants continually seem to invent
new tasks that become boring as soon as their solutions become
known. Easy-to-learn new tasks are preferred over unsolvable or
hard-to-learn tasks. Eventually the numerous skills acquired in this
creative,self-supervised way may get re-used to facilitate the search
www.frontiersin.org
June 2013 | Volume 4 | Article 313 | 1
Schmidhuber
for solutions to external problems,such as ﬁnding food when hungry. While kids keep inventing new problems for themselves, they
move through remarkable developmental stages .
Here I introduce a novel unsupervised algorithmic framework
for training a computational problem solver from scratch, continually searching for the simplest (fastest to ﬁnd) combination
of task and corresponding task-solving skill to add to its growing repertoire, without forgetting any previous skills (Section 2),
or at least without decreasing average performance on previously
solved tasks (Section 6.1). New skills may (partially) re-use previously learned skills. Every new task added to the repertoire is
essentially deﬁned by the time required to invent it, to solve it,
and to demonstrate that no previously learned skills got lost. The
search takes into account that typical problem solvers may learn
to solve tasks outside the growing self-made training set due to
generalization properties of their architectures. The framework
is called PowerPlay because it continually aims
at boosting computational ability and problem-solving capacity, reminiscent of humans or human societies trying to boost
their general power/capabilities/knowledge/skills in playful ways,
even in the absence of externally deﬁned goals, although the skills
learned by this type of pure curiosity may later help to solve
externally posed tasks.
Unlike our ﬁrst implementations of curious/creative/playful
agents from the 1990s ), PowerPlay provably
(by design) does not have any problems with online learning – it
cannot forget previously learned skills, automatically segmenting
its life into a sequence of clearly identiﬁed tasks with explicitly
recorded solutions. Unlike the task search of theoretically optimal creative agents (Section 7.4),
PowerPlay’s task search is greedy, but at least practically feasible.
Some claim that scientists often invent appropriate problems
for their methods, rather than inventing methods to solve given
problems. The present paper formalizes this in a way that may be
more convenient to implement than those of our previous work
 , and describes a simple practical framework for building creative artiﬁcial scientists or
explorers that by design continually come up with the fastest to
ﬁnd, initially novel, but eventually solvable problems.
BASIC IDEAS
In traditional computer science, given some formally deﬁned task,
a search algorithm is used to search a space of solution candidates
until a solution to the task is found and veriﬁed. If the task is hard
the search may take long.
To automatically construct an increasingly general problem
solver, let us expand the traditional search space in an unusual
way, such that it includes all possible pairs of computable tasks
with possibly computable solutions, and problem solvers. Given
an old problem solver that can already solve a ﬁnite known set of
previously learned tasks, a search algorithm is used to ﬁnd a new
pair that provably has the following properties: (1) the new task
cannot be solved by the old problem solver. (2) The new task can
be solved by the new problem solver (some modiﬁcation of the old
one). (3) The new solver can still solve the known set of previously
learned tasks.
Once such a pair is found,the cycle repeats itself. This will result
in a continually growing set of known tasks solvable by an increasingly more powerful problem solver. Solutions to new tasks may
(partially) re-use solutions to previously learned tasks.
Smart search (e.g., Section 4.1 and Algorithm 4.1) orders candidate pairs of the type (task, solver) by computational complexity,
using concepts of optimal universal search , with a bias toward pairs that can be described by
few additional bits of information (given the experience so far)
and that can be validated quickly.
At ﬁrst glance it might seem harder to search for pairs of tasks
and solvers instead of solvers only, due to the apparently larger
search space. However, the additional freedom of inventing the
tasks to be solved may actually greatly reduce the time intervals between problem solver advances, because the system may
often have the option of inventing a rather simple task with an
easy-to-ﬁnd solution.
A new task may be about simplifying the old solver such that it
can still solve all tasks learned so far, but with less computational
resources such as time and storage space (e.g., Section 3.1 and
Algorithm 6.1).
Since the new pair (task, solver) is the ﬁrst one found and validated, the search automatically trades off the time-varying efforts
required to either invent completely new, previously unsolvable
problems, or compressing/speeding up previous solutions. Sometimes it is easier to reﬁne or simplify known skills, sometimes to
invent new skills.
On typical problem solver architectures of personal computers
(PCs) or neural networks (NNs), while a limited known number
of previously learned tasks has become solvable, so too has a large
number of unknown, never-tested tasks (in the ﬁeld of Machine
Learning, this is known as generalization). PowerPlay’s ongoing
search is continually testing (and always trying to go beyond) the
generalization abilities of the most recent solver instance; some of
its search time has to be spent on demonstrating that self-invented
new tasks are not already solvable.
Often, however, much more time will have to be spent on making sure that a newly modiﬁed solver did not forget any of the
possibly many previously learned skills. Problem solver modularization (Section 3.3, especially 3.3.2) may greatly reduce this time
though, making PowerPlay prefer pairs whose validation does
not require the re-testing of too many previously learned skills,
thus decomposing at least part of the search space into somewhat independent regions, realizing divide and conquer strategies
as by-products of its built-in drive to invent and validate novel
tasks/skills as quickly as possible.
A biologically inspired hope is that as the problem solver is
becoming more and more general, it will ﬁnd it easier and easier to solve externally posed tasks (Section 5), just like growing
infants often seem to re-use their playfully acquired skills to solve
teacher-given problems.
OUTLINE OF REMAINDER
Section 2 will introduce basic notation and Variant 1 of
the algorithmic framework PowerPlay, which invokes the
Frontiers in Psychology | Cognitive Science
June 2013 | Volume 4 | Article 313 | 2
Schmidhuber
essential procedures Task Invention, Solver Modiﬁcation,
and Correctness Demonstration. Section 3 will discuss details
of these procedures.
More detailed instantiations of PowerPlay will be described in
Section 4.3 (an evolutionary method, Algorithm 4.3) and Section
4.1 (an asymptotically optimal program search method,Algorithm
As mentioned above, the skills acquired to solve self-generated
tasks may later greatly facilitate solutions to externally posed tasks,
just like the numerous motor skills learned by babies during curious exploration of its world often can be re-used later to maximize
external reward. Sections 5 and 6.1 will discuss variants of the
framework (e.g., Algorithm 6.1) in which some of the tasks can be
deﬁned externally.
Section 6.1 will also describe a natural variant of the framework that explicitly penalizes solution costs (including time and
space complexity), and allows for forgetting aspects of previous
solutions, provided the average performance on previously solved
tasks does not decrease.
Section 7 will point to illustrative experiments (Section 7.8)
described in separate papers , and
discuss relations to previous work.
NOTATION AND ALGORITHMIC FRAMEWORK
POWERPLAY (VARIANT I)
B∗denotes the set of ﬁnite sequences or bitstrings over the binary
alphabet B = {0, 1}, λ the empty string, x, y, z, p, q, r, u strings in
B∗, N the natural numbers, R the real numbers, ϵ ∈R a positive
constant, m, n, n0, k, i, j, k, l non-negative integers, L(x) the number of bits in x (where L(λ) = 0), f, g functions mapping integers
to integers. We write f (n) = O(g(n)) if there exist positive c, n0
such that f(n) ≤cg(n) for all n > n0.
The computational architecture of the problem solver may be
a deterministic universal computer, or a more limited device such
as a ﬁnite state automaton or a feedforward neural network (NN)
 . All such problem solvers can be uniquely encoded
 or implemented on universal computers such as universal Turing Machines
(TM). Therefore, without loss of generality, the remainder of this
paper assumes a ﬁxed universal reference computer whose input
programs and outputs are elements of B∗. A user-deﬁned subset
S ⊂B∗deﬁnes the set of possible problem solvers. For example,
if the problem solver’s architecture is itself a binary universal TM
or a standard computer, then S represents its set of possible programs, or a limited subset thereof – compare Sections 3.2 and 4.1.
If it is a feedforward NN, then S could be a highly restricted subset
of programs encoding the NN’s possible topologies and weights
(ﬂoating point numbers) – compare Section 7.8 and the original
SLIM NN paper .
In what follows, for convenience I will often identify bitstrings
in B∗with things they encode,such as integers,real-valued vectors,
weight matrices, or programs – the context will always make clear
what is meant.
The problem solver’s initial program is called s0. There is a set
of possible task descriptions T ⊂B∗. T may be the inﬁnite set of
all possible computable descriptions of tasks with possibly computable solutions, or just a small subset thereof. For example, a
simple task may require the solver to answer a particular input
pattern with a particular output pattern (more formal details on
pattern recognition tasks are given in Section 3.1.1). Or it may
require the solver to steer a robot toward a goal through a sequence
of actions (more formal details on sequential decision-making
tasks in unknown environments are given in Section 3.1.2). There
is a particular sequence of task descriptions T 1,T 2,. . .,where each
unique Ti ∈T (i = 1, 2, . . .) is chosen or “invented” by a search
method described below such that the solutions of T 1, T 2, . . ., Ti
can be computed by si, the i-th instance of the program, but not
by si−1 (i = 1, 2, . . .). Each Ti consists of a unique problem identi-
ﬁer that can be read by si through some built-in input processing
mechanism ),
and a unique description of a deterministic procedure for determining whether the problem has been solved. Denote T ≤i = {T 1,
. . ., Ti}; T <i = {T 1, . . ., Ti−1}.
A valid task Ti(i > 1) may require solving at least one previously
solved task Tk(k < i) more efﬁciently, by using less resources such
as storage space, computation time, energy, etc., thus achieving a
wow-effect. See Section 3.1.
Tasks and problem solver modiﬁcations are computed and validated by elements of another appropriate set of programs P ⊂B*.
Programs p ∈P may contain instructions for reading and executing (parts of) the code of the present problem solver and reading
(parts of) a recorded history Trace ∈B∗of previous events that
led to the present solver. The algorithmic framework (Algorithm
2) incrementally trains the problem solver by ﬁnding p ∈P that
increase the set of solvable tasks.
TASK INVENTION, SOLVER MODIFICATION,
CORRECTNESS DEMO
A program tested byAlgorithm 2 has to allocate its runtime to solve
three main jobs,namely,Task Invention,Solver Modiﬁcation,
Correctness Demonstration. Now examples of each will be
IMPLEMENTING TASK INVENTION
Part of the job of pi ∈P is to compute Ti ∈T . This will consume
some of the total computation time allocated to pi. Two examples
will be given: pattern recognition tasks are treated in Section 3.1.1;
sequential decision-making tasks in Section 3.1.2.
Example: pattern recognition tasks
In the context of learning to recognize or analyze patterns,Ti could
be a 4-tuple (Ii, Oi, ti, ni) ∈I × O × N × N, where I, O ⊂B∗,
and Ti is solved if si satisﬁes L(si) < ni and needs at most ti discrete time steps to read Ii and compute Oi and halt. Here Ii itself
may be a pair (I 1
i ) ∈B∗× B∗, where I1
i is constrained to be
the address of an image in a given database of patterns, and I2
is a pi-generated “query” that uniquely speciﬁes how the image
should be classiﬁed through target pattern Oi, such that the same
image can be analyzed in different ways during different tasks. For
example, depending on the nature of the invented task sequence,
the problem solver could eventually learn that O = 1 if I 2 = 1001
(suppressing task indices) and the image addressed by I 1 contains
at least one black pixel, or if I 2 = 0111 and the image shows a cow.
Since the deﬁnition of task Ti includes bounds ni, ti on computational resources, Ti may be about solving at least one Tk(k < i)
www.frontiersin.org
June 2013 | Volume 4 | Article 313 | 3
Schmidhuber
Algorithm 2: Algorithmic Framework PowerPlay (Variant I)
Initialize s0 in some way.
for i: = 1, 2, …do
Let a search algorithm (examples in Section 4) create a new candidate program p ∈P.
Give p limited time to do (not necessarily in this order):
∗Task Invention: Let p compute a task T ∈T . See Section 3.1.
∗Solver Modiﬁcation: Let p compute a value of the variable q ∈S ⊂B∗(a candidate for si)
by computing a modiﬁcation of si−1. See Section 3.2.
∗Correctness Demonstration: Let p try to show that T cannot be solved by si−1, but that
T and all Tk(k < i) can be solved by q. See Section 3.3.
until Correctness Demonstration was successful
Set pi: = p;Ti: = T;si: = q; update Trace.
more efﬁciently, corresponding to a wow-effect. This in turn may
also yield more efﬁcient solutions to other tasks Tl(l < i, l ̸= k).
In practical applications one may insist that such efﬁciency gains
must exceed a certain threshold ϵ > 0, to avoid task series causing
sequences of very minor improvements.
Note that ni and ti may be unnecessary in special cases such
as the problem solver being a ﬁxed topology feedforward NN
 whose input and target patterns have constant size
and whose computational efforts per pattern need constant time
and space resources.
Assuming sufﬁciently powerful S, P, in the beginning, trivial
tasks such as simply copying I 2
i onto Oi may be interesting in the
sense that PowerPlay can still validate and accept them, but they
will become boring (inadmissible) as soon as they are solvable by
solutions to previous tasks that generalize to new tasks.
Example: general decision-making tasks in dynamic
environments
In the more general context of general problem solving/sequential
decision making/reinforcement learning/reward optimization
 in unknown environments, there may be a set I ⊂B∗of
possible task identiﬁcation patterns and a set J ⊂B∗of programs
that test properties of bitstrings. Ti could then encode a 4-tuple (Ii,
Ji, ti, ni) ∈I × J × N × N of ﬁnite bitstrings with the following
interpretation: si must satisfy L(si) < ni and may spend at most ti
discrete time steps on ﬁrst reading Ii and then interacting with an
environment through a sequence of perceptions and actions, to
achieve some computable goal deﬁned by Ji.
More precisely, while Ti is being solved within ti time steps, at
any given time 1 ≤t ≤ti, the internal state of the problem solver
at time t is denoted ui(t) ∈B∗; its initial default value is ui(0).
For example, ui(t) may encode the current contents of the internal tape of a TM, or of certain addresses in the dynamic storage
area of a PC, or the present activations of an LSTM recurrent
NN . At time t, si can spend
a constant number of elementary computational instructions to
copy the task description Ti or the present environmental input
xi(t) ∈B∗and a reward signal ri(t) ∈B∗(interpreted as a real number) into parts of ui(t),then update other parts of ui(t) (a function
of ui(t −1)) and compute action yi(t) ∈B∗encoded as a part of
ui(t). yi(t) may affect the environment, and thus future inputs.
If P allows for programs that can dynamically acquire additional physical computational resources such as additional CPUs
and storage, then the above constant number of elementary computational instructions should be replaced by a constant amount
of real time, to be measured by a reliable physical clock.
The sequence of 4-tuples (xi(t), ri(t), ui(t), yi(t)) (t = 1, . . ., ti)
gets recorded by the so-called trace Tracei ∈B∗. If at the end of the
interaction a desirable computable property Ji(Tracei) (computed
by applying program Ji to Tracei) is satisﬁed, then by deﬁnition the
task is solved. The set J of possible Ji may represent an inﬁnite
set of all computable tasks with solutions computable by the given
hardware. For practical reasons, however, the set J of possible Ji
may also be restricted to bit sequences encoding just a few possible
goals. For example, Ji may only encode goals of the form: a robot
arm steered by program or “policy” si has reached a certain target (a desired ﬁnal observation xi(ti) recorded in Tracei) without
measurably bumping into an obstacle along the way, that is, there
were no negative rewards, that is, ri(τ) ≥0 for τ = 1 . . . ti.
If the environment is deterministic, e.g., a digital physics
simulation of a robot, then its current state can be encoded
as part of u(t), and it is straight-forward for Correctness
Demonstration totestwhethersomesi stillcansolveapreviously
solved task Tj(j < i). However, what if the environment is only partially observable, like the real world, and non-stationary, changing in
unknown ways? Then Correctness Demonstration must check
whether si still produces the same action sequence in response to
the input sequence recorded in Tracej (often this replay-based test
will actually be computationally cheaper than a test involving the
environment). Achieving the same goal in a changed environment
must be considered a different task, even if the changes are just due to
noise on the environmental inputs. (Sure, in the real world sj(j > i)
might actually achieve Ji faster than si, given the description of
Ti, but Correctness Demonstration in general cannot know
whether this acceleration was due to plain luck – it must stick to
reproducing Tracej to make sure it did not forget anything.)
See Section 6.2, however, for a less strict PowerPlay variant
whose Correctness Demonstration directly interacts with the
real world to collect sufﬁcient problem-solving statistics through
Frontiers in Psychology | Cognitive Science
June 2013 | Volume 4 | Article 313 | 4
Schmidhuber
repeatedtrials,makingcertainassumptionsabouttheprobabilistic
nature of the environment, and the repeatability of experiments.
IMPLEMENTING SOLVER MODIFICATION
Part of the job of pi ∈P is also to compute si, possibly proﬁting
from having access to si−1, because only few changes of si−1 may
be necessary to come up with an si that goes beyond si−1. For
example, if the problem solver is a standard PC, then just a few
bits of the previous software si−1 may need to be changed.
For practical reasons, the set S of possible si may be greatly
restricted to bit sequences encoding programs that obey the syntax
of a standard programing language such as LISP or Java. In turn,
the programing language describing P should be greatly restricted
such that any pi ∈P can only produce syntactically correct si.
If the problem solver is a feedforward NN with pre-wired,
unmodiﬁable topology, then S will be restricted to those bit
sequences encoding valid weight matrices, si will encode its ith weight matrix, and P will be restricted to those p ∈P that
can produce some si ∈S. Depending on the user-deﬁned programing language, pi may invoke complex pre-wired subprograms
(e.g., well-known learning algorithms) as primitive instructions –
compare separate experimental analysis an axiomatic system A that formally
describes computational properties of the problem solver and possible si, and to allow pi to search the space of possible proofs
derivable from A, using a proof searcher subroutine that systematically generates proofs until it ﬁnds a theorem stating that
si but not si−1 solves T 1, T 2, . . ., Ti (proof search may achieve
this efﬁciently without explicitly re-testing si on T 1, T 2, . . ., Ti).
This could be done like in the Gödel Machine (Section 7.2), which uses an online extension of Universal Search to systematically test proof techniques:
proof-generating programs that may invoke special instructions
for generating axioms and applying inference rules to prolong an
initially empty proof ∈B∗by theorems, which are either axioms
or inferred from previous theorems through rules such as modus
ponens combined with uniﬁcation, e.g., . P can be
easily limited to programs generating only syntactically correct
proofs . A has to subsume axioms describing
how any instruction invoked by some s ∈S will change the state u
of the problem solver from one step to the next (such that proof
techniques can reason about the effects of any si). Other axioms
encode knowledge about arithmetics etc (such that proof techniques can reason about spatial and temporal resources consumed
In what follows, Correctness Demonstrations will be discussed that are less general but sometimes more convenient to
implement.
Keeping track which components of the solver affect which
Often it is possible to partition s ∈S into components, such as
individual bits of the software of a PC, or weights of a NN. Here
the k-th component of s is denoted sk. For each k (k = 1, 2, . . .)
a variable list Lk = (T k
2 , . . .) is introduced. Its initial value
before the start of PowerPlay is Lk
0, an empty list. Whenever pi
found si and Ti at the end of Correctness Demonstration,each
Lk is updated as follows: its new value Lk
i is obtained by appending to Lk
i−1 those Tj /∈Lk
i−1(j = 1, . . . , i) whose current (possibly
revised) solutions now need sk at least once during the solutioncomputing process, and deleting those Tj whose current solutions
do not use sk any more.
PowerPlay’s Correctness Demonstration thus has to test
only tasks in the union of all Lk
i . That is, if the most recent task
does not require changes of many components of s, and if the
changed bits do not affect many previous tasks,then Correctness
Demonstration may be very efﬁcient.
Since every new task added to the repertoire is essentially
deﬁnedbythetimerequiredtoinventit,tosolveit,andtoshowthat
no previous tasks became unsolvable in the process,PowerPlay is
generally“motivated”to invent tasks whose validity check does not
require too much computational effort. That is, PowerPlay will
often ﬁnd pi that generate si−1-modiﬁcations that don’t affect too
many previous tasks, thus decomposing at least part of the spaces
of tasks and their solutions into more or less independent regions,
realizing divide and conquer strategies as by-products. Compare a
recent experimental analysis of this effect By
restricting S to self-delimiting preﬁx codes like those generated
by the Optimal Ordered Problem Solver (OOPS) ,one can now proﬁt from a sometimes particularly efﬁcient
type of Correctness Demonstration, ensuring that differences
between si and si−1 cannot affect solutions to T <i under certain
conditions. More precisely, to obtain si, half the search time is
spent on trying to process Ti ﬁrst by si−1, extending or prolonging si−1 only when the ongoing computation requests to add new
components through special instructions –
then Correctness Demonstration has less to do as the set
T <i is guaranteed to remain solvable, by induction. The other
half of the time is spent on processing Ti by a new sub-program
with new components s′
i, a part of si but not of si−1, where s′
may read si−1 or invoke parts of si−1 as sub-programs to solve
T ≤i – only then Correctness Demonstration has to test si
www.frontiersin.org
June 2013 | Volume 4 | Article 313 | 5
Schmidhuber
not only on Ti but also on T <i for
A simple but not very general way of doing something similar is to interleave Task Invention, Solver Modiﬁcation,
Correctness Demonstration as follows: restrict all p ∈P such
that they must deﬁne Ii: = i as the unique task identiﬁer Ii for Ti
(see Section 3.1.2); restrict all s ∈S such that the input of Ii = i
automatically invokes sub-program s′
i, a part of si but not of si−1
(although s′
i may read si−1 or invoke parts of si−1 as sub-programs
to solve Ti). Restrict Ji to a subset of acceptable computational
outcomes (Section 3.1.2). Run si until it halts and has computed a
novel output acceptable by Ji that is different from all outputs computed by the (halting) solutions to T <i; this novel output becomes
Ti ’s goal. By induction over i,since all previously used components
of si−1 remain unmodiﬁed, the set T <i is guaranteed to remain
solvable, no matter s′
i. That is, Correctness Demonstration
on previous tasks becomes trivial. However, in this simple setup
there is no immediate generalization across tasks like in OOPS
 and the previous paragraph: the trivial
task identiﬁer i will always ﬁrst invoke some s′
i different from
k(k ̸= i), instead of allowing for solving a new task solely by
previously found code.
IMPLEMENTATIONS OF POWERPLAY
PowerPlay is a general framework that allows for plugging
in many differents search and learning algorithms. The present
section will discuss some of them.
IMPLEMENTATION BASED ON OPTIMAL ORDERED PROBLEM
SOLVER OOPS
The i-th problem is to ﬁnd a program pi ∈P that creates si and
Ti and demonstrates that si but not si−1 can solve T 1, T 2, . . ., Ti.
This yields a perfectly ordered problem sequence for a variant of
the Optimal Ordered Problem Solver OOPS 
(Algorithm 4.1).
While a candidate program p ∈P is executed, at any given discrete time step t = 1,2,. . .,its internal state or dynamical storage U
at time t is denoted U(t) ∈B∗(not to be confused with the solver’s
internal state u(t) of Section 3.1.2). Its initial default value is U(0).
E.g., U(t) could encode the current contents of the internal tape
of a TM (to be modiﬁed by p), or of certain cells in the dynamic
storage area of a PC.
Once pi is found,pi,si,Ti,Tracei (if applicable;see Section 3.1.2)
will be saved in unmodiﬁable read-only storage, possibly together
with other data observed during the search so far. This may greatly
facilitate the search for pk, k > i, since pk may contain instructions
for addressing and reading pj, sj, Tj, Tracej(j = 1, . . ., k −1) and
for copying the read code into modiﬁable storage U, where pk may
further edit the code, and execute the result, which may be a useful
subprogram .
Deﬁne a probability distribution P(p) on P to represent the
searcher’s initial bias ). P could be based on program length, e.g.,
P(p) = 2−L(p),or on a probabilistic syntax diagram . See Algorithm 4.1.
OOPS keeps doubling the time limit until there is sufﬁcient
runtime for a sufﬁciently likely program to compute a novel,
previously unsolvable task, plus its solver, which provably does
not forget previous solutions. OOPS allocates time to programs
according to an asymptotically optimal universal search method
 for problems with easily veriﬁable solutions, that is,
solutions whose validity can be quickly tested. Given some problem class, if some unknown optimal program p requires f (k) steps
to solve a problem instance of size k and demonstrate the correctness of the result, then this search method will need at most
O(f (k)/P(p)) = O(f (k)) steps – the constant factor 1/P(p) may be
large but does not depend on k. Since OOPS may re-use previously
generated solutions and solution-computing programs, however,
it may be possible to greatly reduce the constant factor associated
with plain universal search .
The big difference to previous implementations of OOPS is that
PowerPlay has the additional freedom to deﬁne its own tasks. As
always,every new task added to the repertoire is essentially deﬁned
by the time required to invent it, to solve it, and to demonstrate
that no previously learned skills got lost.
Building on existing OOPS source code
Existing OOPS source code uses a FORTHlike universal programing language to deﬁne P. It already contains
a framework for testing new code on previously solved tasks, and
forefﬁcientlyundoingallU-modiﬁcationsof eachtestedprogram.
The source code requires few changes to implement the additional
task search described above.
Alternative problem solvers based on recurrent neural
Recurrent NNs ) are general computers that allow
for both sequential and parallel computations, unlike the strictly
sequential FORTH-like language of Section 4.1.1. They can compute any function computable by a standard PC . The original PowerPlay report used
a fully connected RNN called RNN1 to deﬁne S, where wlk is
the real-valued weight on the directed connection between the lth and k-th neuron. To program RNN1 means to set the weight
matrix s = ⟨wlk⟩. Given enough neurons with appropriate activation functions and an appropriate ⟨wlk⟩, Algorithm 4.1 can be
used to train s. P may itself be the set of weight matrices of
a separate RNN called RNN2, computing tasks for RNN1, and
modiﬁcations of RNN1, using techniques for network-modifying
networks as described in previous work , a particularly suited NN called a self-delimiting NN or SLIM NN
 is used. During program execution or activation spreading in the SLIM NN, lists are used to trace only those
neurons and connections used at least once. This also allows for
efﬁcient resets of large NNs which may use only a small fraction of
their weights per task. Unlike standard RNNs, SLIM NNs are easily combined with techniques of asymptotically optimal program
search (Section 4.1). To address overﬁtting, instead of depending
on pre-wired regularizers and hyper-parameters ,
Frontiers in Psychology | Cognitive Science
June 2013 | Volume 4 | Article 313 | 6
Schmidhuber
Algorithm 4.1: Implementing PowerPlay with Procedure OOPS 
(see text for details) - initialize s0 and u (internal dynamic storage for s ∈S) and U (internal dynamic storage for p ∈P),
where each possible p is a sequence of subprograms p’, p”, p”’.
for i:= 1, 2, …do
set variable time limit tlim:= 1;
let the variable set H be empty;
set Boolean variable DONE: = FALSE
if H is empty then
set tlim := 2tlim; H := {p ∈P: P(p)tlim ≥1}
choose and remove some p from H
while not DONE and less than P(p)tlim time was spent on p do
execute the next time step of the following computation:
1. Let p’ compute some task T ∈T and halt.
2. Let p” compute q ∈S by modifying a copy of si−1, and halt.
3. Let p”’ try to show that q but not si-1 can solve T 1, T 2, …, Ti-1, T.
If p”’ was successful set DONE:=TRUE.
Undo all modiﬁcations of u and U due to p. This does not cost more time than executing p in the
while loop above .
until DONE
set pi:=p; Ti:=T; si:=q;
add a unique encoding of the 5-tuple (i, pi, si, Ti, Tracei) to read-only storage
readable by programs to be tested in the future.
SLIM NNs can in principle learn to select by themselves their own
runtime and their own numbers of free parameters, becoming
fast and slim when necessary. Efﬁcient SLIM NN learning algorithms (LAs) track which weights are used for which tasks (Section
3.3.2), to greatly speed up performance evaluations in response to
limited weight changes. LAs may penalize the task-speciﬁc total
length of connections used by SLIM NNs implemented on the 3dimensional brain-like multi-processor hardware to be expected
in the future. This encourages SLIM NNs to solve many subtasks by subsets of neurons that are physically close . This may be justiﬁed to
the extent that future successful programs turn out to be similar
to previous ones.
IMPLEMENTATION BASED ON STOCHASTIC OR EVOLUTIONARY
A possibly simpler but less general approach is to use an evolutionary algorithm to produce an s-modifying and task-generating
program p as requested by PowerPlay, according to Algorithm
4.3, which refers to the recurrent net problem solver of Section
ADDING EXTERNAL TASKS
The growing repertoire of the problem solver may facilitate learning of solutions to externally posed tasks. For example, one may
modify PowerPlay such that for certain i, Ti is deﬁned externally, instead of being invented by the system itself. In general,
the resulting si will contain an externally inserted bias in form
of code that will make some future self-generated tasks easier to
ﬁnd than others. It should be possible to push the system in a
human-understandable or otherwise useful direction by regularly
inserting appropriate external goals. See Algorithm 6.1.
Another way of exploiting the growing repertoire is to simply copy si for some I and use it as a starting point for a search
for a solution to an externally posed task T, without insisting
that the modiﬁed si also can solve T 1, T 2, . . ., Ti. This may be
much faster than trying to solve T from scratch, to the extent the
solutions to self-generated tasks reﬂect general knowledge (code)
re-usable for T.
In general, however, it will be possible to design external
tasks whose solutions do not proﬁt from those of self-generated
tasks – the latter even may turn out to slow down the search.
On the other hand, in the real world the beneﬁts of curious exploration seem obvious. One should analyze theoretically
and experimentally under which conditions the creation of selfgenerated tasks can accelerate the solution to externally generated
tasks – see for previous simple experimental studies in this vein.
www.frontiersin.org
June 2013 | Volume 4 | Article 313 | 7
Schmidhuber
Algorithm 4.3: PowerPlay for RNNs Using Stochastic or Evolutionary Search
Randomly initialize RNN1’s variable weight matrix ⟨wlk⟩and use the result as s0 (see Section 4.1.2)
for i:= 1, 2, …do
set Boolean variable DONE = FALSE
use a black box optimization algorithm BBOA ) with adaptive parameter vector θ to create some T ∈T (to deﬁne the task input to RNN1; see Section 3.1)
and a modiﬁcation of si−1, the current ⟨wlk⟩of RNN1, thus obtaining a new candidate q ∈S
if q but not si−1 can solve T and all Tk(k < i) (see Sections 3.3, 3.3.2) then
set DONE = TRUE
until DONE
set si:= q; ⟨wlk⟩:= q; Ti:= T; (also store Tracei if applicable, see Section 3.1.2). Use the information stored so far to adapt the
parameters θ of the BBOA, e.g., by gradient-based search , or according to the principles of
evolutionary computation .
SELF-REFERENCE THROUGH NOVEL TASK SEARCH AS AN
EXTERNAL TASK
PowerPlay’s i-th goal is to ﬁnd a pi ∈P that creates Ti and si
(a modiﬁcation of si−1) and shows that si but not si−1 can solve
T ≤i. As s itself is becoming a more and more general problem
solver, s may help in many ways to achieve such goals in selfreferential fashion. For example, the old solver si−1 may be able to
read a unique formal description (provided by pi) of PowerPlay’s
i-th goal, viewing it as an external task, and produce an output
unambiguously describing a candidate for (Ti, si). If s has a theorem prover component (Section 3.3.1), si−1 might even output
a full proof of (Ti, si)’s validity; alternatively pi could just use the
possibly suboptimal suggestions of si−1 to narrow down and speed
up the search. This is one of the reasons why Section 2 already
mentioned that programs p ∈P should contain instructions for
reading (and running) the code of the present problem solver.
SOFTENING TASK ACCEPTANCE CRITERIA OF
The PowerPlay variants above insist that s may not solve new
tasks at the expense of forgetting to solve any previously solved
task within its previously established time and space bounds. For
example,let us consider the sequential decision-making tasks from
Section 3.1.2. Suppose the problem solver can already solve task
Tk = (Ik,Jk,tk,nk) ∈I × J × N × N.A very similar but admissible
new task Ti = (Ik, Jk, ti, nk), (i > k), would be to solve Tk substantially faster: ti < tk – ϵ, as long as Ti is not already solvable by si−1,
and no solution to some Tl(l < i) is forgotten in the process.
Here I discuss variants of PowerPlay that soften the acceptance criteria for new tasks in various ways, for example, by
allowing some of the computations of solutions to previous nonexternal (Section 5) tasks to slow down by a certain amount of
time, provided the sum of their runtimes does not increase. This
also permits the system to invent new previously unsolved tasks at
the expense of slightly increasing time bounds for certain already
solved non-external tasks, but without decreasing the average performance on the latter. Of course, PowerPlay has to be modiﬁed
accordingly, updating average runtime bounds when necessary.
Alternatively, one may allow for trading off space and time
constraints in reasonable ways, e.g., in the style of asymptotically
optimal Universal Search , which essentially trades
one bit of additional space complexity for a runtime speedup
factor of 2.
POWERPLAY VARIANT II: EXPLICITLY PENALIZING TIME AND
SPACE COMPLEXITY
Let us remove time and space bounds from the task deﬁnitions of
Section 3.1.2, since the modiﬁed cost-based PowerPlay framework below (Algorithm 6.1) will handle computational costs (such
as time and space complexity of solutions) more directly. In the
present section, Ti encodes a tuple (Ii, Ji) ∈I × J with interpretation: si must ﬁrst read Ii and then interact with an environment
through a sequence of perceptions and actions, to achieve some
computable goal deﬁned by Ji within a certain maximal time
interval tmax (a positive constant). Let t ′
s(T) be tmax if s cannot
solve task T, otherwise it is the time needed to solve T by s. Let
s(T) be the positive constant lmax if s cannot solve T, otherwise
it is the number of components of s needed to solve task T by
s. The non-negative real-valued reward r(T) for solving T is a
positive constant rnew for self-deﬁned previously unsolvable T,
or user-deﬁned if T is an external task solved by s (Section 5).
The real-valued cost Cost(s, TSET) of solving all tasks in a task
set TSET through s is a real-valued function of: all l′
(for all T ∈TSET), L(s), and 6T∈TSET r(T). For example, the
cost function Cost(s, TSET) = L(s) + α P
T∈TSET [t ′
s(T) −r(T)]
encourages compact and fast solvers solving many different tasks
with the same components of s, where the real-valued positive
parameter α weighs space costs against time costs, and rnew should
exceed tmax to encourage solutions of novel self-generated tasks,
whose cost contributions should be below zero (alternative cost
deﬁnitions could also take into account energy consumption etc.).
Let us keep an analog of the remaining notation of Section
3.1.2, such as ui(t), xi(t), ri(t), yi(t), Tracei, Ji(Tracei). As always,
if the environment is unknown and possibly changing over time,
to test performance of a new solver s on a previous task Tk, only
Tracek is necessary – see Section 3.1.2. As always,let T ≤i denote the
Frontiers in Psychology | Cognitive Science
June 2013 | Volume 4 | Article 313 | 8
Schmidhuber
set containing all tasks T 1, . . ., Ti (note that if Ti = Tk for some
k < i then it will appear only once in T ≤i), and let ϵ > 0 again
deﬁne what is acceptable progress:
ByAlgorithm 6.1,si may forget certain abilities of si−1,provided
that the overall performance as measured by Cost(si, T ≤i) has
improved, either because a new task became solvable, or previous
tasks became solvable more efﬁciently.
Following Section 3.3, Correctness Demonstration can
often be facilitated, for example, by tracking which components of
si are used for solving which tasks (Section 3.3.2).
To further reﬁne this approach, consider that in phase i, the
i (deﬁned in Section 3.3.2) contains all previously learned
tasks whose solutions depend on sk. This can be used to determine the current value Val(sk
i ) of some component sk of s:
i Cost(si, T≤i). It is a simple exercise to invent
PowerPlay variants that do not forget valuable components as
easily as less valuable ones.
The implementations of Sections 4.1 and 4.3 are easily adapted
to the cost-based PowerPlay framework. Compare separate
papers .
PROBABILISTIC POWERPLAY VARIANTS
observable
and/or non-stationary unknown environments Correctness
Demonstration must use Tracek to check whether a new si still
knows how to solve an earlier task Tk(k < i). A less strict variant of
PowerPlay,however,willsimply makecertainassumptionsabout
the probabilistic nature of the environment and the repeatability
of trials, assuming that a limited ﬁxed number of interactions
with the real world are sufﬁcient to estimate the costs c∗
Algorithm 6.1.
Another probabilistic way of softening PowerPlay is to add
new tasks without proof that s won’t forget solutions to previous
tasks, provided Correctness Demonstration can at least show
that the probability of forgetting any previous solution is below
some real-valued positive constant threshold.
DISCUSSION
Here I brieﬂy mention illustrative experiments described in detail
elsewhere and discuss certain
aspects and limitations of PowerPlay. I also discuss related
research, in particular, why the present work is of interest despite
the recent advent of theoretically optimal universal problem
solvers (Section 7.2), and how it can be viewed as a greedy but feasible and sound implementation of the formal theory of creativity
(Section 7.4).
OUTGROWING TRIVIAL TASKS – COMPRESSING PREVIOUS
What prevents PowerPlay from inventing trivial tasks forever by
extreme modularization, simply allocating a previously unused
solver part to each new task, which thus becomes rather quickly
veriﬁable, as its solution does not affect solutions to previous tasks
(Section 3.3.3)? On realistic but general architectures such as PCs
and RNNs, at least once the upper storage size limit of s is reached,
PowerPlay will start “compressing” previous solutions, making s
generalize in the sense that the same relatively short piece of code
(some part of s) helps to solve different tasks.
With many computational architectures, this type of compression will start much earlier though, because new tasks solvable by
partial reuse of earlier discovered code will often be easier to ﬁnd
than new tasks solvable by previously unused parts of s. This also
holds for growing architectures with potentially unlimited storage
Compare also PowerPlay Variant II of Section 6.1 whose
tasks may explicitly require improving the average time and space
complexity of previous solutions by some minimal value.
In general, however, over time the system will ﬁnd it more and
more difﬁcult to invent novel tasks without forgetting previous
solutions, a bit like humans ﬁnd it harder and harder to learn
truly novel behaviors once they are leaving behind the initial rapid
exploration phase typical for babies. Experiments with various
problem solver architectures )
help to analyze such effects in detail.
RELATION TO THEORETICALLY OPTIMAL UNIVERSAL PROBLEM
The new millenium brought universal problem solvers that are
theoretically optimal in a certain sense. The fully self-referential
 Gödel machine may
interact with some initially unknown, partially observable environment to maximize future expected utility or reward by solving
arbitrary user-deﬁned computational tasks. Its initial algorithm
is not hardwired; it can completely rewrite itself without essential limits apart from the limits of computability, but only if a
proof searcher embedded within the initial algorithm can ﬁrst
prove that the rewrite is useful, according to the formalized utility
function taking into account the limited computational resources.
Self-rewrites due to this approach can be shown to be globally
optimal, relative to Gödel’s well-known fundamental restrictions
of provability . To make sure the Gödel machine
is at least asymptotically optimal even before the ﬁrst self-rewrite,
one may initialize it by Hutter’s non-self-referential but asymptotically fastest algorithm for all well-deﬁned problems Hsearch , which uses a hardwired brute force proof searcher and
ignores the costs of proof search. Assuming discrete input/output
domains X/Y ⊂B∗, a formal problem speciﬁcation f: X →Y (say,
a functional description of how integers are decomposed into their
prime factors), and a particular x ∈X (say, an integer to be factorized),Hsearch orders all proofs of an appropriate axiomatic system
by size to ﬁnd programs q that for all z ∈X provably compute f(z)
within time bound tq(z). Simultaneously it spends most of its time
onexecutingtheq withthebestcurrentlyproventimeboundtq(x).
Hsearch is as fast as the fastest algorithm that provably computes
f(z) for all z ∈X,save for a constant factor smaller than 1 + ϵ (arbitrarilysmallreal-valuedϵ > 0)andanf-speciﬁcbutx-independent
additive constant . Given some problem, the Gödel
machine may decide to replace Hsearch by a faster method suffering less from large constant overhead, but even if it doesn’t, its
performance won’t be less than asymptotically optimal.
Why doesn’t everybody use such universal problem solvers
for all computational real-world problems? Because most realworld problems are so small that the ominous constant slowdowns
www.frontiersin.org
June 2013 | Volume 4 | Article 313 | 9
Schmidhuber
Algorithm 6.1: PowerPlay Framework (Variant II) Explicitly Handling Costs of SolvingTasks
Initialize s0 in some way
for i:= 1, 2, …do
Create new global variables Ti ∈T , si ∈S, pi ∈P, ci, c∗
i ∈R (to be ﬁxed by the end of repeat)
Let a search algorithm (Section 4.1) set pi, a new candidate program. Give pi limited time to do:
∗Task Invention: Unless the user speciﬁes Ti (Section 5), let pi set Ti.
∗Solver Modiﬁcation: Let pi set si by computing a modiﬁcation of si-1 (Section 3.2).
∗Correctness Demonstration: Let pi compute ci := Cost(si, T≤i), and c∗
i := Cost(si−1, T≤i)
i −ci > ϵ (minimal savings of costs such as time/space/etc on all tasks so far)
Freeze/store forever pi, Ti, si, ci, c∗
(potentially relevant at least before the ﬁrst self-rewrite) may be
large enough to prevent the universal methods from being feasible.
PowerPlay, on the other hand, is designed to incrementally
build a practical more and more general problem solver that can
solve numerous tasks quickly, not in the asymptotic sense, but by
exploiting to the max its given particular search algorithm and
computational architecture, with all its space and time limitations,
including those reﬂected by constants ignored by the asymptotic
optimality notation.
As mentioned in Section 5, however, one must now analyze
under which conditions PowerPlay’s self-generated tasks can
accelerate the solution to externally generated tasks ).
CONNECTION TO TRADITIONAL ACTIVE LEARNING
Traditional active learning methods such as
AdaBoost have a totally different
set-up and purpose: there the user provides a set of samples
to be learned, then each new classiﬁer in a series of classi-
ﬁers focuses on samples badly classiﬁed by previous classiﬁers.
Open-ended PowerPlay, however, (1) considers arbitrary computational problems (not necessarily classiﬁcation tasks); (2) can
self-invent all computational tasks; (3) takes into account all computational costs, ordering task candidates by time and space complexity, relative to the present knowledge. There is no need for a
pre-deﬁned global set of tasks that each new solver tries to solve
better, instead the task set continually grows based on which task
is easy to invent and validate, given what is already known.
GREEDY IMPLEMENTATION OF ASPECTS OF THE FORMAL
THEORY OF CREATIVITY
The Formal Theory of Creativity considers agents living in initially unknown environments. At any
given time, such an agent uses a reinforcement learning (RL)
method to maximize not only expected
future external reward for achieving certain goals, but also intrinsic reward for improving an internal model of the environmental
responses to its actions, learning to better predict or compress1
1It is hard to overestimate the cognitive signiﬁcance of compressing the observation
history. For example,consider the video-like image sequence perceived by your brain
the growing history of observations inﬂuenced by its behavior,
thus achieving wow-effects, actively learning skills to inﬂuence
the input stream such that it contains previously unknown but
learnable algorithmic regularities. I have argued that the theory explains essential aspects of intelligence including selective
attention, curiosity, creativity, science, art, music, humor, e.g.,
 . Compare recent related work, e.g.,
 .
Like PowerPlay, such a creative agent produces a sequence of
self-generated tasks and their solutions, each task still unsolvable
before learning, yet becoming solvable after learning. The costs of
learning as well as the learning progress are measured, and enter
the reward function. Thus, in the absence of external reward for
reaching user-deﬁned goals, at any given time the agent is motivated to invent a series of additional tasks that maximize future
expected learning progress.
For example, by restricting its input stream to self-generated
pairs (I, O) ∈I × O like in Section 3.1.1, and limiting it to predict
only O, given I (instead of also trying to predict future (I, O) pairs
from previous ones, which the general agent would do), there will
be a motivation to actively generate a sequence of (I, O) pairs such
that the O are ﬁrst subjectively unpredictable from their I but
then become predictable with little effort, given the limitations of
whatever learning algorithm is used.
Below some of PowerPlay’s apparent drawbacks are listed in
light of the above, followed by certain thoughts relativizing those
drawbacks.
as you are moving through your ofﬁce. The natural way of greatly compressing it is
to construct an internal 3D model of the ofﬁce space ). The
3D model allows for re-computing the entire high-resolution video from a compact sequence of very low-dimensional eye coordinates and eye directions. (The
model itself typically can be speciﬁed by far fewer bits of information than needed
to store raw pixel data of a long video.) Even if the 3D model is not quite precise,
only relatively few extra bits will be required to encode the observed deviations
from the predictions of the model. It seems clear that the enormous compression of
sensory inputs achievable through an internal 3D world model is the main reason
for the latter’s existence. Data compression also explains the emergence of ofﬁce
space-independent internal representations of movable objects such as pens. Many
additional examples of data compression in art and science and humor can be found
in previous papers .
Frontiers in Psychology | Cognitive Science
June 2013 | Volume 4 | Article 313 | 10
Schmidhuber
1. Instead of maximizing future expected reward, PowerPlay
is greedy, always trying to ﬁnd the simplest (easiest to ﬁnd
and validate) task to add to the repertoire, or the simplest
way of improving the efﬁciency or compressibility of previous solutions, instead of looking further ahead, as a universal
RL method would do. That is,
PowerPlay may potentially sacriﬁce large long-term gains for
small short-term gains: the discovery of many easily solvable
tasks may at least temporarily prevent it from learning to solve
hard tasks.
However, on general computational architectures such as
RNNs (Section 4.1.2), PowerPlay is expected to soon run
out of easy tasks that are not yet solvable, due to the architecture’s limited capacity and its unavoidable generalization effects
(many never-tried tasks will become solvable by solutions to the
few explicitly tested Ti). Compare Section 7.1.
2. The general creative agent above 
is motivated to improve performance on the entire history of
previous still unsolved tasks, while PowerPlay may discard
much of this history, keeping only a selective list of previously
solved tasks.
However, as the system is interacting with its environment,
one could store the entire continually growing history, and
make sure that T always allows for deﬁning the task of better
compressing the history so far.
3. PowerPlay as in Section 2 has a binary criterion for adding
knowledge (was the new task solvable without forgetting old
solutions?), while the general agent uses a more informative information-theoretic measure.
However, the cost-based PowerPlay framework (Algorithm 6.1) of Section 6 offers similar, more ﬂexible options,
rewarding compression or speedup of solutions to previously
solved tasks.
On the other hand, drawbacks of previous implementations of
formal creativity theory include:
1. Some previous approximative implementations used traditional RL methods
 with theoretically unlimited look-ahead.
But those are limited in many ways and not guaranteed to
work well in partially observable and/or non-stationary environments where the reward function changes over time. They
won’t necessarily generate an optimal sequence of future tasks
or experiments.
2. Theoretically optimal implementations are currently still impractical,for reasons similar to those
discussed in Section 7.2.
Hence PowerPlay may be viewed as a greedy but feasible implementation of certain basic principles of creativity . PowerPlay-based systems are continually motivated to invent new tasks solvable by formerly unknown procedures, or to compress or speed up problem-solving procedures
discovered earlier. Unlike previous implementations, PowerPlay
extracts from the lifelong experience history a sequence of clearly
identiﬁed and separated tasks with explicitly recorded solutions.
By design it cannot suffer from online learning problems affecting
its solver’s performance on previously solved problems.
BEYOND ALGORITHMIC ZERO-SUM TASK-INVENTION GAMES
PowerPlay’s most closely related previous task-inventing system is the dual brain . There,
to address the computational costs of learning, and the costs of
measuring learning progress, computationally powerful encoders
and problem solvers are implemented
as two very general, co-evolving, symmetric, opposing modules
called the right brain and the left brain. Both are able to inﬂuence
the construction of self-modifying probabilistic programs written
in a universal programing language. An internal storage for temporary computational results of the programs is viewed as part of
the changing environment. Each module can suggest experiments
or self-invented computational tasks in the form of probabilistic algorithms to be executed, and make predictions about their
effects, betting intrinsic reward on their outcomes. The opposing
module may accept such a bet in a zero-sum game by making a
contrary prediction, or reject it. In case of acceptance, the winner is determined by executing the experiment and checking its
outcome; the intrinsic reward eventually gets transferred from the
surprised loser to the conﬁrmed winner. Both modules try to maximize reward using a rather general RL algorithm ) designed
for complex stochastic policies (alternative RL algorithms could be
plugged in as well). Thus both modules are motivated to discover
novel tasks exhibiting novel algorithmic patterns/compressibility
(=surprising wow-effects), where the subjective baseline for novelty is given by what the opponent already knows about the (external or internal) world’s repetitive patterns. Since the execution of
any computational or physical action costs something (as it will
reduce the cumulative reward per time ratio), both modules are
motivated to focus on self-invented tasks that involve those parts
of the dynamic world that currently make surprises and learning progress easy, to minimize the costs of identifying promising
experiments and executing them. The system learns a partly hierarchical structure of more and more complex skills or programs
necessary to solve the growing sequence of self-generated tasks,
reusing previously acquired simpler skills where this is beneﬁcial.Experimentalstudiesexhibitseveralsequentialdevelopmental
stages, with and without external reward 
did not have a built-in guarantee that it cannot forget previously
learned skills, while PowerPlay as in Section 2 does (and the
time and space complexity-based variant Algorithm 6.1 of Section
6 can forget only if this improves the average efﬁciency of previous
solutions).
OPPOSING FORCES: IMPROVING GENERALIZATION THROUGH
COMPRESSION, BREAKING GENERALIZATION THROUGH NOVELTY
Two opposing forces are at work in PowerPlay. On the one hand,
the system continually tries to improve previously learned skills,
by speeding them up, and by compressing the used parameters
of the problem solver, reducing its effective size. The compression
drive tends to improve generalization performance, according to
www.frontiersin.org
June 2013 | Volume 4 | Article 313 | 11
Schmidhuber
the principles of Occam’s Razor and Minimum Description Length
(MDL) and Minimum Message Length (MML) . On the other hand, the system also continually tries to
invent new tasks that break the generalization capabilities of the
present solver.
PowerPlay’s time-minimizing search for new tasks automatically manages the trade-off between these opposing forces.
Sometimes it is easier (because fewer computational resources are
required) to invent and solve a completely new, previously unsolvable problem. Sometimes it is easier to compress (or speed up)
solutions to previously invented problems.
RELATION TO GÖDEL’S SEQUENCE OF INCREASINGLY POWERFUL
AXIOMATIC SYSTEMS
In 1931, Kurt Gödel showed that for each sufﬁciently powerful
(ω-) consistent axiomatic system there is a statement that must be
true but cannot be proven from the axioms through an algorithmic theorem-proving procedure . This unprovable
statement can then be added to the axioms, to obtain a more
powerful formal theory in which new formerly unprovable theorems become provable, without affecting previously provable
In a sense, PowerPlay is doing something similar. Assume
the architecture of the solver is a universal computer . Its software s can
be viewed as a theorem-proving procedure implementing certain
enumerable axioms and computable inference rules. PowerPlay
continually tries to modify s such that the previously proven theorems remain provable within certain time bounds, and a new
previously unprovable theorem becomes provable.
FIRST ILLUSTRATIVE EXPERIMENTS
First experiments with PowerPlay were reported in separate
papers ).
Standard NNs as well as SLIM RNNs were
used as computational problem-solving architectures. The weights
of SLIM RNNs can encode essentially arbitrary computable tasks
as well as arbitrary, self-delimiting, halting or non-halting programs solving those tasks (Section 4.1.2). Such programs may
affect both environment (through effectors) and internal states
encoding abstractions of event sequences. For example, in the
experiments a SLIM RNN learned to control a fovea that can
be shifted across a visual scene. The sequences of dynamically
changing sensory inputs from the fovea contributed to the formation of internal SLIM RNN states, that is, vectors of neural
activations encoding possible goals. In open-ended fashion, our
PowerPlay-driven NNs learned to become increasingly general solvers of self-invented tasks. Sometimes they added new
problem-solving procedures to the growing repertoire. Sometimes
they preferred to compress/speed up previously invented skills,
depending on what was computationally easiest at this point in
time. The NNs also exhibited interesting developmental stages,
incrementally moving from apparently simple self-invented problems to more complex ones. Furthermore, it was shown how
a PowerPlay-driven SLIM NN automatically self-modularizes
 , frequently re-using code for previously
invented skills, keeping track which connections affect which tasks
(Section 3.3.2), always trying to invent novel tasks that can be
quickly validated because they do not require too many weight
changes affecting too many previous tasks.
WORDS OF CAUTION
The behavior of PowerPlay is determined by the nature and the
limitations of T , S, P, and its algorithm for searching P. If T
includes all computable task descriptions, and both S and P allow
for implementing arbitrary programs, and the search algorithm
is a general method for search in program space (Section 4), then
there are few limits to what PowerPlay may do ).
It may not be advisable to let a general variant of PowerPlay
loose in an uncontrolled situation, e.g., on a multi-computer network on the internet, possibly with access to control of physical
devices, and the potential to acquire additional computational
and physical resources (Section 3.1.2) through programs executed during PowerPlay. Unlike, say, traditional virus programs,
PowerPlay-based systems will continually change in a way hard
to predict, incessantly inventing and solving novel, self-generated
tasks, only driven by a desire to increase their general problemsolving capacity, perhaps a bit like many humans seek to increase
their power once their basic needs are satisﬁed. This type of
artiﬁcial curiosity/creativity, however, may conﬂict with human
intentions on occasion. On the other hand, unchecked curiosity
may sometimes also be harmful or fatal to the learning system
itself (Section 5) – curiosity can kill the cat.
ACKNOWLEDGMENTS
Thanks to Mark Ring, Bas Steunebrink, Faustino Gomez, Sohrob
Kazerounian, Hung Ngo, Leo Pape, Giuseppe Cuccu, and several
anonymous reviewers, for useful comments.