An Introduction to
Matrix Concentration Inequalities
Joel A. Tropp
24 December 2014
FnTML Draft, Revised
 
For Margot and Benjamin
Introduction
Historical Origins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The Modern Random Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Random Matrices for the People
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Basic Questions in Random Matrix Theory
. . . . . . . . . . . . . . . . . . . . . . .
Random Matrices as Independent Sums . . . . . . . . . . . . . . . . . . . . . . . . .
Exponential Concentration Inequalities for Matrices
. . . . . . . . . . . . . . . . .
The Arsenal of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
About This Monograph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Matrix Functions & Probability with Matrices
Matrix Theory Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Probability with Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The Matrix Laplace Transform Method
Matrix Moments and Cumulants . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The Matrix Laplace Transform Method . . . . . . . . . . . . . . . . . . . . . . . . . .
The Failure of the Matrix Mgf
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A Theorem of Lieb . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Subadditivity of the Matrix Cgf
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Master Bounds for Sums of Independent Random Matrices
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Matrix Gaussian & Rademacher Series
A Norm Bound for Random Series with Matrix Coefﬁcients . . . . . . . . . . . . . .
Example: Some Gaussian Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Example: Matrices with Randomly Signed Entries . . . . . . . . . . . . . . . . . . .
Example: Gaussian Toeplitz Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . .
Application: Rounding for the MaxQP Relaxation
. . . . . . . . . . . . . . . . . . .
Analysis of Matrix Gaussian & Rademacher Series . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A Sum of Random Positive-Semideﬁnite Matrices
The Matrix Chernoff Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Example: A Random Submatrix of a Fixed Matrix . . . . . . . . . . . . . . . . . . . .
Application: When is an Erd˝os–Rényi Graph Connected? . . . . . . . . . . . . . . .
Proof of the Matrix Chernoff Inequalities
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A Sum of Bounded Random Matrices
A Sum of Bounded Random Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . .
Example: Matrix Approximation by Random Sampling . . . . . . . . . . . . . . . .
Application: Randomized Sparsiﬁcation of a Matrix . . . . . . . . . . . . . . . . . .
Application: Randomized Matrix Multiplication . . . . . . . . . . . . . . . . . . . .
Application: Random Features
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Proof of the Matrix Bernstein Inequality . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Results Involving the Intrinsic Dimension
The Intrinsic Dimension of a Matrix
. . . . . . . . . . . . . . . . . . . . . . . . . . .
Matrix Chernoff with Intrinsic Dimension . . . . . . . . . . . . . . . . . . . . . . . .
Matrix Bernstein with Intrinsic Dimension . . . . . . . . . . . . . . . . . . . . . . .
Revisiting the Matrix Laplace Transform Bound
. . . . . . . . . . . . . . . . . . . .
The Intrinsic Dimension Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Proof of the Intrinsic Chernoff Bound
. . . . . . . . . . . . . . . . . . . . . . . . . .
Proof of the Intrinsic Bernstein Bounds
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A Proof of Lieb’s Theorem
Lieb’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Analysis of the Relative Entropy for Vectors . . . . . . . . . . . . . . . . . . . . . . .
Elementary Trace Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The Logarithm of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The Operator Jensen Inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The Matrix Perspective Transformation . . . . . . . . . . . . . . . . . . . . . . . . .
The Kronecker Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The Matrix Relative Entropy is Convex . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Matrix Concentration: Resources
Bibliography
In recent years, random matrices have come to play a major role in computational mathematics,
but most of the classical areas of random matrix theory remain the province of experts. Over
the last decade, with the advent of matrix concentration inequalities, research has advanced to
the point where we can conquer many (formerly) challenging problems with a page or two of
arithmetic.
My aim is to describe the most successful methods from this area along with some interesting
examples that these techniques can illuminate. I hope that the results in these pages will inspire
future work on applications of random matrices as well as reﬁnements of the matrix concentration inequalities discussed herein.
I have chosen to present a coherent body of results based on a generalization of the Laplace
transform method for establishing scalar concentration inequalities. In the last two years, Lester
Mackey and I, together with our coauthors, have developed an alternative approach to matrix
concentration using exchangeable pairs and Markov chain couplings. With some regret, I have
chosen to omit this theory because the ideas seem less accessible to a broad audience of researchers. The interested reader will ﬁnd pointers to these articles in the annotated bibliography.
The work described in these notes reﬂects the inﬂuence of many researchers. These include
Rudolf Ahlswede, Rajendra Bhatia, Eric Carlen, Sourav Chatterjee, Edward Effros, Elliott Lieb,
Roberto Imbuzeiro Oliveira, Dénes Petz, Gilles Pisier, Mark Rudelson, Roman Vershynin, and
Andreas Winter. I have also learned a great deal from other colleagues and friends along the way.
I would like to thank some people who have helped me improve this work. Several readers
informed me about errors in the initial version of this manuscript; these include Serg Bogdanov,
Peter Forrester, Nikos Karampatziakis, and Guido Lagos. The anonymous reviewers tendered
many useful suggestions, and they pointed out a number of errors. Sid Barman gave me feedback
on the ﬁnal revisions to the monograph. Last, I want to thank Léon Nijensohn for his continuing
encouragement.
I gratefully acknowledge ﬁnancial support from the Ofﬁce of Naval Research under awards
N00014-08-1-0883 and N00014-11-1002, the Air Force Ofﬁce of Strategic Research under award
FA9550-09-1-0643, and an Alfred P. Sloan Fellowship. Some of this research was completed at
the Institute of Pure and Applied Mathematics at UCLA. I would also like to thank the California
Institute of Technology and the Moore Foundation.
Joel A. Tropp
Pasadena, CA
December 2012
Revised, March 2014 and December 2014
Introduction
Random matrix theory has grown into a vital area of probability, and it has found applications
in many other ﬁelds. To motivate the results in this monograph, we begin with an overview of
the connections between random matrix theory and computational mathematics. We introduce
the basic ideas underlying our approach, and we state one of our main results on the behavior
of random matrices. As an application, we examine the properties of the sample covariance
estimator, a random matrix that arises in statistics. Afterward, we summarize the other types of
results that appear in these notes, and we assess the novelties in this presentation.
Historical Origins
Random matrix theory sprang from several different sources in the ﬁrst half of the 20th century.
Geometry of Numbers. Peter Forrester [For10, p. v] traces the ﬁeld of random matrix theory to
work of Hurwitz, who deﬁned the invariant integral over a Lie group. Specializing this
analysis to the orthogonal group, we can reinterpret this integral as the expectation of a
function of a uniformly random orthogonal matrix.
Multivariate Statistics. Another early example of a random matrix appeared in the work of John
Wishart [Wis28]. Wishart was studying the behavior of the sample covariance estimator for
the covariance matrix of a multivariate normal random vector. He showed that the estimator, which is a random matrix, has the distribution that now bears his name. Statisticians
have often used random matrices as models for multivariate data [MKB79, Mui82].
Numerical Linear Algebra. In their remarkable work [vNG47, GvN51] on computational methods for solving systems of linear equations, von Neumann and Goldstine considered a
random matrix model for the ﬂoating-point errors that arise from an LU decomposition.1
They obtained a high-probability bound for the norm of the random matrix, which they
1von Neumann and Goldstine invented and analyzed this algorithm before they had any digital computer on which
to implement it! See [Grc11] for a historical account.
CHAPTER 1. INTRODUCTION
took as an estimate for the error the procedure might typically incur. Curiously, in subsequent years, numerical linear algebraists became very suspicious of probabilistic techniques, and only in recent years have randomized algorithms reappeared in this ﬁeld. See
the surveys [Mah11, HMT11, Woo14] for more details and references.
Nuclear Physics. In the early 1950s, physicists had reached the limits of deterministic analytical techniques for studying the energy spectra of heavy atoms undergoing slow nuclear
reactions. Eugene Wigner was the ﬁrst researcher to surmise that a random matrix with
appropriate symmetries might serve as a suitable model for the Hamiltonian of the quantum mechanical system that describes the reaction. The eigenvalues of this random matrix
model the possible energy levels of the system. See Mehta’s book [Meh04, §1.1] for an account of all this.
In each area, the motivation was quite different and led to distinct sets of questions. Later,
random matrices began to percolate into other ﬁelds such as graph theory (the Erd˝os–Rényi
model [ER60] for a random graph) and number theory (as a model for the spacing of zeros of
the Riemann zeta function [Mon73]).
The Modern Random Matrix
By now, random matrices are ubiquitous. They arise throughout modern mathematics and
statistics, as well as in many branches of science and engineering. Random matrices have several different purposes that we may wish to distinguish. They can be used within randomized
computer algorithms; they serve as models for data and for physical phenomena; and they are
subjects of mathematical inquiry. This section offers a taste of these applications. Note that the
ideas and references here reﬂect the author’s interests, and they are far from comprehensive!
Algorithmic Applications
The striking mathematical properties of random matrices can be harnessed to develop algorithms for solving many different problems.
Computing Matrix Approximations. Random matrices can be used to develop fast algorithms
for computing a truncated singular-value decomposition. In this application, we multiply
a large input matrix by a smaller random matrix to extract information about the dominant
singular vectors of the input matrix. The seed of this idea appears in [FKV98, DFK+99].
The survey [HMT11] explains how to implement this method in practice, while the two
monographs [Mah11, Woo14] cover more theoretical aspects.
Sparsiﬁcation. One way to accelerate spectral computations on large matrices is to replace the
original matrix by a sparse proxy that has similar spectral properties. An elegant way to
produce the sparse proxy is to zero out entries of the original matrix at random while
rescaling the entries that remain. This approach was proposed in [AM01, AM07], and the
papers [AKL13, KD14] contain recent innovations. Related ideas play an important role in
Spielman and Teng’s work [ST04] on fast algorithms for solving linear systems.
Subsampling of Data. In large-scale machine learning, one may need to subsample data randomly to reduce the computational costs of ﬁtting a model. For instance, we can combine
1.2. THE MODERN RANDOM MATRIX
random sampling with the Nyström decomposition to obtain a randomized approximation of a kernel matrix. This method was introduced by Williams & Seeger [WS01]. The
paper [DM05] provides the ﬁrst theoretical analysis, and the survey [GM14] contains more
complete results.
Dimension Reduction. A basic template in the theory of algorithms invokes randomized projection to reduce the dimension of a computational problem. Many types of dimension
reduction are based on properties of random matrices. The two papers [JL84, Bou85]
established the mathematical foundations of this approach. The earliest applications in
computer science appear in the work [LLR95]. Many contemporary variants depend on
ideas from [AC09] and [CW13].
Combinatorial Optimization. One approach to solving a computationally difﬁcult optimization problem is to relax (i.e., enlarge) the constraint set so the problem becomes tractable,
to solve the relaxed problem, and then to use a randomized procedure to map the solution
back to the original constraint set [BTN01, §4.3]. This technique is called relaxation and
rounding. For hard optimization problems involving a matrix variable, the analysis of the
rounding procedure often involves ideas from random matrix theory [So09, NRV13].
Compressed Sensing. When acquiring data about an object with relatively few degrees of freedom as compared with the ambient dimension, we may be able to sieve out the important
information from the object by taking a small number of random measurements, where
the number of measurements is comparable to the number of degrees of freedom [GGI+02,
CRT06, Don06]. This observation is now referred to as compressed sensing. Random matrices play a central role in the design and analysis of measurement procedures. For example,
see [FR13, CRPW12, ALMT14, Tro14].
Random matrices also appear as models for multivariate data or multivariate phenomena. By
studying the properties of these models, we may hope to understand the typical behavior of a
data-analysis algorithm or a physical system.
Sparse Approximation for Random Signals. Sparse approximation has become an important
problem in statistics, signal processing, machine learning and other areas. One model for
a “typical” sparse signal poses the assumption that the nonzero coefﬁcients that generate
the signal are chosen at random. When analyzing methods for identifying the sparse set of
coefﬁcients, we must study the behavior of a random column submatrix drawn from the
model matrix [Tro08a, Tro08b].
Demixing of Structured Signals. In data analysis, it is common to encounter a mixture of two
structured signals, and the goal is to extract the two signals using prior information about
the structures. A common model for this problem assumes that the signals are randomly
oriented with respect to each other, which means that it is usually possible to discriminate
the underlying structures. Random orthogonal matrices arise in the analysis of estimation
techniques for this problem [MT14, ALMT14, MT13].
CHAPTER 1. INTRODUCTION
Stochastic Block Model. One probabilistic framework for describing community structure in
a network assumes that each pair of individuals in the same community has a relationship with high probability, while each pair of individuals drawn from different communities has a relationship with lower probability. This is referred to as the stochastic block
model [HLL83]. It is quite common to analyze algorithms for extracting community structure from data by positing that this model holds. See [ABH14] for a recent contribution, as
well as a summary of the extensive literature.
High-Dimensional Data Analysis. More generally, random models are pervasive in the analysis of statistical estimation procedures for high-dimensional data. Random matrix theory
plays a key role in this ﬁeld [MKB79, Mui82, Kol11, BvdG11].
Wireless Communication. Random matrices are commonly used as models for wireless channels. See the book of Tulino and Verdú for more information [TV04].
In these examples, it is important to recognize that random models may not coincide very well
with reality, but they allow us to get a sense of what might be possible in some generic cases.
Theoretical Aspects
Random matrices are frequently studied for their intrinsic mathematical interest. In some ﬁelds,
they provide examples of striking phenomena. In other areas, they furnish counterexamples to
“intuitive” conjectures. Here are a few disparate problems where random matrices play a role.
Combinatorics. An expander graph has the property that every small set of vertices has edges
linking it to a large proportion of the vertices. The expansion property is closely related to
the spectral behavior of the adjacency matrix of the graph. The easiest construction of an
expander involves a random matrix argument [AS00, §9.2].
Numerical Analysis. For worst-case examples, the Gaussian elimination method for solving a
linear system is not numerically stable. In practice, however, stability problems rarely
arise. One explanation for this phenomenon is that, with high probability, a small random
perturbation of any ﬁxed matrix is well conditioned. As a consequence, it can be shown
that Gaussian elimination is stable for most matrices [SST06].
High-Dimensional Geometry. Dvoretzky’s Theorem states that, when N is large, the unit ball
of each N-dimensional Banach space has a slice of dimension n ≈logN that is close to a
Euclidean ball with dimension n. It turns out that a random slice of dimension n realizes
this property [Mil71]. This result can be framed as a statement about spectral properties
of a random matrix [Gor85].
Quantum Information Theory. Random matrices appear as counterexamples for a number of
conjectures in quantum information theory. Here is one instance. In classical information
theory, the total amount of information that we can transmit through a pair of channels
equals the sum of the information we can send through each channel separately. It was
conjectured that the same property holds for quantum channels. In fact, a pair of quantum
channels can have strictly larger capacity than a single channel. This result depends on a
random matrix construction [Has09]. See [HW08] for related work.
1.3. RANDOM MATRICES FOR THE PEOPLE
Random Matrices for the People
Historically, random matrix theory has been regarded as a very challenging ﬁeld. Even now,
many well-established methods are only comprehensible to researchers with signiﬁcant experience, and it may take months of intensive effort to prove new results. There are a small number
of classes of random matrices that have been studied so completely that we know almost everything about them. Yet, moving beyond this terra ﬁrma, one quickly encounters examples where
classical methods are brittle.
We hope to democratize random matrix theory. These notes describe tools that deliver useful information about a wide range of random matrices. In many cases, a modest amount of
straightforward arithmetic leads to strong results. The methods here should be accessible to
computational scientists working in a variety of ﬁelds. Indeed, the techniques in this work have
already found an extensive number of applications.
Basic Questions in Random Matrix Theory
Random matrices merit special attention because they have spectral properties that are quite
different from familiar deterministic matrices. Here are some of the questions we might want to
investigate.
• What is the expectation of the maximum eigenvalue of a random Hermitian matrix? What
about the minimum eigenvalue?
• How is the maximum eigenvalue of a random Hermitian matrix distributed? What is the
probability that it takes values substantially different from its mean? What about the minimum eigenvalue?
• What is the expected spectral norm of a random matrix? What is the probability that the
norm takes a value substantially different from its mean?
• What about the other eigenvalues or singular values? Can we say something about the
“typical” spectrum of a random matrix?
• Can we say anything about the eigenvectors or singular vectors? For instance, is each one
distributed almost uniformly on the sphere?
• We can also ask questions about the operator norm of a random matrix acting as a map between two normed linear spaces. In this case, the geometry of the domain and codomain
play a role.
In this work, we focus on the ﬁrst three questions above. We study the expectation of the extreme
eigenvalues of a random Hermitian matrix, and we attempt to provide bounds on the probability
that they take an unusual value. As an application of these results, we can control the expected
spectral norm of a general matrix and bound the probability of a large deviation. These are the
most relevant problems in many (but not all!) applications. The remaining questions are also
important, but we will not touch on them here. We recommend the book [Tao12] for a friendly
introduction to other branches of random matrix theory.
CHAPTER 1. INTRODUCTION
Random Matrices as Independent Sums
Our approach to random matrices depends on a fundamental principle:
In applications, it is common that a random matrix can be expressed as a sum of
independent random matrices.
The examples that appear in these notes should provide ample evidence for this claim. For now,
let us describe a speciﬁc problem that will serve as an illustration throughout the Introduction.
We hope this example is complicated enough to be interesting but simple enough to elucidate
the main points.
Example: The Sample Covariance Estimator
Let x = (X1,...,Xp) be a complex random vector with zero mean: Ex = 0. The covariance matrix
A of the random vector x is the positive-semideﬁnite matrix
A = E(xx∗) =
The star ∗refers to the conjugate transpose operation, and the standard basis matrix Ejk has
a one in the (j,k) position and zeros elsewhere. In other words, the (j,k) entry of the sample
covariance matrix A records the covariance between the jth and kth entry of the vector x.
One basic problem in statistical practice is to estimate the covariance matrix from data.
Imagine that we have access to n independent samples x1,...,xn, each distributed the same way
as x. The sample covariance estimator Y is the random matrix
The random matrix Y is an unbiased estimator2 for the sample covariance matrix: EY = A. Observe that the sample covariance estimator Y ﬁts neatly into our paradigm:
The sample covariance estimator can be expressed as a sum of independent random matrices.
This is precisely the type of decomposition that allows us to apply the tools in these notes.
Exponential Concentration Inequalities for Matrices
An important challenge in probability theory is to study the probability that a real random variable Z takes a value substantially different from its mean. That is, we seek a bound of the form
P{|Z −EZ| ≥t} ≤
2The formula (1.5.2) supposes that the random vector x is known to have zero mean. Otherwise, we have to make
an adjustment to incorporate an estimate for the sample mean.
1.6. EXPONENTIAL CONCENTRATION INEQUALITIES FOR MATRICES
for a positive parameter t. When Z is expressed as a sum of independent random variables, the
literature contains many tools for addressing this problem. See [BLM13] for an overview.
For a random matrix Z , a variant of (1.6.1) is the question of whether Z deviates substantially
from its mean value. We might frame this question as
P{∥Z −EZ ∥≥t} ≤
Here and elsewhere, ∥·∥denotes the spectral norm of a matrix. As noted, it is frequently possible to decompose Z as a sum of independent random matrices. We might even dream that
established methods for studying the scalar concentration problem (1.6.1) extend to (1.6.2).
The Bernstein Inequality
To explain what kind of results we have in mind, let us return to the scalar problem (1.6.1). First,
to simplify formulas, we assume that the real random variable Z has zero mean: EZ = 0. If not,
we can simply center the random variable by subtracting its mean. Second, and more restrictively, we suppose that Z can be expressed as a sum of independent, real random variables.
To control Z, we rely on two types of information: global properties of the sum (such as its
mean and variance) and local properties of the summands (such as their maximum ﬂuctuation).
These pieces of data are usually easy to obtain. Together, they determine how well Z concentrates around zero, its mean value.
Theorem 1.6.1 (Bernstein Inequality). Let S1,...,Sn be independent, centered, real random variables, and assume that each one is uniformly bounded:
for each k = 1,...,n.
Introduce the sum Z = Pn
k=1 Sk, and let v(Z) denote the variance of the sum:
v(Z) = EZ 2 =
P{|Z| ≥t} ≤2 exp
for all t ≥0.
See [BLM13, §2.8] for a proof of this result. We refer to Theorem 1.6.1 as an exponential concentration inequality because it yields exponentially decaying bounds on the probability that Z
deviates substantially from its mean.
The Matrix Bernstein Inequality
What is truly astonishing is that the scalar Bernstein inequality, Theorem 1.6.1, lifts directly to
matrices. Let us emphasize this remarkable fact:
There are exponential concentration inequalities for the spectral norm of a sum
of independent random matrices.
CHAPTER 1. INTRODUCTION
As a consequence, once we decompose a random matrix as an independent sum, we can harness
global properties (such as the mean and the variance) and local properties (such as a uniform
bound on the summands) to obtain detailed information about the norm of the sum. As in the
scalar case, it is usually easy to acquire the input data for the inequality. But the output of the
inequality is highly nontrivial.
To illustrate these claims, we will state one of the major results from this monograph. This
theorem is a matrix extension of Bernstein’s inequality that was developed independently in the
two papers [Oli10a, Tro11c]. After presenting the result, we give some more details about its interpretation. In the next section, we apply this result to study the covariance estimation problem.
Theorem 1.6.2 (Matrix Bernstein). Let S1,...,Sn be independent, centered random matrices with
common dimension d1 ×d2, and assume that each one is uniformly bounded
for each k = 1,...,n.
Introduce the sum
and let v(Z ) denote the matrix variance statistic of the sum:
v(Z ) = max
∥E(Z Z ∗)∥, ∥E(Z ∗Z )∥
P{∥Z ∥≥t} ≤(d1 +d2)·exp
v(Z )+Lt/3
for all t ≥0.
Furthermore,
2v(Z )log(d1 +d2)+ 1
3L log(d1 +d2).
The proof of this result appears in Chapter 6.
To appreciate what Theorem 1.6.2 means, it is valuable to make a direct comparison with the
scalar version, Theorem 1.6.1. In both cases, we express the object of interest as an independent
sum, and we instate a uniform bound on the summands. There are three salient changes:
• The variance v(Z ) in the result for matrices can be interpreted as the magnitude of the
expected squared deviation of Z from its mean. The formula reﬂects the fact that a general matrix B has two different squares BB∗and B∗B. For an Hermitian matrix, the two
squares coincide.
• The tail bound has a dimensional factor d1+d2 that depends on the size of the matrix. This
factor reduces to two in the scalar setting. In the matrix case, it limits the range of t where
the tail bound is informative.
• We have included a bound for E ∥Z ∥. This estimate is not particularly interesting in the
scalar setting, but it is usually quite challenging to prove results of this type for matrices.
In fact, the expectation bound is often more useful than the tail bound.
1.6. EXPONENTIAL CONCENTRATION INEQUALITIES FOR MATRICES
The latter point deserves ampliﬁcation:
The expectation bound (1.6.6) is the most important aspect of the matrix Bernstein inequality.
For further discussion of this result, turn to Chapter 6. Chapters 4 and 7 contain related results
and interpretations.
Example: The Sample Covariance Estimator
We will apply the matrix Bernstein inequality, Theorem 1.6.2, to measure how well the sample
covariance estimator approximates the true covariance matrix. As before, let x be a zero-mean
random vector with dimension p. Introduce the p×p covariance matrix A = E(xx∗). Suppose we
have n independent samples x1,...,xn with the same distribution as x. Form the p × p sample
covariance estimator
Our goal is to study how the spectral-norm distance ∥Y −A∥between the sample covariance and
the true covariance depends on the number n of samples.
For simplicity, we will perform the analysis under the extra assumption that the ℓ2 norm of
the random vector is bounded: ∥x∥2 ≤B. This hypothesis can be relaxed if we apply a variant of
the matrix Bernstein inequality that reﬂects the typical magnitude of a summand Sk. One such
variant appears in the formula (6.1.6).
We are in a situation where it is quite easy to see how the matrix Bernstein inequality applies.
Deﬁne the random deviation Z of the estimator Y from the true covariance matrix A:
Z = Y −A =
for each index k.
The random matrices Sk are independent, identically distributed, and centered. To apply Theorem 1.6.2, we need to ﬁnd a uniform bound L for the summands, and we need to control the
matrix variance statistic v(Z ).
First, let us develop a uniform bound on the spectral norm of each summand. We may calculate that
The ﬁrst relation is the triangle inequality. The second follows from the assumption that x is
bounded and the observation that
∥A∥= ∥E(xx∗)∥≤E∥xx∗∥= E ∥x∥2 ≤B.
This expression depends on Jensen’s inequality and the hypothesis that x is bounded.
Second, we need to bound the matrix variance statistic v(Z ) deﬁned in (1.6.4). The matrix Z
is Hermitian, so the two squares in this formula coincide with each other:
°°EZ 2°° =
CHAPTER 1. INTRODUCTION
We need to determine the variance of each summand. By direct calculation,
∥xk∥2 · xkx∗
−A2 −A2 + A2¤
The expression H ≼T means that T −H is positive semideﬁnite. We used the norm bound for
the random vector x and the fact that expectation preserves the semideﬁnite order. In the last
step, we dropped the negative-semideﬁnite term −A2. Summing this relation over k, we reach
The matrix is positive-semideﬁnite because it is a sum of squares of Hermitian matrices. Extract
the spectral norm to arrive at
°°°°° ≤B ∥A∥
We have now collected the information we need to analyze the sample covariance estimator.
We can invoke the estimate (1.6.6) from the matrix Bernstein inequality, Theorem 1.6.2, with
the uniform bound L = 2B/n and the variance bound v(Z ) ≤B ∥A∥/n. We attain
E∥Y −A∥= E∥Z ∥≤
2B ∥A∥log(2p)
+ 2B log(2p)
In other words, the error in approximating the sample covariance matrix is not too large when
we have a sufﬁcient number of samples. If we wish to obtain a relative error on the order of ε, we
n ≥2B log(2p)
This selection yields
It is often the case that B = Const· p, so we discover that n = Const·ε−2p logp samples are suf-
ﬁcient for the sample covariance estimator to provide a relatively accurate estimate of the true
covariance matrix A. This bound is qualitatively sharp for worst-case distributions.
The analysis in this section applies to many other examples. We encapsulate the argument
in Corollary 6.2.1, which we use to study several more problems.
History of this Example
Covariance estimation may be the earliest application of matrix concentration tools in random
matrix theory. Rudelson [Rud99], building on a suggestion of Pisier, showed how to use the noncommutative Khintchine inequality [LP86, LPP91, Buc01, Buc05] to obtain essentially optimal
bounds on the sample covariance estimator of a bounded random vector. The tutorial [Ver12] of
Roman Vershynin offers an overview of this problem as well as many results and references. The
analysis of the sample covariance matrix here is adapted from the technical [GT14]. It leads to a
result similar with the one Rudelson obtained in [Rud99].
1.7. THE ARSENAL OF RESULTS
Optimality of the Matrix Bernstein Inequality
Theorem 1.6.2 can be sharpened very little because it applies to every random matrix Z of the
form (1.6.3). Let us say a few words about optimality now, postponing the details to §6.1.2.
Suppose that Z is a random matrix of the form (1.6.3). To make the comparison simpler, we
also insist that each summand Sk is a symmetric random variable; that is, Sk and −Sk have the
same distribution for each index k. Introduce the quantity
⋆= E maxk ∥Sk∥2 .
In §6.1.2, we will argue that these assumptions imply
v(Z ) + L2
v(Z )log(d1 +d2) + L2
⋆log2(d1 +d2)
In other words, the scale of E∥Z ∥2 must depend on the matrix variance statistic v(Z ) and the
average upper bound L2
⋆for the summands. The quantity L = sup∥Sk∥that appears in the matrix
Bernstein inequality always exceeds L⋆, sometimes by a large margin, but they capture the same
type of information.
The signiﬁcant difference between the lower and upper bound in (1.6.7) comes from the dimensional factor log(d1 + d2). There are random matrices Z for which the lower bound gives a
more accurate reﬂection of E∥Z ∥2, but there are also many random matrices where the upper
bound describes the behavior correctly. At present, there is no method known for distinguishing
between these two extremes under the model (1.6.3) for the random matrix.
The tail bound (1.6.5) provides a useful tool in practice, but it is not necessarily the best way
to collect information about large deviation probabilities. To obtain more precise results, we recommend using the expectation bound (1.6.6) to control E∥Z ∥and then applying scalar concentration inequalities to estimate P{∥Z ∥≥E∥Z ∥+ t}. The book [BLM13] offers a good treatment of
the methods that are available for establishing scalar concentration.
The Arsenal of Results
The Bernstein inequality is probably the most familiar exponential tail bound for a sum of independent random variables, but there are many more. It turns out that essentially all of these
scalar results admit extensions that hold for random matrices. In fact, many of the established
techniques for scalar concentration have analogs in the matrix setting.
What’s Here...
This monograph focuses on a few key exponential concentration inequalities for a sum of independent random matrices, and it describes some speciﬁc applications of these results.
Matrix Gaussian Series. A matrix Gaussian series is a random matrix that can be expressed as
a sum of ﬁxed matrices, each weighted by an independent standard normal random variable. This formulation includes a surprising number of examples. The most important
are undoubtedly Wigner matrices and rectangular Gaussian matrices. Another interesting case is a Toeplitz matrix with Gaussian entries. The analysis of matrix Gaussian series
appears in Chapter 4.
CHAPTER 1. INTRODUCTION
Matrix Rademacher Series. A matrix Rademacher series is a random matrix that can be written as a sum of ﬁxed matrices, each weighted by an independent Rademacher random
variable.3 This construction includes things like random sign matrices, as well as a ﬁxed
matrix whose entries are modulated by random signs. There are also interesting examples
that arise in combinatorial optimization. We treat these problems in Chapter 4.
Matrix Chernoff Bounds. The matrix Chernoff bounds apply to a random matrix that can be decomposed as a sum of independent, random positive-semideﬁnite matrices whose maximum eigenvalues are subject to a uniform bound. These results allow us to obtain information about the norm of a random submatrix drawn from a ﬁxed matrix. They are also
appropriate for studying the Laplacian matrix of a random graph. See Chapter 5.
Matrix Bernstein Bounds. The matrix Bernstein inequality concerns a random matrix that can
be expressed as a sum of independent, centered random matrices that admit a uniform
spectral-norm bound. This result has many applications, including the analysis of randomized algorithms for matrix sparsiﬁcation and matrix multiplication. It can also be used
to study the random features paradigm for approximating a kernel matrix. Chapter 6 contains this material.
Intrinsic Dimension Bounds. Some matrix concentration inequalities can be improved when
the random matrix has limited spectral content in most dimensions. In this situation, we
may be able to obtain bounds that do not depend on the ambient dimension. See Chapter 7 for details.
We have chosen to present these results because they are illustrative, and they have already
found concrete applications.
What’s Not Here...
The program of extending scalar concentration results to the matrix setting has been quite fruitful, and there are many useful results beyond the ones that we detail. Let us mention some of the
other tools that are available. For further information, see the annotated bibliography.
First, there are additional exponential concentration inequalities for a sum of independent
random matrices. All of the following results can be established within the framework of this
monograph.
• Matrix Hoeffding. This result concerns a sum of independent random matrices whose
squares are subject to semideﬁnite upper bounds [Tro11c, §7].
• Matrix Bennett. This estimate sharpens the tail bound from the matrix Bernstein inequality [Tro11c, §6].
• Matrix Bernstein, Unbounded Case. The matrix Bernstein inequality extends to the case
where the moments of the summands grow at a controlled rate. See [Tro11c, §6] or [Kol11].
• Matrix Bernstein, Nonnegative Summands. The lower tail of the Bernstein inequality can
be improved when the summands are positive semideﬁnite [Mau03]; this result extends to
the matrix setting. By a different argument, the dimensional factor can be removed from
this bound for a class of interesting examples [Oli13, Thm. 3.1].
3A Rademacher random variable takes the two values ±1 with equal probability.
1.7. THE ARSENAL OF RESULTS
The approach in this monograph can be adapted to obtain exponential concentration for
matrix-valued martingales. Here are a few results from this category:
• Matrix Azuma. This is the martingale version of the matrix Hoeffding bound [Tro11c, §7].
• Matrix Bounded Differences. The matrix Azuma inequality gives bounds for the spectral
norm of a matrix-valued function of independent random variables [Tro11c, §7].
• Matrix Freedman. This result can be viewed as the martingale extension of the matrix
Bernstein inequality [Oli10a, Tro11a].
The technical report [Tro11b] explains how to extend other bounds for a sum of independent
random matrices to the martingale setting.
Polynomial moment inequalities provide bounds for the expected trace of a power of a random matrix. Moment inequalities for a sum of independent random matrices can provide useful
information when the summands have heavy tails or else a uniform bound does not reﬂect the
typical size of the summands.
• Matrix Khintchine. The matrix Khintchine inequality is the polynomial version of the exponential bounds for matrix Gaussian series and matrix Rademacher series. This result is
presented in (4.7.1). See the papers [LP86, Buc01, Buc05] or [MJC+14, Cor. 7.3] for proofs.
• Matrix Moment Inequalities. The matrix Chernoff inequality admits a polynomial variant;
the simplest form appears in (5.1.9). The matrix Bernstein inequality also has a polynomial
variant, stated in (6.1.6). These bounds are drawn from [CGT12a, App.].
The methods that lead to polynomial moment inequalities differ substantially from the techniques in this monograph, so we cannot include the proofs here. The annotated bibliography
includes references to the large literature on moment inequalities for random matrices.
Recently, Lester Mackey and the author, in collaboration with Daniel Paulin and several other
researchers [MJC+14, PMT14], have developed another framework for establishing matrix concentration. This approach extends a scalar argument, introduced by Chatterjee [Cha05, Cha07],
that depends on exchangeable pairs and Markov chain couplings. The method of exchangeable
pairs delivers both exponential concentration inequalities and polynomial moment inequalities
for random matrices, and it can reproduce many of the bounds mentioned above. It also leads
to new results:
• Polynomial Efron–Stein Inequality for Matrices. This bound is a matrix version of the
polynomial Efron–Stein inequality [BBLM05, Thm. 1]. It controls the polynomial moments
of a centered random matrix that is a function of independent random variables [PMT14,
Thm. 4.2].
• Exponential Efron–Stein Inequality for Matrices. This bound is the matrix extension of
the exponential Efron–Stein inequality [BLM03, Thm. 1]. It leads to exponential concentration inequalities for a centered random matrix constructed from independent random
variables [PMT14, Thm. 4.3].
CHAPTER 1. INTRODUCTION
Another signiﬁcant advantage is that the method of exchangeable pairs can sometimes handle
random matrices built from dependent random variables. Although the simplest version of the
exchangeable pairs argument is more elementary than the approach in this monograph, it takes
a lot of effort to establish the more useful inequalities. With some regret, we have chosen not to
include this material because the method and results are accessible to a narrower audience.
Finally, we remark that the modiﬁed logarithmic Sobolev inequalities of [BLM03, BBLM05]
also extend to the matrix setting [CT14]. Unfortunately, the matrix variants do not seem to be as
useful as the scalar results.
About This Monograph
This monograph is intended for graduate students and researchers in computational mathematics who want to learn some modern techniques for analyzing random matrices. The preparation
required is minimal. We assume familiarity with calculus, applied linear algebra, the basic theory of normed spaces, and classical probability theory up through the elementary concentration
inequalities (such as Markov and Bernstein). Beyond the basics, which can be gleaned from any
good textbook, we include all the required background in Chapter 2.
The material here is based primarily on the paper “User-Friendly Tail Bounds for Sums of
Random Matrices” by the present author [Tro11c]. There are several signiﬁcant revisions to this
earlier work:
Examples and Applications. Many of the papers on matrix concentration give limited information about how the results can be used to solve problems of interest. A major part of these
notes consists of worked examples and applications that indicate how matrix concentration inequalities apply to practical questions.
Expectation Bounds. This work collects bounds for the expected value of the spectral norm of
a random matrix and bounds for the expectation of the smallest and largest eigenvalues of
a random symmetric matrix. Some of these useful results have appeared piecemeal in the
literature [CGT12a, MJC+14], but they have not been included in a uniﬁed presentation.
Optimality. We explain why each matrix concentration inequality is (nearly) optimal. This presentation includes examples to show that each term in each bound is necessary to describe
some particular phenomenon.
Intrinsic Dimension Bounds. Over the last few years, there have been some reﬁnements to the
basic matrix concentration bounds that improve the dependence on dimension [HKZ12,
Min11]. We describe a new framework that allows us to prove these results with ease.
Lieb’s Theorem. The matrix concentration inequalities in this monograph depend on a deep
theorem [Lie73, Thm. 6] from matrix analysis due to Elliott Lieb. We provide a complete
proof of this result, along with all the background required to understand the argument.
Annotated Bibliography. We have included a list of the major works on matrix concentration,
including a short summary of the main contributions of these papers. We hope this catalog
will be a valuable guide for further reading.
The organization of the notes is straightforward. Chapter 2 contains background material
that is needed for the analysis. Chapter 3 describes the framework for developing exponential
1.8. ABOUT THIS MONOGRAPH
concentration inequalities for matrices. Chapter 4 presents the ﬁrst set of results and examples,
concerning matrix Gaussian and Rademacher series. Chapter 5 introduces the matrix Chernoff
bounds and their applications, and Chapter 6 expands on our discussion of the matrix Bernstein
inequality. Chapter 7 shows how to sharpen some of the results so that they depend on an intrinsic dimension parameter. Chapter 8 contains the proof of Lieb’s theorem. We conclude with
resources on matrix concentration and a bibliography.
To make the presentation smoother, we have not followed all of the conventions for scholarly
articles in journals. In particular, almost all the citations appear in the notes at the end of each
chapter. Our aim has been to explain the ideas as clearly as possible, rather than to interrupt the
narrative with an elaborate genealogy of results.
Matrix Functions &
Probability with Matrices
We begin the main development with a short overview of the background material that is required to understand the proofs and, to a lesser extent, the statements of matrix concentration
inequalities. We have been careful to provide cross-references to these foundational results, so
most readers will be able to proceed directly to the main theoretical development in Chapter 3
or the discussion of speciﬁc random matrix inequalities in Chapters 4, 5, and 6.
Section 2.1 covers material from matrix theory concerning the behavior of matrix functions. Section 2.2 reviews relevant results from probability, especially the parts involving matrices.
Matrix Theory Background
Let us begin with the results we require from the ﬁeld of matrix analysis.
Conventions
We write R and C for the real and complex ﬁelds. A matrix is a ﬁnite, two-dimensional array of
complex numbers. Many parts of the discussion do not depend on the size of a matrix, so we
specify dimensions only when it really matters. Readers who wish to think about real-valued
matrices will ﬁnd that none of the results require any essential modiﬁcation in this setting.
Spaces of Vectors
The symbol Cd denotes the complex linear space consisting of d-dimensional column vectors
with complex entries, equipped with the usual componentwise addition and multiplication by a
CHAPTER 2. MATRIX FUNCTIONS & PROBABILITY WITH MATRICES
complex scalar. We endow this space with the standard ℓ2 inner product
〈x, y〉= x∗y =
for all x, y ∈Cd.
The symbol ∗denotes the complex conjugate of a number, as well as the conjugate transpose of
a vector or matrix. The inner product induces the ℓ2 norm:
∥x∥2 = 〈x, x〉=
for all x ∈Cd.
Similarly, the real linear space Rd consists of d-dimensional column vectors with real entries,
equipped with the usual componentwise addition and multiplication by a real scalar. The inner
product and ℓ2 norm on Rd are deﬁned by the same relations as for Cd.
Spaces of Matrices
We write Md1×d2 for the complex linear space consisting of d1×d2 matrices with complex entries,
equipped with the usual componentwise addition and multiplication by a complex scalar. It is
convenient to identify Cd with the space Md×1. We write Md for the algebra of d × d square,
complex matrices. The term “algebra” just means that we can multiply two matrices in Md to
obtain another matrix in Md.
Topology & Convergence
We can endow the space of matrices with the Frobenius norm:
for B ∈Md1×d2.
Observe that the Frobenius norm on Md×1 coincides with the ℓ2 norm (2.1.1) on Cd.
The Frobenius norm induces a norm topology on the space of matrices. In particular, given
a sequence {Bn : n = 1,2,3,...} ⊂Md1×d2, the symbol
means that
∥Bn −B∥F →0
Open and closed sets are also deﬁned with respect to the Frobenius-norm topology. Every other
norm topology on Md1×d2 induces the same notions of convergence and open sets. We use the
same topology for the normed linear spaces Cd and Md.
Basic Vectors and Matrices
We write 0 for the zero vector or the zero matrix, while I denotes the identity matrix. Occasionally,
we add a subscript to specify the dimension. For instance, Id is the d ×d identity.
The standard basis for the linear space Cd consists of standard basis vectors. The standard
basis vector ek is a column vector with a one in position k and zeros elsewhere. We also write e
for the column vector whose entries all equal one. There is a related notation for the standard
basis of Md1×d2. We write Ejk for the standard basis matrix with a one in position (j,k) and zeros
2.1. MATRIX THEORY BACKGROUND
elsewhere. The dimension of a standard basis vector and a standard basis matrix is typically
determined by the context.
A square matrix Q that satisﬁes QQ∗= I = Q∗Q is called a unitary matrix. We reserve the
letter Q for a unitary matrix. Readers who prefer the real setting may prefer to regard Q as an
orthogonal matrix.
Hermitian Matrices and Eigenvalues
An Hermitian matrix A is a square matrix that satisﬁes A = A∗. A useful intuition from operator theory is that Hermitian matrices are analogous with real numbers, while general square
matrices are analogous with complex numbers.
We write Hd for the collection of d ×d Hermitian matrices. The set Hd is a linear space over
the real ﬁeld. That is, we can add Hermitian matrices and multiply them by real numbers. The
space Hd inherits the Frobenius-norm topology from Md. We adopt Parlett’s convention [Par98]
that bold Latin and Greek letters that are symmetric around the vertical axis (A, H, ..., Y ; ∆, Θ,
..., Ω) always represent Hermitian matrices.
Each Hermitian matrix A ∈Hd has an eigenvalue decomposition
where Q ∈Md is unitary and Λ ∈Hd is diagonal.
The diagonal entries of Λ are real numbers, which are referred to as the eigenvalues of A. The
unitary matrix Q in the eigenvalue decomposition is not determined completely, but the list of
eigenvalues is unique modulo permutations. The eigenvalues of an Hermitian matrix are often
referred to as its spectrum.
We denote the algebraic minimum and maximum eigenvalues of an Hermitian matrix A by
λmin(A) and λmax(A). The extreme eigenvalue maps are positive homogeneous:
λmin(αA) = αλmin(A)
λmax(αA) = αλmax(A)
There is an important relationship between minimum and maximum eigenvalues:
λmin(−A) = −λmax(A).
The fact (2.1.5) warns us that we must be careful passing scalars through an eigenvalue map.
This work rarely requires any eigenvalues of an Hermitian matrix aside from the minimum
and maximum. When they do arise, we usually order the other eigenvalues in the weakly decreasing sense:
λ1(A) ≥λ2(A) ≥··· ≥λd(A)
for A ∈Hd.
On occasion, it is more natural to arrange eigenvalues in the weakly increasing sense:
2(A) ≤··· ≤λ↑
for A ∈Hd.
To prevent confusion, we will accompany this notation with a reminder.
Readers who prefer the real setting may read “symmetric” in place of “Hermitian.” In this
case, the eigenvalue decomposition involves an orthogonal matrix Q. Note, however, that the
term “symmetric” has a different meaning when applied to random variables!
CHAPTER 2. MATRIX FUNCTIONS & PROBABILITY WITH MATRICES
The Trace of a Square Matrix
The trace of a square matrix, denoted by tr, is the sum of its diagonal entries.
for B ∈Md.
The trace is unitarily invariant:
trB = tr(QBQ∗)
for each B ∈Md and each unitary Q ∈Md.
In particular, the existence of an eigenvalue decomposition (2.1.3) shows that the trace of an
Hermitian matrix equals the sum of its eigenvalues.1
Another valuable relation connects the trace with the Frobenius norm:
F = tr(CC ∗) = tr(C ∗C).
for all C ∈Md1×d2.
This expression follows from the deﬁnitions (2.1.2) and (2.1.6) and a short calculation.
The Semideﬁnite Partial Order
A matrix A ∈Hd is positive semideﬁnite when it satisﬁes
for each vector u ∈Cd.
Equivalently, a matrix A is positive semideﬁnite when it is Hermitian and its eigenvalues are all
nonnegative. Similarly, we say that A ∈Hd is positive deﬁnite when
for each nonzero vector u ∈Cd.
Equivalently, A is positive deﬁnite when it is Hermitian and its eigenvalues are all positive.
Positive-semideﬁnite and positive-deﬁnite matrices play a special role in matrix theory, analogous with the role of nonnegative and positive numbers in real analysis. In particular, observe
that the square of an Hermitian matrix is always positive semideﬁnite. The square of a nonsingular Hermitian matrix is always positive deﬁnite.
The family of positive-semideﬁnite matrices in Hd forms a closed convex cone.2 This geometric fact follows easily from the deﬁnition (2.1.9). Indeed, for each vector u ∈Cd, the condition
A ∈Hd : u∗Au ≥0
describes a closed halfspace in Hd. As a consequence, the family of positive-semideﬁnite matrices in Hd is an intersection of closed halfspaces. Therefore, it is a closed convex set. To see why
this convex set is a cone, just note that
A positive semideﬁnite
αA is positive semideﬁnite for α ≥0.
1This fact also holds true for a general square matrix.
2A convex cone is a subset C of a linear space that is closed under conic combinations. That is, τ1x1 +τ2x2 ∈C for all
x1,x2 ∈C and all τ1,τ2 > 0. Equivalently, C is a set that is both convex and positively homogeneous.
2.1. MATRIX THEORY BACKGROUND
Beginning from (2.1.10), similar considerations show that the family of positive-deﬁnite matrices
in Hd forms an (open) convex cone.
We may now deﬁne the semideﬁnite partial order ≼on the real-linear space Hd using the rule
if and only if
H −A is positive semideﬁnite.
In particular, we write A ≽0 to indicate that A is positive semideﬁnite and A ≻0 to indicate that
A is positive deﬁnite. For a diagonal matrix Λ, the expression Λ ≽0 means that each entry of Λ
is nonnegative.
The semideﬁnite order is preserved by conjugation, a simple fact whose importance cannot
be overstated.
Proposition 2.1.1 (Conjugation Rule). Let A and H be Hermitian matrices of the same dimension,
and let B be a general matrix with compatible dimensions. Then
B AB∗≼BHB∗.
Finally, we remark that the trace of a positive-semideﬁnite matrix is at least as large as its
maximum eigenvalue:
λmax(A) ≤tr A
when A is positive semideﬁnite.
This property follows from the deﬁnition of a positive-semideﬁnite matrix and the fact that the
trace of A equals the sum of the eigenvalues.
Standard Matrix Functions
Let us describe the most direct method for extending a function on the real numbers to a function on Hermitian matrices. The basic idea is to apply the function to each eigenvalue of the
matrix to construct a new matrix.
Deﬁnition 2.1.2 (Standard Matrix Function). Let f : I →R where I is an interval of the real line.
Consider a matrix A ∈Hd whose eigenvalues are contained in I. Deﬁne the matrix f (A) ∈Hd using
an eigenvalue decomposition of A:
In particular, we can apply f to a real diagonal matrix by applying the function to each diagonal
It can be veriﬁed that the deﬁnition of f (A) does not depend on which eigenvalue decomposition A = QΛQ∗that we choose. Any matrix function that arises in this fashion is called a standard
matrix function.
To conﬁrm that this deﬁnition is sensible, consider the power function f (t) = t q for a natural
number q. When A is Hermitian, the power function f (A) = Aq, where Aq is the q-fold product
For an Hermitian matrix A, whenever we write the power function Aq or the exponential eA
or the logarithm log A, we are always referring to a standard matrix function. Note that we only
CHAPTER 2. MATRIX FUNCTIONS & PROBABILITY WITH MATRICES
deﬁne the matrix logarithm for positive-deﬁnite matrices, and non-integer powers are only valid
for positive-semideﬁnite matrices.
The following result is an immediate, but important, consequence of the deﬁnition of a standard matrix function.
Proposition 2.1.3 (Spectral Mapping Theorem). Let f : I →R be a function on an interval I of
the real line, and let A be an Hermitian matrix whose eigenvalues are contained in I. If λ is an
eigenvalue of A, then f (λ) is an eigenvalue of f (A).
When a real function has a power series expansion, we can also represent the standard matrix
function with the same power series expansion. Indeed, suppose that f : I →R is deﬁned on an
interval I of the real line, and assume that the eigenvalues of A are contained in I. Then
f (a) = c0 +
f (A) = c0I+
This formula can be veriﬁed using an eigenvalue decomposition of A and the deﬁnition of a
standard matrix function.
The Transfer Rule
In most cases, the “obvious” generalization of an inequality for real-valued functions fails to hold
in the semideﬁnite order. Nevertheless, there is one class of inequalities for real functions that
extends to give semideﬁnite relationships for standard matrix functions.
Proposition 2.1.4 (Transfer Rule). Let f and g be real-valued functions deﬁned on an interval I
of the real line, and let A be an Hermitian matrix whose eigenvalues are contained in I. Then
f (a) ≤g(a)
for each a ∈I
f (A) ≼g(A).
Proof. Decompose A = QΛQ∗. It is immediate that f (Λ) ≼g(Λ). The Conjugation Rule (2.1.12)
allows us to conjugate this relation by Q. Finally, we invoke Deﬁnition 2.1.2, of a standard matrix
function, to complete the argument.
The Matrix Exponential
For any Hermitian matrix A, we can introduce the matrix exponential eA using Deﬁnition 2.1.2.
Equivalently, we can use a power series expansion:
eA = exp(A) = I+
The Spectral Mapping Theorem, Proposition 2.1.3, implies that the exponential of an Hermitian
matrix is always positive deﬁnite.
We often work with the trace of the matrix exponential:
trexp : A 7−→treA.
This function has a monotonicity property that we use extensively. For Hermitian matrices A
and H with the same dimension,
treA ≤treH.
We establish this result in §8.3.2.
2.1. MATRIX THEORY BACKGROUND
The Matrix Logarithm
We can deﬁne the matrix logarithm as a standard matrix function. The matrix logarithm is also
the functional inverse of the matrix exponential:
for each Hermitian matrix A.
A valuable fact about the matrix logarithm is that it preserves the semideﬁnite order. For positivedeﬁnite matrices A and H with the same dimension,
log A ≼logH.
We establish this result in §8.4.4. Let us stress that the matrix exponential does not have any
operator monotonicity property analogous with (2.1.18)!
Singular Values of Rectangular Matrices
A general matrix does not have an eigenvalue decomposition, but it admits a different representation that is just as useful. Every d1 ×d2 matrix B has a singular value decomposition
where Q1 and Q2 are unitary and Σ is nonnegative diagonal.
The unitary matrices Q1 and Q2 have dimensions d1 × d1 and d2 × d2, respectively. The inner
matrix Σ has dimension d1×d2, and we use the term diagonal in the sense that only the diagonal
entries (Σ)j j may be nonzero.
The diagonal entries of Σ are called the singular values of B, and they are denoted as σj (B).
The singular values are determined completely modulo permutations, and it is conventional to
arrange them in weakly decreasing order:
σ1(B) ≥σ2(B) ≥··· ≥σmin{d1, d2}(B).
There is an important relationship between singular values and eigenvalues. A general matrix
has two squares associated with it, BB∗and B∗B, both of which are positive semideﬁnite. We
can use a singular value decomposition of B to construct eigenvalue decompositions of the two
BB∗= Q1(ΣΣ∗)Q∗
B∗B = Q2(Σ∗Σ)Q∗
The two squares of Σ are square, diagonal matrices with nonnegative entries. Conversely, we can
always extract a singular value decomposition from the eigenvalue decompositions of the two
We can write the Frobenius norm of a matrix in terms of the singular values:
min{d1,d2}
for B ∈Md1×d2.
This expression follows from the expression (2.1.8) for the Frobenius norm, the property (2.1.20)
of the singular value decomposition, and the unitary invariance (2.1.7) of the trace.
CHAPTER 2. MATRIX FUNCTIONS & PROBABILITY WITH MATRICES
The Spectral Norm
The spectral norm of an Hermitian matrix A is deﬁned by the relation
λmax(A), −λmin(A)
For a general matrix B, the spectral norm is deﬁned to be the largest singular value:
∥B∥= σ1(B).
These two deﬁnitions are consistent for Hermitian matrices because of (2.1.20). When applied
to a row vector or a column vector, the spectral norm coincides with the ℓ2 norm (2.1.1).
We will often need the fact that
∥B∥2 = ∥BB∗∥= ∥B∗B∥.
This identity also follows from (2.1.20).
The Stable Rank
In several of the applications, we need an analytic measure of the collinearity of the rows and
columns of a matrix called the stable rank. For a general matrix B, the stable rank is deﬁned as
srank(B) =
The stable rank is a lower bound for the algebraic rank:
1 ≤srank(B) ≤rank(B).
This point follows when we use (2.1.21) and (2.1.23) to express the two norms in terms of the
singular values of B. In contrast to the algebraic rank, the stable rank is a continuous function of
the matrix, so it is more suitable for numerical applications.
An extraordinarily fruitful idea from operator theory is to embed matrices within larger block
matrices, called dilations. Dilations have an almost magical power. In this work, we will use dilations to extend matrix concentration inequalities from Hermitian matrices to general matrices.
Deﬁnition 2.1.5 (Hermitian Dilation). The Hermitian dilation
H : Md1×d2 −→Hd1+d2
is the map from a general matrix to an Hermitian matrix deﬁned by
2.2. PROBABILITY WITH MATRICES
It is clear that the Hermitian dilation is a real-linear map. Furthermore, the dilation retains
important spectral information. To see why, note that the square of the dilation satisﬁes
We discover that the squared eigenvalues of H (B) coincide with the squared singular values of
B, along with an appropriate number of zeros. As a consequence, ∥H (B)∥= ∥B∥. Moreover,
λmax(H (B)) = ∥H (B)∥= ∥B∥.
We will invoke the identity (2.1.28) repeatedly.
One way to justify the ﬁrst relation in (2.1.28) is to introduce the ﬁrst columns u1 and u2 of
the unitary matrices Q1 and Q2 that appear in the singular value decomposition B = Q1ΣQ∗
Then we may calculate that
∥B∥= Re(u∗
1 Bu2) = 1
≤λmax(H (B)) ≤∥H (B)∥= ∥B∥.
Indeed, the spectral norm of B equals its largest singular value σ1(B), which coincides with
1 Bu2 by construction of u1 and u2. The second identity relies on a direct calculation. The ﬁrst
inequality follows from the variational representation of the maximum eigenvalue as a Rayleigh
quotient; this fact can also be derived as a consequence of (2.1.3). The second inequality depends on the deﬁnition (2.1.22) of the spectral norm of an Hermitian matrix.
Other Matrix Norms
There are a number of other matrix norms that arise sporadically in this work. The Schatten
1-norm of a matrix can be deﬁned as the sum of its singular values:
min{d1,d2}
for B ∈Md1×d2.
The entrywise ℓ1 norm of a matrix is deﬁned as
for B ∈Md1×d2.
We always have the relation
for B ∈Md1×d2
because of the Cauchy–Schwarz inequality.
Probability with Matrices
We continue with some material from probability, focusing on connections with matrices.
CHAPTER 2. MATRIX FUNCTIONS & PROBABILITY WITH MATRICES
Conventions
We prefer to avoid abstraction and unnecessary technical detail, so we frame the standing assumption that all random variables are sufﬁciently regular that we are justiﬁed in computing
expectations, interchanging limits, and so forth. The manipulations we perform are valid if we
assume that all random variables are bounded, but the results hold in broader circumstances if
we instate appropriate regularity conditions.
Since the expectation operator is linear, we typically do not use parentheses with it. We instate the convention that powers and products take precedence over the expectation operator.
In particular,
EX q = E(X q).
This position helps us reduce the clutter of parentheses. We sometimes include extra delimiters
when it is helpful for clarity.
Some Scalar Random Variables
We use consistent notation for some of the basic scalar random variables.
Standard normal variables. We reserve the letter γ for a NORMAL(0,1) random variable. That is,
γ is a real Gaussian with mean zero and variance one.
Rademacher random variables. We reserve the letter ϱ for a random variable that takes the two
values ±1 with equal probability.
Bernoulli random variables. A BERNOULLI(p) random variable takes the value one with probability p and the value zero with probability 1−p, where p ∈ . We use the letters δ and
ξ for Bernoulli random variables.
Random Matrices
Let (Ω,F,P) be a probability space. A random matrix Z is a measurable map
Z : Ω−→Md1×d2.
It is more natural to think of the entries of Z as complex random variables that may or may not
be correlated with each other. We reserve the letters X and Y for random Hermitian matrices,
while the letter Z denotes a general random matrix.
A ﬁnite sequence {Zk} of random matrices is independent when
P{Zk ∈Fk for each k} =
k P{Zk ∈Fk}
for every collection {Fk} of Borel subsets of Md1×d2.
Expectation
The expectation of a random matrix Z = [Zjk] is simply the matrix formed by taking the componentwise expectation. That is,
(EZ )jk = EZjk.
2.2. PROBABILITY WITH MATRICES
Under mild assumptions, expectation commutes with linear and real-linear maps. Indeed, expectation commutes with multiplication by a ﬁxed matrix:
E(B Z ) = B (EZ )
E(Z B) = (EZ )B.
In particular, the product rule for the expectation of independent random variables extends to
E(SZ ) = (ES)(EZ )
when S and Z are independent.
We use these identities liberally, without any further comment.
Inequalities for Expectation
Markov’s inequality states that a nonnegative (real) random variable X obeys the probability
P{X ≥t} ≤EX
for t > 0.
The Markov inequality is a central tool for establishing concentration inequalities.
Jensen’s inequality describes how averaging interacts with convexity. Let Z be a random matrix, and let h be a real-valued function on matrices. Then
Eh(Z ) ≤h(EZ )
when h is concave, and
Eh(Z ) ≥h(EZ )
when h is convex.
The family of positive-semideﬁnite matrices in Hd forms a convex cone, and the expectation
of a random matrix can be viewed as a convex combination. Therefore, expectation preserves
the semideﬁnite order:
We use this result many times without direct reference.
The Variance of a Random Hermitian Matrix
The variance of a real random variable Y is deﬁned as the expected squared deviation from the
Var(Y ) = E(Y −EY )2
There are a number of natural extensions of this concept in the matrix setting that play a role in
our theory.
Suppose that Y is a random Hermitian matrix. We can deﬁne a matrix-valued variance:
Var(Y ) = E(Y −EY )2 = EY 2 −(EY )2.
The matrix Var(Y ) is always positive semideﬁnite. We can interpret the (j,k) entry of this matrix
as the covariance between the jth and kth columns of Y :
(Var(Y ))jk = E
(y:j −E y:j )∗(y:k −E y:k)
where we have written y:j for the jth column of Y .
CHAPTER 2. MATRIX FUNCTIONS & PROBABILITY WITH MATRICES
The matrix-valued variance contains a lot of information about the ﬂuctuations of the random matrix. We can summarize Var(Y ) using a single number v(Y ), which we call the matrix
variance statistic:
v(Y ) = ∥Var(Y )∥= ∥E(Y −EY )2∥.
To understand what this quantity means, one may wish to rewrite it as
v(Y ) = sup
E∥(Y u)−E(Y u)∥2 .
Roughly speaking, the matrix variance statistic describes the maximum variance of Y u for any
unit vector u.
The Variance of a Sum of Independent, Random Hermitian Matrices
The matrix-valued variance interacts beautifully with a sum of independent random matrices.
Consider a ﬁnite sequence {Xk} of independent, random Hermitian matrices with common dimension d. Introduce the sum Y = P
k Xk. Then
Var(Y ) = Var
k(Xk −EXk)
(X j −EX j )(Xk −EXk)
k E(Xk −EXk)2
k Var(Xk).
This identity matches the familiar result for the variance of a sum of independent scalar random
variables. It follows that the matrix variance statistic satisﬁes
The fact that the sum remains inside the norm is very important. Indeed, the best general inequalities between v(Y ) and the matrix variance statistics v(Xk) of the summands are
k v(Xk) ≤d · v(Y ).
These relations can be improved in some special cases. For example, when the matrices Xk are
identically distributed, the left-hand inequality becomes an identity.
The Variance of a Rectangular Random Matrix
We will often work with non-Hermitian random matrices. In this case, we need to account for
the fact that a general matrix has two different squares. Suppose that Z is a random matrix with
dimension d1 ×d2. Deﬁne
Var1(Z ) = E
(Z −EZ )(Z −EZ )∗¤
Var2(Z ) = E
(Z −EZ )∗(Z −EZ )
The matrix Var1(Z ) is a positive-semideﬁnite matrix with dimension d1×d1, and it describes the
ﬂuctuation of the rows of Z . The matrix Var2(Z ) is a positive-semideﬁnite matrix with dimension
d2 ×d2, and it reﬂects the ﬂuctuation of the columns of Z . For an Hermitian random matrix Y ,
Var(Y ) = Var1(Y ) = Var2(Y ).
2.3. NOTES
In other words, the two variances coincide in the Hermitian setting.
As before, it is valuable to reduce these matrix-valued variances to a single scalar parameter.
We deﬁne the matrix variance statistic of a general random matrix Z as
v(Z ) = max
∥Var1(Z )∥, ∥Var2(Z )∥
When Z is Hermitian, the deﬁnition (2.2.8) coincides with the original deﬁnition (2.2.4).
To promote a deeper appreciation for the formula (2.2.8), let us explain how it arises from the
Hermitian dilation (2.1.26). By direct calculation,
Var(H (Z )) = E
·(Z −EZ )(Z −EZ )∗
(Z −EZ )∗(Z −EZ )
The ﬁrst identity is the deﬁnition (2.2.3) of the matrix-valued variance. The second line follows
from the formula (2.1.27) for the square of the dilation. The last identity depends on the deﬁnition (2.2.7) of the two matrix-valued variances. Therefore, using the deﬁnitions (2.2.4) and (2.2.8)
of the matrix variance statistics,
v(H (Z )) = ∥Var(H (Z )∥= max
∥Var1(Z )∥, ∥Var2(Z )∥
The second identity holds because the spectral norm of a block-diagonal matrix is the maximum
norm achieved by one of the diagonal blocks.
The Variance of a Sum of Independent Random Matrices
As in the Hermitian case, the matrix-valued variances interact nicely with an independent sum.
Consider a ﬁnite sequence {Sk} of independent random matrices with the same dimension.
Form the sum Z = P
k Sk. Repeating the calculation leading up to (2.2.6), we ﬁnd that
Var1(Z ) =
k Var1(Sk)
Var2(Z ) =
k Var2(Sk).
In summary, the matrix variance statistic of an independent sum satisﬁes
v(Z ) = max
k Var1(Sk)
k Var2(Sk)
This formula arises time after time.
Everything in this chapter is ﬁrmly established. We have culled the results that are relevant to our
discussion. Let us give some additional references for readers who would like more information.
CHAPTER 2. MATRIX FUNCTIONS & PROBABILITY WITH MATRICES
Matrix Analysis
Our treatment of matrix analysis is drawn from Bhatia’s excellent books on matrix analysis [Bha97,
Bha07]. The two books [HJ13, HJ94] of Horn & Johnson also serve as good general references.
Higham’s work [Hig08] is a generous source of information about matrix functions. Other valuable resources include Carlen’s lecture notes [Car10], the book of Petz [Pet11], and the book of
Hiai & Petz [HP14].
Probability with Matrices
The classic introduction to probability is the two-volume treatise [Fel68, Fel71] of Feller. The
book [GS01] of Grimmett & Stirzaker offers a good treatment of probability theory and random
processes at an intermediate level. For a more theoretical presentation, consider the book [Shi96]
of Shiryaev.
There are too many books on random matrix theory for us to include a comprehensive list;
here is a selection that the author ﬁnds useful. Tao’s book [Tao12] gives a friendly introduction to some of the major aspects of classical and modern random matrix theory. The lecture
notes [Kem13] of Kemp are also extremely readable. The survey of Vershynin [Ver12] provides a
good summary of techniques from asymptotic convex geometry that are relevant to random matrix theory. The works of Mardia, Kent, & Bibby [MKB79] and Muirhead [Mui82] present classical
results on random matrices that are particularly useful in statistics, while Bai & Silverstein [BS10]
contains a comprehensive modern treatment. Nica and Speicher [NS06] offer an entrée to the
beautiful ﬁeld of free probability. Mehta’s treatise [Meh04] was the ﬁrst book on random matrix
theory available, and it remains solid.
The Matrix
Laplace Transform Method
This chapter contains the core part of the analysis that ultimately delivers matrix concentration
inequalities. Readers who are only interested in the concentration inequalities themselves or the
example applications may wish to move on to Chapters 4, 5, and 6.
In the scalar setting, the Laplace transform method provides a simple but powerful way to
develop concentration inequalities for a sum of independent random variables. This technique
is sometimes referred to as the “Bernstein trick” or “Chernoff bounding.” For a primer, we recommend [BLM13, Chap. 2].
In the matrix setting, there is a very satisfactory extension of this argument that allows us to
prove concentration inequalities for a sum of independent random matrices. As in the scalar
case, the matrix Laplace transform method is both easy to use and incredibly useful. In contrast
to the scalar case, the arguments that lead to matrix concentration are no longer elementary. The
purpose of this chapter is to install the framework we need to support these results. Fortunately,
in practical applications, all of the technical difﬁculty remains invisible.
We ﬁrst deﬁne matrix analogs of the moment generating function and the cumulant generating function, which pack up information about the ﬂuctuations of a random Hermitian matrix.
Section 3.2 explains how we can use the matrix mgf to obtain probability inequalities for the
maximum eigenvalue of a random Hermitian matrix. The next task is to develop a bound for the
mgf of a sum of independent random matrices using information about the summands. In §3.3,
we discuss the challenges that arise; §3.4 presents the ideas we need to overcome these obstacles. Section 3.5 establishes that the classical result on additivity of cumulants has a companion
in the matrix setting. This result allows us to develop a collection of abstract probability inequalities in §3.6 that we can specialize to obtain matrix Chernoff bounds, matrix Bernstein bounds,
and so forth.
CHAPTER 3. THE MATRIX LAPLACE TRANSFORM METHOD
Matrix Moments and Cumulants
At the heart of the Laplace transform method are the moment generating function (mgf) and
the cumulant generating function (cgf) of a random variable. We begin by presenting matrix
versions of the mgf and cgf.
Deﬁnition 3.1.1 (Matrix Mgf and Cgf). Let X be a random Hermitian matrix. The matrix moment
generating function MX and the matrix cumulant generating function ΞX are given by
MX (θ) = EeθX
ΞX (θ) = log EeθX
Note that the expectations may not exist for all values of θ.
The matrix mgf MX and matrix cgf ΞX contain information about how much the random matrix
X varies. We aim to exploit the data encoded in these functions to control the eigenvalues.
Let us take a moment to expand on Deﬁnition 3.1.1; this discussion is not important for subsequent developments. Observe that the matrix mgf and cgf have formal power series expansions:
MX (θ) = I+
We call the coefﬁcients EX q matrix moments, and we refer to Ψq as a matrix cumulant. The
matrix cumulant Ψq has a formal expression as a (noncommutative) polynomial in the matrix
moments up to order q. In particular, the ﬁrst cumulant is the mean and the second cumulant
is the variance:
Ψ2 = EX 2 −(EX )2 = Var(X )
The matrix variance was introduced in (2.2.3). Higher-order cumulants are harder to write down
and interpret.
The Matrix Laplace Transform Method
In the scalar setting, the Laplace transform method allows us to obtain tail bounds for a random
variable in terms of its mgf. The starting point for our theory is the observation that a similar
result holds in the matrix setting.
Proposition 3.2.1 (Tail Bounds for Eigenvalues). Let Y be a random Hermitian matrix. For all
P{λmax(Y ) ≥t} ≤inf
θ>0 e−θt EtreθY ,
P{λmin(Y ) ≤t} ≤inf
θ<0 e−θt EtreθY .
In words, we can control the tail probabilities of the extreme eigenvalues of a random matrix
by producing a bound for the trace of the matrix mgf. The proof of this fact parallels the classical
argument, but there is a twist.
Proof. We begin with (3.2.1). Fix a positive number θ, and observe that
P{λmax(Y ) ≥t} = P
eθλmax(Y ) ≥eθto
≤e−θt Eeθλmax(Y ) = e−θt Eeλmax(θY )
3.2. THE MATRIX LAPLACE TRANSFORM METHOD
The ﬁrst identity holds because a 7→eθa is a monotone increasing function, so the event does not
change under the mapping. The second relation is Markov’s inequality (2.2.1). The last holds because the maximum eigenvalue is a positive-homogeneous map, as stated in (2.1.4). To control
the exponential, note that
eλmax(θY ) = λmax
The ﬁrst identity depends on the Spectral Mapping Theorem, Proposition 2.1.3, and the fact that
the exponential function is increasing. The inequality follows because the exponential of an Hermitian matrix is positive deﬁnite, and (2.1.13) shows that the maximum eigenvalue of a positivedeﬁnite matrix is dominated by the trace. Combine the latter two displays to reach
P{λmax(Y ) ≥t} ≤e−θt EtreθY .
This inequality is valid for any positive θ, so we may take an inﬁmum to achieve the tightest
possible bound.
To prove (3.2.2), we use a similar approach. Fix a negative number θ, and calculate that
P{λmin(Y ) ≤t} = P
eθλmin(Y ) ≥eθto
≤e−θt Eeθλmin(Y ) = e−θt Eeλmax(θY ).
The function a 7→eθa reverses the inequality in the event because it is monotone decreasing. The
last identity depends on the relationship (2.1.5) between minimum and maximum eigenvalues.
Finally, we introduce the inequality (3.2.3) for the trace exponential and minimize over negative
values of θ.
In the proof of Proposition 3.2.1, it may seem crude to bound the maximum eigenvalue by the
trace. In fact, our overall approach leads to matrix concentration inequalities that are sharp for
speciﬁc examples (see the discussion in §§4.1.2, 5.1.2, and 6.1.2), so we must conclude that the
loss in this bound is sometimes inevitable. At the same time, this maneuver allows us to exploit
some amazing convexity properties of the trace exponential.
We can adapt the proof of Proposition 3.2.1 to obtain bounds for the expectation of the maximum eigenvalue of a random Hermitian matrix. This argument does not have a perfect analog
in the scalar setting.
Proposition 3.2.2 (Expectation Bounds for Eigenvalues). Let Y be a random Hermitian matrix.
Eλmax(Y ) ≤inf
θ log EtreθY ,
Eλmin(Y ) ≥sup
θ log EtreθY .
Proof. We establish the bound (3.2.4); the proof of (3.2.5) is quite similar. Fix a positive number
θ, and calculate that
Eλmax(Y ) = 1
θ E logeλmax(θY ) ≤1
θ log Eeλmax(θY ) = 1
θ log Eλmax
θ log EtreθY .
The ﬁrst identity holds because the maximum eigenvalue is a positive-homogeneous map, as
stated in (2.1.4). The second relation is Jensen’s inequality. The third follows when we use the
Spectral Mapping Theorem, Proposition 2.1.3, to draw the eigenvalue map through the exponential. The ﬁnal inequality depends on the fact (2.1.13) that the trace of a positive-deﬁnite
matrix dominates the maximum eigenvalue.
CHAPTER 3. THE MATRIX LAPLACE TRANSFORM METHOD
The Failure of the Matrix Mgf
We would like the use the Laplace transform bounds from Section 3.2 to study a sum of independent random matrices. In the scalar setting, the Laplace transform method is effective for
studying an independent sum because the mgf and the cgf decompose. In the matrix case, the
situation is more subtle, and the goal of this section is to indicate where things go awry.
Consider an independent sequence {Xk} of real random variables. The mgf of the sum satis-
ﬁes a multiplication rule:
k Xk)(θ) = Eexp
k MXk (θ).
The ﬁrst identity is the deﬁnition of an mgf. The second relation holds because the exponential
map converts a sum of real scalars to a product, and the third relation requires the independence
of the random variables. The last identity, again, is the deﬁnition.
At ﬁrst, we might imagine that a similar relationship holds for the matrix mgf. Consider an
independent sequence {Xk} of random Hermitian matrices. Perhaps,
k MXk (θ).
Unfortunately, this hope shatters when we subject it to interrogation.
It is not hard to ﬁnd the reason that (3.3.2) fails. The identity (3.3.1) depends on the fact that
the scalar exponential converts a sum into a product. In contrast, for Hermitian matrices,
eA+H ̸= eAeH
unless A and H commute.
If we introduce the trace, the situation improves somewhat:
treA+H ≤treAeH
for all Hermitian A and H.
The result (3.3.3) is known as the Golden–Thompson inequality, a famous theorem from statistical physics. Unfortunately, the analogous bound may fail for three matrices:
treA+H+T ̸≤treAeHeT
for certain Hermitian A,H, and T .
It seems that we have reached an impasse.
What if we consider the cgf instead? The cgf of a sum of independent real random variables
satisﬁes an addition rule:
k Xk)(θ) = log Eexp
k ΞXk (θ).
The relation (3.3.4) follows when we extract the logarithm of the multiplication rule (3.3.1). This
result looks like a more promising candidate for generalization because a sum of Hermitian matrices remains Hermitian. We might hope that
k ΞXk (θ).
As stated, this putative identity also fails. Nevertheless, the addition rule (3.3.4) admits a very satisfactory extension to matrices. In contrast with the scalar case, the proof involves much deeper
considerations.
3.4. A THEOREM OF LIEB
A Theorem of Lieb
To ﬁnd the appropriate generalization of the addition rule for cgfs, we turn to the literature on
matrix analysis. Here, we discover a famous result of Elliott Lieb on the convexity properties of
the trace exponential function.
Theorem 3.4.1 (Lieb). Fix an Hermitian matrix H with dimension d. The function
A 7−→trexp(H +log A)
is a concave map on the convex cone of d ×d positive-deﬁnite matrices.
In the scalar case, the analogous function a 7→exp(h + loga) is linear, so this result describes
a new type of phenomenon that emerges when we move to the matrix setting. We present a
complete proof of Theorem 3.4.1 in Chapter 8.
For now, let us focus on the consequences of this remarkable result. Lieb’s Theorem is valuable to us because the Laplace transform bounds from Section 3.2 involve the trace exponential
function. To highlight the connection, we rephrase Theorem 3.4.1 in probabilistic terms.
Corollary 3.4.2. Let H be a ﬁxed Hermitian matrix, and let X be a random Hermitian matrix of
the same dimension. Then
Etrexp(H + X ) ≤trexp
H +log EeX ¢
Proof. Introduce the random matrix Y = eX . Then
Etrexp(H + X ) = Etrexp(H +logY )
≤trexp(H +log EY ) = trexp
H +log EeX ¢
The ﬁrst identity follows from the interpretation (2.1.17) of the matrix logarithm as the functional
inverse of the matrix exponential. Theorem 3.4.1 shows that the trace function is concave in Y ,
so Jensen’s inequality (2.2.2) allows us to draw the expectation inside the function.
Subadditivity of the Matrix Cgf
We are now prepared to generalize the addition rule (3.3.4) for scalar cgfs to the matrix setting.
The following result is fundamental to our approach to random matrices.
Lemma 3.5.1 (Subadditivity of Matrix Cgfs). Consider a ﬁnite sequence {Xk} of independent, random, Hermitian matrices of the same dimension. Then
k log EeθXk
Equivalently,
The parallel between the additivity rule (3.3.4) and the subadditivity rule (3.5.2) is striking.
With our level of preparation, it is easy to prove this result. We just apply the bound from Corollary 3.4.2 repeatedly.
CHAPTER 3. THE MATRIX LAPLACE TRANSFORM METHOD
Proof. Without loss of generality, we assume that θ = 1 by absorbing the parameter into the random matrices. Let Ek denote the expectation with respect to Xk, the remaining random matrices
held ﬁxed. Abbreviate
Ξk = log Ek eXk = log EeXk .
We may calculate that
= EEn trexp
k=1 Xk + Xn
k=1 Xk +log En eXn
= EEn−1 trexp
k=1 Xk + Xn−1 +Ξn
≤EEn−2 trexp
k=1 Xk +Ξn−1 +Ξn
We can introduce iterated expectations because of the tower property of conditional expectation.
To bound the expectation Em for an index m = 1,2,3,...,n, we invoke Corollary 3.4.2 with the
ﬁxed matrix H equal to
This argument is legitimate because Hm is independent from Xm.
The formulation (3.5.2) follows from (3.5.1) when we substitute the expression (3.1.1) for the
matrix cgf and make some algebraic simpliﬁcations.
Master Bounds for Sums of Independent Random Matrices
Finally, we can present some general results on the behavior of a sum of independent random
matrices. At this stage, we simply combine the Laplace transform bounds with the subadditivity
of the matrix cgf to obtain abstract inequalities. Later, we will harness properties of the summands to develop more concrete estimates that apply to speciﬁc examples of interest.
Theorem 3.6.1 (Master Bounds for a Sum of Independent Random Matrices). Consider a ﬁnite
sequence {Xk} of independent, random, Hermitian matrices of the same size. Then
θ log trexp
k log EeθXk
θ log trexp
k log EeθXk
Furthermore, for all t ∈R,
θ>0 e−θt trexp
k log EeθXk
θ<0 e−θt trexp
k log EeθXk
Proof. Substitute the subadditivity rule for matrix cgfs, Lemma 3.5.1, into the two matrix Laplace
transform results, Proposition 3.2.1 and Proposition 3.2.2.
3.7. NOTES
In this chapter, we have focused on probability inequalities for the extreme eigenvalues of a
sum of independent random matrices. Nevertheless, these results also give information about
the spectral norm of a sum of independent, random, rectangular matrices because we can apply
them to the Hermitian dilation (2.1.26) of the sum. Instead of presenting a general theorem, we
ﬁnd it more natural to extend individual results to the non-Hermitian case.
This section includes some historical discussion about the results we have described in this
chapter, along with citations for the results that we have established.
The Matrix Laplace Transform Method
The idea of lifting the “Bernstein trick” to the matrix setting is due to two researchers in quantum information theory, Rudolf Ahlswede and Andreas Winter, who were working on a problem
concerning transmission of information through a quantum channel [AW02]. Their paper contains a version of the matrix Laplace transform result, Proposition 3.2.1, along with a substantial
number of related foundational ideas. Their work is one of the major inspirations for the tools
that are described in these notes.
The statement of Proposition 3.2.1 and the proof that we present appear in the paper [Oli10b]
of Roberto Oliveira. The subsequent result on expectations, Proposition 3.2.2, ﬁrst appeared in
the paper [CGT12a].
Subadditivity of Cumulants
The major impediment to applying the matrix Laplace transform method is the need to produce
a bound for the trace of the matrix moment generating function (the trace mgf). This is where
all the technical difﬁculty in the argument resides.
Ahlswede & Winter [AW02, App.] proposed an approach for bounding the trace mgf of an
independent sum, based on a repeated application of the Golden–Thompson inequality (3.3.3).
Their argument leads to a cumulant bound of the form
log EeXk ¢¢
when the random Hermitian matrices Xk have dimension d. In other words, Ahlswede & Winter
bound the cumulant of a sum in terms of the sum of the maximum eigenvalues of the cumulants. There are cases where the bound (3.7.1) is equivalent with Lemma 3.5.1. For example,
the estimates coincide when each matrix Xk is identically distributed. In general, however, the
estimate (3.7.1) leads to fundamentally weaker results than our bound from Lemma 3.5.1. In
the worst case, the approach of Ahlswede & Winter may produce an unnecessary factor of the
dimension d in the exponent. See [Tro11c, §§3.7, 4.8] for details.
The ﬁrst major technical advance beyond the original argument of Ahlswede & Winter appeared in a paper [Oli10a] of Oliveira. He developed a more effective way to deploy the Golden–
Thompson inequality, and he used this technique to establish a matrix version of Freedman’s
inequality [Fre75]. In the scalar setting, Freedman’s inequality extends the Bernstein concentration inequality to martingales; Oliveira obtained the analogous extension of Bernstein’s inequality for matrix-valued martingales. When specialized to independent sums, his result is quite
CHAPTER 3. THE MATRIX LAPLACE TRANSFORM METHOD
similar with the matrix Bernstein inequality, Theorem 1.6.2, apart from the precise values of the
constants. Oliveira’s method, however, does not seem to deliver the full spectrum of matrix concentration inequalities that we discuss in these notes.
The approach here, based on Lieb’s Theorem, was introduced in the article [Tro11c] by the
author of these notes. This paper was apparently the ﬁrst to recognize that Lieb’s Theorem has
probabilistic content, as stated in Corollary 3.4.2. This idea leads to Lemma 3.5.1, on the subadditivity of cumulants, along with the master tail bounds from Theorem 3.6.1. Note that the two
articles [Oli10a, Tro11c] are independent works.
For a detailed discussion of the beneﬁts of Lieb’s Theorem over the Golden–Thompson inequality, see [Tro11c, §4]. In summary, to get the sharpest concentration results for random matrices, Lieb’s Theorem appears to be indispensible. The approach of Ahlswede & Winter seems
intrinsically weaker. Oliveira’s argument has certain advantages, however, in that it extends from
matrices to the fully noncommutative setting [JZ12].
Subsequent research on the underpinnings of the matrix Laplace transform method has led
to a martingale version of the subadditivity of cumulants [Tro11a, Tro11b]; these works also depend on Lieb’s Theorem. The technical report [GT14] shows how to use a related result, called
the Lieb–Seiringer Theorem [LS05], to obtain upper and lower tail bounds for all eigenvalues of
a sum of independent random Hermitian matrices.
Noncommutative Moment Inequalities
There is a closely related, and much older, line of research on noncommutative moment inequalities. These results provide information about the expected trace of a power of a sum of
independent random matrices. The matrix Laplace transform method, as encapsulated in Theorem 3.6.1, gives analogous bounds for the exponential moments.
Research on noncommutative moment inequalities dates to an important paper [LP86] of
Françoise Lust-Piquard, which contains an operator extension of the Khintchine inequality. Her
result, now called the noncommutative Khintchine inequality, controls the trace moments of a
sum of ﬁxed matrices, each modulated by an independent Rademacher random variable; see
Section 4.7.2 for more details.
In recent years, researchers have generalized many other moment inequalities for a sum of
scalar random variables to matrices (and beyond). For instance, the Rosenthal–Pinelis inequality
for a sum of independent zero-mean random variables admits a matrix version [JZ13, MJC+14,
CGT12a]. We present a variant of the latter result below in (6.1.6). See the paper [JX05] for a good
overview of some other noncommutative moment inequalities.
Finally, and tangentially, we mention that a different notion of matrix moments and cumulants plays a central role in the theory of free probability [NS06].
Quantum Statistical Mechanics
A curious feature of the theory of matrix concentration inequalities is that the most powerful
tools come from the mathematical theory of quantum statistical mechanics. This ﬁeld studies
the bulk statistical properties of interacting quantum systems, and it would seem quite distant
from the ﬁeld of random matrix theory. The connection between these two areas has emerged
because of research on quantum information theory, which studies how information can be encoded, operated upon, and transmitted via quantum mechanical systems.
3.7. NOTES
The Golden–Thompson inequality is a major result from quantum statistical mechanics. Bhatia’s book [Bha97, Sec. IX.3] contains a detailed treatment of this result from the perspective of
matrix theory. For an account with more physical content, see the book of Thirring [Thi02]. The
fact that the Golden–Thompson inequality fails for three matrices can be obtained from simple
examples, such as combinations of Pauli spin matrices [Bha97, Exer. IX.8.4].
Lieb’s Theorem [Lie73, Thm. 6] was ﬁrst established in an important paper of Elliott Lieb on
the convexity of trace functions. His main goal was to establish concavity properties for a function that measures the amount of information in a quantum system. See the notes in Chapter 8
for a more detailed discussion.
Matrix Gaussian Series &
Matrix Rademacher Series
In this chapter, we present our ﬁrst set of matrix concentration inequalities. These results provide spectral information about a sum of ﬁxed matrices, each modulated by an independent
scalar random variable. This type of formulation is surprisingly versatile, and it captures a range
of interesting examples. Our main goal, however, is to introduce matrix concentration in the
simplest setting possible.
To be more precise about our scope, let us introduce the concept of a matrix Gaussian series.
Consider a ﬁnite sequence {Bk} of ﬁxed matrices with the same dimension, along with a ﬁnite
sequence {γk} of independent standard normal random variables. We will study the spectral
norm of the random matrix
This expression looks abstract, but it has concrete modeling power. For example, we can express
a Gaussian Wigner matrix, one of the classical random matrices, in this fashion. But the real value
of this approach is that we can use matrix Gaussian series to represent many kinds of random
matrices built from Gaussian random variables. This technique allows us to attack problems that
classical methods do not handle gracefully. For instance, we can easily study a Toeplitz matrix
with Gaussian entries.
Similar ideas allow us to treat a matrix Rademacher series, a sum of ﬁxed matrices modulated
by random signs. (Recall that a Rademacher random variable takes the values ±1 with equal
probability.) The results in this case are almost identical with the results for matrix Gaussian
series, but they allow us to consider new problems. As an example, we can study the expected
spectral norm of a ﬁxed real matrix after ﬂipping the signs of the entries at random.
In §4.1, we begin with an overview of our results for matrix Gaussian series; very similar results
also hold for matrix Rademacher series. Afterward, we discuss the accuracy of the theoretical
CHAPTER 4. MATRIX GAUSSIAN & RADEMACHER SERIES
bounds. The subsequent sections, §§4.2–4.4, describe what the matrix concentration inequalities tell us about some classical and not-so-classical examples of random matrices. Section 4.5
includes an overview of a more substantial application in combinatorial optimization. The ﬁnal
part §4.6 contains detailed proofs of the bounds. We conclude with bibliographical notes.
A Norm Bound for Random Series with Matrix Coefﬁcients
Consider a ﬁnite sequence {bk} of real numbers and a ﬁnite sequence {γk} of independent standard normal random variables. Form the random series Z = P
k γkbk. A routine invocation of
the scalar Laplace transform method demonstrates that
P{|Z| ≥t} ≤2 exp
where v = Var(Z) =
It turns out that the inequality (4.1.1) extends directly to the matrix setting.
Theorem 4.1.1 (Matrix Gaussian & Rademacher Series). Consider a ﬁnite sequence {Bk} of ﬁxed
complex matrices with dimension d1×d2, and let {γk} be a ﬁnite sequence of independent standard
normal variables. Introduce the matrix Gaussian series
Let v(Z ) be the matrix variance statistic of the sum:
v(Z ) = max{∥E(Z Z ∗)∥, ∥E(Z ∗Z )∥}
2v(Z )log(d1 +d2).
Furthermore, for all t ≥0,
P{∥Z ∥≥t} ≤(d1 +d2) exp
The same bounds hold when we replace {γk} by a ﬁnite sequence {ϱk} of independent Rademacher
random variables.
The proof of Theorem 4.1.1 appears below in §4.6.
Discussion
Let us take a moment to discuss the content of Theorem 4.1.1. The main message is that the
expectation of ∥Z ∥is controlled by the matrix variance statistic v(Z ). Furthermore, ∥Z ∥has a
subgaussian tail whose decay rate depends on v(Z ).
The matrix variance statistic v(Z ) deﬁned in (4.1.3) specializes the general formulation (2.2.8).
The second expression (4.1.4) follows from the additivity property (2.2.11) for the variance of an
independent sum. When the summands are Hermitian, observe that the two terms in the maximum coincide. The formulas (4.1.3) and (4.1.4) are a direct extension of the variance that arises
in the scalar bound (4.1.1).
4.1. A NORM BOUND FOR RANDOM SERIES WITH MATRIX COEFFICIENTS
Figure 4.1:
Schematic of tail bound for matrix Gaussian series. Consider a matrix Gaussian series Z with dimension d1 × d2. The tail probability P{∥Z ∥≥t} admits the upper bound
(d1 +d2) exp(−t2/(2v(Z ))), marked as a dark blue curve. This estimate provides no information
below the level t =
2v(Z )log(d1 +d2). This value, the dark red vertical line, coincides with the
upper bound (4.1.5) for E∥Z ∥. As t increases beyond this point, the tail probability decreases at
a subgaussian rate with variance on the order of v(Z ).
As compared with (4.1.1), a new feature of the bound (4.1.6) is the dimensional factor d1+d2.
When d1 = d2 = 1, the matrix bound reduces to the scalar result (4.1.1). In this case, at least,
we have lost nothing by lifting the Laplace transform method to matrices. The behavior of the
matrix tail bound (4.1.6) is more subtle than the behavior of the scalar tail bound (4.1.1). See
Figure 4.1 for an illustration.
Optimality of the Bounds for Matrix Gaussian Series
One may wonder whether Theorem 4.1.1 provides accurate information about the behavior of a
matrix Gaussian series. The answer turns out to be complicated. Here is the executive summary:
the expectation bound (4.1.5) is always quite good, but the tail bound (4.1.6) is sometimes quite
bad. The rest of this section expands on these claims.
The Expectation Bound
Let Z be a matrix Gaussian series of the form (4.1.2). We will argue that
2v(Z )(1+log(d1 +d2)).
CHAPTER 4. MATRIX GAUSSIAN & RADEMACHER SERIES
In other words, the matrix variance v(Z ) is roughly the correct scale for ∥Z ∥2. This pair of estimates is a signiﬁcant achievement because it is quite challenging to compute the norm of a
matrix Gaussian series in general. Indeed, the literature contains very few examples where explicit estimates are available, especially if one desires reasonable constants.
We begin with the lower bound in (4.1.7), which is elementary. Indeed, since the spectral
norm is convex, Jensen’s inequality ensures that
E∥Z ∥2 = E max
∥Z Z ∗∥, ∥Z ∗Z ∥
∥E(Z Z ∗)∥, ∥E(Z ∗Z )∥
The ﬁrst identity follows from (2.1.24), and the last is the deﬁnition (2.2.8) of the matrix variance.
The upper bound in (4.1.7) is a consequence of the tail bound (4.1.6):
2t P{∥Z ∥≥t} dt
2t dt +2(d1 +d2)
t e−t2/(2v(Z )) dt = E2 +2v(Z )(d1 +d2)e−E2/(2v(Z )).
In the ﬁrst step, rewrite the expectation using integration by parts, and then split the integral at
a positive number E. In the ﬁrst term, we bound the probability by one, while the second term
results from the tail bound (4.1.6). Afterward, we compute the integrals explicitly. Finally, select
E2 = 2v(Z )log(d1 +d2) to complete the proof of (4.1.7).
About the Dimensional Factor
At this point, one may ask whether it is possible to improve either side of the inequality (4.1.7).
The answer is negative unless we have additional information about the Gaussian series beyond
the matrix variance statistic v(Z ).
Indeed, for arbitrarily large dimensions d1 and d2, we can exhibit a matrix Gaussian series
where the left-hand inequality in (4.1.7) is correct. That is, E∥Z ∥2 ≈v(Z ) with no additional
dependence on the dimensions d1 or d2. One such example appears below in §4.2.2.
At the same time, for arbitrarily large dimensions d1 and d2, we can construct a matrix Gaussian series where the right-hand inequality in (4.1.7) is correct. That is, E∥Z ∥2 ≈v(Z )log(d1+d2).
See §4.4 for an example.
We can offer a rough intuition about how these two situations differ from each other. The
presence or absence of the dimensional factor log(d1 + d2) depends on how much the coefﬁcients Bk in the matrix Gaussian series Z commute with each other. More commutativity leads
to a logarithm, while less commutativity can sometimes result in cancelations that obliterate the
logarithm. It remains a major open question to ﬁnd a simple quantity, computable from the
coefﬁcients Bk, that decides whether E∥Z ∥2 contains a dimensional factor or not.
In Chapter 7, we will describe a technique that allows us to moderate the dimensional factor
in (4.1.7) for some types of matrix series. But we cannot remove the dimensional factor entirely
with current technology.
The Tail Bound
What about the tail bound (4.1.6) for the norm of the Gaussian series? Here, our results are less
impressive. It turns out that the large-deviation behavior of the spectral norm of a matrix Gaussian series Z is controlled by a statistic v⋆(Z ) called the weak variance:
E|u∗Zw|2 =
k |u∗Bkw|2.
4.2. EXAMPLE: SOME GAUSSIAN MATRICES
The best general inequalities between the matrix variance statistic and the weak variance are
min{d1,d2}· v⋆(Z )
There are examples of matrix Gaussian series that saturate the lower or the upper inequality.
The classical concentration inequality [BLM13, Thm. 5.6] for a function of independent Gaussian random variables implies that
P{∥Z ∥≥E∥Z ∥+ t} ≤e−t2/(2v⋆(Z )).
Let us emphasize that the bound (4.1.8) provides no information about E∥Z ∥; it only tells us
about the probability that ∥Z ∥is larger than its mean.
Together, the last two displays indicate that the exponent in the tail bound (4.1.6) is sometimes too big by a factor min{d1,d2}. Therefore, a direct application of Theorem 4.1.1 can badly
overestimate the tail probability P{∥Z ∥> t} when the level t is large. Fortunately, this problem
is less pronounced with the matrix Chernoff inequalities of Chapter 5 and the matrix Bernstein
inequalities of Chapter 6.
Expectations and Tails
When studying concentration of random variables, it is quite common that we need to use one
method to assess the expected value of the random variable and a separate technique to determine the probability of a large deviation.
The primary value of matrix concentration inequalities inheres in the estimates
that they provide for the expectation of the spectral norm (or maximum eigenvalue or minimum eigenvalue) of a random matrix.
In many cases, matrix concentration bounds provide reasonable information about the tail decay, but there are other situations where the tail bounds are feeble. In this event, we recommend
applying a scalar concentration inequality to control the tails.
Example: Some Gaussian Matrices
Let us try out our methods on two types of Gaussian matrices that have been studied extensively
in the classical literature on random matrix theory. In these cases, precise information about the
spectral distribution is available, which provides a benchmark for assessing our results. We ﬁnd
that bounds based on Theorem 4.1.1 lead to very reasonable estimates, but they are not sharp.
The advantage of our approach is that it applies to every example, whereas we are making comparisons with specialized techniques that only illuminate individual cases. Similar conclusions
hold for matrices with independent Rademacher entries.
Gaussian Wigner Matrices
We begin with a family of Gaussian Wigner matrices. A d × d matrix Wd from this ensemble
is real-symmetric with a zero diagonal; the entries above the diagonal are independent normal
CHAPTER 4. MATRIX GAUSSIAN & RADEMACHER SERIES
variables with mean zero and variance one:
where {γjk : 1 ≤j < k ≤d} is an independent family of standard normal variables. We can represent this matrix compactly as a Gaussian series:
γjk(Ejk +Ek j ).
The norm of a Wigner matrix satisﬁes
as d →∞, almost surely.
For example, see [BS10, Thm. 5.1]. To make (4.2.2) precise, we assume that {Wd} is an independent sequence of Gaussian Wigner matrices, indexed by the dimension d.
Theorem 4.6.1 provides a simple way to bound the norm of a Gaussian Wigner matrix. We
just need to compute the matrix variance statistic v(Wd). The formula (4.1.4) for v(Wd) asks us
to form the sum of the squared coefﬁcients from the representation (4.2.1):
(Ejk +Ek j )2 =
(Ej j +Ekk) = (d −1)Id.
Since the terms in (4.2.1) are Hermitian, we have only one sum of squares to consider. We have
also used the facts that EjkEk j = Ej j while EjkEjk = 0 because of the condition j < k in the limits
of summation. We see that
(Ejk +Ek j )2
°°°°° = ∥(d −1)Id∥= d −1.
The bound (4.1.5) for the expectation of the norm gives
2(d −1)log(2d).
In conclusion, our techniques overestimate ∥Wd∥by a factor of about
0.5logd. The result (4.2.3)
is not perfect, but it only takes two lines of work. In contrast, the classical result (4.2.2) depends
on a long moment calculation that involves challenging combinatorial arguments.
Rectangular Gaussian Matrices
Next, we consider a d1 ×d2 rectangular matrix with independent standard normal entries:
4.3. EXAMPLE: MATRICES WITH RANDOMLY SIGNED ENTRIES
where {γjk} is an independent family of standard normal variables. We can express this matrix
efﬁciently using a Gaussian series:
There is an elegant estimate [DS02, Thm. 2.13] for the norm of this matrix:
The inequality (4.2.5) is sharp when d1 and d2 tend to inﬁnity while the ratio d1/d2 →const.
See [BS10, Thm. 5.8] for details.
Theorem 4.1.1 yields another bound on the expected norm of the matrix G. In order to compute the matrix variance statistic v(G), we calculate the sums of the squared coefﬁcients from
the representation (4.2.4):
Ej j = d2 Id1,
Ekk = d1 Id2.
The matrix variance statistic (4.1.3) satisﬁes
v(G) = max
∥d2 Id1∥, ∥d1 Id2∥
= max{d1, d2}.
We conclude that
2max{d1, d2}log(d1 +d2).
The leading term is roughly correct because
max{d1, d2} ≤2
The logarithmic factor in (4.2.6) does not belong, but it is rather small in comparison with the
leading terms. Once again, we have produced a reasonable result with a short argument based
on general principles.
Example: Matrices with Randomly Signed Entries
Next, we turn to an example that is superﬁcially similar with the matrix discussed in §4.2.2 but is
less understood. Consider a ﬁxed d1 ×d2 matrix B with real entries, and let {ϱjk} be an independent family of Rademacher random variables. Consider the d1 ×d2 random matrix
ϱjkb jkEjk
In other words, we obtain the random matrix B± by randomly ﬂipping the sign of each entry of
B. The expected norm of this matrix satisﬁes the bound
E∥B±∥≤Const· v1/2 ·log1/4 min{d1, d2},
CHAPTER 4. MATRIX GAUSSIAN & RADEMACHER SERIES
where the leading factor v1/2 satisﬁes
maxj ∥b j:∥2 , maxk ∥b:k∥2ª
We have written b j: for the jth row of B and b:k for the kth column of B. In other words, the
expected norm of a matrix with randomly signed entries is comparable with the maximum ℓ2
norm achieved by any row or column. There are cases where the bound (4.3.1) admits a matching
lower bound. These results appear in [Seg00, Thms. 3.1, 3.2] and [BV14, Cor. 4.7].
Theorem 4.1.1 leads to a quick proof of a slightly weaker result. We simply need to compute
the matrix variance statistic v(B±). To that end, note that
(b jkEjk)(b jkEjk)∗=
Similarly,
(b jkEjk)∗(b jkEjk) =
Therefore, using the formula (4.1.4), we ﬁnd that
v(B±) = max
(b jkEjk)(b jkEjk)∗
(b jkEjk)∗(b jkEjk)
maxj ∥b j:∥2 , maxk ∥b:k∥2ª
We see that v(B±) coincides with v, the leading term (4.3.2) in the established estimate (4.3.1)!
Now, Theorem 4.1.1 delivers the bound
2v(B±)log(d1 +d2).
Observe that the estimate (4.3.3) for the norm matches the correct bound (4.3.1) up to the logarithmic factor. Yet again, we obtain a result that is respectably close to the optimal one, even
though it is not quite sharp.
The main advantage of using results like Theorem 4.1.1 to analyze this random matrix is
that we can obtain a good result with a minimal amount of arithmetic. The analysis that leads
to (4.3.1) involves a specialized combinatorial argument.
Example: Gaussian Toeplitz Matrices
Matrix concentration inequalities offer an effective tool for analyzing random matrices whose
dependency structures are more complicated than those of the classical ensembles. In this section, we consider Gaussian Toeplitz matrices, which have applications in signal processing.
We construct an (unsymmetric) d ×d Gaussian Toeplitz matrix Γd by populating the ﬁrst row
and ﬁrst column of the matrix with independent standard normal variables; the entries along
4.4. EXAMPLE: GAUSSIAN TOEPLITZ MATRICES
each diagonal of the matrix take the same value:
where {γk} is an independent family of standard normal variables. As usual, we represent the
Gaussian Toeplitz matrix as a matrix Gaussian series:
Γd = γ0 I+
where C ∈Md denotes the shift-up operator acting on d-dimensional column vectors:
It follows that C k shifts a vector up by k places, introducing zeros at the bottom, while (C k)∗
shifts a vector down by k places, introducing zeros at the top.
We can analyze this example quickly using Theorem 4.1.1. First, note that
To obtain the matrix variance statistic (4.1.4), we calculate the sum of the squares of the coefﬁcient matrices that appear in (4.4.1). In this instance, the two terms in the variance are the same.
We ﬁnd that
(1+(d −j)+(j −1))Ej j = d Id.
In the second line, we (carefully) switch the order of summation and rewrite the identity matrix
as a sum of diagonal standard basis matrices. We reach
v(Γd) = ∥d Id∥= d.
An application of Theorem 4.1.1 leads us to conclude that
2d log(2d).
CHAPTER 4. MATRIX GAUSSIAN & RADEMACHER SERIES
It turns out that the inequality (4.4.3) is correct up to the precise value of the constant, which
does not seem to be known. Nevertheless, the limiting value is available for the top eigenvalue
of a (scaled) symmetric Toeplitz matrix whose ﬁrst row contains independent standard normal
variables [SV13, Thm. 1]. From this result, we may conclude that
2d log(2d)
Here, we take {Γd} to be a sequence of unsymmetric Gaussian Toeplitz matrices, indexed by the
ambient dimension d. Our simple argument gives the right scaling for this problem, and our
estimate for the constant lies within 21% of the optimal value!
Application: Rounding for the MaxQP Relaxation
Our ﬁnal application involves a more substantial question from combinatorial optimization.
One of the methods that has been proposed for solving a certain optimization problem leads to
a matrix Rademacher series, and the analysis of this method requires the spectral norm bounds
from Theorem 4.1.1. A detailed treatment would take us too far aﬁeld, so we just sketch the
context and indicate how the random matrix arises.
There are many types of optimization problems that are computationally difﬁcult to solve
exactly. One approach to solving these problems is to enlarge the constraint set in such a way that
the problem becomes tractable, a process called “relaxation.” After solving the relaxed problem,
we can use a randomized “rounding” procedure to map the solution back to the constraint set
for the original problem. If we can perform the rounding step without changing the value of
the objective function substantially, then the rounded solution is also a decent solution to the
original optimization problem.
One difﬁcult class of optimization problems has a matrix decision variable, and it requires
us to maximize a quadratic form in the matrix variable subject to a set of convex quadratic constraints and a spectral norm constraint [Nem07]. This problem is referred to as MAXQP. The
desired solution B to this problem is a d1 ×d2 matrix. The solution needs to satisfy several different requirements, but we focus on the condition that ∥B∥≤1.
There is a natural relaxation of the MAXQP problem. When we solve the relaxation, we obtain
a family {Bk : k = 1,2,...,n} of d1 ×d2 matrices that satisfy the constraints
k Bk ≼Id2.
In fact, these two bounds are part of the speciﬁcation of the relaxed problem. To round the family
of matrices back to a solution of the original problem, we form the random matrix
where {ϱk} is an independent family of Rademacher random variables. The scaling factor α > 0
can be adjusted to guarantee that the norm constraint ∥Z ∥≤1 holds with high probability.
What is the expected norm of Z ? Theorem 4.1.1 yields
2v(Z )log(d1 +d2).
4.6. ANALYSIS OF MATRIX GAUSSIAN & RADEMACHER SERIES
Here, the matrix variance statistic satisﬁes
v(Z ) = α2 max
owing to the constraint (4.5.1) on the matrices B1,...,Bn. It follows that the scaling parameter α
should satisfy
2log(d1 +d2)
to ensure that E∥Z ∥≤1. For this choice of α, the rounded solution Z obeys the spectral norm
constraint on average. By using the tail bound (4.1.6), we can even obtain high-probability estimates for the norm of the rounded solution Z .
The important fact here is that the scaling parameter α is usually small as compared with
the other parameters of the problem (d1,d2, n, and so forth). Therefore, the scaling does not
have a massive effect on the value of the objective function. Ultimately, this approach leads to a
technique for solving the MAXQP problem that produces a feasible point whose objective value
is within a factor of
2log(d1 +d2) of the maximum objective value possible.
Analysis of Matrix Gaussian & Rademacher Series
We began this chapter with a concentration inequality, Theorem 4.1.1, for the norm of a matrix
Gaussian series, and we have explored a number of different applications of this result. This
section contains a proof of this theorem.
Random Series with Hermitian Coefﬁcients
As the development in Chapter 3 suggests, random Hermitian matrices provide the natural setting for establishing matrix concentration inequalities. Therefore, we begin our treatment with
a detailed statement of the matrix concentration inequality for a Gaussian series with Hermitian
matrix coefﬁcients.
Theorem 4.6.1 (Matrix Gaussian & Rademacher Series: The Hermitian Case). Consider a ﬁnite
sequence {Ak} of ﬁxed Hermitian matrices with dimension d, and let {γk} be a ﬁnite sequence of
independent standard normal variables. Introduce the matrix Gaussian series
Let v(Y ) be the matrix variance statistic of the sum:
v(Y ) = ∥EY 2∥=
Eλmax (Y ) ≤
2v(Y )logd.
Furthermore, for all t ≥0,
P{λmax (Y ) ≥t} ≤d exp
The same bounds hold when we replace {γk} by a ﬁnite sequence of independent Rademacher
random variables.
CHAPTER 4. MATRIX GAUSSIAN & RADEMACHER SERIES
The proof of this result occupies the rest of the section.
Discussion
Before we proceed to the analysis, let us take a moment to compare Theorem 4.6.1 with the result
for general matrix series, Theorem 4.1.1.
First, we consider the matrix variance statistic v(Y ) deﬁned in (4.6.1). Since Y has zero mean,
this deﬁnition coincides with the general formula (2.2.4). The second expression, in terms of
the coefﬁcient matrices, follows from the additivity property (2.2.6) for the variance of a sum of
independent, random Hermitian matrices.
Next, bounds for the minimum eigenvalue λmin(Y ) follow from the results for the maximum
eigenvalue because −Y has the same distribution as Y . Therefore,
Eλmin(Y ) = Eλmin(−Y ) = −Eλmax(Y ) ≥−
2v(Y )logd.
The second identity holds because of the relationship (2.1.5) between minimum and maximum
eigenvalues. Similar considerations lead to a lower tail bound for the minimum eigenvalue:
P{λmin(Y ) ≤−t} ≤d exp
This result follows directly from the upper tail bound (4.6.3).
This observation points to the most important difference between the Hermitian case and
the general case. Indeed, Theorem 4.6.1 concerns the extreme eigenvalues of the random series
Y instead of the norm. This change amounts to producing one-sided tail bounds instead of twosided tail bounds. For Gaussian and Rademacher series, this improvement is not really useful,
but there are random Hermitian matrices whose minimum and maximum eigenvalues exhibit
different types of behavior. For these problems, it can be extremely valuable to examine the two
tails separately. See Chapter 5 and 6 for some results of this type.
Analysis for Hermitian Gaussian Series
We continue with the proof that matrix Gaussian series exhibit the behavior described in Theorem 4.6.1. Afterward, we show how to adapt the argument to address matrix Rademacher series.
Our main tool is Theorem 3.6.1, the set of master bounds for independent sums. To use this
result, we must identify the cgf of a ﬁxed matrix modulated by a Gaussian random variable.
Lemma 4.6.2 (Gaussian × Matrix: Mgf and Cgf). Suppose that A is a ﬁxed Hermitian matrix, and
let γ be a standard normal random variable. Then
EeγθA = eθ2A2/2
log EeγθA = θ2
Proof. We may assume θ = 1 by absorbing θ into the matrix A. It is well known that the moments
of a standard normal variable satisfy
for q = 0,1,2,....
4.6. ANALYSIS OF MATRIX GAUSSIAN & RADEMACHER SERIES
The formula for the odd moments holds because a standard normal variable is symmetric. One
way to establish the formula for the even moments is to use integration by parts to obtain a
recursion for the (2q)th moment in terms of the (2q −2)th moment.
Therefore, the matrix mgf satisﬁes
(2q)! A2q = I+
¢q = eA2/2.
The ﬁrst identity holds because the odd terms vanish from the series representation (2.1.15) of
the matrix exponential when we take the expectation. To compute the cgf, we extract the logarithm of the mgf and recall (2.1.17), which states that the matrix logarithm is the functional
inverse of the matrix exponential.
We quickly reach results on the maximum eigenvalue of a matrix Gaussian series with Hermitian coefﬁcients.
Proof of Theorem 4.6.1: Gaussian Case. Consider a ﬁnite sequence {Ak} of Hermitian matrices
with dimension d, and let {γk} be a ﬁnite sequence of independent standard normal variables.
Deﬁne the matrix Gaussian series
We begin with the upper bound (4.6.2) for Eλmax(Y ). The master expectation bound (3.6.1) from
Theorem 3.6.1 implies that
Eλmax(Y ) ≤inf
θ log trexp
k log EeγkθAk
θ log trexp
logd + θ2v(Y )
The second line follows when we introduce the cgf from Lemma 4.6.2. To reach the third inequality, we bound the trace by the dimension times the maximum eigenvalue. The fourth line
is the Spectral Mapping Theorem, Proposition 2.1.3. Use the formula (4.6.1) to identify the matrix variance statistic v(Y ) in the exponent. The inﬁmum is attained at θ =
2v(Y )−1 logd. This
choice leads to (4.6.2).
Next, we turn to the proof of the upper tail bound (4.6.3) for λmax(Y ). Invoke the master tail
bound (3.6.3) from Theorem 3.6.1, and calculate that
P{λmax(Y ) ≥t} ≤inf
θ>0 e−θt trexp
k log EeγkθAk
θ>0 e−θt trexp
θ>0 e−θt ·d exp
CHAPTER 4. MATRIX GAUSSIAN & RADEMACHER SERIES
θ>0 e−θt+θ2v(Y )/2.
The steps here are the same as in the previous calculation. The inﬁmum is achieved at θ =
t/v(Y ), which yields (4.6.3).
Analysis for Hermitian Rademacher Series
The inequalities for matrix Rademacher series involve arguments closely related to the proofs for
matrix Gaussian series, but we require one additional piece of reasoning to obtain the simplest
results. First, let us compute bounds for the matrix mgf and cgf of a Hermitian matrix modulated
by a Rademacher random variable.
Lemma 4.6.3 (Rademacher × Matrix: Mgf and Cgf). Suppose that A is a ﬁxed Hermitian matrix,
and let ϱ be a Rademacher random variable. Then
EeϱθA ≼eθ2A2/2
log EeϱθA ≼θ2
Proof. First, we establish a scalar inequality. Comparing Taylor series,
2qq! = ea2/2
The inequality holds because (2q)! ≥(2q)(2q −2)···(4)(2) = 2qq!.
To compute the matrix mgf, we may assume θ = 1. By direct calculation,
2e−A = cosh(A) ≼eA2/2.
The semideﬁnite bound follows when we apply the Transfer Rule (2.1.14) to the inequality (4.6.6).
To determine the matrix cgf, observe that
log EeϱA = logcosh(A) ≼1
The semideﬁnite bound follows when we apply the Transfer Rule (2.1.14) to the scalar inequality
logcosh(a) ≤a2/2 for a ∈R, which is a consequence of (4.6.6).
We are prepared to develop some probability inequalities for the maximum eigenvalue of a
Rademacher series with Hermitian coefﬁcients.
Proof of Theorem 4.6.1: Rademacher Case. Consider a ﬁnite sequence {Ak} of Hermitian matrices, and let {ϱk} be a ﬁnite sequence of independent Rademacher variables. Deﬁne the matrix
Rademacher series
The bounds for the extreme eigenvalues of Y follow from an argument almost identical with the
proof in the Gaussian case. The only point that requires justiﬁcation is the inequality
k log EeϱkθAk
To obtain this result, we introduce the semideﬁnite bound, Lemma 4.6.3, for the Rademacher
cgf into the trace exponential. The left-hand side increases after this substitution because of the
fact (2.1.16) that the trace exponential function is monotone with respect to the semideﬁnite
4.7. NOTES
Analysis of Matrix Series with Rectangular Coefﬁcients
Finally, we consider a series with non-Hermitian matrix coefﬁcients modulated by independent
Gaussian or Rademacher random variables. The bounds for the norm of a rectangular series
follow instantly from the bounds for the norm of an Hermitian series because of a formal device.
We simply apply the Hermitian results to the Hermitian dilation (2.1.26) of the series.
Proof of Theorem 4.1.1. Consider a ﬁnite sequence {Bk} of d1 ×d2 complex matrices, and let {ζk}
be a ﬁnite sequence of independent random variables, either standard normal or Rademacher.
Recall from Deﬁnition 2.1.5 that the Hermitian dilation is the map
This leads us to form the two series
Y = H (Z ) =
k ζkH (Bk).
The second expression for Y holds because the Hermitian dilation is real-linear. Since we have
written Y as a matrix series with Hermitian coefﬁcients, we may analyze it using Theorem 4.6.1.
We just need to express the conclusions in terms of the random matrix Z .
First, we employ the fact (2.1.28) that the Hermitian dilation preserves spectral information:
∥Z ∥= λmax(H (Z )) = λmax(Y ).
Therefore, bounds on λmax(Y ) deliver bounds on ∥Z ∥. In view of the calculation (2.2.10) for the
variance statistic of a dilation, we have
v(Y ) = v(H (Z )) = v(Z ).
Recall that the matrix variance statistic v(Z ) deﬁned in (4.1.3) coincides with the general deﬁnition from (2.2.8). Now, invoke Theorem 4.6.1 to obtain Theorem 4.1.1.
We give an overview of research related to matrix Gaussian series, along with references for the
speciﬁc random matrices that we have analyzed.
Matrix Gaussian and Rademacher Series
The main results, Theorem 4.1.1 and Theorem 4.6.1, have an interesting history. In the precise
form presented here, these two statements ﬁrst appeared in [Tro11c], but we can trace them back
more than two decades.
In his work [Oli10b, Thm. 1], Oliveira established the mgf bounds presented in Lemma 4.6.2
and Lemma 4.6.3. He also developed an ingenious improvement on the arguments of Ahlswede
& Winter [AW02, App.], and he obtained a bound similar with Theorem 4.6.1. The constants in
Oliveira’s result are worse, but the dependence on the dimension is better because it depends
on the number of summands. We do not believe that the approach Ahlswede & Winter describe
in [AW02] can deliver any of these results.
Recently, there have been some minor improvements to the dimensional factor that appears
in Theorem 4.6.1. We discuss these results and give citations in Chapter 7.
CHAPTER 4. MATRIX GAUSSIAN & RADEMACHER SERIES
The Noncommutative Khintchine Inequality
Our theory about matrix Rademacher and Gaussian series should be compared with a classic result, called the noncommutative Khintchine inequality, that was originally due to Lust-
Piquard [LP86]; see also the follow-up work [LPP91]. In its simplest form, this inequality concerns a matrix Rademacher series with Hermitian coefﬁcients:
The noncommutative Khintchine inequality states that
for q = 1,2,3,....
The minimum value of the constant C2q = (2q)!/(2q q!) was obtained in the two papers [Buc01,
Buc05]. Traditional proofs of the noncommutative Khintchine inequality are quite involved, but
there is now an elementary argument available [MJC+14, Cor. 7.3].
Theorem 4.6.1 is the exponential moment analog of the polynomial moment bound (4.7.1).
The polynomial moment inequality is somewhat stronger than the exponential moment inequality. Nevertheless, the exponential results are often more useful in practice. For a more
thorough exploration of the relationships between Theorem 4.6.1 and noncommutative moment
inequalities, such as (4.7.1), see the discussion in [Tro11c, §4].
Application to Random Matrices
It has also been known for a long time that results such as Theorem 4.6.1 and inequality (4.7.1)
can be used to study random matrices.
We believe that the geometric functional analysis literature contains the earliest applications
of matrix concentration results to analyze random matrices. In a well-known paper [Rud99],
Mark Rudelson—acting on a suggestion of Gilles Pisier—showed how to use the noncommutative Khintchine inequality (4.7.1) to study covariance estimation. This work led to a signiﬁcant
amount of activity in which researchers used variants of Rudelson’s argument to prove other
types of results. See, for example, the paper [RV07]. This approach is powerful, but it tends to
require some effort to use.
In parallel, other researchers in noncommutative probability theory also came to recognize
the power of noncommutative moment inequalities in random matrix theory. The paper [JX08]
contains a speciﬁc example. Unfortunately, this literature is technically formidable, which makes
it difﬁcult for outsiders to appreciate its achievements.
The work [AW02] of Ahlswede & Winter led to the ﬁrst “packaged” matrix concentration inequalities of the type that we describe in these lecture notes. For the ﬁrst few years after this work,
most of the applications concerned quantum information theory and random graph theory. The
paper [Gro11] introduced the method of Ahlswede & Winter to researchers in mathematical signal processing and statistics, and it served to popularize matrix concentration bounds.
At this point, the available matrix concentration inequalities were still signiﬁcantly suboptimal. The main advances, in [Oli10a, Tro11c], led to optimal matrix concentration results of the
kind that we present in these lecture notes. These results allow researchers to obtain reasonably
accurate analyses of a wide variety of random matrices with very little effort.
4.7. NOTES
Wigner and Marˇcenko–Pastur
Wigner matrices ﬁrst emerged in the literature on nuclear physics, where they were used to
model the Hamiltonians of reactions involving heavy atoms [Meh04, §1.1]. Wigner [Wig55] showed
that the limiting spectral distribution of a certain type of Wigner matrix follows the semicircle
law. See the book [Tao12, §2.4] of Tao for an overview and the book [BS10, Chap. 2] of Bai &
Silverstein for a complete treatment. The Bai–Yin law [BY93] states that, up to scaling, the maximum eigenvalue of a Wigner matrix converges almost surely to two. See [Tao12, §2.3] or [BS10,
Chap. 5] for more information. The analysis of the Gaussian Wigner matrix that we present here,
using Theorem 4.6.1, is drawn from [Tro11c, §4].
The ﬁrst rigorous work on a rectangular Gaussian matrix is due to Marˇcenko & Pastur [MP67],
who established that the limiting distribution of the squared singular values follows a distribution that now bears their names. The Bai–Yin law [BY93] gives an almost-sure limit for the largest
singular value of a rectangular Gaussian matrix. The expectation bound (4.2.5) appears in a survey article [DS02] by Davidson & Szarek. The latter result is ultimately derived from a comparison
theorem for Gaussian processes due to Férnique [Fer75] and ampliﬁed by Gordon [Gor85]. Our
approach, using Theorem 4.1.1, is based on [Tro11c, §4].
Randomly Signed Matrices
Matrices with randomly signed entries have not received much attention in the literature. The
result (4.3.1) is due to Yoav Seginer [Seg00]. There is also a well-known paper [Lat05] by Rafał
Latała that provides a bound for the expected norm of a Gaussian matrix whose entries have
nonuniform variance. Riemer & Schütt [RS13] have extended the earlier results. The very recent paper [BV14] of Afonso Bandeira and Ramon Van Handel contains an elegant new proof of
Seginer’s result based on a general theorem for random matrices with independent entries. The
analysis here, using Theorem 4.1.1, is drawn from [Tro11c, §4].
Gaussian Toeplitz Matrices
Research on random Toeplitz matrices is surprisingly recent, but there are now a number of papers available. Bryc, Dembo, & Jiang obtained the limiting spectral distribution of a symmetric
Toeplitz matrix based on independent and identically distributed (iid) random variables [BDJ06].
Later, Mark Meckes established the ﬁrst bound for the expected norm of a random Toeplitz matrix based on iid random variables [Mec07]. More recently, Sen & Virág computed the limiting
value of the expected norm of a random, symmetric Toeplitz matrix whose entries have identical
second-order statistics [SV13]. See the latter paper for additional references. The analysis here,
based on Theorem 4.1.1, is new. Our lower bound for the value of E∥Γd∥follows from the results of Sen & Virág. We are not aware of any analysis for a random Toeplitz matrix whose entries
have different variances, but this type of result would follow from a simple modiﬁcation of the
argument in §4.4.
Relaxation and Rounding of MAXQP
The idea of using semideﬁnite relaxation and rounding to solve the MAXQP problem is due to
Arkadi Nemirovski [Nem07]. He obtained nontrivial results on the performance of his method
using some matrix moment calculations, but he was unable to reach the sharpest possible bound.
CHAPTER 4. MATRIX GAUSSIAN & RADEMACHER SERIES
Anthony So [So09] pointed out that matrix moment inequalities imply an optimal result; he also
showed that matrix concentration inequalities have applications to robust optimization. The
presentation here, using Theorem 4.1.1, is essentially equivalent with the approach in [So09],
but we have achieved slightly better bounds for the constants.
A Sum of Random
Positive-Semideﬁnite Matrices
This chapter presents matrix concentration inequalities that are analogous with the classical
Chernoff bounds. In the matrix setting, Chernoff-type inequalities allow us to control the extreme eigenvalues of a sum of independent, random, positive-semideﬁnite matrices.
More formally, we consider a ﬁnite sequence {Xk} of independent, random Hermitian matrices that satisfy
0 ≤λmin(Xk)
λmax(Xk) ≤L
for each index k.
Introduce the sum Y = P
k Xk. Our goal is to study the expectation and tail behavior of λmax(Y )
and λmin(Y ). Bounds on the maximum eigenvalue λmax(Y ) give us information about the norm
of the matrix Y , a measure of how much the action of the matrix can dilate a vector. Bounds
for the minimum eigenvalue λmin(Y ) tell us when the matrix Y is nonsingular; they also provide
evidence about the norm of the inverse Y −1, when it exists.
The matrix Chernoff inequalities are quite powerful, and they have numerous applications.
We demonstrate the relevance of this theory by considering two examples. First, we show how
to study the norm of a random submatrix drawn from a ﬁxed matrix, and we explain how to
check when the random submatrix has full rank. Second, we develop an analysis to determine
when a random graph is likely to be connected. These two problems are closely related to basic
questions in statistics and in combinatorics.
In contrast, the matrix Bernstein inequalities, appearing in Chapter 6, describe how much
a random matrix deviates from its mean value. As such, the matrix Bernstein bounds are more
suitable than the matrix Chernoff bounds for problems that concern matrix approximations.
Matrix Bernstein inequalities are also more appropriate when the variance v(Y ) is small in comparison with the upper bound L on the summands.
Section 5.1 presents the main results on the expectations and the tails of the extreme eigenvalues
of a sum of independent, random, positive-semideﬁnite matrices. Section 5.2 explains how the
CHAPTER 5. A SUM OF RANDOM POSITIVE-SEMIDEFINITE MATRICES
matrix Chernoff bounds provide spectral information about a random submatrix drawn from
a ﬁxed matrix. In §5.3, we use the matrix Chernoff bounds to study when a random graph is
connected. Afterward, in §5.4 we explain how to prove the main results.
The Matrix Chernoff Inequalities
In the scalar setting, the Chernoff inequalities describe the behavior of a sum of independent,
nonnegative random variables that are subject to a uniform upper bound. These results are often
applied to study the number Y of successes in a sequence of independent—but not necessarily
identical—Bernoulli trials with small probabilities of success. In this case, the Chernoff bounds
show that Y behaves like a Poisson random variable. The random variable Y concentrates near
the expected number of successes. Its lower tail has Gaussian decay, while its upper tail drops
off faster than that of an exponential random variable. See [BLM13, §2.2] for more background.
In the matrix setting, we encounter similar phenomena when we consider a sum of independent, random, positive-semideﬁnite matrices whose eigenvalues meet a uniform upper bound.
This behavior emerges from the next theorem, which closely parallels the scalar Chernoff theorem.
Theorem 5.1.1 (Matrix Chernoff). Consider a ﬁnite sequence {Xk} of independent, random, Hermitian matrices with common dimension d. Assume that
0 ≤λmin(Xk)
λmax(Xk) ≤L
for each index k.
Introduce the random matrix
Deﬁne the minimum eigenvalue µmin and maximum eigenvalue µmax of the expectation EY :
µmin = λmin(EY ) = λmin
µmax = λmax(EY ) = λmax
Then, for θ > 0,
Eλmin(Y ) ≥1−e−θ
Eλmax(Y ) ≤eθ −1
Furthermore,
λmin (Y ) ≤(1−ε)µmin
for ε ∈[0,1),
λmax (Y ) ≥(1+ε)µmax
The proof of Theorem 5.1.1 appears below in §5.4.
Discussion
Let us consider some facets of Theorem 5.1.1.
5.1. THE MATRIX CHERNOFF INEQUALITIES
Aspects of the Matrix Chernoff Inequality
In many situations, it is easier to work with streamlined versions of the expectation bounds:
Eλmin(Y ) ≥0.63µmin −L logd,
Eλmax(Y ) ≤1.72µmax +L logd.
We obtain these results by selecting θ = 1 in both (5.1.3) and (5.1.4) and evaluating the numerical
constants.
These simpliﬁcations also help to clarify the meaning of Theorem 5.1.1. On average, λmin(Y )
is not much smaller than λmin(EY ), minus a ﬂuctuation term that reﬂects the maximum size L
of a summand and the ambient dimension d. Similarly, the average value of λmax(Y ) is close to
λmax(EY ), plus the same ﬂuctuation term.
We can also weaken the tail bounds (5.1.5) and (5.1.6) to reach
λmin (Y ) ≤tµmin
≤d e−(1−t)2µmin/2L
for t ∈[0,1), and
λmax (Y ) ≥tµmax
The ﬁrst bound shows that the lower tail of λmin(Y ) decays at a subgaussian rate with variance
L/µmin. The second bound manifests that the upper tail of λmax(Y ) decays faster than that of an
exponential random variable with mean L/µmax. This is the same type of prediction we receive
from the scalar Chernoff inequalities.
As with other matrix concentration results, the tail bounds (5.1.5) and (5.1.6) can overestimate the actual tail probabilities for the extreme eigenvalues of Y , especially at large deviations
from the mean. The value of the matrix Chernoff theorem derives from the estimates (5.1.3)
and (5.1.4) for the expectation of the minimum and maximum eigenvalue of Y . Scalar concentration inequalities may provide better estimates for tail probabilities.
Related Results
We can moderate the dimensional factor d in the bounds for λmax(Y ) from Theorem 5.1.1 when
the random matrix Y has limited spectral content in most directions. We take up this analysis in
Chapter 7.
Next, let us present an important reﬁnement [CGT12a, Thm. A.1] of the bound (5.1.8) that
can be very useful in practice:
Eλmax(Y ) ≤2µmax +8e
E maxk λmax(Xk)
This estimate may be regarded as a matrix version of Rosenthal’s inequality [Ros70]. Observe that
the uniform bound L appearing in (5.1.8) always exceeds the large parenthesis on the right-hand
side of (5.1.9). Therefore, the estimate (5.1.9) is valuable when the summands are unbounded
and, especially, when they have heavy tails. See the notes at the end of the chapter for more
information.
Optimality of the Matrix Chernoff Bounds
In this section, we explore how well bounds such as Theorem 5.1.1 and inequality (5.1.9) describe the behavior of a random matrix Y formed as a sum of independent, random positivesemideﬁnite matrices.
CHAPTER 5. A SUM OF RANDOM POSITIVE-SEMIDEFINITE MATRICES
The Upper Chernoff Bounds
We will demonstrate that both terms in the matrix Rosenthal inequality (5.1.9) are necessary.
More precisely,
µmax + E maxk λmax(Xk)
E maxk λmax(Xk)
Therefore, we have identiﬁed appropriate parameters for bounding Eλmax(Y ), although the constants and the logarithm may not be sharp in every case.
The appearance of µmax on the left-hand side of (5.1.10) is a consequence of Jensen’s inequality. Indeed, the maximum eigenvalue is convex, so
Eλmax(Y ) ≥λmax(EY ) = µmax.
To justify the other term, apply the fact that the summands Xk are positive semideﬁnite to conclude that
Eλmax(Y ) = Eλmax
≥E maxk λmax(Xk).
We have used the fact that λmax(A + H) ≥λmax(A) whenever H is positive semideﬁnite. Average
the last two displays to develop the left-hand side of (5.1.10). The right-hand side of (5.1.10) is
obviously just (5.1.9).
A simple example sufﬁces to show that the logarithm cannot always be removed from the
second term in (5.1.8) or from (5.1.9). For each natural number n, consider the d × d random
is an independent family of BERNOULLI(n−1) random variables and Ekk is the d ×d
matrix with a one in the (k,k) entry an zeros elsewhere. An easy application of (5.1.8) delivers
λmax(Yn) ≤1.72+logd.
Using the Poisson limit of a binomial random variable and the Skorokhod representation, we can
construct an independent family {Qk} of POISSON(1) random variables for which
almost surely as n →∞.
It follows that
Eλmax(Yn) →E maxk Qk ≈const·
Therefore, the logarithm on the second term in (5.1.8) cannot be reduced by a factor larger than
the iterated logarithm loglogd. This modest loss comes from approximations we make when
developing the estimate for the mean. The tail bound (5.1.6) accurately predicts the order of
λmax(Yn) in this example.
The latter example depends on the commutativity of the summands and the inﬁnite divisibility of the Poisson distribution, so it may seem rather special. Nevertheless, the logarithm really
does belong in many (but not all!) examples that arise in practice. In particular, it is necessary in
the application to random submatrices in §5.2.
5.2. EXAMPLE: A RANDOM SUBMATRIX OF A FIXED MATRIX
The Lower Chernoff Bounds
The upper expectation bound (5.1.4) is quite satisfactory, but the situation is murkier for the
lower expectation bound (5.1.3). The mean term appears naturally in the lower bound:
Eλmin(Y ) ≤λmin(EY ) = µmin.
This estimate is a consequence of Jensen’s inequality and the concavity of the minimum eigenvalue. On the other hand, it is not clear what the correct form of the second term in (5.1.3) should
be for a general sum of random positive-semideﬁnite matrices.
Nevertheless, a simple example demonstrates that the lower Chernoff bound (5.1.3) is numerically sharp in some situations. Let X be a d × d random positive-semideﬁnite matrix that
with probability d−1 for each index i = 1,...,d.
It is clear that EX = Id. Form the random matrix
where each Xk is an independent copy of X .
The lower Chernoff bound (5.1.3) implies that
Eλmin(Yn) ≥1−e−θ
The parameter θ > 0 is at our disposal. This analysis predicts that Eλmin(Yn) > 0 precisely when
n > d logd.
On the other hand, λmax(Yn) > 0 if and only if each diagonal matrix d Eii appears at least once
among the summands X1,...,Xn. To determine the probability that this event occurs, notice that
this question is an instance of the coupon collector problem [MR95, §3.6]. The probability of
collecting all d coupons within n draws undergoes a phase transition from about zero to about
one at n = d logd. By reﬁning this argument [Tro11d], we can verify that both lower Chernoff
bounds (5.1.3) and (5.1.5) provide a numerically sharp lower bound for the value of n where the
phase transition occurs. In other words, the lower matrix Chernoff bounds are themselves sharp.
Example: A Random Submatrix of a Fixed Matrix
The matrix Chernoff inequality can be used to bound the extreme singular values of a random
submatrix drawn from a ﬁxed matrix. Theorem 5.1.1 might not seem suitable for this purpose
because it deals with eigenvalues, but we can connect the method with the problem via a simple
transformation. The results in this section have found applications in randomized linear algebra,
sparse approximation, machine learning, and other ﬁelds. See the notes at the end of the chapter
for some additional discussion and references.
A Random Column Submatrix
Let B be a ﬁxed d ×n matrix, and let b:k denote the kth column of this matrix. The matrix can be
expressed as the sum of its columns:
CHAPTER 5. A SUM OF RANDOM POSITIVE-SEMIDEFINITE MATRICES
The symbol ek refers to the standard basis (column) vector with a one in the kth component and
zeros elsewhere; the length of the vector ek is determined by context.
We consider a simple model for a random column submatrix. Let {δk} be an independent
sequence of BERNOULLI(p/n) random variables. Deﬁne the random matrix
That is, we include each column independently with probability p/n, which means that there
are typically about p nonzero columns in the matrix. We do not remove the other columns; we
just zero them out.
In this section, we will obtain bounds on the expectation of the extreme singular values σ1(Z )
and σd(Z ) of the d ×n random matrix Z . More precisely,
Eσ1(Z )2 ≤1.72· p
n ·σ1(B)2 +(logd)·maxk ∥b:k∥2 ,
Eσd(Z )2 ≥0.63· p
n ·σd(B)2 −(logd)·maxk ∥b:k∥2 .
That is, the random submatrix Z gets its “fair share” of the squared singular values of the original
matrix B. There is a ﬂuctuation term that depends on largest norm of a column of B and the
logarithm of the number d of rows in B. This result is very useful because a positive lower bound
on σd(Z ) ensures that the rows of the random submatrix Z are linearly independent.
The Analysis
To study the singular values of Z , it is convenient to deﬁne a d ×d random, positive-semideﬁnite
Y = Z Z ∗=
δj δk (b:j e∗
Note that δ2
k = δk because δk only takes the values zero and one. The eigenvalues of Y determine
the singular values of Z , and vice versa. In particular,
λmax(Y ) = λmax(Z Z ∗) = σ1(Z )2
λmin(Y ) = λmin(Z Z ∗) = σd(Z )2,
where we arrange the singular values of Z in weakly decreasing order σ1(Z ) ≥··· ≥σd(Z ).
The matrix Chernoff inequality provides bounds for the expectations of the eigenvalues of Y .
To apply the result, ﬁrst calculate
(Eδk)b:kb∗
µmax = λmax(EY ) = p
µmin = λmin(EY ) = p
Deﬁne L = maxk ∥b:k∥2, and observe that ∥δkb:kb∗
:k∥≤L for each index k. The simpliﬁed matrix
Chernoff bounds (5.1.7) and (5.1.8) now deliver the result (5.2.1).
5.2. EXAMPLE: A RANDOM SUBMATRIX OF A FIXED MATRIX
A Random Row and Column Submatrix
Next, we consider a model for a random set of rows and columns drawn from a ﬁxed d ×n matrix
B. In this case, it is helpful to use matrix notation to represent the extraction of a submatrix.
Deﬁne independent random projectors
P = diag(δ1,...,δd)
R = diag(ξ1,...,ξn)
where {δk} is an independent family of BERNOULLI(p/d) random variables and {ξk} is an independent family of BERNOULLI(r/n) random variables. Then
is a random submatrix of B with about p nonzero rows and r nonzero columns.
In this section, we will show that
E ∥Z ∥2 ≤3· p
n ·∥B∥2 +2· p logd
·maxk ∥b:k∥2
+2· r logn
·maxj ∥b j:∥2 +(logd)(logn)·maxj,k |b jk|2 .
The notations b j: and b:k refer to the jth row and kth column of the matrix B, while b jk is the
(j,k) entry of the matrix. In other words, the random submatrix Z gets its share of the total
squared norm of the matrix B. The ﬂuctuation terms reﬂect the maximum row norm and the
maximum column norm of B, as well as the size of the largest entry. There is also a weak dependence on the ambient dimensions d and n.
The Analysis
The argument has much in common with the calculations for a random column submatrix, but
we need to do some extra work to handle the interaction between the random row sampling and
the random column sampling.
To begin, we express the squared norm ∥Z ∥2 in terms of the maximum eigenvalue of a random positive-semideﬁnite matrix:
E ∥Z ∥2 = Eλmax((PBR)(PBR)∗)
= Eλmax((PB)R(PB)∗) = E
ξk (PB):k(PB)∗
We have used the fact that RR∗= R, and the notation (PB):k refers to the kth column of the
matrix PB. Observe that the random positive-semideﬁnite matrix on the right-hand side has
dimension d. Invoking the matrix Chernoff inequality (5.1.8), conditional on the choice of P, we
E ∥Z ∥2 ≤1.72· r
n ·Eλmax((PB)(PB)∗)+(logd)·Emaxk ∥(PB):k∥2 .
The required calculation is analogous with the one in the Section 5.2.1, so we omit the details. To
reach a deterministic bound, we still have two more expectations to control.
Next, we examine the term in (5.2.3) that involves the maximum eigenvalue:
Eλmax((PB)(PB)∗) = Eλmax(B∗PB) = Eλmax
CHAPTER 5. A SUM OF RANDOM POSITIVE-SEMIDEFINITE MATRICES
The ﬁrst identity holds because λmax(CC ∗) = λmax(C ∗C) for any matrix C, and PP∗= P. Observe
that the random positive-semideﬁnite matrix on the right-hand side has dimension n, and apply
the matrix Chernoff inequality (5.1.8) again to reach
Eλmax((PB)(PB)∗) ≤1.72· p
d ·λmax(B∗B)+(logn)·maxj ∥b j:∥2 .
Recall that λmax(B∗B) = ∥B∥2 to simplify this expression slightly.
Last, we develop a bound on the maximum column norm in (5.2.3). This result also follows
from the matrix Chernoff inequality, but we need to do a little work to see why. There are more
direct proofs, but this approach is closer in spirit to the rest of our proof.
We are going to treat the maximum column norm as the maximum eigenvalue of a sum of
independent, random diagonal matrices. Observe that
∥(PB):k∥2 =
δj |b jk|2
for each k = 1,...,n.
Using this representation, we see that
maxk ∥(PB):k∥2 = λmax
j=1 δj |b j1|2
j=1 δj |b jn|2
|b j1|2 ,...,|b jn|2¢
To activate the matrix Chernoff bound, we need to compute the two parameters that appear
in (5.1.8). First, the uniform upper bound L satisﬁes
L = maxj λmax
|b j1|2 ,...,|b jn|2¢¢
= maxj maxk |b jk|2 .
Second, to compute µmax, note that
|b j1|2 ,...,|b jn|2¢
|b j1|2 ,...,
∥b:1∥2 ,...,∥b:n∥2¢
Take the maximum eigenvalue of this expression to reach
d ·maxk ∥b:k∥2 .
Therefore, the matrix Chernoff inequality implies
Emaxk ∥(PB):k∥2 ≤1.72· p
d ·maxk ∥b:k∥2 +(logn)·maxj,k |b jk|2 .
On average, the maximum squared column norm of a random submatrix PB with approximately
p nonzero rows gets its share p/d of the maximum squared column norm of B, plus a ﬂuctuation
term that depends on the magnitude of the largest entry of B and the logarithm of the number n
of columns.
Combine the three bounds (5.2.3), (5.2.4), and (5.2.5) to reach the result (5.2.2). We have
simpliﬁed numerical constants to make the expression more compact.
5.3. APPLICATION: WHEN IS AN ERD ˝OS–RÉNYI GRAPH CONNECTED?
Application: When is an Erd˝os–Rényi Graph Connected?
Random graph theory concerns probabilistic models for the interactions between pairs of objects. One basic question about a random graph is to ask whether there is a path connecting
every pair of vertices or whether there are vertices segregated in different parts of the graph. It
is possible to address this problem by studying the eigenvalues of random matrices, a challenge
that we take up in this section.
Background on Graph Theory
Recall that an undirected graph is a pair G = (V,E). The elements of the set V are called vertices.
The set E is a collection of unordered pairs {u,v} of distinct vertices, called edges. We say that the
graph has an edge between vertices u and v in V if the pair {u,v} appears in E. For simplicity, we
assume that the vertex set V = {1,...,n}. The degree deg(k) of the vertex k is the number of edges
in E that include the vertex k.
There are several natural matrices associated with an undirected graph. The adjacency matrix of the graphG is an n×n symmetric matrix A whose entries indicate which edges are present:
We have assumed that edges connect distinct vertices, so the diagonal entries of the matrix A
equal zero. Next, deﬁne a diagonal matrix D = diag(deg(1),...,deg(n)) whose entries list the degrees of the vertices. The Laplacian ∆and normalized Laplacian H of the graph are the matrices
H = D−1/2∆D−1/2.
We place the convention that D−1/2(k,k) = 0 when deg(k) = 0. The Laplacian matrix ∆is always
positive semideﬁnite. The vector e ∈Rn of ones is always an eigenvector of ∆with eigenvalue
These matrices and their spectral properties play a dominant role in modern graph theory.
For example, the graph G is connected if and only if the second-smallest eigenvalue of ∆is
strictly positive. The second smallest eigenvalue of H controls the rate at which a random walk
on the graph G converges to the stationary distribution (under appropriate assumptions). See
the book [GR01] for more information about these connections.
The Model of Erd˝os & Rényi
The simplest possible example of a random graph is the independent model G(n,p) of Erd˝os and
Rényi [ER60]. The number n is the number of vertices in the graph, and p ∈(0,1) is the probability that two vertices are connected. More precisely, here is how to construct a random graph in
G(n,p). Between each pair of distinct vertices, we place an edge independently at random with
probability p. In other words, the adjacency matrix takes the form
1 ≤j < k ≤n
1 ≤k < j ≤n
CHAPTER 5. A SUM OF RANDOM POSITIVE-SEMIDEFINITE MATRICES
An Erdös−Rényi graph in G(100, 0.1)
Figure 5.1: The adjacency matrix of an Erd˝os–Rényi graph. This ﬁgure shows the pattern of
nonzero entries in the adjacency matrix A of a random graph drawn from G(100,0.1). Out of
a possible 4,950 edges, there are 486 edges present. A basic question is whether the graph is
connected. The graph is disconnected if and only if there is a permutation of the vertices so
that the adjacency matrix is block diagonal. This property is reﬂected in the second-smallest
eigenvalue of the Laplacian matrix ∆.
The family {ξjk : 1 ≤j < k ≤n} consists of mutually independent BERNOULLI(p) random variables. Figure 5.3.2 shows one realization of the adjacency matrix of an Erd˝os–Rényi graph.
Let us explain how to represent the adjacency matrix and Laplacian matrix of an Erd˝os–Rényi
graph as a sum of independent random matrices. The adjacency matrix A of a random graph in
G(n,p) can be written as
ξjk (Ejk +Ek j ).
This expression is a straightforward translation of the deﬁnition (5.3.1) into matrix form. Similarly, the Laplacian matrix ∆of the random graph can be expressed as
ξjk (Ej j +Ekk −Ejk −Ek j ).
To verify the formula (5.3.3), observe that the presence of an edge between the vertices j and k
increases the degree of j and k by one. Therefore, when ξjk = 1, we augment the (j, j) and (k,k)
entries of ∆to reﬂect the change in degree, and we mark the (j,k) and (k, j) entries with −1 to
reﬂect the presence of the edge between j and k.
5.3. APPLICATION: WHEN IS AN ERD ˝OS–RÉNYI GRAPH CONNECTED?
Connectivity of an Erd˝os–Rényi Graph
We will obtain a near-optimal bound for the range of parameters where an Erd˝os–Rényi graph
G(n,p) is likely to be connected. We can accomplish this goal by showing that the second smallest eigenvalue of the n ×n random Laplacian matrix ∆= D −A is strictly positive. We will solve
the problem by using the matrix Chernoff inequality to study the second-smallest eigenvalue of
the random Laplacian ∆.
We need to form a random matrix Y that consists of independent positive-semideﬁnite terms
and whose minimum eigenvalue coincides with the second-smallest eigenvalue of ∆. Our approach is to compress the matrix Y to the orthogonal complement of the vector e of ones. To
that end, we introduce an (n −1)×n partial isometry R that satisﬁes
Now, consider the (n −1)×(n −1) random matrix
ξjk ·R (Ej j +Ekk −Ejk −Ek j )R∗.
Recall that {ξjk} is an independent family of BERNOULLI(p) random variables, so the summands
are mutually independent. The Conjugation Rule (2.1.12) ensures that each summand remains
positive semideﬁnite. Furthermore, the Courant–Fischer theorem implies that the minimum
eigenvalue of Y coincides with the second-smallest eigenvalue of ∆because the smallest eigenvalue of ∆has eigenvector e.
To apply the matrix Chernoff inequality, we show that L = 2 is an upper bound for the eigenvalues of each summand in (5.3.4). We have
∥ξjk ·R (Ej j +Ekk −Ejk −Ek j )R∗∥≤|ξjk|·∥R∥·∥Ej j +Ekk −Ejk −Ek j ∥·∥R∗∥≤2.
The ﬁrst bound follows from the submultiplicativity of the spectral norm. To obtain the second
bound, note that ξjk takes 0–1 values. The matrix R is a partial isometry so its norm equals one.
Finally, a direct calculation shows that T = Ej j +Ekk −Ejk −Ek j satisﬁes the polynomial T 2 = 2T ,
so each eigenvalue of T must equal zero or two.
Next, we compute the expectation of the matrix Y .
(Ej j +Ekk −Ejk −Ek j )
(n −1)In −(ee∗−In)
R∗= pn ·In−1.
The ﬁrst identity follows when we apply linearity of expectation to (5.3.5) and then use linearity
of matrix multiplication to draw the sum inside the conjugation by R. The term (n−1)In emerges
when we sum the diagonal matrices. The term ee∗−In comes from the off-diagonal matrix units,
once we note that the matrix ee∗has one in each component. The last identity holds because of
the properties of R displayed in (5.3.4). We conclude that
λmin(EY ) = pn.
This is all the information we need.
To arrive at a probability inequality for the second-smallest eigenvalue λ↑
2(∆) of the matrix ∆,
we apply the tail bound (5.1.5) to the matrix Y . We obtain, for t ∈(0,1),
2(∆) ≤t · pn
λmin(Y ) ≤t · pn
CHAPTER 5. A SUM OF RANDOM POSITIVE-SEMIDEFINITE MATRICES
To appreciate what this means, we may think about the situation where t →0. Then the bracket
tends to e−1, and we see that the second-smallest eigenvalue of ∆is unlikely to be zero when
log(n −1)−pn/2 < 0. Rearranging this expression, we obtain a sufﬁcient condition
p > 2log(n −1)
for an Erd˝os–Rényi graph G(n,p) to be connected with high probability as n →∞. This bound is
quite close to the optimal result, which lacks the factor two on the right-hand side. It is possible
to make this reasoning more precise, but it does not seem worth the fuss.
Proof of the Matrix Chernoff Inequalities
The ﬁrst step toward the matrix Chernoff inequalities is to develop an appropriate semideﬁnite
bound for the mgf and cgf of a random positive-semideﬁnite matrix. The method for establishing
this result mimics the proof in the scalar case: we simply bound the exponential with a linear
Lemma 5.4.1 (Matrix Chernoff: Mgf and Cgf Bound). Suppose that X is a random matrix that
satisﬁes 0 ≤λmin(X ) and λmax(X ) ≤L. Then
log EeθX ≼eθL −1
Proof. Consider the function f (x) = eθx. Since f is convex, its graph lies below the chord connecting any two points on the graph. In particular,
f (x) ≤f (0)+ f (L)−f (0)
for x ∈[0,L].
In detail,
eθx ≤1+ eθL −1
for x ∈[0,L].
By assumption, each eigenvalue of X lies in the interval [0,L]. Thus, the Transfer Rule (2.1.14)
implies that
eθX ≼I+ eθL −1
Expectation respects the semideﬁnite order, so
EeθX ≼I+ eθL −1
The second relation is a consequence of the fact that I + A ≼eA for every matrix A, which we
obtain by applying the Transfer Rule (2.1.14) to the inequality 1+ a ≤ea, valid for all a ∈R.
To obtain the semideﬁnite bound for the cgf, we simply take the logarithm of the semidefinite bound for the mgf. This operation preserves the semideﬁnite order because of the property (2.1.18) that the logarithm is operator monotone.
5.4. PROOF OF THE MATRIX CHERNOFF INEQUALITIES
We break the proof of the matrix inequality into two pieces. First, we establish the bounds
on the maximum eigenvalue, which are slightly easier. Afterward, we develop the bounds on the
minimum eigenvalue.
Proof of Theorem 5.1.1, Maximum Eigenvalue Bounds. Consider a ﬁnite sequence {Xk} of independent, random Hermitian matrices with common dimension d. Assume that
0 ≤λmin(Xk)
λmax(Xk) ≤L
for each index k.
The cgf bound, Lemma 5.4.1, states that
log EeθXk ≼g(θ)·EXk
g(θ) = eθL −1
for θ > 0.
We begin with the upper bound (5.1.4) for Eλmax(Y ). Using the fact (2.1.16) that the trace of
the exponential function is monotone with respect to the semideﬁnite order, we substitute these
cgf bounds into the master inequality (3.6.1) for the expectation of the maximum eigenvalue to
Eλmax(Y ) ≤inf
θ log trexp
g(θ)·λmax(EY )
logd + g(θ)·µmax
In the second line, we use the fact that the matrix exponential is positive deﬁnite to bound the
trace by d times the maximum eigenvalue; we have also identiﬁed the sum as EY . The third line
follows from the Spectral Mapping Theorem, Proposition 2.1.3. Next, we use the fact (2.1.4) that
the maximum eigenvalue is a positive-homogeneous map, which depends on the observation
that g(θ) > 0 for θ > 0. Finally, we identify the statistic µmax deﬁned in (5.1.2). The inﬁmum
does not admit a closed form, but we can obtain the expression (5.1.4) by making the change of
variables θ 7→θ/L.
Next, we turn to the upper bound (5.1.6) for the upper tail of the maximum eigenvalue. Substitute the cgf bounds (5.4.1) into the master inequality (3.6.3) to reach
P{λmax(Y ) ≥t} ≤inf
θ>0 e−θt trexp
θ>0 e−θt ·d exp
The steps here are identical with the previous argument. To complete the proof, make the change
of variables t 7→(1 + ε)µmax. Then the inﬁmum is achieved at θ = L−1 log(1 + ε), which leads to
the tail bound (5.1.6).
The lower bounds follow from a related argument that is slightly more delicate.
CHAPTER 5. A SUM OF RANDOM POSITIVE-SEMIDEFINITE MATRICES
Proof of Theorem 5.1.1, Minimum Eigenvalue Bounds. Once again, consider a ﬁnite sequence {Xk}
of independent, random Hermitian matrices with dimension d. Assume that
0 ≤λmin(Xk)
λmax(Xk) ≤L
for each index k.
The cgf bound, Lemma 5.4.1, states that
log EeθXk ≼g(θ)·EXk
g(θ) = eθL −1
for θ < 0.
Note that g(θ) < 0 for θ < 0, which alters a number of the steps in the argument.
We commence with the lower bound (5.1.3) for Eλmin(Y ). As stated in (2.1.16), the trace
exponential function is monotone with respect to the semideﬁnite order, so the master inequality (3.6.2) for the minimum eigenvalue delivers
Eλmin(Y ) ≥sup
θ log trexp
g(θ)·λmin(EY )
logd + g(θ)·µmin
Most of the steps are the same as in the proof of the upper bound (5.1.4), so we focus on the
differences. Since the factor θ−1 in the ﬁrst and second lines is negative, upper bounds on the
trace reduce the value of the expression. We move to the fourth line by invoking the property
λmax(αA) = αλmin(A) for α < 0, which follows from (2.1.4) and (2.1.5). This piece of algebra
depends on the fact that g(θ) < 0 when θ < 0. To obtain the result (5.1.3), we change variables:
Finally, we establish the bound (5.1.5) for the lower tail of the minimum eigenvalue. Introduce the cgf bounds (5.4.2) into the master inequality (3.6.4) to reach
P{λmin(Y ) ≤t} ≤inf
θ<0 e−θt trexp
θ<0 e−θt ·d exp
The justiﬁcations here match those in with the previous argument. Finally, we make the change
of variables t 7→(1 −ε)µmin. The inﬁmum is attained at θ = L−1 log(1 −ε), which yields the tail
bound (5.1.5).
As usual, we continue with an overview of background references and related work.
5.5. NOTES
Matrix Chernoff Inequalities
Scalar Chernoff inequalities date to the paper [Che52, Thm. 1] by Herman Chernoff. The original
result provides probability bounds for the number of successes in a sequence of independent but
non-identical Bernoulli trials. Chernoff’s proof combines the scalar Laplace transform method
with reﬁned bounds on the mgf of a Bernoulli random variable. It is very common to encounter
simpliﬁed versions of Chernoff’s result, such as [Lug09, Exer. 8] or [MR95, §4.1].
In their paper [AW02], Ahlswede & Winter developed a matrix version of the Chernoff inequality. The matrix mgf bound, Lemma 5.4.1, essentially appears in their work. Ahlswede &
Winter focus on the case of independent and identically distributed random matrices, in which
case their results are roughly equivalent with Theorem 5.1.1. For the general case, their approach
leads to matrix expectation statistics of the form
k λmin(EXk)
k λmax(EXk).
It is clear that their µAW
min may be substantially smaller than the quantity µmin we deﬁned in Theorem 5.1.1. Similarly, their µAW
max may be substantially larger than the quantity µmax that drives
the upper Chernoff bounds.
The tail bounds from Theorem 5.1.1 are drawn from [Tro11c, §5], but the expectation bounds
we present are new. The technical report [GT14] extends the matrix Chernoff inequality to provide upper and lower tail bounds for all eigenvalues of a sum of random, positive-semideﬁnite
matrices. Chapter 7 contains a slight improvement of the bounds for the maximum eigenvalue
in Theorem 5.1.1.
Let us mention a few other results that are related to the matrix Chernoff inequality. First,
Theorem 5.1.1 has a lovely information-theoretic formulation where the tail bounds are stated
in terms of an information divergence. To establish this result, we must restructure the proof and
eliminate some of the approximations. See [AW02, Thm. 19] or [Tro11c, Thm. 5.1].
Second, the problem of bounding the minimum eigenvalue of a sum of random, positivesemideﬁnite matrices has a special character. The reason, roughly, is that a sum of independent,
nonnegative random variables cannot easily take the value zero. A closely related phenomenon
holds in the matrix setting, and it is possible to develop estimates that exploit this observation.
See [Oli13, Thm. 3.1] and [KM13, Thm. 1.3] for two wildly different approaches.
The Matrix Rosenthal Inequality
The matrix Rosenthal inequality (5.1.9) is one of the earliest matrix concentration bounds. In
his paper [Rud99], Rudelson used the noncommutative Khintchine inequality (4.7.1) to establish a specialization of (5.1.9) to rank-one summands. A reﬁnement appears in [RV07], and explicit constants were ﬁrst derived in [Tro08c]. We believe that the paper [CGT12a] contains the
ﬁrst complete statement of the moment bound (5.1.9) for general positive-semideﬁnite summands; see also the work [MZ11]. The constants in [CGT12a, Thm. A.1], and hence in (5.1.9), can
be improved slightly by using the sharp version of the noncommutative Khintchine inequality
from [Buc01, Buc05]. Let us stress that all of these results follow from easy variations of Rudelson’s argument.
The work [MJC+14, Cor. 7.4] provides a self-contained and completely elementary proof of
a matrix Rosenthal inequality that is closely related to (5.1.9). This result depends on different
principles from the works mentioned in the last paragraph.
CHAPTER 5. A SUM OF RANDOM POSITIVE-SEMIDEFINITE MATRICES
Random Submatrices
The problem of studying a random submatrix drawn from a ﬁxed matrix has a long history.
An early example is the paving problem from operator theory, which asks for a maximal wellconditioned set of columns (or a well-conditioned submatrix) inside a ﬁxed matrix. Random selection provides a natural way to approach this question. The papers of Bourgain & Tzafriri [BT87,
BT91] and Kashin & Tzafriri [KT94] study random paving using sophisticated tools from functional analysis. See the paper [NT14] for a summary of research on randomized methods for
constructing pavings. Very recently, Adam Marcus, Dan Spielman, & Nikhil Srivastava [MSS14]
have solved the paving problem completely.
Later, Rudelson and Vershynin [RV07] showed that the noncommutative Khintchine inequality provides a clean way to bound the norm of a random column submatrix (or a random row
and column submatrix) drawn from a ﬁxed matrix. Their ideas have found many applications
in the mathematical signal processing literature. For example, the paper [Tro08a] uses similar
techniques to analyze the perfomance of ℓ1 minimization for recovering a random sparse signal.
The same methods support the paper [Tro08c], which contains a modern proof of the random
paving result [BT91, Thm. 2.1] of Bourgain & Tzafriri.
The article [Tro11d] contains the observation that the matrix Chernoff inequality is an ideal
tool for studying random submatrices. It applies this technique to study a random matrix that
arises in numerical linear algebra [HMT11], and it achieves an optimal estimate for the minimum
singular value of the random matrix that arises in this setting. Our analysis of a random column
submatrix is based on this work. The analysis of a random row and column submatrix is new.
The paper [CD12], by Chrétien and Darses, uses matrix Chernoff bounds in a more sophisticated
way to develop tail bounds for the norm of a random row and column submatrix.
Random Graphs
The analysis of random graphs and random hypergraphs appeared as one of the earliest applications of matrix concentration inequalities [AW02]. Christoﬁdes and Markström developed a matrix Hoeffding inequality to aid in this purpose [CM08]. Later, Oliveira wrote two papers [Oli10a,
Oli11] on random graph theory based on matrix concentration. We recommend these works for
further information.
To analyze the random graph Laplacian, we compressed the Laplacian to a subspace so that
the minimum eigenvalue of the compression coincides with the second-smallest eigenvalue of
the original Laplacian. This device can be extended to obtain tail bounds for all the eigenvalues
of a sum of independent random matrices. See the technical report [GT14] for a development of
this idea.
A Sum of Bounded
Random Matrices
In this chapter, we describe matrix concentration inequalities that generalize the classical Bernstein bound. The matrix Bernstein inequalities concern a random matrix formed as a sum of
independent, random matrices that are bounded in spectral norm. The results allow us to study
how much this type of random matrix deviates from its mean value in the spectral norm.
Formally, we consider an ﬁnite sequence {Sk} of random matrices of the same dimension.
Assume that the matrices satisfy the conditions
for each index k.
Form the sum Z = P
k Sk. The matrix Bernstein inequality controls the expectation and tail behavior of ∥Z ∥in terms of the matrix variance statistic v(Z ) and the uniform bound L.
The matrix Bernstein inequality is a powerful tool with a huge number of applications. In
these pages, we can only give a coarse indication of how researchers have used this result, so we
have chosen to focus on problems that use random sampling to approximate a speciﬁed matrix.
This model applies to the sample covariance matrix in the introduction. In this chapter, we outline several additional examples. First, we consider the technique of randomized sparsiﬁcation,
in which we replace a dense matrix with a sparse proxy that has similar spectral behavior. Second, we explain how to develop a randomized algorithm for approximate matrix multiplication,
and we establish an error bound for this method. Third, we develop an analysis of random features, a method for approximating kernel matrices that has become popular in contemporary
machine learning.
As these examples suggest, the matrix Bernstein inequality is very effective for studying randomized approximations of a given matrix. Nevertheless, when the matrix Chernoff inequality,
Theorem 5.1.1, happens to apply to a problem, it often delivers better results.
CHAPTER 6. A SUM OF BOUNDED RANDOM MATRICES
Section 6.1 describes the matrix Bernstein inequality. Section 6.2 explains how to use the Bernstein inequality to study randomized methods for matrix approximation. In §§6.3, 6.4, and 6.5,
we apply the latter result to three matrix approximation problems. We conclude with the proof
of the matrix Bernstein inequality in §6.6.
A Sum of Bounded Random Matrices
In the scalar setting, the label “Bernstein inequality” applies to a very large number of concentration results. Most of these bounds have extensions to matrices. For simplicity, we focus on the
most famous of the scalar results, a tail bound for the sum Z of independent, zero-mean random
variables that are subject to a uniform bound. In this case, the Bernstein inequality shows that Z
concentrates around zero. The tails of Z make a transition from subgaussian decay at moderate
deviations to subexponential decay at large deviations. See [BLM13, §2.7] for more information
about Bernstein’s inequality.
In analogy, the simplest matrix Bernstein inequality concerns a sum of independent, zeromean random matrices whose norms are bounded above. The theorem demonstrates that the
norm of the sum acts much like the scalar random variable Z that we discussed in the last paragraph.
Theorem 6.1.1 (Matrix Bernstein). Consider a ﬁnite sequence {Sk} of independent, random matrices with common dimension d1 ×d2. Assume that
for each index k.
Introduce the random matrix
Let v(Z ) be the matrix variance statistic of the sum:
v(Z ) = max
∥E(Z Z ∗)∥, ∥E(Z ∗Z )∥
2v(Z )log(d1 +d2)+ 1
3L log(d1 +d2).
Furthermore, for all t ≥0,
P{∥Z ∥≥t} ≤(d1 +d2) exp
v(Z )+Lt/3
The proof of Theorem 6.1.1 appears in §6.6.
Discussion
Let us spend a few moments to discuss the matrix Bernstein inequality, Theorem 6.1.1, its consequences, and some of the improvements that are available.
6.1. A SUM OF BOUNDED RANDOM MATRICES
Aspects of the Matrix Bernstein Inequality
First, observe that the matrix variance statistic v(Z ) appearing in (6.1.1) coincides with the general deﬁnition (2.2.8) because Z has zero mean. To reach (6.1.2), we have used the additivity
law (2.2.11) for an independent sum to express the matrix variance statistic in terms of the summands. Observe that, when the summands Sk are Hermitian, the two terms in the maximum
The expectation bound (6.1.3) shows that E∥Z ∥is on the same scale as the root
v(Z ) of the
matrix variance statistic and the upper bound L for the summands; there is also a weak dependence on the ambient dimension d. In general, all three of these features are necessary. Nevertheless, the bound may not be very tight for particular examples. See Section 6.1.2 for some
Next, let us explain how to interpret the tail bound (6.1.4). The main difference between this
result and the scalar Bernstein bound is the appearance of the dimensional factor d1 +d2, which
reduces the range of t where the inequality is informative. To get a better idea of what this result
means, it is helpful to make a further estimate:
P{∥Z ∥≥t} ≤
(d1 +d2)·e−3t2/(8v(Z )),
t ≤v(Z )/L
(d1 +d2)·e−3t/(8L),
t ≥v(Z )/L.
In other words, for moderate values of t, the tail probability decays as fast as the tail of a Gaussian
random variable whose variance is comparable with v(Z ). For larger values of t, the tail probability decays at least as fast as that of an exponential random variable whose mean is comparable
with L. As usual, we insert a warning that the tail behavior reported by the matrix Bernstein inequality can overestimate the actual tail behavior.
Last, it is helpful to remember that the matrix Bernstein inequality extends to a sum of uncentered random matrices. In this case, the result describes the spectral-norm deviation of the
random sum from its mean value. For reference, we include the statement here.
Corollary 6.1.2 (Matrix Bernstein: Uncentered Summands). Consider a ﬁnite sequence {Sk} of
independent random matrices with common dimension d1 × d2. Assume that each matrix has
uniformly bounded deviation from its mean:
∥Sk −ESk∥≤L
for each index k.
Introduce the sum
and let v(Z ) denote the matrix variance statistic of the sum:
v(Z ) = max
(Z −EZ )(Z −EZ )∗¤°°,
(Z −EZ )∗(Z −EZ )
(Sk −ESk)(Sk −ESk)∗¤°°,
(Sk −ESk)∗(Sk −ESk)
E∥Z −EZ ∥≤
2v(Z )log(d1 +d2)+ 1
3L log(d1 +d2).
Furthermore, for all t ≥0,
P{∥Z −EZ ∥≥t} ≤(d1 +d2)·exp
v(Z )+Lt/3
This result follows as an immediate corollary of Theorem 6.1.1.
CHAPTER 6. A SUM OF BOUNDED RANDOM MATRICES
Related Results
The bounds in Theorem 6.1.1 are stated in terms of the ambient dimensions d1 and d2 of the
random matrix Z . The dependence on the ambient dimension is not completely natural. For
example, consider embedding the random matrix Z into the top corner of a much larger matrix which is zero everywhere else. It turns out that we can achieve results that reﬂect only the
“intrinsic dimension” of Z . We turn to this analysis in Chapter 7.
In addition, there are many circumstances where the uniform upper bound L that appears
in (6.1.3) does not accurately reﬂect the tail behavior of the random matrix. For instance, the
summands themselves may have very heavy tails. In such emergencies, the following expectation bound [CGT12a, Thm. A.1] can be a lifesaver.
E∥Z ∥2¢1/2 ≤
2ev(Z )log(d1 +d2)+4e
E maxk ∥Sk∥2 ¢1/2 log(d1 +d2).
This result is a matrix formulation of the Rosenthal–Pinelis inequality [Pin94, Thm. 4.1].
Finally, let us reiterate that there are other types of matrix Bernstein inequalities. For example, we can sharpen the tail bound (6.1.4) to obtain a matrix Bennett inequality. We can also
relax the boundedness assumption to a weaker hypothesis on the growth of the moments of each
summand Sk. In the Hermitian setting, the result can also discriminate the behavior of the upper
and lower tails, which is a consequence of Theorem 6.6.1 below. See the notes at the end of this
chapter and the annotated bibliography for more information.
Optimality of the Matrix Bernstein Inequality
To use the matrix Bernstein inequality, Theorem 6.1.1, and its relatives with intelligence, one
must appreciate their strengths and weaknesses. We will focus on the matrix Rosenthal–Pinelis
inequality (6.1.6). Nevertheless, similar insights are relevant to the estimate (6.1.3).
The Expectation Bound
Let us present lower bounds to demonstrate that the matrix Rosenthal–Pinelis inequality (6.1.6)
requires both terms that appear. First, the quantity v(Z ) cannot be omitted because Jensen’s
inequality implies that
E ∥Z ∥2 = E max
∥Z Z ∗∥, ∥Z ∗Z ∥
∥E(Z Z ∗)∥, ∥E(Z ∗Z )∥
Under a natural hypothesis, the second term on the right-hand side of (6.1.6) also is essential.
Suppose that each summand Sk is a symmetric random variable; that is, Sk and −Sk have the
same distribution. In this case, an involved argument [LT91, Prop. 6.10] leads to the bound
E ∥Z ∥2 ≥const·E maxk ∥Sk∥2 .
There are examples where the right-hand side of (6.1.7) is comparable with the uniform upper
bound L on the summands, but this is not always so.
In summary, when the summands Sk are symmetric, we have matching estimates
E maxk ∥Sk∥2 ¢1/2i
E ∥Z ∥2¢1/2
v(Z )log(d1 +d2)+
E maxk ∥Sk∥2 ¢1/2 log(d1 +d2)
We see that the bound (6.1.6) must include some version of each term that appears, but the
logarithms are not always necessary.
6.1. A SUM OF BOUNDED RANDOM MATRICES
Examples where the Logarithms Appear
First, let us show that the variance term in (6.1.6) must contain a logarithm. For each natural
number n, consider the d ×d random matrix Z of the form
where {ϱik} is an independent family of Rademacher random variables. An easy application of
the bound (6.1.6) implies that
E ∥Zn∥≤Const·
log(2d)+ 1
pn log(2d)
Using the central limit theorem and the Skorokhod representation, we can construct an independent family {γk} of standard normal random variables for which
almost surely as n →∞.
But this fact ensures that
°°°°° = E maxk |γk| ≈
Therefore, we cannot remove the logarithm from the variance term in (6.1.6).
Next, let us justify the logarithm on the norm of the summands in (6.1.6). For each natural
number n, consider a d ×d random matrix Z of the form
is an independent family of BERNOULLI(n−1) random variables. The matrix Rosenthal–
Pinelis inequality (6.1.6) ensures that
E ∥Zn∥≤Const·
log(2d)+log(2d)
Using the Poisson limit of a binomial random variable and the Skorohod representation, we can
construct an independent family {Qk} of POISSON(1) random variables for which
(Qk −1)Ekk
almost surely as n →∞.
Therefore,
(Qk −1)Ekk
°°°°° = E maxk |Qk −1| ≈const·
In short, the bound we derived from (6.1.6) requires the logarithm on the second term, but it is
suboptimal by a loglog factor. The upper matrix Chernoff inequality (5.1.6) correctly predicts the
appearance of the iterated logarithm in this example, as does the matrix Bennett inequality.
CHAPTER 6. A SUM OF BOUNDED RANDOM MATRICES
The last two examples rely heavily on the commutativity of the summands as well as the
inﬁnite divisibility of the normal and Poisson distributions. As a consequence, it may appear
that the logarithms only appear in very special contexts. In fact, many (but not all!) examples
that arise in practice do require the logarithms that appear in the matrix Bernstein inequality.
It is a subject of ongoing research to obtain a simple criterion for deciding when the logarithms
Example: Matrix Approximation by Random Sampling
In applied mathematics, we often need to approximate a complicated target object by a more
structured object. In some situations, we can solve this problem using a beautiful probabilistic approach called empirical approximation. The basic idea is to construct a “simple” random
object whose expectation equals the target. We obtain the approximation by averaging several
independent copies of the simple random object. As the number of terms in this average increases, the approximation becomes more complex, but it represents the target more faithfully.
The challenge is to quantify this tradeoff.
In particular, we often encounter problems where we need to approximate a matrix by a more
structured matrix. For example, we may wish to ﬁnd a sparse matrix that is close to a given
matrix, or we may need to construct a low-rank matrix that is close to a given matrix. Empirical
approximation provides a mechanism for obtaining these approximations. The matrix Bernstein
inequality offers a natural tool for assessing the quality of the randomized approximation.
This section develops a general framework for empirical approximation of matrices. Subsequent sections explain how this technique applies to speciﬁc examples from the ﬁelds of randomized linear algebra and machine learning.
Let B be a target matrix that we hope to approximate by a more structured matrix. To that end,
let us represent the target as a sum of “simple” matrices:
The idea is to identify summands with desirable properties that we want our approximation to
inherit. The examples in this chapter depend on decompositions of the form (6.2.1).
Along with the decomposition (6.2.1), we need a set of sampling probabilities:
for i = 1,...,N.
We want to ascribe larger probabilities to “more important” summands. Quantifying what “important” means is the most difﬁcult aspect of randomized matrix approximation. Choosing the
right sampling distribution for a speciﬁc problem requires insight and ingenuity.
Given the data (6.2.1) and (6.2.2), we may construct a “simple” random matrix R by sampling:
with probability pi.
This construction ensures that R is an unbiased estimator of the target: ER = B. Even so, the
random matrix R offers a poor approximation of the target B because it has a lot more structure.
6.2. EXAMPLE: MATRIX APPROXIMATION BY RANDOM SAMPLING
To improve the quality of the approximation, we average n independent copies of the random
matrix R. We obtain an estimator of the form
where each Rk is an independent copy of R.
By linearity of expectation, this estimator is also unbiased: E ¯Rn = B. The approximation ¯Rn remains structured when the number n of terms in the approximation is small as compared with
the number N of terms in the decomposition (6.2.1).
Our goal is to quantify the approximation error as a function of the complexity n of the approximation:
E∥¯Rn −B∥≤error(n).
There is a tension between the total number n of terms in the approximation and the error
error(n) the approximation incurs. In applications, it is essential to achieve the right balance.
Error Estimate for Matrix Sampling Estimators
We can obtain an error estimate for the approximation scheme described in Section 6.2.1 as an
immediate corollary of the matrix Bernstein inequality, Theorem 6.1.1.
Corollary 6.2.1 (Matrix Approximation by Random Sampling). Let B be a ﬁxed d1 × d2 matrix.
Construct a d1 ×d2 random matrix R that satisﬁes
Compute the per-sample second moment:
m2(R) = max
∥E(RR∗)∥, ∥E(R∗R)∥
Form the matrix sampling estimator
where each Rk is an independent copy of R.
Then the estimator satisﬁes
E∥¯Rn −B∥≤
2m2(R)log(d1 +d2)
+ 2L log(d1 +d2)
Furthermore, for all t ≥0,
P{∥¯Rn −B∥≥t} ≤(d1 +d2)exp
m2(R)+2Lt/3
Proof. Since R is an unbiased estimator of the target matrix B, we can write
Z = ¯Rn −B = 1
(Rk −ER) =
CHAPTER 6. A SUM OF BOUNDED RANDOM MATRICES
We have deﬁned the summands Sk = n−1(Rk −ER). These random matrices form an independent and identically distributed family, and each Sk has mean zero.
Now, each of the summands is subject to an upper bound:
n (∥Rk∥+∥ER∥) ≤1
n (∥Rk∥+E∥R∥) ≤2L
The ﬁrst relation is the triangle inequality; the second is Jensen’s inequality. The last estimate
follows from our assumption that ∥R∥≤L.
To control the matrix variance statistic v(Z ), ﬁrst note that
v(Z ) = max
= n ·max{∥E(S1S∗
1 )∥, ∥E(S∗
The ﬁrst identity follows from the expression (6.1.2) for the matrix variance statistic, and the
second holds because the summands Sk are identically distributed. We may calculate that
1 ) = n−2 E
(R −ER)(R −ER)∗¤
E(RR∗)−(ER)(ER)∗¤ ≼n−2 E(RR∗).
The ﬁrst relation holds because the expectation of the random positive-semideﬁnite matrix S1S∗
is positive semideﬁnite. The ﬁrst identity follows from the deﬁnition of S1 and the fact that R1
has the same distribution as R. The second identity is a direct calculation. The last relation holds
because (ER)(ER)∗is positive semideﬁnite. As a consequence,
n2 ∥E(RR∗)∥.
n2 ∥E(R∗R)∥.
In summary,
n max{∥E(RR∗)∥, ∥E(R∗R)∥} = m2(R)
The last line follows from the deﬁnition (6.2.4) of m2(R).
We are prepared to apply the matrix Bernstein inequality, Theorem 6.1.1, to the random matrix Z = P
k Sk. This operation results in the statement of the corollary.
Discussion
One of the most common applications of the matrix Bernstein inequality is to analyze empirical
matrix approximations. As a consequence, Corollary 6.2.1 is one of the most useful forms of the
matrix Bernstein inequality. Let us discuss some of the important aspects of this result.
Understanding the Bound on the Approximation Error
First, let us examine how many samples n sufﬁce to bring the approximation error bound in
Corollary 6.2.1 below a speciﬁed positive tolerance ε. Examining inequality (7.3.5), we ﬁnd that
n ≥2m2(R)log(d1 +d2)
+ 2L log(d1 +d2)
E∥¯Rn −B∥≤2ε.
6.2. EXAMPLE: MATRIX APPROXIMATION BY RANDOM SAMPLING
Roughly, the number n of samples should be on the scale of the per-sample second moment
m2(R) and the uniform upper bound L.
The bound (6.2.7) also reveals an unfortunate aspect of empirical matrix approximation. To
make the tolerance ε small, the number n of samples must increase proportional with ε−2. In
other words, it takes many samples to achieve a highly accurate approximation. We cannot avoid
this phenomenon, which ultimately is a consequence of the central limit theorem.
On a more positive note, it is quite valuable that the error bounds (7.3.5) and (7.3.6) involve
the spectral norm. This type of estimate simultaneously controls the error in every linear function of the approximation:
∥¯Rn −B∥≤ε
¯¯tr( ¯RnC)−tr(BC)
when ∥C∥S1 ≤1.
The Schatten 1-norm ∥·∥S1 is deﬁned in (2.1.29). These bounds also control the error in each
singular value σj ( ¯Rn) of the approximation:
∥¯Rn −B∥≤ε
¯¯σj ( ¯Rn)−σj (B)
for each j = 1,2,3,...,min{d1,d2}.
When there is a gap between two singular values of B, we can also obtain bounds for the discrepancy between the associated singular vectors of ¯Rn and B using perturbation theory.
To construct a good sampling estimator R, we ought to control both m2(R) and L. In practice, this demands considerable creativity. This observation hints at the possibility of achieving
a bias–variance tradeoff when approximating B. To do so, we can drop all of the “unimportant”
terms in the representation (6.2.1), i.e., those whose sampling probabilities are small. Then we
construct a random approximation R only for the “important” terms that remain. Properly executed, this process may decrease both the per-sample second moment m2(R) and the upper
bound L. The idea is analogous with shrinkage in statistical estimation.
A General Sampling Model
Corollary 6.2.1 extends beyond the sampling model based on the ﬁnite expansion (6.2.1). Indeed,
we can consider a more general decomposition of the target matrix B:
where µ is a probability measure on a sample space Ω. As before, the idea is to represent the
target matrix B as an average of “simple” matrices B(ω). The main difference is that the family
of simple matrices may now be inﬁnite. In this setting, we construct the random approximation
P{R ∈E} = µ{ω : B(ω) ∈E}
for E ⊂Md1×d2
In particular, it follows that
As we will discuss, this abstraction is important for applications in machine learning.
CHAPTER 6. A SUM OF BOUNDED RANDOM MATRICES
Suboptimality of Sampling Estimators
Another fundamental point about sampling estimators is that they are usually suboptimal. In
other words, the matrix sampling estimator may incur an error substantially worse than the error
in the best structured approximation of the target matrix.
To see why, let us consider a simple form of low-rank approximation by random sampling.
The method here does not have practical value, but it highlights the reason that sampling estimators usually do not achieve ideal results. Suppose that B has singular value decomposition
N = min{d1,d2}.
Given the SVD, we can construct a random rank-one approximation R of the form
with probability
Per Corollary 6.2.1, the error in the associated sampling estimator ¯Rn of B satisﬁes
2log(d1 +d2)
+ 2log(d1 +d2)
On the other hand, a best rank-n approximation of B takes the form Bn = Pn
j=1 σj u j v∗
j , and it
incurs error
∥Bn −B∥= σn+1 ≤
The second relation is Markov’s inequality, which provides an accurate estimate only when the
singular values σ1,...,σn+1 are comparable. In that case, the sampling estimator arrives within a
logarithmic factor of the optimal error. But there are many matrices whose singular values decay
quickly, so that σn+1 ≪(n + 1)−1. In the latter situation, the error in the sampling estimator is
much worse than the optimal error.
Warning: Frobenius-Norm Bounds
We often encounter papers that develop Frobenius-norm error bounds for matrix approximations, perhaps because the analysis is more elementary. But one must recognize that Frobeniusnorm error bounds are not acceptable in most cases of practical interest:
Frobenius-norm error bounds are typically vacuous.
In particular, this phenomenon occurs in data analysis whenever we try to approximate a matrix
that contains white or pink noise.
To illustrate this point, let us consider the ubiquitous problem of approximating a low-rank
matrix corrupted by additive white Gaussian noise:
B = xx∗+αE ∈Md.
The desired approximation of the matrix B is the rank-one matrix Bopt = xx∗. For modeling
purposes, we assume that E has independent NORMAL(0,d−1) entries. As a consequence,
6.3. APPLICATION: RANDOMIZED SPARSIFICATION OF A MATRIX
Now, the spectral-norm error in the desired approximation satisﬁes
∥Bopt −B∥= α∥E∥≈2α.
On the other hand, the Frobenius-norm error in the desired approximation satisﬁes
∥Bopt −B∥F = α∥E∥F ≈α
We see that the Frobenius-norm error can be quite large, even when we ﬁnd the required approximation.
Here is another way to look at the same fact. Suppose we construct an approximation bB of
the matrix B from (6.2.8) whose Frobenius-norm error is comparable with the optimal error:
∥bB −B∥F ≤ε
There is no reason for the approximation bB to have any relationship with the desired approximation Bopt. For example, the approximation bB = αE satisﬁes this error bound with ε = d−1/2 even
though bB consists only of noise.
Application: Randomized Sparsiﬁcation of a Matrix
Many tasks in data analysis involve large, dense matrices that contain a lot of redundant information. For example, an experiment that tabulates many variables about a large number of
subjects typically results in a low-rank data matrix because subjects are often similar with each
other. Many questions that we pose about these data matrices can be addressed by spectral
computations. In particular, factor analysis involves a singular value decomposition.
When the data matrix is approximately low rank, it has fewer degrees of freedom than its
ambient dimension. Therefore, we can construct a simpler approximation that still captures
most of the information in the matrix. One method for ﬁnding this approximation is to replace
the dense target matrix by a sparse matrix that is close in spectral-norm distance. An elegant
way to identify this sparse proxy is to randomly select a small number of entries from the original
matrix to retain. This is a type of empirical approximation.
Sparsiﬁcation has several potential advantages. First, it is considerably less expensive to store
a sparse matrix than a dense matrix. Second, many algorithms for spectral computation operate
more efﬁciently on sparse matrices.
In this section, we examine a very recent approach to randomized sparsiﬁcation due to Kundu
& Drineas [KD14]. The analysis is an immediate consequence of Corollary 6.2.1. See the notes at
the end of the chapter for history and references.
Problem Formulation & Randomized Algorithm
Let B be a ﬁxed d1 ×d2 complex matrix. The sparsiﬁcation problem requires us to ﬁnd a sparse
matrix bB that has small distance from B with respect to the spectral norm. We can achieve this
goal using an empirical approximation strategy.
First, let us express the target matrix as a sum of its entries:
bi j Ei j .
CHAPTER 6. A SUM OF BOUNDED RANDOM MATRICES
Introduce sampling probabilities
for i = 1,...,d1 and j = 1,...,d2.
The Frobenius norm is deﬁned in (2.1.2), and the entrywise ℓ1 norm is deﬁned in (2.1.30). It is
easy to check that the numbers pi j form a probability distribution. Let us emphasize that the
non-obvious form of the distribution (6.3.1) represents a decade of research.
Now, we introduce a d1 ×d2 random matrix R that has exactly one nonzero entry:
·bi j Ei j
with probability pi j .
We use the convention that 0/0 = 0 so that we do not need to treat zero entries separately. It is
immediate that
·bi j Ei j · pi j =
bi j Ei j = B.
Therefore, R is an unbiased estimate of B.
Although the expectation of R is correct, its variance is quite high. Indeed, R has only one
nonzero entry, while B typically has many nonzero entries. To reduce the variance, we combine
several independent copies of the simple estimator:
where each Rk is an independent copy of R.
By linearity of expectation, E ¯Rn = B. Therefore, the matrix ¯Rn has at most n nonzero entries,
and its also provides an unbiased estimate of the target. The challenge is to quantify the error
∥¯Rn −B∥as a function of the sparsity level n.
Performance of Randomized Sparsiﬁcation
The randomized sparsiﬁcation method is clearly a type of empirical approximation, so we can
use Corollary 6.2.1 to perform the analysis. We will establish the following error bound.
E∥¯Rn −B∥≤
F ·max{d1,d2}log(d1 +d2)
+ 4∥B∥ℓ1 log(d1 +d2)
The short proof of (6.3.2) appears below in Section 6.3.3.
Let us explain the content of the estimate (6.3.2). First, the bound (2.1.31) allows us to replace
the ℓ1 norm by the Frobenius norm:
d1d2 ·∥B∥F ≤max{d1, d2}·∥B∥F .
Placing the error (6.3.2) on a relative scale, we see that
4max{d1, d2}log(d1 +d2)
+ 4max{d1, d2}log(d1 +d2)
6.3. APPLICATION: RANDOMIZED SPARSIFICATION OF A MATRIX
The stable rank srank(B), deﬁned in (2.1.25), emerges naturally as a quantity of interest.
Now, suppose that the sparsity level n satisﬁes
n ≥ε−2 ·srank(B)·max{d1, d2}log(d1 +d2)
where the tolerance ε ∈(0,1]. We determine that
Since the stable rank always exceeds one and we have assumed that ε ≤1, this estimate implies
We discover that it is possible to replace the matrix B by a matrix with at most n nonzero entries
while achieving a small relative error in the spectral norm. When srank(B) ≪min{d1, d2}, we
can achieve a dramatic reduction in the number of nonzero entries needed to carry the spectral
information in the matrix B.
Analysis of Randomized Sparsiﬁcation
Let us proceed with the analysis of randomized sparsiﬁcation. To apply Corollary 6.2.1, we need
to obtain bounds for the per-sample variance m2(R) and the uniform upper bound L. The key
to both calculations is to obtain appropriate lower bounds on the sampling probabilities pi j .
2 · |bi j |
2 · |bi j |2
Each estimate follows by neglecting one term in (6.3.3).
First, we turn to the uniform bound on the random matrix R. We have
i j bi j Ei j ∥= max
·|bi j | ≤2∥B∥ℓ1 .
The last inequality depends on the ﬁrst bound in (6.3.3). Therefore, we may take L = 2∥B∥ℓ1.
Second, we turn to the computation of the per-sample second moment m2(R). We have
·(bi j Ei j )(bi j Ei j )∗pi j
Eii = 2d2 ∥B∥2
The semideﬁnite inequality holds because each matrix |bi j |2Eii is positive semideﬁnite and because of the second bound in (6.3.3). Similarly,
E(R∗R) ≼2d1 ∥B∥2
CHAPTER 6. A SUM OF BOUNDED RANDOM MATRICES
In summary,
m2(R) = max
∥E(RR∗)∥, ∥E(R∗R)∥
≤2max{d1,d2}.
This is the required estimate for the per-sample second moment.
Finally, to reach the advertised error bound (6.3.2), we invoke Corollary 6.2.1 with the parameters L = ∥B∥ℓ1 and m2(R) ≤2max{d1,d2}.
Application: Randomized Matrix Multiplication
Numerical linear algebra (NLA) is a well-established and important part of computer science.
Some of the basic problems in this area include multiplying matrices, solving linear systems,
computing eigenvalues and eigenvectors, and solving linear least-squares problems. Historically, the NLA community has focused on developing highly accurate deterministic methods
that require as few ﬂoating-point operations as possible. Unfortunately, contemporary applications can strain standard NLA methods because problems have continued to become larger.
Furthermore, on modern computer architectures, computational costs depend heavily on communication and other resources that the standard algorithms do not manage very well.
In response to these challenges, researchers have started to develop randomized algorithms
for core problems in NLA. In contrast to the classical algorithms, these new methods make random choices during execution to achieve computational efﬁciencies. These randomized algorithms can also be useful for large problems or for modern computer architectures. On the other
hand, randomized methods can fail with some probability, and in some cases they are less accurate than their classical competitors.
Matrix concentration inequalities are one of the key tools used to design and analyze randomized algorithms for NLA problems. In this section, we will describe a randomized method
for matrix multiplication developed by Magen & Zouzias [MZ11, Zou13]. We will analyze this
algorithm using Corollary 6.2.1. Turn to the notes at the end of the chapter for more information
about the history.
Problem Formulation & Randomized Algorithm
One of the basic tasks in numerical linear algebra is to multiply two matrices with compatible
dimensions. Suppose that B is a d1 × N complex matrix and that C is an N ×d2 complex matrix,
and we wish to compute the product BC. The straightforward algorithm forms the product entry
for each i = 1,...,d1 and k = 1,...,d2.
This approach takes O(N ·d1d2) arithmetic operations. There are algorithms, such as Strassen’s
divide-and-conquer method, that can reduce the cost, but these approaches are not considered
practical for most applications.
Suppose that the inner dimension N is substantially larger than the outer dimensions d1 and
d2. In this setting, both matrices B and C are rank-deﬁcient, so the columns of B contain a lot of
linear dependencies, as do the rows of C. As a consequence, a random sample of columns from
B (or rows from C) can be used as a proxy for the full matrix. Formally, the key to this approach
6.4. APPLICATION: RANDOMIZED MATRIX MULTIPLICATION
is to view the matrix product as a sum of outer products:
As usual, b:j denotes the jth column of B, while c j: denotes the jth row ofC. We can approximate
this sum using the empirical method.
To develop an algorithm, the ﬁrst step is to construct a simple random matrix R that provides
an unbiased estimate for the matrix product. To that end, we pick a random index and form a
rank-one matrix from the associated columns of B and row of C. More precisely, deﬁne
p j = ∥b:j ∥2 +∥c j:∥2
for j = 1,2,3,...,N.
The Frobenius norm is deﬁned in (2.1.2). Using the properties of the norms, we can easily check
that (p1,p2,p3,...,pN) forms a bonaﬁde probability distribution. The cost of computing these
probabilities is at most O(N · (d1 + d2)) arithmetic operations, which is much smaller than the
cost of forming the product BC when d1 and d2 are large.
We now deﬁne a d1 ×d2 random matrix R by the expression
with probability p j .
We use the convention that 0/0 = 0 so we do not have to treat zero rows and columns separately.
It is straightforward to compute the expectation of R:
·b:j c j: · p j =
b:j c j: = BC.
As required, R is an unbiased estimator for the product BC.
Although the expectation of R is correct, its variance is quite high. Indeed, R has rank one,
while the rank of BC is usually larger! To reduce the variance, we combine several independent
copies of the simple estimator:
each Rk is an independent copy of R.
By linearity of expectation, E ¯Rn = BC, so we imagine that ¯Rn approximates the product well.
To see whether this heuristic holds true, we need to understand how the error E∥¯Rn −BC∥
depends on the number n of samples. It costs O(n · d1d2) ﬂoating-point operations to determine all the entries of ¯Rn. Therefore, when the number n of samples is much smaller than the
inner dimension N of the matrices, we can achieve signiﬁcant economies over the naïve matrix
multiplication algorithm.
In fact, it requires no computation beyond sampling the row/column indices to express ¯Rn in
the form (6.4.4). This approach gives an inexpensive way to represent the product approximately.
CHAPTER 6. A SUM OF BOUNDED RANDOM MATRICES
Performance of Randomized Matrix Multiplication
To simplify our presentation, we will assume that both matrices have been scaled so that their
spectral norms are equal to one:
∥B∥= ∥C∥= 1.
It is relatively inexpensive to compute the spectral norm of a matrix accurately, so this preprocessing step is reasonable.
Let asr = 1
2(srank(B)+srank(C)) be the average stable rank of the two factors; see (2.1.25) for
the deﬁnition of the stable rank. In §6.4.3, we will prove that
E∥¯Rn −BC∥≤
4·asr·log(d1 +d2)
+ 2·asr·log(d1 +d2)
To appreciate what this estimate means, suppose that the number n of samples satisﬁes
n ≥ε−2 ·asr·log(d1 +d2)
where ε is a positive tolerance. Then we obtain a relative error bound for the randomized matrix
multiplication method
E∥¯Rn −BC∥
This expression depends on the normalization of B and C. The computational cost of forming
the approximation is
O(ε−2 ·asr·d1d2 log(d1 +d2))
arithmetic operations.
In other words, when the average stable rank asr is substantially smaller than the inner dimension N of the two matrices B and C, the random estimate ¯Rn for the product BC achieves a small
error relative to the scale of the factors.
Analysis of Randomized Matrix Multiplication
The randomized matrix multiplication method is just a speciﬁc example of empirical approximation, and the error bound (6.4.5) is an immediate consequence of Corollary 6.2.1.
To pursue this approach, we need to establish a uniform bound on the norm of the estimator
R for the product. Observe that
∥R∥≤maxj ∥p−1
j b:j c j:∥= maxj
∥b:j ∥∥c j:∥
To obtain a bound, recall the value (6.4.3) of the probability p j , and invoke the inequality between geometric and arithmetic means:
∥b:j ∥∥c j:∥
∥b:j ∥2 +∥c j:∥2 ≤1
Since the matrices B and C have unit spectral norm, we can express this inequality in terms of
the average stable rank:
srank(B)+srank(C)
6.5. APPLICATION: RANDOM FEATURES
This is the exactly kind of bound that we need.
Next, we need an estimate for the per-sample second moment m2(R). By direct calculation,
·(b:j c j:)(b:j c j:)∗· p j
∥b:j ∥2 +∥c j:∥2 ·b:j b∗
The semideﬁnite relation holds because each fraction lies between zero and one, and each matrix b:j b∗
:j is positive semideﬁnite. Therefore, increasing the fraction to one only increases in the
matrix in the semideﬁnite order. Similarly,
In summary,
m2(R) = max
∥E(RR∗)∥, ∥E(R∗R)∥
∥BB∗∥, ∥C ∗C∥
The penultimate line depends on the identity (2.1.24) and our assumption that both matrices B
and C have norm one.
Finally, to reach the stated estimate (6.4.5), we apply Corollary 6.2.1 with the parameters L =
asr and m2(R) ≤2·asr.
Application: Random Features
As a ﬁnal application of empirical matrix approximation, let us discuss a contemporary idea
from machine learning called random features. Although this technique may appear more sophisticated than randomized sparsiﬁcation or randomized matrix multiplication, it depends
on exactly the same principles. Random feature maps were proposed by Ali Rahimi and Ben
Recht [RR07]. The analysis in this section is due to David Lopez-Paz et al. [LPSS+14].
Kernel Matrices
Let X be a set. We think about the elements of the set X as (potential) observations that we
would like to use to perform learning and inference tasks. Let us introduce a bounded measure
Φ of similarity between pairs of points in the set:
Φ : X ×X →[−1,+1].
The similarity measure Φ is often called a kernel. We assume that the kernel returns the value +1
when its arguments are identical, and it returns smaller values when its arguments are dissimilar.
We also assume that the kernel is symmetric; that is, Φ(x, y) = Φ(y,x) for all arguments x, y ∈X .
CHAPTER 6. A SUM OF BOUNDED RANDOM MATRICES
A simple example of a kernel is the angular similarity between a pair of points in a Euclidean
Φ(x, y) = 2
π arcsin 〈x, y〉
∥x∥∥y∥= 1−2∡(x, y)
for x, y ∈Rd.
We write ∡(·,·) for the planar angle between two vectors, measured in radians. As usual, we
instate the convention that 0/0 = 0. See Figure 6.1 for an illustration.
Suppose that x1,...,xN ∈X are observations. The kernel matrixG = [gi j ] ∈MN just tabulates
the values of the kernel function for each pair of data points:
gi j = Φ(xi,x j )
for i, j = 1,...,N.
It may be helpful to think about the kernel matrix G as a generalization of the Gram matrix of a
family of points in a Euclidean space. We say that the kernel Φ is positive deﬁnite if the kernel
matrix G is positive semideﬁnite for any choice of observations {xi} ⊂X . We will be concerned
only with positive-deﬁnite kernels in this discussion.
In the Euclidean setting, there are statistical learning methods that only require the inner
product between each pair of observations. These algorithms can be extended to the kernel setting by replacing each inner product with a kernel evaluation. As a consequence, kernel matrices
can be used for classiﬁcation, regression, and feature selection. In these applications, kernels
are advantageous because they work outside the Euclidean domain, and they allow task-speciﬁc
measures of similarity. This idea, sometimes called the kernel trick, is one of the major insights
in modern machine learning.
A signiﬁcant challenge for algorithms based on kernels is that the kernel matrix is big. Indeed, G contains O(N 2) entries, where N is the number of data points. Furthermore, the cost
of constructing the kernel matrix is O(dN2) where d is the number of parameters required to
specify a point in the universe X .
Nevertheless, there is an opportunity. Large data sets tend to be redundant, so the kernel
matrix also tends to be redundant. This manifests in the kernel matrix being close to a low-rank
matrix. As a consequence, we may try to replace the kernel matrix by a low-rank proxy. For some
similarity measures, we can accomplish this task using empirical approximation.
Random Features and Low-Rank Approximation of the Kernel Matrix
In certain cases, a positive-deﬁnite kernel can be written as an expectation, and we can take advantage of this representation to construct an empirical approximation of the kernel matrix. Let
us begin with the general construction, and then we will present a few examples in Section 6.5.3.
Let W be a sample space equipped with a sigma-algebra and a probability measure µ. Introduce a bounded feature map:
ψ : X ×W →[−b,+b]
where b ≥0.
Consider a random variable w taking values in W and distributed according to the measure µ.
We assume that this random variable satisﬁes the reproducing property
Φ(x, y) = Ew
ψ(x;w)·ψ(y;w)
for all x, y ∈X .
The pair (ψ,w) is called a random feature map for the kernel Φ.
6.5. APPLICATION: RANDOM FEATURES
We want to approximate the kernel matrix with a set {x1,...,xN} ⊂X of observations. To do
so, we draw a random vector w ∈W distributed according to µ. Form a random vector z ∈RN by
applying the feature map to each data point with the same choice of the random vector w. That
The vector z is sometimes called a random feature. By the reproducing property (6.5.2) for the
random feature map,
gi j = Φ(xi,x j ) = Ew
ψ(xi;w)·ψ(x j ;w)
for i, j = 1,2,3,...,N.
We can write this relation in matrix form as G = E(zz∗). Therefore, the random matrix R = zz∗is
an unbiased rank-one estimator for the kernel matrix G. This representation demonstrates that
random feature maps, as deﬁned here, only exist for positive-deﬁnite kernels.
As usual, we construct a better empirical approximation of the kernel matrix G by averaging
several realizations of the simple estimator R:
where each Rk is an independent copy of R.
In other words, we are using n independent random features z1,...,zn to approximate the kernel
matrix. The question is how many random features are needed before our estimator is accurate.
Examples of Random Feature Maps
Before we continue with the analysis, let us describe some random feature maps. This discussion
is tangential to our theme of matrix concentration, but it is valuable to understand why random
feature maps exist.
First, let us consider the angular similarity (6.5.1) deﬁned on Rd. We can construct a random
feature map using a classical result from plane geometry. If we draw w uniformly from the unit
sphere Sd−1 ⊂Rd, then
Φ(x; y) = 1−2∡(x, y)
sgn〈x, w〉·sgn〈y, w〉
for all x, y ∈X .
The easy proof of this relation should be visible from the diagram in Figure 6.1. In light of the
formula (6.5.4), we set W = Sd−1 with the uniform measure, and we deﬁne the feature map
ψ(x;w) = sgn〈x, w〉.
The reproducing property (6.5.2) follows immediately from (6.5.4). Therefore, the pair (ψ,w) is a
random feature map for the angular similarity kernel.
Next, let us describe an important class of kernels that can be expressed using random feature
maps. A kernel on Rd is translation invariant if there is a function ϕ : Rd →R for which
Φ(x, y) = ϕ(x −y)
for all x, y ∈Rd.
CHAPTER 6. A SUM OF BOUNDED RANDOM MATRICES
sgn〈x, u〉·sgn〈y, u〉= +1
sgn〈x, u〉·sgn〈y, u〉= −1
Figure 6.1: The angular similarity between two vectors. Let x and y be nonzero vectors in
R2 with angle ∡(x, y). The red region contains the directions u where the product sgn〈x, u〉·
sgn〈y, u〉equals +1, and the blue region contains the directions u where the same product
equals −1. The blue region subtends a total angle of 2∡(x, y), and the red region subtends a
total angle of 2π−2∡(x, y).
Bôchner’s Theorem, a classical result from harmonic analysis, gives a representation for each
continuous, positive-deﬁnite, translation-invariant kernel:
Φ(x, y) = ϕ(x −y) = c
Rd ei〈x, w〉·e−i〈y, w〉dµ(w)
for all x, y ∈Rd.
In this expression, the positive scale factor c and the probability measure µ depend only on the
function ϕ. The formula (6.5.5) yields a (complex-valued) random feature map:
c ei〈x, w〉
where w has distribution µ on Rd.
This map satisﬁes a complex variant of the reproducing property (6.5.2):
Φ(x, y) = Ew
ψC(x;w)·ψC(y;w)∗¤
for all x, y ∈Rd,
where we have written ∗for complex conjugation.
With a little more work, we can construct a real-valued random feature map. Recall that the
kernel Φ is symmetric, so the complex exponentials in (6.5.5) can be written in terms of cosines.
This observation leads to the random feature map
ψ(x;w,U) =
w ∼µ and U ∼UNIFORM[0,2π].
To verify that (ψ,(w,U)) reproduces the kernel Φ, as required by (6.5.2), we just make a short
calculation using the angle-sum formula for the cosine.
6.5. APPLICATION: RANDOM FEATURES
We conclude this section with the most important example of a random feature map from
the class we have just described. Consider the Gaussian radial basis function kernel:
Φ(x, y) = e−α∥x−y∥2/2
for all x, y ∈Rd.
The positive parameter α reﬂects how close two points must be before they are regarded as “similar.” For the Gaussian kernel, Bôchner’s Theorem (6.5.5) holds with the scaling factor c = 1 and
the probability measure µ = NORMAL(0,αId). In summary, we deﬁne
ψ(x;w,U) =
w ∼NORMAL(0,αId) and U ∼UNIFORM[0,2π].
This random feature map reproduces the Gaussian radial basis function kernel.
Performance of the Random Feature Approximation
We will demonstrate that the approximation ¯Rn of the N × N kernel matrix G using n random
features, constructed in (6.5.3), leads to an estimate of the form
E∥¯Rn −G∥≤
2bN ∥G∥log(2N)
+ 2bN log(2N)
In this expression, b is the uniform bound on the magnitude of the feature map ψ. The short
proof of (6.5.7) appears in §6.5.5.
To clarify what this result means, we introduce the intrinsic dimension of the N × N kernel
intdim(G) = srank(G1/2) = trG
The stable rank is deﬁned in Section 2.1.15. We have used the assumption that the similarity
measure is positive deﬁnite to justify the computation of the square root of the kernel matrix,
and trG = N because of the requirement that Φ(x,x) = +1 for all x ∈X . See §7.1 for further
discussion of the intrinsic dimension
Now, assume that the number n of random features satisﬁes the bound
n ≥2bε−2 ·intdim(G)·log(2N),
In view of (6.5.7), the relative error in the empirical approximation of the kernel matrix satisﬁes
We learn that the randomized approximation of the kernel matrix G is accurate when its intrinsic
dimension is much smaller than the number of data points. That is, intdim(G) ≪N.
Analysis of the Random Feature Approximation
The analysis of random features is based on Corollary 6.2.1. To apply this result, we need the
per-sample second-moment m2(R) and the uniform upper bound L. Both are easy to come by.
First, observe that
∥R∥= ∥zz∗∥= ∥z∥2 ≤bN
CHAPTER 6. A SUM OF BOUNDED RANDOM MATRICES
Recall that b is the uniform bound on the feature map ψ, and N is the number of components in
the random feature vector z.
Second, we calculate that
∥z∥2 zz∗¢ ≼bN ·E(zz∗) = bN ·G.
Each random matrix zz∗is positive semideﬁnite, so we can introduce the upper bound ∥z∥2 ≤
bN. The last identity holds because R is an unbiased estimator of the kernel matrix G. It follows
m2(R) = ∥ER2∥≤bN ·∥G∥.
This is our bound for the per-sample second moment.
Finally, we invoke Corollary 6.2.1 with parameters L = bN and m2(R) ≤bN ∥G∥to arrive at
the estimate (6.5.7).
Proof of the Matrix Bernstein Inequality
Now, let us turn to the proof of the matrix Bernstein inequality, Theorem 6.1.1. This result is a
corollary of a matrix concentration inequality for a sum of bounded random Hermitian matrices.
We begin with a statement and discussion of the Hermitian result, and then we explain how the
general result follows.
A Sum of Bounded Random Hermitian Matrices
The ﬁrst result is a Bernstein inequality for a sum of independent, random Hermitian matrices
whose eigenvalues are bounded above.
Theorem 6.6.1 (Matrix Bernstein: Hermitian Case). Consider a ﬁnite sequence {Xk} of independent, random, Hermitian matrices with dimension d. Assume that
λmax(Xk) ≤L
for each index k.
Introduce the random matrix
Let v(Y ) be the matrix variance statistic of the sum:
v(Y ) = ∥EY 2∥=
Eλmax(Y ) ≤
2v(Y )logd + 1
Furthermore, for all t ≥0.
P{λmax (Y ) ≥t} ≤d ·exp
v(Y )+Lt/3
The proof of Theorem 6.6.1 appears below in §6.6.
6.6. PROOF OF THE MATRIX BERNSTEIN INEQUALITY
Discussion
Theorem 6.6.1 also yields information about the minimum eigenvalue of an independent sum
of d-dimensional Hermitian matrices. Suppose that the independent random matrices satisfy
λmin(Xk) ≥−L
for each index k.
Applying the expectation bound (6.6.2) to −Y , we obtain
Eλmin(Y ) ≥−
2v(Y )logd −1
We can use (6.6.3) to develop a tail bound. For t ≥0,
P{λmin(Y ) ≤−t} ≤d ·exp
v(Y )+Lt/3
Let us emphasize that the bounds for λmax(Y ) and λmin(Y ) may diverge because the two parameters L and L can take sharply different values. This fact indicates that the maximum eigenvalue bound in Theorem 6.6.1 is a less strict assumption than the spectral norm bound in Theorem 6.1.1.
Bounds for the Matrix Mgf and Cgf
In establishing the matrix Bernstein inequality, the main challenge is to obtain an appropriate
bound for the matrix mgf and cgf of a zero-mean random matrix whose norm satisﬁes a uniform
bound. We do not present the sharpest estimate possible, but rather the one that leads most
directly to the useful results stated in Theorem 6.6.1.
Lemma 6.6.2 (Matrix Bernstein: Mgf and Cgf Bound). Suppose that X is a random Hermitian
matrix that satisﬁes
λmax(X ) ≤L.
Then, for 0 < θ < 3/L,
1−θL/3 ·EX 2
log EeθX ≼
1−θL/3 ·EX 2.
Proof. Fix the parameter θ > 0. In the exponential eθX , we would like to expose the random
matrix X and its square X 2 so that we can exploit information about the mean and variance. To
that end, we write
eθX = I+θX +(eθX −θX −I) = I+θX + X · f (X )· X ,
where f is a function on the real line:
f (x) = eθx −θx −1
for x ̸= 0
f (0) = θ2
The function f is increasing because its derivative is positive. Therefore, f (x) ≤f (L) when x ≤L.
By assumption, the eigenvalues of X do not exceed L, so the Transfer Rule (2.1.14) implies that
f (X ) ≼f (L)·I.
CHAPTER 6. A SUM OF BOUNDED RANDOM MATRICES
The Conjugation Rule (2.1.12) allows us to introduce the relation (6.6.6) into our expansion (6.6.5)
of the matrix exponential:
eθX ≼I+θX + X (f (L)·I)X = I+θX + f (L)· X 2.
This relation is the basis for our matrix mgf bound.
To obtain the desired result, we develop a further estimate for f (L). This argument involves
a clever application of Taylor series:
f (L) = eθL −θL −1
The second expression is simply the Taylor expansion of the fraction, viewed as a function of
θ. We obtain the inequality by factoring out (θL)2/2 from each term in the series and invoking
the bound q! ≥2·3q−2, valid for each q = 2,3,4,.... Sum the geometric series to obtain the ﬁnal
To complete the proof of the mgf bound, we combine the last two displays:
eθX ≼I+θX +
1−θL/3 · X 2.
This estimate is valid because X 2 is positive semideﬁnite. Expectation preserves the semideﬁnite
1−θL/3 ·EX 2 ≼exp
1−θL/3 ·EX 2
We have used the assumption that X has zero mean. The second semideﬁnite relation follows
when we apply the Transfer Rule (2.1.14) to the inequality 1+ a ≤ea, which holds for a ∈R.
To obtain the semideﬁnite bound for the cgf, we extract the logarithm of the mgf bound using
the fact (2.1.18) that the logarithm is operator monotone.
Proof of the Hermitian Case
We are prepared to establish the matrix Bernstein inequalities for random Hermitian matrices.
Proof of Theorem 6.6.1. Consider a ﬁnite sequence {Xk} of random Hermitian matrices with dimension d. Assume that
λmax(Xk) ≤L
for each index k.
The matrix Bernstein cgf bound, Lemma 6.6.2, provides that
log EeθXk ≼g(θ)·EX 2
for 0 < θ < 3/L.
Introduce the sum Y = P
We begin with the bound (6.6.2) for the expectation Eλmax(Y ). Invoke the master inequality,
relation (3.6.1) in Theorem 3.6.1, to ﬁnd that
Eλmax(Y ) ≤inf
θ log trexp
k log EeθXk
θ log trexp
θ log trexp
g(θ)·EY 2¢
6.6. PROOF OF THE MATRIX BERNSTEIN INEQUALITY
As usual, to move from the ﬁrst to the second line, we invoke the fact (2.1.16) that the trace
exponential is monotone to introduce the semideﬁnite bound (6.6.7) for the cgf. Then we use
the additivity rule (2.2.5) for the variance of an independent sum to identify EY 2. The rest of the
argument glides along a well-oiled track:
Eλmax(Y ) ≤
g(θ)·EY 2¢¢¤
g(θ)· v(Y )
1−θL/3 · v(Y )
In the ﬁrst inequality, we bound the trace of the exponential by the dimension d times the maximum eigenvalue. The next line follows from the Spectral Mapping Theorem, Proposition 2.1.3.
In the third line, we identify the matrix variance statistic v(Y ) from (6.6.1). Afterward, we extract
the logarithm and simplify. Finally, we compute the inﬁmum to complete the proof of (6.6.2).
For reference, the optimal argument is
6L logd +9
2v(Y )logd
2L2t +9(Y )+6L
We recommend using a computer algebra system to conﬁrm this point.
Next, we develop the tail bound (6.6.3) for λmax(Y ). Owing to the master tail inequality (3.6.3),
P{λmax(Y ) ≥t} ≤inf
θ>0 e−θt trexp
k log EeθXk
0<θ<3/L e−θt trexp
0<θ<3/L d e−θt exp
g(θ)· v(Y )
The justiﬁcations are the same as before. The exact value of the inﬁmum is messy, so we proceed
with the inspired choice θ = t/(v(Y )+Lt/3), which results in the elegant bound (6.6.3).
Proof of the General Case
Finally, we explain how to derive Theorem 6.1.1, for general matrices, from Theorem 6.6.1. This
result follows immediately when we apply the matrix Bernstein bounds for Hermitian matrices
to the Hermitian dilation of a sum of general matrices.
Proof of Theorem 6.1.1. Consider a ﬁnite sequence {Sk} of d1 ×d2 random matrices, and assume
for each index k.
We deﬁne the two random matrices
Y = H (Z ) =
CHAPTER 6. A SUM OF BOUNDED RANDOM MATRICES
where H is the Hermitian dilation (2.1.26). The second expression for Y follows from the property that the dilation is a real-linear map.
We will apply Theorem 6.6.1 to analyze ∥Z ∥. First, recall the fact (2.1.28) that
∥Z ∥= λmax(H (Z )) = λmax(Y ).
Next, we express the variance (6.6.1) of the random Hermitian matrix Y in terms of the general
matrix Z . Indeed, the calculation (2.2.10) of the variance statistic of a dilation shows that
v(Y ) = v(H (Y )) = v(Z ).
Recall that the matrix variance statistic v(Z ) deﬁned in (6.1.1) coincides with the general deﬁnition from (2.2.8). Finally, we invoke Theorem 6.6.1 to establish Theorem 6.1.1.
The literature contains a wide variety of Bernstein-type inequalities in the scalar case, and the
matrix case is no different. The applications of the matrix Bernstein inequality are also numerous. We only give a brief summary here.
Matrix Bernstein Inequalities
David Gross [Gro11] and Ben Recht [Rec11] used the approach of Ahlswede & Winter [AW02] to
develop two different versions of the matrix Bernstein inequality. These papers helped to popularize the use matrix concentration inequalities in mathematical signal processing and statistics.
Nevertheless, their results involve a suboptimal variance parameter of the form
This parameter can be signiﬁcantly larger than the matrix variance statistic (6.6.1) that appears
in Theorem 6.6.1. They do coincide in some special cases, such as when the summands are
independent and identically distributed.
Oliveira [Oli10a] established the ﬁrst version of the matrix Bernstein inequality that yields the
correct matrix variance statistic (6.6.1). He accomplished this task with an elegant application
of the Golden–Thompson inequality (3.3.3). His method even gives a result, called the matrix
Freedman inequality, that holds for matrix-valued martingales. His bound is roughly equivalent
with Theorem 6.6.1, up to the precise value of the constants.
The matrix Bernstein inequality we have stated here, Theorem 6.6.1, ﬁrst appeared in the
paper [Tro11c, §6] by the author of these notes. The bounds for the expectation are new. The
argument is based on Lieb’s Theorem, and it also delivers a matrix Bennett inequality. This paper
also describes how to establish matrix Bernstein inequalities for sums of unbounded random
matrices, given some control over the matrix moments.
The research in [Tro11c] is independent from Oliveira’s work [Oli10a], although Oliveira’s paper motivated the subsequent article [Tro11a] and the technical report [Tro11b], which explain
how to use Lieb’s Theorem to study matrix martingales. The technical report [GT14] develops a
Bernstein inequality for interior eigenvalues using the Lieb–Seiringer Theorem [LS05].
For more versions of the matrix Bernstein inequality, see Vladimir Koltchinskii’s lecture notes
from Saint-Flour [Kol11]. In Chapter 7, we present another extension of the matrix Bernstein
inequality that involves a smaller dimensional parameter.
6.7. NOTES
The Matrix Rosenthal–Pinelis Inequality
The matrix Rosenthal–Pinelis inequality (6.1.6) is a close cousin of the matrix Rosenthal inequality (5.1.9). Both results are derived from the noncommutative Khintchine inequality (4.7.1) using the same pattern of argument [CGT12a, Thm. A.1]. We believe that [CGT12a] is the ﬁrst
paper to recognize and state the result (6.1.6), even though it is similar in spirit with the work
in [Rud99]. A self-contained, elementary proof of a related matrix Rosenthal-Pinelis inequality
appears in [MJC+14, Cor. 7.4].
Versions of the matrix Rosenthal–Pinelis inequality ﬁrst appeared in the literature [JX03] on
noncommutative martingales, where they were called noncommutative Burkholder inequalities.
For an application to random matrices, see the follow-up work [JX08] by the same authors. Subsequent papers [JZ12, JZ13] contain related noncommutative martingale inequalities inspired by
the research in [Oli10b, Tro11c].
Empirical Approximation
Matrix approximation by random sampling is a special case of a general method that Bernard
Maurey developed to compute entropy numbers of convex hulls. Let us give a short presentation
of the original context, along with references to some other applications.
Empirical Bounds for Covering Numbers
Suppose that X is a Banach space. Consider the convex hull E = conv{e1,...,eN} of a set of N
points in X , and assume that ∥ek∥≤L. We would like to give an upper bound for the number of
balls of radius ε it takes to cover this set.
Fix a point u ∈E, and express u as a convex combination:
Let x be the random vector in X that takes value ek with probability pk. We can approximate the
point u as an average ¯x = n−1 Pn
k=1 xk of independent copies x1,...,xn of the random vector x.
E ∥¯xn −u∥X = 1
k=1(xk −Ex)
The family {ϱk} consists of independent Rademacher random variables. The ﬁrst inequality depends on the symmetrization procedure [LT91, Lem. 6.3], and the second is Hölder’s. In certain
Banach spaces, a Khintchine-type inequality holds:
E ∥¯xn −u∥X ≤2T2(X )
k=1 E ∥xk∥2
¢1/2 ≤2T2(X )L
The last inequality depends on the uniform bound ∥ek∥≤L. This estimate controls the expected
error in approximating an arbitrary point in E by randomized sampling.
The number T2(X ) is called the type two constant of the Banach space X , and it can be estimated in many concrete instances; see [LT91, Chap. 9] or [Pis89, Chap. 11]. For our purposes,
CHAPTER 6. A SUM OF BOUNDED RANDOM MATRICES
the most relevant example is the Banach space Md1×d2 consisting of d1 ×d2 matrices equipped
with the spectral norm. Its type two constant satisﬁes
log(d1 +d2).
This result follows from work of Tomczak–Jaegermann [TJ74, Thm. 3.1(ii)]. In fact, the space
Md1×d2 enjoys an even stronger property with respect to averages, namely the noncommutative
Khintchine inequality (4.7.1).
Now, suppose that the number n of samples in our empirical approximation ¯xn of the point
u ∈E satisﬁes
Then the probabilistic method ensures that there is a some collection of u1,...,un of points
drawn with repetition from the set {e1,...,eN} that satisﬁes
There are at most N n different ways to select the points uk. It follows that we can cover the
convex hull E = conv{e1,...,eN} in X with at most N n norm balls of radius ε.
History and Applications of Empirical Approximation
Maurey did not publish his ideas, and the method was ﬁrst broadcast in a paper of Pisier [Pis81,
Lem. 1]. Another early reference is the work of Carl [Car85, Lem. 1]. More recently, this covering
argument has been used to study the restricted isomorphism behavior of a random set of rows
drawn from a discrete Fourier transform matrix [RV06].
By now, empirical approximation has appeared in a wide range of applied contexts, although
many papers do not recognize the provenance of the method. Let us mention some examples
in machine learning. Empirical approximation has been used to study what functions can be
approximated by neural networks [Bar93, LBW96]. The same idea appears in papers on sparse
modeling, such as [SSS08], and it supports the method of random features [RR07]. Empirical
approximation also stands at the core of a recent algorithm for constructing approximate Nash
equilibria [Bar14].
It is difﬁcult to identify the earliest work in computational mathematics that invoked the
empirical method to approximate matrices. The paper of Achlioptas & McSherry [AM01] on randomized sparsiﬁcation is one possible candidate.
Corollary 6.2.1, which we use to perform the analysis of matrix approximation by sampling,
does not require the full power of the matrix Bernstein inequality, Theorem 6.1.1. Indeed, Corollary 6.2.1 can be derived from the weaker methods of Ahlswede & Winter [AW02]; for example,
see the papers [Gro11, Rec11].
Randomized Sparsiﬁcation
The idea of using randomized sparsiﬁcation to accelerate spectral computations appears in a
paper of Achlioptas & McSherry [AM01, AM07]. d’Asprémont [d’A11] proposed to use sparsiﬁcation to accelerate algorithms for semideﬁnite programming. The paper [AKL13] by Achlioptas,
Karnin, & Liberty recommends sparsiﬁcation as a mechanism for data compression.
6.7. NOTES
After the initial paper [AM01], several other researchers developed sampling schemes for randomized sparsiﬁcation [AHK06, GT09]. Later, Drineas & Zouzias [DZ11] pointed out that matrix
concentration inequalities can be used to analyze this type of algorithm. The paper [AKL13] re-
ﬁned this analysis to obtain sharper bounds. The simple analysis here is drawn from a recent
note by Kundu & Drineas [KD14].
Randomized Matrix Multiplication
The idea of using random sampling to accelerate matrix multiplication appeared in nascent form
in a paper of Frieze, Kannan, & Vempala [FKV98]. The paper [DK01] of Drineas & Kannan develops this idea in full generality, and the article [DKM06] of Drineas, Kannan, & Mahoney contains
a more detailed treatment. Subsequently, Tamás Sarlós obtained a signiﬁcant improvement in
the performance of this algorithm [Sar06]. Rudelson & Vershynin [RV07] obtained the ﬁrst error
bound for approximate matrix multiplication with respect to the spectral norm. The analysis
that we presented is adapted from the dissertation [Zou13] of Tassos Zouzias, which reﬁnes an
earlier treatment by Magen & Zouzias [MZ11]. See the monographs of Mahoney [Mah11] and
Woodruff [Woo14] for a more extensive discussion.
Random Features
Our discussion of kernel methods is adapted from the book [SS98]. The papers [RR07, RR08] of
Ali Rahimi and Ben Recht proposed the idea of using random features to summarize data for
large-scale kernel machines. The construction (6.5.6) of a random feature map for a translationinvariant, positive-deﬁnite kernel appears in their work. This approach has received a signiﬁcant
amount of attention over the last few years, and there has been a lot of subsequent development.
For example, the paper [KK12] of Kar & Karnick shows how to construct random features for
inner-product kernels, and the paper [HXGD14] of Hamid et al. develops random features for
polynomial kernels. Our analysis of random features using the matrix Bernstein inequality is
drawn from the recent article [LPSS+14] of Lopez-Paz et al. The presentation here is adapted
from the author’s tutorial on randomized matrix approximation, given at ICML 2014 in Beijing.
We recommend the two papers [HXGD14, LPSS+14] for an up-to-date bibliography.
Results Involving
the Intrinsic Dimension
A minor shortcoming of our matrix concentration results is the dependence on the ambient
dimension of the matrix. In this chapter, we show how to obtain a dependence on an intrinsic dimension parameter, which occasionally is much smaller than the ambient dimension. In
many cases, intrinsic dimension bounds offer only a modest improvement. Nevertheless, there
are examples where the beneﬁts are signiﬁcant enough that we can obtain nontrivial results for
inﬁnite-dimensional random matrices.
In this chapter, present a version of the matrix Chernoff inequality that involves an intrinsic dimension parameter. We also describe a version of the matrix Bernstein inequality that
involves an intrinsic dimension parameter. The intrinsic Bernstein result usually improves on
Theorem 6.1.1. These results depend on a new argument that distills ideas from a paper [Min11]
of Stanislav Minsker. We omit intrinsic dimension bounds for matrix series, which the reader
may wish to develop as an exercise.
To give a sense of what these new results accomplish, we revisit some of the examples from
earlier chapters. We apply the intrinsic Chernoff bound to study a random column submatrix of
a ﬁxed matrix. We also reconsider the randomized matrix multiplication algorithm in light of the
intrinsic Bernstein bound. In each case, the intrinsic dimension parameters have an attractive
interpretation in terms of the problem data.
We begin our development in §7.1 with the deﬁnition of the intrinsic dimension of a matrix. In
§7.2, we present the intrinsic Chernoff bound and some of its consequences. In §7.3, we describe the intrinsic Bernstein inequality and its applications. Afterward, we describe the new ingredients that are required in the proofs. Section 7.4 explains how to extend the matrix Laplace
transform method beyond the exponential function, and §7.5 describes a simple but powerful
lemma that allows us to obtain the dependence on the intrinsic dimension. Section 7.6 contains
CHAPTER 7. RESULTS INVOLVING THE INTRINSIC DIMENSION
the proof of the intrinsic Chernoff bound, and §7.7 develops the proof of the intrinsic Bernstein
The Intrinsic Dimension of a Matrix
Some types of random matrices are concentrated in a small number of dimensions, while they
have little content in other dimensions. So far, our bounds do not account for the difference. We
need to introduce a more reﬁned notion of dimension that will help us to discriminate among
these examples.
Deﬁnition 7.1.1 (Intrinsic Dimension). For a positive-semideﬁnite matrix A, the intrinsic dimension is the quantity
intdim(A) = tr A
We interpret the intrinsic dimension as a measure of the number of dimensions where A has
signiﬁcant spectral content.
Let us make a few observations that support this view. By expressing the trace and the norm
in terms of the eigenvalues, we can verify that
1 ≤intdim(A) ≤rank(A) ≤dim(A).
The ﬁrst inequality is attained precisely when A has rank one, while the second inequality is attained precisely when A is a multiple of the identity. The intrinsic dimension is 0-homogeneous,
so it is insensitive to changes in the scale of the matrix A. The intrinsic dimension is not monotone with respect to the semideﬁnite order. Indeed, we can drive the intrinsic dimension to one
by increasing one eigenvalue of A substantially.
Matrix Chernoff with Intrinsic Dimension
Let us present an extension of the matrix Chernoff inequality. This result controls the maximum
eigenvalue of a sum of random, positive-semideﬁnite matrices in terms of the intrinsic dimension of the expectation of the sum.
Theorem 7.2.1 (Matrix Chernoff: Intrinsic Dimension). Consider a ﬁnite sequence {Xk} of random, Hermitian matrices of the same size, and assume that
0 ≤λmin(Xk)
λmax(Xk) ≤L
for each index k.
Introduce the random matrix
Suppose that we have a semideﬁnite upper bound M for the expectation EY :
Deﬁne an intrinsic dimension bound and a mean bound:
d = intdim(M)
µmax = λmax(M).
7.2. MATRIX CHERNOFF WITH INTRINSIC DIMENSION
Then, for θ > 0,
Eλmax(Y ) ≤eθ −1
θ ·L log(2d).
Furthermore,
λmax(Y ) ≥(1+ε)µmax
for ε ≥L/µmax.
The proof of this result appears below in §7.6.
Discussion
Theorem 7.2.1 is almost identical with the parts of the basic matrix Chernoff inequality that concern the maximum eigenvalue λmax(Y ). Let us call attention to the differences. The key advantage is that the current result depends on the intrinsic dimension of the matrix M instead of the
ambient dimension. When the eigenvalues of M decay, the improvement can be dramatic. We
do suffer a small cost in the extra factor of two, and the tail bound is restricted to a smaller range
of the parameter ε. Neither of these limitations is particularly signiﬁcant.
We have chosen to frame the result in terms of the upper bound M because it can be challenging to calculate the mean EY exactly. The statement here allows us to draw conclusions
directly from the upper bound M. These estimates do not follow formally from a result stated for
EY because the intrinsic dimension is not monotone with respect to the semideﬁnite order.
A shortcoming of Theorem 7.2.1 is that it does not provide any information about λmin(Y ).
Curiously, the approach we use to prove the result just does not work for the minimum eigenvalue.
Example: A Random Column Submatrix
To demonstrate the value of Theorem 7.2.1, let us return to one of the problems we studied in
§5.2. We can now develop a reﬁned estimate for the expected norm of a random column submatrix drawn from a ﬁxed matrix.
In this example, we consider a ﬁxed m×n matrix B, and we let {δk} be an independent family
of BERNOULLI(p/n) random variables. We form the random submatrix
k δk b:ke∗
where b:k is the kth column of B. This random submatrix contains an average of p nonzero
columns from B. To study the norm of Z , we consider the positive-semideﬁnite random matrix
Y = Z Z ∗=
This time, we invoke Theorem 7.2.1 to obtain a new estimate for the maximum eigenvalue of Y .
We need a semideﬁnite bound M for the mean EY of the random matrix. In this case, the
exact value is available:
M = EY = p
CHAPTER 7. RESULTS INVOLVING THE INTRINSIC DIMENSION
We can easily calculate the intrinsic dimension of this matrix:
d = intdim(M) = intdim
= intdim(BB∗) = tr(BB∗)
∥B∥2 = srank(B).
The second identity holds because the intrinsic dimension is scale invariant. The last relation is
simply the deﬁnition (2.1.25) of the stable rank. The maximum eigenvalue of M veriﬁes
λmax(M) = p
n λmax(BB∗) = p
The maximum norm L of any term in the sum Y satisﬁes L = maxk ∥b:k∥2.
We may now apply the intrinsic Chernoff inequality. The expectation bound (7.2.1) with θ = 1
E ∥Z ∥2 = Eλmax(Y ) ≤1.72· p
n ·∥B∥2 +log(2srank(B))·maxk ∥b:k∥2 .
In the earlier analysis, we obtained a similar bound (5.2.1). The new result depends on the logarithm of the stable rank instead of logm, the logarithm of the number of rows of B. When the
stable rank of B is small—meaning that many rows are almost collinear—then the revised estimate can result in a substantial improvement.
Matrix Bernstein with Intrinsic Dimension
Next, we present an extension of the matrix Bernstein inequality. These results provide tail
bounds for an independent sum of bounded random matrices that depend on the intrinsic dimension of the variance. This theorem is essentially due to Stanislav Minsker.
Theorem 7.3.1 (Intrinsic Matrix Bernstein). Consider a ﬁnite sequence {Sk} of random complex
matrices with the same size, and assume that
Introduce the random matrix
Let V1 and V2 be semideﬁnite upper bounds for the matrix-valued variances Var1(Z ) and Var2(Z ):
V1 ≽Var1(Z ) = E(Z Z ∗) =
V2 ≽Var2(Z ) = E(Z ∗Z ) =
Deﬁne an intrinsic dimension bound and a variance bound
d = intdim
∥V1∥, ∥V2∥
Then, for t ≥pv +L/3,
P{∥Z ∥≥t} ≤4d exp
The proof of this result appears below in §7.7.
7.3. MATRIX BERNSTEIN WITH INTRINSIC DIMENSION
Discussion
Theorem 7.3.1 is quite similar to Theorem 6.1.1, so we focus on the differences. Although the
statement of Theorem 7.3.1 may seem circumspect, it is important to present the result in terms
of upper bounds V1 and V2 for the matrix-valued variances. Indeed, it can be challenging to calculate the matrix-valued variances exactly. The fact that the intrinsic dimension is not monotone
interferes with our ability to use a simpler result.
Note that the tail bound (7.3.2) now depends on the intrinsic dimension of the block-diagonal
matrix diag(V1,V2). This intrinsic dimension quantity never exceeds the total of the two side
lengths of the random matrix Z . As a consequence, the new tail bound always has a better dimensional dependence than the earlier result. The costs of this improvement are small: We pay
an extra factor of four in the probability bound, and we must restrict our attention to a more
limited range of the parameter t. Neither of these changes is signiﬁcant.
The result does not contain an explicit estimate for E∥Z ∥, but we can obtain such a bound by
integrating the tail inequality (7.3.2). This estimate is similar with the earlier bound (6.1.3), but
it depends on the intrinsic dimension instead of the ambient dimension.
Corollary 7.3.2 (Intrinsic Matrix Bernstein: Expectation Bound). Instate the notation and hypotheses of Theorem 7.3.1. Then
E∥Z ∥≤Const·
v log(1+d)+L log(1+d)
See §7.7.4 for the proof.
Next, let us have a closer look at the intrinsic dimension quantity deﬁned in (7.3.1).
trV1 +trV2
∥V1∥, ∥V2∥
We can make a further bound on the denominator to obtain an estimate in terms of the intrinsic
dimensions of the two blocks:
intdim(V1), intdim(V2)
intdim(V1)+intdim(V2).
This bound reﬂects a curious phenomenon: the intrinsic dimension parameter d is not necessarily comparable with the larger of intdim(V1) or intdim(V2).
The other commentary about the original matrix Bernstein inequality, Theorem 6.1.1, also
applies to the intrinsic dimension result. For example, we can adapt the result to a sum of uncentered, independent, random, bounded matrices. In addition, the theorem becomes somewhat
simpler for a Hermitian random matrix because there is only one matrix-valued variance to deal
with. The modiﬁcations required in these cases are straightforward.
Example: Matrix Approximation by Random Sampling
We can apply the intrinsic Bernstein inequality to study the behavior of randomized methods for
matrix approximation. The following result is an immediate consequence of Theorem 7.3.1 and
Corollary 7.3.2.
Corollary 7.3.3 (Matrix Approximation by Random Sampling: Intrinsic Dimension Bounds). Let
B be a ﬁxed d1 ×d2 matrix. Construct a d1 ×d2 random matrix R that satisﬁes
CHAPTER 7. RESULTS INVOLVING THE INTRINSIC DIMENSION
Let M1 and M2 be semideﬁnite upper bounds for the expected squares:
M1 ≽E(RR∗)
M2 ≽E(R∗R).
Deﬁne the quantities
d = intdim
∥M1∥, ∥M2∥
Form the matrix sampling estimator
where each Rk is an independent copy of R.
Then the estimator satisﬁes
E∥¯Rn −B∥≤Const·
m log(1+d)
+ L log(1+d)
Furthermore, for all t ≥pm +L/3,
P{∥¯Rn −B∥≥t} ≤4d exp
The proof is similar with that of Corollary 6.2.1, so we omit the details.
Application: Randomized Matrix Multiplication
We will apply Corollary 7.3.3 to study the randomized matrix multiplication algorithm from §6.4.
This method results in a small, but very appealing, improvement in the number of samples that
are required. This argument is essentially due to Tassos Zouzias [Zou13].
Our goal is to approximate the product of a d1 × N matrix B and an N × d2 matrix C. We
assume that both matrices B and C have unit spectral norm. The results are stated in terms of
the average stable rank
2(srank(B)+srank(C)).
The stable rank was introduced in (2.1.25). To approximate the product BC, we constructed a
simple random matrix R whose mean ER = BC, and then we formed the estimator
where each Rk is an independent copy of R.
The challenge is to bound the error ∥¯Rn −BC∥.
To do so, let us refer back to our calculations from §6.4. We ﬁnd that
E(RR∗) ≼2·asr·BB∗,
E(R∗R) ≼2·asr·C ∗C.
7.4. REVISITING THE MATRIX LAPLACE TRANSFORM BOUND
Starting from this point, we can quickly improve on our earlier analysis by incorporating the
intrinsic dimension bounds.
It is natural to set M1 = 2·asr ·BB∗and M2 = 2·asr ·C ∗C. We may now bound the intrinsic
dimension parameter
d = intdim
≤intdim(M1)+intdim(M2)
∥BB∗∥+ tr(C ∗C)
= srank(B)+srank(C) = 2·asr.
The ﬁrst inequality follows from (7.3.4), and the second is Deﬁnition 7.1.1, of the intrinsic dimension. The third relation depends on the norm identities (2.1.8) and (2.1.24). Finally, we identify
the stable ranks of B and C and the average stable rank. The calculation of the quantity m proceeds from the same considerations as in §6.4. Thus,
∥M1∥, ∥M2∥
This is all the information we need to collect.
Corollary 7.3.3 now implies that
E∥¯Rn −BC∥≤Const·
asr·log(1+asr)
+ asr·log(1+asr)
In other words, if the number n of samples satisﬁes
n ≥ε−2 ·asr·log(1+asr),
then the error satisﬁes
E ∥¯Rn −BC∥≤Const·
In the original analysis from §6.4, our estimate for the number n of samples contained the term
log(d1 + d2) instead of log(1 + asr). We have replaced the dependence on the ambient dimension of the product BC by a measure of the stable rank of the two factors. When the average
stable rank is small in comparison with the dimension of the product, the analysis based on the
intrinsic dimension offers an improvement in the bound on the number of samples required to
approximate the product.
Revisiting the Matrix Laplace Transform Bound
Let us proceed with the proofs of the matrix concentration inequalities based on intrinsic dimension. The challenge is to identify and remedy the weak points in the arguments from Chapter 3.
After some reﬂection, we can trace the dependence on the ambient dimension in our earlier results to the proof of Proposition 3.2.1. In the original argument, we used an exponential
function to transform the tail event before applying Markov’s inequality. This approach leads
to trouble for the simple reason that the exponential function does not pass through the origin,
which gives undue weight to eigenvalues that are close to zero.
CHAPTER 7. RESULTS INVOLVING THE INTRINSIC DIMENSION
We can resolve this problem by using other types of maps to transform the tail event. The
functions we have in mind are adjusted versions of the exponential. In particular, for ﬁxed θ > 0,
we can consider
ψ1(t) = max
ψ2(t) = eθt −θt −1.
Both functions are nonnegative and convex, and they are nondecreasing on the positive real line.
In each case, ψi(0) = 0. At the same time, the presence of the exponential function allows us to
exploit our bounds for the trace mgf.
Proposition 7.4.1 (Generalized Matrix Laplace Transform Bound). Let Y be a random Hermitian
matrix. Let ψ : R →R+ be a nonnegative function that is nondecreasing on [0,∞). For each t ≥0,
P{λmax(Y ) ≥t} ≤
ψ(t) Etrψ(Y ).
Proof. The proof follows the same lines as the proof of Proposition 3.2.1, but it requires some
additional ﬁnesse. Since ψ is nondecreasing on [0,∞), the bound a ≥t implies that ψ(a) ≥ψ(t).
As a consequence,
λmax(Y ) ≥t
λmax(ψ(Y )) ≥ψ(t).
Indeed, on the tail event λmax(Y ) ≥t, we must have ψ(λmax(Y )) ≥ψ(t). The Spectral Mapping
Theorem, Proposition 2.1.3, indicates that the number ψ(λmax(Y )) is one of the eigenvalues of
the matrix ψ(Y ), so we determine that λmax(ψ(Y )) also exceeds ψ(t).
Returning to the tail probability, we discover that
P{λmax(Y ) ≥t} ≤P
λmax(ψ(Y )) ≥ψ(t)
ψ(t) Eλmax(ψ(Y )).
The second bound is Markov’s inequality (2.2.1), which is valid because ψ is nonnegative. Finally,
P{λmax(Y ) ≥t} ≤
ψ(t) Etrψ(Y ).
The inequality holds because of the fact (2.1.13) that the trace of ψ(Y ), a positive-semideﬁnite
matrix, must be at least as large as its maximum eigenvalue.
The Intrinsic Dimension Lemma
The other new ingredient is a simple observation that allows us to control a trace function applied to a positive-semideﬁnite matrix in terms of the intrinsic dimension of the matrix.
Lemma 7.5.1 (Intrinsic Dimension). Let ϕ be a convex function on the interval [0,∞), and assume
that ϕ(0) = 0. For any positive-semideﬁnite matrix A, it holds that
trϕ(A) ≤intdim(A)·ϕ(∥A∥).
Proof. Since the function a 7→ϕ(a) is convex on the interval [0,L], it is bounded above by the
chord connecting the graph at the endpoints. That is, for a ∈[0,L],
L ·ϕ(L) = a
7.6. PROOF OF THE INTRINSIC CHERNOFF BOUND
The eigenvalues of A fall in the interval [0,L], where L = ∥A∥. As an immediate consequence of
the Transfer Rule (2.1.14), we ﬁnd that
trϕ(A) ≤tr A
∥A∥·ϕ(∥A∥).
Identify the intrinsic dimension of A to complete the argument.
Proof of the Intrinsic Chernoff Bound
With these results at hand, we are prepared to prove our ﬁrst intrinsic dimension result, which
extends the matrix Chernoff inequality.
Proof of Theorem 7.2.1. Consider a ﬁnite sequence {Xk} of independent, random Hermitian matrices with
0 ≤λmin(Xk)
λmax(Xk) ≤L
for each index k.
Introduce the sum
The challenge is to establish bounds for λmax(Y ) that depend on the intrinsic dimension of a
matrix M that satisﬁes M ≽EY . We begin the argument with the proof of the tail bound (7.2.2).
Afterward, we show how to extract the expectation bound (7.2.1).
Fix a number θ > 0, and deﬁne the function ψ(t) = max
for t ∈R. For t ≥0, the
general version of the matrix Laplace transform bound, Proposition 7.4.1, states that
P{λmax(Y ) ≥t} ≤
ψ(t) Etrψ(Y ) =
We have exploited the fact that Y is positive semideﬁnite and the assumption that t ≥0. The
presence of the identity matrix on the right-hand side allows us to draw stronger conclusions
than we could before.
Let us study the expected trace term on the right-hand side of (7.6.1). As in the proof of the
original matrix Chernoff bound, Theorem 5.1.1, we have the estimate
EtreθY ≤trexp
g(θ) = eθL −1
Introduce the function ϕ(a) = ea −1, and observe that
eg(θ)·EY −I
eg(θ)·M −I
= trϕ(g(θ)· M).
The second inequality follows from the assumption that EY ≼M and the monotonicity (2.1.16)
of the trace exponential. Now, apply the intrinsic dimension bound, Lemma 7.5.1, to reach
≤intdim(M)·ϕ
We have used the fact that the intrinsic dimension does not depend on the scaling factor g(θ).
Recalling the notation d = intdim(M) and µmax = ∥M∥, we continue the calculation:
≤d ·eg(θ)·µmax.
CHAPTER 7. RESULTS INVOLVING THE INTRINSIC DIMENSION
We have invoked the trivial inequality ϕ(a) ≤ea, which holds for a ∈R.
Next, introduce the bound (7.6.2) on the expected trace into the probability bound (7.6.1) to
P{λmax(Y ) ≥t} ≤d ·
·e−θt+g(θ)·µmax ≤d ·
·e−θt+g(θ)·µmax.
To control the fraction, we have observed that
ea −1 = 1+
ea −1 ≤1+ 1
We obtain the latter inequality by replacing the convex function a 7→ea −1 with its tangent line
In the estimate (7.6.3), we make the change of variables t 7→(1+ε)µmax. The bound is valid
for all θ > 0, so we can select θ = L−1 log(1 + ε) to minimize the exponential. Altogether, these
steps lead to the estimate
λmax(Y ) ≥(1+ε)µmax
(1+ε)log(1+ε)
Now, instate the assumption that ε ≥L/µmax. The function a 7→(1+ a)log(1+ a) is convex when
a ≥−1, so we can bound it below using its tangent at ε = 0. Thus,
(1+ε)log(1+ε) ≥ε ≥
It follows that the parenthesis in (7.6.4) is bounded by two, which yields the conclusion (7.2.2).
Now, we turn to the expectation bound (7.2.1). Observe that the functional inverse of ψ is the
increasing concave function
ψ−1(u) = 1
θ log(1+u)
Since Y is a positive-semideﬁnite matrix, we can calculate that
Eλmax(Y ) = Eψ−1(ψ(λmax(Y ))) ≤ψ−1(Eψ(λmax(Y )))
= ψ−1(Eλmax(ψ(Y ))) ≤ψ−1(Etrψ(Y )).
The second relation is Jensen’s inequality (2.2.2), which is valid because ψ−1 is concave. The third
relation follows from the Spectral Mapping Theorem, Proposition 2.1.3, because the function ψ
is increasing. We can bound the maximum eigenvalue by the trace because ψ(Y ) is positive
semideﬁnite and ψ−1 is an increasing function.
Now, substitute the bound (7.6.2) into the last display (7.6.5) to reach
Eλmax(Y ) ≤ψ−1(d ·exp(g(θ)·µmax)) = 1
1+d ·eg(θ)·µmax¢
2d ·eg(θ)·µmax¢
log(2d)+ g(θ)·µmax
The ﬁrst inequality again requires the property that ψ−1 is increasing. The second inequality
follows because 1 ≤d ·eg(θ)·µmax, which is a consequence of the fact that the intrinsic dimension
exceeds one and the exponent is nonnegative. To complete the argument, introduce the deﬁnition of g(θ), and make the change of variables θ 7→θ/L. These steps yield (7.2.1).
7.7. PROOF OF THE INTRINSIC BERNSTEIN BOUNDS
Proof of the Intrinsic Bernstein Bounds
In this section, we present the arguments that lead up to the intrinsic Bernstein bounds. That is,
we develop tail inequalities for an independent sum of bounded random matrices that depend
on the intrinsic dimension of the variance.
The Hermitian Case
As usual, Hermitian matrices provide the natural setting for matrix concentration. We begin with
an explicit statement and proof of a bound for the Hermitian case.
Theorem 7.7.1 (Matrix Bernstein: Hermitian Case with Intrinsic Dimension). Consider a ﬁnite
sequence {Xk} of random Hermitian matrices of the same size, and assume that
λmax(Xk) ≤L
for each index k.
Introduce the random matrix
Let V be a semideﬁnite upper bound for the matrix-valued variance Var(Y ):
V ≽Var(Y ) = EY 2 =
Deﬁne the intrinsic dimension bound and variance bound
d = intdim(V )
Then, for t ≥pv +L/3,
P{λmax(Y ) ≥t} ≤4d ·exp
The proof of this result appears in the next section.
Proof of the Hermitian Case
We commence with the results for an independent sum of random Hermitian matrices whose
eigenvalues are subject to an upper bound.
Proof of Theorem 7.7.1. Consider a ﬁnite sequence {Xk} of independent, random, Hermitian matrices with
λmax(Xk) ≤L
for each index k.
Introduce the random matrix
Our goal is to obtain a tail bound for λmax(Y ) that reﬂects the intrinsic dimension of a matrix V
that satisﬁes V ≽Var(Y ).
CHAPTER 7. RESULTS INVOLVING THE INTRINSIC DIMENSION
Fix a number θ > 0, and deﬁne the function ψ(t) = eθt −θt −1 for t ∈R. The general version
of the matrix Laplace transform bound, Proposition 7.4.1, implies that
P{λmax(Y ) ≥t} ≤
ψ(t) Etrψ(Y )
eθY −θY −I
eθt −θt −1
The last identity holds because the random matrix Y has zero mean.
Let us focus on the expected trace on the right-hand side of (7.7.2). Examining the proof of
the original matrix Bernstein bound for Hermitian matrices, Theorem 6.6.1, we see that
EtreθY ≤trexp
g(θ)·EY 2¢
g(θ) = exp
Introduce the function ϕ(a) = ea −1, and observe that
eg(θ)·EY 2 −I
eg(θ)·V −I
The second inequality depends on the assumption that EY 2 = Var(Y ) ≼V and the monotonicity
property (2.1.16) of the trace exponential. Apply the intrinsic dimension bound, Lemma 7.5.1, to
≤intdim(V )·ϕ
≤d ·eg(θ)·v.
We have used the fact that the intrinsic dimension is scale invariant. Then we identiﬁed d =
intdim(V ) and v = ∥V ∥. The last inequality depends on the trivial estimate ϕ(a) ≤ea, valid for all
Substitute the bound (7.7.3) into the probability inequality (7.7.2) to discover that
P{λmax(Y ) ≥t} ≤d ·
eθt −θt −1
·e−θt+g(θ)·v ≤d ·
·e−θt+g(θ)·v.
This estimate holds for any positive value of θ. To control the fraction, we have observed that
ea −a −1 = 1+
ea −a −1 ≤1+ 3
for all a ≥0.
The inequality above is a consequence of the numerical fact
for all a ∈R.
Indeed, the left-hand side of the latter expression deﬁnes a convex function of a, whose minimal
value, attained near a ≈1.30, is strictly positive.
In the tail bound (7.7.4), we select θ = t/(v +Lt/3) to reach
P{λmax(Y ) ≥t} ≤d ·
1+3· (v +Lt/3)2
7.7. PROOF OF THE INTRINSIC BERNSTEIN BOUNDS
This probability inequality is typically vacuous when t2 < v + Lt/3, so we may as well limit our
attention to the case where t2 ≥v + Lt/3. Under this assumption, the parenthesis is bounded
by four, which gives the tail bound (7.7.1). We can simplify the restriction on t by solving the
quadratic inequality to obtain the sufﬁcient condition
We develop an upper bound for the right-hand side of this inequality as follows.
We have used the numerical fact
b for all a,b ≥0. Therefore, the tail bound (7.7.1)
is valid when t ≥pv +L/3.
Proof of the Rectangular Case
Finally, we present the proof of the intrinsic Bernstein inequality, Theorem 7.3.1, for general
random matrices.
Proof of Theorem 7.3.1. Suppose that {Sk} is a ﬁnite sequence of independent random matrices
that satisfy
for each index k.
Form the sum Z = P
k Sk. As in the proof of Theorem 6.1.1, we derive the result by applying
Theorem 7.7.1 to the Hermitian dilation Y = H (Z ). The only new point that requires attention
is the modiﬁcation to the intrinsic dimension and variance terms.
Recall the calculation of the variance of the dilation from (2.2.9):
EY 2 = EH (Z )2 =
The semideﬁnite inequality follows from our assumptions on V1 and V2. Therefore, the intrinsic
dimension quantity in Theorem 7.7.1 induces the deﬁnition in the general case:
d = intdim(V ) = intdim
Similarly,
v = ∥V ∥= max
∥V1∥, ∥V2∥
This point completes the argument.
Proof of the Intrinsic Bernstein Expectation Bound
Finally, let us establish the expectation bound, Corollary 7.3.2, that accompanies Theorem 7.3.1.
CHAPTER 7. RESULTS INVOLVING THE INTRINSIC DIMENSION
Proof of Corollary 7.3.2. Fix a number µ ≥pv. We may rewrite the expectation of ∥Z ∥as an
P{∥Z ∥≥t} dt ≤µ+4d
e−t2/(2v) dt +4d
e−3t/(2L) dt
v e−µ2/(2v) + 8
3dL e−3µ/(2L).
To obtain the ﬁrst inequality, we split the integral at µ, and we bound the probability by one on
the domain of integration [0,µ]. The second inequality holds because
e−t2/(2v), e−3t/(2L)o
≤e−t2/(2v) +e−3t/(2L).
We controlled the Gaussian integral by inserting the factor pv (t/v) ≥1 into the integrand:
e−t2/(2v) dt ≤
(t/v)e−t2/(2v) dt =
v e−µ2/(2v).
To complete the argument, select µ =
2v log(1+d)+ 2
3L log(1+d) to reach
E ∥Z ∥≤µ+4d
v e−(2v log(1+d))/(2v) + 8
3dL e−3((2/3)L log(1+d))/(2L)
2v log(1+d)+ 2
3L log(1+d)+4
The stated bound (7.3.3) follows after we combine terms and agglomerate constants.
At present, there are two different ways to improve the dimensional factor that appears in matrix
concentration inequalities.
First, there is a sequence of matrix concentration results where the dimensional parameter
is bounded by the maximum rank of the random matrix. The ﬁrst bound of this type is due to
Rudelson [Rud99]. Oliveira’s results in [Oli10b] also exhibit this reduced dimensional dependence. A subsequent paper [MZ11] by Magen & Zouzias contains a related argument that gives
similar results. We do not discuss this class of bounds here.
The idea that the dimensional factor should depend on metric properties of the random matrix appears in a paper of Hsu, Kakade, & Zhang [HKZ12]. They obtain a bound that is similar
with Theorem 7.7.1. Unfortunately, their argument is complicated, and the results it delivers are
suboptimal.
Theorem 7.7.1 is essentially due to Stanislav Minsker [Min11]. His approach leads to somewhat sharper bounds than the approach in the paper of Hsu, Kakade, & Zhang, and his method
is easier to understand.
These notes contain another approach to intrinsic dimension bounds. The intrinsic Chernoff
bounds that emerge from our framework are new. The proof of the intrinsic Bernstein bound,
Theorem 7.7.1, can be interpreted as a distillation of Minsker’s argument. Indeed, many of the
speciﬁc calculations already appear in Minsker’s paper. We have obtained constants that are
marginally better.
A Proof of Lieb’s Theorem
Our approach to random matrices depends on some sophisticated ideas that are not usually presented in linear algebra courses. This chapter contains a complete derivation of the results that
undergird our matrix concentration inequalities. We begin with a short argument that explains
how Lieb’s Theorem follows from deep facts about a function called the matrix relative entropy.
The balance of the chapter is devoted to an analysis of the matrix relative entropy. Along the way,
we establish the core properties of the trace exponential function and the matrix logarithm. This
discussion may serve as an introduction to the advanced techniques of matrix analysis.
Lieb’s Theorem
In his 1973 paper on trace functions, Lieb established an important concavity theorem [Lie73,
Thm. 6] for the trace exponential function. As we saw in Chapter 3, this result animates all of our
matrix concentration inequalities.
Theorem 8.1.1 (Lieb). Let H be a ﬁxed Hermitian matrix with dimension d. The map
A 7−→trexp
is concave on the convex cone of d ×d positive-deﬁnite Hermitian matrices.
Section 8.1 contains an overview of the proof of Theorem 8.1.1. First, we state the background
material that we require, and then we show how the theorem follows. Some of the supporting
results are major theorems in their own right, and the details of their proofs will consume the
rest of the chapter.
Conventions
The symbol R++ refers to the set of positive real numbers. We remind the reader of our convention that bold capital letters that are symmetric about the vertical axis (A,H,M,T ,U,Ψ) always
refer to Hermitian matrices. We reserve the letter I for the identity matrix, while the letter Q
always refers to a unitary matrix. Other bold capital letters (B,K ,L) denote rectangular matrices.
CHAPTER 8. A PROOF OF LIEB’S THEOREM
Unless stated otherwise, the results in this chapter hold for all matrices whose dimensions are
compatible. For example, any result that involves a sum A + H includes the implicit constraint
that the two matrices are the same size.
Throughout this chapter, we assume that the parameter τ ∈ , and we use the shorthand
¯τ = 1−τ to make formulas involving convex combinations more legible.
Matrix Relative Entropy
The proof of Lieb’s Theorem depends on the properties of a bivariate function called the matrix
relative entropy.
Deﬁnition 8.1.2 (Matrix Relative Entropy). Let A and H be positive-deﬁnite matrices of the same
size. The entropy of A relative to H is
D(A;H) = tr
A (log A −logH)−(A −H)
The relative entropy can be viewed as a measure of the difference between the matrix A and the
matrix H, but it is not a metric. Related functions arise in quantum statistical mechanics and
quantum information theory.
We need two facts about the matrix relative entropy.
Proposition 8.1.3 (Matrix Relative Entropy is Nonnegative). For positive-deﬁnite matrices A and
H of the same size, the matrix relative entropy D(A;H) ≥0.
Proposition 8.1.3 is easy to prove; see Section 8.3.5 for the short argument.
Theorem 8.1.4 (The Matrix Relative Entropy is Convex). The map (A,H) 7→D(A;H) is convex.
That is, for positive-deﬁnite Ai and Hi of the same size,
τA1 + ¯τA2; τH1 + ¯τH2
for τ ∈ .
Theorem 8.1.4 is one of the crown jewels of matrix analysis. The supporting material for this
result occupies the bulk of this chapter; the argument culminates in Section 8.8.
Partial Maximization
We also require a basic fact from convex analysis which states that partial maximization of a
concave function produces a concave function. We include the simple proof.
Fact 8.1.5 (Partial Maximization). Let f be a concave function of two variables. Then the function
y 7→supx f (x; y) obtained by partial maximization is concave.
Proof. Fix ε > 0. For each pair of points y1 and y2, there are points x1 and x2 that satisfy
f (x1; y1) ≥supx f (x; y1)−ε
f (x2; y2) ≥supx f (x; y2)−ε.
For each τ ∈ , the concavity of f implies that
supx f (x; τy1 + ¯τy2) ≥f (τx1 + ¯τx2; τy1 + ¯τy2)
≥τ· f (x1; y1)+ ¯τ· f (x2; y2)
≥τ·supx f (x; y1)+ ¯τ·supx f (x; y2)−ε.
Take the limit as ε ↓0 to see that the partial supremum is a concave function of y.
8.2. ANALYSIS OF THE RELATIVE ENTROPY FOR VECTORS
A Proof of Lieb’s Theorem
Taking the results about the matrix relative entropy for granted, it is not hard to prove Lieb’s
Theorem. We begin with a variational representation of the trace, which restates the fact that
matrix relative entropy is nonnegative.
Lemma 8.1.6 (Variational Formula for Trace). Let M be a positive-deﬁnite matrix. Then
T logM −T logT +T
Proof. Proposition (8.1.3) states that D(T ;M) ≥0. Introduce the deﬁnition of the matrix relative
entropy, and rearrange to reach
T logM −T logT +T
When T = M, both sides are equal, which yields the advertised identity.
To establish Lieb’s Theorem, we use the variational formula to represent the trace exponential. Then we use the partial maximization result to condense the desired concavity property
from the convexity of the matrix relative entropy.
Proof of Theorem 8.1.1. In the variational formula, Lemma 8.1.6, select M = exp(H + log A) to
trexp(H +log A) = sup
T (H +log A)−T logT +T
The latter expression can be written compactly using the matrix relative entropy:
trexp(H +log A) = sup
tr(T H)+tr A −D(T ; A)
For each Hermitian matrix H, the bracket is a concave function of the pair (T , A) because of
Theorem 8.1.4. We see that the right-hand side of (8.1.2) is the partial maximum of a concave
function, and Fact 8.1.5 ensures that this expression deﬁnes a concave function of A. This observation establishes the theorem.
Analysis of the Relative Entropy for Vectors
Many deep theorems about matrices have analogies for vectors. This observation is valuable
because we can usually adapt an analysis from the vector setting to establish the parallel result
for matrices. In the matrix setting, however, it may be necessary to install a signiﬁcant amount of
extra machinery. If we keep the simpler structure of the vector argument in mind, we can avoid
being crushed in the gears.
The Relative Entropy for Vectors
The goal of §8.2 is to introduce the relative entropy function for positive vectors and to derive
some key properties of this function. Later we will analyze the matrix relative entropy by emulating these arguments.
CHAPTER 8. A PROOF OF LIEB’S THEOREM
Deﬁnition 8.2.1 (Relative Entropy). Let a and h be positive vectors of the same size. The entropy
of a relative to h is deﬁned as
ak (logak −loghk)−(ak −hk)
A variant of the relative entropy arises in information theory and statistics as a measure of the
discrepancy between two probability distributions on a ﬁnite set. We will show that the relative
entropy is nonnegative and convex.
It may seem abusive to recycle the notation for the relative entropy on matrices. To justify
this decision, we observe that
D(a;h) = D
diag(a); diag(h)
where diag(·) maps a vector to a diagonal matrix in the natural way. In other words, the vector
relative entropy is a special case of the matrix relative entropy. Ultimately, the vector case is
easier to understand because diagonal matrices commute.
Relative Entropy is Nonnegative
As we have noted, the relative entropy measures the difference between two positive vectors.
This interpretation is supported by the fact that the relative entropy is nonnegative.
Proposition 8.2.2 (Relative Entropy is Nonnegative). For positive vectors a and h of the same size,
the relative entropy D(a;h) ≥0.
Proof. Let f : R++ →R be a differentiable convex function on the positive real line. The function
f lies above its tangent lines, so
f (a) ≥f (h)+ f ′(h)·(a −h)
for positive a and h.
Instantiate this result for the convex function f (a) = a loga −a, and rearrange to obtain the numerical inequality
a (loga −logh)−(a −h) ≥0
for positive a and h.
Sum this expression over the components of the vectors a and h to complete the argument.
Proposition 8.1.3 states that the matrix relative entropy satisﬁes the same nonnegativity property as the vector relative entropy. The argument for matrices relies on the same ideas as Proposition 8.2.2, and it is hardly more difﬁcult. See §8.3.5 for the details.
The Perspective Transformation
Our next goal is to prove that the relative entropy is a convex function. To establish this claim,
we use an elegant technique from convex analysis. The approach depends on the perspective
transformation, a method for constructing a bivariate convex function from a univariate convex
Deﬁnition 8.2.3 (Perspective Transformation). Let f : R++ →R be a convex function on the positive real line. The perspective ψf of the function f is deﬁned as
ψf : R++ ×R++ →R
ψf (a;h) = a · f (h/a).
8.2. ANALYSIS OF THE RELATIVE ENTROPY FOR VECTORS
The perspective transformation has an interesting geometric interpretation. If we trace the ray
from the origin (0,0,0) in R3 through the point (a,h,ψf (a;h)), it pierces the plane (1,·,·) at the
point (1,h/a, f (h/a)). Equivalently, for each positive a, the epigraph of f is the “shadow” of the
epigraph of ψf (a,·) on the plane (1,·,·).
The key fact is that the perspective of a convex function is convex. This point follows from
the geometric reasoning in the last paragraph; we also include an analytic proof.
Fact 8.2.4 (Perspectives are Convex). Let f : R++ →R be a convex function. Then the perspective
ψf is convex. That is, for positive numbers ai and hi,
ψf (τa1 + ¯τa2; τh1 + ¯τh2) ≤τ·ψf (a1;h1)+ ¯τ·ψf (a2;h2)
for τ ∈ .
Proof. Fix two pairs (a1,h1) and (a2,h2) of positive numbers and an interpolation parameter
τ ∈ . Form the convex combinations
a = τa1 + ¯τa2
h = τh1 + ¯τh2.
We need to bound the perspective ψf (a;h) as the convex combination of its values at ψf (a1;h1)
and ψf (a2;h2). The trick is to introduce another pair of interpolation parameters:
By construction, s ∈ and ¯s = 1−s. We quickly determine that
ψf (a;h) = a · f (h/a)
= a · f (τh1/a + ¯τh2/a)
= a · f (s ·h1/a1 + ¯s ·h2/a2)
s · f (h1/a1)+ ¯s · f (h2/a2)
= τ· a1 · f (h1/a1)+ ¯τ· a2 · f (h2/a2)
= τ·ψf (a1;h1)+ ¯τ·ψf (a2;h2).
To obtain the second identity, we write h as a convex combination. The third identity follows
from the deﬁnitions of s and ¯s. The inequality depends on the fact that f is convex. Afterward,
we invoke the deﬁnitions of s and ¯s again. We conclude that ψf is convex.
When we study standard matrix functions, it is sometimes necessary to replace a convexity
assumption by a stricter property called operator convexity. There is a remarkable extension of
the perspective transform that constructs a bivariate matrix function from an operator convex
function. The matrix perspective has a powerful convexity property analogous with the result in
Fact 8.2.4. The analysis of the matrix perspective depends on a far-reaching generalization of the
Jensen inequality for operator convex functions. We develop these ideas in §§8.4.5, 8.5, and 8.6.
The Relative Entropy is Convex
To establish that the relative entropy is convex, we simply need to represent it as the perspective
of a convex function.
CHAPTER 8. A PROOF OF LIEB’S THEOREM
Proposition 8.2.5 (Relative Entropy is Convex). The map (a,h) 7→D(a;h) is convex. That is, for
positive vectors ai and hi of the same size,
D(τa1 + ¯τa2; τh1 + ¯τh2) ≤τ·D(a1;h1)+ ¯τ·D(a2;h2)
for τ ∈ .
Proof. Consider the convex function f (a) = a −1 −loga, deﬁned on the positive real line. By
direct calculation, the perspective transformation satisﬁes
ψf (a;h) = a (loga −logh)−(a −h)
for positive a and h.
Fact 8.2.4 states that ψf is a convex function. For positive vectors a and h, we can express the
relative entropy as
k ψf (ak;hk),
It follows that the relative entropy is convex.
Similarly, we can express the matrix relative entropy using the matrix perspective transformation. The analysis for matrices is substantially more involved. But, as we will see in §8.8, the
argument ultimately follows the same pattern as the proof of Proposition 8.2.5.
Elementary Trace Inequalities
It is time to begin our investigation into the properties of matrix functions. This section contains
some simple inequalities for the trace of a matrix function that we can establish by manipulating
eigenvalues and eigenvalue decompositions. These techniques are adequate to explain why the
matrix relative entropy is nonnegative. In contrast, we will need more subtle arguments to study
the convexity properties of the matrix relative entropy.
Trace Functions
We can construct a real-valued function on Hermitian matrices by composing the trace with a
standard matrix function. This type of map is called a trace function.
Deﬁnition 8.3.1 (Trace function). Let f : I →R be a function on an interval I of the real line, and
let A be an Hermitian matrix whose eigenvalues are contained in I. We deﬁne the trace function
tr f by the rule
tr f (A) =
i f (λi(A)),
where λi(A) denotes the ith largest eigenvalue of A. This formula gives the same result as composing the trace with the standard matrix function f .
Our ﬁrst goal is to demonstrate that a trace function tr f inherits a monotonicity property from
the underlying scalar function f .
Monotone Trace Functions
Let us demonstrate that the trace of a weakly increasing scalar function induces a trace function
that preserves the semideﬁnite order. To that end, recall that the relation A ≼H implies that
each eigenvalue of A is dominated by the corresponding eigenvalue of H.
8.3. ELEMENTARY TRACE INEQUALITIES
Fact 8.3.2 (Semideﬁnite Order implies Eigenvalue Order). For Hermitian matrices A and H,
λi(A) ≤λi(H)
for each index i.
Proof. This result follows instantly from the Courant–Fischer Theorem:
λi(A) = max
dimL=i min
dimL=i min
The maximum ranges over all i-dimensional linear subspaces L in the domain of A, and we use
the convention that 0/0 = 0. The inequality follows from the deﬁnition (2.1.11) of the semideﬁnite order ≼.
With this fact at hand, the claim follows quickly.
Proposition 8.3.3 (Monotone Trace Functions). Let f : I →R be a weakly increasing function
on an interval I of the real line, and let A and H be Hermitian matrices whose eigenvalues are
contained in I. Then
tr f (A) ≤tr f (H).
Proof. In view of Fact 8.3.2,
tr f (A) =
i f (λi(A)) ≤
i f (λi(H)) = tr f (H).
The inequality depends on the assumption that f is weakly increasing.
Our approach to matrix concentration relies on a special case of Proposition 8.3.3.
Example 8.3.4 (Trace Exponential is Monotone). The trace exponential map is monotone:
treA ≤treH
for all Hermitian matrices A and H.
Eigenvalue Decompositions, Redux
Before we continue, let us introduce a style for writing eigenvalue decompositions that will make
the next argument more transparent. Each d ×d Hermitian matrix A can be expressed as
The eigenvalues λ1 ≥··· ≥λd of A are real numbers, listed in weakly decreasing order. The family
{u1,...,ud} of eigenvectors of A forms an orthonormal basis for Cd with respect to the standard
inner product.
CHAPTER 8. A PROOF OF LIEB’S THEOREM
A Trace Inequality for Bivariate Functions
In general, it is challenging to study functions of two or more matrices because the eigenvectors
can interact in complicated ways. Nevertheless, there is one type of relation that always transfers
from the scalar setting to the matrix setting.
Proposition 8.3.5 (Generalized Klein Inequality). Let fi : I →R and gi : I →R be functions on an
interval I of the real line, and suppose that
i fi(a)gi(h) ≥0
for all a,h ∈I.
If A and H are Hermitian matrices whose eigenvalues are contained in I, then
fi(A)gi(H)
Proof. Consider eigenvalue decompositions A = P
j λj u j u∗
j and H = P
fi(A)gi(H)
j fi(λj )u j u∗
k gi(µk)vkv∗
i fi(λj )gi(µk)
·|〈u j , vk〉|2 ≥0.
We use the deﬁnition of a standard matrix function, we apply linearity of the trace to reorder
the sums, and we identify the trace as a squared inner product. The inequality follows from our
assumption on the scalar functions.
The Matrix Relative Entropy is Nonnegative
Using the generalized Klein inequality, it is easy to prove Proposition 8.1.3, which states that the
matrix relative entropy is nonnegative. The argument echoes the analysis in Proposition 8.2.2 for
the vector case.
Proof of Proposition 8.1.3. Suppose that f : R++ →R is a differentiable, convex function on the
positive real line. Since f is convex, the graph of f lies above its tangents:
f (a) ≥f (h)+ f ′(h)(a −h)
for positive a and h.
Using the generalized Klein inequality, Proposition 8.3.5, we can lift this relation to matrices:
tr f (A) ≥tr
f (H)+ f ′(H)(A −H)
for all positive-deﬁnite A and H.
This formula is sometimes called the (ungeneralized) Klein inequality.
Instantiate the latter result for the function f (a) = a loga −a, and rearrange to see that
D(A;H) = tr
A (log A −logH)−(A −H)
for all positive-deﬁnite A and H.
In other words, the matrix relative entropy is nonnegative.
The Logarithm of a Matrix
In this section, we commence our journey toward the proof that the matrix relative entropy is
convex. The proof of Proposition 8.2.5 indicates that the convexity of the logarithm plays an
important role in the convexity of the vector relative entropy. As a ﬁrst step, we will demonstrate
that the matrix logarithm has a striking convexity property with respect to the semideﬁnite order.
Along the way, we will also develop a monotonicity property of the matrix logarithm.
8.4. THE LOGARITHM OF A MATRIX
An Integral Representation of the Logarithm
Initially, we deﬁned the logarithm of a d ×d positive-deﬁnite matrix A using an eigenvalue decomposition:
To study how the matrix logarithm interacts with the semideﬁnite order, we will work with an
alternative presentation based on an integral formula.
Proposition 8.4.1 (Integral Representation of the Logarithm). The logarithm of a positive number a is given by the integral
Similarly, the logarithm of a positive-deﬁnite matrix A is given by the integral
(1+u)−1I−(A +uI)−1¤
Proof. To verify the scalar formula, we simply use the deﬁnition of the improper integral:
log(1+u)−log(a +u)
= loga + lim
We obtain the matrix formula by applying the scalar formula to each eigenvalue of A and then
expressing the result in terms of the original matrix.
The integral formula from Proposition 8.4.1 is powerful because it expresses the logarithm
in terms of the matrix inverse, which is much easier to analyze. Although it may seem that we
have pulled this representation from thin air, the approach is motivated by a wonderful theory
of matrix functions initiated by Löwner in the 1930s.
Operator Monotone Functions
Our next goal is to study the monotonicity properties of the matrix logarithm. To frame this
discussion properly, we need to introduce an abstract deﬁnition.
Deﬁnition 8.4.2 (Operator Monotone Function). Let f : I →R be a function on an interval I of
the real line. The function f is operator monotone on I when
f (A) ≼f (H)
for all Hermitian matrices A and H whose eigenvalues are contained in I.
CHAPTER 8. A PROOF OF LIEB’S THEOREM
Let us state some basic facts about operator monotone functions. Many of these points follow
easily from the deﬁnition.
• When β ≥0, the weakly increasing afﬁne function t 7→α+βt is operator monotone on each
interval I of the real line.
• The quadratic function t 7→t2 is not operator monotone on the positive real line.
• The exponential map t 7→et is not operator monotone on the real line.
• When α ≥0 and f is operator monotone on I, the function αf is operator monotone on I.
• If f and g are operator monotone on an interval I, then f + g is operator monotone on I.
These properties imply that the operator monotone functions form a convex cone. It also warns
us that the class of operator monotone functions is somewhat smaller than the class of weakly
increasing functions.
The Negative Inverse is Operator Monotone
Fortunately, interesting operator monotone functions do exist. Let us present an important example related to the matrix inverse.
Proposition 8.4.3 (Negative Inverse is Operator Monotone). For each number u ≥0, the function
a 7→−(a+u)−1 is operator monotone on the positive real line. That is, for positive-deﬁnite matrices
−(A +uI)−1 ≼−(H +uI)−1.
Proof. Deﬁne the matrices Au = A+uI and Hu = H+uI. The semideﬁnite relation A ≼H implies
that Au ≼Hu. Apply the Conjugation Rule (2.1.12) to see that
When a positive-deﬁnite matrix has eigenvalues bounded above by one, its inverse has eigenvalues bounded below by one. Therefore,
¢−1 = H1/2
Another application of the Conjugation Rule (2.1.12) delivers the inequality H−1
u . Finally,
we negate this semideﬁnite relation, which reverses its direction.
The Logarithm is Operator Monotone
Now, we are prepared to demonstrate that the logarithm is an operator monotone function. The
argument combines the integral representation from Proposition 8.4.1 with the monotonicity of
the inverse map from Proposition 8.4.3.
Proposition 8.4.4 (Logarithm is Operator Monotone). The logarithm is an operator monotone
function on the positive real line. That is, for positive-deﬁnite matrices A and H,
log A ≼logH.
8.4. THE LOGARITHM OF A MATRIX
Proof. For each u ≥0, Proposition 8.4.3 demonstrates that
(1+u)−1I−(A +uI)−1 ≼(1+u)−1I−(H +uI)−1.
The integral representation of the logarithm, Proposition 8.4.1, allows us to calculate that
(1+u)−1I−(A +uI)−1¤
(1+u)−1I−(H +uI)−1¤
du = logH.
We have used the fact that the semideﬁnite order is preserved by integration against a positive
Operator Convex Functions
Next, let us investigate the convexity properties of the matrix logarithm. As before, we start with
an abstract deﬁnition.
Deﬁnition 8.4.5 (Operator Convex Function). Let f : I →R be a function on an interval I of the
real line. The function f is operator convex on I when
f (τA + ¯τH) ≼τ· f (A)+ ¯τ· f (H)
for all τ ∈ 
and for all Hermitian matrices A and H whose eigenvalues are contained in I. A function g : I →R
is operator concave when −g is operator convex on I.
We continue with some important facts about operator convex functions. Most of these
claims can be derived easily.
• When γ ≥0, the quadratic function t 7→α+βt +γt2 is operator convex on the real line.
• The exponential map t 7→et is not operator convex on the real line.
• When α ≥0 and f is operator convex on I, the function αf is operator convex in I.
• If f and g are operator convex on I, then f + g is operator convex on I.
The operator monotone functions form a convex cone. We also learn that the family of operator
convex functions is somewhat smaller than the family of convex functions.
The Inverse is Operator Convex
The inverse provides a very important example of an operator convex function.
Proposition 8.4.6 (Inverse is Operator Convex). For each u ≥0, the function a 7→(a + u)−1 is
operator convex on the positive real line. That is, for positive-deﬁnite matrices A and H,
(τA + ¯τH +uI)−1 ≼τ·(A +uI)−1 + ¯τ·(H +uI)−1
for τ ∈ .
To establish Proposition 8.4.6, we use an argument based on the Schur complement lemma.
For completeness, let us state and prove this important fact.
CHAPTER 8. A PROOF OF LIEB’S THEOREM
Fact 8.4.7 (Schur Complements). Suppose that T is a positive-deﬁnite matrix. Then
if and only if
B∗T −1B ≼M.
Proof of Fact 8.4.7. To see why this is true, just calculate that
M −B∗T −1B
In essence, we are performing block Gaussian elimination to bring the original matrix into blockdiagonal form. Now, the Conjugation Rule (2.1.12) ensures that the central matrix on the left is
positive semideﬁnite together with the matrix on the right. From this equivalence, we extract the
result (8.4.1).
We continue with the proof that the inverse is operator convex.
Proof of Proposition 8.4.6. The Schur complement lemma, Fact 8.4.7, provides that
whenever T is positive deﬁnite.
Applying this observation to the positive-deﬁnite matrices A +uI and H +uI, we see that
·τA + ¯τH +uI
τ·(A +uI)−1 + ¯τ·(H +uI)−1
Since the top-left block of the latter matrix is positive deﬁnite, another application of Fact 8.4.7
delivers the relation
(τA + ¯τH +uI)−1 ≼τ·(A +uI)−1 + ¯τ·(H +uI)−1.
This is the advertised conclusion.
The Logarithm is Operator Concave
We are ﬁnally prepared to verify that the logarithm is operator concave. The argument is based
on the integral representation from Proposition 8.4.4 and the convexity of the inverse map from
Proposition 8.4.6.
Proposition 8.4.8 (Logarithm is Operator Concave). The logarithm is operator concave on the
positive real line. That is, for positive-deﬁnite matrices A and H,
τ·log A + ¯τ·logH ≼log(τA + ¯τH)
for τ ∈ .
Proof. For each u ≥0, Proposition 8.4.6 demonstrates that
−τ·(A +uI)−1 −¯τ·(H +uI)−1 ≼−(τA + ¯τH +uI)−1.
8.5. THE OPERATOR JENSEN INEQUALITY
Invoke the integral representation of the logarithm from Proposition 8.4.1 to see that
τ·log A + ¯τ·logH = τ·
(1+u)−1I−(A +uI)−1¤
(1+u)−1I−(H +uI)−1¤
τ·(A +uI)−1 + ¯τ·(H +uI)−1¢¤
(1+u)−1I−(τA + ¯τH +uI)−1¤
du = log(τA + ¯τH).
Once again, we have used the fact that integration preserves the semideﬁnite order.
The Operator Jensen Inequality
Convexity is a statement about how a function interacts with averages. By deﬁnition, a function
f : I →R is convex when
f (τa + ¯τh) ≤τ· f (a)+ ¯τ· f (h)
for all τ ∈ and all a,h ∈I.
The convexity inequality (8.5.1) automatically extends from an average involving two terms to
an arbitrary average. This is the content of Jensen’s inequality.
Deﬁnition 8.4.5, of an operator convex function f : I →R, is similar in spirit:
f (τA + ¯τH) ≼τ· f (A)+ ¯τ· f (H)
for all τ ∈ 
and all Hermitian matrices A and H whose eigenvalues are contained in I. Surprisingly, the
semideﬁnite relation (8.5.2) automatically extends to a large family of matrix averaging operations. This remarkable property is called the operator Jensen inequality.
Matrix Convex Combinations
In a vector space, convex combinations provide a natural method of averaging. But matrices
have a richer structure, so we can consider a more general class of averages.
Deﬁnition 8.5.1 (Matrix Convex Combination). Let A1 and A2 be Hermitian matrices. Consider
a decomposition of the identity of the form
Then the Hermitian matrix
1 A1K1 +K ∗
is called a matrix convex combination of A1 and A2.
To see why it is reasonable to call (8.5.3) an averaging operation on Hermitian matrices, let
us note a few of its properties.
• Deﬁnition 8.5.1 encompasses scalar convex combinations because we can take K1 = τ1/2I
and K2 = ¯τ1/2I.
• The matrix convex combination preserves the identity matrix: K ∗
1 IK1 +K ∗
2 IK2 = I.
CHAPTER 8. A PROOF OF LIEB’S THEOREM
• The matrix convex combination preserves positivity:
1 A1K1 +K ∗
for all positive-semideﬁnite A1 and A2.
• If the eigenvalues of A1 and A2 are contained in an interval I, then the eigenvalues of the
matrix convex combination (8.5.3) are also contained in I.
We will encounter a concrete example of a matrix convex combination later when we prove Theorem 8.6.2.
Jensen’s Inequality for Matrix Convex Combinations
Operator convexity is a self-improving property. Even though the deﬁnition of an operator convex function only involves a scalar convex combination, it actually contains an inequality for
matrix convex combinations. This is the content of the operator Jensen inequality.
Theorem 8.5.2 (Operator Jensen Inequality). Let f be an operator convex function on an interval
I of the real line, and let A1 and A2 be Hermitian matrices with eigenvalues in I. Consider a
decomposition of the identity
1 A1K1 +K ∗
1 f (A1)K1 +K ∗
2 f (A2)K2.
Proof. Let us introduce a block-diagonal matrix:
Indeed, the matrix A lies in the domain of f because its eigenvalues fall in the interval I. We can
apply a standard matrix function to a block-diagonal matrix by applying the function to each
There are two main ingredients in the argument. The ﬁrst idea is to realize the matrix convex combination of A1 and A2 by conjugating the block-diagonal matrix A with an appropriate
unitary matrix. To that end, let us construct a unitary matrix
where Q∗Q = I and QQ∗= I.
To see why this is possible, note that the ﬁrst block of columns is orthonormal:
As a consequence, we can choose L1 and L2 to complete the unitary matrix Q. By direct computation, we ﬁnd that
1 A1K1 +K ∗
We have omitted the precise values of the entries labeled ∗because they do not play a role in our
8.6. THE MATRIX PERSPECTIVE TRANSFORMATION
The second idea is to restrict the block matrix in (8.5.5) to its diagonal. To perform this maneuver, we express the diagonalizing operation as a scalar convex combination of two unitary
conjugations, which gives us access to the operator convexity of f . Let us see how this works.
Deﬁne the unitary matrix
The key observation is that, for any block matrix,
Another advantage of this construction is that we can easily apply a standard matrix function to
the block-diagonal matrix.
Together, these two ideas lead to a succinct proof of the operator Jensen inequality. Write
[·]11 for the operation that returns the (1,1) block of a block matrix. We may calculate that
1 A1K1 +K ∗
2U ∗(Q∗AQ)U
2(QU)∗A(QU)
2 f (Q∗AQ)+ 1
2 f ((QU)∗A(QU))
The ﬁrst identity depends on the representation (8.5.5) of the matrix convex combination as
the (1,1) block of Q∗AQ. The second line follows because the averaging operation presented
in (8.5.6) does not alter the (1,1) block of the matrix. In view of (8.5.6), we are looking at the
(1,1) block of the matrix obtained by applying f to a block-diagonal matrix. This is equivalent to
applying the function f inside the (1,1) block, which gives the third line. Last, the semideﬁnite
relation follows from the operator convexity of f on the interval I.
We complete the argument by reversing the steps we have taken so far.
1 A1K1 +K ∗
2Q∗f (A)Q + 1
2U ∗(Q∗f (A)Q)U
1 f (A1)K1 +K ∗
2 f (A2)K2.
To obtain the ﬁrst relation, recall that a standard matrix function commutes with unitary conjugation. The second identity follows from the formula (8.5.6) because diagonalization preserves
the (1,1) block. Finally, we identify the (1,1) block of Q∗f (A)Q just as we did in (8.5.5). This step
depends on the fact that the diagonal blocks of f (A) are simply f (A1) and f (A2).
The Matrix Perspective Transformation
To show that the vector relative entropy is convex, we represented it as the perspective of a convex function. To demonstrate that the matrix relative entropy is convex, we are going to perform
a similar maneuver. This section develops an extension of the perspective transformation that
applies to operator convex functions. Then we demonstrate that this matrix perspective has a
strong convexity property with respect to the semideﬁnite order.
CHAPTER 8. A PROOF OF LIEB’S THEOREM
The Matrix Perspective
In the scalar setting, the perspective transformation converts a convex function into a bivariate
convex function. There is a related construction that applies to an operator convex function.
Deﬁnition 8.6.1 (Matrix Perspective). Let f : R++ →R be an operator convex function, and let A
and H be positive-deﬁnite matrices of the same size. Deﬁne the perspective map
Ψf (A;H) = A1/2 · f
A−1/2H A−1/2¢
The notation A1/2 refers to the unique positive-deﬁnite square root of A, and A−1/2 denotes the
inverse of this square root.
The Conjugation Rule (2.1.12) ensures that all the matrices involved remain positive deﬁnite, so
this deﬁnition makes sense. To see why the matrix perspective extends the scalar perspective,
notice that
Ψf (A;H) = A · f (H A−1)
when A and H commute.
This formula is valid because commuting matrices are simultaneously diagonalizable. We will
use the matrix perspective in a case where the matrices commute, but it is no harder to analyze
the perspective without this assumption.
The Matrix Perspective is Operator Convex
The key result is that the matrix perspective is an operator convex map on a pair of positivedeﬁnite matrices. This theorem follows from the operator Jensen inequality in much the same
way that Fact 8.2.4 follows from scalar convexity.
Theorem 8.6.2 (Matrix Perspective is Operator Convex). Let f : R++ →R be an operator convex
function. Let Ai and Hi be positive-deﬁnite matrices of the same size. Then
τA1 + ¯τA2; τH1 + ¯τH2
for τ ∈ .
Proof. Let f be an operator convex function, and let Ψf be its perspective transform. Fix pairs
(A1,H1) and (A2,H2) of positive-deﬁnite matrices, and choose an interpolation parameter τ ∈
 . Form the scalar convex combinations
A = τA1 + ¯τA2
H = τH1 + ¯τH2.
Our goal is to bound the perspective Ψf (A;H) as a scalar convex combination of its values
Ψf (A1;H1) and Ψf (A2;H2). The idea is to introduce matrix interpolation parameters:
K1 = τ1/2A1/2
K2 = ¯τ1/2A1/2
Observe that these two matrices decompose the identity:
2 K2 = τ· A−1/2A1A−1/2 + ¯τ· A−1/2A2A−1/2 = A−1/2AA−1/2 = I.
This construction allows us to express the perspective using a matrix convex combination, which
gives us access to the operator Jensen inequality.
8.7. THE KRONECKER PRODUCT
We calculate that
Ψf (A;H) = A1/2 · f
A−1/2H A−1/2¢
= A1/2 · f
τ· A−1/2H1A−1/2 + ¯τ· A−1/2H2A−1/2¢
= A1/2 · f
The ﬁrst line is simply the deﬁnition of the matrix perspective. In the second line, we use the
deﬁnition of H as a scalar convex combination. Third, we introduce the matrix interpolation
parameters through the expressions τ1/2A−1/2 = A1/2
K1 and ¯τ1/2A−1/2 = A1/2
K2 and their conjugate transposes. To continue the calculation, we apply the operator Jensen inequality, Theorem 8.5.2, to reach
Ψf (A;H) ≼A1/2 ·
+ ¯τ· A1/2
= τ·Ψf (A1;H1)+ ¯τ·Ψf (A2;H2).
We have also used the Conjugation Rule (2.1.12) to support the ﬁrst relation. Finally, we recall
the deﬁnitions of K1 and K2, and we identify the two matrix perspectives.
The Kronecker Product
The matrix relative entropy is a function of two matrices. One of the difﬁculties of analyzing this
type of function is that the two matrix arguments do not generally commute with each other.
As a consequence, the behavior of the matrix relative entropy depends on the interactions between the eigenvectors of the two matrices. To avoid this problem, we will build matrices that do
commute with each other, which simpliﬁes our task considerably.
The Kronecker Product
Our approach is based on an fundamental object from linear algebra. We restrict our attention
to the simplest version here.
Deﬁnition 8.7.1 (Kronecker Product). Let A and H be Hermitian matrices with dimension d ×d.
The Kronecker product A ⊗H is the d2 ×d2 Hermitian matrix
At ﬁrst sight, the deﬁnition of the Kronecker product may seem strange, but it has many delightful properties. The rest of the section develops the basic facts about this construction.
Linearity Properties
First of all, a Kronecker product with the zero matrix is always zero:
A ⊗0 = 0⊗0 = 0⊗H
for all A and H.
CHAPTER 8. A PROOF OF LIEB’S THEOREM
Next, the Kronecker product is homogeneous in each factor:
(αA)⊗H = α(A ⊗H) = A ⊗(αH)
Furthermore, the Kronecker product is additive in each coordinate:
(A1 + A2)⊗H = A1 ⊗H + A2 ⊗H
A ⊗(H1 + H2) = A ⊗H1 + A ⊗H2.
In other words, the Kronecker product is a bilinear operation.
Mixed Products
The Kronecker product interacts beautifully with the usual product of matrices. By direct calculation, we obtain a simple rule for mixed products:
(A1 ⊗H1)(A2 ⊗H2) = (A1A2)⊗(H1H2).
Since I ⊗I is the identity matrix, the identity (8.7.1) leads to a formula for the inverse of a Kronecker product:
(A ⊗H)−1 =
when A and H are invertible.
Another important consequence of the rule (8.7.1) is the following commutativity relation:
(A ⊗I)(I⊗H) = (I⊗H)(A ⊗I)
for all Hermitian matrices A and H.
This simple fact has great importance for us.
The Kronecker Product of Positive Matrices
As we have noted, the Kronecker product of two Hermitian matrices is itself an Hermitian matrix.
In fact, the Kronecker product preserves positivity as well.
Fact 8.7.2 (Kronecker Product Preserves Positivity). Let A and H be positive-deﬁnite matrices.
Then A ⊗H is positive deﬁnite.
Proof. To see why, observe that
A1/2 ⊗H1/2¢¡
A1/2 ⊗H1/2¢
As usual, A1/2 refers to the unique positive-deﬁnite square root of the positive-deﬁnite matrix
A. We have expressed A ⊗H as the square of an Hermitian matrix, so it must be a positivesemideﬁnite matrix. To see that it is actually positive deﬁnite, we simply apply the inversion
formula (8.7.2) to discover that A ⊗H is invertible.
The Logarithm of a Kronecker Product
As we have discussed, the matrix logarithm plays a central role in our analysis. There is an elegant
formula for the logarithm of a Kronecker product that will be valuable to us.
8.8. THE MATRIX RELATIVE ENTROPY IS CONVEX
Fact 8.7.3 (Logarithm of a Kronecker Product). Let A and H be positive-deﬁnite matrices. Then
log(A ⊗H) = (log A)⊗I+I⊗(logH).
Proof. The argument is based on the fact that the matrix logarithm is the functional inverse of the
matrix exponential. Since the exponential of a sum of commuting matrices equals the product
of the exponentials, we have
= exp(M ⊗I)·exp(I⊗T ).
This formula relies on the commutativity relation (8.7.3). Applying the power series representation of the exponential, we determine that
exp(M ⊗I) =
q!(M ⊗I)q =
⊗I = eM ⊗I.
The second identity depends on the rule (8.7.1) for mixed products, and the last identity follows
from the linearity of the Kronecker product. A similar calculation shows that exp(I⊗T ) = I⊗eT .
In summary,
= eM ⊗eT .
We have used the product rule (8.7.1) again. To complete the argument, simply choose M = log A
and T = logH and take the logarithm of the last identity.
A Linear Map
Finally, we claim that there is a linear map ϕ that extracts the trace of the matrix product from
the Kronecker product. Let A and H be d ×d Hermitian matrices. Then we deﬁne
ϕ(A ⊗H) = tr(AH).
The map ϕ is linear because the Kronecker product A ⊗H tabulates all the pairwise products
of the entries of A and H, and tr(AH) is a sum of certain of these pairwise products. For our
purposes, the key fact is that the map ϕ preserves the semideﬁnite order:
i Ai ⊗Hi ≽0
i ϕ(Ai ⊗Hi) ≥0.
This formula is valid for all Hermitian matrices Ai and Hi. To see why (8.7.5) holds, simply note
that the map can be represented as an inner product:
ϕ(A ⊗H) = ι∗(A ⊗H)ι
ι := vec(Id).
The vec operation stacks the columns of a d ×d matrix on top of each other, moving from left to
right, to form a column vector of length d2.
The Matrix Relative Entropy is Convex
We are ﬁnally prepared to establish Theorem 8.1.4, which states that the matrix relative entropy
is a convex function. This argument draws on almost all of the ideas we have developed over the
course of this chapter.
CHAPTER 8. A PROOF OF LIEB’S THEOREM
Consider the function f (a) = a −1−loga, deﬁned on the positive real line. This function is
operator convex because it is the sum of the afﬁne function a 7→a −1 and the operator convex
function a 7→−loga. The negative logarithm is operator convex because of Proposition 8.4.8.
Let A and H be positive-deﬁnite matrices. Consider the matrix perspective Ψf evaluated at
the commuting positive-deﬁnite matrices A ⊗I and I⊗H:
Ψf (A ⊗I; I⊗H) = (A ⊗I)· f
(I⊗H)(A ⊗I)−1¢
= (A ⊗I)· f
We have used the simpliﬁed deﬁnition (8.6.1) of the perspective for commuting matrices, and we
have invoked the rules (8.7.1) and (8.7.2) for arithmetic with Kronecker products. Introducing the
deﬁnition of the function f , we ﬁnd that
Ψf (A ⊗I; I⊗H) = (A ⊗I)·
A−1 ⊗H −I⊗I−log
= I⊗H −A ⊗I−(A ⊗I)·
= (A log A)⊗I−A ⊗(logH)−(A ⊗I−I⊗H)
To reach the second line, we use more Kronecker product arithmetic, along with Fact 8.7.3, the
law for calculating the logarithm of the Kronecker product. The last line depends on the property
= −log A. Applying the linear map ϕ from (8.7.4) to both sides, we reach
A log A −A logH −(A −H)
We have represented the matrix relative entropy in terms of a matrix perspective.
Let Ai and Hi be positive-deﬁnite matrices, and ﬁx a parameter τ ∈ . Theorem 8.6.2 tells
us that the matrix perspective is operator convex:
τ·(A1 ⊗I)+ ¯τ·(A2 ⊗I); τ·(I⊗H1)+ ¯τ·(I⊗H2)
A1 ⊗I; I⊗H1
A2 ⊗I; I⊗H2
The inequality (8.7.5) states that the linear map ϕ preserves the semideﬁnite order.
τ·(A1 ⊗I)+ ¯τ·(A2 ⊗I); τ·(I⊗H1)+ ¯τ·(I⊗H2)
A1 ⊗I; I⊗H1
A2 ⊗I; I⊗H2
Introducing the formula (8.8.1), we conclude that
τA1 + ¯τA2; τH1 + ¯τH2
The matrix relative entropy is convex.
The material in this chapter is drawn from a variety of sources, ranging from textbooks to lecture
notes to contemporary research articles. The best general sources include the books on matrix analysis by Bhatia [Bha97, Bha07] and by Hiai & Petz [HP14]. We also recommend a set of
notes [Car10] by Eric Carlen. More speciﬁc references appear below.
8.9. NOTES
Lieb’s Theorem
Theorem 8.1.1 is one of the major results in the important paper [Lie73] of Elliott Lieb on convex trace functions. Lieb wrote this paper to resolve a conjecture of Wigner, Yanase, & Dyson
about the concavity properties of a certain measure of information in a quantum system. He
was also motivated by a conjecture that quantum mechanical entropy satisﬁes a strong subadditivity property. The latter result states that our uncertainty about a partitioned quantum system
is controlled by the uncertainty about smaller parts of the system. See Carlen’s notes [Car10] for
a modern presentation of these ideas.
Lieb derived Theorem 8.1.1 as a corollary of another difﬁcult concavity theorem that he developed [Lie73, Thm. 1]. The most direct proof of Lieb’s Theorem is probably Epstein’s argument,
which is based on methods from complex analysis [Eps73]; see Ruskai’s papers [Rus02, Rus05]
for a condensed version of Epstein’s approach. The proof that appears in Section 8.1 is due to
the author of these notes [Tro12]; this technique depends on ideas developed by Carlen & Lieb
to prove some other convexity theorems [CL08, §5].
In fact, many deep convexity and concavity theorems for trace functions are equivalent with
each other, in the sense that the mutual implications follow from relatively easy arguments.
See [Lie73, §5] and [CL08, §5] for discussion of this point.
The Matrix Relative Entropy
Our deﬁnition of matrix relative entropy differs slightly from the usual deﬁnition in the literature
on quantum statistical mechanics and quantum information theory because we have included
an additional linear term. This alteration does not lead to substantive changes in the analysis.
The fact that matrix relative entropy is nonnegative is a classical result attributed to Klein.
See [Pet94, §2] or [Car10, §2.3].
Lindblad [Lin73] is credited with the result that matrix relative entropy is convex, as stated
in Theorem 8.1.4. Lindblad derived this theorem as a corollary of Lieb’s results from [Lie73].
Bhatia [Bha97, Chap. IX] gives two alternative proofs, one due to Connes & Størmer [CS75] and
another due to Petz [Pet86]. There is also a remarkable proof due to Ando [And79, Thm. 7].
Our approach to Theorem 8.1.4 is adapted directly from a recent paper of Effros [Eff09]. Nevertheless, many of the ideas date back to the works cited in the last paragraph.
The Relative Entropy for Vectors
The treatment of the relative entropy for vectors in Section 8.2 is based on two classical methods
for constructing divergences. To show that the relative entropy is nonnegative, we represent it as
a Bregman divergence [Brè67]. To show that the relative entropy is convex, we represent it as an
f -divergence [AS66, Csi67]. Let us say a few more words about these constructions.
Suppose that f is a differentiable convex function on Rd. Bregman considered divergences
of the form
Bf (a;h) := f (a)−
f (h)−〈∇f (h), a −h〉
Since f is convex, the Bregman divergence Bf is always nonnegative. In the vector setting, there
are two main examples of Bregman divergences. The function f (a) = 1
2 leads to the squared
Euclidean distance, and the function f (a) = P
i(ai logai −ai) leads to the vector relative entropy.
Bregman divergences have many geometric properties in common with these two functions. For
an introduction to Bregman divergences for matrices, see [DT07].
CHAPTER 8. A PROOF OF LIEB’S THEOREM
Suppose that f : R++ →R is a convex function. Ali & Silvey [AS66] and Csiszár [Csi67] considered divergences of the form
Cf (a;h) :=
i ai · f (hi/ai).
We recognize this expression as a perspective transformation, so the f -divergence Cf is always
convex. The main example is based on the Shannon entropy f (a) = a loga, which leads to
a cousin of the vector relative entropy. The paper [RW11] contains a recent discussion of f divergences and their applications in machine learning. Petz has studied functions related to
f -divergences in the matrix setting [Pet86, Pet10].
Elementary Trace Inequalities
The material in Section 8.3 on trace functions is based on classical results in quantum statistical
mechanics. We have drawn the arguments in this section from Petz’s survey [Pet94, Sec. 2] and
Carlen’s lecture notes [Car10, Sec. 2.2].
Operator Monotone & Operator Convex Functions
The theory of operator monotone functions was initiated by Löwner [Löw34]. He developed a
characterization of an operator monotone function in terms of divided differences. For a function f , the ﬁrst divided difference is the quantity
f [a,h] :=
f (a)−f (h)
Löwner proved that f is operator monotone on an interval I if and only we have the semideﬁnite
for all {ai} ⊂I and all d ∈N.
This result is analogous with the fact that a smooth, monotone scalar function has a nonnegative
derivative. Löwner also established a connection between operator monotone functions and
Pick functions from the theory of complex variables. A few years later, Kraus introduced the
concept of an operator convex function in [Kra36], and he developed some results that parallel
Löwner’s theory for operator monotone functions.
Somewhat later, Bendat & Sherman [BS55] developed characterizations of operator monotone and operator convex functions based on integral formulas. For example, f is an operator
monotone function on (0,∞) if and only if it can be written in the form
f (t) = α+βt +
u + t dρ(u)
1+u dρ(u) < ∞.
Similarly, f is an operator convex function on [0,∞) if and only if it can be written in the form
f (t) = α+βt +γt2 +
u + t dρ(u)
dρ(u) < ∞.
In both cases, dρ is a nonnegative measure. The integral representation of the logarithm in
Proposition 8.4.1 is closely related to these formulas.
8.9. NOTES
We have taken the proof that the matrix inverse is monotone from Bhatia’s book [Bha97,
Prop. V.1.6]. The proof that the matrix inverse is convex appears in Ando’s paper [And79]. Our
treatment of the matrix logarithm was motivated by a conversation with Eric Carlen at an IPAM
workshop at Lake Arrowhead in December 2010.
For more information about operator monotonicity and operator convexity, we recommend
Bhatia’s books [Bha97, Bha07], Carlen’s lecture notes [Car10], and the book of Hiai & Petz [HP14].
The Operator Jensen Inequality
The paper [HP82] of Hansen & Pedersen contains another treatment of operator monotone and
operator convex functions. The highlight of this work is a version of the operator Jensen inequality. Theorem 8.5.2 is a reﬁnement of this result that was established by the same authors
two decades later [HP03]. Our proof of the operator Jensen inequality is drawn from Petz’s
book [Pet11, Thm. 8.4]; see also Carlen’s lecture notes [Car10, Thm. 4.20].
The Matrix Perspective & the Kronecker Product
We have been unable to identify the precise source of the idea that a bivariate matrix function can
be represented in terms of a matrix perspective. Two important results in this direction appear
in Ando’s paper [And79, Thms. 6 and 7].
f positive and operator concave on (0,∞)
(A,H) 7→(A ⊗I)· f
is operator concave
on pairs of positive-deﬁnite matrices. Similarly,
f operator monotone on (0,∞)
(A,H) 7→(A⊗I)· f
is operator convex
on pairs of positive-deﬁnite matrices. Ando proves that the matrix relative entropy is convex by
applying the latter result to the matrix logarithm. We believe that Ando was the ﬁrst author to
appreciate the value of framing results of this type in terms of the Kronecker product, and we
have followed his strategy here. On the other hand, Ando’s analysis is different in spirit because
he relies on integral representations of operator monotone and convex functions.
In a subsequent paper [KA80], Kubo & Ando constructed operator means using a related approach. They show that
f positive and operator monotone on (0,∞)
(A,H) 7→A1/2 · f
A−1/2H A−1/2¢
is operator concave
on pairs of positive-deﬁnite matrices. Kubo & Ando point out that particular cases of this construction appear in the work of Pusz & Woronowicz [PW75]. This is the earliest citation where we
have seen the matrix perspective black-on-white.
A few years later, Petz introduced a class of quasi-entropies for matrices [Pet86]. These functions also involve a perspective-like construction, and Petz was clearly inﬂuenced by Csiszár’s
work on f -divergences. See [Pet10] for a contemporary treatment.
The presentation in these notes is based on a recent paper [Eff09] of Effros. He showed that
convexity properties of the matrix perspective follow from the operator Jensen inequality, and he
CHAPTER 8. A PROOF OF LIEB’S THEOREM
derived the convexity of the matrix relative entropy as a consequence. Our analysis of the matrix
perspective in Theorem 8.6.2 is drawn from a subsequent paper [ENG11], which removes some
commutativity assumptions from Effros’s argument.
The proof in §8.8 that the matrix relative entropy is convex, Theorem 8.1.4, recasts Effros’s
argument [Eff09, Cor. 2.2] in the language of Kronecker products. In his paper, Effros works with
left- and right-multiplication operators. To appreciate the connection, simply note the identities
(A ⊗I)vec(M) = vec(M A).
(I⊗H)vec(M) = vec(HM).
In other words, the matrix A ⊗I can be interpreted as right-multiplication by A, while the matrix
I ⊗H can be interpreted as left-multiplication by H. (The change in sense is an unfortunate
consequence of the deﬁnition of the Kronecker product.)
Matrix Concentration: Resources
This annotated bibliography describes some papers that involve matrix concentration inequalities. Right now, this presentation is heavily skewed toward theoretical results, rather than applications of matrix concentration.
Exponential Matrix Concentration Inequalities
We begin with papers that contain the most current results on matrix concentration.
• [Tro11c]. These lecture notes are based heavily on the research described in this paper.
This work identiﬁes Lieb’s Theorem [Lie73, Thm. 6] as the key result that animates exponential moment bounds for random matrices. Using this technique, the paper develops
the bounds for matrix Gaussian and Rademacher series, the matrix Chernoff inequalities,
and several versions of the matrix Bernstein inequality. In addition, it contains a matrix
Hoeffding inequality (for sums of bounded random matrices), a matrix Azuma inequality (for matrix martingales with bounded differences), and a matrix bounded difference
inequality (for matrix-valued functions of independent random variables).
• [Tro12]. This note describes a simple proof of Lieb’s Theorem that is based on the joint convexity of quantum relative entropy. This reduction, however, still involves a deep convexity
theorem. Chapter 8 contains an explication of this paper.
• [Oli10a]. Oliveira’s paper uses an ingenious argument, based on the Golden–Thompson
inequality (3.3.3), to establish a matrix version of Freedman’s inequality. This result is,
roughly, a martingale version of Bernstein’s inequality. This approach has the advantage
that it extends to the fully noncommutative setting [JZ12]. Oliveira applies his results to
study some problems in random graph theory.
• [Tro11a]. This paper shows that Lieb’s Theorem leads to a Freedman-type inequality for
matrix-valued martingales. The associated technical report [Tro11b] describes additional
results for matrix-valued martingales.
• [GT14]. This article explains how to use the Lieb–Seiringer Theorem [LS05] to develop tail
bounds for the interior eigenvalues of a sum of independent random matrices. It contains a Chernoff-type bound for a sum of positive-semideﬁnite matrices, as well as several
Bernstein-type bounds for sums of bounded random matrices.
• [MJC+14]. This paper contains a strikingly different method for establishing matrix concentration inequalities. The argument is based on work of Sourav Chatterjee [Cha07] that
MATRIX CONCENTRATION: RESOURCES
shows how Stein’s method of exchangeable pairs [Ste72] leads to probability inequalities.
This technique has two main advantages. First, it gives results for random matrices that are
based on dependent random variables. As a special case, the results apply to sums of independent random matrices. Second, it delivers both exponential moment bounds and polynomial moment bounds for random matrices. Indeed, the paper describes a Bernsteintype exponential inequality and also a Rosenthal-type polynomial moment bound. Furthermore, this work contains what is arguably the simplest known proof of the noncommutative Khintchine inequality.
• [PMT14]. This paper improves on the work in [MJC+14] by extending an argument, based
on Markov chains, that was developed in Chatterjee’s thesis [Cha05]. This analysis leads
to satisfactory matrix analogs of scalar concentration inequalities based on logarithmic
Sobolev inequalities. In particular, it is possible to develop a matrix version of the exponential Efron–Stein inequality in this fashion.
• [CGT12a, CGT12b]. The primary focus of this paper is to analyze a speciﬁc type of procedure for covariance estimation. The appendix contains a new matrix moment inequality
that is, roughly, the polynomial moment bound associated with the matrix Bernstein inequality.
• [Kol11]. These lecture notes use matrix concentration inequalities as a tool to study some
estimation problems in statistics. They also contain some matrix Bernstein inequalities for
unbounded random matrices.
• [GN]. Gross and Nesme show how to extend Hoeffding’s method for analyzing sampling
without replacement to the matrix setting. This result can be combined with a variety of
matrix concentration inequalities.
• [Tro11d]. This paper combines the matrix Chernoff inequality, Theorem 5.1.1, with the
argument from [GN] to obtain a matrix Chernoff bound for a sum of random positivesemideﬁnite matrices sampled without replacement from a ﬁxed collection. The result is
applied to a random matrix that plays a role in numerical linear algebra.
• [CT14]. This paper establishes logarithmic Sobolev inequalities for random matrices, and
it derives some matrix concentration inequalities as a consequence. The methods in the
paper have applications in quantum information theory, although the matrix concentration bounds are inferior to related results derived using Stein’s method.
Bounds with Intrinsic Dimension Parameters
The following works contain matrix concentration bounds that depend on a dimension parameter that may be smaller than the ambient dimension of the matrix.
• [Oli10b]. Oliveira shows how to develop a version of Rudelson’s inequality [Rud99] using
a variant of the argument of Ahlswede & Winter from [AW02]. Oliveira’s paper is notable
because the dimensional factor is controlled by the maximum rank of the random matrix,
rather than the ambient dimension.
• [MZ11]. This work contains a matrix Chernoff bound for a sum of independent positivesemideﬁnite random matrices where the dimensional dependence is controlled by the
maximum rank of the random matrix. The approach is, essentially, the same as the argument in Rudelson’s paper [Rud99]. The paper applies these results to study randomized
matrix multiplication algorithms.
• [HKZ12]. This paper describes a method for proving matrix concentration inequalities
where the ambient dimension is replaced by the intrinsic dimension of the matrix variance. The argument is based on an adaptation of the proof in [Tro11a]. The authors give
several examples in statistics and machine learning.
• [Min11]. This work presents a more reﬁned technique for obtaining matrix concentration
inequalities that depend on the intrinsic dimension, rather than the ambient dimension.
This paper motivated the results in Chapter 7.
The Method of Ahlswede & Winter
Next, we list some papers that use the ideas from the work [AW02] of Ahslwede & Winter to obtain
matrix concentration inequalities. In general, these results have suboptimal parameters, but
they played an important role in the development of this ﬁeld.
• [AW02]. The original paper of Ahlswede & Winter describes the matrix Laplace transform
method, along with a number of other foundational results. They show how to use the
Golden–Thompson inequality to bound the trace of the matrix mgf, and they use this technique to prove a matrix Chernoff inequality for sums of independent and identically distributed random variables. Their main application concerns quantum information theory.
• [CM08]. Christoﬁdes and Markström develop a Hoeffding-type inequality for sums of
bounded random matrices using the approach of Ahlswede & Winter. They apply this result to study random graphs.
• [Gro11]. Gross presents a matrix Bernstein inequality based on the method of Ahlswede &
Winter, and he uses it to study algorithms for matrix completion.
• [Rec11]. Recht describes a different version of the matrix Bernstein inequality, which also
follows from the technique of Ahlswede & Winter. His paper also concerns algorithms for
matrix completion.
Noncommutative Moment Inequalities
We conclude with an overview of some major works on bounds for the polynomial moments
of a noncommutative martingale. Sums of independent random matrices provide one concrete
example where these results apply. The results in this literature are as strong, or stronger, than
the exponential moment inequalities that we have described in these notes. Unfortunately, the
proofs are typically quite abstract and difﬁcult, and they do not usually lead to explicit constants.
Recently there has been some cross-fertilization between noncommutative probability and the
ﬁeld of matrix concentration inequalities.
Note that “noncommutative” is not synonymous with “matrix” in that there are noncommutative von Neumann algebras much stranger than the familiar algebra of ﬁnite-dimensional
matrices equipped with the operator norm.
MATRIX CONCENTRATION: RESOURCES
• [TJ74]. This classic paper gives a bound for the expected trace of an even power of a matrix
Rademacher series. These results are important, but they do not give the optimal bounds.
• [LP86]. This paper gives the ﬁrst noncommutative Khintchine inequality, a bound for the
expected trace of an even power of a matrix Rademacher series that depends on the matrix
• [LPP91]. This work establishes dual versions of the noncommutative Khintchine inequality.
• [Buc01, Buc05]. These papers prove optimal noncommutative Khintchine inequalities in
more general settings, and they obtain sharp constants.
• [JX03, JX08]. These papers establish noncommutative versions of the Burkholder–Davis–
Gundy inequality for martingales. They also give an application of these results to random
matrix theory.
• [JX05]. This paper contains an overview of noncommutative moment results, along with
information about the optimal rate of growth in the constants.
• [JZ13]. This paper describes a fully noncommutative version of the Bennett inequality. The
proof is based on the method of Ahlswede & Winter [AW02].
• [JZ12]. This work shows how to use Oliveira’s argument [Oli10a] to obtain some results for
fully noncommutative martingales.
• [MJC+14]. This work, described above, includes a section on matrix moment inequalities.
This paper contains what are probably the simplest available proofs of these results.
• [CGT12a]. The appendix of this paper contains a polynomial inequality for sums of independent random matrices.
Bibliography
E. Abbé, A. S. Bandeira, and G. Hall. Exact recovery in the stochastic block model. Available at
 June 2014.
N. Ailon and B. Chazelle. The fast Johnson–Lindenstrauss transform and approximate nearest
neighbors. SIAM J. Comput., 39(1):302–322, 2009.
S. Arora, E. Hazan, and S. Kale. A fast random sampling algorithm for sparsifying matrices. In
Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques,
pages 272–279, 2006.
D. Achlioptas, Z. Karnin, and E. Liberty. Near-optimal entrywise sampling for data matrices. In
Advances in Neural Information Processing Systems 26, 2013.
[ALMT14] D. Amelunxen, M. Lotz, M. B. McCoy, and J. A. Tropp. Living on the edge: A geometric theory
of phase transitions in convex optimization. Inform. Inference, 3(3):224–294, 2014. Preprint
available at 
D. Achlioptas and F. McSherry. Fast computation of low rank matrix approximations. In Proceedings of the Thirty-Third Annual ACM Symposium on Theory of Computing, pages 611–618
(electronic). ACM, New York, 2001.
D. Achlioptas and F. McSherry. Fast computation of low-rank matrix approximations. J. Assoc.
Comput. Mach., 54(2):Article 10, 2007. (electronic).
T. Ando. Concavity of certain maps on positive deﬁnite matrices and applications to Hadamard
products. Linear Algebra Appl., 26:203–241, 1979.
S. M. Ali and S. D. Silvey. A general class of coefﬁcients of divergence of one distribution from
another. J. Roy. Statist. Soc. Ser. B, 28:131–142, 1966.
N. Alon and J. H. Spencer. The probabilistic method. Wiley-Interscience Series in Discrete Mathematics and Optimization. Wiley-Interscience [John Wiley & Sons], New York, second edition,
2000. With an appendix on the life and work of Paul Erd˝os.
R. Ahlswede and A. Winter. Strong converse for identiﬁcation via quantum channels. IEEE
Trans. Inform. Theory, 48(3):569–579, Mar. 2002.
A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Trans. Inform. Theory, 39(3):930–945, May 1993.
S. Barman. An approximate version of Carathéodory’s theorem with applications to approximating Nash equilibria and dense bipartite subgraphs. Available at 
1406.2296, June 2014.
[BBLM05] S. Boucheron, O. Bousquet, G. Lugosi, and P. Massart. Moment inequalities for functions of
independent random variables. Ann. Probab., 33(2):514–560, 2005.
BIBLIOGRAPHY
W. Bryc, A. Dembo, and T. Jiang. Spectral measure of large random Hankel, Markov and Toeplitz
matrices. Ann. Probab., 34(1):1–38, 2006.
R. Bhatia. Matrix Analysis. Number 169 in Graduate Texts in Mathematics. Springer, Berlin,
R. Bhatia. Positive Deﬁnite Matrices. Princeton Univ. Press, Princeton, NJ, 2007.
S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities using the entropy method.
Ann. Probab., 31(3):1583–1614, 2003.
S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities. Oxford University Press,
Oxford, 2013. A nonasymptotic theory of independence, With a foreword by Michel Ledoux.
J. Bourgain. On Lipschitz embedding of ﬁnite metric spaces in Hilbert space. Israel J. Math.,
52(1-2):46–52, 1985.
L. M. Brègman. A relaxation method of ﬁnding a common point of convex sets and its application to the solution of problems in convex programming. ˘Z. Vyˇcisl. Mat. i Mat. Fiz., 7:620–631,
J. Bendat and S. Sherman. Monotone and convex operator functions. Trans. Amer. Math. Soc.,
79:58–71, 1955.
Z. Bai and J. W. Silverstein. Spectral analysis of large dimensional random matrices. Springer
Series in Statistics. Springer, New York, second edition, 2010.
J. Bourgain and L. Tzafriri. Invertibility of “large” submatrices with applications to the geometry
of Banach spaces and harmonic analysis. Israel J. Math., 57(2):137–224, 1987.
J. Bourgain and L. Tzafriri. On a problem of Kadison and Singer. J. Reine Angew. Math., 420:1–43,
A. Ben-Tal and A. Nemirovski. Lectures on modern convex optimization. MPS/SIAM Series on
Optimization. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA; Mathematical Programming Society (MPS), Philadelphia, PA, 2001. Analysis, algorithms, and engineering applications.
A. Buchholz. Operator Khintchine inequality in non-commutative probability. Math. Ann.,
319:1–16, 2001.
A. Buchholz. Optimal constants in Khintchine-type inequalities for Fermions, Rademachers
and q-Gaussian operators. Bull. Pol. Acad. Sci. Math., 53(3):315–321, 2005.
A. S. Bandeira and R. Van Handel. Sharp nonasymptotic bounds on the norm of random matrices with independent entries. Available at Aug. 2014.
P. Bühlmann and S. van de Geer. Statistics for high-dimensional data. Springer Series in Statistics. Springer, Heidelberg, 2011. Methods, theory and applications.
Z. D. Bai and Y. Q. Yin. Limit of the smallest eigenvalue of a large-dimensional sample covariance matrix. Ann. Probab., 21(3):1275–1294, 1993.
Inequalities of Bernstein–Jackson-type and the degree of compactness in Banach
spaces. Ann. Inst. Fourier (Grenoble), 35(3):79–118, 1985.
E. Carlen. Trace inequalities and quantum entropy: an introductory course. In Entropy and the
quantum, volume 529 of Contemp. Math., pages 73–140. Amer. Math. Soc., Providence, RI, 2010.
BIBLIOGRAPHY
S. Chrétien and S. Darses. Invertibility of random submatrices via tail-decoupling and a matrix
Chernoff inequality. Statist. Probab. Lett., 82(7):1479–1487, 2012.
R. Y. Chen, A. Gittens, and J. A. Tropp. The masked sample covariance estimator: An analysis
using matrix concentration inequalities. Inform. Inference, 1(1), 2012. doi:10.1093/imaiai/
R. Y. Chen, A. Gittens, and J. A. Tropp. The masked sample covariance estimator: An analysis
using matrix concentration inequalities. ACM Report 2012-01, California Inst. Tech., Pasadena,
CA, Feb. 2012.
S. Chatterjee. Concentration Inequalities with Exchangeable Pairs. ProQuest LLC, Ann Arbor,
MI, 2005. Thesis (Ph.D.)–Stanford University.
S. Chatterjee. Stein’s method for concentration inequalities. Probab. Theory Related Fields,
138:305–321, 2007.
H. Chernoff. A measure of asymptotic efﬁciency for tests of a hypothesis based on the sum of
observations. Ann. Math. Statistics, 23:493–507, 1952.
E. A. Carlen and E. H. Lieb. A Minkowski type trace inequality and strong subadditivity of quantum entropy. II. Convexity and concavity. Lett. Math. Phys., 83(2):107–126, 2008.
D. Cristoﬁdes and K. Markström. Expansion properties of random Cayley graphs and vertex
transitive graphs via matrix martingales. Random Structures Algs., 32(8):88–100, 2008.
[CRPW12] V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The Convex Geometry of Linear
Inverse Problems. Found. Comput. Math., 12(6):805–849, 2012.
E. J. Candès, J. Romberg, and T. Tao. Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information. IEEE Trans. Inform. Theory, 52(2):489–509,
A. Connes and E. Størmer. Entropy for automorphisms of I I1 von Neumann algebras. Acta
Math., 134(3-4):289–306, 1975.
I. Csiszár. Information-type measures of difference of probability distributions and indirect
observations. Studia Sci. Math. Hungar., 2:299–318, 1967.
R. Y. Chen and J. A. Tropp. Subadditivity of matrix ϕ-entropy and concentration of random
matrices. Electron. J. Probab., 19(27):1–30, 2014.
K. L. Clarkson and D. P. Woodruff. Low rank approximation and regression in input sparsity
time. In STOC’13—Proceedings of the 2013 ACM Symposium on Theory of Computing, pages
81–90. ACM, New York, 2013.
A. d’Asprémont. Subsampling algorithms for semideﬁnite programming. Stoch. Syst., 1(2):274–
305, 2011.
P. Drineas, A. Frieze, R. Kannan, S. Vempala, and V. Vinay. Clustering in large graphs and matrices. In Proceedings of the Tenth Annual ACM-SIAM Symposium on Discrete Algorithms , pages 291–299. ACM, New York, 1999.
P. Drineas and R. Kannan. Fast Monte Carlo algorithms for approximate matrix multiplication.
In Proc. 42nd IEEE Symp. Foundations of Computer Science (FOCS), pages 452–259, 2001.
P. Drineas, R. Kannan, and M. W. Mahoney. Fast Monte Carlo algorithms for matrices. I. Approximating matrix multiplication. SIAM J. Comput., 36(1):132–157, 2006.
BIBLIOGRAPHY
P. Drineas and M. Mahoney. On the Nyström method for approximating a Gram matrix for
improved kernel-based learning. J. Mach. Learn. Res., 6:2153–2175, 2005.
D. L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52(4):1289–1306, Apr. 2006.
K. R. Davidson and S. J. Szarek. Local operator theory, random matrices, and Banach spaces. In
W. B. Johnson and J. Lindenstrauss, editors, Handbook of Banach Space Geometry, pages 317–
366. Elsevier, Amsterdam, 2002.
I. S. Dhillon and J. A. Tropp. Matrix nearness problems with Bregman divergences. SIAM J.
Matrix Anal. Appl., 29(4):1120–1146, 2007.
P. Drineas and A. Zouzias. A note on element-wise matrix sparsiﬁcation via a matrix-valued
Bernstein inequality. Inform. Process. Lett., 111(8):385–389, 2011.
E. G. Effros. A matrix convexity approach to some celebrated quantum inequalities. Proc. Natl.
Acad. Sci. USA, 106(4):1006–1008, Jan. 2009.
A. Ebadian, I. Nikoufar, and M. E. Gordji. Perspectives of matrix convex functions. Proc. Natl.
Acad. Sci. USA, 108(18):7313–7314, 2011.
H. Epstein. Remarks on two theorems of E. Lieb. Comm. Math. Phys., 31:317–325, 1973.
P. Erd˝os and A. Rényi. On the evolution of random graphs. Magyar Tud. Akad. Mat. Kutató Int.
Közl., 5:17–61, 1960.
W. Feller. An introduction to probability theory and its applications. Vol. I. Third edition. John
Wiley & Sons, Inc., New York-London-Sydney, 1968.
W. Feller. An introduction to probability theory and its applications. Vol. II. Second edition. John
Wiley & Sons, Inc., New York-London-Sydney, 1971.
X. Fernique. Regularité des trajectoires des fonctions aléatoires gaussiennes. In École d’Été
de Probabilités de Saint-Flour, IV-1974, pages 1–96. Lecture Notes in Math., Vol. 480. Springer,
Berlin, 1975.
A. Frieze, R. Kannan, and S. Vempala. Fast Monte Carlo algorithms for ﬁnding low-rank approximations. In Proc. 39th Ann. IEEE Symp. Foundations of Computer Science (FOCS), pages
370–378, 1998.
P. J. Forrester.
Log-gases and random matrices, volume 34 of London Mathematical Society
Monographs Series. Princeton University Press, Princeton, NJ, 2010.
S. Foucart and H. Rauhut. A mathematical introduction to compressive sensing. Applied and
Numerical Harmonic Analysis. Birkhäuser/Springer, New York, 2013.
D. A. Freedman. On tail probabilities for martingales. Ann. Probab., 3(1):100–118, Feb. 1975.
A. C. Gilbert, S. Guha, P. Indyk, S. Muthukrishnan, and M. Strauss. Near-optimal sparse Fourier
representations via sampling. In Proceedings of the Thirty-Fourth Annual ACM Symposium on
Theory of Computing, pages 152–161. ACM, New York, 2002.
A. Gittens and M. Mahoney. Revisiting the Nyström method for improved large-scale machine
learning. J. Mach. Learn. Res., 2014. To appear. Preprint available at 
1303.1849.
D. Gross and V. Nesme. Note on sampling without replacing from a ﬁnite collection of matrices.
Available at 
BIBLIOGRAPHY
Y. Gordon. Some inequalities for Gaussian processes and applications. Israel J. Math., 50(4):265–
289, 1985.
C. Godsil and G. Royle. Algebraic Graph Theory. Number 207 in Graduate Texts in Mathematics.
Springer, 2001.
J. F. Grcar. John von Neumann’s analysis of Gaussian elimination and the origins of modern
numerical analysis. SIAM Rev., 53(4):607–682, 2011.
D. Gross. Recovering low-rank matrices from few coefﬁcients in any basis. IEEE Trans. Inform.
Theory, 57(3):1548–1566, Mar. 2011.
G. R. Grimmett and D. R. Stirzaker. Probability and random processes. Oxford University Press,
New York, third edition, 2001.
A. Gittens and J. A. Tropp. Error bounds for random matrix approximation schemes. ACM
Report 2014-01, California Inst. Tech., Nov. 2009. Available at 
A. Gittens and J. A. Tropp. Tail bounds for all eigenvalues of a sum of random matrices. ACM
Report 2014-02, California Inst. Tech., 2014. Available at 
H. H. Goldstine and J. von Neumann. Numerical inverting of matrices of high order. II. Proc.
Amer. Math. Soc., 2:188–202, 1951.
M. B. Hastings. Superadditivity of communication complexity using entangled inputs. Nature
Phys., 5:255–257, 2009.
N. J. Higham. Functions of Matrices: Theory and Computation. Society for Industrial and Applied Mathematics, Philadelphia, PA, 2008.
R. A. Horn and C. R. Johnson. Topics in matrix analysis. Cambridge University Press, Cambridge,
1994. Corrected reprint of the 1991 original.
R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge Univ. Press, 2nd edition, 2013.
D. Hsu, S. M. Kakade, and T. Zhang. Tail inequalities for sums of random matrices that depend
on the intrinsic dimension. Electron. Commun. Probab., 17:no. 14, 13, 2012.
P. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social Networks, 5(2):109–137, 1983.
N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions. SIAM Rev., 53(2):217–288, June
F. Hansen and G. K. Pedersen. Jensen’s inequality for operators and Löwner’s theorem. Math.
Ann., 258(3):229–241, 1982.
F. Hansen and G. K. Pedersen. Jensen’s operator inequality. Bull. London Math. Soc., 35(4):553–
564, 2003.
F. Hiai and D. Petz. Introduction to Matrix Analysis and Applications. Springer, Feb. 2014.
P. Hayden and A. Winter. Counterexamples to the maximal p-norm multiplicity conjecture for
all p > 1. Comm. Math. Phys., 284(1):263–280, 2008.
[HXGD14] R. Hamid, Y. Xiao, A. Gittens, and D. DeCoste. Compact random feature maps. In Proc. 31st Intl.
Conf. Machine Learning, Beijing, July 2014.
BIBLIOGRAPHY
W. B. Johnson and J. Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. In
Conference in modern analysis and probability , volume 26 of Contemp. Math., pages 189–206. Amer. Math. Soc., Providence, RI, 1984.
M. Junge and Q. Xu.
Noncommutative Burkholder/Rosenthal inequalities.
Ann. Probab.,
31(2):948–995, 2003.
M. Junge and Q. Xu. On the best constants in some non-commutative martingale inequalities.
Bull. London Math. Soc., 37:243–253, 2005.
M. Junge and Q. Xu. Noncommutative Burkholder/Rosenthal inequalities II: Applications. Israel
J. Math., 167:227–282, 2008.
M. Junge and Q. Zeng. Noncommutative martingale deviation and Poincaré type inequalities
with applications. Available at Nov. 2012.
M. Junge and Q. Zeng. Noncommutative Bennett and Rosenthal inequalities. Ann. Probab.,
41(6):4287–4316, 2013.
F. Kubo and T. Ando. Means of positive linear operators. Math. Ann., 246(3):205–224, 1979/80.
A. Kundu and P. Drineas. A note on randomized element-wise matrix sparsiﬁcation. Available
at Apr. 2014.
T. Kemp. Math 247a: Introduction to random matrix theory. Available at 
ucsd.edu/~tkemp/247A/247A.Notes.pdf, 2013.
P. Kar and H. Karnick. Random feature maps for dot product kernels. In Proc. 15th Intl. Conf.
Artiﬁcial Intelligence and Statistics (AISTATS), 2012.
V. Koltchinskii and S. Mendelson. Bounding the smallest singular value of a random matrix
without concentration. Available at Dec. 2013.
V. Koltchinskii. Oracle inequalities in empirical risk minimization and sparse recovery problems,
volume 2033 of Lecture Notes in Mathematics. Springer, Heidelberg, 2011. Lectures from the
38th Probability Summer School held in Saint-Flour, 2008, École d’Été de Probabilités de Saint-
Flour. [Saint-Flour Probability Summer School].
F. Kraus. Über konvexe Matrixfunktionen. Math. Z., 41(1):18–42, 1936.
B. Kashin and L. Tzafriri. Some remarks on coordinate restriction of operators to coordinate
subspaces. Insitute of Mathematics Preprint 12, Hebrew University, Jerusalem, 1993–1994.
R. Latała. Some estimates of norms of random matrices. Proc. Amer. Math. Soc., 133(5):1273–
1282, 2005.
W. S. Lee, P. L. Bartlett, and R. C. Williamson. Efﬁcient agnostic learning of neural networks with
bounded fan-in. IEEE Trans. Inform. Theory, 42(6):2118–2132, Nov. 1996.
E. H. Lieb.
Convex trace functions and the Wigner–Yanase–Dyson conjecture.
Adv. Math.,
11:267–288, 1973.
G. Lindblad. Entropy, information and quantum measurements. Comm. Math. Phys., 33:305–
322, 1973.
N. Linial, E. London, and Y. Rabinovich. The geometry of graphs and some of its algorithmic
applications. Combinatorica, 15(2):215–245, 1995.
K. Löwner. Über monotone Matrixfunktionen. Math. Z., 38(1):177–216, 1934.
BIBLIOGRAPHY
F. Lust-Piquard. Inégalités de Khintchine dans Cp (1 < p < ∞). C. R. Math. Acad. Sci. Paris,
303(7):289–292, 1986.
F. Lust-Piquard and G. Pisier. Noncommutative Khintchine and Paley inequalities. Ark. Mat.,
29(2):241–260, 1991.
[LPSS+14] D. Lopez-Paz, S. Sra, A. Smola, Z. Ghahramani, and B. Schölkopf. Randomized nonlinear component analysis. In Proc. 31st Intl. Conf. Machine Learning, Beijing, July 2014.
E. H. Lieb and R. Seiringer. Stronger subadditivity of entropy. Phys. Rev. A, 71:062329–1–9, 2005.
M. Ledoux and M. Talagrand.
Probability in Banach Spaces: Isoperimetry and Processes.
Springer, Berlin, 1991.
G. Lugosi. Concentration-of-measure inequalities. Available at 
~lugosi/anu.pdf, 2009.
M. Mahoney. Randomized algorithms for matrices and data. Found. Trends Mach. Learning,
3(2):123–224, Feb. 2011.
A. Maurer. A bound on the deviation probability for sums of non-negative random variables.
JIPAM. J. Inequal. Pure Appl. Math., 4(1):Article 15, 6 pp. (electronic), 2003.
M. W. Meckes. On the spectral norm of a random Toeplitz matrix. Electron. Comm. Probab.,
12:315–325 (electronic), 2007.
M. L. Mehta. Random matrices, volume 142 of Pure and Applied Mathematics (Amsterdam).
Elsevier/Academic Press, Amsterdam, third edition, 2004.
V. D. Milman.
A new proof of A. Dvoretzky’s theorem on cross-sections of convex bodies.
Funkcional. Anal. i Priložen., 5(4):28–37, 1971.
S. Minsker. Some extensions of Bernstein’s inequality for self-adjoint operators. Available at
 Nov. 2011.
L. Mackey, M. I. Jordan, R. Y. Chen, B. Farrell, and J. A. Tropp. Matrix concentration inequalities
via the method of exchangable pairs. Ann. Probab., 42(3):906–945, 2014. Preprint available at
 
K. V. Mardia, J. T. Kent, and J. M. Bibby. Multivariate analysis. Academic Press [Harcourt Brace
Jovanovich, Publishers], London-New York-Toronto, Ont., 1979. Probability and Mathematical
Statistics: A Series of Monographs and Textbooks.
H. L. Montgomery. The pair correlation of zeros of the zeta function. In Analytic number theory
 , pages 181–193. Amer.
Math. Soc., Providence, R.I., 1973.
V. A. Marˇcenko and L. A. Pastur. Distribution of eigenvalues in certain sets of random matrices.
Mat. Sb. (N.S.), 72 (114):507–536, 1967.
R. Motwani and P. Raghavan. Randomized Algorithms. Cambridge Univ. Press, Cambridge, 1995.
A. Marcus, D. A. Spielman, and N. Srivastava. Interlacing families II: Mixed characteristic polynomials and the Kadison–Singer problem. Ann. Math., June 2014. To appear. Preprint available
at 
M. B. McCoy and J. A. Tropp. The achievable performance of convex demixing. Available at
 Sep. 2013.
BIBLIOGRAPHY
M. McCoy and J. A. Tropp. Sharp recovery thresholds for convex deconvolution, with applications. Found. Comput. Math., Apr. 2014. Preprint available at 
R. J. Muirhead. Aspects of multivariate statistical theory. John Wiley & Sons Inc., New York, 1982.
Wiley Series in Probability and Mathematical Statistics.
A. Magen and A. Zouzias. Low rank matrix-valued Chernoff bounds and approximate matrix
multiplication. In Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete
Algorithms, pages 1422–1436. SIAM, Philadelphia, PA, 2011.
A. Nemirovski. Sums of random symmetric matrices and quadratic optimization under orthogonality constraints. Math. Prog. Ser. B, 109:283–317, 2007.
A. Naor, O. Regev, and T. Vidick. Efﬁcient rounding for the noncommutative Grothendieck inequality (extended abstract). In STOC’13—Proceedings of the 2013 ACM Symposium on Theory
of Computing, pages 71–80. ACM, New York, 2013.
A. Nica and R. Speicher. Lectures on the combinatorics of free probability, volume 335 of London
Mathematical Society Lecture Note Series. Cambridge University Press, Cambridge, 2006.
D. Needell and J. A. Tropp. Paved with good intentions: analysis of a randomized block Kaczmarz method. Linear Algebra Appl., 441:199–221, 2014.
R. I. Oliveira. Concentration of the adjacency matrix and of the Laplacian in random graphs
with independent edges. Available at Feb. 2010.
R. I. Oliveira. Sums of random Hermitian matrices and an inequality by Rudelson. Electron.
Commun. Probab., 15:203–212, 2010.
R. I. Oliveira. The spectrum of random k-lifts of large graphs (with possibly large k). J. Combinatorics, 1(3/4):285–306, 2011.
R. I. Oliveira. The lower tail of random quadratic forms, with applications to ordinary least
squares and restricted eigenvalue properties.
Available at 
2903, Dec. 2013.
B. N. Parlett. The symmetric eigenvalue problem, volume 20 of Classics in Applied Mathematics. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA, 1998. Corrected
reprint of the 1980 original.
D. Petz. Quasi-entropies for ﬁnite quantum systems. Rep. Math. Phys., 23(1):57–65, 1986.
D. Petz. A survey of certain trace inequalities. In Functional analysis and operator theory , volume 30 of Banach Center Publ., pages 287–298. Polish Acad. Sci., Warsaw, 1994.
D. Petz. From f -divergence to quantum quasi-entropies and their use. Entropy, 12(3):304–325,
D. Petz. Matrix analysis with some applications. Available at bolyai.cs.elte.hu/~petz/
matrixbme.pdf, Feb. 2011.
I. Pinelis. Optimum bounds for the distributions of martingales in Banach spaces. Ann. Probab.,
22(4):1679–1706, 1994.
G. Pisier. Remarques sur un résultat non publié de B. Maurey. In Seminar on Functional Analysis, 1980–1981, pages Exp. No. V, 13. École Polytech., Palaiseau, 1981.
G. Pisier. The volume of convex bodies and Banach space geometry, volume 94 of Cambridge
Tracts in Mathematics. Cambridge University Press, Cambridge, 1989.
BIBLIOGRAPHY
D. Paulin, L. Mackey, and J. A. Tropp. Efron–Stein inequalities for random matrices. Available at
 Aug. 2014.
W. Pusz and S. L. Woronowicz. Functional calculus for sesquilinear forms and the puriﬁcation
map. Rep. Mathematical Phys., 8(2):159–170, 1975.
B. Recht. A simpler approach to matrix completion. J. Mach. Learn. Res., 12:3413–3430, 2011.
H. P. Rosenthal. On subspaces of Lp (p > 2) spanned by sequences of independent random
variables. Israel J. Math., 8:273–303, 1970.
A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in Neural
Information Processing Systems 20, pages 1177–1184, Vancouver, Dec. 2007.
A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: replacing minimization with
randomization in learning. In Advances in Neural Information Processing Systems 21, 2008.
S. Riemer and C. Schütt.
On the expectation of the norm of random matrices with nonidentically distributed entries. Electron. J. Probab., 18:no. 29, 13, 2013.
M. Rudelson. Random vectors in the isotropic position. J. Funct. Anal., 164:60–72, 1999.
M. B. Ruskai. Inequalities for quantum entropy: A review with conditions for equality. J. Math.
Phys., 43(9):4358–4375, Sep. 2002.
M. B. Ruskai. Erratum: Inequalities for quantum entropy: A review with conditions for equality
[J. Math. Phys. 43, 4358 ]. J. Math. Phys., 46(1):0199101, 2005.
M. Rudelson and R. Vershynin. Sparse reconstruction by convex relaxation: Fourier and Gaussian measurements. In Proc. 40th Ann. Conf. Information Sciences and Systems (CISS), Mar.
M. Rudelson and R. Vershynin. Sampling from large matrices: An approach through geometric
functional analysis. J. Assoc. Comput. Mach., 54(4):Article 21, 19 pp., Jul. 2007. (electronic).
M. D. Reid and R. C. Williamson. Information, divergence and risk for binary experiments. J.
Mach. Learn. Res., 12:731–817, 2011.
T. Sarlós. Improved approximation algorithms for large matrices via random projections. In
Proc. 47th Ann. IEEE Symp. Foundations of Computer Science (FOCS), pages 143–152, 2006.
Y. Seginer. The expected norm of random matrices. Combin. Probab. Comput., 9:149–166, 2000.
A. N. Shiryaev. Probability, volume 95 of Graduate Texts in Mathematics. Springer-Verlag, New
York, second edition, 1996. Translated from the ﬁrst Russian edition by R. P. Boas.
A. M.-C. So. Moment inequalities for sums of random matrices and their applications in optimization. Math. Prog. Ser. A, Dec. 2009. (electronic).
B. Schölkopf and S. Smola. Learning with Kernels. MIT Press, 1998.
S. Shalev-Shwartz and N. Srebro.
Low ℓ1-norm and guarantees on sparsiﬁability.
ICML/COLT/UAI Sparse Optimization and Variable Selection Workshop, July 2008.
A. Sankar, D. A. Spielman, and S.-H. Teng. Smoothed analysis of the condition numbers and
growth factors of matrices. SIAM J. Matrix Anal. Appl., 28(2):446–476, 2006.
D. A. Spielman and S.-H. Teng. Nearly-linear time algorithms for graph partitioning, graph sparsiﬁcation, and solving linear systems. In Proceedings of the 36th Annual ACM Symposium on
Theory of Computing, pages 81–90 (electronic), New York, 2004. ACM.
BIBLIOGRAPHY
C. Stein. A bound for the error in the normal approximation to the distribution of a sum of
dependent random variables. In Proc. 6th Berkeley Symp. Math. Statist. Probab., Berkeley, 1972.
Univ. California Press.
A. Sen and B. Virág. The top eigenvalue of the random Toeplitz matrix and the sine kernel. Ann.
Probab., 41(6):4050–4079, 2013.
T. Tao. Topics in random matrix theory, volume 132 of Graduate Studies in Mathematics. American Mathematical Society, Providence, RI, 2012.
W. Thirring. Quantum mathematical physics. Springer-Verlag, Berlin, second edition, 2002.
Atoms, molecules and large systems, Translated from the 1979 and 1980 German originals by
Evans M. Harrell II.
N. Tomczak-Jaegermann. The moduli of smoothness and convexity and the Rademacher averages of trace classes Sp(1 ≤p < ∞). Studia Math., 50:163–182, 1974.
J. A. Tropp. On the conditioning of random subdictionaries. Appl. Comput. Harmon. Anal.,
25:1–24, 2008.
J. A. Tropp. Norms of random submatrices and sparse approximation. C. R. Math. Acad. Sci.
Paris, 346(23-24):1271–1274, 2008.
J. A. Tropp.
The random paving property for uniformly bounded matrices.
Studia Math.,
185(1):67–82, 2008.
J. A. Tropp. Freedman’s inequality for matrix martingales. Electron. Commun. Probab., 16:262–
270, 2011.
J. A. Tropp. User-friendly tail bounds for matrix martingales. ACM Report 2011-01, California
Inst. Tech., Pasadena, CA, Jan. 2011.
J. A. Tropp. User-friendly tail bounds for sums of random matrices. Found. Comput. Math.,
August 2011.
J. A. Tropp. Improved analysis of the subsampled randomized Hadamard transform. Adv. Adapt.
Data Anal., 3(1-2):115–126, 2011.
J. A. Tropp. From joint convexity of quantum relative entropy to a concavity theorem of Lieb.
Proc. Amer. Math. Soc., 140(5):1757–1760, 2012.
J. A. Tropp. Convex recovery of a structured signal from independent random measurements.
In Sampling Theory, a Renaissance. Birkhäuser Verlag, 2014. To appear. Available at http://
arXiv.org/abs/1405.1102.
A. M. Tulino and S. Verdú. Random matrix theory and wireless communications. Number 1(1)
in Foundations and Trends in Communications and Information Theory. Now Publ., 2004.
R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Compressed
sensing, pages 210–268. Cambridge Univ. Press, Cambridge, 2012.
J. von Neumann and H. H. Goldstine. Numerical inverting of matrices of high order. Bull. Amer.
Math. Soc., 53:1021–1099, 1947.
E. P. Wigner. Characteristic vectors of bordered matrices with inﬁnite dimensions. Ann. of Math.
(2), 62:548–564, 1955.
J. Wishart. The generalised product moment distribution in samples from a multivariate normal
population. Biometrika, 20A(1–2):32–52, 1928.