Spectral Collaborative Filtering
Department of Computer Science
University of Illinois at Chicago
 
Chun-Ta Lu
Department of Computer Science
University of Illinois at Chicago
 
Department of Computer Science
Peking University
Beijing, China
 
Jiawei Zhang
IFM Lab, Department of Computer
Florida State University
 
Philip S. Yu
Department of Computer Science
University of Illinois at Chicago
Institute for Data Science
Tsinghua University
Beijing, China
 
Despite the popularity of Collaborative Filtering (CF), CF-based
methods are haunted by the cold-start problem, which has a significantly negative impact on users’ experiences with Recommender
Systems (RS). In this paper, to overcome the aforementioned drawback, we ﬁrst formulate the relationships between users and items
as a bipartite graph. Then, we propose a new spectral convolution
operation directly performing in the spectral domain, where not
only the proximity information of a graph but also the connectivity information hidden in the graph are revealed. With the proposed spectral convolution operation, we build a deep recommendation model called Spectral Collaborative Filtering (SpectralCF).
Beneﬁting from the rich information of connectivity existing in the
spectral domain, SpectralCF is capable of discovering deep connections between users and items and therefore, alleviates the coldstart problem for CF. To the best of our knowledge, SpectralCF
is the ﬁrst CF-based method directly learning from the spectral
domains of user-item bipartite graphs. We apply our method on
several standard datasets. It is shown that SpectralCF signiﬁcantly
outperforms state-of-the-art models. Code and data are available
at 
CCS CONCEPTS
• Information systems →Recommender systems; • Computing methodologies →Neural networks;
Recommender Systems, Spectrum, Collaborative Filtering
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from .
RecSys ’18, October 2–7, 2018, Vancouver, BC, Canada
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5901-6/18/10...$15.00
 
Figure 1: A toy example of a user-itembipartite graph B with
edges representing observed user-item interactions. Red circles and green rectangles denote users and items, respectively.
ACM Reference Format:
Lei Zheng, Chun-Ta Lu, Fei Jiang, Jiawei Zhang, and Philip S. Yu. 2018. Spectral Collaborative Filtering. In Twelfth ACM Conference on Recommender
Systems (RecSys ’18), October 2–7, 2018, Vancouver, BC, Canada. ACM, New
York, NY, USA, 10 pages. 
INTRODUCTION
The eﬀectiveness of recommender systems (RS) often relies on how
well users’ interests or preferences can be understood and interactions between users and items can be modeled. Collaborative Filtering (CF) is one of the widely used and prominent techniques
for RS. The underlying assumption of the CF approach is that if
a user u1 shares a common item with another user u2, u1 is also
likely to be interested in other items liked by u2. Although CF has
been successfully applied to many recommendation applications,
the cold-start problem is considered as one of its major challenges
 . The problem arises when a user interacted with a very small
number of items. Consequently, the user shares few items with
other users, and eﬀectively recommending for the user becomes a
challenging task for RS.
If we formulate the relationships between users and items as a
bipartite graph1, we argue that the connectivity information of the
graph can play an important role for tackling the cold-start problem. For example, let us see a bipartite graph B in Figure 1. A coldstart user u1 only interacts with item i1. Since u1 shares i1 with
1In this paper, we use the terminology "graph" to refer to the graph/network structure
of data and "network" for the architecture of machine learning models.
RecSys ’18, October 2–7, 2018, Vancouver, BC, Canada
Lei Zheng, Chun-Ta Lu, Fei Jiang, Jiawei Zhang, and Philip S. Yu
user u2 and user u3, as a result, three items (i2, i3 and i4) connected
with u2 or u3 can all be recommended to u1 by a CF-based model.
However, a natural and important question arises: which one in
the three items is the most reliable recommendation for u1? The
key to answer the question lies in the user-item connectivity information. In fact, if we take a look at the connections of the graph, it
is clear that there is only one path existing between u1 and i2 (or
i3), while two paths connect u1 to i4. Thus, compared with i2 and
i3, obviously, i4 is a more reliable recommendation for u1.
However, existing CF-based methods, including model-based and
memory-based approaches, often suﬀer from the diﬃculty of modeling the connectivity information. Previous model-based approaches,
such as Matrix Factorization (MF) , are usually designed to
approximate the direct connections (or proximities). However, indirect connectivity information hidden in the graph structures is
rarely capturedby traditional model-based approaches. For instance,
it is formidable for them to model the number of paths between u1
andi4 in Figure 1. Whereas a number of memory-based approaches
 is introduced to model the connectivity information, these
methods often rely on pre-deﬁned similarity functions. However,
in the real world, deﬁning an appropriate similarity function suitable for diverse application cases is never an easy task.
Spectral graph theory studies connections between combinatorial properties of a graph and the eigenvalues of matrices associated to the graph, such as the laplacian matrix (see Deﬁnition
2.4 in Section 2). In general, the spectrum of a graph focuses on the
connectivity of the graph, instead of the geometrical proximity. To
see how does the spectral domain come to help for recommendations and better understand the advantages of viewing a user-item
bipartite graph in the spectral perspective, let us revisit the toy example shown in Figure 1. For the bipartite graph B, we visually
plot its vertices in one speciﬁc frequency domain. Although vertices do not come with coordinates, a popular way to draw them in
a space is to use eigenvectors of a laplacian matrix associated with
the graph to supply coordinates . Figure 2 shows that, compared with i2 and i3, i4 becomes closer to u1 in the space2. Thus,
when transformed into the frequency domain, i4 is revealed to be
a more suitable choice than i2 or i3 for u1. The underlying reason
is that the connectivity information of the graph has been uncovered in the frequency domain, where the relationships between
vertices depend on not only their proximity but also connectivity.
Thus, exploiting the spectrum of a graph can help better explore
and identify the items to be recommended.
Inspired by the recent progress in node/graph classiﬁcation methods, we propose a spectral graph theory based method to
leverage the broad information existing in the spectral domain to
overcome the aforementioned drawbacks and challenges. Speciﬁcally, to conquer the diﬃculties (see Section 3.3) of directly learning from the spectral domain for recommendations, we ﬁrst present
a new spectral convolution operation (see Eq. (10)), which is approximated by a polynomial to dynamically amplify or attenuate
each frequency domain. Then, we introduce a deep recommendation model, named Spectral Collaborative Filtering (SpectralCF),
2In spectral graph theory, smaller (or larger) eigenvalues of the associated laplacian
matrix corresponds to lower- (or higher-) frequency domains. In Figure 1, we plot
each vertex j at the point (µ1(j), µ2(j)), where µl (j) indicates the jth value of the lth
eigenvector of the laplacian matrix L.
Figure 2: Vertices of the bipartite graph in Figure 1 are plotted in a frequency domain. Note that the vertices not shown
above are omitted for simplicity.
built by multiple proposed spectral convolution layers. SpectralCF
directly performs collaborative ﬁltering in the spectral domain.
The key contributions of this work can be summarized as follows:
• Novelty: To the best of our knowledge, it is the ﬁrst CFbased method directly learning from the spectral domains
of user-item bipartite graphs.
• A deep recommendation model: We propose a new spectral convolution operation performing in the spectral domain. Stacked by multiple layers of the proposed spectral
convolution operation, a deep recommendation model, named
Spectral Collaborative Filtering (SpectralCF), is introduced.
• Strong Performance: In the experiments, SpectralCF outperforms state-of-the-art comparative models. It is shown
that SpectralCF eﬀectively utilizes the rich information of
connectivity existing in the spectral domain to ease the coldstart problem.
The rest of the paper is organized as follows. In Section 2, we
provide preliminary concepts. Section 3 describes SpectralCF in detail. Experiments are presented in Section 4 to analyze SpectralCF
and demonstrate its eﬀectiveness compared with state-of-the-art
techniques for RS. In Section 5, we give a short review of the works
related to our study. Finally, conclusions are presented in Section
DEFINITIONS AND PRELIMINARIES
In this section, we present the background and preliminaries of this
study. Throughout the paper, we denote scalars by either lowercase
or uppercase letters, vectors by boldfaced lowercase letters, and
matrices by boldfaced uppercase letters. Unless otherwise speci-
ﬁed, all vectors are considered to be column vectors. Let I denote
an identity matrix, and 1 and 0 denote matrices of ones and zeros,
respectively. In addition, we deﬁne the following deﬁnitions in this
Deﬁnition 2.1. (Bipartite Graph). A bipartite user-item graph with
N vertices and E edgesfor recommendationsis deﬁnedas B = {U, I, E},
where U and I are two disjoint vertex sets of users and items. Every
edge e ∈E has the form e = (u,i) where u ∈U and i ∈I and
denotes that user u has interacted with item i in the training set.
Spectral Collaborative Filtering
RecSys ’18, October 2–7, 2018, Vancouver, BC, Canada
Deﬁnition 2.2. (Implicit Feedback Matrix). An implicit feedback
matrix R is a |U| × |I| matrix deﬁned as following:
if (ur,ij) interaction is observed,
otherwise.
Deﬁnition 2.3. (Adjacent Matrix). For the bipartite graph B, its
corresponding adjacent matrix A can be deﬁned as:
where A is an N × N matrix.
Deﬁnition 2.4. (Laplacian Matrix). The random walk laplacian
matrix L is deﬁned as L = I −D−1A, where I is the N × N identity matrix and D is the N × N diagonal degree matrix deﬁned as
This paper focuses on the recommendation problem with implicit feedbacks, where we only observe whether a person has viewed/liked/clicked
an item and do not observe explicit ratings. Let I+
denote the set
of all items liked by user i and I−
i denote the remaining items. We
deﬁne the recommendation problem which we study in this paper
as the following:
Deﬁnition 2.5. (Problem Deﬁnition). Given a user set U and an
item set I, for each user u ∈U who has liked/clicked/viewed an
item set I+
u ⊆I, we aim to recommend a ranked list of items from
u that are of interests to the user.
PROPOSED MODEL
In this section, we ﬁrst describe the process of performing a graph
fourier transform on a bipartite graph B for recommendations. Then
we propose to place a novel spectral convolution ﬁlter on vertices
(users and items) of the bipartite graph to dynamically ﬁlter the
contributions of each frequency component in the spectral domain.
Later, a polynomial approximation is employed to overcome the
shortcomings of the proposed convolution operation. Finally, with
the approximate convolution operation, we introduce our ﬁnal recommender system, named Spectral Collaborative Filtering, stacked
by multiple spectral convolution layers.
Graph Fourier Transform
Deﬁnition 3.1. (Graph Signal). Given any graph G = {V, E},
where V and E are a vertex and an edge set, respectively, a graph
signal is deﬁned as a state vector x ∈R |V|×1 over all vertices in the
graph, where xj is the jth value of x observed at the jth vertex of G.
The classical fourier transform is deﬁned as an expansion of a
function f in terms of the complex exponentials as:
f (x)e−2πiξdx,
where i is an imaginary number, and the complex exponentials
(e−2πiξ ) form an orthonormal basis.
Analogously, the graph fourier transform is deﬁned as an expansion of an observed graph signal in terms of the eigenvectors of
the graph laplacian L, and the eigenvectors serve as a basis in the
spectral domain. Let us assume that a graph signal (x ∈R |V|×1) is
observed on a graph G, we deﬁne the graph fourier transform and
its inverse on G as:
x(j)µl (j)
ˆx(l)µl(j),
where x(j), ˆx(l) and µl (j) denote the jth, lth and jth value of x, ˆx and
µl, respectively; µl denotes the lth eigenvector of L; ˆx represents a
graph signal which has been transformed into the spectral domain.
For simplicity, we rewrite Eq. (4) in the matrix form as ˆx = U ⊺x
and x = U ˆx, respectively, where U = {µ0, µ1, ..., µl, ..., µN −1} are
eigenvectors of L.
In particular, for a bipartite graph B, assume that there are two
types of graph signals: xu ∈R |U |×1 and xi ∈R |I|×1, associated
with user and item vertices, respectively. We transform them into
the spectral domain and vice versa as :
Spectral Convolution Filtering
The broad information of graph structures exists in the spectral
domain, and diﬀerent types of connectivity information between
users and items can be uncovered in diﬀerent frequency domains.
It is desirable to dynamically adjust the importance of each frequency domain for RS.
To this end, we propose a convolution ﬁlter, parameterized by
θ ∈RN , asдθ (Λ) = diaд([θ0λ0,θ1λ1, ...,θN −1λN −1]) into the spectral domain as:
= Uдθ (Λ)U ⊺
where xunew and xinew are new graph signals on B learned by the
ﬁlter дθ (Λ), and Λ = {λ0, λ1, ..., λN −1} denotes eigenvalues of the
graph laplacian matrix L.
In Eq. (6), a convolution ﬁlterдθ (Λ) is placed on a spectral graph
, and each value of θ is responsible for boosting or
diminishing each corresponding frequency component. The eigenvector matrix U in Eq. (6) is used to perform an inverse graph
fourier transform.
Polynomial Approximation
Recall that we proposeda convolution operation, as shown in Eq. (6),
to directly perform in the spectral domain. Although the ﬁlter is
able to dynamically measure contributions of each frequency component for the purpose of recommendations, there are two limitations. First, as shown in Eq. (6), the learning complexity of the
ﬁlter is O(N ), where N is the number of vertices. That is, unlike
classical Convolutional Neural Networks (CNNs), the number of
parameters of the ﬁlter is linear to the dimensionality of data. It
constrains the scalability of the proposed ﬁlter. Second, the learned
graph signals (xunew ∈R |U |×1 and xinew ∈R |I|×1) are vectors. It
means that each vertex of users or items is represented by a scalar
feature. However, a vector for every user and item is necessary to
model the deep and complex connections between users and items.
RecSys ’18, October 2–7, 2018, Vancouver, BC, Canada
Lei Zheng, Chun-Ta Lu, Fei Jiang, Jiawei Zhang, and Philip S. Yu
; U, Λ, Θ′
; U, Λ, Θ′
Figure 3: The feed-forward procedure of SpectralCF. The function sp(:;U, Λ, Θ) denotes the spectral convolution operation
shown in Eq. (10).
The ﬁrst limitation can be overcome by using a polynomial approximation. We ﬁrst demonstrate that the set of all convolution
ﬁlters Sд = {дθ (Λ) = diaд([θ0λ0,θ1λ1, ...,θN −1λN −1]),θ ∈RN }
is equal to the set of ﬁnite-order polynomials Sh = {hθ ′(Λ) =
θ′pΛp,θ ′ ∈RN }.
Proposition 3.1. Sh is equal to Sд.
Proof. Let us consider an instance hθ ′(Λ) ∈Sh. Then,hθ ′(Λ) =
θ′pΛp = diaд([
λN −1]). So,hθ ′(Λ) ∈Sд. Now, consider a convolution ﬁlterдθ (Λ) ∈
Sд. Then, there must exist a polynomial function ϕ(λ) =
that interpolates through all pairs (λi,θiλi) for i ∈{0, 1, ..., N −1}.
The maximum degree of such a polynomial is at most N −1 as
there are maximum N points to interpolate. Therefore, дθ (Λ) =
apΛp = ha(Λ) ∈Sh.
Now, we can approximate the convolution ﬁlters by using ﬁrst
P polynomials as the following:
In this way, the learning complexity of the ﬁlter becomes O(P),
where P is a hyper-parameter, and independent from the number
vertices. Specially, we limit the order of the polynomial, P, to 1 in
order to avoid over-ﬁtting. By substituting Eq. (7) into Eq. (6), we
Furthermore, it is beneﬁcial to further decrease the number of parameters by setting θ′ = θ′
1. As a result, Eq. (8) becomes:
= θ′(UU ⊺+ U ΛU ⊺)
where θ′ is a scalar.
For the second limitation, one can generalize the graph signals
(xu ∈R |U |×1 and xi ∈R |I|×1) to C-dimensional graph signals:
Xu ∈R |U |×C andXi ∈R |I|×C. Hence, Eq. (9) becomes
(UU ⊺+U ΛU ⊺)
θ′. To take one step further, we generalize
the ﬁlter parameter θ′ to a matrix of ﬁlter parameters Θ′ ∈RC×F
with C input channels and F ﬁlters. As a result, our ﬁnal spectral
convolution operation is shown as the following:
(UU ⊺+ UΛU ⊺)
where Xunew ∈R |U |×F and Xinew ∈R |I|×F denote convolution
results learned with F ﬁlters from the spectral domain for users and
items, respectively; σ denotes the logistic sigmoid function.
In fact, Eq. (10) is a general version of Eq. (9) as it is equivalent
to perform Eq. (9) in C input channels with F ﬁlters. Hereafter, the
proposed convolution operation as shown in Eq. (10) is denoted as
a function sp(:;U, Λ, Θ′), which is parameterized by U, Λ and Θ′.
Multi-layer Model
Given user vectors Xu and item vectors Xi, new graph singals
(Xunew and Xinew) in Eq. (10) are convolution results learned from
the spectral domain with a parameter matrix Θ′ ∈RC×F. As in
classical CNNs, one can regard Eq. (10) as a propagation rule to
build a deep neural feed-forward network based model, which we
refer as Spectral Collaborative Filtering (SpectralCF).
Similar to word embedding techniques, we ﬁrst randomly initialize user vectors Xu
0 and item vectors Xi
0. Taking Xu
inputs, a K layered deep spectralCF can be formulated as:
...sp | {z }
...;U, Λ, Θ′
K−1 ∈RF ×F is a matrix of ﬁlter parameters for the kth
k denote the convolution ﬁltering results of the
kth layer.
In order to utilize features from all layers of SpectralCF, we further concatenate them into our ﬁnal latent factors of users and
1 , ..., Xu
1, ..., Xi
where Vu ∈R |U |×(C+KF ) and Vi ∈R |I|×(C+KF ).
In terms of the loss function, the conventional BPR loss suggested in is employed. BPR is a pair-wise loss to address the implicit data for recommendations. Unlike point-wise based methods
 , BPR learns a triple (r, j, j′), where item j is liked/clicked/viewed
by user r and item j′ is not. By maximizing the preference diﬀerence between j and j′, BPR assumes that the user i prefers item j
over the unobserved item j′. In particular, given a user matrix V u
and an item matrix V i as shown in Eq. (12), the loss function of
Spectral Collaborative Filtering
RecSys ’18, October 2–7, 2018, Vancouver, BC, Canada
Algorithm 1: SpectralCF
Input: Training set: D := {(r, j, j′)|r ∈U ∧j ∈I+
number of epochs E, batch size B, number of layers K,
dimension of latent factors C, number of ﬁlters F,
regularization term λreд, learning rate λ, laplacian matrix L
and its corresponding eigenvectors U and eigenvalues Λ.
Output: Model’s parameter set: Ψ = {Θ′
1, ..., Θ′
1 Randomly initialize Xu
0 from a Gaussian distribution
N(0.01, 0.02);
2 for e = 1, 2, · · · , E do
Generate the eth batch of size B by uniformly sampling from U,
for k = 0, 1, · · · , K −1 do
Calculate X u
k+1 and X i
k+1 by using Eq. (10);
Concatenate [X u
1 , ..., X u
K ] into V u and
1, ..., X i
K ] into V i;
Estimate gradients ∂L
∂Ψe by back propagation;
Update Ψe+1 according to the procedure of RMSprop
optimization ;
11 return ΨE.
SpectralCF is given as:
L = arg min
(r,j,j′)∈D
+λreд(||V u ||2
2 + ||V i ||2
where vur and vi
j denote rth and jth column of V u and V i, respectively; λreд represents the weight on the regularization terms. The
training data D is generated as:
D = {(r, j, j′)|r ∈U ∧j ∈I+
Optimization and Prediction
At last, RMSprop is used to minimize the loss function. The
RMSprop is an adaptive version of gradient descent which adaptively controls the step size with respect to the absolute value of
the gradient. It is done by scaling the updated value of each weight
by a running average of its gradient norm.
As shown in Algorithm 1, for a batch of randomly sampled triple
(r, j, j′), we update parameters in each epoch using the gradients
of the loss function. After the training process, with optimized Θ,
0, we derive the user r’s preference over item j asvur
The ﬁnal item recommendation for a user r is given according to
the ranking criterion as Eq. (15).
r : j1 ≽j2 ≽... ≽jn ⇒vu
j2 > ... > vu
EXPERIMENTS
As discussed in the introduction section, leveraging the connectivity information in a user-item bipartite graph is essentially important for an eﬀective recommendation model. In this section, we argue that, directly learning from the spectral domain, the proposed
SpectraCF can reveal the rich information of graph structures existing in the spectral domain for making better recommendations.
One may ask the following research questions:
Table 1: The hyper-parameter setting of SpectralCF.
Hyper-parameters
16 16 0.001
RQ1: How much does SpectralCF beneﬁt from the connectivity
information learned from the spectral domain?
RQ2: Does SpectralCF learn from the spectral domain in an eﬀective way?
RQ3: Compared with traditional methods, can SpectralCF better
counter the cold-start problem?
In this section, in order to answer the questions above, we conduct
experiments to compare SpectralCF with state-of-the-art models.
Comparative Methods
To validate the eﬀectiveness of SpectralCF, we compare it with
six state-of-the-art models. The comparative models can be categorized into two groups: (1) CF-based Models: To answer RQ1,
we compare SpectralCF with four state-of-the-art CF-based methods (ItemKNN, BPR, eALS and NCF) which ignore the information
in the spectral domain; (2) Graph-based Models: For RQ2, we are
interested in how eﬀectively does SpetralCF learn the connectivity
information from the spectral domain. We therefore compare SpectralCF with two graph-based models: GNMF and GCMC. Although
the two models are also CF-based, we term them as graph-based
models since they learn the structural information from a bipartite
graph. These two groups of comparative models are summarized
• ItemKNN : ItemKNN is a standard neighbor-based collaborative ﬁltering method. The model ﬁnds similar items
for a user based on their similarities.
• BPR : We use Bayesian Personalized Ranking based
Matrix Factorization. BPR introduces a pair-wise loss into
the Matrix Factorization to be optimized for ranking .
• eALS : This is a state-of-the-art matrix factorization based
method for item recommendation. This model takes all unobserved interactions as negative instances and weighting
them non-uniformly by the item popularity.
• NCF : Neural Collaborative Filtering fuses matrix factorization and Multi-Layer Perceptron (MLP) to learn from
user-item interactions. The MLP endows NCF with the ability of modelling non-linearities between users and items.
• GNMF : Graph regularized Non-negative Matrix Factorization considers the graph structures by seeking a matrix
factorization with a graph-based regularization.
• GCMC : Graph Convolutional Matrix Completion utilizes a graph auto-encoder to learn the connectivity information of a bipartite interaction graph for latent factors of
users and items.
Please note that, GNMF and GCMC are originally designed for explicit datasets. For a fair comparison, we follow the setting of 
to adapt them for implicit data.
RecSys ’18, October 2–7, 2018, Vancouver, BC, Canada
Lei Zheng, Chun-Ta Lu, Fei Jiang, Jiawei Zhang, and Philip S. Yu
Figure 4: Eﬀects of hyper-parameter K in terms of Recall@20 and MAP@20 in the dataset of MovieLens-1M.
We test our method as well as comparative models on three publicly available datasets3:
• MovieLens-1M : This movie rating dataset has been
widely used to evaluate collaborative ﬁltering algorithms.
We used the version containing 1,000,209 ratings from 6,040
users for 3,900 movies. While it is a dataset with explicit
feedbacks, we follow the convention that transforms
it into implicit data, where each entry is marked as 0 or 1
indicating whether the user has rated the item. After transforming, we retain a dataset of 1.0% density.
• HetRec : This dataset has been released by the Second
International Workshop on Information Heterogeneity and
Fusion in Recommender Systems4. It is an extension of MovieLens-
10M dataset and contains 855,598 ratings, 2,113 users and
10,197 movies. After converting it into implicit data as MovieLens-
1M, we obtain a dataset of 0.3% density.
• Amazon Instant Video : The dataset consists of 426,922
users, 23,965 videos and 583,933 ratings from Amazon.com.
Similarly, we transformed it into implicit data and removed
users with less than 5 interactions. As a result, a dataset of
0.12% density is obtained.
Experimental Setting
Ideally, a recommendation model should not only be able to retrieve all relevant items out of all items but also provide a rank for
each user where relevant items are expected to be ranked in the top.
Therefore, in our experiments, we use Recall@M and MAP@M
to evaluate the performance of the top-M recommendations. Recall@M is employed to measure the fraction of relevant items retrieved out of all relevant items. MAP@M is used for evaluating the
ranking performance of RS. The Recall@M for each user is then de-
Recall@M = #items the user likes among the top M
total number of items the user likes
The ﬁnal results reported are average recall over all users.
For each dataset, we randomly select 80% items associated with
each user to constitute the training set and use all the remaining as
the test set. For each evaluation scenario, we repeat the evaluation
ﬁve times with diﬀerent randomly selected training sets and the
average performance is reported in the following sections.
3MovieLens-1M and HetRec are available at and Amazon Instant Video can be found at 
4 
We use a validation set from the training set of each dataset to
ﬁnd the optimal hyper-parameters of comparative methods introduced in the Section 4.1. For ItemKNN, we employ the cosine distance to measure item similarities. The dimensions of latent factors
for BPR, eALS and GNMF are searched from {8,16,32,64,128} via the
validation set. The hyperparameter λ of eALS is selected from 0.001
to 0.04. Since the architecture of a multi-layer perceptron (MLP) is
diﬃcult to optimize, we follow the suggestion from the original paper to employ a three-layer MLP with the shape of (32, 16, 8)
for NCF. The dropout rate of nodes for GCMC is searched from
{0.3,0.4,0.5,0.6,0.7,0.8}. Our SpectralCF has one essential hyper-parameter:
K. Figure 4 shows how the performances of SpectralCF vary as K
is set from 1 to 5 on the validation set of MovieLens-1M. As we can
see, in terms of Recall@20 and MAP@20, SpectralCF reaches its
best performances when K is ﬁxed as 3. Other hyper-parameters
of SpectralCF are empirically set and summarized in Table 1, where
λ denotes the learning rate of RMSprop. Our models are implemented in TensorFlow .
Experimental Results (RQ1 and RQ2)
In Figure 5, we compare SpectralCF with four CF-based models
and two graph-based models in terms of Recall@M on all three
datasets. Overall, when M is varied from 20 to 100, SpectralCF
consistently yields the best performance across all cases. Among
CF-based comparative models, ItemKNN gives the worst performances in all three datasets, indicating the necessity of modeling users’ personalized preferences rather than just recommending
similar items to users. For graph-based models (GNMF and GCMC),
they generally underperform CF-based models such as BPR and
NCF. The unsatisfying performance of GNMF shows that adding
a graph-based regularization is not suﬃcient to capture complex
structures of graphs. Though GCMC directly performs on a useritem bipartite graph, each vertex in the graph is only allowed to
learn from its neighbors. This constrains its ability of capturing
global structures in the graph. Among all comparative models, beneﬁting from its capability of modeling non-linear relationships between users and items, NCF beats all other models and becomes the
strongest one. However, none of models above are able to directly
perform in the spectral domain. They lose the rich information in
the domain and as a result, SpectralCF greatly outperforms NCF
by 16.1%, 16.2% and 28.0% in the dataset of MovieLen-1M, HetRec
and Amazon Instant Video, respectively.
In Figure 6, we compare SpectralCF with all comparative models in terms of MAP@M. Again, when M is in a range from 20
to 100, SpectralCF always yields the best performance. Neighborbased ItemKNN performs the worst among all models. It further
shows the advantages of modeling users’ personalized preferences.
Compared with NCF and BPR, graph-based models (GNMF and
GCMC) again fail to show convincing ranking performances measured by MAP@M. For CF-based models, while NCF beats other
CF-based models in the dataset of HetRec, BPR shows itself as a
strong model for ranking, owing to its pairwise ranking loss. It
slightly outperforms NCF on average in the datasets of MovieLens-
1M and Amazon Instant Video. However, SpectralCF improves BPR
by 15.9%, 64.9% and 47.5% in the dataset of MovieLen-1M, HetRec
and Amazon Instant Video, respectively.
Spectral Collaborative Filtering
RecSys ’18, October 2–7, 2018, Vancouver, BC, Canada
SpectralCF
(a) MovieLens-1M
SpectralCF
(b) HetRec
SpectralCF
(c) Amazon Instant Video
Figure 5: Performance comparison in terms of recall@M with M varied from 20 to 100. Errors bars are 1-standard deviation.
SpectralCF
(a) MovieLens-1M
SpectralCF
(b) HetRec
SpectralCF
(c) Amazon Instant Video
Figure 6: Performance comparison in terms of MAP@M with M varied from 20 to 100. Errors bars are 1-standard deviation.
Overall, as shown in Figure 5 and 6, not surprisingly, the performances of all models decline as the dataset becomes sparse. However, SpectralCF always outperforms all comparative models regardless of the sparsities of the datasets. By comparing spectralCF
with traditional CF-based models, we demonstrate that the rich information of connectivity existing in the spectral domain assists
SpectralCF in learning better latent factors of users and items. By
comparing SpectralCF with graph-based models, we show that SpectralCF can eﬀectively learn from the spectral domain.
Quality of Recommendations for Cold-start
Users (RQ3)
To answer RQ3, in this section, we conduct an experiment to investigate the quality of recommendations made by SpectralCF for
cold-start users. To this end, in the dataset of MovieLens-1M, we
build training sets with diﬀerent degrees of sparsity by varying
the number of items associated with each user, denoted as P, from
one to ﬁve. All the remaining items associated with users are used
as the test set. We compare SpectralCF with BPR, which is widely
known and also shown as a strong ranking performer in Figure 6.
The test results are reported in the Table 2.
In Table 2, it is shown that, suﬀering from the cold-start problem, the performances of BPR and SpectralCF inevitably degrade.
However, regardless of the number of items associated with users,
SpectralCF consistently outperforms BPR in terms of Recall@20
and MAP@20. On average, SpectralCF improves BPR by 36.8% and
Table 2: Performance Comparison in terms of Recall@20
and MAP@20 in the sparse training sets. In the dataset of
MovieLens-1M, we vary the number of items associated with
each users, denoted as P, from 1 to 5. The average results are
reported and the best results are in bold. The standard deviation is shown in parentheses.
SpectralCF 0.031
Improvement
SpectralCF 0.019
Improvement
33.8% in Recall@20 and MAP@20, respectively. Hence, it is demonstrated that compared with BPR, spectralCF can better handle coldstart users and provide more reliable recommendations.
RELATED WORKS
There are two categories of studies related to our work: deep learning based RS and graph-based RS. In this section, we will ﬁrst
RecSys ’18, October 2–7, 2018, Vancouver, BC, Canada
Lei Zheng, Chun-Ta Lu, Fei Jiang, Jiawei Zhang, and Philip S. Yu
brieﬂy review existing works in the area of deep RS. Then, we focus
on presenting recent works on graph-based RS. Despite all these
approaches, SpectralCF is the ﬁrst model to directly learn latent
factors of users and items from the spectral domains of user-item
bipartite graphs.
Deep Recommender Systems
One of the early works utilizing deep learning for RS builds a Restricted Boltzmann Machines (RBM) based method to model users
using their rating preferences . Although the method is still
a relatively shallow model, it slightly outperforms Matrix Factorization technique and shows the promising future for deep recommender systems. In , a generative model and a discriminative
model are employed to play a minimax game. The two models are
iteratively optimized and achieve promising results for the item
recommendation problem. Inspired by , proposed a CF
Neural Autoregressive Distribution Estimator (CF-NADE) model
for collaborative ﬁltering tasks. CF-NADE shares parameters between diﬀerent ratings. presents to utilize a Multilayer Perceptron (MLP) to model user-item interactions.
A number of researchers proposedto builda hybrid recommender
systems to counter the sparsity problem. introduce Convolutional Neural Networks (CNN) and Deep Belief Network (DBN) to
assist representation learning for music data. As such, their model
is able to extract latent factors of songs without ratings while CF
based techniques like MF are unable to handle these songs. These
approaches above pre-train embeddings of users and items with
matrix factorization and utilize deep models to ﬁne-tune the learned
item features based on item content. In and , multi-view
deep models are built to utilize item information from more than
one domain. integrates a CNN with PMF to analyze documents
associated with items to predict users’ future explicit ratings. 
leverage two parallel neural networks to jointly model latent factors of users and items. To incorporate visual signals into RS, 
propose CNN-based models to incorporate visual signals into RS.
They make use of visual features extracted from product images
using deep networks to enhance the performance of RS. investigates how to leverage the multi-view information to improve
the quality of recommender systems. jointly trains wide linear
models and deep neural networks for video recommendations. 
and utilize RNN to consider word orders and extract complex
semantics for recommendations. applies an attention mechanism on a sequence of models to adaptively capture the change
of criteria of editors. leverages an attentional model to learn
adaptive user embeddings. A survey on the deep learning based RS
with more works on this topic can be found in .
Graph-based Recommender Systems
In order to learn latent factors of users and items from graphs,
a number of researchers have proposed graph-based RS. develops a semi-supervised learning model on graphs for document
recommendation. The model combines multiple graphs in order to
measure item similarities. In , they propose to model the checkin behaviors of users and a graph-based preference propagation
algorithm for point of interest recommendation. The proposed solution exploits both the geographical and temporal inﬂuences in an
integrated manner. addresses the problem of personalized tag
recommendation by modeling it as a "query and ranking" problem.
Inspired by the recent success of graph/node embedding methods,
 proposes a graph convolution network based model for recommendations. In , a graph auto-encoder learns the structural information of a graph for latent factors of users and items. adds
graph-based regularizations into the matrix factorization model to
learn graph structures. Graph-regularized methods are developed
for the problemm of matrix completion in . combines a
convolutional neural network and a recurrent neural network to
model the dynamic rating generation process. Although this work
also considers the spectral domain, they learn from a graph constructed from side information, such as genres or actors for movies.
In contrast, our method learns directly from user-item bipartite
graphs and does not require the side information. Thus, this work
is not comparable to our method.
Additionally, some scholars have proposed to incorporate the
heterogeneous information on a graph for recommendations. 
suggests a general latent factor model for entities in a graph. 
introduces a recommendation model for implicit data by taking
advantage of diﬀerent item similarity semantics in the graph. 
introduces a semantic path based personalized recommendation
method to predict the rating scores of users on items.
However, all works above are diﬀerent from ours because they
fail to consider the rich information in the spectral domains of useritem bipartite graphs. Also, our study focuses on learning from
the implicit feedbacks, and leaves incorporating the heterogeneous
information in a graph and the item content for future works.
CONCLUSIONS
It is shown that the rich information of connectivity existing in
the spectral domain of a bipartite graph is helpful for discovering
deep connections between users and items. In this paper, we introduce a new spectral convolution operation to directly learn latent
factors of users and items from the spectral domain. Furthermore,
with the proposed operation, we build a deep feed-forward neural
network based recommendation model, named Spectral Collaborative Filtering (SpectralCF). Due to the rich information of connectivity existing in the spectral domain, compared with previous
works, SpectralCF is capable of discovering deep connections between users and items and therefore, alleviates the cold-start problem for CF. To the best of our knowledge, SpectralCF is the ﬁrst
CF-based method directly learning from the spectral domains of
user-item bipartite graphs. We believe that it shows the potential
of conducting CF in the spectral domain, and will encourage future
works in this direction.
In comparison with fourstate-of-the-art CF-based and two graphbased models, SpectralCF achieved 20.1% and 42.6% improvements
averaging on three standard datasets in terms of Recall@M and
MAP@M, respectively.
Additionally, in the experiments, by varying the number of items
associated with each user from 1 to 5, we build training sets with
diﬀerent degrees of sparsity to investigate the quality of recommendations made by SpectralCF for cold-start users. By comparing SpectralCF with BPR, on average, SpectralCF improves BPR
by 36.8% and 33.8% in Recall@20 and MAP@20, respectively. It is
Spectral Collaborative Filtering
RecSys ’18, October 2–7, 2018, Vancouver, BC, Canada
validated that SpectralCF can eﬀectively ameliorate the cold-start
ACKNOWLEDGMENTS
This work is supported in part by NSF through grants IIS-1526499,
IIS-1763325, and CNS-1626432, and NSFC 61672313. This work is
also partially supported by NSF through grant IIS-1763365 and by
FSU through the startup package and FYAP award.