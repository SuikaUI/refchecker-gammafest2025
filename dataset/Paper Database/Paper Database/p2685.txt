Machine Learning in Automated Text Categorization
Fabrizio Sebastiani
Consiglio Nazionale delle Ricerche, Italy
The automated categorization (or classiﬁcation) of texts into predeﬁned categories has witnessed a
booming interest in the last ten years, due to the increased availability of documents in digital form
and the ensuing need to organize them. In the research community the dominant approach to this
problem is based on machine learning techniques: a general inductive process automatically builds
a classiﬁer by learning, from a set of preclassiﬁed documents, the characteristics of the categories.
The advantages of this approach over the knowledge engineering approach (consisting in the
manual deﬁnition of a classiﬁer by domain experts) are a very good eﬀectiveness, considerable
savings in terms of expert manpower, and straightforward portability to diﬀerent domains. This
survey discusses the main approaches to text categorization that fall within the machine learning
paradigm. We will discuss in detail issues pertaining to three diﬀerent problems, namely document
representation, classiﬁer construction, and classiﬁer evaluation.
Categories and Subject Descriptors: H.3.1 [Information storage and retrieval]: Content analysis and indexing—Indexing methods; H.3.3 [Information storage and retrieval]: Information search and retrieval—Information ﬁltering; H.3.3 [Information storage and retrieval]:
Systems and software—Performance evaluation (eﬃciency and eﬀectiveness); I.2.3 [Artiﬁcial
Intelligence]: Learning—Induction
General Terms: Algorithms, Experimentation, Theory
Additional Key Words and Phrases: Machine learning, text categorization, text classiﬁcation
1. INTRODUCTION
In the last ten years content-based document management tasks (collectively known
as information retrieval – IR) have gained a prominent status in the information
systems ﬁeld, due to the increased availability of documents in digital form and
the ensuing need to access them in ﬂexible ways. Text categorization (TC – aka
text classiﬁcation, or topic spotting), the activity of labelling natural language texts
with thematic categories from a predeﬁned set, is one such task. TC dates back
to the early ’60s, but only in the early ’90s it became a major subﬁeld of the
information systems discipline, thanks to increased applicative interest and to the
availability of more powerful hardware. TC is now being applied in many contexts,
ranging from document indexing based on a controlled vocabulary, to document
ﬁltering, automated metadata generation, word sense disambiguation, population
of hierarchical catalogues of Web resources, and in general any application requiring
document organization or selective and adaptive document dispatching.
Until the late ’80s the most popular approach to TC, at least in the “operational” (i.e. real-world applications) community, was a knowledge engineering (KE)
one, consisting in manually deﬁning a set of rules encoding expert knowledge on
Address: Istituto di Elaborazione dell’Informazione, Consiglio Nazionale delle Ricerche, Via G.
Moruzzi, 1, 56124 Pisa (Italy). E-mail: 
F. Sebastiani
how to classify documents under the given categories. In the ’90s this approach has
increasingly lost popularity (especially in the research community) in favour of the
machine learning (ML) paradigm, according to which a general inductive process
automatically builds an automatic text classiﬁer by learning, from a set of preclassiﬁed documents, the characteristics of the categories of interest. The advantages
of this approach are an accuracy comparable to that achieved by human experts,
and a considerable savings in terms of expert manpower, since no intervention from
either knowledge engineers or domain experts is needed for the construction of the
classiﬁer or for its porting to a diﬀerent set of categories. It is the ML approach to
TC that this paper concentrates on.
Current-day TC is thus a discipline at the crossroads of ML and IR, and as such it
shares a number of characteristics with other tasks such as information/knowledge
extraction from texts and text mining [Knight 1999; Pazienza 1997]. There is still
considerable debate on where the exact border between these disciplines lies, and the
terminology is still evolving. “Text mining” is increasingly being used to denote all
the tasks that, by analyzing large quantities of text and detecting usage patterns, try
to extract probably useful (although only probably correct) information. According
to this view, TC is an instance of text mining. TC enjoys quite a rich literature now,
but this is still fairly scattered1. Although two international journals have devoted
special issues to this topic [Joachims and Sebastiani 2001; Lewis and Hayes 1994],
there are no systematic treatments of the subject: there are neither textbooks nor
journals entirely devoted to TC yet, and [Manning and Sch¨utze 1999, Chapter16]
is the only chapter-length treatment of the subject. As a note, we should warn the
reader that the term “automatic text classiﬁcation” has sometimes been used in
the literature to mean things quite diﬀerent from the ones discussed here. Aside
from (i) the automatic assignment of documents to a predeﬁned set of categories,
which is the main topic of this paper, the term has also been used to mean (ii) the
automatic identiﬁcation of such a set of categories ,
or (iii) the automatic identiﬁcation of such a set of categories and the grouping of
documents under them , a task usually called text clustering, or
(iv) any activity of placing text items into groups, a task that has thus both TC
and text clustering as particular instances [Manning and Sch¨utze 1999].
This paper is organized as follows. In Section 2 we formally deﬁne TC and its
various subcases, and in Section 3 we review its most important applications. Section 4 describes the main ideas underlying the ML approach to classiﬁcation. Our
discussion of text classiﬁcation starts in Section 5 by introducing text indexing, i.e.
the transformation of textual documents into a form that can be interpreted by a
classiﬁer-building algorithm and by the classiﬁer eventually built by it. Section 6
tackles the inductive construction of a text classiﬁer from a “training” set of preclassiﬁed documents. Section 7 discusses the evaluation of text classiﬁers. Section
8 concludes, discussing open issues and possible avenues of further research for TC.
2. TEXT CATEGORIZATION
1A fully searchable bibliography on TC created and maintained by this author is available at
 
Machine Learning in Automated Text Categorization
2.1 A deﬁnition of text categorization
Text categorization is the task of assigning a Boolean value to each pair ⟨dj, ci⟩∈
D×C, where D is a domain of documents and C = {c1, . . . , c|C|} is a set of predeﬁned
categories. A value of T assigned to ⟨dj, ci⟩indicates a decision to ﬁle dj under ci,
while a value of F indicates a decision not to ﬁle dj under ci. More formally, the task
is to approximate the unknown target function ˘Φ : D × C →{T, F} (that describes
how documents ought to be classiﬁed) by means of a function Φ : D × C →{T, F}
called the classiﬁer (aka rule, or hypothesis, or model) such that ˘Φ and Φ “coincide
as much as possible”. How to precisely deﬁne and measure this coincidence (called
eﬀectiveness) will be discussed in Section 7.1. From now on we will assume that:
—The categories are just symbolic labels, and no additional knowledge (of a procedural or declarative nature) of their meaning is available.
—No exogenous knowledge (i.e. data provided for classiﬁcation purposes by an external source) is available; therefore, classiﬁcation must be accomplished on the
basis of endogenous knowledge only (i.e. knowledge extracted from the documents). In particular, this means that metadata such as e.g. publication date,
document type, publication source, etc. is not assumed to be available.
The TC methods we will discuss are thus completely general, and do not depend on
the availability of special-purpose resources that might be unavailable or costly to
develop. Of course, these assumptions need not be veriﬁed in operational settings,
where it is legitimate to use any source of information that might be available or
deemed worth developing [D´ıaz Esteban et al. 1998; Junker and Abecker 1997].
Relying only on endogenous knowledge means classifying a document based solely
on its semantics, and given that the semantics of a document is a subjective notion,
it follows that the membership of a document in a category cannot be
decided deterministically. This is exempliﬁed by the phenomenon of inter-indexer
inconsistency [Cleverdon 1984]: when two human experts decide whether to classify
document dj under category ci, they may disagree, and this in fact happens with
relatively high frequency. A news article on Clinton attending Dizzy Gillespie’s
funeral could be ﬁled under Politics, or under Jazz, or under both, or even under
neither, depending on the subjective judgment of the expert.
2.2 Single-label vs. multi-label text categorization
Diﬀerent constraints may be enforced on the TC task, depending on the application.
For instance we might need that, for a given integer k, exactly k (or ≤k, or ≥k)
elements of C be assigned to each dj ∈D. The case in which exactly 1 category
must be assigned to each dj ∈D is often called the single-label (aka non-overlapping
categories) case, while the case in which any number of categories from 0 to |C|
may be assigned to the same dj ∈D is dubbed the multi-label (aka overlapping
categories) case. A special case of single-label TC is binary TC, in which each
dj ∈D must be assigned either to category ci or to its complement ci.
From a theoretical point of view, the binary case (hence, the single-label case too)
is more general than the multi-label, since an algorithm for binary classiﬁcation can
also be used for multi-label classiﬁcation: one needs only transform the problem
F. Sebastiani
of multi-label classiﬁcation under {c1, . . . , c|C|} into |C| independent problems of
binary classiﬁcation under {ci, ci}, for i = 1, . . . , |C|. However, this requires that
categories are stochastically independent of each other, i.e. that for any c′, c′′ the
value of ˘Φ(dj, c′) does not depend on the value of ˘Φ(dj, c′′) and viceversa; this
is usually assumed to be the case (applications in which this is not the case are
discussed in Section 3.5). The converse is not true: an algorithm for multi-label
classiﬁcation cannot be used for either binary or single-label classiﬁcation. In fact,
given a document dj to classify, (i) the classiﬁer might attribute k > 1 categories to
dj, and it might not be obvious how to choose a “most appropriate” category from
them; or (ii) the classiﬁer might attribute to dj no category at all, and it might not
be obvious how to choose a “least inappropriate” category from C.
In the rest of the paper, unless explicitly mentioned, we will deal with the binary
case. There are various reasons for this:
—The binary case is important in itself because important TC applications, including ﬁltering (see Section 3.3), consist of binary classiﬁcation problems (e.g.
deciding whether dj is about Jazz or not). In TC, most binary classiﬁcation problems feature unevenly populated categories (e.g. much fewer documents are about
Jazz than are not) and unevenly characterized categories (e.g. what is about Jazz
can be characterized much better than what is not).
—Solving the binary case also means solving the multi-label case, which is also
representative of important TC applications, including automated indexing for
Boolean systems (see Section 3.1).
—Most of the TC literature is couched in terms of the binary case.
—Most techniques for binary classiﬁcation are just special cases of existing techniques for the single-label case, and are simpler to illustrate than these latter.
This ultimately means that we will view classiﬁcation under C = {c1, . . . , c|C|} as
consisting of |C| independent problems of classifying the documents in D under a
given category ci, for i = 1, . . . , |C|. A classiﬁer for ci is then a function Φi : D →
{T, F} that approximates an unknown target function ˘Φi : D →{T, F}.
2.3 Category-pivoted vs. document-pivoted text categorization
There are two diﬀerent ways of using a text classiﬁer. Given dj ∈D, we might want
to ﬁnd all the ci ∈C under which it should be ﬁled (document-pivoted categorization
– DPC); alternatively, given ci ∈C, we might want to ﬁnd all the dj ∈D that should
be ﬁled under it (category-pivoted categorization – CPC). This distinction is more
pragmatic than conceptual, but is important since the sets C and D might not be
available in their entirety right from the start. It is also relevant to the choice of
the classiﬁer-building method, as some of these methods (see e.g. Section 6.9) allow
the construction of classiﬁers with a deﬁnite slant towards one or the other style.
DPC is thus suitable when documents become available at diﬀerent moments in
time, e.g. in ﬁltering e-mail. CPC is instead suitable when (i) a new category c|C|+1
may be added to an existing set C = {c1, . . . , c|C|} after a number of documents have
already been classiﬁed under C, and (ii) these documents need to be reconsidered
for classiﬁcation under c|C|+1 . DPC is used more often than
CPC, as the former situation is more common than the latter.
Machine Learning in Automated Text Categorization
Although some speciﬁc techniques apply to one style and not to the other (e.g. the
proportional thresholding method discussed in Section 6.1 applies only to CPC),
this is more the exception than the rule: most of the techniques we will discuss
allow the construction of classiﬁers capable of working in either mode.
2.4 “Hard” categorization vs. ranking categorization
While a complete automation of the TC task requires a T or F decision for each
pair ⟨dj, ci⟩, a partial automation of this process might have diﬀerent requirements.
For instance, given dj ∈D a system might simply rank the categories in C =
{c1, . . . , c|C|} according to their estimated appropriateness to dj, without taking
any “hard” decision on any of them. Such a ranked list would be of great help to a
human expert in charge of taking the ﬁnal categorization decision, since she could
thus restrict the choice to the category (or categories) at the top of the list, rather
than having to examine the entire set. Alternatively, given ci ∈C a system might
simply rank the documents in D according to their estimated appropriateness to ci;
symmetrically, for classiﬁcation under ci a human expert would just examine the
top-ranked documents instead than the entire document set. These two modalities
are sometimes called category-ranking TC and document-ranking TC [Yang 1999],
respectively, and are the obvious counterparts of DPC and CPC.
Semi-automated, “interactive” classiﬁcation systems [Larkey and Croft 1996] are
useful especially in critical applications in which the eﬀectiveness of a fully automated system may be expected to be signiﬁcantly lower than that of a human
expert. This may be the case when the quality of the training data (see Section
4) is low, or when the training documents cannot be trusted to be a representative
sample of the unseen documents that are to come, so that the results of a completely
automatic classiﬁer could not be trusted completely.
In the rest of the paper, unless explicitly mentioned, we will deal with “hard”
classiﬁcation; however, many of the algorithms we will discuss naturally lend themselves to ranking TC too (more details on this in Section 6.1).
3. APPLICATIONS OF TEXT CATEGORIZATION
TC goes back to Maron’s seminal work on probabilistic text classiﬁcation.
Since then, it has been used for a number of diﬀerent applications, of which we here
brieﬂy review the most important ones. Note that the borders between the diﬀerent
classes of applications listed here are fuzzy and somehow artiﬁcial, and some of these
may be considered special cases of others. Other applications we do not explicitly
discuss are speech categorization by means of a combination of speech recognition
and TC [Myers et al. 2000; Schapire and Singer 2000], multimedia document categorization through the analysis of textual captions [Sable and Hatzivassiloglou 2000],
author identiﬁcation for literary texts of unknown or disputed authorship [Forsyth
1999], language identiﬁcation for texts of unknown language [Cavnar and Trenkle
1994], automated identiﬁcation of text genre [Kessler et al. 1997], and automated
essay grading [Larkey 1998].
3.1 Automatic indexing for Boolean information retrieval systems
The application that has spawned most of the early research in the ﬁeld [Borko and
Bernick 1963; Field 1975; Gray and Harley 1971; Heaps 1973; Maron 1961], is that
F. Sebastiani
of automatic document indexing for IR systems relying on a controlled dictionary,
the most prominent example of which is that of Boolean systems. In these latter
each document is assigned one or more keywords or keyphrases describing its content, where these keywords and keyphrases belong to a ﬁnite set called controlled
dictionary, often consisting of a thematic hierarchical thesaurus (e.g. the NASA thesaurus for the aerospace discipline, or the MESH thesaurus for medicine). Usually,
this assignment is done by trained human indexers, and is thus a costly activity.
If the entries in the controlled vocabulary are viewed as categories, text indexing
is an instance of TC, and may thus be addressed by the automatic techniques described in this paper. Recalling Section 2.2, note that this application may typically
require that k1 ≤x ≤k2 keywords are assigned to each document, for given k1, k2.
Document-pivoted TC is probably the best option, so that new documents may be
classiﬁed as they become available. Various text classiﬁers explicitly conceived for
document indexing have been described in the literature; see e.g. [Fuhr and Knorz
1984; Robertson and Harding 1984; Tzeras and Hartmann 1993].
Automatic indexing with controlled dictionaries is closely related to automated
metadata generation. In digital libraries one is usually interested in tagging documents by metadata that describe them under a variety of aspects (e.g. creation
date, document type or format, availability, etc.).
Some of these metadata are
thematic, i.e. their role is to describe the semantics of the document by means of
bibliographic codes, keywords or keyphrases. The generation of these metadata
may thus be viewed as a problem of document indexing with controlled dictionary,
and thus tackled by means of TC techniques.
3.2 Document organization
Indexing with a controlled vocabulary is an instance of the general problem of document base organization. In general, many other issues pertaining to document
organization and ﬁling, be it for purposes of personal organization or structuring of
a corporate document base, may be addressed by TC techniques. For instance, at
the oﬃces of a newspaper incoming “classiﬁed” ads must be, prior to publication,
categorized under categories such as Personals, Cars for Sale, Real Estate,
etc. Newspapers dealing with a high volume of classiﬁed ads would beneﬁt from
an automatic system that chooses the most suitable category for a given ad. Other
possible applications are the organization of patents into categories for making their
search easier [Larkey 1999], the automatic ﬁling of newspaper articles under the appropriate sections (e.g. Politics, Home News, Lifestyles, etc.), or the automatic
grouping of conference papers into sessions.
3.3 Text ﬁltering
Text ﬁltering is the activity of classifying a stream of incoming documents dispatched in an asynchronous way by an information producer to an information
consumer [Belkin and Croft 1992]. A typical case is a newsfeed, where the producer is a news agency and the consumer is a newspaper [Hayes et al. 1990]. In
this case the ﬁltering system should block the delivery of the documents the consumer is likely not interested in (e.g. all news not concerning sports, in the case
of a sports newspaper). Filtering can be seen as a case of single-label TC, i.e. the
classiﬁcation of incoming documents in two disjoint categories, the relevant and the
Machine Learning in Automated Text Categorization
irrelevant. Additionally, a ﬁltering system may also further classify the documents
deemed relevant to the consumer into thematic categories; in the example above,
all articles about sports should be further classiﬁed according e.g. to which sport
they deal with, so as to allow journalists specialized in individual sports to access
only documents of prospective interest for them. Similarly, an e-mail ﬁlter might
be trained to discard “junk” mail [Androutsopoulos et al. 2000; Drucker et al. 1999]
and further classify non-junk mail into topical categories of interest to the user.
A ﬁltering system may be installed at the producer end, in which case it must
route the documents to the interested consumers only, or at the consumer end, in
which case it must block the delivery of documents deemed uninteresting to the
consumer. In the former case the system builds and updates a “proﬁle” for each
consumer [Liddy et al. 1994], while in the latter case (which is the more common,
and to which we will refer in the rest of this section) a single proﬁle is needed.
A proﬁle may be initially speciﬁed by the user, thereby resembling a standing
IR query, and is updated by the system by using feedback information provided
(either implicitly or explicitly) by the user on the relevance or non-relevance of the
delivered messages. In the TREC community [Lewis 1995c] this is called adaptive
ﬁltering, while the case in which no user-speciﬁed proﬁle is available is called either
routing or batch ﬁltering, depending on whether documents have to be ranked in
decreasing order of estimated relevance or just accepted/rejected. Batch ﬁltering
thus coincides with single-label TC under |C| = 2 categories; since this latter is
a completely general TC task some authors [Hull 1994; Hull et al. 1996; Schapire
et al. 1998; Sch¨utze et al. 1995], somewhat confusingly, use the term “ﬁltering” in
place of the more appropriate term “categorization”.
In information science document ﬁltering has a tradition dating back to the
’60s, when, addressed by systems of various degrees of automation and dealing
with the multi-consumer case discussed above, it was called selective dissemination
of information or current awareness . The
explosion in the availability of digital information has boosted the importance of
such systems, which are nowadays being used in contexts such as the creation of
personalized Web newspapers, junk e-mail blocking, and Usenet news selection.
Information ﬁltering by ML techniques is widely discussed in the literature: see
e.g. [Amati and Crestani 1999; Iyer et al. 2000; Kim et al. 2000; Tauritz et al. 2000;
Yu and Lam 1998].
3.4 Word sense disambiguation
Word sense disambiguation (WSD) is the activity of ﬁnding, given the occurrence
in a text of an ambiguous (i.e. polysemous or homonymous) word, the sense of this
particular word occurrence. For instance, bank may have (at least) two diﬀerent
senses in English, as in the Bank of England (a ﬁnancial institution) or the bank
of river Thames (a hydraulic engineering artifact).
It is thus a WSD task to
decide which of the above senses the occurrence of bank in Last week I borrowed
some money from the bank has. WSD is very important for many applications,
including natural language processing, and indexing documents by word senses
rather than by words for IR purposes. WSD may be seen as a TC task once we view word occurrence contexts as
documents and word senses as categories. Quite obviously, this is a single-label TC
F. Sebastiani
case, and one in which document-pivoted TC is usually the right choice.
WSD is just an example of the more general issue of resolving natural language ambiguities, one of the most important problems in computational linguistics. Other examples, which may all be tackled by means of TC techniques along
the lines discussed for WSD, are context-sensitive spelling correction, prepositional
phrase attachment, part of speech tagging, and word choice selection in machine
translation; see [Roth 1998] for an introduction.
3.5 Hierarchical categorization of Web pages
TC has recently aroused a lot of interest also for its possible application to automatically classifying Web pages, or sites, under the hierarchical catalogues hosted
by popular Internet portals. When Web documents are catalogued in this way,
rather than issuing a query to a general-purpose Web search engine a searcher may
ﬁnd it easier to ﬁrst navigate in the hierarchy of categories and then restrict her
search to a particular category of interest.
Classifying Web pages automatically has obvious advantages, since the manual
categorization of a large enough subset of the Web is infeasible.
Unlike in the
previous applications, it is typically the case that each category must be populated
by a set of k1 ≤x ≤k2 documents. CPC should be chosen so as to allow new
categories to be added and obsolete ones to be deleted.
With respect to previously discussed TC applications, automatic Web page categorization has two essential peculiarities:
(1) The hypertextual nature of the documents: links are a rich source of information,
as they may be understood as stating the relevance of the linked page to the
linking page. Techniques exploiting this intuition in a TC context have been
presented in [Attardi et al. 1998; Chakrabarti et al. 1998b; F¨urnkranz 1999;
G¨overt et al. 1999; Oh et al. 2000] and experimentally compared in [Yang et al.
(2) The hierarchical structure of the category set: this may be used e.g. by decomposing the classiﬁcation problem into a number of smaller classiﬁcation
problems, each corresponding to a branching decision at an internal node.
Techniques exploiting this intuition in a TC context have been presented in
[Dumais and Chen 2000; Chakrabarti et al. 1998a; Koller and Sahami 1997;
McCallum et al. 1998; Ruiz and Srinivasan 1999; Weigend et al. 1999].
4. THE MACHINE LEARNING APPROACH TO TEXT CATEGORIZATION
In the ’80s the most popular approach (at least in operational settings) for the
creation of automatic document classiﬁers consisted in manually building, by means
of knowledge engineering (KE) techniques, an expert system capable of taking TC
decisions. Such an expert system would typically consist of a set of manually deﬁned
logical rules, one per category, of type
if ⟨DNF formula⟩then ⟨category⟩
A DNF (“disjunctive normal form”) formula is a disjunction of conjunctive clauses;
the document is classiﬁed under ⟨category⟩iﬀit satisﬁes the formula, i.e. iﬀit
satisﬁes at least one of the clauses. The most famous example of this approach is
Machine Learning in Automated Text Categorization
((wheat & farm)
(wheat & commodity)
(bushels & export)
(wheat & tonnes)
(wheat & winter & ¬ soft))
then Wheat else ¬ Wheat
Rule-based classiﬁer for the Wheat category; keywords are indicated in italic, categories
are indicated in Small Caps .
the Construe system [Hayes et al. 1990], built by Carnegie Group for the Reuters
news agency. A sample rule of the type used in Construe is illustrated in Figure
The drawback of this approach is the knowledge acquisition bottleneck well-known
from the expert systems literature. That is, the rules must be manually deﬁned by
a knowledge engineer with the aid of a domain expert (in this case, an expert in the
membership of documents in the chosen set of categories): if the set of categories
is updated, then these two professionals must intervene again, and if the classiﬁer
is ported to a completely diﬀerent domain (i.e. set of categories) a diﬀerent domain
expert needs to intervene and the work has to be repeated from scratch.
On the other hand, it was originally suggested that this approach can give very
good eﬀectiveness results: Hayes et al. reported a .90 “breakeven” result
(see Section 7) on a subset of the Reuters test collection, a ﬁgure that outperforms
even the best classiﬁers built in the late ’90s by state-of-the-art ML techniques.
However, no other classiﬁer has been tested on the same dataset as Construe,
and it is not clear whether this was a randomly chosen or a favourable subset of
the entire Reuters collection. As argued in [Yang 1999], the results above do not
allow us to state that these eﬀectiveness results may be obtained in general.
Since the early ’90s, the ML approach to TC has gained popularity and has eventually become the dominant one, at least in the research community . In this approach a general inductive process (also called the learner) automatically builds a classiﬁer for a category
ci by observing the characteristics of a set of documents manually classiﬁed under ci or ci by a domain expert; from these characteristics, the inductive process
gleans the characteristics that a new unseen document should have in order to be
classiﬁed under ci. In ML terminology, the classiﬁcation problem is an activity of
supervised learning, since the learning process is “supervised” by the knowledge of
the categories and of the training instances that belong to them2.
The advantages of the ML approach over the KE approach are evident. The
engineering eﬀort goes towards the construction not of a classiﬁer, but of an automatic builder of classiﬁers (the learner). This means that if a learner is (as it often
is) available oﬀ-the-shelf, all that is needed is the inductive, automatic construction
of a classiﬁer from a set of manually classiﬁed documents. The same happens if
a classiﬁer already exists and the original set of categories is updated, or if the
classiﬁer is ported to a completely diﬀerent domain.
2Within the area of content-based document management tasks, an example of an unsupervised
learning activity is document clustering (see Section 1).
F. Sebastiani
In the ML approach the preclassiﬁed documents are then the key resource. In the
most favourable case they are already available; this typicaly happens for organizations which have previously carried out the same categorization activity manually
and decide to automate the process. The less favourable case is when no manually
classiﬁed documents are available; this typicaly happens for organizations which
start a categorization activity and opt for an automated modality straightaway.
The ML approach is more convenient than the KE approach also in this latter
case. In fact, it is easier to manually classify a set of documents than to build and
tune a set of rules, since it is easier to characterize a concept extensionally (i.e. to
select instances of it) than intensionally (i.e. to describe the concept in words, or
to describe a procedure for recognizing its instances).
Classiﬁers built by means of ML techniques nowadays achieve impressive levels
of eﬀectiveness (see Section 7), making automatic classiﬁcation a qualitatively (and
not only economically) viable alternative to manual classiﬁcation.
4.1 Training set, test set, and validation set
The ML approach relies on the availability of an initial corpus Ω= {d1, . . . , d|Ω|} ⊂
D of documents preclassiﬁed under C = {c1, . . . , c|C|}. That is, the values of the
total function ˘Φ : D × C →{T, F} are known for every pair ⟨dj, ci⟩∈Ω× C. A
document dj is a positive example of ci if ˘Φ(dj, ci) = T , a negative example of ci if
˘Φ(dj, ci) = F.
In research settings (and in most operational settings too), once a classiﬁer Φ has
been built it is desirable to evaluate its eﬀectiveness. In this case, prior to classiﬁer
construction the initial corpus is split in two sets, not necessarily of equal size:
—a training(-and-validation) set T V = {d1, . . . , d|T V |}. The classiﬁer Φ for categories C = {c1, . . . , c|C|} is inductively built by observing the characteristics of
these documents;
—a test set T e = {d|T V |+1, . . . , d|Ω|}, used for testing the eﬀectiveness of the classiﬁers. Each dj ∈T e is fed to the classiﬁer, and the classiﬁer decisions Φ(dj, ci)
are compared with the expert decisions ˘Φ(dj, ci). A measure of classiﬁcation effectiveness is based on how often the Φ(dj, ci) values match the ˘Φ(dj, ci) values.
The documents in T e cannot participate in any way in the inductive construction of
the classiﬁers; if this condition were not satisﬁed the experimental results obtained
would likely be unrealistically good, and the evaluation would thus have no scientiﬁc
character [Mitchell 1996, page 129]. In an operational setting, after evaluation has
been performed one would typically re-train the classiﬁer on the entire initial corpus,
in order to boost eﬀectiveness. In this case the results of the previous evaluation
would be a pessimistic estimate of the real performance, since the ﬁnal classiﬁer
has been trained on more data than the classiﬁer evaluated.
This is called the train-and-test approach. An alternative is the k-fold crossvalidation approach , in which k diﬀerent classiﬁers Φ1, . . . , Φk are built by partitioning the initial corpus into k disjoint sets
T e1, . . . , T ek and then iteratively applying the train-and-test approach on pairs
⟨T Vi = Ω−T ei, T ei⟩.
The ﬁnal eﬀectiveness ﬁgure is obtained by individually
computing the eﬀectiveness of Φ1, . . . , Φk, and then averaging the individual re-
Machine Learning in Automated Text Categorization
sults in some way.
In both approaches it is often the case that the internal parameters of the classi-
ﬁers must be tuned, by testing which values of the parameters yield the best eﬀectiveness. In order to make this optimization possible, in the train-and-test approach
the set {d1, . . . , d|T V |} is further split into a training set T r = {d1, . . . , d|T r|}, from
which the classiﬁer is built, and a validation set V a = {d|T r|+1, . . . , d|T V |} (sometimes called a hold-out set), on which the repeated tests of the classiﬁer aimed
at parameter optimization are performed; the obvious variant may be used in the
k-fold cross-validation case. Note that, for the same reason why we do not test a
classiﬁer on the documents it has been trained on, we do not test it on the documents it has been optimized on: test set and validation set must be kept separate3.
Given a corpus Ω, one may deﬁne the generality gΩ(ci) of a category ci as the
percentage of documents that belong to ci, i.e.:
gΩ(ci) = |{dj ∈Ω| ˘Φ(dj, ci) = T }|
The training set generality gT r(ci), validation set generality gV a(ci), and test set
generality gT e(ci) of ci may be deﬁned in the obvious way.
4.2 Information retrieval techniques and text categorization
Text categorization heavily relies on the basic machinery of IR. The reason is that
TC is a content-based document management task, and as such it shares many
characteristics with other IR tasks such as text search.
IR techniques are used in three phases of the text classiﬁer life cycle:
(1) IR-style indexing is always performed on the documents of the initial corpus
and on those to be classiﬁed during the operational phase;
(2) IR-style techniques (such as document-request matching, query reformulation,
. . . ) are often used in the inductive construction of the classiﬁers;
(3) IR-style evaluation of the eﬀectiveness of the classiﬁers is performed.
The various approaches to classiﬁcation diﬀer mostly for how they tackle (2), although in a few cases non-standard approaches to (1) and (3) are also used. Indexing, induction and evaluation are the themes of Sections 5, 6 and 7, respectively.
5. DOCUMENT INDEXING AND DIMENSIONALITY REDUCTION
5.1 Document indexing
Texts cannot be directly interpreted by a classiﬁer or by a classiﬁer-building algorithm. Because of this, an indexing procedure that maps a text dj into a compact
representation of its content needs to be uniformly applied to training, validation
and test documents. The choice of a representation for text depends on what one
regards as the meaningful units of text (the problem of lexical semantics) and the
meaningful natural language rules for the combination of these units (the problem
3From now on, we will take the freedom to use the expression “test document” to denote any
document not in the training set and validation set. This includes thus any document submitted
to the classiﬁer in the operational phase.
F. Sebastiani
of compositional semantics). Similarly to what happens in IR, in TC this latter
problem is usually disregarded4, and a text dj is usually represented as a vector of
term weights ⃗dj = ⟨w1j, . . . , w|T |j⟩, where T is the set of terms (sometimes called
features) that occur at least once in at least one document of T r, and 0 ≤wkj ≤1
represents, loosely speaking, how much term tk contributes to the semantics of
document dj. Diﬀerences among approaches are accounted for by
(1) diﬀerent ways to understand what a term is;
(2) diﬀerent ways to compute term weights.
A typical choice for (1) is to identify terms with words. This is often called either the
set of words or the bag of words approach to document representation, depending
on whether weights are binary or not.
In a number of experiments [Apt´e et al. 1994; Dumais et al. 1998; Lewis 1992a]
it has been found that representations more sophisticated than this do not yield
signiﬁcantly better eﬀectiveness, thereby conﬁrming similar results from IR [Salton
and Buckley 1988]. In particular, some authors have used phrases, rather than
individual words, as indexing terms [Fuhr et al. 1991; Sch¨utze et al. 1995; Tzeras
and Hartmann 1993], but the experimental results found to date have not been
uniformly encouraging, irrespectively of whether the notion of “phrase” is motivated
—syntactically, i.e. the phrase is such according to a grammar of the language ;
—statistically, i.e. the phrase is not grammatically such, but is composed of a
set/sequence of words whose patterns of contiguous occurrence in the collection
are statistically signiﬁcant .
Lewis [1992a] argues that the likely reason for the discouraging results is that,
although indexing languages based on phrases have superior semantic qualities,
they have inferior statistical qualities with respect to word-only indexing languages:
a phrase-only indexing language has “more terms, more synonymous or nearly
synonymous terms, lower consistency of assignment (since synonymous terms are
not assigned to the same documents), and lower document frequency for terms”
[Lewis 1992a, page 40]. Although his remarks are about syntactically motivated
phrases, they also apply to statistically motivated ones, although perhaps to a
smaller degree. A combination of the two approaches is probably the best way to
go: Tzeras and Hartmann obtained signiﬁcant improvements by using noun
phrases obtained through a combination of syntactic and statistical criteria, where
a “crude” syntactic method was complemented by a statistical ﬁlter (only those
syntactic phrases that occurred at least three times in the positive examples of
a category ci were retained). It is likely that the ﬁnal word on the usefulness of
phrase indexing in TC has still to be told, and investigations in this direction are
still being actively pursued [Caropreso et al. 2001; Mladeni´c and Grobelnik 1998].
As for issue (2), weights usually range between 0 and 1 , and for ease of exposition we will assume they always do. As a special
case, binary weights may be used ; whether binary or non-binary weights are used depends on the
classiﬁer learning algorithm used. In the case of non-binary indexing, for determining the weight wkj of term tk in document dj any IR-style indexing technique
that represents a document as a vector of weighted terms may be used. Most of
the times, the standard tfidf function is used ,
tfidf(tk, dj) = #(tk, dj) · log
where #(tk, dj) denotes the number of times tk occurs in dj, and #T r(tk) denotes
the document frequency of term tk, i.e. the number of documents in T r in which
tk occurs. This function embodies the intuitions that (i) the more often a term
occurs in a document, the more it is representative of its content, and (ii) the more
documents a term occurs in, the less discriminating it is5. Note that this formula
(as most other indexing formulae) weights the importance of a term to a document
in terms of occurrence considerations only, thereby deeming of null importance
the order in which the terms occur in the document and the syntactic role they
In other words, the semantics of a document is reduced to the collective
lexical semantics of the terms that occur in it, thereby disregarding the issue of
compositional semantics .
In order for the weights to fall in the interval and for the documents to be
represented by vectors of equal length, the weights resulting from tfidf are often
normalized by cosine normalization, given by:
tfidf(tk, dj)
s=1(tfidf(ts, dj))2
Although normalized tfidf is the most popular one, other indexing functions have
also been used, including probabilistic techniques [G¨overt et al. 1999] or techniques
for indexing structured documents [Larkey and Croft 1996]. Functions diﬀerent
from tfidf are especially needed when T r is not available in its entirety from the
start and #T r(tk) cannot thus be computed, as e.g. in adaptive ﬁltering; in this
case approximations of tfidf are usually employed [Dagan et al. 1997, Section 4.3].
Before indexing, the removal of function words (i.e. topic-neutral words such as
articles, prepositions, conjunctions, etc.) is almost always performed 6. Concerning stemming
(i.e. grouping words that share the same morphological root), its suitability to TC
is controversial. Although, similarly to unsupervised term clustering (see Section
5.5.1) of which it is an instance, stemming has sometimes been reported to hurt
eﬀectiveness , the recent tendency is to adopt it,
5There exist many variants of tfidf, that diﬀer from each other in terms of logarithms, normalization or other correction factors. Formula 1 is just one of the possible instances of this class; see
[Salton and Buckley 1988; Singhal et al. 1996] for variations on this theme.
6One application of TC in which it would be inappropriate to remove function words is author
identiﬁcation for documents of disputed paternity.
In fact, as noted in [Manning and Sch¨utze
1999, page 589], “it is often the ‘little’ words that give an author away (for example, the relative
frequencies of words like because or though)”.
F. Sebastiani
as it reduces both the dimensionality of the term space (see Section 5.3) and the
stochastic dependence between terms (see Section 6.2).
Depending on the application, either the full text of the document or selected
parts of it are indexed. While the former option is the rule, exceptions exist. For
instance, in a patent categorization application Larkey indexes only the title,
the abstract, the ﬁrst 20 lines of the summary, and the section containing the claims
of novelty of the described invention. This approach is made possible by the fact
that documents describing patents are structured. Similarly, when a document title
is available, one can pay extra importance to the words it contains [Apt´e et al. 1994;
Cohen and Singer 1999; Weiss et al. 1999]. When documents are ﬂat, identifying
the most relevant part of a document is instead a non-obvious task.
5.2 The Darmstadt Indexing Approach
The AIR/X system [Fuhr et al. 1991] occupies a special place in the literature on
indexing for TC. This system is the ﬁnal result of the AIR project, one of the most
important eﬀorts in the history of TC: spanning a duration of more than ten years
[Knorz 1982; Tzeras and Hartmann 1993], it has produced a system operatively
employed since 1985 in the classiﬁcation of corpora of scientiﬁc literature of O(105)
documents and O(104) categories, and has had important theoretical spin-oﬀs in
the ﬁeld of probabilistic indexing [Fuhr 1989; Fuhr and Buckley 1991]7.
The approach to indexing taken in AIR/X is known as the Darmstadt Indexing
Approach (DIA) [Fuhr 1985].
Here, “indexing” is used in the sense of Section
3.1, i.e. as using terms from a controlled vocabulary, and is thus a synonym of
TC . The idea that underlies the DIA is the use of a much wider set of “features”
than described in Section 5.1. All other approaches mentioned in this paper view
terms as the dimensions of the learning space, where terms may be single words,
stems, phrases, or (see Sections 5.5.1 and 5.5.2) combinations of any of these. In
contrast, the DIA considers properties (of terms, documents, categories, or pairwise
relationships among these) as basic dimensions of the learning space. Examples of
—properties of a term tk: e.g. the idf of tk;
—properties of the relationship between a term tk and a document dj: e.g. the tf
of tk in dj; or the location (e.g. in the title, or in the abstract) of tk within dj;
—properties of a document dj: e.g. the length of dj;
—properties of a category ci: e.g. the training set generality of ci.
For each possible document-category pair, the values of these features are collected
in a so-called relevance description vector ⃗rd(dj, ci). The size of this vector is determined by the number of properties considered, and is thus independent of speciﬁc
terms, categories or documents , and its experiments, have also been richly
documented in a series of papers and doctoral theses written in German. The interested reader
may consult [Fuhr et al. 1991] for a detailed bibliography.
Machine Learning in Automated Text Categorization
functions are applied in order to yield a single value to be included in ⃗rd(dj, ci)); in
this way an abstraction from speciﬁc terms, categories or documents is achieved.
The main advantage of this approach is the possibility to consider additional
features that can hardly be accounted for in the usual term-based approaches, e.g.
the location of a term within a document, or the certainty with which a phrase was
identiﬁed in a document. The term-category relationship is described by estimates,
derived from the training set, of the probability P(ci|tk) that a document belongs to
category ci, given that it contains term tk (the DIA association factor)8. Relevance
description vectors ⃗rd(dj, ci) are then the ﬁnal representations that are used for the
classiﬁcation of document dj under category ci.
The essential ideas of the DIA – transforming the classiﬁcation space by means
of abstraction and using a more detailed text representation than the standard
bag-of-words approach – have not been taken up by other researchers so far. For
new TC applications dealing with structured documents or categorization of Web
pages, these ideas may become of increasing importance.
5.3 Dimensionality reduction
Unlike in text retrieval, in TC the high dimensionality of the term space (i.e. the
large value of |T |) may be problematic. In fact, while typical algorithms used in text
retrieval (such as cosine matching) can scale to high values of |T |, the same does
not hold of many sophisticated learning algorithms used for classiﬁer induction . Because of this, before classiﬁer
induction one often applies a pass of dimensionality reduction (DR), whose eﬀect
is to reduce the size of the vector space from |T | to |T ′| ≪|T |; the set T ′ is called
the reduced term set.
DR is also beneﬁcial since it tends to reduce overﬁtting, i.e. the phenomenon
by which a classiﬁer is tuned also to the contingent characteristics of the training
data rather than just the constitutive characteristics of the categories. Classiﬁers
which overﬁt the training data are good at re-classifying the data they have been
trained on, but much worse at classifying previously unseen data. Experiments
have shown that in order to avoid overﬁtting a number of training examples roughly
proportional to the number of terms used is needed; Fuhr and Buckley [1991, page
235] have suggested that 50-100 training examples per term may be needed in TC
tasks. This means that if DR is performed, overﬁtting may be avoided even if a
smaller amount of training examples is used. However, in removing terms the risk
is to remove potentially useful information on the meaning of the documents. It is
then clear that, in order to obtain optimal (cost-)eﬀectiveness, the reduction process
must be performed with care. Various DR methods have been proposed, either from
the information theory or from the linear algebra literature, and their relative merits
have been tested by experimentally evaluating the variation in eﬀectiveness that a
given classiﬁer undergoes after application of the function to the term space.
There are two distinct ways of viewing DR, depending on whether the task is
performed locally (i.e. for each individual category) or globally:
8Association factors are called adhesion coeﬃcients in many early papers on TC; see e.g. [Field
1975; Robertson and Harding 1984].
F. Sebastiani
—local DR: for each category ci, a set T ′
i of terms, with |T ′
i | ≪|T |, is chosen for
classiﬁcation under ci . This means that diﬀerent subsets of ⃗dj are used when
working with the diﬀerent categories. Typical values are 10 ≤|T ′
—global DR: a set T ′ of terms, with |T ′| ≪|T |, is chosen for the classiﬁcation
under all categories C = {c1, . . . , c|C|} .
This distinction usually does not impact on the choice of DR technique, since most
such techniques can be used (and have been used) for local and global DR alike
(supervised DR techniques – see Section 5.5.1 – are exceptions to this rule). In
the rest of this section we will assume that the global approach is used, although
everything we will say also applies to the local approach.
A second, orthogonal distinction may be drawn in terms of the nature of the
resulting terms:
—DR by term selection: T ′ is a subset of T ;
—DR by term extraction: the terms in T ′ are not of the same type of the terms in
T (e.g. if the terms in T are words, the terms in T ′ may not be words at all),
but are obtained by combinations or transformations of the original ones.
Unlike in the previous distinction, these two ways of doing DR are tackled by very
diﬀerent techniques; we will address them separately in the next two sections.
5.4 Dimensionality reduction by term selection
Given a predetermined integer r, techniques for term selection (also called term
space reduction – TSR) attempt to select, from the original set T , the set T ′ of
terms (with |T ′| ≪|T |) that, when used for document indexing, yields the highest
eﬀectiveness. Yang and Pedersen have shown that TSR may even result in
a moderate (≤5%) increase in eﬀectiveness, depending on the classiﬁer, on the
aggressivity
|T ′| of the reduction, and on the TSR technique used.
Moulinier et al. have used a so-called wrapper approach, i.e. one in which T ′
is identiﬁed by means of the same learning method which will be used for building
the classiﬁer [John et al. 1994].
Starting from an initial term set, a new term
set is generated by either adding or removing a term. When a new term set is
generated, a classiﬁer based on it is built and then tested on a validation set. The
term set that results in the best eﬀectiveness is chosen. This approach has the
advantage of being tuned to the learning algorithm being used; moreover, if local
DR is performed, diﬀerent numbers of terms for diﬀerent categories may be chosen,
depending on whether a category is or is not easily separable from the others.
However, the sheer size of the space of diﬀerent term sets makes its cost prohibitive
for standard TC applications.
A computationally easier alternative is the ﬁltering approach [John et al. 1994],
i.e. keeping the |T ′| ≪|T | terms that receive the highest score according to a
function that measures the “importance” of the term for the TC task. We will
explore this solution in the rest of this section.
Machine Learning in Automated Text Categorization
5.4.1 Document frequency. A simple and eﬀective global TSR function is the document frequency #T r(tk) of a term tk, i.e. only the terms that occur in the highest
number of documents are retained. In a series of experiments Yang and Pedersen
 have shown that with #T r(tk) it is possible to reduce the dimensionality by
a factor of 10 with no loss in eﬀectiveness (a reduction by a factor of 100 bringing
about just a small loss).
This seems to indicate that the terms occurring most frequently in the collection
are the most valuable for TC. As such, this would seem to contradict a well-known
“law” of IR, according to which the terms with low-to-medium document frequency
are the most informative ones [Salton and Buckley 1988]. But these two results do
not contradict each other, since it is well-known that
the large majority of the words occurring in a corpus have a very low document
frequency; this means that by reducing the term set by a factor of 10 using document
frequency, only such words are removed, while the words from low-to-medium to
high document frequency are preserved. Of course, stop words need to be removed
in advance, lest only topic-neutral words are retained [Mladeni´c 1998].
Finally, note that a slightly more empirical form of TSR by document frequency
is adopted by many authors, who remove all terms occurring in at most x training
documents (popular values for x range from 1 to 3), either as the only form of DR
[Maron 1961; Ittner et al. 1995] or before applying another more sophisticated form
[Dumais et al. 1998; Li and Jain 1998]. A variant of this policy is removing all terms
that occur at most x times in the training set , with popular values for x ranging from 1 
to 5 .
5.4.2 Other information-theoretic term selection functions. Other more sophisticated information-theoretic functions have been used in the literature, among
which the DIA association factor [Fuhr et al. 1991], chi-square [Caropreso et al.
2001; Galavotti et al. 2000; Sch¨utze et al. 1995; Sebastiani et al. 2000; Yang and
Pedersen 1997; Yang and Liu 1999], NGL coeﬃcient [Ng et al. 1997; Ruiz and Srinivasan 1999], information gain [Caropreso et al. 2001; Larkey 1998; Lewis 1992a;
Lewis and Ringuette 1994; Mladeni´c 1998; Moulinier and Ganascia 1996; Yang and
Pedersen 1997; Yang and Liu 1999], mutual information [Dumais et al. 1998; Lam
et al. 1997; Larkey and Croft 1996; Lewis and Ringuette 1994; Li and Jain 1998;
Moulinier et al. 1996; Ruiz and Srinivasan 1999; Taira and Haruno 1999; Yang and
Pedersen 1997], odds ratio [Caropreso et al. 2001; Mladeni´c 1998; Ruiz and Srinivasan 1999], relevancy score [Wiener et al. 1995], and GSS coeﬃcient [Galavotti
et al. 2000]. The mathematical deﬁnitions of these measures are summarized for
convenience in Table 19. Here, probabilities are interpreted on an event space of
documents (e.g. P(tk, ci) denotes the probability that, for a random document x,
term tk does not occur in x and x belongs to category ci), and are estimated by
9For better uniformity Table 1 views all the TSR functions in terms of subjective probability. In
some cases such as #(tk, ci) and χ2(tk, ci) this is slightly artiﬁcial, since these two functions are
not usually viewed in probabilistic terms. The formulae refer to the “local” (i.e. category-speciﬁc)
forms of the functions, which again is slightly artiﬁcial in some cases (e.g. #(tk, ci)). Note that
the NGL and GSS coeﬃcients are here named after their authors, since they had originally been
given names that might generate some confusion if used here.
F. Sebastiani
Denoted by
Mathematical form
Document frequency
DIA association factor
Information gain
IG(tk, ci)
P (t, c) · log
P (t) · P (c)
Mutual information
MI(tk, ci)
P (tk, ci)
P (tk) · P (ci)
Chi-square
χ2(tk, ci)
|Tr| · [P (tk, ci) · P (tk, ci) −P (tk, ci) · P (tk, ci)]2
P (tk) · P (tk) · P (ci) · P (ci)
NGL coeﬃcient
NGL(tk, ci)
|Tr| · [P (tk, ci) · P (tk, ci) −P (tk, ci) · P (tk, ci)]
P (tk) · P (tk) · P (ci) · P (ci)
Relevancy score
RS(tk, ci)
log P (tk|ci) + d
P (tk|ci) + d
Odds Ratio
OR(tk, ci)
P (tk|ci) · (1 −P (tk|ci))
(1 −P (tk|ci)) · P (tk|ci)
GSS coeﬃcient
GSS(tk, ci)
P (tk, ci) · P (tk, ci) −P (tk, ci) · P (tk, ci)
Main functions used for term space reduction purposes. Information gain is also known
as expected mutual information; it is used under this name by Lewis [1992a, page 44] and Larkey
 . In the RS(tk, ci) formula d is a constant damping factor.
counting occurrences in the training set. All functions are speciﬁed “locally” to a
speciﬁc category ci; in order to assess the value of a term tk in a “global”, categoryindependent sense, either the sum fsum(tk) = P|C|
i=1 f(tk, ci), or the weighted average fwavg(tk) = P|C|
i=1 P(ci)f(tk, ci), or the maximum fmax(tk) = max|C|
i=1 f(tk, ci)
of their category-speciﬁc values f(tk, ci) are usually computed.
These functions try to capture the intuition that the best terms for ci are the
ones distributed most diﬀerently in the sets of positive and negative examples of
ci. However, interpretations of this principle vary across diﬀerent functions. For
instance, in the experimental sciences χ2 is used to measure how the results of an
observation diﬀer (i.e. are independent) from the results expected according to an
initial hypothesis (lower values indicate lower dependence). In DR we measure how
independent tk and ci are. The terms tk with the lowest value for χ2(tk, ci) are
thus the most independent from ci; since we are interested in the terms which are
not, we select the terms for which χ2(tk, ci) is highest.
While each TSR function has its own rationale, the ultimate word on its value
is the eﬀectiveness it brings about.
Various experimental comparisons of TSR
functions have thus been carried out [Caropreso et al. 2001; Galavotti et al. 2000;
Mladeni´c 1998; Yang and Pedersen 1997]. In these experiments most functions
listed in Table 1 (with the possible exception of MI) have improved on the results
of document frequency. For instance, Yang and Pedersen have shown that,
with various classiﬁers and various initial corpora, sophisticated techniques such
Machine Learning in Automated Text Categorization
as IGsum(tk, ci) or χ2
max(tk, ci) can reduce the dimensionality of the term space
by a factor of 100 with no loss (or even with a small increase) of eﬀectiveness.
Collectively, the experiments reported in the above-mentioned papers seem to indicate that {ORsum, NGLsum, GSSmax} > {χ2
max, IGsum} > {#wavg, χ2
{MImax, MIwavg}, where “>” means “performs better than”. However, it should
be noted that these results are just indicative, and that more general statements
on the relative merits of these functions could be made only as a result of comparative experiments performed in thoroughly controlled conditions and on a variety
of diﬀerent situations (e.g. diﬀerent classiﬁers, diﬀerent initial corpora, . . . ).
5.5 Dimensionality reduction by term extraction
Given a predetermined |T ′| ≪|T |, term extraction attempts to generate, from
the original set T , a set T ′ of “synthetic” terms that maximize eﬀectiveness. The
rationale for using synthetic (rather than naturally occurring) terms is that, due to
the pervasive problems of polysemy, homonymy and synonymy, the original terms
may not be optimal dimensions for document content representation.
for term extraction try to solve these problems by creating artiﬁcial terms that
do not suﬀer from them. Any term extraction method consists in (i) a method
for extracting the new terms from the old ones, and (ii) a method for converting
the original document representations into new representations based on the newly
synthesized dimensions. Two term extraction methods have been experimented in
TC, namely term clustering and latent semantic indexing.
5.5.1 Term clustering. Term clustering tries to group words with a high degree
of pairwise semantic relatedness, so that the groups (or their centroids, or a representative of them) may be used instead of the terms as dimensions of the vector
space. Term clustering is diﬀerent from term selection, since the former tends to
address terms synonymous (or near-synonymous) with other terms, while the latter
targets non-informative terms10.
Lewis [1992a] was the ﬁrst to investigate the use of term clustering in TC. The
method he employed, called reciprocal nearest neighbour clustering, consists in creating clusters of two terms that are one the most similar to the other according to some measure of similarity. His results were inferior to those obtained by
single-word indexing, possibly due to a disappointing performance by the clustering
method: as Lewis [1992a, page 48] says, “The relationships captured in the clusters
are mostly accidental, rather than the systematic relationships that were hoped
Li and Jain view semantic relatedness between words in terms of their
co-occurrence and co-absence within training documents. By using this technique
in the context of a hierarchical clustering algorithm they witnessed only a marginal
eﬀectiveness improvement; however, the small size of their experiment (see Section
6.11) hardly allows any deﬁnitive conclusion to be reached.
Both [Lewis 1992a; Li and Jain 1998] are examples of unsupervised clustering,
since clustering is not aﬀected by the category labels attached to the documents.
Baker and McCallum provide instead an example of supervised clustering, as
10Some term selection methods, such as wrapper methods, also address the problem of redundancy.
F. Sebastiani
the distributional clustering method they employ clusters together those terms that
tend to indicate the presence of the same category, or group of categories. Their
experiments, carried out in the context of a Na¨ıve Bayes classiﬁer (see Section
6.2), showed only a 2% eﬀectiveness loss with an aggressivity of 1000, and even
showed some eﬀectiveness improvement with less aggressive levels of reduction.
Later experiments by Slonim and Tishby have conﬁrmed the potential of
supervised clustering methods for term extraction.
5.5.2 Latent semantic indexing. Latent semantic indexing is a DR technique developed in IR in order to address the problems deriving from the use of synonymous, near-synonymous and polysemous words
as dimensions of document representations. This technique compresses document
vectors into vectors of a lower-dimensional space whose dimensions are obtained
as combinations of the original dimensions by looking at their patterns of cooccurrence. In practice, LSI infers the dependence among the original terms from
a corpus and “wires” this dependence into the newly obtained, independent dimensions. The function mapping original vectors into new vectors is obtained by
applying a singular value decomposition to the matrix formed by the original document vectors. In TC this technique is applied by deriving the mapping function
from the training set and then applying it to training and test documents alike.
One characteristic of LSI is that the newly obtained dimensions are not, unlike in
term selection and term clustering, intuitively interpretable. However, they work
well in bringing out the “latent” semantic structure of the vocabulary used in the
corpus. For instance, Sch¨utze et al. [1995, page 235] discuss the classiﬁcation under
category Demographic shifts in the U.S. with economic impact of a document that was indeed a positive test instance for the category, and that contained,
among others, the quite revealing sentence “The nation grew to 249.6 million
people in the 1980s as more Americans left the industrial and agricultural heartlands for the South and West”. The classiﬁer decision was incorrect when local DR had been performed by χ2-based term selection retaining the
top original 200 terms, but was correct when the same task was tackled by means
of LSI. This well exempliﬁes how LSI works: the above sentence does not contain
any of the 200 terms most relevant to the category selected by χ2, but quite possibly the words contained in it have concurred to produce one or more of the LSI
higher-order terms that generate the document space of the category. As Sch¨utze
et al. [1995, page 230] put it, “if there is a great number of terms which all contribute a small amount of critical information, then the combination of evidence is
a major problem for a term-based classiﬁer”. A drawback of LSI, though, is that if
some original term is particularly good in itself at discriminating a category, that
discrimination power may be lost in the new vector space.
Wiener et al. use LSI in two alternative ways: (i) for local DR, thus creating
several category-speciﬁc LSI representations, and (ii) for global DR, thus creating a
single LSI representation for the entire category set. Their experiments showed the
former approach to perform better than the latter, and both approaches to perform
better than simple TSR based on Relevancy Score (see Table 1).
Sch¨utze et al. experimentally compared LSI-based term extraction with
χ2-based TSR using three diﬀerent classiﬁer learning techniques (namely, linear
Machine Learning in Automated Text Categorization
discriminant analysis, logistic regression and neural networks). Their experiments
showed LSI to be far more eﬀective than χ2 for the ﬁrst two techniques, while both
methods performed equally well for the neural network classiﬁer.
For other TC works that use LSI or similar term extraction techniques see e.g.
[Hull 1994; Li and Jain 1998; Sch¨utze 1998; Weigend et al. 1999; Yang 1995].
6. INDUCTIVE CONSTRUCTION OF TEXT CLASSIFIERS
The inductive construction of text classiﬁers has been tackled in a variety of ways.
Here we will deal only with the methods that have been most popular in TC, but
we will also brieﬂy mention the existence of alternative, less standard approaches.
We start by discussing the general form that a text classiﬁer has. Let us recall
from Section 2.4 that there are two alternative ways of viewing classiﬁcation: “hard”
(fully automated) classiﬁcation and ranking (semi-automated) classiﬁcation.
The inductive construction of a ranking classiﬁer for category ci ∈C usually
consists in the deﬁnition of a function CSVi : D → that, given a document
dj, returns a categorization status value for it, i.e. a number between 0 and 1 that,
roughly speaking, represents the evidence for the fact that dj ∈ci. Documents
are then ranked according to their CSVi value. This works for “document-ranking
TC”; “category-ranking TC” is usually tackled by ranking, for a given document
dj, its CSVi scores for the diﬀerent categories in C = {c1, . . . , c|C|}.
The CSVi function takes up diﬀerent meanings according to the learning method
used: for instance, in the “Na¨ıve Bayes” approach of Section 6.2 CSVi(dj) is deﬁned
in terms of a probability, whereas in the “Rocchio” approach discussed in Section
6.7 CSVi(dj) is a measure of vector closeness in |T |-dimensional space.
The construction of a “hard” classiﬁer may follow two alternative paths. The
former consists in the deﬁnition of a function CSVi : D →{T, F}. The latter
consists instead in the deﬁnition of a function CSVi : D → , analogous to the
one used for ranking classiﬁcation, followed by the deﬁnition of a threshold τi such
that CSVi(dj) ≥τi is interpreted as T while CSVi(dj) < τi is interpreted as F 11.
The deﬁnition of thresholds will be the topic of Section 6.1. In Sections 6.2 to
6.12 we will instead concentrate on the deﬁnition of CSVi, discussing a number of
approaches that have been applied in the TC literature. In general we will assume
we are dealing with “hard” classiﬁcation; it will be evident from the context how and
whether the approaches can be adapted to ranking classiﬁcation. The presentation
of the algorithms will be mostly qualitative rather than quantitative, i.e. will focus
on the methods for classiﬁer learning rather than on the eﬀectiveness and eﬃciency
of the classiﬁers built by means of them; this will instead be the focus of Section 7.
6.1 Determining thresholds
There are various policies for determining the threshold τi, also depending on the
constraints imposed by the application. The most important distinction is whether
the threshold is derived analytically or experimentally.
The former method is possible only in the presence of a theoretical result that
indicates how to compute the threshold that maximizes the expected value of the
11Alternative methods are possible, such as training a classiﬁer for which some standard, prede-
ﬁned value such as 0 is the threshold. For ease of exposition we will not discuss them.
F. Sebastiani
eﬀectiveness function [Lewis 1995a]. This is typical of classiﬁers that output probability estimates of the membership of dj in ci (see Section 6.2) and whose eﬀectiveness is computed by decision-theoretic measures such as utility (see Section 7.1.3);
we thus defer the discussion of this policy to Section 7.1.3.
When such a theoretical result is not available one has to revert to the latter
method, which consists in testing diﬀerent values for τi on a validation set and
choosing the value which maximizes eﬀectiveness. We call this policy CSV thresholding [Cohen and Singer 1999; Schapire et al. 1998; Wiener et al. 1995]; it is also
called Scut in [Yang 1999]. Diﬀerent τi’s are typically chosen for the diﬀerent ci’s.
A second, popular experimental policy is proportional thresholding [Iwayama and
Tokunaga 1995; Larkey 1998; Lewis 1992a; Lewis and Ringuette 1994; Wiener et al.
1995], also called Pcut in [Yang 1999]. This policy consists in choosing the value of
τi for which gV a(ci) is closest to gT r(ci), and embodies the principle that the same
percentage of documents of both training and test set should be classiﬁed under ci.
For obvious reasons, this policy does not lend itself to document-pivoted TC.
Sometimes, depending on the application, a ﬁxed thresholding policy is applied, whereby it is
stipulated that a ﬁxed number k of categories, equal for all dj’s, are to be assigned
to each document dj. This is often used, for instance, in applications of TC to
automated document indexing [Field 1975; Lam et al. 1999]. Strictly speaking,
however, this is not a thresholding policy in the sense deﬁned at the beginning of
Section 6, as it might happen that d′ is classiﬁed under ci, d′′ is not, and CSVi(d′) <
CSVi(d′′). Quite clearly, this policy is mostly at home with document-pivoted TC.
However, it suﬀers from a certain coarseness, as the fact that k is equal for all
documents (nor could this be otherwise) allows no ﬁne-tuning.
In his experiments Lewis [1992a] found the proportional policy to be superior to probability thresholding when microaveraged eﬀectiveness was tested but
slightly inferior with macroaveraging (see Section 7.1.1). Yang found instead
CSV thresholding to be superior to proportional thresholding (possibly due to her
category-speciﬁc optimization on a validation set), and found ﬁxed thresholding to
be consistently inferior to the other two policies. The fact that these latter results
have been obtained across diﬀerent classiﬁers no doubt reinforce them.
In general, aside from the considerations above, the choice of the thresholding
policy may also be inﬂuenced by the application; for instance, in applying a text
classiﬁer to document indexing for Boolean systems a ﬁxed thresholding policy
might be chosen, while a proportional or CSV thresholding method might be chosen
for Web page classiﬁcation under hierarchical catalogues.
6.2 Probabilistic classiﬁers
Probabilistic classiﬁers view CSVi(dj)
in terms of P(ci|⃗dj), i.e. the probability that a document represented by a vector
⃗dj = ⟨w1j, . . . , w|T |j⟩of (binary or weighted) terms belongs to ci, and compute this
probability by an application of Bayes’ theorem, given by
P(ci|⃗dj) = P(ci)P(⃗dj|ci)
Machine Learning in Automated Text Categorization
In (3) the event space is the space of documents: P(⃗dj) is thus the probability
that a randomly picked document has vector ⃗dj as its representation, and P(ci) the
probability that a randomly picked document belongs to ci.
The estimation of P(⃗dj|ci) in (3) is problematic, since the number of possible
vectors ⃗dj is too high (the same holds for P(⃗dj), but for reasons that will be
clear shortly this will not concern us).
In order to alleviate this problem it is
common to make the assumption that any two coordinates of the document vector
are, when viewed as random variables, statistically independent of each other; this
independence assumption is encoded by the equation
P(⃗dj|ci) =
Probabilistic classiﬁers that use this assumption are called Na¨ıve Bayes classiﬁers,
and account for most of the probabilistic approaches to TC in the literature . The “na¨ıve”
character of the classiﬁer is due to the fact that usually this assumption is, quite
obviously, not veriﬁed in practice.
One of the best-known Na¨ıve Bayes approaches is the binary independence classiﬁer [Robertson and Sparck Jones 1976], which results from using binary-valued
vector representations for documents.
In this case, if we write pki as short for
P(wkx = 1|ci), the P(wkj|ci) factors of (4) may be written as
P(wkj|ci) = pwkj
ki (1 −pki)1−wkj = (
)wkj(1 −pki)
We may further observe that in TC the document space is partitioned into two
categories12, ci and its complement ci, such that P(ci|⃗dj) = 1 −P(ci|⃗dj). If we
plug in (4) and (5) into (3) and take logs we obtain
log P(ci|⃗dj) = log P(ci) +
log(1 −pki) −log P(⃗dj)
log(1 −P(ci|⃗dj)) = log(1 −P(ci)) +
log(1 −pki) −log P(⃗dj)
where we write pki as short for P(wkx = 1|ci). We may convert (6) and (7) into a
single equation by subtracting componentwise (7) from (6), thus obtaining
1 −P(ci|⃗dj)
1 −P(ci) +
wkj log pki(1 −pki)
pki(1 −pki) +
log 1 −pki
12Cooper has pointed out that in this case the full independence assumption of (4) is not
actually made in the Na¨ıve Bayes classiﬁer; the assumption needed here is instead the weaker
linked dependence assumption, which may be written as
dj|ci) = Q|T |
P (wkj |ci)
P (wkj |ci) .
F. Sebastiani
P (ci| ⃗dj)
1−P (ci| ⃗dj) is an increasing monotonic function of P(ci|⃗dj), and may thus
be used directly as CSVi(dj). Note also that log
1−P (ci) and P|T |
k=1 log 1−pki
1−pki are constant for all documents, and may thus be disregarded13. Deﬁning a classiﬁer for category ci thus basically requires estimating the 2|T | parameters {p1i, p1i, . . . , p|T |i, p|T |i}
from the training data, which may be done in the obvious way. Note that in general
the classiﬁcation of a given document does not require to compute a sum of |T | factors, as the presence of P|T |
k=1 wkj log
pki(1−pki)
pki(1−pki) would imply; in fact, all the factors
for which wkj = 0 may be disregarded, and this accounts for the vast majority of
them, since document vectors are usually very sparse.
The method we have illustrated is just one of the many variants of the Na¨ıve
Bayes approach, the common denominator of which is (4).
A recent paper by
Lewis is an excellent roadmap on the various directions that research on
Na¨ıve Bayes classiﬁers has taken; among these are the ones aiming
—to relax the constraint that document vectors should be binary-valued. This looks
natural, given that weighted indexing techniques accounting for the “importance” of tk for dj play a key role in IR.
—to introduce document length normalization. The value of log
P (ci| ⃗dj)
1−P (ci| ⃗dj) tends to
be more extreme (i.e. very high or very low) for long documents (i.e. documents
such that wkj = 1 for many values of k), irrespectively of their semantic relatedness to ci, thus calling for length normalization. Taking length into account is
easy in non-probabilistic approaches to classiﬁcation (see e.g. Section 6.7), but
is problematic in probabilistic ones . One possible
answer is to switch from an interpretation of Na¨ıve Bayes in which documents are
events to one in which terms are events [Baker and McCallum 1998; McCallum
et al. 1998; Chakrabarti et al. 1998a; Guthrie et al. 1994]. This accounts for
document length naturally but, as noted in [Lewis 1998], has the drawback that
diﬀerent occurrences of the same word within the same document are viewed as
independent, an assumption even more implausible than (4).
—to relax the independence assumption. This may be the hardest route to follow,
since this produces classiﬁers of higher computational cost and characterized by
harder parameter estimation problems [Koller and Sahami 1997]. Earlier eﬀorts
in this direction within probabilistic text search have
not shown the performance improvements that were hoped for. Recently, the
fact that the binary independence assumption seldom harms eﬀectiveness has
also been given some theoretical justiﬁcation [Domingos and Pazzani 1997].
The quotation of text search in the last paragraph is not casual. Unlike other types
of classiﬁers, the literature on probabilistic classiﬁers is inextricably intertwined
with that on probabilistic search systems ,
since these latter attempt to determine the probability that a document falls in the
13This is not true, however, if the “ﬁxed thresholding” method of Section 6.1 is adopted. In fact,
for a ﬁxed document dj the ﬁrst and third factor in the formula above are diﬀerent for diﬀerent
categories, and may therefore inﬂuence the choice of the categories under which to ﬁle dj.
Machine Learning in Automated Text Categorization
A decision tree equivalent to the DNF rule of Figure 1. Edges are labelled by terms and
leaves are labelled by categories (underlining denotes negation).
category denoted by the query, and since they are the only search systems that take
relevance feedback, a notion essentially involving supervised learning, as central.
6.3 Decision tree classiﬁers
Probabilistic methods are quantitative (i.e. numeric) in nature, and as such have
sometimes been criticized since, eﬀective as they may be, are not easily interpretable
by humans. A class of algorithms that do not suﬀer from this problem are symbolic
(i.e. non-numeric) algorithms, among which inductive rule learners (which we will
discuss in Section 6.4) and decision tree learners are the most important examples.
A decision tree (DT) text classiﬁer is a
tree in which internal nodes are labelled by terms, branches departing from them
are labelled by tests on the weight that the term has in the test document, and
leafs are labelled by categories. Such a classiﬁer categorizes a test document dj by
recursively testing for the weights that the terms labeling the internal nodes have
in vector ⃗dj, until a leaf node is reached; the label of this node is then assigned to
dj. Most such classiﬁers use binary document representations, and thus consist of
binary trees. An example DT is illustrated in Figure 2.
There are a number of standard packages for DT learning, and most DT approaches to TC have made use of one such package. Among the most popular ones
are ID3 , C4.5 and C5 . TC eﬀorts based on experimental DT packages include [Dumais et al.
1998; Lewis and Ringuette 1994; Weiss et al. 1999].
A possible method for learning a DT for category ci consists in a “divide and
conquer” strategy of (i) checking whether all the training examples have the same
F. Sebastiani
label (either ci or ci); (ii) if not, selecting a term tk, partitioning T r into classes
of documents that have the same value for tk, and placing each such class in a
separate subtree. The process is recursively repeated on the subtrees until each leaf
of the tree so generated contains training examples assigned to the same category ci,
which is then chosen as the label for the leaf. The key step is the choice of the term
tk on which to operate the partition, a choice which is generally made according to
an information gain or entropy criterion. However, such a “fully grown” tree may
be prone to overﬁtting, as some branches may be too speciﬁc to the training data.
Most DT learning methods thus include a method for growing the tree and one for
pruning it, i.e. for removing the overly speciﬁc branches. Variations on this basic
schema for DT learning abound [Mitchell 1996, Section 3].
DT text classiﬁers have been used either as the main classiﬁcation tool [Fuhr et al.
1991; Lewis and Catlett 1994; Lewis and Ringuette 1994], or as baseline classiﬁers
[Cohen and Singer 1999; Joachims 1998], or as members of classiﬁer committees [Li
and Jain 1998; Schapire and Singer 2000; Weiss et al. 1999].
6.4 Decision rule classiﬁers
A classiﬁer for category ci built by an inductive rule learning method consists of
a DNF rule, i.e. of a conditional rule with a premise in disjunctive normal form
(DNF), of the type illustrated in Figure 114. The literals (i.e. possibly negated
keywords) in the premise denote the presence (non-negated keyword) or absence
(negated keyword) of the keyword in the test document dj, while the clause head
denotes the decision to classify dj under ci. DNF rules are similar to DTs in that
they can encode any Boolean function. However, an advantage of DNF rule learners
is that they tend to generate more compact classiﬁers than DT learners.
Rule learning methods usually attempt to select from all the possible covering
rules (i.e. rules that correctly classify all the training examples) the “best” one
according to some minimality criterion. While DTs are typically built by a topdown, “divide-and-conquer” strategy, DNF rules are often built in a bottom-up
fashion. Initially, every training example dj is viewed as a clause η1, . . . , ηn →γi,
where η1, . . . , ηn are the terms contained in dj and γi equals ci or ci according to
whether dj is a positive or negative example of ci. This set of clauses is already a
DNF classiﬁer for ci, but obviously scores high in terms of overﬁtting. The learner
applies then a process of generalization in which the rule is simpliﬁed through a
series of modiﬁcations (e.g. removing premises from clauses, or merging clauses)
that maximize its compactness while at the same time not aﬀecting the “covering”
property of the classiﬁer. At the end of this process, a “pruning” phase similar in
spirit to that employed in DTs is applied, where the ability to correctly classify all
the training examples is traded for more generality.
DNF rule learners vary widely in terms of the methods, heuristics and criteria
employed for generalization and pruning. Among the DNF rule learners that have
been applied to TC are Charade [Moulinier and Ganascia 1996], DL-ESC [Li and
Yamanishi 1999], Ripper [Cohen 1995a; Cohen and Hirsh 1998; Cohen and Singer
14Many inductive rule learning algorithms build decision lists (i.e. arbitrarily nested if-then-else
clauses) instead of DNF rules; since the former may always be rewritten as the latter we will
disregard the issue.
Machine Learning in Automated Text Categorization
1999], Scar [Moulinier et al. 1996], and Swap-1 [Apt´e et al. 1994].
While the methods above use rules of propositional logic (PL), research has also
been carried out using rules of ﬁrst order logic (FOL), obtainable through the use of
inductive logic programming methods. Cohen [1995a] has extensively compared PL
and FOL learning in TC (for instance, comparing the PL learner Ripper with its
FOL version Flipper), and has found that the additional representational power
of FOL brings about only modest beneﬁts.
6.5 Regression methods
Various TC eﬀorts have used regression models . Regression denotes the
approximation of a real-valued (instead than binary, as in the case of classiﬁcation)
function ˘Φ by means of a function Φ that ﬁts the training data [Mitchell 1996,
page 236]. Here we will describe one such model, the Linear Least Squares Fit
(LLSF) applied to TC by Yang and Chute . In LLSF, each document dj has
two vectors associated to it: an input vector I(dj) of |T | weighted terms, and an
output vector O(dj) of |C| weights representing the categories (the weights for this
latter vector are binary for training documents, and are non-binary CSV s for test
documents). Classiﬁcation may thus be seen as the task of determining an output
vector O(dj) for test document dj, given its input vector I(dj); hence, building a
classiﬁer boils down to computing a |C|×|T | matrix ˆ
M such that ˆ
MI(dj) = O(dj).
LLSF computes the matrix from the training data by computing a linear leastsquares ﬁt that minimizes the error on the training set according to the formula
M = arg minM ∥MI −O∥F, where arg minM(x) stands as usual for the M for which
x is minimum, ∥V ∥F
ij represents the so-called Frobenius norm
of a |C|×|T | matrix, I is the |T |×|T r| matrix whose columns are the input vectors
of the training documents, and O is the |C| × |T r| matrix whose columns are the
output vectors of the training documents. The ˆ
M matrix is usually computed by
performing a singular value decomposition on the training set, and its generic entry
ˆmik represents the degree of association between category ci and term tk.
The experiments of [Yang and Chute 1994; Yang and Liu 1999] indicate that
LLSF is one of the most eﬀective text classiﬁers known to date. One of its disadvantages, though, is that the cost of computing the ˆ
M matrix is much higher than
that of many other competitors in the TC arena.
6.6 On-line methods
A linear classiﬁer for category ci is a vector ⃗ci = ⟨w1i, . . . , w|T |i⟩belonging to
the same |T |-dimensional space in which documents are also represented, and such
that CSVi(dj) corresponds to the dot product P|T |
k=1 wkiwkj of ⃗dj and ⃗ci. Note that
when both classiﬁer and document weights are cosine-normalized (see (2)), the dot
product between the two vectors corresponds to their cosine similarity, i.e.
S(ci, dj) = cos(α) =
k=1 wki · wjk
which represents the cosine of the angle α that separates the two vectors. This is
F. Sebastiani
the similarity measure between query and document computed by standard vectorspace IR engines, which means in turn that once a linear classiﬁer has been built,
classiﬁcation can be performed by invoking such an engine. Practically all search
engines have a dot product ﬂavour to them, and can therefore be adapted to doing
TC with a linear classiﬁer.
Methods for learning linear classiﬁers are often partitioned in two broad classes,
batch methods and on-line methods.
Batch methods build a classiﬁer by analysing the training set all at once. Within
the TC literature, one example of a batch method is linear discriminant analysis,
a model of the stochastic dependence between terms that relies on the covariance
matrices of the categories [Hull 1994; Sch¨utze et al. 1995]. However, the foremost
example of a batch method is the Rocchio method; because of its importance in
the TC literature this will be discussed separately in Section 6.7. In this section we
will instead concentrate on on-line classiﬁers.
On-line (aka incremental) methods build a classiﬁer soon after examining the
ﬁrst training document, and incrementally reﬁne it as they examine new ones. This
may be an advantage in the applications in which T r is not available in its entirety
from the start, or in which the “meaning” of the category may change in time,
as e.g. in adaptive ﬁltering. This is also apt to applications (e.g. semi-automated
classiﬁcation, adaptive ﬁltering) in which we may expect the user of a classiﬁer to
provide feedback on how test documents have been classiﬁed, as in this case further
training may be performed during the operating phase by exploiting user feedback.
A simple on-line method is the perceptron algorithm, ﬁrst applied to TC in
[Sch¨utze et al. 1995; Wiener et al. 1995] and subsequently used in [Dagan et al.
1997; Ng et al. 1997]. In this algorithm, the classiﬁer for ci is ﬁrst initialized by
setting all weights wki to the same positive value. When a training example dj
(represented by a vector ⃗dj of binary weights) is examined, the classiﬁer built so
far classiﬁes it. If the result of the classiﬁcation is correct nothing is done, while if
it is wrong the weights of the classiﬁer are modiﬁed: if dj was a positive example of
ci then the weights wki of “active terms” (i.e. the terms tk such that wkj = 1) are
“promoted” by increasing them by a ﬁxed quantity α > 0 (called learning rate),
while if dj was a negative example of ci then the same weights are “demoted” by
decreasing them by α. Note that when the classiﬁer has reached a reasonable level
of eﬀectiveness, the fact that a weight wki is very low means that tk has negatively
contributed to the classiﬁcation process so far, and may thus be discarded from the
representation. We may then see the perceptron algorithm (as all other incremental
learning methods) as allowing for a sort of “on-the-ﬂy term space reduction” [Dagan
et al. 1997, Section 4.4]. The perceptron classiﬁer has shown a good eﬀectiveness
in all the experiments quoted above.
The perceptron is an additive weight-updating algorithm. A multiplicative variant of it is Positive Winnow [Dagan et al. 1997], which diﬀers from perceptron
because two diﬀerent constants α1 > 1 and 0 < α2 < 1 are used for promoting and
demoting weights, respectively, and because promotion and demotion are achieved
by multiplying, instead of adding, by α1 and α2. Balanced Winnow [Dagan
et al. 1997] is a further variant of Positive Winnow, in which the classiﬁer consists of two weights w+
ki for each term tk; the ﬁnal weight wki used in
computing the dot product is the diﬀerence w+
ki. Following the misclassiﬁca-
Machine Learning in Automated Text Categorization
tion of a positive instance, active terms have their w+
ki weight promoted and their
ki weight demoted, whereas in the case of a negative instance it is w+
ki that gets
demoted while w−
ki gets promoted (for the rest, promotions and demotions are as in
Positive Winnow). Balanced Winnow allows negative wki weights, while in
the perceptron and in Positive Winnow the wki weights are always positive. In
experiments conducted by Dagan et al. , Positive Winnow showed a better
eﬀectiveness than perceptron but was in turn outperformed by (Dagan et al.’s own
version of) Balanced Winnow.
Other on-line methods for building text classiﬁers are Widrow-Hoff, a reﬁnement of it called Exponentiated Gradient and Sleeping Experts [Cohen and Singer 1999], a version
of Balanced Winnow. While the ﬁrst is an additive weight-updating algorithm,
the second and third are multiplicative. Key diﬀerences with the previously described algorithms are that these three algorithms (i) update the classiﬁer not only
after misclassifying a training example, but also after classifying it correctly, and
(ii) update the weights corresponding to all terms (instead of just active ones).
Linear classiﬁers lend themselves to both category-pivoted and document-pivoted
TC. For the former the classiﬁer ⃗ci is used, in a standard search engine, as a query
against the set of test documents, while for the latter the vector ⃗dj representing
the test document is used as a query against the set of classiﬁers {⃗c1, . . . ,⃗c|C|}.
6.7 The Rocchio method
Some linear classiﬁers consist of an explicit proﬁle (or prototypical document) of the
category. This has obvious advantages in terms of interpretability, as such a proﬁle
is more readily understandable by a human than, say, a neural network classiﬁer.
Learning a linear classiﬁer is often preceded by local TSR; in this case, a proﬁle
of ci is a weighted list of the terms whose presence or absence is most useful for
discriminating ci.
The Rocchio method is used for inducing linear, proﬁle-style classiﬁers. It relies
on an adaptation to TC of the well-known Rocchio’s formula for relevance feedback
in the vector-space model, and it is perhaps the only TC method rooted in the IR
tradition rather than in the ML one. This adaptation was ﬁrst proposed by Hull
 , and has been used by many authors since then, either as an object of research
in its own right [Ittner et al. 1995; Joachims 1997; Sable and Hatzivassiloglou 2000;
Schapire et al. 1998; Singhal et al. 1997], or as a baseline classiﬁer [Cohen and
Singer 1999; Galavotti et al. 2000; Joachims 1998; Lewis et al. 1996; Schapire and
Singer 2000; Sch¨utze et al. 1995], or as a member of a classiﬁer committee [Larkey
and Croft 1996] (see Section 6.11).
Rocchio’s method computes a classiﬁer ⃗ci = ⟨w1i, . . . , w|T |i⟩for category ci by
means of the formula
{dj∈P OSi}
|POSi| −γ ·
where wkj is the weight of tk in document dj, POSi = {dj ∈T r | ˘Φ(dj, ci) = T } and
NEGi = {dj ∈T r | ˘Φ(dj, ci) = F}. In this formula, β and γ are control parameters
that allow setting the relative importance of positive and negative examples. For
instance, if β is set to 1 and γ to 0 the Rocchio classiﬁer, and (b) the k-NN
classiﬁer. Small crosses and circles denote positive and negative training instances, respectively.
The big circles denote the “inﬂuence area” of the classiﬁer. Note that, for ease of illustration,
document similarities are here viewed in terms of Euclidean distance rather than, as more common,
in terms of dot product or cosine.
Joachims 1998; Sch¨utze et al. 1995]), the proﬁle of ci is the centroid of its positive
training examples.
A classiﬁer built by means of the Rocchio method rewards
the closeness of a test document to the centroid of the positive training examples,
and its distance from the centroid of the negative training examples. The role of
negative examples is usually de-emphasized, by setting β to a high value and γ to
a low one (e.g. Cohen and Singer , Ittner et al. , and Joachims 
use β = 16 and γ = 4).
This method is quite easy to implement, and is also quite eﬃcient, since learning
a classiﬁer basically comes down to averaging weights. In terms of eﬀectiveness,
instead, a drawback is that if the documents in the category tend to occur in
disjoint clusters (e.g. a set of newspaper articles lebelled with the Sports category
and dealing with either boxing or rock-climbing), such a classiﬁer may miss most of
them, as the centroid of these documents may fall outside all of these clusters (see
Figure 3a). More generally, a classiﬁer built by the Rocchio method, as all linear
classiﬁers, has the disadvantage that it divides the space of documents linearly.
This situation is graphically depicted in Figure 3a, where documents are classiﬁed
within ci if and only if they fall within the circle.
Note that even most of the
positive training examples would not be classiﬁed correctly by the classiﬁer.
6.7.1 Enhancements to the basic Rocchio framework. One issue in the application of the Rocchio formula to proﬁle extraction is whether the set NEGi should
be considered in its entirety, or whether a well-chosen sample of it, such as the
set NPOSi of near-positives (deﬁned as “the most positive amongst the negative
training examples”), should be selected from it, yielding
{dj∈P OSi}
|POSi| −γ ·
{dj∈NP OSi}
Machine Learning in Automated Text Categorization
{dj∈NP OSi}
|NP OSi| factor is more signiﬁcant than P
|NEGi|, since
near-positives are the most diﬃcult documents to tell apart from the positives. Using near-positives corresponds to the query zoning method proposed for IR by
Singhal et al. . This method originates from the observation that when the
original Rocchio formula is used for relevance feedback in IR, near-positives tend to
be used rather than generic negatives, as the documents on which user judgments
are available are among the ones that had scored highest in the previous ranking.
Early applications of the Rocchio formula to TC 
generally did not make a distinction between near-positives and generic negatives.
In order to select the near-positives Schapire et al. issue a query, consisting of
the centroid of the positive training examples, against a document base consisting
of the negative training examples; the top-ranked ones are the most similar to this
centroid, and are then the near-positives. Wiener et al. instead equate the
near-positives of ci to the positive examples of the sibling categories of ci, as in
the application they work on (TC with hierarchical category sets) the notion of a
“sibling category of ci” is well-deﬁned. A similar policy is also adopted in [Ng et al.
1997; Ruiz and Srinivasan 1999; Weigend et al. 1999].
By using query zoning plus other enhancements (TSR, statistical phrases, and
a method called dynamic feedback optimization), Schapire et al. have found
that a Rocchio classiﬁer can achieve an eﬀectiveness comparable to that of a stateof-the-art ML method such as “boosting” (see Section 6.11.1) while being 60 times
quicker to train. These recent results will no doubt bring about a renewed interest
for the Rocchio classiﬁer, previously considered an underperformer [Cohen and
Singer 1999; Joachims 1998; Lewis et al. 1996; Sch¨utze et al. 1995; Yang 1999].
6.8 Neural networks
A neural network (NN) text classiﬁer is a network of units, where the input units
represent terms, the output unit(s) represent the category or categories of interest,
and the weights on the edges connecting units represent dependence relations. For
classifying a test document dj, its term weights wkj are loaded into the input units;
the activation of these units is propagated forward through the network, and the
value of the output unit(s) determines the categorization decision(s). A typical
way of training NNs is backpropagation, whereby the term weights of a training
document are loaded into the input units, and if a misclassiﬁcation occurs the error
is “backpropagated” so as to change the parameters of the network and eliminate
or minimize the error.
The simplest type of NN classiﬁer is the perceptron [Dagan et al. 1997; Ng et al.
1997], which is a linear classiﬁer and as such has been extensively discussed in
Section 6.6. Other types of linear NN classiﬁers implementing a form of logistic
regression have also been proposed and tested by Sch¨utze et al. and Wiener
et al. , yielding very good eﬀectiveness.
A non-linear NN [Lam and Lee 1999; Ruiz and Srinivasan 1999; Sch¨utze et al.
1995; Weigend et al. 1999; Wiener et al. 1995; Yang and Liu 1999] is instead a
network with one or more additional “layers” of units, which in TC usually represent
higher-order interactions between terms that the network is able to learn. When
comparative experiments relating non-linear NNs to their linear counterparts have
been performed, the former have yielded either no improvement [Sch¨utze et al.
F. Sebastiani
1995] or very small improvements [Wiener et al. 1995] over the latter.
6.9 Example-based classiﬁers
Example-based classiﬁers do not build an explicit, declarative representation of the
category ci, but rely on the category labels attached to the training documents
similar to the test document. These methods have thus been called lazy learners,
since “they defer the decision on how to generalize beyond the training data until
each new query instance is encountered” [Mitchell 1996, pag 244].
The ﬁrst application of example-based methods (aka memory-based reasoning
methods) to TC is due to Creecy, Masand and colleagues [Creecy et al. 1992;
Masand et al. 1992]; examples include [Joachims 1998; Lam et al. 1999; Larkey
1998; Larkey 1999; Li and Jain 1998; Yang and Pedersen 1997; Yang and Liu 1999].
Our presentation of the example-based approach will be based on the k-NN (for “k
nearest neighbours”) algorithm used by Yang . For deciding whether dj ∈ci,
k-NN looks at whether the k training documents most similar to dj also are in
ci; if the answer is positive for a large enough proportion of them, a positive decision is taken, and a negative decision is taken otherwise. Actually, Yang’s is a
distance-weighted version of k-NN , since the
fact that a most similar document is in ci is weighted by its similarity with the test
document. Classifying dj by means of k-NN thus comes down to computing
CSVi(dj) =
dz∈T rk(dj)
RSV (dj, dz) · [[˘Φ(dz, ci)]]
where T rk(dj) is the set of the k documents dz which maximize RSV (dj, dz) and
1 if α = T
0 if α = F
The thresholding methods of Section 6.1 can then be used to convert the real-valued
CSVi’s into binary categorization decisions. In (9), RSV (dj, dz) represents some
measure or semantic relatedness between a test document dj and a training document dz; any matching function, be it probabilistic or vector-based , from a ranked IR system may be
used for this purpose. The construction of a k-NN classiﬁer also involves determining (experimentally, on a validation set) a threshold k that indicates how many topranked training documents have to be considered for computing CSVi(dj). Larkey
and Croft use k = 20, while Yang has found 30 ≤k ≤45 to yield
the best eﬀectiveness. Anyhow, various experiments have shown that increasing the
value of k does not signiﬁcantly degrade the performance.
Note that k-NN, unlike linear classiﬁers, does not divide the document space
linearly, hence does not suﬀer from the problem discussed at the end of Section
6.7. This is graphically depicted in Figure 3b, where the more “local” character of
k-NN with respect to Rocchio can be appreciated.
This method is naturally geared towards document-pivoted TC, since ranking the
training documents for their similarity with the test document can be done once
for all categories. For category-pivoted TC one would need to store the document
ranks for each test document, which is obviously clumsy; DPC is thus de facto the
only reasonable way to use k-NN.
Machine Learning in Automated Text Categorization
A number of diﬀerent experiments (see Section 7.3) have shown k-NN to be quite
eﬀective. However, its most important drawback is its ineﬃciency at classiﬁcation
time: while e.g. with a linear classiﬁer only a dot product needs to be computed
to classify a test document, k-NN requires the entire training set to be ranked
for similarity with the test document, which is much more expensive. This is a
drawback of “lazy” learning methods, since they do not have a true training phase
and thus defer all the computation to classiﬁcation time.
6.9.1 Other example-based techniques. Various example-based techniques have
been used in the TC literature. For example, Cohen and Hirsh implement
an example-based classiﬁer by extending standard relational DBMS technology with
“similarity-based soft joins”. In their Whirl system they use the scoring function
CSVi(dj) = 1 −
dz∈T rk(dj)
(1 −RSV (dj, dz))[[˘Φ(dz,ci)]]
as an alternative to (9), obtaining a small but statistically signiﬁcant improvement
over a version of Whirl using (9). In their experiments this technique outperformed
a number of other classiﬁers, such as a C4.5 decision tree classiﬁer and the Ripper
CNF rule-based classiﬁer.
A variant of the basic k-NN approach is proposed by Galavotti et al. , who
reinterpret (9) by redeﬁning [[α]] as
1 if α = T
−1 if α = F
The diﬀerence from the original k-NN approach is that if a training document
dz similar to the test document dj does not belong to ci, this information is not
discarded but weights negatively in the decision to classify dj under ci.
A combination of proﬁle- and example-based methods is presented in [Lam and
Ho 1998]. In this work a k-NN system is fed generalized instances (GIs) in place of
training documents. This approach may be seen as the result of
—clustering the training set, thus obtaining a set of clusters Ki = {ki1, . . . , ki|Ki|};
—building a proﬁle G(kiz) (“generalized instance”) from the documents belonging
to cluster kiz by means of some algorithm for learning linear classiﬁers (e.g.
Rocchio, Widrow-Hoff);
—applying k-NN with proﬁles in place of training documents, i.e. computing
RSV (dj, G(kiz)) · |{dj ∈kiz| ˘Φ(dj, ci) = T }|
|{dj ∈kiz}|
· |{dj ∈kiz}|
RSV (dj, G(kiz)) · |{dj ∈kiz| ˘Φ(dj, ci) = T }|
where |{dj∈kiz| ˘Φ(dj,ci)=T }|
|{dj∈kiz}|
represents the “degree” to which G(kiz) is a positive
instance of ci, and |{dj∈kiz}|
represents its weight within the entire process.
This exploits the superior eﬀectiveness (see Figure 3) of k-NN over linear classiﬁers
while at the same time avoiding the sensitivity of k-NN to the presence of “outliers”
F. Sebastiani
Learning support vector classiﬁers. The small crosses and circles represent positive and
negative training examples, respectively, whereas lines represent decision surfaces. Decision surface
σi (indicated by the thicker line) is, among those shown, the best possible one, as it is the middle
element of the widest set of parallel decision surfaces (i.e. its minimum distance to any training
example is maximum). Small boxes indicate the support vectors.
(i.e. positive instances of ci that “lie out” of the region where most other positive
instances of ci are located) in the training set.
6.10 Building classiﬁers by support vector machines
The support vector machine (SVM) method has been introduced in TC by Joachims
 and subsequently used in [Drucker et al. 1999; Dumais et al. 1998; Dumais and Chen 2000; Klinkenberg and Joachims 2000; Taira and Haruno 1999;
Yang and Liu 1999]. In geometrical terms, it may be seen as the attempt to ﬁnd,
among all the surfaces σ1, σ2, . . . in |T |-dimensional space that separate the positive from the negative training examples (decision surfaces), the σi that separates
the positives from the negatives by the widest possible margin, i.e. such that the
separation property is invariant with respect to the widest possible traslation of σi.
This idea is best understood in the case in which the positives and the negatives
are linearly separable, in which case the decision surfaces are (|T |−1)-hyperplanes.
In the 2-dimensional case of Figure 4, various lines may be chosen as decision
surfaces. The SVM method chooses the middle element from the “widest” set of
parallel lines, i.e. from the set in which the maximum distance between two elements
in the set is highest. It is noteworthy that this “best” decision surface is determined
by only a small set of training examples, called the support vectors.
The method described is applicable also to the case in which the positives and the
negatives are not linearly separable. Yang and Liu experimentally compared
the linear case (namely, when the assumption is made that the categories are linearly
separable) with the non-linear case on a standard benchmark, and obtained slightly
better results in the former case.
As argued by Joachims , SVMs oﬀer two important advantages for TC:
Machine Learning in Automated Text Categorization
—term selection is often not needed, as SVMs tend to be fairly robust to overﬁtting
and can scale up to considerable dimensionalities;
—no human and machine eﬀort in parameter tuning on a validation set is needed, as
there is a theoretically motivated, “default” choice of parameter settings, which
has also been shown to provide the best eﬀectiveness.
Dumais et al. have applied a novel algorithm for training SVMs which brings
about training speeds comparable to computationally easy learners such as Rocchio.
6.11 Classiﬁer committees
Classiﬁer committees (aka ensembles) are based on the idea that, given a task that
requires expert knowledge to perform, k experts may be better than one if their
individual judgments are appropriately combined. In TC, the idea is to apply k
diﬀerent classiﬁers Φ1, . . . , Φk to the same task of deciding whether dj ∈ci, and then
combine their outcome appropriately. A classiﬁer committee is then characterized
by (i) a choice of k classiﬁers, and (ii) a choice of a combination function.
Concerning issue (i), it is known from the ML literature that, in order to guarantee good eﬀectiveness, the classiﬁers forming the committee should be as independent as possible [Tumer and Ghosh 1996]. The classiﬁers may diﬀer for the indexing
approach used, or for the inductive method, or both. Within TC, the avenue which
has been explored most is the latter .
Concerning issue (ii), various rules have been tested. The simplest one is majority
voting (MV), whereby the binary outputs of the k classiﬁers are pooled together,
and the classiﬁcation decision that reaches the majority of k+1
votes is taken (k
obviously needs to be an odd number) [Li and Jain 1998; Liere and Tadepalli 1997].
This method is particularly suited to the case in which the committee includes
classiﬁers characterized by a binary decision function CSVi : D →{T, F}.
second rule is weighted linear combination (WLC), whereby a weighted sum of the
CSVi’s produced by the k classiﬁers yields the ﬁnal CSVi. The weights wj reﬂect
the expected relative eﬀectiveness of classiﬁers Φj, and are usually optimized on
a validation set [Larkey and Croft 1996].
Another policy is dynamic classiﬁer
selection (DCS), whereby among committee {Φ1, . . . , Φk} the classiﬁer Φt most
eﬀective on the l validation examples most similar to dj is selected, and its judgment
adopted by the committee [Li and Jain 1998]. A still diﬀerent policy, somehow
intermediate between WLC and DCS, is adaptive classiﬁer combination (ACC),
whereby the judgments of all the classiﬁers in the committee are summed together,
but their individual contribution is weighted by their eﬀectiveness on the l validation
examples most similar to dj [Li and Jain 1998].
Classiﬁer committees have had mixed results in TC so far. Larkey and Croft
 have used combinations of Rocchio, Na¨ıve Bayes and k-NN, all together or
in pairwise combinations, using a WLC rule. In their experiments the combination
of any two classiﬁers outperformed the best individual classiﬁer (k-NN), and the
combination of the three classiﬁers improved an all three pairwise combinations.
These results would seem to give strong support to the idea that classiﬁer committees can somehow proﬁt from the complementary strengths of their individual
members. However, the small size of the test set used (187 documents) suggests
F. Sebastiani
that more experimentation is needed before conclusions can be drawn.
Li and Jain have tested a committee formed of (various combinations of)
a Na¨ıve Bayes classiﬁer, an example-based classiﬁer, a decision tree classiﬁer, and
a classiﬁer built by means of their own “subspace method”; the combination rules
they have worked with are MV, DCS and ACC. Only in the case of a committee
formed by Na¨ıve Bayes and the subspace classiﬁer combined by means of ACC
the committee has outperformed, and by a narrow margin, the best individual
classiﬁer (for every attempted classiﬁer combination ACC gave better results than
MV and DCS). This seems discouraging, especially in the light of the fact that
the committee approach is computationally expensive (its cost trivially amounts
to the sum of the costs of the individual classiﬁers plus the cost incurred for the
computation of the combination rule). Again, it has to be remarked that the small
size of their experiment (two test sets of less than 700 documents each were used)
does not allow to draw deﬁnitive conclusions on the approaches adopted.
6.11.1 Boosting. The boosting method [Schapire et al. 1998; Schapire and Singer
2000] occupies a special place in the classiﬁer committees literature, since the k
classiﬁers Φ1, . . . , Φk forming the committee are obtained by the same learning
method (here called the weak learner). The key intuition of boosting is that the
k classiﬁers should be trained not in a conceptually parallel and independent way,
as in the committees described above, but sequentially. In this way, in training
classiﬁer Φi one may take into account how classiﬁers Φ1, . . . , Φi−1 perform on
the training examples, and concentrate on getting right those examples on which
Φ1, . . . , Φi−1 have performed worst.
Speciﬁcally, for learning classiﬁer Φt each ⟨dj, ci⟩pair is given an “importance
weight” ht
ij (where h1
ij is set to be equal for all ⟨dj, ci⟩pairs15), which represents how
hard to get a correct decision for this pair was for classiﬁers Φ1, . . . , Φt−1. These
weights are exploited in learning Φt, which will be specially tuned to correctly solve
the pairs with higher weight. Classiﬁer Φt is then applied to the training documents,
and as a result weights ht
ij are updated to ht+1
ij ; in this update operation, pairs
correctly classiﬁed by Φt will have their weight decreased, while pairs misclassiﬁed
by Φt will have their weight increased. After all the k classiﬁers have been built, a
weighted linear combination rule is applied to yield the ﬁnal committee.
In the BoosTexter system [Schapire and Singer 2000], two diﬀerent boosting
algorithms are tested, using a one-level decision tree weak learner.
The former
algorithm is
explicitly geared towards the maximization of microaveraged eﬀectiveness, whereas
the latter (AdaBoost.MR) is aimed at minimizing ranking loss (i.e. at getting a
correct category ranking for each individual document). In experiments conducted
over three diﬀerent test collections, Schapire et al. have shown AdaBoost
to outperform Sleeping Experts, a classiﬁer that had proven quite eﬀective in
the experiments of [Cohen and Singer 1999]. Further experiments by Schapire and
Singer showed AdaBoost to outperform, aside from Sleeping Experts, a
Na¨ıve Bayes classiﬁer, a standard (non-enhanced) Rocchio classiﬁer, and Joachims’
15Schapire et al. also show that a simple modiﬁcation of this policy allows optimization of
the classiﬁer based on “utility” (see Section 7.1.3).
Machine Learning in Automated Text Categorization
 PrTFIDF classiﬁer.
A boosting algorithm based on a “committee of classiﬁer sub-committees” that
improves on the eﬀectiveness and (especially) the eﬃciency of AdaBoost.MH
is presented in [Sebastiani et al. 2000]. An approach similar to boosting is also
employed by Weiss et al. , who experiment with committees of decision trees
each having an average of 16 leaves , eventually combined by using
the simple MV rule as a combination rule; similarly to boosting, a mechanism for
emphasising documents that have been misclassiﬁed by previous decision trees is
used. Boosting-based approaches have also been employed in [Escudero et al. 2000;
Iyer et al. 2000; Kim et al. 2000; Li and Jain 1998; Myers et al. 2000].
6.12 Other methods
Although in the previous sections we have tried to give an overview as complete
as possible of the learning approaches proposed in the TC literature, it would be
hardly possible to be exhaustive.
Some of the learning approaches adopted do
not fall squarely under one or the other class of algorithms, or have remained
somehow isolated attempts. Among these, the most noteworthy are the ones based
on Bayesian inference networks [Dumais et al. 1998; Lam et al. 1997; Tzeras and
Hartmann 1993], genetic algorithms [Clack et al. 1997; Masand 1994], and maximum
entropy modelling [Manning and Sch¨utze 1999].
7. EVALUATION OF TEXT CLASSIFIERS
As for text search systems, the evaluation of document classiﬁers is typically conducted experimentally, rather than analytically. The reason is that, in order to evaluate a system analytically (e.g. proving that the system is correct and complete)
we would need a formal speciﬁcation of the problem that the system is trying to
solve (e.g. with respect to what correctness and completeness are deﬁned), and the
central notion of TC (namely, that of membership of a document in a category) is,
due to its subjective character, inherently non-formalisable.
The experimental evaluation of a classiﬁer usually measures its eﬀectiveness
(rather than its eﬃciency), i.e. its ability to take the right classiﬁcation decisions.
7.1 Measures of text categorization eﬀectiveness
7.1.1 Precision and recall. Classiﬁcation eﬀectiveness is usually measured in terms
of the classic IR notions of precision (π) and recall (ρ), adapted to the case of
TC. Precision wrt ci (πi) is deﬁned as the conditional probability P(˘Φ(dx, ci) =
T |Φ(dx, ci) = T ), i.e. as the probability that if a random document dx is classi-
ﬁed under ci, this decision is correct. Analogously, recall wrt ci (ρi) is deﬁned as
P(Φ(dx, ci) = T | ˘Φ(dx, ci) = T ), i.e. as the probability that, if a random document
dx ought to be classiﬁed under ci, this decision is taken. These category-relative
values may be averaged, in a way to be discussed shortly, to obtain π and ρ, i.e.
values global to the entire category set. Borrowing terminology from logic, π may
be viewed as the “degree of soundness” of the classiﬁer wrt C, while ρ may be
viewed as its “degree of completeness” wrt C. As deﬁned here, πi and ρi are to be
understood as subjective probabilities, i.e. as measuring the expectation of the user
that the system will behave correctly when classifying an unseen document under
F. Sebastiani
expert judgments
The contingency table for category ci.
Category set
expert judgments
C = {c1, . . . , c|C|}
The global contingency table.
ci. These probabilities may be estimated in terms of the contingency table for ci
on a given test set (see Table 2). Here, FPi (false positives wrt ci, aka errors of
commission) is the number of test documents incorrectly classiﬁed under ci; T Ni
(true negatives wrt ci), T Pi (true positives wrt ci) and FNi (false negatives wrt ci,
aka errors of omission) are deﬁned accordingly. Estimates (indicated by carets) of
precision wrt ci and recall wrt ci may thus be obtained as
T Pi + FPi
T Pi + FNi
For obtaining estimates of π and ρ, two diﬀerent methods may be adopted:
—microaveraging: π and ρ are obtained by summing over all individual decisions:
T P + FP =
i=1(T Pi + FPi)
T P + FN =
i=1(T Pi + FNi)
where “µ” indicates microaveraging. The “global” contingency table (Table 3) is
thus obtained by summing over category-speciﬁc contingency tables.
—macroaveraging : precision and recall are ﬁrst evaluated “locally” for each category, and then “globally” by averaging over the results of the diﬀerent categories:
where “M” indicates macroaveraging.
These two methods may give quite diﬀerent results, especially if the diﬀerent categories have very diﬀerent generality. For instance, the ability of a classiﬁer to
behave well also on categories with low generality (i.e. categories with few positive training instances) will be emphasized by macroaveraging and much less so by
Machine Learning in Automated Text Categorization
microaveraging. Whether one or the other should be used obviously depends on
the application requirements. From now on, we will assume that microaveraging is
used; everything we will say in the rest of Section 7 may be adapted to the case of
macroaveraging in the obvious way.
7.1.2 Other measures of eﬀectiveness. Measures alternative to π and ρ and commonly used in the ML literature, such as accuracy (estimated as ˆA =
T P +T N+F P +F N )
and error (estimated as ˆE =
T P +T N+F P +F N = 1 −ˆA), are not widely used in TC.
The reason is that, as Yang points out, the large value that their denominator typically has in TC makes them much more insensitive to variations in the
number of correct decisions (T P + T N) than π and ρ. Besides, if A is the adopted
evaluation measure, in the frequent case of a very low average generality the trivial
rejector (i.e. the classiﬁer Φ such that Φ(dj, ci) = F for all dj and ci) tends to
outperform all non-trivial classiﬁers . If A is
adopted, parameter tuning on a validation set may thus result in parameter choices
that make the classiﬁer behave very much like the trivial rejector.
A non-standard eﬀectiveness measure is proposed by Sable and Hatzivassiloglou
[2000, Section 7], who suggest to base π and ρ not on “absolute” values of success
and failure (i.e. 1 if Φ(dj, ci) = ˘Φ(dj, ci) and 0 if Φ(dj, ci) ̸= ˘Φ(dj, ci)), but on values
of relative success (i.e. CSVi(dj) if ˘Φ(dj, ci) = T and 1 −CSVi(dj) if ˘Φ(dj, ci) =
F). This means that for a correct (resp. wrong) decision the classiﬁer is rewarded
(resp. penalized) proportionally to its conﬁdence in the decision. This proposed
measure does not reward the choice of a good thresholding policy, and is thus unﬁt
for autonomous (“hard”) classiﬁcation systems. However, it might be appropriate
for interactive (“ranking”) classiﬁers of the type used in [Larkey 1999], where the
conﬁdence that the classiﬁer has in its own decision inﬂuences category ranking
and, as a consequence, the overall usefulness of the system.
7.1.3 Measures alternative to eﬀectiveness. In general, criteria diﬀerent from effectiveness are seldom used in classiﬁer evaluation.
For instance, eﬃciency, although very important for applicative purposes, is seldom used as the sole yardstick,
due to the volatility of the parameters on which the evaluation rests. However,
eﬃciency may be useful for choosing among classiﬁers with similar eﬀectiveness.
An interesting evaluation has been carried out by Dumais et al. , who have
compared ﬁve diﬀerent learning methods along three diﬀerent dimensions, namely
eﬀectiveness, training eﬃciency (i.e. the average time it takes to build a classiﬁer
for category ci from a training set T r), and classiﬁcation eﬃciency (i.e. the average
time it takes to classify a new document dj under category ci).
An important alternative to eﬀectiveness is utility, a class of measures from decision theory that extend eﬀectiveness by economic criteria such as gain or loss.
Utility is based on a utility matrix such as that of Table 4, where the numeric values uT P , uF P , uF N and uT N represent the gain brought about by a true positive,
false positive, false negative and true negative, respectively; both uT P and uT N are
greater than both uF P and uF N. “Standard” eﬀectiveness is a special case of utility, i.e. the one in which uT P = uT N > uF P = uF N. Less trivial cases are those in
which uT P ̸= uT N and/or uF P ̸= uF N; this is the case e.g. in spam ﬁltering, where
failing to discard a piece of junk mail (FP) is a less serious mistake than discarding
F. Sebastiani
Category set
expert judgments
C = {c1, . . . , c|C|}
The utility matrix.
a legitimate message (FN) [Androutsopoulos et al. 2000]. If the classiﬁer outputs
probability estimates of the membership of dj in ci, then decision theory provides
analytical methods to determine thresholds τi, thus avoiding the need to determine
them experimentally (as discussed in Section 6.1). Speciﬁcally, as Lewis [1995a]
reminds, the expected value of utility is maximized when
(uF P −uT N)
(uF N −uT P ) + (uF P −uT N)
which, in the case of “standard” eﬀectiveness, is equal to 1
The use of utility in TC is discussed in detail by Lewis [1995a]. Other works
where utility is employed are [Amati and Crestani 1999; Cohen and Singer 1999;
Hull et al. 1996; Lewis and Catlett 1994; Schapire et al. 1998]. Utility has become
popular within the text ﬁltering community, and the TREC “ﬁltering track” evaluations have been using it since long [Lewis 1995c]. The values of the utility matrix
are extremely application-dependent. This means that if utility is used instead of
“pure” eﬀectiveness, there is a further element of diﬃculty in the cross-comparison
of classiﬁcation systems (see Section 7.3), since for two classiﬁers to be experimentally comparable also the two utility matrices must be the same.
Other eﬀectiveness measures diﬀerent from the ones discussed here have occasionally been used in the literature; these include adjacent score [Larkey 1998],
coverage [Schapire and Singer 2000], one-error [Schapire and Singer 2000], Pearson
product-moment correlation [Larkey 1998], recall at n [Larkey and Croft 1996], top
candidate [Larkey and Croft 1996], top n [Larkey and Croft 1996]. We will not
attempt to discuss them in detail. However, their use shows that, although the TC
community is making consistent eﬀorts at standardising experimentation protocols,
we are still far from universal agreement on evaluation issues and, as a consequence,
from understanding precisely the relative merits of the various methods.
7.1.4 Combined eﬀectiveness measures. Neither precision nor recall make sense
in isolation of each other. In fact the classiﬁer Φ such that Φ(dj, ci) = T for all
dj and ci (the trivial acceptor) has ρ = 1. When the CSVi function has values in
 one only needs to set every threshold τi to 0 to obtain the trivial acceptor. In
this case π would usually be very low (more precisely, equal to the average test set
generality
i=1 gT e(ci)
)16. Conversely, it is well-known from everyday IR practice
that higher levels of π may be obtained at the price of low values of ρ.
16From this one might be tempted to infer, by symmetry, that the trivial rejector always has
π = 1. This is false, as π is undeﬁned (the denominator is zero) for the trivial rejector (see Table
5). In fact, it is clear from its deﬁnition (π =
T P +F P ) that π depends only on how the positives
(TP + F P ) are split between true positives TP and the false positives F P , and does not depend
Machine Learning in Automated Text Categorization
C-precision
Trivial Rejector
Trivial Acceptor
Trivial “Yes” Collection
Trivial “No” Collection
Trivial cases in TC.
In practice, by tuning τi a function CSVi : D →{T, F} is tuned to be, in the
words of Riloﬀand Lehnert , more liberal (i.e. improving ρi to the detriment
of πi) or more conservative (improving πi to the detriment of ρi)17. A classiﬁer
should thus be evaluated by means of a measure which combines π and ρ18. Various
such measures have been proposed, among which the most frequent are:
(1) 11-point average precision: threshold τi is repeatedly tuned so as to allow ρi
to take up values of 0.0, .1, . . . , .9, 1.0; πi is computed for these 11 diﬀerent
values of τi, and averaged over the 11 resulting values. This is analogous to the
standard evaluation methodology for ranked IR systems, and may be used
(a) with categories in place of IR queries. This is most frequently used for
document-ranking classiﬁers ;
(b) with test documents in place of IR queries and categories in place of documents. This is most frequently used for category-ranking classiﬁers . In this case if macroaveraging is used it needs to be
redeﬁned on a per-document, rather than per-category basis.
This measure does not make sense for binary-valued CSVi functions, since in
this case ρi may not be varied at will.
at all on the cardinality of the positives. There is a breakup of “symmetry” between π and ρ
here because, from the point of view of classiﬁer judgment (positives vs. negatives; this is the
dichotomy of interest in trivial acceptor vs. trivial rejector) the “symmetric” of ρ (
T P +F N ) is not
T P +F P ) but c-precision (πc =
F P +T N ), the “contrapositive” of π. In fact, while ρ=1 and
πc=0 for the trivial acceptor, πc=1 and ρ=0 for the trivial rejector.
17While ρi can always be increased at will by lowering τi, usually at the cost of decreasing πi,
πi can usually be increased at will by raising τi, always at the cost of decreasing ρi. This kind of
tuning is only possible for CSVi functions with values in ; for binary-valued CSVi functions
tuning is not always possible, or is anyway more diﬃcult .
18An exception is single-label TC, in which π and ρ are not independent of each other: if a
document dj has been classiﬁed under a wrong category cs (thus decreasing πs) this also means
that it has not been classiﬁed under the right category ct (thus decreasing ρt). In this case either
π or ρ can be used as a measure of eﬀectiveness.
F. Sebastiani
(2) the breakeven point, i.e. the value at which π equals ρ . This is obtained by a process analogous to the one
used for 11-point average precision: a plot of π as a function of ρ is computed
by repeatedly varying the thresholds τi; breakeven is the value of ρ (or π) for
which the plot intersects the ρ = π line. This idea relies on the fact that by
decreasing the τi’s from 1 to 0, ρ always increases monotonically from 0 to 1
and π usually decreases monotonically from a value near 1 to
i=1 gT e(ci).
If for no values of the τi’s π and ρ are exactly equal, the τi’s are set to the
value for which π and ρ are closest, and an interpolated breakeven is computed
as the average of the values of π and ρ19.
(3) the Fβ function [van Rijsbergen 1979, Chapter 7], for some 0 ≤β ≤+∞ , where
Fβ = (β2 + 1)πρ
Here β may be seen as the relative degree of importance attributed to π and ρ.
If β = 0 then Fβ coincides with π, whereas if β = +∞then Fβ coincides with
ρ. Usually, a value β = 1 is used, which attributes equal importance to π and
ρ. As shown in [Moulinier et al. 1996; Yang 1999], the breakeven of a classiﬁer
Φ is always less or equal than its F1 value.
Once an eﬀectiveness measure is chosen, a classiﬁer can be tuned (e.g. thresholds
and other parameters can be set) so that the resulting eﬀectiveness is the best
achievable by that classiﬁer. Tuning a parameter p (be it a threshold or other)
is normally done experimentally. This means performing repeated experiments on
the validation set with the values of the other parameters pk ﬁxed (at a default
value, in the case of a yet-to-be-tuned parameter pk, or at the chosen value, if the
parameter pk has already been tuned) and with diﬀerent values for parameter p.
The value that has yielded the best eﬀectiveness is chosen for p.
7.2 Benchmarks for text categorization
Standard benchmark collections that can be used as initial corpora for TC are
publically available for experimental purposes. The most widely used is the Reuters
collection, consisting of a set of newswire stories classiﬁed under categories related
to economics. The Reuters collection accounts for most of the experimental work in
TC so far. Unfortunately, this does not always translate into reliable comparative
19Breakeven, ﬁrst proposed by Lewis [1992a, 1992b], has been recently criticized. Lewis himself
 points out that breakeven is not a good eﬀectiveness measure, since
(i) there may be no parameter setting that yields the breakeven; in this case the ﬁnal breakeven
value, obtained by interpolation, is artiﬁcial; (ii) to have ρ equal π is not necessarily desirable,
and it is not clear that a system that achieves high breakeven can be tuned to score high on other
eﬀectiveness measures. Yang also notes that when for no value of the parameters π and ρ
are close enough, interpolated breakeven may not be a reliable indicator of eﬀectiveness.
Machine Learning in Automated Text Categorization
results, in the sense that many of these experiments have been carried out in subtly
diﬀerent conditions.
In general, diﬀerent sets of experiments may be used for cross-classiﬁer comparison only if the experiments have been performed
(1) on exactly the same collection (i.e. same documents and same categories);
(2) with the same “split” between training set and test set;
(3) with the same evaluation measure and, whenever this measure depends on some
parameters (e.g. the utility matrix chosen), with the same parameter values.
Unfortunately, a lot of experimentation, both on Reuters and on other collections,
has not been performed with these caveat in mind: by testing three diﬀerent classiﬁers on ﬁve popular versions of Reuters, Yang has shown that a lack of
compliance with these three conditions may make the experimental results hardly
comparable among each other. Table 6 lists the results of all experiments known
to us performed on ﬁve major versions of the Reuters benchmark: Reuters-22173
“ModLewis” (column #1), Reuters-22173 “ModApt´e” (column #2), Reuters-22173
“ModWiener” (column #3), Reuters-21578 “ModApt´e” (column #4) and Reuters-
21578 “ModApt´e” (column #5)20. Only experiments that have computed either
a breakeven or F1 have been listed, since other less popular eﬀectiveness measures
do not readily compare with these.
Note that only results belonging to the same column are directly comparable.
In particular, Yang showed that experiments carried out on Reuters-22173
“ModLewis” (column #1) are not directly comparable with those using the other
three versions, since the former strangely includes a signiﬁcant percentage (58%)
of “unlabelled” test documents which, being negative examples of all categories,
tend to depress eﬀectiveness. Also, experiments performed on Reuters-21578 
“ModApt´e” (column #5) are not comparable with the others, since this collection
is the restriction of Reuters-21578 “ModApt´e” to the 10 categories with the highest
generality, and is thus an obviously “easier” collection.
Other test collections that have been frequently used are
—the OHSUMED collection, set up by Hersh et al. and used in [Joachims
1998; Lam and Ho 1998; Lam et al. 1999; Lewis et al. 1996; Ruiz and Srinivasan 1999; Yang and Pedersen 1997]21. The documents are titles or title-plusabstract’s from medical journals (OHSUMED is actually a subset of the Medline
document base); the categories are the “postable terms” of the MESH thesaurus.
—the 20 Newsgroups collection, set up by Lang and used in [Baker and
McCallum 1998; Joachims 1997; McCallum and Nigam 1998; McCallum et al.
1998; Nigam et al. 2000; Schapire and Singer 2000]. The documents are messages
posted to Usenet newsgroups, and the categories are the newsgroups themselves.
20 The Reuters-21578 collection may be freely downloaded for experimentation purposes from
 and is now considered the “standard”
variant of Reuters. We do not cover experiments performed on variants of Reuters diﬀerent from the
ﬁve listed because the small number of authors that have used the same variant makes the reported
results diﬃcult to interpret. This includes experiments performed on the original Reuters-22173
“ModHayes” [Hayes et al. 1990] and Reuters-21578 “ModLewis” [Cohen and Singer 1999].
21The OHSUMED collection may be freely downloaded for experimentation purposes from
ftp://medir.ohsu.edu/pub/ohsumed
F. Sebastiani
# of documents
# of training documents
# of test documents
# of categories
Results reported by
(non-learning)
[Yang 1999]
probabilistic
[Dumais et al. 1998]
probabilistic
[Joachims 1998]
probabilistic
[Lam et al. 1997]
.443 (MF1)
probabilistic
[Lewis 1992a]
probabilistic
[Li and Yamanishi 1999]
probabilistic
[Li and Yamanishi 1999]
probabilistic
[Yang and Liu 1999]
decision trees
[Dumais et al. 1998]
decision trees
[Joachims 1998]
decision trees
[Lewis and Ringuette 1994]
decision rules
[Apt´e et al. 1994]
decision rules
[Cohen and Singer 1999]
SleepingExperts
decision rules
[Cohen and Singer 1999]
decision rules
[Li and Yamanishi 1999]
decision rules
[Moulinier and Ganascia 1996]
decision rules
[Moulinier et al. 1996]
regression
[Yang 1999]
regression
[Yang and Liu 1999]
BalancedWinnow
on-line linear
[Dagan et al. 1997]
Widrow-Hoff
on-line linear
[Lam and Ho 1998]
batch linear
[Cohen and Singer 1999]
batch linear
[Dumais et al. 1998]
batch linear
[Joachims 1998]
batch linear
[Lam and Ho 1998]
batch linear
[Li and Yamanishi 1999]
neural network
[Ng et al. 1997]
neural network
[Yang and Liu 1999]
neural network
[Wiener et al. 1995]
example-based
[Lam and Ho 1998]
example-based
[Joachims 1998]
example-based
[Lam and Ho 1998]
example-based
[Yang 1999]
example-based
[Yang and Liu 1999]
[Dumais et al. 1998]
[Joachims 1998]
[Li and Yamanishi 1999]
[Yang and Liu 1999]
AdaBoost.MH
[Schapire and Singer 2000]
[Weiss et al. 1999]
Bayesian net
[Dumais et al. 1998]
Bayesian net
[Lam et al. 1997]
.542 (MF1)
Comparative results among diﬀerent classiﬁers obtained on ﬁve diﬀerent version of
Unless otherwise noted, entries indicate the microaveraged breakeven point; within
parentheses, “M” indicates macroaveraging and “F1” indicates use of the F1 measure.
Boldface indicates the best performer on the collection.
—the AP collection, used in [Cohen 1995a; Cohen 1995b; Cohen and Singer 1999;
Lewis and Catlett 1994; Lewis and Gale 1994; Lewis et al. 1996; Schapire and
Singer 2000; Schapire et al. 1998].
We will not cover the experiments performed on these collections for the same
reasons as those illustrated in Footnote 20, i.e. because in no case a signiﬁcant
enough number of authors have used the same collection in the same experimental
conditions, thus making comparisons diﬃcult.
7.3 Which text classiﬁer is best?
The published experimental results, and especially those listed in Table 6, allow us
to attempt some considerations on the comparative performance of the TC methods
discussed. However, we have to bear in mind that comparisons are reliable only
when based on experiments performed by the same author under carefully controlled conditions. They are instead more problematic when they involve diﬀerent
experiments performed by diﬀerent authors. In this case various “background con-
Machine Learning in Automated Text Categorization
ditions”, often extraneous to the learning algorithm itself, may inﬂuence the results.
These may include, among others, diﬀerent choices in pre-processing (stemming,
etc.), indexing, dimensionality reduction, classiﬁer parameter values, etc., but also
diﬀerent standards of compliance with safe scientiﬁc practice (such as tuning parameters on the test set rather than on a separate validation set), which often are
not discussed in the published papers.
Two diﬀerent methods may thus be applied for comparing classiﬁers [Yang 1999]:
—direct comparison: classiﬁers Φ′ and Φ′′ may be compared when they have been
tested on the same collection Ω, usually by the same researchers and with the
same background conditions. This is the more reliable method.
—indirect comparison: classiﬁers Φ′ and Φ′′ may be compared when
(1) they have been tested on collections Ω′ and Ω′′, respectively, typically by
diﬀerent researchers and hence with possibly diﬀerent background conditions;
(2) one or more “baseline” classiﬁers Φ1, . . . , Φm have been tested on both Ω′
and Ω′′ by the direct comparison method.
Test 2 gives an indication on the relative “hardness” of Ω′ and Ω′′; using this and
the results from Test 1 we may obtain an indication on the relative eﬀectiveness
of Φ′ and Φ′′. For the reasons discussed above, this method is less reliable.
A number of interesting conclusions can be drawn from Table 6 by using these two
methods. Concerning the relative “hardness” of the ﬁve collections, if by Ω′ >
Ω′′ we indicate that Ω′ is a harder collection that Ω′′, there seems to be enough
evidence that Reuters-22173 “ModLewis” ≫Reuters-22173 “ModWiener” > Reuters-
22173 “ModApt´e” ≈Reuters-21578 “ModApt´e” > Reuters-21578 “ModApt´e”.
These facts are unsurprising; in particular, the ﬁrst and the last inequalities are a
direct consequence of the peculiar characteristics of Reuters-22173 “ModLewis” and
Reuters-21578 “ModApt´e” discussed in Section 7.2.
Concerning the relative performance of the classiﬁers, remembering the considerations above we may attempt a few conclusions:
—Boosting-based classiﬁer committees, support vector machines, example-based
methods, and regression methods deliver top-notch performance. There seems to
be no suﬃcient evidence to decidedly opt for either method; eﬃciency considerations or application-dependent issues might play a role in breaking the tie.
—Neural networks and on-line linear classiﬁers work very well, although slightly
worse than the previously mentioned methods.
—Batch linear classiﬁers (Rocchio) and probabilistic Na¨ıve Bayes classiﬁers look
the worst of the learning-based classiﬁers. For Rocchio, these results conﬁrm
earlier results by Sch¨utze et al. , who had found three classiﬁers based on
linear discriminant analysis, linear regression, and neural networks, to perform
about 15% better than Rocchio. However, recent results by Schapire et al. 
rank Rocchio along the best performers once near-positives are used in training.
—The data in Table 6 are hardly suﬃcient to say anything about decision trees.
However, the work by Dumais et al. in which a decision tree classiﬁer was
shown to perform nearly as well as their top performing system (a SVM classiﬁer)
will probably renew the interest in decision trees, an interest that had dwindled
F. Sebastiani
after the unimpressive results reported in earlier literature [Cohen and Singer
1999; Joachims 1998; Lewis and Catlett 1994; Lewis and Ringuette 1994].
—By far the lowest performance is displayed by Word, a classiﬁer implemented
by Yang and not including any learning component22.
Concerning Word and no-learning classiﬁers, for completeness we should recall
that one of the highest eﬀectiveness values reported in the literature for the Reuters
collection (a .90 breakeven) belongs to Construe, a manually constructed classiﬁer. However, this classiﬁer has never been tested on the standard variants of
Reuters mentioned in Table 6, and it is not clear [Yang 1999] whether the (small)
test set of Reuters-22173 “ModHayes” on which the .90 breakeven value was obtained was chosen randomly, as safe scientiﬁc practice would demand. Therefore,
the fact that this ﬁgure is indicative of the performance of Construe, and of the
manual approach it represents, has been convincingly questioned [Yang 1999].
It is important to bear in mind that the considerations above are not absolute
statements (if there may be any) on the comparative eﬀectiveness of these TC
methods. One of the reasons is that a particular applicative context may exhibit
very diﬀerent characteristics from the ones to be found in Reuters, and diﬀerent
classiﬁers may respond diﬀerently to these characteristics. An experimental study
by Joachims involving support vector machines, k-NN, decision trees, Rocchio and Na¨ıve Bayes, showed all these classiﬁers to have similar eﬀectiveness on
categories with ≥300 positive training examples each. The fact that this experiment involved the methods which have scored best (support vector machines, k-NN)
and worst (Rocchio and Na¨ıve Bayes) according to Table 6 shows that applicative
contexts diﬀerent from Reuters may well invalidate conclusions drawn on this latter.
Finally, a note is worth about statistical signiﬁcance testing. Few authors have
gone to the trouble of validating their results by means of such tests. These tests
are useful for verifying how strongly the experimental results support the claim that
a given system Φ′ is better than another system Φ′′, or for verifying how much a
diﬀerence in the experimental setup aﬀects the measured eﬀectiveness of a system
Φ. Hull and Sch¨utze et al. have been among the ﬁrst to work in this
direction, validating their results by means of the Anova test and the Friedman
test; the former is aimed at determining the signiﬁcance of the diﬀerence in eﬀectiveness between two methods in terms of the ratio between this diﬀerence and the
eﬀectiveness variability across categories, while the latter conducts a similar test by
using instead the rank positions of each method within a category. Yang and Liu
 deﬁne a full suite of signiﬁcance tests, some of which apply to microaveraged
and some to macroaveraged eﬀectiveness. They apply them systematically to the
comparison between ﬁve diﬀerent classiﬁers, and are thus able to infer ﬁne-grained
conclusions about their relative eﬀectiveness. For other examples of signiﬁcance
testing in TC see [Cohen 1995a; Cohen 1995b; Cohen and Hirsh 1998; Joachims
1997; Koller and Sahami 1997; Lewis et al. 1996; Wiener et al. 1995].
22Word is based on the comparison between documents and category names, each treated as a
vector of weighted terms in the vector space model. Word was implemented by Yang with the
only purpose of determining the diﬀerence in eﬀectiveness that adding a learning component to
a classiﬁer brings about. Word is actually called STR in [Yang 1994; Yang and Chute 1994].
Another no-learning classiﬁer is proposed in [Wong et al. 1996].
Machine Learning in Automated Text Categorization
8. CONCLUSION
Automated TC is now a major research area within the information systems discipline, thanks to a number of factors
—Its domains of application are numerous and important, and given the proliferation of documents in digital form they are bound to increase dramatically in
both number and importance.
—It is indispensable in many applications in which the sheer number of the documents to be classiﬁed and the short response time required by the application
make the manual alternative implausible.
—It can improve the productivity of human classiﬁers in applications in which no
classiﬁcation decision can be taken without a ﬁnal human judgment [Larkey and
Croft 1996], by providing tools that quickly “suggest” plausible decisions.
—It has reached eﬀectiveness levels comparable to those of trained professionals.
The eﬀectiveness of manual TC is not 100% anyway [Cleverdon 1984] and, more
importantly, it is unlikely to be improved substantially by the progress of research.
The levels of eﬀectiveness of automated TC are instead growing at a steady pace,
and even if they will likely reach a plateau well below the 100% level, this plateau
will probably be higher that the eﬀectiveness levels of manual TC.
One of the reasons why from the early ’90s the eﬀectiveness of text classiﬁers has
dramatically improved, is the arrival in the TC arena of ML methods that are
backed by strong theoretical motivations.
Examples of these are multiplicative
weight updating (e.g. the Winnow family, Widrow-Hoff, etc.), adaptive resampling (e.g. boosting) and support vector machines, which provide a sharp contrast
with relatively unsophisticated and weak methods such as Rocchio. In TC, ML
researchers have found a challenging application, since datasets consisting of hundreds of thousands of documents and characterized by tens of thousands of terms
are widely available. This means that TC is a good benchmark for checking whether
a given learning technique can scale up to substantial sizes. In turn, this probably
means that the active involvement of the ML community in TC is bound to grow.
The success story of automated TC is also going to encourage an extension of its
methods and techniques to neighbouring ﬁelds of application. Techniques typical
of automated TC have already been extended successfully to the categorization of
documents expressed in slightly diﬀerent media; for instance:
—very noisy text resulting from optical character recognition [Ittner et al. 1995;
Junker and Hoch 1998]. In their experiments Ittner et al. have found that,
by employing noisy texts also in the training phase (i.e. texts aﬀected by the same
source of noise that is also at work in the test documents), eﬀectiveness levels
comparable to those obtainable in the case of standard text can be achieved.
—speech transcripts [Myers et al. 2000; Schapire and Singer 2000]. For instance,
Schapire and Singer classify answers given to a phone operator’s request
“How may I help you?”, so as to be able to route the call to a specialized operator according to call type.
Concerning other more radically diﬀerent media, the situation is not as bright . The reason for this is that capturing real semantic content of
non-textual media by automatic indexing is still an open problem. While there are
systems that attempt to detect content e.g. in images by recognising shapes, colour
distributions and texture, the general problem of image semantics is still unsolved.
The main reason is that natural language, the language of the text medium, admits far fewer variations than the “languages” employed by the other media. For
instance, while the concept of a house can be “triggered” by relatively few natural
language expressions such as house, houses, home, housing, inhabiting, etc., it
can be triggered by far more images: the images of all the diﬀerent houses that
exist, of all possible colours and shapes, viewed from all possible perspectives, from
all possible distances, etc. If we had solved the multimedia indexing problem in
a satisfactory way, the general methodology that we have discussed in this paper
for text would also apply to automated multimedia categorization, and there are
reasons to believe that the eﬀectiveness levels could be as high. This only adds to
the common sentiment that more research in automated content-based indexing for
multimedia documents is needed.
Acknowledgements
This paper owes a lot to the suggestions and constructive criticism of Norbert Fuhr
and David Lewis. Thanks also to Umberto Straccia for comments on an earlier
draft and to Alessandro Sperduti for many fruitful discussions.