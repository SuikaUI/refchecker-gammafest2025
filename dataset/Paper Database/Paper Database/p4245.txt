Robust and Efficient Estimation by
Minimising a Density Power Divergence
BY AYANENDRANATH BASU
Applied Statistics Unit, Indian Statistical Institute,
203 B. T. Road, Calcutta 700 035, India
IAN R. HARRIS
Department of Mathematics, Northern Arizona University,
Flagstaff, Arizona 86011, U.S.A.
NILS L. HJORT
Department of Mathematics and Statistics, University of Oslo,
P.B. 1053 Blindern, N-0316 Oslo, Norway
AND M. C. JONES
Department of Statistics, The Open University,
Milton Keynes, MK7 6AA, United Kingdom
A minimum divergence estimation method is developed for robust parameter estimation
and model fitting. The proposed approach uses new density-based divergences which, unlike
existing density-based minimum divergence methods (e.g. minimum Hellinger distance estimation), avoid the use of nonparametric density estimation and associated complications
such as bandwidth selection. The proposed class of 'density power divergences' is indexed
by a single parameter a which can be varied to study the trade-off between robustness
and efficiency. The method can be viewed as a robust extension of maximum likelihood
estimation, since the class of divergences contains the Kullback-Leibler divergence when
a = 0. Choices of a near zero afford robustness while retaining efficiency close to that of
maximum likelihood.
Some key words: Asymptotic efficiency; Divergence; Influence function; Maximum likelihood; Robustness.
1. INTRODUCTION
In parametric estimation, density-based minimum divergence methods, i.e. methods
which estimate the parameter through minimising a data-based estimate of some appropriate divergence between the assumed model density and the "true" density underlying
the data, have a long history. These procedures include the classical maximum likelihood
method as well as the minimum chi-square methods based on the families of chi-square
distances studied by several authors . Beran , using Hellinger distance, was the first to use the technique of
density-based minimum divergence estimation in continuous models to develop parameter
estimates with good robustness properties relative to maximum likelihood. Among others,
Tamura & Boos and Simpson have followed up on this line of research. Under some regularity conditions these methods have full asymptotic efficiency at the model.
However, in continuous models the methods suffer from the drawback that it is necessary
to use some nonparametric smoothing technique such as kernel density estimation to produce a continuous estimate of the true density they therefore involve all the associated
complications such as bandwidth selection. See also Cao, Cuevas & Fraiman . Basu
& Lindsay considered another modification of this approach where the model is
smoothed with the same kernel as the data to reduce the dependence of the procedure on
the smoothing method.
The present paper introduces a new family of density-based divergence measures, to be
called density power divergences. The family is indexed by a single parameter a
which controls the trade-off between robustness and asymptotic efficiency of the parameter
estimates which are the minimisers of this family of divergences. When a= 0, the density
power divergence is the Kullback-Leibler divergence and the
method is maximum likelihood estimation; when a= 1, the divergence is the mean squared
error, and a robust but inefficient minimum mean squared error estimator ensues. For any a,
the estimation procedure has the considerable advantage of not requiring any nonparametric
smoothing. Various examples are explored to investigate the interplay between robustness
and efficiency. It is found that some of the estimators have strong robustness properties with
little loss in asymptotic efficiency relative to maximum likelihood under model conditions.
The rest of the paper is organised as follows. In §2 we develop the estimation procedure considered in this paper, discuss some of its properties, and establish the asymptotic
normality of the estimators. A robust model choice criterion is also suggested. In §3 we
investigate the performance of the estimators in several common parametric families, study
the breakdown of the methods in the normal model, and illustrate the performance of the
method in some examples. In §4 we develop robust regression procedures utilising these
ideas. Concluding remarks are presented in §5. Our work is related to, but different from,
that of Windham ; see §2.2.
2. THE DENSITY POWER DIVERGENCE AND RELATED INFERENCE
2·1. The minimum L 2 distance estimator
Consider a parametric family of models { Ft}, indexed by the unknown parameter t E
n c Rs, possessing densities {ft} with respect to Lebesgue measure, and let 9 be the class
of all distributions having densities with respect to Lebesgue measure. Define the minimum
L2 distance, or minimum mean squared error, functional T1(·) by the requirement that for:
every Gin 9,
J{g(z)- fr1(G)(z)} 2 dz = minj{g(z)- ft(z)} 2 dz
where g is the density of G. (For the sake of keeping a clear focus in our presentations we
have defined the class of densities 9 as above, but the results hold for discrete models as
well.) Normally T1 (G) would indeed exist and be unique, and we shall assume this to be
the case. Suppose also that the parametric family is identifiable in the sense that t 1 =!= t 2
implies that { z: ft 1 ( z) =!= ft 2 ( z)} is a set of positive Lebesgue measure. The minimum L2
distance functional is then Fisher consistent in the sense that T1 ( F8 ) = e, uniquely.
Note that the L2 distance J {g ( z) - ft ( z) P dz between g and ft can be represented as
J fl(z) dz- 2 J ft(z) dG(z) + C; the quantity Cis independent of the parameter t, so does
not affect the minimisation procedure. Given a random sample X 1 , ... , Xn from the true
distribution with density g, one can actually minimise
j ft2(z) dz- 2 j ft(z) dGn(z) = j H(z) dz- 2n-1 t ft(Xi)
with respect to t, where Gn is the empirical distribution function, to obtain the minimum
L2 distance estimator of the best fitting parameter. Notice that this does not require a
smooth nonparametric estimate of g, in contrast to the work of Cao et al. .
Under differentiability of the model and appropriate regularity conditions, the minimum
L2 distance estimators can be obtained by solving the estimating equation
n-1 t Ut(Xi)ft(Xi)- j Ut(z)ff(z) dz = 0,
where Ut ( z) = a log !t ( z) I at is the maximum likelihood score function. Note that the above
estimating equation is unbiased when g = ft.
For the sake of illustration, let { Ft} be a location model, with location parameter t, in
which case J fl(z) dz is independent oft, and the minimum L2 distance estimator is now
the maximiser of 'L-i ft(Xi), with corresponding estimating equation 'L-i Ut(Xi)ft(Xi) = 0.
This contrasts with the maximum likelihood estimator which maximises 'L-i log ft(Xi), with
the corresponding estimating equation being :Ei Ut(Xi) = 0. For a random variable X in
the exponential family with t being the mean value parameter, Ut(z) equals (z- t)/CJ2 ,
where CJ2 is the variance of X; thus the sample mean is the maximum likelihood estimator
for the mean parameter in these families, suggesting the robustness problems of maximum
likelihood. On the other hand, for several parametric models such as the normal, Ut(z)ft(z)
is a bounded function of z for fixed t, as is the influence function of the minimum L 2
distance functional. This downweighting of the score function is probabilistic, rather than
geometric, in the sense that greater downweighting is provided for observations that have
lower probabilities of occurrence under the model, as opposed to for observations that are
simply far from other observations.
A few examples of the robustness of some variants of the minimum L2 distance estimator in the normal model have been presented by Brown & Hwang , while studying
minimising the L 2 distance between a normal density and a histogram estimating g. Consideration of the small contribution of outliers to L 2 distance based on histograms or kernel
density estimates makes this robustness intuitively apparent. See also Terrell , Hjort
 and Jones & Hjort .
Unfortunately, however, the robustness of the minimum L 2 distance estimator is achieved
at a fairly stiff price in asymptotic efficiency, as we will see later. In order to generate robust
estimates with better efficiencies we introduce a family of divergences, and the estimators
obtained by minimising these divergences, bridging the gap between maximum likelihood
and minimum L 2 . Many of these estimators combine strong robustness properties with
high asymptotic efficiency.
2· 2. The minimum density power divergence estimator
Define the divergence da(g, f) between density functions g and f to be
da(g, f) = j { fl+a(z) - ( 1 + ±) g(z)fa(z) + ~gl+a(z)} dz
When a= 0, the integrand in the expression (2.4) is undefined, and we define the divergence
do(g, f) as
do(g, f)= lim da(g, f)= jg(z) log(g(z)/ f(z)) dz.
Notice that d0(g, f) is a version of the Kullback-Leibler divergence.
In the estimation procedure that we discuss in this paper, we are most interested in
smaller values of a ~ 0, say between zero and one, although values greater than one can
be considered too. The procedure typically becomes less and less efficient as a increases as
we will see later.
THEOREM 2 ·1. The quantity da (g, f) is a divergence in that it is nonnegative for all
g, f E Q and is equal to zero if and only if f
g almost everywhere.
Proof. Suppose a > 0. Consider the integrand at a fixed value y. Factor out the term
gl+a(y)ja and define x = f(y)jg(y). To show that da(g, f) is nonnegative it is sufficient
to show that the factored integrand, Ia(x) = axl+a- (1 + a)xa + 1, is nonnegative for all
x ~ 0. Clearly Ia(1) = 0, and in fact this is the unique minimum for x ~ 0; the derivative
of Ia is strictly negative for all x < 1, zero for x = 1 and strictly positive for all x > 1.
Thus da(g, f) is nonnegative and is equal to zero if and only if g = f identically.
When a = 0, it is well known that the above Kullback-Leibler divergence is nonnegative
and is equal to zero if and only if g
The family of divergences da, as a function of a, will be called the class of density power
divergences. Under the set-up of the previous section, the following is a simple consequence
of Theorem 2.1: for any given a the minimum density power divergence functional at G,
defined by the requirement da(g, fr(c)) = mintEn da(g, ft), is Fisher consistent; we will
denote this functional by Ta (G). In addition, the minimum density power divergence
estimator e, generated by minimising
with respect tot, is weakly consistent for()= Ta(G) as well (see Theorem 2.2). We assume
here that Ta( G) exists and is unique, as will normally be the case. Verifying this is perhaps
most easily done on a case by case basis, and would depend on the parameter space and
the complexity of the {ft} family as well as on the true density g.
Consider the functional T0 (·). Given the data, T0(Gn) maximises flogft(z) dGn(z), and
is therefore the maximum likelihood estimate of the parameter if it exists. On the other
hand, the value a = 1 gives precisely the £ 2 distance between the densities discussed in
§2.1. Thus, for 0 < a< 1, the class of density power divergences provides a smooth bridge
between the £ 2 distance and the Kullback-Leibler divergence.
Some motivation for the form of the divergence (2.4) can be obtained by again looking
at the location model, where J Jl+a(z) dz is independent oft. In this case, the proposed
estimators maximise I:i ft(Xi), with the corresponding estimating equations having the
n L Ut(Xi)fta(Xi) = 0.
Equation (2.6) can be viewed as a weighted version of the efficient maximum likelihood score
equation. When a > 0, (2.6) provides a relative-to-the-model downweighting for outlying
observations; observations that are are wildly discrepant with respect to the model will get
nearly zero weights. In the fully efficient case a = 0, all observations, including very severe
outliers, get weights equal to one. By choosing a value of a close to zero, one makes all
the weights closer to 1 compared to the minimum L 2 method, improving the asymptotic
efficiency of the procedure. The proposed estimators, therefore, represent compromises
between efficiency and robustness, with the degree of compromise controlled by the tuning
parameter a.
For general families it can be checked easily that the estimating equations have the form
Un(t) =I Ut(z)ftl+a(z) dz- n-1 t Ut(Xi)ft01(Xi) = 0.
Again this estimating equation is unbiased when g = ft· Notice that this has the appealing
advantage that it does not require a smooth estimate of g which is necessary in other
robust density-based minimum divergence approaches ;
thus the bandwidth selection problem and rate of convergence results for the kernel density
estimator are no longer relevant.
We now present the asymptotic distribution of the minimum density power divergence
estimators, when the data are generated from the true distribution G not necessarily in the
model. (In the following, e represents the best fitting value of the parameter, whereas t
denotes a generic element of 0.) Let X 1, ... , Xn be independent and identically distributed
with distribution G with corresponding density g' Ta (G) = e = ( el' ... ' e s)' and let (j = Bn
be the minimiser of (2.5). Let K = K(O) be the covariance matrix ofT = ff(X)ut(X)
under G i.e.
K =I uo(z)unz)fta(z)g(z) dz- ~o~f and
~o =I uo(z)f~(z) g(z)dz.
For any given a, make the following assumptions:
Al: The distributions Ft and G have common support, so that the set A on which the
densities are greater than zero is independent oft.
A2: There is an open subset w of the parameter space n containing the best fitting
parameter 0 such that for almost all z E A, and all t E w, the density ft(z) is three times
differentiable with respect to t and the third partial derivatives are continuous with respect
A3: The integral J Jl+a(z) dz can be differentiated three times with respect to t, and
the derivative can be taken under the integral sign.
A4: The matrix J = J(t), defined by
J(t) =I uo(z)unz)jJ+a(z) dz +I (io(z)- auo(z)uf(z))(g(z)- fo(z))f~(z) dz,
where it(x) = -o{ut(x)}jot, the so called information function of the model, is positive
definite for all t E w.
AS: There exist functions Mjkl(x) such that I83Vn,t(x)j8tj8tk8tll :S Mjkl(x) for all
t E w, where Ea[Mjkl(X)] < oo for all j, k and l, where Ea denotes expectation with
respect to G.
THEOREM 2·2. Under the above conditions, with probability tending to 1 as n ---+ oo,
there exists en such that
(i) en is consistent fore, and
(ii) n 112 ((Jn - e) is asymptotically multivariate normal with (vector) mean zero and
covariance matrix J-1 K J-1 .
The proof of Theorem 2.2 follows closely the proof of Theorem 6.4.1 of Lehmann 
(which is for the maximum likelihood estimator) with appropriate modifications to cope
with our density power divergence and the allowance of distributions outside the model.
The proof is omitted to save space; full details may be obtained from the first author.
For simplicity of notation, the subscript a has been dropped from the quantities e, Hn,
Vn,t, Un, Mjkl, as well as the matrices J and K. In addition, en will revert to e in what
follows. The simplified formulae occurring when G is in the model will be considered in §3.
Note that the divergence given by (2.4) is close to a weighted L2 distance 
in the sense that, for fixed a, and f close to g, da(g, f) becomes close to
Observe how minimum L2 corresponds (exactly) to a unit weighting, maximum likelihood
corresponds to a 1/ g weighting, and minimum density power divergence for 0 < a < 1
corresponds to an intermediate 1/ g'Y for 0 < 1 < 1 weighting. Unlike (2.10), however, the
beauty of (2.4) is that, ignoring the last term because it does not depend on f, g appears
only as a multiplier of terms in f. Thus while f will be replaced by ft, g can appropriately
be replaced by its empirical version, and there is no need to introduce any smoothing into
the formulation. (The same holds, of course, for maximum likelihood estimation.)
The idea of downweighting with respect to the model rather than the data is also the
motivating principle of Windham . Windham describes a fixed point algorithm that
also uses density power weighting. Windham's procedure is equivalent to choosing t such
I:i Ut(Xi)ftOi.(Xi)
l::i ft(Xi)
f Ut(z)Jl+a(z) dz
J fl+a(z) dz
If ft is a location family then (2.11) reduces to (2.6), and thus for this special case Windham's procedure is identical to ours. In general (2.11) does not reduce to (2.7). Insights
into the relationship between the two methods and practical comparisons between them
are the subject of a further paper currently in preparation.
2·3. Influence function and standard error
Let G€(z) = (1 - E)G(z) + EXy(z), 0 < E < 1, where Xy(z) is the distribution function
of the random variable which puts all its mass on y. By direct differentiation of equation
(2. 7) (with G€ in place of the implicit Gn) with respect to E, one gets the influence function
of the density power divergence functional to be
and ~0 and J are as in (2.8) and (2.9). Assuming that J and ~0 are finite, this is a bounded
function of y whenever u0(y)f0(y) is bounded. This is true, for example, for any a > 0 in the
normal location-scale problem, unlike other density based minimum divergence procedures
such as those based on the Hellinger distance. The influence functions for the estimation
of the normal mean when a = 1 are plotted in Figure 1 for several values of a; note their
redescending nature for all a > 0.
* * * Figure 1 about here
The asymptotic variance of ( yri times) the minimum density power divergence estimator
can be consistently estimated in a sandwich fashion by using the above influence function.
Let Ki = u0(Xi)f0(Xi)- ~0 , and~ be the corresponding quantity evaluated at 1f, with Gn
in place of G. Let K = (n- 1t1 I:i(KiKi ). Then the asymptotic variance of yri times
the parameter estimates can be consistently estimated by J-1 K J-1, where J is obtained
from J by replacing() with 1f, with Gn in place of G. Consistent estimates of the asymptotic
variance of the method can also be obtained by the jackknife and bootstrap techniques.
2· 4. Equivariance
The maximum likelihood method has two important equivariance properties; estimates
are equivariant with respect to both reparametrisations and transformation of the data.
Our minimum density power divergence method shares the first general property: if the
model is reparametrised to 'ljJ = 'ljJ(()) with a one-one transformation, then the density power
divergence estimate of 'ljJ is simply ,(jj = 'l/J(1f), in terms of the density power divergence
estimate of(), using the same a. This follows from definition (2.5).
The second maximum likelihood property does not generally hold for the new estimation
method, however. If data are transformed from Xi toY;= h(Xi), then the minimum density
power divergence estimator, say B*, is defined as the minimiser of
j {ft(~(y)) lf(y) I}Ha dy- n-1(1 + a-1) t {ft(Xi) l~'(h(Xi)) IV,
where Xi = ~(Y;) is the inverse transformation. We see by comparison with (2.5) that B* is
equal to 1J only if e(y) is a non-zero constant. Thus the estimation method is equivariant
under a Y; = aXi + b type data transformation, but not under other transformations (unless
2· 5. Hypothesis testing
As a consequence of Theorem 2.2, one can readily construct Wald and score type
tests for the null hypothesis H0 : e = 00 . We work under model conditions although it
is possible to test hypotheses about e outside the model too, where e = Ta(G). Under the null, the asymptotic distribution of yln(1J- 00 ) is normal with mean zero and
covariance matrix C(B) = J-1(B0)K(B0)J-1(B0).
As a result the Wald type statistic
n(e- B0fC- 1(1J)(e- 00) has an asymptotic x2(s) distribution. Similarly the score type
statistic given by nU'!:(B0)K-1(B0)Un(B0 ) has, under the null, the same asymptotic x2 (s)
distribution and is asymptotically equivalent to the Wald type statistic.
For the composite null hypothesis, let e = ( Bf, Off where 01 lies in an s1-dimensional
subspace of n. Consider the null hypothesis H0 : 01 = 00,1 where 02 is unspecified. Let
{j = (Yf, B'ff and {jN = (err, g~Tf be the minimum density power divergence estimates
of the parameter without any restriction and under the null hypothesis respectively. Let
Cii1(B) and Kii1(B) be the s1 X s1 blocks corresponding to e1 in c-1(0) and K- 1(0). Also
let U1,n be the component of the density power score function corresponding to 01 . Then
the Wald type and the score type statistics, given by n(1J1 -
B0,1fCii1(1J)(1J1 -
nU1,n(BN )Kii (BN )U1,n(BN) are both asymptotically x2(s1).
2· 6. A robust model choice criterion
Model choice criteria of the Akaike information variety penalise a model's achieved maximum log-likelihood with a term which depends suitably on the complexity of the candidate
model. The arguments used to motivate and construct these criteria are typically asymptotic in nature, relying on the large-sample behaviour of maximum likelihood estimators.
A similar route can be followed for the present type of robust estimators, working with da
of (2.4) instead of d0 . We will in fact argue in favour of the following strategy. For each
candidate model M, compute the da divergence estimate eM, which by the theory above
has an associated estimated variance matrix of the form n-1l;{RMJ-;,l. Then evaluate the
robust information criterion
In the end choose the model with the smallest value of RIC. The limiting version of this
as a tends to zero can be shown to be the same as maximising the achieved log-likelihood
minus Tr(J-;}KM) (involving suitable a= 0 definitions of JM and KM, see §2.3). But this
is quite close to the traditional Akaike method which takes the view that the models are
(approximately) correct, in particular entailing JM = KM (with a= 0); that is, the trace
above becomes the number of parameters in the model.
To show how RIC evolves, agree first to put
P = j Jf+a(y) dy- (1 + ;) E{f;(xn+l) I data},
which is a fixed constant away from being the distance da(g, Jo(-)) from truth to estimated
model, and the conditional expectation operation is with respect to a new observation Xn+l,
independent of previous data. We think of p as the predictive quality of the estimated
model, and aim for models with as small p values as possible. Note, using (2.5), that
Hn(e), being the minimum of an empirical process, will tend to undershoot the real p. A
more balanced method is via cross validation,
writing ~i) for the estimate obtained by leaving Xi out of the data set. The Px estimator
is almost unbiased for p, and can indeed be used as a model selection criterion.
It is fruitful to work out an approximation which is less intensive computationally. This
can be done via influence functions. We find
involving the leave-Xi-out version of the empirical distribution in addition to xxi as in §2.3.
This leads to
f~i) (Xi) · ft(1- n-1aufL),
where h, fii and L = J-1(uJia- [),with [ = n- 1 2:::~ 1 uJ·t, are the natural empirical
versions of fo(Xi), uo(Xi) and Ia(G, Xi)· But this yields
Px · Hn(e) + (1 +a) n-2 L ftuTL = Hn(e) + (1 +a) n-1Tr(J-1 K)
after some calculations.
There is an alternative route to establishing this RIC formula, more akin to deductions
one may find in the literature for the AIC criterion. The derivation above, however, does not
presume that the models worked with are actually correct, and exhibits the cross-validation
formula as a selection criterion of separate interest.
3. SPECIAL PARAMETRIC FAMILIES: EFFICIENCY, BREAKDOWN AND EXAMPLES
Suppose that the true distribution g belongs to the parametric family {ft}, e being the
true value of the parameter. Then the formulae for J, K and ~0 simplify to
J =I uo(z)unz)fJ+a(z) dz,
K =I uo(z)unz)fJ+2a(z) dz- ~o~f and
~o =I uo(z)fJ+a(z) dz.
Note that in the limit a--+ 0, J and K both become equal to the classic Fisher information.
These formulae can be used to investigate the asymptotic efficiency of the estimators, and
in particular to judge how much is lost relative to the maximum likelihood estimator under
model conditions. In the following subsection, some examples for particular parametric
families are considered. We will define the asymptotic relative efficiency of an estimator to
be the ratio of the asymptotic variance of the maximum likelihood estimator to that of the
estimator in question.
3·1. Efficiencies for particular families
(a) Mean of univariate normal. For a location family ~0 = 0. Letting fo be the N(p,, a 2)
density with known a2 and u0 the score function with respect to the mean parameter p,,
elementary integration gives
The asymptotic variance of n 112 times the estimator of p, is then given by
Since the asymptotic variance of n 112 times the maximum likelihood estimator is a2 , the
asymptotic relative efficiency of the density power divergence estimator is easy to compute.
For a= 0.25 it is 0.941, for example, already quite close to one. Results for different values
of a are given in the first row of Table 1.
* * * Table 1 about here * * *
(b) Standard deviation of univariate normal. Again, let fe be the N (J-L, a-2) density but
treat both parameters as unknown. Calculations for the two 2 x 2 matrices J and K show
that both have zeros off the diagonals, that is, the estimators fl and & are asymptotically
independent. The limiting distribution for y'n([l- J-L) is therefore as found in case (a) even
when a- is unknown.
Here, we concentrate on estimation of a-. Lengthy calculations show that the asymptotic
variance of n112 times the estimator is
(1 + a)2 {
...,.-----'-.:.._,:..-=-= - a
(2 + a 2)2
(1 + 2a)5/2
where Q( a) = 1 + 3a + 5a2 + 7 a 3 + 6a4 + 2a5. Efficiency calculations (compare with a-2 /2)
are presented in the second row of Table 1. Small a density power divergence estimation
continues to retain high efficiency. The values in Table 1 clearly show that the minimum
L2 distance estimators of J-L and a- are quite inefficient; see also Hjort .
(c) Exponential distribution. For the density fe(x) = e-l exp( -x/B), X> 0, the quantities K and J in the asymptotic variance of n112 times the minimum density power divergence
estimator of () are given by
K = { 1 + 4a2 a2
} e-(2+2a)
1 + a2 e-(2+a)
The asymptotic variance is then given by
(1 + a) 2 P( a)B2
(1 + a 2)2(1 + 2a)3
where P(a) = 1+4a+9a2+14a3+13a4+8a5+4a6 . Again the asymptotic variance ofn112
times the maximum likelihood estimator is ()2 , so the asymptotic relative efficiencies are
easily obtained. They are given for certain a in the third row of Table 1. Again, efficiencies
remain high for small a.
(d) Mean of multivariate normal. The family is Np(/-L, I;). The limiting covariance
matrix of n 112 times the minimum density power divergence estimator of J-L (whether or not
I; is known) can be shown to be
Thus one loses efficiency for increasing p if a is kept fixed.
(e) Poisson distribution. Calculation of the asymptotic variance of the estimator can
be carried out numerically, although not via a closed-form formula. It involves an infinite
but rapidly convergent sum. In Table 1 we also provide the asymptotic relative efficiencies
of the estimators for two different values of the mean parameter A and several choices of
a. Note that the Poisson results are very similar to those for normal f-t for A 2: 10.
3· 2. Breakdown in the normal distribution
The breakdown point of an estimator, crudely described as the proportion of bad observations that an estimator can tolerate before it becomes completely uninformative, is
one of the descriptors of the robustness of the method. Here we determine the gross-error
breakdown point of the minimum
density power divergence estimator of the parameters of the normal distribution under a
particular contamination.
Let a> 0 and let g be the N(~-t, a2) density, written c/Jp,, 17 (·) = a-1¢(a-1(·- ~-t)), ft the
N(m, s2) density and q(z) = (1 - c)g(z) + cb'x(z), where 6 is the Dirac delta function and
x --+ oo. The data are a random sample from q and the target parameters are () = (~-t, a).
Consider the maximiser of
(1 +a) I q(z)fta(z) dz- a I fta+l(z) dz
(1 + a){(1- c) I cPp,,C7(z)¢~,s(z) dz + c I 6x(z)¢~,8 (z) dz}- a I ¢~~~(z) dz
with respect to m and s. If location breakdown occurs, the value of m which maximises
the above goes to oo, if scale breakdown occurs, the maximising value of s goes to 0 or oo
 .
To evaluate 1/J( m, s), the following result, provable by elementary calculations, is useful:
( )A-.a ( )d = exp[-a(c-m)2/{2(s2+ad2)}]
'f'c d Z 'f'm s Z
(211')a/2 8a(1+~~2 )
Write A= ajs. It follows that 1/J1(m,A)
(211')a12aa'ljJ(m,s) is given by
1/JI(m, A) =
Aa ((1 + a)(1- c) exp[-a(~-t- m)2 /{2(s2 + aa2)}]
(1 + aA2)1/2
c(1 +a) exp{ -aA2(x- m)2 /(2a2)}- (1 + aa)l/2) .
We now wish to maximise this quantity over A (rather than s) and m. First, 1jJ1(m, 0) =
0. For A> 0, 1/J1(m, A) consists essentially of two ridges which have heights
Aa{(1+a)(1-c)
(1 + aA2)1/2 -
(1 + a)l/2
at m = 1-t
Aa { c(1 +a) - (1 + aa)l/2} at m = x.
If them= x ridge height is negative, i.e. E < K
a/(1 + a) 312 , A= 0 would be optimal
if the m = p ridge height is negative for all A > 0 too. The latter happens if E > 1 - K.
However, 1- K < E < K is impossible because K < 1/2. So, A = 0 cannot maximise
7);1(m,A).
However, if the m = x ridge height is positive, the value along this ridge tends to oo as
A --+ oo. The values along the m = p ridge, however, stay finite: even if they are positive
somewhere, they will have a finite maximum at a finite A and tend to a negative quantity
as A --+ oo. That is, the maximum, if the m = x ridge is positive, is at m = x and s = 0.
This makes sense because for enough bad points, the normal fit tends to match 6x with
mean x --+ oo and variance zero. This is simultaneous location and scale breakdown in the
sense that location "explodes" and scale "implodes" .
Breakdown therefore occurs if
The breakdown point increases monotonically from zero when a ~ 0 (in line with the zero
breakdown of the maximum likelihood estimator which can easily be shown separately)
to 1/(2J2) = 0.354 when a = 1. (In fact, the breakdown continues to increase until its
maximal value of 2/(3J3) = 0.385 at a= 2, but by then the efficiency of the estimator is
unacceptably low.)
3· 3. Examples
In our first example we consider Newcomb's light speed data . The data
set can be found in many elementary texts, including Moore & McCabe . The
data were also analysed by Brown & Hwang , who were trying to fit the "best
approximating normal distribution" to the corresponding histogram. The limiting case of
their approach generates the normal distribution whose mean and standard deviation are
the minimum L 2 distance estimates of p and CJ under a normal model. This estimator, it
was observed, quite successfully downweighted the extreme outliers in the Newcomb data.
* * * Table 2 and Figure 2 about here * * *
For this dataset, Table 2 gives the values of the minimum density power divergence
estimates of p and CJ for various values of a under the normal model. These estimators
exhibit strong outlier resistance properties even for quite small values of a. When a is
as small as 0.1 (for which the minimum density power divergence estimator of u has an
efficiency loss of only 2.4% under the model) the estimate of a is 5.39, fairly close to
the estimate obtained for a = 1. A visual representation of this is provided in Figure 2,
where the normal densities N(P,, &2), for a= 1, 0.5, 0.25, 0.1 and 0 are superimposed on a
histogram of the Newcomb data. Except when the maximum likelihood estimator is used,
all the normal densities fit the main body of the histogram quite well, even the one with
In the second example our estimation method is applied to chemical mutagenicity data
previously analysed by Simpson in the context of minimum Hellinger distance estimation. In the sex linked recessive lethal test in drosophila (fruit flies), male flies are
exposed to different doses of a chemical to be screened. They are then mated with unexposed females and for each male the number of daughter flies carrying a recessive lethal
mutation on the X chromosome is noted. One such experiment with 34 males resulted in
23, 7, 3 and 1 males having 0, 1, 2 and 91 such daughters respectively. Note that the last
value of 91 is a very large outlier. Simpson considered a Poisson fit for these data, and
found that the minimum Hellinger distance estimate of the mean parameter A successfully
downweights the large outlier, unlike the maximum likelihood method.
* * * Table 3 about here * * *
Here we compute the minimum density power divergence estimates for these data under
the Poisson(.\) model. The results are presented in Table 3. As expected the more robust
members of the family downweight the large outlier successfully. However, what is more
interesting is that this downweighting can be observed even for very small values of a. The
procedure apparently loses robustness for some a between 0.01 and 0.001. For comparison,
the maximum likelihood estimate of A after deleting this outlier is 0.394, and the minimum
Hellinger distance estimate of A for these data (with and without the outlier) is 0.364.
The last example involves hypothesis testing in the normal model on a set of telephone
line fault data presented in Welch , which was also previously analysed by Simpson
 . The data in Table 4 represent the difference of the inverse fault rates between the
test and the control in 14 matched pairs. Here we do a parametric test under the N(p,, u2)
model of the hypothesis H0 : p, = 0 versus H 1: p, > 0, where u is unspecified, using the above
data. We perform one-sided Wald type and score type tests of the null hypothesis.
* * * Tables 4 and 5 about here * * *
For a random sample X 1, X2 , ... , Xn from the N(fJ, CJ2) distribution, letting B = (Jla, &a)
and eN = (0, fa) be the unrestricted and the null estimates of() = ·(f-1,, 0"), the Wald and
score type statistics Wa and Ra have the form
a- (1 + ~)3/4&
(1 + 2a)3/4 [ n
L xi exp( -~) .
Under the null hypothesis these statistics have asymptotic N(O, 1) distributions.
The results of our analysis of the telephone fault data using the Wald type test are
presented in Table 5, where the statistics and their one-sided p-values are presented for
several values of a, the p-values being calculated under a normal distribution. Because of
the presence of the large outlier the likelihood based methods fail to detect the improvement
of the test method over the control; the more robust methods provide a better picture of
the comparison of the two sets of data. Similar results (not shown) arose when using the
score type test.
In the above examples, we successfully used a simple bisection method for the one
parameter case and Newton-Raphson in the two parameter cases, with fast results. Computational questions for larger and more difficult problems are left for future research.
4. DENSITY POWER DIVERGENCE ESTIMATION IN REGRESSION MODELS
It is important to extend the estimation methods to regression type situations, where
response data y are to be explained through covariate information x. Here we propose such
an extension. We also indicate briefly how statistical inference using the resulting robust
regression estimators can be carried out.
4 ·1. Estimation method
Assume that a parametric regression model f 13 (y I x) is proposed for data ( x1 , y1),
... , (xn, Yn), where the model family is smooth in its, say, p-dimensional parameter (3.
The standard assumption in such situations is that the Y;s are conditionally independent
given x1, ... , Xn- The estimation methods we propose below are intended to work in all
such cases, and inference can be carried out conditionally on the observed covariate values.
We think of the xis as coming from a suitable covariate distribution Q in the covariate measurement space X. Thus averages n-1 'Ef=1 h(xi) will under very mild ergodic conditions
tend in probability to limits J h(x) dQ(x) = EQh(x), provided these are finite.
Let there be a true density g(y I x) for Y given X = x. Consider the x-conditional
version of the divergence (2.4),
da(g(·l x), f~(·l x)) = J {JJ+a(y I x)- ( 1 + ±) g(y I x)f$(Y I x) + ±gl+a(y I x)} dy,
from true density g(·l x) to parametrically modelled f~(·l x). Our proposal is to use 13, the
parameter value that minimises
Hn(f3) = n-1 ~ J
JJ+a(y I Xi) dy-
1 +a n-1 ~ f$(li I Xi)·
Observe that this tends almost surely to EQJ{JJ+a(ylx)- (1 + a-1)f$(Yix)g(ylx)}
dy. But this means that Hn(f3) plus the term a-1EQ J gl+a(y I x) dy, which is parameterindependent, tends to the natural overall divergence measure
Da[truth, model]= J
da[g(·l x), f~(·l x)] Q(dx).
4 · 2. Large-sample behaviour
Results from §2 can be generalised to the present setting, under mild regularity conditions. The first result is that 13 tends in probability to the least false parameter j30 that
minimises (4.3). Note that what is the 'best parametric approximation' j~0 (y I x) actually
depends not only on the real g(y I x) but also on the distribution of covariates. The Q
distribution is irrelevant only if the model is correct.
Next consider the limit distribution of j3.
This involves the model score function
u~(y I x) which is now 8log f~(y I x)/8!3 and the model information function i~(y I x) =
-82 log f~(y I x)/8f38j3r. The vector of first derivatives of Hn(f3), modulo a multiplicative
constant that we remove, is
Un(f3) = n-1 :t u~(Yi I Xi)f$(Yi I xi) - n-1 :t J
u~(y I Xi)JJ+a(y I Xi) dy.
And the second order derivatives are
In(f3) = n-1 L { au~(Yi I xi)u~(Yi I xif- i~(Yi I Xi)} f$(Yi I xi)
- n-1 :t J {
(1 + a)u~(y I xi)u~(y I xif- i~(y I xi)} JJ+a(y I xi) dy.
Of course, Un(13) = 0. Also, note that Un(f3n) has mean zero, where f3n minimises (4.3)
when Q is the empirical distribution of the covariates. Consider the variance matrix of
yn Un(f3n), conditionally on the x/s. This is Kn(f3n), where
Kn(f3) = n-1 :f: I u13(y I xi)uf3(Y I xi)T g(y I xi)f$a(y I xi) dy- n-1 :f: ~i~T
and ~i = I u13(y I xi)g(y I xi)f';J(Y I xi) dy. To save space, we shall not be explicit about
regularity conditions in this section, but we shall assume sufficient conditions to be in force
to ensure (i) that -In(iJn) tends in probability to a positive definite J, for each sequence iJn
such that iJn- f3n goes to zero in probability; (ii) that the matrix Kn(f3n) tends in probability
to a positive definite K; and (iii) that yn Un(f3n) tends in distribution to N(O, K). It then
follows that
These regularity requirements are not strict. The first and second essentially involve the
law of large numbers, with some extra continuity and/ or uniformity required for the first,
while the third holds under Lindeberg type circumstances.
The matrices in ( 4.4) can be expressed in terms of expectations with respect to the
covariate. In fact
J = EQ I Uf3o(Y I x)uf3o(Y I xf JJ:a(y I x) dy
+a EQ I Uf3o(Y I x)uf3o(Y I xf {JJ:a(y I x) - g(y I x)f$0 (y I x)} dy
- EQ I if30 (Y I x){JJ:a(y I x)- g(y I x)f$0 (Y I x)} dy
K = EQ I Uf3o(Y I x)uf30 (Y I x)T g(y I x)f$~(y I x) dy- EQ~~T.
where~= I Uf30 (y I x)g(y I x)f';J0 (Y I x) dy.
For small a the ( 4.3) divergence is close to a Q-weighted version of x-conditional
Kullback-Leibler divergence, which corresponds to ordinary maximum likelihood analysis. With a = 1 the method would correspond to a form of L2 regression analysis. Also
note that when all covariates are equal the estimation methods and performance results of
Section 2 are essentially retrieved.
Estimators are necessary for J and K in order to carry out inference, and such are
readily constructed. Considering K first, estimate integrals with respect to g(y I x) using
averaging over Yj 's. For example,
where~= n-1 'Lj=1 ufj(Yj I xi)f;(Yj I Xi)· For J there are a couple of natural possibilities.
One proposal stems from looking at J as the limit in probability of -Eln(80 ). Note that
these estimators are nonparametric and model-robust; their construction does not require
the parametric model to hold. The important point is that J- 1 K J- 1 will converge in
probability to the real limiting variance matrix for yfii{J, even outside parametric model
conditions. Again, there are also other ways of estimating the variance via jackknifing and
bootstrapping.
The simplifications under model conditions are that
K = EQ I u~o(Y I x)u~0 (Y I xf JJ: 2a(y I x) dy- EQ~~T,
where~= J U~0 (y I x)JJ:a(y I x) dy.
4·3. Example: robust linear regression
Take the standard linear regression model Yi = xf f3 + aei, where f3 and the xi's are
p-dimensional and the e/s are independent standard normals. The minimum da method is
to solve the p + 1 equations
n-1 L q/}!(ei)eiXi = 0,
n-1 t ¢;a(ei)(e~- 1) =I (v2 - 1)¢1+a(v) dv
where ei = (yi-x!fJ)/& and¢;= ¢0,1 . A suitably engineered iterative computational scheme
will find the solutions ({J, &).
Using the large-sample results above we may derive the approximate distribution of
the estimators. For illustrational purposes we are content here to give the results under
the model conditions of linearity and normality. Calculations, not given, show that the
variance matrix in the approximating normal distribution for 7J is
This is the natural analogue of the result for the normal location parameter in (3.3). The
efficiency relative to the maximum likelihood estimator is the same as discussed there (see,
in particular, Table 1). Likewise, for a, we find the same efficiency figures as in the normal
scale model as in (c) of §3.1; again, see Table 1.
The density power divergence methodology also extends directly to, for example, robust
Poisson regression. The model choice methodology of §2.6 can also with some effort be
generalised to the present framework with covariates, resulting in a version of (2.12).
5. CONCLUDING REMARKS
This paper has introduced a general family of divergences, indexed by a parameter
a, which generates a corresponding family of estimators. This family includes maximum
likelihood estimation as the limiting case of a = 0. It is shown that increasing a leads to
estimators which are far more robust than the maximum likelihood estimators, and have
little loss in efficiency. Several examples suggest that an a of between 0.1 and 0.25 will
work well. The method can be applied to any parametric family, and also to models with
covariates, as the extension to regression situations shows. One of the main advantages
of this family of divergences over other proposed families such as the Hellinger distance
is that no smoothing of the empirical density function is needed in the case of continuous
densities.
There can be no universal way of selecting an appropriate a parameter when applying
our estimation methods. It specifies the underlying distance measure and typically dictates
to what extent the resulting methods become more statistically robust than the maximum
likelihood methods, and should be thought of as an algorithmic parameter. One way of
selecting it is to fix the efficiency loss, at the ideal parametric model employed, at some low
level, like five or ten percent. A related idea is to fix the maximum level of the influence
curve at some acceptable level. Other ways could in some practical applications involve
prior notions of the extent of contamination of the model.
ACKNOWLEDGEMENTS
The authors would like to thank Professor Probal Chaudhuri for helpful comments.