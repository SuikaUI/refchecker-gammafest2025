Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 357–361,
Berlin, Germany, August 7-12, 2016. c⃝2016 Association for Computational Linguistics
Character-based Neural Machine Translation
Marta R. Costa-juss`a and Jos´e A. R. Fonollosa
TALP Research Center
Universitat Polit`ecnica de Catalunya, Barcelona
{marta.ruiz,jose.fonollosa}@upc.edu
Neural Machine Translation (MT) has
reached state-of-the-art results. However,
one of the main challenges that neural MT
still faces is dealing with very large vocabularies and morphologically rich languages.
In this paper, we propose a neural MT
system using character-based embeddings
in combination with convolutional and
highway layers to replace the standard
lookup-based word representations. The
resulting unlimited-vocabulary and afﬁxaware source word embeddings are tested
in a state-of-the-art neural MT based on
an attention-based bidirectional recurrent
neural network. The proposed MT scheme
provides improved results even when the
source language is not morphologically
rich. Improvements up to 3 BLEU points
are obtained in the German-English WMT
Introduction
Machine Translation (MT) is the set of algorithms
that aim at transforming a source language into
a target language. For the last 20 years, one of
the most popular approaches has been statistical
phrase-based MT, which uses a combination of
features to maximise the probability of the target sentence given the source sentence . Just recently, the neural MT approach
has appeared and obtained state-of-the-art results.
Among its different strengths neural MT does
not need to pre-design feature functions beforehand; optimizes the entire system at once because
it provides a fully trainable model; uses word embeddings so that words (or
minimal units) are not independent anymore; and
is easily extendable to multimodal sources of information . As for weaknesses,
neural MT has a strong limitation in vocabulary
due to its architecture and it is difﬁcult and computationally expensive to tune all parameters in the
deep learning structure.
In this paper, we use the neural MT baseline
system from , which follows an encoder-decoder architecture with attention, and introduce elements from the characterbased neural language model .
The translation unit continues to be the word, and
we continue using word embeddings related to
each word as an input vector to the bidirectional
recurrent neural network (attention-based mechanism). The difference is that now the embeddings
of each word are no longer an independent vector, but are computed from the characters of the
corresponding word. The system architecture has
changed in that we are using a convolutional neural network (CNN) and a highway network over
characters before the attention-based mechanism
of the encoder.
This is a signiﬁcant difference
from previous work which
uses the neural MT architecture from without modiﬁcation to deal with subword units (but not including unigram characters).
Subword-based representations have already
been explored in Natural Language Processing (NLP), e.g.
for POS tagging , name entity recognition , parsing , normalization or learning
word representations . These previous works show
different advantages of using character-level information. In our case, with the new character-
based neural MT architecture, we take advantage
of intra-word information, which is proven to be
extremely useful in other NLP applications , especially when dealing with morphologically rich
languages. When using the character-based source
word embeddings in MT, there ceases to be unknown words in the source input, while the size
of the target vocabulary remains unchanged. Although the target vocabulary continues with the
same limitation as in the standard neural MT system, the fact that there are no unknown words
in the source helps to reduce the number of unknowns in the target. Moreover, the remaining unknown target words can now be more successfully
replaced with the corresponding source-aligned
words. As a consequence, we obtain a signiﬁcant
improvement in terms of translation quality (up to
3 BLEU points).
The rest of the paper is organized as follows.
Section 2 brieﬂy explains the architecture of the
neural MT that we are using as a baseline system. Section 3 describes the changes introduced in
the baseline architecture in order to use characterbased embeddings instead of the standard lookupbased word representations. Section 4 reports the
experimental framework and the results obtained
in the German-English WMT task. Finally, section 5 concludes with the contributions of the paper and further work.
Neural Machine Translation
Neural MT uses a neural network approach to
compute the conditional probability of the target sentence given the source sentence . The approach
used in this work follows the encoder-decoder architecture.First, the
encoder reads the source sentence s = (s1, ..sI)
and encodes it into a sequence of hidden states
h = (h1, ..hI).
Then, the decoder generates a
corresponding translation t = t1, ..., tJ based on
the encoded sequence of hidden states h. Both encoder and decoder are jointly trained to maximize
the conditional log-probability of the correct translation.
This baseline autoencoder architecture is improved with a attention-based mechanism , in which the encoder uses
a bi-directional gated recurrent unit (GRU). This
GRU allows for a better performance with long
sentences. The decoder also becomes a GRU and
each word tj is predicted based on a recurrent hidden state, the previously predicted word tj−1, and
a context vector. This context vector is obtained
from the weighted sum of the annotations hk,
which in turn, is computed through an alignment
model αjk (a feedforward neural network). This
neural MT approach has achieved competitive results against the standard phrase-based system in
the WMT 2015 evaluation .
Character-based Machine Translation
Word embeddings have been shown to boost the
performance in many NLP tasks, including machine translation. However, the standard lookupbased embeddings are limited to a ﬁnite-size vocabulary for both computational and sparsity reasons. Moreover, the orthographic representation
of the words is completely ignored. The standard
learning process is blind to the presence of stems,
preﬁxes, sufﬁxes and any other kind of afﬁxes in
As a solution to those drawbacks, new alternative character-based word embeddings have been
recently proposed for tasks such as language modeling , parsing or POS tagging . Even
in MT , where authors use the
character transformation presented in both in the source
and target. However, they do not seem to get clear
improvements. Recently, propose a combination of word and characters in neural MT.
For our experiments in neural MT, we selected
the best character-based embedding architecture
proposed by Kim et al. for language modeling. As the Figure 1 shows, the computation of the representation of each word starts
with a character-based embedding layer that associates each word (sequence of characters) with
a sequence of vectors. This sequence of vectors
is then processed with a set of 1D convolution
ﬁlters of different lengths (from 1 to 7 characters) followed with a max pooling layer. For each
convolutional ﬁlter, we keep only the output with
the maximum value. The concatenation of these
max values already provides us with a representation of each word as a vector with a ﬁxed length
equal to the total number of convolutional ker-
nels. However, the addition of two highway layers
was shown to improve the quality of the language
model in so we also kept these
additional layers in our case. The output of the
second Highway layer will give us the ﬁnal vector representation of each source word, replacing
the standard source word embedding in the neural
machine translation system.
*)+,)$-)./0.
-1&"&-2)".
)3')!!#$45
-/$8/(,2#/$.
0#(2)"5./0.!#00)")$2.
6&9./,27,2./0.
)&-1.0#(2)"
:#41;&<.=&<)"
:#41;&<.=&<)"
:#41;&<.=&<)"5
Figure 1: Character-based word embedding
In the target size we are still limited in vocabulary by the softmax layer at the output of the network and we kept the standard target word embeddings in our experiments. However, the results
seem to show that the afﬁx-aware representation of
the source words has a positive inﬂuence on all the
components of the network. The global optimization of the integrated model forces the translation
model and the internal vector representation of the
target words to follow the afﬁx-aware codiﬁcation
of the source words.
Experimental framework
This section reports the data used, its preprocessing, baseline details and results with the enhanced
character-based neural MT system.
We used the German-English WMT data1 including the EPPS, NEWS and Commoncrawl. Preprocessing consisted of tokenizing, truecasing,
normalizing punctuation and ﬁltering sentences
with more than 5% of their words in a language
1 
other than German or English. Statistics are shown
in Table 1.
Table 1: Corpus details. Number of sentences (S),
words (W), vocabulary (V) and out-of-vocabularywords (OOV) per set and language (L). M standing
for millions, k standing for thousands.
Baseline systems
The phrase-based system was built using Moses
 , with standard parameters
such as grow-ﬁnal-diag for alignment, Good-
Turing smoothing of the relative frequencies, 5gram language modeling using Kneser-Ney discounting, and lexicalized reordering, among others. The neural-based system was built using the
software from DL4MT2 available in github. We
generally used settings from previous work : networks have an embedding of 620
and a dimension of 1024, a batch size of 32, and
no dropout. We used a vocabulary size of 90 thousand words in German-English. Also, as proposed
in we replaced unknown words
(UNKs) with the corresponding source word using
the alignment information.
Table 3 shows the BLEU results for the baseline
systems (including phrase and neural-based, NN)
and the character-based neural MT (CHAR). We
also include the results for the CHAR and NN
systems with post-processing of unknown words,
which consists in replacing the UNKs with the corresponding source word (+Src), as suggested in
 . BLEU results improve by almost 1.5 points in German-to-English and by more
than 3 points in English-to-German. The reduction
in the number of unknown words (after postprocessing) goes from 1491 (NN) to 1260 (CHAR)
in the direction from German-to-English and from
3148 to 2640 in the opposite direction. Note the
2 
Berichten zufolge hofft Indien darber hinaus auf einen Vertrag zur Verteidigungszusammenarbeit zwischen den beiden Nationen .
reportedly hopes India , in addition to a contract for the defence cooperation between the two nations .
according to reports , India also hopes to establish a contract for the UNK between the two nations .
according to reports , India hopes to see a Treaty of Defence Cooperation between the two nations .
India is also reportedly hoping for a deal on defence collaboration between the two nations .
der durchtrainierte Mainzer sagt von sich , dass er ein “ ambitionierter Rennradler “ ist .
the will of Mainz says that he a more ambitious .
the UNK Mainz says that he is a “ ambitious , . “
the UNK in Mainz says that he is a ’ ambitious racer ’ .
the well-conditioned man from Mainz said he was an “ ambitious racing cyclist . “
die GDL habe jedoch nicht gesagt , wo sie streiken wolle , so dass es schwer sei , die Folgen konkret vorherzusehen .
the GDL have , however , not to say , where they strike , so that it is difﬁcult to predict the consequences of concrete .
however , the UNK did not tell which they wanted to UNK , so it is difﬁcult to predict the consequences .
however , the UNK did not say where they wanted to strike , so it is difﬁcult to predict the consequences .
the GDL have not said , however , where they will strike , making it difﬁcult to predict exactly what the consequences will be .
die Premierminister Indiens und Japans trafen sich in Tokio .
the Prime Minister of India and Japan in Tokyo .
the Prime Minister of India and Japan met in Tokyo
the Prime Ministers of India and Japan met in Tokyo
India and Japan prime ministers meet in Tokyo
wo die Beamten es aus den Augen verloren .
where the ofﬁcials lost sight of
where the ofﬁcials lost it out of the eyes
where ofﬁcials lose sight of it
causing the ofﬁcers to lose sight of it
Table 2: Translation examples.
Table 3: De-En BLEU results.
number of out-of-vocabulary words of the test set
is shown in Table 1.
The character-based embedding has an impact
in learning a better translation model at various
levels, which seems to include better alignment,
reordering, morphological generation and disambiguation. Table 2 shows some examples of the
kind of improvements that the character-based
neural MT system is capable of achieving compared to baseline systems. Examples 1 and 2 show
how the reduction of source unknowns improves
the adequacy of the translation. Examples 3 and 4
show how the character-based approach is able to
handle morphological variations. Finally, example
5 shows an appropriate semantic disambiguation.
Conclusions
Neural MT offers a new perspective in the way
MT is managed. Its main advantages when compared with previous approaches, e.g.
statistical
phrase-based, are that the translation is faced with
trainable features and optimized in an end-to-end
scheme. However, there still remain many challenges left to solve, such as dealing with the limitation in vocabulary size.
In this paper we have proposed a modiﬁcation to
the standard encoder/decoder neural MT architecture to use unlimited-vocabulary character-based
source word embeddings.
The improvement in
BLEU is about 1.5 points in German-to-English
and more than 3 points in English-to-German.
As further work, we are currently studying different alternatives to extend
the character-based approach to the target side of
the neural MT system.
Acknowledgements
This work is supported by the 7th Framework Program of the European Commission through the International Outgoing Fellowship Marie Curie Action and also by the Spanish Ministerio de Econom´ıa y Competitividad and
European Regional Developmend Fund, contract
TEC2015-69266-P (MINECO/FEDER, UE).