AI Safety and Reproducibility: Establishing
Robust Foundations for the Neuropsychology of
Human Values
Gopal P. Sarma
1∗, Nick J. Hay
2†‡, and Adam Safron
1. School of Medicine, Emory University, Atlanta, GA USA
2. Vicarious AI, San Francisco, CA USA
3. Department of Psychology, Northwestern University, Evanston IL USA
We propose the creation of a systematic eﬀort to identify and replicate key ﬁndings in neuropsychology and
allied ﬁelds related to understanding human values. Our aim is to ensure that research underpinning the
value alignment problem of artiﬁcial intelligence has been suﬃciently validated to play a role in the design
of AI systems.
Anthropomorphic Design of
Superintelligent AI Systems
There has been considerable discussion in recent years about the consequences of achieving
human-level artiﬁcial intelligence. In a survey of
top researchers in computer science, an aggregate
forecast of 352 scientists assigned a 50% probability of human-level machine intelligence being
realized within 45 years. In the same survey, 48%
responded that greater emphasis should be placed
on minimizing the societal risks of AI, an emerging
area of study known as “AI safety” .
A distinct area of research within AI safety concerns software systems whose capacities substantially exceed that of human beings along every
dimension, that is, superintelligence . Within
the framework of superintelligence theory, a core
research topic known as the value alignment problem is to specify a goal structure for autonomous
agents compatible with human values. The logic
behind the framing of this problem is the following:
∗Email: 
†Email: 
‡The views expressed herein are those of the author and do
not necessarily reﬂect the views of Vicarious AI.
§Email: 
Current software and AI systems are brittle and
primitive, showing little capacity for generalized
intelligence. However, ongoing research advances
suggest that future systems may someday show
ﬂuid intelligence, creativity, and true thinking capacity. Deﬁning the parameters of goal-directed behavior will be a necessary component of designing
such systems. Because of the complex and intricate
nature of human behavior and values, an emerging train of thought in the AI safety community is
that such a goal structure will have to be inferred
by software systems themselves, rather than preprogrammed by their human designers. Russell
summarizes the notion of indirect inference of human values by stating three principles that should
guide the development of AI systems :
1. The machine’s purpose must be to maximize
the realization of human values. In particular,
it has no purpose of its own and no innate
desire to protect itself.
2. The machine must be initially uncertain about
what those human values are. The machine
may learn more about human values as it goes
along, but it may never achieve complete certainty.
3. The machine must be able to learn about hu-
 
Establishing Robust Foundations for the Neuropsychology of Human Values
man values by observing the choices that we
humans make.
In other words, rather than have a detailed
ethical taxonomy programmed into them, AI
systems should infer human values by observing
and emulating our behavior .
alignment perspective on building safe, superintelligent agents is a natural extension of a broader
set of questions related to the moral status of
artiﬁcial intelligence and issues related to the architectural transparency and intelligibility of such
software-based agents. Many of these questions
are important for systems whose capabilities fall
well short of superintelligence, but which can
nonetheless have signiﬁcant impact on the world.
For instance, medical diagnostic systems which
arrive at highly unusual and diﬃcult to interpret
diagnostic plans may ultimately do great harm if
patients do not respond the way the AI system had
predicted. In the medical setting, intelligible AI
systems can ensure that healthcare workers are not
subsequently forced to reason about circumstances
that would not have ordinarily arisen via human
diagnostics. Many researchers believe that similar
situations will arise in industries ranging from
transportation, to insurance, to cybersecurity .
A signiﬁcant tension that has arisen in the AI
safety community is between those researchers
and those more oriented towards longer-term,
superintelligence-related concerns . Are these
two sets of issues fundamentally in opposition
to one another? Does researching safety issues
arising from superintelligence necessarily entail
disregarding more contemporary concerns? Our
ﬁrm belief is that the answer to this question is
“no." We are of the viewpoint that there is an organic continuum extending between contemporary
and long-term AI safety issues and that individuals
and research groups can freely pursue both sets of
issues without tension. One of the purposes of this
article is to argue that not only can research related
to superintelligence be grounded in contemporary
concerns, but moreover, that there is a wealth of
existing work across a wide variety of ﬁelds that
is of direct relevance to superintelligence. This
perspective should be reassuring to researchers
who are either skeptical of or have yet to form an
opinion on the intellectual validity of long-term
issues in AI safety. As we see it, there is no shortage
of concrete research problems that can be pursued
within a familiar academic setting.
To give a speciﬁc instance of this viewpoint, in a
recent article, we argued that ideas from aﬀective
neuroscience and related ﬁelds may play a key role
in developing AI systems that can acquire human
values. The broader context of this proposal is an inverse reinforcement learning (IRL) type paradigm
in which an AI system infers the underlying utility function of an agent by observing its behavior.
Our perspective is that a neuropsychological understanding of human values may play a role in characterizing the initially uncertain structure that the AI
system reﬁnes over time. Having a more accurate
initial goal structure may allow an agent to learn
from fewer examples. For a system that is actively
taking actions and having an impact on the world,
a more eﬃcient learning process can directly translate into a lower risk of adverse outcomes. Moreover, systems built with human-inspired architectures may help to address issues of transparency
and intelligibility that we cited earlier , but
in the novel context of superintelligence. As an
example, we suggested that human values could
be schematically and informally decomposed into
three components: 1) mammalian values, 2) human
cognition, and 3) several millennia of human social and
cultural evolution . This decomposition is simply one possible framing of the problem. There are
major controversies within these ﬁelds and many
avenues to approach the question of how neuroscience and cognitive psychology can inform the
design of future AI systems . We refer to this
broader perspective, i.e. building AI systems which
possess structural commonalities with the human
mind, as anthropomorphic design.
Formal Models of Human Values
and the Reproducibility Crisis
The connection between value alignment and
research in the biological and social sciences
intertwines this work with another major topic
in contemporary scientiﬁc discussion, the repro-
Establishing Robust Foundations for the Neuropsychology of Human Values
ducibility crisis.
Systematic studies conducted
recently have uncovered astonishingly low rates
of reproducibility in several areas of scientiﬁc
inquiry . Although we do not know what
the “reproducibility distribution” looks like for the
entirety of science, the shared incentive structures
of academia suggest that we should view all
research with some amount of skepticism.
How then do we prioritize research to be the focus of targeted replication eﬀorts? Surely all results
do not merit the same level of scrutiny. Moreover,
all areas likely have “linchpin results,” which if
veriﬁed, will increase researchers’ conﬁdence substantially in entire bodies of knowledge. Therefore,
a challenge for modern science is to eﬃciently
identify areas of research and corresponding
linchpin results that merit targeted replication
eﬀorts . A natural strategy to pursue is to focus
such eﬀorts around major scientiﬁc themes or
research agendas. The Reproducibility Projects
of the Center for Open Science, for example, are
targeted initiatives aimed replicating key results in
psychology and cancer biology .
In a similar spirit, we propose a focused eﬀort
aimed at investigating and replicating results which
underpin the neuropsychology of human values.
Artiﬁcial intelligence has already been woven into
the fabric of modern society, a trend that will only
increase in scope and pace in the coming decades.
If, as we strongly believe, a neuropsychological understanding of human values plays a role in the
design of future AI systems, it essential that this
knowledge base is thoroughly validated.
Discussion and Future Directions
We have deliberately left this commentary brief
and open-ended. The topic is broad enough that it
merits substantial discussion before proceeding. In
addition to the obvious questions of which subjects
and studies should fall under the umbrella of the
reproducibility initiative that we are proposing,
it is also worth asking how such an eﬀort will be
coordinated. Furthermore, this initiative should
also be an opportunity to take advantage of novel
scientiﬁc practices aimed at improving research
quality, such as pre-prints, post-publication peer
review, and pre-registration of study design.
The speciﬁc task of replication is likely only
applicable to a subset of results that are relevant
to anthropomorphic design. There are legitimate
scientiﬁc disagreements in these ﬁelds and many
theories and frameworks that have yet to achieve
consensus. Therefore, in addition to identifying
those studies that are suﬃciently concrete and
precise to be the focus of targeted replication
eﬀorts, it is also our aim to identify “linchpin”
controversies that are of high-value to resolve, for
example, via special issues in journals, workshops,
or more rapid, iterated discussion among experts.
We make a few remarks about possible starting
One source of candidate high-value
linchpin ﬁndings would be those used by frameworks for understanding the nature of emotions.
The extent of innate contributions to emotions
is hotly debated, with positions ranging from
emotions having their origins in conserved
evolutionary programs to more recent
suggestions that emotions are for the most part
constructed through social inference .
For example, Barrett suggests that the existing
aﬀective neuroscience and ethological literature
may be based on questionable interpretations of
studies of limited generalizability and uncertain
reliability of research methods . A related
discipline is contemplative neuroscience, a ﬁeld
aimed at correlating introspective insights with
a neuroscientiﬁc understanding of the brain.
Highly skilled meditators from the Tibetan
Buddhist tradition and others have claimed to have
signiﬁcant insight into human emotions , an
understanding which is likely relevant to developing a rigorous characterization of human values.
Other frameworks worth considering in depth
are models of social-emotional learning based
on predictive coding and Bayesian inference .
In these models,
uniquely human cognition
and aﬀect arises from factors such as extensive
early dependency for homeostatic regulation (e.g.
ﬁne-CT ﬁbers contributing to analgesia through
vagal stimulation ). It has been proposed
that this dependence leads to models of self that are
strongly shaped by the need to predict the minds
Establishing Robust Foundations for the Neuropsychology of Human Values
of others with whom the developing individual
interacts. These reciprocal relationships may be
the basis for the kind of joint attention and joint
intentionality emphasized by Tomasello and others
as a basis for uniquely human social cognition .
In terms of strategies for organizing this
literature, we favor an open science or wiki-style
approach in which individuals suggest high-value
studies and topics to be the focus of targeted replication eﬀorts. Knowledgeable researchers can then
debate these proposals in either a structured (such
as the RAND Corporation’s Delphi protocol )
or unstructured format until consensus is achieved
on how best to proceed. As we have discussed in
the previous section, The Center for Open Science
has demonstrated that reproducibility eﬀorts
targeting large bodies of literature are achievable
with modest resources .
Our overarching message:
From philosophers
pursuing fundamental theories of ethics, to artists
immersed in crafting compelling emotional narratives,
to ordinary individuals struggling with personal
challenges, deep engagement with the nature of human
values is a fundamental part of the human experience.
As AI systems become more powerful and widespread,
such an understanding may also prove to be important
for ensuring the safety of these systems. We propose that
enhancing the reliability of our knowledge of human
values should be a priority for researchers and funding
agencies concerned about AI safety and existential risks.
We hope this brief note brings to light an important
set of contemporary scientiﬁc issues and we are
eager to collaborate with other researchers in order
to take informed next steps.
Acknowledgements
We would like to thank Owain Evans and several
anonymous reviewers for insightful discussions on
the topics of value alignment and reproducibility
in psychology and neuroscience.
Gopal P. Sarma
0000-0002-9413-6202
Nick J. Hay
0000-0002-8037-5843
Adam Safron
0000-0002-3102-7623