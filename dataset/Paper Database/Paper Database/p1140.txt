Institute of Mathematical Statistics
LECTURE NOTES — MONOGRAPH SERIES
ESTIMATING FUNCTIONS FOR DISCRETELY OBSERVED
DIFFUSIONS: A REVIEW
Michael S0rensen
University of Aarhus
Several estimating functions for discretely observed diffusion processes are reviewed. First we discuss simple explicit estimating functions based on Gaussian approximations to the transition density. The
corresponding estimators often have considerable bias, a problem that
can be avoided by using martingale estimating functions. These, on
the other hand, are rarely explicit and therefore often require a considerable computational effort. We review results on how to choose an
optimal martingale estimating function and on asymptotic properties
of the estimators. Martingale estimating functions based on polynomials of the increments of the observed process or on eigenfunctions for
the generator of the diffusion model are considered in more detail. The
theory is illustrated by examples. In particular, the Cox-Ingersoll-Ross
model is considered.
Key Words: Approximate likelihood function; asymptotic normality;
bias; consistency; Cox-Ingersoll-Ross model; eigenfunctions; inference
for diffusion processes; martingale estimating functions; optimal inference; polynomial estimating functions; quasi likelihood.
Introduction
Diffusion processes often provide a useful alternative to the discrete time
stochastic processes traditionally used in time series analysis as models for
observations at discrete time points of a phenomenon that develops dynamically in time. In many fields of application it is natural to model the dynamics in continuous time, whereas dynamic modelling in discrete time contains
an element of arbitrariness. This is particularly so when the time between
observations is not equidistant.
Statistical inference for diffusion processes based on discrete time observations can only rarely be based on the likelihood function as this is usually
not explicitly available. The likelihood function is a product of transition
densities, as follows easily from the fact that diffusions are Markov processes,
but explicit expressions for the transition densities are only known in some
special cases. One way around this problem is to find good approximations to
the likelihood function by means of simulation methods for diffusions. This
computer-intensive approach has been pursued in Pedersen .
Another solution is to base the inference on estimating functions. In this
paper we review a number of recent contributions to this approach.
The likelihood theory for continuously observed diffusions is well studied. In practice, however, diffusions are not observed continuously, but only
at discrete time points or for instance through an electronic filter. There is
therefore a need of methods which are applicable in statistical practice, and
in recent years this has inspired quite a lot of work on estimation for discretely observed diffusions. The need has been particularly acute in finance
where diffusion models must be fitted to time series of stock prices, interest
rates or currency exchange rates in order to price derivative assets such as
In Section 2 we discuss simple explicit estimating functions based on
Gaussian approximations to the transition density. The corresponding estimators often have considerable bias, a problem which we discuss in some
detail. When the distance between the observation times is sufficiently small,
they are, however, useful in practice. Asymptotic results substantiating this
claim are reviewed. The bias problems, to a large extend, can be avoided by
using martingale estimating functions instead, which are treated in Section
3. Martingale estimating functions are, on the other hand, rarely explicit,
and therefore often requires a considerable computational effort. We review
results on how to choose an optimal martingale estimating function and on
asymptotic properties of the estimators. Martingale estimating functions
based on polynomials of the increments of the observed process or on eigenfunctions for the generator of the diffusion model are considered in more
detail. A different kind of estimating functions, by which the bias problems
discussed in Section 2 can also be avoided, and which have the advantage
of being explicit, were recently proposed by Kessler . Unfortunately,
these can not be discussed in this relatively short review paper.
Simple explicit estimating functions
We consider one-dimensional diffusion processes defined as solutions of the
following class of stochastic differential equations
DIFFUSION PROCESSES
where W is a standard Wiener process. We assume that the drift b and the
diffusion coefficient σ are known apart from the parameter θ which varies
in a subset Θ of ΊR
d. They are assumed to be smooth enough to ensure
the existence of a unique weak solution for all θ in θ. The assumption
that the drift and the diffusion coefficient do not depend on time is not
essential for several of the estimating functions discussed in this paper which
can be modified in a straightforward way to diffusions that are not timehomogeneous. Also the assumption that X is one-dimensional is in several
cases not needed, but is made to simplify the exposition. The statistical
problem considered in this paper is to draw inference about the parameter
θ on the basis of observations of the diffusion X at discrete time points:
to = 0 < ίi < • < tn. The likelihood function for θ
based on Xto, Xtί,
• , Xtn is
Ln(θ) = f[p(Ai,Xti_1,Xti;θ),
where Δ; = tι — tj_i and where y \-ϊ p(Δ, re, y; θ) is the density of X& given
XQ = x when θ is the true parameter value.
The transition density p is only rarely explicitly known, and when Δ is
not small, it can be far from Gaussian. We can, however, obtain a number
of useful estimating functions by replacing p by approximations. When Δ
is small, we can approximate p by a normal density function. Expressions
for the conditional moments of XΔ given XQ can usually not be found, so in
order to get an explicit estimating function, the mean value is approximated
by x + b(x;θ)A and the variance by σ
By using this approximate
Gaussian transition density, we obtain an approximate likelihood function,
which equals the likelihood function for the Euler-Maruyama approximation
 to the solution of (2.1). The corresponding
score function is
where v(x; θ) = σ
2{x; 0), and where deb denotes the vector of partial derivatives with respect to θ. Vectors are column vectors. It is, of course, assumed
that the partial derivatives in (2.3) exist. Throughout this paper, whenever a
derivative appears its existence is implicitly assumed in order to avoid statements of obvious conditions. The cί-dimensional estimating function (2.3)
is biased because we have used rather crude approximations for the mean
value and the variance of the transition distribution. Therefore it can only
be expected to yield reasonable estimators when the Δj's are small, and we
can only expect these estimators to be consistent and asymptotically normal
if the asymptotics is not only that the length of the observation interval, £n,
goes to infinity, but also that the Δj's go to zero.
First consider the estimating function obtained by deleting the quadratic
terms from (2.3):
ffi;:ff (2.4)
To simplify the exposition, we have here assumed that the observation times
are equidistant, i.e. that Δ; = Δ for all i. This is the form (2.3) takes in
cases where the diffusion coefficient is completely known, i.e. when it does
not depend on 0, but (2.4) can obviously also be used when the diffusion
coefficient depends on θ. Another way of obtaining this estimating function
is by discretizing the score function based on continuous observation of the
diffusion process X in the time interval [0, tn] . The discretization is done by replacing Ito-integrals and Riemannintegrals by Ito-Riemann sums. The estimator θn obtained from (2.4), which
can also be thought of as a weighted least squares estimator, was studied by
Dorogovicev , Prakasa Rao and Florens-Zmirou 
in the case where the diffusion coefficient is constant and the parameter θ is
one-dimensional. Under various regularity conditions these authors showed
that θn is consistent provided Δ n —>> 0 and nΔ n —> oo, where it is assumed
that the time between observations, Δ n, depends on the sample size n. Note
that nΔ n = tn is the lenght of the observation interval. To prove asymptotic
normality a stronger condition is needed. Prakasa Rao assumed
that Δ n tends to zero sufficiently fast that nΔ^ —> 0, and referred to this
condition as a rapidly increasing experimental design assumption. Florens-
Zmirou made the slightly weaker assumption that nΔ^ —> 0 in her result
on asymptotic normality. We shall not state the results of these authors in
details as a more general result will be given below.
A different type of asymptotics, which has turned out to be relevant in
several applications, was studied by Genon-Catalot . She considered
the situation where the length of the observation interval nΔ n is fixed and
the diffusion coefficient is a constant σ
2 tending to zero as the number of
observations n tends to infinity. Under reasonable regularity conditions she
showed that the estimator θn based on (2.4) is consistent provided σ ~ n~^
where β > 0.5 and asymptotically normal (and asymptotically efficient)
under the additional condition β < 1.
These various asymptotic results indicate that estimators based on (2.3)
or (2.4) behave reasonably well in practice when the time between observa-
DIFFUSION PROCESSES
tions Δ is sufficiently small. This has been confirmed in simulation studies,
see e.g. Kloeden et al. . However, when Δ is not small, the estimators
can be severely biased, as demonstrated in simulation studies by Pedersen
 and Bibby and S0rensen . In practice it can be difficult to determine whether in concrete models Δ is sufficiently small for the estimators
to work well.
Estimation based on (2.3) or (2.4) has been popular in the econometric
literature under the name the generalized method of moments, a somewhat
odd name, as the method is obviously not a method of moments, except
approximately.
The problem with the simple estimating functions (2.4) and (2.3) is that
they can be strongly biased. An idea about the magnitude of the bias can
be obtained from the expansions 
= x) = x + Δb{x;θ) + ±A
2{b{x;θ)dxb(x;θ)
VΆτθ(XΛ\X0 = x) = Aυ(x;θ) + A
Ίb(x;θ)dxυ(x;θ)
+ υ(x; θ){dxb{x; θ) + \d
xv(x; θ)}] + O(Δ
where EQ and Var# denote expectation and variance, respectively, when θ is
the true parameter value, and where d\ denotes the second partial derivative
with respect to x. Suppose X is an ergodic diffusion with invariant probability measure μ# when θ is the true parameter value. If XQ ~ μe, we find, by
(2.5), the following expression for the bias of the estimating function (2.4)
Eθ(Hn(θ))=
2nEμθ{dθb(θ)[b(θ)dxb(θ)/v(θ)+
For a function (x,θ) «-> g{x;θ) we use the notation Eμθ(g(θ))
/g(x; θ)dμo(x). When the initial distribution is different from μ#, (2.7) is, by
the ergodic theorem , still
a good estimate of the bias provided the number of observations is sufficiently
large. Under weak standard regularity conditions (e.g. conditions similar to
Condition 3.3 below) it follows that the asymptotic bias (n -» oo, Δ fixed)
of the estimator θn derived from (2.4) is
AEμβ{dθb(θ)[b(θ)dxb(θ)/v(θ)+
2Eμβ{(dθb(θ))Vv(θ)}
in the case of a one-dimensional parameter. The expression analogous to
(2.7) for the estimating function (2.3) is
Eθ(Hn(θ))=
±AnEμβ{dθlogv(θ)
[ \b{θ)dxlogv(θ) + dxb(θ)
as is easily seen from (2.6). The fact that the bias of the estimating function
(2.3) is of order nΔ when the diffusion coefficient depends on θ indicates that
the corresponding estimator has a considerable bias even for small values of
Δ. The reason is that in deriving (2.3) we used an approximation of the
variance of the transition distribution that was too crude, see the discussion
Example 2.1 Consider the Cox-Ingersoll-Ross model, which is widely used
in mathematical finance to model interst rates . The model is given by the stochastic differential equation
dXt = (α + ΘXt)dt + σy/XtdWu
Xo = ^o > 0,
where 0 < 0 and σ > 0. The model has also been used in other applications,
e.g. mathematical biology, for a long time. The state space is (0, oo).
It is not difficult to derive an estimator for the parameter vector (α, θ, σ
from (2.3). To simplify things we assume equidistant sampling times.
χo)(n Σ?=i Xu-ι)~ — ΣΓ=i Xti.
For parameter values where X is ergodic, an expression for the bias
of these estimators when n is large can easily be found using the ergodic
theorem and the fact that the invariant probability measure for the Cox-
Ingersoll-Ross model is a gamma distribution. The bias can for some parameter values be dramatic even for rather small values of Δ, see Bibby and
S0rensen .
The bias considerations above raise the question whether better estimators
can be obtained by improving the approximations of the mean and variance
in the Gaussian approximation to the transition distribution. Useful approximations were derived by Kessler under the following condition.
DIFFUSION PROCESSES
Condition 2.2 For every θ the functions b(x θ) and σ(χ-,θ) are K times
continuously differentiable with respect to x and the derivatives are of polynomial growth in x uniformly in θ.
In order to formulate Kessler's expansions we need the generator of the
diffusion process given by (2.1), i.e. the differential operator
Lθ = b(x;θ)—+ lυ(x;θ)4-ή.
With the definition
~~ %4f{χ),
where f(x) = x, and where L\ denotes i-fold application of the differential
operator £#, Kessler proved that
o = x)= rk(A,x;θ)
provided k < K/2 + 1. Note that (2.5) is a particular case of (2.12). The
dependence of the O-term on x and θ has been suppressed here. Kessler
 gave an upper bound for the term O(Δ
Λ + 1) which is uniform in θ.
For fixed x,y and θ the function (y — rfc(Δ,α;;0))
2 is a polynomial of
order 2k in Δ. Define g
χ θ(y), j = 0,1,
(y-rk(A,x;θ))
andΓfc(Δ,z;0) by
Kessler showed that
~ rk{Δ, x θ))
= x)= Γ^(Δ, x; θ) + O ( Δ *
for k < K/2 + 1. Also in this case he gave an upper bound for the term
We can now obtain an approximation to the likelihood function (2.2),
which is considerably better than the approximation we used above, by replacing the transition density y \-> p(Δ,£,y;0) by a normal density with
mean value r*fc(Δ, x; θ) and variance I\+i(Δ, x\ θ) with k < K/2. The corresponding estimating function (approximate score function) is
We have again allowed the time between observations to vary.
Example 2.3 To avoid complicated expressions we consider as an example
the Ornstein-Uhlenbeck process
dXt = ΘXtdt + σdWu ^o = so,
where θ € IR and σ > 0. Long, but easy, calculations show that the estimators for θ and σ
2 based on Hn are
= Δ " V 2 Q n - 1 - 1 )
To simplify matters we have assumed that the observation times are equidistant. There are, in fact, two solution for 0, but a moments reflection reveals
that the other solution is not a good estimator.
Suppose X is ergodic with invariant probability measure μg, all moments
of which are finite. Then we find, using (2.12) and (2.14), that the bias of
the estimating function Hn is of order O modified the approximate Gaussian likehood function we used to derive the estimating function (2.15) by replacing the functions \og{Tk+ι{Δi,x',θ)/(Δiv(x;θ))}
Aiv(x;θ)/Γk+ι{Δi,x]θ)
by Taylor expansions to order k. The estimating
function derived from Kessler's approximate likelihood function of order k
DIFFUSION PROCESSES
differs only from Hn
by terms of order O approximate likelihood function, for which he gave results under (essentially) the following conditions.
Condition 2.4
1) For every θ there exists a constant CQ such that
\b(x;θ)-b(y;θ)\ +
\σ(x;θ)-σ(y;θ)\<Cθ\x-y\
for all x and y in the state space.
2)'mfXiθυ(x,θ)
3) The functions b(x; θ) and σ(x\ θ) and all their partial x-deriυatives up
to order K are three times differentiate with respect to θ for all x in the
state space. All these derivatives with respect to θ are of polynomial growth
in x uniformly in θ.
4) The process X is ergodic for every θ with invariant probability measure
μ$. All polynomial moments of μe are finite.
5) For allp>0
and for all θ supt Eθ(\Xt\
Kessler further assumed that θ = (α, β) belongs to a compact subset Θ
2, that the drift depends only on a and that the diffusion coefficient
depends only on β. Moreover, he imposed an obvious identifiability condition. The assumption that θ belongs to a compact set is only made to
avoid technical problems concerning the existence of a maximum of the approximate likelihood function. Kessler proved the following result
about the asymptotic properties of the estimator θk^n which maximizes his
approximate likelihood function. The observation times are assumed to be
equidistant with spacing Δ n, which depends on the sample size.
Theorem 2.5 Assume that k < K/2 and that Condition 2.2 and Condition
2.4 hold. Then for all θ e θ
in PQ-probability
as n —>> oo, provided Δ n —> 0 and n Δ n —» oo.
//, in addition, nΔ
2 A : + 1 —• 0 and θ G int θ , then as n -» oo
in distribution under Pg, where
_ ( Eμβ[(dab(a))
The estimating functions considered in this section were all derived from
an approximate (or pseudo) likelihood function. This has the advantage
that if there are more than one solution to the estimating equation, we can
choose the one that is the global maximum point for the pseudo likelihood
function. The estimating functions considered in the next section do not
generally have this property.
Martingale estimating functions
The problems caused by the bias of the estimating functions considered in
Section 2 can most conveniently be avoided by using martingale estimating
functions. We shall therefore in this section, for the same kind of data as
those considered in Section 2, study estimating functions of the form
where the function g(Δ,£,y;0) satisfies
p(Δ, x, y; θ)p{A, x, y; θ)dy = 0
for all x, Δ and θ. Here, as in the previous section, y ι-> p(Δ, x, y; θ) denotes
the transition density, i.e. the density of X& given Xo = x. In most cases
it is not easy to find g's that satisfy (3.2) since p is usually not known, but
such #'s can always be found numerically, as we shall see later. Under (3.2)
Gn(θ) is a martingale when θ is the true parameter value. In particular,
Gn(θ) is an unbiased estimating function. If θ is d-dimensional, we usually
take g to be d-dimensional too.
With the bias problem out of the way, the question of how to choose the
estimating function in an optimal way becomes more interesting. Godambe
and Heyde gave criteria for choosing within a class of martingale
estimating functions the one which is closest to the true (but for diffusion
models usually not explicitly known) score function (fixed sample criterion)
or the one which has the smallest asymptotic variance as the number of
observations tends to infinity (asymptotic criterion).
Suppose we have N real valued functions hj(Δ,α;,y;0), j = 1, — , JV,
each of which satisfies (3.2) and which are all natural choices for defining a
martingale estimating function. Then every function of the form
DIFFUSION PROCESSES
g(A,x,y;θ) = ^ • ( Δ ^
where αj(Δ,£;0), j = l,
,iV, are arbitrary functions, can be used to
define a martingale estimating function by (3.1). If θ is d-dimensional, we
will usually try to find d-dimensional α's. Let Q denote the class of ddimensional martingale estimating functions of the form (3.1) with g given
by (3.3). The following result by Kessler tells how to find the optimal
estimating function in the sense of Godambe and Heyde within the
class Q. We need the further assumption that for fixed Δ,a; and θ the
functions /ij(Δ,z,y;0), j = 1, • ,iV are square integrable with respect to
the transition distribution. Then the set of all real-valued functions of the
form (3.3) is a (finite dimensional and hence closed) linear sub-space of
2(p(A,x,y,θ)dy).
We denote this subspace by
Theorem 3.1 Suppose the transition density p is differentiable with respect
to θ and that for all fixed A,x and θ the functions de{ logp, i = 1,
belong to L
2(p(A,x,y\θ)dy).
the projection in L
2(p(A,x,y;θ)dy)
of d$i logp onto Ή(Δ,rr;0), and define
where ά^ is the d-dimensional vector (α| l 5
T (Γ denotes transposition). //g*(Δ,x,y;fl) is continuously differentiable with respect to θ for all
fixed Δ, x and y, then G^(θ) is the optimal estimating function within Q with
respect to the asymptotic criterion as well as to the fixed sample criterion of
Godambe and Heyde .
The (Xβ 's are determined by the following linear equations
/ a*u(A,x;θ)
=B<(Δ,x;β),
\a*Ni(A,x;θ) J
for i = 1,
, d, where C = {CM} and Bi = (bψ,
are given by
ckι(A,x;θ)=
I hk(A,x,y;θ)hι{A,x,y;θ)p{A,x,y;θ)dy
bf{Δ,χ ,θ) = I hj(A,x,y θ)dθip(A,x,y θ)dy.
When the functions hj(A,x,y;θ),
, JV are linearly independent in
2(p(Δ,x,y;0)ώ/), the matrix C is obviously invertible. The condition that
the estimating function is differentiable with respect to θ is really only a
technical matter in the Godambe-Heyde theory, and the estimating function
given by (3.5) is no doubt also the most efficient in the class Q under a
weaker condition. From (3.6), (3.7) and (3.8) we see that it is not difficult
to impose conditions on the functions /ij, j — 1,
, TV which ensure that
g* is continuously differentible with respect to θ. Note that under weak
conditions ensuring that differentiation and integration can be interchanged
(e.g. Condition 3.3 below), the 6j's given by (3.8) can also be expressed as
Results similar to Theorem 3.1 hold for general Markov processes and
for more general classes of martingale estimating functions than those given
by (3.3), see Kessler .
We next give a result about the asymptotic behaviour of the estimator
obtained from a general martingale estimating function Gn(θ) of the form
(3.1) with g given by (3.3), where the ay's are d-dimensional and the Λj's
satisfy (3.2). We do this under the assumption that the diffusion is ergodic,
which is ensured by the following condition. Here s(x; θ) denotes the density
of the scale measure of X:
= exp ( where x# is an arbitrary point in the interior of the state space of X.
Condition 3.2 The following holds for all θ G Θ:
s{x\θ)dx — oo
[s(x; θ)υ(x; θ^dx
= A{θ) < oo.
If the state space of X is not the whole real line, the integration limits —oo
and oo should be changed accordingly. Under Condition 3.2 the process
X is ergodic with an invariant probability measure μ$ which has density
DIFFUSION PROCESSES
[A(θ)s(x;θ)υ(x;θ)]~~
1 with respect to the Lebesgue measure. Define a probability measure Qfr on IR
Q$(x, y) = μθ{x) x p(Δ, x, y; 0).
For a function # : IR
2 4 E w e use the notation Q$(g) = f gdQfi.
The predictable quadratic variation of the martingale Gn(θ), when θ is
the true parameter value, is
(G(θ))n = £ A(A, Xti_, Θ)
TC(A, Xu_, Θ)A(A, Xu_, θ),
where A(Δ,α;;0)ij = aij(A,x;θ).
As above αij denotes the j'th coordinate
of the d-dimensional vector OL{. We impose the following condition on the
estimating functions. Prom now on ΘQ will denote the true value of θ.
Condition 3.3 The following holds for all θ G Θ:
1) The function g is continuously differentiable with respect to θ for all Δ,#
and y . The functions
(rr,y) H * 9^5j(Δ,rr,2/;^), i,j = 1,
,d, where gj
denotes the j'th coordinate of g, are locally dominated square integrable with
respect to Q$Q, and the matrix D(ΘQ) given by
id = Qh(dΘgi(Δ;θ0)) = £ Q£
D(θo)id = Qh(dΘjgi(Δ;θ0)) = £ Q£o[aki(A;θo)dθjhk(A
is invertible.
2) Each coordinate of the function (x,y) •->• ff(Δ,x,y;θ) is in L
Theorem 3.4 Suppose ΘQ G int Θ and that the Conditions 3.2 and 3.3 hold.
Then an estimator θn that solves the estimating equation
exists with a probability tending to one as n -> oo under PQ0 . Moreover, as
in probability under P^o, and
N{O,D(θo)-
ιV(θo)(D(θo)-
in distribution under Pβ0, where
= Eμβn (A(A; Θ)
TC(A; Θ)A(A; θ)).
Theorem 3.4 can be proved along the same lines as Theorem 3.3 in Bibby and
S0rensen , see also Kessler and Kessler and S0rensen .
Similar proofs of similar results can be found in several papers. Here Condition 3.1 (c) in Bibby and S0rensen has been omitted because Lemma
3.1 in Bibby and S0rensen remains valid without this condition as
follows from Theorem 1.1 in Billingsley and the central limit theorem for martingales in Billingsley . In fact, a multivariate version of
the central limit theorem is needed here, but in the relatively simple ergodic
case considered here this easily follows from the one-dimensional result by
applying the Cramer-Wold device.
Under Condition 3.3 the 6j's given by (3.8) can also be expressed by
(3.9), so D(θo) = —V(ΘQ) for the optimal estimating function G*(0) since
here the α's are given by (3.6). Hence the asymptotic covariance matrix of
the estimator based on G*(0) is given by
Polynomial estimating functions
Let us first consider linear martingale estimating functions, i.e. estimating
functions of the type
where a is d-dimensional and where
F(A,x;θ) = EΘ(XA\XO = x).
In most cases the mean value of the transition distribution is not explicitly
known so that it must be determined numerically. This is, however, relatively easy to do using suitable methods from Kloeden and Platen .
It is certainly much easier than to determine the entire transition density
numerically. Estimating functions of the type (3.15) were studied in Bibby
and S0rensen .
The optimal linear estimating function is 
Φ(Δ,x;0) = V<iτθ{XA\Xo = x).
Calculation of a derivative of a function that has to be determined numerically is a considerably more demanding numerical problem than determination of the function itself. Pedersen proposed a numerical procedure
DIFFUSION PROCESSES
for determining dβF(A^x;θ) by simulation, which works in practice, but
it is easier to use the following approximation to the optimal estimating
Khn(θ) =Σdθb(Xtι_1;θ)v(Xti_1',θ)-
which is obtained from K*n by inserting in the weight function dgF/Φ the
first order approximations to F and Φ given by (2.5) and (2.6). The estimating function K\^n can also be obtained from the estimating function (2.4) by
subtracting its compensator in order to turn it into a martingale and thus
remove its bias, see Bibby and S0rensen .
It is very important that we have only made approximations in the weight
function and not in the term Xt. - F ί Δ ^ X ^ fl), since such an approximation would destroy the martingale property, and hence the unbiasedness,
and would thus reintroduce the problems encountered in Section 2. An approximation of the weights c^F/Φ only implies a certain loss of efficiency.
Bibby and S0rensen showed that expansions in powers of Δ of the
asymptotic variances of the estimators based on K{n and K^n agree up to
and including terms of order O(Δ
2), so for small values of Δ there is not
much loss of efficiency in using the approximation. Calculations and simulations for a number of examples indicate that the loss of efficiency is often
rather small, see Bibby and S0rensen .
The linear estimating functions are useful when mainly the drift depends
on the parameter θ. If only the diffusion coefficient depends on 0, while the
drift is completely known, the linear estimating equations do not work. If
the diffusion coefficient depends considerably on 0, it is an advantage to use
second order polynomial estimating functions of the type
K2,n(θ) = Y^{a{^i,Xti_ι-θ)[Xti-F{^Xti_ι
The optimal estimating function, K^n, of this type is given by
n.* r τ.^ _
dθΦ(x;θ)η(x;θ)-dθF(x;θ)*(x;θ)
^F(rr; fl)»7(a:; β) - dθΦ(x; θ)Φ(x; θ)
where the Δ's have been omitted,
η(x;θ)=Eθ([XA-F(x;θ))
Φ(x; θ) = EΘ([XA - F(x; 0)]
4|XO = x) - Φ(z; θf.
An approximation to the optimal quadratic estimating function is
*(«> = Σ {
a'vfχt"~'.^ ί*« -
Δ» *•<-• • *>]
This estimating function is similar to (2.3), but it is unbiased and therefore generally gives a far better estimator. It is obtained from the optimal
quadratic estimating function, K^^ by using Gaussian approximations to
(3.23) and (3.24), i.e. η(x]θ)=0 and Φ(z;0)=2Φ(α;;0)
2, and then using the
first order approximations given by (2.5) and (2.6). Again it is important
that we only make approximations in the weights α and /?, so that the unbiasedness is preserved.
Quadratic estimating functions were treated in Bibby and Bibby
S0rensen . Higher order polynomial estimating functions were
investigated by Pedersen and Kessler . Some times there can be
good reasons to omit lower order terms in a polynomial estimating function,
for an example of this see Bibby and S0rensen .
Example 3.5 Let us return to the Cox-Ingersoll-Ross model considered in
Example 2.1. For this model the optimal estimating function given by (3.21)
(3.22) can be explicitly found , but the
corresponding estimating equation must be solved numerically. In the case
of equidistant sampling times the approximately optimal estimating function
(3.25) yields the following explicit estimators Ϋ
DIFFUSION PROCESSES
where F(Δ,x\a,θ)
= [(a + θx)e
ΘA - a]/θ and φ*(Δ,x\a,θ)
2ΘA - 2(α + θx)e
2. The estimators exist provided the expression for e
θnA is positive. A simulation study in Bibby and S0rensen 
indicates that these estimators are quite good.
Estimating equations based on eigenfunctions
The polynomial estimating functions are a generalization of the method of
moments to Markov processes. They can also be thought of as approximations to the true score function, which are likely to be good when the
time between observations is small enough that the transition density is not
too far from being Gaussian. There is therefore no reason to believe that
polynomial estimating functions are in general the best possible choise when
the time between observations is large and the transition distribution is far
from Gaussian. We shall therefore conclude this paper by discussing a type
of martingale estimating functions that can be more closely tailored to the
type of diffusion model under consideration. These estimating functions were
proposed and studied by Kessler and S0rensen .
A twice differentiate function φ(x\ θ) is called an eigenfunction for the
generator LQ (given by (2.10)) of the diffusion process (2.1) if
where the real number λ(θ) is called the eigenvalue corresponding to φ{x\ θ).
Under weak regularity conditions, see Kessler and Sorensen ,
Eθ(φ(XA]θ)\X0
We can therefore define a martingale estimating function by (3.1) with
,*,y; θ) = Σ <*i(Δ,x\ 0)to(y; θ) - e"
A^(a;; 0)],
where φι( ; 0),
, ΦN{Ί θ) are eigenfunctions for LQ with eigenvalues
The optimal estimating function of this type is given by (3.6) with
ckl(A,χ θ) = I φk(y;θ)φι(y;θ)p(A,x,y]θ)dy
bf(A,x;θ) = -fdΘiφj(y,θ)p(A,x^θ)dy
Aφj(x,θ)}.
Statistical inference based on this optimal estimating function is invariant
under twice continuously differentiable transformations of data, see Kessler
and S0rensen . After such a transformation the data are, by Ito's formula, still observations from a certain diffusion process, and the eigenfunctions transform in exactly the way needed to keep the optimal estimating
function invariant. Inference based on polynomial estimating functions is
obviously not invariant under transformations of the data.
Apart from this theoretical advantage, the optimal estimating functions
discussed here have clear numerical advantages over the optimal polynomial
estimating functions. As discussed earlier, determination of quantities like
d$F in (3.17) is a difficult numerical problem. In (3.30) the derivative is
under the integral sign, which makes determination of the optimal weights
in estimating functions of the type (3.28) a much simpler numerical problem
than the similar problem for polynomial estimating functions. Moreover,
EQ(Φ(XΔ\Θ)\X§
= x) is explicitly known, so numerical inaccuracies cannot
destroy the martingale property and the unbiasedness of these estimating
functions. It might in some applications be reasonable to obtain a quick estimator by reducing the numerical accuracy when determining the weights,
αy, j = 1,
, N. For the estimating equations based on eigenfunctions this
only implies a certain loss of efficiency, whereas the consistency of the estimators is preserved. It is also worth noting that for models where all eigenfunctions are polynomials or polynomials of the same function, the optimal
weights given by (3.29) and (3.30) can be explicitly calculated, see Kessler
and S0rensen . The disadvantage of these estimating functions, on the
other hand, is that it is not always possible to find eigenfunction for the generator of a given diffusion model. In such cases the polynomial estimating
functions, in particular the quadratic, provide a very useful alternative.
Example 3.6 For the Cox-Ingersoll-Ross model the eigenfunctions are the
Laguerre polynomials, and we obtain the polynomial estimating functions
discussed in the previous subsection, see Example 3.5.
Example 3.7 A more interesting example is the class of diffusions which
dXt = -0tan(Xt)cJt + dW
For θ > \ the process X is an ergodic diffusion on the interval (-π/2,7r/2),
which can be thought of as an Ornstein-Uhlenbeck process on a finite interval. The eigenfunctions are φi{x;θ) = Cf(sin(x)), i = 0,1,
DIFFUSION PROCESSES
eigenvalues i(θ + i/2), i = 0,1,
, where Cf is the Gegenbauer polynomial
of order i. The optimal estimating function based on any set of eigenfunctions can be found explicitly, see Kessler and S0rensen . The optimal
estimating function based on the first non-trivial eigenfunction, sin(x), is
When Δ is small the optimal estimating function can be approximated by
which yields the explicit estimator
/Σ?=1sin(^_Jsin(^)\
Σi=ism .
Acknowledgement: Thanks are due to a referee for a careful reading of
the manuscript.