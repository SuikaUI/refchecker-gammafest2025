Data re-uploading for a universal quantum classiﬁer
Adri´an P´erez-Salinas1,2, Alba Cervera-Lierta1,2, Elies Gil-Fuster3, and Jos´e I. Latorre1,2,4,5
1Barcelona Supercomputing Center
2Institut de Ci`encies del Cosmos, Universitat de Barcelona, Barcelona, Spain
3Dept. F´ısica Qu`antica i Astrof´ısica, Universitat de Barcelona, Barcelona, Spain.
4Nikhef Theory Group, Science Park 105, 1098 XG Amsterdam, The Netherlands.
5Center for Quantum Technologies, National University of Singapore, Singapore.
A single qubit provides suﬃcient computational capabilities to construct a universal
quantum classiﬁer when assisted with a classical subroutine.
This fact may be surprising since a single qubit only oﬀers a simple
superposition of two states and single-qubit
gates only make a rotation in the Bloch sphere.
The key ingredient to circumvent these limitations is to allow for multiple data re-uploading.
A quantum circuit can then be organized as
a series of data re-uploading and single-qubit
processing units. Furthermore, both data reuploading and measurements can accommodate multiple dimensions in the input and several categories in the output, to conform to
a universal quantum classiﬁer. The extension
of this idea to several qubits enhances the eﬃciency of the strategy as entanglement expands
the superpositions carried along with the classiﬁcation. Extensive benchmarking on diﬀerent examples of the single- and multi-qubit
quantum classiﬁer validates its ability to describe and classify complex data.
Introduction
Quantum circuits that make use of a small number of
quantum resources are of most importance to the ﬁeld
of quantum computation.
Indeed, algorithms that
need few qubits may prove relevant even if they do
not attempt any quantum advantage, as they may be
useful parts of larger circuits.
A reasonable question to ask is what is the lower
limit of quantum resources needed to achieve a given
computation.
A naive estimation for the quantum
cost of a new proposed quantum algorithm is often
made based on analogies with classical algorithms.
But this may be misleading, as classical computation
can play with memory in a rather diﬀerent way as
quantum computers do. The question then turns to
the more reﬁned problem of establishing the absolute
minimum of quantum resources for a problem to be
We shall here explore the power and minimal needs
of quantum circuits assisted with a classical subroutine to carry out a general supervised classiﬁcation
task, that is, the minimum number of qubits, quantum operations and free parameters to be optimized
classically. Three elements in the computation need
renewed attention.
The obvious ﬁrst concern is to
ﬁnd a way to upload data in a quantum computer.
Then, it is necessary to ﬁnd the optimal processing
of information, followed by an optimal measurement
strategy. We shall revisit these three issues in turn.
The non-trivial step we take here is to combine the
ﬁrst two, which is data uploading and processing.
There exist several strategies to design a quantum
classiﬁer. In general, they are inspired in well-known
classical techniques such as artiﬁcial neural networks
 or kernel methods used in classical machine
learning . Some of these proposals encode
the data values into a quantum state amplitude, which
is manipulated afterward. These approaches need an
eﬃcient way to prepare and access to these amplitudes.
State preparation algorithms are in general
costly in terms of quantum gates and circuit depth,
although some of these proposals use a speciﬁc state
preparation circuit that only require few single-qubit
gates. The access to the states that encode the data
can be done eﬃciently by using a quantum random
access memory (QRAM) . However, this is experimentally challenging and the construction of a QRAM
is still under development. Other proposals exploit
hybrid quantum-classical strategies . The classical parts can be used to construct the correct encoding circuit or as a minimization method to extract
the optimal parameters of the quantum circuit, such
as the angles of the rotational gates. In the ﬁrst case,
the quantum circuit computes the hardest instances of
the classical classiﬁcation algorithm as, for example,
the inner products needed to obtain a kernel matrix.
In the second case, the data is classiﬁed directly by
using a parametrized quantum circuit, whose variables
are used to construct a cost function that should be
minimized classically. This last strategy is more convenient for a Near Intermediate Scale Quantum computation (NISQ) since, in general, it requires shortdepth circuits, and its variational core makes it more
resistant to experimental errors.
Our proposal belongs to this last category, the parametrized quantum
Accepted in Quantum 2020-01-27, click title to verify. Published under CC-BY 4.0.
 
classiﬁers.
A crucial part of a quantum classiﬁcation algorithm
is how data is encoded into the circuit.
based on kernel methods design an encoding circuit
which implements a feature map from the data space
to the qubits Hilbert space. The construction of this
quantum feature map may vary depending on the algorithm, but common strategies make use of the quantum Fourier transform or introduce data in multiple
qubits using one- and two-qubit gates . Both
the properties of the tensor product and the entanglement generated in those encoding circuits capture the
non-linearities of the data. In contrast, we argue that
there is no need to use highly sophisticated encoding
circuits nor a signiﬁcant number of qubits to introduce these non-linearities. Single-qubit rotations applied multiple times along the circuit generate highly
non-trivial functions of the data values. The main difference between our approach and the ones described
above is that the circuit is not divided between the
encoding and processing parts, but implements both
multiple times along the algorithm.
Data re-uploading is considered as a manner of solving the limitations established by the no-cloning theorem.
Quantum computers cannot copy data, but
classical devices can. For instance, a neural network
takes the same input many times when processing the
data in the hidden layer neurons. An analogous quantum neural network can only use quantum data once.
Therefore, it makes sense to re-upload classical data
along a quantum computation to bypass this limitation on the quantum circuit. By following this line
of thought, we present an equivalence between data
re-uploading and the Universal Approximation Theorem applied to artiﬁcial neural networks . Just
as a network composed of a single hidden layer with
enough neurons can reproduce any continuous function, a single-qubit classiﬁer can, in principle, achieve
the same by re-uploading the data enough times.
The single-qubit classiﬁer illustrates the computational power that a single qubit can handle. This proposal is to be added to other few-qubit benchmarks
in machine learning . The input redundancy has
also been proposed to construct complex encoding in
parametrized quantum circuits and in the construction of quantum feature maps .
other proposals mentioned in the previous paragraphs
are focused on representing classically intractable or
very complex kernel functions with few qubits. On
the contrary, the focus of this work is to distill the
minimal amount of quantum resources, i.e., the number of qubits and gates, needed for a given classiﬁcation task quantiﬁed in terms of the number of qubits
and unitary operations. The main result of this work
is, indeed, to show that there is a trade-oﬀbetween
the number of qubits needed to perform classiﬁcation
and multiple data re-uploading. That is, we may use
fewer qubits at the price of re-entering data several
times along the quantum computation.
We shall illustrate the power of a single- and multiqubit classiﬁers with data re-uploading with a series
of examples. First, we classify points in a plane that
is divided into two areas. Then, we extend the number of regions on a plane to be classiﬁed. Next, we
consider the classiﬁcation of multi-dimensional patterns and, ﬁnally, we benchmark this quantum classiﬁer with non-convex ﬁgures.
For every example,
we train a parametrized quantum circuit that carries out the task and we analyze its performance in
terms of the circuit architecture, i.e., for single- and
multi-qubit classiﬁers with and without entanglement
between qubits.
This paper is structured as follows. First, in Section 2, we present the basic structure of a single-qubit
quantum classiﬁer. Data and processing parameters
are uploaded and re-uploaded using one-qubit general rotations.
For each data point, the ﬁnal state
of the circuit is compared with the target state assigned to its class, and the free parameters of the circuit are updated accordingly using a classical minimization algorithm. Next, in Section 3, we motivate
the data re-uploading approach by using the Universal Approximation Theorem of artiﬁcial neural networks.
In Section 4, we introduce the extension of
this classiﬁer to multiple qubits. Then, in Section 5,
we detail the minimization methods used to train the
quantum classiﬁers. Finally, in Section 6, we benchmark single- and multi-qubit quantum classiﬁers de-
ﬁned previously with problems of diﬀerent dimensions
and complexity and compare their performance respect to classical classiﬁcation techniques. The conclusions of this proposal for a quantum classiﬁer are
exposed in Section 7.
Structure of a single-qubit quantum
The global structure of any quantum circuit can be
divided into three elements: uploading of information onto a quantum state, processing of the quantum state, and measurement of the ﬁnal state. It is
far from obvious how to implement each of these elements optimally to perform a speciﬁc operation. We
shall now address them one at a time for the task of
classiﬁcation.
Re-uploading classical information
To load classical information onto a quantum circuit is
a highly non-trivial task . A critical example is the
processing of big data. While there is no in-principle
obstruction to upload large amounts of data onto a
state, it is not obvious how to do it.
The problem we address here is not related to a
large amount of data. It is thus possible to consider a
Accepted in Quantum 2020-01-27, click title to verify. Published under CC-BY 4.0.
quantum circuit where all data are loaded in the coeﬃcients of the initial wave function . In
the simplest of cases, data are uploaded as rotations of
qubits in the computational basis. A quantum circuit
would then follow that should perform some classiﬁcation.
This strategy would be insuﬃcient to create a universal quantum classiﬁer with a single qubit. A ﬁrst
limitation is that a single qubit only has two degrees
of freedom, thus only allowing to represent data in
a two-dimensional space.
No quantum classiﬁer in
higher dimensions can be created if this architecture
is to be used. A second limitation is that, once data
is uploaded, the only quantum circuit available is a
rotation in the Bloch sphere. It is easy to prove that
a single rotation cannot capture any non-trivial separation of patterns in the original data.
We need to turn to a diﬀerent strategy, which turns
out to be inspired by neural networks. In the case of
feed-forward neural networks, data are entered in a
network in such a way that they are processed by subsequent layers of neurons. The key idea is to observe
that the original data are processed several times, one
for each neuron in the ﬁrst hidden layer.
speaking, data are re-uploaded onto the neural network. If neural networks were aﬀected by some sort
of no-cloning theorem, they could not work as they
do. Coming back to the quantum circuit, we need to
design a new architecture where data can be introduced several times into the circuit.
The central idea to build a universal quantum classiﬁer with a single qubit is thus to re-upload classical
data along with the computation. Following the comparison with an artiﬁcial neural network with a single
hidden layer, we can represent this re-upload diagrammatically, as it is shown in Figure 1. Data points in a
neural network are introduced in each processing unit,
represented with squares, which are the neurons of the
hidden layer. After the neurons process these data, a
ﬁnal neuron is necessary to construct the output to be
analyzed. Similarly, in the single-qubit quantum classiﬁer, data points are introduced in each processing
unit, which this time corresponds to a unitary rotation. However, each processing unit is aﬀected by the
previous ones and re-introduces the input data. The
ﬁnal output is a quantum state to be analyzed as it
will be explained in the next subsections.
The explicit form of this single-qubit classiﬁer is
shown in Figure 2. Classical data are re-introduced
several times in a sequence interspaced with processing units. We shall consider the introduction of data
as a rotation of the qubit. This means that data from
three-dimensional space, ⃗x, can be re-uploaded using
unitaries that rotate the qubit U(⃗x). Later processing
units will also be rotations as discussed later on. The
whole structure needs to be trained in the classiﬁcation of patterns.
As we shall see, the performance of the single-qubit
Neural network
Quantum classiﬁer
Figure 1: Simpliﬁed working schemes of a neural network
and a single-qubit quantum classiﬁer with data re-uploading.
In the neural network, every neuron receives input from all
neurons of the previous layer.
In contrast with that, the
single-qubit classiﬁer receives information from the previous
processing unit and the input (introduced classically). It processes everything all together and the ﬁnal output of the
computation is a quantum state encoding several repetitions
of input uploads and processing parameters.
quantum classiﬁer will depend on the number of reuploads of classical data. This fact will be explored
in the results section.
Processing along re-uploading
The single-qubit classiﬁer belongs to the category of
parametrized quantum circuits. The performance of
the circuit is quantiﬁed by a ﬁgure of merit, some
speciﬁc χ2 to be minimized and deﬁned later.
need, though, to specify the processing gates present
in the circuit in terms of a classical set of parameters.
Given the simple structure of a single-qubit circuit
presented in Figure 2, the data is introduced in a simple rotation of the qubit, which is easy to characterize. We just need to use arbitrary single-qubit rotations U(φ1, φ2, φ3) ∈SU(2). We will write U(⃗φ) with
⃗φ = (φ1, φ2, φ3). Then, the structure of the universal
quantum classiﬁer made with a single qubit is
U(⃗φ, ⃗x) ≡U(⃗φN)U(⃗x) . . . U(⃗φ1)U(⃗x),
which acts as
|ψ⟩= U(⃗φ, ⃗x)|0⟩.
The ﬁnal classiﬁcation of patterns will come from
the results of measurements on |ψ⟩. We may introduce
the concept of processing layer as the combination
L(i) ≡U(⃗φi)U(⃗x),
so that the classiﬁer corresponds to
U(⃗φ, ⃗x) = L(N) . . . L(1),
where the depth of the circuit is 2N. The more layers
the more representation capabilities the circuit will
have, and the more powerful the classiﬁer will become. Again, this follows from the analogy to neural
Accepted in Quantum 2020-01-27, click title to verify. Published under CC-BY 4.0.
(a) Original scheme
(b) Compressed scheme
Figure 2: Single-qubit classiﬁer with data re-uploading. The
quantum circuit is divided into layer gates L(i), which constitutes the classiﬁer building blocks. In the upper circuit,
each of these layers is composed of a U(⃗x) gate, which uploads the data, and a parametrized unitary gate U(⃗φ). We
apply this building block N times and ﬁnally compute a cost
function that is related to the ﬁdelity of the ﬁnal state of
the circuit with the corresponding target state of its class.
This cost function may be minimized by tunning the ⃗φi parameters. Eventually, data and tunable parameters can be
introduced with a single unitary gate, as illustrated in the
bottom circuit.
networks, where the size of the intermediate hidden
layer of neurons is critical to represent complex functions.
There is a way to compactify the quantum circuit
into a shorter one. This can be done if we incorporate
data and processing angles in a single step. Then, a
layer would only need a single rotation to introduce
data and tunable parameters, i.e. L(i) = U(⃗φ, ⃗x). In
addition, each data point can be uploaded with some
weight wi. These weights will play a similar role as
weights in artiﬁcial neural networks, as we will see in
the next section. Altogether, each layer gate can be
⃗θi + ⃗wi ◦⃗x
where ⃗wi ◦⃗x =
is the Hadamard
product of two vectors. In case the data points have
dimension lesser than three, the rest of ⃗x components
are set to zero. Such an approach reduces the depth of
the circuit by half. Further combinations of layers into
fewer rotations are also possible, but the nonlinearity
inherent to subsequent rotations would be lost, and
the circuit would not be performing well.
Notice that data points are introduced linearly into
the rotational gate.
Non-linearities will come from
the structure of these gates. We chose this encoding
function as we believe it is one of the lesser biased
ways to encode data with unknown properties. Due
to the structure of single-qubit unitary gates, we will
see that this encoding is particularly suited for data
with rotational symmetry. Still, it can also classify
other kinds of data structures.
We can also apply
other encoding techniques, e.g. the ones proposed in
 , but for the scope of this work, we have
just tested the linear encoding strategy as a proof of
concept of the performance of this quantum classiﬁer.
It is also possible to enlarge the dimensionality of
the input space in the following way. Let us extend
the deﬁnition of i-th layer to
where each data point is divided into k vectors of dimension three. In general, each unitary U could absorb as many variables as freedom in an SU(2) unitary. Each set of variables act at a time, and all of
them have been shown to the circuit after k iterations.
Then, the layer structure follows. The complexity of
the circuit only increases linearly with the size of the
input space.
Measurement
The quantum circuit characterized by a series of processing angles {θi} and weights {wi} delivers a ﬁnal
state |ψ⟩, which needs to be measured. The results
of each measurement are used to compute a χ2 that
quantiﬁes the error made in the classiﬁcation. The
minimization of this quantity in terms of the classical
parameters of the circuit can be organized using any
preferred supervised machine learning technique.
The critical point in the quantum measurement is
to ﬁnd an optimal way to associate outputs from the
observations to target classes. The fundamental guiding principle to be used is given by the idea of maximal orthogonality of outputs . This is easily established for a dichotomic classiﬁcation, where one of
two classes A and B have to be assigned to the ﬁnal
measurement of the single qubit. In such a case it
is possible to measure the output probabilities P(0)
for |0⟩and P(1) for |1⟩.
A given pattern could be
classiﬁed into the A class if P(0) > P(1) and into B
otherwise. We may reﬁne this criterium by introducing a bias. That is, the pattern is classiﬁed as A if
P(0) > λ, and as B otherwise. The λ is chosen to optimize the success of classiﬁcation on a training set.
Results are then checked on an independent validation
The assignment of classes to the output reading of
a single qubit becomes an involved issue when many
classes are present. For the sake of simplicity, let us
mention two examples for the case of classiﬁcation to
four distinct classes. One possible strategy consists on
comparing the probability P(0) to four sectors with
three thresholds: 0 ≤λ1 ≤λ2 ≤λ3 ≤1. Then, the
value of P(0) will fall into one of them, and classiﬁcation is issued. A second, more robust assignment is
obtained by computing the overlap of the ﬁnal state
to one of the states of a label states-set. This statesset is to be chosen with maximal orthogonality among
Accepted in Quantum 2020-01-27, click title to verify. Published under CC-BY 4.0.
Figure 3: Representation in the Bloch sphere of four and six
maximally orthogonal points, corresponding to the vertices
of a tetrahedron and an octahedron respectively. The singlequbit classiﬁer will be trained to distribute the data points in
one of these vertices, each one representing a class.
all of them. This second method needs from the maximally orthogonal points in the Bloch sphere. Figure
3 shows the particular cases that can be applied to
a classiﬁcation task of four and six classes. In general, a good measurement strategy may need some
prior computational eﬀort and reﬁned tomography of
the ﬁnal state. Since we are proposing a single-qubit
classiﬁer, the tomography protocol will only require
three measurements.
It is possible to interpret the single-qubit classi-
ﬁer in terms of geometry. The classiﬁer opens a 2dimensional Hilbert space, i.e., the Bloch sphere. As
we encode data and classiﬁcation within the parameters deﬁning rotations, this Hilbert space is enough
to achieve classiﬁcation. Any operation L(i) is a rotation on the Bloch sphere surface. With this point
of view in mind, we can easily see that we can classify any point using only one unitary operation. We
can transport any point to any other point on the
Bloch sphere by nothing else than choosing the angles of rotation properly.
However, this does not
work for several data, as the optimal rotation for some
data points could be very inconvenient for some other
points. However, if more layers are applied, each one
will perform a diﬀerent rotation, and many diﬀerent
rotations together have the capability of enabling a
feature map.
Data embedded in this feature space
can be easily separated into classes employing the regions on the Bloch sphere.
A ﬁdelity cost function
We propose a very simple cost function motivated by
the geometrical interpretation introduced above. We
want to force the quantum states |ψ(⃗θ, ⃗w, ⃗x)⟩to be as
near as possible to one particular state on the Bloch
sphere. The angular distance between the label state
and the data state can be measured with the relative
ﬁdelity between the two states . Thus, our aim
is to maximize the average ﬁdelity between the states
at the end of the quantum circuit and the label states
corresponding to their class. We deﬁne the following
cost function that carries out this task,
f(⃗θ, ⃗w) =
1 −|⟨˜ψs|ψ(⃗θ, ⃗w, ⃗xµ)⟩|2
where | ˜ψs⟩is the correct label state of the µ data
point, which will correspond to one of the classes.
A weighted ﬁdelity cost function
We shall next deﬁne a reﬁned version of the previous
ﬁdelity cost function to be minimized.
The set of
maximally orthogonal states in the Bloch sphere, i.e.,
the label states, are written as |ψc⟩, where c is the
class. Each of these label states represents one class
for the classiﬁer. Now, we will follow the lead usually
taken in neural network classiﬁcation.
Let us deﬁne the quantity
Fc(⃗θ, ⃗w, ⃗x) = |⟨˜ψc|ψ(⃗θ, ⃗w, ⃗x)⟩|2,
where M is the total number of training points, | ˜ψc⟩
is the label state of the class c and |ψ(⃗θ, ⃗w, ⃗x)⟩is the
ﬁnal state of the qubit at the end of the circuit. This
ﬁdelity is to be compared with the expected ﬁdelity of
a successful classiﬁcation, Yc(⃗x). For example, given
a four-class classiﬁcation and using the vertices of a
tetrahedron as label states (as shown in Figure 3),
one expects Ys(⃗x) = 1, where s is the correct class,
and Yr(⃗x) = 1/3 for the other r classes. In general,
Yc(⃗x) can be written as a vector with one entry equal
to 1, the one corresponding to the correct class, and
the others containing the overlap between the correct
class label state and the other label states.
With these deﬁnitions, we can construct a cost
function which turns out to be inspired by conventional cost functions in artiﬁcial neural networks. By
weighting the ﬁdelities of the ﬁnal state of the circuit
with all label states, we deﬁne the weighted ﬁdelity
cost function as
wf(⃗α, ⃗θ, ⃗w) = 1
αcFc(⃗θ, ⃗w, ⃗xµ) −Yc(⃗xµ)
where M is the total number of training points, C
is the total number of classes, ⃗xµ are the training
points and ⃗α = (α1, · · · , αC) are introduced as class
weights to be optimized together with ⃗θ and ⃗w parameters. This weighted ﬁdelity has more parameters
than the ﬁdelity cost function. These parameters are
the weights for the ﬁdelities.
The main diﬀerence between the weighted ﬁdelity
cost function of Eq. (9) and the ﬁdelity cost function
of Eq. (7) is how many overlaps do we need to compute. The χ2
wf requires as many ﬁdelities as classes
every time we run the optimization subroutine, while
f needs just one. This is not such a big diﬀerence for a few classes and only one qubit. It is possible
Accepted in Quantum 2020-01-27, click title to verify. Published under CC-BY 4.0.
to measure any state with a full tomography process
which, for one qubit, is achievable. However, for many
diﬀerent classes, we expect that one measurement will
be more eﬃcient than many.
Besides the weighted ﬁdelity cost function being
costlier than the ﬁdelity cost function, there is another
qualitative diﬀerence between both. The ﬁdelity cost
function forces the parameters to reach the maximum
in ﬁdelities. Loosely speaking, this ﬁdelity moves the
qubit state to where it should be. The weighted ﬁdelity forces the parameters to be close to a speciﬁed
conﬁguration of ﬁdelities. It moves the qubit state to
where it should be and moves it away from where it
should not. Therefore, we expect that the weighted ﬁdelity will work better than the ﬁdelity cost function.
Moreover, this extra cost in terms of the number of
parameters of the weighted ﬁdelity cost function will
only aﬀect the classical minimization part of the algorithm. In a sense, we are increasing the classical
processing part to reduce the quantum resources required for the algorithm, i.e. the number of quantum
operations (layers). This fact gain importance in the
NISQ computation era.
Universality of the single-qubit classiﬁer
After analyzing several classiﬁcation problems, we obtain evidence that the single-qubit classiﬁer introduced above can approximate any classiﬁcation function up to arbitrary precision. In this section, we provide the motivation for this statement based on the
Universal Approximation Theorem (UAT) of artiﬁcial
neural networks .
Universal Approximation Theorem
Theorem– Let Im = m be the m-dimensional unit
cube and C(Im) the space of continuous functions in
Im. Let the function ϕ : R →R be a nonconstant,
bounded and continuous function and f : Im →R
a function.
Then, for every ϵ > 0, there exists an
integer N and a function h : Im →R, deﬁned as
αi ϕ (⃗wi · ⃗x + bi) ,
with αi, bi ∈R and ⃗wi ∈Rm, such that h is an approximate realization of f with precision ϵ, i.e.,
|h(⃗x) −f(⃗x)| < ϵ
for all ⃗x ∈Im.
In artiﬁcial neural networks, ϕ is the activation
function, ⃗wi are the weights for each neuron, bi are the
biases and αi are the neuron weights that construct
the output function. Thus, this theorem establishes
that it is possible to reconstruct any continuous function with a single layer neural network of N neurons.
The proof of this theorem for the sigmoidal activation
function can be found in Ref. . This theorem was
generalized for any nonconstant, bounded and continuous activation function in Ref. . Moreover, Ref.
 presents the following corollary of this theorem:
ϕ could be a nonconstant ﬁnite linear combination of
periodic functions, in particular, ϕ could be a nonconstant trigonometric polynomial.
Universal Quantum Circuit Approximation
The single-qubit classiﬁer is divided into several layers
which are general SU(2) rotational matrices. There
exist many possible decompositions of an SU(2) rotational matrix. In particular, we use
U(⃗φ) = U(φ1, φ2, φ3) = eiφ2σzeiφ1σyeiφ3σz,
where σi are the conventional Pauli matrices. Using
the SU(2) group composition law, we can rewrite the
above parametrization in a single exponential,
U(⃗φ) = ei⃗ω(⃗φ)·⃗σ,
with ⃗ω(⃗φ) =
ω1(⃗φ), ω2(⃗φ), ω3(⃗φ)
ω1(⃗φ) = d N sin ((φ2 −φ3)/2) sin (φ1/2) ,
ω2(⃗φ) = d N cos ((φ2 −φ3)/2) sin (φ1/2) ,
ω3(⃗φ) = d N sin ((φ2 + φ3)/2) cos (φ1/2) ,
cos ((φ2 + φ3)/2) cos (φ1/2).
The single-qubit classiﬁer codiﬁes the data points
into ⃗φ parameters of the U unitary gate. In particular, we can re-upload data together with the tunable
parameters as deﬁned in Eq. (5), i.e.
⃗φ(⃗x) = (φ1(⃗x), φ2(⃗x), φ3(⃗x)) = ⃗θ + ⃗w ◦⃗x.
U(⃗x) = UN(⃗x)UN−1(⃗x) · · · U1(⃗x) =
ei⃗ω(⃗φi(⃗x))·⃗σ,
Next, we apply the Baker-Campbell-Hausdorﬀ(BCH)
formula to the above equation,
U(⃗x) = exp
⃗ω(⃗φi(⃗x)) · ⃗σ + Ocorr
Notice that the remaining BCH terms Ocorr are
also proportional to Pauli matrices due to [σi, σj] =
Accepted in Quantum 2020-01-27, click title to verify. Published under CC-BY 4.0.
Each ⃗ω terms are trigonometric functions, unconstant, bounded and continuous. Then
⃗ω(⃗φi(⃗x)) =
ω1(⃗φi(⃗x)), ω2(⃗φi(⃗x)), ω3(⃗φi(⃗x))
ω1(⃗θi + ⃗wi ◦⃗x), ω2(⃗θi + ⃗wi ◦⃗x), ω3(⃗θi + ⃗wi ◦⃗x)
= (f1(⃗x), f2(⃗x), f3(⃗x)) .
We still have to deal with the remaining terms
Ocorr of the BCH expansion.
Instead of applying
such expansion, we can use again the SU(2) group
composition law to obtain the analytical formula of
U(⃗x) = ei⃗ξ(⃗x)·⃗σ, where ⃗ξ(⃗x) will be an inextricably
trigonometric function of ⃗x. The Ocorr terms are proportional to ⃗σ matrices, so Ocorr = ⃗ϱ(⃗x) · ⃗σ for some
function ⃗ϱ(⃗x). Then,
U(⃗x) = ei⃗ξ(⃗x)·⃗σ = ei⃗f(⃗x)·⃗σ+i⃗ϱ(⃗x)·⃗σ.
Thus, Ocorr terms can be absorbed in ⃗f(⃗x).
For each data point ⃗x, we obtain a ﬁnal state that
will contain these ⃗ξ(⃗x) functions.
With all training points, we construct a cost function that can include new parameters αc for each class if we use the
weighted ﬁdelity cost function of Eq. (9). The function obtained from the combination of ⃗ξ(x) and αc is
expected to be complex enough to probably represent
almost any continuous function. However, more parameters are necessary to map this argument with the
UAT expression.
If we compare the parameters of the UAT with the
single-qubit circuit parameters, the ⃗wi will correspond
with the weights, the ⃗θi with the biases bi, the number
of layers N of the quantum classiﬁer will correspond
with the number of neurons in the hidden layer and
⃗ω functions with the activation functions ϕ.
We have explained why it is necessary to re-upload
the data at each layer and why a single qubit could
be a universal classiﬁer. As has been stated before,
an artiﬁcial neural network introduces the data points
in each hidden neuron, weights them and adds some
bias. Here we cannot just copy each data point because the non-cloning theorem, so we have to reupload it at each layer.
From single- to multi-qubit quantum
The single-qubit classiﬁer cannot carry any quantum
advantage respect classical classiﬁcation techniques
such as artiﬁcial neural networks.
In the previous
sections, we have deﬁned a quantum mechanical version of a neural network with a single hidden layer. In
general, a huge amount of hidden neurons is necessary
to approximate a target function with a single layer.
To circumvent this inconvenience, more hidden layers
are introduced, leading eventually to the concept of
deep neural networks.
By using the single-qubit classiﬁer formalism that
we have introduced in the previous sections, we propose its generalization to more qubits. The introduction of multiple qubits to this quantum classiﬁer may
improve its performance as more hidden layers improve the classiﬁcation task of an artiﬁcial neural network. With the introduction of entanglement between
these qubits, we reduce the number of layers of our
classiﬁer as well as propose a quantum classiﬁcation
method that can achieve quantum advantage.
Figure 1 shows the analogy between a neural network with a single hidden layer and a single-qubit
classiﬁer.
The generalization of this analogy is not
so obvious.
A multi-qubit classiﬁer without entanglement could have some similarities with a convolutional neural network, where each qubit could represent a neural network by itself. However, it is not clear
if the introduction of entanglement between qubits
can be understood as a deep neural network architecture. The discussion around this analogy as well
as an extended study of the performance of a multiqubit classiﬁer is beyond the scope of this work. In
the next subsections, we present a general proposal
for a multi-qubit classiﬁer which we compare with the
single-qubit one in Section 6.
Measurement strategy and cost function
for a multi-qubit classiﬁer
With a single-qubit classiﬁer, the measurement strategy consisting on comparing the ﬁnal state of the
circuit with a pre-deﬁned target state was achievable. Experimentally, one needs to perform a quantum state tomography protocol of only three measurements.
However, if more qubits are to be considered, tomography protocols become exponentially
expensive in terms of number of measurements.
We propose two measurement strategies for a multiqubit classiﬁer. The ﬁrst one is the natural generalization of the single-qubit strategy, although it will
become unrealizable for a large number of qubits. We
compare the ﬁnal state of the circuit with one of the
states of the computational basis, one for each class.
The second strategy consist on focusing in one qubit
and depending on its state associate one or other class.
This is similar to previous proposals of binary multiqubit classiﬁers , although we add the possibility of
multiclass classiﬁcation by introducing several thresholds (see Section 2).
Another part that should be adapted is the deﬁnition of the cost function. In particular, we use diﬀerent functions for each strategy explained above.
For the ﬁrst strategy, we use the ﬁdelity cost function of Eq. (7). Its generalization to more qubits is
straightforward. However, the orthogonal states used
Accepted in Quantum 2020-01-27, click title to verify. Published under CC-BY 4.0.
Ansatz with no entanglement
Ansatz with entanglement
Figure 4: Two-qubit quantum classiﬁer circuit without entanglement (top circuit) and with entanglement (bottom
circuit). Here, each layer includes a rotation with data reuploading in both qubits plus a CZ gate if there is entanglement. The exception is the last layer, which does not have
any CZ gate associated to it. For a ﬁxed number of layers,
the number of parameters to be optimized doubles the one
needed for a single-qubit classiﬁer.
for a multi-qubit classiﬁer are taken as the computational basis states. A more sophisticated set of states
could be considered to improve the performance of
this method.
For the second strategy, we use the weighted ﬁdelity
cost function. As stated above, we just focus on one
qubit, thus
Fc,q(⃗θ, ⃗w, ⃗x) = ⟨˜ψc|ρq(⃗θ, ⃗w, ⃗x)| ˜ψc⟩,
where ρq is the reduced density matrix of the qubit to
be measured. Then, the weighted ﬁdelity cost function can be adapted as
wf(⃗α, ⃗θ, ⃗w) =
αc,qFc,q(⃗θ, ⃗w, ⃗xµ) −Yc(⃗xµ)
where we average over all Q qubits that form the classiﬁer. Eventually, we can just measure one of these
qubits, reducing the number of parameters to be optimized.
Quantum circuits examples
The deﬁnition of a multi-qubit quantum classiﬁer circuit could be as free as is the deﬁnition of a multilayer neural network.
In artiﬁcial neural networks,
it is far from obvious what should be the number of
hidden layers and neurons per layer to perform some
task. Besides, it is, in general, problem-dependent.
For a multi-qubit quantum classiﬁer, there is extra
degree of freedom in the circuit-design: how to introduce the entanglement. This is precisely an open
problem in parametrized quantum circuits: to ﬁnd a
(a) Ansatz with no entanglement
(b) Ansatz with entanglement
Figure 5: Four-qubit quantum classiﬁer circuits.
entanglement (top circuit), each layer is composed by four
parallel rotations. With entanglement (bottom circuit) each
layer includes a parallel rotation and two parallel CZ gates.
The order of CZ gates alternates in each layer between (1)-
(2) and (3)-(4) qubits and (2)-(3) and (1)-(4) qubits. The
exception is in the last layer, which does not contain any CZ
gate. For a ﬁxed number of layers, the number of parameters
to be optimized quadruples the ones needed for a single-qubit
classiﬁer.
correct ansatz for the entangling structure of the circuit.
Figures 4 and 5 show the explicit circuits used in
this work. For a two-qubit classiﬁer without entanglement, and similarly for a four-qubit classiﬁer, we identify each layer as parallel rotations on all qubits. We
introduce the entanglement using CZ gates between
rotations that are absorbed in the deﬁnition of layer.
For two-qubit classiﬁer with entanglement, we apply a
CZ gate after each rotation with exception of the last
layer. For a four-qubit classiﬁer, two CZ gates are applied after each rotation alternatively between (1)-(2)
and (3)-(4) qubits and (2)-(3) and (1)-(4) qubits.
The number of parameters needed to perform the
optimization doubles the ones needed for a singlequbit classiﬁer for the two-qubit classiﬁer and quadruples for the four-qubit classiﬁer. For N layers, the circuit depth is N for the non-entangling classiﬁers and
2N for the entangling classiﬁers.
Minimization methods
The practical training of a parametrized single-qubit
or multi-qubit quantum classiﬁer needs minimization
in the parameter space describing the circuit. This
Accepted in Quantum 2020-01-27, click title to verify. Published under CC-BY 4.0.
is often referred as a hybrid algorithm, where classical and quantum logic coexist and beneﬁt from one
To be precise, the set of {θi} angles and
{wi} weights, together with αq,l parameters if applicable, forms a space to be explored in search of a minimum χ2. In parameter landscapes as big as the ones
treated here, or in regular neural network classiﬁcation, the appearance of local minima is ultimately unavoidable. The composition of rotation gates renders
a large product of independent trigonometric functions.
It is thus clear to see that our problem will
be overly populated with minima. The classical minimizer can easily get trapped in a not optimal one.
Our problem is reduced to minimizing a function
of many parameters. For a single-qubit classiﬁer, the
number of parameters is (3 + d)N where d is the dimension of the problem, i.e. the dimension of ⃗x, and
N is the number of layers. Three of these parameters
are the rotational angles and the other d correspond
with the ⃗wi weight. If using the weighted ﬁdelity cost
function, we should add C extra parameters, one for
each class.
In principle, one does not know how is the parameter landscape of the cost function to be minimized. If
the cost function were, for example, a convex function,
a downhill strategy would be likely to work properly.
The pure downhill strategy is known as gradient descent.
In machine learning, the method commonly
used is a Stochastic Gradient Descent (SGD) .
There is another special method of minimization
known as L-BFGS-B . This method has been used
in classical machine learning with very good results
The results we present from now on are starred by
the L-BFGS-B algorithm, as we found it is accurate
and relatively fast. We used open source software 
as the core of the minimization with own made functions to minimize. The minimizer is taken as a black
box whose parameters are set by default. As this is
the ﬁrst attempt of constructing a single- or multiqubit classiﬁer, further improvements can be done on
the hyperparameters of minimization.
Nevertheless we have also tested a SGD algorithm
for the ﬁdelity cost function. This whole algorithm
has been developed by us following the steps from .
The details can be read in Appendix A. In general, we
found that L-BFGS-B algorithm is better than SGD.
This is something already observed in classical neural networks.
When the training set is small, it is
often more convenient to use a L-BFGS-B strategy
than a SGD. We were forced to use small training
sets due to computational capabilities for our simulations. Numerical evidences on this arise when solving
the problems we face for these single- and multi-qubit
classiﬁers with classical standard machine learning libraries . This can be understood with a simple
argument. Neural networks or our quantum classiﬁer
are supposed to have plenty of local minima. Neural
networks have got huge products of non linear functions. The odds of having local minima are then large.
In the quantum circuits side, there are nothing but
trigonometric functions. In both cases, if there are a
lot of training points it is more likely to ﬁnd some of
them capable of getting us out of local minima. If this
is the case, SGD is more useful for being faster. On
the contrary, when the training set is small, we have
to pick an algorithm less sensitive to local minima,
such as the L-BFGS-B.
Benchmark of a single- and multiqubit classiﬁer
We can now tackle some classiﬁcation problems. We
will prove that a single-qubit classiﬁer can perform
a multi-class classiﬁcation for multi-dimensional data
and that a multi-qubit classiﬁer, in general, improves
these results.
We construct several classiﬁers with diﬀerent number of layers. We then train the circuits with a training set of random data points to obtain the values of
the free parameters {θi} and {wi} for each layer and
{αi} when applicable. We use the cost functions de-
ﬁned in Eq. (9) and Eq. (7). Then, we test the performance of each classiﬁer with a test set independently
generated and one order of magnitud greater than the
training set. For the sake of reproducibility, we have
ﬁxed the same seed to generate all data points. For
this reason, the test and training set points are the
same for all problems. For more details, we provide
the explicit code used in this work .
We run a single-, two- and four-qubit classiﬁers,
with and without entanglement, using the two cost
functions described above.
We benchmark several
classiﬁers formed by L = 1, 2, 3, 4, 5, 6, 8 and 10 layers.
In the following subsections, we describe the particular problems addressed with these single- and multiqubit classiﬁers with data re-uploading. We choose
four problem types:
a simple binary classiﬁcation,
a classiﬁcation of a ﬁgure with multiple patterns, a
multi-dimensional classiﬁcation and a non-convex ﬁgure.
The code used to deﬁne and benchmark the singleand multi-qubit quantum classiﬁer is open and can be
found in Ref. .
Simple example: classiﬁcation of a circle
Let us start with a simple example. We create a random set of data on a plane with coordinates ⃗x =
(x1, x2) with xi ∈[−1, 1]. Our goal is to classify these
points according to x2
2 < r2, i.e. if they are inside
or outside of a circle of radius r. The value of the radius is chosen in such a way that the areas inside and
outside it are equal, that is, r =
π, so the probability of success if we label each data point randomly
Accepted in Quantum 2020-01-27, click title to verify. Published under CC-BY 4.0.
Table 1: Results of the single- and multi-qubit classiﬁers with data re-uploading for the circle problem. Numbers indicate the
success rate, i.e. number of data points classiﬁed correctly over total number of points. Words “Ent.” and “No Ent.” refer
to considering entanglement between qubits or not, respectively. We have used the L-BFGS-B minimization method with the
weighted ﬁdelity and ﬁdelity cost functions. For this problem, both cost functions lead to high success rates. The multi-qubit
classiﬁer increases this success rate but the introduction of entanglement does not aﬀect it signiﬁcantly.
is 50%. We create a train dataset with 200 random
We then validate the single-qubit classiﬁer
against a test dataset with 4000 random points.
The results of this classiﬁcation are written in Table 1. With the weighted ﬁdelity cost function, the
single-qubit classiﬁer achieves more than 90% of success with only two layers, that is, 12 parameters. The
results are worse with the ﬁdelity cost function. For
a two-qubit and a four-qubit classiﬁer, two layers are
required to achieve 96% of success rate, that is, 22 parameters for the two-qubit and 42 for the four-qubit.
The introduction of entanglement does not change the
result in any case. The results show a saturation of
the success rate.
Considering more layers or more
qubits does not change this success rate.
The characterization of a closed curved is a hard
problem for an artiﬁcial neural network that works
in a linear regime, although enough neurons, i.e. linear terms, can achieve a good approximation to any
function. On the contrary, the layers of a single-qubit
classiﬁer are rotational gates, which have an intrinsic
non-linear behavior. In a sense, a circle becomes an
easy function to classify as a linear function is for an
artiﬁcial neural network. The circle classiﬁcation is,
in a sense, trivial for a quantum classiﬁer. We need
to run these classiﬁers with more complex ﬁgures or
problems to test their performance.
It is interesting to compare classiﬁers with diﬀerent
number of layers.
Figure 6 shows the result of the
classiﬁcation for a single-qubit classiﬁer of 1, 2, 4 and
8 layers. As with only one layer the best classiﬁcation
that can be achieved consist on dividing the plane in
half, with two layers the classiﬁer catches the circular
shape. As we consider more layers, the single-qubit
classiﬁer readjust the circle to match the correct radius.
Figure 6: Results of the circle classiﬁcation obtained with a
single-qubit classiﬁer with diﬀerent number of layers using the
L-BFGS-B minimizer and the weighted ﬁdelity cost function.
With one layer, the best that the classiﬁer can do is to divide
the plane in half.
With two layers, it catches the circular
shape which is readjusted as we consider more layers.
Classiﬁcation of multiple patterns
We want to show now that the single-qubit classiﬁer
can solve multi-class problems. We divide a 2D plane
into several regions and assign a label to each one.
We propose the following division: three regions corresponding to three circular sectors and the intermediate space between them. We call this problem the
3-circles problem. This is a hardly non-linear problem and, consequently, diﬃcult to solve for a classical
neural network in terms of computational power.
Accepted in Quantum 2020-01-27, click title to verify. Published under CC-BY 4.0.
Table 2: Results of the single- and multi-qubit classiﬁers with data re-uploading for the 3-circles problem. Numbers indicate
the success rate, i.e. number of data points classiﬁed correctly over total number of points. Words “Ent.” and “No Ent.”
refer to considering entanglement between qubits or not, respectively. We have used the L-BFGS-B minimization method with
the weighted ﬁdelity and ﬁdelity cost functions. Weighted ﬁdelity cost function presents better results than the ﬁdelity cost
function. The multi-qubit classiﬁer reaches 0.90 success rate with a lower number of layers than the single-qubit classiﬁer.
The introduction of entanglement slightly increases the success rate respect the non-entangled circuit.
Table 2 shows the results for this four-class problem. For a single-qubit classiﬁer, a maximum of 92%
of success is achieved with 10 layers, i.e. 54 parameters. From these results, it seems that this problem
also saturates around 91% of success. However, the
introduction of more qubits and entanglement makes
possible this result possible with less parameters. For
two qubits with entanglement, 4 layers are necessary
to achieve the same success as with a single-qubit, i.e.
34 parameters. For four qubits without entanglement
4 layers are also required. Notice also that, although
the number of parameters increases signiﬁcantly with
the number of qubits, some of the eﬀective operations
are performed in parallel.
There is an eﬀect that arises from this more complex classiﬁcation problem: local minima. Notice that
the success rate can decrease when we add more layers
into our quantum classiﬁer.
As with the previous problem, it is interesting to
compare the performance in terms of sucess rate of
classiﬁers with diﬀerent number of layers. Figure 7
shows the results for a two-qubit classiﬁer with no entanglement for 1, 3, 4 and 10 layers. Even with only
one layer, the classiﬁer identiﬁes the four regions, being the more complicated to describe the central one.
As we consider more layers, the classiﬁer performs
better and adjust these four regions.
Classiﬁcation in multiple dimensions
As explained in Section 2, there is no restriction in
uploading multidimensional data. We can upload up
to three values per rotation since this is the degrees of
freedom of a SU(2) matrix. If the dimension of data is
larger than that, we can just split the data vector into
subsets and upload each one at a time, as described
explicitly in Eq. (6). Therefore, there is no reason to
limit the dimension of data to the number of degrees
of freedom of a qubit. We can in principle upload any
Figure 7: Results for the 3-circles problem using a singlequbit classiﬁer trained with the L-BFGS-B minimizer and the
weighted ﬁdelity cost function. With one layer, the classiﬁer
intuits the four regions although the central one is diﬃcult
to tackle.
With more layers, this region is clearer for the
classiﬁer and it tries to adjust the circular regions.
kind of data if we apply enough gates.
Following this idea we will now move to a more
complicated classiﬁcation using data with 4 coordinates.
We use as a problem the four-dimensional
sphere, i.e.
classifying data points according to
4 < 2/π. Similarly with the previous
problems, xi ∈[−1, 1] and the radius has been chosen
such that the volume of the hypersphere is half of the
total volume. This time, we will take 1000 random
points as the training set because the total volume
Accepted in Quantum 2020-01-27, click title to verify. Published under CC-BY 4.0.
Table 3: Results of the single- and multi-qubit classiﬁers with data re-uploading for the four-dimensional hypersphere problem.
Numbers indicate the success rate, i.e. the number of data points classiﬁed correctly over the total number of points. Words
“Ent.” and “No Ent.” refer to considering entanglement between qubits or not, respectively. We have used the L-BFGS-B
minimization method with the weighted ﬁdelity and ﬁdelity cost functions. The ﬁdelity cost function gets stuck in some local
minima for the multi-qubit classiﬁers. The results obtained with the weighted ﬁdelity cost function are much better, reaching
the 0.98 with only two layers for the four-qubit classiﬁer. Here, the introduction of entanglement improves signiﬁcantly the
performance of the multi-qubit classiﬁer.
Table 4: Results of the single- and multi-qubit classiﬁers with data re-uploading for the three-class annulus problem. Numbers
indicate the success rate, i.e. the number of data points classiﬁed correctly over the total number of points. Words “Ent.” and
“No Ent.” refer to considering entanglement between qubits or not, respectively. We have used the L-BFGS-B minimization
method with the weighted ﬁdelity and ﬁdelity cost functions. The weighted ﬁdelity cost function presents better success rates
than the ﬁdelity cost function. The multi-qubit classiﬁers improve the results obtained with the single-qubit classiﬁer but the
using of entanglement does not introduce signiﬁcant changes.
increases.
Results are shown in Table 3.
A single-qubit
achieves 97% of success with eight layers (82 parameters) using the weighted ﬁdelity cost function. Results
are better if we consider more qubits. For two qubits,
the best result is 98% and it only requires three entangled layers (62 parameters).
For four qubits, it
achieves 98% success rate with two layers with entanglement, i.e. 82 parameters.
Classiﬁcation of non-convex ﬁgures
As a ﬁnal benchmark, we propose the classiﬁcation of
a non-convex pattern. In particular, we classify the
points of an annulus with radii r1 =
0.8 −2/π and
We ﬁx three classes: points inside the
small circle, points in the annulus and points outside
the big circle. So, besides it being a non-convex classiﬁcation task, it is also a multi-class problem. A simpler example, with binary classiﬁcation, can be found
in Appendix B.
The results are shown in Table 4. It achieves 93% of
success with a single-qubit classiﬁer with 10 layers and
a weighted ﬁdelity cost function. With two qubits, it
achieves better results, 94% with three layers. With
four qubits, it reaches a 96% success rate with only
two layers with entanglement.
It is interesting to observe how the single-qubit classiﬁer attempts to achieve the maximum possible results as we consider more and more layers. Figure 8
shows this evolution in terms of the number of layers
for a single-qubit classiﬁer trained with the weighted
ﬁdelity cost function. It requires four layers to learn
that there are three concentric patterns and the addition of more layers adjusts these three regions.
Comparison with classical classiﬁers
It is important to check if our proposal is in some
sense able to compete with actual technology of su-
Accepted in Quantum 2020-01-27, click title to verify. Published under CC-BY 4.0.
Figure 8: Results obtained with the single-qubit classiﬁer for the annulus problem, using the weighted ﬁdelity cost function
during the training. The better results are obtained with a 10 layers classiﬁer (93% of success rate). As we consider more
qubits and entanglement, we can increase the success rate up to 96%, as shows Table 4.
pervised machine learning. To do so we have used the
standard machine learning library scikit-learn 
and solved the same problems as we have solved with
the quantum classiﬁer.
We have included the four
problems presented in the main paper plus ﬁve extra
problems analyzed in Appendix B. The aim of this
classical benchmarking is not to make an extended
review of what classical machine learning is capable
to perform. The aim is to compare our simple quantum classiﬁer to simple models such as shallow neural
networks and simple support vector machines.
The technical details of the classical classiﬁcation
are the following: the neural network has got one hidden layer with 100 neurons, a ReLu activation function and the solver lbfgs by scikit-learn. The support vector machine is the default sklearn.svm.SVC.
Some changes in the initialization parameters were
tested with no signiﬁcant diﬀerences.
Table 5 compares the best performance of a neural
network, support vector classiﬁer (SVC), the singlequbit classiﬁer with ﬁdelity cost function and singlequbit classiﬁer with a weighted ﬁdelity cost function.
In all problems, the performance of the single-qubit
classiﬁer is, at least, comparable with the classical
methods. In some problems, like the 3-circles problem
and the binary annulus problem, the results of the
single-qubit classiﬁer are better than with the classical
Conclusions
We have proposed a single-qubit classiﬁer that can
represent multidimensional complex ﬁgures. The core
of this quantum classiﬁer is the data re-uploading.
This formalism allows circumventing the limitations
of the no-cloning theorem to achieve a correct generalization of an artiﬁcial neural network with a single
layer. In that sense, we have applied the Universal
Approximation Theorem to prove the universality of
a single-qubit classiﬁer.
The structure of this classiﬁer is the following.
Data and processing parameters are uploaded multiple times along the circuit by using one-qubit rotations. The processing parameters of these rotations
are diﬀerent at each upload and should be optimized
using a classical minimization algorithm. To do so, we
have deﬁned two cost functions: one inspired in the
traditional neural networks cost functions (weighted
ﬁdelity cost function) and the other, simpler, consisting of the computation of the ﬁdelity of the ﬁnal state
with respect to a target state.
These target states
are deﬁned to be maximally orthogonal among themselves. Then, the single-qubit classiﬁer ﬁnds the optimal rotations to separate the data points into diﬀerent
regions of the Bloch sphere, each one corresponding
with a particular class.
The single-qubit classiﬁer can be generalized to a
larger number of qubits. This allows the introduction
of entanglement between these qubits by adding twoqubit gates between each layer of rotations. We use
a particular entangling ansantz as a proof of concept.
Accepted in Quantum 2020-01-27, click title to verify. Published under CC-BY 4.0.
Classical classiﬁers
Quantum classiﬁer
Hypersphere
Non-Convex
Binary annulus
Wavy Lines
Table 5: Comparison between single-qubit quantum classiﬁer and two well-known classical classiﬁcation techniques: a neural
network (NN) with a single hidden layer composed of 100 neurons and a support vector classiﬁer (SVC), both with the default
parameters as deﬁned in scikit-learn python package. We analyze nine problems: the ﬁrst four are presented in Section 6
and the remaining ﬁve in Appendix B. Results of the single-qubit quantum classiﬁer are obtained with the ﬁdelity and weighted
ﬁdelity cost functions, χ2
wf deﬁned in Eq. (7) and Eq. (9) respectively. This table shows the best success rate, being
1 the perfect classiﬁcation, obtained after running ten times the NN and SVC algorithms and the best results obtained with
single-qubit classiﬁers up to 10 layers.
The exploration of other possible ansatzes is out of
the scope of this work.
We have benchmarked several quantum classiﬁers
of this kind, made of a diﬀerent number of layers,
qubits and with and without entanglement. The patterns chosen to test these classiﬁers are the points
inside and outside of a circle (simple example) and
similarly for a four-dimensional hypersphere (multidimensional example); a two dimensional region composed by three circles of diﬀerent size (multiple classes
example); and the points outside and inside of an annulus (non-convex example). In all cases, the singlequbit classiﬁer achieves more than 90% of the success
rate. The introduction of more qubits and entanglement increases this success and reduces the number
of layers required. The weighted ﬁdelity cost function
turns out to be more convenient to achieve better results than the ﬁdelity cost function. In all problems,
the probability to get stuck in a local minima increases
with the number of layers, an expected result from an
optimization problem involving several parameters.
In summary, we have proposed a quantum classiﬁer
model that seems to be universal by exploiting the
non-linearities of the single-qubit rotational gates and
by re-uploading data several times.
Acknowledgements
APS and JIL acknowledge CaixaBank for its support
of this work through Barcelona Supercomputing Center project CaixaBank Computaci´on Cu´antica. The
authors acknowledge the interesting discussions with
Quantic group team members. The authors also acknowledge Xanadu Quantum Computing, in particular Shahnawaz Ahmed, for writing a tutorial of this
quantum classiﬁer using its full-stack library Pennylane .