Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2283–2288,
Austin, Texas, November 1-5, 2016. c⃝2016 Association for Computational Linguistics
Supervised Attentions for Neural Machine Translation
Zhiguo Wang
Abe Ittycheriah
T.J. Watson Research Center
1101 Kitchawan Rd, Yorktown Heights, NY 10598
{hmi, zhigwang, abei}@us.ibm.com
In this paper, we improve the attention or
alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs.
We simply compute
the distance between the machine attentions
and the “true” alignments, and minimize this
cost in the training procedure.
Our experiments on large-scale Chinese-to-English task
show that our model improves both translation and alignment qualities signiﬁcantly over
the large-vocabulary neural machine translation system, and even beats a state-of-the-art
traditional syntax-based system.
Introduction
Neural machine translation (NMT) has gained popularity in recent two years , especially for the attentionbased models of Bahdanau et al. .
The attention model plays a crucial role in NMT,
as it shows which source word(s) the model should
focus on in order to predict the next target word.
However, the attention or alignment quality of NMT
is still very low .
In this paper, we alleviate the above issue by utilizing the alignments (human annotated data or machine alignments) of the training set.
alignments of all the training sentence pairs, we add
an alignment distance cost to the objective function. Thus, we not only maximize the log translation
probabilities, but also minimize the alignment distance cost. Large-scale experiments over Chineseto-English on various test sets show that our best
method for a single system improves the translation quality signiﬁcantly over the large vocabulary
NMT system (Section 5) and beats the state-of-theart syntax-based system.
Neural Machine Translation
As shown in Figure 1, attention-based NMT is an encoder-decoder network.
the encoder employs a bi-directional recurrent neural network to encode the source sentence x =
(x1, ..., xl), where l is the sentence length (including the end-of-sentence ⟨eos⟩), into a sequence of
hidden states h = (h1, ..., hl), each hi is a concatenation of a left-to-right −→
hi and a right-to-left ←−
Given h, the decoder predicts the target translation by maximizing the conditional log-probability
of the correct translation y∗= (y∗
m is the sentence length (including the end-ofsentence). At each time t, the probability of each
word yt from a target vocabulary Vy is:
p(yt|h, y∗
1) = g(st, y∗
where g is a two layer feed-forward neural network
over the embedding of the previous word y∗
the hidden state st. The st is computed as:
st = q(st−1, y∗
i=1 (αt,i · ←−h i)
i=1 (αt,i · −→h i)
where q is a gated recurrent units, Ht is a weighted
sum of h; the weights, α, are computed with a two
layer feed-forward neural network r:
exp{r(st−1, hi, y∗
k=1 exp{r(st−1, hk, y∗
We put all αt,i (t = 1...m, i = 1...l) into a matrix
A′, we have a matrix (alignment) like (c) in Figure 2,
where each row (for each target word) is a probability distribution over the source sentence x.
The training objective is to maximize the conditional log-probability of the correct translation y∗
(↵ti · −h i)
(↵ti · −!h i)
i=1 exp(et,i)
Figure 1: The architecture of attention-based NMT . The source sentence x = (x1, ..., xl) with length l,
xl is an end-of-sentence token ⟨eos⟩on the source side. The reference translation is y∗= (y∗
1, ..., y∗
m) with length m, similarly,
m is the target side ⟨eos⟩. ←−
hi are bi-directional encoder states. αt,j is the attention probability at time t, position j. Ht
is the weighted sum of encoding states. st is a hidden state. ot is an output state. Another one layer neural network projects ot to
the target output vocabulary, and conducts softmax to predict the probability distribution over the output vocabulary. The attention
model (the right box) is a two layer feedforward neural network, At,j is an intermediate state, then another layer converts it into a
real number et,j, the ﬁnal attention probability at position j is αt,j.
given x with respect to the parameters θ
θ∗= arg max
t |xn, y∗n
where n is the n-th sentence pair (xn, y∗n) in the
training set, N is the total number of pairs.
Alignment Component
The attentions, αt,1...αt,l, in each step t play an important role in NMT. However, the accuracy is still
far behind the traditional MaxEnt alignment model
in terms of alignment F1 score . Thus, in this section, we explicitly add
an alignment distance to the objective function in
Eq. 5. The “truth” alignments for each sentence pair
can be from human annotated data, unsupervised or
supervised alignments or MaxEnt ).
Given an alignment matrix A for a sentence pair
(x, y) in Figure 2 (a), where we have an end-ofsource-sentence token ⟨eos⟩= xl, and we align all
the unaligned target words (y∗
3 in this example) to
⟨eos⟩, also we force y∗
m (end-of-target-sentence) to
be aligned to xl with probability one. Then we conduct two transformations to get the probability distribution matrices ((b) and (c) in Figure 2).
Simple Transformation
The ﬁrst transformation simply normalizes each
row. Figure 2 (b) shows the result matrix A∗. The
last column in red dashed lines shows the alignments
of the special end-of-sentence token ⟨eos⟩.
Smoothed Transformation
Given the original alignment matrix A, we create a
matrix A∗with all points initialized with zero. Then,
for each alignment point At,i = 1, we update A∗
by adding a Gaussian distribution, g(µ, σ), with a
window size w (t-w, ... t ... t+w). Take the A1,1 = 1
for example, we have A∗1,1 += 1, A∗1,2 += 0.61, and
A∗1,3 += 0.14 with w=2, g(µ, σ)=g(0, 1). Then we
normalize each row and get (c). In our experiments,
we use a shape distribution, where σ = 0.5.
Objectives
Alignment Objective: Given the “true” alignment
A∗, and the machine attentions A′ produced by
NMT model, we compute the Euclidean distance
bewteen A∗and A′.
d(A′, A∗) =
(A′t,i −A∗t,i)2.
0.57 0.35 0.08
0.39 0.39 0.18 0.04
0.26 0.42 0.26 0.06
0.06 0.26 0.42
Figure 2: Alignment transformation. A special token, ⟨eos⟩, is introduced to the source sentence, we align all the unaligned target
3 in this case) to ⟨eos⟩. (a): the original alignment matrix A from GIZA++ or MaxEnt aligner. (b): simple normalization
by rows (probability distribution over the source sentence x). (c): smoothed transformation followed by normalization by rows,
and typically, we always align end-of-source-sentence xl to end-of-target-sentence ym by probability one.
NMT Objective: We plug Eq. 6 to Eq. 5, we have
θ∗= arg max
t |xn, y∗n
−d(A′n, A∗n)
There are two parts: translation and alignment, so
we can optimize them jointly, or separately (e.g. we
ﬁrst optimize alignment only, then optimize translation). Thus, we divide the network in Figure 1 into
alignment A and translation T parts:
• A: all networks before the hidden state st,
• T: the network g adapted the agreementbased learning , and introduced a combined objective
that takes into account both translation directions
(source-to-target and target-to-source) and an agreement term between the two alignment directions.
By contrast, our approach directly uses and optimizes NMT parameters using the “supervised”
alignments.
Experiments
Data Preparation
We run our experiments on Chinese to English task.
The training corpus consists of approximately 5 million sentences available within the DARPA BOLT
Chinese-English task. The corpus includes a mix of
newswire, broadcast news, and webblog. We do not
include HK Law, HK Hansard and UN data. The
Chinese text is segmented with a segmenter trained
on CTB data using conditional random ﬁelds (CRF).
Our development set is the concatenation of several tuning sets (GALE Dev, P1R6 Dev, and Dev
12) initially released under the DARPA GALE program. The development set is 4491 sentences in total. Our test sets are NIST MT06 (1664 sentences)
, MT08 news (691 sentences), and MT08 web (666
sentences).
For all NMT systems, the full vocabulary size of
the training set is 300k. In the training procedure,
we use AdaDelta to update model
parameters with a mini-batch size 80.
Mi et al. , the output vocabulary for each
mini-batch or sentence is a sub-set of the full vocabulary. For each source sentence, the sentencelevel target vocabularies are union of top 2k most
frequent target words and the top 10 candidates of
the word-to-word/phrase translation tables learned
single system
Tree-to-string
0.95 34.93
0.94 31.12 12.90 0.90 23.45 17.72
Cov. LVNMT 
0.92 35.59 10.71 0.89 30.18 15.33 0.97 27.48 16.67
+Alignment
0.95 35.71 10.38 0.93 30.73 14.98 0.96 27.38 16.24
0.95 28.59 16.99 0.92 24.09 20.89 0.97 20.48 23.31
0.95 35.95 10.24 0.92 30.95 14.62 0.97 26.76 17.04
0.96 36.76
0.94 31.24 14.80 0.96 28.35 15.61
0.96 36.44 10.16 0.94 30.66 15.01 0.96 26.67 16.72
0.95 36.80
0.93 31.74 14.02 0.96 27.53 16.21
0.96 36.95
0.94 32.43 13.61 0.97 28.63 15.80
Table 1: Single system results in terms of (TER-BLEU)/2 (T-B, the lower the better) on 5 million Chinese to English training set.
BP denotes the brevity penalty. NMT results are on a large vocabulary (300k) and with UNK replaced. The second column shows
different alignments (Zh →En (one direction), GDFA (“grow-diag-ﬁnal-and”), and MaxEnt . A,
T, and J mean optimize alignment only, translation only, and jointly. Gau. denotes the smoothed transformation.
from ‘fast align’ . The maximum
length of a source phrase is 4. In the training time,
we add the reference in order to make the translation
reachable.
The Cov. LVNMT system is a re-implementation
of the enhanced NMT system of Mi et al. ,
which employs a coverage embedding model and
achieves better performance over large vocabulary
NMT Jean et al. . The coverage embedding
dimension of each source word is 100.
Following Jean et al. , we dump the alignments, attentions, for each sentence, and replace
UNKs with the word-to-word translation model or
the aligned source word.
Our SMT system is a hybrid syntax-based tree-tostring model , a simpli-
ﬁed version of the joint decoding . We parse the Chinese side
with Berkeley parser, and align the bilingual sentences with GIZA++ and MaxEnt. and extract Hiero and tree-to-string rules on the training set. Our
two 5-gram language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07), respectively.As suggested by
Zhang , NMT systems can achieve better results with the help of those monolingual corpora. In
this paper, our NMT systems only use the bilingual
data. We tune our system with PRO to minimize (TER- BLEU)/2 1 on the development set.
Translation Results
Table 1 shows the translation results of all systems. The syntax-based statistical machine translation model achieves an average (TER-BLEU)/2 of
13.36 on three test sets. The Cov. LVNMT system
achieves an average (TER-BLEU)/2 of 14.24, which
is about 0.9 points worse than Tree-to-string SMT
system. Please note that all systems are single systems. It is highly possible that ensemble of NMT
systems with different random seeds can lead to better results over SMT.
We test three different alignments:
• Zh →En (one direction of GIZA++),
• GDFA (the “grow-diag-ﬁnal-and” heuristic
merge of both directions of GIZA++),
• MaxEnt (trained on 67k hand-aligned sentences).
1The metric used for optimization in this work is (TER-
BLEU)/2 to prevent the system from using sentence length alone
to impact BLEU or TER. Typical SMT systems use target word
count as a feature and it has been observed that BLEU can be
optimized by tweaking the weighting of the target word count
with no improvement in human assessments of translation quality. Conversely, in order to optimize TER shorter sentences can
be produced. Optimizing the combination of metrics alleviates
this effect .
The alignment quality improves from Zh →En to
MaxEnt. We also test different optimization strategies: J (jointly), A (alignment only), and T (translation model only). A combination, A →T, shows
that we optimize A only ﬁrst, then we ﬁx A and only
update T part. Gau. denotes the smoothed transformation (Section 3.2). Only the last row uses the
smoothed transformation, all others use the simple
transformation.
Experimental results in Table 1 show some interesting results. First, with the same alignment, J
joint optimization works best than other optimization strategies (lines 3 to 6). Unfortunately, breaking down the network into two separate parts (A and
T) and optimizing them separately do not help (lines
3 to 5). We have to conduct joint optimization J in
order to get a comparable or better result (lines 3, 5
and 6) over the baseline system.
Second, when we change the training alignment
seeds (Zh →En, GDFA, and MaxEnt) NMT model
does not yield signiﬁcant different results (lines 6 to
Third, the smoothed transformation (J + Gau.)
gives some improvements over the simple transformation (the last two lines), and achieves the best
result (1.2 better than LVNMT, and 0.3 better than
Tree-to-string). In terms of BLEU scores, we conduct the statistical signiﬁcance tests with the signtest of Collins et al. , the results show that the
improvements of our J + Gau. over LVNMT are
signiﬁcant on three test sets (p < 0.01).
At last, the brevity penalty (BP) consistently gets
better after we add the alignment cost to NMT objective. Our alignment objective adjusts the translation
length to be more in line with the human references
accordingly.
Alignment Results
Table 2 shows the alignment F1 scores on the alignment test set (447 hand aligned sentences).
MaxEnt model is trained on 67k hand-aligned sentences, and achieves an F1 score of 75.96. For NMT
systems, we dump the alignment matrixes and convert them into alignments with following steps. For
each target word, we sort the alphas and add the max
probability link if it is higher than 0.2. If we only
tune the alignment component (A in line 3), we improve the alignment F1 score from 45.76 to 47.87.
74.86 77.10 75.96
Cov LVNMT 51.11 41.42 45.76
+Alignment
50.88 45.19 47.87
53.18 49.37 51.21
50.29 44.90 47.44
53.71 49.33 51.43
54.29 48.02 50.97
53.88 48.25 50.91
44.42 55.25 49.25
48.90 55.38 51.94
Table 2: Alignment F1 scores of different models.
And we further boost the score to 50.97 by tuning
alignment and translation jointly (J in line 7). Interestingly, the system using MaxEnt produces more
alignments in the output, and results in a higher recall. This suggests that using MaxEnt can lead to a
sharper attention distribution, as we pick the alignment links based on the probabilities of attentions,
the sharper the distribution is, more links we can
pick. We believe that a sharp attention distribution
is a great property of NMT.
Again, the best result is J + Gau. in the last row,
which signiﬁcantly improves the F1 by 5 points over
the baseline Cov. LVNMT system. When we use
MaxEnt alignments, J + Gau. smoothing gives us
about 1.7 points gain over J system. So it looks interesting to run another J + Gau. over GDFA alignment.
Together with the results in Table 1, we conclude
that adding the alignment cost to the training objective helps both translation and alignment significantly.
Conclusion
In this paper, we utilize the “supervised” alignments,
and put the alignment cost to the NMT objective
function. In this way, we directly optimize the attention model in a supervised way.
Experiments
show signiﬁcant improvements in both translation
and alignment tasks over a very strong LVNMT system.
Acknowledgment
We thank the anonymous reviewers for their useful