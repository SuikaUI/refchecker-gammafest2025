Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2587–2597
Melbourne, Australia, July 15 - 20, 2018. c⃝2018 Association for Computational Linguistics
Attacking Visual Language Grounding with Adversarial Examples:
A Case Study on Neural Image Captioning
Hongge Chen1*, Huan Zhang23*, Pin-Yu Chen3, Jinfeng Yi4, and Cho-Jui Hsieh2
1MIT, Cambridge, MA 02139, USA
2UC Davis, Davis, CA 95616, USA
3IBM Research, NY 10598, USA
4JD AI Research, Beijing, China
 , 
 , , 
*Hongge Chen and Huan Zhang contribute equally to this work
Visual language grounding is widely studied in modern neural image captioning systems, which typically adopts an
encoder-decoder framework consisting of
two principal components:
a convolutional neural network (CNN) for image
feature extraction and a recurrent neural
network (RNN) for language caption generation. To study the robustness of language grounding to adversarial perturbations in machine vision and perception,
we propose Show-and-Fool, a novel algorithm for crafting adversarial examples
in neural image captioning.
The proposed algorithm provides two evaluation
approaches, which check whether neural
image captioning systems can be mislead
to output some randomly chosen captions
or keywords. Our extensive experiments
show that our algorithm can successfully
craft visually-similar adversarial examples
with randomly targeted captions or keywords, and the adversarial examples can
be made highly transferable to other image
captioning systems. Consequently, our approach leads to new robustness implications of neural image captioning and novel
insights in visual language grounding.
Introduction
In recent years, language understanding grounded
in machine vision and perception has made remarkable progress in natural language processing
(NLP) and artiﬁcial intelligence (AI), such as image captioning and visual question answering. Image captioning is a multimodal learning task and
has been used to study the interaction between language and vision models . It
takes an image as an input and generates a language caption that best describes its visual contents, and has many important applications such
as developing image search engines with complex
natural language queries, building AI agents that
can see and talk, and promoting equal web access for people who are blind or visually impaired.
Modern image captioning systems typically adopt
an encoder-decoder framework composed of two
principal modules: a convolutional neural network
(CNN) as an encoder for image feature extraction
and a recurrent neural network (RNN) as a decoder
for caption generation.
This CNN+RNN architecture includes popular image captioning models such as Show-and-Tell ,
Show-Attend-and-Tell and NeuralTalk .
Recent studies have highlighted the vulnerability of CNN-based image classiﬁers to adversarial
examples: adversarial perturbations to benign images can be easily crafted to mislead a well-trained
classiﬁer, leading to visually indistinguishable adversarial examples to human .
In this study, we investigate a more challenging problem in visual
language grounding domain that evaluates the robustness of multimodal RNN in the form of a
CNN+RNN architecture, and use neural image
captioning as a case study. Note that crafting adversarial examples in image captioning tasks is
strictly harder than in well-studied image classiﬁcation tasks, due to the following reasons: (i) class
attack v.s.
caption attack: unlike classiﬁcation
tasks where the class labels are well deﬁned, the
output of image captioning is a set of top-ranked
captions. Simply treating different captions as distinct classes will result in an enormous number
of classes that can even precede the number of
training images. In addition, semantically similar
Figure 1: Adversarial examples crafted by Showand-Fool using the targeted caption method. The
target captioning model is Show-and-Tell , the original images are selected from
the MSCOCO validation set, and the targeted captions are randomly selected from the top-1 inferred
caption of other validation images.
captions can be expressed in different ways and
hence should not be viewed as different classes;
and (ii) CNN v.s.
CNN+RNN: attacking RNN
models is signiﬁcantly less well-studied than attacking CNN models. The CNN+RNN architecture is unique and beyond the scope of adversarial
examples in CNN-based image classiﬁers.
In this paper, we tackle the aforementioned
challenges by proposing a novel algorithm called
Show-and-Fool.
We formulate the process of
crafting adversarial examples in neural image captioning systems as optimization problems with
novel objective functions designed to adopt the
CNN+RNN architecture. Speciﬁcally, our objective function is a linear combination of the distortion between benign and adversarial examples
as well as some carefully designed loss functions.
The proposed Show-and-Fool algorithm provides
two approaches to craft adversarial examples in
neural image captioning under different scenarios:
1. Targeted caption method: Given a targeted
caption, craft adversarial perturbations to any
image such that its generated caption matches
the targeted caption.
2. Targeted keyword method: Given a set of
keywords, craft adversarial perturbations to
any image such that its generated caption
contains the speciﬁed keywords.
The captioning model has the freedom to make sentences with target keywords in any order.
As an illustration, Figure 1 shows an adversarial
example crafted by Show-and-Fool using the targeted caption method. The adversarial perturbations are visually imperceptible while can successfully mislead Show-and-Tell to generate the targeted captions. Interestingly and perhaps surprisingly, our results pinpoint the Achilles heel of the
language and vision models used in the tested image captioning systems. Moreover, the adversarial examples in neural image captioning highlight
the inconsistency in visual language grounding between humans and machines, suggesting a possible weakness of current machine vision and perception machinery. Below we highlight our major
contributions:
• We propose Show-and-Fool, a novel optimization based approach to crafting adversarial examples in image captioning. We provide two
types of adversarial examples, targeted caption
and targeted keyword, to analyze the robustness
of neural image captioners. To the best of our
knowledge, this is the very ﬁrst work on crafting adversarial examples for image captioning.
• We propose powerful and generic loss functions
that can craft adversarial examples and evaluate
the robustness of the encoder-decoder pipelines
in the form of a CNN+RNN architecture. In particular, our loss designed for targeted keyword
attack only requires the adversarial caption to
contain a few speciﬁed keywords; and we allow the neural network to make meaningful sentences with these keywords on its own.
• We conduct extensive experiments on the
MSCOCO dataset. Experimental results show
that our targeted caption method attains a 95.8%
attack success rate when crafting adversarial examples with randomly assigned captions. In addition, our targeted keyword attack yields an
even higher success rate.
We also show that
attacking CNN+RNN models is inherently different and more challenging than only attacking
CNN models.
• We also show that Show-and-Fool can produce
highly transferable adversarial examples:
adversarial image generated for fooling Showand-Tell can also fool other image captioning
models, leading to new robustness implications
of neural image captioning systems.
Related Work
In this section, we review the existing work on visual language grounding, with a focus on neural
image captioning. We also review related work
on adversarial attacks on CNN-based image classiﬁers. Due to space limitations, we defer the second part to the supplementary material.
Visual language grounding represents a family of multimodal tasks that bridge visual and
natural language understanding.
Typical examples include image and video captioning , visual dialog , visual question answering , visual storytelling , natural question generation , and image generation from captions . In this paper, we focus on
studying the robustness of neural image captioning
models, and believe that the proposed method also
sheds lights on robustness evaluation for other visual language grounding tasks using a similar multimodal RNN architecture.
Many image captioning methods based on deep
neural networks (DNNs) adopt a multimodal RNN
framework that ﬁrst uses a CNN model as the
encoder to extract a visual feature vector, followed by a RNN model as the decoder for caption generation. Representative works under this
framework include , which are mainly differed by the underlying CNN and RNN architectures, and whether
or not the attention mechanisms are considered.
Other lines of research generate image captions
using semantic information or via a compositional
approach .
The recent work in 
touched upon the robustness of neural image captioning for language grounding by showing its insensitivity to one-word (foil word) changes in the
language caption, which corresponds to the untargeted attack category in adversarial examples. In
this paper, we focus on the more challenging targeted attack setting that requires to fool the captioning models and enforce them to generate prespeciﬁed captions or keywords.
Methodology of Show-and-Fool
Overview of the Objective Functions
We now formally introduce our approaches to
crafting adversarial examples for neural image
captioning. The problem of ﬁnding an adversarial example for a given image I can be cast as the
following optimization problem:
c · loss(I + δ) + ∥δ∥2
I + δ ∈[−1, 1]n.
Here δ denotes the adversarial perturbation to I.
2 = ∥(I + δ) −I∥2
2 is an ℓ2 distance metric
between the original image and the adversarial image. loss(·) is an attack loss function which takes
different forms in different attacking settings. We
will provide the explicit expressions in Sections
3.2 and 3.3. The term c > 0 is a pre-speciﬁed regularization constant. Intuitively, with larger c, the
attack is more likely to succeed but at the price of
higher distortion on δ. In our algorithm, we use
a binary search strategy to select c. The box constraint on the image I ∈[−1, 1]n ensures that the
adversarial example I + δ ∈[−1, 1]n lies within a
valid image space.
For the purpose of efﬁcient optimization, we
convert the constrained minimization problem in
(1) into an unconstrained minimization problem
by introducing two new variables y ∈Rn and
w ∈Rn such that
y = arctanh(I) and w = arctanh(I + δ) −y,
where arctanh denotes the inverse hyperbolic tangent function and is applied element-wisely. Since
tanh(yi + wi) ∈[−1, 1], the transformation will
automatically satisfy the box constraint. Consequently, the constrained optimization problem in
(1) is equivalent to
c · loss(tanh(w + y))
+∥tanh(w + y) −tanh(y)∥2
In the following sections, we present our designed
loss functions for different attack settings.
Targeted Caption Method
Note that a targeted caption is denoted by
S = (S1, S2, ..., St, ..., SN),
where St indicates the index of the t-th word in
the vocabulary list V, S1 is a start symbol and SN
indicates the end symbol. N is the length of caption S, which is not ﬁxed but does not exceed a
predeﬁned maximum caption length. To encourage the neural image captioning system to output
the targeted caption S, one needs to ensure the log
probability of the caption S conditioned on the image I + δ attains the maximum value among all
possible captions, that is,
log P(S|I + δ) = max
S′∈Ωlog P(S′|I + δ),
where Ωis the set of all possible captions. It is
also common to apply the chain rule to the joint
probability and we have
log P(S′|I+δ) =
1, ..., S′
captioning
t|I + δ, S′
1, ..., S′
t−1) is usually computed
by a RNN/LSTM cell f, with its hidden state ht−1
and input S′
zt = f(ht−1, S′
t−1) and pt = softmax(zt),
where zt := [z(1)
, ..., z(|V|)
] ∈R|V| is a vector of the logits (unnormalized probabilities) for
each possible word in the vocabulary. The vector
pt represents a probability distribution on V with
each coordinate p(i)
deﬁned as:
t = i|I + δ, S′
1, ..., S′
Following the deﬁnition of softmax function:
1, ..., S′
t−1) = exp(z(S′
Intuitively, to maximize the targeted caption’s
probability, we can directly use its negative log
probability (5) as a loss function. The inputs of
the RNN are the ﬁrst N −1 words of the targeted
caption (S1, S2, ..., SN−1).
lossS,log-prob(I + δ) = −log P(S|I + δ)
log P(St|I + δ, S1, ..., St−1).
Applying (5) to (2), the formulation of targeted
caption method given a targeted caption S is:
w∈Rnc · lossS,log prob(tanh(w + y))
+ ∥tanh(w + y) −tanh(y)∥2
Alternatively, using the deﬁnition of the softmax function,
log P(S′|I + δ) =
−constant,
(3) can be simpliﬁed as
log P(S|I + δ) ∝
Instead of making each z(St)
as large as possible, it is sufﬁcient to require the target word St
to attain the largest (top-1) logit (or probability)
among all the words in the vocabulary at position
t. In other words, we aim to minimize the difference between the maximum logit except St, denoted by maxk∈V,k̸=St{z(k)
}, and the logit of St,
denoted by z(St)
. We also propose a ramp function
on top of this difference as the ﬁnal loss function:
lossS,logits(I+δ) =
max{−ϵ, max
k̸=St{z(k)
where ϵ > 0 is a conﬁdence level accounting for
the gap between maxk̸=St{z(k)
} and z(St)
> maxk̸=St{z(k)
} + ϵ, the corresponding
term in the summation will be kept at −ϵ and does
not contribute to the gradient of the loss function,
encouraging the optimizer to focus on minimizing
other terms where z(St)
is not large enough.
Applying the loss (7) to (1), the ﬁnal formulation of targeted caption method given a targeted
caption S is
max{−ϵ, max
k̸=St{z(k)
+ ∥tanh(w + y) −tanh(y)∥2
We note that has reported that in CNN-based image classiﬁcation, using logits in the attack loss function can produce
better adversarial examples than using probabilities, especially when the target network deploys
some gradient masking schemes such as defensive
distillation . Therefore, we
provide both logit-based and probability-based attack loss functions for neural image captioning.
Targeted Keyword Method
In addition to generating an exact targeted caption by perturbing the input image, we offer an
intermediate option that aims at generating captions with speciﬁc keywords, denoted by K :=
{K1, · · · , KM} ⊂V. Intuitively, ﬁnding an adversarial image generating a caption with speciﬁc
keywords might be easier than generating an exact
caption, as we allow more degree of freedom in
caption generation. However, as we need to ensure
a valid and meaningful inferred caption, ﬁnding an
adversarial example with speciﬁc keywords in its
caption is difﬁcult in an optimization perspective.
Our target keyword method can be used to investigate the generalization capability of a neural captioning system given only a few keywords.
In our method, we do not require a target keyword Kj, j ∈[M] to appear at a particular position. Instead, we want a loss function that allows Kj to become the top-1 prediction (plus a
conﬁdence margin ϵ) at any position. Therefore,
we propose to use the minimum of the hinge-like
loss terms over all t ∈[N] as an indication of Kj
appearing at any position as the top-1 prediction,
leading to the following loss function:
lossK,logits =
t∈[N]{max{−ϵ,max
k̸=Kj{z(k)
We note that the loss functions in (4) and (5)
require an input S′
t−1 to predict zt for each t ∈
{2, . . . , N}. For the targeted caption method, we
use the targeted caption S as the input of RNN.
In contrast, for the targeted keyword method we
no longer know the exact targeted sentence, but
only require the presence of speciﬁed keywords in
the ﬁnal caption. To bridge the gap, we use the
originally inferred caption S0 = (S0
1, · · · , S0
from the benign image as the initial input to RNN.
Speciﬁcally, after minimizing (8) for T iterations,
we run inference on I + δ and set the RNN’s input
S1 as its current top-1 prediction, and continue this
process. With this iterative optimization process,
the desired keywords are expected to gradually appear in top-1 prediction.
Another challenge arises in targeted keyword
method is the problem of “keyword collision”.
When the number of keywords M ≥2, more
than one keywords may have large values of
maxk̸=Kj{z(k)
at a same position t. For
example, if dog and cat are top-2 predictions for
the second word in a caption, the caption can either start with “A dog ...” or “A cat ...”. In this
case, despite the loss (8) being very small, a caption with both dog and cat can hardly be generated, since only one word is allowed to appear at
the same position. To alleviate this problem, we
deﬁne a gate function gt,j(x) which masks off all
the other keywords when a keyword becomes top-
1 at position t:
A, if arg maxi∈V z(i)
x, otherwise,
where A is a predeﬁned value that is signiﬁcantly
larger than common logits values. Then (8) becomes:
t∈[N]{gt,j(max{−ϵ, max
k̸=Kj{z(k)
The log-prob loss for targeted keyword method is
discussed in the Supplementary Material.
Experiments
Experimental Setup and Algorithms
We performed extensive experiments to test the effectiveness of our Show-and-Fool algorithm and
study the robustness of image captioning systems
under different problem settings. In our experiments1, we use the pre-trained TensorFlow implementation2 of Show-and-Tell 
1Our source code is available at:
 
huanzhang12/ImageCaptioningAttack
2 
research/im2txt
with Inception-v3 as the CNN for visual feature
extraction. Our testbed is Microsoft COCO (MSCOCO) data set. Although some
more recent neural image captioning systems can
achieve better performance than Show-and-Tell,
they share a similar framework that uses CNN
for feature extraction and RNN for caption generation, and Show-and-Tell is the vanilla version
of this CNN+RNN architecture. Indeed, we ﬁnd
that the adversarial examples on Show-and-Tell
are transferable to other image captioning models such as Show-Attend-and-Tell 
and NeuralTalk23, suggesting that the attention
mechanism and the choice of CNN and RNN architectures do not signiﬁcantly affect the robustness. We also note that since Show-and-Fool is
the ﬁrst work on crafting adversarial examples for
neural image captioning, to the best of our knowledge, there is no other method for comparison.
We use ADAM to minimize our loss functions
and set the learning rate to 0.005. The number of
iterations is set to 1, 000. All the experiments are
performed on a single Nvidia GTX 1080 Ti GPU.
For targeted caption and targeted keyword methods, we perform a binary search for 5 times to ﬁnd
the best c: initially c = 1, and c will be increased
by 10 times until a successful adversarial example
is found. Then, we choose a new c to be the average of the largest c where an adversarial example
can be found and the smallest c where an adversarial example cannot be found. We ﬁx ϵ = 1 except
for transferability experiments. For each experiment, we randomly select 1,000 images from the
MSCOCO validation set. We use BLEU-1 , BLEU-2, BLEU-3, BLEU-
4, ROUGE and METEOR scores to evaluate the correlations
between the inferred captions and the targeted captions. These scores are widely used in NLP community and are adopted by image captioning systems for quality assessment. Throughout this section, we use the logits loss (7)(9). The results of
using the log-prob loss (5) are similar and are reported in the supplementary material.
Targeted Caption Results
Unlike the image classiﬁcation task where all possible labels are predeﬁned, the space of possible
captions in a captioning system is almost inﬁnite.
However, the captioning system is only able to
3 
Table 1: Summary of targeted caption method
(Section 3.2) and targeted keyword method (Section 3.3) using logits loss. The ℓ2 distortion of
adversarial noise ∥δ∥2 is averaged over successful adversarial examples. For comparison, we also
include CNN based attack methods (Section 4.5).
Experiments
Success Rate
targeted caption
C&W on CNN
I-FGSM on CNN
Table 2: Statistics of the 4.2% failed adversarial
examples using the targeted caption method and
logits loss (7). All correlation scores are computed
using the top-5 inferred captions of an adversarial image and the targeted caption (higher score
means better targeted attack performance).
ℓ2 Distortion
output relevant captions learned from the training set. For instance, the captioning model cannot generate a passive-voice sentence if the model
was never trained on such sentences. Therefore,
we need to ensure that the targeted caption lies in
the space where the captioning system can possibly generate. To address this issue, we use the
generated caption of a randomly selected image
(other than the image under investigation) from
MSCOCO validation set as the targeted caption S.
The use of a generated caption as the targeted caption excludes the effect of out-of-domain captioning, and ensures that the target caption is within
the output space of the captioning network.
Here we use the logits loss (7) plus a ℓ2 distortion term (as in (2)) as our objective function. A
successful adversarial example is found if the inferred caption after adding the adversarial perturbation δ is exactly the same as the targeted caption.
In our setting, 1,000 ADAM iterations take about
38 seconds for one image. The overall success
rate and average distortion of adversarial perturbation δ are shown in Table 1. Among all the tested
images, our method attains 95.8% attack success
Moreover, our adversarial examples have
small ℓ2 distortions and are visually identical to
the original images, as displayed in Figure 1. We
also examine the failed adversarial examples and
summarize their statistics in Table 2. We ﬁnd that
their generated captions, albeit not entirely identical to the targeted caption, are in fact highly correlated to the desired one. Overall, the high success
rate and low ℓ2 distortion of adversarial examples
clearly show that Show-and-Tell is not robust to
targeted adversarial perturbations.
Targeted Keyword Results
In this task, we use (9) as our loss function, and
choose the number of keywords M = {1, 2, 3}.
We run an inference step on I + δ every T = 5
iterations, and use the top-1 caption as the input
of RNN/LSTMs. Similar to Section 4.2, for each
image the targeted keywords are selected from the
caption generated by a randomly selected validation set image. To exclude common words like
“a”, “the”, “and”, we look up each word in the
targeted sentence and only select nouns, verbs, adjectives or adverbs. We say an adversarial image is
successful when its caption contains all speciﬁed
keywords. The overall success rate and average
distortion are shown in Table 1. When compared
to the targeted caption method, targeted keyword
method achieves an even higher success rate (at
least 96% for 3-keyword case and at least 97%
for 1-keyword and 2-keyword cases).
shows an adversarial example crafted from our
targeted keyword method with three keywords -
“dog”, “cat” and “frisbee”. Using Show-and-Fool,
the top-1 caption of a cake image becomes “A dog
and a cat are playing with a frisbee” while the adversarial image remains visually indistinguishable
to the original one. When M = 2 and 3, even if we
cannot ﬁnd an adversarial image yielding all speciﬁed keywords, we might end up with a caption
that contains some of the keywords (partial success). For example, when M = 3, Table 3 shows
the number of keywords appeared in the captions
(M′) for those failed examples (not all 3 targeted
keywords are found). These results clearly show
that the 4% failed examples are still partially successful: the generated captions contain about 1.5
targeted keywords on average.
Transferability of Adversarial Examples
It has been shown that in image classiﬁcation
tasks, adversarial examples found for one machine
Figure 2: An adversarial example (∥δ∥2 = 1.284)
of an cake image crafted by the Show-and-Fool
targeted keyword method with three keywords -
“dog”, “cat” and “frisbee”.
Table 3: Percentage of partial success with different c in the 4.0% failed images that do not contain
all the 3 targeted keywords.
learning model may also be effective against another model, even if the two models have different architectures . However, unlike image classiﬁcation where correct labels are made explicit, two
different image captioning systems may generate
quite different, yet semantically similar, captions
for the same benign image.
In image captioning, we say an adversarial example is transferable when the adversarial image found on model
A with a target sentence SA can generate a similar
(rather than exact) sentence SB on model B.
In our setting, model A is Show-and-Tell, and
we choose Show-Attend-and-Tell 
as model B.
The major differences between
Show-and-Tell and Show-Attend-and-Tell are the
addition of attention units in LSTM network for
caption generation, and the use of last convolutional layer (rather than the last fully-connected
layer) feature maps for feature extraction.
use Inception-v3 as the CNN architecture for both
models and train them on the MSCOCO 2014 data
set. However, their CNN parameters are different
due to the ﬁne-tuning process.
Table 4: Transferability of adversarial examples from Show-and-Tell to Show-Attend-and-Tell, using
different ϵ and c. ori indicates the scores between the generated captions of the original images and the
transferred adversarial images on Show-Attend-and-Tell. tgt indicates the scores between the targeted
captions on Show-and-Tell and the generated captions of transferred adversarial images on Show-Attendand-Tell. A smaller ori or a larger tgt value indicates better transferability. mis measures the differences
between captions generated by the two models given the same benign image (model mismatch). When
C = 1000, ϵ = 10, tgt is close to mis, indicating the discrepancy between adversarial captions on the two
models is mostly bounded by model mismatch, and the adversarial perturbation is highly transferable.
Figure 3: A highly transferable adversarial example (∥δ∥2 = 15.226) crafted by Show-and-Tell targeted caption method, transfers to Show-Attendand-Tell, yielding similar adversarial captions.
To investigate the transferability of adversarial
examples in image captioning, we ﬁrst use the targeted caption method to ﬁnd adversarial examples
for 1,000 images in model A with different c and ϵ,
and then transfer successful adversarial examples
(which generate the exact target captions on model
A) to model B. The generated captions by model
B are recorded for transferability analysis. The
transferability of adversarial examples depends on
two factors: the intrinsic difference between two
models even when the same benign image is used
as the input, i.e., model mismatch, and the transferability of adversarial perturbations.
To measure the mismatch between Show-and-
Tell and Show-Attend-and-Tell, we generate captions of the same set of 1,000 original images
from both models, and report their mutual BLEU,
ROUGE and METEOR scores in Table 4 under
the mis column. To evaluate the effectiveness of
transferred adversarial examples, we measure the
scores for two set of captions: (i) the captions of
original images and the captions of transferred adversarial images, both generated by Show-Attendand-Tell (shown under column ori in Table 4); and
(ii) the targeted captions for generating adversarial
examples on Show-and-Tell, and the captions of
the transferred adversarial image on Show-Attendand-Tell (shown under column tgt in Table 4).
Small values of ori suggest that the adversarial
images on Show-Attend-and-Tell generate significantly different captions from original images’
captions. Large values of tgt suggest that the adversarial images on Show-Attend-and-Tell generate similar adversarial captions as on the Showand-Tell model.
We ﬁnd that increasing c or ϵ
helps to enhance transferability at the cost of larger
(but still acceptable) distortion. When C = 1, 000
and ϵ = 10, Show-and-Fool achieves the best
transferability results: tgt is close to mis, indicating that the discrepancy between adversarial captions on the two models is mostly bounded by the
intrinsic model mismatch rather than the transferability of adversarial perturbations, and implying
that the adversarial perturbations are easily transferable. In addition, the adversarial examples generated by our method can also fool NeuralTalk2.
When c = 104, ϵ = 10, the average ℓ2 distortion,
BLEU-4 and METEOR scores between the original and transferred adversarial captions are 38.01,
0.440 and 0.473, respectively. The high transferability of adversarial examples crafted by Show-
and-Fool also indicates the problem of common
robustness leakage between different neural image
captioning models.
Attacking Image Captioning v.s.
Attacking Image Classiﬁcation
In this section we show that attacking image captioning models is inherently more challenging
than attacking image classiﬁcation models. In the
classiﬁcation task, a targeted attack usually becomes harder when the number of labels increases,
since an attack method needs to change the classi-
ﬁcation prediction to a speciﬁc label over all the
possible labels. In the targeted attack on image
captioning, if we treat each caption as a label,
we need to change the original label to a speciﬁc
one over an almost inﬁnite number of possible labels, corresponding to a nearly zero volume in the
search space. This constraint forces us to develop
non-trivial methods that are signiﬁcantly different
from the ones designed for attacking image classi-
ﬁcation models.
To verify that the two tasks are inherently different, we conducted additional experiments on
attacking only the CNN module using two stateof-the-art image classiﬁcation attacks on ImageNet dataset.
Our experiment setup is as follows.
Each selected ImageNet image has a label corresponding to a WordNet synset ID. We
randomly selected 800 images from ImageNet
dataset such that their synsets have at least one
word in common with Show-and-Tell’s vocabulary, while ensuring the Inception-v3 CNN (Showand-Tell’s CNN) classify them correctly.
we perform Iterative Fast Gradient Sign Method
(I-FGSM) and Carlini and
Wagner’s (C&W) attack on these images.
The attack target labels are randomly chosen and their synsets also
have at least one word in common with Showand-Tell’s vocabulary. Both I-FGSM and C&W
achieve 100% targeted attack success rate on the
Inception-v3 CNN. These adversarial examples
were further employed to attack Show-and-Tell
model. An attack is considered successful if any
word in the targeted label’s synset or its hypernyms up to 5 levels is presented in the resulting
caption. For example, for the chain of hypernyms
‘broccoli’⇒‘cruciferous vegetable’⇒‘vegetable,
veggie, veg’⇒‘produce, green goods, green groceries, garden truck’⇒‘food, solid food’, we include ‘broccoli’,‘cruciferous’,‘vegetable’,‘veggie’
and all other following words. Note that this criterion of success is much weaker than the criterion we use in the targeted caption method, since a
caption with the targeted image’s hypernyms does
not necessarily leads to similar meaning of the targeted image’s captions. To achieve higher attack
success rates, we allow relatively larger distortions
and set ϵ∞= 0.3 (maximum ℓ∞distortion) in I-
FGSM and κ = 10, C = 100 in C&W. However, as shown in Table 1, the attack success rates
are only 34.5% for I-FGSM and 22.4% for C&W,
respectively, which are much lower than the success rates of our methods despite larger distortions. This result further conﬁrms that performing targeted attacks on neural image captioning requires a careful design (as proposed in this paper),
and attacking image captioning systems is not a
trivial extension to attacking image classiﬁers.
Conclusion
In this paper, we proposed a novel algorithm,
Show-and-Fool, for crafting adversarial examples
and providing robustness evaluation of neural image captioning. Our extensive experiments show
that the proposed targeted caption and keyword
methods yield high attack success rates while the
adversarial perturbations are still imperceptible to
human eyes. We further demonstrate that Showand-Fool can generate highly transferable adversarial examples. The high-quality and transferable
adversarial examples in neural image captioning
crafted by Show-and-Fool highlight the inconsistency in visual language grounding between humans and machines, suggesting a possible weakness of current machine vision and perception machinery. We also show that attacking neural image
captioning systems are inherently different from
attacking CNN-based image classiﬁers.
Our method stands out from the well-studied
adversarial learning on image classiﬁers and CNN
models. To the best of our knowledge, this is the
very ﬁrst work on crafting adversarial examples
for neural image captioning systems. Indeed, our
Show-and-Fool algorithm1 can be easily extended
to other applications with RNN or CNN+RNN architectures. We believe this paper provides potential means to evaluate and possibly improve the robustness (for example, by adversarial training or
data augmentation) of a wide range of visual language grounding and other NLP models.