Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 670–680,
October 25-29, 2014, Doha, Qatar. c⃝2014 Association for Computational Linguistics
Chinese Poetry Generation with Recurrent Neural Networks
Xingxing Zhang and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
 , 
We propose a model for Chinese poem
generation based on recurrent neural networks which we argue is ideally suited to
capturing poetic content and form.
generator jointly performs content selection (“what to say”) and surface realization
(“how to say”) by learning representations
of individual characters, and their combinations into one or more lines as well
as how these mutually reinforce and constrain each other. Poem lines are generated incrementally by taking into account
the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical
n-grams. Experimental results show that
our model outperforms competitive Chinese poetry generation systems using both
automatic and manual evaluation methods.
Introduction
Classical poems are a signiﬁcant part of China’s
cultural heritage. Their popularity manifests itself
in many aspects of everyday life, e.g., as a means
of expressing personal emotion, political views,
or communicating messages at festive occasions
as well as funerals.
Amongst the many different types of classical Chinese poetry, quatrain and
regulated verse are perhaps the best-known ones.
Both types of poem must meet a set of structural,
phonological, and semantic requirements, rendering their composition a formidable task left to the
very best scholars.
An example of a quatrain is shown in Table 1.
Quatrains have four lines, each ﬁve or seven characters long.
Characters in turn follow speciﬁc
phonological patterns, within each line and across
lines. For instance, the ﬁnal characters in the second, fourth and (optionally) ﬁrst line must rhyme,
Missing You
红豆生南国，(* Z P P Z)
Red berries born in the warm southland.
枝？(P P Z Z P)
How many branches ﬂush in the spring?
愿君多采撷，(* P P Z Z)
Take home an armful, for my sake,
思。(* Z Z P P)
As a symbol of our love.
Table 1: An example of a 5-char quatrain exhibiting one of the most popular tonal patterns.
The tone of each character is shown at the end of
each line (within parentheses); P and Z are shorthands for Ping and Ze tones, respectively; * indicates that the tone is not ﬁxed and can be either.
Rhyming characters are shown in boldface.
whereas there are no rhyming constraints for the
third line. Moreover, poems must follow a prescribed tonal pattern. In traditional Chinese, every character has one tone, Ping (level tone) or Ze
(downward tone). The poem in Table 1 exempli-
ﬁes one of the most popular tonal patterns . Besides adhering to the above formal criteria, poems must exhibit concise and accurate use
of language, engage the reader/hearer, stimulate
their imagination, and bring out their feelings.
In this paper we are concerned with generating traditional Chinese poems automatically. Although computers are no substitute for poetic creativity, they can analyze very large online text
repositories of poems, extract statistical patterns,
maintain them in memory and use them to generate many possible variants. Furthermore, while
amateur poets may struggle to remember and apply formal tonal and structural constraints, it is relatively straightforward for the machine to check
whether a candidate poem conforms to these requirements. Poetry generation has received a fair
amount of attention over the past years (see the
discussion in Section 2), with dozens of computational systems written to produce poems of varying sophistication. Beyond the long-term goal of
building an autonomous intelligent system capable of creating meaningful poems, there are potential short-term applications for computer generated poetry in the ever growing industry of electronic entertainment and interactive ﬁction as well
as in education.
An assistive environment for
poem composition could allow teachers and students to create poems subject to their requirements, and enhance their writing experience.
We propose a model for Chinese poem generation based on recurrent neural networks. Our generator jointly performs content selection (“what
to say”) and surface realization (“how to say”).
Given a large collection of poems, we learn representations of individual characters, and their combinations into one or more lines as well as how
these mutually reinforce and constrain each other.
Our model generates lines in a poem probabilistically: it estimates the probability of the current
line given the probability of all previously generated lines. We use a recurrent neural network to
learn the representations of the lines generated so
far which in turn serve as input to a recurrent language model which generates the
current line. In contrast to previous approaches
 , our
generator makes no Markov assumptions about the
dependencies of the words within a line and across
We evaluate our approach on the task of quatrain generation and the references therein). Most approaches employ templates to construct poems according to a set of constraints (e.g., rhyme, meter, stress, word frequency) in combination with
corpus-based and lexicographic resources.
example, the Haiku poem generator presented in
Wu et al. and Tosa et al. produces
poems by expanding user queries with rules extracted from a corpus and additional lexical resources.
Netzer et al. generate Haiku
with Word Association Norms, Agirrezabal et
al. compose Basque poems using patterns
based on parts of speech and WordNet , and Oliveira presents a generation
algorithm for Portuguese which leverages semantic and grammar templates.
A second line of research uses genetic algorithms for poem generation . Manurung et al. argue that at a basic level
all (machine-generated) poems must satisfy the
constraints of grammaticality (i.e., a poem must
syntactically well-formed), meaningfulness (i.e., a
poem must convey a message that is meaningful
under some interpretation) and poeticness (i.e., a
poem must exhibit features that distinguishes it
from non-poetic text, e.g., metre). Their model
generates several candidate poems and then uses
stochastic search to ﬁnd those which are grammatical, meaningful, and poetic.
A third line of research draws inspiration from
statistical machine translation (SMT) and related text-generation applications such as summarization.
Greene et al. infer meters
(stressed/unstressed syllable sequences) from a
corpus of poetic texts which they subsequently
use for generation together with a cascade of
weighted ﬁnite-state transducers interpolated with
IBM Model 1. Jiang and Zhou generate
Chinese couplets (two line poems) using a phrasebased SMT approach which translates the ﬁrst line
to the second line. He et al. extend this algorithm to generate four-line quatrains by sequentially translating the current line from the previous
one. Yan et al. generate Chinese quatrains
based on a query-focused summarization framework. Their system takes a few keywords as input
and retrieves the most relevant poems from a corpus collection. The retrieved poems are segmented
into their constituent terms which are then grouped
into clusters. Poems are generated by iteratively
selecting terms from clusters subject to phonological, structural, and coherence constraints.
Our approach departs from previous work in
two important respects. Firstly, we model the tasks
of surface realization and content selection jointly
ShiXueHanYing
Candidate lines
First line
generation
generation
Figure 1: Poem generation with keywords spring, lute, and drunk. The keywords are expanded into
phrases using a poetic taxonomy. Phrases are then used to generate the ﬁrst line. Following lines are
generated by taking into account the representations of all previously generated lines.
using recurrent neural networks. Structural, semantic, and coherence constraints are captured
naturally in our framework, through learning the
representations of individual characters and their
combinations. Secondly, generation proceeds by
taking into account multi-sentential context rather
than the immediately preceding sentence.
work joins others in using continuous representations to express the meaning of words and phrases
 and
how these may be combined in a language modeling context . More
recently, continuous translation models based on
recurrent neural networks have been proposed as
a means to map a sentence from the source language to sentences in the target language .
These models are evaluated on the task of rescoring n-best lists of translations. We use neural networks more directly to perform the actual poem
generation task.
The Poem Generator
As common in previous work we assume that our generator operates in an interactive context. Speciﬁcally, the
user supplies keywords (e.g., spring, lute, drunk)
highlighting the main concepts around which the
poem will revolve. As illustrated in Figure 1, our
generator expands these keywords into a set of related phrases. We assume the keywords are restricted to those attested in the ShiXueHanYing poetic phrase taxonomy . The latter contains 1,016 manual clusters
of phrases (Liu, 1735); each cluster is labeled with
a keyword id describing general poem-worthy topics. The generator creates the ﬁrst line of the poem
based on these keywords. Subsequent lines are
generated based on all previously generated lines,
subject to phonological (e.g., admissible tonal patterns) and structural constraints (e.g., whether the
quatrain is ﬁve or seven characters long).
To create the ﬁrst line, we select all phrases
corresponding to the user’s keywords and generate all possible combinations satisfying the tonal
pattern constraints. We use a language model to
rank the generated candidates and select the bestranked one as the ﬁrst line in the poem. In implementation, we employ a character-based recurrent neural network language model interpolated with a Kneser-Ney trigram
and ﬁnd the n-best candidates with a stack decoder (see Section 3.5 for details). We then generate the second line based on the ﬁrst one, the
third line based on the ﬁrst two lines, and so on.
Our generation model computes the probability
of line Si+1 = w1,w2,...,wm, given all previously
generated lines S1:i(i ≥1) as:
P(Si+1|S1:i) =
P(w j+1|w1:j,S1:i)
Equation (1), decomposes P(Si+1|S1:i) as the product of the probability of each character w j in
the current line given all previously generated
characters w1:j−1 and lines S1:i.
This means
that P(Si+1|S1:i) is sensitive to previously generated content and currently generated characters.
The estimation of the term P(w j+1|w1:j,S1:i)
lies at the heart of our model. We learn representations for S1:i, the context generated so far,
using a recurrent neural network whose output
serves as input to a second recurrent neural network used to estimate P(wj+1|w1:j,S1:i). Figure 2
illustrates the generation process for the (j + 1)th
character w j+1 in the (i + 1)th line Si+1.
lines S1:i are converted into vectors v1:i with a
convolutional sentence model (CSM; described in
Section 3.1).
Next, a recurrent context model
(RCM; see Section 3.2) takes v1:i as input and
outputs uj
i , the representation needed for generating w j+1 ∈Si+1. Finally, u1
ﬁrst j characters w1:j in line Si+1 serve as input to
a recurrent generation model (RGM) which estimates P(w j+1 = k|w1:j,S1:i) with k ∈V, the probability distribution of the (j + 1)th character over
all words in the vocabulary V. More formally, to
estimate P(w j+1|w1:j,S1:i) in Equation (1), we apply the following procedure:
vi = CSM(Si)
i = RCM(v1:i, j)
P(w j+1|w1:j,S1:i) = RGM(w1:j+1,u1:j
We obtain the probability of the (i + 1)th sentence P(Si+1|S1:i), by running the RGM in (2c)
above m −1 times (see also Equation (1)). In the
following, we describe how the different components of our model are obtained.
Convolutional Sentence Model (CSM)
The CSM converts a poem line into a vector. In
principle, any model that produces vector-based
representations of phrases or sentences could be
used . We opted for the convolutional sentence
model proposed in Kalchbrenner and Blunsom
 as it is n-gram based and does not make
use of any parsing, POS-tagging or segmentation
tools which are not available for Chinese poems.
Their model computes a continuous representation
for a sentence by sequentially merging neighboring vectors (see Figure 3).
Let V denote the character vocabulary in our
corpus; L ∈Rq×|V| denotes a character embedding matrix whose columns correspond to character vectors (q represents the hidden unit size).
Such vectors can be initialized randomly or obtained via a training procedure (Mikolov et al.,
Let w denote a character with index k;
e(w) ∈R|V|×1 is a vector with zero in all positions
except e(w)k = 1; T l ∈Rq×Nl is the sentence representation in the lth layer, where Nl is the number of columns in the lth layer (Nl = 1 in the
i (k ̸= j)
1-of-N encoding of
w j=(0,...,1,...,0)
P(w j+1|w1:j,S1:i)
Generation of the (j + 1)th character w j+1 in the (i + 1)th line Si+1.
The recurrent context model (RCM) takes i lines as input (represented by vectors v1,...,vi) and creates context vectors for the recurrent generation
model (RGM). The RGM estimates the probability P(w j+1|w1:j,S1:i).
top layer); Cl,n ∈Rq×n is an array of weight matrices which compress neighboring n columns in
the lth layer to one column in the (l + 1)th layer.
Given a sentence S = w1,w2,...,wm, the ﬁrst layer
is represented as:
T 1 = [L·e(w1),L·e(w2),...,L·e(wm)]
The (l +1)th layer is then computed as follows:
:,j+i−1 ⊙Cl,n
Nl+1 = Nl −n+1
1 ≤j ≤Nl+1
where T l is the representation of the previous
layer l, Cl,n a weight matrix, ⊙element-wise vector product, and σ a non-linear function. We compress two neighboring vectors in the ﬁrst two layers and three neighboring vectors in the remaining
layers. Speciﬁcally, for quatrains with seven characters, we use C1,2, C2,2, C3,3, C4,3 to merge vectors in each layer (see Figure 3); and for quatrains
with ﬁve characters we use C1,2, C2,2, C3,3.
Far off I watch the waterfall plunge to the
long river.
Figure 3: Convolutional sentence model for 7-char
quatrain. The ﬁrst layer has seven vectors, one
for each character. Two neighboring vectors are
merged to one vector in the second layer with
weight matrix C1,2. In other layers, either two or
three neighboring vectors are merged.
Recurrent Context Model (RCM)
The RCM takes as input the vectors representing
the i lines generated so far and reduces them to a
single context vector which is then used to generate the next character (see Figure 2). We compress
the i previous lines to one vector (the hidden layer)
and then decode the compressed vector to different
character positions in the current line. The output
layer consists thus of several vectors (one for each
position) connected together. This way, different
aspects of the context modulate the generation of
different characters.
Let v1,...,vi
(vi ∈Rq×1) denote the vectors of
the previous i lines; hi ∈Rq×1 is their compressed
representation (hidden layer) which is obtained
with matrix M ∈Rq×2q; matrix Uj decodes hi to
i ∈Rq×1 in the (i + 1)th line. The computation
of the RCM proceeds as follows:
hi = σ(M ·
i = σ(Uj ·hi)
where σ is a non-linear function such as sigmoid
and m the line length. Advantageously, lines in
classical Chinese poems have a ﬁxed length of ﬁve
or seven characters. Therefore, the output layer of
the recurrent context model only needs two weight
matrices (one for each length) and the number of
parameters still remains tractable.
Recurrent Generation Model (RGM)
As shown in Figure 2, the RGM estimates the
probability distribution of the next character (over
the entire vocabulary) by taking into account the
context vector provided by the RCM and the
1-of-N encoding of the previous character. The
RGM is essentially a recurrent neural network language model with an auxiliary input layer, i.e., the context vector from
the RCM. Similar strategies for encoding additional information have been adopted in related
language modeling and machine translation work
 .
Let Si+1 = w1,w2,...,wm
denote the line
generated.
estimate P(w j+1|w1:j,S1:i), however, since the ﬁrst
i lines have been encoded in the context vector uj
we compute P(w j+1|w1:j,uj
i ) instead. Therefore,
the probability P(Si+1|S1:i) becomes:
P(Si+1|S1:i) =
P(w j+1|w1:j,uj
Let |V| denote the size of the character vocabulary. The RGM is speciﬁed by a number of matrices. Matrix H ∈Rq×q (where q represents the
hidden unit size) transforms the context vector to
a hidden representation; matrix X ∈Rq×|V| transforms a character to a hidden representation, matrix R ∈Rq×q implements the recurrent transformation and matrix Y ∈R|V|×q decodes the hidden
representation to weights for all words in the vocabulary. Let w denote a character with index k
in V; e(w) ∈R|V|×1 represents a vector with zero
in all positions except e(w)k = 1, rj is the hidden
layer of the RGM at step j, and yj+1 the output of
the RGM, again at step j. The RGM proceeds as
rj = σ(R·rj−1 +X ·e(w j)+H ·uj
yj+1 = Y ·rj
where σ is a nonlinear function (e.g., sigmoid).
The probability of the (j +1)th word given the
previous j words and the previous i lines is estimated by a softmax function:
P(w j+1 = k|w1:j,uj
exp(yj+1,k)
k=1 exp(yj+1,k)
We obtain P(Si+1|S1:i) by multiplying all the terms
in the right hand-side of Equation (6).
The objective for training is the cross entropy errors of the predicted character distribution and the
actual character distribution in our corpus.
l2 regularization term is also added to the objective. The model is trained with back propagation
through time with sentence length being the time step. The objective
is minimized by stochastic gradient descent. During training, the cross entropy error in the output
layer of the RGM is back-propagated to its hidden and input layers, then to the RCM and ﬁnally
to the CSM. The same number of hidden units
(q = 200) is used throughout (i.e., in the RGM,
RCM, and CSM). In our experiments all parameters were initialized randomly, with the exception of the word embedding matrix in the CSM
which was initialized with word2vec embeddings
 obtained from our poem
corpus . To compute the
probability of a character, we estimate the probability of its class and then multiply it by the probability of the character conditioned on the class. In
our experiments we used 82 (square root of |V|)
classes which we obtained by applying hierarchical clustering on character embeddings. This strategy outperformed better known frequency-based
classing methods 
on our task.
Our poem generator models content selection
and lexical choice and their interaction, but does
not have a strong notion of local coherence,
as manifested in poetically felicitous line-to-line
transitions. In contrast, machine translation models have been particularly successful at generating adjacent lines (couplets). To enhance coherence, we thus interpolate
our model with two machine translation features
(i.e., inverted phrase translation model feature and
inverted lexical weight feature). Also note, that
in our model surface generation depends on the
last observed character and the state of the hidden
layer before this observation. This way, there is no
explicitly deﬁned context, and history is captured
implicitly by the recurrent nature of the model.
This can be problematic for our texts which must
obey certain stylistic conventions and sound poetic. In default of a better way of incorporating
poeticness into our model, we further interpolate it
with a language model feature (i.e., a Kneser-Ney
trigram model).
Throughout our experiments,
we use the
RNNLM toolkit to train the character-based recurrent neural network language model . Kneser-Ney n-grams were trained with
KenLM .
Our decoder is a stack decoder similar to Koehn
et al. . In addition, it implements the tonal
pattern and rhyming constraints necessary for generating well-formed Chinese quatrains. Once the
ﬁrst line in a poem is generated, its tonal pattern
is determined. During decoding, phrases violating this pattern are ignored. As discussed in Section 1, the ﬁnal characters of the second and the
fourth lines must rhyme. We thus remove during
decoding fourth lines whose ﬁnal characters do not
rhyme with the second line. Finally, we use MERT
training to learn feature weights for
the decoder.
Experimental Design
We created a corpus of classical Chinese
poems by collating several online resources: Tang
Poems, Song Poems, Song Ci, Ming Poems, Qing
Poems, and Tai Poems.
The corpus consists
of 284,899 poems in total. 78,859 of these are
quatrains and were used for training and evaluating our model.1
Table 2 shows the different
partitions of this dataset (POEMLM) into training (QTRAIN)2, validation (QVALID) and testing
Half of the poems in QVALID and
QTEST are 5-char quatrains and the other half
are 7-char quatrains. All poems except QVALID
1The data used in our experiments can be downloaded
 
php?page=resources.
2Singleton characters in QTRAIN (6,773 in total) were replaced by <R> to reduce data sparsity.
Characters
15,624,283
Table 2: Dataset partitions of our poem corpus.
and QTEST were used for training the characterbased language models (see row POEMLM in Table 2). We also trained word2vec embeddings on
POEMLM. In our experiments, we generated quatrains following the eight most popular tonal patterns according to Wang .
Perplexity Evaluation
Evaluation of machinegenerated poetry is a notoriously difﬁcult task.
Our evaluation studies were designed to assess
Manurung et al.’s criteria of grammaticality, meaningfulness, and poeticness.
As a sanity check, we ﬁrst measured the perplexity of our
model with respect to the goldstandard.
Intuitively, a better model should assign larger probability (and therefore lower perplexity) to goldstandard poems.
BLEU-based Evaluation
We also used BLEU
to evaluate our model’s ability to generate the second, third and fourth line given previous goldstandard lines.
A problematic aspect of this evaluation is the need for human-authored references
(for a partially generated poem) which we do not
have. We obtain references automatically following the method proposed in He et al. . The
main idea is that if two lines share a similar topic,
the lines following them can be each other’s references. Let A and B denote two adjacent lines
in a poem, with B following A. Similarly, let line
B′ follow line A′ in another poem. If lines A and
A′ share some keywords in the same cluster in the
Shixuehanying taxonomy, then B and B′ can be
used as references for both A and A′. We use this
algorithm on the Tang Poems section of our corpus
to build references for poems in the QVALID and
QTEST data sets. Poems in QVALID (with autogenerated references) were used for MERT training and Poems in QTEST (with auto-generated references) were used for BLEU evaluation.
Human Evaluation
Finally, we also evaluated
the generated poems by eliciting human judg-
Perplexity
Table 3: Perplexities for different models.
Speciﬁcally, we invited 30 experts3 on
Chinese poetry to assess the output of our generator (and comparison systems). These experts
were asked to rate the poems using a 1–5 scale on
four dimensions: ﬂuency (is the poem grammatical and syntactically well-formed?), coherence (is
the poem thematically and logically structured?),
meaningfulness (does the poem convey a meaningful message to the reader?)
and poeticness
(does the text display the features of a poem?).
We also asked our participants to evaluate system
outputs by ranking the generated poems relative to
each other as a way of determining overall poem
quality .
Participants rated the output of our model and
three comparison systems. These included He et
al.’s SMT-based model (SMT), Yan et al.’s
 summarization-based system (SUM), and
a random baseline which creates poems by randomly selecting phrases from the Shixuehanying
taxonomy given some keywords as input.
also included human written poems whose content
matched the input keywords. All systems were
provided with the same keywords (i.e., the same
cluster names in the ShiXueHanYing taxonomy).
In order to compare all models on equal footing,
we randomly sampled 30 sets of keywords (with
three keywords in each set) and generated 30 quatrains for each system according to two lengths,
namely 5-char and 7-char. Overall, we obtained
ratings for 300 (5×30×2) poems.
The results of our perplexity evaluation are summarized in Table 3. We compare our RNN-based
poem generator (RNNPG) against Mikolov’s
 recurrent neural network language model
(RNNLM) and a 5-gram language model with
Kneser-Ney smoothing (KN5). All models were
trained on QTRAIN and tuned on QVALID. The
perplexities were computed on QTEST. Note that
327 participants were professional or amateur poets and
three were Chinese literature students who had taken at least
one class on Chinese poetry composition.
Table 4: BLEU-2 scores on 5-char and 7-char quatrains. Given i goldstandard lines, BLEU-2 scores are
computed for the next (i+1)th lines.
Poeticness
Table 5: Mean ratings elicited by humans on 5-char and 7-char quatrains. Diacritics ** (p < 0.01)
and * (p < 0.05) indicate our model (RNNPG) is signiﬁcantly better than all other systems except Human.
Diacritics ++ (p < 0.01) and + (p < 0.05) indicate Human is signiﬁcantly better than all other systems.
the RNNPG estimates the probability of a poem
line given at least one previous line. Therefore, the
probability of a quatrain assigned by the RNNPG
is the probability of the last three lines. For a fair
comparison, RNNLM and KN5 only leverage the
last three lines of each poem during training, validation and testing. The results in Table 3 indicate
that the generation ability of the RNNPG is better
than KN5 and RNNLM. Note that this perplexitystyle evaluation is not possible for models which
cannot produce probabilities for gold standard poems. For this reason, other related poem generators are not
included in the table.
The results of our evaluation using BLEU-2 are
summarized in Table 4. Here, we compare our
system against the SMT-based poem generation
model of He et al. .4
Their system is a
linear combination of two translation models (one
with ﬁve features and another one with six). Our
model uses three of their features, namely the inverted phrase translation model feature, the lexical
weight feature, and a Kneser-Ney trigram feature.
Unfortunately, it is not possible to evaluate Yan
et al.’s summarization-based system with
BLEU, as it creates poems as a whole and there is
no obvious way to generate next lines with their
4Our re-implementation of their system delivered very
similar scores to He et al. .
For example, we obtained an average BLEU-1 of 0.167 for 5-char quatrains and
0.428 for 7-char quatrains compared to their reported scores
of 0.141 and 0.380, respectively.
algorithm. The BLEU scores in Table 4 indicate
that, given the same context lines, the RNNPG is
better than SMT at generating what to say next.
BLEU scores should be, however, viewed with
some degree of caution. Aside from being an approximation of human judgment , BLEU might be unnecessarily conservative for poem composition which by its very
nature is a creative endeavor.
The results of our human evaluation study are
shown in Table 5. Each column reports mean ratings for a different dimension (e.g., ﬂuency, coherence). Ratings for 5-char and 7-char quatrains
are shown separately.
The last column reports
rank scores for each system . In a ranked list of N items (N = 5 here), the
score of the ith ranked item is (N−i)
(N−1). The numerator indicates how many times a systems won in
pairwise comparisons, while the denominator normalizes the score.
With respect to 5-char quatrains, RNNPG is
signiﬁcantly better than Random, SUM and SMT
on ﬂuency, coherence, meaningfulness, poeticness
and ranking scores (using a t-test). On all dimensions, human-authored poems are rated as significantly better than machine-generated ones, with
the exception of overall ranking. Here, the difference between RNNPG and Human is not signiﬁcant.
We obtain similar results with 7-char
quatrains. In general, RNNPG seems to perform
better on the shorter poems.
The mean ratings
Egrets stood, peeping ﬁshes.
Budding branches are full of romance.
Water was still, reﬂecting mountains.
Plum blossoms are invisible but adorable.
The wind went down by nightfall,
With the east wind comes Spring.
as the moon came up by the tower.
Where on earth do I come from?
Table 6: Example output produced by our model (RNNPG).
are higher and the improvements over other systems are larger. Also notice, that the score margins between the human- and machine-written poems become larger for 7-char quatrains. This indicates that the composition of 7-char quatrains is
more difﬁcult compared to 5-char quatrains. Table 6 shows two example poems (5-char and 7char) produced by our model which received high
scores with respect to poeticness.
Interestingly, poems generated by SUM5 are
given ratings similar to Random.
In fact SUM
is slightly worse (although not signiﬁcantly) than
Random on all dimensions, with the exception of
coherence. In the human study reported in Yan et
al. , SUM is slightly better than SMT. There
are several reasons for this discrepancy. We used
a more balanced experimental design: all systems
generated poems from the same keywords which
were randomly chosen. We used a larger dataset
to train the SMT model compared to Yan et al.
(284,899 poems vs 61,960). The Random baseline
is not a straw-man; it selects phrases from a taxonomy of meaningful clusters edited by humans and
closely related to the input keywords.
Conclusions
In this paper we have presented a model for Chinese poem generation based on recurrent neural
networks. Our model jointly performs content selection and surface realization by learning representations of individual characters and their combinations within and across poem lines. Previous
work on poetry generation has mostly leveraged
contextual information of limited length (e.g., one
sentence). In contrast, we introduced two recurrent neural networks (the recurrent context model
and recurrent generation model) which naturally
5We made a good-faith effort to re-implement their poem
generation system. We are grateful to Rui Yan for his help
and technical advice.
capture multi-sentential content. Experimental results show that our model yields high quality poems compared to the state of the art. Perhaps unsurprisingly, our human evaluation study revealed
that machine-generated poems lag behind humangenerated ones. It is worth bearing in mind that
poetry composition is a formidable task for humans, let alone machines.
And that the poems
against which our output was compared have been
written by some of the most famous poets in Chinese history!
Avenues for future work are many and varied.
We would like to generate poems across different languages and genres (e.g., Engish sonnets or
Japanese haiku). We would also like to make the
model more sensitive to line-to-line transitions and
stylistic conventions by changing its training objective to a combination of cross-entropy error and
BLEU score. Finally, we hope that some of the
work described here might be of relevance to other
generation tasks such as summarization, conceptto-text generation, and machine translation.
Acknowledgments
We would like to thank Eva Halser for valuable
discussions on the machine translation baseline.
We are grateful to the 30 Chinese poetry experts
for participating in our rating study. Thanks to
Gujing Lu, Chu Liu, and Yibo Wang for their help
with translating the poems in Table 6 and Table 1.