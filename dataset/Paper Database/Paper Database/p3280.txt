The SpiNNaker Project
This paper describes the design of a massively parallel computer that is suitable for
computational neuroscience modeling of large-scale spiking neural networks in
biological real time.
By Steve B. Furber, Fellow IEEE, Francesco Galluppi, Steve Temple, and
Luis A. Plana, Senior Member IEEE
ABSTRACT | The
architecture
(SpiNNaker) project aims to deliver a massively parallel millioncore computer whose interconnect architecture is inspired by the
connectivity characteristics of the mammalian brain, and which is
suited to the modeling of large-scale spiking neural networks in
biological real time. Specifically, the interconnect allows the
transmission of a very large number of very small data packets,
each conveying explicitly the source, and implicitly the time, of a
single neural action potential or ‘‘spike.’’ In this paper, we review
the current state of the project, which has already delivered
systems with up to 2500 processors, and present the real-time
event-driven programming model that supports flexible access to
the resources of the machine and has enabled its use by a wide
range of collaborators around the world.
KEYWORDS | Brain modeling; multicast algorithms; multiprocessor interconnection networks; neural network hardware;
parallel programming
I. INTRODUCTION
THE spiking neural network architecture (SpiNNaker)
project is motivated by the grand challenge of understanding how information is represented and processed in the
brain . Most of the frontiers of science are concerned
with the very small, such as subatomic particles, or the
very large, such as exploring the outer regions of the
universe. Yet there remains a great unsolved scientific
mystery at a very human scale: how does the brain, an
organ that we could readily hold in our hands and observe
with the naked eye, perform its role that is so central to all
of our lives?
‘‘Wet’’ neuroscience has told us a great deal about the
basic componentVthe neuronVfrom which the brain is
constructed. Brain imaging tells us yet more about how
activity moves around the brain as we perform certain
mental functions. The former is concerned with individual
neurons up to groups of tens or perhaps hundreds; the
latter looks at the collective activity of many millions of
neurons. But between these scales there are a few orders of
magnitude of scale for which there exists no scientific
instrument except the computer model, and it is at these
intermediate scales, we suggest, that all the interesting
information processing takes place.
Our conclusion is that, if we wish to fully understand
how the brain represents and processes information, we
need to build computer models to test hypotheses of how
the brain works.
A. Neurons and Spikes
What sort of computer is required for such brain
modeling to work?
The human brain is generally viewed as comprising
somewhat under 100 billion neurons, where each neuron
is a multiple-input–single-output device.
There is some debate about the role of the more
numerous glial cells that form the structure upon which
the neurons build the brain, and, in particular, the role of
astrocyte cells in synaptic plasticity , so any generalpurpose system should aim to accommodate these issues in
case they prove to be important.
Neurons communicate principally through action
potentials, or ‘‘spikes.’’ These are simply asynchronous
impulses where, as a result of the electrochemical
regeneration process used to ensure the reliable propagation of these signals along long biological ‘‘wires,’’
information is conveyed only in the identity of the neuron
that spiked and the time at which it spiked. The height and
the width of the impulse are largely invariant at the
receiving synapse. This has led to the widespread adoption
of the address event representation (AER) encoding of
Manuscript received October 2, 2013; revised January 4, 2014; accepted February 2,
2014. Date of publication February 27, 2014; date of current version April 28, 2014. This
work was supported by the Engineering and Physical Sciences Research Council
(EPSRC) under GrantEP/G015740/01.
The authors are with the School of Computer Science, University of Manchester,
Manchester M13 9PL, U.K. (e-mail: ;
 ; ; ).
Digital Object Identifier: 10.1109/JPROC.2014.2304638
0018-9219  2014 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/
redistribution requires IEEE permission. See for more information.
Proceedings of the IEEE | Vol. 102, No. 5, May 2014
neural activity , , where the information flow in a
network is represented as a time series of neural identifiers.
There are some notable exceptions to the completeness
of the AER view of information flow. Some neurons
transport and emit neuromodulators, such as dopamine,
that have a global effect on neurons within a neighborhood
region; other neurons make direct contact through ‘‘gap’’
junctions that make an electrical connection from one
neuron to its neighbor. However, in much of the brain, the
primary real-time information flow is in the spikes that, in
a model, are represented by AER. A general-purpose
computer-modeling platform should offer mechanisms to
support these other information flows while giving firstclass support to AER ‘‘spikes.’’
B. Computer Models
What computer power and architecture are required to
support a real-time model of the human brain?
The simplest estimate of an answer to this question
suggests that there are around 1015 synapses in the brain,
with inputs firing at an average rate of 101 Hz, and each
synaptic event requires perhaps 102 instructions to update
the state of the postsynaptic neuron and implement any
synaptic plasticity algorithm. These figures lead to an
estimate of 1018 operations per second, the performance of
an exascale machine. Exascale high-performance computers do not yet exist, though recently the Chinese Tianhe 2
machine has achieved 3  1016 floating-point operations
per second , so exascale computing is not too far away.
However, raw computer performance is not the only
issue here. The communication patterns in the brain are
based on sending very small ‘‘packets’’ of information
through complex paths to many targets. High-performance
computers, on the other hand, are generally optimized for
point-to-point communication of large data packets. This
mismatch leads to significant inefficiency in the mapping
of brain-scale spiking neural networks onto conventional
cluster machines and high-performance computers.
C. SpiNNaker
The SpiNNaker machine is a computer designed
specifically to support the sorts of communication found
in the brain. Recognizing the huge computational requirements of the task, SpiNNaker is based on massively parallel
computation, and the architecture will accommodate up to
a million microprocessor cores, the limit being defined by
budget and architectural convenience rather than anything
fundamental.
The key innovation in the SpiNNaker architecture is the
communications infrastructure, which is optimized to carry
very large numbers of very small packets, in contrast to the
conventional cluster and high-performance computer communications system which, as noted above, are optimized for
large data packets. Each packet carries a single neural ‘‘spike’’
event in a 40-b packet, 32 b of which are the AER identifier of
the neuron that spiked and 8 b are management bits
identifying the packet type, and such like. (The choice of a
32-b AER identifier is not a fundamental limitation of the
architecture, and could be increased in a future implementation to accommodate larger neural models.) The time of
the AER spike is implicit; the communications infrastructure
can deliver a packet in much less than a millisecond, which is
the requirement for real-time neural modeling.
Although SpiNNaker’s design is centered on packetswitched support for AER ‘‘spikes,’’ it can also support non-
AER information flows through the same communication
mechanism delivering discrete (typically 1 ms) updates to
continuously variable parameters.
In order to achieve efficient massively parallel operation, SpiNNaker’s design accepts certain compromises,
one of which is the requirement for deterministic
operation. The asynchronous nature of the communications system leads to nondeterministic ordering of packet
reception, and occasionally packets may be dropped to
avoid communication deadlock. It is possible to reimpose
deterministic operation and lockstep operation to match a
conventional sequential model under certain conditions,
but this is not the natural or most efficient way to operate
the machine.
D. Paper Organization
This paper is a review of the SpiNNaker project and a
tutorial on the use of the machine. The contributions and
structure of the paper are as follows.
We present an overview of the architecture
(Section II) and of the hardware implementation
(Section III).
We present the system software (Section IV), describe the event-driven software model (Section V),
the API that supports this (Section VI), and a
simple example program that runs on top of the
API (Section VII).
We present the partitioning and configuration
manager (PACMAN, Section VIII) that conceals
the physical structure of the machine.
Finally, we describe some typical applications that
run on the machine (Section IX), our future plans
for larger scale machines (Section X), discuss
related work (Section XI), and draw our conclusions (Section XII) from our experience with the
machine at this stage in its development.
II. ARCHITECTURE OVERVIEW
A detailed description of the architecture of the machine
has been presented earlier , so here we present the key
features of the architecture that are germane to what
follows (see Fig. 1).
A SpiNNaker machine is a homogeneous 2-D multiple
instruction, multiple data array of processing nodes where
each node incorporates 18 ARM968 processor cores each
with 96 kB of local memory, 128 MB of shared memory, a
Furber et al: The SpiNNaker Project
Vol. 102, No. 5, May 2014 | Proceedings of the IEEE
packet router, and general system support peripherals. Each
processor core is a general-purpose 200-MHz 32-b integer
processor with no floating-point hardware, so arithmetic is
generally implemented as fixed point.
A. Achievable Performance
Each core can model a few hundred point neuron
models, such as leaky integrate and fire or Izhikevich’s
model, with the order of 1000 input synapses to each
neuron. In practice, a number of different constraints may
limit the number of neurons a processor core can support
in real time, but often the compute budget is dominated by
input connectionsVan incoming spike passing through an
individual synapseVwhich imposes an upper limit on the
(number of neurons)  (number of inputs per neuron) 
(mean input firing rate). In principle, a processor core can
support up to 10 million connections/s, though the current
software implementation saturates at about half this
throughput, and plastic synapse models reduce it considerably further.
B. Spikes and Packets
The key innovation in the SpiNNaker architecture is a
lightweight multicast packet-routing mechanism that
supports the very high connectivity found in biological
brains. The mechanism is an extension of conventional
AER , . When the software running on a processor
identifies that a neuron should emit a spike, it simply issues
a packet that identifies the spiking neuron. The issuing
processor has no idea of where that packet will be conveyed
toVthat is entirely the responsibility of the routing fabric.
Each node incorporates a packet router that inspects
each packet to look at its source, and routes it accordingly
to any subset of its 18 local processors and/or any subset of
its six neighbor nodes using multicast transmission (which
has been shown to be optimal for neural applications )
in a 2-D triangular mesh. The selected routes are
determined by tables in the router that are initialized
when the application is loaded into the machine.
As the packet source identifier is 32 b, it is infeasible to
implement full routing tables for every possible source, so
a number of optimizations are employed to keep the table
sizes reasonable.
The tables are implemented using content addressable memory (CAM), and entries are required
only for those packets that pass through a node.
The CAM uses four states: match 0, match 1,
match all, and no match. This allows a single CAM
entry to route all of those neurons in a population
with common routing requirements.
Where no CAM entry matches a source identifier,
a default routing mechanism allows the packet to
pass straight through the node.
These optimizations allow a routing table with 1024
entries to be sufficient at each node. We will return to the
matter of initializing these tables in Section VIII.
C. Processor Disposition
Each SpiNNaker node selects one of its 18 processor
cores to act as ‘‘monitor processor.’’ This selection is
flexible for fault-tolerance reasons. Once selected, the
monitor is assigned an operating system support role.
Sixteen of the remaining processors are assigned application support roles, and the 18th processor is held in reserve
as a fault-tolerance spare, though on a proportion of nodes,
the 18th processor may be faulty as nodes with only 17
functional processors are accepted in production to
enhance yield.
III. CHIPS, PACKAGES, BOARDS,
AND SYSTEMS
The physical implementation of the SpiNNaker architecture
has also been described in detail elsewhere , so again we
will restrict ourselves here to the relevant highlights.
A. Chips and Packages
Each SpiNNaker node is implemented in a single
19-mm square 300 ball grid array package. The package
houses a custom-designed multiprocessor system-on-chip
integrated circuit that includes the 18 ARM968 processors, each with its local 32-kB instruction memory and
64-kB data memory, interconnected through a self-timed
Fig. 1. Principal architectural components of a SpiNNaker node.
Furber et al: The SpiNNaker Project
Proceedings of the IEEE | Vol. 102, No. 5, May 2014
network on chip to various on-chip shared resources and a
second chip, a 128-MB low-power mobile dual-data-rate
(DDR) SDRAM. The two chips are stacked onto the
package substrate and interconnected using gold wire
bonding (Fig. 3). The aggregate SDRAM bandwidth has
been measured to be 900 MB/s .
The packages are then assembled onto printed circuit
boards (PCBs; see Fig. 2). The chip-to-chip connections on
the PCB are direct wired connections using a self-timed
2-of-7 non-return-to-zero protocol to transmit 4-b symbols
with two wire transitions, plus one wire transition for the
acknowledge response.
In principle, these direct connections could be used to
build a SpiNNaker machine of arbitrary size, but for
practical reasons the machine is constructed from 48-node
PCBs, and the PCB-to-PCB connections use high-speed
serial links where eight chip-to-chip links are multiplexed
through each serial link using Xilinx Spartan6 fieldprogrammable gate arrays (FPGAs).
C. Systems
SpiNNaker systems of varying sizes can then be
assembled from one or more of the 48-node PCBs. There
is also a smaller four-node board that is very convenient for
training, development, and mobile robotics. The largest
machine, incorporating over a million ARM processor
cores, will comprise 1200 48-node boards in ten machine
room cabinets and will require up to 75 kW of electrical
power (peak).
IV. SPINNAKER SYSTEM SOFTWARE
SpiNNaker software can be categorized into that which
runs on the SpiNNaker system itself and that which runs
on other systems, some of which may interact with
SpiNNaker. The majority of software that runs on the
SpiNNaker chips is written in C. This software can be
subdivided into control software (a primitive operating
system) and application software which performs the
user’s computations.
The primary interface between SpiNNaker systems and
the outside world is Ethernet and IP-based protocols. Every
SpiNNaker chip has an Ethernet interface and typically one
chip per PCB uses this interface. This is used to download
code and data to SpiNNaker and to gather results from
applications. For some applications, this (100 Mb/s) interface is a bottleneck on getting data to and from SpiNNaker,
and we are investigating the use of gigabit links provided by
FPGAs on SpiNNaker PCBs to improve this.
A. SpiNNaker Software
The control software that runs on SpiNNaker systems
is known as the SpiNNaker Control and Monitor Program
(SC&MP). The SpiNNaker chips contain primary bootstrap code which allows the loading of code via the
Ethernet interface or the interchip links, and this is used to
load SC&MP, initially via an Ethernet interface to a single
chip. SC&MP is then propagated to the entire system over
the interchip links; it runs continuously on the core that
has been selected as the monitor processor and provides a
range of services to the outside world to allow applications
to be loaded on the remaining 16 or 17 application cores on
each chip.
Fig. 3. Inside a SpiNNaker package. The SpiNNaker chip is mounted on
the substrate, then a 128-MB mobile DDR SDRAM is stacked on top of it,
and the connections are made inside the package with gold wire
bonding. The packaging was carried out by Unisem Europe Ltd.
Fig. 2. A 48-node SpiNNaker PCB. This circuit board incorporates
48 SpiNNaker packages (center) with a total of 864 ARM968 processor
cores, three FPGAs (top) for high-speed inter-PCB communications
through serial advanced technology attachment connectors (top left
and right), with onboard power regulation (bottom).
Furber et al: The SpiNNaker Project
Vol. 102, No. 5, May 2014 | Proceedings of the IEEE
A simple packet protocol known as SpiNNaker
Datagram Protocol (SDP) is used within the SpiNNaker
system. SC&MP acts as a router for SDP packets allowing
them to be sent to or from any core in the system and also
via Ethernet to external endpoints. This protocol forms the
basis for application loading and high-level communication
between SpiNNaker chips and/or external machines.
Within individual chips, SDP packets are exchanged
between cores using a shared-memory interface. Between
chips, SDP is transported as sequences of point-to-point
packets conveyed by the interchip links. To carry SDP out
of the system, the packets are embedded in UDP/IP
packets and sent via the Ethernet interface to external
endpoints.
A SpiNNaker ‘‘application’’ is a program that runs on
one or many of the application cores on a SpiNNaker
system. It will typically be written in C and utilize either
SDP or multicast packets for its communication needs.
Because of the limited code and data size provided by the
on-chip memories in SpiNNaker, there is little room for
operating system support and so only minimal ancillary
code can be loaded along with the application. Each
application is linked with a support library known as
SpiNNaker Application Runtime Kernel (SARK). SARK
provides startup code for the application core to set up the
runtime environment for the application. It also provides a
library of functions for the application such as memory
allocation and interrupt control. SARK also maintains a
communications interface with SC&MP running on the
monitor processor that allows the application to communicate with and be controlled by other SpiNNaker chips or
external systems. Protocols running on top of SDP are used
to achieve this functionality.
An application is built using an ARM cross compiler
and linked with SARK and any other runtime libraries that
it requires. The output file from the linking stage is
converted to a format known as application load and
execute (APLX) which is understood by a simple loader
which is part of SC&MP. The APLX file can then be
downloaded to the SpiNNaker system where it is loaded
into the appropriate parts of memory of the relevant
application cores by the SC&MP loader.
Most SpiNNaker applications make use of an event
management library known as the Spin1 API. This provides
facilities for associating common interrupts with event
handling code and for managing queues of events. While
the processor is not processing events it is in a low-power
sleep mode. This API can be viewed as a software layer
between the user’s application and the underlying
hardware. To facilitate SpiNNaker program development
using the API, an emulator has been developed which
provides the same set of library calls as the Spin1 API but
which runs on a Linux workstation. This allows users
without SpiNNaker hardware to develop and debug
SpiNNaker applications and to familiarize themselves
with the programming model.
B. Host Software
We refer to the workstation that controls a SpiNNaker
system as the ‘‘host.’’ A variety of SpiNNaker-related host
software has been developed within the project. A number
of tools have been developed to download applications to
SpiNNaker systems. The ‘‘ybug’’ program provides a
command line interface for this function and also allows
scripted control of the system. A number of application
programmer interfaces (APIs) that implement interfaces
based on SDP have been developed in C, Perl, and Python.
These allow programmed control of a SpiNNaker system to
allow applications to be downloaded and controlled and
results uploaded.
In addition, a number of ‘‘visualizer’’ applications have
been produced which allow the results of SpiNNaker
applications to be viewed on the host system. The simplest
of these just allows plain text output to be displayed on the
host while more sophisticated visualizers display data in
graphical form such as the raster plot of firing spikes in a
neural network simulation or the potentials inside a single
neuron (Fig. 4).
The provision of input data to SpiNNaker applications
can also require host software to provide these data. One
such application is the ‘‘spike server’’ which is used to
provide spikes (neural events) in real time to a neural
simulation running on SpiNNaker.
A significant part of the SpiNNaker software effort has
been the development of programs that map complex
problems onto the SpiNNaker hardware. A typical example
is a neural network simulation where individual neurons
or groups of neurons have to be allocated to cores in the
system and the routing tables set up to allow them to
communicate appropriately for the connectivity of the
network. The ‘‘PACMAN’’ program, which is described in
Section VIII, is typical of this class of program .
Fig. 5 shows the arrangement of the various software
components which make up a SpiNNaker system.
Fig. 4. Example output from a SpiNNaker visualizer.
Furber et al: The SpiNNaker Project
Proceedings of the IEEE | Vol. 102, No. 5, May 2014
V. EVENT-DRIVEN SOFTWARE MODEL
The programming model employed on SpiNNaker is that of a
real-time event-driven system. The application processors
have a base state, which is halted and waiting for an interrupt,
contributing to the overall energy efficiency of the system. In
the standard neural modeling application, there are three
principal events that cause the processor to wake up.
An incoming spike packet. This will usually cause
the processor to initiate a direct memory access
(DMA) transfer from SDRAM of the synaptic data
structures associated with the source of this spike.
DMA complete. Once the synaptic data have been
transferred, the processor must process the data.
One-millisecond timer tick. Each processor has a
local timer that marks the passage of time, and
each millisecond (typically, the interval is programmable) the processor will compute a further
integration step in the neuron dynamics.
Of course, these events are asynchronous and unpredictable, so the software running on the processor must be
capable of prioritizing the events and handling multiple
overlapping requests. This is achieved through the use of a
real-time kernel that underpins the event-driven operation
of each application processor, and presents a straightforward API to the user, who can build applications on top of
the API entirely in C.
VI. SPINNAKER APPLICATION
PROGRAMMING INTERFACE
The SpiNNaker application programming interface (spin1
API) provides an execution environment that supports a
lightweight, event-driven programming model. A central goal
of the model is to save energy by keeping the cores in a lowpower state, only responding to events of interest. To this
effect, application programs do not control execution flow;
they can only indicate the functions, referred to as callbacks,
to be executed when specific events, such as the arrival of a
packet, the completion of a DMA transfer, or the lapse of a
periodic time interval, occur. The callback mechanism is also
used to hide the details of the interrupt subsystem, which is
handled directly and efficiently by the API.
Fig. 6 shows the basic architecture of the event-driven
framework. Application developers write callback routines
that are associated with events of interest and register
them with the API at a priority level, which defines them
as queueable or non-queueable. When the corresponding
event occurs, the scheduler either executes the callback
immediately and atomically (in the case of a nonqueueable callback) or places it into a scheduling queue
at a position according to its priority (in the case of a
queueable callback). When control is returned to the
dispatcher (following the completion of a callback) the
highest priority queueable callback is executed. Queueable
callbacks do not necessarily execute atomically: they may
be preempted by non-queueable callbacks if a
corresponding event occurs during their execution.
The dispatcher goes to sleep (in the low-power
consumption ‘‘wait for interrupt’’ state, where the processor
core clock is turned off) when the callback queues are empty
and will be awakened by any event. Application developers
can designate one non-queueable callback as the preeminent
callback, which has the highest priority and can preempt
other non-queueable callbacks as well as all queueable ones.
The API provides support for callbacks to control entry and
exit from critical sections to prevent higher priority callbacks
interrupting them at a bad time, e.g., during access to a
shared resource.
This real-time kernel is scalable to very large numbers
of processors, but is best suited to relatively simple models
running on each processor. Clearly, the system will come
to a halt if no events are generated, and real-time
performance will be lost if a processor is overwhelmed
by incoming events. In practice, careful mapping of a
model onto the system can avoid both eventualities.
Fig. 5. The various software components running on the host machine, the root node, and other SpiNNaker nodes.
Furber et al: The SpiNNaker Project
Vol. 102, No. 5, May 2014 | Proceedings of the IEEE
VII. EXAMPLE LOW-LEVEL
APPLICATION
As a simple example of a parallel program that runs on top of
the SpiNNaker API, here are the key features of a simple
example that implements Conway’s Life cellular automaton.
First, the program should include the API calls:
#include hspin1api:hi
Then, we need routines to set up the initial state of the
automaton and the routing tables. In this case, setting up
the routing tables is by far the most complex aspect of the
programming task as the Life neighbor connections must
be established between processors across chip boundaries.
void set up route tables
ðuintchip; uintcoreÞf. . .g
void init Life state ðuintchip; uintcoreÞf. . .g
Now we must define the event-driven callback routines.
In this example, the relevant events are timer tick and an
incoming packet:
void tick callback ðuintticks; uintdummyÞf. . .g
void pkt in ðuintkey; uintdataÞf. . .g
The simulation is started on each processor from
c_main. The chip and core addresses are found, then the
initialization routines are called:
void c main ðvoidÞ
uint chip ¼ spin1 get chip id ðÞ;
uint core ¼ spin1 get core id ðÞ;
set up route tables ðchip; coreÞ;
init Life state ðchip; coreÞ;
The timer period is set to 1 ms, and the event callbacks
are set up with appropriate priorities (packet received is
usually at the highest priority):
spin1 set timer tickð1000Þ;
spin1 callback on ðTIMER TICK;
tick callback; 1Þ;
spin1 callbackonðMC PACKET RECEIVED,
pkt in; 1Þ;
Finally, the simulation is started:
spin1 startðÞ;
Fig. 6. Event-driven software framework.
Furber et al: The SpiNNaker Project
Proceedings of the IEEE | Vol. 102, No. 5, May 2014
VIII. PARTIONING AND
CONFIGURATION MANAGER
The example described in Section VII shows how APIbased applications can set up the simulation parameters,
SDRAM content, and routing tables with an algorithmic
process. While for simple or highly structured problems
this is possible, modeling networks with arbitrary interconnectivity and arbitrary neural types is a problem where
a further level of abstraction can be introduced. Configuring a million-core machine, with each core modeling up
to a thousand neurons and a million synapses, rapidly
becomes an intractable problem: one billion neurons need
to be mapped and one trillion synapses need to be routed
to implement a user-specified model.
To solve this problem, we introduce PACMAN , a
software layer that enables users to write their model using
a standardized interface, translate it, and run it on
SpiNNaker. The software is designed to keep different
concerns separated: users interface with the platform
through domain-specific, neural languages already present
in the scientific milieu, such as PyNN or Nengo .
PACMAN is the set of algorithms that translate a model
into machine-executable code. Such algorithms operate on
data representing the network model, information about
the system (topology, fault status, etc.), and methods for
data structure translation.
PACMAN maps, routes, and translates network models
using populations of neurons and projections between them,
rather than single neurons and synapses. This approach
reduces the complexity of the algorithms involved in the
translation process, by exploiting the hierarchies present in a
neural network. This choice is justified by studies on the
structure of the central nervous systems, where functionally
segregated areas are interconnected by axonal pathways ,
and where cortical areas show a remarkably regular laminar
structure, with different layers of neurons stereotypically
connected in a canonical circuit . Finally, many neural
languages , , , use this abstraction natively,
making it a natural choice.
Using a neural language as a user interface makes the
platform more accessible to nonexperts, giving the users a
familiar environment to develop models and analyze
results, while hiding the complexity of configuring a
parallel system and encouraging model sharing across
different platforms. The translation process is performed
by PACMAN as illustrated in Fig. 7, which shows the flow
of the algorithms used to translate and execute the models
(left) and the data representations they work on (right).
The model is represented in terms of populations and
projections in the model view. It is then partitioned,
splitting populations, while preserving their interconnectivity structure, accordingly to machine-specific constraints, depending on the neural and synaptic capacity
of each core. The model is represented in a digraph-like
structure (PACMAN view), and then mapped and routed
on a physical machine instance (system view), using the
information present in the system library. Finally, the
whole model is translated into machine-executable code
for each component (ARM cores, SDRAM, routers), using
the translation mechanism stored in the model library,
loaded onto the system, and executed.
A simple example network is illustrated in Fig. 8 (left):
excitatory and inhibitory populations are recurrently
interconnected. The ratio of excitatory to inhibitory
neurons is set to 4 : 1 to keep a balance between excitation
and inhibition.
The network can be represented in PyNN , first by
creating the two populations of neurons, for a total of n
neurons, with a set of parameters:
cell params ¼ f‘tau refrac’: 5.0, ‘v thresh’:50.0,
‘v reset’:60.0, ‘tau m’: 20.0, ‘tau syn E’: 5.0,
‘tau syn I’: 10.0, ‘v rest’:49.0,
‘cm’: 0.2}
ex ¼ Populationðn  n=5; IF curr exp;
cell paramsÞ
in ¼ Populationðn=5; IF curr exp, cell paramsÞ
The resting potential is located above the threshold
potential to induce spontaneous firing in all cells. Populations
are interconnected by means of a FixedProbability-Connector,
which connects all the neurons in the presynaptic population
Fig. 7. The flow of algorithms (left) and the data representations they
work on (right) within PACMAN.
Fig. 8. Example network (left) with one excitatory and one inhibitory
population with a size ratio of 4 : 1 is mapped by PACMAN onto 60
processors on four SpiNNaker chips (right).
Furber et al: The SpiNNaker Project
Vol. 102, No. 5, May 2014 | Proceedings of the IEEE
to all the neurons in the postsynaptic population with a probability p, weight w, and delay d.
con ¼ FixedProbabilityConnector ðp connect ¼
e e ¼ Projection ðex; ex; con; target ¼
‘excitatory’)
e i ¼ Projection ðex; in; con; target ¼
‘excitatory’)
i i ¼ Projection ðin; in; con; target ¼
‘inhibitory’)
i e ¼ Projection ðin; ex; con; target ¼
‘inhibitory’)
All the projections coming from the excitatory population
target excitatory synapses; conversely, all the projections
coming from the inhibitory population target inhibitory
PACMAN automatically partitions and maps the
network as illustrated in Fig. 8 (right), which shows an
example where the total number of neurons n is 6000, and
each core maps 100 neurons. As a result, the model needs
to be partitioned into 48 excitatory and 12 inhibitory
subgroups, each to be allocated to a single core of a
physical machine, with the system library providing the
geometry (in this case, a four-chip board) and the
functional status of the platform. The model library
provides the translation methods for the IF_curr_exp
neuron type (a leaky integrate and fire with exponential
decaying synapses), its parameters, and its synapses. Fig. 9
shows results of 1 s of simulation in the form of a raster
plot, where each dot represents a spike from a neuron
(ordinate) in time (abscissa). Red (blue) dots represent
spikes from excitatory (inhibitory) neurons; the interconnectivity parameters are set to give rise to the
oscillatory activity shown in the figure.
IX. TYPICAL APPLICATIONS
In this section, we review some scenarios highlighting the
flexibility of the SpiNNaker platform, and present an
experiment running on a robot equipped with AER sensors
and a 48-node SpiNNaker board.
With the hardware and software infrastructure presented in the previous sections we have simulated networks
with up to 250 000 neurons and 80 million synapses in real
time on a 48-node SpiNNaker board (as shown in Fig. 2)
within a power budget of 1 W per SpiNNaker package
(containing a SpiNNaker chip and a 128-MB SDRAM; see
Fig. 3). In terms of spike delivery (the dominant cost in
neural simulations ) and power consumption, these
experiments show 1.8 billion connections per second, using
a few nanojoules per event and per neuron , and
represent the maximum sustainable throughput of the
system with the current software infrastructure.
Good power efficiency has also been demonstrated in a
biologically plausible model of cortical microcircuitry
inspired by previous work , , comprising 10 000
Izhikevich neurons, replicating spiking dynamics found in
the cortex, and 40 million synapses in real time , while
the flexibility of the platform can be used to explore novel
algorithms for learning .
A. Interface With Nengo
While with PyNN it is possible to define arbitrary
network structures, using the neural engineering framework (NEF) , it is possible to encode functions and
dynamical systems in networks of spiking neurons. Using
the NEF, it is possible to build complex cognitive
architectures such as SPAUN , a spike-based functional
model of the brain that makes comparisons with human
neural and behavioral data possible. SpiNNaker has,
therefore, been interfaced with Nengo , the software
that implements the NEF, enabling users to create neural
networks by specifying the functions to be computed .
Nengo translates the functions into neural circuitry by
calculating neuronal and connectivity parameters, while
PACMAN distributes and configures the model on the
board. Through the use of the NEF, SpiNNaker becomes a
‘‘neural computational box’’: input values and vectors are
encoded in spiking activity using the NEF principles
directly on the SpiNNaker board. The desired computation
is performed in real time by spiking neurons, and output
values and vectors are decoded from spiking activity.
Interfacing with Nengo shows how different front–ends
can be interfaced with PACMAN and how flexibly the
platform can be programmed with specialized neural
kernels, such as the ones performing the NEF encoding
and decoding processes.
B. Interface With AER sensors
Biological inspiration is not confined to the exploration
of computational architectures and methods, but is also
Fig. 9. Raster plot of the results of running the simulation of the
network shown in Fig. 8.Each dot represents one neural spike; red dots
are excitatory neurons, and blue dots are inhibitory neurons.
Oscillatory activity is visible across the network.
Furber et al: The SpiNNaker Project
Proceedings of the IEEE | Vol. 102, No. 5, May 2014
extended to neuromorphic sensors. Millisecond-precise
pulse encoding has been used to explain the ability of the
visual system to process information and to recognize
complex, dynamical scenes quickly . With the first
observable differences in the temporal lobe starting after
150 ms of the stimulus onset, and with several synaptic stages
required to arrive at the infero-temporal cortex (IT, the visual
area where object recognition takes place), neurons can emit
at most one spike to encode the information, and are believed
to encode it in the spike timing .
AER sensors can be used to exploit the temporal
characteristics of sensory information with event-based
approaches. Silicon retinae – , for example, take
inspiration from their biological counterparts to implement
an alternative approach to frame-based image processing on a
neuromorphic substrate. Each pixel operates asynchronously, sending an AER message within a few microseconds of a
local light intensity change without having to wait for a
complete frame to be scanned, resulting in a reduction of
latency and redundancy in visual information transmission.
For an example showing the benefits of event-based over
frame-based systems, see the European Union ‘‘Convolution
Address Event Representation (AER) Vision Architecture for
Real-Time’’ project .
These sensors use native event-based processing and AER
representation to encode sensory information, and can,
therefore, be interconnected directly to SpiNNaker, which
acts as an event-based computing platform. In collaboration
with the Instituto de Microelectronica de Seville (Sevilla,
Spain) we have connected a silicon retina to SpiNNaker
using an FPGA , which translates incoming retinal AER
events to the self-timed 2-of-7 protocol used by SpiNNaker
interchip links, directly injecting spikes (MC packets) into
the packet-switched network fabric. Using this mechanism,
the sensor is represented on SpiNNaker as a ‘‘virtual chip.’’
At the model level, the silicon retina can be instantiated in
pol 0; pol 1 ¼ p:instantiate retinað Þ
creating two populations (‘‘pol_0’’ and ‘‘pol_1’’, one for
each ‘‘polarity,’’ encoding increasing and decreasing
luminance, respectively) where neurons are topographically organized in a 2-D visual field. These populations
produce spikes whenever the silicon retina emits an event,
and can arbitrarily be interconnected to other populations
in the model. PACMAN automatically maps each population to a specific model instantiation, preserving the
connectivity information.
Analogous interfaces with AER sensors have been
developed in collaboration with the Institute of Neuroinformatics (Zurich, Switzerland; using the DVS sensor 
and the ‘‘silicon cochlea’’ ), with the Biology Group at
the University of Osaka (Osaka, Japan; using a sensor
inspired by the sustained and transient responses of the
retina ), and with the Institute of Vision (Paris, France;
using the ATIS silicon retina ).
C. Integration With Robotic Platforms
While integration with AER sensors exploits the eventdriven nature of the system, interfacing it with robotic
platforms in real environments shows SpiNNaker’s realtime characteristics.
As with AER sensors, the robotic platform becomes
available at the model level using PyNN or Nengo, while the
system is configured automatically using PACMAN, enabling
message transmission to and from the robot and the sensors
through a small customized interface board . The robot is
a custom omnidirectional mobile platform, with embedded
low-level motor control and elementary sensory systems,
developed by the Neuroscientific System Theory group of the
Technische Universita¨t Mu¨nchen (Munich, Germany). The
overall system is a standalone, autonomous, reconfigurable
robotic platform with no personal computer in the loop.
We demonstrate a closed perception–action loop in an
example where the robot agent has to discriminate between
two different stimuli and move toward the preferred one
(a ‘‘þ’’), while backing off from the detractor (an ‘‘’’).
This is a small model that uses less than 10% of the
resources on the 48-node board, but it serves to illustrate a
number of the capabilities of the system.
The network structure used is represented in Fig. 10. The
two populations representing the different polarities of a
128 128 silicon retina are instantiated, as illustrated in
Section IX-B. These populations are connected to four
different feature maps, representing the result of the
convolution between the retinal input and a kernel
represented as the white insert in the four feature maps in
Fig. 10, where the black lines represent excitatory connections while the white surround represents inhibitory flanks.
This operation, computed in parallel by all feature maps by
means of spiking events, is similar to the one performed by
the mammalian primary visual cortex, where cells are
selectively active accordingly to the stimulus orientation
 , as previously done in a model of visual attention
running on a four-node SpiNNaker board . Different
feature maps inhibit each other in order to enhance response
contrast. The following layer behaves as a local combination
of oriented edge detectors, similar to the first layers of the
HMAX model, a model of object recognition inspired by the
visual cortex . If the ‘‘þ’’ is recognized (as a combination
of vertical and horizontal edges), the agent is driven forward
toward the preferred stimulus; conversely, if an ‘‘’’ is
detected as a combination of þ=45 oriented lines, the
robot moves backward.
Robot movements are controlled by the output population, comprising two ‘‘motor’’ neurons (one for moving forward and one for moving backward), represented by the two
vertical bars in Fig. 10.
The retina and the robot are accessible through PyNN,
which is also used to describe the rest of the network
model, performing different steps of visual processing and
orienting its response to the location where a preferred
stimulus is detected.
Furber et al: The SpiNNaker Project
Vol. 102, No. 5, May 2014 | Proceedings of the IEEE
X. FUTURE PLANS
Current SpiNNaker hardware has seen use across the
computational neuroscience and neurorobotic communities. All of the major hardware functions required to build
larger machines have now been developed and tested, and
the remaining tasks to build larger machines are now
primarily related to the manufacture of further packages
A major commitment over the next two years is to deliver a
machinewith atleasthalfamillionprocessorsasacontribution
to the European Union Flagship Human Brain Project (HBP),
where SpiNNaker willbeone of the neuromorphic ‘‘platforms’’
offered to the wider HBP community.
An earlier, less formal, commitment is to demonstrate
the capability of SpiNNaker to support a real-time implementation of the University of Waterloo (Waterloo, ON,
Canada) SPAUN model . This is expected to require a
system of around 36 48-node SpiNNaker boards, or 30 000
processors, though this estimate should come down with
Nengo support for sparse connectivity and reduced firing
rates, and will be a solid demonstration of the capability of
the SpiNNaker machine as a platform to support large-scale
real-time spiking neural models.
XI. RELATED WORK
While SpiNNaker represents a particular combination of
digital many-core computing with a lightweight communications infrastructure tuned to modeling large-scale
spiking neural networks in biological real time, there are a
Fig. 10. Example robotic closed perception–action loop. A ‘‘þ’’ is shown to the robot, which extracts and combines the vertical and horizontal
lines, moving forward. Gray kernels and dashed lines represent the fact that the pathways for the ‘‘’’ detection are not activated,
as a ‘‘þ’’ is presented.
Furber et al: The SpiNNaker Project
Proceedings of the IEEE | Vol. 102, No. 5, May 2014
number of other designs that take a different approach to
achieve similar end goals . These various approaches
can be classified according to whether they use digital or
analog technology to model the neurons and synapses, the
communications topology employed, and the support for
synaptic plasticity.
Digital models may be implemented on conventional
general-purpose computers, including cluster machines
and high-performance computers, or on special-purpose
hardware such as FPGAs , , graphics professor
units , or custom silicon . Analog models may
be subthreshold , whereupon biological real-time
performance is achievable, or above threshold , where
the circuits are likely to be much faster than biological real
time. Notable large-scale projects include the following.
The Stanford Neurogrid employs subthreshold analog circuits with digital spanning tree AER
communications for real-time neural modeling. Neurogrid can model a million neurons in real
time while consuming only 3 W. It combines
unicast and multicast digital routing with analog
signaling across a local ‘‘diffusion network.’’
The IBM neurosynaptic core employs custom
digital circuits to achieve a one-to-one correspondence between the hardware and software simulation
models. It is intended to form a generic cognitive
subsystem . It uses AER communication.
The Heidelberg HICANN system employs
wafer-scale above threshold analog circuits that
operate at 104x biological real time using a twolayer AER protocol, one layer for intrawafer
communication and a second layer for interwafer
communication.
The Cambridge BlueHive system employs digital
circuits on FPGAs to deliver real-time performance.
The communication is not pure AER; multicast is
implemented using a set of ‘‘fan-out’’ messages that
carry the destination, weight, and delay.
These examples illustrate the diversity of approaches
taken to address the problem of modeling large-scale
systems of spiking neurons in real time or faster. There
are arguments on both sides of the analog/digital divide
(for example, energy-efficiency favors analog, whereas
flexibility and repeatability favors digital), and on most
other design decisions, so the area is still wide open to new
ideas, and rather lacking in robust benchmarks that can
be used to make quantitative comparisons between alternative approaches.
XII. CONCLUSION
The SpiNNaker project has been 15 years since conception
and eight years in (funded) execution. Much time and
effort has gone into understanding the brain-modeling
problem domain and developing the architecture, silicon,
and software infrastructure. While the software development will be ongoing, the architecture and silicon are now
working reliably and delivering very much as originally
anticipated .
The process of delivering the potential of the SpiNNaker
platform is now underway, and early indications are largely
positive. The platform is proving flexible, relatively easy to
use (though there is always room for improvement in this
dimension), and capable of delivering useful results across a
wide range of application areas.
As the platform is scaled up toward the ultimate
million-core machine, new challenges will emerge, particularly in the area of management, application mapping and
loading performance, the observability of activity within
the machine, and most notably with debugging large-scale
models running on the machine. All of these are ongoing
areas of research and development, but with help and
feedback from a growing (and so far very forgiving)
community of users, and secure funding within the HBP
alongside a number of other funded projects that will
support extensive use of the platform at the University of
Manchester [including a European Research Council
Advanced Grant and several Engineering and Physical
Sciences Research Council (EPSRC)-funded collaborations],
we are committed to continued improvement of the
capabilities of the platform.
The time is right to scale up our ambition to understand the information processing principles at work in the
brain, and the SpiNNaker platform has been designed to
deliver a broad capability to support this ambition. The
next five years will be crucial in determining the extent to
which we can succeed in delivering a platform with the
capabilities required to support the global brain research
program. h
Acknowledgment
The SpiNNaker project has benefited from contributions from many people in the team at the University
Manchester, collaborators at the universities of
Southampton, Cambridge, and Sheffield, industry partners,
and many external collaborators, only some of whom has
there been space to mention in the text, but the authors
would like to note here the contribution of J. Conradt of
the Technische Universita¨t Mu¨nchen (Munich, Germany)
who developed the robot platform shown in Fig. 10. They
particularly wish also to acknowledge the benefits accrued
from participation in the Capo Caccia and Telluride
neuromorphic workshops, and they are grateful to the
organizers for the opportunities for collaborations that have
emerged from these workshops. The authors would also
like to acknowledge the helpful comments and feedback
from the anonymous reviewers of the first draft of this
Furber et al: The SpiNNaker Project
Vol. 102, No. 5, May 2014 | Proceedings of the IEEE