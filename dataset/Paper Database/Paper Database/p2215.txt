Electronic copy available at: 
The Reproducibility Project: A model of large-scale collaboration for empirical research on
reproducibility
Open Science Collaboration1
The goal of science is to accumulate knowledge that answers questions such as “How do
things work?” and “Why do they work that way?” Scientists use a variety of methodologies to
describe, predict, and explain natural phenomena. These methods are so diverse that it is
difficult to define a unique scientific method, although all scientific methodologies share the
assumption of repeatability . This can be defined narrowly, as in the repetition of a simulation by re-executing a
program, or, more broadly, as in the repetition of an experimental paradigm by re-administering
the procedures in a new setting. The key value of repeatability is its ability to verify the validity
of prior results, ensuring that we can build a base of scientific knowledge that does not depend
on the authority or reliability of a single, original source.
Repeatability, which we will refer to as reproducibility or replicability, provides an
additional, more subtle, contribution to the scientific process: guarding against false-positive
findings. False-positives are observed effects that were inferred to have occurred because of
features of the research design but actually occurred because of chance factors. Scientific
knowledge is often gained by drawing inferences about a population based on data collected
from a sample of individuals. Since this represents an example of induction, the knowledge
gained in this way is always uncertain. The best a researcher can do is estimate the likelihood
that the research findings are a product of ordinary random sampling variability. This provides a
measure of the confidence they may have in the result. Independently reproducing the results
reduces the probability that the original finding occurred by chance alone and, therefore,
increases the confidence researchers may have in the inference. In this way, reproducing results
guards against the proliferation of false-positive findings.
Given the benefits of replication to knowledge-building, one might expect that evidence
of reproducibility would be published frequently. Surprisingly, this is not the case. Publishing
replications of research procedures is rare . One recent review of
psychological science estimated that only 0.15% of published studies were attempts to directly
replicate a previous finding . As a consequence, there is a proliferation of
scientific findings, but little systematic effort to verify their validity, possibly leading to a
proliferation of irreproducible results . Despite the low occurrence of published replication studies, there is evidence that
scientists believe in the value of replication and support its inclusion as part of the public record.
For example, a survey of almost 1300 psychologists found support for reserving at least 20% of
journal space to direct replications .
In this chapter, we first briefly review why replications are highly valued but rarely
published. Then we describe a collaborative effort—the Reproducibility Project—to estimate the
rate and predictors of reproducibility in psychological science. Finally, we detail how we are
conducting this project as a large-scale, distributed, open collaboration. A description of the
procedures and challenges may assist and inspire other teams to conduct similar projects in other
areas of science.
Current Incentive Structures Discourage Replication
The ultimate purpose of science is the accumulation of knowledge. The most exciting
science takes place on the periphery of knowledge, where researchers suggest novel ideas,
consider new possibilities, and delve into the unknown. As a consequence, innovation is a
highly prized scientific contribution, and the generation of new theories, new methods, and new
evidence is highly rewarded. Replications, in contrast, do not break new ground; they instead
assess whether previous innovations are accurate. As a result, there are currently few incentives
for conducting and publishing direct replications of previously published research .
Current journal publication practices discourage replications . Journal editors hope to maximize the impact of their journals, and are
inclined to encourage contributions that are associated with the greatest prestige. As a
consequence, all journals encourage innovative research, and few actively solicit replications,
whether successful or unsuccessful . An obvious response to these
publication practices is to create journals devoted to publishing replications or null results. Of
the many attempts to start such a journal over the last 30 years, none have succeeded. Several
versions exist today (e.g., 
 but challenges remain: defining a journal as a publisher of
papers that no other journal will publish ensures its low status . It is not in a
scientist’s interest to publish in such a journal.
Because prestigious journals do not have a strong incentive to publish replications,
researchers do not have a strong incentive to conduct them . Scientists make reasonable assessments of how they should spend their
time. Publication is the central means of career advancement for scientists. Given the choice
between replication and pursuing novelty, career researchers commonly conclude that their time
is better spent pursuing novel research. This may be especially true for researchers that do not
yet have academic tenure. The business of science therefore encourages researchers to conduct
novel research rather than replicate findings that have already been published.
Complicating matters is the presence of additional forces rewarding positive over
negative results. A common belief is that it is easier to obtain a negative result erroneously than
it is to obtain a positive result erroneously. This is true when using statistical techniques and
sample sizes designed to detect differences , and when designs are
underpowered .
Although both of these features characterize most fields of psychological research, there is no
reason that researchers cannot design studies so that they will be informative no matter what the
outcome might be . There are many reasons why a null result may be
observed erroneously such as imprecise measurement, poor experimental design, or other forms
of random error . There are also many reasons why a
positive result may be observed erroneously such as introducing artifacts into the research design
 , experimenter bias, demand characteristics, systematic apparatus
malfunction, or other forms of systematic error . Further, false positives can
be inflated through selective reporting and adventurous data analytic strategies . There is presently little basis other than power of research designs to systematically
prefer positive results compared to negative results. Decisions about whether to take a positive
or negative result seriously should be based on evaluation of the research design, not the research
Layered on top of legitimate epistemological considerations are cultural forces that favor
significant and consistent results over inconsistent or ambiguous results. These incentives encourage researchers to
obtain and publish positive, significant results, and to suppress or ignore inconsistencies that
disrupt the aesthetic appeal of the findings. As examples, researchers might decide to stop data
collection if preliminary analyses suggest that the findings will be unlikely to reach conventional
significance, examine multiple variables or conditions and report only the subset that “worked,”
or selectively report studies, accepting those that confirm the hypothesis as effective designs and
dismissing those that do not confirm the hypothesis as pilots or methodologically flawed because
they fail to support the hypothesis . These practices, and others, can
increase the likelihood of positive results and the aesthetic appeal of the findings, while also
inflating the likelihood that the results are false positives . This is not to
say that researchers engage in these practices with deliberate intent to deceive or manufacture
false effects. Rather, these are natural consequences of motivated reasoning .
When a particular outcome is better for the self, then decision-making can be influenced by
factors that maximize the likelihood of that outcome. Researchers may tend to carry out novel
scientific studies with a confirmatory bias such that they—without conscious intent—guide
themselves to find support for their hypotheses .
Publishing Incentives Combined with a Lack of Replication Incentives May Reduce
Reproducibility
The strong incentives to publish novel, positive, and clean results may lead to problems.
For one, the presence of these incentives leads to a larger proportion of false positives, which
produces a misleading literature and makes it more likely that future research will be based on
claims that are actually false. Any individual result is ambiguous; but because the truth value of
a claim is based on the aggregate of individual observations, ignoring particular results
undermines the accuracy of a field’s collective knowledge, both by inflating the true size of the
effect,and by concealing potential limitations to the effect’s generalizability. Knowing the rate
of false positives in the published literature would clarify the magnitude of the problem, and
indicate whether significant intervention is needed. However, there is very little empirical
evidence on the rate of false positives. Simulations, surveys, and reasoned arguments provide
some evidence that the false positive-rate could be very high . For example, asking psychologists about the proportion of
research findings that would be reproduced from their journals in a direct replication yielded an
estimate of 53% . The two known empirical estimates of non-random
samples of studies in biomedicine provide disturbing reproducibility estimates of 25% or less
 . There are few other existing attempts to estimate the
rate of false positives in any field of science.
The theme of this article is reproducibility, and the focus of this section is on the primary
concern of irreproducibility: that the original results are false. Note, however, that the
reproducibility rate is not necessarily equivalent to the false positive rate. The maximum
reproducibility rate is 1 minus the rate of false positives tolerated by a field. The ubiquitous
alpha level of .05 implies a false-positive tolerance of 5%, meaning a reproducibility rate of 95%.
However, in practice, there are many reasons why a true effect may fail to replicate. A lowpowered replication, one with an insufficient number of data points to observe a difference
between conditions, can fail for mathematical rather than empirical reasons.
The reproducibility rate can be lowered further for other reasons. Imprecise reporting
practices can inadvertently omit crucial details necessary to make research designs reproducible.
Description of the methodology—a core feature of scientific practice—may become more
illustrative than substantive. This could be exacerbated by editorial trends encouraging shortreport formats . Even when the chance to offer additional online
material about methods occurs, it may not be taken. For example, we conducted a Google
Scholar search on articles published in Psychological Science—a short-report format journal—
for the year 2011. We found that only 16.8% of articles included the phrase “Supplemental
Material” denoting additional material available online, even without considering whether or not
that material gave a full accounting of methodology. As a consequence, when replication does
occur, the replicating researchers may find reproduction of the original procedure difficult
because key elements of the methodology were not published. This makes it difficult both to
clarify the conditions under which an effect can be observed and to build a cumulative base of
knowledge.
In sum, false positives and weak methodological specification are both challenges for
reproducibility. The current system of incentives in science does not reward researchers for
conducting or reporting replications. As a consequence, there is little opportunity to estimate the
reproducibility rate, to filter out those initial effects that were false positives, and to improve
specification of those initial effects that are true but specified inadequately. We developed the
Reproducibility Project to examine these issues by generating an empirical estimate of
reproducibility and to identify the predictors of reproducibility.
The Reproducibility Project
The Reproducibility Project began in November 2011 with the goal of empirically
estimating the reproducibility of psychological science. The concept was simple: Take a sample
of findings from the published literature in psychology and see how many of them could be
replicated. The implementation, however, is more difficult than the conception. Replicating a
large number of findings to produce an estimate of reproducibility is a mammoth undertaking,
requiring much time and diverse skills. Given the incentive structures for publishing, only a
person who does not mind stifling their own career success would take on such an effort on their
own even if they valued the goal. Our solution was to minimize the costs for any one researcher
by making it a massively collaborative project.
The Reproducibility Project is an open collaboration to which anyone can contribute
according to their skills and available resources. Project tasks are distributed among the research
team, minimizing the demand on each individual contributor, but still allowing for a large-scale
research design. As of this writing , 106 researchers have joined the project, a
complete research protocol has been established, and 50 replication studies are underway or
completed. The project, though incomplete, has already provided important lessons about
conducting such large-scale, distributed projects. The remainder of this chapter describes the
design of the project, what can be learned from the results, and the lessons for conducting a
large-scale collaboration that could be translated to similar efforts in other disciplines.
Project design. To estimate the rate and predictors of reproducibility in the
psychological sciences, we selected a quasi-random sample of studies from three prominent
psychological journals (Journal of Personality and Social Psychology, Journal of Experimental
Psychology: Learning, Memory, and Cognition, and Psychological Science) from the 2008
publication year. We selected studies for replication as follows: Beginning with the first issue of
2008, the first 30 articles that appeared in each journal made up the initial sample. As project
members starting attempting to replicate studies, additional articles were added to the eligible
pool in groups of ten. We pursued this strategy to balance two competing issues. We wanted to
minimize selection biases by having only a small group of articles available for selection at any
one time. Simultaneously, we needed to have a sufficient number of articles available to match
studies with replication teams with resources and expertise.
Each article in the sampling frame was reviewed with a standard coding procedure
( The coding procedure documented: (1) the essential descriptors of
the article such as authors, topic, and main idea; (2) the key finding from one of the studies and
key statistics associated with that finding such as sample size and effect size; (3) features of the
design requiring specialized samples, procedures, or instrumentation; and (4) any other unusual
2 Linked resources are also available via the Reproducibility Project’s page on the Open Science Framework
website: 
or notable features of the study. This coding provided the basis for researchers to rapidly review
and identify a study that they could potentially replicate. Also, coding all articles from the
sampling frame will allow systematic comparison of the articles replicated with those that were
available but not replicated.
Most articles contain more than one study. For the purposes of the Reproducibility
Project, we aimed to replicate a single key finding from a single study. By default, the last study
reported in a given article was the target of replication. If a replication of that study was not
feasible, then the second to last study was considered. If no studies were feasible to replicate,
then the article was excluded from the replication sample. A study was considered feasible for
replication if its primary result could be evaluated with a single inference test, and if a replication
team on the project had sufficient access to the study’s population of interest, materials,
procedure, and expertise. Although we worked to make our sample as representative as possible,
study designs that are difficult to reproduce for practical reasons are less likely to be included. In
psychology, for example, studies with children and clinical samples tend to be more resource
intensive than others. Likewise, it is infeasible to replicate study designs with large samples,
many measurements over time, a focus on one-time historical events, or expensive
instrumentation. It is not obvious whether studies with significant resource challenges would
have more or less reproducible findings as compared to those that have fewer resource
challenges.
Maximizing replication quality. A central concern for the Reproducibility Project was
the quality of replication attempts. Sloppy, non-identical, or under-powered replications would
be unlikely to replicate the original finding, even if that original finding was true. As a
consequence, the study protocol involved many features to maximize quality and make the
replications fair. As a first step, each replication attempt was conducted with a sufficient number
of observations so that replications of true findings would be likely. For each eligible study, a
post-hoc power analysis was performed on the effect of interest from the original study. The
power analysis determined the samples necessary for 80%, 90%, and 95% power to detect a
significant effect consistent with the prior result using the same analysis procedures. Replication
teams planned their sample size aiming for the highest feasible power. All studies were designed
to achieve at least 80% power, and about three fourths of the studies conducted to date have an
anticipated power of 90% or higher.
In another step to maximize replication quality, replication teams contacted the original
authors of each study to request copies of project materials and clarify any important procedures
that did not appear in the original report ( As of this writing, authors
of every original article have shared their materials to assist in the replication efforts, with one
exception. In the exceptional case, the original authors declined to share all materials that they
had created, and declined to disclose the source of materials that they did not own so that the
replication team could seek permission for their use. Even so, a replication attempt of that study
is underway with the replication team using its own judgment on how to best implement the
Next, for all studies, the replication team developed a research methodology that
reproduced the original design as faithfully as possible. Methodologies were written following a
standard template and included measurement instruments, a detailed project procedure, and a
data-analysis plan. Prior to finalizing the procedure, one or two Reproducibility Project
contributors who were not a part of the replication team reviewed this proposed methodology.
The methodology was also sent to the original authors for their review. If the original authors
raised concerns about the design quality, the replication teams attempted to address them. If the
design concerns could not be addressed, those concerns were documented as a priori concerns
raised by the original authors. The evaluations of the original authors were documented as:
endorsing the methods of the replication, raising concerns based on informed judgment or
speculation (that are not part of the published record as constraints on the design), raising
concerns that are based on published empirical evidence of the constraints on the effect, or no
response. This review process minimized design deficiencies in advance of conducting the study
and also obtained explicit ratings of the design quality in advance. These steps should make it
easier to detect post-hoc rationalization if the replication results violate researchers’ expectations.
Some studies that were originally conducted in a laboratory were amenable to replication
via the Internet. Using the web is an excellent method for recruiting additional power for human
research, but it could also alter the likelihood of observing the original effects. Thus, we label
such studies “secondary replications.” These studies remained eligible to be claimed for
“primary replications”—doing the study in the laboratory following the original demonstration.
As of this writing, there were 11 secondary web replications underway in addition to the 50
primary replications. This provides an opportunity to evaluate systematically whether the
change in setting affects reproducibility.
Upon finalization, the replication methodology was registered and added to an online
repository. At this point, data collection could start. After data collection, the replication teams
conducted confirmatory analyses following the registered methodology. The results and
interpretation were documented and submitted to a team member (who was not part of the
replication team) for review. In most cases, an additional attempt was made to contact the
authors of the original study in order to share the results of the replication attempt and to consult
with them as to whether any part of the data collection or data analysis process may have
deviated from that of the original study. Finally, the results of the replication attempt were
written into a final manuscript, which was logged in the central project repository. As additional
replication attempts are completed, the repository is updated and a more complete picture of the
reproducibility of the sample emerges ( 
The project is ongoing. In principle, there need not be an end date. Just as ordinary
science accumulates evidence about the truth-value of claims continuously, the Reproducibility
Project could accumulate evidence about the reproducibility, and ultimately truth value, of its
particular sample of claims continuously. In the nearer term, the collaborative team will
establish a closing date for replication projects to be included in an initial aggregate report. That
aggregate report will provide an estimate of the reproducibility rate of psychological science, and
examine predictors of reproducibility such as the publishing journal, the precision of the original
estimate, and the existence of other replications in the published literature.
What Can and Cannot be Learned from the Reproducibility Project
The Reproducibility Project will produce an estimate of the reproducibility rate of
psychological science. In fact, it will produce multiple estimates, as there are multiple ways to
conceive of evaluating replication . For example, a standard
frequentist solution is to test whether the effect reaches statistical significance with the same
ordinal pattern of means as the original study. An alternative approach is to evaluate whether the
meta-analytic combination of the original observation and replication produces a significant
effect. A third possibility is to test whether the replication effect is significantly different from
the original effect size estimate. Each of these will reveal distinct reproducibility rates, and each
offers a distinct interpretation. Notably, none of the possible interpretations will answer the
question that is ultimately of interest: At what rate are the conclusions of published research
Of the Studies Investigated, Which of Their Conclusions are True?
The relationship between the validity of a study's results and the validity of the
conclusions derived from those results is, at best, indirect. Replication only addresses the
validity of the results. If the original authors used flawed inferential statistics, then replicating
the result may say nothing of the accuracy of the conclusion . Similarly, if
the study used a confounded manipulation, and that confound explains the reported results rather
than the original interpretation, then the interpretation is incorrect regardless of whether the
result is reproducible. More generally, replication cannot help with misinterpretation. Piaget’s
 demonstrations of object permanence and other developmental phenomena are
among the most replicable findings in psychology. Simultaneously, many of his interpretations
of these results appear to have been incorrect .
Reinterpretation of old results is the ordinary process of scientific progress. That
progress is facilitated by having valid results to reinterpret. Piaget's conclusions may have been
overthrown, but his empirical results still provide the foundation for much of developmental
psychology. The experimental paradigms he designed were so fruitful, in part, because the
results they generate are so easily replicated. In this sense, reproducibility is essential for
theoretical generativity. The Reproducibility Project offers the same contribution as other
replications toward increasing confidence in the truth of conclusions. Findings that replicate in
the Reproducibility Project are ones that are more likely to replicate in the future. The aggregate
results will provide greater confidence in the validity of the findings, whether or not the
conclusions are correct.
Of all Published Studies, What is the Rate of True Findings?
It is of great importance to know the rate of valid findings in a given field. Even under
the best of circumstances, at least some findings will be false due to random chance or simple
human error. While there is a concern that we may be far from the ideal , there are little systematic data in any field and
hardly any in psychology. There are at least two barriers to obtaining empirical data on the rate
of true findings. The first is that accumulating such data across a large sample of findings
requires a range of expertise and a supply of labor that is difficult to assemble. In that respect,
one of the contributions of the Reproducibility Project is to show how this can be accomplished.
The second is that failure to replicate a result is not synonymous with the result being a false
positive. The Reproducibility Project’s base replication rate will be an estimate of
reproducibility, not of false positives. To estimate the false-positive rate, we need to know how
factors influence reproducibility.
The Reproducibility Project attempts to minimize the other factors that are knowable and
undesirable (e.g., low power and poor replication design), and to estimate the influence of others.
Here we summarize three possible interpretations of a failure to replicate the results of an
original study, and indicate how we are addressing them:
Interpretation 1: The original effect was false. The original result could have occurred
by chance (e.g., setting alpha = .05 anticipates a 5% false-positive rate), by fraud, or
unintentionally by exploiting flexible research practices in design, analysis, or reporting
 .
Interpretation 2: The replication was not sufficiently powered to detect the true
effect (i.e., the replication is false). Just as positive results occur by chance when there is no
result to detect (alpha = .05), negative results occur by chance when there is a result to detect
(beta or power). Most studies are very underpowered . Fair replication attempts should strive for adequate
power. The Reproducibility Project sets 80% as the baseline standard power for replication
attempts and encourages higher levels of power where possible. The actual power
of our replications can be used as a predictor in the model estimating the effect of power on
reproducibility, and as a way to estimate the false-negative rate among replications. For example,
an average power of 85% across replications would lead us to expect a false-negative rate of
15% on chance alone. Given the high power of all designs, this rate should be low.
Interpretation 3: The replication methodology differed from the original
methodology on unconsidered features that were critical for obtaining the true effect.
There is no such thing as an exact replication. A replication necessarily differs somehow, or else
it would not be a replication. For example, in behavioral research, even if the same participants
are used, their state and experience differs. Likewise, even if the same location, procedures, and
apparatus are used, the history and social context have changed. There are infinite dimensions of
sample, setting, procedure, materials, and instrumentation that could be conditions for obtaining
an effect. Keeping with the principle of Occam’s razor, these variables are assumed irrelevant
until proven otherwise. Consequently, authors almost never observe these cautions when writing
about effects. We understand that if an effect is interpreted as existing only for the original
circumstances, with no explanatory value outside of that lone occasion, its usefulness for future
research and application is severely limited.
Part of standard research practice is to understand the conditions necessary to elicit an
effect. Does it depend on the color of the walls? The hardness of the pencils used? The
characteristics of the sample? The context of measurement? How the materials are
administered? There is an infinite number of possible conditions, and a smaller number of
plausible conditions, that could be necessary for obtaining an effect.
A replication attempt will necessarily differ in many ways from the original
demonstration. The key question is whether a failure to replicate could plausibly be attributed to
any of these differences. The answer may rest upon what aspect of the original effect each
difference violates:
1. Published constraints on the effect. Does the original interpretation of the effect
suggest conditions that are violated by the replication attempt? If the original
interpretation is that the effect will only occur for women, and the replication attempt
includes men, then it is not a fair replication. The existing interpretation (and perhaps
empirical evidence) already imposes that constraint. Replication is not expected.
Replication teams are expected to avoid violating these constraints as much as possible in
the Reproducibility Project. Offering original authors an opportunity to review the
design provides another opportunity to identify and address these constraints. When the
constraints cannot be addressed completely, they are documented as potential predictors
of reproducibility.
2. Constraints on the effect, identified a priori. An infinitely precise description requires
infinite journal space, and thus every method section is necessarily an abridged summary.
Thus, there may be design choices that are known (to the original experimenters, if to no
one else) to be crucial to obtaining the reported results, but not described in print. By
contacting the original authors prior to conducting the replication attempt, the
Reproducibility Project minimizes this flaw in the published record.
3. Constraints on the effect, identified post hoc. Constraints identified beforehand are
distinct from the reasoning or speculation that occurs after a failed replication attempt.
There are many differences between any replication and its original, and subsequent
investigation may determine that one of these differences, in fact, was crucial to
obtaining the original results. That is, the original effect is not reproducible as originally
interpreted, but is reproducible with the newly discovered constraints. The
Reproducibility Project only initiates this process: For studies that do not replicate,
interested researchers may search for potential reasons why. This might include
additional studies that manipulate the factors identified as possible causes of the
replication failure. Such research will produce a better understanding of the phenomenon.
4. Errors in implementation or analysis for the original study, replication study, or
both. Errors happen. What researchers think or report they did might not be what they
actually did. Discrepancies in results can occur because of mistakes. There is no obvious
difference between “original” or “replication” studies in the likelihood of errors occurring.
The Reproducibility Project cannot control errors in original studies, but it can make
every effort to minimize their occurrence in the replication studies. For example, it is
conceivable that the Reproducibility Project will fail to replicate studies because some
team members are incompetent in the design and execution of the replication projects.
We cannot rule out this possibility entirely, but we can introduce procedures to minimize
its impact and maximize the likelihood of identifying whether competence is playing a
role. For example, features of the replication team (e.g., relevant experience, degrees,
publishing record) can be used as predictors of reproducibility.
The key lesson from this section is that failure to replicate does not unambiguously
suggest that the original effect is false. The Reproducibility Project examines all of the
possibilities described above in its evaluation of reproducibility. Some can be addressed
effectively with design. For example, all studies will have at least 80% power to detect the
original effect, and the power of the test will be evaluated as a predictor for likelihood of
replication. Also, differences between original and replication methods will be minimized by
obtaining original materials whenever possible and by collaborating with original authors to
identify and resolve all possible published or a priori identifiable design constraints. Finally,
original authors and other members of the collaborative team review and evaluate the
methodology and analysis to minimize the likelihood of errors in the replications, and the
designs, materials, and data are made available publicly in order to improve the likelihood of
identifying errors. Notwithstanding the ambiguity surrounding the interpretation of a replication
failure, the key value of replication remains: As data accumulate, the precision of the effect
estimate increases.
What Practices Lead to More Replicable Findings?
Perhaps the most promising possible contribution of the Reproducibility Project will be to
provide empirical evidence of the correlates of reproducibility, or to make a more informed
assessment of the reproducibility of existing results. Researchers have no shortage of hypotheses
as to what research practices would lead to higher replicability rates .
Without systematic data, there is no way to test these hypotheses . Note that this is a correlational study, so it is possible that some
third factor, such as the authors’ conscientiousness, is the joint cause of both the adoption of a
particular research practice and high replicability. However, the lack of a correlation between
certain practices and higher replicability rates is—assuming sufficient statistical power—more
directly interpretable, suggesting that researchers should look elsewhere for methods that will
meaningfully increase the validity of published findings.
Like any research effort, the most important factor for success of the Reproducibility
Project is the quality and execution of its design. The quality of the design, execution of
replications, and ultimate interpretations of the findings will define the extent to which the
Reproducibility Project can provide information about the reproducibility of psychological
science. As with all research, that responsibility rests with the team conducting the research.
For the last section of this chapter, we summarize the strategies we are pursuing to conduct an
open, large-scale, collaborative project with the highest quality standards that we can achieve
 .
Coordinating the Reproducibility Project
The success of the Reproducibility Project hinges on effective collaboration among a
large number of contributors. In business and science, large-scale efforts have routinely
provided important contributions. Sending an astronaut to the moon, creating a feature film, and
sequencing the human genome are testaments to the power of collaboration and social
coordination. However, most large-scale projects are highly resourced with money, staff, and
administration in order to assure success. Further, most large-scale efforts are backed by
leadership that has direct control over the contributors through employment or other strong
incentives, giving contributors compelling reasons to do their part for the project.
The Reproducibility Project differs from the modal large-scale project because it is light
on resources and light on leadership. Almost all contributors are donating their time and drawing
on whatever resources they have available to conduct replications. Project leaders cannot require
action because the contributors are volunteers. How can such a project succeed? Why would
any individual contributor choose to participate?
The Reproducibility Project team draws its project-design principles from open-source
software communities that developed important software such as the Linux operating system and
the Firefox web browser. These communities achieved remarkable success under similar
conditions. In this section, we describe the strategies used for coordinating the Reproducibility
Project so that other groups can draw on the project design to pursue similar scientific projects.
An insightful treatment of these project principles and strategies is provided in Michael Nielsen’s
 book Reinventing Discovery.
The challenges to solve are the following: (1) recruiting contributors; (2) defining tasks
so that contributors know what they need to do and can do it; (3) ensuring high-quality
contributions; (4) coordinating effectively so that contributions can be aggregated; and (5)
getting contributors to follow through on their commitments. The next sections describe the
variety of strategy the project uses to address these challenges.
Clear articulation of the project goals and approach
Defining project goals is so obvious that it is easy to overlook. Prospective contributors
must know what the project will accomplish (and how) to decide whether they want to contribute.
The Reproducibility Project’s primary goal is to estimate the reproducibility of psychological
science. It aims to accomplish that goal by conducting replications of a sample of published
studies from major journals in psychology. The extent to which prospective contributors find the
goal and approach compelling will influence the likelihood that they volunteer their time and
resources. Further, once the team is assembled, a clear statement of purpose and approach bonds
the team and facilitates coordination. This goal and approach is included in every
communication about the Reproducibility Project.
Crowdsourcing
Even though potential contributors may find the project goal compelling, they recognize
that they could never conduct so many replications by themselves. The Reproducibility Project’s
goal of replicating dozens of studies is appealing because it has the potential to impact the field,
but actually replicating that many studies is daunting. One solution is crowdsourcing.
Crowdsourcing requires two things: modularity and many contributors.
Modularity is the extent to which a project can be separated into independent components
and then recombined later. The Reproducibility Project team is distributed across dozens of
universities in multiple countries. Contributors have other responsibilities and demands on their
time. That means things like hosting a team meeting with all 100+ members attending at the
same time—in person or virtually—is implausible. Also, if contributors are highly dependent on
each other, then the time delay is multiplicative. Delay by one affects all. Project coordination
is simplified by orders of magnitude if the project can be broken into pieces so that contributors
can work independently.
The Reproducibility Project is highly modularized. Individuals or small teams conduct
replications independently. Some replications are completed very rapidly; others over a longer
time-scale. Barriers to progress are isolated to the competing schedules and responsibilities of
the small replication teams.
Besides accelerating progress, modularizing is attractive to volunteer contributors
because they have complete control over how to fit the contribution into their schedules.
Modularization is useful, but it will provide limited value if there are only a few contributors.
One way for crowdsourcing to overcome this problem is to have a low barrier to entry.
Low Barrier to Entry
Breaking up a large project into pieces reduces the amount of contribution required by
any single contributor. For volunteers with busy lives, this is vital. Many contributors to the
Reproducibility Project are graduate students. If participation in the Reproducibility Project
required that they stop their other research for a semester, they would hurt their career prospects.
The Reproducibility Project encourages small contributions so that contributors can volunteer
their services without incurring inordinate costs to their other professional responsibilities.
Even with effective modularization, prospective contributors may have difficulty in
estimating the workload required when making the initial commitment to contribute. Not
knowing what is required is a formidable barrier to entry. The Reproducibility Project provides
specific documentation to reduce this barrier. In particular, prospective contributors can review
studies available for replication rapidly in a summary spreadsheet, consult with a team member
whose role is to connect available studies to new contributors with appropriate skills and
resources, and review the replication protocol that provides instruction for every stage of the
process. Effective supporting material and personnel simplify the process of joining the project.
Leverage Available Skills
Collaborations can be particularly effective when they incorporate researchers with
distinct skill sets. A problem that is very difficult for a non-expert may be trivial for an expert.
Further, there are many potential contributors that do not have resources or skills to do the
central task: conducting a replication. In any large-scale project, there are additional
administrative, documentation, or consulting tasks that can be defined and modularized. The
Reproducibility Project has administrative contributors with specified roles and contributors who
assist by documenting and coding the studies available for replication. There are also consultants
for common issues such as data analysis.
Collaborative Tools and Documentation
As a distributed project, the Reproducibility Project coordination must embrace
asynchronous schedules. Communication among the entire team occurs via an email listserv
( that maintains a record of all
communications. New ideas, procedural issues, project plans, and task assignments are
discussed on the listserv, with the exception of routine procedural tasks which are managed via
email between administration team members and specific researchers. Decisions resulting from
team discussion are codified in project documentation that is managed with Google Docs and the
Open Science Framework (OSF; 
The project documentation includes:
1. Replication projects tracksheet ( A tracking spreadsheet
containing information about each replication project’s current progress, updated by
replication researchers as they complete phases of their projects.
2. Executive summary ( A high-level overview of the
Reproducibility Project’s goals, scope, and methods.
3. Researcher guide ( A step-by-step guide to conducting a
replication study for the project.
4. Template for replication reports ( A formatted template for
a replication project’s planned introduction and methods, as well as final report,
containing descriptions of what content should be included in each section.
5. Standard communication text for correspondence with original study authors
( Suggested text for requesting copies of project materials
and clarifying any important procedures that did not appear in the original report, and for
recontacting original authors to request their assessment of the proposed study design.
6. Project analysis plan ( A working document for planning final
analyses across all replication studies.
7. Project report draft ( An evolving summary report for the
8. The lists of studies currently available for replication ( or
available but requiring specialized resources ( as well as the list
of studies not yet available ( for helping researchers
select studies to replicate.
9. Support materials for coding articles//studies and accuracy-checking coded information,
including a form with instructions for coding articles ( a list of
articles needing to be coded ( articles coded via the form so that
coders can see examples of coded articles ( and a working
document with coded studies awaiting accuracy-checking ( 
10. The list of researchers who have earned authorship credit, as well as the list of
contributors to be thanked in acknowledgments ( for authors to
claim authorship and for authors and contributors to confirm their names and institutional
affiliations
11. Internal documents for use by the administrative team, including email templates
( and checklists for administrative tasks such as setting up
new replication researchers and handling incoming article coding submissions
( 
12. OSF replication project pages ( The Reproducibility
Project’s page on the Open Science Framework, containing links to all replication project
Project coordinators and administrators manage the documentation, and individual contributors
use and edit documentation files to guide their project responsibilities. For example, in the
project-tracking spreadsheet, each replication is summarized in a row of the sheet with columns
describing features of the replication, project progress, and outcomes. The replication team
keeps their project row up-to-date, with occasional prompting from administrative staff. Clear
documentation promotes a feeling of group membership and collaborative action, even when the
action is occurring asynchronously. Clear documentation also improves adherence to standards
and assists quality control.
Dedicated Administrative Support
Researchers are busy and not always inclined to maintain clear documentation. For
coordinating large projects, the benefits of clear documentation are so great that finding
dedicated volunteers or devoting resources to paying an administrator is essential to ensure
accurate, effective supporting documentation. It is easy to underestimate the importance of
administrative support, but disastrous to do so. Effective projects have a central repository of
knowledge about the project that is available, accurate, and easy to comprehend for all project
contributors. The Reproducibility Project has even higher standards because the project
documentation is available publicly. People that have not been involved in the project since its
inception must be able to comprehend and navigate the documentation easily. This cannot be
done without one or more personnel dedicated to maintaining that documentation.
Design and Operational Standards
The project documentation defines a standard workflow for conducting a replication
project. This workflow is designed to maximize the quality of the replication, make explicit the
standards and expectations of each replication, and minimize the workload for the individual
contributors. With a full specification of the workflow, templates for report writing, and material
support for correspondence with original study authors, the replicating teams can smoothly
implement the project’s standard procedures and focus their energies on the unique elements of
the replication study design and data collection to conduct the highest quality replication possible.
The highly defined workflow also makes it easy to track progress of one’s own
replication—and those of others. Each stage of the project has explicitly defined milestones,
described in the project’s researcher guide, and team members denote on the project tracksheet
when each stage is completed. At a glance, viewers of the tracksheet can see the status of all
projects. Besides its information value, tracking progress provides normative information for the
research teams regarding whether they are keeping up with the progress of other teams. Without
that information, individual contributors would have little basis for social comparison and also
little sense of whether the project as a whole is making progress.
Modularization is maintained by strong project coordination and documentation. When
an individual or team claims a study for replication, it is removed from the list of available
studies on the tracksheet. Team members do not need to consult all other team members for
identifying projects. They just consult the project documentation. Documentation and
operational standards provide the individual contributors with knowledge of (1) what is
happening in the project; (2) their role in the project; and (3) what they must do to fulfill their
Light Leadership with Strong Communication
Large-scale, distributed projects flounder without leadership. However, leadership
cannot be overly directive when the project is staffed by volunteers. Project leaders are
responsible for facilitating communication and discussion and then guiding the team to decisions
and action. Without someone taking responsibility for the latter, projects will stall with endless
discussion and no resolution.
To maximize project investment, individual contributors should have the experience that
their opinions about the project design matter and can impact the direction of the project.
Simultaneously, there must be sufficient leadership to avoid having each contributor feel like
they shoulder inordinate responsibility for decision-making. Contributors vary in the extent to
which they desire to shape different aspects of the project. Some have strong opinions about the
standard format of the replication report; others would rather step on a nail than spend time on
that. To balance this, the Reproducibility Project leadership promotes open discussion without
requiring contribution. Simultaneously, leadership defines a timeline for decision-making, takes
responsibility for reviewing and integrating opinions, and makes recommendations for action
Open Practices
The Reproducibility Project is an open project. This means that anyone can join, that
expectations of contributors are defined explicitly in advance, and that the project discussion,
design, materials, and data are all available publicly. Openness promotes accountability among
the team. Individuals have made public commitments to project activities. This transparency
minimizes free-riding and other common conflicts that emerge in collaborative research.
Openness also promotes accountability to the public. Replication teams are trying to reproduce
research designs and results published by others. The value of the evidence accumulated by the
Reproducibility Project relies on these replications being completed to a high standard. Making
all project materials available provides a strong incentive for the replication teams to do an
excellent job. Further, openness increases the likelihood that errors will be identified and
addressed. In addition to public accessibility, the Reproducibility Project builds in error
checking by requiring each replication team to contact original authors to invite critique of their
study design prior to data collection, and by having members review and critique each others’
project reports.
Participation Incentives
Why participate in a large-scale project? What is in it for the individual contributor? The
best designed and coordinated project will still fail if contributors have no reason to participate
voluntarily. The Reproducibility Project has a variety of incentives that may each have
differential impact on individual contributors. For one, many contributors have an intrinsic
interest in the research question and confidence that the project design will be revealing. Second,
contributors note that estimating the reproducibility of psychological science has important
implications for the discipline. As such, contributing to the Reproducibility Project is conceived
as a service to the field. The evidence it produces will offer an opportunity to improve scientific
practices.
Another class of incentives is experiential. Some contributors express an experiential
interest in being part of a large-scale collaboration, of trying open science practices, or of
conducting a direct replication. For some, this may be for the hedonic pleasure of working with
a group or trying out new practices. For others, this may be conceived as a training opportunity.
It might also be an opportunity to conduct a replication of a study that one finds interesting for
potential extension in later research. Alternatively, it could be an opportunity to look closely at
how another laboratory designed and conducted its study to learn about research design or
analysis practices.
A final class of incentives are the more traditional academic rewards. The most obvious
is publication. Publication is the basis of reward, advancement, and reputation building . Contributors to the Reproducibility Project earn co-authorship on publication about the
project and its findings. The relative impact for each individual contributor is most certainly
reduced by the fact that there are many contributors. However, other factors that offset this to
some degree. For one, the Reproducibility Project is in progress and has nonetheless produced
two publications already: this chapter and an introduction to the project in a high-profile
psychology journal, Perspectives on Psychological Science .
Further, given the intense interest in the project and the high-quality design (in our humble
opinion), the ultimate publication or publications have a reasonable chance of being published in
a prestigious journal and having a high impact on psychology and beyond. The Reproducibility
Project has been covered in a variety of major scientific and media outlets including Science,
Nature, and the Chronicle for Higher Education. If the project meets its promise, the
contributors will be noted through personal satisfaction and public recognition for having
contributed to an important study for psychological science.
Certainly, no contributor will establish a research career using publications with the Open
Science Collaboration exclusively. But publications from this collaboration may be a useful
supplement to one’s more independent research contributions. If nothing else, they provide an
added bonus for the more intrinsic factors that motivate contributions to the Reproducibility
Project — a desire to determine the reproducibility of psychological science and assist in
improving scientific practices.
Conclusion
The Reproducibility Project is the first attempt to systematically, empirically estimate the
reproducibility of a subdiscipline of science. It draws on the lessons of open-source projects in
software development: leveraging individuals’ opinions about how things should be done while
providing strong coordination to enable progress. What will be learned from the Reproducibility
Project is still undetermined. But, if the current progress is any indicator, the high investment of
its contributors and the substantial interest and attention by observers suggest that the
Reproducibility Project could provide a useful initial estimate of the reproducibility of
psychological science, and perhaps inspire other disciplines to pursue similar efforts.
Systematic data on replicability does not exist, which will hopefully be addressed by the
Reproducibility Project. If large numbers of findings fail to replicate, that will strengthen the
hand of the reform movements and lead to a significant reevaluation of the literature. If most
findings replicate satisfactorily—as many as would be expected given our statistical power
estimates—then that will suggest a different course of action. More likely, perhaps, is that the
results will be somewhere in between and will help generate hypotheses about particular
practices that could improve or damage reproducibility.
We close by noting that even in the best of circumstances, the results of any study—
including the Reproducibility Project—should be approached with a certain amount of
skepticism. While we attempt to conduct replication attempts that are as similar as possible to
the original study, it is always possible that “small” differences in method may turn out to be
crucial. Thus, while a failure to replicate should decrease confidence in a finding, one does not
want to make too much out of a single failure . Rather, the results of the
Reproducibility Project should be understood as an opportunity to learn whether current practices
require attention or revision. Can we do science better? If so, how? Ultimately, we hope that
we can make a contribution to answering these questions.