Dynamic Graph CNN for Learning on Point Clouds
YUE WANG, Massachusetts Institute of Technology
YONGBIN SUN, Massachusetts Institute of Technology
ZIWEI LIU, UC Berkeley / ICSI
SANJAY E. SARMA, Massachusetts Institute of Technology
MICHAEL M. BRONSTEIN, Imperial College London / USI Lugano
JUSTIN M. SOLOMON, Massachusetts Institute of Technology
Fig. 1. Point cloud segmentation using the proposed neural network. Bottom: schematic neural network architecture. Top: Structure of the feature
spaces produced at different layers of the network, visualized as the distance from the red point to all the rest of the points (shown left-to-right are the input
and layers 1-3; rightmost figure shows the resulting segmentation). Observe how the feature space structure in deeper layers captures semantically similar
structures such as wings, fuselage, or turbines, despite a large distance between them in the original input space.
Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output
of most 3D data acquisition devices. While hand-designed features on point
clouds have long been proposed in graphics and vision, however, the recent
overwhelming success of convolutional neural networks (CNNs) for image
analysis suggests the value of adapting insight from CNN to the point cloud
world. Point clouds inherently lack topological information so designing
Authors’ addresses: Yue Wang, Massachusetts Institute of Technology, yuewang@
csail.mit.edu; Yongbin Sun, Massachusetts Institute of Technology, ;
Ziwei Liu, UC Berkeley / ICSI, ; Sanjay E. Sarma, Massachusetts
Institute of Technology, ; Michael M. Bronstein, Imperial College
London / USI Lugano, ; Justin M. Solomon, Massachusetts
Institute of Technology, .
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from .
© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.
0730-0301/2019/1-ART1 $15.00
 
a model to recover topology can enrich the representation power of point
clouds. To this end, we propose a new neural network module dubbed Edge-
Conv suitable for CNN-based high-level tasks on point clouds including
classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into
existing architectures. Compared to existing modules operating in extrinsic
space or treating each point independently, EdgeConv has several appealing
properties: It incorporates local neighborhood information; it can be stacked
applied to learn global shape properties; and in multi-layer systems affinity
in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model
on standard benchmarks including ModelNet40, ShapeNetPart, and S3DIS.
CCS Concepts: • Computing methodologies →Neural networks; Pointbased models; Shape analysis;
Additional Key Words and Phrases: point cloud, classification, segmentation
ACM Reference Format:
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein,
and Justin M. Solomon. 2019. Dynamic Graph CNN for Learning on Point
Clouds. ACM Trans. Graph. 1, 1, Article 1 , 13 pages. https:
//doi.org/10.1145/3326362
ACM Trans. Graph., Vol. 1, No. 1, Article 1. Publication date: January 2019.
 
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon
INTRODUCTION
Point clouds, or scattered collections of points in 2D or 3D, are
arguably the simplest shape representation; they also comprise
the output of 3D sensing technology including LiDAR scanners
and stereo reconstruction. With the advent of fast 3D point cloud
acquisition, recent pipelines for graphics and vision often process
point clouds directly, bypassing expensive mesh reconstruction or
denoising due to efficiency considerations or instability of these
techniques in the presence of noise. A few of the many recent
applications of point cloud processing and analysis include indoor
navigation [Zhu et al. 2017], self-driving vehicles [Liang et al. 2018;
Qi et al. 2017a; Wang et al. 2018b], robotics [Rusu et al. 2008b], and
shape synthesis and modeling [Golovinskiy et al. 2009; Guerrero
et al. 2018].
These modern applications demand high-level processing of point
clouds. Rather than identifying salient geometric features like corners and edges, recent algorithms search for semantic cues and
affordances. These features do not fit cleanly into the frameworks
of computational or differential geometry and typically require
learning-based approaches that derive relevant information through
statistical analysis of labeled or unlabeled datasets.
In this paper, we primarily consider point cloud classification
and segmentation, two model tasks in point cloud processing. Traditional methods for solving these problems employ handcrafted
features to capture geometric properties of point clouds [Lu et al.
2014; Rusu et al. 2009, 2008a]. More recently, the success of deep
neural networks for image processing has motivated a data-driven
approach to learning features on point clouds. Deep point cloud processing and analysis methods are developing rapidly and outperform
traditional approaches in various tasks [Chang et al. 2015].
Adaptation of deep learning to point cloud data, however, is far
from straightforward. Most critically, standard deep neural network
models require input data with regular structure, while point clouds
are fundamentally irregular: Point positions are continuously distributed in the space, and any permutation of their ordering does
not change the spatial distribution. One common approach to process point cloud data using deep learning models is to first convert
raw point cloud data into a volumetric representation, namely a 3D
grid [Maturana and Scherer 2015; Wu et al. 2015]. This approach,
however, usually introduces quantization artifacts and excessive
memory usage, making it difficult to go to capture high-resolution
or fine-grained features.
State-of-the-art deep neural networks are designed specifically to
handle the irregularity of point clouds, directly manipulating raw
point cloud data rather than passing to an intermediate regular representation. This approach was pioneered by PointNet [Qi et al. 2017b],
which achieves permutation invariance of points by operating on
each point independently and subsequently applying a symmetric
function to accumulate features. Various extensions of PointNet
consider neighborhoods of points rather than acting on each independently [Qi et al. 2017c; Shen et al. 2017]; these allow the network
to exploit local features, improving upon performance of the basic
model. These techniques largely treat points independently at local
scale to maintain permutation invariance. This independence, however, neglects the geometric relationships among points, presenting
a fundamental limitation that cannot capture local features.
To address these drawbacks, we propose a novel simple operation,
called EdgeConv, which captures local geometric structure while
maintaining permutation invariance. Instead of generating point
features directly from their embeddings, EdgeConv generates edge
features that describe the relationships between a point and its
neighbors. EdgeConv is designed to be invariant to the ordering of
neighbors, and thus is permutation invariant. Because EdgeConv
explicitly constructs a local graph and learns the embeddings for
the edges, the model is capable of grouping points both in Euclidean
space and in semantic space.
EdgeConv is easy to implement and integrate into existing deep
learning models to improve their performance. In our experiments,
we integrate EdgeConv into the basic version of PointNet without
using any feature transformation. We show the resulting network
achieves state-of-the-art performance on several datasets, most notably ModelNet40 and S3DIS for classification and segmentation.
Key Contributions. We summarize the key contributions of our
work as follows:
• We present a novel operation for learning from point clouds,
EdgeConv, to better capture local geometric features of point
clouds while still maintaining permutation invariance.
• We show the model can learn to semantically group points by
dynamically updating a graph of relationships from layer to layer.
• We demonstrate that EdgeConv can be integrated into multiple
existing pipelines for point cloud processing.
• We present extensive analysis and testing of EdgeConv and show
that it achieves state-of-the-art performance on benchmark datasets.
• We release our code to facilitate reproducibility and future research. 1
RELATED WORK
Hand-Crafted Features. Various tasks in geometric data processing and analysis—including segmentation, classification, and matching—
require some notion of local similarity between shapes. Traditionally,
this similarity is established by constructing feature descriptors that
capture local geometric structure. Countless papers in computer vision and graphics propose local feature descriptors for point clouds
suitable for different problems and data structures. A comprehensive
overview of hand-designed point features is out of the scope of this
paper, but we refer the reader to [Biasotti et al. 2016; Guo et al. 2014;
Van Kaick et al. 2011] for discussion.
Broadly speaking, one can distinguish between extrinsic and intrinsic descriptors. Extrinsic descriptors usually are derived from the
coordinates of the shape in 3D space and includes classical methods
like shape context [Belongie et al. 2001], spin images [Johnson and
Hebert 1999], integral features [Manay et al. 2006], distance-based
descriptors [Ling and Jacobs 2007], point feature histograms [Rusu
et al. 2009, 2008a], and normal histograms [Tombari et al. 2011], to
name a few. Intrinsic descriptors treat the 3D shape as a manifold
whose metric structure is discretized as a mesh or graph; quantities
1 
ACM Trans. Graph., Vol. 1, No. 1, Article 1. Publication date: January 2019.
Dynamic Graph CNN for Learning on Point Clouds
Fig. 2. Left: Computing an edge feature, eij (top), from a point pair, xi and xj (bottom). In this example, hΘ() is instantiated using a fully connected layer,
and the learnable parameters are its associated weights. Right: The EdgeConv operation. The output of EdgeConv is calculated by aggregating the edge
features associated with all the edges emanating from each connected vertex.
expressed in terms of the metric are invariant to isometric deformation. Representatives of this class include spectral descriptors
such as global point signatures [Rustamov 2007], the heat and wave
kernel signatures [Aubry et al. 2011; Sun et al. 2009], and variants
[Bronstein and Kokkinos 2010]. Most recently, several approaches
wrap machine learning schemes around standard descriptors [Guo
et al. 2014; Shah et al. 2013].
Deep learning on geometry. Following the breakthrough results of
convolutional neural networks (CNNs) in vision [Krizhevsky et al.
2012; LeCun et al. 1989], there has been strong interest to adapt
such methods to geometric data. Unlike images, geometry usually
does not have an underlying grid, requiring new building blocks
replacing convolution and pooling or adaptation to a grid structure.
As a simple way to overcome this issue, view-based [Su et al. 2015;
Wei et al. 2016] and volumetric representations [Klokov and Lempitsky 2017; Maturana and Scherer 2015; Tatarchenko et al. 2017; Wu
et al. 2015]—or their combination [Qi et al. 2016]—“place” geometric
data onto a grid. More recently, PointNet [Qi et al. 2017b,c] exemplifies a broad class of deep learning architectures on non-Euclidean
data (graphs and manifolds) termed geometric deep learning [Bronstein et al. 2017]. These date back to early methods to construct
neural networks on graphs [Scarselli et al. 2009], recently improved
with gated recurrent units [Li et al. 2016] and neural message passing [Gilmer et al. 2017]. Bruna et al. and Henaff et al. 
generalized convolution to graphs via the Laplacian eigenvectors
[Shuman et al. 2013]. Computational drawbacks of this foundational
approach were alleviated in follow-up works using polynomial [Defferrard et al. 2016; Kipf and Welling 2017; Monti et al. 2017b, 2018],
or rational [Levie et al. 2017] spectral filters that avoid Laplacian
eigendecomposition and guarantee localization. An alternative definition of non-Euclidean convolution employs spatial rather than
spectral filters. The Geodesic CNN (GCNN) is a deep CNN on meshes
generalizing the notion of patches using local intrinsic parameterization [Masci et al. 2015]. Its key advantage over spectral approaches
is better generalization as well as a simple way of constructing
directional filters. Follow-up work proposed different local charting techniques using anisotropic diffusion [Boscaini et al. 2016]
or Gaussian mixture models [Monti et al. 2017a; Veličković et al.
2017]. In [Halimi et al. 2018; Litany et al. 2017b], a differentiable
functional map [Ovsjanikov et al. 2012] layer was incorporated into
a geometric deep neural network, allowing to do intrinsic structured
prediction of correspondence between nonrigid shapes.
The last class of geometric deep learning approaches attempts
to pull back a convolution operation by embedding the shape into
a domain with shift-invariant structure such as the sphere [Sinha
et al. 2016], torus [Maron et al. 2017], plane [Ezuz et al. 2017], sparse
network lattice [Su et al. 2018], or spline [Fey et al. 2018].
Finally, we should mention geometric generative models, which
attempt to generalize models such as autoencoders, variational autoencoders (VAE) [Kingma and Welling 2013], and generative adversarial networks (GAN) [Goodfellow et al. 2014] to the non-Euclidean
setting. One of the fundamental differences between these two settings is the lack of canonical order between the input and the output
vertices, thus requiring an input-output correspondence problem
to be solved. In 3D mesh generation, it is commonly assumed that
the mesh is given and its vertices are canonically ordered; the generation problem thus amounts only to determining the embedding
of the mesh vertices. Kostrikov et al. proposed SurfaceNets
based on the extrinsic Dirac operator for this task. Litany et al.
[2017a] introduced the intrinsic VAE for meshes and applied it to
shape completion; a similar architecture was used by Ranjan et al.
 for 3D face synthesis. For point clouds, multiple generative
architectures have been proposed [Fan et al. 2017; Li et al. 2018b;
Yang et al. 2018].
OUR APPROACH
We propose an approach inspired by PointNet and convolution
operations. Instead of working on individual points like PointNet,
however, we exploit local geometric structures by constructing a
local neighborhood graph and applying convolution-like operations
on the edges connecting neighboring pairs of points, in the spirit
of graph neural networks. We show in the following that such an
operation, dubbed edge convolution (EdgeConv), has properties lying
between translation-invariance and non-locality.
Unlike graph CNNs, our graph is not fixed but rather is dynamically updated after each layer of the network. That is, the set of
k-nearest neighbors of a point changes from layer to layer of the
network and is computed from the sequence of embeddings. Proximity in feature space differs from proximity in the input, leading
to nonlocal diffusion of information throughout the point cloud. As
ACM Trans. Graph., Vol. 1, No. 1, Article 1. Publication date: January 2019.
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon
Fig. 3. Model architectures: The model architectures used for classification (top branch) and segmentation (bottom branch). The classification model takes
as input n points, calculates an edge feature set of size k for each point at an EdgeConv layer, and aggregates features within each set to compute EdgeConv
responses for corresponding points. The output features of the last EdgeConv layer are aggregated globally to form an 1D global descriptor, which is used to
generate classification scores for c classes. The segmentation model extends the classification model by concatenating the 1D global descriptor and all the
EdgeConv outputs (serving as local descriptors) for each point. It outputs per-point classification scores for p semantic labels. ⊕: concatenation. Point cloud
transform block: The point cloud transform block is designed to align an input point set to a canonical space by applying an estimated 3 × 3 matrix. To
estimate the 3 × 3 matrix, a tensor concatenating the coordinates of each point and the coordinate differences between its k neighboring points is used.
EdgeConv block: The EdgeConv block takes as input a tensor of shape n × f , computes edge features for each point by applying a multi-layer perceptron
(mlp) with the number of layer neurons defined as {a1, a2, ..., an }, and generates a tensor of shape n × an after pooling among neighboring edge features.
a connection to existing work, Non-local Neural Networks [Wang
et al. 2018a] explored similar ideas in the video recognition field,
and follow-up work by Xie et al. proposed using non-local
blocks to denoise feature maps to defend against adversarial attacks.
Edge Convolution
Consider an F-dimensional point cloud with n points, denoted by
X = {x1, . . . , xn} ⊆RF . In the simplest setting of F = 3, each point
contains 3D coordinates xi = (xi,yi,zi); it is also possible to include
additional coordinates representing color, surface normal, and so
on. In a deep neural network architecture, each subsequent layer
operates on the output of the previous layer, so more generally the
dimension F represents the feature dimensionality of a given layer.
We compute a directed graph G = (V, E) representing local point
cloud structure, where V = {1, . . . ,n} and E ⊆V × V are the
vertices and edges, respectively. In the simplest case, we construct
G as the k-nearest neighbor (k-NN) graph of X in RF . The graph
includes self-loop, meaning each node also points to itself. We define
edge features as eij = hΘ(xi, xj), where hΘ : RF × RF →RF ′ is a
nonlinear function with a set of learnable parameters Θ.
Finally, we define the EdgeConv operation by applying a channelwise symmetric aggregation operation □(e.g., Í or max) on the
edge features associated with all the edges emanating from each
vertex. The output of EdgeConv at the i-th vertex is thus given by
hΘ(xi, xj).
Making analogy to convolution along images, we regard xi as the
central pixel and {xj : (i, j) ∈E} as a patch around it (see Figure 2). Overall, given an F-dimensional point cloud with n points,
EdgeConv produces an F ′-dimensional point cloud with the same
number of points.
Choice of h and □. The choice of the edge function and the aggregation operation has a crucial influence on the properties of
EdgeConv. For example, when x1, . . . , xn represent image pixels
on a regular grid and the graph G has connectivity representing
patches of fixed size around each pixel, the choice θm · xj as the
edge function and sum as the aggregation operation yields standard
convolution:
Here, Θ = (θ1, . . . ,θM) encodes the weights of M different filters.
Each θm has the same dimensionality as x, and · denotes the Euclidean inner product.
A second choice of h is
hΘ(xi, xj) = hΘ(xi),
ACM Trans. Graph., Vol. 1, No. 1, Article 1. Publication date: January 2019.
Dynamic Graph CNN for Learning on Point Clouds
encoding only global shape information oblivious of the local neighborhood structure. This type of operation is used in PointNet, which
can thus be regarded as a special case of EdgeConv.
A third choice of h adopted by Atzmon et al. is
hΘ(xi, xj) = hΘ(xj)
(hθ(xj))д(u(xi, xj)),
where д is a Gaussian kernel and u computes pairwise distance in
Euclidean space.
A fourth option is
hΘ(xi, xj) = hΘ(xj −xi).
This encodes only local information, treating the shape as a collection of small patches and losing global structure.
Finally, a fifth option that we adopt in this paper is an asymmetric
edge function
hΘ(xi, xj) = ¯hΘ(xi, xj −xi).
This explicitly combines global shape structure, captured by the
coordinates of the patch centers xi, with local neighborhood information, captured by xj −xi. In particular, we can define our operator
by notating
ijm = ReLU(θm · (xj −xi) + ϕm · xi),
which can be implemented as a shared MLP, and taking
j:(i,j)∈E e′
where Θ = (θ1, . . . ,θM,ϕ1, . . . ,ϕM)
Dynamic graph update
Our experiments suggest that it is beneficial to recompute the graph
using nearest neighbors in the feature space produced by each layer.
This is a crucial distinction of our method from graph CNNs working
on a fixed input graph. Such a dynamic graph update is the reason
for the name of our architecture, the Dynamic Graph CNN (DGCNN).
With dynamic graph updates, the receptive field is as large as the
diameter of the point cloud, while being sparse.
At each layer we have a different graph G(l) = (V(l), E(l)), where
the l-th layer edges are of the form (i, ji1), . . . , (i, jikl ) such that
ji1, . . . ,x(l)
jikl are the kl points closest to x(l)
i . Put differently, our
architecture learns how to construct the graph G used in each layer
rather than taking it as a fixed constant constructed before the
network is evaluated. In our implementation, we compute a pairwise
distance matrix in feature space and then take the closest k points
for each single point.
Properties
Permutation Invariance. Consider the output of a layer
j:(i,j)∈E hΘ(xi, xj)
and a permutation operator π. The output of the layer x′
i is invariant
to permutation of the input xj because max is a symmetric function
(other symmetric functions also apply). The global max pooling
operator to aggregate point features is also permutation-invariant.
Translation Invariance. Our operator has a “partial” translation
invariance property, in that our choice of edge functions (7) explicitly
exposes the part of the function that can be translation-dependent
and optionally can be disabled. Consider a translation applied to xj
and xi; we can show that part of the edge feature is preserved when
shifting by T. In particular, for the translated point cloud we have
ijm = θm · (xj +T −(xi +T)) + ϕm · (xi +T)
= θm · (xj −xi) + ϕm · (xi +T).
If we only consider xj −xi by taking ϕm = 0, then the operator
is fully invariant to translation. In this case, however, the model reduces to recognizing an object based on an unordered set of patches,
ignoring the positions and orientations of patches. With both xj −xi
and xi as input, the model takes account into the local geometry of
patches while keeping global shape information.
Comparison to existing methods
DGCNN is related to two classes of approaches, PointNet and graph
CNNs, which we show to be particular settings of our method. We
summarize different methods in Table 1.
PointNet is a special case of our method with k = 1, yielding
a graph with an empty edge set E = ∅. The edge function used
in PointNet is hΘ(xi, xj) = hΘ(xi), which considers global but not
local geometry. PointNet++ tries to account for local structure by applying PointNet in a local manner. In our parlance, PointNet++ first
constructs the graph according to the Euclidean distances between
the points, and in each layer applies a graph coarsening operation.
For each layer, some points are selected using farthest point sampling (FPS); only the selected points are preserved while others are
directly discarded after this layer. In this way, the graph becomes
smaller after the operation applied on each layer. In contrast to
DGCNN, PointNet++ computes pairwise distances using point input coordinates, and hence their graphs are fixed during training.
The edge function used by PointNet++ is hΘ(xi, xj) = hΘ(xj), and
the aggregation operation is also a max.
Among graph CNNs, MoNet [Monti et al. 2017a], ECC [Simonovsky
and Komodakis 2017], Graph Attention Networks [Veličković et al.
2017], and the concurrent work [Atzmon et al. 2018] are the most
related approaches. Their common denominator is a notion of a
local patch on a graph, in which a convolution-type operation can
be defined.2
Specifically, Monti et al. [2017a] use the graph structure to compute a local “pseudo-coordinate system” u in which the neighborhood vertices are represented; the convolution is then defined as an
M-component Gaussian mixture
θm · (xj ⊙дwn (u(xi, xj))),
whereд is a Gaussian kernel, ⊙is the elementwise (Hadamard) product, {w1, . . . ,wN } encode the learnable parameters of the Gaussians
(mean and covariance), and {θ1, . . . ,θM } are the learnable filter coefficients. (11) is an instance of our general operation (1), with a
2[Simonovsky and Komodakis 2017; Veličković et al. 2017] can be considered instances
of [Monti et al. 2017a], with the difference that the weights are constructed employing
features from adjacent nodes instead of graph structure; [Atzmon et al. 2018] is also
similar except that the weighting function is hand-designed.
ACM Trans. Graph., Vol. 1, No. 1, Article 1. Publication date: January 2019.
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon
Aggregation
Edge Function
Learnable parameters
PointNet [Qi et al. 2017b]
hΘ(xi, xj) = hΘ(xi)
PointNet++ [Qi et al. 2017c]
hΘ(xi, xj) = hΘ(xj)
MoNet [Monti et al. 2017a]
hθ m,wn (xi, xj) = θm · (xj ⊙дw n (u(xi, xj)))
PCNN [Atzmon et al. 2018]
hθ m (xi, xj) = (θm · xj)д(u(xi, xj))
Table 1. Comparison to existing methods. The per-point weight wi in [Atzmon et al. 2018] effectively is computed in the first layer and could be carried
onward as an extra feature; we omit this for simplicity.
Fig. 4. Structure of the feature spaces produced at different stages of our shape classification neural network architecture, visualized as the distance
between the red point to the rest of the points. For each set, Left: Euclidean distance in the input R3 space; Middle: Distance after the point cloud transform
stage, amounting to a global transformation of the shape; Right: Distance in the feature space of the last layer. Observe how in the feature space of deeper
layers semantically similar structures such as shelves of a bookshelf or legs of a table are brought close together, although they are distant in the original space.
ACM Trans. Graph., Vol. 1, No. 1, Article 1. Publication date: January 2019.
Dynamic Graph CNN for Learning on Point Clouds
particular edge function
hθ m,wn (xi, xj) = θm · (xj ⊙дw n (u(xi, xj)))
and □= Í. Again, their graph structure is fixed, andu is constructed
based on the degrees of nodes.
[Atzmon et al. 2018] can be seen as a special case of [Monti et al.
2017a] with д as predefined Gaussian functions. Removing learnable
parameters (w1, . . . ,wN ) and constructing a dense graph from point
clouds, we have
(θm · xj)д(u(xi, xj)),
where u is the pairwise distance between xi and xj in Euclidean
While MoNet and other graph CNNs assume a given fixed graph
on which convolution-like operations are applied, to our knowledge
our method is the first for which the graph changes from layer to
layer and even on the same input during training when learnable
parameters are updated. This way, our model not only learns how
to extract local geometric features, but also how to group points in a
point cloud. Figure 4 shows the distance in different feature spaces,
exemplifying that the distances in deeper layers carry semantic
information over long distances in the original embedding.
EVALUATION
In this section, we evaluate the models constructed using EdgeConv
for different tasks: classification, part segmentation, and semantic
segmentation. We also visualize experimental results to illustrate
key differences from previous work.
Classification
Data. We evaluate our model on the ModelNet40 [Wu et al. 2015]
classification task, consisting in predicting the category of a previously unseen shape. The dataset contains 12,311 meshed CAD
models from 40 categories. 9,843 models are used for training and
2,468 models are for testing. We follow verbatim the experimental
settings of Qi et al. [2017b]. For each model, 1,024 points are uniformly sampled from the mesh faces; the point cloud is rescaled to
fit into the unit sphere. Only the (x,y,z) coordinates of the sampled points are used, and the original meshes are discarded. During
the training procedure, we augment the data by randomly scaling
objects and perturbing the object and point locations.
Architecture. The network architecture used for the classification
task is shown in Figure 3 (top branch without spatial transformer
network). We use four EdgeConv layers to extract geometric features. The four EdgeConv layers use three shared fully-connected
layers (64, 64, 128, 256). We recompute the graph based on the features of each EdgeConv layer and use the new graph for next layer.
The number k of nearest neighbors is 20 for all EdgeConv layers (for
the last row in Table 2, k is 40). Shortcut connections are included
to extract multi-scale features and one shared fully-connected layer
(1024) to aggregate multi-scale features, where we concatenate features from previous layers to get a 64+64+128+256=512 dimensional
point cloud. Then, a global max/sum pooling is used to get the
point cloud global feature, after which two fully-connected layers
(512, 256) are used to transform the global feature. Dropout with
keep probability of 0.5 is used in the last two fully-connected layers.
All layers include LeakyReLU and batch normalization. The number
k was chosen using a validation set. We split the training data to
80% for training and 20% for validation to search the best k. After
k is chosen, we retrain the model on the whole training data and
evaluate the model on the testing data. Other hyperparameters were
chosen in a similar ways.
Training. We use SGD with learning rate 0.1, and we reduce the
learning rate until 0.001 using cosine annealing [Loshchilov and
Hutter 2017]. The momentum for batch normalization is 0.9, and
we do not use batch normalization decay. The batch size is 32 and
the momentum is 0.9.
Results. Table 2 shows the results for the classification task. Our
model achieves the best results on this dataset. Our baseline using
a fixed graph determined by proximity in the input point cloud
is 1.0% better than PointNet++. An advanced version including
dynamical graph recomputation achieves the best results on this
dataset. All the experiments are performed with point clouds that
contain 1024 points except last row. We further test out model
with 2048 points. The k used for 2048 points is 40 to maintain the
same density. Note that PCNN [Atzmon et al. 2018] uses additional
augmentation techniques like randomly sampling 1024 points out
of 1200 points during both training and testing.
Class Accuracy
3DShapeNets [Wu et al. 2015]
VoxNet [Maturana and Scherer 2015]
Subvolume [Qi et al. 2016]
VRN (single view) [Brock et al. 2016]
VRN (multiple views) [Brock et al. 2016]
ECC [Simonovsky and Komodakis 2017]
PointNet [Qi et al. 2017b]
PointNet++ [Qi et al. 2017c]
Kd-net [Klokov and Lempitsky 2017]
PointCNN [Li et al. 2018a]
PCNN [Atzmon et al. 2018]
Ours (baseline)
Ours 
Table 2. Classification results on ModelNet40.
Model Complexity
We use the ModelNet40 [Wu et al. 2015] classification experiment
to compare the complexity of our model to previous state-of-the-art.
Table 3 shows that our model achieves the best tradeoff between the
model complexity (number of parameters), computational complexity (measured as forward pass time), and the resulting classification
Our baseline model using the fixed k-NN graph outperforms the
previous state-of-the-art PointNet++ by 1.0% accuracy, at the same
time being 7 times faster. A more advanced version of our model
including a dynamically-updated graph computation outperforms
PointNet++, PCNN by 2.2% and 0.6% respectively, while being much
ACM Trans. Graph., Vol. 1, No. 1, Article 1. Publication date: January 2019.
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon
Model size(MB)
Accuracy(%)
PointNet (Baseline) [Qi et al. 2017b]
PointNet [Qi et al. 2017b]
PointNet++ [Qi et al. 2017c]
PCNN [Atzmon et al. 2018]
Ours (Baseline)
Table 3. Complexity, forward time, and accuracy of different models
more efficient. The number of points in each experiment is also 1024
in this section.
More Experiments on ModelNet40
We also experiment with various settings of our model on the ModelNet40 [Wu et al. 2015] dataset. In particular, we analyze the effectiveness of the different distance metrics, explicit usage of xi −xj,
and more points.
Table 4 shows the results. “Centralization” denotes using concatenation of xi and xi −xj as the edge features rather than concatenating xi and xj. “Dynamic graph recomputation” denotes we
reconstruct the graph rather than using a fixed graph. Explicitly
centralizing each patch by using the concatenation of xi and xi −xj
leads to about 0.5% improvement for overall accuracy. By dynamically updating graph, there is about 0.7% improvement, and Figure 4
also suggests that the model can extract semantically meanigful
features. Using more points further improves the overall accuracy
We also experiment with different numbersk of nearest neighbors
as shown in Table 5. For all experiments, the number of points
is still 1024. While we do not exhaustively experiment with all
possible k, we find with large k that the performance degenerates.
This confirms our hypothesis that for certain density, with large
k the Euclidean distance fails to approximate geodesic distance,
destroying the geometry of each patch.
We further evaluate the robustness of our model (trained on
1,024 points with k = 20) to point cloud density. We simulate the
environment that random input points drops out during testing.
Figure 5 shows that even half of points is dropped, the model still
achieves reasonable results. With fewer than 512 points, however,
performance degenerates dramatically.
Mean Class Accuracy(%)
Overall Accuracy(%)
Table 4. Effectiveness of different components. CENT denotes centralization, DYN denotes dynamical graph recomputation, and MPOINTS denotes
experiments with 2048 points
Fig. 5. Left: Results of our model tested with random input dropout. The
model is trained with number of points being 1024 and k being 20. Right:
Point clouds with different number of points. The numbers of points are
shown below the bottom row.
Number of nearest neighbors (k)
Class Accuracy(%)
Accuracy(%)
Table 5. Results of our model with different numbers of nearest neighbors.
Part Segmentation
Data. We extend our EdgeConv model architectures for part segmentation task on ShapeNet part dataset [Yi et al. 2016]. For this
task, each point from a point cloud set is classified into one of a
few predefined part category labels. The dataset contains 16,881
3D shapes from 16 object categories, annotated with 50 parts in
total. 2,048 points are sampled from each training shape, and most
sampled point sets are labeled with less than six parts. We follow
the official train/validation/test split scheme as Chang et al. 
in our experiment.
Architecture. The network architecture is illustrated in Figure 3
(bottom branch). After a spatial transformer network, three Edge-
Conv layers are used. A shared fully-connected layer (1024) aggregates information from the previous layers. Shortcut connections
are used to include all the EdgeConv outputs as local feature descriptors. At last, three shared fully-connected layers (256, 256, 128)
are used to transform the pointwise features. Batch-norm, dropout,
and ReLU are included in the similar fashion to our classification
Training. The same training setting as in our classification task
is adopted. A distributed training scheme is further implemented
on two NVIDIA TITAN X GPUs to maintain the training batch size.
Results. We use Intersection-over-Union (IoU) on points to evaluate our model and compare with other benchmarks. We follow
the same evaluation scheme as PointNet: The IoU of a shape is
computed by averaging the IoUs of different parts occurring in that
shape, and the IoU of a category is obtained by averaging the IoUs
of all the shapes belonging to that category. The mean IoU (mIoU) is
finally calculated by averaging the IoUs of all the testing shapes. We
ACM Trans. Graph., Vol. 1, No. 1, Article 1. Publication date: January 2019.
Dynamic Graph CNN for Learning on Point Clouds
Fig. 6. Our part segmentation testing results for tables, chairs and lamps.
compare our results with PointNet [Qi et al. 2017b], PointNet++ [Qi
et al. 2017c], Kd-Net [Klokov and Lempitsky 2017], LocalFeatureNet
[Shen et al. 2017], PCNN [Atzmon et al. 2018], and PointCNN [Li
et al. 2018a]. The evaluation results are shown in Table 6. We also
visually compare the results of our model and PointNet in Figure 7.
More examples are shown in Figure 6.
Intra-cloud distances. We next explore the relationships between
different point clouds captured using our features. As shown in
Figure 8, we take one red point from a source point cloud and
compute its distance in feature space to points in other point clouds
from the same category. An interesting finding is that although
points are from different sources, they are close to each other if they
are from semantically similar parts. We evaluate on the features
after the third layer of our segmentation model for this experiment.
Segmentation on partial data. Our model is robust to partial data.
We simulate the environment that part of the shape is dropped
from one of six sides (top, bottom, right, left, front and back) with
different percentages. The results are shown in Figure 9. On the left,
Ground truth
Fig. 7. Compare part segmentation results. For each set, from left to right:
PointNet, ours and ground truth.
the mean IoU versus “keep ratio” is shown. On the right, the results
for an airplane model are visualized.
Indoor Scene Segmentation
Data. We evaluate our model on Stanford Large-Scale 3D Indoor
Spaces Dataset (S3DIS) [Armeni et al. 2016] for a semantic scene
segmentation task. This dataset includes 3D scan point clouds for 6
indoor areas including 272 rooms in total. Each point belongs to one
of 13 semantic categories—e.g. board, bookcase, chair, ceiling, and
beam—plus clutter. We follow the same setting as Qi et al. [2017b],
where each room is split into blocks with area 1m × 1m, and each
point is represented as a 9D vector (XYZ, RGB, and normalized
spatial coordinates). 4,096 points are sampled for each block during
training process, and all points are used for testing. We also use
the same 6-fold cross validation over the 6 areas, and the average
evaluation results are reported.
The model used for this task is similar to part segmentation model,
except that a probability distribution over semantic object classes is
generated for each input point and no categorical vector is used here.
We compare our model with both PointNet [Qi et al. 2017b] and
PointNet baseline, where additional point features (local point density, local curvature and normal) are used to construct handcrafted
features and then fed to an MLP classifier. We further compare our
work with [Engelmann et al. 2017] and PointCNN [Li et al. 2018a].
Engelmann et al. present network architectures to enlarge
the receptive field over the 3D scene. Two different approaches
are proposed in their work: MS+CU for multi-scale block features
with consolidation units; G+RCU for the grid-blocks with recurrent
ACM Trans. Graph., Vol. 1, No. 1, Article 1. Publication date: January 2019.
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon
PointNet++
LocalFeatureNet
Table 6. Part segmentation results on ShapeNet part dataset. Metric is mIoU(%) on points.
Source points
Other point clouds from the same category
Fig. 8. Visualize the Euclidean distance (yellow: near, blue: far) between
source points (red points in the left column) and multiple point clouds
from the same category in the feature space after the third EdgeConv layer.
Notice source points not only capture semantically similar structures in
the point clouds that they belong to, but also capture semantically similar
structures in other point clouds from the same category.
consolidation Units. We report evaluation results in Table 7, and
visually compare the results of PointNet and our model in Figure 10.
Fig. 9. Left: The mean IoU (%) improves when the ratio of kept points increases. Points are dropped from one of six sides (top, bottom, left, right, front
and back) randomly during evaluation process. Right: Part segmentation
results on partial data. Points on each row are dropped from the same side.
The keep ratio is shown below the bottom row. Note that the segmentation
results of turbines are improved when more points are included.
PointNet (baseline) [Qi et al. 2017b]
PointNet [Qi et al. 2017b]
MS + CU(2) [Engelmann et al. 2017]
G + RCU [Engelmann et al. 2017]
PointCNN [Li et al. 2018a]
Table 7. 3D semantic segmentation results on S3DIS. MS+CU for multi-scale
block features with consolidation units; G+RCU for the grid-blocks with
recurrent consolidation Units.
DISCUSSION
In this work we propose a new operator for learning on point cloud
and show its performance on various tasks. Our model suggests
that local geometric features are important to 3D recognition tasks,
even after introducing machinery from deep learning.
While our architectures easily can be incorporated as-is into
existing pipelines for point cloud-based graphics, learning, and
vision, our experiments also indicate several avenues for future
research and extension. Some details of our implementation could be
revised and/or re-engineered to improve efficiency or scalability, e.g.
incorporating fast data structures rather than computing pairwise
ACM Trans. Graph., Vol. 1, No. 1, Article 1. Publication date: January 2019.
Dynamic Graph CNN for Learning on Point Clouds
Ground truth
Real color
Fig. 10. Semantic segmentation results. From left to right: PointNet, ours, ground truth and point cloud with original color. Notice our model outputs smoother
segmentation results, for example, wall (cyan) in top two rows, chairs (red) and columns (magenta) in bottom two rows.
distances to evaluate k-nearest neighbors queries. We also could
consider higher-order relationships between larger tuples of points,
rather than considering them pairwise. Another possible extension
is to design a non-shared transformer network that works on each
local patch differently, adding flexibility to our model.
Our experiments suggest that intrinsic features can be equally
valuable if not more valuable than point coordinates; developing a
practical and theoretically-justified framework for balancing intrinsic and extrinsic considerations in a learning pipeline will require
insight from theory and practice in geometry processing. Given this,
we will consider applications of our techniques to more abstract
point clouds coming from applications like document retrieval and
image processing rather than 3D geometry; beyond broadening
the applicability of our technique, these experiments will provide
insight into the role of geometry in abstract data processing.
ACKNOWLEDGMENTS
The authors acknowledge the generous support of Army Research
Office grant W911NF-12-R-0011, of Air Force Office of Scientific
Research award FA9550-19-1-0319, of National Science Foundation
grant IIS-1838071, of ERC Consolidator grant No. 724228 (LEMAN),
from an Amazon Research Award, from the MIT-IBM Watson AI
Laboratory, from the Toyota-CSAIL Joint Research Center, from the
Skoltech-MIT Next Generation Program, and from Google Faculty
ACM Trans. Graph., Vol. 1, No. 1, Article 1. Publication date: January 2019.
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon
Research Award. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and
do not necessarily reflect the views of these organizations.