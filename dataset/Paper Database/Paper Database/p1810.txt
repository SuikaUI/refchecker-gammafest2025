Hierarchical Recurrent Neural Network for Skeleton Based Action Recognition
Yong Du, Wei Wang, Liang Wang
Center for Research on Intelligent Perception and Computing, CRIPAC
Nat’l Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences
{yong.du, wangwei, wangliang}@nlpr.ia.ac.cn
Human actions can be represented by the trajectories of
skeleton joints. Traditional methods generally model the
spatial structure and temporal dynamics of human skeleton
with hand-crafted features and recognize human actions by
well-designed classiﬁers. In this paper, considering that recurrent neural network (RNN) can model the long-term contextual information of temporal sequences well, we propose
an end-to-end hierarchical RNN for skeleton based action
recognition. Instead of taking the whole skeleton as the input, we divide the human skeleton into ﬁve parts according to human physical structure, and then separately feed
them to ﬁve subnets. As the number of layers increases, the
representations extracted by the subnets are hierarchically
fused to be the inputs of higher layers. The ﬁnal representations of the skeleton sequences are fed into a single-layer
perceptron, and the temporally accumulated output of the
perceptron is the ﬁnal decision. We compare with ﬁve other
deep RNN architectures derived from our model to verify
the effectiveness of the proposed network, and also compare with several other methods on three publicly available
datasets. Experimental results demonstrate that our model
achieves the state-of-the-art performance with high computational efﬁciency.
1. Introduction
As an important branch of computer vision, action recognition has a wide range of applications, e.g., intelligent
video surveillance, robot vision, human-computer interaction, game control, and so on . Traditional studies
about action recognition mainly focus on recognizing actions from videos recorded by 2D cameras. But actually,
human actions are generally represented and recognized in
the 3D space. Human body can be regarded as an articulated system including rigid bones and hinged joints which
are further combined into four limbs and a trunk . Human actions are composed of the motions of these limbs
and trunk which are represented by the movements of hu-
Fully Connected Layer
Softmax Layer
Layer8 Layer9
Figure 1: An illustrative sketch of the proposed hierarchical recurrent neural network. The whole skeleton is divided
into ﬁve parts, which are fed into ﬁve bidirectional recurrent neural networks (BRNNs). As the number of layers
increases, the representations extracted by the subnets are
hierarchically fused to be the inputs of higher layers. A
fully connected layer and a softmax layer are performed on
the ﬁnal representation to classify the actions.
man skeleton joints in the 3D space . Currently, reliable
joint coordinates can be obtained from the cost-effective
depth sensor using the real-time skeleton estimation algorithms . Effective approaches should be investigated
for skeleton based action recognition.
Human skeleton based action recognition is generally
considered as a time series problem , in which the
characteristics of body postures and their dynamics over
time are extracted to represent a human action. Most of
the existing skeleton based action recognition methods explicitly model the temporal dynamics of skeleton joints by
using Temporal Pyramids (TPs) and Hidden
Markov Models (HMMs) . The TPs methods
are generally restricted by the width of the time windows
and can only utilize limited contextual information. As for
HMMs, it is very difﬁcult to obtain the temporal aligned sequences and the corresponding emission distributions. Recently, recurrent neural networks (RNNs) with Long-Short
Term Memory (LSTM) neurons have been used for
action recognition . All this work just uses single layer RNN as a sequence classiﬁer without part-based
feature extraction and hierarchical fusion.
In this paper, taking full advantage of deep RNN in modelling the long-term contextual information of temporal sequences, we propose a hierarchical RNN for skeleton based
action recognition. Fig. 1 shows the architecture of the proposed network, in which the temporal representations of
low-level body parts are modeled by bidirectional recurrent
neural networks (BRNNs) and combined into the representations of high-level parts.
Human body can be roughly decomposed into ﬁve parts,
e.g., two arms, two legs and one trunk, and human actions
are composed of the movements of these body parts. Given
this fact, we divide the human skeleton into the ﬁve corresponding parts, and feed them into ﬁve bidirectionally recurrently connected subnets (BRNNs) in the ﬁrst layer. To
model the movements from the neighboring skeleton parts,
we concatenate the representation of the trunk subnet with
those of the other four subnets, respectively, and then input these concatenated results to four BRNNs in the third
layer as shown in Fig. 1. With the similar procedure, the
representations of the upper body, the lower body and the
whole body are obtained in the ﬁfth and seventh layers, respectively. Up to now, we have ﬁnished the representation
learning of skeleton sequences. Finally, a fully connected
layer and a softmax layer are performed on the obtained representation to classify the actions. It should be noted that,
to overcome the vanishing gradient problem when training
RNN , we adopt LSTM neurons in the last BRNN
In the experiments, we compare with ﬁve other deep
RNN architectures derived from our proposed model to verify the effectiveness of the proposed network, and compare
with several methods on three publicly available datasets.
Experimental results demonstrate that our method achieves
the state-of-the-art performance with high computational
efﬁciency. The main contributions of our work can be summarized as follows. Firstly, to the best of our knowledge,
we are the ﬁrst to provide an end-to-end solution for skeleton based action recognition by using hierarchical recurrent
neural network. Secondly, by comparing with other ﬁve derived deep RNN architectures, we verify the effectiveness
of the necessary parts of the proposed network, e.g., bidirectional network, LSTM neurons in the last BRNN layer,
hierarchical skeleton part fusion. Finally, we demonstrate
that our proposed model can handle skeleton based action
recognition very well without sophisticated preprocessing.
The remainder of this paper is organized as follows. In
Section 2, we introduce the related work on skeleton based
action recognition. In Section 3, we ﬁrst review the background of RNN and LSTM, and then illustrate the details of
the proposed network. Experimental results and discussion
are presented in Section 4. Finally, we conclude the paper
in Section 5.
2. Related Work
In this section, we brieﬂy review the existing literature
that closely relates to the proposed model, including three
categories of approaches representing temporal dynamics
by local features, sequential state transitions and RNN.
Approaches with local features
By clustering the extracted joints into ﬁve parts, Wang et al. use the spatial
and temporal dictionaries of the parts to represent actions,
which can capture the spatial structure of human body and
movements. Chaudhry et al. encode the skeleton structure with a spatial-temporal hierarchy, and exploit Linear
Dynamical Systems to learn the dynamic features. Vemulapalli et al. utilize rotations and translations to represent the 3D geometric relationships of body parts in Lie
group, and then employ Dynamic Time Warping (DTW)
and Fourier Temporal Pyramid (FTP) to model the temporal dynamics. Instead of modelling temporal evolution of
features, Luo et al. develop a novel dictionary learning method combined with Temporal Pyramid Matching, to
keep the temporal dynamics. To represent both human motions and correlative objects, Wang et al. ﬁrst extract
the local occupancy patterns from the appearance around
skeleton joints, and then process them with FTP to obtain
temporal structure. Zanﬁr et al. propose a moving pose
descriptor for capturing postures and skeleton joints. Using ﬁve joints coordinates and their temporal differences as
inputs, Cho and Chen perform action recognition with a
hybrid multi-layer perceptron. In the above methods, the local temporal dynamics is generally represented within a certain time window or differential quantities, it cannot globally capture the temporal evolution of actions.
Approaches with sequential state transitions
al. extract local features of individual and partial combinations of joints, and train HMMs to capture the action dynamics. Based on skeletal joints features, Wu and
Shao adopt a deep forward neural network to estimate
the emission probabilities of the hidden states in HMM,
and then infer action sequences. To accurately calculate the
similarity between two sequences with Dynamic Manifold
Warping, Gong et al. perform both temporal segmentation and alignment with structured time series representations. Though HMM can model the temporal evolution
of actions, the input sequences have to be segmented and
aligned, which in itself is a very difﬁcult task.
Approaches with RNN
The combination of RNN and
perceptron can directly classify sequences without any segmentation. By obtaining sequential representations with a
3D convolutional neural network, Baccouche et al. propose a LSTM-RNN to recognize actions. Regarding the
histograms of optical ﬂow as inputs, Grushin et al. use
LSTM-RNN for robust action recognition and achieve good
results on KTH dataset. Considering that LSTM-RNNs employed in and are both unidirectional with only one
hidden layer, Lefebvre et al. propose a bidirectional
LSTM-RNN with one forward hidden layer and one backward hidden layer for gesture classiﬁcation.
All the work above just uses RNN as a sequence classi-
ﬁer while we propose an end-to-end solution including both
feature learning and sequence classiﬁcation. Considering
the fact that human actions are composed of the motions of
human body parts, we use RNN in a hierarchical way.
3. Our Model
In order to put our proposed model into context, we ﬁrst
review recurrent neural network (RNN) and Long-Short
Term Memory neuron (LSTM). Then we propose a hierarchical bidirectional RNN to solve the problem of skeleton
based action recognition. Finally, ﬁve relevant deep RNNs
with different architectures are also introduced.
3.1. Review of RNN and LSTM
The main difference between RNN and the feedforward
networks is the presence of feedback loops which produce
the recurrent connection in the unfolded network. With the
recurrent structure, RNN can model the contextual information of a temporal sequence. Given an input sequence
x = (x0, . . . , xT −1), the hidden states of a recurrent layer
h = (h0, . . . , hT −1) and the output of a single hidden
layer RNN y = (y0, . . . , yT −1) can be derived as follows .
ht = H(Wxhxt + Whhht−1 + bh)
yt = O(Whoht + bo)
where Wxh, Whh, Who denote the connection weights from
the input layer x to the hidden layer h, the hidden layer h
to itself and the hidden layer to the output layer y, respectively. bh and bo are two bias vectors, H(·) and O(·) are the
activation functions in the hidden layer and the output layer.
Generally, it is very difﬁcult to train RNNs (especially
deep RNNs) with the commonly-used activation functions,
e.g., tanh and sigmoid functions, due to the vanishing gradient and error blowing up problems . To solve these
problems, the Long-Short Term Memory (LSTM) architecture has been proposed , which replaces the nonlinear units in traditional RNNs. Fig. 2 illustrates a LSTM
memory block with a single cell.
It contains one selfconnected memory cell c and three multiplicative units, i.e.,
the input gate i, the forget gate f and the output gate o,
which can store and access the long range contextual information of a temporal sequence.
The activations of the memory cell and three gates are
given as follows:
it = σ(Wxixt + Whiht−1 + Wcict−1 + bi)
Forget gate
Output gate
Input gate
Figure 2: Long Short-Term Memory block with one cell .
f t = σ(Wxfxt + Whfht−1 + Wcfct−1 + bf)
ct = f tct−1 + ittanh(Wxcxt + Whcht−1 + bc)
ot = σ(Wxoxt + Whoht−1 + Wcoct + bo)
ht = ottanh(ct)
where σ(·) is the sigmoid function, and all the matrices W
are the connection weights between two units.
In order to utilize the past and future context for every
point in the sequence, Schuster and Paliwal proposed
the bidirectional recurrent neural network (BRNN), which
presents the sequence forwards and backwards to two separate recurrent hidden layers. These two recurrent hidden
layers share the same output layer. A bidirectional recurrent
neural network is illustrated in Fig. 3. It should be noted
that we can easily obtain LSTM-BRNN just by replacing
the nonlinear units in Fig. 3 with LSTM blocks.
Forward layer
Backward layer
Figure 3: The architecture of bidirectional recurrent neural
network .
3.2. Hierarchical RNN for Skeleton Based Action
Recognition
According to human physical structure, the human skeleton can be decomposed into ﬁve parts, e.g., two arms, two
legs and one trunk. Simple human actions are performed
by only one part of them, e.g., punching forward and kicking forward mainly depend on swinging the arms and legs,
respectively. Some actions come from moving the upper
body or the lower body, e.g., bending down mainly relates
Input layer
BRNNs (bl1)
Fusion layer
Tanh-BRNNs
Tanh-BRNNs
Fusion layer
layer (fl3)
BRNN (bl4)
layer (fc)
Figure 4: The architecture of our proposed model.
to the upper body. More complex actions are composed of
the motions of these ﬁve parts, e.g., running and swimming
need to coordinate the moving of arms, legs and the trunk.
To effectively recognize various human actions, modelling
the movements of these individual parts and their combinations is very necessary. Beneﬁting from the power of RNN
to access the contextual information, we propose a hierarchical bidirectional RNN for skeleton based action recognition. Different from traditional methods modelling the spatial structure and temporal dynamics with hand-crafted features and recognizing actions by well-designed classiﬁers,
our model provides an end-to-end solution for action recognition.
The framework of the proposed model is shown in Fig.
4. We can see that our model is composed of 9 layers, i.e.,
bl1 −bl4, fl1 −fl3, fc, and sl, each of which presents different structures and thus plays different role in the whole
network. In the ﬁrst layer bl1, the ﬁve skeleton parts are
fed into ﬁve corresponding bidirectionally recurrently connected subnets (BRNNs). To model the neighboring skeleton parts, e.g., left arm-trunk, right arm-trunk, left leg-trunk,
and right leg-trunk, we combine the representation of the
trunk subnet with that of the other four subnets to obtain
four new representations in the fusion layer fl1. Similar to
the layer bl1, these resulting four representations are separately fed into four BRNNs in the layer bl2. To model the
upper and lower body, the representations of the left armtrunk and right arm-trunk BRNNs are further combined to
obtain the upper body representation while the representations of the left leg-trunk and right leg-trunk BRNNs are
combined to obtain the lower body representation in the
layer fl2. Finally, the newly obtained two representations
are fed into two BRNNs in the layer bl3, and the representations of these two BRNNs in the layer bl3 are fused again
to represent the whole body in the layer fl3. The temporal
dynamics of the whole body representation is further modelled by another BRNN in the layer bl4. From a viewpoint
of feature learning, these stacked BRNNs can be considered
to extract the spatial and temporal features of the skeleton
sequences. After obtaining the ﬁnal features of the skeleton
sequence, a fully connected layer fc and a softmax layer
sm are performed to classify the action.
As mentioned in Section 3.1, the LSTM architecture can
effectively overcome the vanishing gradient problem while
training RNNs . However, we just adopt LSTM
neurons in the last recurrent layer (bl4).
The ﬁrst three
BRNN layers all use the tanh activation function. This is a
trade-off between improving the representation ability and
avoiding overﬁtting. Generally, the number of weights in a
LSTM block is several times more than that in a tanh neuron. It is very easy to overﬁt the network with limited training sequences.
3.3. Training
Training the proposed model contains a forward pass and
a backward pass.
Forward pass: For the i-th BRNN layer bli at time t, given
the q-th inputs It
iq and tanh activation function, the corresponding q-th representations of the forward layer and backward layer −→h t
iq are deﬁned as follows
iq = tanh(WIiq
h iq ) (8)
iq = tanh(WIiq
h iq ) (9)
where all the matrices W, vectors b are the corresponding
connection weights and biases.
For the following fusion layer fli at time t, the pth newly concatenated representation as the input of the
(i + 1)-th BRNN layer bli+1 is
(i+1)p = −→h t
where  denotes the concatenation operator, −→h t
ij and ←−h t
are the hidden representations of the forward layer and
backward layer of the j-th part in the i-th BRNN layer, −→h t
ik from the k-th part in the i-th layer.
For the last BRNN layer bl4 with LSTM neurons, the forward output −→h t
bl4 and backward output ←−h t
bl4 can be derived
from Eqn. (3-7).
Combining −→h t
bl4 and ←−h t
bl4 as the input to the fully connected layer fc, the output Ot of the layer fc is
h bl4 , W←
h bl4 are the connection weights from the
forward and backward layers of bl4 to the layer fc.
Finally, the outputs of the layer fc are accumulated
across the T frame sequence, and the accumulated results
{Ak} are normalized by the softmax function to get each
class probability p(Ck):
Here there are C classes of human actions.
The objective function of our model is to minimize the
maximum-likelihood loss function :
δ(k −r)p(Ck|Ωm)
where δ(·) is the Kronecker function, and r denotes the
groundtruth label of the sequence Ωm. There are M sequences in the training set Ω.
Backward pass: We use the back-propagation through
time (BPTT) algorithm to obtain the derivatives of
the objective function with respect to all the weights, and
minimize the objective function by stochastic gradient descent .
3.4. Five Comparative Architectures
In order to verify the effectiveness of the proposed network, we compare with other ﬁve different architectures
derived from our proposed model. As illustrated before,
we propose a hierarchically bidirectional RNN (HBRNN-
L) for skeleton based action recognition (the sufﬁx “-L”
means that only the last recurrent layer consists of LSTM
neurons, and the rest likewise). To prove the importance
of the bidirectional connection, a similar network with unidirectional connection is proposed, which is called hierarchically unidirectional RNN (HURNN-L). To verify the
role of part-based feature extraction and hierarchical fusion, we compare a deep bidirectional RNN (DBRNN-L),
which is directly stacked with several RNNs with the whole
human skeleton as the input.
Furthermore, we compare
a deep unidirectional RNN (DURNN-L) which does not
adopt both the bidirectional connection and the hierarchical fusion. To further investigate whether LSTM neurons in
the last recurrent layer are useful to overcome the vanishing/exploding problem in RNN, we examine another two
architectures DURNN-T and DBRNN-T. Here DURNN-T
and DBRNN-T are the similar networks to DURNN-L and
DBRNN-L, but with the tanh activation function in all layers. It should be noted that all the six architectures have ﬁve
learnable layers, i.e., four recurrent hidden layers and one
fully connected layer. And the number of neurons in the
fully connected layer is equal to that of action categories.
4. Experiments
In this section, we evaluate our model and compare with
other ﬁve different architectures and several recent work on
three benchmark datasets: MSR Action3D Dataset ,
Berkeley Multimodal Human Action Dataset (Berkeley
MHAD) , and Motion Capture Dataset HDM05 .
We also discuss the overﬁtting issues and the computational
efﬁciency of the proposed model.
4.1. Evaluation Datasets
MSR Action3D Dataset: It is generated by a Microsoft
Kinect-like depth sensor, which is widely used in action
recognition. This dataset consists of 20 actions performed
by 10 subjects in an unconstrained way for two or three
times, 557 valid samples with 22077 frames. All sequences
are captured in 15 FPS, and each frame in a sequence contains 20 skeleton joints. The low accuracy of the skeleton
joint coordinates and the partial fragment missing in some
sequences make this dataset very challenging.
Berkeley MHAD: It is captured by a multimodal acquisition system, in which an optical motion capture system
is used to capture the 3D position of active LED markers
with the frequency of 480 Hz. This dataset contains 659
sequences for 11 actions performed by 12 subjects with 5
repetitions of each action. In each frame of the sequence,
there are 35 joints accurately extracted according to the 3D
marker trajectory.
Motion Capture Dataset HDM05: It is captured by an
optical marker-based technology with the frequency of 120
Hz, which contains 2337 sequences for 130 actions performed by 5 non-professional actors, and 31 joints in each
To our knowledge, this dataset is currently the
largest depth sequence database which provides the skeleton joint coordinates for action recognition. As stated in ,
some samples of these 130 actions should be classiﬁed into
the same category, e.g., jogging starting from air and jogging starting from ﬂoor are the same action, jogging 2 steps
and jogging 4 steps belong to the same “jogging” action.
After sample combination, the actions are reduced to 65 categories.
4.2. Data Preprocessing and Parameter Settings
In our proposed model, all the human skeleton joints
are divided into ﬁve parts, i.e., two arms, two legs and one
trunk, which are illustrated in Fig. 5. We can see that there
are 4 joints for each part in MSR Action3D dataset. For
Berkeley MHAD and HDM05 datasets, the joint numbers
of arms, legs and the trunk are listed as follows: 7, 7, 7 and
Given that human actions are independent of its absolute
spatial position, we normalize the skeleton joints to an uni-
ﬁed coordinate system. The origin of the coordinate system
Table 1: The parameter settings of our proposed model and the ﬁve compared models on three evaluation datasets. The DU.T
is short for DURNN-T, and the rest likewise. The LLi indicates the i-th learnable layer (bli in HBRNN-L).
MSR Action3D
Berkeley MHAD & HDM05
15 × 2 × 5
15 × 2 × 5
30 × 2 × 4
30 × 2 × 4
60 × 2 × 2
60 × 2 × 2
40 × 2 × 1
60 × 2 × 1
is deﬁned as follows
O = (Jhip center + Jhip left + Jhip right)/3
where Jhip center is the 3D coordinate of the hip center, and
the other two have the similar meanings.
To improve the signal to noise ratio of the raw data, we
adopt a simple Savitzky-Golay smoothing ﬁlter to preprocess the data. The ﬁlter is designed as follows
fi = (−3xi−2+12xi−1+17xi+12xi+1−3xi+2)/35 (16)
where xi denotes the skeleton joint coordinate in the i-th
frame, and fi denotes the ﬁltering result.
Considering that the trajectories of the skeleton joints
vary smoothly, we sample the frames from the sequences in
the ﬁxed interval to reduce the computation cost. There are
every 16 frames sampled for the Berkeley MHAD dataset
and every 4 frames for the HDM05 dataset.
sample frames from MSR Action3D dataset due to the limited frame rates (15 FPS) and average length (less than 40
Tab. 1 shows the parameter settings of our proposed
model and the ﬁve compared models on three evaluation
datasets. Each value in the table indicates the number of
neurons used in the corresponding layer, e.g., the number
30 × 5 (LL1, HU.L) means that each unidirectional subnet
in the ﬁrst learnable layer of HURNN-L has 30 neurons, and
the number 15×2×5 (LL1, HB.L) indicates that each bidirectional subnet in the ﬁrst BRNN layer (bl1) of HBRNN-L
has 15×2 neurons. These six networks on the same dataset
have roughly the same number of weights.
It should be noted that the results of all the comparative
methods on the three datasets are from their corresponding
4.3. Experimental Results and Analysis
MSR Action3D Dataset: Although there are several validation methods summarized in on this dataset, we follow the standard protocol provided in . In this standard
protocol, the dataset is divided into three action sets AS1,
AS2 and AS3. The samples of subjects 1, 3, 5, 7, 9 are used
for training while the samples of subjects 2, 4, 6, 8, 10 are
used for testing. We compare the proposed model HBRNN-
L with Li et al. , Chen et al. , Gowayyed et al. ,
065$FWLRQ''DWDVHW
%HUNHOH\0+$'
Figure 5: The human skeleton joints are divided into ﬁve
parts in these three datasets.
Vemulapalli et al. and other ﬁve variant architectures
DURNN-T, DBRNN-T, DURNN-L, DBRNN-L, HURNN-
L. The experimental results are shown in Tab. 2. We can
see that our proposed HBRNN-L achieves the best average
accuracy and outperforms the four methods in 
with hand-crafted features, and the performances of two derived models HURNN-L and DBRNN-L are promising. It
should be noted that although Chen et al. and Vemulapalli et al. achieve the best performance in action sets
AS1 and AS3, respectively, our HBRNN-L outperforms
them with respect to the average accuracy. Furthermore,
HBRNN-L performs consistently well on these three action
sets, which indicates that HBRNN-L is more robust to various data.
Experimental results on the MSR Action3D
Li et al., 2010 
Chen et al., 2013 
Gowayyed et al., 2013 
Vemulapalli et al., 2014 
The fact that HBRNN-L obtains higher average accuracy than HURNN-L, DBRNN-L and DURNN-L, proves
the importance of bidirectional connection and hierarchical
feature extraction. All the networks with LSTM neurons
in the last recurrent layer (with sufﬁx “-L”) are better than
their corresponding networks with tanh activation functions
(with sufﬁx “-T”), which veriﬁes the effectiveness of LSTM
neurons in the proposed network.
HorizontalArmWave
ForwardPunch
TennisServe
PickupAndThrow
(a) AS1 - 93.33%
HighArmWave
DrawCircle
TwoHandWave
ForwardKick
SideBoxing
(b) AS2 - 94.64%
ForwardKick
PickupAndThrow
TennisServe
TennisSwing
(c) AS3 - 95.50%
Figure 6: Confusion matrices of HBRNN-L on MSR Action3D dataset.
The confusion matrices on the three action sets are
shown in Fig. 6.
We can see that the misclassiﬁcations
mainly occur among several very similar actions. For example in Fig. 6a, the action “PickupAndThrow” is often misclassiﬁed to “Bend” while the action “Forward-
Punch” is misclassiﬁed to “TennisServe”. Actually, “PickupAndThrow” just has one more “throw” move than “Bend”,
and the “throw” move often holds few frames in the sequence.
So it is very difﬁcult to distinguish these two
actions. The actions “ForwardPunch” and “TennisServe”
share a large overlap in the sequences. Distinguishing them
is also very challenging with only joint coordinates.
Berkeley MHAD: We follow the experimental protocol
proposed in on this dataset.
The 384 sequences of
the ﬁrst 7 subjects are used for training while the 275 sequences of the last 5 subjects are used for testing. We compare our proposed model with Oﬂi et al. , Vantigodi
et al. , Vantigodi et al. , Kapsouras et al. ,
Chaudhry et al. , as well as DURNN-T, DBRNN-T,
DURNN-L, DBRNN-L, HURNN-L. The experimental results are shown in Tab. 3.
We can see that HBRNN-L
achieves the 100% accuracy with a simple preprocessing
and performs better than those ﬁve derived RNN architectures, which proves the advantages of the proposed model
once again. Meanwhile, the six RNN architectures obtain
higher accuracy than Oﬂi et al. , Vantigodi et al. ,
Vantigodi et al. , Kapsouras et al. , and comparable
results with Chaudhry et al. , which means that our proposed model provides an effective end-to-end solution for
modelling temporal dynamics in action sequences.
HDM05: We follow the experimental protocol proposed in
 and perform 10-fold cross validation on this dataset. We
compare our proposed model with Cho and Chen and
other ﬁve architectures DURNN-T, DBRNN-T, DURNN-
L, DBRNN-L, HURNN-L. The experimental results are
showed in Tab. 4. The proposed model HBRNN-L obtains the state-of-the-art accuracy of 96.92% with the stan-
Table 3: Experimental results on the Berkeley MHAD.
Oﬂi et al., 2014 
Vantigodi et al., 2013 
Vantigodi et al., 2014 
Kapsouras et al., 2014 
Chaudhry et al., 2013 
Chaudhry et al., 2013 
dard deviation of 0.50. The derived models HURNN-L,
DBRNN-L and DURNN-L also obtain excellent results.
Table 4: Experimental results on the HDM05.
Cho and Chen, 2013 
(a) 97.52%
(b) 96.04%
Figure 7: Two typical confusion matrices of HBRNN-L on
the HDM05 dataset. The numbers on the horizontal and
vertical axes correspond to the action categories .
Two typical confusion matrices of the 10-fold crossvalidation from HBRNN-L are shown in Fig. 7. We can
see that our model performs well on most of the actions. The misclassiﬁcations mainly come from the following categories: “5-depositHighR”, “6-depositLowR”, “7depositMiddleR”, “10-grabHighR”, “11-grabLowR”, and
“12-grabMiddleR”. Further checking the “grab” and “deposit” related skeleton sequences, we ﬁnd that these two
categories of actions share the similar spatial and temporal variations. Both of them can be decomposed into three
sub-actions in chronological order: stretching out one hand,
grabbing or depositing something, and drawing back the
The minor differences between grabbing and depositing something make it difﬁcult to distinguish these two
kinds of actions. It should be noted that although the original 130 actions are reduced to 65 categories, there are still
several confusing categories, e.g., “39-sitDownChair” and
“42-sitDownTable”, which should belong to the same action. Without the context of actions, e.g., recognizing chair
and table from their appearances, it is very difﬁcult to distinguish these actions just from skeleton streams.
4.4. Discussion
Overﬁtting issues: The experiments show that the models with sufﬁx “-L” are easy to overﬁt while the others with
sufﬁx “-T” always underﬁt during training. It may be the
vanishing gradient problem by using tanh activation function in all the layers. In order to overcome the overﬁtting
problem in our proposed HBRNN-L and other derived networks with sufﬁx “-L”, we adopt the strategies like adding
the input noise, weight noise and early stopping . In
our practice, we ﬁnd that adding the weight noise is more
effective than adding the input noise, and the commonlyused dropout strategy does not work here. For the underﬁtting problem of the models with sufﬁx “-T”, we use
the retraining strategy by tuning learning rate and adding
various levels of input noise and weight noise.
Computational efﬁciency: We take the Berkeley MHAD
dataset for an example to illustrate the efﬁciency of
HBRNN-L. With C++ implementation on a CPU 3.2GHz
system, we spend 50s for each epoch consisting of 384 sequences (average 127ms per sequence) during training. After about 30 epochs, we can get an accuracy greater than
98%. During testing, it takes 52.46 ms per sequence (about
234 frames per sequence).
It should be mentioned that
HURNN-L, which achieves comparable performance with
HBRNN-L, runs much faster and is more suitable for online
applications.
5. Conclusion and Future Work
In this paper, we proposed an end-to-end hierarchical recurrent neural network for skeleton based action recognition. We ﬁrst divide the human skeleton into ﬁve parts, and
then feed them to ﬁve subnets. As the number of layers
increases, the representations in the subnets are hierarchically fused to be the inputs of higher layers. A perceptron is
performed on the learned representations of the skeleton sequences to obtain the ﬁnal recognition results. Experimental results on three publicly available datasets demonstrate
the effectiveness of the proposed network.
As we analyzed on the HDM05 and MSR Action3D
datasets, the similar human actions are very difﬁcult to be
distinguished just from the skeleton joints. In the future, we
will consider to combine more features into the proposed
hierarchical recurrent neural network, e.g., object appearance.
Acknowledgement
This work was supported by the National Basic Research
Program of China (2012CB316300) and National Natural Science Foundation of China (61175003, 61135002,
61202328, 61420106015, U1435221).