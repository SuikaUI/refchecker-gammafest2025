HEURISTICS FOR INTEGER PROGRAMMING USING
SURROGATE CONSTRAINTS
Fred Glover, University o[Colorado
This paper proposes a class of surrogate constraint heuristics for obtaining
approximate, 'near optimal solutions to integer programming problems. These heuris·
tics are based on a simple framework that illuminates the character of several carlier
heuristic proposals and provides a variety of new alternatives. The paper also pro·
poses additional heuristics that can be used either to supplement the surrogate constraint procedures or to provide independent solution strategies. Preliminary compulotional results are reported for applying one of these aJternatives to a class of
nonlinear generalized set covering problems involving approximately 100 constraints
and 300-500 integer variables. The solutions obtained by the tested procedure had
objective function values twice as good as values obtained by standard approaches
(e.g., reducing the best objective function values of other methods from 85 to 40 on
the average. Total solution time for the tested procedure ranged from ten to twenty
seconds on the CDC 6600.
INTRODUCTION
Heuristic solution methods for integer programming have maintained a noticeably separate existence from algorithms. Algorithms have long constituted the more
respectable side of the family, assuring an optimal solution in a finite number of steps.
Methods that merely claim to be clever, and do not boast an entourage of supporting
theorems and proofs, are accorded a lower status. Algorithms are conceived in analytic
purity in the high citadels of academic research, heuristics are midwifed by expediency in the dark corners of the practitioner's lair.
Recently, however, there has been a growing recognition that the algorithms
are not always successful, and that their heuristic cousins deserve a chance to prove
their mettle. Partly this comes from an emerging awareness that algorithms and heuristics are not as different as once supposed - algorithms, after all, are merely fastidious
heuristics in which epsilons and deltas abide by the dictates of mathematical etiquette.
It may even be said that algorithms exhibit a somewhat compulsive aspect, being
denied the freedom that would allow an occasional inconsistency or an exception to
ultimate convergence. (Unfortunately. ultimate convergence sometimes acquires a religious significance; it seems not to happen in this world.)
The heuristic approach, robust and boisterous, may have special advantages in
terrain too rugged or varied for algorithms. In fact, those who are fond of blurring
distinctions suggest that an algorithm worth its salt is one with "heuristic power."
If there is any merit to this view, it would be useful to single out specific
heuristic principles that appear to show promise. The distillation of such principles can
lead to the discovery of new possibilities and to more effective solutions.
HEURfSTfCS FOR fNTEGER PROGRAMMfNG
One such principle comes from the surrogate constrain t approach. This approach
is based on the idea that it may be possible to capture useful information not directly
available from the original problem constraints, taken one at a time, by forming a
non~negative linear combination of these constraints. The combination serves as a
proxy (or surrogate) for the others.
The surrogate constraint framework provides a convenient organizing principle
around which a class of heuristics can be formed. For example, as will be shown, the
Senju-Toyoda method , for 0·1 capital budgeting problems, and the Kochenberger-McCarl-Wyman method , for more general integer programming problems,
constitute two attractively simple members of this class.
The general surrogate constraint framework substantially increases the available
solution strategies. At the same time, the framework enlarges the classes of problems
that are potentially accessible to solution. Several important auxiliary heuristics amplify these considerations.
Framework for Surrogate Constraint Heuristics
The general procedural composition of the surrogate constraint framework is
quite simple and may be stated as a sequence of four principal steps.
1. Generate one or more surrogate constraints.
2. Determine one or more starting solutions.
3. By reference to the surrogate constraint(s), periodically or regularly updated,
establish measures of the goodness of increasing and decreasing each variable.
4. Sequentially change- increase or decrease- the values of the variables, singly
or in blocks, in accordance with their goodness measures, and keep track of the best
solution(s) found in the process.
[n spite of the simplicity of this framework, a variety of specific heuristic
methods, old and new, are encompassed within it. It is especially important to note,
for example, that no restrictions are imposed on the problem coefficients (such as
nonnegativity) or on the character of the starting solution (such as feasibility). This
feature by itself immediately distinguishes this framework from less general methods
such as . Other features provide other significant departures from less general
methods. The departure is caused by the general character of the surrogate constraints
themselves and by the methods that have been developed for exploiting them.
The use of the framework for surrogate constraints, beginning with immediate
and natural uses and progressing to higher level considera tions, will be discussed.
First-leve[ aspects of implementation can be summarized for each of the four
steps. These aspects draw upon ideas relating to surrogate constraints in other contexts, ideas which are equally appropriate in the heuristic setting. We mention these
ideas only in brief oulline, not because they lack importance, but because our purpose
here chiefly is to sketch broad considerations and to discuss possible variants before
proceeding to more refined strategies.
Step 1. The generation of surrogate constraints can be carried out by the "adaptive weighting" proposals of or the linear programming proposals of . For
DECISION SCIENCES
example, one can simply normalize and sum a current set of critical constraints, or one
can weigh the constraints with values of dual multipliers from the solution to an
associated linear program, using auxiliary considerations to reflect desired representations of sign conditions. Information for a trial solution may be incorporated into the
process, as in Step 2 or Step 4. Surrogate constraints also can be used as a guide to
gelling trial solutions in the first place, e.g., by solving a relaxed problem in which
surrogate constraints replace certain subsets of the original constraints.
Step 2. To obtain starting solutions, which need not be feasible, as already
noted, one may begin with all variables a) at values equal to their upper or lower
bounds, b) at values given by reference to an LP solution, c) at values perturbed from
another starting poin t, and d) at values set by prior search.
Step 3. The goodness measure may be stated in terms of ratios of objective
function coefficients to surrogate constraint coefficients, as suggested in the original
surrogate constraint proposals, and as implicitly used in the heuristic methods of [9J
and [tOJ. Weighted sums and products, and especially conditional measures which
make use of thresholds, may be used as alternatives or supplements to ratios.
Step 4. To guide the process of changing the values of the variables, surrogate
constraint ratios coupled with sign conditions give a natural indication of whether a
value should be raised or lowered. Indeed, an appropriate way to exploit signed ratios
has not yet emerged in the heuristic literature, and some confusion has reigned over
this area. The way to take advantage of these signed ratios has been developed in the
selection rules attending the introduction of surrogate constraints in [3J.
For changes more complex than one at a time changes, relative goodness measures can also be used. For example, if x' is the current trial solution and x" IS a
candidate for the next trial solution (a trial solution is one generated at any stage of
the procedure, and is not the same as the best solution found to date), then the
relative goodness measure may be a(cx'-cx") + P(s(x') - s(x"», where ex is the objective function to be minimized, s(x) .. 0 is the surrogate constraint, and a and pare
decision parameters. (Note that the determination of these parameters can be treated
as identifying an appropriate normalization for the surrogate constrain!.) One can also
use s(x')-s"(x") in place of s(x') -s(x"), where s(x).;;o is a surrogate constraint
determined relative to x', and s"(x) E; 0 is a surrogate constraint determined relative to
It is important to note that the foregoing discussion does not impose any necessary prohibition against moving across boundaries of feasibility (in' either direction),
though the parameters used in decision rules do change under different conditions.
From a purely descriptive point of view, the methods of Senju-Toyoda and
Kochenberger-McCarl-Wyman are readily identified as examples of the preceding
framework. Specifically, the weights giving rise to the sunogate constraints implicitly
relied on in these procedures are the reciprocal of the constant terms of the original
constraints (one of the forms of normalization suggested in ), where these constant
terms are kept updated to reflect the current assignment of values to the problem
HEURISTICS FOR INTEGER PROGRAMMING
variables. The effective gradients employed in these methods then turn out to be the
surrogate constraint ratios already discussed.
Advanced Implementations
W. now turn to augmenting heuristic considerations, each of which involves new
ideas that extend the strategic possibilities available to surrogate constraint frarne·
works and to other heuristic frameworks as well. We will discuss these under the
headings of "scatter search," "oscillating assignment," and "strongly detennined and
consistent variables."
Scatter Search. This method may b. used to generate starting solutions and trial
solutions. Instead of being limited to a single preliminary effort, as are most proce·
dures for generating such solutions, this approach uses a succession of coordinated
initializations. These are purposely (ie., nonrandornly) generated to take account of
characteristics in various parts of the solution space. In particular, scatter search builds
upon a kindred strategy suggested a number of years ago in (5) , by orienting its
explorations systematically relative to a set of reference points. Reference points may
consist, for example, of the extreme points of a simplex obtained by truncating the LP
basis cone, or of good solutions obtained by prior problem-solving efforts.
The approach begins by identifying a convex combination, or weighted center of
gravity, of the reference points. This central point, together with subsets of the initial
reference points, is then used to define new subregions. Thereupon, analogous central
points of the subregions are examined in a logical sequence (e.g., generally sweeping
from smaller objective function values to larger ones). Finally, these latter points are
rounded to obtain the desired starting solutions. (Rounding, for any problems other
than those with the simplest structures, should be either an iterative or a generalized
adjacency procedure, following (5), to accomodate interdependencies in the problem
variables_)
A variety of forms of scatter search is possible. The flavor of the approach can
be conveyed by one of its simpler versions, illustrated by Figure I. Each of the points
numbered I through 16 is the central point of an apparent subregion of the simplex A,
B, C_ Node 8 is the center of A, B, C itself. Here A, B, C mayor may not constitute
the orginal reference points (which could, for example, have been 6, 7, II or 4, 5,12,
13). The choice depends on the distribution of the original points relative to each
other and the feasible region generally. Thus, for example, it can be desirable to use
envelopes containing derived reference points, and to use weightings that bias central
points away from centroids. When scatter search is carried out relative to reference
points that lie on asingle line, then it is reduced to a form oflineacsearch, such as that
employed by Himer (71 and Jeroslow and Smith (8).
In general, to keep the effort within desirable limits for larger dimensions, the
points generated according to the example pattein may be examined in their indicated
numerical sequence (or in an alternative sequence dictated by the slope of the objective function contour) until a convenient cutoff point is reached. Or one may examine
a less refined collection of points.
DECISION SCIENCES
Because scatter search may be applied to reference points obtained from a
historical progression of solution attempts and may also be used to influence this
progression, the approach is conveniently suited to application with learning strategies_
Oscillating Assigllmelll. This procedure may be used to determine values to be
given to variables, as in Step 4 of the surrogate constraint framework. The procedure
may be viewed as a unification and extension of outside-in and inside-out procedures.
An outside-in approach, exemplified by the Senju-Toyoda method, begins at a distance
from the feasible region and changes the values of the variables (as a rule, unidirectionally) until feasibility is attained. By contrast, an inside-out procedure, exemplified by
the Kochenberger-McCarl-Wyman approach, starts from inside the feasible region
(after an initialization step, if required) and changes values of the variables until no
improving moves remain except moveS that would force an exit from this region.
The oscillating assignment procedure modifies the form of these procedures and
integrates the results into a single method. The first modification is to discard the
stopping rule of the outside-in approach upon entering the feasible region, in favor of
continuing to go deeper along a path dictated by the surrogate constraints. Thereupon,
HEURISTICS FOR INTEGER PROGRAMMING
the procedure reverses, working back toward the periphery, taking a trajectory de·
signed to obtain improving solutions.
The second modification complements the first. When proceeding from inside
the feasible region to a point where no improving move exists except one that violates
feasibility, the method is allowed to follow a path that goes outside the feasible region.
Then a return to the feasible region is initiated, again guided by the surrogate con·
straints. The oscillating assignment procedure, which integrates these two modifica·
tions by simple alternation, takes the depth of penetration beyond the feasibility
boundaries as a decision rule of the process. The process can also be applied relative to
implicit perturbations of the feasibility boundaries, thereby creating secondary oscilla·
tions in selected regions of the solution space. I
Strongly Determined and Consistent Variables. The notions of strongly deter·
mined and consistent variables provide a useful adjunct to the foregoing heuristic
ideas. A variable may be called strongly determined if its assigned value cannot be
changed (by the criteria available on a particular solution attempt), except by inducing
a disruptive effect on the objective function value or on the values of the other
variables.
To illustrate, consider the process of solving a 0·1 knapsack problem created by
the generation of a surrogate constraint. Some of the problem variables will typically
have highly profitable ratios and others will have highly unprofitable ratios. The
variables with ratios at the extremes qualify as being strongly determined, since they
are nearly compelled to assume particular values. The notion of strongly determined
variables is a continuum, not an either-or classification, ie., some variables are more
strongly determined than others.)
A consistent variable may be defined as one that frequently is strongly deter·
mined at a particular value or within a narrow range relative to the fluctuations of
other variables. Its consistency may be illustrated by reference to the approach first
proposed for creating a good surrogate constraint , which is one of the simplest and
most convenient approaches. In this approach, a succession of surrogate constraints are
generated which are candidates for selection. The procedure heuristically or algorith·
mically solves the knapsack problem for each candidate constraint. Weights of com·
ponent constraints violated by the knapsack solution are then increased, and weights
of component constraints satisfied by this solution are decreased, either strictly or in a
relative sense. The amount of increase or decrease for a given constraint depends on
the extent to which the constraint is violated or satisfied!
I The inside~ut and outside-in moves of the oscillating assignment approach can be replaced
by other pairs of complementary moves, as for example, constructive and destructive moves in the
context of scheduling and routing problems.
2 A variation involving multiple surrogate constraints solves a succession of surrogate
problems, each composed of a collection of candidate constraints. In this case, the amount of
change in the weights that yield a particular surrogate constraint also depends on the degree to
which the constraint is satisfied by the surrogate problem solution. An alternative is to divide the
problem constrain ts among the surrogate constraints, either adaptively or based on a priori identification of structure.
DECISION SGENCES
Good candidate constraints give rise to the most restrictive knapsack solutions;
that is, good constraints are those whose optimal knapsack solutions; that is, good
constraints are those whose optimal knapsack solutions yield the tightest objective
function bounds and come closest to satisfying the overall feasibility conditions for
the original problem. Consistent variables may be defIned as those most frequently
strongly determined, relative to the good solutions and constraints. Consistent vari·
abies can also be defIned in temlS of heuristic solutions to the original problem itself.
Uke the stlOngly determined concept, the consistent concept involves a continuum;
some variables may be more consistent than others.
There is clearly a healthy latitude in the possible operational speciftcation of
what makes up a consistent variable, as there is in most heuristic notions (e.g., in the
specifIcation of a strong surrogate constraint itself). Experimentation and context
determine whether a variable should be regarded as more consistent if it strongly
receives a particular value in 3 out of 5 cases, or if it less strongly receives a value in 4
out of 5 cases. Segregation of various types of solutions may accommodate the possi·
bility that a variable may be strongly determined at one value in some solutions, and
strongly determined at another value in other solutions. Consideration of clusterings
and interactions among subsets of variables provides logical ways of extending the
However, it is worth stressing that it is the use of strongly determined and
consistent variables which provides their raison d'elre. Use is based on three conjec·
tures: fIrst, that a variable which is highly consistent over a subset of good solutions is
very likely to receive its preferred value- or lie within its preferred range- in optimal
and near-optimal solutions; second, that once some variables are assigned specific
values or constrained to narrow ranges, other variables that previously seemed not
particularly consistent will now become a good deal more so; and, third, that the
operation of imposing narrow restrictions on selected variables will yield increasingly
reliable measures of the relative consistency of remaining variables, given the imposed
restrictions.
These conjectures, of course, refer to general tendencies and not to invariant
occurrences. They are inapplicable to single constraint plOblems, except by allowing
an augmentation of the constraint set with cuts. However, the underlying rationale
suggests the following heuristic plOcedure, which may be imbedded in the general
surrogate constraint framework:
I. Select one or more variables with the greatest relative consistencies, and
constrain these to their preferred values or ranges. Restrict attention to variables that
have not previously been given specifIc values or to variables whose ranges have not
been significantly restricted on a recent iteration.
2. Determine new relative consistencies for the variables on the basis of the
restriction of Step I.
3. Repeat the process until all variables have been constrained to specifIc values
(e.g., by progressive narrowing).
This procedure and the others previously described may be applied singly or in a
selected combination, within the surrogate constraint framework or outside it.
HEURISTICS FOR INTEGER PROGRAMMING
An IUustration
We now illustrate one of the several surrogate constraint heuristics proposed
above-the oscillating assignment heuristic- by applying it to the knapsack problem. In
this setting, the knapsack constraint may be viewed as a surrogate constraint. The
oscillating assignment heuristic forms the basis of the preliminary computational test·
ing cited in the final section.
The knapsack problem is written as:
and where aj' Cj > 0 for all j. The variables are indexed so that cj1aj;;' cK/aK for j';; K.
The version of oscillating assignment heuristic to be illustrated involves only a
simple first order oscillation.
To describe the procedure, the following definitions are used:
Indexes of variables that currently = I
Indexes of variables that currently = 0
The I Set except for the element most recently transferred
to this set.
The 0 Set except for the element most recently transferred
to this set.
The I Set and the 0 Set implicitly identify the current solution. We provide rules for
creating the next solution as follows:
Generating the Next Solution:
Currellt Solution Is Feasible: Transfer the smallest indexed member of the
O-M Set that satisfies lu';; aj .;; Uo to the I Set.
Curren I Solution Is Infeasible: Transfer the largest indexed member of the
I·M Set that satisfies LI .;; aj';; UI to the 0 Set.
In our particular illustration, we specify values for lu, Uo and LI, U I.
Lo, Uo: Set La = 0, Uo =~(i.e .• the parameters are nonrestrictive) unless
the transfer of the smallest indexed element of the O-M set to the I Set would create
an infeasible solution. Then replace Uo with the average of the aj for members of the
DECISION SCIENCES
L I, VJ' Set LI = 0, U I = = (I.e., the parameters are nonrestrictive) unless
the transfer of the largest indexed element of the I·M set to the 0 set would create a
feasible solution. Then replace LI with the average of the aj for members of the I·M
Finally, we specify the rule for generating the trial solution.
TriJIl Solution:
Current Solution Is Feasible: Transfer, temporarily, for the purpose of the
trial solution only, the member of the O·M Set to the I Set that has the largest Cj value
of those that yield a feasible transfer. If ties occur for the largest Cj, pick the one with
the smallest index. Repeat this process until the transfer of any further elements of the
O·M set would destroy feasibility.
Current Solution Is Infeasible: Transfer, temporarily, the member of the
I·M set to the 0 set that has the smallest Cj value of those that yield a feasible transfer.
If a single transfer doesn't yield a feasible solution, select the smallest Cj and repeat. If
ties occur for the smallest '1, pick the one with the largest index. Once feasibility is
attained by this rule, compfete the trial solution by the rule for the case where the
solution is currently feasible.
This oscillating assignment heuristic will be illustrated for the following prob·
Maximize ISxl + 21x2 + 16x3 + ISx4 + 14x5 + 12x6 + 8x7 + 9x8
17xI + 20x2 + 16x3 + 19x4 + 15x5 + 13x6 + 9x7 + IlxS~61
Table I shows the stages of the heuristic. The element most recently transferred into
the I Set from the O·M Set is marked with an asterisk to indicate tha t it is excluded
from the I·M Set. Since the 0 Set is the complement of the I Set, only the exceptional
member of the 0 set that does not also belong to the O·M set is indicated. For
simplicity, the process begins at the first point where LI and U I become restrictive
(i.e., after transferring indexes 1,2,3 from the 0 set to the I set), and continues for 5
iterations. The trial solution is represented by its associated I Set.
Current SoluHon
Infeasible
Infeasible
I ,3,5,6,S"
Exceptional Member
HEURISTICS FOR INTEGER PROGRAMMING
The solution whose I Set is 1,3,5,6, obtained on the 3rd and 4th iterations, is
optimal for this problem. The procedure can be carried on for a larger number of
iterations to an arbitrary or adaptively determined cutoff. The oscillating assignment
procedure has the ability to identify good solutions that differ considerably from the
solution obtained by the ordinary best ratio determinations. In applications where the
knapsack constraint truly represents a surrogate constraint, the form of the constraint
may vary from iteration to iteration, and trial solutions may, of course, be based on a
larger set of feasibility considerations.
Preliminary Computational Experience
A version of the oscillating assignment approach similar to the one illustrated
was preliminarily tested on a class of nonlinear generalized covering problems.
These covering problems occurred in a real world scheduling situation involving
300 to 500 integer variables and approximately 100 constraints with positive right.
hand sides. The objective is to minimize a sum of squared deviations of the selected
covering from specified target values.
Although the heuristics of do not apply to this problem directly, they
can be generalized via the surrogate constraint framework previously outlined to provide augmented methods that are capable of dealing with the present structure. These
augmented methods provide a basis for comparing an oscillating assignment version of
the general surrogate approach with procedures that are more nearly standard.
The first step of the testing was to produce augmented methods that utilized the
surrogate constraint weights impliCitly proposed in (i.e., as incorporated into
the effective gradients of these references). However, these natural extensions of gave rise to relatively poor solutions. Consequently, the augmented methods were
modified to utilize alternative surrogate conslraints. The criteria for generating lhese
conslraints were the same as those incorporated into the oscillating assignment ap·
proach, taking account of which component constraints were locally more restrictive
in terms of lhe nonlinear objective. This change succeeded in reducing lhe best objective function values obtained by these augmented methods by at least 30 percent, on
the average from around 130 to 85. By contrast, the oscillating assignment procedure
produced solutions whose objective function values were only half of these new values,
ranging on the average from 30-45. The solution times for this superior method were
all between 10 and 20 seconds on the CDC 6600, for a total of 50 trial runs with data
taken from the setting in which the problems arose.
Details of this computational study, which is presently being extended to additional classes of integer programming problems with accompanying refinement of the
internal decision rules, will be reported elsewhere. However, these preliminary results
strongly suggest the usefulness of appropriately designed surrogate constraint heuristics in practical settings.
DECISION SCIENCES