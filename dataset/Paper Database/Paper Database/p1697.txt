Active Learning for Reward Estimation in
Inverse Reinforcement Learning⋆
Manuel Lopes1, Francisco Melo2, and Luis Montesano3
1 Instituto de Sistemas e Robótica - Instituto Superior Técnico
Lisboa, Portugal
 
2 Carnegie Mellon University
Pittsburgh, PA, USA
 
3 Universidad de Zaragoza
Zaragoza, Spain
 
Abstract. Inverse reinforcement learning addresses the general problem of recovering a reward function from samples of a policy provided by
an expert/demonstrator. In this paper, we introduce active learning for
inverse reinforcement learning. We propose an algorithm that allows the
agent to query the demonstrator for samples at speciﬁc states, instead
of relying only on samples provided at “arbitrary” states. The purpose
of our algorithm is to estimate the reward function with similar accuracy as other methods from the literature while reducing the amount
of policy samples required from the expert. We also discuss the use of
our algorithm in higher dimensional problems, using both Monte Carlo
and gradient methods. We present illustrative results of our algorithm in
several simulated examples of diﬀerent complexities.
Introduction
We address the general problem of learning from demonstration. In this class of
problems, an agent is given a set of sample situation-action pairs by a demonstrator, from which it must recover the overall demonstrated behavior and/or
corresponding task description. In this paper we are particularly interested in
recovering the task description. In other words, the agent infers the underlying task that the demonstrator is trying to solve. From this task description,
the agent can then construct its own policy to solve the recovered task. One
interesting aspect of this approach is that it can accommodate for diﬀerences
between the demonstrator and the learner . The learner is not just replicating
the observed trajectory, but is inferring the “reason” behind such behavior.
⋆Work partially supported by the ICTI and FCT, under the CMU-Portugal Program,
the (POS_C) program that includes FEDER funds and the projects ptdc/eeaacr/70174/2006, (FP6-IST-004370) RobotCub and (FP7-231640) Handle.
W. Buntine et al. (Eds.): ECML PKDD 2009, Part II, LNAI 5782, pp. 31–46, 2009.
⃝Springer-Verlag Berlin Heidelberg 2009
M. Lopes, F. Melo, and L. Montesano
We formalize our problem using Markov decision processes (MDP). Within
this formalism, the demonstration consists of a set of state-action pairs and the
compact task representation takes the form of a reward function. Learning from
demonstration in MDPs has been explored in diﬀerent ways in the literature
 , and is usually known as inverse reinforcement learning. The seminal
paper gives the ﬁrst formal treatment of inverse reinforcement learning as
well as several algorithms to compute a reward description from a demonstration.
This problem has since been addressed in several other works .
The general IRL problem poses several interesting challenges to be dealt with.
On one hand, the process of searching for the “right” reward function typically
requires the underlying MDP to be solved multiple times, making this process
potentially computationally expensive in large problems. Furthermore, it is unreasonable to assume that the desired policy is completely speciﬁed, as this is
impractical in problems with more than a few dozen states, or that there is no
noise in the demonstration. Finally, the IRL problem is ill-posed, in the sense
that there is not a single reward function that renders a given policy optimal
and also there are usually multiple optimal policies for the same reward function . This means that, even if the desired policy is completely speciﬁed to
the learner, the problem remains ill-posed, as additional criteria are necessary
to disambiguate between multiple rewards yielding the same optimal policy.
Probabilistic sample-based approaches to the IRL problem partly address these issues, alleviating the requirement for complete and correct demonstration while restricting the set of possible solution rewards. These approaches
allow the solution to IRL to be “better conditioned” by increasing the size of
the demonstration and are robust to suboptimal actions in the demonstration1.
However, this will typically require a large amount of data (samples) for a good
estimate of the reward function to be recovered.
In this paper we propose the use of active learning to partly mitigate the
need for large amounts of data during learning. We adopt a Bayesian approach
to IRL, following . The idea behind active learning is to reduce the data
requirements of learning algorithms by actively selecting potentially informative
samples, in contrast with random sampling from a predeﬁned distribution .
In our case we use this idea to reduce the number of samples required from the
expert, and only ask the expert to demonstrate the desired behavior at the most
informative states. We compute the posterior distribution over possible reward
functions and use this information to actively select the states whose action the
expert should provide. Experimental results show that our approach generally
reduces the amount of data required to learn the reward function. Also, it is
more adequate in terms of interaction with the expert, as it requires the expert
to illustrate the desired behavior on fewer instances.
1 By considering demonstrations in which suboptimal actions can also be sampled,
the learner is provided with a ranking of actions instead of just an indication of
the optimal actions. Demonstrations are thus “more informative”, enforcing a more
constrained set of possible reward functions than in the general case where only
optimal policies are provided. This, in turn, simpliﬁes the search problem.
Active Learning for Reward Estimation in IRL
Background
In this section we review some background material on MDPs and inverse reinforcement learning.
Markov Decision Processes
A Markov decision process (MDP) is a tuple (X, A, P, r, γ), where X represents
the ﬁnite state-space, A the ﬁnite action space, P the transition probabilities, r
the reward function and is γ a discount factor. Pa(x, y) denotes the probability
of transitioning from state x to state y when action a is taken. The purpose of
the agent is to choose the action sequence {At} maximizing
γtr(Xt, At) | X0 = x
A policy is a mapping π : X × A → , where π(x, a) is the probability
of choosing action a ∈A in state x ∈X. Associated with any such policy
there is a value-function V π, V π(x) = Eπ [∞
t=0 γtr(Xt, At) | X0 = x] ,where
the expectation is now taken with respect to policy π. For any given MDP there
exists at least one policy π∗such that
V π∗(x) ≥V π(x).
Any such policy is an optimal policy for that MDP and the corresponding value
function is denoted by V ∗.
Given any policy π, the following recursion holds
V π(x) = rπ(x) + γ
Pπ(x, y)V π(y)
where Pπ(x, y) = 
a∈A π(x, a)Pa(x, y) and rπ(x) = 
a∈A π(x, a)r(x, a). For
the particular case of the optimal policy π∗, the above recursion becomes
V ∗(x) = max
⎣r(x, a) + γ
Pa(x, y)V ∗(y)
We also deﬁne the Q-function associated with a policy π as
Qπ(x, a) = r(x, a) + γ
Pa(x, y)V π(y)
Sometimes, it will be convenient to write the above expressions using vector
notation, leading to the expressions
Vπ = rπ + γPπVπ
a = ra + γPaVπ
a∈A[ra + γPaV∗]
a = ra + γPaV∗,
where ra and Qa denote the ath columns of matrices r and Q, respectively.
M. Lopes, F. Melo, and L. Montesano
Bayesian Inverse Reinforcement Learning
As seen above, an MDP describes a sequential decision making problem in which
an agent must choose its actions so as to maximize the total discounted reward.
In this sense, the reward function in an MDP encodes the task of the agent.
Inverse reinforcement learning (IRL) deals with the problem of recovering the
task representation (i.e., the reward function) given a demonstration of the task
to be performed (i.e., the desired policy). In this paper, similarly to , IRL is
cast as an inference problem, in which the agent is provided with a noisy sample
of the desired policy from which it must estimate a reward function explaining
the policy.
Our working assumption is that there is one reward function, rtarget, that
the demonstrator wants the agent to maximize. We denote the corresponding
optimal Q-function by Q∗
target. Given this reward function, the demonstrator will
choose an action a ∈A in state x ∈X with probability
P [Ademo = a | Xdemo = x, rtarget] =
target(x,a)
target(x,b) ,
where η is a non-negative constant.
We consider the demonstration as a sequence D of state-action pairs,
D = {(x1, a1), (x2, a2), . . . , (xn, an)} ,
From (2), for any given r-function, the likelihood of a pair (x, a) ∈X × A is
Lr(x, a) = P [(x, a) | r] =
b∈A eηQ∗r(x,b) ,
where we denoted by Q∗
r(x, a) the optimal Q-function associated with reward
r. The constant η can be seen as a conﬁdence parameter that translates the
conﬁdence of the agent on the demonstration. Note that, according to the above
likelihood model, evaluating the likelihood of a state-action pair given a reward
r requires the computation of Q∗
r. This can be done, for example, using dynamic
programming, which requires knowledge of the transition probabilities P. In the
remainder of the paper, we assume these transtion probabilities are known.
Assuming independence between the state-action pairs in the demonstration,
the likelihood of the demonstration D is
Lr(xi, ai).
Given a prior distribution P [r] over the space of possible reward functions, we
P [r | D] ∝Lr(D)P [r] .
The posterior distribution takes into account the demonstration and prior information and will provide the information used to actively select samples to
Active Learning for Reward Estimation in IRL
be included in the demonstration. From this distribution we can extract several
policies and rewards, for instance the mean policy πD:
πD(x, a) =
πr(x, a)P [r | D] dr,
or the maximum a posteriori
P [r | D] ,
We conclude this section by describing two methods used to address the IRL
problem within this Bayesian framework.
Two Methods for Bayesian IRL
So far, we cast the IRL problem as an inference problem. In the continuation
we describe two methods to address this inference problem, one that directly
approximates the maximum given in (4) and another that estimates the complete
posterior distribution P [r | D] .
Gradient-based IRL. The ﬁrst approach considers a uniform prior over the
space of possible rewards. This means that maximizing P [r | D] is equivalent
to maximizing the likelihood Lr(D). To this purpose, we implement a gradientascent algorithm on the space of rewards, taking advantage of the structure of
the underlying MDP. A similar approach has been adopted in .
We start by writing the log-likelihood of the demonstration given r:
log(Lr(xi, ai)).
We can now write
[∇rΛr(D)]xa =
Lr(xi, ai)
∂Lr(xi, ai)
To compute ∇rLr(x, a), we observe that
∇rLr(x, a) = dLr
dQ∗(x, a)dQ∗
dr (x, a).
Computing the derivative of Lr with respect to each component of Q∗yields
(x, a) = ηLr(x, a)
δyb(x, a) −Lr(y, b)δy(x)
with x, y ∈X and a, b ∈A. In the above expression, δu(v) denotes the Kronecker
delta function.
M. Lopes, F. Melo, and L. Montesano
To compute dQ∗
dr , we recall that Q∗
a = ra + γPa(I −γPπ∗)−1rπ∗. We also note
that, except for those points in reward space where the policy is not diﬀerentiable
with respect to r — corresponding to situations in which a small change in a
particular component of the reward function induces a change in a component
of the policy, — the policy remains unchanged under a small variation in the
reward function. We thus consider the approximation
drzu (x, a) ≈
∂rzu (x, a)
that ignores the dependence of the policy on r. The gradient estimate thus
obtained corresponds to the actual gradient except near those reward functions
on which the policy is not diﬀerentiable2. Considering the above approximation,
and letting T = I −γPπ∗, we have
(x, a) = δzu(x, a) + γ
Pa(x, y)T−1(y, z)π∗(z, u),
with x, y, z ∈X and a, u ∈A.
Putting everything together, the method essentially proceeds by considering
some initial estimate r0 and then use the gradient computation outlined above
to perform the update
rt+1 = rt + αt∇rΛrt(D)
MCMC IRL. The second approach, proposed in , uses the Monte-Carlo
Markov chain (MCMC) algorithm to approximate the posterior P [r | D]. The
MCMC algorithm thus generates a set of sample reward functions, {r1, . . . , rN},
distributed according to the target distribution, P [r | D]. Then,
P [r | D] ≈1
In the MCMC algorithm, these samples correspond to a sample trajectory of
a Markov chain designed so that its invariant distribution matches the target
distribution, P [r | D] .
We implement PolicyWalk, an MCMC algorithm described in . In this
particular variation, the reward space is discretized into a uniform grid and
the MCMC samples jump between neighboring nodes in this grid (see Fig. 1).
In other words, a new sample is obtained from the current sample to one of
the neighboring nodes in the grid. The new sample is accepted according to the
ratio between the posterior probabilities of the current and new samples. Reward
functions with higher (posterior) probability are thus selected more often than
those with lower probability, and the method is guaranteed to sample according
to the true posterior distribution.
A problem with this method is that, for large-dimensional problems, it generally requires a large number of sample rewards to ensure that the estimate
of P [r | D] is accurately represented by the sample set. We refer to the result
2 It is noteworthy that Rademacher’s theorem guarantees that the set of such reward
functions is null-measured. We refer to for further details.
Active Learning for Reward Estimation in IRL
Fig. 1. Representation of the PolicyWalk variation of MCMC. We refer to for
in , in which the number of samples N required to ensure an estimation error
bounded by ε must be O(M 2 log(1/ε)), where M is the dimension of the reward
function. This means that the number of samples grows (roughly) quadratically
with the dimension of the reward space. Furthermore, this result assumes that
∥r∥∞≤1/M. As noted in , this condition on r can be ensured by rescaling
the reward function, which does not aﬀect the optimal policy. It does, however,
aﬀect the likelihood function and, consequently, P [r | D]. Whenever such rescaling is not possible or convenient and the rewards can only be bounded by some
value C, the previous lower-bound on the sample size roughly deteriorates to
O(M 6e2M log(1/ε)), which quickly becomes prohibitive for large M.
Active Learning for Reward Estimation
In the previous section we discussed two possible (Bayesian) approaches to the
IRL problem. In these approaches, the agent is provided with a demonstration
D, consisting of pairs (xi, ai) of states and corresponding actions. From this
demonstration the agent must identify the underlying target task.
In the active learning setting, we now assume that, after some initial batch
of data D, the agent has the possibility to query the expert for the action at
particular states chosen by the agent. In this section we propose a criterion to
select such states and discuss how this can be used within the IRL framework.
We also discuss on-line versions of the methods in the previous section that are
able to cope with the successive additional data provided by the expert as a
result of the agent’s queries.
Active Sampling
The active learning strategies presented below rely on the uncertainty about the
parameter to be estimated to select new data points. As discussed in Section 2,
the parameter to be estimated in the IRL setting is the task description, i.e., the
reward function. Unfortunately, the relation between rewards and policies is not
one-to-one, making active learning in this setting more complex than in other
This is easily seen by thinking of a scenario in which all possible reward functions give rise to the same policy in a given state x (this can be the case if there is
M. Lopes, F. Melo, and L. Montesano
only one action available in state x). This means that a large uncertainty in the
reward function does not necessarily translate into uncertainty in terms of which
action to choose in state x. Therefore, in some scenarios it may not be possible
to completely disambiguate the reward function behind the demonstration.
In our setting, we have access to an estimate of the posterior distribution
over the space of possible reward functions, P [r | D]. From this distribution, the
agent should be able to choose a state and query the expert about the corresponding action in such a way that the additional sample is as useful/informative
as possible. Notice that the posterior distribution, P [r | D], does not diﬀerentiate between states (it is a distribution over functions) and, as such, a standard
variance criterion cannot be used directly.
We are interested in ﬁnding a criterion to choose the states to query the
demonstrator so as to recover the correct reward (or, at least, the optimal target
behavior) while requiring signiﬁcantly less data than if the agent was provided
with randomly chosen state-action pairs. To this purpose, we deﬁne the set
Rxa(p) as the set of reward functions r such that πr(x, a) = p. Now for each pair
(x, a) ∈X × A, the distribution P [r | D] in turn induces a distribution over the
possible values p for π(x, a). This distribution can be characterized by means of
the following density
¯μxa(p) = P [π(x, a) = p | D] = P [r ∈Rxa(p) | D] .
Using the above distribution, the agent can now query the demonstrator about
the correct action in states where the uncertainty on the policy is larger, i.e., in
states where ¯μxa exhibits larger “spread”.
One possibility is to rely on some measure of entropy associated with ¯μxa.
Given that ¯μxa corresponds to a continuous distribution, the appropriate concept is that of diﬀerential entropy. Unfortunately, as is well-known, diﬀerential
entropy as a measure of uncertainty does not exhibit the same appealing properties as its discrete counterpart. To tackle this diﬃculty, we simply consider a
partition of the interval I = into K subintervals Ik, with Ik = ( k
k = 0, . . . , K−1, and I0 = [0, 1/K]. We can now deﬁne a new discrete probability
distribution
μxa(k) = P [π(x, a) ∈Ik | D] =
¯μxa(p)dp,
k = 1, . . . , K.
The distribution μxa thus deﬁned is a discretized version of the density in (10),
for which we can compute the associated (Shannon) entropy, H(μxa). As such,
for each state x ∈X, we deﬁne the mean entropy as
H(μxa) = −1
μxa(k) log μxa(k)
and let the agent query the expert about the action to be taken at the state x∗
x∗= arg max
Active Learning for Reward Estimation in IRL
with ties broken arbitrarily. Given the estimate (9) for P [r | D], this yields
IIk(πi(x, a)),
where πi is the policy associated with the ith reward sampled in the MC method
and IIk is the indicator function for the set Ik. This ﬁnally yields
IIk(πi(x, a)) log
i IIk(πi(x, a))
It is worth mentioning that, in the context of IRL, there are two main sources
of uncertainty in recovering the reward function. One depends on the natural
ambiguity of the problem: for any particular policy, there are typically multiple
reward functions that solve the IRL problem. This type of ambiguity appear
even with perfect knowledge of the policy, and is therefore independent of the
particular process by which states are sampled. The other source of uncertainty
arises from the fact that the policy is not accurately speciﬁed in certain states.
This class of ambiguity can be addressed by sampling these states until the policy
is properly speciﬁed. Our entropy-based criterion does precisely this.
Active IRL
We conclude this section by describing how the active sampling strategy above
can be combined with the IRL methods in Section 2.3.
Algorithm 1. General active IRL algorithm
Require: Initial demo D
1: Estimate P [r | D] using general MC algorithm
2: for all x ∈X do
Compute ¯H(x)
4: end for
5: Query action for x∗= arg maxx ¯H(x)
6: Add new sample to D
7: Return to 1
The fundamental idea is simply to use the data from an initial demonstration
to compute a ﬁrst posterior P [r | D], use this distribution to query further states,
recompute P [r | D], and so on. This yields the general algorithm summarized
in Algorithm 1. We note that running MCMC and recompute P [r | D] at each
iteration is very time consuming, even more so in large-dimensional problems.
For eﬃciency, step 1 can take advantage of several optimizations of MCMC such
as sequential and hybrid monte-carlo.
In very large dimensional spaces, however, the MC-based approach becomes
computationally too expensive. We thus propose an approximation to the general Algorithm 1 that uses the gradient-based algorithm in Section 2.3. The idea
M. Lopes, F. Melo, and L. Montesano
Algorithm 2. Active gradient-based IRL algorithm
Require: Initial demo D
1: Compute r∗as in (4)
2: Estimate P [r | D] in a neighborhood of r∗
3: for all x ∈X do
Compute ¯H(x)
5: end for
6: Query action for x∗= arg maxx ¯H(x)
7: Add new sample to D
8: Return to 1
behind this method is to replace step 1 in Algorithm 1 by two steps, as seen in Algorithm 2. The algorithm thus proceeds by computing the maximum-likelihood
estimate r∗as described in Section 2.3. It then uses Monte-Carlo sampling to
approximate P [r | D] in a neighborhood Bε(r∗) of r∗and uses this estimate to
compute H(x) as in Algorithm 1. The principle behind this second approach
is that the policy πr∗should provide a reasonable approximation to the target policy. Therefore, the algorithm can focus on estimating P [r | D] only in a
neighborhood of r∗. As expected, this signiﬁcantly reduces the computational
requirements of the algorithm.
It is worth mentioning that the ﬁrst method, relying on standard MC sampling, eventually converges to the true posterior distribution as the number of
samples goes to inﬁnity. The second method ﬁrst reaches a local maximum of
the posterior and then only estimates the posterior around that point. If the
posterior is unimodal, it is expectable that the second method brings signiﬁcant
advantages in computational terms; however, if the posterior is multimodal, this
local approach might not be able properly represent the posterior distribution. In
any case, as discussed in Section 2.3, in high-dimensional problems, the MCMC
method requires a prohibitive amount of samples to provide accurate estimates,
rendering such approach inviable.
Simulations
We now illustrate the application of the proposed algorithms in several problems
of varying complexity.
Finding the Maximum of a Quadratic Function
We start by a simple problem of ﬁnding the maximum of a quadratic function.
Such a problem can be described by the MDP in Fig. 2, where each state corresponds to a discrete value between −1 and 1. The state-space thus consists of
21 states and 2 actions that we denote al and ar. Each action moves the agent
deterministically to the contiguous state in the corresponding direction (al to the
left, ar to the right). For simplicity, we consider a reward function parameterized
using a two-dimensional parameter vector θ, yielding
Active Learning for Reward Estimation in IRL
Fig. 2. Simple MDP where the agent must ﬁnd the maximum of a function
r(x) = θ1(x −θ2)2,
corresponding to a quadratic function with a (double) zero at θ2 and concavity
given by θ1. For the MDP thus deﬁned, the optimal policy either moves the
agent toward the state in which the maximum is attained (if θ1 < 0) or toward
one of the states ±1 (if θ1 > 0).
For our IRL problem, we consider the reward function, r(x) = −(x −0.15)2,
for which the agent should learn the parameter θ from a demonstration. The
initial demonstration consisted on the optimal actions for the extreme states:
D = {(−1.0, ar), (−0.9, ar), (−0.8, ar), (0.8, al), (0.9, al), (1.0, al)}
and immediately establishes that θ1 < 0.
Figure 3 presents the results obtained using Algorithm 1, with the conﬁdence
parameter in the likelyhood function set to η = 500 and N = 400 in the MCMC
estimation. The plots on the left represent the reward functions sampled in the
MCMC step of the algorithm and the plots on the right the corresponding average policy πD. In the depicted run, the queried states were x = 0 at iteration 1,
x = 0.3 at iteration 2, x = 0.1 at iteration 3, and x = 0.2 at iteration 4. It is
clear from the ﬁrst iteration that the initial demonstration only allows the agent
to place θ2 somewhere in the interval [−0.8, 0.8] (note the spread in the sampled
reward functions). Subsequent iterations show the distribution to concentrate
on the true value of θ2 (visible in the fact that the sampled rewards all exhibit
a peak around the true value). Also, the policy clearly converges to the optimal
policy in iteration 5 and the corresponding variance decreases to 0.
We conclude by noting that our algorithm is roughly implementing the bisection method, known to be an eﬃcient method to determine the maximum of a
function. This toy example provides a ﬁrst illustration of our active IRL algorithm at work and the evolution of the posterior distributions over r along the
iterations of the algorithm.
Puddle World
We now illustrate the application of our algorithm in a more complex problem. This problem is known in the reinforcement learning literature as the puddle world. The puddle world consists in a continuous-state MDP in which an
agentmust reach a goal region while avoiding a penalty region (the “puddle”), as
M. Lopes, F. Melo, and L. Montesano
Fig. 3. Sample iterations (1st, 2nd and 5th) of Algorithm 1 in the problem of Fig. 2.
On the left are samples obtained from P [r | D] and on the right the corresponding πD,
showing the mean the and variance of the optimal action for each state (1- move right,
0- move left).
depicted in Fig. 4. This example illustrates our active IRL algorithm at work in
a more complex problem that can still visualized.
The MDP has a continuous state-space, consisting of the unit square, and
a discrete action-space that includes the four actions N (north), S (south), E
(east), and W (west). Each action moves the agent 0.05 in the corresponding
direction. Since the MDP has a continuous state-space, exact solution methods
are not available. We adopt a batch approximate RL method known as ﬁtted
Active Learning for Reward Estimation in IRL
Penalty zone
Fig. 4. Representation of the puddle world and a possible value function
Q-iteration that essentially samples the underlying MDP and uses regression to
approximate the optimal Q-function . The fact that we must resort to function approximation implies that the exact optimal policy cannot be recovered
but only an approximation thereof. This will somewhat impact the ability of our
algorithm to properly estimate the posterior P [r | D].
In the puddle world, the reward function can be represented as
r(x) = rgoal exp
(x −μgoal)2/α
+ rpuddle exp
(x −μpuddle)2/α
where rgoal and rpuddle represent the reward and maximum penalty received in
the goal position and in the center of the puddle, respectively. The parameters
μgoal and μpuddle deﬁne the location of the goal and puddle, respectively. The
parameter α is ﬁxed a priori and roughly deﬁnes the width of both regions. For
our IRL problem, the agent should learn the parameters μgoal, μpuddle, rgoal, and
rpuddle from a demonstration.
Figure 5 presents two sample iterations of Algorithm 1. To solve the MDP we
ran ﬁtted Q-iteration with a batch of 3, 200 sample transitions. We ran MCMC
with N = 800. Notice that after the ﬁrst iteration (using the initial demonstration), the MCMC samples are already spread around the true parameters. At
each iteration, the algorithm is allowed to query the expert in 10 states. In the
depicted run, the algorithm queried states around the goal region — to pinpoint
the goal region — and around the puddle — to pinpoint the puddle region.
Random Scenarios
We now illustrate the application of our approach in random scenarios with different complexity. We also discuss the scalability of our algorithm and statistical
signiﬁcance of the results.
These general MDPs in this section consist of squared grid-worlds with varying
number of states. At each state, the agent has 4 actions available (N, S, E, W),
that moves the agent in the corresponding direction. We divide our results in
two classes, corresponding to parameterized rewards and general rewards.
M. Lopes, F. Melo, and L. Montesano
(a) Iteration 1.
(b) Iteration 3.
Fig. 5. Two sample iterations of Algorithm 1. The red stars (∗) represent the target
values for the parameters μgoal and μpuddle. The green and blue dots (·) represents the
sampled posterior distribution over possible value of these parameters. The circles (◦)
denotes the states included in the demonstration.
Parameterized Rewards. We start by considering a simple parameterization
of the reward function of the form δx∗(x). Therefore, the only parameter to be
learnt is the position of the goal state x∗in the grid.
We applied Algorithm 2 to a 15 × 15 grid-world. The estimation in step 2 of
the algorithm uses N = 15. At each iteration, the agent is allowed to query the
expert in 10 states. Figure 6(b) shows the error between the estimated policy
and the target policy as a function of the size of the demonstration, averaged
over 50 independent trials. Our approach clearly outperforms random sampling,
attaining the same error while requiring about 1/3 of the samples.
We conclude by noting that we chose to run Algorithm 2 in this scenarion since
(as discussed in Section 2.3, the MCMC component in Algorithm 1 does not scale
well with the number of states. Indeed, for a similar scenario with 100 states,
the MCMC-based algorithm required around 12, 000 MC samples, for each of
which an MDP must be solved. In that same 100-state scenarion, Algorithm 2
required around 50 gradient steps and then 20 MC samples to compute the local
approximation of the posterior, thus requiring a total of 70 MDPs to be solved.
Non-parameterized reward. We now consider a more general situation, in
which the reward function is a vector r in the |X|-dimensional unit square. In
this case, the reward value is merely a real-valued function r : X →[0; 1], and
the problem is signiﬁcantly more complex than in the previous case.
We applied Algorithm 2 to a 10 × 10 grid-world. The estimation in step 2 of
the algorithm uses N = 40. At each iteration, the agent is allowed to query the
expert in 2 states. Figure 6(a) shows the error between the estimated policy and
the target policy as a function of the size of the demonstration, averaged over 50
independent trials. In this case, it is clear that there is no apparent advantage
in using the active learning approach. Whatever small advantage there may be
is clearly outweighed by the added computational cost.
Active Learning for Reward Estimation in IRL
Fig. 6. Performance of Algorithm 2 comparing active sampling vs. random sampling
as a function of the demonstration size. (a) Results with parameterized rewards in a
15 × 15 grid-world. (b) Results with general (non-parameterized) rewards in a 10 × 10
grid-world.
These results illustrate, in a sense, some of the issues already discussed in
Section 3.1. When considering a non-parameterized form for the reward function
and a prior over possible rewards that is state-wise independent, there is not
enough structure in the problem to generalize the observed policy from observed
states to non-observed states. In fact, the space of general (non-parameterized)
reward functions has enough degrees of freedom to yield any possible policy. In
this case, any sampling criterion will, at best, provide only a mild advantage
over uniform sampling. On the other hand, when using parameterized rewards
or a prior that weights positively ties between the reward in diﬀerent states
(e.g., an Ising prior ), the policy in some states restricts the possible policies
on other states. In this case, sampling certain states can certainly contribute to
disambiguate the policy in other states, bringing signiﬁcant advantages to an
active sampling approach over a uniform sampling approach.
Conclusions
In this paper we introduced the ﬁrst active learning algorithm explicitly designed
to estimate rewards from a noisy and sampled demonstration of an unknown
optimal policy. We used a full Bayesian approach and estimate the posterior
probability of each action in each state, given the demonstration. By measuring
the state-wise entropy in this distribution, the algorithm is able to select the potentially most informative state to be queried to the expert. This is particularly
important when the cost of providing a demonstration is high.
As discussed in Section 4, our results indicate that the eﬀectiveness of active
learning in the described IRL setting may greatly depend on the prior knowledge about the reward function or the policy. In particular, when considering
M. Lopes, F. Melo, and L. Montesano
parameterized policies or priors that introduce relations (in terms of rewards)
between diﬀerent states, our approach seems to lead to encouraging results. In
the general (non-parameterized) case, or when the prior “decorrelates” the reward in diﬀerent states, we do not expect active learning to bring a signiﬁcant
advantage. We are currently conducting further experiments to gain a clearer
understanding on this particular issue.
We conclude by noting that active learning has been widely applied to numerous settings distinct from IRL. In some of these settings there are even theoretical
results that state the improvements or lack thereof arising from considering active sampling instead of random sampling . To the extent of our knowledge,
ours is the ﬁrst paper in which active learning is applied within the context
of IRL. As such, many new avenues of research naturally appear. In particular,
even if the ambiguities inherent to IRL problems make it somewhat distinct from
other settings, we believe that it should be possible (at least in some problems)
to theoretically asses the usefulness of active learning in IRL.