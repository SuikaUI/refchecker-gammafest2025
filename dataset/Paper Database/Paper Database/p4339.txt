Compositional Visual Generation with
Composable Diffusion Models
Nan Liu1⋆, Shuang Li2⋆, Yilun Du2⋆
Antonio Torralba2, and Joshua B. Tenenbaum2
1 University of Illinois Urbana-Champaign
2 Massachusetts Institute of Technology
 , {lishuang,yilundu,torralba,jbt}@mit.edu
Abstract. Large text-guided diffusion models, such as DALL-E 2, are
able to generate stunning photorealistic images given natural language
descriptions. While such models are highly flexible, they struggle to understand the composition of certain concepts, such as confusing the attributes of different objects or relations between objects. In this paper,
we propose an alternative structured approach for compositional generation using diffusion models. An image is generated by composing a set
of diffusion models, with each of them modeling a certain component
of the image. To do this, we interpret diffusion models as energy-based
models in which the data distributions defined by the energy functions
may be explicitly combined. The proposed method can generate scenes
at test time that are substantially more complex than those seen in
training, composing sentence descriptions, object relations, human facial
attributes, and even generalizing to new combinations that are rarely
seen in the real world. We further illustrate how our approach may be
used to compose pre-trained text-guided diffusion models and generate
photorealistic images containing all the details described in the input
descriptions, including the binding of certain object attributes that have
been shown difficult for DALL-E 2. These results point to the effectiveness of the proposed method in promoting structured generalization for
visual generation.
Keywords: Compositionality, Diffusion Models, Energy-based Models,
Visual Generation
Introduction
Our understanding of the world is highly compositional in nature. We are able to
rapidly understand new objects from their components, or compose words into
complex sentences to describe the world states we encounter . We are able
⋆indicates equal contribution.
Correspondence to: Shuang Li < >, Yilun Du < >
Webpage: 
 
Nan Liu*, Shuang Li*, Yilun Du* (equal contribution)
(b) Composing Language Descriptions (Composed GLIDE)
“A red car parked
in a desert” AND
“hills behind the
car” AND “Aurora
in the sky”
“The sun setting in
a horizon” AND “A
house next to a
pond” AND “Hills
in the background”
“A house with snow
on the roof” AND
“The house behind a
tree” AND “A car in
front of a tree”
(NOT Female) AND
Smiling AND
(NOT Glasses)
(e) Composing Facial Attributes
Blonde hair AND
(NOT glasses)
“A Ferris wheel” AND
“A lake right next to
the Ferris wheel”
AND “Buildings next
to the lake”
“A cloudy blue sky”
AND “A mountain in
the horizon” AND
“Cherry Blossoms in
front of the mountain”
“Palm trees on both
sides of the street”
AND “Pink Sunset in
the horizon” AND “A
car moving away”
(c) Composing Objects
Obj1 (0.1, 0.5) AND
Obj2 (0.5, 0.3) AND
Obj3 (0.5, 0.65) AND
Obj4 (0.7, 0.5)
Obj1 (0.1, 0.65) AND
Obj2 (0.3, 0.55) AND
Obj3 (0.5, 0.45) AND
Obj4 (0.7, 0.3)
(d) Composing Object Relations
“A large purple metal
cube to the left of a
large gray rubber
cube” AND “A large
purple metal cube to
the right of a large
yellow rubber sphere”
“A large yellow rubber
cylinder to the right
of a small gray metal
cube” AND “A large
yellow rubber cylinder
below a large red
rubber cube”
(a) Composing Language Descriptions (Composed Stable Diffusion)
“A photo of cherry
blossom trees” AND
“Sun dog” AND
“Green grass”
“A church” AND
“Lightning in the
background” AND
“A beautiful pink sky”
“A stone castle
surrounded by lakes
and trees,” AND
“Black and white”
“A mystical tree ”
AND “A dark
magical pond”
AND “Dark”
“A stone castle
surrounded by lakes
and trees,” AND (NOT
“Black and white”)
“A mystical tree ”
AND “A dark
magical pond”
AND (NOT “Dark”)
Fig. 1: Our method allows compositional visual generation across a variety of domains,
such as language descriptions, objects, object relations, and human attributes.
to make “infinite use of finite means” , i.e., repeatedly reuse and recombine
concepts we have acquired in a potentially infinite manner. We are interested in
constructing machine learning systems to have such compositional capabilities,
particularly in the context of generative modeling.
Existing text-conditioned diffusion models such as DALL-E 2 have recently made remarkable strides towards compositional generation, and are capable of generating photorealistic images given textual descriptions. However, such
systems are not fully compositional and generate incorrect images when given
more complex descriptions . An underlying difficulty may be that such
models encode text descriptions as fixed-size latent vectors. However, as textual
descriptions become more complex, more information needs to be squeezed into
the fixed-size vector. Thus it is impossible to encode arbitrarily complex textual
descriptions.
In this work, we propose to factorize the compositional generation problem,
using different diffusion models to capture different subsets of a compositional
specification. These diffusion models are then explicitly composed together to
Composable Diffusion
generate an image. By explicitly factorizing the compositional generative modeling problem, our method can generalize to significantly more complex combinations that are unseen during training.
Such an explicit form of compositionality has been explored before under
the context of Energy-Based Models (EBMs) . However, directly training
EBMs has been proved to be unstable and hard to scale. We show that diffusion models can be interpreted as implicitly parameterized EBMs, which can be
further composed for image generation, significantly improving training stability
and image quality.
Our proposed method enables zero-shot compositional generation across different domains as shown in Figure 1. First, we illustrate how our approach may
be applied to large pre-trained diffusion models, such as Stable Diffusion 
and GLIDE , to compose multiple text descriptions. Next, we illustrate how
our approach can be applied to compose objects and object relations, enabling
zero-shot generalization to a larger number of objects. Finally, we illustrate how
our framework can compose different facial attributes to generate human faces.
Contributions: In this paper, we introduce an approach towards compositional visual generation using diffusion models. First, we show that diffusion
models can be composed by interpreting them as energy-based models, and
drawing on this connection, we demonstrate how to compose diffusion models together. Second, we propose two compositional operators, Conjunction and
Negation, on top of diffusion models that allow us to compose concepts in different domains during inference without any additional training. We show that
the proposed method enables effective zero-shot combinatorial generalization,
i.e. generating images with more complicated compositions of concepts. Finally,
we evaluate our method on composing language descriptions, objects, object
relations, and human facial attributes. Our method can generate high-quality
images containing all the concepts and outperforms baselines by a large margin.
For example, the accuracy of our method is 24.02% higher than the best baseline
for composing three objects in specified positions on the CLEVR dataset.
Related Work
Controllable Image Generation. Our work is related to existing work on
controllable image generation. One type of approach towards controllable image
generation specifies the underlying content of an image utilizing text through
GANs , VQ-VAEs , or diffusion models . An alternative type of
approach towards controllable image generation manipulates the underlying attributes in an image . In contrast, we are interested in compositionally
controlling the underlying content of an image at test time, generating images
that exhibit compositions of multiple types of image content. Thus, most relevant to our work, existing work has utilized EBMs to compose different factors
describing a scene . We illustrate how we may implement such probabilistic composition on diffusion models, achieving better performance.
Nan Liu*, Shuang Li*, Yilun Du* (equal contribution)
Diffusion Models. Diffusion models have emerged as a promising class of generative models that formulates the data-generating process as an iterative denoising procedure . The denoising procedure can be seen as parameterizing the
gradients of the data distribution , which is similar to EBMs .
Diffusion models have recently shown great promise in image generation tasks
 , enabling effective image editing , text conditioning , and image inpainting . The iterative, gradient-based sampling of diffusion models
enable us to compose multiple factors during inference. While diffusion models
have been developed for image generation , they have further proven successful in the generation of waveforms , 3D shapes , decision making ,
and text , suggesting that our proposed composition operators may further
be applied in such domains.
Background
Denoising Diffusion Models
Denoising Diffusion Probabilistic Models (DDPMs) are a class of generative
models where generation is modeled as a denoising process. Starting from a
sampled noise, the diffusion model performs T denoising steps until a sharp image
is formed. In particular, the denoising process produces a series of intermediate
images with decreasing levels of noise, denoted as xT , xT −1, ..., x0, where xT is
sampled from a Gaussian prior and x0 is the final output image.
DDPMs construct a forward diffusion process by gradually adding Gaussian
noise to the ground truth image. A diffusion model then learns to revert this
noise corruption process. Both the forward processes q(xt|xt−1) and the reverse
process pθ(xt−1|xt) are modeled as the products of Markov transition probabilities:
\lab e l {eq:
rize} \begi
n {al ign e d} q (
{0:T}) &= q(\vx _0)\prod _{t=1}^{T} q(\vx _t | \vx _{t-1}), \;\;\;\;\; p_\theta (\vx _{T:0}) = p(\vx _T)\prod _{t=T}^{1} p_\theta (\vx _{t-1} | \vx _t), \end {aligned}
where q(x0) is the real data distribution and p(xT ) is a standard Gaussian prior.
A generative process pθ(xt−1|xt) is trained to generate realistic images by
approximating the reverse process through variational inference. Each step of the
generative process is a Gaussian distribution N with a learned mean µθ(xt, t)
and covariance matrix σ2
t I, where I is the identity matrix.
\begin {a li g
ed} p_ \th et
x _ {t-1}| \
t ) &:= \N \bigl (\mu _\theta (\vx _t, t), \sigma _t^2 I \bigl ) = \N \bigl (\vx _{t} - \epsilon _\theta (\vx _t, t \bigl ), \sigma _t^2 I). \end {aligned}
The mean µθ(xt, t) is represented by a perturbation ϵθ(xt, t) to a noisy image
xt. The goal is to remove the noise gradually by predicting a less noisy image at
timestep xt−1 given a noisy image xt. To generate real images, we sample xt−1
from t = T to t = 1 using the parameterized marginal distribution pθ(xt−1|xt),
with an individual step corresponding to:
\v x _ { t-1} = \ v x _t -
\epsilon _\theta (\vx _t, t) + \N (0, \sigma _t^2 I). \label {eqn:diffusion_langevin}
The generated images become more realistic over multiple iterations.
Composable Diffusion
Sentence 1 (!!):
“Road leading into
the mountains”
Sentence n (!"):
“Yellow trees on the
side of the road”
Score "#($$, &| !!)
Generated Image ('')
Conjunction
Score "#($$, &| !")
Image at iteration t ('&)
Fig. 2: Compositional generation. Our method can compose multiple concepts during inference and generate images containing all the concepts without further training.
We first send an image from iteration t and each of the concepts to the diffusion model
to generate a set of scores {ϵθ(xt, t|c1), . . . , ϵθ(xt, t|cn)}. We then compose different
concepts using the proposed compositional operators, such as conjunction, to denoise
the generated image. The final image is obtained after T iterations.
Energy Based Models
Energy-Based Models (EBMs) are a class of generative models where
the data distribution is modeled using an unnormalized probability density.
Given an image x ∈RD, the probability density of image x is defined as:
\la b el {eq:ebm} p_{\theta }(\vx ) \propto e^{-E_{\theta }(\vx )},
where the energy function Eθ(x) : RD →R is a learnable neural network.
A gradient based MCMC procedure, Langevin dynamics , is then used to
sample from the unnormalized probability distribution to iteratively refine the
generated image x:
{ eq:langevi n } \v x
_ t = \vx _{t - 1} - \frac {\lambda }{2}\nabla _{\vx } E_{\theta }(\vx _{t-1}) + \mathcal {N}(0, \sigma _t^2 I).
The sampling procedure used by diffusion models in Equation (3) is functionally
similar to the sampling procedure used by EBMs in Equation (5). In both settings, images are iteratively refined starting from a Gaussian noise, with a small
amount of additional noise added at each iterative step.
Our approach
In this section, we first introduce how we interpret diffusion models as energybased models in section 4.1 and then introduce how we compose diffusion models
for visual generation in section 4.2.
Diffusion Models as Energy Based Models
The sampling procedure of diffusion models in Equation (3) and EBMs in Equation (5) are functionally similar. At a timestep t, in diffusion models, images are
updated using a learned denoising network ϵθ(xt, t) while in EBMs, images are
updated using the gradient of the energy function ∇xEθ(xt) ∝∇x log pθ(xt).
Nan Liu*, Shuang Li*, Yilun Du* (equal contribution)
The denoising network ϵθ(xt, t) is trained to predict the underlying score of the
data distribution when the number of diffusion steps increases to infinity. Similarly, an EBM is trained so that ∇xEθ(xt) corresponds to the score
of the data distribution as well. In this sense, ϵθ(xt, t) and ∇xEθ(xt) are functionally the same, and the underlying sampling procedure in Equation (3) and
Equation (5) are equivalent. We may view a trained diffusion model ϵθ(xt, t) as
an implicitly parameterized EBM. Such parameterization enables us to apply
previous techniques for composing EBMs to diffusion models.
Composing EBMs. Previous EBMs have shown good performance on
compositional visual generation. Given n independent EBMs, E1
θ(x), · · · , En
the functional form of EBMs in Equation (4) enables us to compose multiple
separate EBMs together to obtain a new EBM. The composed distribution can
be represented as:
\label {e q :1
} p_ { \ t ex
t {c o mp os
vx ) \propto p_{\theta }^1(\vx ) \cdots p_{\theta }^n(\vx ) \propto e^{- \sum _{i=1}^{n} E_{\theta }^i(\vx ) },
θ(x) is the probability density of image x (Equation (4)). Langevin
dynamics is then used to iteratively refine the generated image x .
x _t = \vx _{t - 1} - \frac {\lambda }{2}\nabla _{\vx } \left (\sum _{i=1}^n E_{\theta }^i(\vx _{t-1}) \right ) + \mathcal {N}(0, \sigma _t^2 I).
Composing Diffusion Models. By leveraging the interpretation that diffusion
models are functionally similar to EBMs, we may compose diffusion models in a
similar way. The generative process and the score function of a diffusion model
can be represented as pi
θ(xt−1|xt) and ϵi
θ(xt, t), respectively. If we treat the
score functions in diffusion models as the learned gradient of energy functions
in EBMs, the score function of the composed diffusion model can be written
θ(xt, t). Thus the generative process of composing multiple diffusion
models becomes:
\label {eqn:dif f u
e} p_ {\t ex
compose}}( \vx _{t-1}|\vx _t) = \N \left (\vx _t - \sum _{i=1}^n \epsilon ^i_\theta (\vx _t, t), \sigma _t^2 I \right ).
A complication of parameterizing a gradient field of EBM ∇xEθ(xt) with a
learned score function ϵθ(xt, t) is that the gradient field may not be conservative, and thus does not correspond to a valid probability density. However, as
discussed in , explicitly parameterizing the learned function ϵθ(xt, t) as the
gradient of EBM achieves similar performance as the non-conservative parameterization of diffusion models, suggesting this is not problematic.
Compositional Generation through Diffusion Models
Next, we discuss how we compose diffusion models for image generation. We
aim to generate images conditioned on a set of concepts {c1, c2, . . . , cn}. To do
Composable Diffusion
this, we represent each concept ci using a diffusion model, which can be composed to generate images. Inspired by EBMs , we define two compositional
operators, conjunction (AND) and negation (NOT), to compose diffusion
models. We learn a set of diffusion models representing the conditional probability distribution p(x|ci) given concept ci and an unconditional probability
distribution p(x).
Concept Conjunction (AND). We aim to generate images containing certain
attributes. Following , the conditional probability can be factorized as:
p(\vx | \ v c _ 1 , \l dot s , \vc _n)
to p(\vx , \vc _1, \ldots , \vc _n) = p(\vx ) \prod _{i=1}^n p(\vc _i|\vx ). \label {eq:conj}
Here we assume the concepts are conditionally independent given x. We can
represent p(ci|x) using the combination of a conditional distribution p(x|ci)
and an unconditional distribution p(x), with both of them are parameterized
as diffusion models p(ci|x) ∝p(x|ci)
p(x) . The expression of p(ci|x) corresponds to
the implicit classifier that represents the likelihood of x exhibiting concept ci.
Substituting p(ci|x) in Equation 9, we can rewrite Equation 9 as:
p(\vx | \ v c _ 1 , \l
n) \ propto p(\vx ) \prod _{i=1}^n \frac {p(\vx |\vc _i)}{p(\vx )}. \label {eq:conj_works}
We sample from this resultant distribution using Equation (8) with the composed
score function ˆϵ(xt, t):
\hat { \ epsilo n }
t) = \epsi l on _\t he
a (\vx _t, t) + \sum _{i=1}^n w_i \bigl (\epsilon _\theta (\vx _t, t | \vc _i) - \epsilon _{\theta }(\vx _t, t)\bigl ), \vspace {-5pt}
where wi is a hyperparameter corresponding to the temperature scaling on concept ci. We can generate images with the composed concepts using the following
generative process:
p_{compose}( \v x _
t- 1 }|\vx _t) :
N \bigl (\vx _t - \hat {\epsilon }(\vx _t, t), \sigma _t^2 I \bigl ). \label {eqn:diffusion_mod}
In the setting in which image generation is conditioned on a single concept, the
above sampling procedure reduces to the classifier-free guidance .
Concept Negation (NOT). In concept negation, we aim to generate realistic
images with the absence of a certain factor ˜cj. However, the negation of a concept
can be ill-defined. For example, the negation of “dark” can be “bright” or random
noises. Thus we generate images conditioned other concepts as well to make the
generated images look real. Following , concept negation can be represented as
the composed probability distribution p(x|not ˜cj, ci). Similarly, we refactorize
the joint probability distribution as:
p(\vx |\t ext {not }\ tild e { \ vc } _j, \vc
_i) \propto p(\vx , \text {not }\tilde {\vc }_j, \vc _i) \propto p(\vx ) \frac { p(\vc _i|\vx )}{p(\tilde {\vc }_j|\vx )}. \label {eqn:negation_first}
Nan Liu*, Shuang Li*, Yilun Du* (equal contribution)
Algorithm 1 Code for Composing Diffusion Models
1: Require Diffusion model ϵθ(xt, t|c), scales wi and w, covariance matrix σ2
2: // Code for conjunction
3: Initialize sample xT ∼N(0, I)
4: for t = T, . . . , 1 do
ϵi ←ϵθ(xt, t|ci)
// compute conditional scores for each concept ci
ϵ ←ϵθ(xt, t)
// compute unconditional score
i=1 wi(ϵi−ϵ)
// sampling
8: end for
10: // Code for negation
11: Initialize sample xT ∼N(0, I)
12: for t = T, . . . , 1 do
˜ϵj ←ϵθ(xt, t|˜cj)
// compute conditional score for the negated concept ˜cj
ϵi ←ϵθ(xt, t|ci)
// compute conditional score for concept ci
ϵ ←ϵθ(xt, t)
// compute unconditional score
 ϵ + w(ϵi −˜ϵj)
// sampling
17: end for
Using the factorization p(ci|x) ∝p(x|ci)
p(x) , we can rewrite Equation (13) as:
p(\vx |\t ext {not }\tild
e {\vc }_j, \vc _i) \propto p(\vx )\frac {p(\vx | \vc _i)}{p(\vx | \tilde {\vc }_j)}
We may construct the composed score funcion ˆϵ(x, t) as:
\sma ll \hat { \e p s
lon }( \vx _ t , t) = \epsi
on _\theta (\vx _t, t) + w \bigl (\epsilon _\theta (\vx _t, t | \vc _i) - \epsilon _\theta (\vx _t, t | \tilde {\vc }_j)\bigl ).
where w is the hyperparameter that controls the strength of the negation. We
can generate samples using this composed score function and Equation 12.
Algorithm 1 provides the pseudo-code for composing diffusion models using
concept conjunction and negation. Our method can compose pre-trained diffusion models during inference without any additional training. Please see the full
derivation details for both operators in appendix F.
Experiment Setup
CLEVR. CLEVR is a synthetic dataset containing objects with different
shapes, colors, and sizes. The training set consists of 30,000 images at 128 × 128
resolution. Each image contains 1 ∼5 objects and a 2D coordinate (x, y) label
indicating that the image contains an object at (x, y). In our experiments, the
2D coordinate label is the coordinate of one object in the image.
Relational CLEVR. Relational CLEVR contains relational descriptions
between objects in the image, such as “a red cube to the left of a blue cylinder”.
Composable Diffusion
The training dataset contains 50, 000 images at 128×128 resolution. Each training image contains 1 ∼5 objects and one label describing a relation between
two objects. If there is only one object in the image, the second object and their
relation in the relational description are both nulls.
FFHQ. FFHQ is a real-world human face dataset. The original FFHQ
dataset consists of 70,000 human face images without labels. annotates three
binary attributes, including smile, gender, and glasses, for the images using pretrained classifiers. In total, there are 51,067 images labeled by the classifiers.
Evaluation Metrics
Binary classification accuracy. During testing, we evaluate the performance
of the proposed method and baselines on three different settings. The first
test setting, 1 Component, generates images conditioned on a single concept (matching the training distribution). The second and third test settings,
2 Components and 3 Components, generate images by composing two and
three concepts, respectively, using the conjunction and negation operators. They
are used to evaluate the models’ generalization ability to new combinations.
For each task, we use the training data (real images) to train a binary classifier that takes an image and a concept, e.g. ‘smiling’, as input, and predicts
whether the image contains or represents the concept. We then apply this classifier to a generated image, checking whether it faithfully captures each of the
concepts. In each test setting, each method generates 5, 000 images for evaluation. The accuracy of the method is the percentage of generated images capturing
all the concepts (See appendix B).
Fr´echet Inception Distance (FID) is a commonly used metric for evaluating
the quality of generated images. It uses a pre-trained inception model to
extract features for the generated images and real images, and measures their
feature similarity. Specifically, we use Clean-FID to evaluate the generated
images. FID is usually computed on 50, 000 generated images, but we use 5, 000
images in our experiments.
Experiments
We compare the proposed method and baselines (section 6.1) on compositional
generation in different domains. We show results of composing natural language
descriptions (section 6.2), objects (section 6.3), object relational descriptions
(section 6.4), and human facial attributes (appendix A). Results analysis are
shown in section 6.5.
We compare our method with baselines for compositional visual generation.
StyleGAN2-ADA is the state-of-the-art GAN method for both unconditional and conditional image generation.
Nan Liu*, Shuang Li*, Yilun Du* (equal contribution)
“A blue bird on a
tree” AND “A red
car behind the
tree” AND “A
green forest in the
background”
“A green tree swaying
in the wind” AND “A
red brick house
located behind a tree”
AND “A healthy lawn
in front of the house”
“A pink sky in
the horizon”
AND “A sailboat
at the sea” AND
“Overwater
bungalows”
“A starry night
sky” AND “A
polar bear in a
“A white church
sitting on a hill”
AND “Aurora in
Composed GLIDE (Ours)
“A pink sky” AND
“A blue mountain
in the horizon”
AND “Cherry
Blossoms in front
of the mountain”
Fig. 3: Composing Language Descriptions. We develop Composed GLIDE (Ours),
a version of GLIDE that utilizes our compositional operators to combine textual
descriptions, without further training. We compare it to the original GLIDE, which directly encodes the descriptions as a single long sentence. Our approach more accurately
captures text details, such as the “overwater bungalows” in the third example.
StyleGAN2 is one of the state-of-the-art GAN methods for unconditional
image generation. To enable compositional image generation, we optimize the
latent code z by decreasing the binary classification loss of the generated image
and the given label. We use the resultant latent code to generate images.
LACE uses pre-trained classifiers to generate energy scores in the latent
space of the pre-trained StyleGAN2 model. To enable compositional image synthesis, LACE uses compositional operators .
GLIDE is a recently released text-conditioned diffusion model for image
generation. For composing language descriptions, we use the pre-trained GLIDE
released by OpenAI. For the rest tasks, we use the GLIDE code and train a
model on each task.
Energy-based models (EBM) is the first paper using EBMs for compositional visual generation. They propose three compositional operators for composing different concepts. Our work is inspired by , but we compose diffusion
models and achieve better results.
Composing Language Descriptions
Our approach can effectively compose natural language descriptions. We first
show the image generation results of the pre-trained diffusion model, GLIDE
 , in Figure 3. We develop Composed GLIDE, a version of GLIDE that utilizes
our compositional operators to combine textual descriptions, without further
training. We compare this model to the original GLIDE model.
In Figure 3, GLIDE takes a single long sentence as input, for example, “A
pink sky in the horizon, a sailboat at the sea, and overwater bungalows”. In
Composable Diffusion
Obj1 (0.1, 0.5) AND Obj2 (0.3, 0.5) AND Obj3 (0.5, 0.5)
AND Obj4 (0.7, 0.5) AND Obj5 (0.9, 0.5)
Obj1 (0.2, 0.65) AND Obj2 (0.2, 0.4) AND Obj3 (0.5, 0.5)
AND Obj4 (0.7, 0.4) AND Obj5 (0.7, 0.65)
Obj1 (0.2, 0.65) AND Obj2 (0.3, 0.5) AND Obj3 (0.4, 0.4) AND Obj4 (0.5,
0.3) AND Obj5 (0.6, 0.4) AND Obj6 (0.7, 0.5) AND Obj7 (0.8, 0.65)
Obj1 (0.1, 0.5) AND Obj2 (0.3, 0.5) AND Obj3 (0.5, 0.5) AND Obj4 (0.7,
0.5) AND Obj5 (0.9, 0.5) AND Obj6 (0.5, 0.65) AND Obj7 (0.5, 0.3)
Obj1Obj2Obj3Obj4
Fig. 4: Composing Objects. Our method can compose multiple objects while baseline methods either miss objects or generate objects at wrong positions.
Table 1: Quantitative evaluation of 128 × 128 image generation results on CLEVR.
The binary classification accuracy (Acc) and FID scores are reported. Our method
outperforms baselines on all three test settings.
1 Component
2 Components
3 Components
StyleGAN2-ADA 
StyleGAN2 
GLIDE 
contrast, Composed GLIDE composes several short sentences using the concept
conjunction operator, e.g. “A pink sky in the horizon” AND “A sailboat at the
sea” AND “Overwater bungalows”. While both GLIDE and Composed GLIDE
can generate reasonable images containing objects described in the text prompt,
our approach with the compositional operators can more accurately capture
text details, such as the presence of “a polar bear” in the first example and the
“overwater bungalows” in the third example.
Composing Objects
Given a set of 2D object positions, we aim to generate images containing objects
at those positions.
Qualitative results. We compare the proposed method and baselines on composing objects in Figure 4. We only show the concept conjunction here because
the object positions are not binary values, and thus negation of object positions
is not interpretable. Given a set of object position labels, we compose them to
generate images. Our model can generate images of objects at certain locations,
while the baseline methods either miss objects or generate incorrect objects.
Quantitative results. As shown in Table 1, our method outperforms baselines
by a large margin. The binary classification accuracy of our method is 15.88%
higher than the best baseline, EBM, in the 1 component test setting and is
Nan Liu*, Shuang Li*, Yilun Du* (equal contribution)
“A large blue metal cube to the left of a small yellow metal sphere” AND
“A large blue metal cube in front of a large cyan metal cylinder”
“A small brown metal sphere below a small green metal sphere” AND
“A small brown metal sphere behind a large gray rubber cube”
Fig. 5: Composing Visual Relations. Image generation results on the Relational
CLEVR dataset. Our model is trained to generate images conditioned on a single
object relation, but during inference, our model can compose multiple object relations,
generating better results than baselines.
Table 2: Quantitative evaluation of 128×128 image generation results on the Relational
CLEVR dataset. The binary classification accuracy (Acc) and FID score on three test
settings are reported. Although EBM performs well on binary classification accuracy,
its FID score is much lower than other methods. Our method achieves comparable or
better results than baselines.
1 Component
2 Components
3 Components
StyleGAN2-ADA 
StyleGAN2 
GLIDE 
24.02% higher than EBM in the more challenging 3 Components setting. Our
method is more effective in zero-shot compositional generalization. In addition,
our method can generate images with lower FID scores, indicating the generated
images are more similar to real images.
Composing Object Relations
Qualitative results. We further compare the proposed approach and baselines
on composing object relational descriptions in Figure 5. Our model is trained to
generate images conditioned on a single object relation, but it can compose multiple object relations during inference without additional training. Both LACE
and StyleGAN2 fail to capture object relations in the input sentences, but EBM
and our method can correctly compose multiple object relations. Our method
generates higher-quality images compared with EBM, e.g. the object boundaries
are sharper in our results than EBM. Surprisingly, DALL-E 2 and GLIDE can
generate high-quality images, but they fail to understand object relations.
Composable Diffusion
Quantitative results. Similarly to experiments in section 6.3, we evaluate
the proposed method and baselines on three test settings in Table 2. We train
a binary classifier to evaluate whether an image contains objects that satisfy
the input relational descriptions. For binary classification accuracy, our method
outperforms StyleGAN2, LACE, and GLIDE on all three test settings. EBMs
perform well on composing relational descriptions, but their FID scores are
much worse than other methods, i.e. their generated images are not realistic.
StyleGAN2-ADA can obtain better accuracy and FID than our approach, but
it cannot compose multiple concepts.
Results analysis
We show our composed results on image generation and the results generated
conditioned on each individual sentence description in Figure 6. We provide
four successfully composed examples, where the generated images contain all
the concepts mentioned in the input sentences.
Failure cases. We observed three main failure cases of the proposed method.
The first one is that the pre-trained diffusion models do not understand certain
concepts, such as “person” in (b). This is because the pre-trained diffusion model,
GLIDE , is trained to avoid generating human images. The second type of
failure is because the diffusion models confuse the objects’ attributes. In (c),
the generated image contains “a red bear” while the input is “a bear in a red
forest”. The third type of failure is because the composition does not work, e.g.
the “bird-shape and flower-color object” and the “dog-fur and sofa-shape object”
in (d). Such failures usually happen when the objects are in the center of the
Conclusion
In this paper, we compose diffusion models for image generation. By interpreting
diffusion models as energy-based models, we may explicitly compose them and
generate images with significantly more complex combinations that are never
seen during training. We propose two compositional operators, concept conjunction and negation, allowing us to compose diffusion models during the inference
time without any additional training. The proposed composable diffusion models
can generate images conditioned on sentence descriptions, objects, object relations, and human facial attributes, and can generalize to new combinations that
are rarely seen in the real world. These results demonstrate the effectiveness of
the proposed method for compositional visual generation.
A limitation of our current approach is that while we can compose multiple
diffusion models together, they are instances of the same model. We found limited success when composing diffusion models trained on different datasets. In
contrast, compositional generation with EBMs can successfully compose multiple separately trained models. Incorporating additional structures into diffusion
models from EBMs , such as a conservative score field, can be a promising
direction towards compositions of separately trained diffusion models.
Nan Liu*, Shuang Li*, Yilun Du* (equal contribution)
“A flower”
“A bird” AND
“A flower”
(b) Diffusion model fails
“A person”
“A bus” AND
“A person”
“A bear in a red
“A car stuck in
the forest”
“A bear in a red
forest” AND “A car
stuck in the forest”
(c) Diffusion model confuses object attributes
(d) Composition fails
“A forest”
“A camel” AND
“A forest”
(a) Successful Examples
“An abandoned
“A forest covered
with snow”
“An abandoned vehicle”
AND “A forest covered
with snow”
“A dog sitting in
the living room”
“A couch” AND “A dog
sitting in the living room”
“A yellow flower field” “A horse” AND “A
yellow flower field”
“A desert”
“A boat” AND
“A desert”
Fig. 6: Qualitative results. Successful examples (a) and failure examples (b-d) generated by the proposed method. There are three main types of failures: (b) The pretrained diffusion model does not understand certain concepts, such as “person”. (c) The
pre-trained diffusion model confuses objects’ attributes. (d) The composition fails. This
usually happens when the objects are in the center of images.
Composable Diffusion