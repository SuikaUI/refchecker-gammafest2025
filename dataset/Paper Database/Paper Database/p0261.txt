Deep Feature Learning with Relative Distance
Comparison for Person Re-identiﬁcation
Shengyong Ding , Liang Lin , Guangrun Wang , Hongyang Chao
Sun Yat-sen University, Guangzhou 510006, China
Identifying the same individual across diﬀerent scenes is an important yet dif-
ﬁcult task in intelligent video surveillance. Its main diﬃculty lies in how to
preserve similarity of the same person against large appearance and structure
variation while discriminating diﬀerent individuals. In this paper, we present
a scalable distance driven feature learning framework based on the deep neural
network for person re-identiﬁcation, and demonstrate its eﬀectiveness to handle
the existing challenges. Speciﬁcally, given the training images with the class
labels (person IDs), we ﬁrst produce a large number of triplet units, each of
which contains three images, i.e. one person with a matched reference and a
mismatched reference. Treating the units as the input, we build the convolutional neural network to generate the layered representations, and follow with
the L2 distance metric. By means of parameter optimization, our framework
tends to maximize the relative distance between the matched pair and the mismatched pair for each triplet unit. Moreover, a nontrivial issue arising with
the framework is that the triplet organization cubically enlarges the number
of training triplets, as one image can be involved into several triplet units. To
overcome this problem, we develop an eﬀective triplet generation scheme and an
optimized gradient descent algorithm, making the computational load mainly
depends on the number of original images instead of the number of triplets. On
several challenging databases, our approach achieves very promising results and
outperforms other state-of-the-art approaches.
Person Re-identiﬁcation, Deep Learning, Distance Comparison
 
December 14, 2015
1. Introduction
Person re-identiﬁcation, the aim of which is to match the same individual
across multiple cameras, has attracted widespread attention in recent years due
to its wide applications in video surveillance. It is the foundation of threat
detection, behavioral understanding and other applications. Despite the considerable eﬀorts of computer vision researchers, however, it is still an unsolved
problem due to the dramatic variations caused by light, viewpoint and pose
changes . Figure 1 shows some typical examples from two cameras.
Figure 1: Typical examples of pedestrians shot by diﬀerent cameras. Each column corresponds
to one person. Huge variations exist due to the light, pose and view point changes.
There are two crucial components, i.e. feature representations and distance
metric in person re-identiﬁcation systems. In these two components, feature
representation is more fundamental because it is the foundation of distance
The features used in person re-identiﬁcation range from the color
histogram , spatial cooccurrence representation model , attributes model
 to combination of multiple features . These handcrafted features can
hardly be optimal in practice because of the diﬀerent viewing conditions that
prevail .
Given a particular feature representation, a distance function is
learned to construct a similarity measure with good similarity constraints
. Although the eﬀectiveness of the distance function has been demonstrated, it
heavily relies on the quality of the features selected, and such selection requires
deep domain knowledge and expertise .
In this paper, we present a scalable distance driven feature leaning framework via the convolutional network to learn representations for the person reidentiﬁcation problem. Unlike the traditional deep feature learning methods
aimed at minimizing the classiﬁcation error, in our framework, features are
learned to maximize the relative distance. More speciﬁcally, we train the network through a set of triplets. Each triplet contains three images, i.e. a query
image, one matched reference (an image of the same person as that in the
query image) and one mismatched reference. The network produces features
with which the L2 distance between the matched pair and the mismatched pair
should be as large as possible for each triplet. This encourages the distances between matched pairs to take smaller values than those between the mismatched
pairs. Figure 2 illustrates the overall principles. As discussed in , the tripletbased model is a natural model for the person re-identiﬁcation problem for two
main reasons. First, the intra-class and inter-class variation can vary significantly for diﬀerent classes, and it may thus be inappropriate to require the
distance between a matched pair or mismatched pair to fall within an absolute
Second, person re-identiﬁcation training images are relatively scarce,
and the triplet-based training model can generate more constraints for distance
learning, thereby helping to alleviate the over-ﬁtting problem.
Similar to traditional neural networks, our triplet-based model also uses
gradient descent algorithms in solving the parameters. Owing to limitations in
memory size, it is impossible to load all the triplets for a given labeled image
set into the memory to calculate the gradient. A practical means is to train
the network iteratively in mini-batches, that is, in each iteration, a subset of
the triplets are generated and the network parameters are then updated with
the gradient derived from that batch. However, as we will see in the later sections, randomly generating the triplets at each iteration is ineﬃcient as only
a small number of distance constraints are imposed on the images within the
triplets. Therefore we propose a more eﬃcient triplet generation scheme. In
each iteration, we randomly select a small number of classes (persons) from the
dataset and generate the triplets using only those images, which guarantees that
Figure 2: Illustration of deep feature learning via relative distance maximization. The network
is trained by a set of triplets to produce eﬀective feature representations with which the true
matched images are closer than the mismatched images.
only a small number of images are selected in each iteration and rich distance
constraints are imposed. In our proposed triplet generation scheme, one image
can occur in several triplets in each iteration with a high degree of probability,
and we thus design an extended network propagation algorithm to avoid recalculating the gradients of the same images. Our triplet generation scheme and
the extended network propagation algorithm render the overall computational
load of our model dependent mainly on the number of the training images, not
on the number of triplets. Our approach also enables us to use the existing
deep learning implementations to solve our model parameters with only slight
modiﬁcations. In summary, we make two contributions to the literature:
1) A scalable deep feature learning method for person re-identiﬁcation via
maximum relative distance.
2) An eﬀective learning algorithm for which the training cost mainly depends
on the number of images rather than the number of triplets.
The remainder of this paper is organized as follows.
In section two, we
review the related work on person re-identiﬁcation problems. In section three,
we present our formulation and network architecture. In section four, we derive
the algorithms for solving the model parameters using gradient descent methods
for a small triplet set. In section ﬁve, we show how to train the network in batch
mode with an eﬃcient triplet generation scheme, and in section six, we present
our experimental results. Section seven concludes our work.
2. Related Work
Feature representation and distance metric are the two main components of
person re-identiﬁcation systems. The existing approaches to person re-identiﬁcation
tasks primarily make use of handcrafted features such as color and texture histograms .
To increase their representative capability, features have
been designed to carry spatial information .
For example, Farezena et
al. utilized the symmetry property of pedestrian images to propose a method
called Symmetry Driven Accumulation of Local Features (SDALF) which is robust to background clutter . The body conﬁguration-based pictorial structure
features have been also well studied to cope with individual variations .
In addition to handcrafted feature designs, some studies addressed learning
features for person re-identiﬁcation tasks. For example, Gray and Tao proposed the use of Adaboost to learn eﬀective representations from an ensemble
of local features. Zhao et al. proposed the learning of mid-level features
from hierarchical clusters of patches.
Another important research direction in person re-identiﬁcation is distance
Zheng et al.
 formulated the distance learning as a Probabilistic Relative Distance Comparison model (PRDC) to maximize the likelihood
that correctly matched pairs will have a smaller distance between them than
incorrectly matched pairs. In addition, Mignon and Jurie proposed Pairwise
Constrained Component Analysis (PCCA) to project the original data into a
lower dimensional space , in which the distance between pairs has the desired properties. Li et al. introduced a locally adaptive thresholding rule to
metric learning models (LADF), and reported that it achieved good perfor-
mance on person re-identiﬁcation tasks . RankSVM has also been proposed
for learning a subspace in which the matched images have a higher rank than
the mismatched images for a given query. There are also a number of general
distance learning methods that have been rarely exploited in the context of
person re-identiﬁcation problems .
Inspired by the success of deep learning, there are also some literatures
applying neural network models to address the person re-identiﬁcation problems.
Dong Yi et al.
 applied a deep neural network to learn pair-wise
similarity and achieved state-of-the-art performance. Hao Liu et al. presented a Set-Label Model, which applies DBN (Deep Belief Network) and NCA
(Neighborhood Component Analysis) on the proposed concatenated features of
the query image and the gallery image to improve the person re-identiﬁcation
performance. Xu et al. adopted a cluster sampling algorithm for reidentifying persons with templates.
 proposed a deep learning
framework for learning ﬁlter pairs that tries to automatically encode the photometric transforms across cameras. Our work diﬀers from these methods in its
loss function and learning algorithm.
The model most similar to that proposed herein was introduced by Wang
et al. and involved learning features for ﬁne-grained image retrieval. They
borrowed the network architecture designed by Krizhevsky et al. , and pretrained the network using soft-max loss function.
It is unclear whether the
triplet-based deep model can be eﬀectively trained from triplets without other
pre-training techniques. Here, we extend the triplet-based model to the person
re-identiﬁcation problem with an eﬃcient learning algorithm and triplet generation scheme. We demonstrate its eﬀectiveness without pre-training techniques
using a relatively simple network .
3.1. Formulation
Our objective is to use a deep convolutional network to learn eﬀective feature
representations that can satisfy the relative distance relationship under the L2
distance. In other words, we apply a deep convolutional network to produce the
feature for each image. And with these generated features, we require the distances between matched pairs should be smaller than those between mismatched
pairs as depicted in Figure 3.
Image Space
Feature Space
Convolutional Network
Share parameter W
Share parameter W
Figure 3: Illustration of maximizing the distance for person re-identiﬁcation. The L2 distance
in the feature space between the matched pair should be smaller than the mismatched pair in
each triplet.
In our model, the relative distance relationship is reﬂected by a set of triplet
units {Oi} where Oi =< O1
i >, in which O1
i are a matched pair
i are a mismatched pair. Let W = {Wj} denote the network
parameters and FW (I) denote the network output of image I, i.e. feature representation for image I. For a training triplet Oi, the desired feature should
satisfy the following condition under the L2 norm.
i ) −FW (O2
i )|| < ||FW (O1
i ) −FW (O3
or equally:
i ) −FW (O2
i )||2 < ||FW (O1
i ) −FW (O3
Here, we use the squared form to facilitate the partial derivative calculation.
For a given training set O={Oi}, the relative distance constraints are converted
to the minimization problem of the following objective, i.e.
maximizing the
distance between matched pairs and mismatched pairs, where n is the number
of the training triplets.
f(W, O) = Σn
i=1 max{||FW (O1
i ) −FW (O2
i )||2 −||FW (O1
i ) −FW (O3
i )||2, C}
The role of the max operation with the constant C is to prevent the overall value
of the objective function from being dominated by easily identiﬁable triplets,
which is similar to the technique widely used in hinge-loss functions. We set
C=-1 throughout the paper.
Note the network in our model still takes one image as input both for training
and testing as the conventional convolutional network does. The triplet-based
loss function is introduced for parameter optimization in the training stage.
During the testing, we feed each testing image to the trained network to get its
feature and use these features for performance evaluation under the normal L2
3.2. Network Architecture
All existing person re-identiﬁcation datasets are relatively small, and we thus
designed a simpliﬁed network architecture for our model. Figure 4 shows the
overall network architecture, which comprises ﬁve layers. The ﬁrst and third
layers are convolutional layers and the second and fourth layers are pooling layers. The ﬁrst convolutional layer includes 32 kernels of size 5×5×3 with a stride
of 2 pixels. The second convolutional layer takes the pooled output of the ﬁrst
convolutional layer as input and ﬁlters it with 32 kernels of size 5×5×32 with a
stride of 1 pixel. The ﬁnal 400 dimensional layer is fully connected to the pooled
output of the second convolutional layer with the following normalization:
Let {xi} denote the output before normalization, with the normalized output
then calculated by:
Note that this normalization diﬀers from the normalization scheme applied
by Krizhevsky et al. over diﬀerent channels. Our normalization ensures
that the distance derived from each triplet cannot easily exceeds the margin
C so that more triplet constraints can take eﬀect for the whole objective function. Accordingly, the back propagation process accounts for the normalization
operation using the chain rule during calculation of the partial derivative.
convolution
max pooling
convolution
max pooling
full connection
Figure 4: An illustration of the network architecture. The ﬁrst and third layers are convolutional layers and the second and fourth layers are max pooling layers. The ﬁnal layer is a full
connection layer.
We use overlapped max pooling for the pooling operation. More precisely,
the pooling operation can be thought of as comprising a grid of pooling units
spaced s pixels apart, with each summarizing a neighborhood of size z × z
centered at the location of the pooling unit. We set s=1 and z=2 in our network.
For the neuron activation functions, we use Rectiﬁed Linear Units to transform
the neuron inputs, thereby speeding up the learning process and achieving good
performance, as discussed in .
4. Learning Algorithm
In this section, we show how to solve the network given a ﬁxed set of training
triplets. We assume the memory is suﬃciently large to load all of the triplets.
The procedures for generating triplets from labeled images and training the
network using the batch mode is relegated to the next section.
4.1. Triplet-based gradient descent algorithm
We ﬁrst present a direct learning algorithm derived from the deﬁnition of the
objective function. For ease of exposition, we introduce d(W, Oi), which denotes
the diﬀerence in distance between the matched pair and the mismatched pair
in the triplet Oi.
d(W, Oi) = ||FW (O1
i ) −FW (O2
i )||2 −||FW (O1
i ) −FW (O3
and the objective function can be rewritten as,
f(W, O) = ΣOi max{d(W, Oi), C}
Then the partial derivative of the objective becomes
∂Wj = ΣOih(Oi)
if d(W, Oi) > C;
if d(W, Oi) <= C;
By the deﬁnition of d(W, Oi), we can obtain the gradient of d(W, Oi) as follows:
= 2(FW (O1
i ) −FW (O2
i ))′ · ∂FW (O1
i ) −∂FW (O2
i ) −FW (O3
i ))′ · ∂FW (O1
i ) −∂FW (O3
We can now see that the gradient on each triplet can be easily calculated given
the values of FW (O1
i ), FW (O2
i ), FW (O3
i ) and ∂FW (O1
can be obtained by separately running the standard forward and backward
propagation for each image in the triplet. As the algorithm needs to go through
all of the triplets to accumulate the gradients for each iteration, we call it the
triplet-based gradient descent algorithm. Algorithm 1 shows the overall process.
Algorithm 1: Triplet-based gradient descent algorithm
Training samples {Oi};
The network parameters {Wj}
1: while t < T do
for all training triplet Oi do
Calculate FW (O1
i ),FW (O2
i ),FW (O3
i ) by forward propagation;
Calculate ∂FW (O1
by back propagation;
Calculate ∂d(W,Oi)
according to equation 9;
Increment the gradient
∂Wj according to equation 7, 8;
11: end while
4.2. Image-based gradient descent algorithm
In the triplet-based gradient descent algorithm, the number of network propagations depends on the number of training triplets in each iteration, with each
triplet involving three rounds of forward and backward propagation during the
calculation of the gradient.
However, if the same image occurs in diﬀerent
triplets, the forward and backward propagation of that image can be reused.
Recognition of this potential shortcut inspired us to look for an optimized algorithm in which the network propagation executions depend only on the number
of distinct images in the triplets. Before considering that algorithm, we ﬁrst
review the way in which the standard propagation algorithm is deduced in the
conventional CNN learning algorithm, where the objective function often takes
the following form. Here n is the number of training images.
f(I1, I2, ..., In) = 1
i=1loss(FW (Ii))
As the objective function is deﬁned as the sum of the loss function on each
image Ii, we have:
∂loss(FW (Ii))
This shows that we can calculate the gradient of the loss function for each
image separately and then sum these image-based gradients to obtain the overall
gradient of the objective function. In the case of a single image, the gradient
can be calculated recursively by the chain rule, which is given as follows.
∂loss(FW (Ii))
= ∂loss(FW (Ii))
∂loss(FW (Ii))
= ∂loss(FW (Ii))
In the above equations, W l represents the network parameters at the lth layer
i represents the feature maps of the image Ii at the same layer. The
Equation 12 holds because Xl
i depends on the parameter W l and the Equation
13 holds because the feature maps at the (l + 1)th layer depend on those at the
lth layer. As the partial derivative of the loss function with respect to the output
feature can be simply calculated according to the loss function deﬁnition, the
gradient on each image can be calculated recursively. Simple summation of the
image gradients produces the overall gradient of the objective function.
We now turn to the triplet-based objective function and show that the overall gradient can also be obtained from the image-based gradients, which can
be calculated separately. The diﬃculty lies in the impossibility of writing the
objective function directly as the sum of the loss functions on the images, as
in Equation 10, because it takes the following form, where n is the number of
i=1loss(FW (O1
i ), FW (O2
i ), FW (O3
However, because the loss function for each triplet is still deﬁned on the outputs
of the images in each triplet, this objective function can also be seen as follows,
k} represents the set of all the distinct images in the triplets, i.e. {I′
i } and m is the number of the images in the triplets.
f = f(FW (I′
1), FW (I′
2), ..., FW (I′
k) is some function of the feature map Xl
k at the lth layer, the
objective function can also be seen as follows:
2, ..., Xl
Then the derivative rule gives us the following equations with Xl
k depending
on W l and Xl+1
depending on Xl
The ﬁrst equation shows the gradient of the loss function with respect to the
network parameters takes image-based form (summation over images) and tells
us how to get this gradient given
k for all k. Actually,
∂W l can be obtained
k with αk = ∂Xl
∂W l whose computation only relies on image I′
∂W l for all the layers, then we get the overall gradient of the triplet-based
loss function, i.e. ∆W =
The second equation tells us how to get the partial derivative of the loss
function with respect to the feature map of each image I′
k at the lth layer, i.e.
k recursively. More precisely, if we have known the partial derivative with
respect to the feature maps of the upper layer, say the (l + 1)th layer, then
the derivative with respect to this layer can be derived by simply multiplying a
matrix ∂Xl+1
which can also be calculated for each image I′
k separately.
So if we get the partial derivative of the loss function with respect to the
output (feature map of the top layer) of each image, i.e.
k), we can get the
∂W by applying Equation 18 and Equation 17 recursively (standard
backward propagation). Luckily, the derivative with respect to the output of
each image can be easily obtained as follows since it is deﬁned analytically on
∂max{||FW (O1
i ) −FW (O2
i )||2 −||FW (O1
i ) −FW (O3
i )||2, C}
Algorithm 3 provides the details of calculating
k). As the algorithm
shows, we need to collect the derivative from each triplet. If the triplet contains
the target image I′
k and the distance d(W, Oi) is greater than the constant C
(implementing the max operation in equation 3), then this triplet contributes
its derivative with respect to FW (I′
The form of this derivative depends
on the position where the image I′
k appears in the triplet Oi as listed in the
algorithm. Otherwise, this triplet will be simply passed. With this image-based
gradient calculation method, the whole training process is given in Algorithm
2. It is not hard to see that our optimized learning algorithm is very similar
to the traditional neural network algorithm except that calculating the partial
derivative with respect to the output of one image for the triplet-based loss
function relies on the outputs of other images while the traditional loss function
does not. This optimized learning algorithm has two obvious merits:
1. We can apply a recent deep learning implementation framework such as
Caﬀe simply by modifying the loss layer.
2. The number of network propagation executions can be reduced to the
number of distinct images in the triplets, a crucial advantage for large scale
5. Batch Learning and Triplet Generation
Suppose that we have a labelled dataset with M classes (persons) and that
each class has N images. The number of possible triplets would be M(M −
1)N 2(N −1). It would be impossible to load all of these triplets into the memory to train the network even for a moderate dataset. It is thus necessary to
train the network using the batch mode, which allows it to be trained iteratively. In each iteration, only a small part of triplets are selected from all the
possible triplets, and these triplets are used to calculate the gradient and then
Algorithm 2: Image-based gradient descent algorithm
Training triplets {Oi};
The network parameters W;
1: Collect all the distinct images {I′
k} in {Oi};
2: while t < T do
Calculate the outputs for each image I′
k by forward propagation;
for all I′
k) for image I′
k according to Algorithm 3;
k) using back propagation;
Increment the partial derivative:
W t = W t−1 −λt
12: end while
to update the network parameters. There are several ways to select triplets from
the full population of triplets. The simplest method is to select them randomly.
However, in random selection, the distinct image size is approximately three
times of the selected triplet size because each triplet contains three images, and
the likelihood of two triplets sharing the same image is very low. This triplet
generation approach is very ineﬃcient because only a few distance constraints
are placed on the selected images in each iteration. Instead, according to our
optimized gradient derivation, we know that the number of network propagations depends on the number of images contained in the triplets. So we should
produce more triplets to train the model with the same number of images in
each iteration. This leads to our following triplet generation scheme. In each iteration, we select a ﬁxed number of classes (persons), and for each image in each
Algorithm 3: Partial derivative with respect to the output of image I′
Training triplets {Oi}, image I′
The partial derivative:
2: for all Oi =< O1
if d(W, Oi) > C then
k)+ = 2(FW (O3
i ) −FW (O2
else if I′
k)−= 2(FW (O1
i ) −FW (O2
else if I′
k)+ = 2(FW (O1
i ) −FW (O3
12: end for
class, we randomly construct a large number of triplets, in which the matched
references are randomly selected from the same class and the mismatched references are randomly selected from the remaining selected classes. This policy
ensures large amounts of distance constraints are posed on a small number of
images, which can be loaded into the limited memory in each iteration. And
with the increasing number of iterations are executed, the sampled triplets still
can cover all the possible triplet pattern, ensuring the model to converge to a
local minimum.
As a comparison, suppose the memory can only load 300 images (a typical case for 2G GPU memory device). Then in the random triplet generation
scheme, only about 100 triplets can be applied to train the model in one iteration. However, our proposed scheme can use thousands of triplets to train
the model without obvious computation load increase. Algorithm 4 gives the
complete batch training process. As described in the ablation study section,
our proposed triplet generation scheme shows obvious advantages both in convergence time and matching rate.
Algorithm 4: Learning deep features from relative distance comparison
in the batch mode
Labelled training images {Ii};
Network Parameters W;
1: while t < T do
Randomly select a subset of classes (persons) from the training set;
Collect images from the selected classes: {I′
Construct a set of triplets from the selected classes;
for all I′
Run forward propagation for I′
Calculate the partial derivative of the loss function with respect to
k) according to Algorithm 3;
Run the standard backward propagation for I′
Accumulate the gradient: ∆W+ = ∆W(I′
W t = W t−1 −λt∆W;
14: end while
6. Experiments
6.1. Datasets and Evaluation Protocol
We used two well-known and challenging datasets, i.e., iLIDS and VIPeR
 , for our experiments. Both datasets contain a set of persons, each of whom
has several images captured by diﬀerent cameras. All the images were resized
to 250 × 100 pixels to train our network.
iLIDS dataset The iLIDS dataset was constructed from video images
captured in a busy airport arrival hall. It features 119 pedestrians, with 479
images normalized to 128 × 64 pixels. The images come from non-overlapping
cameras, and were subject to quite large illumination changes and occlusions.
On average, there are four images of each individual pedestrian.
VIPeR dataset The VIPeR dataset contains two views of 632 pedestrians. The pair of images of each pedestrian was captured by diﬀerent cameras
under diﬀerent viewpoint, pose and light conditions. It is the most challenging
dataset in the person re-identiﬁcation arena owing to the huge variance and
discrepancy.
0DWFKLQJ5DWH
35'&
$GDERRVW
/011
;LQJV
/íQRUP
%KDW
2XUV
Figure 5: Performance comparison using CMC curves on i-LIDS dataset.
Evaluation Protocol We adopted the widely used cumulative match curve
(CMC) approach for quantitive evaluation. We randomly selected about
half of the persons for training (69 for iLIDS and 316 for VIPeR), with the
Table 1: Performance of diﬀerent models on i-LIDS dataset.
remainder used for testing. To obtain the CMC values, we divided the testing
set into a gallery set and a probe set, with no overlap between them. The gallery
set comprised one image for each person. For each image in the probe set, we
returned the n nearest images in the gallery set using the L2 distance with the
features produced by the trained network. If the returned list contained an
image featuring the same person as that in the query image, this query was
considered as success of rank n. We repeated the procedure 10 times, and used
the average rate as the metric.
6.2. Performance Comparison
Training Setting The weights of the ﬁlters and the full connection parameters were initialized from two zero-mean Gaussian distributions with standard
deviation 0.01 and 0.001 respectively. The bias terms were set with the constant 0. We generated the triplets as follows. In each iteration, we selected 40
persons and generate 80 triplets for each person. When there were less than
10 triplets whose distance constraints could not be satisﬁed, i.e. the distance
between the matched pair is larger than the distance between the mismatched
pair, the learning process was taken as converged.
Figure 6: Search examples on iLIDS dataset. Each column represents a ranking result with
the top image being the query and the rest images being the returned list. The image with
the red bounding box is the matched one.
0DWFKLQJ5DWH
H%L&RY
H6'&
35'&
D35'&
.,660(
6DO0DWFK
0W0&0/
2XUV
Figure 7: Performance comparison using CMC curves on VIPeR dataset.
Comparison on iLIDS dataset Using the iLIDS dataset, we compared
our method with PRDC and other metric learning methods (i.e. Adaboost ,
Xing’s , LMNN , ITML , PLS , Bhat. , L1-norm and MCC
 ). The features were an ensemble of color histograms and texture histograms,
as described in . We used 69 persons for training and the rest for testing (the
same setting as used by the compared methods). Figure 5 shows the curves of
the various models, and Table 1 shows the top 1 , top 5, top 10, top 15, top 20
and top 30 performance. Our method achieved rank-1 accuracy 52.1%, which
clearly outperformed the other methods. Figure 6 shows several query examples
for the iLIDS dataset. In this ﬁgure, each column represents a ranking result
with the top image being the query image. The matched one in the returned
list is marked by a red bounding box.
Comparison on VIPeR dataset Using the VIPeR dataset, we compared our method with such state-of-the-art methods as MtMCML , LMLF
 , SDALF , eBiCov , eSDC , PRDC , aPRDC , PCCA ,
KISSME , LF and SalMatch . Half of the persons were used for
training, and the rest for testing (the same setting as used by the compared
methods). Figure 7 presents the CMC curves of the various models, and Table 2 presents the top 1 , top 5, top 10, top 15, top 20 and top 30 ranking
results. Our method achieved rank-1 accuracy 40.5% that clearly outperformed
Figure 8: Search examples on VIPeR dataset. Each column represents a ranking result with
the top image being the query and the rest images being the returned list. The image with
the red bounding box is the matched one.
Table 2: Performance of diﬀerent models on VIPeR dataset.
most available benchmarking methods. Figure 8 shows some query examples
for the VIPeR dataset. Each column represents a ranking result with the top
image being the query image and the rest being the result list. The matched
one in the returned list is highlighted by a red bounding box. This ﬁgure shows
the diﬃculty of this dataset. Actually, in the failed examples (rank 1 image
does contain the same person as the query), the images ranked higher than the
matched one often look more closer to the query image as in columns 2-7.
6.3. Ablation Studies of Learning
In this section, we explore the learning details on the VIPeR dataset, as it
is more challenging and contains more images.
Data Augmentation Data augmentation is an important mechanism for
alleviating the over-ﬁtting problem. In our implementation, we crop a center
region 230 × 80 in size with a small random perturbation for each image to
augment the training data. Such augmentation is critical to the performance,
particularly when the training dataset is small. In our experiment, the performance declined by 33 percent without it.
Normalization Normalization is a common approach in CNN networks ,
but these networks normalize the feature map over diﬀerent channels. In our
model, the output feature is normalized to 1 under the L2 norm. Without this
normalization, the top 1 performance drops by 25 percent. Normalization also
helps to reduce the convergence time. In our experiment, the learning process
roughly converged in four 4,000 iterations with normalization and in roughly
7,000 without it.
feature maps of
the rst convolutional layer
feature maps of
the second convolutional layer
Figure 9: Visualization of feature maps generated by our approach.
Triplet Generation The triplet generation scheme aﬀects the convergence
time and matching rate, as pointed out in previous sections. We compared the
model’s performance under two triplet generation schemes. In the ﬁrst scheme,
we selected 40 persons in each iteration, and constructed 80 triplets for each
person using the images of those 40 persons. In the second scheme, we again
selected 40 persons in each iteration, but constructed only one triplet for each
person (approximating random selection). The ﬁrst scheme achieved its best
performance in about 4,000 iterations while the second scheme achieved its best
performance (90 percent matching rate of the ﬁrst scheme) in 20,000 iterations.
However, the training time in each iteration for these two schemes is almost the
same as we expected.
Implementation Detail We implemented our model based on the Caﬀe
framework , with only the data layer and loss layer replaced. We trained the
network on a GTX 780 GPU with 2G memory. When there were fewer than
10 triplets whose distance constraints had been violated, the model was taken
as converged. Our model usually converged in less than one hour thanks to its
simpliﬁed network architecture and eﬀective triplet generation scheme.
Feature map visualization In addition, we visualize the intermediate features generated by our model to validate the eﬀectiveness of representation
learning. Figure 9 shows two examples, where we present some feature maps
of the ﬁrst and the second convolutional layers, respectively. As we expect, the
lower layer feature maps tend to have strong responses at the edges, showing
some characteristics of low level features.
7. Conclusion
In this paper, we present a scalable deep feature learning model for person
re-identiﬁcation via relative distance comparison. In this model, we construct a
CNN network that is trained by a set of triplets to produce features that can satisfy the relative distance constraints organized by that triplet set. To cope with
the cubically growing number of triplets, we present an eﬀective triplet generation scheme and an extended network propagation algorithm to eﬃciently train
the network iteratively. Our learning algorithm ensures the overall computation
load mainly depends on the number of training images rather than the number
of triplets. The results of extensive experiments demonstrate the superior performance of our model compared with the state-of-the-art methods. In future
research, we plan to extend our model to more datasets and tasks.
Acknowledgement
This work was supported by the National Natural Science Foundation of
China (No.
61173082 and No.61173081), Guangdong Science and Technol-
ogy Program (No. 2012B031500006), Guangdong Natural Science Foundation
(No. S2013050014548), Special Project on Integration of Industry, Education
and Research of Guangdong Province (No.
2012B091000101), and Program
of Guangzhou Zhujiang Star of Science and Technology (No. 2013J2200067).
Corresponding authors of this work are L. Lin and H. Chao.
References