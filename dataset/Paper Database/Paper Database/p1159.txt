IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 50, NO. 3, MARCH 2002
Optimization Algorithms Exploiting
Unitary Constraints
Jonathan H. Manton, Member, IEEE
Abstract—This paper presents novel algorithms that iteratively
converge to a local minimum of a real-valued function
) subject to the constraint that the columns of the complex-valued matrix
are mutually orthogonal and have unit norm. The algorithms are derived by reformulating the constrained optimization
problem as an unconstrained one on a suitable manifold. This significantly reduces the dimensionality of the optimization problem.
Pertinent features of the proposed framework are illustrated by
using the framework to derive an algorithm for computing the
eigenvector associated with either the largest or the smallest eigenvalue of a Hermitian matrix.
Index Terms—Constrained optimization, eigenvalue problems,
optimization on manifolds, orthogonal constraints.
I. INTRODUCTION
HIS paper derives novel algorithms for numerically minimizing a cost function
subject to the
orthogonality constraint
denotes Hermitian transpose, and
is the identity matrix. The complex-valued
case is considered for generality; the results in this paper remain valid if all quantities are restricted to being real-valued. It
has been shown recently that the geometrically correct setting for this constrained minimization problem is on the Stiefel
manifold in general and on the Grassmann manifold if
possesses the symmetrical property that
any unitary .
Publisher Item Identifier S 1053-587X(02)01329-6.
It is candidly stated that in terms of convergence speed
computational
complexity,
algorithms
are not necessarily the best algorithms for any given cost
function. Rather, the justification for this work is that it is
believed to be the first work that presents general purpose and
ready-to-use algorithms for solving optimization problems
with complex-valued orthogonality constraints. Indeed, the
only prerequisite for being able to implement the four main
optimization algorithms (namely, Algorithm 15 of Section V-A,
Algorithm 17 of Section V-B, Algorithm 24 of Section VII-A,
and Algorithm 26 of Section VII-B) is to be able to compute
the derivative and Hessian of a cost function
as defined in Section II. The relevance of these algorithms to
signal processing is now delineated.
A. Applications
As the opening paragraph of states, many signal processing tasks involve the constrained minimization of the function
is a possibly time-varying
covariance matrix. If the constraint is
, then it is by
now well known , that the minimum occurs when the
columns of
span the same subspace as spanned by the eigenvectors associated with the
smallest eigenvalues of
an example where
for any unitary
. Therefore, the algorithms in this paper, when applied to the specific
cost function
, complement the growing
literature on subspace estimation and tracking problems , ,
 , , , , , , with applications in antenna array processing , , frequency estimation , and
so forth. This is discussed further in Section VIII.
Other problems in linear algebra can also be expressed as orthogonally constrained minimization problems , . Examples include finding the singular value decomposition (SVD)
of a matrix and computing a total least-squares solution .
Whereas iterative methods, such as those presented here, have
yet to outperform traditional methods for solving linear algebra
problems in general, one advantage of iterative methods is their
applicability to adaptive engineering applications where minor
corrections to present estimates need to be performed regularly.
Another advantage is their computational robustness; iterative
refinement can be used to improve a solution obtained by traditional means.
When no closed-form solution exists to a problem, it becomes necessary to use an iterative method. Several examples
appearing in the more recent literature are now given.
The joint diagonalization problem, which appears in blind
source separation , , blind beamforming , , ,
and other , applications, is to find a unitary matrix
1053–587X/02$17.00 © 2002 IEEE
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 50, NO. 3, MARCH 2002
such that for given matrices
, the matrices
are all (approximately) diagonal.
It can be posed as an orthogonally constrained minimization
problem by choosing the cost function
to be the sum of
the squares of the off-diagonal elements of the
Eq. (20)]).
The weighted low-rank approximation problem is to find
a matrix having a prespecified rank and that best approximates
a given matrix under a weighted norm. It is shown in that
the weighted low rank approximation problem can be reformulated as an orthogonally constrained minimization problem, and
moreover, it is proved that the formulation is the most natural
one because it uses the least possible number of parameters. A
similar idea is used in to reformulate the convolutive reduced rank Wiener filtering problem as an orthogonally constrained minimization problem. (Prior to this reformulation, the
authors were unaware of any general solution to the convolutive
reduced-rank Wiener filtering problem.)
In summary, a significant number of engineering problems
can be formulated naturally as an orthogonally constrained minimization problem. The present paper not only provides novel
algorithms for solving such problems, but it provides a general
framework in which existing methods can be better understood.
B. Related Work
Although the references in show that the theory behind
the minimization of a cost function on a manifold was already
being studied in the seventies, general-purpose algorithms for
solving orthogonally constrained minimization problems did
not appear until the 1990s , , .
The key principle in , , was to exploit the geometry of the constraint surface, and in all cases, the algorithms
performed a series of descent steps, with each descent step taken
along a geodesic. (A geodesic is the generalization of a straight
line to curved surfaces.)
The present paper breaks with tradition by not moving
along geodesics. The reason for this is now explained. There
is no inherent connection between the (Riemannian) geometry imposed in , , and on the constraint surface
and an arbitrary cost function
(See Section IX for greater detail.) That is to say, although
moving along geodesics is a sensible thing to do, it is not
the only sensible thing that can be done. A disadvantage of
moving along geodesics is the computational cost involved in
computing the path of a geodesic. This paper, by choosing not
to follow geodesics, is able to achieve a modest reduction in
the computational complexity of the algorithms.
The main distinction of the present paper, however, is that
it considers the complex-valued case. The reason why the algorithms in , , and do not generalize immediately
to the complex-valued case is because a nonconstant cost function
cannot be analytic. This means that the gradient and Hessian of
on a complex Riemannian manifold are not well defined. Although the solution is straightforward—simply treat
as a function of the real and imaginary
components of
) whenever appropriate—it necessitates the algorithms for the complex-valued
case to be derived from scratch.
C. Outline of Paper
Two types of algorithms are derived in this paper. The first
type is based on the traditional steepest descent algorithm coupled with Armijo’s method for choosing the step size at each
iteration . Although steepest descent algorithms were derived in , only the case of
being a column vector was considered, and no step size rule was given. Steepest descent-type
algorithms were not explicitly considered in , which concentrated instead on conjugate gradient and Newton-type methods.
The second type of algorithm derived in this paper is based
on the traditional Newton algorithm . Although derived
similar Newton type algorithms to the ones presented here (albeit for the real-valued case only), there is an important difference; in , the pertinent manifold was locally parameterized
by using the exponential map, whereas this paper locally parameterizes the manifold by a Euclidean projection of the tangent space onto the manifold. This difference affects the computational complexity and the rate of convergence of the algorithms. The performance of the algorithms resulting from the
two different parameterizations was compared in for a particular cost function, and it was shown that the Euclidean-projection-based parameterization resulted in less computational
complexity and faster convergence. (For other cost functions,
the converse may well be true. A more detailed discussion appears in Section IX.)
Remark: Each algorithm is not simply an application of the
steepest descent or Newton method in some parameter space of
reduced dimension. The novel feature is that the local cost function to which the steepest descent or Newton method is applied
changes at each iteration.
The reason for presenting both steepest descent and Newton
type algorithms is because each one has its own advantages
and disadvantages. Steepest descent-type algorithms coupled
with Armijo’s step-size rule almost always converge to a local
minimum . However, their rate of convergence is only
linear, meaning that asymptotically, the number of correct
digits increases by a fixed amount per iteration (see 
for a precise definition). Newton-type algorithms, by using
second-order derivatives, are able to achieve quadratic convergence, meaning that the number of correct digits ultimately
doubles per iteration. This faster rate of convergence comes
with two disadvantages: increased computational complexity
per iteration and no guarantee that the algorithm will converge
to a local minimum. Indeed, without appropriate checks, the
Newton method will converge to the closest critical point,
whether it is a local maximum, local minimum, or a saddle
point. In practice, the steepest descent and Newton algorithms
are often used together; a few iterations of the steepest descent
algorithm are performed first to move close to a local minimum
before the Newton algorithm is applied.
The rest of this paper is organized as follows. Section II defines the derivative and Hessian of a cost function
and is the only prerequisite for being able to implement the algorithms in this paper. The theory behind these algorithms is
covered in Sections III–VII. Section III derives formulae for calculating the critical point of a quadratic function defined on various vector spaces. These formulae are required in subsequent
MANTON: OPTIMIZATION ALGORITHMS EXPLOITING UNITARY CONSTRAINTS
sections of the paper. Section IV introduces the complex Stiefel
manifold, its tangent space, and the Euclidean projection operator. Section V derives two algorithms for minimizing a cost
function on the Stiefel manifold. Similarly, Section VI introduces the Grassmann manifold, and Section VII derives algorithms for minimizing a cost function on the Grassmann manifold. A worked example, which is of independent interest in
its own right, is given in Section VIII. Section IX discusses various aspects of the optimization framework presented here. Section X concludes the paper.
Notation: The superscripts
denote transpose and
Hermitian transpose, respectively. The Frobenius norm is used
throughout, that is,
, where tr
is the trace
operator. The vec operator vec
is the vector obtained by
stacking the columns of the matrix
on top of each other.
Kronecker’s product is denoted by
. The symbol
the identity matrix whose size can be determined from its context. Similarly,
denotes the
-byidentity matrix, whereas
denotes the
-bymatrix with ones along the diagonal.
A square matrix
satisfying
is called unitary.
Given a matrix
satisfying
, its complement
is defined to be any matrix that satisfies
is not uniquely defined, implicit in any statement involving
is that the statement holds
for any choice of
. The square matrix
is skew-Hermitian
. It is Hermitian if
. The subspace
spanned by the columns of a matrix
is denoted by
expression
denotes a (possibly matrix-valued) function
remains bounded as
denotes absolute value (or norm). The symbol
denote the real and imaginary parts of a complex quantity, respectively.
II. SECOND-ORDER APPROXIMATION
This paper chooses to express the second-order Taylor series
approximation of an arbitrary (but sufficiently differentiable)
in the form
is the derivative of
evaluated at
are the Hessian of
evaluated at
ensure uniqueness, it is required that
Remark: For the real-valued case
, the matrix
should be omitted from (1).
in (1) was chosen because the Euclidean inner product
is the unique inner
product inducing the Frobenius norm
space. The consequence of this choice is that the
th element of
Although similar formulae can be derived for
often easier to determine
analogously to the
following example.
Example 1: If
Hermitian and
proving that
last equality following from the fact that vec
. Note that
, as required.
III. CRITICAL POINTS OF A QUADRATIC FUNCTION
This section derives formulae for finding critical points1 of
the quadratic function
defined by
are arbitrary matrices satisfying
, subject to
being restricted
to one of the following three vector spaces. The reason for considering these particular vector spaces will become clear in Sections V-B and VII-B. Let
given matrices such that
is skew-Hermitian, and
is arbitrary.
is skew-Hermitian and has zero diagonal elements (
), whereas
is arbitrary.
is arbitrary.
is not analytic, it is necessary to think of it as a
quadratic function in the real and imaginary parts of
consequence is that the vector spaces
are treated as
real vector spaces.
Proposition 2: Let
be a real vector space (such
), and define
as in (6). The point
is a critical point of
if and only if
satisfying the linear constraints
Proof: Let
be an arbitrary basis for
is the dimension of
1A critical point of a function is a point at which the first-order directional
derivatives are all zero. A nondegenerate quadratic function has a unique critical
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 50, NO. 3, MARCH 2002
MATLAB CODE THAT COMPUTES THE CRITICAL POINT OF THE QUADRATIC FUNCTION
~g(Z) = < trfZ
Dg + (1=2)vecfZg H vecfZg + (1=2)<fvecfZg C vecfZgg, WHERE Z IS RESTRICTED TO BE OF THE FORM Z = XA + X B WITH
A SKEW-HERMITIAN (cf. SECTION III). IT IS REQUIRED THAT [X X ]
[X X ] = I, H = H
AND C = C . (Xc CORRESPONDS TO X .)
is a critical point, then
, (8) shows that this requirement is equivalent to (7).
A matrix expression for
is given later in
Proposition 3. If
, however, a simple matrix expression does not exist. In order for the algorithms in this
paper to be immediately implementable, the Matlab code for
solving (7) for
is given in Table I. (It works
by forming a basis
as in the proof of Proposition 2 and then
to make (8) zero.) If
, then the following modifications must be made to Table I. The dimension
defined on
line 2 should be changed to
, and lines
5 to 7 (the
loop labeled “Diagonal elements of
be omitted.
Proposition 3: Define
as in (6). A critical point
occurs when
, then (9) simplifies to
Proof: Since
are restricted to lie in
must be expressible as
(7) implies that
must satisfy
for all matrices
. Splitting all terms into their real and imaginary parts proves that
must be given by (9). If
it is readily seen that (9) can be written in the complex-valued
form (10).
MANTON: OPTIMIZATION ALGORITHMS EXPLOITING UNITARY CONSTRAINTS
IV. COMPLEX STIEFEL MANIFOLD
This section derives a number of fundamental properties of
the complex Stiefel manifold. Although the complex Stiefel
manifold has been studied from a number of perspectives in the
literature , , the author has been unable to find explicit
statements of this section’s results elsewhere.
Definition 4 (Stiefel Manifold): The complex Stiefel manifold
is the set
The complex Stiefel manifold embeds naturally in
inherits the usual topology on
, and in particular, it is a
compact manifold.
The projection of an arbitrary matrix
onto the Stiefel manifold is defined to be the point on the Stiefel manifold closest
in the Euclidean norm. There is no unique solution if
does not have full column rank. Proposition 7, which is shown
later, proves the converse; there is a unique solution if
full column rank.
Definition 5 (Projection): Let
The projection operator
onto the Stiefel
is defined to be
The following useful lemma follows immediately from the
are unitary.
Lemma 6: If
are unitary matrices,
Proposition 7: Let
matrix. Then,
is well defined. Moreover, if the SVD of
, which, when combined with Lemma 6, proves that (14) has the unique solution
, it is sufficient to show that
with equality if and only if
. The inequality holds because
, where the latter is implied by
if and only if
proving the only if part.
Associated with each point of a manifold is a vector space
called the tangent space. The tangent space of an abstract manifold is only unique up to isomorphism. For example, two different representations of the tangent space about each point of
the real Stiefel manifold appear in and . The representation in comes from the Lie group structure, whereas the representation in comes from the embedding of the real Stiefel
manifold in
. This paper chooses to use a representation
of the tangent space that comes from the projection operator
This choice fits in naturally with the optimization scheme proposed in the next section.
, and consider the perturbed point
for some matrix
and scalar
sufficiently small,
has full rank, and hence,
is well defined.) Since
does not fill up the
space, certain directions
do not cause
to move away from
increases. The collection of directions
is called the normal
. The tangent space is defined to be the
orthogonal complement of the normal space. (Orthogonality is
with respect to the Euclidean inner product because this inner
product induces the Euclidean norm; see Section II.) The tangent and normal spaces are determined as follows.
Remark: It can be shown that the above nonstandard definition of a tangent space meets all the criteria required of a tangent
space in differential geometry. Moreover, the above definition
leads to a concrete representation of the tangent space, which is
the most suitable one for this paper.
satisfying
arbitrary matrix
is uniquely decomposable as
is skew-Hermitian,
is arbitrary, and
is Hermitian.
Furthermore
Proof: That
is a unique decomposition of
is clear. Define
derivative at
. This constraint is most easily enforcible by expressing
is skew-Hermitian,
is arbitrary, and
is Hermitian. Then,
is equivalent
. By definition,
is the closest point on the Stiefel
manifold to
must minimize
for sufficiently small . The minimum occurs when
(which is a consequence of Lemma 10 shown later),
completing the proof.
Lemma 8 shows that
, and in fact,
it can be shown that
sufficiently
small. This leads to the following definition.
Definition 9 (Normal Space): The normal space
of the Stiefel manifold
Lemma 8 suggests that the tangent space consists of directions of the form
. The following lemma confirms
that these directions are indeed orthogonal to the normal space.
Lemma 10: Let
. Then, for any skew-Hermitian
, arbitrary
, and Hermitian
. That is,
is orthogonal to
, and furthermore,
Proof: Since
is skew-Hermitian and
is Hermitian,
. Furthermore,
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 50, NO. 3, MARCH 2002
Definition 11 (Tangent Space): The tangent space
of the Stiefel manifold
It is readily verified that
is a real vector space. Its dimension, when considered as a vector space over
, is computed
as follows. Since
complex-valued elements, it
contributes an amount
to the overall dimension of
is skew-Hermitian, it is completely specified once its
complex-valued upper diagonal
elements are given as well as its
diagonal elements, which
must be purely imaginary. Thus, the overall dimension is
The second-order approximation of the projection from the
tangent space to the Stiefel manifold is required in the next section.
Proposition 12: If
Proof: Lemma 8 proves that the first-order term is
be such that
, direct expansion shows that
must satisfy
Subject to this constraint,
must also minimize
for sufficiently small . Applying the Lagrange multiplier technique proves that
V. OPTIMIZATION ON THE COMPLEX STIEFEL MANIFOLD
This section derives two algorithms (one is a modified
steepest descent method, and the other is a modified Newton
method) for minimizing
subject to
is real valued. Note that if the cost
is such that
for any unitary
, then the algorithms in Section VII should
be used instead.
The main principle behind optimization on manifolds is to
rewrite the optimization problem in terms of a local parameterization at each iteration. A local parameterization about the
is a mapping
an open subset
of the vector space
to the Stiefel
manifold with the property that any point
sufficiently close to
can be uniquely written as
. There are an infinite number of local parameterizations to choose from. For instance, the exponential map (which
is only defined after the Stiefel manifold is endowed with a Riemannian structure) is used in as a local parameterization.
The present paper proposes to use the local parameterization
defined by
is the projection operator (see Definition 5). Later, lemma 13
will prove that
is well defined. This local parameterization
is not only simpler than the one in , but simulations in 
demonstrate that it leads to faster convergence in certain applications.
Lemma 13: For arbitrary
the matrix
has full rank. In particular, the local parameterization
is well defined.
Proof: As in Definition 11, write
did not have full rank, then there would exist a nonzero
. Premultiplying
shows that this implies
. However, this is
not possible since
, being skew-Hermitian, has purely imaginary eigenvalues. That
is well-defined now follows from
Proposition 7.
Given a local parameterization
, the local cost function is
defined to be
, which is the composition of
instance, if
, then the local cost function
Whereas the cost function
is defined on a
-dimensional
vector space (
-space has
dimensions when considered
as a vector space over
), the local cost function
is defined on
-dimensional vector space [cf. (18)]. This reduction
in dimension is especially significant when
The general framework for optimization on manifolds is as
follows. Given a point
. Move to the new
, and repeat until convergence. Sections V-A
and B propose two ways of choosing
at each step, leading to
two algorithms for solving orthogonally constrained optimization problems.
A. Modified Steepest Descent on the Complex Stiefel Manifold
This section derives an algorithm for minimizing
subject to
. It requires the evaluation of
first derivative
at each step.
For a given
be the local
cost function
is a vector
space, the well-known steepest descent algorithm (see, for instance, ) can be used to find a
, which locally minimizes
. However, since the range of
covers only part of
the Stiefel manifold, it is more sensible to perform just a single
descent step using the local cost function
Performing a descent step requires the computation of the
gradient of
. The gradient is only defined once
is given an inner product. Ideally, the inner product should be
chosen to make the level sets of
approximately spherical
 . However, since
is not known in advance, it is necessary to make an arbitrary choice for the inner product. Just as the
Euclidean inner product is customarily used for optimization on
, the inner product
is commonly used on
[see for the derivation of
(20) in the real-valued case].
The reason why (20) is a natural choice for an inner product
is because it has the following geometrically pleasing interpretation. Express
be the matrix whose elements are all zero except for the
th element, which is one. Then, under (20), the “elementary” tangent
directions
for appropriate values of
are mutually orthogonal and have unit norm [the norm
of a tangent direction
]. This is a
MANTON: OPTIMIZATION ALGORITHMS EXPLOITING UNITARY CONSTRAINTS
desirable property because the perturbation
obtained by rotating the th column of
in the direction of the
th column of
by an angle of
, whereas the perturbation
is obtained by rotating both the
th and th columns of
by an angle of
The gradient
of the local cost function
the origin is, by definition, the unique
holds for all
. The steepest descent direction is
the negative of the gradient. It is expressible in terms of the first
derivative of
as follows.
Theorem 14 (Steepest Descent): Given the cost function
be the local cost function about a given point
. The steepest descent
direction of
at the origin
under the canonical inner
product (20) is
is any matrix such that
Proof: It follows from Lemma 8 and (23) that
The following calculation shows that
makes (21) and (24) equivalent
where the last equality follows from Lemma 10 and the fact that
. Finally, it can be shown that
is skew-Hermitian, verifying that
is an element of the
tangent space
and completing the proof.
Remark: Although one choice of
in (23) is the derivative
(see Section II), other choices are possible since (23)
only requires
to be the derivative of
in the tangent
directions
. This fact sometimes can be exploited
to simplify the computation of
Once the steepest descent direction
, which is defined
in (22), has been calculated, it is necessary to choose a positive
. The Armijo step size
rule states that
should be chosen to satisfy the inequalities
Rule (28) ensures that the step
will “significantly”
decrease the cost, whereas (29) ensures that the step
would not be a better choice. A straightforward method for
finding a suitable
is to keep on doubling
until (29) no
longer holds and then halving
until it satisfies (28). [From
(21), it is readily seen that such a
can always be found.]
Consolidating the above ideas yields the following algorithm.
The algorithm almost always converges to a local minimum, the
exception being if it lands directly on a saddle point first. Indeed,
it is proved in that the Armijo step size rule ensures that
decreases to a critical point, provided
is differentiable
and that the level sets of
are bounded. The latter criteria is
always true here because the Stiefel manifold
is compact.
Algorithm 15 (Modified Steepest Descent on Stiefel Manifold): Given a cost function
, the following algorithm almost always converges to a local minimum of
subject to the constraint that
. It requires that a
satisfying (23) can be computed for any
. Set step size
2) Compute
, which is the derivative of
[cf. (23)].
3) Compute the descent direction
4) Evaluate
is sufficiently small, then stop.
, then set
and repeat Step 5. [The projection
can be evaluated
using the SVD; see Proposition 7.]
, then set
, and repeat Step 6.
. Go to Step 2.
B. Modified Newton Method on the Complex Stiefel Manifold
This section derives an alternative algorithm for minimizing
subject to
. It requires the evaluation of
and its first two derivatives at each step. Unlike Algorithm 15,
the algorithm in this section only converges to a local minimum
if it is initialized to a point sufficiently close to the local minimum. However, when it does converge, it achieves2 a quadratic
rate of convergence. By comparison, Algorithm 15 exhibits only
a linear rate of convergence.
As in Section V-A, let
be the local cost
about the point
This section proposes to take a Newton step at each iteration
based on a quadratic approximation of
at the origin. Unlike
in Section V-A, it is not necessary to give
The following proposition derives the second-order Taylor series approximation of the local cost function
at the origin.
Proposition 16: Given the cost function
be the local cost function about a given
. Then, for any
are the derivative
and Hessian of
, respectively (cf. Section II).
2Although this fact is not proved here, the convergence proofs in and
 can be adapted to the algorithms in this paper.
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 50, NO. 3, MARCH 2002
Proof: Using Proposition 12 and (1) gives
The result now follows from the fact that tr
for any matrix
The Newton step is defined to be the value of
confined to
the tangent space
at which the quadratic approximation in (30) has its critical point. The location of the critical point
can be found by applying the theory in Section III; observe that
in Section III is the vector space
If the Hessian of
is singular, then the critical point of the
quadratic approximation of the local cost function is not unique.
It is therefore important to ensure that the cost function
not possess symmetries. The two most common symmetries that
may possess are
unitary and
is a diagonal unitary
matrix. In the former case, the theory in Section VII-B must be
used. In the latter case, it suffices to restrict
is the vector space defined in Section III. This necessitates a
minor modification of the following algorithm; see Remark 1
Algorithm 17 (Modified Newton Method on Stiefel Manifold): Given a twice-differentiable cost function
the following algorithm attempts to converge to a local minimum of
subject to the constraint that
requires that the derivative
and the Hessian
at the point
can be computed for
2) Compute
, which is the derivative of
, which is the Hessian of
, as defined in
Section II. If
is sufficiently small, then stop.
3) Compute
is defined in Table I. (See Remark 1 later.)
, then abort. [The projection
can be evaluated using the SVD; see Proposition 7.]
5) Use the SVD to compute
to the last
columns of
. Go to Step 2.
for any diagonal unitary matrix
then the following modifications must be made to
in Table I. The dimension
defined on line 2 should be
changed to
, and lines 5 to 7 (the
loop labeled “Diagonal elements of
”) should be
2) The quantity
of Algorithm 17 is equal to
in Step 4 of Algorithm 15, that is, it equals the norm of the gradient of
3) Step 5 of Algorithm 17 results in
As is the case with all Newton-type algorithms, Algorithm
17 can fail to take a descent direction if the current point
not sufficiently close to a minimum to ensure that the Hessian
is positive definite. If Algorithm 17 fails to take a descent step,
several iterations of Algorithm 15 can be used to move closer to
a minimum before restarting Algorithm 17.
VI. COMPLEX GRASSMANN MANIFOLD
If the cost function
is such that
any unitary matrix
, it should be minimized on the Grassmann
manifold rather than on the Stiefel manifold. This is because the
Grassmann manifold treats points
as being equivalent, leading to a further reduction in the dimension of the optimization problem.
This section derives a number of fundamental properties of
the complex Grassmann manifold. The derivation of the results
in this and the next section parallels that done in the last two
sections for the complex Stiefel manifold. However, unlike the
complex Stiefel manifold, the complex Grassmann manifold
does not embed in
space. Therefore, the derivations here
are not identical to the derivations in previous sections.
Definition
Manifold): The
complex Grassmann manifold
is the set of all
-dimensional complex subspaces of
There is a close connection between the Grassmann and
Stiefel manifolds. If
is a point on the Stiefel
manifold, then its
columns form an orthonormal basis for a
-dimensional subspace. That is, if
denotes the subspace
spanned by the columns of
. Conversely, every point in
obtainable in this way.
For the purposes of this paper, the Grassmann manifold is
best thought of as a quotient space of the Stiefel manifold. This
is now explained. Define two points
Stiefel manifold to be equivalent and denoted
MANTON: OPTIMIZATION ALGORITHMS EXPLOITING UNITARY CONSTRAINTS
exists a unitary matrix
clear that
if and only if
. Therefore, there is
a one-to-one correspondence between points on the Grassmann
and equivalence classes of
Since the Grassmann manifold does not embed in
space, the projection of an arbitrary matrix
onto the Grassmann manifold is defined in terms of the projection onto the
Stiefel manifold. There is no unique solution if
have full column rank. Later, Proposition 20 will prove the
converse; there is a unique solution if
has full column rank.
Definition 19 (Projection): Let
matrix. The projection operator
Grassmann manifold
is defined to be
was used in earlier sections to denote projection
onto the Stiefel manifold, henceforth,
refers to the projection
operator onto the Grassmann manifold, unless otherwise stated.
Proposition 20: Let
matrix. Then,
. Moreover, if the SVD of
, and if the
decomposition of
Proof: Proposition 7 implies that
is well defined and
. Moreover,
. Finally,
follows immediately from [9, Th. 5.2.1].
Remark: The usefulness of expressing the projection in
terms of the SVD or
decomposition is that both
are elements of
Since the Grassmann manifold is a quotient space of the
Stiefel manifold, its tangent space is a subspace of the Stiefel
manifold’s tangent space. This fact can also be seen from
the nonstandard definition of a tangent space in terms of the
projection operator given in Section IV; certain elements
the tangent space of the Stiefel manifold will no longer cause
to move away from
increases. Specifically,
for a given
be an element
of the tangent space of
. Applying Lemma 8 shows that
, provided
is invertible. Thus, the
subspace generated by
is not part of the tangent space of
. Conversely,
proving that the tangent space of
is the subspace
generated by matrices of the form
Definition 21 (Tangent Space): Let
. Then, the
tangent space
of the Grassmann
The dimension of
, which is considered to be a
vector space over
[cf. (18)].
The second-order approximation of the projection from the
tangent space to the Grassmann manifold is required in the next
Proposition 22: Let
element of the tangent space at
of the Grassmann manifold
Proof: From Proposition 12
VII. OPTIMIZATION ON THE COMPLEX GRASSMANN MANIFOLD
Whereas Section V derived algorithms for minimizing a general cost function
subject to
, this section derives specialized algorithms for the case when
either one or both of the following assumptions.
and unitary
and invertible
Such symmetries occur in subspace tracking , , as
well as in blind identification , . In the former case, it is
because a subspace is invariant to unitary transformations (that
), whereas in the latter case, it is caused by
the inability of second-order statistics to discriminate between
unitary transformations of certain parameters of interest.
Whereas Section V considered
as a function on the Stiefel
, this section considers
as a function on the
Grassmann manifold. Specifically, since
satisfies A1), there
exists a function
The local cost function
about the point
, is defined to be (cf.
Section V)
is the projection onto the Grassmann manifold defined
in Definition 19. (It follows from Lemma 13 that
is defined for all
.) One advantage of using the Grassmann manifold rather than the Stiefel manifold is that
dimensions, whereas
in Section V has
dimensions.
From a numerical point of view, it is not practical to work explicitly with the cost function
. Instead, the local cost function
(43) can be rewritten in the alternative form
-Factor” operator defined as follows. If
decomposition of the matrix
is defined to be the first
columns of
. That (44)
is equivalent to (43) follows immediately from Proposition 20.
Using the same framework as in Section V, Section VII-A
derives a modified steepest descent algorithm, whereas Section VII-B derives a modified Newton algorithm.
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 50, NO. 3, MARCH 2002
A. Modified Steepest Descent on the Complex Grassmann
This section derives an algorithm for minimizing
subject to
satisfies either A1) or both A1)
and A2) in Section VII. It requires the evaluation of
its derivative
at each iteration.
The steepest descent direction of the local cost function (43)
is only defined once
is given an inner product structure. Since
, the natural choice of an
inner product on
is the one obtained by restricting
the canonical inner product (20) on
fact, when restricted to
, (20) simplifies to the Euclidean inner product
Theorem 23 (Steepest Descent): Given the cost function
be the local cost
function about a given point
is defined
in (42). The steepest descent direction of
at the origin
under the canonical inner product (45) is
is any matrix such that
Proof: It is clear that
the projection operator onto the Stiefel manifold (Definition 5).
is given by (24). The following calculation shows
makes (21) and (24) equivalent for
last equality follows from the
Finally, it is readily seen that
, verifying that
element of the tangent space
and completing the
Remark: Although one choice of
in (47) is the derivative
(see Section II), other choices are possible since (47)
only requires
to be the derivative of
in the tangent
directions
Combining Theorem 23 with the Armijo step-size rule described in Section V-A leads to the following analog of Algorithm 15.
Algorithm 24 (Modified Steepest Descent on Grassmann
Manifold): Given a differentiable cost function
that satisfies (A1) in Section VII, the following algorithm
almost always converges to a local minimum of
to the constraint that
. It requires that a matrix
satisfying (47) can be computed for any
. Set step size
2) Compute
, which is the derivative of
[cf. (47)].
3) Compute the descent direction
4) Evaluate
is sufficiently
small, then stop.
, then set
, and repeat Step 5. [The
-Factor operator qf
defined in (44).]
, then set
, and repeat Step 6.
. Go to Step 2.
If the cost function
satisfies A2) in Section VII, then Algorithm 24 simplifies slightly; under A2),
holds for any full rank matrix
. Therefore, the
-Factor operator can be omitted from Steps 5 and 6 of
Algorithm 24.
B. Modified Newton Method on the Complex Grassmann
This section derives another algorithm for minimizing
subject to
satisfies either A1) or both
A1) and A2) in Section VII. It requires the evaluation of
and its first two derivatives at each iteration. Unlike Algorithm
24, the algorithm in this section only converges to a local
minimum if it is initialized to a point sufficiently close to the
local minimum. However, when it does converge, it achieves a
quadratic rate of convergence, as opposed to the linear rate of
convergence exhibited by Algorithm 24.
By definition, the Newton step moves to the critical point of
the quadratic approximation of the local cost function (43). The
quadratic approximation is given in the following proposition.
Proposition 25: Given a cost function
satisfying A1) of Section VII, let
be the local
cost function about a given point
is defined in (42). Then, for any
are the derivative
and Hessian of
, respectively (see Section II). If
A2) of Section VII, then (51) simplifies to
Proof: It is clear that
the projection operator onto the Stiefel manifold (Definition 5).
is given by (30) in general. If
satisfies A2), then
. Thus, Proposition 22 shows
from which (52) follows from (1).
MANTON: OPTIMIZATION ALGORITHMS EXPLOITING UNITARY CONSTRAINTS
The location of the critical point of (51) can be found by applying Proposition 3; observe that
in Section III is the vector
Algorithm 26 (Modified Newton Method on Grassmann Manifold): Given a cost function
, which is twice differentiable and satisfies A1) of Section VII, the following algorithm attempts to converge to a local minimum of
to the constraint that
. It requires that the derivative
and the Hessian
can be computed for any
2) Compute
, which is the derivative of
, which is the Hessian of
, as defined in
Section II. If
is sufficiently small,
then stop.
3) Compute the Newton step
as follows. Set
, then abort. [The
operator qf
is defined in (44).]
. Go to Step 2.
Remark: The quantity
in Step 2 of
Algorithm 26 equals
in Step 4 of Algorithm 24, that
is, it equals the norm of the gradient of
If the cost function
satisfies (A2) of Section VII, then
Proposition 25 shows that
in Step 3 of Algorithm 26 simplifies to
. Furthermore, as in
Section VII-A, the
-Factor operator can be omitted in Step 4
of Algorithm 26.
As is the case with all Newton-type algorithms, Algorithm
26 can fail to take a descent direction if the current point
not sufficiently close to a minimum to ensure that the Hessian
is positive definite. If Algorithm 26 fails to take a descent step,
several iterations of Algorithm 24 can be used to move closer to
a minimum before restarting Algorithm 26.
VIII. COMPUTING AN EXTREME EIGENVECTOR
A. Introduction
It is well known , that for a Hermitian matrix
achieves its minimum, subject to
corresponds to a minimal eigenvector,
that is, an eigenvector associated with the smallest eigenvalue of
. This section specializes Algorithm 24, which is the modified
steepest descent on the Grassmann manifold algorithm, to this
particular cost function. An attractive feature of this specialization is that the Armijo step size rule is replaced by the optimal
step size rule.
The reasons for deriving a novel algorithm for computing a
minimal eigenvector are now listed.
1) It serves as a worked example of how to apply the optimization algorithms in this paper.
2) It is used to demonstrate that the algorithms in this paper
can have significantly different properties compared with
classical algorithms for solving the same problem.
3) It can be used in a number of signal processing applications , that require the computation and subsequent tracking of a minimal eigenvector.
4) It has several advantages over existing algorithms for
computing a minimal eigenvector.
Computing a minimal eigenvector appears to be intrinsically
more difficult than computing a maximal eigenvector , ,
 , , , , , – . The two standard methods
that almost always converge to a minimal eigenvector are the
inverse iteration method (described later) and steepest descent
methods , . Other methods, such as Newton methods
 , Rayleigh quotient iterations , and so forth, converge to
the “nearest” eigenvector rather than to a minimal eigenvector.
One advantage of the novel algorithm derived here is that unlike the recently proposed steepest descent algorithms in and
 , it takes a step of optimal size at each iteration. This feature
is particularly attractive in tracking applications where
over time. Furthermore, it suggests that in certain applications
at least, the Euclidean-projection-based parameterization of the
Grassmann manifold in this paper is a more useful choice than
the geodesic-based parameterization used in and ; it does
not appear to be possible to compute the optimal step size for
the algorithms in and .
The other advantages are that the algorithm is guaranteed to
converge to a minimal eigenvector, provided the initial vector
is not orthogonal to the space spanned by the minimal eigenvectors, and unlike the classical inverse iteration method, the
algorithm is not sensitive to closely spaced eigenvalues. These
properties are proved in the following section and corroborated
by simulations in Section VIII-C.
B. Algorithm and Its Derivation
The notation and results in Sections II and VII are used here
with the minor change that the matrices
are replaced
by the vectors
and . The steepest descent direction
in Step 3 of Algorithm 24 is readily calculated from Example 1
of Section II under the assumption that
It is convenient to interpret
as a weighted average of the
eigenvalues of
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 50, NO. 3, MARCH 2002
be the eigendecomposition of
is Dirac’s delta function, and let
be such that
is a weighted sum of the eigenvalues
. In particular
-Factor operator appearing in Algorithm 24 is readily
evaluated when applied to a vector. Indeed
for arbitrary
. It will be shown that the decrease in cost
can be expressed in terms of the following variables.
are both real-valued since
since it is assumed that
. Straightforward
manipulation shows that
Differentiating (62) with respect to
and setting the result to
zero shows that the greatest decrease in cost occurs when
the unique positive root of the quadratic equation
denote this optimal value. It is interesting to note (cf.
Step 6 of Algorithm 24) that
Algorithm 24 specializes to the following. Note that the expressions for
in Step 3 of Algorithm 27 are equivalent
to those in (60). In addition, note that
must be real
Algorithm 27 (Computing a Minimal Eigenvector): Let
be an arbitrary Hermitian matrix. The following algorithm converges to a minimal eigenvector of
with probability
one (see Theorem 28 later).
1) Randomly choose an
with unit norm (
2) Compute the descent direction
is sufficiently small, then stop.
3) Compute
to the positive root of
. Renormalize by setting
Go to Step 2.
Before proving global convergence, two properties of Algorithm 27 are stated. Algorithm 27 is invariant to shifts; replacing
has no effect. This supports
the empirical evidence (see Section VIII-C) that closely spaced
eigenvalues, which are known to reduce severely the rate of convergence of power methods , do not affect the performance
of Algorithm 27. Algorithm 27 is also invariant to orthogonal
changes of coordinates. That is, if Algorithm 27 produces the
, then replacing
will produce the sequence
Theorem 28 (Global Convergence): Let
be the initial
vector chosen in Step 1 of Algorithm 27. If
is the smallest
eigenvalue of
and there exists an eigenvector
satisfying
, then Algorithm 27 converges
to an eigenvector
satisfying
Proof: Referring to Algorithm 27, since
is an eigenvector of
, it is clear that Algorithm 27
converges to an eigenvector
be the eigenvalue
associated with . Assume to the contrary that
must then be orthogonal to
, this implies
. It will be
shown below that one iteration of Algorithm 27 increases
if the step size
, it follows that
. This means there will come a time when
, and hence, (65) will also hold for all subsequent
iterations. This contradicts
, proving that
To show that (65) implies that
will increase, note first
that direct substitution proves that
if and only if (65) holds.
That is, (65) implies that
will increase unless
However, the latter cannot occur because, from (58),
and therefore,
can never be zero.
Finally, it is remarked that Algorithm 26 may also be applied
to the cost function
(see Example 1), Step 3 of Algorithm
26 becomes
satisfies the linear
C. Simulations
This section studies the convergence rate of Algorithm 27 and
compares it with traditional methods for calculating extremal
eigenvectors. It is demonstrated that the performance of Algorithm 27 is relatively insensitive to the actual eigenvalue distribution.
The inverse iteration method for finding an eigenvector of
the matrix
associated with the eigenvalue having the smallest
MANTON: OPTIMIZATION ALGORITHMS EXPLOITING UNITARY CONSTRAINTS
convergence
descent and inverse iteration algorithms when applied to the matrix
A = diagf1; 1:01; 1:02; 1:03; 1:04g.
convergence
descent and inverse iteration algorithms when applied to the matrix
A = diagf1; 2; 3; 4; 5g.
absolute value3 is to generate a sequence
of vectors according to the rule
Figs. 1–4 compare the inverse iteration method (68) with the
steepest descent method (Algorithm 27). Figs. 1 and 3 show
that Algorithm 27 outperforms (68) if the eigenvalues of
closely spaced, whereas Figs. 2 and 4 demonstrate that the converse holds as well. This is now explained in more detail.
It is well-known that the convergence rate of the power and
inverse iteration methods applied to the matrix
critically
3It is important to note that steepest descent algorithms converge to the
smallest eigenvalue, whereas the inverse iteration method converges to the
eigenvalue having the smallest absolute value. Similarly, the power method
(which will be mentioned later) converges to the eigenvalue having the largest
absolute value, whereas steepest ascent algorithms converge to the largest
eigenvalue. Therefore, when comparing algorithms, it is important to choose
A to be positive definite.
Graph comparing the convergence rates of the steepest descent and
inverse iteration algorithms when applied to ten randomly generated 20-by-20
matrices with eigenvalues uniformly distributed between 10 and 11.
Graph comparing the convergence rates of the steepest descent and
inverse iteration algorithms when applied to ten randomly generated 20-by-20
matrices with eigenvalues uniformly distributed between 0 and 1.
depends on the eigenvalue distribution of
. Indeed, replacing
for some constant
(which is known as a
shift in the literature) significantly alters the convergence rate of
(68). In comparison, Section VIII shows that such shifts do not
alter Algorithm 27 at all. It is therefore expected that the inverse
iteration method will exhibit convergence rates ranging from
extremely poor to extremely good, depending on the eigenvalue
distribution of
, whereas Algorithm 27 is expected to achieve
a steady rate of convergence over a wide range of eigenvalue
distributions.
This hypothesis was tested by plotting the log of the
error, which is defined as
is the smallest eigenvalue of
the iteration number
. Since the eigenvalues are closely
spaced, Algorithm 27 significantly outperforms (68). Conversely, Fig. 2 shows that (68) outperforms Algorithm 27
when applied to the matrix
and 4 suggest that this behavior is typical. Fig. 4 shows the
performance of the two algorithms when applied to ten randomly generated 20-by-20 matrices with eigenvalues uniformly
distributed between 0 and 1. The same ten matrices were then
shifted so that their eigenvalues lay between 10 and 11 (that is,
was replaced with
) and the results plotted in
Fig. 3. Whereas the performance of Algorithm 27 is unaltered,
(68) performs badly in Fig. 3 but exceptionally well in Fig. 4.
The steepest ascent method, which is obtained by replacing
in Algorithm 27, was compared with the power
method for converging to an eigenvector associated with the
largest eigenvalue of
. The power method updates
according to the rule [cf. (68)]
Figs. 5 and 6 were generated analogously to Figs. 3 and 4. They
demonstrate that Algorithm 27 achieves a convergence rate that
is much less sensitive to the location of the eigenvalues of
than the power method does.
Finally, the rapid convergence of the Newton method (Algorithm 26 applied to the cost function
in Section VIII) for finding a minimal eigenvector is illustrated
in Fig. 7. (The Newton method converged to the exact answer
up to machine precision on the third iteration.) Note that two
iterations of Algorithm 27 were performed before running the
Newton method since otherwise, the Newton method would fail
to converge to a minimal eigenvector.
IX. DISCUSSION
This section discusses the conceptual differences between the
optimization approach in this paper and the approach in . It
also gives a qualitative description of when the Newton algorithms here are expected to outperform the Newton algorithms
It is first noted that the general framework in Section V for
minimizing a function on a manifold—namely, given a point
on the manifold, apply a single iteration of the steepest descent or Newton algorithm to the local cost function
is a local parameterization about
move to the new point
and repeat—is more general
than the framework in . Choosing
to be the exponential
map (which corresponds to using geodesics to locally parameterise the manifold) results in the Newton algorithm in . The
differences between the algorithms can therefore be understood
by determining what effect the choice of the local parameterization
has on the computational complexity and the rate
of convergence of the algorithms.
As mentioned earlier, the local parameterization used in this
paper is computationally simpler to compute than the one in .
The asymptotic rate of convergence of the modified Newton algorithms here is the same as for the Newton methods in , that
is, they all asymptotically achieve a quadratic rate of convergence. However, for a given cost function, it can be expected that
one algorithm will converge faster than the other one. (Which
Graph comparing the convergence rates of the steepest ascent and
power method algorithms when applied to ten randomly generated 20-by-20
matrices with eigenvalues uniformly distributed between 10 and 11.
Graph comparing the convergence rates of the steepest ascent and
power method algorithms when applied to ten randomly generated 20-by-20
matrices with eigenvalues uniformly distributed between 0 and 1.
Graph showing the rapid convergence of the Newton method when
used to find the minimal eigenvector of the matrix A = diagf1; 2; 3; 4; 5g.
MANTON: OPTIMIZATION ALGORITHMS EXPLOITING UNITARY CONSTRAINTS
algorithm is the faster depends on the cost function.) The following example is used to explain this phenomenon.
Consider the ordinary Newton method applied to the two
cost functions
. Note that
the same function expressed in different coordinate systems.
Specifically,
is the mapping
from Cartesian to polar coordinates. The Newton method approximates the cost function by a quadratic cost at each iteration.
is quadratic about any point
the Newton method applied to
converges in a single iteration.
However, since
is not quadratic about any
, the Newton method will not converge in a single
iteration when applied to
. Conversely, a “Newton algorithm
in polar coordinates” would converge in one iteration when applied to
but take longer to converge when applied to
In the above example, the change of coordinates
is analogous to the local parameterization
used to define Newton algorithms on a manifold. In a qualitative sense, it implies that the
algorithm that achieves the faster convergence for a particular
cost function
is the algorithm that uses the local parameterization
(either the exponential map in or the Euclidean projection operator in this paper), resulting in the local
cost function
more closely approximating a quadratic
function. Simulations in show that for one particular class
of cost functions, the algorithms here converge faster than the
algorithms in . However, the preceding argument suggests
that there may exist another class of cost functions for which
the converse is true.
Since the performance of the algorithms depends on the
choice of local parameterization
relative to the cost
, it is worthwhile understanding the motivation
behind the choice of local parameterization here and in .
In , the Stiefel manifold was made into a Riemannian manifold by endowing it with its canonical metric.4 A connection
function5 (the Levi-Civita connection) was also given to the
manifold. Doing this made it possible to calculate the gradient
and Hessian of a function on the manifold. Roughly speaking,
the classical formula for computing the Newton step was generalized in by replacing the first- and second-order derivatives
in Newton’s formula by the gradient and Hessian of the cost
function on the manifold. Newton’s formula results in a vector
pointing in the direction to move, and since following a geodesic
corresponds to walking in a straight line, it is natural for to
interpret Newton’s formula as requiring the Newton step to be
taken along a geodesic.
The motivation for considering a different approach in this
paper is that there does not seem to be an intrinsic connection
between the Riemannian geometry of the Stiefel manifold and
the minimization of an arbitrary cost function. Why follow a
geodesic, which is costly to compute, if it is not essential?
The algorithms in this paper avoid giving the Stiefel manifold a metric structure and a connection function. They are able
to do this because they never compute a gradient or a Hessian of
4A metric is an inner product structure given to each tangent space that varies
in a smooth way .
5A connection function is required before “second-order derivatives” can be
meaningfully calculated on a manifold.
a function on the manifold. Instead, at each iteration, they form
a local cost function whose domain is a vector space and not
a manifold. Although the derivative and Hessian of this local
cost function can only be calculated once the vector space is
given an inner product structure (which is tantamount to giving
the Stiefel manifold a metric structure except that there is no
smoothness requirement), the Newton step is independent of the
inner product structure chosen. Therefore, the Newton algorithms in this paper can claim (for better or for worse) to be unrelated to any Riemannian structure put on the manifold. (They
do, however, depend on the choice of norm used to define the
projection operator.)
While the approach in depends on the Riemannian structure given to the Stiefel manifold, the approach in this paper
depends on the projection operator used to define the local parameterization. The Euclidean norm in Definition 5 was chosen
somewhat arbitrarily; a number of interesting cost functions can
be written in terms of a Euclidean norm so that it seemed sensible to use the Euclidean norm to define the projection operator
as well. A positive consequence of this choice is that it allows
the optimal step size to be computed in Section VIII for a particular cost function.
X. CONCLUSION
This paper derived novel algorithms for the minimization of a
cost function
subject to the constraint
The key feature of the algorithms is that they reduce the dimensionality of the optimization problem by reformulating the optimization problem as an unconstrained one on either the Stiefel
or Grassmann manifolds. A consequence of this is that the convergence properties of the algorithms may be different from
those of traditional methods. To verify this assertion, the algorithms were applied to the problem of finding an eigenvector
associated with the smallest eigenvalue of a Hermitian matrix.
Simulations showed that the performance of the resulting algorithms exhibited quite different behavior from the traditional
power and inverse iteration methods for computing an extremal
eigenvector.
ACKNOWLEDGMENT
The author wishes to thank R. Mahony for many fruitful
discussions centered around the problem of minimizing a cost
function on a manifold. The author also wishes to thank the
anonymous reviewers for their insightful comments, which led
to a significantly improved presentation of the paper.