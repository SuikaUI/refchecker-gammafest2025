Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 781–787,
Beijing, China, July 26-31, 2015. c⃝2015 Association for Computational Linguistics
Aspect-Level Cross-lingual Sentiment Classiﬁcation
with Constrained SMT
Patrik Lambert
Universitat Pompeu Fabra, Barcelona, Spain
 
Most cross-lingual sentiment classiﬁcation (CLSC) research so far has been performed at sentence or document level.
Aspect-level CLSC, which is more appropriate for many applications, presents the
additional difﬁculty that we consider subsentential opinionated units which have to
be mapped across languages. In this paper, we extend the possible cross-lingual
sentiment analysis settings to aspect-level
speciﬁc use cases. We propose a method,
based on constrained SMT, to transfer
opinionated units across languages by preserving their boundaries.
We show that
cross-language sentiment classiﬁers built
with this method achieve comparable results to monolingual ones, and we compare different cross-lingual settings.
Introduction
Sentiment analysis (SA) is the task of analysing
opinions, sentiments or emotions expressed towards entities such as products, services, organisations, issues, and the various attributes of these
entities .
The analysis may be performed at the level of a document (blog post, review) or sentence. However, this is not appropriate
for many applications because the same document
or sentence can contain positive opinions towards
speciﬁc aspects and negative ones towards other
aspects. Thus a ﬁner analysis can be conducted
at the level of the aspects of the entities towards
which opinions are expressed, identifying for each
opinionated unit elements such as its target, polarity and the polar words used to qualify the target.
The two main SA approaches presented in the
literature are (i) a machine learning approach,
mostly supervised learning with features such as
opinion words, dependency information, opinion
shifters and quantiﬁers and (ii) a lexicon-based approach, based on rules involving opinion words
and phrases, opinion shifters, contrary clauses
(but), etc. Thus in most SA systems we may distinguish three types of resources and text:
TRAIN Resources (collection of training examples, lexicons) used to train the classiﬁer.
TEST Opinions to be analysed.
OUT Outcome of the analysis. It depends on the
level of granularity. At the document or sentence
level, it is the polarity of each document or sentence. At the aspect level, it may the set of opinion
targets with their polarity.
The internet multilingualism and the globalisation of products and services create situations in
which these three types of resources are not all
in the same language. In these situations, a language transfer is needed at some point to perform
the SA analysis or to understand its results, thus
called cross-lingual sentiment analysis (CLSA).
Sentences or documents are handy granularity
levels for CLSA because the labels are not related
to speciﬁc tokens and thus are not affected by a
language transfer. At the aspect level, labels are
attached to a speciﬁc opinionated unit formed by
a sequence of tokens. When transferring these annotations into another language, the opinionated
units in the two languages have thus to be mapped.
This paper is one of the ﬁrst ones to address
CLSA at aspect level (see Section 3). It makes
the following speciﬁc contributions:
(i) an extended deﬁnition of CLSA including
use cases and settings speciﬁc to aspect-level
analyses (Section 2);
(ii) a method to perform the language transfer
preserving the opinionated unit boundaries.
This avoids the need of mapping source and
target opinionated units after the language
transfer via methods such as word alignment
(Section 4);
The paper also reports (in Section 5) experiments
comparing different settings described in Section 2.
Use Cases and Settings
We can think of the following use cases for CLSA:
Use case I. There are opinions we want to analyse, but we do not avail of a SA system to perform
this analysis. We thus want to predict the polarity
of opinions expressed in a language LTEST using a classiﬁer in another language LTRAIN. We
can assume that the language LOUT of the analysis
outcome1 is the same as the one of the opinions. In
this case, equation 1 applies, yielding CLSA settings a and b as follows (see also Figure 1).
LTRAIN ̸= LTEST ; LOUT = LTEST
(a) available training resources are transferred
into the test language to build a classiﬁer in the
test language.
(b) we translate the test into the language of the
classiﬁer, classify the opinions in the test, and then
transfer back the analysis outcome into the source
language by projecting the labels or/and opinionated units onto the test set.
OUTLT RAIN
Figure 1: Use case I settings. SA refers to Sentiment Analisys, T to Translation, Proj to Projection and Learn to Learning, and the prime symbol designs a language into which a set has been
automatically translated.
Use case II. We may have training resources in
the language of the opinions, but we need the re-
1As mentioned above, at the aspect level, the outcome of
the analysis may be a set of opinion targets with their polarity. It may also be more complex, such as a set of opinion
expressions with their respective target, polarity, holder and
time . The outcome may need to be in another language as the opinions themselves. For example, a company
based in China may survey the opinions of their Spanishspeaking customers, and then transfer the SA outcome into
Chinese so that their marketing department can understand it.
sult of the analysis in a different language. Here,
the inequality of Eq. 2 applies, yielding CLSA settings c and d as follows (see also Figure 2).
LOUT ̸= LTEST
(c) LTRAIN = LTEST ; the test opinions are
ﬁrst analysed in their language, then the analysis
outcome is transferred into the desired language.
(d) LTRAIN = LOUT ; the test set is ﬁrst transferred into the desired outcome language, and the
SA is performed in this language.
Figure 2: Use case II settings.
Use case II only makes sense for aspect-level
analysis,2 and to our knowledge, it was not addressed in the literature so far.
Use case III. We want to beneﬁt from data
available in several languages, either to have more
examples and improve the classiﬁer accuracy, or to
have a broader view of the opinions under study.
In this paper we focus on use cases I and II.
Related Work
The main CLSC approaches described in the literature are via lexicon transfer, via corpus transfer,
via test translation and via joint classiﬁcation.
In the lexicon transfer approach, a source sentiment lexicon is transferred into the target language
and a lexicon-based classiﬁer is build in the target language. Approaches to transfer lexica include machine translation (MT) , Wordnet , relations between
dictionaries represented in graphs , or triangulation .
The corpus transfer approach consists of transferring a source training corpus into the target language and building a corpus-based classiﬁer in the
target language. Banea et al. follow this
approach, translating an annotated corpus via MT.
Balamurali et al. use linked Wordnets to
2For document and sentence-level classiﬁcation, the outcome is a set of polarity labels independent on language.
replace words in training and test corpora by their
(language-independent) synset identiﬁers. Gui et
al. reduce negative transfer in the process
of transfer learning. Popat et al. perform
CLSA with clusters as features, bridging target
and source language clusters with word alignment.
In the test translation approach, test sentences
from the target language are translated into the
source language and they are classiﬁed using a
source language classiﬁer .
Work on joint classiﬁcation includes training a classiﬁer with features from multilingual
views ,
co-training , joint learning ,
structural correspondence learning or mixture
models . Gui et al. compare several of these approaches.
Brooke et al. and Balamurali et al.
 conclude that at document level, it is
cheaper to annotate resources in the target language than building CLSA systems.
not be true at aspect level, in which the annotation cost is much higher. In any case, when the
skills to build such annotated resources are lacking, CLSA may be the only option. In language
pairs in which no high-quality MT systems are
available, MT may not be an appropriate transfer method . However, Balahur and Turchi conclude that MT systems can be used to build sentiment analysis systems that can obtain comparable
performances to the one obtained for English.
All this work was performed at sentence or document level.
Zhou et al. and Lin et al.
 work at the aspect level, but they focus on
cross-lingual aspect extraction. Haas and Versley
 use CLSA for individual syntactic nodes,
however they need to map target-language and
source-language nodes with word alignment.
Language Transfer
In aspect-level SA, there may be several opinionated segments in each sentence. When performing a language transfer, each segment in the target
language has to be mapped to its corresponding
segment in the source language. This may not be
an obvious task at all. For example, if a standard
MT system is used for language translation, the
source opinionated segment may be reordered and
split in several parts in the target language. Then
the different parts have to be mapped to the original segment with a method such as word alignment, which may introduce errors and may leave
some parts without a corresponding segment in
the source language. To avoid these problems, we
could translate only the opinionated segments, independently of each other. However, the context
of these segments, which may be useful for some
applications, would then be lost. Furthermore, the
translation quality would be worse than when the
segments are translated within the whole sentence
To solve these problems, we translate the whole
sentences but with reordering constraints ensuring that the opinionated segments are preserved
during translation. That is, the text between the
relevant segment boundaries is not reordered nor
mixed with the text outside these boundaries.3
Thus the text in the target language segment comes
only from the corresponding source language segment.
We use the Moses statistical MT (SMT)
toolkit to perform the translation. In Moses, these reordering constraints are
implemented with the zone and wall tags, as indicated in Figure 3. Moses also allows mark-up
to be directly passed to the translation, via the x
tag. We use this functionality to keep track, via the
tags <ou[id][-label]> and </ou[id]>, of
the segment boundaries (ou stands for Opinionated Unit), of the opinionated segment identiﬁer
([id]) and, for training and evaluation purposes,
of the polarity label ([-label]). In the example
of Figure 3, the id is 1 and the label is P.
CLSA experiments
In order to compare CLSA settings a and b (of use
case I), we needed data with opinion annotations at
the aspect level, in two different languages and in
the same domain. We used the OpeNER4 opinion
corpus,5 and more speciﬁcally the opinion expression and polarity label annotations of the hotel review component, in Spanish and English. We split
the data in training (train) and evaluation (test) sets
as indicated in Table 1.
The SMT system was trained on freely avail-
3However, reordering within the segment text is allowed.
4 
5Described in deliverable D5.42 (page 6) at:
 
This corpus will be freely available from June 2016 on, and
until then can be used for research purposes.
Source: On the other hand <zone> <x translation="ou1-P">x</x> <wall/> a big advantage <wall/> <x translation="/ou1">x</x> </zone> of the hostel is its placement
Translation: por otra parte <ou1-P>una gran ventaja</ou1> del hostal es su colocaci´on
Figure 3: Source text with reordering constraint mark-up as well as code to pass tags, and its translation.
Table 1: Number of documents (Docs), words and
opinionated units (Op. Units) in the OpeNER annotated data for English (EN) and Spanish (ES).
able data from the 2013 workshop on Statistical Machine Translation6 . We also
crawled monolingual data in the hotel booking
domain, from booking.com and TripAdvisor.com.
From these in-domain data we extracted 100k and
50k word corpora, respectively for data selection and language model (LM) interpolation tuning. We selected the data closest to the domain in
the English-Spanish parallel corpora via a crossentropy-based method ,
using the open source XenC tool . The size of available and selected corpora
are indicated in the ﬁrst 4 rows of Table 2. The LM
was an interpolation of LMs trained with the target
part of the parallel corpora and with the rest of the
Booking and Trip Advisor data (last 2 rows of Table 2). We used Moses Experiment Management
System with all default options to
build the SMT system.7
Because the common crawl corpus contained
English sentences in the Spanish side, we applied
an LM-based ﬁlter to select only sentence pairs in
which the Spanish side was better scored by the
Spanish LM than with the English LM, and conversely for the English side.
We conducted supervised sentiment classiﬁcation experiments for settings a and b of use case
I (see Section 2). We trained and evaluated classiﬁers on the annotated data (Table 1), using as
features the tokens (unigrams) within opinion expressions, and SP (Strong Positive), P (Positive),
N (Negative) and SN (Strong Negative) as la-
6 
7We kept selected parallel data of the common crawl corpus for tuning and test. We obtained BLEU scores of 42 and
45 in the English–Spanish and Spanish–English directions.
Common Crawl
Europarl v7
News Commentary
Trip Advisor
Table 2: Size of the available and selected corpora
(in million words) in English (EN) and Spanish
(ES) used to train the SMT system.
Figure 4: Experiments corresponding to group of
rows 1 of Table 3. “mono” refers to monolingual
and “CL a” and “CL b” refer to settings a and b of
use case I (Sec. 2).
bels. We performed the experiments with the weka
toolkit , using a ﬁlter to convert strings into word vectors, and two learning algorithms: SVMs and bagging with Fast Decision
Tree Learner as base algorithm.
Figure 4 represents the experiments conducted
with the EN test set. A monolingual classiﬁer in
English is trained with the EN training set, and
evaluated with the EN test set (1 mono). The re-
Table 3: Accuracy (in %) achieved by the different
systems. LM Filter and No Fil(ter) refer to the
presence or not of the LM ﬁlter for the common
crawl parallel corpus. “Bag.” refers to bagging.
sults are reported in the ﬁrst row of Table 3. To
evaluate cross-lingual setting a, the ES training set
is translated into English (see Section 4), and an
English classiﬁer is trained on the translated data
and evaluated on the EN test set (1 CL a). To evaluate setting b, the EN test set is translated into
Spanish, and this translated test is used to evaluate a classiﬁer trained on the ES training set (1 CL
b). With this very simple classiﬁer, we achieve
up to 83.4% accuracy in the monolingual case.
With cross-lingual settings, we loose from about
4% to 8% accuracy, and with the higher quality
SMT system (LM ﬁlter), CL-b setting is slightly
better than CL-a.
The same three experiments were conducted for
the ES test set (last three rows of Table 3). We
achieved an accuracy of 81.1% in the monolingual case. Here the CL-b setting achieved a clearly
better accuracy than the CL-a setting (at least 5%
more), and only from 2.3% to 3.5% below the
monolingual one.
Thus with the higher quality
SMT system, it is always better to translate the test
data (CL-b setting) than the training corpus.
Comparing the SVM classiﬁcation accuracy in
the “LM Filter” and “No Fil” columns, we can see
the effect of introducing noise in the MT system.
We observe that the results were more affected by
the translation of the test (-2.2% and -0.8% accuracy) than the training set (+0.5% accuracy in both
cases). This agrees with the intuition than errors in
the test directly affect the results and thus may be
more harmful than in the training set, where they
may hardly affect the results if they represent infrequent examples.
Regarding use case II, setting c implies a translation of the analysis outcome. We can use our
method to translate the relevant opinionated units
with their predicted label in their test sentence
context, and extract the relevant information in the
outcome language. In setting d, the test is translated in the same way as in setting b.
Conclusions and Perspectives
We extended the possible CLSA settings to aspectlevel speciﬁc use cases. We proposed a method,
based on constrained SMT, to transfer opinionated
units across languages by preserving their boundaries. With this method, we built cross-language
sentiment classiﬁers achieving comparable results
to monolingual ones (from about 4 to 8% and 2.3
to 3.5% loss in accuracy depending on the language and machine learning algorithm). We observed that improving the MT quality had more
impact in settings using a translated test than a
translated training corpus. With the higher MT
quality system, we achieved better accuracy by
translating the test than the training corpus.
As future work, we plan to investigate the exact effect of the reordering constraints in terms of
possible translation model phrase pairs and target
language model n-grams which may not be used
depending on the constraint parameters, in order
to ﬁnd the best conﬁguration.
Acknowledgements
This work has received funding from the Seventh Framework Program of the European Commission through the Intra-European Fellowship
 Marie Curie Actions.
We also acknowledge partners of the
OpeNER project, in particular Montse Cuadros,
for providing us with the aspect-level annotated