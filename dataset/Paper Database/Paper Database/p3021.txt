SuperGlue: Learning Feature Matching with Graph Neural Networks
Paul-Edouard Sarlin1∗Daniel DeTone2
Tomasz Malisiewicz2
Andrew Rabinovich2
1 ETH Zurich
2 Magic Leap, Inc.
This paper introduces SuperGlue, a neural network that
matches two sets of local features by jointly ﬁnding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal
transport problem, whose costs are predicted by a graph
neural network. We introduce a ﬂexible context aggregation
mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments
Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end
training from image pairs. SuperGlue outperforms other
learned approaches and achieves state-of-the-art results on
the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can
be readily integrated into modern SfM or SLAM systems.
The code and trained weights are publicly available at
github.com/magicleap/SuperGluePretrainedNetwork.
1. Introduction
Correspondences between points in images are essential
for estimating the 3D structure and camera poses in geometric computer vision tasks such as Simultaneous Localization and Mapping (SLAM) and Structure-from-Motion
(SfM). Such correspondences are generally estimated by
matching local features, a process known as data association. Large viewpoint and lighting changes, occlusion, blur,
and lack of texture are factors that make 2D-to-2D data association particularly challenging.
In this paper, we present a new way of thinking about the
feature matching problem. Instead of learning better taskagnostic local features followed by simple matching heuristics and tricks, we propose to learn the matching process
from pre-existing local features using a novel neural architecture called SuperGlue. In the context of SLAM, which
typically decomposes the problem into the visual feature extraction front-end and the bundle adjustment or pose
estimation back-end, our network lies directly in the middle
– SuperGlue is a learnable middle-end (see Figure 1).
Super Glue
Detector & Descriptor
Deep Front-End
Back-End Optimizer
Deep Middle-End Matcher
Figure 1: Feature matching with SuperGlue. Our approach establishes pointwise correspondences from off-theshelf local features: it acts as a middle-end between handcrafted or learned front-end and back-end. SuperGlue uses a
graph neural network and attention to solve an assignment
optimization problem, and handles partial point visibility
and occlusion elegantly, producing a partial assignment.
In this work, learning feature matching is viewed as
ﬁnding the partial assignment between two sets of local
features. We revisit the classical graph-based strategy of
matching by solving a linear assignment problem, which,
when relaxed to an optimal transport problem, can be solved
differentiably. The cost function of this optimization is predicted by a Graph Neural Network (GNN). Inspired by the
success of the Transformer , it uses self- (intra-image)
and cross- (inter-image) attention to leverage both spatial
relationships of the keypoints and their visual appearance.
This formulation enforces the assignment structure of the
predictions while enabling the cost to learn complex priors, elegantly handling occlusion and non-repeatable keypoints. Our method is trained end-to-end from image pairs
– we learn priors for pose estimation from a large annotated
dataset, enabling SuperGlue to reason about the 3D scene
and the assignment. Our work can be applied to a variety of
multiple-view geometry problems that require high-quality
feature correspondences (see Figure 2).
∗Work done at Magic Leap, Inc. for a Master’s degree. The author thanks
his academic supervisors: Cesar Cadena, Marcin Dymczyk, Juan Nieto.
 
inliers: 59/68
scene0743_00/frame-000000
scene0743_00/frame-001275
inliers: 81/85
scene0744_00/frame-000585
scene0744_00/frame-002310
Figure 2: SuperGlue correspondences.
For these two
challenging indoor image pairs, matching with SuperGlue
results in accurate poses while other learned or handcrafted
methods fail (correspondences colored by epipolar error).
We show the superiority of SuperGlue compared to both
handcrafted matchers and learned inlier classiﬁers. When
combined with SuperPoint , a deep front-end, Super-
Glue advances the state-of-the-art on the tasks of indoor and
outdoor pose estimation and paves the way towards end-toend deep SLAM.
2. Related work
Local feature matching is generally performed by i) detecting interest points, ii) computing visual descriptors,
iii) matching these with a Nearest Neighbor (NN) search,
iv) ﬁltering incorrect matches, and ﬁnally v) estimating a
geometric transformation. The classical pipeline developed
in the 2000s is often based on SIFT , ﬁlters matches
with Lowe’s ratio test , the mutual check, and heuristics
such as neighborhood consensus , and ﬁnds a
transformation with a robust solver like RANSAC .
Recent works on deep learning for matching often focus on learning better sparse detectors and local descriptors from data using Convolutional Neural Networks (CNNs). To improve their discriminativeness,
some works explicitly look at a wider context using regional
features or log-polar patches . Other approaches
learn to ﬁlter matches by classifying them into inliers and
outliers . These operate on sets of matches,
still estimated by NN search, and thus ignore the assignment
structure and discard visual information. Works that learn
to perform matching have so far focused on dense matching or 3D point clouds , and still exhibit the same
limitations. In contrast, our learnable middle-end simultaneously performs context aggregation, matching, and ﬁltering in a single end-to-end architecture.
Graph matching
problems are usually formulated as
quadratic assignment problems, which are NP-hard, requiring expensive, complex, and thus impractical solvers .
For local features, the computer vision literature of the
2000s uses handcrafted costs with many heuristics, making it complex and brittle. Caetano et al. learn
the cost of the optimization for a simpler linear assignment,
but only use a shallow model, while our SuperGlue learns a
ﬂexible cost using a deep neural network. Related to graph
matching is the problem of optimal transport – it is a
generalized linear assignment with an efﬁcient yet simple
approximate solution, the Sinkhorn algorithm .
Deep learning for sets such as point clouds aims at designing permutation equi- or invariant functions by aggregating information across elements. Some works treat all
elements equally, through global pooling or instance normalization , while others focus on a
local neighborhood in coordinate or feature space .
Attention can perform both global and datadependent local aggregation by focusing on speciﬁc elements and attributes, and is thus more ﬂexible. By observing that self-attention can be seen as an instance of a Message Passing Graph Neural Network on a complete
graph, we apply attention to graphs with multiple types of
edges, similar to , and enable SuperGlue to learn
complex reasoning about the two sets of local features.
3. The SuperGlue Architecture
Motivation: In the image matching problem, some regularities of the world could be leveraged: the 3D world is
largely smooth and sometimes planar, all correspondences
for a given image pair derive from a single epipolar transform if the scene is static, and some poses are more likely
than others. In addition, 2D keypoints are usually projections of salient 3D points, like corners or blobs, thus correspondences across images must adhere to certain physical
constraints: i) a keypoint can have at most a single correspondence in the other image; and ii) some keypoints will
be unmatched due to occlusion and failure of the detector.
An effective model for feature matching should aim at ﬁnding all correspondences between reprojections of the same
3D points and identifying keypoints that have no matches.
We formulate SuperGlue (see Figure 3) as solving an optimization problem, whose cost is predicted by a deep neural
network. This alleviates the need for domain expertise and
heuristics – we learn relevant priors directly from the data.
Formulation: Consider two images A and B, each with a
set of keypoint positions p and associated visual descriptors
d – we refer to them jointly (p, d) as the local features.
Positions consist of x and y image coordinates as well as a
detection conﬁdence c, pi := (x, y, c)i. Visual descriptors
di ∈RD can be those extracted by a CNN like SuperPoint
Attentional Graph Neural Network
score matrix
Attentional Aggregation
Optimal Matching Layer
assignment
visual descriptor
descriptors
Sinkhorn Algorithm
normalization
Figure 3: The SuperGlue architecture. SuperGlue is made up of two major components: the attentional graph neural
network (Section 3.1), and the optimal matching layer (Section 3.2). The ﬁrst component uses a keypoint encoder to map
keypoint positions p and their visual descriptors d into a single vector, and then uses alternating self- and cross-attention
layers (repeated L times) to create more powerful representations f. The optimal matching layer creates an M by N score
matrix, augments it with dustbins, then ﬁnds the optimal partial assignment using the Sinkhorn algorithm (for T iterations).
or traditional descriptors like SIFT. Images A and B have
M and N local features, indexed by A := {1, ..., M} and
B := {1, ..., N}, respectively.
Partial Assignment: Constraints i) and ii) mean that correspondences derive from a partial assignment between the
two sets of keypoints. For the integration into downstream
tasks and better interpretability, each possible correspondence should have a conﬁdence value. We consequently
deﬁne a partial soft assignment matrix P ∈ M×N as:
Our goal is to design a neural network that predicts the assignment P from two sets of local features.
3.1. Attentional Graph Neural Network
Besides the position of a keypoint and its visual appearance, integrating other contextual cues can intuitively increase its distinctiveness. We can for example consider its
spatial and visual relationship with other co-visible keypoints, such as ones that are salient , self-similar ,
statistically co-occurring , or adjacent .
other hand, knowledge of keypoints in the second image
can help to resolve ambiguities by comparing candidate
matches or estimating the relative photometric or geometric transformation from global and unambiguous cues.
When asked to match a given ambiguous keypoint, humans look back-and-forth at both images: they sift through
tentative matching keypoints, examine each, and look for
contextual cues that help disambiguate the true match from
other self-similarities . This hints at an iterative process
that can focus its attention on speciﬁc locations.
We consequently design the ﬁrst major block of Super-
Glue as an Attentional Graph Neural Network (see Figure 3). Given initial local features, it computes matching
descriptors fi ∈RD by letting the features communicate
with each other. As we will show, long-range feature aggregation within and across images is vital for robust matching.
Keypoint Encoder:
The initial representation (0)xi for
each keypoint i combines its visual appearance and location.
We embed the keypoint position into a highdimensional vector with a Multilayer Perceptron (MLP) as:
(0)xi = di + MLPenc (pi) .
This encoder enables the graph network to later reason
about both appearance and position jointly, especially when
combined with attention, and is an instance of the “positional encoder” popular in language processing .
Multiplex Graph Neural Network: We consider a single
complete graph whose nodes are the keypoints of both images. The graph has two types of undirected edges – it is a
multiplex graph . Intra-image edges, or self edges,
Eself, connect keypoints i to all other keypoints within the
same image. Inter-image edges, or cross edges, Ecross, connect keypoints i to all keypoints in the other image. We use
the message passing formulation to propagate information along both types of edges. The resulting multiplex
Graph Neural Network starts with a high-dimensional state
for each node and computes at each layer an updated representation by simultaneously aggregating messages across
all given edges for all nodes.
i be the intermediate representation for element
i in image A at layer ℓ. The message mE→i is the result of
the aggregation from all keypoints {j : (i, j) ∈E}, where
E ∈{Eself, Ecross}. The residual message passing update for
all i in A is:
where [· || ·] denotes concatenation. A similar update can
be simultaneously performed for all keypoints in image B.
A ﬁxed number of layers L with different parameters are
chained and alternatively aggregate along the self and cross
edges. As such, starting from ℓ= 1, E = Eself if ℓis odd
and E = Ecross if ℓis even.
Self-Attention
Self-Attention
Cross-Attention
Cross-Attention
Figure 4: Visualizing self- and cross-attention. Attentional aggregation builds a dynamic graph between keypoints. Weights αij are shown as rays. Self-attention (top)
can attend anywhere in the same image, e.g. distinctive locations, and is thus not restricted to nearby locations. Crossattention (bottom) attends to locations in the other image,
such as potential matches that have a similar appearance.
Attentional Aggregation:
An attention mechanism performs the aggregation and computes the message mE→i.
Self edges are based on self-attention and cross edges
are based on cross-attention. Akin to database retrieval, a
representation of i, the query qi, retrieves the values vj of
some elements based on their attributes, the keys kj. The
message is computed as a weighted average of the values:
where the attention weight αij is the Softmax over the keyquery similarities: αij = Softmaxj
The key, query, and value are computed as linear projections of deep features of the graph neural network. Considering that query keypoint i is in the image Q and all source
keypoints are in image S, (Q, S) ∈{A, B}2, we can write:
Each layer ℓhas its own projection parameters, learned and
shared for all keypoints of both images. In practice, we
improve the expressivity with multi-head attention .
Our formulation provides maximum ﬂexibility as the
network can learn to focus on a subset of keypoints based
on speciﬁc attributes (see Figure 4). SuperGlue can retrieve
or attend based on both appearance and keypoint location
as they are encoded in the representation xi. This includes
attending to a nearby keypoint and retrieving the relative
positions of similar or salient keypoints. This enables representations of the geometric transformation and the assignment. The ﬁnal matching descriptors are linear projections:
i = W · (L)xA
and similarly for keypoints in B.
3.2. Optimal matching layer
The second major block of SuperGlue (see Figure 3) is
the optimal matching layer, which produces a partial assignment matrix. As in the standard graph matching formulation, the assignment P can be obtained by computing a
score matrix S ∈RM×N for all possible matches and maximizing the total score P
i,j Si,jPi,j under the constraints
in Equation 1. This is equivalent to solving a linear assignment problem.
Score Prediction:
Building a separate representation for
all M × N potential matches would be prohibitive. We instead express the pairwise score as the similarity of matching descriptors:
Si,j =< f A
j >, ∀(i, j) ∈A × B,
where < ·, · > is the inner product. As opposed to learned
visual descriptors, the matching descriptors are not normalized, and their magnitude can change per feature and during
training to reﬂect the prediction conﬁdence.
Occlusion and Visibility:
To let the network suppress
some keypoints, we augment each set with a dustbin so that
unmatched keypoints are explicitly assigned to it. This technique is common in graph matching, and dustbins have also
been used by SuperPoint to account for image cells that
might not have a detection. We augment the scores S to ¯S
by appending a new row and column, the point-to-bin and
bin-to-bin scores, ﬁlled with a single learnable parameter:
¯Si,N+1 = ¯SM+1,j = ¯SM+1,N+1 = z ∈R.
While keypoints in A will be assigned to a single keypoint
in B or the dustbin, each dustbin has as many matches as
there are keypoints in the other set: N, M for dustbins
in A, B respectively. We denote as a =
⊤the number of expected matches for each
keypoint and dustbin in A and B. The augmented assignment ¯P now has the constraints:
¯P1N+1 = a
¯P⊤1M+1 = b.
Sinkhorn Algorithm: The solution of the above optimization problem corresponds to the optimal transport between discrete distributions a and b with scores ¯S.
entropy-regularized formulation naturally results in the desired soft assignment, and can be efﬁciently solved on GPU
with the Sinkhorn algorithm . It is a differentiable
version of the Hungarian algorithm , classically used
for bipartite matching, that consists in iteratively normalizing exp(¯S) along rows and columns, similar to row and
column Softmax. After T iterations, we drop the dustbins
and recover P = ¯P1:M,1:N.
By design, both the graph neural network and the optimal matching layer are differentiable – this enables backpropagation from matches to visual descriptors. SuperGlue
is trained in a supervised manner from ground truth matches
M = {(i, j)} ⊂A × B. These are estimated from ground
truth relative transformations – using poses and depth maps
or homographies. This also lets us label some keypoints
I ⊆A and J ⊆B as unmatched if they do not have any reprojection in their vicinity. Given these labels, we minimize
the negative log-likelihood of the assignment ¯P:
log ¯Pi,N+1 −
log ¯PM+1,j.
This supervision aims at simultaneously maximizing the
precision and the recall of the matching.
3.4. Comparisons to related work
The SuperGlue architecture is equivariant to permutation
of the keypoints within an image. Unlike other handcrafted
or learned approaches, it is also equivariant to permutation
of the images, which better reﬂects the symmetry of the
problem and provides a beneﬁcial inductive bias. Additionally, the optimal transport formulation enforces reciprocity
of the matches, like the mutual check, but in a soft manner,
similar to , thus embedding it into the training process.
SuperGlue vs. Instance Normalization :
Attention,
as used by SuperGlue, is a more ﬂexible and powerful context aggregation mechanism than instance normalization,
which treats all keypoints equally, as used by previous work
on feature matching .
SuperGlue vs. ContextDesc : SuperGlue can jointly
reason about appearance and position while ContextDesc
processes them separately.
Moreover, ContextDesc is a
front-end that additionally requires a larger regional extractor, and a loss for keypoints scoring. SuperGlue only needs
local features, learned or handcrafted, and can thus be a simple drop-in replacement for existing matchers.
SuperGlue vs. Transformer : SuperGlue borrows the
self-attention from the Transformer, but embeds it into a
graph neural network, and additionally introduces the crossattention, which is symmetric. This simpliﬁes the architecture and results in better feature reuse across layers.
4. Implementation details
SuperGlue can be combined with any local feature detector and descriptor but works particularly well with Super-
Point , which produces repeatable and sparse keypoints
– enabling very efﬁcient matching. Visual descriptors are
bilinearly sampled from the semi-dense feature map. For
a fair comparison to other matchers, unless explicitly mentioned, we do not train the visual descriptor network when
training SuperGlue. At test time, one can use a conﬁdence
threshold (we choose 0.2) to retain some matches from the
soft assignment, or use all of them and their conﬁdence in a
subsequent step, such as weighted pose estimation.
Architecture details:
All intermediate representations
(key, query value, descriptors) have the same dimension
D = 256 as the SuperPoint descriptors. We use L = 9 layers of alternating multi-head self- and cross-attention with
4 heads each, and perform T = 100 Sinkhorn iterations.
The model is implemented in PyTorch , contains 12M
parameters, and runs in real-time on an NVIDIA GTX 1080
GPU: a forward pass takes on average 69 ms (15 FPS) for
an indoor image pair (see Appendix C).
Training details: To allow for data augmentation, Super-
Point detect and describe steps are performed on-the-ﬂy as
batches during training. A number of random keypoints are
further added for efﬁcient batching and increased robustness. More details are provided in Appendix E.
5. Experiments
5.1. Homography estimation
We perform a large-scale homography estimation experiment using real images and synthetic homographies with
both robust (RANSAC) and non-robust (DLT) estimators.
Dataset: We generate image pairs by sampling random homographies and applying random photometric distortions to
real images, following a recipe similar to .
The underlying images come from the set of 1M distractor images in the Oxford and Paris dataset , split into
training, validation, and test sets.
Homography estimation AUC
SuperPoint
NN + mutual
NN + PointCN
NN + OANet
Table 1: Homography estimation. SuperGlue recovers almost all possible matches while suppressing most outliers.
Because SuperGlue correspondences are high-quality, the
Direct Linear Transform (DLT), a least-squares based solution with no robustness mechanism, outperforms RANSAC.
Baselines: We compare SuperGlue against several matchers applied to SuperPoint local features – the Nearest Neighbor (NN) matcher and various outlier rejectors: the mutual
NN constraint, PointCN , and Order-Aware Network
(OANet) . All learned methods, including SuperGlue,
are trained on ground truth correspondences, found by projecting keypoints from one image to the other. We generate
homographies and photometric distortions on-the-ﬂy – an
image pair is never seen twice during training.
Metrics: Match precision (P) and recall (R) are computed
from the ground truth correspondences. Homography estimation is performed with both RANSAC and the Direct
Linear Transformation (DLT), which has a direct leastsquares solution. We compute the mean reprojection error
of the four corners of the image and report the area under
the cumulative error curve (AUC) up to a value of 10 pixels.
Results: SuperGlue is sufﬁciently expressive to master homographies, achieving 98% recall and high precision (see
Table 1). The estimated correspondences are so good that
a robust estimator is not required – SuperGlue works even
better with DLT than RANSAC. Outlier rejection methods like PointCN and OANet cannot predict more correct
matches than the NN matcher itself, overly relying on the
initial descriptors (see Figure 6 and Appendix A).
5.2. Indoor pose estimation
Indoor image matching is very challenging due to the
lack of texture, the abundance of self-similarities, the complex 3D geometry of scenes, and large viewpoint changes.
As we show in the following, SuperGlue can effectively
learn priors to overcome these challenges.
Dataset: We use ScanNet , a large-scale indoor dataset
composed of monocular sequences with ground truth poses
and depth images, and well-deﬁned training, validation, and
test splits corresponding to different scenes. Previous works
select training and evaluation pairs based on time difference or SfM covisibility , usually computed using SIFT. We argue that this limits the difﬁculty of
the pairs, and instead select these based on an overlap score
computed for all possible image pairs in a given sequence
using only ground truth poses and depth. This results in
signiﬁcantly wider-baseline pairs, which corresponds to the
current frontier for real-world indoor image matching. Discarding pairs with too small or too large overlap, we select
230M training and 1500 test pairs.
As in previous work , we report
the AUC of the pose error at the thresholds (5◦, 10◦, 20◦),
where the pose error is the maximum of the angular errors
in rotation and translation. Relative poses are obtained from
essential matrix estimation with RANSAC. We also report
the match precision and the matching score , where
a match is deemed correct based on its epipolar distance.
Pose estimation AUC
NN + mutual
ContextDesc
NN + ratio test
NN + ratio test
NN + NG-RANSAC
NN + OANet
SuperPoint
NN + mutual
NN + distance + mutual
NN + PointCN
NN + OANet
Table 2: Wide-baseline indoor pose estimation. We report the AUC of the pose error, the matching score (MS)
and precision (P), all in percents %. SuperGlue outperforms
all handcrafted and learned matchers when applied to both
SIFT and SuperPoint.
SIFT + NN + ratio test
SIFT + NN + OANet
SIFT + SuperGlue
SuperPoint + NN + mutual
SuperPoint + NN + OANet
SuperPoint + SuperGlue
AUC@20° (%)
Figure 5: Indoor and outdoor pose estimation. Super-
Glue works with SIFT or SuperPoint local features and consistently improves by a large margin the pose accuracy over
OANet, a state-of-the-art outlier rejection neural network.
Baselines:
We evaluate SuperGlue and various baseline
matchers using both root-normalized SIFT and SuperPoint features. SuperGlue is trained with correspondences and unmatched keypoints derived from ground truth
poses and depth. All baselines are based on the Nearest
Neighbor (NN) matcher and potentially an outlier rejection method. In the “Handcrafted” category, we consider
the mutual check, the ratio test , thresholding by descriptor distance, and the more complex GMS . Methods in the “Learned” category are PointCN , and its
follow-ups OANet and NG-RANSAC . We retrain
PointCN and OANet on ScanNet for both SuperPoint and
SIFT with the classiﬁcation loss using the above-deﬁned
correctness criterion and their respective regression losses.
For NG-RANSAC, we use the original trained model. We
do not include any graph matching methods as they are orders of magnitude too slow for the number of keypoints that
we consider (>500). Other local features are evaluated as
reference: ORB with GMS, D2-Net , and ContextDesc using the publicly available trained models.
Results: SuperGlue enables signiﬁcantly higher pose accuracy compared to both handcrafted and learned matchers
(see Table 2 and Figure 5), and works well with both SIFT
and SuperPoint. It has a signiﬁcantly higher precision than
other learned matchers, demonstrating its higher representation power. It also produces a larger number of correct
matches – up to 10 times more than the ratio test when applied to SIFT, because it operates on the full set of possible
matches, rather than the limited set of nearest neighbors.
SuperGlue with SuperPoint achieves state-of-the-art results
on indoor pose estimation. They complement each other
well since repeatable keypoints make it possible to estimate
a larger number of correct matches even in very challenging
situations (see Figure 2, Figure 6, and Appendix A).
5.3. Outdoor pose estimation
As outdoor image sequences present their own set of
challenges (e.g., lighting changes and occlusion), we train
and evaluate SuperGlue for pose estimation in an outdoor
setting. We use the same evaluation metrics and baseline
methods as in the indoor pose estimation task.
Dataset: We evaluate on the PhotoTourism dataset, which
is part of the CVPR’19 Image Matching Challenge . It
is a subset of the YFCC100M dataset and has ground
truth poses and sparse 3D models obtained from an off-theshelf SfM tool . All learned methods are trained
on the larger MegaDepth dataset , which also has depth
maps computed with multi-view stereo. Scenes that are in
the PhotoTourism test set are removed from the training set.
Similarly as in the indoor case, we select challenging image pairs for training and evaluation using an overlap score
computed from the SfM covisibility as in .
Results: As shown in Table 3, SuperGlue outperforms all
baselines, at all relative pose thresholds, when applied to
both SuperPoint and SIFT. Most notably, the precision of
the resulting matching is very high (84.9%), reinforcing the
analogy that SuperGlue “glues” together local features.
Pose estimation AUC
ContextDesc
NN + ratio test
NN + ratio test
NN + NG-RANSAC
NN + OANet
SuperPoint
NN + mutual
NN + OANet
Table 3: Outdoor pose estimation. Matching SuperPoint
and SIFT features with SuperGlue results in signiﬁcantly
higher pose accuracy (AUC), precision (P), and matching
score (MS) than with handcrafted or other learned methods.
NN + mutual
No Graph Neural Net
No cross-attention
No positional encoding
Smaller (3 layers)
Full (9 layers)
Table 4: Ablation of SuperGlue. While the optimal matching layer alone improves over the baseline Nearest Neighbor matcher, the Graph Neural Network explains the majority of the gains brought by SuperGlue. Both cross-attention
and positional encoding are critical for strong gluing, and a
deeper network further improves the precision.
5.4. Understanding SuperGlue
Ablation study: To evaluate our design decisions, we repeat the indoor experiments with SuperPoint features, but
this time focusing on different SuperGlue variants. This ablation study, presented in Table 4, shows that all SuperGlue
blocks are useful and bring substantial performance gains.
When we additionally backpropagate through the Super-
Point descriptor network while training SuperGlue, we observe an improvement in AUC@20◦from 51.84 to 53.38.
This conﬁrms that SuperGlue is suitable for end-to-end
learning beyond matching.
Visualizing Attention: The extensive diversity of self- and
cross-attention patterns is shown in Figure 7 and reﬂects the
complexity of the learned behavior. A detailed analysis of
the trends and inner-workings is performed in Appendix D.
6. Conclusion
This paper demonstrates the power of attention-based
graph neural networks for local feature matching. Super-
Glue’s architecture uses two kinds of attention: (i) self-attention, which boosts the receptive ﬁeld of local descriptors,
and (ii) cross-attention, which enables cross-image communication and is inspired by the way humans look back-
-and-forth when matching images. Our method elegantly
handles partial assignments and occluded points by solving
an optimal transport problem. Our experiments show that
SuperGlue achieves signiﬁcant improvement over existing
approaches, enabling highly accurate relative pose estimation on extreme wide-baseline indoor and outdoor image
pairs. In addition, SuperGlue runs in real-time and works
well with both classical and learned features.
In summary, our learnable middle-end replaces handcrafted heuristics with a powerful neural model that simultaneously performs context aggregation, matching, and ﬁltering in a single uniﬁed architecture. We believe that, when
combined with a deep front-end, SuperGlue is a major milestone towards end-to-end deep SLAM.
SuperPoint + NN + distance threshold
SuperPoint + NN + OANet
SuperPoint + SuperGlue
NN+distance
inliers: 9/23
scene0711_00/frame-001680
scene0711_00/frame-001995
NN+distance
inliers: 1/15
scene0768_00/frame-001095
scene0768_00/frame-003435
NN+distance
inliers: 3/21
scene0755_00/frame-000120
scene0755_00/frame-002055
NN+distance
inliers: 7/26
scene0713_00/frame-001605
scene0713_00/frame-001680
inliers: 55/95
scene0711_00/frame-001680
scene0711_00/frame-001995
inliers: 0/27
scene0768_00/frame-001095
scene0768_00/frame-003435
inliers: 17/60
scene0755_00/frame-000120
scene0755_00/frame-002055
inliers: 37/165
scene0713_00/frame-001605
scene0713_00/frame-001680
inliers: 109/115
scene0711_00/frame-001680
scene0711_00/frame-001995
inliers: 41/44
scene0768_00/frame-001095
scene0768_00/frame-003435
inliers: 60/74
scene0755_00/frame-000120
scene0755_00/frame-002055
inliers: 169/180
scene0713_00/frame-001605
scene0713_00/frame-001680
NN+distance
inliers: 19/137
0217/159485210_fc63a49e68_o.jpg
0217/2919662679_7445df830b_o.jpg
NN+distance
inliers: 43/170
piazza_san_marco/58751010_4849458397
piazza_san_marco/18627786_5929294590
inliers: 70/177
0217/159485210_fc63a49e68_o.jpg
0217/2919662679_7445df830b_o.jpg
inliers: 99/484
piazza_san_marco/58751010_4849458397
piazza_san_marco/18627786_5929294590
inliers: 350/353
0217/159485210_fc63a49e68_o.jpg
0217/2919662679_7445df830b_o.jpg
inliers: 275/276
piazza_san_marco/58751010_4849458397
piazza_san_marco/18627786_5929294590
Homography
NN+distance
H: 338.7px
2867c6274d259fc81d6cf08a43d225
H: 161.8px
2867c6274d259fc81d6cf08a43d225
2867c6274d259fc81d6cf08a43d225
Figure 6: Qualitative image matches. We compare SuperGlue to the Nearest Neighbor (NN) matcher with two outlier
rejectors, handcrafted and learned, in three environments. SuperGlue consistently estimates more correct matches (green
lines) and fewer mismatches (red lines), successfully coping with repeated texture, large viewpoint, and illumination changes.
Figure 7: Visualizing attention. We show self- and cross-attention weights αij at various layers and heads. SuperGlue exhibits a diversity of patterns: it can focus on global or local context, self-similarities, distinctive features, or match candidates.
In the following pages, we present additional experimental details, quantitative results, qualitative examples of SuperGlue in action, detailed timing results, as well as visualizations and analysis of the learned attention patterns.
A. Detailed results
A.1. Homography estimation
Qualitative results:
A full page of qualitative results of
SuperGlue matching on synthetic and real homographies
can be seen in Figure 13.
Synthetic dataset: We take a more detailed look at the homography evaluation from Section 5.1. Figure 8 shows the
match precision at several correctness pixel thresholds and
the cumulative error curve of homography estimation. SuperGlue dominates across all pixel correctness thresholds.
NN + RANSAC
NN + PointCN + DLT
NN + OANet + DLT
SuperGlue + DLT
NN + Mutual + RANSAC
NN + PointCN + RANSAC
NN + OANet + RANSAC
SuperGlue + RANSAC
Precision (%)
Correct homographies (%)
Correctness threshold (px)
Estimation error (px)
Match precision
Homography accuracy
Figure 8: Details of the homography evaluation. Super-
Glue exhibits higher precision and homography accuracy at
all thresholds. High precision results in more accurate estimation with DLT than with RANSAC.
Illumination
SuperPoint
NN + mutual
NN + PointCN
NN + OANet
Table 5: Generalization to real data. We show the precision (P) and recall (R) of the methods trained on our synthetic homography dataset (see Section 5.1) on the viewpoint and illumination subsets of the HPatches dataset.
While trained on synthetic homographies, SuperGlue generalizes well to real data.
HPatches: We assess the generalization ability of Super-
Glue on real data with the HPatches dataset, as done in
previous works . This dataset depicts planar scenes
with ground truth homographies and contains 295 image
pairs with viewpoint changes and 285 pairs with illumination changes. We evaluate the models trained on the synthetic dataset (see Section 5.1). The HPatches experiment
is summarized in Table 5. As previously observed in the
synthetic homography experiments, SuperGlue has significantly higher recall than all matchers relying on the NN
We attribute the remaining gap in recall to several challenging pairs for which SuperPoint does not detect enough repeatable keypoints. Nevertheless, syntheticdataset trained SuperGlue generalizes well to real data.
A.2. Indoor pose estimation
Qualitative results: More visualizations of matches computed by SuperGlue on indoor images are shown in Figure 14, and highlight the extreme difﬁculty of the widebaseline image pairs that constitute our evaluation dataset.
ScanNet: We present more details regarding the results on
ScanNet (Section 5.2), only analyzing the methods which
use SuperPoint local features. Figure 9 plots the cumulative
pose estimation error curve and the trade-off between precision and number of correct matches. We compute the correctness from the reprojection error (using the ground truth
depth and a threshold of 10 pixels), and, for keypoints with
invalid depth, from the symmetric epipolar error. We obtain
curves by varying the conﬁdence thresholds of PointCN,
OANet, and SuperGlue. At evaluation, we use the original
value 0.5 for the former two, and 0.2 for SuperGlue.
NN + mutual
NN + distance + mutual
NN + PointCN
NN + OANet
Correct poses (%)
Precision (%)
Estimation error (deg)
Number of correct matches
Pose accuracy
Match precision
Figure 9: Details of the ScanNet evaluation. Poses estimated with SuperGlue are more accurate at all error thresholds. SuperGlue offers the best trade-off between precision
and number of correct matches, which are both critical for
accurate and robust pose estimation.
Approx. AUC 
ContextDesc
NN + ratio test
NN + ratio test
NN + OANet*
NN + OANet
SuperPoint
NN + mutual
NN + OANet
Table 6: Outdoor pose estimation on YFCC100M pairs.
The evaluation is performed on the same image pairs as
in OANet using both their approximate and our exact
AUC. SuperGlue consistently improves over the baselines
when using either SIFT and SuperPoint.
A.3. Outdoor pose estimation
Qualitative results: Figure 15 shows additional results on
the Phototourism test set and the MegaDepth validation set.
YFCC100M: While the PhotoTourism and Zhang et
al.’s test sets are both based on YFCC100M , they
use different scenes and pairs. For the sake of comparability, we also evaluate SuperGlue on the same evaluation
pairs as in OANet , using their evaluation metrics. We
include an OANet model (*) retrained on their training set
(instead of MegaDepth) using root-normalized SIFT. The
results are shown in Table 6.
As observed in Section 5.3 when evaluating on the PhotoTourism dataset, SuperGlue consistently improves over
all baselines for both SIFT and SuperPoint. For SIFT, the
improvement over OANet is decreased, which we attribute
to the signiﬁcantly higher overlap and lower difﬁculty of the
pairs used by . While the approximate AUC tends to
overestimate the accuracy, it results in an identical ranking
of the methods. The numbers for OANet with SIFT and SuperPoint are consistent with the ones reported in their paper.
Correctly localized queries (%)
# features
D2-Net 
UR2KID 
SuperPoint+NN+mutual
SuperPoint+SuperGlue
Table 7: Visual localization on Aachen Day-Night. Super-
Glue signiﬁcantly improves the performance of SuperPoint
for localization, reaching new state-of-the-art results with
comparably fewer keypoints.
B. SuperGlue for visual localization
Visual localization:
While two-view relative pose estimation is an important fundamental problem, advances in
image matching can directly beneﬁt practical tasks like visual localization , which aims at estimating the absolute pose of a query image with respect to a 3D model.
Moreover, real-world localization scenarios exhibit signiﬁcantly higher scene diversity and more challenging conditions, such as larger viewpoint and illumination changes,
than phototourism datasets of popular landmarks.
Evaluation:
The Aachen Day-Night benchmark 
evaluates local feature matching for day-night localization.
We extract up to 4096 keypoints per images with Super-
Point, match them with SuperGlue, triangulate an SfM
model from posed day-time database images, and register night-time query images with the 2D-2D matches and
COLMAP . The evaluation server1 computes the percentage of queries localized within several distance and
orientation thresholds.
As reported in Table 7, Super-
Point+SuperGlue performs similarly or better than all existing approaches despite using signiﬁcantly fewer keypoints.
Figure 10 shows challenging day-night image pairs.
1 
keypoints: 3713:4096
inliers: 446/502
query/night/nexus5x/IMG_20161227_191152.jpg
db/1736.jpg
keypoints: 2586:4096
inliers: 289/305
query/night/nexus5x/IMG_20161227_191819.jpg
db/2178.jpg
keypoints: 4096:3413
inliers: 596/648
query/night/nexus5x/IMG_20161227_173326.jpg
db/2307.jpg
keypoints: 4096:4096
inliers: 189/230
query/night/nexus5x/IMG_20161227_172811.jpg
db/1900.jpg
keypoints: 3001:4096
inliers: 612/642
query/night/nexus5x/IMG_20161227_191640.jpg
db/886.jpg
Figure 10: Matching challenging day-night pairs with SuperGlue. We show predicted correspondences between nighttime queries and day-time databases images of the Aachen Day-Night dataset. The correspondences are colored as RANSAC
inliers in green or outliers in red. Although the outdoor training set has few night images, SuperGlue generalizes well to such
extreme illumination changes. Moreover, it can accurately match building facades with repeated patterns like windows.
SuperGlue inference time
Graph Neural Network
Optimal Matching Layer
Number of keypoints per image
Figure 11: SuperGlue detailed inference time.
Glue’s two main blocks, the Graph Neural Network and the
Optimal Matching Layer, have similar computational costs.
For 512 and 1024 keypoints per image, SuperGlue runs at
14.5 and 11.5 FPS, respectively.
C. Timing and model parameters
We measure the run-time of SuperGlue and its
two major blocks, the Graph Neural Network and the Optimal Matching Layer, for different numbers of keypoints per
image. The measurements are performed on an NVIDIA
GeForce GTX 1080 GPU across 500 runs. See Figure 11.
Model Parameters:
The Keypoint Encoder MLP
has 5 layers, mapping positions to dimensions of size
(32, 64, 128, 256, D), yielding 100k parameters. Each layer
has the three projection matrices, and an extra WO to deal
with the multi-head output. The message update MLP has
2 layers and maps to dimensions (2D, D). Both MLPs use
BatchNorm and ReLUs. Each layer has 0.66M parameters.
SuperGlue has 18 layers, with a total of 12M parameters.
D. Analyzing attention
Quantitative analysis: We compute the spatial extent of
the attention weights – the attention span – for all layers
and all keypoints. The self-attention span corresponds to the
distance in pixel space between one keypoint i and all the
others j, weighted by the attention weight αij, and averaged
for all queries. The cross-attention span corresponds to the
average distance between the ﬁnal predicted match and all
the attended keypoints j. We average the spans over 100
ScanNet pairs and plot in Figure 12 the minimum across all
heads for each layer, with 95% conﬁdence intervals.
The spans of both self- and cross-attention tend to decrease throughout the layers, by more than a factor of 10 between the ﬁrst and the last layer. SuperGlue initially attends
to keypoints covering a large area of the image, and later
focuses on speciﬁc locations – the self-attention attends to
a small neighborhood around the keypoint, while the crossattention narrows its search to the vicinity of the true match.
Intermediate layers have oscillating spans, hinting at a more
complex process.
Average distance (px)
Average distance (px)
Layer index
Layer index
Self-Attention span
Cross-Attention span
Figure 12: Attention spans throughout SuperGlue. We
plot the attention span, a measure of the attention’s spatial
dispersion, vs. layer index. For both types of attention, the
span tends to decrease deeper in the network as SuperGlue
focuses on speciﬁc locations. See an example in Figure 16.
Qualitative example: We analyze the attention patterns of
a speciﬁc example in Figure 16. Our observations are consistent with the attention span trends reported in Figure 12.
E. Experimental details
In this section, we provide details on the training and
evaluation of SuperGlue.
The trained models and the
evaluation code and image pairs are publicly available at
github.com/magicleap/SuperGluePretrainedNetwork.
Choice of indoor dataset:
Previous works on inlier
classiﬁcation evaluate indoor pose estimation
on the SUN3D dataset . Camera poses in SUN3D are
estimated from SIFT-based sparse SfM, while ScanNet
leverages RGB-D fusion and optimization , resulting
in signiﬁcantly more accurate poses. This makes ScanNet
more suitable for generating accurate correspondence
labels and evaluating pose estimation.
We additionally
noticed that the SUN3D image pairs used by Zhang et
al. have generally small baseline and rotation angle.
This makes the essential matrix estimation degenerate 
and the angular translation error ill-deﬁned. In contrast,
our ScanNet wide-baseline pairs have signiﬁcantly more
diversity in baselines and rotation, and thus do not suffer
from the aforementioned issues.
Homography estimation – Section 5.1:
The test set
contains 1024 pairs of 640×480 images. Homographies
are generated by applying random perspective, scaling, rotation, and translation to the original full-sized images,
to avoid bordering artifacts.
We evaluate with the 512
top-scoring keypoints detected by SuperPoint with a Non-
Maximum Suppression (NMS) radius of 4 pixels. Correspondences are deemed correct if they have a reprojection
error lower than 3 pixels. We use the OpenCV function
findHomography with 3000 iterations and a RANSAC
inlier threshold of 3 pixels.
Indoor pose estimation – Section 5.2: The overlap score
between two images A and B is the average ratio of pixels
in A that are visible in B (and vice versa), after accounting for missing depth values and occlusion (by checking
for consistency in the depth). We train and evaluate with
pairs that have an overlap score in [0.4, 0.8].
For training, we sample at each epoch 200 pairs per scene, similarly as in .
The test set is generated by subsampling the sequences by 15 and subsequently randomly sampling 15 pairs for each of the 300 sequences. We resize
all ScanNet images and depth maps to 640×480. We detect up to 1024 SuperPoint keypoints (using the publicly
available trained model2 with NMS radius of 4) and 2048
SIFT keypoints (using OpenCV’s implementation). Poses
are computed by ﬁrst estimating the essential matrix with
OpenCV’s findEssentialMat and RANSAC with an
inlier threshold of 1 pixel divided by the focal length,
followed by recoverPose.
In contrast with previous
works , we compute a more accurate AUC using explicit integration rather than coarse histograms. The
precision (P) is the average ratio of the number of correct
matches over the total number of estimated matches. The
matching score (MS) is the average ratio of the number of
correct matches over the total number of detected keypoints.
It does not account for the pair overlap and decreases with
the number of covisible keypoints. A match is deemed correct if its epipolar distance is lower than 5 · 10−4.
Outdoor pose estimation – Section 5.3: For training on
Megadepth, the overlap score is the ratio of triangulated
keypoints that are visible in the two images, as in .
We sample pairs with an overlap score in [0.1, 0.7] at each
epoch. We evaluate on all 11 scenes of the PhotoTourism
dataset and reuse the overlap score based on bounding
boxes computed by Ono et al. , with a selection range
of [0.1, 0.4]. Images are resized so that their longest dimension is equal to 1600 pixels and rotated upright using their
EXIF data. We detect 2048 keypoints for both SIFT and SuperPoint (with an NMS radius of 3). The epipolar correctness threshold is here 10−4. Other evaluation parameters
are identical to the ones used for the indoor evaluation.
Training of SuperGlue:
For training on homography/indoor/outdoor data, we use the Adam optimizer 
with a constant leaning rate of 10−4
for the ﬁrst
200k/100k/50k iterations, followed by an exponential decay of 0.999998/0.999992/0.999992 until iteration 900k.
When using SuperPoint features, we employ batches with
32/64/16 image pairs and a ﬁxed number of 512/400/1024
keypoints per image. For SIFT features we use 1024 keypoints and 24 pairs. Due to the limited number of training
scenes, the outdoor model weights are initialized with the
homography model weights. Before the keypoint encoder,
2github.com/magicleap/SuperPointPretrainedNetwork
the keypoints are normalized by the largest dimension of the
Ground truth correspondences M and unmatched sets
I and J are generated by ﬁrst computing the M × N reprojection matrix between all detected keypoints using the
ground truth homography or pose and depth. Correspondences are entries with a reprojection error that is a minimum along both rows and columns, and that is lower than
a given threshold: 3, 5, and 3 pixels for homographies,
indoor, and outdoor matching respectively. For homographies, unmatched keypoints are simply the ones that do not
appear in M. For indoor and outdoor matching, because
of errors in the pose and depth, unmatched keypoints must
additionally have a minimum reprojection error larger than
15 and 5 pixels, respectively. This allows us to ignore labels
for keypoints whose correspondences are ambiguous, while
still providing some supervision through the normalization
induced by the Sinkhorn algorithm.
Ablation study – Section 5.4: The “No Graph Neural Net”
baseline replaces the Graph Neural Network with a single
linear projection, but retains the Keypoint Encoder and the
Optimal Matching Layer. The “No cross-attention” baseline
replace all cross-attention layers by self-attention: it has the
same number of parameters as the full model, and acts like
a Siamese network. The “No positional encoding” baseline
simply removes the Keypoint Encoder and only uses the visual descriptors as input.
End-to-end training – Section 5.4: Two copies of Super-
Point, for detection and description, are initialized with the
original weights. The detection network is frozen and gradients are propagated through the descriptor network only,
ﬂowing from SuperGlue - no additional losses are used.
SuperPoint + NN + distance
SuperPoint + NN + OANet
SuperPoint + SuperGlue
NN+distance
H: 2940.6px
152e84d06c18954f28ea6ff7ab3e59e
NN+distance
H: 407.4px
2544360bcc5a74a8db8ac9b57237d8b
NN+distance
H: 216.7px
2ffdde83e28cc6babada95eee4c112
NN+distance
H: 419.1px
2d7fb16b2e614b188abdfae5be9cde4
NN+distance
H: 4545.0px
2d245d76fd62b05b5e12bccb3e6c36ea
NN+distance
H: 174.3px
17cb2ca5b1a62662fc29712d33aa4c
NN+distance
H: 235.2px
2f6246c13582a1496eb5ceb59eca8b66
H: 458.1px
152e84d06c18954f28ea6ff7ab3e59e
2544360bcc5a74a8db8ac9b57237d8b
H: 196.7px
2ffdde83e28cc6babada95eee4c112
H: 230.7px
2d7fb16b2e614b188abdfae5be9cde4
H: 3024.8px
2d245d76fd62b05b5e12bccb3e6c36ea
17cb2ca5b1a62662fc29712d33aa4c
2f6246c13582a1496eb5ceb59eca8b66
152e84d06c18954f28ea6ff7ab3e59e
2544360bcc5a74a8db8ac9b57237d8b
2ffdde83e28cc6babada95eee4c112
2d7fb16b2e614b188abdfae5be9cde4
H: 675.5px
2d245d76fd62b05b5e12bccb3e6c36ea
17cb2ca5b1a62662fc29712d33aa4c
2f6246c13582a1496eb5ceb59eca8b66
NN+distance
H: 313.6px
221_v_tabletop_3_1
NN+distance
H: 214.3px
237_v_there_4_1
221_v_tabletop_3_1
H: 210.7px
237_v_there_4_1
221_v_tabletop_3_1
237_v_there_4_1
NN+distance
inliers: 10/29
inliers: 20/49
inliers: 81/88
Figure 13: More homography examples. We show point correspondences on our synthetic dataset (see Section 5.1), on real
image pairs from HPatches (see Appendix A.1), and a checkerboard image captured by a webcam. SuperGlue consistently
estimates more correct matches (green lines) and fewer mismatches (red lines), successfully coping with repeated texture,
large viewpoint, and illumination changes.
SuperPoint + NN + distance
SuperPoint + NN + OANet
SuperPoint + SuperGlue
NN+distance
inliers: 48/61
scene0758_00/frame-000165
scene0758_00/frame-000510
NN+distance
inliers: 13/25
scene0722_00/frame-000045
scene0722_00/frame-000735
inliers: 100/111
scene0758_00/frame-000165
scene0758_00/frame-000510
inliers: 47/92
scene0722_00/frame-000045
scene0722_00/frame-000735
inliers: 151/156
scene0758_00/frame-000165
scene0758_00/frame-000510
inliers: 92/101
scene0722_00/frame-000045
scene0722_00/frame-000735
Very difﬁcult
NN+distance
inliers: 0/10
scene0738_00/frame-000885
scene0738_00/frame-001065
NN+distance
inliers: 8/22
scene0726_00/frame-000135
scene0726_00/frame-000210
NN+distance
inliers: 2/14
scene0752_00/frame-000075
scene0752_00/frame-001440
NN+distance
inliers: 4/72
scene0713_00/frame-001320
scene0713_00/frame-002025
NN+distance
inliers: 1/13
scene0747_00/frame-000000
scene0747_00/frame-001530
inliers: 12/133
scene0738_00/frame-000885
scene0738_00/frame-001065
inliers: 19/56
scene0726_00/frame-000135
scene0726_00/frame-000210
inliers: 31/107
scene0752_00/frame-000075
scene0752_00/frame-001440
inliers: 21/199
scene0713_00/frame-001320
scene0713_00/frame-002025
inliers: 13/212
scene0747_00/frame-000000
scene0747_00/frame-001530
inliers: 94/97
scene0738_00/frame-000885
scene0738_00/frame-001065
inliers: 86/88
scene0726_00/frame-000135
scene0726_00/frame-000210
inliers: 117/120
scene0752_00/frame-000075
scene0752_00/frame-001440
inliers: 218/223
scene0713_00/frame-001320
scene0713_00/frame-002025
inliers: 45/69
scene0747_00/frame-000000
scene0747_00/frame-001530
Too difﬁcult
NN+distance
inliers: 7/11
scene0737_00/frame-000930
scene0737_00/frame-001095
NN+distance
inliers: 2/16
scene0806_00/frame-000225
scene0806_00/frame-001095
NN+distance
inliers: 0/9
scene0721_00/frame-000375
scene0721_00/frame-002745
inliers: 16/26
scene0737_00/frame-000930
scene0737_00/frame-001095
inliers: 3/33
scene0806_00/frame-000225
scene0806_00/frame-001095
inliers: 3/127
scene0721_00/frame-000375
scene0721_00/frame-002745
inliers: 0/1
scene0737_00/frame-000930
scene0737_00/frame-001095
inliers: 1/16
scene0806_00/frame-000225
scene0806_00/frame-001095
inliers: 0/0
scene0721_00/frame-000375
scene0721_00/frame-002745
Figure 14: More indoor examples. We show both Difﬁcult and Very Difﬁcult ScanNet indoor examples for which Super-
Glue works well, and three Too Difﬁcult examples where it fails, either due to unlikely motion or lack of repeatable keypoints
(last two rows). Correct matches are green lines and mismatches are red lines. See details in Section 5.2.
SuperPoint + NN + distance
SuperPoint + NN + OANet
SuperPoint + SuperGlue
NN+distance
inliers: 245/323
0443/3486089448_f0c4440d7a_b.jpg
0443/487271312_641ccc54a1_o.jpg
NN+distance
inliers: 72/314
0185/2828522268_356de44aef_o.jpg
0185/1877097604_ebb595af7e_o.jpg
NN+distance
inliers: 55/124
0047/2060227337_833e6a055f_o.jpg
0047/2051415868_f55d430c0c_o.jpg
NN+distance
inliers: 24/250
0181/3376977056_3bfb1fd64d_o.jpg
0181/1557994125_27bfc663e0_o.jpg
inliers: 703/729
0443/3486089448_f0c4440d7a_b.jpg
0443/487271312_641ccc54a1_o.jpg
inliers: 208/329
0185/2828522268_356de44aef_o.jpg
0185/1877097604_ebb595af7e_o.jpg
inliers: 143/325
0047/2060227337_833e6a055f_o.jpg
0047/2051415868_f55d430c0c_o.jpg
inliers: 4/156
0181/3376977056_3bfb1fd64d_o.jpg
0181/1557994125_27bfc663e0_o.jpg
inliers: 794/794
0443/3486089448_f0c4440d7a_b.jpg
0443/487271312_641ccc54a1_o.jpg
inliers: 474/475
0185/2828522268_356de44aef_o.jpg
0185/1877097604_ebb595af7e_o.jpg
inliers: 318/318
0047/2060227337_833e6a055f_o.jpg
0047/2051415868_f55d430c0c_o.jpg
inliers: 457/478
0181/3376977056_3bfb1fd64d_o.jpg
0181/1557994125_27bfc663e0_o.jpg
NN+distance
inliers: 80/197
united_states_capitol/98169888_3347710852
united_states_capitol/26757027_6717084061
NN+distance
inliers: 13/175
london_bridge/78916675_4568141288
london_bridge/19481797_2295892421
NN+distance
inliers: 50/194
piazza_san_marco/15148634_5228701572
piazza_san_marco/06795901_3725050516
NN+distance
inliers: 28/194
piazza_san_marco/43351518_2659980686
piazza_san_marco/06795901_3725050516
NN+distance
inliers: 22/162
st_pauls_cathedral/37347628_10902811376
st_pauls_cathedral/30776973_2635313996
NN+distance
inliers: 6/204
london_bridge/94185272_3874562886
london_bridge/49190386_5209386933
inliers: 170/586
united_states_capitol/98169888_3347710852
united_states_capitol/26757027_6717084061
inliers: 101/199
london_bridge/78916675_4568141288
london_bridge/19481797_2295892421
inliers: 246/672
piazza_san_marco/15148634_5228701572
piazza_san_marco/06795901_3725050516
inliers: 43/325
piazza_san_marco/43351518_2659980686
piazza_san_marco/06795901_3725050516
inliers: 53/229
st_pauls_cathedral/37347628_10902811376
st_pauls_cathedral/30776973_2635313996
inliers: 16/410
london_bridge/94185272_3874562886
london_bridge/49190386_5209386933
inliers: 239/243
united_states_capitol/98169888_3347710852
united_states_capitol/26757027_6717084061
inliers: 325/327
london_bridge/78916675_4568141288
london_bridge/19481797_2295892421
inliers: 322/330
piazza_san_marco/15148634_5228701572
piazza_san_marco/06795901_3725050516
inliers: 166/194
piazza_san_marco/43351518_2659980686
piazza_san_marco/06795901_3725050516
inliers: 142/154
st_pauls_cathedral/37347628_10902811376
st_pauls_cathedral/30776973_2635313996
inliers: 4/126
london_bridge/94185272_3874562886
london_bridge/49190386_5209386933
Figure 15: More outdoor examples. We show results on the MegaDepth validation and the PhotoTourism test sets. Correct
matches are green lines and mismatches are red lines. The last row shows a failure case, where SuperGlue focuses on the
incorrect self-similarity. See details in Section 5.3.
Estimated correspondences
inliers: 169/180
scene0713_00/frame-001605
scene0713_00/frame-001680
Self-Attention
Cross-Attention
Self-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Self-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Self-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Self-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Self-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Self-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Self-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Self-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Self-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Self-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Self-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Self-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Self-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Self-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Self-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Cross-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Cross-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Cross-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Cross-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Cross-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Cross-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Cross-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Cross-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Cross-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Cross-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Cross-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Cross-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Cross-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Cross-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Cross-Attention
scene0713_00/frame-001605
scene0713_00/frame-001680
Figure 16: Attention patterns across layers. For this image pair (correctly matched by SuperGlue), we look at three speciﬁc
keypoints that can be matched with different levels of difﬁculty: the easy keypoint, the medium keypoint, and the difﬁcult
keypoint. We visualize self- and cross-attention weights (within images A and B, and from A to B, respectively) of selected
layers and heads, varying the edge opacity with αij. The self-attention initially attends all over the image (row 1), and gradually focuses on a small neighborhood around each keypoint (last row). Similarly, some cross-attention heads focus on candidate matches, and successively reduce the set that is inspected. The easy keypoint is matched as early as layer 9, while more
difﬁcult ones are only matched at the last layer. Similarly as in Figure 12, the self- and cross-attention spans generally shrink
throughout the layers. They however increase in layer 11, which attends to other locations – seemingly distinctive ones – that
are further away. We hypothesize that SuperGlue attempts to disambiguate challenging matches using additional context.