This document is downloaded from DR‑NTU ( 
Nanyang Technological University, Singapore.
CayleyNets : graph convolutional neural networks
with complex rational spectral filters
Levie, Ron; Monti, Federico; Bresson, Xavier; Bronstein, Michael M.
Levie, R., Monti, F., Bresson, X., & Bronstein, M. M.  . CayleyNets : graph convolutional
neural networks with complex rational spectral filters. IEEE Transactions on Signal
Processing, 67(1), 97‑109. doi:10.1109/tsp.2018.2879624
 
 
© 2018 IEEE. Personal use of this material is permitted. Permission from IEEE must be
obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new
collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted
component of this work in other works. The published version is available at:
 
Downloaded on 27 Mar 2025 02:51:47 SGT
CayleyNets: Graph Convolutional Neural Networks
with Complex Rational Spectral Filters
Ron Levie∗, Federico Monti∗, Xavier Bresson and Michael M. Bronstein
The rise of graph-structured data such as social networks, regulatory networks, citation
graphs, and functional brain networks, in combination with resounding success of deep learning
in various applications, has brought the interest in generalizing deep learning models to
non-Euclidean domains. In this paper, we introduce a new spectral domain convolutional
architecture for deep learning on graphs. The core ingredient of our model is a new class of
parametric rational complex functions (Cayley polynomials) allowing to eﬃciently compute
spectral ﬁlters on graphs that specialize on frequency bands of interest. Our model generates
rich spectral ﬁlters that are localized in space, scales linearly with the size of the input data
for sparsely-connected graphs, and can handle diﬀerent constructions of Laplacian operators.
Extensive experimental results show the superior performance of our approach, in comparison to
other spectral domain convolutional architectures, on spectral image classiﬁcation, community
detection, vertex classiﬁcation and matrix completion tasks.
Introduction
In many domains, one has to deal with large-scale data with underlying non-Euclidean structure.
Prominent examples of such data are social networks, genetic regulatory networks, functional
networks of the brain, and 3D shapes represented as discrete manifolds. The recent success of
deep neural networks and, in particular, convolutional neural networks (CNNs) have raised the
interest in geometric deep learning techniques trying to extend these models to data residing on
graphs and manifolds. In this paper we focus on spectral graph CNNs. Geometric deep learning
approaches, and speciﬁcally spectral graph CNNs, have been successfully applied to computer
graphics and vision , brain imaging , and drug design problems, to mention
a few. For a comprehensive presentation of methods and applications of deep learning on graphs
and manifolds, we refer the reader to the review paper .
Related work
The earliest neural network formulation on graphs was proposed by and , combining random
walks with recurrent neural networks (their paper has recently enjoyed renewed interest in ).
The ﬁrst CNN-type architecture on graphs was proposed by . One of the key challenges of
extending CNNs to graphs is the lack of vector-space structure and shift-invariance making the
classical notion of convolution elusive. Bruna et al. formulated convolution-like operations in the
∗The ﬁrst two authors have contributed equally and are listed alphabetically.
Ron Levie is with the Institute of Mathematics, Technische Universität Berlin, Berlin 10623, Germany (e-mail:
 ).
Federico Monti is with the Institute of Computational Science, Universitá della Svizzera italiana, Lugano 6900,
Switzerland (e-mail: ).
Xavier Bresson is with the School of Computer Science and Engineering and the Data Science and AI Center
at Nanyang Technological University (NTU), Singapore (email: ). He is supported by NRF
Fellowship NRFF2017-10.
Michael M. Bronstein is with Imperial College London (UK), USI Lugano (Switzerland), and Intel Perceptual
Computing (Israel).
 
spectral domain, using the graph Laplacian eigenbasis as an analogy of the Fourier transform .
 proposed an eﬃcient ﬁltering scheme using recurrent Chebyshev polynomials applied on the
Laplacian operator. As opposed to , Chebyshev ﬁlters are deﬁned as functions R →R applied
on the spectrum, as in . This makes ﬁlters learned on one graph generalizable to other graphs.
 simpliﬁed this architecture using ﬁlters operating on 1-hop neighborhoods of the graph. 
proposed a Diﬀusion CNN architecture based on powers of the degree-normalized transition matrix.
 (and later, ) proposed a spatial-domain generalization of CNNs to graphs using local patch
operators represented as Gaussian mixture models, showing a signiﬁcant advantage of such models
in generalizing across diﬀerent graphs. In , spectral graph CNNs were extended to multiple
graphs and applied to matrix completion and recommender system problems.
Main contribution
In this paper, we construct graph CNNs employing an eﬃcient spectral ﬁltering scheme based on
the new class of Cayley polynomials that enjoys similar advantages of the Chebyshev ﬁlters 
such as localization and linear complexity in the number of edges. The main advantage of our
ﬁlters over is their ability to detect narrow frequency bands of importance during training, and
to specialize on them while being well-localized on the graph.
We demonstrate experimentally
that this aﬀords our method greater ﬂexibility, making it perform better than ChebNets on a broad
range of graph learning problems.
We use a, a, and A to denote scalars, vectors, and matrices, respectively. ¯z denotes the conjugate of
a complex number, Re{z} its real part, and i = √−1 denotes the imaginary unit. diag(a1, . . . , an)
denotes an n × n diagonal matrix with diagonal elements a1, . . . , an. Diag(A) = diag(a11, . . . , ann)
denotes an n × n diagonal matrix obtained by setting to zero the oﬀ-diagonal elements of A.
Oﬀ(A) = A −Diag(A) denotes the matrix containing only the oﬀ-diagonal elements of A. I is the
identity matrix and A ◦B denotes the Hadamard (element-wise) product of matrices A and B.
Proofs are given in the appendix.
Spectral techniques for deep learning on graphs
Spectral graph theory
Let G = ({1, . . . , n}, E, W) be an undirected weighted graph, represented by a symmetric adjacency
matrix W = (wij). We deﬁne wij = 0 if (i, j) /∈E and wij > 0 if (i, j) ∈E. We denote by Nk,m the
k-hop neighborhood of vertex m, containing vertices that are at most k edges away from m. The
unnormalized graph Laplacian is an n × n symmetric positive-semideﬁnite matrix ∆u = D −W,
where D = diag(P
j̸=i wij) is the degree matrix. The normalized graph Laplacian is deﬁned as
∆n = D−1/2∆uD−1/2 = I −D−1/2WD−1/2. In the following, we use the generic notation ∆to
refer to some Laplacian.
Since both normalized and unnormalized Laplacian are symmetric and positive semi-deﬁnite
matrices, they admit an eigendecomposition ∆= ΦΛΦ⊤, where Φ = (φ1, . . . φn) are the orthonormal eigenvectors and Λ = diag(λ1, . . . , λn) is the diagonal matrix of corresponding non-negative
eigenvalues (spectrum) 0 = λ1 ≤λ2 ≤. . . ≤λn. The eigenvectors play the role of Fourier atoms in
classical harmonic analysis and the eigenvalues can be interpreted as (the square of) frequencies.
Given a signal f = (f1, . . . , fn)⊤on the vertices of graph G, its graph Fourier transform is given
by ˆf = Φ⊤f. Given two signals f, g on the graph, their spectral convolution can be deﬁned as the
element-wise product of the Fourier transforms, f ⋆g = Φ
 (Φ⊤g) ◦(Φ⊤f)
= Φ diag(ˆg1, . . . , ˆgn)ˆf,
which corresponds to the property referred to as the Convolution Theorem in the Euclidean case.
Spectral CNNs
 used the spectral deﬁnition of convolution to generalize CNNs on graphs, with a spectral
convolutional layer of the form
Φk ˆGl,l′Φ⊤
Here the n × p and n × q matrices Fin = (f in
1 , . . . , f in
p ) and Fout = (f out
, . . . , f out
) represent
respectively the p- and q-dimensional input and output signals on the vertices of the graph,
Φk = (φ1, . . . , φk) is an n × k matrix of the ﬁrst eigenvectors, ˆGl,l′ = diag(ˆgl,l′,1, . . . , ˆgl,l′,k) is
a k × k diagonal matrix of spectral multipliers representing a learnable ﬁlter in the frequency
domain, and ξ is a nonlinearity (e.g., ReLU) applied on the vertex-wise function values. Pooling is
performed by means of graph coarsening, which, given a graph with n vertices, produces a graph
with n′ < n vertices and transfers signals from the vertices of the ﬁne graph to those of the coarse
one. Assuming k = O(n) Laplacian eigenvectors are used, a spectral convolutional layer requires
O(pqk) = O(n) parameters to train.
In addition, an informal approach for keeping the ﬁlters
localized in the spectral domain was proposed. The idea is to learn just a few spectral coeﬃcients
of the ﬁlter, and obtain the rest using interpolation. This also keeps the number of ﬁlter parameters
O(1). The spatial locality property simulates local receptive ﬁelds , and is important for the
interpretability of convolutions as ﬁlters. Moreover, for spatial implementation of (1), such as
ChebNet and CayleyNet, small receptive ﬁelds typically indicate sparse implementations.
This framework has several major drawbacks. First, the computation of the forward and inverse
graph Fourier transforms incur expensive O(n2) multiplication by the matrices Φ, Φ⊤, as there is
no FFT-like algorithms on general graphs.
Second, the spectral ﬁlter coeﬃcients are basis dependent, and consequently, a spectral CNN
model learned on one graph cannot be transferred to another graph.
This is as opposed to ,
where the frequency responses of the ﬁlters coeﬃcients are represented as ˆgi = g(λi), where g(λ)
is a smooth transfer function of frequency λ. Applying such ﬁlter to signal f can be expressed
as Gf = g(∆)f = Φg(Λ)Φ⊤f = Φ diag(g(λ1), . . . , g(λn))Φ⊤f, where applying a function to a
matrix is understood in the operator functional calculus sense (applying the function to the matrix
eigenvalues).
It is noteworthy to mention alternative functional calculus driven approaches to deﬁne convolution. In ﬁlters are deﬁned as functions of the adjacency matrix g(W), and in the problem
of ordering the eigenvalues of W according to a natural notion of frequency was addressed.
 used polynomial ﬁlters represented in the Chebyshev basis
applied to rescaled frequency ˜λ ∈[−1, 1]; here, α is the (r + 1)-dimensional vector of polynomial
coeﬃcients parametrizing the ﬁlter and optimized for during the training, and Tj(λ) = 2λTj−1(λ)−
Tj−2(λ) denotes the Chebyshev polynomial of degree j deﬁned in a recursive manner with T1(λ) = λ
and T0(λ) = 1. Chebyshev polynomials form an orthogonal basis for the space of polynomials
of order r on [−1, 1]. Applying the ﬁlter is performed by gα( ˜∆)f, where ˜∆= 2λ−1
n ∆−I is the
rescaled Laplacian such that its eigenvalues ˜Λ = 2λ−1
n Λ −I are in the interval [−1, 1].
Such an approach has several important advantages. First, since gα( ˜∆) = Pr
j=0 αjTj( ˜∆)
contains only matrix powers, additions, and multiplications by scalar, it can be computed avoiding
the explicit expensive O(n3) computation of the Laplacian eigenvectors. Furthermore, due to the
recursive deﬁnition of the Chebyshev polynomials, the computation of the ﬁlter gα(∆)f entails
applying the Laplacian r times, resulting in O(rn) operations assuming that the Laplacian is a
sparse matrix with O(1) non-zero elements in each row (a valid hypothesis for most real-world
graphs that are sparsely connected). From another point of view, in each multiplication by the
Laplacian, neighbors in the graph exchange data, and there are overall r such neighbor exchanges.
Second, the number of parameters is O(1) as r is independent of the graph size n. Third, since the
Laplacian is a local operator aﬀecting only 1-hop neighbors of a vertex and a polynomial of degree
r of the Laplacian aﬀects only r-hops, the resulting ﬁlters have guaranteed spatial localization.
A key disadvantage of Chebyshev ﬁlters is the fact that using polynomials makes it hard to
produce narrow-band ﬁlters, as such ﬁlters require very high order r, and produce unwanted
non-local ﬁlters. This deﬁciency is especially pronounced when the Laplacian has clusters of
eigenvalues concentrated around a few frequencies with large spectral gap (Figure 3, second to the
Indeed, in ChebNets the Laplacian eigenvalues are contracted to the band [−1, 1], and the
clusters of eigenvalues become very concentrated. Now, the frequency response of the Chebyshev
ﬁlter is a polynomial in [−1, 1], which is unable to separate the individual eigenvalues in the
clusters due to an uncertainty principle. Such a behavior is characteristic of graphs with community
structures, which is very common in many real-world graphs, for instance, social networks.
Let us explain the above phenomenon more accurately. Recall that Chebyshev polynomials are
Tn(cos(θ)) = cos(nθ),
and form an orthonormal basis of the weighted Lebesgue space L2
. When the
variable is changed via x = cos(θ), the Chebyshev basis maps to the cosine basis in [0, π], and the
maps to the space L2(0, π) with the standard Lebesgue measure. Now,
suppose that we want to represent a band-pass ﬁlter on the narrow band [cos(b), cos(a)] ⊂[−1, 1],
with small cos(b −a). Under the change of variable, this band maps to [a, b] with small b −a = ϵ.
In this case, since the characteristic function of [a, b] is the shrinking dilation of the characteristic
function of [−1
2] (up to translation), it’s Fourier coeﬃcients are samples from the stretching
dilation of the Fourier transform of the characteristic function of [−1
2]. As a result, the number
of coeﬃcients required to approximate a band pass ﬁlter up to some ﬁxed tolerance is inverse
proportional to the size of the band. More generally, the number of Chebyshev coeﬃcients required
for approximating a ﬁlter having features in a given scale, is inverse proportional to the scale.
When the Laplacian has a cluster of eigenvalues concentrated around one frequency, a ﬁlter that
separates these eigenvalues must have features in scale proportional to the radius of the eigenvalue
cluster. Therefore, the number of Chebyshev coeﬃcients must be inverse proportional to the cluster
To overcome this major drawback, we need a new class of ﬁlters, that both entail O(r) neighbor
exchanges, and are able to specialize in narrow bands in frequency.
Cayley ﬁlters
A key construction of this paper is a family of complex ﬁlters that enjoy the advantages of Chebyshev
ﬁlters while avoiding some of their drawbacks.
We deﬁne a Cayley polynomial of order r to be a
real-valued function with complex coeﬃcients,
gc,h(λ) = c0 + 2Re
cj(hλ −i)j(hλ + i)−jo
where c = (c0, . . . , cr) is a vector of one real coeﬃcient and r complex coeﬃcients and h > 0 is the
spectral zoom parameter, that will be discussed later. A Cayley ﬁlter G is a spectral ﬁlter deﬁned
on real signals f by
Gf = gc,h(∆)f = c0f + 2Re{
cj(h∆−iI)j(h∆+ iI)−jf},
where the parameters c and h are optimized during training. Similarly to the Chebyshev ﬁlters,
Cayley ﬁlters involve basic matrix operations such as powers, additions, multiplications by scalars,
and also inversions. This implies that application of the ﬁlter Gf can be performed without explicit
expensive eigendecomposition of the Laplacian operator. Cayley ﬁlters are special cases of ﬁlters
based on general rational functions of the Laplacian, namely ARMA ﬁlters . For a general
rational functions of the Laplacian, calculating the denominator requires a matrix inversion. When
the ﬁlter is based on arbitrary coeﬃcients, there is no guarantee that the matrix inversions are
calculated stably. Guaranteeing stable inversions for arbitrary ﬁlter coeﬃcients is important, since
the coeﬃcients follow an unknown path during training.
For general ARMA ﬁlters, the ﬁlter can
acquire poles arbitrarily close to the spectrum of ∆during training, so there is no uniform analysis
of convergence of the approximate inversions in the ﬁlter computation. The motivation to use the
subclass of Cayley ﬁlters over general rational functions is to guarantee that inversion is uniformly
stable. Namely, the number of iterations required for a given approximation error is independent
of the ﬁlter coeﬃcients, as long as the coeﬃcients are bounded. Moreover, the lower number of
parameters in Cayley polynomials in comparison to general rational functions may be beneﬁcial for
avoiding overﬁtting.
In the following, we show that Cayley ﬁlters are analytically well behaved; in particular,
any smooth spectral ﬁlter can be represented as a Cayley polynomial, and low-order ﬁlters are
localized in the spatial domain. We also discuss numerical implementation and compare Cayley and
Chebyshev ﬁlters.
We show that Cayley ﬁlters deﬁned on sparse Laplacians with O(1) non-zero
elements takes O(n) operations, similarly to Chebyshev ﬁlters.
Analytic properties
Cayley ﬁlters are best understood through the Cayley transform, from which their name derives.
Denote by eiR = {eiθ : θ ∈R} the unit complex circle. The Cayley transform C(x) = x−i
smooth bijection between R and eiR \ {1}. The complex matrix C(h∆) = (h∆−iI)(h∆+ iI)−1
obtained by applying the Cayley transform to the scaled Laplacian h∆has its spectrum in eiR
and is thus unitary. Since z−1 = z for z ∈eiR, we can write cjCj(h∆) = cjC−j(h∆). Therefore,
using 2Re{z} = z + z, any Cayley ﬁlter (4) can be written as a conjugate-even Laurent polynomial
w.r.t. C(h∆),
cjCj(h∆) + cjC−j(h∆).
Since the spectrum of C(h∆) is in eiR, the operator Cj(h∆) can be thought of as a multiplication
by a pure harmonic in the frequency domain eiR for any integer power j,
Cj(h∆) = Φdiag
j, . . . ,
A Cayley ﬁlter can be thus seen as a multiplication by a ﬁnite Fourier expansions in the frequency
domain eiR. Since (5) is conjugate-even, it is a (real-valued) trigonometric polynomial.
Note that any spectral ﬁlter can be formulated as a Cayley ﬁlter. Indeed, spectral ﬁlters g(∆)
are speciﬁed by the ﬁnite sequence of values g(λ1), . . . , g(λn), which can be interpolated by a
trigonometric polynomial. Moreover, since trigonometric polynomials are smooth, we expect low
order Cayley ﬁlters to be well localized in some sense on the graph, as discussed later.
Finally, in deﬁnition (4) we use complex coeﬃcients. If cj ∈R then (5) is an even cosine
polynomial, and if cj ∈iR then (5) is an odd sine polynomial. Since the spectrum of h∆is in R+,
it is mapped to the lower half-circle by C, on which both cosine and sine polynomials are complete
and can represent any spectral ﬁlter. However, it is beneﬁcial to use general complex coeﬃcients,
since complex Fourier expansions are overcomplete in the lower half-circle, thus describing a larger
variety of spectral ﬁlters of the same order without increasing the computational complexity of the
Spectral zoom
To understand the essential role of the parameter h in the Cayley ﬁlter, consider C(h∆). Multiplying
∆by h dilates its spectrum, and applying C on the result maps the non-negative spectrum to
the complex half-circle. The greater h is, the more the spectrum of h∆is spread apart in R+,
resulting in better spacing of the smaller eigenvalues of C(h∆). On the other hand, the smaller h
is, the further away the high frequencies of h∆are from ∞, the better spread apart are the high
frequencies of C(h∆) in eiR (see Figure 1). Tuning the parameter h allows thus to ‘zoom’ in to
diﬀerent parts of the spectrum, resulting in ﬁlters specialized in diﬀerent frequency bands.
Figure 1: Eigenvalues of the unnormalized Laplacian h∆u of the 15-communities graph mapped on
the complex unit half-circle by means of Cayley transform with spectral zoom values (left-to-right)
h = 0.1, 1, and 10. The ﬁrst 15 frequencies carrying most of the information about the communities
are marked in red. Larger values of h zoom (right) on the low frequency band.
Numerical properties
The numerical core of the Cayley ﬁlter is the computation of Cj(h∆)f for j = 1, . . . , r, performed
in a sequential manner. Let y0, . . . , yr denote the solutions of the following linear recursive system,
(h∆+ iI)yj = (h∆−iI)yj−1 , j = 1, . . . , r.
Note that sequentially approximating yj in (6) using the approximation of yj−1 in the right hand
side is stable, since C(h∆) is unitary and thus has condition number 1.
Equations (6) can be solved with matrix inversion exactly, but it costs O(n3). An alternative is
to use the Jacobi method,1 which provides approximate solutions ˜yj ≈yj. Let J = −(Diag(h∆+
iI))−1Oﬀ(h∆+iI) be the Jacobi iteration matrix associated with equation (6). For the unnormalized
Laplacian, J = (hD + iI)−1hW. Jacobi iterations for approximating (6) for a given j have the
bj =(Diag(h∆+ iI))−1(h∆−iI)˜yj−1,
initialized with ˜y(0)
= bj and terminated after K iterations, yielding ˜yj = ˜y(K)
. We denote ˜y0 = y0.
The application of the approximate Cayley ﬁlter is given by f
Gf = c0˜y0 + 2Re Pr
j=1 cj ˜yj ≈Gf,
and takes O(rKn) operations under the previous assumption of a sparse Laplacian. The method
can be improved by normalizing ∥˜yj∥2 = ∥f∥2.
Next, we give an error bound for the approximate ﬁlter. For the unnormalized Laplacian, let
d = maxj{dj,j} and κ = ∥J∥∞=
h2d2+1 < 1. For the normalized Laplacian, we assume that
(h∆n + iI) is dominant diagonal, which gives κ = ∥J∥∞< 1.
Proposition 1. Under the above assumptions,
≤2MκK, where M = √n Pr
j=1 j |cj| in the general case, and M = Pr
j=1 j |cj| if the
graph is regular.
Proposition 1 is pessimistic in the general case, while requires strong assumptions in the regular
case. We ﬁnd that in most real life situations the behavior is closer to the regular case. It also
follows from Proposition 1 that smaller values of the spectral zoom h result in faster convergence,
giving this parameter an additional numerical role of accelerating convergence.
1We remind that the Jacobi method for solving Ax = b consists in decomposing A = Diag(A) + Oﬀ(A) and
obtaining the solution iteratively as x(k+1) = −(Diag(A))−1Oﬀ(A)x(k) + (Diag(A))−1b.
Last, note that a Cayley ﬁlter with Jacobi approximation, is based on powers of the Jacobi
matrix J, in addition to (h∆−iI). The Jacobi matrix can be viewed as a general representation
matrix of the graph, replacing the standard Laplacian with a matrix that respects the connectivity
of the graph (general representation matrices were considered in ). This is also true for (h∆−iI).
In this point of view, learning h is interpreted as learning a general representation matrix. Learning
h can be also viewed as learning a normalization of the weights of the graph. The problem of
learning the topology of the graph was studied e.g. in .
Complexity
In practice, an accurate inversion of (h∆+ iI) is not required, since the approximate inverse is
combined with learned coeﬃcients, which “compensate”, as necessary, for the inversion inaccuracy.
Such behavior is well-documented in the literature in other contexts of model compression and
accelerated convergence of iterative algorithms. For example, in , sparse signal coding are
learned by unrolling iterative shrinkage algorithms (FISTA) into a neural network, where each layer
emulates an iteration of the original algorithm but has extra learnable parameters. It is shown
that FISTA networks with just a few layers outperform hundreds or thousands of iterations of the
original algorithm thanks to the learnable parameters. The above phenomenon is also a common
observation for solving sparse linear equations for compressed sensing tasks (see e.g ).
In a CayleyNet for a ﬁxed graph, we ﬁx the number of Jacobi iterations. Since the convergence
rate depends on κ, that depends on the graph, diﬀerent graphs may need diﬀerent numbers of
iterations. The convergence rate also depends on h. Since there is a trade-oﬀbetween the spectral
zoom amount h, and the accuracy of the approximate inversion, and since h is a learnable parameter,
the training ﬁnds the right balance between the spectral zoom amount and the inversion accuracy.
To formulate computational complexity results, we consider the case where the number of
vertices n is “big”. To formalize this, we consider a sequence of graphs indexed by n, and study the
asymptotics as n →∞. When the graph is sampled from a continuous entity, like a manifold, this
asymptotic analysis has a precise meaning. Otherwise, the asymptotic analysis is just a formal
way of saying “big n”. For every constant of a graph, e.g d, κ, we add the subscript n, indicating
the number of vertices of the graph. We assume that there is a global constant C, such that the
number of edges is bounded by Cn. For the unnormalized Laplacian, we assume that dn and hn
are bounded, which gives κn < a < 1 for some a independent of n. For the normalized Laplacian,
we assume that κn < a < 1.
These assumptions pose regularity on the sequence of graphs. By
Proposition 1, ﬁxing the number of Jacobi iterations K and the order of the ﬁlter r, independently
of n, keeps the Jacobi error controlled. As a result, the number of parameters of the Cayley ﬁlters
can be kept O(1), and for a Laplacian modeled as a sparse matrix, applying a Cayley ﬁlter on a
signal takes O(n) operations. Indeed, the Jacobi matrix J has the same connectivity as the graph,
including edges connecting each vertex to itself. In each Jacobi iteration, J is applied K times,
which means that vertices exchange information with their neighbors K times, and one time in
the initialization due to one multiplication by (h∆−iI). The Jacobi approximation of C(h∆) is
computed r times, for the r coeﬃcients of the ﬁlter, and thus overall there are (K + 1)r neighbor
exchanges in the method. Thus, for a sparsely connected graph with O(n) edges, applying a Cayley
ﬁlter on a signal takes O((K + 1)rn) operations.
Localization
Unlike Chebyshev ﬁlters that have the small r-hop support, Cayley ﬁlters are rational functions
supported on the whole graph. However, it is still true that Cayley ﬁlters are well localized on the
graph. Let G be a Cayley ﬁlter and δm denote a delta-function on the graph, deﬁned as one at
vertex m and zero elsewhere. We show that Gδm decays fast, in the following sense:
Deﬁnition 2 (Exponential decay on graphs). Let f be a signal on the vertices of graph G,
1 ≤p ≤∞, and 0 < ϵ < 1. Denote by S ⊆{1, . . . , n} a subset of the vertices and by Sc its
complement. We say that the Lp-mass of f is supported in S up to ϵ if ∥f|Sc∥p ≤ϵ ∥f∥p, where
Figure 2: Filters (spatial domain, top and spectral domain, bottom) learned by CayleyNet (left)
and ChebNet (center, right) on the MNIST dataset. Cayley ﬁlters are able to realize larger supports
for the same order r.
f|Sc = (fl)l∈Sc is the restriction of f to Sc. We say that f has (graph) exponential decay about
vertex m, if there exists some γ ∈(0, 1) and c > 0 such that for any k, the Lp-mass of f is supported
in Nk,m up to cγk. Here, Nk,m is the k-hop neighborhood of m.
Remark 3. Note that Deﬁnition 2 is analogous to classical exponential decay on Euclidean space:
|f(x)| ≤Rγ−x iﬀfor every ball Bρ of radius ρ about 0, ∥f|Bcρ∥∞≤cγ−ρ ∥f∥∞with c =
Theorem 4. Let G be a Cayley ﬁlter of order r. Then, Gδm has exponential decay about m in
L2, with constants c = 4M
∥Gδm∥2 and γ = κ1/r (where M and κ are from Proposition 1).
Cayley vs Chebyshev
Below, we compare the two classes of ﬁlters:
Chebyshev as a special case of Cayley. For a regular graph with D = dI, using Jacobi inversion
based on zero iteration, we get that any Cayley ﬁlter of order r is a polynomial of ∆in the
monomial base
j. In this situation, a Chebyshev ﬁlter, which is a real valued polynomial of
∆, is a special case of a Cayley ﬁlter.
Spectral zoom and stability. Generally, both Chebyshev polynomials and trigonometric polynomials
give stable approximations, optimal for smooth functions. However, this crude statement is oversimpliﬁed. One of the drawbacks in Chebyshev ﬁlters is the fact that the spectrum of ∆is always
mapped to [−1, 1] in a linear manner, making it hard to specialize in small frequency bands. In
Cayley ﬁlters, this problem is mitigated with the help of the spectral zoom parameter h. As an
example, consider the community detection problem discussed in the next section. A graph with
strong communities has a cluster of small eigenvalues near zero. Ideal ﬁlters g(∆) for extracting
the community information should be able to focus on this band of frequencies. Approximating
such ﬁlters with Cayley polynomials, we zoom in to the band of interest by choosing the right h,
and then project g onto the space of trigonometric polynomials of order r, getting a good and
stable approximation (Figure 3, bottom). However, if we project g onto the space of Chebyshev
polynomials of order r, the interesting part of g concentrated on a small band is smoothed out
and lost (Figure 3, second to the last). Thus, projections are not the right way to approximate
such ﬁlters, and the stability of orthogonal polynomials cannot be invoked.
On the other hand, if
we want to approximate g on the small band using polynomials, ignoring the behavior away from
this band, the approximation will be unstable away from this band; small perturbations in g will
result in big perturbations in the Chebyshev ﬁlter away from the band. This is due to the fact
that any polynomial diverges at inﬁnity, and for an asymptotically small band and polynomials of
ﬁxed order, “away from the band behaves like inﬁnity.” For this reason, we say that Cayley ﬁlters
are more stable than Chebyshev ﬁlters.
Regularity.
We found that in practice, low-order Cayley ﬁlters are able to model both very
concentrated impulse-like ﬁlters, and wider Gabor-like ﬁlters. Cayley ﬁlters are able to achieve a
wider range of ﬁlter supports with less coeﬃcients than Chebyshev ﬁlters (Figure 2), making the
Cayley class more regular than Chebyshev.
Complexity. Under the assumption of sparse Laplacians, both Cayley and Chebyshev ﬁlters incur
linear complexity O(n). Besides, the new ﬁlters are equally simple to implement as Chebyshev
ﬁlters; as seen in Eq.7, they boil down to simple sparse matrix-vector multiplications providing a
GPU friendly implementation.
Experimental settings
We test the proposed CayleyNets reproducing the experiments of and using ChebNet
 as our main baseline method.
Pooling and graph coarsening was performed identically to
 . The hyperparameters are identical to the original experiments, and not optimized. All the
methods were implemented in TensorFlow . The experiments were executed on a machine with
a 3.5GHz Intel Core i7 CPU, 64GB of RAM, and NVIDIA Titan X GPU with 12GB of RAM.
SGD+Momentum and Adam optimization methods were used to train the models in MNIST
and the rest of the experiments, respectively. Training and testing were always done on disjoint
Community detection
We start with an experiment on a synthetic graph consisting of 15 communities with strong
connectivity within each community and sparse connectivity across communities (Figure 3, top).
Though rather simple, such a dataset allows to study the behavior of diﬀerent algorithms in
controlled settings. On this graph, we generate noisy step signals, deﬁned as fi = 1 + σi if i belongs
to the community, and fi = σi otherwise, where σi ∼N(0, 0.3) is Gaussian i.i.d. noise. The
goal is to classify each such signal according to the community it belongs to. The neural network
architecture used for this task consisted of a spectral convolutional layer (based on Chebyshev or
Cayley ﬁlters) with 32 output features, a mean pooling layer, and a softmax classiﬁer for producing
the ﬁnal classiﬁcation into one of the 15 classes.
No regularization has been exploited in this
setting. The classiﬁcation accuracy is shown in Figure 3 (second to the top) along with examples
of learned ﬁlters (bottom two). We observe that CayleyNet signiﬁcantly outperforms ChebNet
for smaller ﬁlter orders, with an improvement as large as 80%. Studying the ﬁlter responses,
we note that due to the capability to learn the spectral zoom parameter, CayleyNet allows to
generate band-pass ﬁlters in the low-frequency band that discriminate well the communities (Figure
3 bottom).
Complexity
We experimentally validated the computational complexity of our model applying ﬁlters of diﬀerent
order r to synthetic 15-community graphs of diﬀerent size n using exact matrix inversion and
approximation with diﬀerent number of Jacobi iterations (Figure 6 in Appendix C). All times
have been computed running 30 times the considered models and averaging the ﬁnal results. As
expected, approximate inversion guarantees O(n) complexity. We further conclude that typically
very few Jacobi iterations are required (Figure 4 shows that our model with just one Jacobi iteration
outperforms ChebNet for low-order ﬁlters on the community detection problem).
Following , for a toy example, we approached the classical MNIST digits classiﬁcation as
a learning problem on graphs. Each pixel of an image is a vertex of a graph (regular grid with
8-neighbor connectivity), and pixel color is a signal on the graph. We used a graph CNN architecture
with two spectral convolutional layers based on Chebyshev and Cayley ﬁlters (producing 32 and 64
output features, respectively), interleaved with pooling layers performing 4-times graph coarsening
using the Graclus algorithm , and ﬁnally a fully-connected layer (this architecture replicates
Figure 3: Left: synthetic 15-communities graph. Right: community detection accuracy of ChebNet
and CayleyNet (top); normalized responses of four diﬀerent ﬁlters learned by ChebNet (middle)
and CayleyNet (bottom). Grey vertical lines represent the frequencies of the normalized Laplacian
(˜λ = 2λ−1
n λ −1 for ChebNet and C(λ) = (hλ −i)/(hλ + i) unrolled to a real line for CayleyNet).
Note how thanks to spectral zoom property Cayley ﬁlters can focus on the band of low frequencies
(dark grey lines) containing most of the information about communities.
Accuracy %
Figure 4: Community detection test accuracy as function of ﬁlter order r. Shown are exact matrix
inversion (dashed) and approximate Jacobi with diﬀerent number of iterations (colored). For
reference, ChebNet is shown (dotted).
the classical LeNet5, , architecture, which is shown for comparison).
SGD+Momentum with
learning rate equal to 0.02, momentum m = 0.9, dropout probability p = 0.5 and weight decay
coeﬃcient γ = 5 · 10−4 have been applied as described in . MNIST classiﬁcation results are
reported in Table 1. CayleyNet (11 Jacobi iterations) achieves the same (near perfect) accuracy
as ChebNet with ﬁlters of lower order (r = 12 vs 25). Examples of ﬁlters learned by ChebNet
and CayleyNet are shown in Figure 2.
0.1776 +/- 0.06079 sec and 0.0268 +/- 0.00841 sec are
respectively required by CayleyNet and ChebNet for analyzing a batch of 100 images at test time.
Table 1: Test accuracy obtained with diﬀerent methods on the MNIST dataset.
Order Accuracy #Params
Citation network
Next, we address the problem of vertex classiﬁcation on graphs using the popular CORA citation
graph . Each of the 2,708 vertices of the CORA graph represents a scientiﬁc paper, and
an undirected unweighted edge represents a citation (5,429 edges in total). For each vertex, a
1,433-dimensional binary feature vector representing the content of the paper is given. The task is
to classify each vertex into one of the 7 groundtruth classes, of labels.
In the semi-supervised
problem (transductive learning), the features of all vertices are known, but labels are given just
for a subset of the nodes. The task is to learn a mapping, that takes the features at the nodes as
inputs, and gives the labels as outputs. The mapping is trained by minimizing the label error at
the nodes with known labels. After training, the the mapping is tested over the nodes in which the
labels were unknown during training.
To present a deep comparison with recent state-of-the-art architectures, we analyze the performance of our model in two diﬀerent settings: the classic semi-supervised problem presented in
 with 140 training samples, 500 validation samples and 1,000 test samples and a relaxed
version of this that exploits 1,708 vertices for training, 500 for validation and 500 for testing. We
opted for a larger amount of training samples in our second experiment, in order to provide an
estimate of the quality of CayleyNet in a situation which is less prone to overﬁtting. This provides
a better overview of the goodness of the considered construction, since richer ﬁlters are less likely
to produce lower performance, as opposed to the typical behavior when the available data is scarce.
Cayley operators with matrix inversion have been considered in both settings for our solution.
DCNN , GCN , MoNet and GAT have been used as terms of comparison.
On the standard split, we train CayleyNet by realizing ﬁlters as linear combinations of neighborhood descriptors obtained by applying Cayley ﬁlters on the signal of features. Two versions of
CayleyNet have been implemented for this setting: a lightweight one exploiting two convolutional
layers with 16 and 7 output features and a heavier one requiring 64 and 7 output features. This
provides a valuable term of comparison with both the solutions presented in and as
same number of parameters is respectively required by our implementations. Normalized Laplacian
has been used as reference. Adam with learning rate equal to 5 · 10−3, dropout probability p = 0.6
and weight decay coeﬃcient γ = 5 · 10−4 have been used for training. Table 2 presents the results
we obtained with our solution, average performance over 50 runs are reported to guarantee accurate
estimates. Performance of GCN, MoNet and GAT have been obtained from the respective papers,
DCNN has been trained with 1 diﬀusion layer and 1 hop to guarantee same number of parameters.
Our lighter version of CayleyNet outperforms DCNN, GCN and MoNet, while being defeated only
by the recent GAT (which however exploits an attention mechanism for better discriminating
relevant neighbors). Our heavier CayleyNet shows a signiﬁcant drop in performance, likely because
of overﬁtting on the small training set.
On our extended split, we analyze the behavior of CayleyNet and ChebNet for a variety of
diﬀerent polynomial orders. Two spectral convolutional layers with 16 and 7 outputs features
have been used for implementing the two architectures.
Adam with learning rate equal to 10−3,
dropout probability p = 0.5 and weight decay with coeﬃcient γ = 5 · 10−4 have been used for
training. Figure 5 presents the results of our analysis. Since ChebNet requires Laplacians with
spectra bounded in [−1, 1], we consider both the normalized Laplacian (the two left ﬁgures), and
the scaled unnormalized Laplacian (2∆/λmax −I), where ∆is the unnormalized Laplacian and
λmax is its largest eigenvalue (the two right ﬁgures). For fair comparison, we ﬁx the order of the
ﬁlters (top ﬁgures) and the overall number of network parameters (bottom ﬁgures). In the bottom
Table 2: Test accuracy of diﬀerent methods on the
standard split of the CORA dataset.
72.3 ± 0.8 %
CayleyNet64 features 81.0 ± 0.5 %
81.6 ± 0.4 %
MoNet 
81.7 ± 0.5 %
CayleyNet16 features 81.9 ± 0.7 %
83.0 ± 0.7 %
Table 3: Test accuracy of diﬀerent methods
on the extended split of the CORA dataset.
86.01 ± 0.24 %
86.64 ± 0.55 %
ChebNet 87.07 ± 0.72 %
CayleyNet 88.09 ± 0.60 %
MoNet 
88.38 ± 0.46 %
88.65 ± 0.58 %
ﬁgures, the Cayley ﬁlters are restricted to even cosine polynomials by considering only real ﬁlter
coeﬃcients. The best CayleyNets consistently outperform the best ChebNets requiring at the
same time less parameters (CayleyNet with order r and complex coeﬃcients requires a number
of parameters equal to ChebNet with order 2r). To further complete our analysis, we present
the performance obtained by DCNN, GCN, MoNet and GAT on our extended split (Table 3).
Two convolutional layers with order r = 1, 1 head / 1 gaussian kernel, 16 and 7 outputs features
have been used for GAT and MoNet2; 3 convolutional layers with 32 and 16 hidden features have
been used for GCN; 2 diﬀusion layers with 10 hidden features and 2 diﬀusion hops for DCNN.
GCN, MoNet and GAT have been trained with mean cross-entropy, dropout probability p = 0.5
and weight decay with coeﬃcient γ = 5 · 10−4, DCNN has been trained with hinge loss and no
regularization (as reported in ). CayleyNet appears as the third best approach for solving the
considered semi-supervised classiﬁcation task, and outperforms other spectral CNN methods.
Accuracy %
Normalized Laplacian
Accuracy %
Scaled unnormalized Laplacian
Accuracy %
Normalized Laplacian
Accuracy %
Scaled unnormalized Laplacian
Figure 5: ChebNet (blue) and CayleyNet (orange) test accuracies obtained on the CORA dataset for
diﬀerent polynomial orders.
Polynomials with complex coeﬃcients (top two) and real coeﬃcients
(bottom two) have been exploited with CayleyNet in the two analysis. Orders 1 to 6 have been
used in both comparisons.
Recommender system
In our ﬁnal experiment, we applied CayleyNet to recommendation system, formulated as matrix
completion problem on user and item graphs, . The task is, given a sparsely sampled matrix of
scores assigned by users (columns) to items (rows), to ﬁll in the missing scores. The similarities
2Filters have been realized as: XW0 + ˜AXW1; with ˜A the corresponding learned adjacency matrix. Attention
has been computed for GAT only at second layer to ensure same number of parameters.
between users and items are given in the form of column and row graphs, respectively. 
approached this problem as learning with a Recurrent Graph CNN (RGCNN) architecture, using
an extension of ChebNets to matrices deﬁned on multiple graphs in order to extract spatial features
from the score matrix; these features are then fed into an RNN producing a sequential estimation of
the missing scores. Here, we repeated verbatim their experiment on the MovieLens dataset ( ),
replacing Chebyshev ﬁlters with Cayley ﬁlters.
Following , to train our model we uniformly
split the available training scores in two sets of equal dimension, 50% of the provided scores (data
scores) are used to initialize the input matrix while the remaining 50% are used as training labels.
SRGCNN is trained to reconstruct the missing labels from the few given data scores. At test time
we initialize the input matrix only with the considered data scores to provide the network the
same conditions it observed at training time. We used separable RGCNN architecture with two
CayleyNets of order r = 4 employing 15 Jacobi iterations.
Adam with learning rate equal to 10−3
and regularization coeﬃcient γ = 10−10 have been used for training3 The results are reported in
To present a complete comparison we further extended the experiments reported in 
by training sRGCNN with ChebNets of order 8, this provides an architecture with same number of
parameters as the exploited CayleyNet (23K coeﬃcients). Our version of sRGCNN outperforms
all the competing methods, including the previous result with Chebyshev ﬁlters reported in .
sRGCNNs with Chebyshev polynomials of order 4 and 8 respectively require 0.0698 +/- 0.00275
sec and 0.0877 +/- 0.00362 sec at test time, sRGCNN with Cayley polynomials of order 4 and 15
jacobi iterations requires 0.165 +/- 0.00332 sec.
Table 4: Performance (RMSE) of diﬀerent matrix completion methods on the MovieLens dataset.
IMC 
GRALS 
sRGCNNCheby,r=4 0.929
sRGCNNCheby,r=8 0.925
sRGCNNCayley
Conclusion
In this paper, we introduced a new eﬃcient spectral graph CNN architecture that scales linearly
with the dimension of the input data. Our architecture is based on a new class of complex rational
Cayley ﬁlters that are localized in space, can represent any smooth spectral transfer function, and
are highly regular. The key property of our model is its ability to specialize in narrow frequency
bands with a small number of ﬁlter parameters, while still preserving locality in the spatial domain.
Experimental results on the MNIST, CORA and MovieLens datasets show the good performance
of our construction wrt a variety of other approaches, and the superior performance with respect
to other spectral CNN methods.
Proof of Proposition 1
First note the following classical result for the approximation of Ax = b using the Jacobi method:
if the initial condition is x(0) = 0, then (x −x(k)) = Jkx. In our case, note that if we start with
initial condition ˜y(0)
= 0, the next iteration gives ˜y(0)
= bj, which is the initial condition from our
3Values obtained from MGCNN implementation available at 
construction. Therefore, since we are approximating yj = C(h∆)˜yj−1 by ˜yj = ˜y(K)
C(h∆)˜yj−1 −˜yj = JK+1C(h∆)˜yj−1
Deﬁne the approximation error in C(h∆)jf by
Cj(h∆)f −˜yj
∥Cj(h∆)f∥2
By the triangle inequality, by the fact that Cj(h∆) is unitary, and by (8)
Cj(h∆)f −C(h∆)˜yj−1
∥Cj(h∆)f∥2
+ ∥C(h∆)˜yj−1 −˜yj∥2
∥Cj(h∆)f∥2
Cj−1(h∆)f −˜yj−1
∥Cj−1(h∆)f∥2
JK+1C(h∆)˜yj−1
∥C(h∆)˜yj−1∥2
2 (1 + ej−1)
where the last inequality is due to
∥˜yj−1∥2 ≤
Cj−1(h∆)f −˜yj−1
= ∥f∥2 + ∥f∥2 ej−1.
Now, using standard norm bounds, in the general case we have
by κ = ∥J∥∞we have
ej ≤ej−1 + √n ∥J∥K+1
(1 + ej−1)
=(1 + √nκK+1)ej−1 + √nκK+1.
The solution of this recurrent sequence is
ej ≤(1 + √nκK+1)j −1 = j√nκK+1 + O(κ2K+2).
If we use the version of the algorithm, in which each ˜yj is normalized, we get by (11) ej ≤
ej−1 + √nκK+1. The solution of this recurrent sequence is
ej ≤j√nκK+1.
We denote in this case Mj = j√n
In case the graph is regular, we have D = dI. In the non-normalized Laplacian case,
J = −(hdI + iI)−1h(∆−dI) =
hd + i(dI −∆) =
The spectral radius of ∆is bounded by 2d. This can be shown as follows. a value λ is not an
eigenvalue of ∆(namely it is a regular value) if and only if (∆−λI) is invertible. Moreover, the
matrix (∆−λI) is strictly dominant diagonal for any |λ| > 2d. By Levy–Desplanques theorem
( Theorem 6.1.10), any strictly dominant diagonal matrix is invertible, which means that all of
the eigenvalues of ∆are less than 2d in their absolute value. As a result, the spectral radius of
(dI −∆) is realized on the smallest eigenvalue of ∆, namely it is |d −0| = d. This means that the
specral radius of J is
h2d2+1. As a result ∥J∥2 =
h2d2+1 = κ. We can now continue from (11)
ej ≤ej−1 + ∥J∥K+1
(1 + ej−1) = ej−1 + κK+1(1 + ej−1).
As before, we get ej ≤jκK+1 + O(κ2K+2), and ej ≤jκK+1 if each ˜yj is normalized. We denote in
this case Mj = j.
In the case of the normalized Laplacian of a regular graph, the spectral radius of ∆n is bounded
by 2, and the diagonal entries are all 1. Equation (10) in this case reads
h+i(I −∆n), and J has spectral radius
h2+1. Thus ∥J∥2 =
h2+1 = κ and we continue as
before to get
ej ≤jκK+1 and Mj = j.
In all cases, we have by the triangle inequality
Cj(h∆)f −˜yj
∥Cj(h∆)f∥2
|cj| ej ≤2
Mj |cj| κK+1.
Proof of Theorem 4
In this proof we approximate Gδm by g
Gδm. Note that the signal δm is supported on one vertex,
and in the calculation of g
Gδm, each Jacobi iteration increases the support of the signal by 1-hop.
Therefore, the support of g
Gδm is the r(K + 1)-hop neighborhood Nr(K+1),m of m. Denoting
l = r(K + 1), and using Proposition 1, we get
Gδm −Gδm|Nl,m
Gδm −Gδm|Nl,m
2 ≤4MκK+1 ∥δm∥2
= 4M(κ1/r)l.
Computational Complexity
In Figure 6 we compare the computational complexity of CayleyNey and ChebNet on our community
detection problem.
Back propagation
In this section we show how to diﬀerentiate Cayley ﬁlters Gc,h = gc,h(∆) with respect to the
complex coeﬃcient vector c = (c0, . . . , cr) and h. Since working with complex parameters is not
standard, we explain in detail how this is done. One approach is to simply treat each complex
parameter cj = cR
j as the pair of real parameters (cR
j), and to explicitly formulate
Cayley polynomial with real numbers. This brute force formulation is suitable for automatic back
propagation in software like TensorFlow. To justify the calculation from a theoretical standpoint,
it is more convenient to consider a general calculus of variation approach to gradient descent.
Our goal is to minimize the loss function with respect to all of the coeﬃcients of all ﬁlters in
the network. Note that minimization is a set operation in its nature. Namely, a minimal value of a
set doesn’t depend on additional structures endowed on the set, such as inner product, topology,
Riemannian structure, and so on. This means that we are free to use the vector space structure
and the inner product of our choice to deﬁne the gradient.
Consider a generic Cayley ﬁlter, applied on the real valued signal f
Gc,hf = c0f +
2Re{cjCj(h∆)f}.
S(c, h) = F(Gc,hf)
denote the dependency of the loss function on (c, h). Our goal is to deﬁne an inner product, and
calculate a gradient of S(c, h) with respect to the complex coeﬃcient vector a = (c, h). Given an
inner product structure on the space of coeﬃcients, a (variational) gradient at the point a of a
scalar valued function S is a vector D[S; a] such that for any other coeﬃcient vector η
S(a + ϵη) −S(a) = ϵ ⟨D[S; a], η⟩+ o(ϵ)
where ϵ denotes a scalar. If there is no such vector D[S; a], S is not diﬀerentiable at a by deﬁnition.
This deﬁnition can be extended to vector valued functions.
It can be shown that under the standard complex dot product of the coeﬃcient space, S is not
diﬀerentiable in general. However, by deﬁning a new inner product in the coeﬃcient space, there
is a way to make S diﬀerentiable. For intuition, consider the vector space C with the classical
inner product ⟨z, w⟩= zw. Here, the space of diﬀerentiable functions S : C →C are the analytic
functions. However, if we deﬁne the new inner product Re ⟨z, w⟩= zRwR + zIwI, C is isometrically
isomorphic to R2 with the standard real inner product. Moreover, if we treat the image space C
of S as R2, the space of diﬀerentiable functions S : R2 →R2 is the richer space of classical real
diﬀerentiable functions. This procedure of deﬁning a real vector space from a complex one is called
realiﬁcation ( page 117).
Given a general complex Hilbert space H, it’s realiﬁcation is the real Hilbert space HR deﬁned
to be H restricted to multiplication by real scalars. The inner product of two vectors f, g in HR is
deﬁned to be the real part of the inner product in H, Re ⟨f, g⟩.
In our case, we treat the coeﬃcient space as R2r. Indeed, h and c0 are real, and the rest of the
coeﬃcients are complex. Let us diﬀerentiate Gc,hf with respect to c by deﬁnition.
Gc+ϵη,hf −Gc,hf
2ηjCj(h∆)f
Thus, by the deﬁnition of the inner product in the (realiﬁcated) coeﬃcient space, the diﬀerential of
Gc,hf with respect to c is given by the vector Dc[Gc,hf; (h, c)] with vector valued entries
Dc[Gc,hf; (h, c)]j =
Next we show that back propagation works in the realiﬁcated space R2r in the usual way. By
(12) and by the chain rule, we can write up to o(ϵ)
S(c + ϵη) −S(c)
≈∇F(Gc,hf) · Re {Dc[Gc,hf]η}
where ∇denotes the gradient with respect to the signal, and · is the usual real dot product between
real valued signals. Let us write in short ∇F = ∇F(Gc,hf), and obtain
S(c + ϵη) −S(c)
2ηjCj(h∆)f
[∇F · f]η0 +
2[∇F · Cj(h∆)f]ηj
= Re ⟨∇F · Dc[Gc,hf; (h, c)] , η⟩.
This shows, by the deﬁnition of the inner product in the (realiﬁcated) coeﬃcient space, that
∇S = ∇F · D[p; (c, f)].
Namely, the chain rule works in the usual way in the realiﬁcated space R2r.
Next we calculate the partial derivative with respect to the spectral zoom h. We start by
standard Calculus on the function gc,h(λ), λ ∈R, and get
∂hgc,h(λ) =
cjjCj−1(hλ)C′(hλ)λ −cjjC−j−1(hλ)C′(hλ)λ
where C′(x) = (x + i)−1(1 −C(x)). We can interpret this as a calculation on the spectrum of
Gc,h = gc,h(∆). Then, by the fact that ∆is a bounded normal operator, we can carry the
calculation to Gc,h = gc,h(∆) via functional calculus. We thus obtain
Dh[Gc,hf; (h, c)] = C′(h∆)∆
Cj−1(h∆) −jcjC−j−1(h∆)
where C′(h∆) = (h∆+ iI)−1(I −C(h∆)).
Acknowledgment
MB and FM are partially supported by ERC Consolidator Grant No. 724228 (LEMAN), Google
Faculty Research Awards, Amazon AWS ML Research Award, Royal Society Wolfson Research
Merit Award, and Rudolf Diesel fellowship at the Institute for Advanced Studies, TU Munich.