GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond
Yue Cao1,3∗, Jiarui Xu2,3∗, Stephen Lin3, Fangyun Wei3, Han Hu3
1School of Software, Tsinghua University
2Hong Kong University of Science and Technology
3Microsoft Research Asia
 , , {stevelin,fawe,hanhu}@microsoft.com
The Non-Local Network (NLNet) presents a pioneering
approach for capturing long-range dependencies, via aggregating query-speciﬁc global context to each query position. However, through a rigorous empirical analysis, we
have found that the global contexts modeled by non-local
network are almost the same for different query positions
within an image. In this paper, we take advantage of this
ﬁnding to create a simpliﬁed network based on a queryindependent formulation, which maintains the accuracy of
NLNet but with signiﬁcantly less computation. We further
observe that this simpliﬁed design shares similar structure
with Squeeze-Excitation Network (SENet). Hence we unify
them into a three-step general framework for global context
modeling. Within the general framework, we design a better
instantiation, called the global context (GC) block, which
is lightweight and can effectively model the global context.
The lightweight property allows us to apply it for multiple
layers in a backbone network to construct a global context
network (GCNet), which generally outperforms both simpliﬁed NLNet and SENet on major benchmarks for various
recognition tasks. The code and conﬁgurations are released
at 
1. Introduction
Capturing long-range dependency, which aims to extract the global understanding of a visual scene, is proven
to beneﬁt a wide range of recognition tasks, such as image/video classiﬁcation, object detection and segmentation . In convolution neural networks, as the
convolution layer builds pixel relationship in a local neighborhood, the long-range dependencies are mainly modeled
by deeply stacking convolution layers. However, directly
repeating convolution layers is computationally inefﬁcient
∗Equal contribution. This work is done when Yue Cao and Jiarui Xu
are interns at Microsoft Research Asia.
Figure 1: Visualization of attention maps (heatmaps) for
different query positions (red points) in a non-local block
on COCO object detection. The three attention maps are all
almost the same. More examples are in Figure 2.
and hard to optimize . This would lead to ineffective
modeling of long-range dependency, due in part to difﬁculties in delivering messages between distant positions.
To address this issue, the non-local network is proposed to model the long-range dependencies using one
layer, via self-attention mechanism . For each query position, the non-local network ﬁrst computes the pairwise relations between the query position and all positions to form
an attention map, and then aggregates the features of all positions by weighted sum with the weights deﬁned by the
attention map. The aggregated features are ﬁnally added to
the features of each query position to form the output.
The query-speciﬁc attention weights in the non-local network generally imply the importance of the corresponding
positions to the query position. While visualizing the queryspeciﬁc importance weights would help the understanding
in depth, such analysis was largely missing in the original
paper. We bridge this regret, as in Figure 1, but surprisingly
observe that the attention maps for different query positions
are almost the same, indicating only query-independent dependency is learnt. This observation is further veriﬁed by
statistical analysis in Table 1 that the distance between the
attention maps of different query positions is very small.
Based on this observation, we simplify the non-local
block by explicitly using a query-independent attention map
for all query positions. Then we add the same aggregated
features using this attention map to the features of all query
positions for form the output. This simpliﬁed block has signiﬁcantly smaller computation cost than the original non-
 
local block, but is observed with almost no decrease in accuracy on several important visual recognition tasks. Furthermore, we ﬁnd this simpliﬁed block shares similar structure
with the popular Squeeze-Excitation (SE) Network .
They both strengthen the original features by the same features aggregated from all positions but differentiate each
other by choices on the aggregation strategy, transformation
and strengthening functions. By abstracting these functions,
we reach a three-step general framework which uniﬁes both
the simpliﬁed NL block and the SE block: (a) a context
modeling module which aggregates the features of all positions together to form a global context feature; (b) a feature
transform module to capture the channel-wise interdependencies; and (c) a fusion module to merge the global context
feature into features of all positions.
The simpliﬁed NL block and SE block are two instantiations of this general framework, but with different implementations of the three steps. By comparison study on
each step, we ﬁnd both the simpliﬁed non-local block and
the SE block are sub-optimal, that each block has a part of
the steps advancing over the other. By a combination of the
optimal implementation at each step, we reach a new instantiation of the general framework, called global context
(GC) block. The new block shares the same implementation
with the simpliﬁed NL block on the context modeling (using
global attention pooling) and fusion (using addition) steps,
while shares the same transform step (using two-layer bottleneck) with SE block. The GC block is shown to perform
better than both the simpliﬁed non-local block and SE block
on multiple visual recognition tasks.
Like SE block, the proposed GC block is also lightweight which allows it to be applied to all residual blocks in
the ResNet architecture, in contrast to the original non-local
block which is usually applied after one or a few layers due
to its heavy computation. The GC block strengthened network is named global context network (GCNet). On COCO
object detection/segmentation, the GCNet outperforms NL-
Net and SENet by 1.9% and 1.7% on APbox, and 1.5% and
1.5% on APmask, respectively, with just a 0.07% relative increase in FLOPs. In addition, GCNet yields signiﬁcant performance gains over three general visual recognition tasks:
object detection/segmentation on COCO (2.7%↑on APbbox,
and 2.4%↑on APmask over Mask R-CNN with FPN and
ResNet-50 as backbone ), image classiﬁcation on ImageNet (0.8%↑on top-1 accuracy over ResNet-50 ), and
action recognition on Kinetics (1.1%↑on top-1 accuracy
over the ResNet-50 Slow-only baseline ), with less than
a 0.26% increase in computation cost.
2. Related Work
Deep architectures. As convolution networks have recently achieved great success in large-scale visual recognition tasks, a number of attempts have been made to improve
the original architecture in a bid to achieve better accuracy
 . An important direction of network design is
to improve the functional formulations of basic components
to elevate the power of deep networks. ResNeXt and
Xception adopt group convolution to increase cardinality. Deformable ConvNets design deformable convolution to enhance geometric modeling ability. Squeeze-
Excitation Networks adopt channel-wise rescaling to
explicitly model channel dependencies.
Our global context network is a new backbone architecture, with novel GC blocks to enable more effective global
context modeling, offering superior performances on a wide
range of vision tasks, such as object detection, instance segmentation, image classiﬁcation and action recognition.
Long-range dependency modeling.
The recent approaches for long-range dependency modeling can be categorized into two classes. The ﬁrst is to adopt self-attention
mechanism to model the pairwise relations. The second is
to model the query-independent global context.
Self-attention mechanisms have recently been successfully applied in various tasks, such as machine translation
 , graph embedding , generative modeling ,
and visual recognition . is one of the
ﬁrst attempts to apply a self-attention mechanism to model
long-range dependencies in machine translation. extends self-attention mechanisms to model the relations between objects in object detection. NLNet adopts selfattention mechanisms to model the pixel-level pairwise relations. CCNet accelerates NLNet via stacking two
criss-cross blocks, and is applied to semantic segmentation.
However, NLNet actually learns query-independent attention maps for each query position, which is a waste of computation cost to model pixel-level pairwise relations.
To model the global context features, SENet , GENet
 , and PSANet perform rescaling to different channels to recalibrate the channel dependency with global context. CBAM recalibrates the importance of different
spatial positions and channels both via rescaling. However,
all these methods adopt rescaling for feature fusion which
is not effective enough for global context modeling.
The proposed GCNet can effectively model the global
context via addition fusion as NLNet (which is heavyweight and hard to be integrated to multiple layers), with
the lightweight property as SENet (which adopts scaling and is not effective enough for global context modeling).
Hence, via more effective global context modeling, GCNet
can achieve better performance than both NLNet and SENet
on major benchmarks for various recognition tasks.
3. Analysis on Non-local Networks
In this section, we ﬁrst review the design of the nonlocal block . To give an intuitive understanding, we
Figure 2: Visualization of attention maps (heatmaps) for different query positions (red points) in a non-local block on COCO
object detection. In the same image, the attention maps of different query points are almost the same. Best viewed in color.
visualize the attention maps across different query positions
generated by a widely-used instantiation of the non-local
block. To statistically analyze its behavior, we average the
distances (cosine distance and Jensen-Shannon divergence)
between the attention maps of all query positions.
(b) Simpliﬁed NL block (Eqn 2)
HW x 1 x 1
(a) NL block
Figure 3: Architecture of non-local block (Embedded Gaussian) and its simpliﬁed version. The feature maps are shown
by their dimensions, e.g. CxHxW. ⊗is matrix multiplication, and ⊕is broadcast element-wise addition. For two matrices with different dimensions, broadcast operations ﬁrst
broadcast features in each dimension to match the dimensions of the two matrices.
3.1. Revisiting the Non-local Block
The basic non-local block aims at strengthening the
features of the query position via aggregating information
from other positions.
We denote x={xi}Np
i=1 as the feature map of one input instance (e.g., an image or video),
where Np is the number of positions in the feature map (e.g.,
Np=H·W for image, Np=H·W·T for video). x and z denote
the input and output of the non-local block, respectively,
which have the same dimensions. The non-local block can
then be expressed as
zi = xi + Wz
f (xi, xj)
(Wv · xj),
where i is the index of query positions, and j enumerates all
possible positions. f (xi, xj) denotes the relationship between position i and j, and has a normalization factor C (x).
Wz and Wv denote linear transform matrices (e.g., 1x1 convolution). For simpliﬁcation, we denote ωij = f(xi,xj)
normalized pairwise relationship between position i and j.
To meet various needs in practical applications, four instantiations of the non-local block with different ωij are designed, namely Gaussian, Embedded Gaussian, Dot product, and Concat: (a) Gaussian denotes that f in ωij is
the Gaussian function, deﬁned as ωij=
exp(⟨xi,xj⟩)
m exp(⟨xi,xm⟩);
(b) Embedded Gaussian is a simple extension of Gaussian, which computes similarity in an embedding space,
deﬁned as ωij=
exp(⟨Wqxi,Wkxj⟩)
m exp(⟨Wqxi,Wkxm⟩); (c) For Dot product, f in ωij is deﬁned as a dot-product similarity, formulated as ωij= ⟨Wqxi,Wkxj⟩
; (d) Concat is deﬁned literally, as
ωij= ReLU(Wq[xi,xj])
. The most widely-used instantiation,
Embedded Gaussian, is illustrated in Figure 3(a).
The non-local block can be regarded as a global context
modeling block, which aggregates query-speciﬁc global
context features (weighted averaged from all positions via
a query-speciﬁc attention map) to each query position. As
attention maps are computed for each query position, the
time and space complexity of the non-local block are both
quadratic to the number of positions Np.
APbbox APmask
cosine distance
input output
0.397 0.062 0.177
E-Gaussian
0.402 0.012 0.020
Dot product
0.405 0.020 0.015
0.393 0.003 0.004
input output
0.345 0.056 0.056
E-Gaussian
0.358 0.003 0.004
Dot product
0.353 0.095 0.099
0.354 0.048 0.049
Table 1: Statistical analysis on four instantiations of nonlocal blocks. ‘input’ denotes the input of non-local block
(xi), ‘output’ denotes the output of the non-local block (zi−
xi), ‘att’ denotes the attention map of query positions (ωi).
3.2. Analysis
Visualization
To intuitively understand the behavior of
the non-local block, we ﬁrst visualize the attention maps
for different query positions.
As different instantiations
achieve comparable performance , here we only visualize the most widely-used version, Embedded Gaussian,
which has the same formulation as the block proposed in
 . Since attention maps in videos are hard to visualize and understand, we only show visualizations on the object detection/segmentation task, which takes images as input. Following the standard setting of non-local networks
for object detection , we conduct experiments on Mask
R-CNN with FPN and Res50, and only add one non-local
block right before the last residual block of res4.
In Figure 2, we randomly select six images from the
COCO dataset, and visualize three different query positions (red points) and their query-speciﬁc attention maps
(heatmaps) for each image. We surprisingly ﬁnd that for
different query positions, their attention maps are almost the same. To verify this observation statistically, we
analyze the distances between the global contexts of different query positions.
Statistical Analysis
Denote vi as the feature vector for
position i.
The average distance measure is deﬁned as
avg dist =
j=1 dist (vi, vj), where dist(·, ·)
is the distance function between two vectors.
Cosine distance is a widely-used distance measure,
deﬁned as dist(vi, vj)=(1 −cos(vi, vj))/2.
compute the cosine distance between three kinds of vectors, the non-local block inputs (vi=xi, ‘input’ in Table
1), the non-local block outputs before fusion (vi=zi-xi,
‘output’ in Table 1), and the attention maps of query
positions (vi=ωi, ‘att’ in Table 1). The Jensen-Shannon
divergence (JSD) is adopted to measure the statistical distance between two probability distributions, as
dist (vi, vj)= 1
vik+vjk + vjk log
As the summation over each attention map ωi is 1 (in Gaussian and E-Gaussian), we can regard each ωi as a discrete
probability distribution. Hence we compute JSD between
the attention maps (vi=ωi) for Gaussian and E-Gaussian.
Results for two distance measures on two standard tasks
are shown in Table 1. First, large values of cosine distance
in the ‘input’ column show that the input features for the
non-local block can be discriminated across different positions. But the values of cosine distance in ‘output’ are quite
small, indicating that global context features modeled by the
non-local block are almost the same for different query positions. Both distance measures on attention maps (‘att’) are
also very small for all instantiations, which again veriﬁes
the observation from visualization. In other words, although
a non-local block intends to compute the global context speciﬁc to each query position, the global context after training
is actually independent of query position. Hence, there is
no need to compute query-speciﬁc global context for each
query position, allowing us to simplify the non-local block.
4.1. Simplifying the Non-local Block
As different instantiations achieve comparable performance on both COCO and Kinetics, as shown in Table 1,
here we adopt the most widely-used version, Embedded
Gaussian, as the basic non-local block. Based on the observation that the attention maps for different query positions are almost the same, we simplify the non-local block
by computing a global (query-independent) attention map
and sharing this global attention map for all query positions. Following the results in that variants with and
without Wz achieve comparable performance, we omit Wz
in the simpliﬁed version. Our simpliﬁed non-local block is
exp (Wkxj)
m=1 exp (Wkxm)
(Wv · xj),
where Wk and Wv denote linear transformation matrices.
This simpliﬁed non-local block is illustrated in Figure 3(b).
To further reduce the computational cost of this simpli-
ﬁed block, we apply the distributive law to move Wv outside
of the attention pooling, as
zi = xi + Wv
exp (Wkxj)
m=1 exp (Wkxm)
This version of the simpliﬁed non-local block is illustrated
in Figure 4(b). The FLOPs of the 1x1 conv Wv is reduced
from O(HWC2) to O(C2).
Different from the traditional non-local block, the second
term in Eqn 3 is independent to the query position i, which
means this term is shared across all query positions i. We
thus directly model global context as a weighted average of
(a) Global context
modeling framework
(d) Global context (GC) block
(b) Simpliﬁed NL block (Eqn 3)
Context Modeling
HW x 1 x 1
conv (1x1)
Context Modeling
conv (1x1)
LayerNorm, ReLU
conv (1x1)
C/r x 1 x 1
C/r x 1 x 1
HW x 1 x 1
(c) SE block
conv (1x1)
conv (1x1)
C/r x 1 x 1
C/r x 1 x 1
Global avg pooling
Figure 4: Architecture of the main blocks. The feature maps are shown as feature dimensions, e.g. CxHxW denotes a
feature map with channel number C, height H and width W. ⊗denotes matrix multiplication, ⊕denotes broadcast elementwise addition, and ⊙denotes broadcast element-wise multiplication.
the features at all positions, and aggregate (add) the global
context features to the features at each query position. In
experiments, we directly replace the non-local (NL) block
with our simpliﬁed non-local (SNL) block, and evaluate accuracy and computation cost on three tasks, object detection
on COCO, ImageNet classiﬁcation, and action recognition,
shown in Table 2(a), 4(a) and 5. As we expect, the SNL
block achieves comparable performance to the NL block
with signiﬁcantly lower FLOPs.
4.2. Global Context Modeling Framework
As shown in Figure 4(b), the simpliﬁed non-local block
can be abstracted into three procedures: (a) global attention
pooling, which adopts a 1x1 convolution Wk and softmax
function to obtain the attention weights, and then performs
the attention pooling to obtain the global context features;
(b) feature transform via a 1x1 convolution Wv; (c) feature aggregation, which employs addition to aggregate the
global context features to the features of each position.
We regard this abstraction as a global context modeling
framework, illustrated in Figure 4(a) and deﬁned as
where (a) P
j αjxj denotes the context modeling module which groups the features of all positions together via
weighted averaging with weight αj to obtain the global context features (global attention pooling in the simpliﬁed NL
(SNL) block); (b) δ(·) denotes the feature transform to
capture channel-wise dependencies (1x1 conv in the SNL
block); and (c) F(·, ·) denotes the fusion function to aggregate the global context features to the features of each position (broadcast element-wise addition in the SNL block).
Interestingly, the squeeze-excitation (SE) block proposed in is also an instantiation of our proposed framework. Illustrated in Figure 4(c), it consists of: (a) global
average pooling for global context modeling (set αj =
in Eqn. 4), named the squeeze operation in SE block; (b)
a bottleneck transform module (let δ(·) in Eqn. 4 be one
1x1 convolution, one ReLU, one 1x1 convolution and a sigmoid function, sequentially), to compute the importance for
each channel, named the excitation operation in SE block;
and (c) a rescaling function for fusion (let F(·, ·) in Eqn. 4
be element-wise multiplication), to recalibrate the channelwise features. Different from the non-local block, this SE
block is quite lightweight, allowing it to be applied to all
layers with only a slight increase in computation cost.
4.3. Global Context Block
Here we propose a new instantiation of the global context modeling framework, named the global context (GC)
block, which has the beneﬁts of both the simpliﬁed nonlocal (SNL) block with effective modeling on long-range
dependency, and the squeeze-excitation (SE) block with
lightweight computation.
In the simpliﬁed non-local block, shown in Figure 4(b),
the transform module has the largest number of parameters, including from one 1x1 convolution with C·C parameters. When we add this SNL block to higher layers,
e.g. res5, the number of parameters of this 1x1 convolution, C·C=2048·2048, dominates the number of parameters
of this block.
To obtain the lightweight property of the
SE block, this 1x1 convolution is replaced by a bottleneck
transform module, which signiﬁcantly reduces the number
of parameters from C·C to 2·C·C/r, where r is the bottleneck
ratio and C/r denotes the hidden representation dimension
of the bottleneck. With default reduction ratio set to r=16,
the number of params for transform module can be reduced
to 1/8 of the original SNL block. More results on different
values of bottleneck ratio r are shown in Table 2(e).
As the two-layer bottleneck transform increases the dif-
ﬁculty of optimization, we add layer normalization inside
the bottleneck transform (before ReLU) to ease optimization, as well as to act as a regularizer that can beneﬁt generalization. As shown in Table 2(d), layer normalization can
signiﬁcantly enhance object detection and instance segmentation on COCO.
The detailed architecture of the global context (GC)
block is illustrated in Figure 4(d), formulated as
zi = xi + Wv2ReLU
where αj =
m eWkxm is the weight for global attention
pooling, and δ(·) = Wv2ReLU(LN(Wv1(·))) denotes the
bottleneck transform. Speciﬁcally, our GC block consists
of: (a) global attention pooling for context modeling; (b)
bottleneck transform to capture channel-wise dependencies;
and (c) broadcast element-wise addition for feature fusion.
Since the GC block is lightweight, it can be applied
in multiple layers to better capture the long-range dependency with only a slight increase in computation cost. Taking ResNet-50 for ImageNet classiﬁcation as an example,
GC-ResNet-50 denotes adding the GC block to all layers
(c3+c4+c5) in ResNet-50 with a bottleneck ratio of 16. GC-
ResNet-50 increases ResNet-50 computation from ∼3.86
GFLOPs to ∼3.87 GFLOPs, corresponding to a 0.26% relative increase.
Also, GC-ResNet-50 introduces ∼2.52M
additional parameters beyond the ∼25.56M parameters required by ResNet-50, corresponding to a ∼9.86% increase.
Global context can beneﬁt a wide range of visual recognition tasks, and the ﬂexibility of the GC block allows it to
be plugged into network architectures used in various computer vision problems. In this paper, we apply our GC block
to three general vision tasks – image recognition, object detection/segmentation and action recognition – and observe
signiﬁcant improvements in all three.
Relationship to non-local block.
As the non-local
block actually learns query-independent global context, the
global attention pooling of our global context block models
the same global context as the NL block but with significantly lower computation cost. As the GC block adopts
the bottleneck transform to reduce redundancy in the global
context features, the numbers of parameters and FLOPs are
further reduced. The FLOPs and number of parameters of
the GC block are signiﬁcantly lower than that of NL block,
allowing our GC block to be applied to multiple layers with
just a slight increase in computation, while better capturing
long-range dependency and aiding network training.
Relationship to squeeze-excitation block. The main
difference between the SE block and our GC block is the
fusion module, which reﬂects the different goals of the two
blocks. The SE block adopts rescaling to recalibrate the importance of channels but inadequately models long-range
dependency. Our GC block follows the NL block by utilizing addition to aggregate global context to all positions for
capturing long-range dependency. The second difference is
the layer normalization in the bottleneck transform. As our
GC block adopts addition for fusion, layer normalization
can ease optimization of the two-layer architecture for the
bottleneck transform, which can lead to better performance.
Third, global average pooling in the SE block is a special
case of global attention pooling in the GC block. Results
in Table 2(f) and 4(b) show the superiority of our GCNet
compared to SENet.
5. Experiments
To evaluate the proposed method, we carry out experiments on three basic tasks, object detection/segmentation
on COCO , image classiﬁcation on ImageNet , and
action recognition on Kinetics . Experimental results
demonstrate that the proposed GCNet generally outperforms both non-local networks (with lower FLOPs) and
squeeze-excitation networks (with comparable FLOPs).
5.1. Object Detection/Segmentation on COCO
We investigate our model on object detection and instance segmentation on COCO 2017 , whose train set is
comprised of 118k images, validation set of 5k images, and
test-dev set of 20k images. We follow the standard setting
 of evaluating object detection and instance segmentation
via the standard mean average-precision scores at different
boxes and the mask IoUs, respectively.
Setup. Our experiments are implemented with PyTorch
 . Unless otherwise noted, our GC block of ratio r=16 is
applied to stage c3, c4, c5 of ResNet/ResNeXt.
Training. We use the standard conﬁguration of Mask
R-CNN with FPN and ResNet/ResNeXt as the backbone architecture. The input images are resized such that
their shorter side is of 800 pixels . We trained on 8
GPUs with 2 images per GPU (effective mini batch size of
16). The backbones of all models are pretrained on ImageNet classiﬁcation , then all layers except for c1 and c2
are jointly ﬁnetuned with detection and segmentation heads.
Unlike stage-wise training with respect to RPN in , endto-end training like in is adopted for our implementation, yielding better results. Different from the conventional ﬁnetuning setting , we use Synchronized Batch-
Norm to replace frozen BatchNorm. All models are trained
for 12 epochs using Synchronized SGD with a weight decay of 0.0001 and momentum of 0.9, which roughly corresponds to the 1x schedule in the Mask R-CNN benchmark
 . The learning rate is initialized to 0.02, and decays by
a factor of 10 at the 9th and 11th epochs. The choice of
hyper-parameters also follows the latest release of the Mask
R-CNN benchmark .
(a) Block design
APbbox APbbox
APmask APmask
#param FLOPs
44.4M 279.4G
46.5M 288.7G
45.4M 279.4G
44.5M 279.4G
46.9M 279.6G
(b) Positions
APbbox APbbox
APmask APmask
#param FLOPs
44.4M 279.4G
46.9M 279.6G
46.9M 279.6G
(c) Stages
APbbox APbbox
APmask APmask
#param FLOPs
44.4M 279.4G
44.5M 279.5G
45.2M 279.5G
45.9M 279.4G
46.9M 279.6G
(d) Bottleneck design
APbbox APbbox
APmask APmask
#param FLOPs
44.4M 279.4G
64.4M 279.6G
r16 (ratio 16)
46.9M 279.6G
46.9M 279.6G
r16+LN+ReLU 39.4
46.9M 279.6G
(e) Bottleneck ratio
APbbox APbbox
APmask APmask
#param FLOPs
44.4M 279.4G
54.4M 279.6G
49.4M 279.6G
46.9M 279.6G
45.7M 279.5G
(f) Pooling and fusion
APbbox APbbox
APmask APmask
#param FLOPs
44.4M 279.4G
avg+scale (SE)
46.9M 279.5G
46.9M 279.5G
46.9M 279.6G
46.9M 279.6G
Table 2: Ablation study based on Mask R-CNN, using
ResNet-50 as backbone with FPN, for object detection and
instance segmentation on COCO 2017 validation set.
Ablation Study
The ablation study is done on COCO 2017 validation set.
The standard COCO metrics including AP, AP50, AP75 for
both bounding boxes and segmentation masks are reported.
Block design.
Following , we insert 1 non-local
block (NL), 1 simpliﬁed non-local block (SNL), or 1 global
context block (GC) right before the last residual block of
c4. Table 2(a) shows that both SNL and GC achieve performance comparable to NL with fewer parameters and less
computation, indicating redundancy in computation and parameters in the original non-local design.
Furthermore,
adding the GC block in all residual blocks yields higher performance (1.1%↑on APbbox and 0.9%↑on APmask) with a
slight increase of FLOPs and #params.
Positions. The NL block is inserted after the residual
block (afterAdd), while the SE block is integrated after the
last 1x1 conv inside the residual block (after1x1). In Table
2(b), we investigate both cases with GC block and they yield
similar results. Hence we adopt after1x1 as the default.
Stages. Table 2(c) shows the results of integrating the
GC block at different stages. All stages beneﬁt from global
context modeling in the GC block (0.7%-1.7%↑on APbbox
and APmask). Inserting to c4 and c5 both achieves better
performance than to c3, demonstrating that better semantic
features can beneﬁt more from the global context modeling.
With slight increase in FLOPs, inserting the GC block to
all layers (c3+c4+c5) yields even higher performance than
inserting to only a single layer.
Bottleneck design. The effects of each component in
the bottleneck transform are shown in Table 2(d). w/o ratio denotes the simpliﬁed NLNet using one 1x1 conv as the
transform, which has much more parameters compared to
the baseline. Even though r16 and r16+ReLU have much
fewer parameters than the w/o ratio variant, two layers are
found to be harder to optimize and lead to worse performance than a single layer. So LayerNorm (LN) is exploited
to ease optimization, leading to performance similar to w/o
ratio but with much fewer #params.
Bottleneck ratio. The bottleneck design is intended to
reduce redundancy in parameters and provide a tradeoff between performance and #params. In Table 2(e), we alter the
ratio r of bottleneck. As the ratio r decreases (from 32 to
4) with increasing number of parameters and FLOPs, the
performance improves consistently (0.8%↑on APbbox and
0.5%↑on APmask), indicating that our bottleneck strikes a
good balance of performance and parameters. It is worth
noting that even with a ratio of r=32, the network still outperforms the baseline by large margins.
Pooling and fusion. The different choices on pooling
and fusion are ablated in Table 2(f). First, it shows that addition is more effective than scaling in the fusion stage. It is
surprising that attention pooling only achieves slightly better results than vanilla average pooling. This indicates that
how global context is aggregated to query positions (choice
of fusion module) is more important than how features from
all positions are grouped together (choice in context modeling module). It is worth noting that, our GCNet (att+add)
signiﬁcantly outperforms SENet, because of effective modeling of long-range dependency with attention pooling for
context modeling, and addition for feature aggregation.
Experiments on Stronger Backbones
We evaluate our GCNet on stronger backbones, by replacing ResNet-50 with ResNet-101 and ResNeXt-101
 , adding Deformable convolution to multiple layers
(c3+c4+c5) and adopting the Cascade strategy .
The results of our GCNet with GC blocks integrated in all
layers (c3+c4+c5) with bottleneck ratios of 4 and 16 are re-
(a) test on validation set
APbbox APbbox
APmask APmask
35.9 279.4G
+GC r16 39.4
37.6 279.6G
38.3 279.6G
38.3 354.0G
+GC r16 41.1
39.6 354.3G
39.8 354.3G
39.9 357.9G
+GC r16 42.4
40.5 358.2G
40.9 358.2G
41.3 536.9G
+Cascade +GC r16 45.9
42.1 537.2G
42.7 537.3G
X101+DCN baseline
43.7 547.5G
+Cascade +GC r16 47.9
44.1 547.8G
44.0 547.8G
(b) test on test-dev set
41.8 536.9G
+Cascade +GC r16 46.5
43.1 537.2G
43.3 537.3G
X101+DCN baseline
44.3 547.5G
+Cascade +GC r16 48.3
45.0 547.8G
45.0 547.8G
Table 3: Results of GCNet (ratio 4 and 16) with stronger
backbones on COCO 2017 validation and test-dev sets.
ported. Table 3(a) presents detailed results on the validation
set. It is worth noting that even when adopting stronger
backbones, the gain of GCNet compared to the baseline
is still signiﬁcant, which demonstrates that our GC block
with global context modeling is complementary to the capacity of current models. For the strongest backbone, with
deformable convolution and cascade RCNN in ResNeXt-
101, our GC block can still boost performance by 0.8%↑
on APbbox and 0.5%↑on APmask. To further evaluate our
proposed method, the results on the test-dev set are also reported, shown in Table 3(b). On test-dev, strong baselines
are also boosted by large margins by adding GC blocks,
which is consistent with the results on validation set. These
results demonstrate the robustness of our proposed method.
5.2. Image Classiﬁcation on ImageNet
ImageNet is a benchmark dataset for image classi-
ﬁcation, containing 1.28M training images and 50K validation images from 1000 classes. We follow the standard
setting in to train deep networks on the training set and
report the single-crop top-1 and the top-5 errors on the validation set. Our preprocessing and augmentation strategy
follows the baseline proposed in and . To speed up
the experiments, all the reported results are trained via two
stages. We ﬁrst train standard ResNet-50 for 120 epochs
on 8 GPUs with 64 images per GPU (effective batch size
of 512) with 5 epochs of linear warmup. Second, we insert
newly-designed blocks into the model trained in the ﬁrst
stage and ﬁnetune for other 40 epochs with a 0.02 initial
learning rate. The baseline also follows this two-stage training but without adding new blocks in second stage. Cosine
(a) Block Design
Top-1 Acc Top-5 Acc #params(M) FLOPs(G)
(b) Pooling and fusion
Top-1 Acc Top-5 Acc #params(M) FLOPs(G)
avg+scale (SENet)
Table 4: Ablation study of GCNet with ResNet-50 on image classiﬁcation on ImageNet validation set.
#params(M)
Table 5: Results of GCNet and NLNet based on Slow-only
baseline using R50 as backbone on Kinetics validation set.
learning rate decay is used for both training and ﬁne-tuning.
Block Design. As done for block design on COCO, results on different blocks are reported in Table 4(a). GC
block performs slightly better than NL and SNL blocks with
fewer parameters and less computation, which indicates the
versatility and generalization ability of our design. By inserting GC blocks in all residual blocks (c3+c4+c5), the performance is further boosted (by 0.82%↑on top-1 accuracy
compared to baseline) with marginal computational overhead (0.26% relative increase on FLOPs).
Pooling and fusion. The functionality of different pooling and fusion methods is also investigated on image classiﬁcation. Comparing Table 4(b) with Table 2(f), it is seen
that attention pooling has greater effect in image classiﬁcation, which could be one of missing ingredients in .
Also, attention pooling with addition (GCNet) outperforms
vanilla average pooling with scale (SENet) by 0.44% on
top-1 accuracy with almost the same #params and FLOPs.
5.3. Action Recognition on Kinetics
For human action recognition, we adopt the widely-used
Kinetics dataset, which has ∼240k training videos and
20k validation videos in 400 human action categories. All
models are trained on the training set and tested on the validation set. Following , we report top-1 and top-5 recognition accuracy. We adopt the slow-only baseline in , the
best single model to date that can utilize weights inﬂated
 from the ImageNet pretrained model. This inﬂated 3D
strategy greatly speeds up convergence compared to
training from scratch. All the experiment settings explicitly
follow ; the slow-only baseline is trained with 8 frames
(8 × 8) as input, and multi(30)-clip validation is adopted.
The ablation study results are reported in Table 5. For
Kinetics experiments, the ratio of GC blocks is set to 4.
First, when replacing the NL block with the simpliﬁed NL
block and GC block, the performance can be regarded as
on par (0.19%↓and 0.11%↓in top-1 accuracy, 0.15%↑and
0.14%↑in top-5 accuracy). As in COCO and ImageNet,
adding more GC blocks further improves results and outperforms NL blocks with much less computation.
6. Conclusion
The pioneering work for long-range dependency modeling, non-local networks, intends to model query-speciﬁc
global context, but only models query-independent context.
Based on this, we simplify non-local networks and abstract
this simpliﬁed version to a global context modeling framework. Then we propose a novel instantiation of this framework, the GC block, which is lightweight and can effectively model long-range dependency. Our GCNet is constructed via applying GC blocks to multiple layers, which
generally outperforms simpliﬁed NLNet and SENet on major benchmarks for various recognition tasks.