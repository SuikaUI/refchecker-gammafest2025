Transfer Learning in Collaborative Filtering for Sparsity Reduction
Weike Pan, Evan W. Xiang, Nathan N. Liu and Qiang Yang
Department of Computer Science and Engineering
Hong Kong University of Science and Technology, Hong Kong
{weikep, wxiang, nliu, qyang}@cse.ust.hk
Data sparsity is a major problem for collaborative ﬁltering
(CF) techniques in recommender systems, especially for new
users and items. We observe that, while our target data are
sparse for CF systems, related and relatively dense auxiliary
data may already exist in some other more mature application
domains. In this paper, we address the data sparsity problem
in a target domain by transferring knowledge about both users
and items from auxiliary data sources. We observe that in
different domains the user feedbacks are often heterogeneous
such as ratings vs. clicks. Our solution is to integrate both
user and item knowledge in auxiliary data sources through
a principled matrix-based transfer learning framework that
takes into account the data heterogeneity. In particular, we
discover the principle coordinates of both users and items in
the auxiliary data matrices, and transfer them to the target
domain in order to reduce the effect of data sparsity. We
describe our method, which is known as coordinate system
transfer or CST, and demonstrate its effectiveness in alleviating the data sparsity problem in collaborative ﬁltering. We
show that our proposed method can signiﬁcantly outperform
several state-of-the-art solutions for this problem.
Introduction
Collaborative Filtering (CF) was proposed to predict the missing values in an incomplete matrix, i.e. user-item rating matrix. A major difﬁculty in CF is
the data sparsity problem, because most users can only access a limited number of items. This is especially true for
newly created online services, where overﬁtting can easily
happen when we learn a model, causing signiﬁcant performance degradation.
For a typical recommendation service provider such as
movie rental services, there may not be sufﬁcient user-item
rating records of a new customer or a new product. Mathematically, we call such data sparse, where the useful information is scattered and few. Using these data matrices
for recommendation may result in low-quality results due to
overﬁtting. To address this problem, some service providers
turn to explicitly ask the newly registered customers to rate
some selected items, such as some most sparsely rated jokes
in a joke recommender system . All rights reserved.
Goldberg 2007). However, methods like this may degrade
the customer’s experience and satisfaction with the system,
or even cause the customer churn if the customer is pushed
too much. Other methods, such as those that make use of
the implicit user feedbacks ,
may produce good results, but still rely on tracking the users’
behavior on the products to be predicted continuously.
More recently, researchers have introduced transfer learning methods for solving the data sparsity problem , , . These methods are aimed at making use
of the data from other recommender systems, referred to
as the auxiliary domain, and transfer the knowledge that
are consistent in different domains to the target domain.
 proposed to apply multi-task
learning to collaborative ﬁltering, but their studied problem was different, as it does not consider any auxiliary
information sources. Instead, it formulates a multiple binary classiﬁcation problem in the same CF matrix, one
for each user.
For the transfer learning methods, uses common latent features when factorizing multiple matrices and consider the cluster-level rating patterns as potential candidates to be transferred from the auxiliary domain.
However, there are two limitations of these methods due
to certain assumptions that are often not met in practice.
Firstly, they require the user preferences expressed in the
auxiliary and target domains to be homogeneous such as
a common rating scale of 1-5. In practice, the data from
the auxiliary domain may not be ratings at all but implicit feedbacks, such as user click records, represented as
Secondly, methods like assume that both users and items in an auxiliary data
source are related to the target data, while in practice it is
often much easier to ﬁnd an auxiliary data source with either similar users or similar items but not both. For example, there exist complementary systems such as LaunchCast
and Youtube serving different content (music vs. video) to
the same users or competitive services such as Amazon and
Barnes&Nobel selling similar products to different users.
In this paper, we propose a principled matrix factorization based framework named as coordinate system transfer
(CST) for transferring both user and item knowledge from
an auxiliary domain. In designing the CST algorithm, we
Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence (AAAI-10)
Figure 1: Illustration of the Two-Sided Transfer Learning Method (Coordinate System Transfer) in a 2-D plane.
have to overcome the aforementioned limitations of existing
methods. First, we have to decide how to integrate the heterogeneous forms of user feedbacks, which are explicit ratings in the target domain and implicit feedbacks in the auxiliary domain. Second, we must incorporate both the user
and item knowledge from the auxiliary domain in a ﬂexible
way. We observe that these two challenges are related to
each other, and are similar to the two fundamental problems
in transfer learning; that is, in deciding what to transfer and
how to transfer in transfer learning .
Our main idea is to discover the common latent information which is shared in both auxiliary and target domains.
Although the user feedbacks in auxiliary and target domains
may be heterogeneous, we ﬁnd that for many users, their
latent tastes which represent their intrinsic preference structure in some subspace are similar. For example, for movie
renters, their preferences on drama, comedy, action, crime,
adventure, documentary and romance expressed in their explicit rating records in the target domain, are similar to that
in their implicit click records on other movies in an auxiliary
domain. We assume that there is a ﬁnite set of tastes, referred to as principle coordinates, which characterize the domain independent preference structure of users and thus can
be used to deﬁne a common coordinate system for representing users. On the other hand, we can have another coordinate
system for representing items’ main factors, i.e. director, actors, prices, 3ds Max techniques, etc. In the proposed solution CST, we ﬁrst use sparse matrix tri-factorization on the
auxiliary data to discover the principle coordinates for constructing the coordinate systems for users and items, which
answers the question of what knowledge to transfer. We then
use a novel regularization technique in order to adapt the
coordinate systems for modeling target domain data, which
addresses the problem of how to transfer the knowledge.
Our Solution: Coordinate System Transfer
Problem Formulation
We use boldface uppercase letters, such as Y, to denote matrices, and Yu:, Y:i, yui to denote the uth row, ith column
and the entry located at (u, i) of Y, respectively. I denotes
the identity matrix of appropriate dimension.
In our problem setting, we have a target domain where we
wish to solve our CF problem. In addition, we also have an
auxiliary domain which is similar to the target domain. The
auxiliary domain can be partitioned into two parts: a user
part and an item part, which share common users and items,
respectively, with the target domain. We call them the user
side and item side, respectively.
We use n as the number of users and m the number of
items in the target domain, and we use R ∈Sn×m as the
observed sparse rating matrix, where S is the set of observed
user feedbacks, i.e. S = {1, 2, 3, 4, 5}. Here, rui ∈S is
the rating given by user u on item i. Y ∈{0, 1}n×m is the
corresponding indicator matrix, with yui = 1 if user u has
rated item i, and yui = 0 otherwise.
For the auxiliary domain, we use R(1), R(2) to denote
data matrices from auxiliary data sources that share common
users and items with R, respectively. The sets of observed
user feedbacks, S(1), S(2), of the two data matrices are often
not the same as S of the target domain, i.e. S(1) = S(2) =
{0, 1} . Our goal is to make use of R(1), R(2) to help predict
the missing values in R, which is illustrated in Figure 1.
Coordinate System Transfer: Our Two-sided
Transfer Learning Solution
In our solution, known as coordinate system transfer or CST,
we ﬁrst discover an auxiliary domain subspace where we can
ﬁnd some principle coordinates. These principle coordinates
can be used to bridge two domains, and ensure knowledge
transfer. Our algorithm is shown in Algorithm 1, which is
described in two major steps, as described below.
Step 1: Coordinate System Construction
In step 1, we ﬁrst ﬁnd the principle coordinates of the auxiliary domain data. The principle coordinates in a CF system
can be obtained via Singular Value Decomposition (SVD)
on a full-rating matrix, if we ask every newly joined user
to rate those system selected items. Typically, each princi-
ple coordinate represents a semantic concept related to the
user’s taste or item’s factor. However, both our auxiliary
and target domain data are represented by sparse matrices
instead of full matrices, as many rating values are missing.
Hence, we use the sparse SVD on
auxiliary data R(i), i = 1, 2,
U(i),V(i),B(i) ||Y(i) ⊙(R(i) −U(i)B(i)V(i)T )||2
where B(i) = diag(σ(i)
1 , . . . , σ(i)
j , . . . , σ(i)
d ) is a diagonal
matrix, σ(i)
≥. . . ≥σ(i)
≥0 are eigenvalues, and
U(i)T U(i) = I, V(i)T V(i) = I ensures that their columns
are othornomal. Note that each column of U(i), V(i) represents a semantic concept; i.e. user taste in collaborative ﬁltering or document theme in information retrieval. Those columns are the principle coordinates in the
low-dimensional space, and for this reason, we call our approach the coordinate system transfer.
Deﬁnition (Coordinate System) A coordinate system is a
matrix with columns of orthonormal bases (principle coordinates), where the columns are located in descending order
according to their corresponding eigenvalues.
Figure 1 shows two coordinate systems in the auxiliary
domain, one for users and the other for items. We represent
these two coordinate systems using two matrices as,
U0 = U(1), V0 = V(2) .
where the matrices U(1), V(2) consist of top d principle coordinates from (1).
Step 2: Coordinate System Adaptation
In Step 2 of the CST algorithm (Algorithm 1), we adapt the
principle coordinates discovered in the previous step to the
target domain.
After obtaining the coordinate systems from the auxiliary
data R(1), R(2), the latent user tastes and item factors are
captured by the coordinate systems and can be transferred
to the target domain R. In the target domain, we denote the
two coordinate systems as U, V for users and items, respectively, which are also required to be orthonomal according
to the deﬁnition of coordinate system, that is UT U = I,
VT V = I. Instead of requiring the two coordinate systems
from the auxiliary domain and target domain to be exactly
the same, i.e. U = U0, V = V0, we relax this requirement
and only require them to be similar. We believe that though
two domains are related, the latent user tastes and item factors in two domains can still be a bit different due to the
domain speciﬁc contexture, i.e. advertisements or promotions on the service provider’s website. Hence, we replace
the constraint U = U0, V = V0 with two additional regularization terms ||U −U0||2
F , ||V −V0||2
Further, in order to allow more freedom of rotation and
scaling, we adopt the tri-factorization method , , and allow the rating matrix to be
factorized into three parts, one for the user side coordinate
system U, a second part for the item side coordinate system
V, and the third part B to allow rotation and scaling between
Algorithm 1 CST: Coordinate System Transfer.
Input: The training data R, the auxiliary data R(1), R(2)
Output: U, V, B.
Step 1. Apply sparse SVD on auxiliary data R(1), R(2),
and obtain two principle coordinate systems U0 = U(1),
V0 = V(2). Initialize the target coordinate systems with
U = U0, V = V0.
Step 2.1. Fix U, V, and estimate B from ||Y ⊙(R −
UBVT )|| = 0 .
Step 2.2. Fix B, and update U, V via alternative gradient descent method on Grassmann manifold , .
until Convergence
the two coordinate systems. Note that the problems in , are quite different from ours,
as they require that U, V are non-negative, R is full, and
 even requires U = V in clustering.
We obtain the following optimization problem for CST
formulation,
U,V,B ||Y ⊙(R −UBVT )||
2 ||U −U0||2
2 ||V −V0||2
UT U = I, VT V = I
where B is different from B(i) in (1), as it is not required to
be diagonal, but can be full, and the effect of B is not only
scaling as that of B(i), but also rotation when fusing two coordinate systems via UBVT , and hence more ﬂexible than
two-part factorization. After we learn U, V, B, each missing entry rui in R can be predicted via Uu:BV T
i: , where Uu:,
Vi: are the user u’s latent tastes and item i’s latent factors,
respectively. The tradeoff parameters ρu and ρv represent
the conﬁdence on the auxiliary data. When ρu, ρv →∞,
(U, V) = (U0, V0), it means that the two latent coordinate systems are exactly the same in two domains. When
ρu = ρv = 0, (3) reduces to the spectral matrix completion method named OptSpace , which means that we do not make use of any
auxiliary knowledge; instead, we achieve matrix completion
with the observed ratings of the target domain data only.
Finally, the optimization problem can be solved efﬁciently
via an alternative method, by (a) ﬁxing U, V, where the
inner matrix B can then be solved analytically , and (b) ﬁxing B, where U, V can
be alternatively solved on the Grassman manifold through
a projected gradient descent method , . The complete twosided transfer learning solution is given in Algorithm 1.
The alternative method used in Algorithm 1 monotonically decreases the objective function (3), and hence ensures convergence to local minimum. The time complexity
of CST and other baseline methods are reported in Table 2,
where k is the iteration number, p(p > n, m) is the number of non-zeno entries in the rating matrix R, and d is the
number of of latent features. Note, d and k are usually quite
small, i.e. d < 20, k < 50 in our experiments.
Experimental Results
Data Sets and Evaluation Metrics
We evaluate the proposed method using two movie rating
data sets Netﬂix1 and MovieLens2. The Netﬂix rating data
contains more than 108 ratings with values in {1, 2, 3, 4, 5},
which are given by more than 4.8×105 users on around 1.8×
104 movies. The MovieLens rating data contains more than
107 ratings with values in {1, 2, 3, 4, 5}, which are given by
more than 7.1 × 104 users on around 1.1 × 104 movies. The
data set used in the experiments is constructed as follows,
• we ﬁrst randomly extract a 104 × 104 dense rating matrix
R from the Netﬂix data, and take the sub-matrices R =
R1∼5000,1∼5000 as the target rating matrix, and R(1) =
R1∼5000,5001∼10000 as the user side auxiliary data, so that
R and R(1) share only common users but not common
• we then extract an item side auxiliary data R(2) of size
5000 × 5000 from the MovieLens data by identifying
the movies appearing both in MovieLens and Netﬂix.
Clearly, R and R(2) share only common items but no
• ﬁnally, to simulate heterogenous auxiliary and target domain data, we adopt the pre-processing approach on R(1), R(2), by relabeling 1, 2, 3
ratings in R(1), R(2) as 0, and then 4, 5 ratings as 1.
In all of our experiments, the target domain rating set from
R is randomly split into training and test sets, TR, TE,
with 50% ratings, respectively. TR, TE ⊂{(u, i, rui) ∈
N × N × {1, 2, 3, 4, 5}|1 ≤u ≤n, 1 ≤i ≤m}. TE is kept
unchanged, while different number of observed ratings for
each user, 10, 20, 30, 40, are randomly picked from TR for
training, with different sparsity levels of 0.2%, 0.4%, 0.6%,
0.8% correspondingly. The ﬁnal data set used in the experiments is summarized in Table 1.
Table 1: Description of target and auxiliary data (all matrices
are of size 5000 × 5000).
target (training)
target (test)
auxiliary (user side)
auxiliary (item side)
We adopt two evaluation metrics: Mean Absolute Error
(MAE) and Root Mean Square Error (RMSE),
(u,i,rui)∈TE
|rui −ˆrui|/|TE|
(u,i,rui)∈TE
(rui −ˆrui)2/|TE|
1 
2 
where rui and ˆrui are the true and predicted ratings, respectively, and |TE| is the number of test ratings. In all experiments, we run 10 random trials when generating the required
number of observed ratings for each user from the target
training rating set TR, and averaged results are reported.
Baselines and Parameter Settings
We compare our CST method with two non-transfer learning
methods: the average ﬁlling method (AF), LFM , and the transfer learning method CMF . Note, the codebook in CBT and RMGM constructed from a 0/1 matrix of implicit feedbacks is always a
full matrix of 1s only, and does not reﬂect any cluster-level
rating patterns, hence both CBT and RMGM are not applicable to our problem. As mentioned before, CST reduces to
the spectral matrix completion method, OptSpace when no auxiliary data exist. Thus, we also report the performance of OptSpace when
studying the effect of using the auxiliary data.
We study the following six average ﬁlling (AF) methods,
(¯ru· + ¯r·i)/2
bu· + ¯r·i
¯ru· + b·i
¯r + bu· + b·i
where ¯ru· = P
i yuirui/ P
i yui is the average rating of user
u, ¯r·i = P
u yuirui/ P
u yui is the average rating of item
i, bu· = P
i yui(rui −¯r·i)/ P
i yui is the bias of user u,
u yui(rui −¯ru·)/ P
u yui is the bias of item i, and
u,i yuirui/ P
u,i yui is the global average rating. We
use ˆrui = ¯r+bu·+b·i as it performs best in our experiments.
For LFM, CMF, CST and OptSpace, different latent dimensions {5, 10, 15} are tried; for LFM and CMF, different tradeoff parameters {0.001, 0.01, 0.1, 1, 10, 100} and
{0.1, 0.5, 1, 5, 10}are tried, respectively, and best results are
reported; for CST, the performance are quite stable using
different tradeoff parameters, and the result using ρu/n =
ρv/m = 1 are reported.
The best results of using different parameters as described
in the previous section are reported in Table 2. We can make
the following observations:
• CST performs signiﬁcantly better than all other baselines
in all sparsity levels;
• for the non-transfer learning methods of AF and LFM, we
can see that AF always performs better than LFM, which
shows the usefulness of smoothing for sparse data;
• for the transfer learning method, CMF beats two nontransfer learning baselines, AF, LFM, in all sparsity levels, which demonstrates the usefulness of transfer learning methods for sparse data; however, we can see that
CMF is still worse than CST, which can be explained
Table 2: Prediction performance of average ﬁlling (AF), latent factorization model (LFM), collective matrix factorization
(CMF), and coordinate system transfer (CST). Numbers in boldface (i.e. 0.7481) are the best results among all methods.
Without Transfer
With Transfer
(sparsity)
0.7764 ± 0.0008
0.8934 ± 0.0005
0.7642 ± 0.0024
0.7481 ± 0.0014
0.7430 ± 0.0006
0.8243 ± 0.0019
0.7238 ± 0.0012
0.7056 ± 0.0008
0.7311 ± 0.0005
0.7626 ± 0.0008
0.7064 ± 0.0008
0.6907 ± 0.0006
0.7248 ± 0.0004
0.7359 ± 0.0008
0.6972 ± 0.0007
0.6835 ± 0.0008
0.9853 ± 0.0011
1.0830 ± 0.0000
0.9749 ± 0.0033
0.9649 ± 0.0019
0.9430 ± 0.0006
1.0554 ± 0.0016
0.9261 ± 0.0014
0.9059 ± 0.0013
0.9280 ± 0.0005
0.9748 ± 0.0012
0.9058 ± 0.0009
0.8855 ± 0.0010
0.9202 ± 0.0003
0.9381 ± 0.0010
0.8955 ± 0.0007
0.8757 ± 0.0011
Time Complexity
O(kpd2 + k max(n, m)d3)
O(kpd2 + k max(n, m)d3)
O(kpd3 + kd6)
by the more ﬂexibility of tri-factorization method used in
CST in (3).
Note that by setting the tradeoff parameter ρu and ρv in
(3) to 0, the CST model become equivalent to the OPTSpace
model , which considers no auxiliary domain information, neither initialization
nor regularization. To gain a deeper understanding of CST
and more carefully assess the beneﬁt from the auxiliary domain, we compared the performance of CST and OPTSpace
at different data sparsity levels when the parameter d, the
number of latent features, is increased from 5 to 15. The
results in RMSE are shown in Figure 2 (the results in MAE
are quite similar). We can see that:
• the performance of OPTSpace consistently deteriorates as
d increases, which is due to that more ﬂexible models are
more likely to suffer from overﬁtting given sparse data;
• in contrast to OPTSpace, CST consistently improves as d
increases which demonstrates how the auxiliary domain
knowledge based initialization and regularization techniques can help avoid overﬁtting even for highly ﬂexible
• the relative improvement is more signiﬁcant when fewer
ratings are observed, which conﬁrms the effectiveness of
CST in alleviating data sparsity.
Related Work
Latent factorization model (LFM) is
a widely used method in collaborative ﬁltering, which seeks
an appropriate low-rank approximation of the rating matrix
R with two latent feature matrices, one for users and one for
items. For any missing entry in R, it can be predicted by the
production of those two latent feature matrices.
Collective Matrix Factorization (CMF) is a multi-task learning (MTL) 
version of LFM, which jointly factorizes multiple matrices with correspondences between rows and columns while
sharing latent features of matching rows and columns in different matrices. Note, there are at least three differences
compared to the CST method, (a) CMF is an MTL style algorithm, which does not distinguish auxiliary domain from
(a) Observed = 10.
(b) Observed = 20.
(c) Observed = 30.
(d) Observed = 40.
Figure 2: Comparison of CST and OPTSpace at different
sparsity level with increasing d .
target domain, whereas CST is an adaptation style algorithm, which focuses on improving performance in the target domain by transferring knowledge from but not to the
auxiliary domain. Hence CST is more efﬁcient especially
when the auxiliary data is dense, and more secure for privacy considerations, (b) CMF is less ﬂexible in that it requries R, R(1) and R, R(2) to share exactly the same user
and item latent features, respectively, while CST relaxes
this assumption and only requires two corresponding coordinate systems to be similar, (c) CMF is a bi-factorization
method (i.e. R = UVT ), so the latent features U, V have
to capture both domain dependent effects and independent
shared knowledge, while CST inherits the ﬂexibility of trifactorization (i.e., R = UBVT ) method 
by absorbing domain dependent effects such as advertisements and promotions into the inner matrix B, while U, V
are principle coordinates of users and items that are more
consistent across different domains.
Codebook Transfer (CBT) is
a recently developed heterogenous transfer learning method
for collaborative ﬁltering, which contains two steps of codebook construction and codebook expansion, and achieves
knowledge transfer with the assumption that both auxiliary and target data share the cluster-level rating patterns
(codebook). Rating-Matrix Generative Model (RMGM) is derived and extended from the
FMM generative model , and we can consider RMGM as an MTL version of CBT with the same assumption. Note, both CBT and RMGM are limited to explicit rating matrices only, and can not achieve knowledge
transfer from an implicit rating matrix with values of 0/1 to
an explicit one with values of 1-5, as it requires two rating
matrices to share the cluster-level rating patterns. Also, CBT
and RMGM can neither make use of user side nor item side
shared information, and only take a general explicit rating
matrix as its auxiliary input. Hence, both CBT and RMGM
are not applicable to the problem studied in this paper.
Table 3: Summary of some related work.
Algorithm Style
Adaptation
Multi-Task
Latent Features
We summarize the above related work in Table 3 from
the perspective of transferred knowledge (what to transfer)
and algorithm style (how to transfer).
CST can also be
considered as a two-sided extension in matrix form of onesided domain adaptation methods in vector form , which is proposed not for collaborative ﬁltering but classiﬁcation problems and achieve knowledge transfer via incorporatingthe model parameters learned
from the auxiliary domain as prior knowledge.
Conclusions and Future Work
In this paper, we presented a novel transfer learning method,
CST, for alleviating the data sparsity problem in collaborative ﬁltering. Our method ﬁrst ﬁnds a subspace where
coordinate systems are used for knowledge transfer, then
uses the transferred knowledge to adapt to the target domain
data. The novelty of our algorithm includes using both the
user and item side information in an integrated way. Experimental results show that CST performs signiﬁcantly better than several state-of-the-art methods at various sparsity
levels. Our experimental study clearly demonstrates (a) the
usefulness of transferring two coordinate systems from the
auxiliary data (what to transfer), and (b) the effectiveness
of incorporating two-sided auxiliary knowledge via a regularized tri-factorization method, thus addressing the how to
transfer question for CF. For future works, we will study on
how to extend CST in other heterogeneous settings, e.g., for
transferring knowledge between movies, music, books, etc.
Acknowledgement
We thank the support of RGC-NSFC Joint Research Grant
N HKUST624/09.