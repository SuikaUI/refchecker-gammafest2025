Low-rank Bilinear Pooling for Fine-Grained Classiﬁcation
Shu Kong, Charless Fowlkes
Dept. of Computer Science
University of California, Irvine
{skong2, fowlkes}@ics.uci.edu
Pooling second-order local feature statistics to form
a high-dimensional bilinear feature has been shown to
achieve state-of-the-art performance on a variety of ﬁnegrained classiﬁcation tasks. To address the computational
demands of high feature dimensionality, we propose to represent the covariance features as a matrix and apply a lowrank bilinear classiﬁer. The resulting classiﬁer can be evaluated without explicitly computing the bilinear feature map
which allows for a large reduction in the compute time as
well as decreasing the effective number of parameters to be
To further compress the model, we propose classiﬁer
co-decomposition that factorizes the collection of bilinear
classiﬁers into a common factor and compact per-class
terms. The co-decomposition idea can be deployed through
two convolutional layers and trained in an end-to-end architecture.
We suggest a simple yet effective initialization that avoids explicitly ﬁrst training and factorizing the
larger bilinear classiﬁers. Through extensive experiments,
we show that our model achieves state-of-the-art performance on several public datasets for ﬁne-grained classiﬁcation trained with only category labels. Importantly, our
ﬁnal model is an order of magnitude smaller than the recently proposed compact bilinear model , and three orders smaller than the standard bilinear CNN model .
1. Introduction and Related Work
Fine-grained categorization aims to distinguish subordinate categories within an entry-level category, such as
identifying the bird species or particular models of aircraft.
Compared to general purpose visual categorization problems, ﬁne-grained recognition focuses on the characteristic challenge of making subtle distinctions (low inter-class
variance) despite highly variable appearance due to factors
such as deformable object pose (high intra-class variance).
Fine-grained categorization is often made even more challenging by factors such as large number of categories and
the lack of training data.
One approach to dealing with such nuisance parameters
has been to exploit strong supervision, such as detailed partlevel, keypoint-level and attribute annotations .
These methods learn to localize semantic parts or keypoints
and extract corresponding features which are used as a
holistic representation for ﬁnal classiﬁcation. Strong supervision with part annotations has been shown to signiﬁcantly
improve the ﬁne-grained recognition accuracy. However,
such supervised annotations are costly to obtain.
To alleviate the costly collection of part annotations,
some have proposed to utilize interactive learning . Partially supervised discovery of discriminative parts from category labels is also a compelling approach , especially given the effectiveness of training with web-scale
datasets . One approach to unsupervised part discovery uses saliency maps, leveraging the observation
that sparse deep CNN feature activations often correspond
to semantically meaningful regions . Another recent approach selects parts from a pool of patch candidates by searching over patch triplets, but relies heavily
on training images being aligned w.r.t the object pose. Spatial transformer networks are a very general formulation that explicitly model latent transformations that align
feature maps prior to classiﬁcation. They can be trained
end-to-end using only classiﬁcation loss and have achieved
state-of-the-art performance on the very challenging CUB
bird dataset , but the resulting models are large and stable optimization is non-trivial.
Recently, a surprisingly simple method called bilinear
pooling has achieved state-of-the-art performance on
a variety of ﬁne-grained classiﬁcation problems.
Bilinear pooling collects second-order statistics of local features
over a whole image to form holistic representation for classiﬁcation. Second-order or higher-order statistics have been
explored in a number of vision tasks (see e.g. ). In the
context of ﬁne-grained recognition, spatial pooling introduces invariance to deformations while second-order statistics maintain selectivity.
However, the representational power of bilinear features
 
Figure 1: We explore models that perform classiﬁcation using second order statistics of a convolutional feature map (a)
as input (e.g., VGG16 layer conv5 3). Architecture of (b) full bilinear model , (c) recently proposed compact bilinear
model , and (d) our proposed low-rank bilinear pooling model (LRBP). Our model captures second order statistics without
explicitly computing the pooled bilinear feature, instead using a bilinear classiﬁer that uses the Frobenius norm as the classi-
ﬁcation score. A variant of our architecture that exploits co-decomposition and computes low-dimensional bilinear features
is sketched in Figure 4.
comes at the cost of very high-dimensional feature representations (see Figure 1 (b)), which induce substantial
computational burdens and require large quantities of training data to ﬁt. To reduce the model size, Gao et al. 
proposed using compact models based on either random
Maclaurin or tensor sketch . These methods approximate the classiﬁer applied to bilinear pooled feature
by the Hadamard product of projected local features with a
large random matrix (Figure 1 (c)). These compact models maintain similar performance to the full bilinear feature
with a 90% reduction in the number of learned parameters.
The original bilinear pooling work of Lin et al. and the
compact models of Gao et al. ignore the algebraic structure
of the bilinear feature map; instead they simply vectorize
and apply a linear classiﬁer. Inspired by work on the bilinear SVM , we instead propose to use a bilinear classiﬁer
applied to the bilinear feature which is more naturally represented as a (covariance) matrix. This representation not
only preserves the structural information, but also enables
us to impose low-rank constraint to reduce the degrees of
freedom in the parameter to be learned.
Our model uses a symmetric bilinear form so computing
the conﬁdence score of our bilinear classiﬁer amounts to
evaluating the squared Frobenius norm of the projected local features. We thus term our mechanism maximum Frobenius margin. This means that, at testing time, we do not
need to explicitly compute the bilinear features, and thus
computation time can be greatly reduced under some circumstances, e.g. channel number is larger than spatial size.
We show empirically this results in improved classiﬁcation
performance, reduces the model size and accelerates feedforward computation at test time.
To further compress the model for multi-way classiﬁcation tasks, we propose a simple co-decomposition approach
to factorize the joint collection of classiﬁer parameters to
obtain a even more compact representation. This multilinear co-decomposition can be implemented using two separate linear convolutional layers, as shown in Figure 1 (d).
Rather than ﬁrst training a set of classiﬁers and then performing co-decomposition of the parameters, we suggest a
simple yet effective initialization based on feature map activation statistics which allows for direct end-to-end training.
We show that our ﬁnal model achieves the state-of-theart performance on several public datasets for ﬁne-grained
classiﬁcation by using only the category label. It is worth
noting that the set of parameters learned in our model is ten
times smaller than the recently proposed compact bilinear
model , and a hundred times smaller than the original
full bilinear CNN model .
2. Bilinear Features Meet Bilinear SVMs
To compute the bilinear pooled features for an image,
we ﬁrst feed the image into a convolutional neural network
(CNN), as shown in Figure 1 (a), and extract feature maps at
a speciﬁc layer, say VGG16 conv5 3 after rectiﬁcation. We
denote the feature map by X ∈Rh×w×c, where h, w and
c indicate the height, width and number of feature channels
and denote the feature vector at a speciﬁc location by xi ∈
Rc where the spatial coordinate index i ∈[1, hw]. For each
local feature we compute the outer product, xixT
(pool) the resulting matrices over all hw spatial locations to
produce a holistic representation of the image of dimension
c2. This computation can be written in matrix notation as
i , where X ∈Rc×hw is a matrix by
reshaping X in terms of the third mode. XXT captures
the second-order statistics of the feature activations and is
closely related to the sample covariance matrix.
In the bilinear CNN model as depicted in Figure 1
(b), the bilinear pooled feature is reshaped into a vector
z = vec(XXT ) ∈Rc2 prior to being fed into a linear classiﬁer 1.
1Various normalization can be applied here, e.g. sign square root power
normalization and ℓ2 normalization. We ignore for now the normalization notations for presentational brevity, and discuss normalization in Section 5.1.
Figure 2: The mean and standard deviation of the eigenvalues the weight matrix W for 200 linear SVM classiﬁers
applied to bilinear features. As the plot suggests, a large
part of the spectrum is typically concentrated around 0 with
a few large positive and negative eigenvalues. The middle
of the spectrum is excluded here for clarity.
Given N training images, we can learn a linear classiﬁer
for a speciﬁc class parameterized by w ∈Rc2 and b. Denote
the bilinear feature for image-i by zi and its binary class
label as yi = ±1 for i = 1, . . . , N. The standard softmargin SVM training objective is given by:
max(0, 1 −yiwT zi + b) + λ
2.1. Maximum Frobenius Margin Classiﬁer
We can write an equivalent objective to Equation 1 using
the matrix representation of the bilinear feature as:
max(0, 1 −yitr(WT XiXT
i ) + b) + λ
It is straightforward to show that Equation 2 is a convex
optimization problem w.r.t. the parameter W ∈Rc×c and
is equivalent to the linear SVM.
Theorem 1 Let w∗
∈Rc2 be the optimal solution of
the linear SVM in Equation 1 over bilinear features, then
W∗= mat(w∗) ∈Rc×c is the optimal solution in Equation 2. Moreover, W∗= W∗T .
To give some intuition about this claim, we write the optimal solution to the two SVM problems in terms of the
Lagrangian dual variables α associated with each training
where αi ≥0, ∀i = 1, . . . , N,
rank of linear SVM
Figure 3: Average accuracy of low-rank linear SVMs. In
this experiment we simply use singular value decomposition applied to the set of full rank SVM’s for all classes
to generate low-rank classiﬁers satisfying a hard rank constraint (no ﬁne-tuning).
Very low rank classiﬁers still
achieve good performance.
As z = vec(XXT ), it is easy to see that w∗= vec(W∗) 2.
Since W∗is a sum of symmetric matrices, it must also be
symmetric.
From this expansion, it can be seen that W∗is the difference of two positive semideﬁnite matrices corresponding to
the positive and negative training examples. It is informative to compare Equation 3 with the eigen decomposition of
W∗=ΨΣΨT = Ψ+Σ+ΨT
+ + Ψ−Σ−ΨT
+ −Ψ−|Σ−|ΨT
where Σ+ and Σ−are diagonal matrices containing only
positive and negative eigenvalues, respectively, and Ψ+ and
Ψ−are the eigenvectors corresponding to those eigenvalues. Setting U+ = Ψ+Σ
+ and U−= Ψ−|Σ−|
2 , we have
In general it will not be the case that the positive
and negative components of the eigendecomposition correspond to the dual decomposition (e.g., that U+UT
yi=1 αiXiXT
i ) since there are many possible decompositions into a difference of psd matrices. However, this decomposition motivates the idea that W∗may well have a
good low-rank decomposition. In particular we know that
rank(W∗) < min(N, c) so if the amount of training data
is small relative to c, W∗will necessarily be low rank. Even
with large amounts of training data, SVMs often produce
dual variables α which are sparse so we might expect that
the number of non-zero αs is less than c.
Low rank parameterization:
To demonstrate this lowrank hypothesis empirically, we plot in Figure 2 the sorted
average eigenvalues with standard deviation of the 200 classiﬁers trained on bilinear pooled features from the CUB
vec(mat(w)) = w.
Bird dataset . From the ﬁgure, we can easily observe
that a majority of eigenvalues are close to zero and an order
smaller in magnitude than the largest ones.
This motivates us to impose low-rank constraint to reduce the degrees of freedom in the parameters of the classi-
ﬁer. We use singular value decomposition to generate a low
rank approximation of each of the 200 classiﬁers, discarding those eigenvectors whose corresponding eigenvalue has
small magnitude. As shown in Figure 3, a rank 10 approximation of the learned classiﬁer achieves nearly the same
classiﬁcation accuracy as the full rank model. This suggests
the set of classiﬁers can be represented by 512 × 10 × 200
parameters rather than the full set of 5122×200 parameters.
Low-rank Hinge Loss:
In this paper, we directly impose
a hard low-rank constraint rank(W) = r ≪c by using
the parameterization in terms of U+ and U−, where U+ ∈
Rc×r/2 and U−∈Rc×r/2. This yields the following (nonconvex) learning objective:
H(Xi, U+, U−, b) + λ
2 R(U+, U−)
where H(·) is the hinge loss and R(·) is the regularizer. The
hinge loss can be written as:
H(Xi, U+, U−, b) ≡max(0, 1 −yi{tr( ˜
WT ˜X)} + b)
While the hinge loss is convex in ˜
W, it is no longer convex
in the parameters U+, U−we are optimizing.3
Alternately, we can write the score of the low-rank bilinear classiﬁer as a difference of matrix norms which yields
the following expression of the hinge-loss:
H(Xi, U+, U−, b)
= max(0, 1 −yi{tr(U+UT
i ) −tr(U−UT
= max(0, 1 −yi{∥UT
This expression highlights a key advantage of the bilinear
classiﬁer, namely that we never need to explicitly compute
the pooled bilinear feature XiXT
Regularization:
In the hinge-loss, the parameters U+
and U−are independent of each other. However, as noted
previously, there exists a decomposition of the optimal full
rank SVM in which the positive and negative subspaces are
3Instead of a hard rank constraint, one could utilize the nuclear norm
as a convex regularizer on ˜
W. However, this wouldn’t yield the computational beneﬁts during training that we highlight here.
orthogonal. We thus modify the standard ℓ2 regularization
to include a positive cross-term ∥UT
F that favors an
orthogonal decomposition. 4 This yields the ﬁnal objective:
H(Xi, U+, U−, b)
2.2. Optimization by Gradient Descent
We call our approach the maximum Frobenius norm
SVM. It is closely related to the bilinear SVM of Wolf et
al. , which uses a bilinear decomposition W ≈UVT .
Such non-convex bilinear models with hard rank constraints
are often optimized via alternating descent 
or ﬁt using convex relaxations based on the nuclear norm
 . However, our parameterization is actually quadratic
in U+, U−and hence can’t exploit the alternating or cyclic
descent approach.
Instead, we optimize the objective function 9 using
stochastic gradient descent to allow end-to-end training of
both the classiﬁer and CNN feature extractor via standard
backpropagation. As discussed in the literature, model performance does not appear to suffer from non-convexity during training and we have no problems ﬁnding local minima
with good test accuracy . The partial derivatives of our
model are straightforward to compute efﬁciently
∇U+ =2λ(U+UT
+U+ + U−UT
if H(Xi, U+, U−, b) ≤0
i U+, if H(Xi, U+, U−, b) > 0
∇U−=2λ(U−UT
if H(Xi, U+, U−, b) ≤0
i U−, if H(Xi, U+, U−, b) > 0
if H(Xi, U+, U−, b) ≤0
−yi, if H(Xi, U+, U−, b) > 0
3. Classiﬁer Co-Decomposition for Model
Compression
In many applications such as ﬁne-grained classiﬁcation,
we are interested in training a large collection of classiﬁers
and performing k-way classiﬁcation. It is reasonable to expect that these classiﬁers should share some common structure (e.g., some feature map channels may be more or less
informative for a given k-way classiﬁcation task). We thus
propose to further reduce the number of model parameters
4The original ℓ2 regularization is given by ∥W∥2
F where the
cross-term actually discourages orthogonality.
Figure 4: Another conﬁguration of our proposed architecture that explicitly computes the bilinear pooling over codecomposed features of lower dimension.
by performing a co-decomposition over the set of classi-
ﬁers in order to isolate shared structure, similar to multi-task
learning frameworks (e.g., ).
Suppose we have trained K Frobenius norm SVM classiﬁers for each of K classes. Denoting the kth classiﬁer
parameters as Uk = [U+k, U−k] ∈Rc×r, we consider the
following co-decomposition:
∥Uk −PVk∥2
where P ∈Rc×m is a projection matrix that reduces the
feature dimensionality from c to m < c, and Vk ∈Rm×r
is the new lower-dimensional classiﬁer for the kth class.
Although there is no unique solution to problem Equation 11, we can make the following statement
Theorem 2 The optimal solution of P to Equation 11 spans
the subspace of the singular vectors corresponding to the
largest m singular values of [U1, . . . , UK].
Therefore, without loss of generality, we can add a constraint that P is a orthogonal matrix without changing the
value of the minimum and use SVD on the full parameters
of the K classiﬁers to obtain P and Vk’s.
In practice, we would like to avoid ﬁrst learning full
classiﬁers Uk and then solving for P and {Vk}. Instead,
we implement P ∈Rc×m in our architecture by adding a
1 × 1 × c × m convolution layer, followed by the new bilinear classiﬁer layer parameterized by Vk’s. In order to
provide a good initialization for P, we can run the CNN
base architecture on training images and perform PCA on
the resulting feature map activations in order to estimate a
good subspace for P. We ﬁnd this simple initialization of P
with randomly initialized Vk’s followed by ﬁne-tuning the
whole model achieves state-of-the-art performance.
4. Analysis of Computational Efﬁciency
In this section, we study the computational complexity
and model size in detail, and compare our model to several
closely related bilinear methods, including the full bilinear
model and two compact bilinear models by Random
Maclaurin and Tensor Sketch.
We consider two variants of our proposed low-rank bilinear pooling (LRBP) architecture. In the ﬁrst, dubbed
LRBP-I and depicted in Figure 1 (d), we use the Frobenius
norm to compute the classiﬁcation score (see Equation 8).
This approach is preferred when hw < m. In the second,
dubbed LRBP-II and depicted in Figure 4, we apply the feature dimensionality reduction using P and then compute the
pooled bilinear feature explicitly and compute the classiﬁcation score according to second line of Equation 8. This
has a computational advantage when hw > m.
Table 1 provides a detailed comparison in terms of feature dimension, the memory needed to store projection
and classiﬁer parameters, and computational complexity
of producing features and classiﬁer scores. In particular,
we consider this comparison for the CUB200-2011 bird
dataset which has K = 200 classes. A conventional
setup for achieving good performance of the compact bilinear model is that d = 8, 192 as reported in . Our model
achieves similar or better performance using a projection
P ∈R512×100, so that m = 100, and using rank r = 8 for
all the classiﬁers.
From Table 1, we can see that Tensor Sketch and our
model are most appealing in terms of model size and computational complexity. It is worth noting that the size of
our model is a hundred times smaller than the full bilinear model, and ten times smaller than Tensor Sketch. In
practice, the complexity of computing features in our model
O(hwmc + hwm2) is not much worse than Tensor Sketch
O(hw(c + d log(d)), as m2 ≈d, mc < d log(d) and
m ≪c. Perhaps the only trade-off is the computation in
classiﬁcation step, which is a bit higher than the compact
5. Experiment Evaluation
In this section, we provide details of our model implementation along with description of methods we compare to. We then investigate design-choices of our model,
i.e. the classiﬁer rank and low-dimensional subspace determined by projection P. Finally we report the results on
four commonly used ﬁne-grained benchmark datasets and
describe several methods for generating qualitative visualizations that provide understanding of image features driving model performance.
5.1. Implementation Details
We implemented our classiﬁer layers within matconvnet
toolbox and train using SGD on a single Titan X GPU.
We use the VGG16 model which is pretrained on ImageNet, removing the fully connected layers, and inserting
a co-decomposition layer, normalization layer and our bilinear classiﬁers. We use PCA to initialize P as described
Table 1: A comparison of different compact bilinear models in terms of dimension, memory, and computational complexity.
The bilinear pooled features are computed over feature maps of dimension h × w × c for a K-way classiﬁcation problem.
For the VGG16 model on an input image of size 448 × 448 we have h = w = 28 and c = 512. The Random Maclaurin and
Tensor Sketch models, which are proposed in based on polynomial kernel approximation, compute a feature of dimension
d. It is shown that these methods can achieve near-maximum performance with d = 8, 192. For our model, we set m = 100
and r = 8, corresponding to the reduced feature dimension and the rank of our low-rank classiﬁer, respectively. Numbers in
brackets indicate typical values when bilinear pooling is applied after the last convolutional layer of VGG16 model over the
CUB200-2011 bird dataset where K = 200. Model size only counts the parameters above the last convolutional layer.
Full Bilinear
Random Maclaurin
Tensor Sketch
Feature Dim
Feature computation
O(hw(c + d log d))
O(hwmc + hwm2)
Classiﬁcation comp.
Feature Param
2cd [40MB]
cm [200KB]
cm [200KB]
Classiﬁer Param
Kd [K·32KB]
Kd [K·32KB]
Krm [K·3KB]
Krm [K·3KB]
Total (K = 200)
Kc2 [200MB]
2cd + Kd [48MB]
2c + Kd [8MB]
cm + Krm [0.8MB]
cm + Krm [0.8MB]
in Section 3, and randomly initialize the classiﬁers. We initially train only the classiﬁers, and then ﬁne-tune the whole
network using a batch size of 12 and a small learning rate
of 10−3, periodically annealed by 0.25, weight decay of
5 × 10−4 and momentum 0.9. The code and trained model
will be released to the public.
We ﬁnd that proper feature normalization provides a
non-trivial improvement in performance. Our observation
is consistent with the literature on applying normalization to
deal with visual burstiness . The full bilinear CNN
and compact bilinear CNN consistently apply sign square
root and ℓ2 normalization on the bilinear features. We can
apply these normalization methods for our second conﬁguration (described in Section 4). For our ﬁrst conﬁguration,
we don’t explicitly compute the bilinear feature maps. Instead we ﬁnd that sign square root normalization on feature
maps at conv5 3 layer results in performance on par with
other bilinear pooling methods while additional ℓ2 normalization harms the performance.
5.2. Conﬁguration of Hyperparameters
Two hyperparameters are involved in specifying our architecture, the dimension m in the subspace determined by
P ∈Rc×m and the rank r of the classiﬁers Vk ∈Rm×r
for k = 1, . . . , K. To investigate these two parameters in
our model, we conduct an experiment on CUB-200-2011
bird dataset , which contains 11, 788 images of 200 bird
species, with a standard training and testing set split. We do
not use any part annotation or masks provided in the dataset.
We ﬁrst train a full-rank model on the bilinear pooled
features and then decompose each classiﬁer using eigenvalue decomposition and keep the largest magnitude eigenvalues and the corresponding vectors to produce a rank-r
classiﬁer. After obtaining low-rank classiﬁers, we apply codecomposition as described in Section 3 to obtain projector
P and compact classiﬁers Vk’s. We did not perform ﬁnetuning of these models but this quick experiment provides
a good proxy for ﬁnal model performance over a range of
architectures.
We plot the classiﬁcation accuracy vs. rank r and reduced dimension m (rDim) in Figure 5, the average reconstruction ﬁdelity measured by peak signal-to-noise ratio to
the original classiﬁer parameters Uk versus rank r and dimension m in Figure 6, and model size versus rank r and
dimension m in Figure 7.
As can be seen, the reconstruction ﬁdelity (measured in
the peak signal-to-noise ratio) is a good guide to model performance prior to ﬁne tuning. Perhaps surprisingly, even
with r = 8 and m = 100, our model achieves nearmaximum classiﬁcation accuracy on this dataset (Figure 5)
with model parameters compressed by a factor of 100 over
the full model (Figure 7). Based on this analysis, we set
r = 8 and m = 100 for our quantitative benchmark experiments.
5.3. Baseline Methods
We use VGG16 as the base model in all comparison
to be consistent with previous work .
Fully Connected layers (FC-VGG16):
We replace the
last fully connected layer of VGG16 base model with a randomly initialized K-way classiﬁcation layer and ﬁne-tune.
We refer this as “FC-VGG16” which is commonly a strong
baseline for a variety of computer vision tasks. As VGG16
only takes input image of size 224×224, we resize all inputs
for this method.
Improved Fisher Encoding (Fisher):
Fisher encoding has recently been used as an encoding and pooling alternative to the fully connected layers . Consistent
with , we use the activations at layer conv5 3 (prior
Figure 5: Classiﬁcation accuracy on
CUB-200 dataset vs. reduced dimension (m) and rank (r).
Figure 6: Reconstruction ﬁdelity of
classiﬁer parameters measured by
peak signal-to-noise ratio versus reduced dimension (m) and rank (r).
Figure 7: The learned parameter size
versus reduced dimension (m) and
Table 3: Summary statistics of datasets.
# train img.
# test img.
Airplane 
to ReLU) as local features and set the encoding to use 64
GMM components for the Fisher vector representation.
Full Bilinear Pooling (Full Bilinear):
We use full bilinear pooling over the conv5 3 feature maps (termed “symmetric structure” in ) and apply element-wise sign
square root normalization and ℓ2 normalization prior to
classiﬁcation.
Compact Bilinear Pooling:
We report two methods proposed in using Random Maclaurin and Tensor Sketch.
Like Full Bilinear model, element-wise sign square root
normalization and ℓ2 normalization are used. We set the
projection dimension d = 8, 192, which is shown to be suf-
ﬁcient for reaching close-to maximum accuracy . For
some datasets, we use the code released by the authors to
train the model; otherwise we display the performance reported in .
5.4. Quantitative Benchmarking Experiment
We compare state-the-art methods on four widely used
ﬁne-grained classiﬁcation benchmark datasets, CUB-200-
2011 Bird dataset , Aircrafts , Cars , and describing texture dataset (DTD) . All these datasets provide ﬁxed train and test split. We summarize the statistics of
datasets in Table 3. In training all models, we only use the
category label without any part or bounding box annotation
provided by the datasets. We list the performance of these
methods in Table 2 and highlight the parameter size of the
models trained on CUB-200 dataset in the last row.
From the comparison, we can clearly see that Fisher
vector pooling not only provides a smaller model than
FC-VGG16, but also consistently outperforms it by a notable margin. All the bilinear pooling methods, including
ours, achieve similar classiﬁcation accuracy, outperforming Fisher vector pooling by a signiﬁcant margin on these
datasets except DTD. However, our model is substantially
more compact than the other methods based on bilinear features. To the best of our knowledge, our model achieves the
state-of-the-art performance on these datasets without part
annotation , and even outperforms several recently
proposed methods trained that use supervised part annotation . Although there are more sophisticated methods in
literature using detailed annotations such as parts or bounding box , our model relies only on the category label. These advantages make our model appealing not only
for memory-constrained devices, but also in weakly supervised ﬁne-grained classiﬁcation in which detailed part annotations are costly to obtain while images with category
label are nearly free and computation during model training
becomes the limiting resource.
5.5. Qualitative Visualization
To better understand our model, we adopt three different approaches to visualizing the model response for speciﬁc input images.
In the ﬁrst method, we feed an input image to the trained model, and compute responses
Y = [U+1, U−1, . . . , U+k, U−k, . . . , U+K, U−K]T X
from the bilinear classiﬁer layer.
Based on the groundtruth class label, we create a modiﬁed response ¯Y by zeroing out the part corresponding to negative Frobenius score
F ) for the ground-truth class, and the part to the
positive Frobenius scores (∥UT
F ) in the remaining classiﬁers, respectively. This is similar to approaches used for
visualizing HOG templates by separating the positive and
Table 2: Classiﬁcation accuracy and parameter size of: a fully connected network over VGG16 , Fisher vector , Full
bilinear CNN , Random Maclaurin , Tensor Sketch , and our method. We run Random Maclaurin and Tensor Sketch
with the code provided in with their conventional conﬁguration (e.g. projection dimension d = 8192).
Full Bilinear
Random Maclaurin
Tensor Sketch
LRBP (Ours)
Airplane 
param. size (CUB)
Figure 8: (Best seen in color.) In each panel depicting a different bird species, the four columns show the input images
and the visualization maps using three different methods as described in Section 5.5. We can see our model tends to ignore
features in the cluttered background and focus on the most distinct parts of the birds.
negative components of the weight vector. To visualize the
result, we treat ¯Y as the target and backpropagate the difference to the input image space, similar to . For the
second visualization, we compute the magnitude of feature
activations averaged across feature channels used by the bilinear classiﬁer. Finally, we produce a third visualization
by repeatedly remove superpixels from the input image, selecting the one that introduces minimum drop in classiﬁcation score This is similar to . In Figure 8, we show
some randomly selected images from four different classes
in CUB-200-2011 dataset and their corresponding visualizations.
The visualizations all suggest that the model is capable
of ignoring cluttered backgrounds and focuses primarily on
the bird and even on speciﬁc discriminative parts of each
bird. Moreover, the highlighted activation region changes
w.r.t the bird size and context, as shown in the ﬁrst panel
of Figure 8. For the species “010.red winged blackbird”,
“012.yellow headed blackbird” and “013.bobolink”, the
most distinctive parts, intuitively, are the red wings, yellow head and neck, and yellow nape, respectively.
model naturally appears to respond to and localize these
parts. This provides a partial explanation as to why simple global pooling achieves such good results without an
explicit spatial transformer or cross-channel pooling architecture (e.g. )
6. Conclusion
We have presented an approach for training a very compact low-rank classiﬁcation model that is able to leverage
bilinear feature pooling for ﬁne-grained classiﬁcation while
avoiding the explicit computation of high-dimensional bilinear pooled features. Our Frobenius norm based classiﬁer
allows for fast evaluation at test time and makes it easy to
impose hard, low-rank constraints during training, reducing
the degrees of freedom in the parameters to be learned and
yielding an extremely compact feature set. The addition
of a co-decomposition step projects features into a shared
subspace and yields a further reduction in computation and
parameter storage. Our ﬁnal model can be initialized with a
simple PCA step followed by end-to-end ﬁne tuning.
Our ﬁnal classiﬁer model is one to two orders of magnitude smaller than existing approaches and achieves stateof-the-art performance on several public datasets for ﬁnegrained classiﬁcation by using only the category label
(without any keypoint or part annotations).
these results will form a basis for future experiments such
as training on weakly supervised web-scale datasets ,
pooling multiple feature modalities and further compression
of models for use on mobile devices.
Acknowledgement
This project is supported by NSF grants IIS-1618806,
IIS-1253538, DBI-1262547 and a hardware donation from