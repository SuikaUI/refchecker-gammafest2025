Received June 14, 2021, accepted June 18, 2021, date of publication June 24, 2021, date of current version July 1, 2021.
Digital Object Identifier 10.1109/ACCESS.2021.3092054
Developing an Efficient Feature Engineering and
Machine Learning Model for Detecting
IoT-Botnet Cyber Attacks
MRUTYUNJAYA PANDA
1, ABD ALLAH A. MOUSA
2, AND ABOUL ELLA HASSANIEN3,4
1Department of Computer Science and Applications, Utkal University, Odisha 751004, India
2Department of Mathematics and Statistics, College of Science, Taif University, Taif 21944, Saudi Arabia
3Scientiﬁc Research Group in Egypt (SRGE), Giza 12613, Egypt
4Faculty of Computers and Information, Cairo University, Giza 12613, Egypt
Corresponding author: Abd Allah A. Mousa ( )
This work was supported by the Taif University Researchers Supporting Project, Taif University, Taif, under Grant TURSP-2020/48.
ABSTRACT The proliferation of Internet of Things (IoT) systems and smart digital devices, has perceived
them targeted by network attacks. Botnets are vectors buttoned up which the attackers grapple the control of
IoT systems and comportment venomous activities. To confront this challenge, efﬁcient machine learning
and deep learning with suitable feature engineering are suggested to detect and protect the network from such
vulnerabilities in the future. For the efﬁcient detection of cyber attacks, the representative dataset shall be
well-structured for training the model and then validating the proposed system to develop an optimal security
model. In this research, we used the UNSW-NB15, a new IoT-Botnet dataset (a noisy and imbalanced dataset)
to classify cyber-attacks. K-Medoid sampling and scatter search-based feature engineering techniques are
used to obtain a representative dataset with optimal feature subsets. To validate the proposed methodologies,
three most recent machine learning (ML) methods including (i) JChaid∗- a recent upgrade version to
Chi-square automatic interaction detection (CHAID) decision tree-based, (ii) A2DE (a semi-naive Bayesian
averaged two-dependence estimator), & (iii) HGC- a hybrid of Genetic algorithm with K-means clustering
and two deep learning (DL) methods such as (i) Deep Multilayer perceptron (DMLP) & (ii) Convolutional
neural network (CNN) based classiﬁers are employed. From the extensive experimental analysis, it is
pronounced that scatter search-based DMLP classiﬁer outperforms the other competing models in terms
of (i) highest detection rate with100% accuracy, 100% macro-averaged precision, 100% macro-averaged
recall & 100% macro-averaged F1-score and (ii) low computational complexity with the least training time
of 4.7 seconds & testing time of 0.61 seconds.
INDEX TERMS
Network intrusions, IoT, botnet, K-Medoids, scatter search, DMLP, A2DE, JChaid∗,
CNN, K-means clustering, genetic algorithm, macro precision, macro recall, macro F1-score, UNSW-
NB15 dataset.
I. INTRODUCTION
Network attacks or intrusions are collections of events transmitted through network packets that pose a threat to the con-
ﬁdentiality, availability, and integrity of the IoT network, for
the incapability of today’s ﬁrewalls mechanism to detect and
block such a current cybersecurity attack scenario. Further,
with the extensive usage of smart digital devices in an IoT
network environment, secure communications amongst such
interconnected devices are the need of the day as the network
The associate editor coordinating the review of this manuscript and
approving it for publication was Seifedine Kadry
vulnerabilities are complex and very costly to get removed
from such an IoT network system.
Recently it is observed that an efﬁcient network intrusion
detection system could not only be able to detect modern
security attacks, including zero-day attacks, but also could
prevent them from future occurrences. Network intrusion
detection systems (NIDS) can be thought of either as misuse
detection or anomaly detection. While many business organizations prefer to use misuse detection, anomaly detection is
still considered an immature one, attracting many researchers
to research anomaly detection in IoT networks to detect
new or rare attacks. Some of the anomalies in IoT networks
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see 
VOLUME 9, 2021
M. Panda et al.: Developing Efficient Feature Engineering and ML Model for Detecting IoT-Botnet Cyber Attacks
include worms, port, network scan, TCP based and UDP
based events, ﬂash crowd, ﬂow bomb, the small or large size
of the network packets, etc. The recent literatures outline
some security vulnerabilities that are exploited in many IoT
network for not having any generalized security standard to
be maintained by the IoT devices , where it is seen that
a technical glitch in smart lightbulbs design process could
be used for a ‘‘bricking attack’’ that quash the whole city’s
trafﬁc lights. Similarly, the attack against Honeywell was
focused on to go beyond malignant detection to identify new
surge of USB attack types, keeping human substantiation part
of security, or a set of Z-Wave devices. Z-Wave is a dominion
method used to coalesce sensors and actuators over RF and
accomplish smart home and ofﬁce automation services .
A Black Hole attack is administering on a real-world Z-Wave
network to adorn a conventional routing attack that makes
the most of the bare susceptibility and most crucial DDoS
attacks – . Investigators have been manifesting easiness
in hacking motor cars, the ballot box, and power stations.
It is exempliﬁed ransomware attacks against abode humidiﬁers and uncovers fragilities in engraft mechanical-heart.
Recently, several researchers have declared dynamism to create an effective customer brain-computer interface to slash a
person’s brain as a subsequent safe appearance.
A conventional botnet comprises imperiled workstations or
servers, which are termed, Zombies. A group of zombie computers (or bots) is known as botnets. In computing, Zombies
are workstations connected through the internet usually get
infected with malware that allows the hacker or intruder to
take control of them and perform the malicious activities
without the knowledge of the owner. In this case, both owner
and the intruder control these infected workstations in the botnet and try to transfer information through any non-standard
communication channels (or covert channel) which are not
allowed by computer security policies.
From IoT perspectives, the botnet is an imperiled IoT
device network, including a camcorder, routers, smart sensors, or any smart devices with embedded system design, etc.
that are too infected with malware. Using IP protocol for data
transmission over the internet poisoned with malicious software authorizes cyber-hackers to control the devices through
a Trojan horse virus payload. These bot herders having ill
intentions can perform automated and simultaneous malicious activities over interconnected IoT devices resulting in
several of the following botnet attacks as DDoS (Distributed
Denial of service), credential-stufﬁng attacks, network application attacks to purloin data, keystroke logging, identity
theft, unsolicited mail (spamming) and unlawful ingression
to all IoT network-connected device .
While comparing IoT botnet with the conventional ones,
it is observed that both have the centralized and Peer to
Peer (P2P) decentralized architecture but have differences
in devices being used. Further, IoT botnet has a difﬁcult
detection mechanism and can have a huge impact on the
IoT system for its quick and wide spread contrary to its
counterpart .
IoT botnet malicious software uses either active or passive type’s proliferation mechanism. In inactive cases, these
botnet malware applications spread by creating their clones.
When any smart IoT device gets connected with such an
infected device either through Wi-Fi, USB drive, or FTP
servers, they become infectious. It makes them part of the
botnet, and the propagation of the virus continues spreading
further , .
From several IoT applications including Smart healthcare,
smart agriculture, smart homes, smart grid, and intelligent
transportation to name a few, it is observed that IoT devices
are prone to threats due to their following inherent characteristics and hence, a proper mechanism is sought for their
secured implementation.
• IoT devices use lossy wireless links which makes them
vulnerable to cyber-attack. Further, the resource limitations and lack of standardization in IoT systems make
it quite difﬁcult to develop an effective security mechanism to deal with such threats.
• Due to heterogeneous architecture and diverse applications of IoT systems, it poses real challenges to develop
a security scheme that may be applicable for all with
different application objectives.
• IoT systems are exposed to new vulnerabilities as old
devices are being replaced with new devices due to
changes in user’s preferences.
• Because of the ubiquitous nature of the IoT devices,
physical threats such as man-made or nature-made are
inevitable for many applications, a context-aware security scheme shall be adapted to effectively deal with such
a situation.
• Further, due to the huge deployment of tiny IoT devices
in IoT environments, huge data are been collected and
analyzed for effective decision-making to safeguard the
interest of the customer and organization at large. This
needs a proper security mechanism so that eavesdropping of the devices can be avoided.
• Traditional security mechanisms including data encryption, ﬁrewalls, access control with network and application layer security, etc., are found to be ineffective
to address the inherent IoT system limitations. Hence,
machine learning and deep learning approaches may be
a solution to address these issues .
In addition, due to the large-scale deployment of smart IoT
devices in an IoT system, manufacturers ﬁnd difﬁculty in
providing a separate key for each device, rather uses the same
user name and password for all the same kind of devices. So,
as because IoT devices are not being developed with an intrinsic security mechanism, which makes them more endangered
while connected to a network. It is also observed that due to
the ever-developing nature of the IoT botnets, signature-based
network anomaly detection (or the traditional ones) methods
failed to detect them efﬁciently, which demands a separate
IoT botnet detection mechanism in contrast to traditional
botnet approaches .
VOLUME 9, 2021
M. Panda et al.: Developing Efficient Feature Engineering and ML Model for Detecting IoT-Botnet Cyber Attacks
It is customary to note here that due to IoT cyber-attacks’
diverse nature, mere patching of IoT devices against the most
common known attacks is not a viable solution. Simultaneously, without going into more details into the IoT security
infrastructure and smart device features, it is suggested to
perform some counter security vulnerabilities pertinent to the
attacks compromised IoT devices against the unseen attacks
or intrusions.
To address these emerging security and privacy issues
that arise out of the IoT environment and to understand the
intelligence embedded thereto, suitable feature engineering
techniques and efﬁcient machine learning approaches are
recommended to detect network intrusions and IoT botnet
attacks .
It is also opined that to deal with such IoT security issues,
machine learning, and deep learning model is quite efﬁcient,
which uses the past behavior of the system to develop a
secured intelligent model , .
From all the above discussions, we are motivated to
explore further research in using an efﬁcient feature selection
approach with suitable machine learning and deep learning
methodologies to build a security-based intelligent model to
detect IoT botnet efﬁciently.
To explore further in this emerging area of research, this
paper aims to apply feature engineering and machine learning
techniques for the efﬁcient detection of IoT botnet attacks in
an IoT anomaly detection system.
Our main contributions are as follows:
– We present an efﬁcient feature subset selection approach
using promising metaheuristic methods such as: Scatter
search and K-Medoid sampling to create the best features.
– We propose several classiﬁcation methods such as
A2DE Bayesian classiﬁer, JChaid∗decision tree, deep
learning-based convolutional neural network (CNN) classiﬁer, Deep MLP Classiﬁer, and unsupervised classi-
ﬁcation using hybrid K-means clustering and genetic
algorithm (HGC) on the reduced features for optimal
detection of IoT botnet attacks.
– A new IoT botnet attack dataset, UNSW-NB15, is used
for extensive simulation to reveal the effectiveness of the
proposed IoT based network intrusion detection system
– Several performance evaluation metrics such as micro
and macro averages to precision, recall, and F1-score
apart from prediction accuracy, training, and testing time,
are considered to understand the efﬁcacy of the proposed
IoT botnet detection model.
The rest of this paper is assembled as follows. While Section 2
dispenses the related works, Section 3 confers a short chronicle of the dataset. Next, Section 4 talks through the feature selection process; machine learning and deep learning
classiﬁcation methods employed are presented in Section 5.
Section 6 accorded to the experimental setup and discussed
the results obtained compared to others’ similar work available in the literature. Finally, Section 7 brings out the
conclusions.
II. RELATED RESEARCH
The real challenge in today’s network intrusion detection system (NIDS) lies in spotting the unseen and obscure malicious
software as the malware programmer writes different bypassing techniques for information tuck away to avert detection by
IDS. With the tremendous growth in internet users, cybersecurity warning such as zero-day attacks has impacted the life
miserably in terms of magnitude and potency. Even though
machine learning is explored for its effectiveness in detecting
network intrusions in DARPA KDDCup 1999 and NSL-KDD
datasets in the past several years, it is still fuzzy in deciding
the best classiﬁer in detecting network intrusions.
In recent times, with the development of IoT technology,
tremendous real-life smart applications have gained attention
from several manufacturers to improve human life quality.
IoT devices being resource constraints and no speciﬁc built-in
security mechanism, it becomes easy for the attackers to
attack the device and the IoT system as a whole. Hence,
the attackers shifted their ambitions to attack the organization
rather than individuals. It has led to the urgent develop an
efﬁcient IDS to detect new unseen and trailblazing malware,
in an IoT system.
Mahmud Hossain et al. discussed the undetermined
problems in IoT, considering IoT devices are insecure and
opined for priority in IoT security research as such devices
are easily compromised by the investigators for their various
limitations in terms of mobility, workability, and battery life
along with their diverse applications ranging from sensors to
IBM PC clones.
Pahl et al. describe in their paper that even though
there is some related work found in IoT, still it is attracting the
attention of researchers for its popularity in today’s livelihood
and designed an anomaly-based detector and ﬁrewall for
IoT system using K-Means and BIRCH clustering with a
predictive accuracy of 96.3%. Brun et al. used an expert
system based on Dense Random Neural Network to detect
IoT security threats that were arisen out of DoS attacks and
Denial of sleep attacks.
Diro et al. use deep neural networks and their variant
for network attack detection using cloud-to-things architecture in an open-source dataset with 98.27% and 96.75%
accuracy, respectively. Usmonov et al. delineate the challenges faced in protecting data privacy during data transfer by
IoT components in an embedded environment and proposed
using digital watermarking techniques to counter these.
Anthi et al. constituted an IoT-based intrusion
detection system in a dataset collected for several days
using the Wireshark packet sniffer tool and applied several data mining methods for efﬁcient detection of probing DoS attacks. Ukil et al. use smartphone-based
anomaly-based attack detection in IoT healthcare using big
data analytics and biomedical signal and image processing.
Kozik et al. engrossed cloud architecture-based extreme
learning machines in Netﬂow arrangement data for IoT network attack classiﬁcation with an accuracy of 99% for scanning attacks and 95% for infected host attacks.
VOLUME 9, 2021
M. Panda et al.: Developing Efficient Feature Engineering and ML Model for Detecting IoT-Botnet Cyber Attacks
Hasan et al. compared several well-known machine
learning algorithms in detecting cyber-attacks in IoT network
sites in an open-source synthetic dataset collected from Kaggle and found that Random forest, decision tree, and Artiﬁcial
neural network all present the same classiﬁcation accuracy
of 99.4%. Koroniotis et al. proposed four data mining techniques: C 4.5 Decision tree, Naïve Bayes, Artiﬁcial
neural network, and Association rule mining to detect IoT
botnets in USNW-NB15 dataset. They conclude that C4.5 is
the best classiﬁer with 93.23% accuracy and the lowest
false positive rate at 6.77% compared to Naïve Bayes with
72.73%, 27.27%; Association rule mining 86.45%, 13.55%
and 63.97%, 36.03% respectively for ANN.
Kumar et al. proposed a robust, lightweight solution
by applying a random forest algorithm to detect IoT botnets
at an early stage most accurately (almost 100 %) in less time
with very minimal memory consumption. Derhab et al. 
advocated for the use of feature engineering to IoT botnet
unbalanced dataset with Synthetic minority over-sampling –
Nominal Continuous (SMOTE-NC) and classify the botnet
attacks by using temporal deep learning methods and compared the suitability of their proposed approach with two
conventional ML methods such as Random forest and logistic
regression; and two popular deep learning as Long short-term
memory (LSTM) and Convolutional neural network (CNN).
They implemented all on IoT dataset from the University of
New South Wales and concluded that TCNN predicts the
attacks with 99.998% accuracy and takes 424 seconds to
train the model, in comparison to 99.9654%, 762 seconds
for LSTM; 99.9973%, 419 seconds for CNN; 99.2858%, 709
seconds in Logistic regression and 97.4586%, 191 seconds
for Random Forest respectively.
Bagui and Li presented the usefulness of random
over-sampling and random under-sampling with SMOTE
to deal with highly imbalanced and less imbalanced intrusion detection datasets to improve the classiﬁcation accuracy of the classiﬁer. They evaluated the model by using
artiﬁcial neural network (ANN) for attack detection using
macro precision; macro recall, and macro F1 score for several sampling techniques and found that SMOTE-Random
under-sampling
outperforms
precision;
recall; 82.75% macro F1-score with 342 seconds training
Nguyen et al. introduced a federated learning-based
machine learning approach for detecting backdoor attacks
in IoT intrusion detection systems on several datasets. They
chose Federated learning with the K-means clustering technique as a defensive approach to classify the model either
as an intrusion or Benign. Lin et al. , on the other
hand, proposed to use biologically inspired algorithms (Arti-
ﬁcial Fish Swarm optimization-AFSO) for feature selection along with a Support Vector Machine (SVM) classiﬁer
for efﬁcient detection of botnet attacks. They reported that
AFSO+SVM presents a slight increase in detection accuracy
(97.76%, 97.30%) but faster training time (19843 seconds,
22831 seconds) when used with genetic algorithms for botnet
detection process.
From the extensive literature survey, it is conspicuous that
IoT security is still not matured. Various states of an affair
with this new and ﬂourishing ﬁeld are being come across
III. DATASET USED
The most commonly used dataset in the research study of
network induction detection systems are brieﬂy discussed
The DARPA 98 (Defense Advanced Research Projects
Agency) dataset was initially brought on by Lincoln
Laboratory, USA, to judge the intrusion detection system.
This data was comprised of 4GB binary data, collected over
seven weeks.
The KDDCup 1999 dataset , an enhanced version of
the earlier DARPA 1998 dataset, is severely imbalanced in
intrusive and normal classes but still useful in distinguishing
inpouring attacks and normal kinship even today.
NSL-KDD is not a new of its kind, as this is the reﬁned version of the KDD 1998 dataset developed by Ali A. Ghorbani
in Network Security Laboratory (NSL) and seems to have
addressed some of the ingrained complications, but still have
problems as talk about by McHugh .
University of Brescia, Italy, explain an IDS dataset, namely
UNIBS in which tcpdump was used to collect the
network trafﬁc in a cluster of 20 workstations ﬂowing the
observed devil to detect the DoS attacks only. The problem
lies in the dataset is not having any other features except
network packets with no labels to get more information.
The IDS dataset developed by Tezpur University (TUIDS),
India , created features caused during the ﬂow level
process in a physical testbed without including the features
that might have occurred during the ﬂow capturing process.
The dataset features include attacks concerning DoS, DDoS,
and probing or scanning attacks only.
The USW-NB15 is the 100GB synthetic dataset used in this
research, coined by Mustafa et al. at the University of
New South Wales, Canberra, Australia IXIA Perfect Storm
tool in the Cyber Range Laboratory. This dataset is generated
in PCAP ﬁles with a mixture of normal and intrusive trafﬁc
and other notable features.
From all these discussions, it is apparent that research
concerning generating realistic IoT botnet attack scenarios
is not yet explored fully as many available datasets do not
come up with new features while other datasets do not include
IoT-generated network trafﬁc. Hence, researchers need to
explore further to evaluate efﬁcient machine learning mechanisms to address the lacunas found thereto.
Elucidation of the UNSW-NB15 Dataset:
UNSW-NB15 IoT botnet dataset has the following attack
categories:
(i) Fuzzers: these attacks include protocol fuzzing that
may act as proxy altering packets on the ﬂy and echo
them. Normally, fuzzers embroil crashes allow the attacker to clinch over the IoT
devices, notwithstanding the encryption control procedures.
Some examples of fuzzers attacks include skewed packets,
stack overﬂows, etc.
(ii) Backdoors: A backdoor attack is a malicious software through which hackers can get prohibited access to the
website. The malware is installed via weak network entry
points such as old plug-ins and input ﬁelds. These attacks
are stealthy, and hence most of the time, the hackers get
undetected.
(iii) Analysis: These attacks are also referred to as active
surveillance. It is a type of computer attack in which the
cybercriminals engross the target to obtain details about the
intrusions. In military applications, reconnaissance is used
to gather information by entering into the enemy territory.
In computer security, reconnaissance is the ﬁrst step before
taking the next step to exploit the target computer system.
The examples include port scanning to locate some open and
vulnerable ports.
(iv) Denial of Service (DoS): This type of attack compromises the available network resources from intended
users and makes the IoT network devices perform isolated.
DDoS (Distributed DoS) attacks in IoT botnet scenarios are
cases where many intruders try to intrude into the extensive
network to attack a single IoT device.
(v) Shellcode: Shellcode seldom has to do with shell
scripting, but it is considered as an IoT botnet attack scenario through which the cybercriminals tries to chronicle the code executed by a compiled program owing to
susceptible use and open a remote cell (for example- an
exemplar of command-line interpreter). Then, the remote
shell is used further to infect the victim’s system. With
proper inputting to a targeted program, the shellcode attack
considers being a very transparent and efﬁcient means of
(vi) Reconnaissance attacks: These attacks thought of as
a general knowledge congregation attack, can transpire both
logically and physically. Trafﬁc analysis, packet sniffers, etc.,
fall under this category.
(vii) Worms: A computer worm is a malicious software
that spreads from one IoT device to another and can clone
itself without human interplay. More importantly, it does not
need to afﬁx itself in the target program for causing harm
to the devices. An example of such a worm is a person who
always tries to recline and escape.
(viii) Generic: The generic attacks are based on ciphers,
which is virtually a prang attack on ciphers’ secret keys.
These attacks usually can be run solo without having
implementation details crypto-graphical primordial. Linear and differential cryptanalysis, correlation attacks, algebraic attacks are examples that fall in the generic attack
(ix) Exploits: These attacks are malware that takes precedence over application software or operating system threats.
Such an attack example is known threats in Microsoft
IV. FEATURE ENGINEERING AND CLASSIFICATION
The feature engineering approach to select the best feature
subset from the original set is NP-hard and sometimes computationally complex . However, with proper designing of
the feature subset selection method combined with classiﬁcation algorithms, it will enhance the classiﬁer’s performance.
This section aims to describe the characteristics of the
proposed Scatter Search and K-Medoid sampling evolutionary metaheuristics for resolving feature engineering problems in classiﬁcation. The application of scatter search and
K-Medoid sampling in feature engineering and that in the
Intrusion detection system are not explored by researchers,
as we could not ﬁnd any such from the internet.
Both Scatter search (ScS) established by Glover 
and Genetic algorithm (GA) launched by Holland are
nature-inspired algorithms, population-based, evolutionary
optimization methods with some fundamental differences
between them. While GA is coined initially to carry out
hyperplane sampling, it is also being used for the optimization
process. Scatter search is designed to execute as a surrogate
constraint relaxation heuristic to ﬁnd solutions to integer
programming problems instead.
Following are some of the steps carried out by natureinspired evolutionary algorithms to ﬁnd an optimal solution
to hand problems, irrespective of their working procedures.
(i) Establishing, nurturing, and functioning the search techniques with a set of population elements or vectors.
(ii) Generate new vectors by fusing the previously available
(iii) Discovering and holding on to the best vectors out of
many in step-(i) based on performance quality measures.
A. SCATTER SEARCH
Scatter search is an evolutionary search strategy that
tried to solve NP-hard problems successfully. The diverseness and ampliﬁcation strategies of scatter search make it
a promising feature engineering method for optimal feature
subset selection . A systematic un-segregated approach is
used in Scatter search rather than the volatile process for optimization . The search is complete when the termination
condition is above a speciﬁed threshold value, or no further
performance enhancement is found.
The prime objective of using scatter search is to show
the computational experiments result with the best accuracy
than others with reduced attributes. While comparing with
the Genetic algorithm (GA), it is observed that even though
both are evolutionary optimization families, scatter search
provides solutions of higher average quality than its GA
variants .
The pseudo-code for the scatter search optimization is outlined in Figure 1. In Figure 1, the initial population (Pop) shall
be a large set incorporating good solutions. The Population
size (Pop_size) is calculated as the square of the number of
attributes present.
VOLUME 9, 2021
M. Panda et al.: Developing Efficient Feature Engineering and ML Model for Detecting IoT-Botnet Cyber Attacks
FIGURE 1. Pseudo-code of evolutionary scatter search.
B. K-MEDOID SEARCH
K-Medoids sampling is a supervised similarity-based clustering method used for feature subset selection in the classiﬁcation process. To eliminate inessential features while
comparing with other features, feature engineering via clustering similarity may be embraced for implementation. The
most popular prototype-based K-means clustering, which
provides globular-shaped clusters, is a choice to achieve this.
Still, the strain in deﬁning the initial centroid and number of
clusters (K) in K-means clustering goes against it.
information
K-Medoids sampling rather than centroid, as was K-Means
clustering. Hence, we propose K-Medoids as a surrogate to
K-Means clustering for unsupervised feature subset selection.
The validation of feature subset selection is done by
using simpliﬁed silhouette (SS) criteria for its computational
advantages over original silhouette (Sil), even though both
have the value ranging from -1 for wrong clustering to +1 for
very well clustering and if around zero, then the observation
may fall in either of the two clusters. The original silhouette describes the average distance of a data point to
others in the same cluster and all other data points in the
neighboring cluster with a computational cost of O(mn2).
Simpliﬁed silhouette(SS), on the other hand, measures the
distance of a data point to a cluster as the distances between
data points and cluster Medoids (or centroids in case of
K-means clustering) with a complexity of O(mkn). Here, n
represents the number of data points, m is the number of
features in each data point, and k is several clusters. For
several clusters that are less than the number of data points,
a simpliﬁed silhouette is used for its computational simplicity
compared to the original silhouette, which is the basis for
choosing SS in K-Medoid sampling for the feature engineering process. Further, supervised SS ﬁlter (S3F) , a variation of SS is available, which measures cross-correlations
between the independent variables, but it is reported that no
substantial improvement in supervised computational complexity is achieved with S3F .
The silhouette value enables us to understand the clustering’s quality by estimating the average distance between
clusters under observations. The silhouette chart exhibit how
near data points in one cluster to the points in adjoining clusters. The sampling strategy for K-Medoids clustering works
iteratively with k-initial clusters, and then cluster updates
VOLUME 9, 2021
M. Panda et al.: Developing Efficient Feature Engineering and ML Model for Detecting IoT-Botnet Cyber Attacks
FIGURE 2. Procedure for K-Medoids sampling search technique.
are made based on some randomized inter-changes .
In this paper, the Symmetrical uncertainty distance function
is used to obtain the closest representatives for each data
point and then improve during the interchange process. This
process continues till termination criteria are satisﬁed. The
pseudo-code for K-Medoids is presented in Figure 2.
Here, the limitation of pre-assigning K-value in K-Medoids
is nulliﬁed using SSF, where the number of clusters can
automatically be assigned . Earlier, researchers use the
hit and trial method for various K and then select the best
K for maximum silhouette value. Still, there was always a
chance of being trapped in sub-optimal solutions. In SSF, K’s
minimum and the maximum value are chosen, and the ﬁnal
value of K is selected within that range to obtain the best
silhouette value.
C. CLASSIFICATION METHODS
This unit discusses the potential applications in detecting
IoT Botnet attacks. The popular machine learning and deep
learning algorithms were considered for envisaging their
suitability in IoT network intrusion detection systems such
as A2DE (Averaged two-dependence Estimator), JChaid∗
(Decision tree), MLP classiﬁer (Multilayer Perceptron), and
Convolutional neural network (CNN).
D. AnDE (AVERAGED N-DEPENDENCE ESTIMATOR)
AnDE (Averaged n-dependence Estimator) with n
being the level of independence is a semi-naïve Bayesian
method that softens the conditional probability independence
assumption most of its theoretical and computational requirements. AnDE hypothesizes naïve Bayes (NB) as A0DE and
A0DE equivalent to A1DE to the next level of dependence.
The proposed A2DE, for AnDE, n = 2; while keeping the
naïve Bayes method’s properties, it is different from other
naïve Bayes variants with minimum computational overhead
and classiﬁcation considering the conditional dependence of
the contained attribute on any two features excluding class
attribute. Further, it is observed that A2DE improves the
classiﬁcation accuracy in a large dataset . The working
principle of A2DE is as follows: Initially, during the training
phase, A2DE initializes the number of records, constants,
probability estimation values, and the joint frequency values.
Then, in the prediction phase, the A2DE classiﬁer predicts the
class by even outsourcing one dependence estimator to the
number of records available in the dataset. Further, the low
bias characteristics and single-pass learning through training
data of A2DE make it popular for big data analytics with high
accuracy , linear time complexity but with high variance,
and increased space complexity .
E. JChaid∗DECISION TREE ALGORITHM
JChaid∗ exploratory model is a recent upgrade to
Chi-square automatic interaction detection (CHAID) decision tree induction algorithm where the co-relationships
among the predictor class variable and dependent variables
are measured in an automated process to build the classiﬁcation tree. The original CHAID algorithm created by
Gordon V. Kass in 1980 is a non-parametric method (also
known as XAID) that works on adjusted signiﬁcance testing (Bonferroni testing) for classiﬁcation through interaction
among the variables in the dataset . For ordinal dependent
variables, the JChaid∗algorithm uses the F-statistical test to
VOLUME 9, 2021
M. Panda et al.: Developing Efficient Feature Engineering and ML Model for Detecting IoT-Botnet Cyber Attacks
check any other signiﬁcant class predictor variable’s presence
to a dependent variable. In contrast, the chi-square test is used
if the dependent variable is categorical.
While comparing with other popular decision tree methods
such as Classiﬁcation and Regression Trees (CART), JChaid∗
decision tree suits our application for the following reasons:
• JChaid∗is capable to deal with a dataset having a
dependent variable either categorical type with two or
more categories and/ or continuous ones and categorical
independent variable type, whereas CART can only deal
with a dataset those have binary or continuous dependant
and independent variable types.
• Next, while CART performs binary split only by
default, JChaid∗does multiway split. The depth of the
tree is obtained accordingly, to get the knowledge of
the bias (underﬁtting) and variance (overﬁtting) while
selecting the decision tree model.
• Further, JChaid∗presents a better feature selection process by preventing the overﬁtting problem. This is
because JChaid∗splits a node if a signiﬁcance criterion
is satisﬁed. In comparison, for the IoT botnet dataset,
which is imbalanced and noisy, CART may cause over-
ﬁtting by creating the unstable tree structure for a small
change in the dataset and may also create underﬁtting
for the class imbalance.
F. MULTILAYER PERCEPTRON (MLP) NEURAL NETWORK
CLASSIFIER
The multilayer perceptron is a ﬁnite directed acyclic graph
that is the most common neural network architecture to produce a good generalization. The trained model can provide
unerring output for the new, unseen inputs . To enhance
the performance of the MLP neural network model’s generalization ability by avoiding overtraining, early stopping criteria using a separate validation set and regularization method
are proposed by many researchers. Early stopping criteria in
MLP classiﬁer give an idea about the number of iterations
that can be run before the model begins to overﬁt. In this
evaluation, Levenberg–Marquart (LM) algorithm based on
the gradient descent method can provide faster convergence
to the model built.
Further, the MLP classiﬁer uses the L2 regularization
method to avoid overﬁtting during training by penalizing weights in sizeable magnitude. When MLP classiﬁer
is used for classiﬁcation, predicting accuracy with low
cross-entropy loss function can be achieved by doing feature scaling or standardizing the feature with zero mean and
variance = 1. Further, the time complexity of the MLP classiﬁer is O(m.n.z.i.kp), where m training samples, n features,
p hidden layers, each containing k neurons, z output neurons,
and the number of iterations (i).
G. DEEP LEARNING VIA MULTILAYER PERCEPTRON
DMLP, also known as a deep feedforward neural network, is considered an archetypal deep learning model
where backpropagation trains the model. For optimization of the process, MLP uses logistic loss function and
quadratic penalty with Broyden-Fletcher-Goldfarb-Shanno
(BFGS) algorithm as limited- main memory (LM) based
family of the Quasi-Newton method (QNM), as optimization
routine. To train a deep MLP (DMLP) classiﬁer, the following parameters are to be set properly: number of hidden layers (should be chosen intelligently, as the higher
value can increase the training time), batch size, activation functions in the hidden and output layers, seed size,
loss function, convergence tolerance value (a very small
value of tolerance can lead to high predictive accuracy but
with more number of iterations), a quadratic penalty by
ridge factor and pool size to obtain a parallel calculation
of loss function and gradient when multiple CPU cores are
present. The performance of the DMLP largely depends upon
the optimal hyperparameter setting and large training data
H. CNN (CONVOLUTIONAL NEURAL NETWORKS)
Convolutional Neural Networks (CNN) is a well-liked deep
learning model, has pulled off many state-of-the-art executions on classiﬁcation tasks . Initially, CNN found
applications in image processing where it differs in having
a convolutional ﬁlter (or kernels) compared to the fully
connected neural network. There are three components in
a CNN: convolutional layer, pooling layer, and output or
classiﬁcation layer. While the convolutional layer and pooling layer are used for feature extraction, the output classiﬁcation layer connected at the end of the network performs
intrusion detection. Both forward and backward propagation is used during the training of CNN. During forward
propagation, the input sample is fed into the CNN, and the
output is observed and compared with the actual. Then the
error (if any) is minimized by backpropagation by updating
network parameters , . Whether simple, deep, and
intricate, the architecture of CNN largely depends on the
problem at hand regarding design and number of convolutional layers. In CNN, we use a 2 × 2 max pooling layer
to downscale the input samples by 2, picking the maximum
value in the pooling layer sub-region, resulting from which
convolutional layer is subtle to features by a factor of 2 in
comparison to the previous layer. The ﬁxed ﬁlter size is
used; however, a suitable ﬁlter size may be chosen for the
best result . During training, hyper-parameter settings,
including weights, bias, learning rate, mini-batch size, weight
penalty, number of iterations, input, and hidden layer dropout
rate, etc., are to be tuned optimally, so that the performance
of CNN is maximized. To minimize the error, multithreaded
mini-batch
stochastic-gradient
training CNN.
The motivation behind proposing a deep learning classiﬁer in comparison to traditional machine learning ones
is its superior performance in dealing with large datasets
and its capability to automatic data extraction from complex
depictions.
VOLUME 9, 2021
M. Panda et al.: Developing Efficient Feature Engineering and ML Model for Detecting IoT-Botnet Cyber Attacks
FIGURE 3. Proposed experimental setup.
I. HYBRID GENETIC ALGORITHM WITH K-MEANS
CLUSTERING (HGC)
Unsupervised learning or clustering is used for classiﬁcation
using similarity checks among data records. For simplicity
in operation, K-means and Fuzzy C-means (FCM) clustering algorithms were found immense popularity in machine
learning applications. Still, the clustering quality effectively
depends on the centroid’s tenacity and an optimal number
of clusters. To alleviate such problems, a hybrid GenClust
method that combines K-means clustering with a genetic
algorithm to enhance the performance of K-means in the
classiﬁcation process via clustering is proposed . In this,
clustering quality is improved in each generation so that
K-means can attain the optimal global solution, rather than
trapped in local optima, as was in the earlier case. Further,
the cluster number is determined randomly between 2 to a
maximum of the sum of objects in the training dataset.
Similarly, the centroid placement is obtained in chromosome form, where half of the chromosomes are used in
the random form and the other in the deterministic form.
Rahman et al. enhanced the previously developed Gen-
Clust and renamed it GenClust++ (HGC) with the mediation
of fast hill-climbing cycles of K-MEANS. The genetic operator is not only faster than its predecessor but also generates
better clustering quality. Further, the computational complexity of HGC is O(n) in selecting the initial population.
The main motivation behind using HGC is to explore the
effectiveness of a clustering algorithm in the detection of
attacks in a large IoT botnet dataset.
V. PROPOSED APPROACH
The proposed experimental framework is shown in Figure 3.
All the experiments are carried out in an HP pavilion
laptop with 1TB HDD, 8GB RAM, 2.67GHz CPU in
windows 10 environment using Python and Java.
Initially, the UNSW-NB15 dataset is collected from the
UCI machine repository that contains 1, 75, 413 training
samples with 45 features . The dataset is extremely noisy,
and hence proper care is taken to replace the missing values with the global most common attribute value from each
column to ﬁll the NaN (Not a Number) values as a data
pre-processing task to make it ready for applications. Further,
the feature engineering approach with K-Medoid sampling
and meta-heuristic scatter search are used to ﬁnd the best
features. It is needless to state that the best features with
appropriate machine learning will enable the administrator to
make efﬁcient and effective decision-making.
Here, ﬁve different classiﬁers: JChaid∗decision tree,
A2DE Bayesian, Deep MLP, CNN, and unsupervised HGC
are used to understand the efﬁcacy of the proposed approach.
The parameter settings of all the classiﬁers are provided
below for a better understanding of the proposed approach.
A. PARAMETER SETTINGS OF JChaid∗AND A2DE
CLASSIFIER
For JChaid∗classiﬁer; batch size = 100, a minimum number
of instances per leaf = 02, minimum description length correction is used with conﬁdence factor = 0.25 and signiﬁcance level = 0.05 are used.
For A2DE also, a bath size of 100 is used for experiments.
B. PARAMETER SETTINGS OF DMLP CLASSIFIER
The sigmoid activation function is used in the output layer,
whereas soft plus or logistic activation function in the hidden
layer as f(x) = ln(1 + ex), loss function as Squared error for
MLP Classiﬁer: loss(a, b) = (a-b)2, pool size, as number of
cores in the CPU = 1, ridge penalty factor for the quadratic
penalty on the weights = 0.01, tolerance parameter for the
delta values = 1E-06, number of hidden units = 2 (more
hidden units does not guarantee the best accuracy), batch
size = 100 as number of instances to be processed for the
batch prediction that is preferred only to improve the runtime
for larger datasets. Feature engineering is used for converting
Nominal attributes to binary using an unsupervised ﬁlter, and
missing values are replaced globally.
C. PARAMETER SETTINGS OF CNN
Deep neural network (Convolutional Neural Network) is
implemented in this research with dropout regularization and
Rectiﬁed Linear Units, where training is done with multithreaded mini-batch gradient descent. Here, we use two
convolutional layers with patch size 5 × 5 and pool size
2 × 2, each with 20 and 100 feature maps. Mini batch size
of 100 is chosen, along with hidden layer dropout rate = 0.5,
input layer dropout rate = 0.2, max iterations = 1000 and
weight penalty = 1E-08.
D. PARAMETER SETTINGS OF HGC
Initial population size for genetic algorithm = 30, the max
iterations for ﬁnal k-means = 50, max iterations for
generations = 60, random number seed used = 10,
start-chromosome-selection-generation (the generation after
chromosome
operation) = 50.
E. PERFORMANCE METRICS
In this research, we propose to use micro and macro averages
to Precision, Recall and F-score apart from classiﬁcation
accuracy to conclude with an intelligent decision making for
selecting the best ML/DL model to efﬁciently detect IoT
botnet attacks. From the literature survey, it is observed that
to date, no researchers in IoT botnet detection used these
metrics to evaluate the classiﬁer performances, which motivated us to consider those measures to evaluate our proposed
approach. Further, computational complexity in terms of
training and testing times is considered for the effectiveness
of the model.
To evaluate the model, a confusion matrix is used to
juxtapose the actual target value (intrusion or normal) with
ML/DL model predicted values. This way, the confusion
matrix presents a comprehensive view of model performance
in terms of decision-making with kinds of errors incurred.
From the obtained confusion matrix, the following crucial
terms are calculated: true positives (TP)-model predicted
value matches with the actual ones; false positives (FP or
type-1 error)-actual value was negative but model predicted
as positive; True negative (TN)-actual negative value matches
with the model predicted negative value and False negative
(FN or Type-2 error) - The model predicted a negative value
while actually, it is positive.
Now, the several commonly used performance metrics are
deﬁned in equation (1) to equation (4).
Accuracy = (TP + TN)
(TP + FP + TN + FN)
Precision = TP
Recall = TP
F-1score = 2 ∗
(Pr ecision × Re call)
(Pr ecision + Re call)
Now, it is opined by many researchers that accuracy is not
the best metric all the time, as it gives an indication about the
True Positives and True negatives( with high misclassiﬁcation cost) but, in intrusion detection model, FN and FP poses
the serious consequences. Hence, F1-score is proposed which
takes FN and FP into account for efﬁcient classiﬁcation of
intrusions for an imbalanced and noisy dataset. Similarly,
precision is preferable when FP is more important than FN,
and recall is used in cases where FN outshines FP. 100%
precision indicates that 100% of the predicted positive cases
are turned out to be actual positive cases. Similarly, 100%
recall means 100% of the actual positive cases are correctly
predicted by the model. In real-life applications, it is seen
that when precision goes up, recall decreases and vice-versa,
hence F1-score is coined which amalgamate both and can be
calculated as the harmonic mean of the precision and recall.
F1-score is also fuzzy in the sense that one is in dilemma
to understand whether Recall or precision is maximized by
the classiﬁcation model, but a good F1-score ( perfect when
F1-score = 1; failure, when F1-score = 0) indicates that the
model is correctly identifying intrusions with low false alarms
(low FP and low FN).
Since 100% accuracy alone is not a good idea to judge
the efﬁciency of the classiﬁer model for possible chances of
overﬁtting, several other metrics such as precision, recall, and
F1-score need to be investigated. So, we propose to evaluate
the classiﬁer with other metrics such as micro and macro
averaged precision, recall, and F1-score to get more
A macro-average will enumerate the metric independently
for each class and then take the mean by treating all
classes equally including minority or rare classes, whereas
a micro-average metric will bundle the contributions of all
classes to compute the average metric by favoring the majority class. These matrices are deﬁned in equation (5) to equation (10).
Micro averaged precision
= (TP1 + TP2)
(TP1 + TP2 + FP1 + FP2)
VOLUME 9, 2021
M. Panda et al.: Developing Efficient Feature Engineering and ML Model for Detecting IoT-Botnet Cyber Attacks
TABLE 1. Experimental results in IoT Botnet UNSW-NB15 dataset with 1, 75,341 samples & 45 attributes.
Macro averaged precision
= (P1 + P2)
Micro averaged Recall
= (TP1 + TP2)
(TP1 + TP2 + FN1 + FN2)
Macro averaged Recall
= (R1 + R2)
Micro averaged F-1 score
(Micro_avg_ Pr ecision × Micro_avg_Re call)
(Micro_avg_ Pr ecision + Micro_avg_Re call)
Macro averaged F1-score
= (F1 + F2)
Here, it is to be noted that the micro average is calculated
for all the classes (class 1 = normal, class 2 = Attack) and the
macro average is the average of precision, recall and F-score
for each class.
Further, as described in , macro-average to precision,
recall, and F1-score are more sensitive to the class-imbalance
problem in comparison to their respective micro-averages,
they produce less score. It is also suggested that macro averaged metric to be chosen for the sense of the effectiveness
of the model while micro average metric shall be used when
large classes are available in a test data collection , but at
the end, the real decision lies in relative less misclassiﬁcation
cost for the classes.
VI. RESULTS AND DISCUSSION
The experimental results obtained are provided in Table 1.
The original 45 attributes, including class labels, are reduced
to 5 and 3 numbers after applying a feature engineering
approach with K-Medoid sampling and scatter search
respectively.
It can be observed from Table 1 that micro averaged precision, micro averaged recall, and micro averaged F1-score
were the same but at the same time feigned high, ranging
from (69.71%, 100%) for CNN; (72.61%, 100%) for DMLP,
(92.465%, 99.62%) for JChaid∗and (92.46%, 99.61%) for
A2DE using K-Medoid and Scatter search feature engineering respectively. The results obtained with the use of scatter
search and deep learning methods (DMLP and CNN) are
encouraging with 100% micro-averages to precision, recall,
and F1-score which makes them promising for most correctly
identifying the threats in IoT botnet scenario. Next, as suggested in , to understand the effectiveness of the ML/DL
model, macro metrics-based evaluations were proposed in
this research to gain further insights. Since deep learningbased classiﬁers (DMLP and CNN) and HGC presents 100%
accuracy, they were considered for comparisons with other
existing works available to date.
Macro averaged precision increasing from micro averaged
precision insinuated that the false positive to true positive
ratio is going down. The macro averaged recall decreasing
inferred that the ratio of the false-negative to true positive is
going up. This suggested that the false positives are going
down for these sets of experiments, and the false negatives
are going up. Further, the macro averaged F1-score is going
down for A2DE and JChaid∗classiﬁer but with no change for
DMLP, CNN, and HGC.
experimental
in Table 1, it is observed that the scatter search (ScS) method
combined with all classiﬁers outperforms the K-Medoid
sampling-based counterpart at the cost of more training and
testing time. This suggests the ScS method is a better feature
selection method in comparison to K-Medoids sampling.
VOLUME 9, 2021
M. Panda et al.: Developing Efficient Feature Engineering and ML Model for Detecting IoT-Botnet Cyber Attacks
TABLE 2. Comparison with other’s work (I).
Further, from Table 1, while comparing with other machine
learning approaches, it is found that the clustering approach
(HGC) with ScS presents a 100% score to all micro and
macro averaged metrics, but it left out with many unclassiﬁed
instances, which is a cause of concern while choosing this
model for efﬁcient IoT botnet detection.
Next, it is also observed from Table 1 that combining
ScS with the deep learning-based methods such as Deep
MLP classiﬁer and Convolutional neural network classiﬁer
produces 100% micro and macro-averaged precision, recall,
and F-score by both, which makes them equally suitable in
IoT botnet detection. However, CNN + ScS takes a little
more time (163.24 seconds, 1.19 seconds) in comparison to
DMLP + ScS (4.7 seconds and 0.61 seconds) for building
and classifying the model respectively, makes DMLP + ScS
the winner in our proposed research.
While comparing with other’s related work, it can
be inherited from Table 2 that our proposed scatter
search based CNN and DMLP approach outperforms
under-sampling
Oversampling Technique (RU-SMOTE) and Random
under-sampling with Adaptive Synthetic Sampling Method
(RU-ADASYN) performed on the local machine as
well as in AWS spark big data environment with 100% in
each macro-averaged precision, recall, and F1-score, whereas
RU-SMOTE (local machine), RU-SMOTE (AWS with
Spark), RU-ADASYN (local machine) and RU-ADASYN
(AWS with Spark) produces (39.98%,76.42%,44.36%),
(36.01%,74.69%, 39.19%), (39.98%,76.42%,4 4.36%) and
(36.87%, 73.77%, 38.65%) for macro-averaged precision,
recall, and F1-score respectively. Also, it is quite evident from
table 2 that DMLP + ScS is the fastest among all the methods
by taking only 4.7 seconds to build the model followed by
CNN with 163.24 seconds, in comparison to 1498.34 seconds
by RU-SMOTE (local machine), 186 seconds by RU-SMOTE
(AWS with Spark), 1713.95 seconds by RU-ADASYN (local
machine) and 186 seconds by RU-ADASYN (AWS with
From Table 3, It is also veriﬁed that the proposed scatter
search-based CNN and DMLP approach outperforms all the
state-of-the-art classiﬁers including Naïve Bayes (NB),
Decision tree (DT), Association rule mining (ARM), and
Artiﬁcial neural network (ANN). The proposed DMLP + ScS
and CNN + ScS both produce 100% accuracy in IoT botnet
detection, whereas the accuracy of 72.73%, 93.23%, 86.45%,
and 63.97% are obtained by using NB, DT, ARM, and ANN
respectively .
To have more validation of our obtained results, it is compared with Random forest (RF) , Support vector machine
(SVM) , Gaussian Naïve Byes (GNB) , and Extra
trees as a variant of Random forest which is presented
in Table 4.
From Table 4, it is inherited that our proposed approach
seems to perform the same with random forest with
100% accuracy while outperforming SVM (91%) and
GNB (92%) . Further, it is seen that our proposed
approach DMLP + ScS and CNN + ScS along with random
forest produces 100% to precision, recall, and F1-score
followed by extra trees with a score of 99%, 98%, and
98%; NB (91%, 79%, and 88%) and GNB (92%,
88%, and 91%) respectively.
Research Challenges Addressed:
1.1 Since there is scarcity in systematic IoT-botnet datasets,
it’s challenging to conclude the available ones. Still,
NSBW-15 is a recent data that is used in this paper for
experimental work.
2.2 Efﬁcient feature engineering models are sought to
address the resource consumed by complex models,
which is successfully addressed in this research using
K-Medoids and Scatter search.
3.3 Lower detection accuracy is addressed using efﬁcient deep neural network-based classiﬁers (DMLP and
CNN) and classiﬁcation via clustering by combining
Genetic algorithms and K-Means clustering (HGC)
to avoid the problems that occur due to imbalanced
data. Hybrid Genetic algorithm and K-Means clustering
VOLUME 9, 2021
M. Panda et al.: Developing Efficient Feature Engineering and ML Model for Detecting IoT-Botnet Cyber Attacks
TABLE 3. Comparison with other’s work (II).
TABLE 4. Comparison with other’s work (III).
could not classify some instances, but no incorrectly
classiﬁed instances.
4.4 Our proposed approach successfully addresses the
lightweight model for detecting IoT-Botnet attacks with
low computational power, less training time, low test
time, and high detection rate.
5.5 Even though the research work in this ﬁeld is going on,
still low performance in the real world environment is
a concern. Hence, more research to be done as a future
scope to explore further possibilities.
VII. CONCLUSION AND FUTURE SCOPE
We have proposed a feature engineering-based machine
learning/ deep learning solution for the early detection of IoT
botnets in-home networks. A comprehensive performance
evaluation of our proposed deep learning approaches (DMLP
and CNN) and unsupervised HGC combining with efﬁcient scatter search (ScS) based feature engineering method
achieves 100% accuracy with zero false alarm rate in detecting network intrusions.
To gain further insights, we used macro averaged precision,
recall, and F1-score to evaluate the proposed approach apart
from micro-averaged ones and compared it with several existing works in the recent literature. From extensive analysis
with existing literature, it is concluded that our proposed
DMLP + ScS is the winner followed by CNN + ScS (Both
with 100% accuracy, 100% macro and micro-average to precision, recall and F1-score) in comparison to SVM, ARM,
GNB, Extra Trees, A2DE, JChaid∗tree, HGC, RU-SMOTE
( local and AWS with Spark) and RU-ADASYN ( local and
AWS with Spark). Apart from being most accurate and zero
false alarm rate by DMLP + ScS, it is fastest with less
training time and testing time (4.7 seconds, 0.61 seconds)
makes it a lightweight approach. Looking into the importance
of a reliable dataset to build an efﬁcient ML/DL model, this
research encourages for further investigation to inspect the
adequacy of the proposed approach with unalike datasets.
In the future, the authors intend to apply edge computing,
cloud computing and blockchain integration with ML/DL
approaches in the real-time scenario to ﬁnd its suitability for
efﬁcient detection of IoT-botnet cyber-attacks.