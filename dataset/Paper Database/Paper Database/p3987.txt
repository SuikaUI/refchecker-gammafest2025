Multi-View Spectral Clustering via Structured Low-Rank Matrix Factorization
Yang Wang† and Lin Wu‡
†The University of New South Wales, Kensington, Sydney, Australia
‡ The University of Queensland, Brisbane, Australia
 , 
Multi-view data clustering attracts more attention
than their single view counterparts due to the
fact that leveraging multiple independent and complementary information from multi-view feature
spaces outperforms the single one.
Multi-view
Spectral Clustering aims at yielding the data partition agreement over their local manifold structures
by seeking eigenvalue-eigenvector decompositions.
Among all the methods, Low-Rank Representation (LRR) is effective, by exploring the multiview consensus structures beyond the low-rankness
to boost the clustering performance. However, as
we observed, such classical paradigm still suffers
from the following stand-out limitations for multiview spectral clustering of (1) overlooking the ﬂexible local manifold structure, caused by (2) aggressively enforcing the low-rank data correlation
agreement among all views, such strategy therefore
cannot achieve the satisﬁed between-views agreement; worse still, (3) LRR is not intuitively ﬂexible
to capture the latent data clustering structures. In
this paper, we present the structured LRR by factorizing into the latent low-dimensional data-cluster
representations, which characterize the data clustering structure for each view. Upon such representation, (b) the laplacian regularizer is imposed
to be capable of preserving the ﬂexible local manifold structure for each view. (c) We present an iterative multi-view agreement strategy by minimizing
the divergence objective among all factorized latent
data-cluster representations during each iteration of
optimization process, where such latent representation from each view serves to regulate those from
other views, such intuitive process iteratively coordinates all views to be agreeable. (d) We remark
that such data-cluster representation can ﬂexibly
encode the data clustering structure from any view
with adaptive input cluster number. To this end, (e)
a novel non-convex objective function is proposed
via the efﬁcient alternating minimization strategy.
The complexity analysis are also presented. The
extensive experiments conducted against the realworld multi-view datasets demonstrate the superiority over state-of-the-arts.
Introduction
Spectral clustering [Ng et al., 2001; Zelnik-Manor and Perona, 2004; Cai and Chen, 2015; Nie et al., 2011] aims at exploring the local nonlinear manifold (spectral graph)1 structure [Hou et al., 2015; Tao et al., 2016], attracting great attention within recent years. With the development of information technology, multi-view spectral clustering, due to the
fact of outperforming the single view counterparts by leveraging the complementary information from multi-view spaces.
As implied by multi-view research [Xu et al., 2015; Xu et
al., 2013; Wang et al., 2016a], an individual view is not capable of being faithful for effective multi-view learning. Therefore, exploring multi-view information is necessary, and has
been demonstrated by a wide spectrum of applications e.g.,
similarity search [Liu et al., 2015; Wang et al., 2015b;
Wu et al., 2013b; Wu et al., 2015; Wu et al., 2013a; Wang et
al., 2014a; Wang et al., 2015a; Wang et al., 2017a; Wang et
al., 2013b], human action recognition [Jones and Shao, 2014;
Shao et al., 2016; Wu et al., 2017b; Wang et al., 2016b;
Wang et al., 2014b; Wu et al., 2017a; Wu and Wang, 2017;
Wang et al., 2012] etc..
Essentially, given the complementary information from
multi-views, the critical issue of multi-view clustering is to
achieve the multi-view clustering agreement/consensus [Xu
et al., 2013; Gui et al., 2014; Gao et al., 2015] to yield
a substantial superior clustering performance over the single view paradigm.
Numerous multi-view based methods
are proposed for spectral clustering.
[Huang et al., 2012;
Bickel and Scheffer., 2004; Wang et al., 2013a] performs
multi-view information incorporation into the clustering process by optimizing certain objective loss function.
fusion strategy can also be developed by concatenating the
multi-view features into a uniform one [Huang et al., 2010],
upon which the similarity matrix is calculated for further
multi-view spectral clustering. As mentioned by [Wang et
al., 2015c; Wang et al., 2017b], such strategy will be more
likely to destroy the inherent property of original feature representations within each view, hence resulting into a worse
1In the rest of this paper, we will alternatively use nonlinear manifold structure or spectral graph structure
 
performance; worse still, sometimes, as indicated by the experimental reports from our previous research [Wang et al.,
2015c], it may even be inferior to the clustering performance
with a single view. In contrast, Late fusion strategy [Greene
and Cunningham, 2009] conducts spectral clustering performance for each view, and then combining multiple them afterwards, which, however, cannot achieve the multi-view agreement, without collaborating with each other.
Canonical Correlation Analysis (CCA) based methods
[Blaschko and Lampert., 2008; Chaudhuri et al., 2009] for
multi-view spectral clustering project the data from multiview feature spaces onto one common lower dimensional
subspace, where the spectral clustering is subsequently conducted.
One limitation of such method lies in the fact
that such common lower-dimensional subspace cannot ﬂexibly characterize the local manifold structures from heterogeneous views, resulting into an inferior performance. Kumar et al. [Kumar et al., 2011] proposed a state-of-the-art
co-regularized spectral clustering for multi-view data. Similarly, a co-training [Blum and Mitchell, 1998; Wang and
Zhou, 2010] model is proposed for this problem [Kumar and
Daume, 2011].
One assumption for above work [Kumar et al., 2011;
Kumar and Daume, 2011] is the scenario with noise corruption free for each view. However, it is not easily met. To
this end, Low-Rank Representation (LRR) [Xia et al., 2014;
Liu et al., 2010; Wang et al., 2015c; Liu and Yan, 2011;
Liu et al., 2013] is proposed. As summarized in [Wang et
al., 2016a], where the basic idea is to decompose data representation into a view-dependent noise corruption term and a
common low rank based representation shared by all views,
leading to common data afﬁnity matrix for clustering; The
effectiveness of low-rank model also leads to numerous research on multi-view subspace learning [Ding and Fu, 2016;
Dingg and Fu, 2014] applied to the pattern recognition ﬁeld.
LRR tries a common multi-view low-rank representation,
but overlooks the distinct manifold structures. To remedy
the limitations, inspired by the latest development of graph
regularized LRR [Yin et al., 2016; Yin et al., 2015], we recently proposed another iterative views agreement strategy
[Wang et al., 2016a] with graph regularized Low-rank Representation for multi-view spectral clustering, named LR-
RGL for short, to characterize the non-linear manifold structure from each view, LRRGL couples LRR with multi-graph
regularization, where each one can characterize the viewdependent non-linear local data manifold structure [Zhuang
et al., 2016]. A novel iterative view agreement process is
proposed of optimizing the proposed, where, during each iteration, the low-rank representation yielded from each view
serves as the constraint to regulate the representation learning from other views, to achieve the consensus, implemented
by applying Linearized Alternating Direction Method with
Adaptive Penalty [Lin et al., 2011].
Despite the effectiveness of LRRGL, we still identify the
following non-trivial observations that are not addressed by
LRRGL to obtain the further improvement
• It is less ﬂexible for Zi yielded by low-rank constraint to
capture the ﬂexible latent data similarity that can encode
the more rich similarity information than Zi over Xi;
Figure 1: The visualization results of the multi-view (please
refer to section 4 for speciﬁc multi-view features) afﬁnity
matrix between ours and LRRGL over NUS data; The more
whiter for diagonal blocks, the more ideally the cluster is
to characterize the data objects within the larger similarity,
meanwhile, the more blacker for non-diagonal blocks, the
more reasonable the non-similarity data objects are unlikely
to cluster together. For such result, we can see the diagonal blocks from 3rd to the 8th of our method are more whiter
than those of LRRGL, leading to the result that the surrounding black non-diagonal blocks of our method are more salient
than those of LRRGL, which demonstrate the advantages of
our method via a latent factorized data-cluster representation
over LRRGL
that can be better solved by matrix factorization.
• LRRGL mainly focused on yielding the low-rank primal data similarity matrix Zi derived from Xi. However,
such primal Zi is less intuitive to understand and less effective to reveal the ideal data clustering structure for the
ith view, as well as multi-views. Hence, it will prevent
that achieving the better multi-view spectral clustering
performance. The structured consensus loss term imposed over Zi(i ∈V ) may not effectively achieve the
consensus regarding the multi-view spectral clustering.
Our Contributions
This paper is the extension of our recent work [Wang et al.,
2016a], upon that, we deliver the following novel contributions to achieve the further improvement over multi-view
spectral clustering
• Instead of focusing on primal low-rank data similarity
matrix Zi such that i = 1, . . . , V , we perform a symmetric matrix factorization over Zi into the data-cluster indicator matrix, so that such latent factorization provides
the better chance to preserve the ideal cluster structure
besides ﬂexible manifold structure for each view.
• We impose the laplacian regularizer over factorized
data-clustered representation to further characterize the
nonlinear local manifold structure for each view. We
remark that the factorized data-cluster matrix can effectively encode the clustering structure, we provide an example to illustrate this in Fig.1. To reach the multi-view
clustering agreement, we set the same clustering number
Table 1: The Notations Summarization
Explanation
Feature Representation Matrix for the ith view.
Feature noise matrix for the ith view.
Self-expressive similarity matrix for the ith view.
Data-cluster matrix for the ith view.
Data similarity matrix over Xi for the ith view.
The rank of the matrix A.
Updated matrix A at the kth iteration.
Matrix Multiplication between A and B.
The number of data objects.
The dimension of the feature space for the ith view.
The matrix transpose.
Frobenius norm.
Nuclear norm.
ℓ1 norm of matrix seen as a long vector.
The matrix inverse computation.
Identity matrix with the size of d × d.
ℓ2 norm of a vector.
Trace operator over the square matrix.
inner product.
The set of all views.
The kth data cluster.
The cardinality of Ck.
Cardinality of the set V.
The lth row of the matrix A.
The mth column of the matrix A.
The (l, m)th entry of the matrix A.
for all views to the data-clustering representation for all
• We impose the consensus loss term to minimize the divergence among all the latent data-cluster matrix instead
of Zi to achieve the multi-view spectral clustering agreement.
• To implement all the above insights, we propose a novel
objective function, and an efﬁcient alternating optimization strategy together with the complexity analysis to
solve the objective function; moreover, we deliver the
intuitions of iterative multi-view agreement over the factorized latent data-cluster representation during each iteration of our optimization strategy, that will eventually
lead to the multi-view clustering agreement.
• Extensive experiments over real-world multi-view data
sets demonstrate the advantages of our technique over
the state-of-the-arts including our recently proposed
LRRGL [Wang et al., 2016a].
Recently another elegant graph based PCA method [Tang et
al., 2017] is proposed spectral clustering with out-of-sample
case. Unlike this effective technique, we study the multi-view
case to address the effective consensus for spectral clustering.
We summarize the main notations in Table 1.
Structured Low-Rank Matrix Factorization
to Spectral Clustering
We get started from each single view e.g., the ith view as
2||Xi −XiZi −Ei||2
F + ||Zi||∗+ β||Ei||1,
where θ and β are the trade-off parameters, as aforementioned, we always adopt Di to be Xi, so that Xi can be decomposed as clean component XiZi and another corrupted
component Ei for the ith view. One can easily verify that
rank(XiZi) ≤rank(Zi), hence minimizing rank(Zi) is equivalent to bounding the low-rank structure of clean component
Now we are ready to deeply investigate ||Zi||∗for the ith
view. Following [Recht et al., 2010], we reformulate the nuclear norm ||Zi||∗as
Ui,Vi,Zi=UiV T
F + ||Vi||2
where Ui ∈Rn×d and Vi ∈Rn×d; d is always less than di
since high-dimensional data objects always characterize the
low-rank structure.
Notes regarding Ui and Vi for multi-view
spectral clustering
Before further discussing the low-rank matrix factorization,
one may consider the following notes that the factorized Ui
and Vi may need to satisfy in the context of both the withinview data structure preserving and multi-view spectral clustering agreement:
1. The low-rank data structure should be characterized by
the factorized Ui or Vi for the ith view, especially to
characterize the underlying data clustering structure.
2. The factorized latent factors should well encode the
manifold structure for the ith view, which, as previously
mentioned, is critical to the spectral clustering performance.
3. Either the row based matrix Ui or column based matrix Vi is considered to meet the above two notes? if
so, which one? One may claim both to be considered,
which, however, may inevitably raise more parameters
to be tuned.
4. Not only the factorized latent low-dimensional factors
e.g., Ui or Vi, should meet the above notes within each
view e.g., the ith view, but also need the same scale to
unify all views to reach possible agreement.
To address all the above notes, in what follows, we will
present our technique of data-cluster based structured lowrank matrix factorization.
Data-cluster (landmark) based Structured
Low-Rank Matrix factorization
We aim at factorizing Zi as an approximate symmetric lowrank data-cluster matrix to minimize the reconstruction error
Zi ||Xi −XiZi||2
where we assume the rank of Zi is ki, such that ki is related to
the data cluster number for the ith view. As indicated by [Liu
et al., 2013], minimizing the Eq.(3) is equivalent to ﬁnding
the optimal rank ki approximation relying on skinny singular
value decomposition of Xi = V ΣU T to yield the following
optimal solution
where Ui ∈Rn×ki, such that ki denotes the top ki principle basis of Xi. Here we follow the assumption in [Kuang
et al., 2012] to see ki as the cluster number of data objects
within the ith view, and the data-cluster symmetric matrix
factorization has been widely adopted by the numerous existing research including semi-supervised learning [Wang et al.,
2014c; Wu et al., 2016], metric fusion [Wang et al., 2017b]
and clustering [Cai and Chen, 2015]. We aim at solving the
following equivalent low-rank minimization over Zi via the
clustered symmetric matrix factorization below
Ui,Zi=UiU T
where we often minimize the following for derivative convenience with respect to Ui
Ui,Zi=UiU T
Following Eqs.
(3) and (4), we initialize the
Ui ∈Rn×ki via a k means clustering over Xi and normalize Ui(j, k) =
|Ck| provided Xi(·, j) i.e., the jth data object
is assigned to Ck. By such normalization, all the columns of
Ui are orthonormal; moreover, they are within the same magnitude so as to perform the agreement minimization. Furthermore, such factorization can well address the aforementioned
challenges, it is worthwhile to summarize them below
• The data cluster structure can be well encoded by such
low-rank data-cluster representation within each view.
The setting Ui = Vi can avoid the more parameters and
importance weight discussion provided Ui ̸= Vi.
• More importantly, inspired by the reasonable assumption hold by all the multi-view clustering research [Kumar et al., 2011; Kumar and Daume, 2011; Bickel and
Scheffer., 2004; Wang et al., 2015c]. As indicated by
[Wang et al., 2015c], the ideal multi-view clustering performance is that the common underlying data clustering
structure is shared by all the views; we naturally set all
the Ui with the same size by adopting the same value
for ki = d(i = 1, . . . , V ) i.e., the clustering number,
upon the same data objects number n for all views, so
that the feasible loss functions can be developed to seek
the multi-view clustering agreement with the same clustering number for all views.
For spectral clustering from each view, we preserve the
nonlinear local manifold structure of Xi via such low-rank
data-cluster representation Ui for the ith view, which can be
formulated as
jHi(j, j) −
i HiUi) −Tr(U T
i WiUi) = Tr(U T
k ∈Rd is the kth row vector of Ui ∈Rn×d representing the linear correlation between xk and xj(j ̸= k) in
the ith view; Wi(j, k) encodes the similarity between xj and
xk for the ith view; Hi is a diagonal matrix with its kth diagonal entry to be the summation of the kth row of Wi, and
Li = Hi −Wi is the graph laplacian matrix for the ith view.
Following [Wang et al., 2016a], we choose Gaussian kernel
to deﬁne W i
Wi(j, k) = e−
We aim to minimize the difference of low-rank based datacluster representations for all views via a mutual consensus
loss function term to coordinate all views to reach clustering
agreement, while structuring such representation with laplacian regularizer to encode the local manifold structure for
each view.
Unlike the traditional LRR to achieve the common data
similarity by all views, we propose to learn a variety of factorized low-rank data-cluster representations for different views
to preserve the ﬂexible local manifold structure while achieving the data cluster structure for each view, upon which, the
consensus loss term is imposed to achieve the multi-view consensus, leading to our iterative views agreement in the next
The Objective Function with structured
low-rank Matrix factorized representation
We propose the objective function with structured low-rank
representation Ui for each view e.g., the ith view with factorized low-rank via Eq. (6) data-clustered representation via
Eq.(3). Then we have the following
Ui,Ei(i∈V )
minimize ||Zi||∗via Eq.(6)
noise and corruption robustness
Graph Structured Regularization
||Ui −Uj||2
Views-agreement
i = 1, . . . , V, Xi = XiUiU T
i + Ei, Ui ≥0,
• Ui ∈Rn×d denotes the factorized low-rank data-cluster
representation of Xi for the ith view.
makes Ui to be structured with local manifold structure
for the ith view. ||Ei||1 is responsible for possible noise
with Xi. λ1, λ2, β are all trade-off parameters.
• One reasonable assumption hold by a lot of multi-view
clustering research [Gao et al., 2013; Kumar et al., 2011;
Bickel and Scheffer., 2004; Kumar and Daume, 2011]
is that all the views should share the similar underlying clustering structure. P
i,j∈V ||Ui −Uj||2
achieve the views-agreement regarding the factorized
low-rank representations Ui from all |V | views; unlike
the traditional LRR method to enforce an identical representation, we construct different Ui for each view, then
further minimize their divergence to generate a viewagreement.
• Ui ≥0 is a non-negative constraint, through Xi =
XiZi + Ei = XiUiU T
i + Ei for the ith view.
Eq.(9) is non-convex, we hence alternately optimize each
variable while ﬁxing the others; that is, updating all the Ui
and Ei(i ∈{1, . . . , V }) in an alternative way until the convergence is reached. As solving all the {Ui, Ei}(i ∈V ) pairs
shares the similar optimization strategy, only the ith view is
presented. To this end, we introduce two auxiliary variables
Di and Gi, then solving the Eq.(9) with respect to Ui, Ei, Di
and Gi that can be written as follows
Ui,Ei,Di,Gi
F + λ1||Ei||1
+ λ2Tr(UiLiU T
||Ui −Uj||2
s.t. Xi = DiU T
i + Ei, Di = XiUi, Gi = Ui, Gi ≥0,
where Di ∈Rdi×d, we will show the intuition for the auxiliary variable relationship Di = XiUi by introducing the
augmented lagrangian function based on Eq.(10) below
L(Ui, Ei, Di, Gi, Ki
F + λ1||Ei||1 + λ2Tr(U T
||Ui −Uj||2
1, Xi −DiU T
2, Ui −Gi⟩+ ⟨Ki
3, Di −XiUi⟩
2 (||Xi −DiU T
F + ||Ui −Gi||2
F + ||Di −XiUi||2
1 ∈Rdi×n, Ki
2 ∈Rn×d and Ki
3 ∈Rdi×d are Lagrange multipliers, µ > 0 is a penalty parameter.
From Eq.(11), we can easily show the intuition on Di =
XiUi, that is,
• minimizing ||Xi −DiU T
F w.r.t. Di is similar
as dictionary learning, while pop out the U T
i as corresponding representations learning, both of them reconstruct the Xi for the ith view. Besides the above intuition, it is quite simple to optimize only single U T
merging the other into Di.
Optimization Strategy
We minimize Eq.(11) by updating each variable while ﬁxing
the others.
Minimizing Ui is to resolve Eq.(12)
F + λ2Tr(U T
||Ui −Uj||2
1, Xi −DiU T
2, Ui −Gi⟩+ ⟨Ki
3, Di −XiUi⟩
2 (||Xi −DiU T
F + ||Ui −Gi||2
F + ||Di −XiUi||2
We set the derivative of Eq.(12) w.r.t. Ui to be the zero
matrix, which yields the Eq.(13) below
= Ui + 2λ2LiUi + β
1)T Di + Ki
i Di + µET
i Di + µ(Ui −Gi)
i XiUi = 0,
where 0 ∈Rn×d shares the same size as Ui. Rearranging the
other terms further yields the following
Ui = (2λ2Li + (1 + β(|V | −1) + µ)In −µXT
with computational complexity O(n3)
1)T −µUiDT
The bottleneck of computing Eq.(14) lies in the inverse matrix computation over the matrix of the size Rn×n causing
the computational complexity O(n3), which is computationally prohibitive provided that n is large. Therefore, we turn to
update each row of Ui; without loss of generality, we present
the derivative with respect to Ui(l, ·) as
Ui(l, ·) + Ui(l, ·)
(2λ2Li(k, l) −µ(XT
i Xi)(k, l))
1)T (l, ·)Di + µUi(l, ·)DT
2(l, ·) −XT
i (l, ·)Ki
(Ui(l, ·) −Uj(l, ·))
 Ui(l, ·) + ET
i (l, ·)Di −Gi(l, ·)
where 0 ∈Rd denotes the vector of the size d with all entries to be 0, Ui(l, ·) ∈Rd represents the lth row of Ui; we
rearrange the terms to yield the following
Inﬂuences from other views
(2λ2Li(k, l) −µ(XT
i Xi)(k, l)))Id + DT
with computational complexity O(d3)
i (l, ·)Ki
 Gi(l, ·) −ET
i (l, ·)Di
2(l, ·) −(Ki
1)T (l, ·)Di
Id ∈Rd×d is the identity matrix.
Complexity discussion for the row updating strategy for
Unlike the closed form regarding Ui, it is apparent that the
major computational complexity lies in the inverse matrix
computation over the size of Rd×d, which leads to O(d3) according to Eq.(17), which is much smaller than O(n3). Besides, as d is set as the cluster number across all views; moreover, aforementioned, it should be less than the inherent rank
of Xi, and hence a small value. Upon the above facts, it is
tremendously efﬁcient via O(d3) to sequentially update each
row of Ui.
Intuitions for views agreement
The iterative views clustering agreement can be immediately
captured via the terms underlined in Eq.(17). Speciﬁcally,
during each iteration, the Ui(l, ·) is updated via the inﬂuence
from others view, while served as the constraint to generate
Uj(l, ·)(j ̸= i), the divergence among all Ui(l, ·) is decreased
gradually towards an agreement for all views, such process
repeats until the convergence is reached.
Unlike the existing LRR method by directly imposing the
common representation, our iterative multi-view agreement
can better preserve the ﬂexible manifold structure for each
view meanwhile achieve the multi-view agreement, which
will be critical to ﬁnal multi-view spectral clustering.
Remark. After the whole Ui is updated for the ith view, we
simply perform a K-means clustering over it to assign each
data object to one cluster exclusively. Then normalized each
column of Ui to form an orthonormal matrix.
The optimization process regarding Di is equivalent to the
i, Xi −DiU T
i −Ei > + < Ki
3, Di −XiUi >
 ||Xi −DiUi −Ei||2
F + ||Di −XiUi||2
We get the derivative with respect to Di, then it yields the
following closed form updating rule
3 + µ(2Xi −Ei)Ui
  Id + U T
where the major computational complexity lies in the inverse
computation over matrix (Id +U T
i Ui) ∈Rd×d, resulting into
O(d3), as aforementioned, that is the same as updating each
row of Ui, and hence quite efﬁcient.
it is equivalent to solving the following:
Ei λ1||Ei||1 + µ
2 ||Ei − .
Input: Xi(i = 1, . . . , V ), d, λ1, λ2, β
Output: Ui, Di, Ei, Gi(i ∈V )
1 Initialize: Ui , Li(i = 1, . . . , V ) computation, set all
entries of Ki
1 , Gi , Ki
2 to be 0, initialize Ei 
with sparse noise as 20% entries corrupted with
uniformly distributed noise over [-5,5], µ = 10−3,
ϵ1 = 10−3, ϵ2 = 10−1
3 for i ∈V do
Sequentially update each row of Ui according to
Orthonormalized each column of Ui.
Update Ei:
Ei[k + 1] = S λ1
µ[k] (Xi −DiU T
Update Gi:
Gi[k + 1] = Ui[k] + Ki
1[k + 1] = Ki
1[k] + µ(Xi −DiU T
i [k] −Ei[k])
2[k + 1] = Ki
2[k] + µ(Ui[k] −Gi[k])
3[k + 1] = Ki
3[k] + µ(Di[k] −XiUi[k])
Update µ according to [Lin et al., 2011]
whether converged
if ||Xi −DiU T
i [k + 1] −Ei[k + 1]||/||Xi|| < ϵ1 and
max{ξ||Ui[k + 1] −Ui[k]||, µ[k]||Gi[k + 1] −
Gi[k]||, µ[k]||Ei[k + 1] −Ei[k]||} < ϵ2 then
Remove the ith view from the view set as
Ui[N] = Ui[k + 1], s.t. N is any positive integer.
23 Return Ui[k + 1], Di[k + 1], Ei[k + 1], Gi[k + 1]
(i = 1, . . . , V )
Updating Ki
We update Lagrange multipliers Ki
1 + µ(Xi −DiUi −Ei)
2 + µ(Ui −Gi)
3 + µ(Di −XiUi)
Following [Wang et al., 2016a], µ is tuned using the adaptive updating strategy [Lin et al., 2011] to yield a faster convergence.
The optimization strategy alternatively updates
each variable while ﬁxing others until the convergence, which
is summarized by Algorithm 1.
Notes regarding Algorithm 1
It is worthwhile to highlight some critical notes regarding the
Algorithm 1 below
• We initialize the Ui ∈Rn×d for all views, such that
each entry of Ui represents similarity between each
data object and one of the d anchors (cluster representatives), which can be seen as the centers from the clusters
generated from the k-means or spectral clustering.
• For our initialization, we adopt the spectral clustering
outcome with the clustering number to be d, where the
similarity matrix is calculated via the original Xi feature
representation within each view, then the Ui (i, j) entry i.e., the similarity between the ith data object and the
jth anchor is yielded via Eq.(8). The laplacian matrix
Li(i = 1, . . . , V ) are computed once ofﬂine also within
the original Xi feature representation.
• More importantly, we set the identical value of d(the
cluster number) to the column size of Ui (i
1, ··, V ) ∈Rn×d for all the views. We remark that the
above initial setting for Ui with the same d is reasonable, as stated before all the views should share the
similar underlying data clustering structure. This fact
also implies that the initialized Ui is reasonably not
divergent a lot among all views.
Convergence discussion
Often, the above alternating minimization strategy can be
seen as the coordinate descent method. According to [Bertsekas, 1999], the sequences (Ui, Di, Ei, Gi) above will eventually converge to a stationary point. However, we are not
sure whether the converged stationary point is a global optimum, as it is not jointly convex to all the variables above.
Clustering
Following [Wang et al., 2016a], once the converged Ui(i =
1, . . . , V ) are ready, all column vectors of Ui(i = 1, . . . , V )
while set small entries under given threshold τ to be 0. Afterwards, the similarity matrix for the ith view between the jth
and kth data objects as
Wi(j, k) = to yield
ﬁnal outcome of d data groups.
Table 2: Data sets.
UCI digits
Color (1500)
LSS 
PHOG (252)
SIFT 
RGSIFT 
SURF 
# of classes
Experiments
We adopt the data sets mentioned in [Wang et al., 2016a] below:
• UCI handwritten Digit set2:
consists of features for
hand-written digits (0-9), with 6 features and contains
2000 samples with 200 in each category. Analogous to
[Lin et al., 2011; Wang et al., 2016a], we choose two
views as 76 Fourier coefﬁcients (FC) of the character
shapes and the 216 proﬁle correlations.
• Animal with Attribute (AwA)3: consists of 50 kinds of
animals described by 6 features (views): Color histogram ( CQ, 2688-dim), local self-similarity , pyramid HOG (PHOG, 252-dim), SIFT
 , Color SIFT , and
SURF . Following [Wang et al., 2016a], 80
images for each category and get 4000 images in total.
• NUS-WIDE-Object (NUS) [Chua et al., 2009]: 30000
images from 31 categories.
5 views are adopted using 5 features as provided by the website
65dimensional color histogram (CH), 226-dimensional
color moments (CM), 145-dimensional color correlation
(CORR), 74-dimensional edge estimation (EDH), and
129-dimensional wavelet texture (WT).
• PASCAL VOC 20125:
we select 20 categories with
11530 images, two views are constructed with Color features (1500-dim) and HOG features (250 dim). Among
them, 5600 images are selected by removing the images
with multiple categories.
We summarize the above throughout Table 2.
The following state-of-the-art baselines used in [Wang et al.,
2016a] are compared:
• MFMSC: concatenating multi-features to be the multiview representation for similarity matrix, the spectral
clustering is then conducted [Huang et al., 2010].
• Multi-feature representation similarity aggregation for
spectral clustering (MAASC) [Huang et al., 2012].
• Canonical
Correlation
(CCAMSC) [Chaudhuri et al.,
Projecting
multi-view data into a common subspace, then perform
spectral clustering.
2 
3 
4lms.comp.nus.edu.sg/research/NUS-WIDE.html
5 
• Co-regularized
multi-view
clustering
(CoMVSC) [Kumar et al.,
It regularizes
the eigenvectors of view-dependent graph laplacians
and achieve consensus clusters across views.
• Co-training [Kumar and Daume, 2011]: Alternately
modify one view’s Laplacian eigenspace by learning
from the other views ’s eigenspace, the spectral clustering is then conducted.
• Robust Low-Rank Representation method (RLRR) [Xia
et al., 2014], after obtaining the data similarity matrix,
upon which, the spectral clustering is performed to be
the ﬁnal multi-view spectral clustering result.
• Low-rank
Representation
(LRRGL) [Wang et al., 2016a] regularizer over the
non-factorized low-rank representations,
of which corresponds to one view to preserve the
individual manifold structure, while iteratively boost all
these low-rank representations to reach agreement. The
ﬁnal multi-view spectral clustering is performed upon
the similarity representations
Experimental Settings and Parameters Study
We implement these competitors under the experimental setting as mentioned in [Wang et al., 2016a]. Following [Wang
et al., 2016a], σ in Eq.(8) is learned via [Zelnik-Manor and
Perona, 2004], and s = 20 to construct s-nearest neighbors
for Eq.(8). We adopt two standard metrics: clustering accuracy (ACC) and normalized mutual information (NMI) as the
metric deﬁned as Eq.(29)
i=1 δ(map(ri), li)
where ri denotes the cluster label of xi, and li denotes the
true class label, n is the total number of images, δ(x, y) is the
function that equals one if x = y and equals zero otherwise,
and map(ri) is the permutation mapping function that maps
each cluster label ri to the equivalent label from the database.
Meanwhile the NMI is formulated below
j=1 ni,j log ni,j
i=1 ni log ni
j=1 ˆnj log ˆnj
where ni is the sample number in cluster Ci (1 ⩽i ⩽c), ˆnj
is the sample number from class Lj (1 ⩽j ⩽c), and ni,j denotes the sample number in the intersection between Ci and
Remark. Following [Wang et al., 2016a], we repeated the
running 10 times, and their averaged mean value for multiview spectral clustering for all methods is reported. For each
method including ours, we input the clustering number as the
number of ground-truth classes from all data sets.
Feature noise modeling for robustness: Following [Siyahjani et al., 2015; Wang et al., 2016a], 20% feature elements
are corrupted with uniform distribution over the range [5,-
5], which is consistent to the practical setting while matching
with LRRGL,RLRR and our method.
Following [Wang et al., 2016a], We set λ1 = 2 in Eq.(9)
for sparse noise term. We test ACC and NMI over different
value of λ2 and β in Eq.(9) in the next subsection.
Table 3: ACC results.
UCI digits
MFMSC [Huang et al., 2010]
MAASC [Huang et al., 2012]
CCAMSC[Chaudhuri et al., 2009]
CoMVSC[Kumar et al., 2011]
Co-training[Kumar and Daume, 2011]
RLRR[Xia et al., 2014]
LRRGL[Wang et al., 2016a]
Table 4: NMI results.
UCI digits
MFMSC [Huang et al., 2010]
MAASC [Huang et al., 2012]
CCAMSC[Chaudhuri et al., 2009]
CoMVSC[Kumar et al., 2011]
Co-training[Kumar and Daume, 2011]
RLRR[Xia et al., 2014]
LRRGL[Wang et al., 2016a]
Validation over factorized low-rank latent
data-cluster representation
First, we will would like to validate our method regarding the
multi-graph regularization and iterative views agreement over
factorized latent data-cluster representation.
Following [Wang et al., 2016a], we test λ2 and β within
the interval [0.001,10], with one parameter while ﬁxing the
value of the other parameter, the ACC results are shown in
Fig. 2, where we have
• Increasing β will improve the performance, and vice
versa; that is, increasing λ2 will improve the performance.
• The clustering metric ACC increases when both λ2 and
β increase.
Based on the above, we choose a balance pair values: λ2 =
0.7 and β = 0.2 for our method.
According to Table 3 and Table 4, the following identiﬁcation can be drawn, note that we mainly deliver the analysis
between our method and LRRGL, as the analysis over other
competitors have been detailed in [Wang et al., 2016a].
• First, our method outperforms LRRGL, implying the
effectiveness of the factorized latent data-cluster representation, as it can better encode the data-cluster representation for each view as well as all views. We provide
more insights about that in Fig. 3.
• Second, both our method and LRRGL outperforms the
model of learning a common low-dimensional subspace
among multi-view data, as indicated by [Wang et al.,
2016a] it is incapable of encoding local graph structures
within a single subspace.
• Our method and LRRGL are more effective under noise
corruptions than other methods. More analysis can be
referred to our conference version [Wang et al., 2016a].
Figure 2: Study over λ2 and β over latent factorized data-cluster representation on three datasets.
• Our method achieves the best performance over PAS-
CAL VOC 2012 under the selected two views via the
tuned the parameters.
We present Fig.
3 to show more intuitions on why
our method with the multi-view afﬁnity matrix yielded from
factorized data-cluster representation outperforms the primal
similarity matrix for LRRGL. For example,
• For UCI dataset, i.e., the multi-view afﬁnity matrix illustrated in Fig. 3(a) and 3(d), we can see the both 4th and
5th diagonal blocks of our method in Fig. 3(d) are more
whiter than those of LRRGL illustrated in Fig.
meanwhile the surrounding non-diagonal black blocks
e.g., (4, 5)th and (5, 4)th are more black than those of
textbfLRRGL.
• For AwA dataset, the diagonal blocks of our method
from the 2nd to the 6th are whiter than those of LRRGL,
leading to a slight deeper black color over the surrounding non-diagonal blocks than LRRGL.
• The similar conclusions also hold for NUS dataset, we
can see the diagonal blocks from 3rd to the 8th of our
method are more whiter than those of LRRGL, leading to the result that the surrounding black non-diagonal
blocks of our method are more salient than those of LR-
From the above observations, we can safely infer the advantages of the afﬁnity matrix representation yielded by our factorized latent data-cluster representation over the primal afﬁnity matrix of LRRGL for Multi-view spectral clustering.
Conclusion
In this paper, we propose to learn a clustered low-rank representation via structured matrix factorization for multi-view
spectral clustering. Unlike the existing methods, we propose
an iterative strategy of intuitively achieving the multi-view
spectral clustering agreement by minimizing the betweenview divergences in terms of the factorized latent dataclustered representation for each view. Upon that, we impose
the graph Laplacian regularizer over such low-dimensional
data-cluster representation, so as to adapt to the multi-view
spectral clustering, as demonstrated by the extensive experiments.
The future work includes the following directions: The
graph regularized low-rank embedding out-of-sample case
has been researched [Nie et al., 2011], and will be applied for
multi-view out-of-sample scenario. Unlike the pre-deﬁned
graph similarity value, inspired by [Nie et al., 2017], we will
simultaneously learn and achieve the consensus graph clustering result and graph structure i.e., graph similarity. Besides,
the latest non-parametric graph construction model [Nie et
al., 2016b] will also be incorporated for multi-view spectral
clustering. The practice of our method can be improved by
reducing the tuned parameters further. Upon that, we will
also investigate the problem of learning the weight [Nie et
al., 2017; Nie et al., 2016a; Cai et al., 2013] for each view.