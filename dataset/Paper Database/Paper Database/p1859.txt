Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113,
Baltimore, Maryland USA, June 26–27, 2014. c⃝2014 Association for Computational Linguistics
EU-BRIDGE MT: Combined Machine Translation
∗Markus Freitag, ∗Stephan Peitz, ∗Joern Wuebker, ∗Hermann Ney,
‡Matthias Huck, ‡Rico Sennrich, ‡Nadir Durrani,
‡Maria Nadejde, ‡Philip Williams, ‡Philipp Koehn,
†Teresa Herrmann, †Eunah Cho, †Alex Waibel
∗RWTH Aachen University, Aachen, Germany
‡University of Edinburgh, Edinburgh, Scotland
†Karlsruhe Institute of Technology, Karlsruhe, Germany
∗{freitag,peitz,wuebker,ney}@cs.rwth-aachen.de
‡{mhuck,ndurrani,pkoehn}@inf.ed.ac.uk
‡ 
‡ , 
†{teresa.herrmann,eunah.cho,alex.waibel}@kit.edu
This paper describes one of the collaborative efforts within EU-BRIDGE to
further advance the state of the art in
machine translation between two European language pairs, German→English
and English→German.
Three research
institutes involved in the EU-BRIDGE
project combined their individual machine
translation systems and participated with a
joint setup in the shared translation task of
the evaluation campaign at the ACL 2014
Eighth Workshop on Statistical Machine
Translation .
We combined up to nine different machine
translation engines via system combination. RWTH Aachen University, the University of Edinburgh, and Karlsruhe Institute of Technology developed several
individual systems which serve as system combination input. We devoted special attention to building syntax-based systems and combining them with the phrasebased ones.
The joint setups yield empirical gains of up to 1.6 points in BLEU
and 1.0 points in TER on the WMT newstest2013 test set compared to the best single systems.
Introduction
EU-BRIDGE1 is a European research project
which is aimed at developing innovative speech
translation technology.
This paper describes a
1 
joint WMT submission of three EU-BRIDGE
project partners.
RWTH Aachen University
(RWTH), the University of Edinburgh (UEDIN)
and Karlsruhe Institute of Technology (KIT) all
provided several individual systems which were
combined by means of the RWTH Aachen system
combination approach . As
distinguished from our EU-BRIDGE joint submission to the IWSLT 2013 evaluation campaign , we particularly focused on translation of news text (instead of talks) for WMT. Besides, we put an emphasis on engineering syntaxbased systems in order to combine them with our
more established phrase-based engines. We built
combined system setups for translation from German to English as well as from English to German. This paper gives some insight into the technology behind the system combination framework
and the combined engines which have been used
to produce the joint EU-BRIDGE submission to
the WMT 2014 translation task.
The remainder of the paper is structured as follows: We ﬁrst describe the individual systems by
RWTH Aachen University (Section 2), the University of Edinburgh (Section 3), and Karlsruhe
Institute of Technology (Section 4).
present the techniques for machine translation system combination in Section 5. Experimental results are given in Section 6. We ﬁnally conclude
the paper with Section 7.
RWTH Aachen University
RWTH employs both the
phrase-based (RWTH scss) and the hierarchical
(RWTH hiero) decoder implemented in RWTH’s
publicly available translation toolkit Jane . The model
weights of all systems have been tuned with standard Minimum Error Rate Training 
on a concatenation of the newstest2011 and newstest2012 sets.
RWTH used BLEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit
 is used. All RWTH systems include the standard set of models provided
by Jane. Both systems have been augmented with
a hierarchical orientation model and a cluster language model . The phrasebased system (RWTH scss) has been further improved by maximum expected BLEU training similar to . The latter has been
performed on a selection from the News Commentary, Europarl and Common Crawl corpora based
on language and translation model cross-entropies
 .
University of Edinburgh
UEDIN contributed phrase-based and syntaxbased systems to both the German→English and
the English→German joint submission.
Phrase-based Systems
UEDIN’s phrase-based systems have been trained using the Moses toolkit
 , replicating the settings described in . The features
include: a maximum sentence length of 80, growdiag-ﬁnal-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM used at runtime, a lexically-driven 5-gram
operation sequence model (OSM) , msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features , a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding , cube pruning , with a stack size of 1000 during tuning and
5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using Kneser-
Ney smoothed 7-gram models as additional factors
in phrase translation models . UEDIN has furthermore built OSM models over POS and morph sequences following
Durrani et al. .
The English→German
system additionally comprises a target-side LM
over automatically built word classes .
UEDIN has applied syntactic prereordering and compound
splitting of the source
side for the German→English system. The systems have been tuned on a very large tuning set
consisting of the test sets from 2008-2012, with
a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN
phrase-based 1 system, UEDIN phrase-based 2
augments word classes as additional factor and
learns an interpolated target sequence model over
cluster IDs. Furthermore, it learns OSM models
over POS, morph and word classes.
Syntax-based Systems
UEDIN’s syntax-based systems follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu
 .
The open source Moses
implementation has been employed to extract
GHKM rules . Composed rules are extracted in
addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule,
a maximum depth of ﬁve, and a maximum size of
ﬁve. Singleton hierarchical rules are dropped.
The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation
probabilities, lexical translation probabilities in
both directions, word and phrase penalty, a rule
rareness penalty, a monolingual PCFG probability,
and a 5-gram language model. UEDIN has used
the SRILM toolkit to train the language model and relies on KenLM for language
model scoring during decoding. Model weights
are optimized to maximize BLEU. 2000 sentences
from the newstest2008-2012 sets have been selected as a development set.
The selected sentences obtained high sentence-level BLEU scores
when being translated with a baseline phrasebased system, and each contain less than 30 words
for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning
using Moses’ hierarchical decoder on the English target-language side.
For English→German, UEDIN has trained various string-to-tree GHKM syntax systems which
differ with respect to the syntactic annotation. A
tree-to-string system and a string-to-string system
(with rules that are not syntactically decorated)
have been trained as well. The English→German
UEDIN GHKM system names in Table 3 denote:
UEDIN GHKM S2T (ParZu):
A string-to-tree
system trained with target-side syntactic annotation obtained with ParZu . It uses a modiﬁed syntactic label
set, target-side compound splitting, and additional syntactic constraints.
UEDIN GHKM S2T (BitPar):
A string-to-tree
system trained with target-side syntactic
annotation obtained with BitPar (Schmid,
UEDIN GHKM S2T (Stanford):
A string-totree system trained with target-side syntactic
annotation obtained with the German Stanford Parser .
UEDIN GHKM S2T (Berkeley):
A string-totree system trained with target-side syntactic
annotation obtained with the German Berkeley Parser .
UEDIN GHKM T2S (Berkeley):
A tree-tostring system trained with source-side syntactic annotation obtained with the English
Berkeley Parser .
UEDIN GHKM S2S (Berkeley):
A string-tostring system.
The extraction is GHKMbased with syntactic target-side annotation
from the German Berkeley Parser, but we
strip off the syntactic labels. The ﬁnal grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules
that have been added from plain phrase-based
extraction .
Karlsruhe Institute of Technology
The KIT translations are
generated by an in-house phrase-based translations system . The provided News
Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation
The monolingual part of those parallel
corpora, the News Shufﬂe corpus for both directions and additionally the Gigaword corpus for
German→English are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in , using
newstest2012 and newstest2013 as development
and test data respectively.
Compound splitting 
is performed on the source side of the corpus for
German→English translation before training. In
order to improve the quality of the web-crawled
Common Crawl corpus, noisy sentence pairs are
ﬁltered out using an SVM classiﬁer as described
by Mediani et al. .
The word alignment for German→English is
generated using the GIZA++ toolkit . For English→German, KIT uses discriminative word alignment .
Phrase extraction and scoring is done using the
Moses toolkit . Phrase pair
probabilities are computed using modiﬁed Kneser-
Ney smoothing as in .
In both systems KIT applies short-range reorderings and longrange reorderings 
based on POS tags to perform
source sentence reordering according to the target
language word order. The long-range reordering
rules are applied to the training corpus to create
reordering lattices to extract the phrases for the
translation model.
In addition, a tree-based reordering model trained
on syntactic parse trees as well as a lexicalized reordering model are
Language models are trained with the SRILM
toolkit and use modiﬁed Kneser-
Ney smoothing.
Both systems utilize a language model based on automatically learned
word classes using the MKCLS algorithm . The English→German system comprises
language models based on ﬁne-grained part-ofspeech tags . In addition, a bilingual language model is used as well as a discriminative word lexicon using source context to
guide the word choices in the target sentence.
In total, the English→German system uses the
following language models: two 4-gram wordbased language models trained on the parallel data
and the ﬁltered Common Crawl data separately,
two 5-gram POS-based language models trained
on the same data as the word-based language models, and a 4-gram cluster-based language model
trained on 1,000 MKCLS word classes.
The German→English system uses a 4-gram
word-based language model trained on all monolingual data and an additional language model
trained on automatically selected data .
Again, a 4-gram cluster-based
language model trained on 1000 MKCLS word
classes is applied.
System Combination
System combination is used to produce consensus translations from multiple hypotheses which
are outputs of different translation engines. The
consensus translations can be better in terms of
translation quality than any of the individual hypotheses. To combine the engines of the project
partners for the EU-BRIDGE joint setups, we apply a system combination implementation that has
been developed at RWTH Aachen University.
The implementation of RWTH’s approach to
machine translation system combination is described in . This approach
includes an enhanced alignment and reordering
framework. Alignments between the system outputs are learned using METEOR . A confusion network is then built
using one of the hypotheses as “primary” hypothesis. We do not make a hard decision on which
of the hypotheses to use for that, but instead combine all possible confusion networks into a single
lattice. Majority voting on the generated lattice
is performed using the prior probabilities for each
system as well as other statistical models, e.g. a
special n-gram language model which is learned
on the input hypotheses. Scaling factors of the
models are optimized using the Minimum Error
Rate Training algorithm. The translation with the
best total score within the lattice is selected as consensus translation.
In this section, we present our experimental results
on the two translation tasks, German→English
and English→German.
The weights of the individual system engines have been optimized on
different test sets which partially or fully include
newstest2011 or newstest2012. System combination weights are either optimized on newstest2011
or newstest2012. We kept newstest2013 as an unseen test set which has not been used for tuning
the system combination or any of the individual
German→English
The automatic scores of all individual systems
as well as of our ﬁnal system combination submission are given in Table 1. KIT, UEDIN and
RWTH are each providing one individual phrasebased system output. RWTH (hiero) and UEDIN
(GHKM) are providing additional systems based
on the hierarchical translation model and a stringto-tree syntax model.
The pairwise difference
of the single system performances is up to 1.3
points in BLEU and 2.5 points in TER.
German→English, our system combination parameters are optimized on newstest2012. System
combination gives us a gain of 1.6 points in BLEU
and 1.0 points in TER for newstest2013 compared
to the best single system.
In Table 2 the pairwise BLEU scores for all individual systems as well as for the system combination output are given. The pairwise BLEU score
of both RWTH systems (taking one as hypothesis
and the other one as reference) is the highest for all
pairs of individual system outputs. A high BLEU
score means similar hypotheses. The syntax-based
system of UEDIN and RWTH scss differ mostly,
which can be observed from the fact of the lowest pairwise BLEU score.
Furthermore, we can
see that better performing individual systems have
higher BLEU scores when evaluating against the
system combination output.
In Figure 1 system combination output is compared to the best single system KIT. We distribute
the sentence-level BLEU scores of all sentences of
newstest2013. To allow for sentence-wise evaluation, all bi-, tri-, and four-gram counts are initialized with 1 instead of 0. Many sentences have
been improved by system combination. Nevertheless, some sentences fall off in quality compared
to the individual system output of KIT.
English→German
The results of all English→German system setups
are given in Table 3. For the English→German
translation task, only UEDIN and KIT are con-
newstest2011
newstest2012
newstest2013
RWTH hiero
UEDIN GHKM S2T (Berkeley)
Table 1: Results for the German→English translation task. The system combination is tuned on newstest2012, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are signiﬁcantly better than the best single system
with p < 0.05.
RWTH hiero
RWTH hiero
Table 2: Cross BLEU scores for the German→English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as hypothesis and the other one as reference.)
newstest2011
newstest2012
newstest2013
UEDIN phrase-based 1
UEDIN phrase-based 2
UEDIN GHKM S2T (ParZu)
UEDIN GHKM S2T (BitPar)
UEDIN GHKM S2T (Stanford)
UEDIN GHKM S2T (Berkeley)
UEDIN GHKM T2S (Berkeley)
UEDIN GHKM S2S (Berkeley)
Table 3: Results for the English→German translation task. The system combination is tuned on newstest2011, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are signiﬁcantly better than
the best single system with p < 0.05. Italic font indicates system combination results that are signiﬁcantly
better than the best single system with p < 0.1.
tributing individual systems. KIT is providing a
phrase-based system output, UEDIN is providing
two phrase-based system outputs and six syntaxbased ones (GHKM). For English→German, our
system combination parameters are optimized on
newstest2011. Combining all nine different system outputs yields an improvement of 0.5 points
in BLEU and 1.7 points in TER over the best single system performance.
In Table 4 the cross BLEU scores for all
English→German systems are given. The individual system of KIT and the syntax-based ParZu system of UEDIN have the lowest BLEU score when
scored against each other. Both approaches are
quite different and both are coming from different institutes. In contrast, both phrase-based systems pbt 1 and pbt 2 from UEDIN are very similar and hence have a high pairwise BLEU score.
Table 4: Cross BLEU scores for the German→English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as reference and the other one as hypothesis.)
amount sentences
distribution
German→English newstest2013 test set comparing system combination output against the best
individual system.
As for the German→English translation direction,
the best performing individual system outputs are
also having the highest BLEU scores when evaluated against the ﬁnal system combination output.
In Figure 2 system combination output is compared to the best single system pbt 2. We distribute
the sentence-level BLEU scores of all sentences
of newstest2013. Many sentences have been improved by system combination. But there is still
room for improvement as some sentences are still
better in terms of sentence-level BLEU in the individual best system pbt 2.
Conclusion
We achieved signiﬁcantly better translation performance with gains of up to +1.6 points in BLEU
and -1.0 points in TER by combining up to nine
different machine translation systems. Three different research institutes (RWTH Aachen University, University of Edinburgh, Karlsruhe Institute
of Technology) provided machine translation engines based on different approaches like phrase-
amount sentences
distribution
English→German newstest2013 test set comparing system combination output against the best
individual system.
based, hierarchical phrase-based, and syntaxbased.
For English→German, we included six
different syntax-based systems, which were combined to our ﬁnal combined translation. The automatic scores of all submitted system outputs for
the actual 2014 evaluation set are presented on the
WMT submission page.2 Our joint submission is
the best submission in terms of BLEU and TER for
both translation directions German→English and
English→German without adding any new data.
Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Framework Programme under grant
agreement no 287658.
Rico Sennrich has received funding from the
Swiss National Science Foundation under grant
P2ZHP1 148717.
2