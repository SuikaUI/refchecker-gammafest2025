Proceedings of the 9th International Workshop on Semantic Evaluation , pages 451–463,
Denver, Colorado, June 4-5, 2015. c⃝2015 Association for Computational Linguistics
SemEval-2015 Task 10: Sentiment Analysis in Twitter
Sara Rosenthal
Columbia University
 
Saif M Mohammad
National Research Council Canada
 
Preslav Nakov
Qatar Computing Research Institute
 
Alan Ritter
The Ohio State University
 
Svetlana Kiritchenko
National Research Council Canada
 
Veselin Stoyanov
 
In this paper, we describe the 2015 iteration of
the SemEval shared task on Sentiment Analysis in Twitter. This was the most popular sentiment analysis shared task to date with more
than 40 teams participating in each of the last
three years. This year’s shared task competition consisted of ﬁve sentiment prediction subtasks. Two were reruns from previous years:
(A) sentiment expressed by a phrase in the
context of a tweet, and (B) overall sentiment
of a tweet. We further included three new subtasks asking to predict (C) the sentiment towards a topic in a single tweet, (D) the overall sentiment towards a topic in a set of tweets,
and (E) the degree of prior polarity of a phrase.
Introduction
Social media such as Weblogs, microblogs, and discussion forums are used daily to express personal
thoughts, which allows researchers to gain valuable
insight into the opinions of a very large number of
individuals, i.e., at a scale that was simply not possible a few years ago. As a result, nowadays, sentiment analysis is commonly used to study the public opinion towards persons, objects, and events. In
particular, opinion mining and opinion detection are
applied to product reviews , for
agreement detection , and even
for sarcasm identiﬁcation .
Early work on detecting sentiment focused on
newswire text . As later
research turned towards social media, people realized this presented a number of new challenges.
Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these
new media, and were explored by a number of researchers . Later, specialized shared
tasks emerged, e.g., at SemEval , which compared teams
against each other in a controlled environment using the same training and testing datasets. These
shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon
 .1
Below, we discuss the public evaluation done as
part of SemEval-2015 Task 10. In its third year, the
SemEval task on Sentiment Analysis in Twitter has
once again attracted a large number of participants:
41 teams across ﬁve subtasks, with most teams participating in more than one subtask.
This year the task included reruns of two legacy
subtasks, which asked to detect the sentiment expressed in a tweet or by a particular phrase in a
tweet. The task further added three new subtasks.
The ﬁrst two focused on the sentiment towards a
given topic in a single tweet or in a set of tweets,
respectively. The third new subtask focused on determining the strength of prior association of Twitter terms with positive sentiment; this acts as an intrinsic evaluation of automatic methods that build
Twitter-speciﬁc sentiment lexicons with real-valued
sentiment association scores.
1 
In the remainder of this paper, we ﬁrst introduce
the problem of sentiment polarity classiﬁcation and
our subtasks. We then describe the process of creating the training, development, and testing datasets.
We list and brieﬂy describe the participating systems, the results, and the lessons learned. Finally,
we compare the task to other related efforts and we
point to possible directions for future research.
Task Description
Below, we describe the ﬁve subtasks of SemEval-
2015 Task 10 on Sentiment Analysis in Twitter.
• Subtask A. Contextual Polarity Disambiguation: Given an instance of a word/phrase in the
context of a message, determine whether it expresses a positive, a negative or a neutral sentiment in that context.
• Subtask B. Message Polarity Classiﬁcation:
Given a message, determine whether it expresses
a positive, a negative, or a neutral/objective sentiment. If both positive and negative sentiment are
expressed, the stronger one should be chosen.
• Subtask C. Topic-Based Message Polarity
Classiﬁcation: Given a message and a topic, decide whether the message expresses a positive, a
negative, or a neutral sentiment towards the topic.
If both positive and negative sentiment are expressed, the stronger one should be chosen.
• Subtask D. Detecting Trend Towards a Topic:
Given a set of messages on a given topic from
the same period of time, classify the overall sentiment towards the topic in these messages as
(a) strongly positive, (b) weakly positive, (c) neutral, (d) weakly negative, or (e) strongly negative.
• Subtask E. Determining Strength of Association of Twitter Terms with Positive Sentiment
(Degree of Prior Polarity): Given a word/phrase,
propose a score between 0 (lowest) and 1 (highest) that is indicative of the strength of association
of that word/phrase with positive sentiment. If a
word/phrase is more positive than another one, it
should be assigned a relatively higher score.
In this section, we describe the process of collecting and annotating our datasets of short social media text messages. We focus our discussion on the
2015 datasets; more detail about the 2013 and the
2014 datasets can be found in 
and .
Data Collection
Subtasks A–D
First, we gathered tweets that express sentiment
about popular topics.
For this purpose, we extracted named entities from millions of tweets, using a Twitter-tuned NER system .
Our initial training set was collected over a one-year
period spanning from January 2012 to January 2013.
Each subsequent Twitter test set was collected a few
months prior to the corresponding evaluation. We
used the public streaming Twitter API to download
the tweets.
We then identiﬁed popular topics as those named
entities that are frequently mentioned in association
with a speciﬁc date . Given this
set of automatically identiﬁed topics, we gathered
tweets from the same time period which mentioned
the named entities. The testing messages had different topics from training and spanned later periods.
The collected tweets were greatly skewed towards
the neutral class. In order to reduce the class imbalance, we removed messages that contained no
sentiment-bearing words using SentiWordNet as a
repository of sentiment words. Any word listed in
SentiWordNet 3.0 with at least one sense having a
positive or a negative sentiment score greater than
0.3 was considered a sentiment-bearing word.2
For subtasks C and D, we did some manual pruning based on the topics.
First, we excluded topics that were incomprehensible, ambiguous (e.g.,
Barcelona, which is a name of a sports team and also
of a place), or were too general (e.g., Paris, which is
a name of a big city). Second, we discarded tweets
that were just mentioning the topic, but were not really about the topic. Finally, we discarded topics
with too few tweets, namely less than 10.
2Filtering based on an existing lexicon does bias the dataset
to some degree; however, note that the text still contains sentiment expressions outside those in the lexicon.
Instructions: Subjective words are ones which convey an opinion or sentiment. Given a Twitter message, identify
whether it is objective, positive, negative, or neutral. Then, identify each subjective word or phrase in the context of the
sentence and mark the position of its start and end in the text boxes below. The number above each word indicates its
position. The word/phrase will be generated in the adjacent textbox so that you can conﬁrm that you chose the correct
range. Choose the polarity of the word or phrase by selecting one of the radio buttons: positive, negative, or neutral.
If a sentence is not subjective please select the checkbox indicating that “There are no subjective words/phrases”. If
a tweet is sarcastic, please select the checkbox indicating that “The tweet is sarcastic”. Please read the examples and
invalid responses before beginning if this is your ﬁrst time answering this hit.
Figure 1: The instructions we gave to the workers on Mechanical Turk, followed by a screenshot.
We selected high-frequency target terms from the
Sentiment140 and the Hashtag Sentiment tweet corpora .
In order to reduce the skewness towards the neutral class, we
selected terms from different ranges of automatically determined sentiment values as provided by
the corresponding Sentiment140 and Hashtag Sentiment lexicons. The term set comprised regular English words, hashtagged words (e.g., #loveumom),
misspelled or creatively spelled words (e.g., parlament or happeeee), abbreviations, shortenings, and
slang. Some terms were negated expressions such
as no fun. .) We annotated these terms for degree of sentiment manually. Further details about the data collection and the annotation process can be found in
Section 3.2.2 as well as in .
The trial dataset consisted of 200 instances, and
no training dataset was provided. Note, however,
that the trial data was large enough to be used as a
development set, or even as a training set. Moreover, the participants were free to use any additional
manually or automatically generated resources when
building their systems for subtask E. The testset included 1,315 instances.
Annotation
Below we describe the data annotation process.
Subtasks A–D
We used Amazon’s Mechanical Turk for the annotations of subtasks A–D. Each tweet message was
annotated by ﬁve Mechanical Turk workers, also
known as Turkers.
The annotations for subtasks
A–D were done concurrently, in a single task. A
Turker had to mark all the subjective words/phrases
in the tweet message by indicating their start and
end positions and to say whether each subjective
word/phrase was positive, negative, or neutral (subtask A). He/she also had to indicate the overall polarity of the tweet message in general (subtask B)
as well as the overall polarity of the message towards the given target topic (subtasks C and D). The
instructions we gave to the Turkers, along with an
example, are shown in Figure 1. We further made
available to the Turkers several additional examples,
which we show in Table 1.
Providing all the required annotations for a given
tweet message constituted a Human Intelligence
Task, or a HIT. In order to qualify to work on our
HITs, a Turker had to have an approval rate greater
than 95% and should have completed at least 50 approved HITs.
Authorities are only too aware that Kashgar is 4,000 kilometres (2,500 miles) from Beijing but only a tenth of
the distance from the Pakistani border, and are desperate to ensure instability or militancy does not leak over the
frontiers.
Taiwan-made products stood a good chance of becoming even more competitive thanks to wider access to overseas
markets and lower costs for material imports, he said.
“March appears to be a more reasonable estimate while earlier admission cannot be entirely ruled out,” according
to Chen, also Taiwan’s chief WTO negotiator.
friday evening plans were great, but saturday’s plans didnt go as expected – i went dancing & it was an ok club,
but terribly crowded :-(
WHY THE HELL DO YOU GUYS ALL HAVE MRS. KENNEDY! SHES A FUCKING DOUCHE
AT&T was okay but whenever they do something nice in the name of customer service it seems like a favor, while
T-Mobile makes that a normal everyday thin
obama should be impeached on TREASON charges. Our Nuclear arsenal was TOP Secret. Till HE told our enemies
what we had. #Coward #Traitor
My graduation speech: “I’d like to thanks Google, Wikipedia and my computer!” :D #iThingteens
Table 1: List of example sentences and annotations we provided to the Turkers. All subjective phrases are italicized
and color-coded: positive phrases are in green, negative ones are in red, and neutral ones are in blue.
I would love to watch Vampire Diaries :) and some Heroes! Great combination
I would love to watch Vampire Diaries :) and some Heroes! Great combination
I would love to watch Vampire Diaries :) and some Heroes! Great combination
I would love to watch Vampire Diaries :) and some Heroes! Great combination
I would love to watch Vampire Diaries :) and some Heroes! Great combination
I would love to watch Vampire Diaries :) and some Heroes! Great combination
Table 2: Example of a sentence annotated for subjectivity on Mechanical Turk. Words and phrases that were marked
as subjective are in bold italic. The ﬁrst ﬁve rows are annotations provided by Turkers, and the ﬁnal row shows their
intersection. The last column shows the token-level accuracy for each annotation compared to the intersection.
We further discarded the following types of message annotations:
• containing overlapping subjective phrases;
• marked as subjective but having no annotated
subjective phrases;
• with every single word marked as subjective;
• with no overall sentiment marked;
• with no topic sentiment marked.
Recall that each tweet message was annotated by
ﬁve different Turkers. We consolidated these annotations for subtask A using intersection as shown in
the last row of Table 2. A word had to appear in 3/5
of the annotations in order to be considered subjective. It further had to be labeled with a particular
polarity (positive, negative, or neutral) by three of
the ﬁve Turkers in order to receive that polarity label. As the example shows, this effectively shortens
the spans of the annotated phrases, often to single
words, as it is hard to agree on long phrases.
Twitter2013-train
Twitter2013-dev
Twitter2013-test
SMS2013-test
Twitter2014-test
Twitter2014-sarcasm
LiveJournal2014-test
Twitter2015-test
Table 3: Dataset statistics for subtask A.
We also experimented with two alternative methods for combining annotations: (i) by computing
the union of the annotations for the sentence, and
(ii) by taking the annotations by the Turker who has
annotated the highest number of HITs. However,
our manual analysis has shown that both alternatives
performed worse than using the intersection.
Twitter2013-train
Twitter2013-dev
Twitter2013-test
SMS2013-test
Twitter2014-test
Twitter2014-sarcasm
LiveJournal2014-test
Twitter2015-test
Table 4: Dataset statistics for subtask B.
Table 5: Twitter-2015 statistics for subtasks C & D.
For subtasks B and C, we consolidated the tweetlevel annotations using majority voting, requiring
that the winning label be proposed by at least three
of the ﬁve Turkers; we discarded all tweets for which
3/5 majority could not be achieved. As in previous
years, we combined the objective and the neutral labels, which Turkers tended to mix up.
We used these consolidated annotations as gold
labels for subtasks A, B, C & D. The statistics for all
datasets for these subtasks are shown in Tables 3, 4,
and 5, respectively. Each dataset is marked with the
year of the SemEval edition it was produced for. An
annotated example from each source (Twitter, SMS,
LiveJournal) is shown in Table 6; examples for sentiment towards a topic can be seen in Table 7.
Subtask E asks systems to propose a numerical
score for the positiveness of a given word or phrase.
Many studies have shown that people are actually
quite bad at assigning such absolute scores: interannotator agreement is low, and annotators struggle even to remain self-consistent.
In contrast, it
is much easier to make relative judgments, e.g., to
say whether one word is more positive than another.
Moreover, it is possible to derive an absolute score
from pairwise judgments, but this requires a much
larger number of annotations. Fortunately, there are
schemes that allow to infer more pairwise annotations from less judgments.
One such annotation scheme is MaxDiff , which is widely used in market surveys
 ; it was also used in a previous SemEval task .
In MaxDiff, the annotator is presented with four
terms and asked which term is most positive and
which is least positive. By answering just these two
questions, ﬁve out of six pairwise rankings become
known. Consider a set in which a judge evaluates A,
B, C, and D. If she says that A and D are the most
and the least positive, we can infer the following:
A > B, A > C, A > D, B > D, C > D. The responses to the MaxDiff questions can then be easily
translated into a ranking for all the terms and also
into a real-valued score for each term. We crowdsourced the MaxDiff questions on CrowdFlower, recruiting ten annotators per MaxDiff example. Further details can be found in Section 6.1.2. of .
Lower & Upper Bounds
When building a system to solve a task, it is good
to know how well we should expect it to perform.
One good reference point is agreement between annotators. Unfortunately, as we derive annotations by
agreement, we cannot calculate standard statistics
such as Kappa. Instead, we decided to measure the
agreement between our gold standard annotations
(derived by agreement) and the annotations proposed by the best Turker, the worst Turker, and the
average Turker (with respect to the gold/consensus
annotation for a particular message). Given a HIT,
we just calculate the overlaps as shown in the last
column in Table 2, and then we calculate the best,
the worst, and the average, which are respectively
13/13, 9/13 and 11/13, in the example. Finally, we
average these statistics over all HITs that contributed
to a given dataset, to produce lower, average, and
upper averages for that dataset. The accuracy (with
respect to the gold/consensus annotation) for different averages is shown in Table 8. Since the overall
polarity of a message is chosen based on majority,
the upper bound for subtask B is 100%. These averages give a good indication about how well we can
expect the systems to perform. We can see that even
if we used the best annotator for each HIT, it would
still not be possible to get perfect accuracy, and thus
we should also not expect it from a system.
Message-Level
Why would you [still]- wear shorts when it’s this cold?! I [love]+ how Britain
see’s a bit of sun and they’re [like ’OOOH]+ LET’S STRIP!’
[Sorry]- I think tonight [cannot]- and I [not feeling well]- after my rest.
LiveJournal
[Cool]+ posts , dude ; very [colorful]+ , and [artsy]+ .
Twitter Sarcasm
[Thanks]+ manager for putting me on the schedule for Sunday
Table 6: Example annotations for each source of messages. The subjective phrases are marked in [. . .], and are
followed by their polarity (subtask A); the message-level polarity is shown in the last column (subtask B).
Message-Level
Topic-Level
leeds united
Saturday without Leeds United is like Sunday dinner it doesn’t
feel normal at all (Ryan)
demi lovato
Who are you tomorrow? Will you make me smile or just bring
me sorrow? #HottieOfTheWeek Demi Lovato
Table 7: Example of annotations in Twitter showing differences between topic- and message-level polarity.
Twitter2013-train
Twitter2013-dev
Twitter2013-test
SMS2013-test
Livejournal2014-test
Twitter2014-test
Sarcasm2014-test
Twitter2015-test
Table 8: Average (over all HITs) overlap of the gold annotations with the worst, average, and the worst Turker
for each HIT, for subtasks A and B.
Tweets Delivery
Due to restrictions in the Twitter’s terms of service,
we could not deliver the annotated tweets to the participants directly. Instead, we released annotation
indexes and labels, a list of corresponding Twitter
IDs, and a download script that extracts the corresponding tweets via the Twitter API.3
As a result, different teams had access to different number of training tweets depending on when
they did the downloading. However, our analysis
has shown that this did not have a major impact and
many high-scoring teams had less training data compared to some lower-scoring ones.
3 
Subtasks A-C: Phrase-Level,
Message-Level, and Topic-Level Polarity
The participating systems were required to perform
a three-way classiﬁcation, i.e., to assign one of the
folowing three labels: positive, negative or objective/neutral. We evaluated the systems in terms of a
macro-averaged F1 score for predicting positive and
negative phrases/messages.
We ﬁrst computed positive precision, Ppos as follows: we found the number of phrases/messages
that a system correctly predicted to be positive,
and we divided that number by the total number
of examples it predicted to be positive.
To compute positive recall, Rpos, we found the number of
phrases/messages correctly predicted to be positive
and we divided that number by the total number
of positives in the gold standard. We then calculated an F1 score for the positive class as follows
Fpos = 2PposRpos
Ppos+Rpos . We carried out similar computations for the negative phrases/messages, Fneg. The
overall score was then computed as the average of
the F1scores for the positive and for the negative
classes: F = (Fpos + Fneg)/2.
We provided the participants with a scorer that
outputs the overall score F, as well as P, R, and
F1 scores for each class (positive, negative, neutral)
and for each test set.
Subtask D: Overall Polarity Towards a
This subtask asks to predict the overall sentiment of
a set of tweets towards a given topic. In other words,
to predict the ratio ri of positive (posi) tweets to the
number of positive and negative sentiment tweets in
the set of tweets about the i-th topic:
ri = Posi/(Posi + Negi)
Note, that neutral tweets do not participate in the
above formula; they have only an indirect impact on
the calculation, similarly to subtasks A–C.
We use the following two evaluation measures for
subtask D:
• AvgDiff (ofﬁcial score): Calculates the absolute difference betweeen the predicted r′
the gold ri for each i, and then averages this
difference over all topics.
• AvgLevelDiff (unofﬁcial score): This calculation is the same as AvgDiff, but with r′
ri ﬁrst remapped to ﬁve coarse numerical categories: 5 (strongly positive), 4 (weakly positive), 3 (mixed), 2 (weakly negative), and 1
(strongly negative). We deﬁne this remapping
based on intervals as follows:
– 5: 0.8 < x ≤1.0
– 4: 0.6 < x ≤0.8
– 3: 0.4 < x ≤0.6
– 2: 0.2 < x ≤0.4
– 1: 0.0 ≤x ≤0.2
Subtask E: Degree of Prior Polarity
The scores proposed by the participating systems
were evaluated by ﬁrst ranking the terms according to the proposed sentiment score and then comparing this ranked list to a ranked list obtained
from aggregating the human ranking annotations.
We used Kendall’s rank correlation (Kendall’s τ)
as the ofﬁcial evaluation metric to compare the
ranked lists .
We also calculated
scores for Spearman’s rank correlation , as an unofﬁcial score.
Afﬁliation
CIS-positiv
University of Munich
CLaC-SentiPipe
CLaC Labs, Concordia University
Arizona State University
East China Normal University
Universitat Polit`ecnica de Val`encia
Gradiant-Analytics
AtlantTIC Center, University of Vigo
iitpsemeval
Indian Institute of Technology, Patna
IIIT, Hyderabad
IST, INESC-ID
Institute of Acoustics, Chinese Academy of Sciences
FAU Erlangen-N¨urnberg
Aix-Marseille University
RGUSentimentMiners123
Robert Gordon University
The University of Melbourne
IIIT, Hyderabad
Nanyang Technological University, Singapore
Fondazione Bruno Kessler
Peking University
Beihang University
Swarthmore College
Swarthmore College
Swarthmore College
Swarthmore College
Swiss-Chocolate
Zurich University of Applied Sciences
TwitterHawk
University of Massachusetts, Lowell
Universidad de las Am`ericas Puebla, Mexico
University of International Relations
UMDuluth-CS8761
University of Minnesota, Duluth
University of Bari Aldo Moro
University of Trento
Universitat Pompeu Fabra
WarwickDCS
University of Warwick
Bauhaus-Universit¨at Weimar
International Software School, Wuhan University
Computer School, Wuhan University
Hong Kong University of Science and Technology
Peking University
Table 9: The participating teams and their afﬁliations.
Participants and Results
The task attracted 41 teams: 11 teams participated in
subtask A, 40 in subtask B, 7 in subtask C, 6 in subtask D, and 10 in subtask E. The IDs and afﬁliations
of the participating teams are shown in Table 9.
Subtask A: Phrase-Level Polarity
The results (macro-averaged F1 score) for subtask A are shown in Table 10.
The ofﬁcial
results on the new Twitter2015-test dataset are
shown in the last column, while the ﬁrst ﬁve
columns show F1 on the 2013 and on the 2014
progress test datasets:4 Twitter2013-test, SMS2013test, Twitter2014-test, Twitter2014-sarcasm, and
LiveJournal2014-test. There is an index for each result showing the relative rank of that result within
the respective column.
The participating systems
are ranked by their score on the Twitter2015-test
dataset, which is the ofﬁcial ranking for subtask A;
all remaining rankings are secondary.
4Note that the 2013 and the 2014 test datasets were made
available for development, but it was explicitly forbidden to use
them for training.
2013: Progress
2014: Progress
2015: Ofﬁcial
WarwickDCS
TwitterHawk
iitpsemeval
UMDuluth-CS8761
Table 10: Results for subtask A: Phrase-Level Polarity. The systems are ordered by their score on the Twitter2015
test dataset; the rankings on the individual datasets are indicated with a subscript.
There were less participants this year, probably
due to having a new similar subtask: C. Notably,
many of the participating teams were newcomers.
We can see that all systems beat the majority
class baseline by 25-40 F1 points absolute on all
datasets. The winning team unitn (using deep convolutional neural networks) achieved an F1 of 84.79
on Twitter2015-test, followed closely by KLUEless
(using logistic regression) with F1=84.51.
Looking at the progress datasets, we can see that
unitn was also ﬁrst on both progress Tweet datasets,
and second on SMS and on LiveJournal. KLUEless won SMS and was second on Twitter2013-test.
The best result on LiveJournal was achieved by IOA,
who were also second on Twitter2014-test and third
on the ofﬁcial Twitter2015-test. None of these teams
was ranked in top-3 on Twitter2014-sarcasm, where
the best team was GTI, followed by WarwickDCS.
Compared to 2014, there is an improvement on
Twitter2014-test from 86.63 in 2014 (NRC-Canada)
to 87.12 in 2015 (unitn).
The best result on
Twitter2013-test of 90.10 (unitn) this year is very
close to the best in 2014 (90.14 by NRC-Canada).
Similarly, the best result on LiveJournal stays exactly the same, i.e., F1=85.61 . However, there is slight degradation for SMS2013-test from 89.31 (ECNU) in
2014 to 88.62 (KLUEless) in 2015.
The results
also degraded for Twitter2014-sarcasm from 82.75
(senti.ue) to 81.53 (GTI).
Subtask B: Message-Level Polarity
The results for subtask B are shown in Table 11.
Again, we show results on the ﬁve progress test
datasets from 2013 and 2014, in addition to those
for the ofﬁcial Twitter2015-test datasets.
Subtask B attracted 40 teams, both newcomers
and returning, similarly to 2013 and 2014.
managed to beat the baseline with the exception
of one system for Twitter2015-test, and one for
Twitter2014-test. There is a cluster of four teams
at the top: Webis (ensemble combining four Twitter sentiment classiﬁcation approaches that participated in previous editions) with an F1 of 64.84, unitn
with 64.59, lsislif (logistic regression with special
weighting for positives and negatives) with 64.27,
and INESC-ID (word embeddings) with 64.17.
The last column in the table shows the results for
the 2015 sarcastic tweets. Note that, unlike in 2014,
this time they were not collected separately and did
not have a special #sarcasm tag; instead, they are a
subset of 75 tweets from Twitter2015-test that were
ﬂagged as sarcastic by the human annotators. The
top system is IOA with an F1 of 65.77, followed by
INESC-ID with 64.91, and NLP with 63.62.
Looking at the progress datasets, we can see that
the second ranked unitn is also second on SMS and
on Twitter2014-test, and third on Twitter2013-test.
INESC-ID in turn is third on Twitter2014-test and
also third on Twitter2014-sarcasm. Webis and lsislif
were less strong on the progress datasets.
2013: Progress
2014: Progress
2015: Ofﬁcial
Swiss-Chocolate
CLaC-SentiPipe
TwitterHawk
Gradiant-Analytics
CIS-positiv
iitpsemeval
WarwickDCS
RGUSentimentMiners123
UMDuluth-CS8761
Table 11: Results for subtask B: Message-Level Polarity. The systems are ordered by their score on the Twitter2015
test dataset; the rankings on the individual datasets are indicated with a subscript. Systems with late submissions for
the progress test datasets are marked with a ⋆.
Compared to 2014, there is improvement on
Twitter2013-test from 72.12 (TeamX) to 72.80
(Splusplus),
Twitter2014-test
(TeamX) to 74.42 (Spluplus), on Twitter2014sarcasm
(NRC-Canada)
(Gradiant-Analytics),
and on LiveJournal from
74.84 (NRC-Canada) to 75.34 (Splusplus), but not
on SMS: 70.28 (NRC-Canada) vs. 68.49 (ECNU).
TwitterHawk
WarwickDCS
UMDuluth-CS8761
Table 12: Results for Subtask C: Topic-Level Polarity.
The systems are ordered by the ofﬁcial 2015 score.
avgLevelDiff
TwitterHawk
UMDuluth-CS8761
Table 13: Results for Subtask D: Trend Towards a
Topic. The systems are sorted by the ofﬁcial 2015 score.
Subtask C: Topic-Level Polarity
The results for subtask C are shown in Table 12.
This proved to be a hard subtask, and only three
of the seven teams that participated in it managed
to improve over a majority vote baseline.
three teams, TwitterHawk (using subtask B data
to help with subtask C) with F1=50.51, KLUEless
(which ignored the topics as if it was subtask B) with
F1=45.48, and Whu-Nlp with F1=40.70, achieved
scores that outperform the rest by a sizable margin:
15-25 points absolute more than the fourth team.
Note that, despite the apparent similarity, subtask
C is much harder than subtask B: the top-3 teams
achieved an F1 of 64-65 for subtask B vs. an F1 of
41-51 for subtask C. This cannot be blamed on the
class distribution, as the difference in performance
of the majority class baseline is much smaller: 30.3
for B vs. 26.7 for C.
Finally, the last column in the table reports the
results for the 75 sarcastic 2015 tweets. The winner here is KLUEless with an F1 of 39.26, followed by TwitterHawk with F1=31.30, and then by
UMDuluth-CS8761 with F1=29.91.
Subtask D: Trend Towards a Topic
The results for subtask D are shown in Table 13.
This subtask is closely related to subtask C (in fact,
one obvious way to solve D is to solve C and then
to calculate the proportion), and thus it has attracted
the same teams, except for one. Again, only three
of the participating teams managed to improve over
the baseline; not suprisingly, those were the same
three teams that were in top-3 for subtask C. However, the ranking is different from that in subtask
C, e.g., TwitterHawk has dropped to third position,
while KLUEless and Why-Nlp have each climbed
one position up to positions 1 and 2, respectively.
note that avgDiff and avgLevelDiff
yielded the same rankings.
Subtask E: Degree of Prior Polarity
Ten teams participated in subtask E. Many chose
an unsupervised approach and leveraged newlycreated and pre-existing sentiment lexicons such as
the Hashtag Sentiment Lexicon, the Sentiment140
Lexicon , the MPQA Subjectivity Lexicon , and SentiWordNet , among others. Several participants further automatically created their own sentiment lexicons from large collections of tweets. Three teams, including the winner
INESC-ID, adopted a supervised approach and used
word embeddings (supplemented with lexicon features) to train a regression model.
The results are presented in Table 14. The last row
shows the performance of a lexicon-based baseline.
For this baseline, we chose the two most frequently
used existing, publicly available, and automatically
generated sentiment lexicons: Hashtag Sentiment
Lexicon and Sentiment140 Lexicon .5 These lexicons have real-valued sentiment scores for most of the terms in the test set.
For negated phrases, we use the scores of the corresponding negated entries in the lexicons. For each
term, we take its score from the Sentiment140 Lexicon if present; otherwise, we take the term’s score
from the Hashtag Sentiment Lexicon. For terms not
found in any lexicon, we use the score of 0, which
indicates a neutral term in these lexicons. The top
three teams were able to improve over the baseline.
5 
Kendall’s τ
Spearman’s ρ
coefﬁcient
coefﬁcient
CLaC-SentiPipe
UMDuluth-CS8761-10
IHS-RD-Belarus
iitpsemeval
RGUSentminers123
Table 14: Results for Subtask E: Degree of Prior Polarity.
The systems are ordered by their Kendall’s τ
score, which was the ofﬁcial score.
Discussion
As in the previous two years, almost all systems used
supervised learning. Popular machine learning approaches included SVM, maximum entropy, CRFs,
and linear regression. In several of the subtasks, the
top system used deep neural networks and word embeddings, and some systems beneﬁted from special
weighting of the positive and negative examples.
Once again, the most important features were
those derived from sentiment lexicons. Other important features included bag-of-words features, hashtags, handling of negation, word shape and punctuation features, elongated words, etc. Moreover, tweet
pre-processing and normalization were an important
part of the processing pipeline.
Note that this year we did not make a distinction between constrained and unconstrained systems, and participants were free to use any additional data, resources and tools they wished to.
Overall, the task has attracted a total of 41 teams,
which is comparable to previous editions: there were
46 teams in 2014, and 44 in 2013. As in previous
years, subtask B was most popular, attracting almost
all teams (40 out of 41). However, subtask A attracted just a quarter of the participants (11 out of
41), compared to about half in previous years, most
likely due to the introduction of two new, very related subtasks C and D (with 6 and 7 participants,
respectively).
There was also a ﬁfth subtask (E,
with 10 participants), which further contributed to
the participant split.
We should further note that our task was part of
a larger Sentiment Track, together with three other
closely-related tasks, which were also interested in
sentiment analysis: Task 9 on CLIPEval Implicit Polarity of Events, Task 11 on Sentiment Analysis of
Figurative Language in Twitter, and Task 12 on Aspect Based Sentiment Analysis. Another related task
was Task 1 on Paraphrase and Semantic Similarity in
Twitter, from the Text Similarity and Question Answering track, which also focused on tweets.
Conclusion
We have described the ﬁve subtasks organized as
part of SemEval-2015 Task 10 on Sentiment Analysis in Twitter: detecting sentiment of terms in context (subtask A), classiﬁying the sentiment of an
entire tweet, SMS message or blog post (subtask
B), predicting polarity towards a topic (subtask C),
quantifying polarity towards a topic (subtask D),
and proposing real-valued prior sentiment scores for
Twitter terms (subtask E). Over 40 teams participated in these subtasks, using various techniques.
We plan a new edition of the task as part of
SemEval-2016, where we will focus on sentiment
with respect to a topic, but this time on a ﬁvepoint scale, which is used for human review ratings
on popular websites such as Amazon, TripAdvisor,
Yelp, etc. From a research perspective, moving to an
ordered ﬁve-point scale means moving from binary
classiﬁcation to ordinal regression.
We further plan to continue the trend detection
subtask, which represents a move from classiﬁcation
to quantiﬁcation, and is on par with what applications need. They are not interested in the sentiment
of a particular tweet but rather in the percentage of
tweets that are positive/negative.
Finally, we plan a new subtask on trend detection,
but using a ﬁve-point scale, which would get us even
closer to what business (e.g. marketing studies), and
researchers, (e.g. in political science or public policy), want nowadays. From a research perspective,
this is a problem of ordinal quantiﬁcation.
Acknowledgements
The authors would like to thank SIGLEX for supporting subtasks A–D, and the National Research
Council Canada for funding subtask E.