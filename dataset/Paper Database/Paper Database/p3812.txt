Realtime Multi-Person 2D Pose Estimation using Part Afﬁnity Fields ∗
Tomas Simon
Shih-En Wei
Yaser Sheikh
The Robotics Institute, Carnegie Mellon University
{zhecao,shihenw}@cmu.edu
{tsimon,yaser}@cs.cmu.edu
We present an approach to efﬁciently detect the 2D pose
of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Afﬁnity
Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance,
irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and
their association via two branches of the same sequential
prediction process. Our method placed ﬁrst in the inaugural COCO 2016 keypoints challenge, and signiﬁcantly exceeds the previous state-of-the-art result on the MPII Multi-
Person benchmark, both in performance and efﬁciency.
1. Introduction
Human 2D pose estimation—the problem of localizing
anatomical keypoints or “parts”—has largely focused on
ﬁnding body parts of individuals . Inferring the pose of multiple people in images, especially socially engaged individuals, presents a unique set
of challenges. First, each image may contain an unknown
number of people that can occur at any position or scale.
Second, interactions between people induce complex spatial interference, due to contact, occlusion, and limb articulations, making association of parts difﬁcult. Third, runtime
complexity tends to grow with the number of people in the
image, making realtime performance a challenge.
A common approach is to employ
a person detector and perform single-person pose estimation for each detection.
These top-down approaches directly leverage existing techniques for single-person pose
estimation , but suffer
from early commitment: if the person detector fails–as it
is prone to do when people are in close proximity–there is
no recourse to recovery. Furthermore, the runtime of these
∗Video result: 
Figure 1. Top: Multi-person pose estimation. Body parts belonging to the same person are linked. Bottom left: Part Afﬁnity Fields
(PAFs) corresponding to the limb connecting right elbow and right
wrist. The color encodes orientation. Bottom right: A zoomed in
view of the predicted PAFs. At each pixel in the ﬁeld, a 2D vector
encodes the position and orientation of the limbs.
top-down approaches is proportional to the number of people: for each detection, a single-person pose estimator is
run, and the more people there are, the greater the computational cost. In contrast, bottom-up approaches are attractive
as they offer robustness to early commitment and have the
potential to decouple runtime complexity from the number
of people in the image. Yet, bottom-up approaches do not
directly use global contextual cues from other body parts
and other people. In practice, previous bottom-up methods do not retain the gains in efﬁciency as the ﬁnal parse requires costly global inference. For example, the
seminal work of Pishchulin et al. proposed a bottom-up
approach that jointly labeled part detection candidates and
associated them to individual people. However, solving the
integer linear programming problem over a fully connected
graph is an NP-hard problem and the average processing
time is on the order of hours. Insafutdinov et al. built
on with stronger part detectors based on ResNet 
and image-dependent pairwise scores, and vastly improved
the runtime, but the method still takes several minutes per
image, with a limit on the number of part proposals. The
pairwise representations used in , are difﬁcult to regress
precisely and thus a separate logistic regression is required.
 
(b) Part Confidence Maps
(c) Part Affinity Fields
(a) Input Image
(d) Bipartite Matching
(e) Parsing Results
Figure 2. Overall pipeline. Our method takes the entire image as the input for a two-branch CNN to jointly predict conﬁdence maps for
body part detection, shown in (b), and part afﬁnity ﬁelds for parts association, shown in (c). The parsing step performs a set of bipartite
matchings to associate body parts candidates (d). We ﬁnally assemble them into full body poses for all people in the image (e).
In this paper, we present an efﬁcient method for multiperson pose estimation with state-of-the-art accuracy on
multiple public benchmarks. We present the ﬁrst bottom-up
representation of association scores via Part Afﬁnity Fields
(PAFs), a set of 2D vector ﬁelds that encode the location
and orientation of limbs over the image domain. We demonstrate that simultaneously inferring these bottom-up representations of detection and association encode global context sufﬁciently well to allow a greedy parse to achieve
high-quality results, at a fraction of the computational cost.
We have publically released the code for full reproducibility, presenting the ﬁrst realtime system for multi-person 2D
pose detection.
Fig. 2 illustrates the overall pipeline of our method. The
system takes, as input, a color image of size w × h (Fig. 2a)
and produces, as output, the 2D locations of anatomical keypoints for each person in the image (Fig. 2e). First, a feedforward network simultaneously predicts a set of 2D con-
ﬁdence maps S of body part locations (Fig. 2b) and a set
of 2D vector ﬁelds L of part afﬁnities, which encode the
degree of association between parts (Fig. 2c). The set S =
(S1, S2, ..., SJ) has J conﬁdence maps, one per part, where
Sj ∈Rw×h, j ∈{1 . . . J}. The set L = (L1, L2, ..., LC)
has C vector ﬁelds, one per limb1, where Lc ∈Rw×h×2,
c ∈{1 . . . C}, each image location in Lc encodes a 2D vector (as shown in Fig. 1). Finally, the conﬁdence maps and
the afﬁnity ﬁelds are parsed by greedy inference (Fig. 2d)
to output the 2D keypoints for all people in the image.
2.1. Simultaneous Detection and Association
Our architecture, shown in Fig. 3, simultaneously predicts detection conﬁdence maps and afﬁnity ﬁelds that encode part-to-part association. The network is split into two
branches: the top branch, shown in beige, predicts the con-
ﬁdence maps, and the bottom branch, shown in blue, predicts the afﬁnity ﬁelds. Each branch is an iterative predic-
1We refer to part pairs as limbs for clarity, despite the fact that some
pairs are not human limbs (e.g., the face).
Stage t, (t ≥2)
Convolution
Figure 3. Architecture of the two-branch multi-stage CNN. Each
stage in the ﬁrst branch predicts conﬁdence maps St, and each
stage in the second branch predicts PAFs Lt. After each stage, the
predictions from the two branches, along with the image features,
are concatenated for next stage.
tion architecture, following Wei et al. , which reﬁnes
the predictions over successive stages, t ∈{1, . . . , T}, with
intermediate supervision at each stage.
The image is ﬁrst analyzed by a convolutional network
(initialized by the ﬁrst 10 layers of VGG-19 and ﬁnetuned), generating a set of feature maps F that is input to
the ﬁrst stage of each branch. At the ﬁrst stage, the network
produces a set of detection conﬁdence maps S1 = ρ1(F)
and a set of part afﬁnity ﬁelds L1 = φ1(F), where ρ1 and
φ1 are the CNNs for inference at Stage 1. In each subsequent stage, the predictions from both branches in the previous stage, along with the original image features F, are
concatenated and used to produce reﬁned predictions,
ρt(F, St−1, Lt−1), ∀t ≥2,
φt(F, St−1, Lt−1), ∀t ≥2,
where ρt and φt are the CNNs for inference at Stage t.
Fig. 4 shows the reﬁnement of the conﬁdence maps and
afﬁnity ﬁelds across stages. To guide the network to iteratively predict conﬁdence maps of body parts in the ﬁrst
branch and PAFs in the second branch, we apply two loss
functions at the end of each stage, one at each branch respectively. We use an L2 loss between the estimated predictions and the groundtruth maps and ﬁelds. Here, we weight
the loss functions spatially to address a practical issue that
Figure 4. Conﬁdence maps of the right wrist (ﬁrst row) and PAFs
(second row) of right forearm across stages. Although there is confusion between left and right body parts and limbs in early stages,
the estimates are increasingly reﬁned through global inference in
later stages, as shown in the highlighted areas.
some datasets do not completely label all people. Speciﬁcally, the loss functions at both branches at stage t are:
W(p) · ∥St
W(p) · ∥Lt
j is the groundtruth part conﬁdence map, L∗
groundtruth part afﬁnity vector ﬁeld, W is a binary mask
with W(p) = 0 when the annotation is missing at an image location p. The mask is used to avoid penalizing the
true positive predictions during training. The intermediate
supervision at each stage addresses the vanishing gradient
problem by replenishing the gradient periodically . The
overall objective is
2.2. Conﬁdence Maps for Part Detection
To evaluate fS in Eq. (5) during training, we generate
the groundtruth conﬁdence maps S∗from the annotated 2D
keypoints. Each conﬁdence map is a 2D representation of
the belief that a particular body part occurs at each pixel
location. Ideally, if a single person occurs in the image,
a single peak should exist in each conﬁdence map if the
corresponding part is visible; if multiple people occur, there
should be a peak corresponding to each visible part j for
each person k.
We ﬁrst generate individual conﬁdence maps S∗
each person k. Let xj,k ∈R2 be the groundtruth position of
body part j for person k in the image. The value at location
p ∈R2 in S∗
j,k is deﬁned as,
j,k(p) = exp
−||p −xj,k||2
Figure 5. Part association strategies. (a) The body part detection
candidates (red and blue dots) for two body part types and all
connection candidates (grey lines). (b) The connection results using the midpoint (yellow dots) representation: correct connections
(black lines) and incorrect connections (green lines) that also satisfy the incidence constraint. (c) The results using PAFs (yellow
arrows). By encoding position and orientation over the support of
the limb, PAFs eliminate false associations.
where σ controls the spread of the peak. The groundtruth
conﬁdence map to be predicted by the network is an aggregation of the individual conﬁdence maps via a max operator,
j(p) = max
Gaussian 1
Gaussian 2
Gaussian 1
Gaussian 2
We take the maximum of
the conﬁdence maps instead
of the average so that the
precision of close by peaks
remains distinct, as illustrated in the right ﬁgure. At test time, we predict conﬁdence
maps (as shown in the ﬁrst row of Fig. 4), and obtain body
part candidates by performing non-maximum suppression.
2.3. Part Afﬁnity Fields for Part Association
Given a set of detected body parts (shown as the red and
blue points in Fig. 5a), how do we assemble them to form
the full-body poses of an unknown number of people? We
need a conﬁdence measure of the association for each pair
of body part detections, i.e., that they belong to the same
person. One possible way to measure the association is to
detect an additional midpoint between each pair of parts
on a limb, and check for its incidence between candidate
part detections, as shown in Fig. 5b. However, when people
crowd together—as they are prone to do—these midpoints
are likely to support false associations (shown as green lines
in Fig. 5b). Such false associations arise due to two limitations in the representation: (1) it encodes only the position,
and not the orientation, of each limb; (2) it reduces the region of support of a limb to a single point.
To address these limitations, we present a novel feature
representation called part afﬁnity ﬁelds that preserves both
location and orientation information across the region of
support of the limb (as shown in Fig. 5c). The part afﬁnity is a 2D vector ﬁeld for each limb, also shown in Fig. 1d:
for each pixel in the area belonging to a particular limb, a
2D vector encodes the direction that points from one part of
the limb to the other. Each type of limb has a corresponding
afﬁnity ﬁeld joining its two associated body parts.
Consider a single limb shown in the ﬁgure below. Let
xj1,k and xj2,k be the groundtruth positions of body parts j1
and j2 from the limb c for person k in the image. If a point
p lies on the limb, the value
c,k(p) is a unit vector
that points from j1 to j2; for
all other points, the vector is
zero-valued.
To evaluate fL in Eq. 5 during training, we deﬁne the
groundtruth part afﬁnity vector ﬁeld, L∗
c,k, at an image point
if p on limb c, k
otherwise.
Here, v = (xj2,k −xj1,k)/||xj2,k −xj1,k||2 is the unit vector in the direction of the limb. The set of points on the limb
is deﬁned as those within a distance threshold of the line
segment, i.e., those points p for which
0 ≤v · (p −xj1,k) ≤lc,k and |v⊥· (p −xj1,k)| ≤σl,
where the limb width σl is a distance in pixels, the limb
length is lc,k = ||xj2,k −xj1,k||2, and v⊥is a vector perpendicular to v.
The groundtruth part afﬁnity ﬁeld averages the afﬁnity
ﬁelds of all people in the image,
where nc(p) is the number of non-zero vectors at point p
across all k people (i.e., the average at pixels where limbs
of different people overlap).
During testing, we measure association between candidate part detections by computing the line integral over the
corresponding PAF, along the line segment connecting the
candidate part locations. In other words, we measure the
alignment of the predicted PAF with the candidate limb that
would be formed by connecting the detected body parts.
Speciﬁcally, for two candidate part locations dj1 and dj2,
we sample the predicted part afﬁnity ﬁeld, Lc along the line
segment to measure the conﬁdence in their association:
Lc (p(u)) ·
||dj2 −dj1||2
where p(u) interpolates the position of the two body parts
dj1 and dj2,
p(u) = (1 −u)dj1 + udj2.
In practice, we approximate the integral by sampling and
summing uniformly-spaced values of u.
Figure 6. Graph matching. (a) Original image with part detections
(b) K-partite graph (c) Tree structure (d) A set of bipartite graphs
2.4. Multi-Person Parsing using PAFs
We perform non-maximum suppression on the detection
conﬁdence maps to obtain a discrete set of part candidate locations. For each part, we may have several candidates, due
to multiple people in the image or false positives (shown in
Fig. 6b). These part candidates deﬁne a large set of possible
limbs. We score each candidate limb using the line integral
computation on the PAF, deﬁned in Eq. 10. The problem of
ﬁnding the optimal parse corresponds to a K-dimensional
matching problem that is known to be NP-Hard (shown
in Fig. 6c). In this paper, we present a greedy relaxation that
consistently produces high-quality matches. We speculate
the reason is that the pair-wise association scores implicitly
encode global context, due to the large receptive ﬁeld of the
PAF network.
Formally, we ﬁrst obtain a set of body part detection
candidates DJ for multiple people, where DJ = {dm
for j ∈{1 . . . J}, m ∈{1 . . . Nj}}, with Nj the number
of candidates of part j, and dm
∈R2 is the location
of the m-th detection candidate of body part j.
part detection candidates still need to be associated with
other parts from the same person—in other words, we need
to ﬁnd the pairs of part detections that are in fact connected limbs. We deﬁne a variable zmn
j1j2 ∈{0, 1} to indicate whether two detection candidates dm
connected, and the goal is to ﬁnd the optimal assignment
for the set of all possible connections, Z
for j1, j2 ∈{1 . . . J}, m ∈{1 . . . Nj1}, n ∈{1 . . . Nj2}}.
If we consider a single pair of parts j1 and j2 (e.g., neck
and right hip) for the c-th limb, ﬁnding the optimal association reduces to a maximum weight bipartite graph matching problem . This case is shown in Fig. 5b. In this
graph matching problem, nodes of the graph are the body
part detection candidates Dj1 and Dj2, and the edges are
all possible connections between pairs of detection candidates. Additionally, each edge is weighted by Eq. 10—the
part afﬁnity aggregate. A matching in a bipartite graph is a
subset of the edges chosen in such a way that no two edges
share a node. Our goal is to ﬁnd a matching with maximum
weight for the chosen edges,
Zc Ec = max
where Ec is the overall weight of the matching from limb
type c, Zc is the subset of Z for limb type c, Emn is the
part afﬁnity between parts dm
j2 deﬁned in Eq. 10.
Eqs. 13 and 14 enforce no two edges share a node, i.e., no
two limbs of the same type (e.g., left forearm) share a part.
We can use the Hungarian algorithm to obtain the optimal matching.
When it comes to ﬁnding the full body pose of multiple
people, determining Z is a K-dimensional matching problem. This problem is NP Hard and many relaxations
exist. In this work, we add two relaxations to the optimization, specialized to our domain. First, we choose a minimal
number of edges to obtain a spanning tree skeleton of human pose rather than using the complete graph, as shown in
Fig. 6c. Second, we further decompose the matching problem into a set of bipartite matching subproblems and determine the matching in adjacent tree nodes independently,
as shown in Fig. 6d. We show detailed comparison results
in Section 3.1, which demonstrate that minimal greedy inference well-approximate the global solution at a fraction
of the computational cost. The reason is that the relationship between adjacent tree nodes is modeled explicitly by
PAFs, but internally, the relationship between nonadjacent
tree nodes is implicitly modeled by the CNN. This property
emerges because the CNN is trained with a large receptive
ﬁeld, and PAFs from non-adjacent tree nodes also inﬂuence
the predicted PAF.
With these two relaxations, the optimization is decomposed simply as:
We therefore obtain the limb connection candidates for each
limb type independently using Eqns. 12- 14. With all limb
connection candidates, we can assemble the connections
that share the same part detection candidates into full-body
poses of multiple people. Our optimization scheme over
the tree structure is orders of magnitude faster than the optimization over the fully connected graph .
3. Results
We evaluate our method on two benchmarks for multiperson pose estimation: (1) the MPII human multi-person
dataset and (2) the COCO 2016 keypoints challenge
dataset . These two datasets collect images in diverse
scenarios that contain many real-world challenges such as
crowding, scale variation, occlusion, and contact. Our approach set the state-of-the-art on the inaugural COCO 2016
Subset of 288 images as in 
Deepcut 
Iqbal et al. 
DeeperCut 
Full testing set
DeeperCut 
Iqbal et al. 
Ours (one scale)
Table 1. Results on the MPII dataset. Top: Comparison result on
the testing subset. Middle: Comparison results on the whole testing set. Testing without scale search is denoted as “(one scale)”.
Fig. 6d (sep)
Table 2. Comparison of different structures on our validation set.
keypoints challenge , and signiﬁcantly exceeds the previous state-of-the-art result on the MPII multi-person benchmark. We also provide runtime analysis to quantify the efﬁciency of the system. Fig. 10 shows some qualitative results
from our algorithm.
3.1. Results on the MPII Multi-Person Dataset
For comparison on the MPII dataset, we use the
toolkit to measure mean Average Precision (mAP) of
all body parts based on the PCKh threshold. Table 1 compares mAP performance between our method and other
approaches on the same subset of 288 testing images as
in , and the entire MPI testing set, and self-comparison
on our own validation set. Besides these measures, we compare the average inference/optimization time per image in
seconds. For the 288 images subset, our method outperforms previous state-of-the-art bottom-up methods by
8.5% mAP. Remarkably, our inference time is 6 orders of
magnitude less. We report a more detailed runtime analysis in Section 3.3. For the entire MPII testing set, our
method without scale search already outperforms previous
state-of-the-art methods by a large margin, i.e., 13% absolute increase on mAP. Using a 3 scale search (×0.7, ×1 and
×1.3) further increases the performance to 75.6% mAP. The
mAP comparison with previous bottom-up approaches indicate the effectiveness of our novel feature representation,
PAFs, to associate body parts. Based on the tree structure,
our greedy parsing method achieves better accuracy than a
graphcut optimization formula based on a fully connected
graph structure .
In Table 2, we show comparison results on different
skeleton structures as shown in Fig. 6 on our validation set,
i.e., 343 images excluded from the MPII training set. We
train our model based on a fully connected graph, and compare results by selecting all edges (Fig. 6b, approximately
solved by Integer Linear Programming), and minimal tree
edges (Fig. 6c, approximately solved by Integer Linear Pro-
Normalized distance
Mean average precision %
GT detection
GT connection
PAFs with mask
Two-midpoint
One-midpoint
Normalized distance
Mean average precision %
Figure 7. mAP curves over different PCKh threshold on MPII validation set. (a) mAP curves of self-comparison experiments. (b)
mAP curves of PAFs across stages.
gramming, and Fig. 6d, solved by the greedy algorithm presented in this paper). Their similar performance shows that
it sufﬁces to use minimal edges. We trained another model
that only learns the minimal edges to fully utilize the network capacity—the method presented in this paper—that is
denoted as Fig. 6d (sep). This approach outperforms Fig. 6c
and even Fig. 6b, while maintaining efﬁciency. The reason
is that the much smaller number of part association channels (13 edges of a tree vs 91 edges of a graph) makes it
easier for training convergence.
Fig. 7a shows an ablation analysis on our validation
set. For the threshold of PCKh-0.5, the result using PAFs
outperforms the results using the midpoint representation,
speciﬁcally, it is 2.9% higher than one-midpoint and 2.3%
higher than two intermediate points. The PAFs, which encodes both position and orientation information of human
limbs, is better able to distinguish the common cross-over
cases, e.g., overlapping arms. Training with masks of unlabeled persons further improves the performance by 2.3%
because it avoids penalizing the true positive prediction in
the loss during training. If we use the ground-truth keypoint
location with our parsing algorithm, we can obtain a mAP
of 88.3%. In Fig. 7a, the mAP of our parsing with GT detection is constant across different PCKh thresholds due to no
localization error. Using GT connection with our keypoint
detection achieves a mAP of 81.6%. It is notable that our
parsing algorithm based on PAFs achieves a similar mAP
as using GT connections (79.4% vs 81.6%). This indicates
parsing based on PAFs is quite robust in associating correct
part detections. Fig. 7b shows a comparison of performance
across stages. The mAP increases monotonically with the
iterative reﬁnement framework. Fig. 4 shows the qualitative
improvement of the predictions over stages.
3.2. Results on the COCO Keypoints Challenge
The COCO training set consists of over 100K person instances labeled with over 1 million total keypoints (i.e. body
parts). The testing set contains “test-challenge”, “test-dev”
and “test-standard” subsets, which have roughly 20K images each. The COCO evaluation deﬁnes the object key-
Test-challenge
G-RMI 
G-RMI 
Table 3. Results on the COCO 2016 keypoint challenge. Top: results on test-challenge. Bottom: results on test-dev (top methods
only). AP50 is for OKS = 0.5, APL is for large scale persons.
point similarity (OKS) and uses the mean average precision (AP) over 10 OKS thresholds as main competition metric . The OKS plays the same role as the IoU in object
detection. It is calculated from scale of the person and the
distance between predicted points and GT points. Table 3
shows results from top teams in the challenge. It is noteworthy that our method has lower accuracy than the top-down
methods on people of smaller scales (APM). The reason is
that our method has to deal with a much larger scale range
spanned by all people in the image in one shot. In contrast, top-down methods can rescale the patch of each detected area to a larger size and thus suffer less degradation
at smaller scales.
GT Bbox + CPM 
SSD + CPM 
Ours - 6 stages
+ CPM reﬁnement
Table 4. Self-comparison experiments on the COCO validation set.
In Table 4, we report self-comparisons on a subset of
the COCO validation set, i.e., 1160 images that are randomly selected. If we use the GT bounding box and a single person CPM , we can achieve a upper-bound for
the top-down approach using CPM, which is 62.7% AP.
If we use the state-of-the-art object detector, Single Shot
MultiBox Detector (SSD) , the performance drops 10%.
This comparison indicates the performance of top-down approaches rely heavily on the person detector. In contrast,
our bottom-up method achieves 58.4% AP. If we reﬁne the
results of our method by applying a single person CPM on
each rescaled region of the estimated persons parsed by our
method, we gain an 2.6% overall AP increase. Note that
we only update estimations on predictions that both methods agree well enough, resulting in improved precision and
recall. We expect a larger scale search can further improve
the performance of our bottom-up method. Fig. 8 shows a
breakdown of errors of our method on the COCO validation set. Most of the false positives come from imprecise
localization, other than background confusion. This indicates there is more improvement space in capturing spatial
dependencies than in recognizing body parts appearances.
Number of people
Runtime (ms)
CNN processing
[.626] C75
[.826] C50
[.918] Loc
[.692] C75
[.862] C50
[.919] Loc
[.588] C75
[.811] C50
[.929] Loc
(a) PR curve - all people
(b) PR curve - large scale person
(c) PR curve - medium scale person
(d) Runtime analysis
Figure 8. AP performance on COCO validation set in (a), (b), and (c) for Section 3.2, and runtime analysis in (d) for Section 3.3.
Figure 9. Common failure cases: (a) rare pose or appearance, (b) missing or false parts detection, (c) overlapping parts, i.e., part detections
shared by two persons, (d) wrong connection associating parts from two persons, (e-f): false positives on statues or animals.
3.3. Runtime Analysis
To analyze the runtime performance of our method, we
collect videos with a varying number of people. The original frame size is 1080×1920, which we resize to 368×654
during testing to ﬁt in GPU memory. The runtime analysis is performed on a laptop with one NVIDIA GeForce
GTX-1080 GPU. In Fig. 8d, we use person detection and
single-person CPM as a top-down comparison, where the
runtime is roughly proportional to the number of people in
the image. In contrast, the runtime of our bottom-up approach increases relatively slowly with the increasing number of people. The runtime consists of two major parts: (1)
CNN processing time whose runtime complexity is O(1),
constant with varying number of people; (2) Multi-person
parsing time whose runtime complexity is O(n2), where n
represents the number of people. However, the parsing time
does not signiﬁcantly inﬂuence the overall runtime because
it is two orders of magnitude less than the CNN processing time, e.g., for 9 people, the parsing takes 0.58 ms while
CNN takes 99.6 ms. Our method has achieved the speed of
8.8 fps for a video with 19 people.
4. Discussion
Moments of social signiﬁcance, more than anything
else, compel people to produce photographs and videos.
Our photo collections tend to capture moments of personal
signiﬁcance: birthdays, weddings, vacations, pilgrimages,
sports events, graduations, family portraits, and so on. To
enable machines to interpret the signiﬁcance of such photographs, they need to have an understanding of people in
images. Machines, endowed with such perception in realtime, would be able to react to and even participate in the
individual and social behavior of people.
In this paper, we consider a critical component of such
perception: realtime algorithms to detect the 2D pose of
multiple people in images. We present an explicit nonparametric representation of the keypoints association that encodes both position and orientation of human limbs. Second, we design an architecture for jointly learning parts detection and parts association. Third, we demonstrate that
a greedy parsing algorithm is sufﬁcient to produce highquality parses of body poses, that maintains efﬁciency even
as the number of people in the image increases. We show
representative failure cases in Fig. 9. We have publicly released our code (including the trained models) to ensure full
reproducibility and to encourage future research in the area.
Acknowledgements
We acknowledge the effort from the authors of the MPII
and COCO human pose datasets. These datasets make 2D
human pose estimation in the wild possible. This research
was supported in part by ONR Grants N00014-15-1-2358
and N00014-14-1-0595.