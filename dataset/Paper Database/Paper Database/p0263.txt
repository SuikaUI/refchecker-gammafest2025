GAN-based Synthetic Medical Image Augmentation
for increased CNN Performance
in Liver Lesion Classiﬁcation
Maayan Frid-Adar, Idit Diamant, Eyal Klang, Michal Amitai, Jacob Goldberger,
and Hayit Greenspan, Member, IEEE
Abstract—Deep learning methods, and in particular convolutional neural networks (CNNs), have led to an enormous
breakthrough in a wide range of computer vision tasks, primarily
by using large-scale annotated datasets. However, obtaining such
datasets in the medical domain remains a challenge. In this paper,
we present methods for generating synthetic medical images
using recently presented deep learning Generative Adversarial
Networks (GANs). Furthermore, we show that generated medical
images can be used for synthetic data augmentation, and improve
the performance of CNN for medical image classiﬁcation. Our
novel method is demonstrated on a limited dataset of computed
tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). We ﬁrst exploit GAN architectures
for synthesizing high quality liver lesion ROIs. Then we present a
novel scheme for liver lesion classiﬁcation using CNN. Finally, we
train the CNN using classic data augmentation and our synthetic
data augmentation and compare performance. In addition, we
explore the quality of our synthesized examples using visualization and expert assessment. The classiﬁcation performance
using only classic data augmentation yielded 78.6% sensitivity
and 88.4% speciﬁcity. By adding the synthetic data augmentation
the results increased to 85.7% sensitivity and 92.4% speciﬁcity.
We believe that this approach to synthetic data augmentation can
generalize to other medical classiﬁcation applications and thus
support radiologists’ efforts to improve diagnosis.
Index Terms—Image synthesis, data augmentation, convolutional neural networks, generative adversarial network, deep
learning, liver lesions, lesion classiﬁcation.
I. INTRODUCTION
HE greatest challenge in the medical imaging domain is
how to cope with the small datasets and limited amount
of annotated samples – , especially when employing
supervised machine learning algorithms that require labeled
data and larger training examples. In medical imaging tasks,
annotations are made by radiologists with expert knowledge on
the data and task. Most annotations of medical images are time
consuming. This is especially true for precise annotations, such
as the segmentations of organs or lesions into multiple 2-D
slices and 3-D volumes. Although public medical datasets are
available online, and grand challenges have been publicized,
M. Frid-Adar, I. Diamant and H. Greenspan are with the Department
of Biomedical Engineering, Tel Aviv University, Tel Aviv, Israel (e-mail:
 ; ; ).
E. Klang and M. Amitai are with the Department of Diagnostic Imaging, The Chaim Sheba Medical Center, Tel-Hashomer, Israel (e-mail:
 ; ).
J. Goldberger is with the Faculty of Engineering, Bar-Ilan University,
Ramat-Gan, Israel (e-mail: ).
most datasets are still limited in size and only applicable to
speciﬁc medical problems. Collecting medical data is a complex and expensive procedure that requires the collaboration
of researchers and radiologists .
Researchers attempt to overcome this challenge by using
data augmentation. The most common data augmentation
methods include simple modiﬁcations of dataset images such
as translation, rotation, ﬂip and scale. Using classic data
augmentation to improve the training process of networks is
a standard procedure in computer vision tasks . However,
little additional information can be gained from small modiﬁcations to the images (e.g. the translation of the image a few
pixels to the right). Synthetic data augmentation of high quality
examples is new, sophisticated type of data augmentation.
Synthetic data examples learned using a generative model
enable more variability and enrich the dataset to further
improve the system training process.
One such promising approach inspired by game theory for
training a model that syntheses images is known as Generative
Adversarial Networks (GANs) . The model consists of two
networks that are trained in an adversarial process where
one network generates fake images and the other network
discriminates between real and fake images repeatedly. GANs
have gained great popularity in the computer vision community and different variations of GANs were recently proposed
for generating high quality realistic natural images – .
Interesting applications of GAN include generating images of
one style from another style (image-to-image translation) 
and image inpainting using GAN .
Recently, several medical imaging applications have applied the GAN framework – . Most studies have employed the image-to-image GAN technique to create label-tosegmentation translation, segmentation-to-image translation or
medical cross modality translations. Costa et al. trained a
fully-convolutional network to learn retinal vessel segmentation images. Then they learned the translation from the binary
vessel tree to a new retinal image. Dai et al. trained
GAN to create segmentation images of the lung ﬁelds and
the heart from chest X-ray images. Xue et al. referred to
the two GAN networks as a Segmentor and Critic, and learned
the translation between brain MRI images and a brain tumor
binary segmentation map. In Nie et al. , A patch-based
GAN was trained for translation between brain CT images
and the corresponding MRI images. They further suggested
an auto-context model for image reﬁnement. Ben-Cohen et
 
al. also introduced a cross modality image generation
using GAN, from abdominal CT image to a PET scan image
that highlights liver lesions. Some studies have been inspired
by the GAN method for image inpainting. Schlegl et al. 
trained GAN with healthy patches of the retinal area to learn
the data distribution of healthy tissue. Then they tested the
GAN on patches of both unseen healthy and anomalous data
for anomaly detection in retinal images.
The problem of limited data in the medical imaging ﬁeld
prompted us to explore methods for synthetic data augmentation to enlarge medical datasets. In the current study, we focus
on improving results in the speciﬁc task of liver lesion classiﬁcation. We applied the GAN framework to synthesize high
quality liver lesion images (hereon we use interchangeably the
terms lesion images and lesion ROIs).
The liver is one of three most common sites for metastatic
cancer along with the bone and lungs . According to
the World Health Organization, in 2012 alone, cancer accounted for 8.2 million deaths worldwide of which 745,000
were caused by liver cancer . Focal liver lesions can
be malignant and manifest metastases, or be benign (e.g.
hemangioma or hepatic cysts). Computed tomography (CT) is
one of the most common and robust imaging techniques for the
detection, diagnosis and follow up of liver lesions . Thus,
there is a great need and interest in developing automated
diagnostic tools based on CT images to assists radiologists in
the diagnosis of liver lesions.
Previous studies have presented methods for automatic classiﬁcation of focal liver lesions in CT images – . Gletsos
et al. used texture features for liver lesion classiﬁcation
into four categories including the normal liver parenchyma
class. They applied a hierarchical classiﬁer of neural networks
at each level. Chang et al. obtained three kind of features
for each tumor, including texture, shape, and kinetic curve
on segmented tumors. Backward elimination was used to
select the best combination of features through binary logistic
regression analysis to classify the tumors. Diamant et al.
 applied the bag-of-visual-words (BoVW) method learned
from image patches. They used two dictionaries for lesion
interior and boundary regions. Based on the two dictionaries
they generated histograms for each lesion ROI. The ﬁnal
classiﬁcation was made using SVM.
In the current work we used deep learning methodology for
the task of liver lesion classiﬁcation. Deep learning convolutional neural networks (CNNs) has emerged as a powerful
tool in computer vision. In recent years many medical imaging
studies have applied CNNs and reported improved performance for a broad range of medical tasks . We combine
synthetic liver lesion generation using GAN with our proposed
CNN for liver lesion classiﬁcation.
The contributions of this work are the following:
1) Synthesis of high quality focal liver lesions from CT
images using generative adversarial networks (GANs).
2) Design of a CNN-based solution for the liver lesion
classiﬁcation task, with comparable results to state-ofthe-art methods.
3) Augmentation of the CNN training set, using the generated synthetic data - for improved classiﬁcation results.
II. LIVER LESION CLASSIFICATION
In this section we ﬁrst describe the data and their characteristics. Then we elaborate on the CNN architecture for the
liver lesion classiﬁcation task. The main challenge is the small
amount of data available for training the CNN. In the next
section we describe methods to artiﬁcially enlarge the data.
The dataset used in this work contains cases of liver lesions
collected from Sheba Medical Center by searching medical
records for cases of cysts, metastases and hemangiomas. Cases
were acquired from 2009 to 2014 using two CT scanners: a
General Electric (GE) Healthcare scanner and a Siemens Medical System scanner, with the following parameters: 120kVp,
140-400mAs and 1.25-5.0 mm slice thickness. Cases were
collected with the approval of the institution’s Institutional
Review Board.
Figure 1 shows examples of the input data and the ROI
extraction process. The dataset was made up of 182 portalphase 2-D CT scans (Figure 1a): 53 cysts, 64 metastases,
65 hemangiomas. An expert radiologist marked the margin of
each lesion and determined its corresponding diagnosis which
was established by biopsy or a clinical follow-up. This serves
as our ground truth.
Liver lesions vary considerably in shape, contrast and size
(10 - 102mm). They also vary within categories. In addition,
they are located in interior sections of the liver or near its
boundary where the surrounding parenchyma tissue of the
lesions changes. Each type of lesion has its own characteristics: Cysts are non-enhancing water-attenuation circumscribed lesions. Metastases are hypoattenuating, have softtissue attenuation and less well-deﬁned margins than cysts, and
hemangiomas show typical features of discontinuous nodular
peripheral enhancement, with ﬁll-in on delayed images .
Despite this detailed description, some characteristics may be
confusing, in particular for metastasis and hemangioma lesions
(see Figure 1a). Metastases can contain areas of higher density,
probably prominent blood vessels or calciﬁcations that can be
mistaken for hemangiomas attributes. Hemangiomas are benign tumors and metastases are malignant lesions derived from
different primary cancers. Thus, the correct identiﬁcation of a
lesion as metastasis or hemangioma is especially important.
The input to our classiﬁcation system are ROIs of lesions
cropped from CT scans using the radiologist’s annotations. The
ROIs are extracted to capture the lesion and its surrounding
tissue relative to its size. Due to the large variability in lesion
sizes, this results in varying size ROIs (Figure 1b).
B. CNN Architecture
The architecture of the liver lesion classiﬁcation system we
propose is shown in Figure 2. CNNs are widely used for
solving image classiﬁcation tasks in computer vision . CNN
architectures for medical imaging have also been introduced
 , , , usually containing fewer convolutional layers
because of the small datasets and smaller input size. Our
classiﬁcation CNN gets ﬁxed size input ROIs of 64×64, with
Fig. 1. (a) Dataset examples of cyst, metastasis and hemangioma liver lesions. (b) ROI extraction process from a 2-D CT slice of the liver. All ROIs are
resized to a uniform size.
an intensity range rescaled to (0, 1). The architecture consists
of three pairs of convolutional layers where each convolutional
layer is followed by a max-pooling layer, and two dense fullyconnected layers ending with a soft-max layer to determine the
network predictions to classify lesions into three classes. We
use ReLU as activation functions. The network had approx.
1.3M parameters. In addition, to further reducing overﬁtting,
we incorporated a dropout layer with a probability of 0.5
during training.
Training Procedure. The mean value of the training images
was subtracted from each image fed into the CNN. For training
we used a batch size of 64 with a learning rate of 0.001 for 150
epochs. We used stochastic gradient descent optimization with
Nesterov momentum updates , where instead of evaluating
the gradient at the current position we evaluated it at the “lookahead” position which improves the optimization process.
III. GENERATING SYNTHETIC LIVER LESIONS
The main problem in training the network described above
is the lack of a large labeled training dataset. To enlarge the
training data and improve the classiﬁcation results in the liver
lesion classiﬁcation task, we augmented the data in two ways:
1) Classic augmentation that includes varieties of known image
manipulations on given data examples; 2) Synthesis of new
examples which are learned from the data examples using
generative models. We start with an overview of standard data
augmentation techniques and then describe our new method
of generating synthetic liver lesion images using generative
adversarial networks (GANs).
A. Classic Data Augmentation
Even a small CNN has thousands of parameters that need
to be trained. When using deep networks with multiple layers
or dealing with limited numbers of training images, there
is a danger of overﬁtting. The standard solution to reduce
overﬁtting is data augmentation that artiﬁcially enlarges the
dataset . Classic augmentation techniques on gray-scale
images include mostly afﬁne transformations such as translation, rotation, scaling, ﬂipping and shearing , . In
Fig. 2. The architecture of the liver lesion classiﬁcation CNN.
order to preserve the liver lesion characteristics we avoided
transformations that cause shape deformation (like shearing).
In addition, we kept the ROI centered around the lesion.
Each lesion ROI was ﬁrst rotated Nrot times at random
angles θ = [0◦, ..., 180◦]. Afterwards, each rotated ROI was
ﬂipped Nflip times (up-down,left-right), translated Ntrans
times where we sampled random pairs of [x, y] pixel values between (−p, p) related to the lesion diameter (d) by
Lesion ROI and augmentation examples of translation, rotation,
ﬂipping and scaling.
p = min(4, 0.1 × d). Finally the ROI was scaled Nscale
times from a stochastic range of scales s = [0.1 × d, 0.4 × d].
The scale was implemented by changing the amount of tissue
around the lesion in the ROI. As a result of the augmentation
process, the total number of augmentations was N = Nrot ×
(1 + Nflip + Ntrans + Nscale). An example lesion and its
corresponding augmentations are shown in Figure 3. All the
ROIs were resized to ﬁt a uniform size of 64×64 pixels using
bicubic interpolation.
B. Generative Adversarial Networks for Lesion Synthesis
GANs are a speciﬁc framework of a generative model.
The generative model aims to implicitly learn the data distribution pdata from a set of samples x(1), ..., x(m) (e.g.
images) to further generate new samples drawn from the
learned distribution. We explored two variants of GANs for
synthesizing labeled lesions, as shown in Figure 4: one that
generates labeled examples for each lesion class separately
and the other that incorporates class conditioning to generate
labeled examples all at once.
We started with the ﬁrst GAN variant, the Deep Convolutional GAN (DCGAN). We followed the architecture proposed
by Radford et al. , where both the G and D networks are
deep CNNs. They suggested architectural guidelines for stable
GAN training and modiﬁcations of the original GAN proposed
by Goodfellow et al. , which have become the basis for
many recent GAN papers , , . The model consists
of two neural networks that are trained simultaneously (see
Figure 4a). The ﬁrst network is termed the discriminator and
is denoted D. The role of the discriminator is to discriminate
between the real and fake samples. It is inputted a sample
x and outputs D(x), its probability of being a real sample.
The second network is termed the generator and is denoted
G. The generator synthesizes samples that D will consider to
be real samples with high probability. G gets input samples
z(1), ..., z(m) from a known simple distribution pz, usually a
uniform distribution, and maps G(z) to the image space of
distribution pg. The goal of G is to achieve pg = pdata.
Adversarial networks are trained by optimizing the following loss function of a two-player minimax game:
D Ex∼pdata log D(x) + Ez∼pz[log (1 −D(G(z)))]
The discriminator is trained to maximize D(x) for images
with x ∼pdata and to minimize D(x) for images with
x ≁pdata. The generator produces images G(z) to fool D
during training such that D(G(z)) ∼pdata. Therefore, the
Fig. 4. (a) DCGAN architecture. (b) ACGAN architecture (Figure is taken
from ).
Fig. 5. Generator architecture (of deep convolutional GAN).
generator is trained to maximize D(G(z)), or equivalently
minimize 1−D(G(z)). During training the generator improves
in its ability to synthesize more realistic images while the
discriminator improves in its ability to distinguish the real
from the synthesized images. Hence the moniker of adversarial
Generator Architecture: The generator network takes a vector of 100 random numbers drawn from a uniform distribution
as input and outputs a liver lesion image of size 64 × 64 × 1
as shown in Figure 5. The network architecture consists
of a fully connected layer reshaped to size 4 × 4 × 1024
and four fractionally-strided convolutional layers to up-sample
the image with a 5 × 5 kernel size. A fractionally-strided
convolution (known also as ‘deconvolution’) can be interpreted
as expanding the pixels by inserting zeros in between them.
Convolution over the expanded image will result in a larger
output image. Batch-normalization is applied to each layer
of the network, except for the output layer. Normalizing
responses to have zero mean and unit variance over the entire
mini-batch stabilizes the GAN learning process and prevents
the generator from collapsing all samples to a single point .
ReLU activation functions are applied to all layers except the
output layer which uses a tanh activation function.
Discriminator Architecture: The discriminator network has
a typical CNN architecture that takes the input image of size
64 × 64 × 1 (lesion ROI), and outputs one decision: is this
lesion real or fake? The network consists of four convolution
layers with a kernel size of 5 × 5 and a fully connected layer.
Strided convolutions are applied to each convolution layer to
reduce spatial dimensionality instead of using pooling layers.
Batch-normalization is applied to each layer of the network,
except for the input and output layers. Leaky ReLU activation
functions f(x) = max (x, leak × x) are applied to all layers
except the output layer which uses the Sigmoid function for
the likelihood probability (0, 1) score of the image.
Training Procedure: We trained the DCGAN to synthesize liver lesion ROIs for each lesion category separately.
The training process was done iteratively for the generator
and the discriminator. We used mini-batches of m=64 lesion ROI examples x(1)
, ..., x(m)
for each lesion type l ∈
(Cyst, Metastasis, Hemangioma) and m=64 noise samples
z(1), ..., z(m) drawn from uniform distribution between [−1, 1].
The only preprocessing steps used involved scaling the training
images to the range of the tanh activation function (−1, 1). In
the Leaky ReLU, the slope of the leak was set to leak = 0.2.
Weights were initialized to a zero-centered normal distribution
with standard deviation of 0.02. We applied stochastic gradient
descent with the Adam optimizer , an adaptive moment
estimation that incorporates the ﬁrst and second moments
of the gradients, controlled by parameters β1 = 0.5 and
β2 = 0.999 respectively. We used a learning rate of 0.0002
for 70 epochs.
C. Conditional Lesion Synthesis
The second GAN variant is the Auxiliary Classiﬁer GAN
(ACGAN). Conditional GANs are an extension of the GAN
model, that enable the model to be conditioned on external
information to improve the quality of the generated samples.
GAN architectures that incorporate the class labels to produce
labeled samples were introduced by , , . Odena et
al. suggested that instead of feeding the discriminator with
side information , the discriminator should be tasked with
reconstructing side information. This is done by modifying
the discriminator to contain an auxiliary decoder network that
outputs the class label in addition to the real or fake decision
(see Figure 4b). We followed the architecture proposed in 
with minor modiﬁcations for synthesizing the labeled lesions
of all three types. ACGANs generator architecture is similar
to the DCGANs architecture described in section III-B with
class embedding in addition to the input noise samples. The
ACGAN discriminator architecture modiﬁed the DCGAN to
have kernels of size 3×3 with strided convolutions every odd
layer and incorporates a dropout of 0.5 in every layer except
for the last layer. We use the ACGAN discriminator without
these modiﬁcations after optimizing for our small dataset. The
discriminator auxiliary decoder classiﬁed the three classes of
Training Procedure: The training parameters were similar to the ones described in III-B except that we used
a learning rate of 0.0001 for 50 epochs. Our training
inputs included liver lesion ROIs and their corresponding
(xl, yl)(1), ..., (xl, yl)(m)
l ∈(Cyst, Metastasis, Hemangioma), and noise samples
z(1), ..., z(m) drawn from uniform distribution between [−1, 1].
The loss function needed to be modiﬁed to incorporate the label information. For simpliﬁcation, let us write the basic GAN
discriminator maximization equation over the log-likelihood
(similar to Equation 1) as:
L = E[log P(S = real|Xreal)] + E[log P(S = fake|Xfake)]
where P(S|X) = D(X) and Xfake = G(z). The generator is
trained to minimize that objective. In ACGAN, the discriminator outputs P(S|X), P(C|X) = D(X), and Xfake = G(c, z)
where C is the class label. The loss has two parts:
Ls = E[log P(S = real|Xreal)] + E[log P(S = fake|Xfake)]
Lc = E[log P(C = c|Xreal)] + E[log P(C = c|Xfake)]
The discriminator is trained to maximize Ls + Lc and the
generator is trained to maximize Lc −Ls.
IV. EXPERIMENTS AND RESULTS
In the following we present a set of experiments and
results. To test the classiﬁcation results, we employed the
CNN architecture described in Section II-B. We then analyzed
the effects of data augmentation using synthetic liver lesions,
as compared to classical data augmentation methodology. We
implemented the two methods for synthetic lesion generation,
as described in Sections III-B and III-C. In our experimentations we found that the Deep Convolutional GAN (DCGAN)
method performed better. We therefore focus on that method
in the results presented below. A comparison between the
ACGAN and the DCGAN results will be presented in Section
A. Dataset Evaluation and Implementation Details
In all experiments and evaluations we used 3-fold cross
validation with case separation at the patient level. The number
of examples in each fold was (63, 63, 62) and each contained
a balanced number of cyst, metastasis and hemangioma lesion
ROIs. We evaluated the classiﬁcation performance using a total
classiﬁcation accuracy measure. Additionally, we calculated
confusion matrices and sensitivity and speciﬁcity measures for
each lesion category. All these measures are presented in the
following equations:
Total Accuracy =
Amount of lesions
Sensitivity =
Specificity =
where for each lesion category, positives (P) are examples
from this category and negatives (N) are examples from the
other two categories.
For the implementation of the liver lesion classiﬁcation
CNN we used the Keras framework . For the implementation of the GAN architectures we used the TensorFlow
framework . All training processes were performed using
an NVIDIA GeForce GTX 980 Ti GPU.
Fig. 6. Synthetic liver lesion ROIs generated with DCGAN for each category: (a) Cyst examples (b) Metastasis examples (c) Hemangioma examples.
Fig. 7. Experiment ﬂowchart for evaluating synthetic data augmentation in
the task of classifying liver lesion ROIs.
B. Evaluation of the Synthetic Data Augmentation
Figure 7 presents the ﬂowchart for the experiment conducted
to evaluate the results from synthetic data augmentation: We
started by examining the effects of using only classic data
augmentation for the liver lesion classiﬁcation task (our baseline). We then synthesized liver lesion ROIs using GAN and
examined the classiﬁcation results after adding the synthesized
lesion ROIs to the training set. A detailed description of each
step is provided next.
1) Classical data augmentation: As our baseline, we used
classical data augmentation (see section III-A). We refer to
this network as CNN-AUG. We recorded the classiﬁcation
results for the liver lesion classiﬁcation CNN for increasing
amounts of data augmentation over the original training
set. We trained the network and evaluated the results
separately for each set of data images (that included the
original images and additional classic augmented images),
as follows: Let {Daug}9
i=1 be the data groups that include
increasing amounts of augmented examples for each training.
During testing time, we used the same data examples for
all evaluations. In order to examine the effect of adding
increasing numbers of examples, we formed the data groups
additively such that D1
aug ⊂... ⊂D9
ﬁrst data group was only made up of the original ROIs.
For each original ROI, we produced a large number of
augmentations (Nrot = 30, Nflip = 3, Ntrans = 7 and
Nscale = 5), resulting in N = 480 augmented images per
lesion ROI and overall ∼30, 000 examples per folder. Then,
we selected the images for the data groups by sampling
randomly augmented examples such that for each original
lesion we sampled the same augmentation volume. To
summarize the augmentation data group preparation process,
the number of samples added to each fold (in our 3-folds)
{0, 500, 1000, 2000, 3000, 5000, 7500, 10000, 15000}.
The training process was conducted by cross-validation
over 3-folds, such that for each training group, the training
examples were from two folds.
2) Synthetic data augmentation: The second step of the
experiment consisted of generating synthetic liver lesion
ROIs for data augmentation using GAN. We refer to this
network as CNN-AUG-GAN. We took the optimal point for
the classic augmentation Doptimal
and used this group of
data to train the GAN. Since our dataset was too small for
effective training, we incorporated classic augmentation for
the training process. We employed the DCGAN architecture
to train each lesion class separately, using the same 3-fold
cross validation process and the same data partition. After
the generator had learned each lesion class data distribution
separately, it was able to synthesize new examples by using
an input vector of normal distributed samples (“noise”).
Figure 6 presents examples of synthesized liver lesion ROIs
from each class. The same approach that was applied in
step one of the experiment when constructing the data
groups was also applied in step two: We collected large
numbers of synthetic lesions for all three lesion classes, and
constructed data groups {Dsynth}6
j=1 of synthetic examples
additively. To keep the classes balanced, we sampled the
same number of synthetic ROIs for each class. To summarize
the synthetic augmentation data group preparation process,
the number of samples added to each fold (in our 3-folds)
was {100×3, 500×3, 1000×3, 2000×3, 3000×3, 4000×3}.
Results of the GAN-based synthetic augmentation experiment are shown in Figure 8. The baseline results (classical
augmentation) are shown in red. We see the total accuracy
results for the lesion classiﬁcation task, for each group of data.
Fig. 8. Total accuracy results for liver lesion classiﬁcation of cysts, metastases and hemangiomas with the increase of training set size. The red line shows
the effect of adding classic data augmentation and the blue line shows the effect of adding synthetic data augmentation.
CONFUSION MATRIX FOR THE OPTIMAL CLASSICAL DATA
AUGMENTATION GROUP (CNN-AUG)
True \ Auto
Sensitivity
Speciﬁcity
CONFUSION MATRIX FOR THE OPTIMAL SYNTHETIC DATA
AUGMENTATION GROUP (CNN-AUG-GAN)
True \ Auto
Sensitivity
Speciﬁcity
When no augmentations were applied, a result of 57% was
achieved; this may be due to overﬁtting over the small number
of training examples (∼63 samples per fold). The results
improved as the number of training examples increased, up to
saturation around 78.6% where adding more augmented data
examples failed to improve the classiﬁcation results. We note
that the saturation starts with D6
aug = 5000 samples per fold.
We deﬁne this point as i=optimal where the smallest number
of augmented samples were used. The confusion matrix for
the optimal point appears in Table I.
The blue line in Figure 8 shows the total accuracy results for
the lesion classiﬁcation task for the synthetic data augmentation scenario. The classiﬁcation results improved from 78.6%
with no synthesized lesions to 85.7% for Doptimal
5000 + 3000 = 8000 samples per fold. The confusion matrix
for the best classiﬁcation results using synthetic data augmentation is presented in Table II.
C. Visualization using t-SNE
To further analyze the results, we used the t-SNE visualization. The t-SNE algorithm for dimensionality reduction enables the embedding of high-dimensional data into
a two dimensional space . The high-dimensional data
for visualization are features extracted from the last layer
of a trained liver lesion classiﬁcation CNN. We trained the
CNN in two scenarios: one with the classic augmented data
examples (CNN-AUG) and one with the synthesized data
examples (CNN-AUG-GAN). Afterwards, for each scenario,
we extract the features of real images from the test set and their
classic augmentations. We then used the t-SNE to illustrate the
features, as shown in Figure 9 (a) and (b), respectively.
We note that the cyst category, shown in red, shows a more
distinct localization in the t-SNE space. This characteristic
correlates well with the more distinctive features of the cyst
class as compared to metastases or hemangiomas. Metastases
and hemangiomas have confusing features, which is indicated
here in the perceived overlap and accounts for the lower
sensitivity and speciﬁcity results than in the cyst class. When
using the synthetic data augmentation, the t-SNE visualization
exhibited in general better separating power. This can provide
intuition for the increase in classiﬁcation performance.
D. Expert Assessment of Synthetic Data
Human annotators have been shown to evaluate the visual
quality of samples generated by GANs , . In our study,
we were interested to explore two key points: Is the synthesized lesions appearance a realistic one? Is the set of lesions
generated sufﬁciently distinct to enable classiﬁcation amongst
the three lesion categories? These issues were explored with
the help of two expert radiologists.
We created an automatic application which was presented
to two independent radiologists, with two tasks. One task was
to classify each presented lesion ROI image into one of three
classes: cyst, metastasis or hemangioma. The second task was
Fig. 9. T-SNE embedding of Cysts (red), Metastases (blue) and Hemangiomas (green) real lesion ROIs. (a) Features extracted from CNN-AUG (b) Features
extracted from CNN-AUG-GAN.
SUMMARY OF EXPERTS’ ASSESSMENT OF LESION ROI
Classiﬁcation Accuracy
Is ROI Real?
Total Score
Total Score
235\302=77.8%
189\302=62.5%
209\302=69.2%
177\302=58.6%
to distinguish between real lesion images and synthetic lesion
images. The experts were given, in random order, lesion ROIs
from the original dataset of 182 real lesions and 120 additional
synthesized lesions. Both our algorithm results and the expert
radiologists’ results were compared against the ground truth
classiﬁcation.
Table III summarizes the experts’ results. We note the overall low results, on the order of 60%, in identifying whether the
lesions shown were true or fake. In the lesion categorization
task, Expert 1 and Expert 2 classiﬁed correctly in 77.8%
and 69.2% of the cases, respectively. Overall, the radiologists
agreed on the lesion class on 222 out of 302 lesions (73.5%),
with a correct classiﬁcation of 185 out of 302 lesions. In
addressing these results, it is important to note that the task we
deﬁned was not consistent with existing clinical workﬂow. The
radiologist is trained to make a decision based on the entire
3D volume, with support from additional anatomical context,
medical history context, and more. Here, we challenged the
radiologists to reach a decision based on a single 2-D ROI
image. In this scenario, the baseline CNN solution is similar
in performance to the human expert. Using the GAN-based
augmentation, an increase of approx 7% is achieved.
As a ﬁnal note, we observe that for both experts, the
classiﬁcation performances for the real lesions and the synthesized lesions were similar. which suggests that our synthetic
generated lesions were meaningful in appearance.
E. Comparison with Other Classiﬁcation Methods
Table IV compares the best classiﬁcation results between
the DCGAN and ACGAN models. As described above, we
PERFORMANCE COMPARISON FOR LIVER LESION CLASSIFICATION
BETWEEN GENERATIVE MODELS
Sensitivity
Speciﬁcity
CNN-AUG-GAN (DCGAN)
CNN-AUG-GAN (ACGAN)
ACGAN discriminator
used synthetic augmentations generated using the DCGAN for
training the classiﬁcation CNN (CNN-AUG-GAN). Training
the classiﬁcation CNN with synthetic augmentations generated
using the ACGAN, yield improved results in comparison of
using only classic augmentations, but degraded results in comparison to the DCGAN. The ACGAN discriminator contains
an auxiliary classiﬁer. Thus, after training the ACGAN, we can
use the learned discriminator as an autonomous component
to test directly the test set performance. Using this method
resulted in ∼2% decrease in performance.
In our ﬁnal experiment, we compared our CNN classi-
ﬁcation results for classic augmentation (CNN-AUG) and
synthetic augmentation (CNN-AUG-GAN), to a recently
published state-of-the-art liver lesion categorization method,
termed BoVW-MI . The BoVW-MI method is an enhancement of the BoVW model. It learns a task-driven dictionary
of the most relevant visual words per task using a mutual
information measure. In order to compare between the approaches, using the datasets of the current work, we ran the
BoVW-MI method using the speciﬁed optimized parameters
for the liver lesion classiﬁcation task, as found in : A
patch size of 11 × 11, a word size with a 10 PCA coefﬁcient,
a dictionary size of 750 words and a MI threshold of 35%.
We trained the BoVW-MI in 3-fold cross validation using the
same lesion partitions. Table V compares the sensitivity and
speciﬁcity results of our best results to the BOVW-MI results.
V. DISCUSSION AND CONCLUSIONS
This work focused on generating synthetic medical images
with GAN for data augmentation to enlarge small datasets and
PERFORMANCE COMPARISON FOR LIVER LESION CLASSIFICATION BETWEEN CNN AND BOVW-MI
CNN-AUG-GAN
Sensitivity
Speciﬁcity
Sensitivity
Speciﬁcity
Sensitivity
Speciﬁcity
Metastases
Hemangiomas
Weighted Average
improve performance on classiﬁcation tasks using CNN. Our
relatively small dataset reﬂects the size of datasets available
to most researchers in the medical imaging community (by
contrast to the computer vision community where large scale
datasets are available).
We tested our hypothesis that adding synthesized examples would improve classiﬁcation results. The experimental
setup is depicted in Figure 7. The experiment was carried
out on a limited dataset of three liver lesion categories of
cysts, metastases and hemangiomas. Each class has its unique
features but there is also considerable intra-variability between
classes, mostly for the metastases and hemangiomas. We
classiﬁed the three categories using a CNN architecture. In
running the experiment, we found that increasing the size
of the training data groups with the standard augmentation
(translation, rotation, ﬂip, scale), improved training results up
to a certain volume of augmented data, where adding more
data did not improve the results (Figure 8). Table I shows
the results for the optimal point achieved using the commonly
used classic augmentation.
In the second step of the experiment we used GANs to
generate new examples learned from our small dataset. The
best generated liver lesion samples were produced by using
the Deep Convolution GAN (DCGAN) for each lesion class
separately. Starting from the optimal point where classic
augmentation reached saturation, we applied increasing sizes
of synthetic data. We saw an improvement in the classiﬁcation
results from 78.6% to 85.7% total accuracy (Figure 8). We see
increase in the sensitivity and speciﬁcity of the metastasis and
hemangiomas classes. It seems that the synthetic data samples
generated from a given dataset distribution, using GAN, can
add additional variability to the input dataset (Figure 9), that
in turn leads to better performance.
Evaluations of the quality of the synthesized liver lesions
were made by two expert radiologists. Although the experiment was not conducted in the regular radiologist working
environment, and proved to be a challenging task for them, we
ﬁnd it of interest that both experts had the same classiﬁcation
accuracy results for the real set, as well as the synthesized
lesions set (Table III), indicating to us the validity of the lesion
generation process.
In this study, our goal was to assess to what extent synthesized lesions can improve the performance of another system
behind the scenes. Our results show that the synthesized
lesions have meaningful visualizations and more importantly
meaningful features and can be incorporated into computer
aided algorithms.
We tested another generative model that incorporated labels in the training process. Both GANs were trained using
supervised learning with liver lesion class labels. The DC-
GAN trained each lesion class separately while the ACGAN
trained all three lesion classes at once. In recent computer
vision studies , , training a GAN that combines label
information improved the visualization quality of samples over
GANs that did not utilize the label information to generate
samples of many classes together. Somewhat surprisingly,
we found that for our dataset, challenging the discriminator
network to perform two tasks (distinguishing real or fake and
classifying lesions into 3 categories), resulted in poor results in
comparison the DCGAN model. Using synthetic augmentation
generated using the ACGAN, we were not able to improve the
results over the CNN-AUG-GAN (Table IV).
As a ﬁnal experiment, we compared the performance of the
CNN - based system which we propose in this work, to nonnetwork state-of-the-art methods for liver lesion classiﬁcation
(Table V). Our suggested CNN architecture for classiﬁcation
that employs classic augmentation performed on a par with
the BoVW-MI method with the same ROI input. Using
synthetic data augmentation in our CNN architecture led to
the best performance.
There are several limitations to this work. One possible
extension could be an increase from 2-D to 3-D input volumes,
using 3-D analysis CNN. We trained separate GANs for each
lesion class which increased the training complexity. Investigation of GAN architectures that generate multi-class samples
together would be worthwhile. The quality of the generated
lesion samples could possibly be improved by incorporating
unlabeled data to improve the GAN learning process .
Further analysis into modiﬁcations of the training loss to
incorporate regularization terms for the L1-norm or L2-norm,
can be investigated as well , . In the future, we plan
to extend our work to additional medical domains that can
beneﬁt from synthesis of lesions for improved training.
In conclusion, we presented a method that uses the generation of synthetic medical images for data augmentation to
improve performance on a medical problem with limited data.
We demonstrated this technique on a liver lesion classiﬁcation
task and achieved an improvement of ∼7% using synthetic
augmentation over the classic augmentation. We introduced
a CNN-based architecture for the liver lesion classiﬁcation
task, that achieves state-of-the-art results. We believe that
other medical problems can beneﬁt from using synthetic
augmentation, and that the presented approach can lead to
stronger and more robust radiology support systems.