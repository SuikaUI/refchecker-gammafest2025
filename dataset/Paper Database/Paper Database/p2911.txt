Channel Pruning for Accelerating Very Deep Neural Networks
Xi’an Jiaotong University
Xi’an, 710049, China
 
Xiangyu Zhang
Megvii Inc.
Beijing, 100190, China
 
Megvii Inc.
Beijing, 100190, China
 
In this paper, we introduce a new channel pruning
method to accelerate very deep convolutional neural networks.
Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer,
by a LASSO regression based channel selection and least
square reconstruction. We further generalize this algorithm
to multi-layer and multi-branch cases.
Our method reduces the accumulated error and enhance the compatibility
with various architectures. Our pruned VGG-16 achieves
the state-of-the-art results by 5× speed-up along with only
0.3% increase of error. More importantly, our method is
able to accelerate modern networks like ResNet, Xception
and suffers only 1.4%, 1.0% accuracy loss under 2× speedup respectively, which is signiﬁcant. Code has been made
publicly available1.
1. Introduction
Recent CNN acceleration works fall into three categories: optimized implementation (e.g., FFT ), quantization (e.g., BinaryNet ), and structured simpliﬁcation
that convert a CNN into compact one . This work focuses on the last one.
Structured simpliﬁcation mainly involves: tensor factorization , sparse connection , and channel pruning .
Tensor factorization factorizes a convolutional
layer into several efﬁcient ones (Fig. 1(c)). However, feature map width (number of channels) could not be reduced,
which makes it difﬁcult to decompose 1 × 1 convolutional
layer favored by modern networks (e.g., GoogleNet ,
ResNet , Xception ). This type of method also introduces extra computation overhead. Sparse connection deactivates connections between neurons or channels (Fig. 1(b)).
Though it is able to achieves high theoretical speed-up ratio,
the sparse convolutional layers have an ”irregular” shape
*This work was done when Yihui He was an intern at Megvii Inc.
1 
number of channels
(a) (b) (c) (d)
Figure 1. Structured simpliﬁcation methods that accelerate CNNs:
(a) a network with 3 conv layers. (b) sparse connection deactivates some connections between channels. (c) tensor factorization
factorizes a convolutional layer into several pieces. (d) channel
pruning reduces number of channels in each layer (focus of this
which is not implementation friendly. In contrast, channel
pruning directly reduces feature map width, which shrinks
a network into thinner one, as shown in Fig. 1(d). It is efﬁcient on both CPU and GPU because no special implementation is required.
Pruning channels is simple but challenging because removing channels in one layer might dramatically change
the input of the following layer. Recently, training-based
channel pruning works have focused on imposing
sparse constrain on weights during training, which could
adaptively determine hyper-parameters. However, training
from scratch is very costly and results for very deep CNNs
on ImageNet have been rarely reported. Inference-time attempts have focused on analysis of the importance
of individual weight. The reported speed-up ratio is very
In this paper, we propose a new inference-time approach
for channel pruning, utilizing redundancy inter channels.
Inspired by tensor factorization improvement by feature
maps reconstruction , instead of analyzing ﬁlter weights
 , we fully exploits redundancy inter feature maps.
Speciﬁcally, given a trained CNN model, pruning each layer
 
Figure 2. Channel pruning for accelerating a convolutional layer.
We aim to reduce the number of channels of feature map B, while
minimizing the reconstruction error on feature map C. Our optimization algorithm (Sec. 3.1) performs within the dotted box,
which does not involve nonlinearity. This ﬁgure illustrates the situation that two channels are pruned for feature map B. Thus corresponding channels of ﬁlters W can be removed. Furthermore,
even though not directly optimized by our algorithm, the corresponding ﬁlters in the previous layer can also be removed (marked
by dotted ﬁlters). c, n: number of channels for feature maps B and
C, kh × kw: kernel size.
is achieved by minimizing reconstruction error on its output
feature maps, as showned in Fig. 2. We solve this minimization problem by two alternative steps: channels selection and feature map reconstruction. In one step, we ﬁgure
out the most representative channels, and prune redundant
ones, based on LASSO regression. In the other step, we
reconstruct the outputs with remaining channels with linear
least squares. We alternatively take two steps. Further, we
approximate the network layer-by-layer, with accumulated
error accounted. We also discuss methodologies to prune
multi-branch networks (e.g., ResNet , Xception ).
For VGG-16, we achieve 4× acceleration, with only
1.0% increase of top-5 error. Combined with tensor factorization, we reach 5× acceleration but merely suffer 0.3%
increase of error, which outperforms previous state-of-thearts. We further speed up ResNet-50 and Xception-50 by
2× with only 1.4%, 1.0% accuracy loss respectively.
2. Related Work
There has been a signiﬁcant amount of work on accelerating CNNs. Many of them fall into three categories: optimized implementation , quantization , and structured
simpliﬁcation .
Optimized implementation based methods 
accelerate convolution, with special convolution algorithms
like FFT . Quantization reduces ﬂoating point
computational complexity.
Sparse connection eliminates connections between neurons . prunes connections based on
weights magnitude. could accelerate fully connected
layers up to 50×. However, in practice, the actual speed-up
maybe very related to implementation.
Tensor factorization decompose weights
into several pieces. accelerate fully connected
layers with truncated SVD. factorize a layer into 3 × 3
and 1 × 1 combination, driven by feature map redundancy.
Channel pruning removes redundant channels on feature
maps. There are several training-based approaches. regularize networks to improve accuracy. Channel-wise
SSL reaches high compression ratio for ﬁrst few conv
layers of LeNet and AlexNet . could work
well for fully connected layers. However, training-based
approaches are more costly, and the effectiveness for very
deep networks on large datasets is rarely exploited.
Inference-time channel pruning is challenging, as reported by previous works . Some works 
focus on model size compression, which mainly operate the
fully connected layers. Data-free approaches results
for speed-up ratio (e.g., 5×) have not been reported, and
requires long retraining procedure. select channels via
over 100 random trials, however it need long time to evaluate each trial on a deep network, which makes it infeasible
to work on very deep models and large datasets. is even
worse than naive solution from our observation sometimes
(Sec. 4.1.1).
3. Approach
In this section, we ﬁrst propose a channel pruning algorithm for a single layer, then generalize this approach to
multiple layers or the whole model. Furthermore, we discuss variants of our approach for multi-branch networks.
3.1. Formulation
Fig. 2 illustrates our channel pruning algorithm for a single convolutional layer. We aim to reduce the number of
channels of feature map B, while maintaining outputs in
feature map C. Once channels are pruned, we can remove
corresponding channels of the ﬁlters that take these channels as input. Also, ﬁlters that produce these channels can
also be removed. It is clear that channel pruning involves
two key points.
The ﬁrst is channel selection, since we
need to select proper channel combination to maintain as
much information. The second is reconstruction. We need
to reconstruct the following feature maps using the selected
Motivated by this, we propose an iterative two-step algorithm. In one step, we aim to select most representative
channels. Since an exhaustive search is infeasible even for
tiny networks, we come up with a LASSO regression based
method to ﬁgure out representative channels and prune redundant ones. In the other step, we reconstruct the outputs
with remaining channels with linear least squares. We alternatively take two steps.
Formally, to prune a feature map with c channels, we
consider applying n×c×kh×kw convolutional ﬁlters W on
N ×c×kh ×kw input volumes X sampled from this feature
map, which produces N × n output matrix Y. Here, N is
the number of samples, n is the number of output channels,
and kh, kw are the kernel size. For simple representation,
bias term is not included in our formulation. To prune the
input channels from c to desired c′ (0 ≤c′ ≤c), while
minimizing reconstruction error, we formulate our problem
as follow:
subject to ∥β∥0 ≤c′
∥·∥F is Frobenius norm. Xi is N × khkw matrix sliced
from ith channel of input volumes X, i = 1, ..., c. Wi is
n × khkw ﬁlter weights sliced from ith channel of W. β
is coefﬁcient vector of length c for channel selection, and
βi (ith entry of β) is a scalar mask to ith channel (i.e. to
drop the whole channel or not). Notice that, if βi = 0, Xi
will be no longer useful, which could be safely pruned from
feature map. Wi could also be removed. c′ is the number
of retained channels, which is manually set as it can be calculated from the desired speed-up ratio. For whole-model
speed-up (i.e. Section 4.1.2), given the overall speed-up, we
ﬁrst assign speed-up ratio for each layer then calculate each
Optimization
Solving this ℓ0 minimization problem in Eqn. 1 is NP-hard.
Therefore, we relax the ℓ0 to ℓ1 regularization:
subject to ∥β∥0 ≤c′, ∀i ∥Wi∥F = 1
λ is a penalty coefﬁcient. By increasing λ, there will be
more zero terms in β and one can get higher speed-up ratio.
We also add a constrain ∀i ∥Wi∥F = 1 to this formulation,
which avoids trivial solution.
Now we solve this problem in two folds. First, we ﬁx W,
solve β for channel selection. Second, we ﬁx β, solve W to
reconstruct error.
(i) The subproblem of β. In this case, W is ﬁxed. We
solve β for channel selection. This problem can be solved
by LASSO regression , which is widely used for
model selection.
LASSO(λ) = arg min
subject to ∥β∥0 ≤c′
Here Zi = XiWi
⊤(size N×n). We will ignore ith channels
if βi = 0.
(ii) The subproblem of W. In this case, β is ﬁxed. We
utilize the selected channels to minimize reconstruction error. We can ﬁnd optimized solution by least squares:
Y −X′(W′)⊤
βcXc] (size
N × ckhkw).
W′ is n × ckhkw reshaped W, W′ =
[W1 W2 ... Wi ... Wc]. After obtained result W′, it is reshaped back to W. Then we assign βi ←βi ∥Wi∥F , Wi ←
Wi/ ∥Wi∥F . Constrain ∀i ∥Wi∥F = 1 satisﬁes.
We alternatively optimize (i) and (ii). In the beginning,
W is initialized from the trained model, λ = 0, namely no
penalty, and ∥β∥0 = c. We gradually increase λ. For each
change of λ, we iterate these two steps until ∥β∥0 is stable.
After ∥β∥0 ≤c′ satisﬁes, we obtain the ﬁnal solution W
from {βiWi}. In practice, we found that the two steps iteration is time consuming. So we apply (i) multiple times,
until ∥β∥0 ≤c′ satisﬁes. Then apply (ii) just once, to obtain
the ﬁnal result. From our observation, this result is comparable with two steps iteration’s. Therefore, in the following
experiments, we adopt this approach for efﬁciency.
Discussion: Some recent works (though training based) also introduce ℓ1-norm or LASSO. However, we
must emphasis that we use different formulations. Many of
them introduced sparsity regularization into training loss,
instead of explicitly solving LASSO. Other work solved
LASSO, while feature maps or data were not considered
during optimization. Because of these differences, our approach could be applied at inference time.
3.2. Whole Model Pruning
Inspired by , we apply our approach layer by layer
sequentially. For each layer, we obtain input volumes from
the current input feature map, and output volumes from the
output feature map of the un-pruned model. This could be
formalized as:
subject to ∥β∥0 ≤c′
Different from Eqn. 1, Y is replaced by Y′, which is from
feature map of the original model. Therefore, the accumulated error could be accounted during sequential pruning.
3.3. Pruning Multi-Branch Networks
The whole model pruning discussed above is enough for
single-branch networks like LeNet , AlexNet and
VGG Nets . However, it is insufﬁcient for multi-branch
networks like GoogLeNet and ResNet . We mainly
focus on pruning the widely used residual structure (e.g.,
ResNet , Xception ). Given a residual block shown
in Fig. 3 (left), the input bifurcates into shortcut and residual
Input (c0)
sampled (c0' )
Figure 3. Illustration of multi-branch enhancement for residual
block. Left: original residual block. Right: pruned residual block
with enhancement, cx denotes the feature map width. Input channels of the ﬁrst convolutional layer are sampled, so that the large
input feature map width could be reduced. As for the last layer,
rather than approximate Y2, we try to approximate Y1 + Y2 directly (Sec. 3.3 Last layer of residual branch).
branch. On the residual branch, there are several convolutional layers (e.g., 3 convolutional layers which have spatial
size of 1 × 1, 3 × 3, 1 × 1, Fig. 3, left). Other layers except the ﬁrst and last layer can be pruned as is described
previously. For the ﬁrst layer, the challenge is that the large
input feature map width (for ResNet, 4 times of its output)
can’t be easily pruned, since it’s shared with shortcut. For
the last layer, accumulated error from the shortcut is hard to
be recovered, since there’s no parameter on the shortcut. To
address these challenges, we propose several variants of our
approach as follows.
Last layer of residual branch: Shown in Fig. 3, the
output layer of a residual block consists of two inputs: feature map Y1 and Y2 from the shortcut and residual branch.
We aim to recover Y1 + Y2 for this block. Here, Y1, Y2
are the original feature maps before pruning. Y2 could be
approximated as in Eqn. 1. However, shortcut branch is
parameter-free, then Y1 could not be recovered directly. To
compensate this error, the optimization goal of the last layer
is changed from Y2 to Y1−Y′
1+Y2, which does not change
our optimization. Here, Y′
1 is the current feature map after
previous layers pruned. When pruning, volumes should be
sampled correspondingly from these two branches.
Illustrated
Fig. 3(left), the input feature map of the residual block
could not be pruned, since it is also shared with the shortcut branch.
In this condition, we could perform feature
map sampling before the ﬁrst convolution to save computation. We still apply our algorithm as Eqn. 1. Differently,
we sample the selected channels on the shared feature maps
to construct a new input for the later convolution, shown
in Fig. 3(right). Computational cost for this operation could
be ignored. More importantly, after introducing feature map
sampling, the convolution is still ”regular”.
Filter-wise pruning is another option for the ﬁrst convolution on the residual branch. Since the input channels
of parameter-free shortcut branch could not be pruned, we
apply our Eqn. 1 to each ﬁlter independently (each ﬁlter chooses its own representative input channels). Under
single layer acceleration, ﬁlter-wise pruning is more accurate than our original one. From our experiments, it improves 0.5% top-5 accuracy for 2× ResNet-50 (applied on
the ﬁrst layer of each residual branch) without ﬁne-tuning.
However, after ﬁne-tuning, there’s no noticeable improvement. In addition, it outputs ”irregular” convolutional layers, which need special library implementation support. We
do not adopt it in the following experiments.
4. Experiment
We evaluation our approach for the popular VGG Nets
 , ResNet , Xception on ImageNet , CIFAR-
10 and PASCAL VOC 2007 .
For Batch Normalization , we ﬁrst merge it into convolutional weights, which do not affect the outputs of the
networks. So that each convolutional layer is followed by
ReLU . We use Caffe for deep network evaluation,
and scikit-learn for solvers implementation. For channel pruning, we found that it is enough to extract 5000 images, and 10 samples per image, which is also efﬁcient (i.e.
several minutes for VGG-16 2). On ImageNet, we evaluate the top-5 accuracy with single view. Images are resized
such that the shorter side is 256. The testing is on center
crop of 224 × 224 pixels. We could gain more performance
with ﬁne-tuning. We use a batch size of 128 and learning
rate 1e−5. We ﬁne-tune our pruned models for 10 epoches
(less than 1/10 iterations of training from scratch). The augmentation for ﬁne-tuning is random crop of 224 × 224 and
4.1. Experiments with VGG-16
VGG-16 is a 16 layers single-branch convolutional
neural network, with 13 convolutional layers. It is widely
used in recognition, detection and segmentation, etc. Single
view top-5 accuracy for VGG-16 is 89.9%3.
Single Layer Pruning
In this subsection, we evaluate single layer acceleration performance using our algorithm in Sec. 3.1. For better understanding, we compare our algorithm with two naive channel selection strategies. ﬁrst k selects the ﬁrst k channels.
max response selects channels based on corresponding ﬁlters that have high absolute weights sum . For fair comparison, we obtain the feature map indexes selected by each
2On Intel Xeon E5-2670 CPU
3 
increase of error (%)
max response
max response
max response
speed-up ratio
increase of error (%)
max response
speed-up ratio
max response
speed-up ratio
max response
Figure 4. Single layer performance analysis under different speed-up ratios (without ﬁne-tuning), measured by increase of error. To verify
the importance of channel selection refered in Sec. 3.1, we considered two naive baselines. ﬁrst k selects the ﬁrst k feature maps. max
response selects channels based on absolute sum of corresponding weights ﬁlter . Our approach is consistently better (smaller is
of them, then perform reconstruction (Sec. 3.1 (ii)). We
hope that this could demonstrate the importance of channel
selection. Performance is measured by increase of error after a certain layer is pruned without ﬁne-tuning, shown in
As expected, error increases as speed-up ratio increases.
Our approach is consistently better than other approaches in
different convolutional layers under different speed-up ratio. Unexpectedly, sometimes max response is even worse
than ﬁrst k. We argue that max response ignores correlations between different ﬁlters. Filters with large absolute
weight may have strong correlation. Thus selection based
on ﬁlter weights is less meaningful. Correlation on feature
maps is worth exploiting. We can ﬁnd that channel selection
affects reconstruction error a lot. Therefore, it is important
for channel pruning.
Also notice that channel pruning gradually becomes
hard, from shallower to deeper layers. It indicates that shallower layers have much more redundancy, which is consistent with . We could prune more aggressively on shallower layers in whole model acceleration.
Increase of top-5 error (1-view, baseline 89.9%)
Jaderberg et al. ( ’s impl.)
Asym. 
Filter pruning 
(ﬁne-tuned, our impl.)
Ours (without ﬁne-tune)
Ours (ﬁne-tuned)
Table 1. Accelerating the VGG-16 model using a speedup
ratio of 2×, 4×, or 5× (smaller is better).
Whole Model Pruning
Shown in Table 1, whole model acceleration results under
2×, 4×, 5× are demonstrated.
We adopt whole model
pruning proposed in Sec. 3.2.
Guided by single layer
experiments above, we prune more aggressive for shallower layers. Remaining channels ratios for shallow layers (conv1_x to conv3_x) and deep layers (conv4_x)
is 1 : 1.5. conv5_x are not pruned, since they only contribute 9% computation in total and are not redundant.
After ﬁne-tuning, we could reach 2× speed-up without
losing accuracy. Under 4×, we only suffers 1.0% drops.
Consistent with single layer analysis, our approach outper-
Increase of top-5 error (1-view, 89.9%)
Asym. 3D 
Asym. 3D (ﬁne-tuned) 
Our 3C (ﬁne-tuned)
Table 2. Performance of combined methods on the VGG-16 model
 using a speed-up ratio of 4× or 5×. Our 3C solution outperforms previous approaches (smaller is better).
forms previous channel pruning approach (Li et al. ) by
large margin. This is because we fully exploits channel redundancy within feature maps. Compared with tensor factorization algorithms, our approach is better than Jaderberg
et al. , without ﬁne-tuning. Though worse than Asym.
 , our combined model outperforms its combined Asym.
3D (Table 2). This may indicate that channel pruning is
more challenging than tensor factorization, since removing
channels in one layer might dramatically change the input
of the following layer. However, channel pruning keeps the
original model architecture, do not introduce additional layers, and the absolute speed-up ratio on GPU is much higher
(Table 3).
Since our approach exploits a new cardinality, we further
combine our channel pruning with spatial factorization 
and channel factorization . Demonstrated in Table 2,
our 3 cardinalities acceleration (spatial, channel factorization, and channel pruning, denoted by 3C) outperforms previous state-of-the-arts. Asym. 3D (spatial and channel factorization), factorizes a convolutional layer to three
parts: 1 × 3, 3 × 1, 1 × 1.
We apply spatial factorization, channel factorization, and
our channel pruning together sequentially layer-by-layer.
We ﬁne-tune the accelerated models for 20 epoches, since
they are 3 times deeper than the original ones. After ﬁnetuning, our 4× model suffers no degradation. Clearly, a
combination of different acceleration techniques is better
than any single one. This indicates that a model is redundant in each cardinality.
Comparisons of Absolute Performance
We further evaluate absolute performance of acceleration
on GPU. Results in Table 3 are obtained under Caffe ,
CUDA8 and cuDNN5 , with a mini-batch of 32
on a GPU (GeForce GTX TITAN X). Results are averaged
from 50 runs. Tensor factorization approaches decompose
weights into too many pieces, which heavily increase overhead. They could not gain much absolute speed-up. Though
our approach also encountered performance decadence, it
generalizes better on GPU than other approaches. Our results for tensor factorization differ from previous research
 , maybe because current library and hardware prefer single large convolution instead of several small ones.
Comparisons with Training from Scratch
Though training a compact model from scratch is timeconsuming (usually 120 epoches), it worths comparing our
approach and from scratch counterparts. To be fair, we evaluated both from scratch counterpart, and normal setting network that has the same computational complexity and same
architecture.
Shown in Table 4, we observed that it’s difﬁcult for
from scratch counterparts to reach competitive accuracy.
our model outperforms from scratch one.
Our approach
successfully picks out informative channels and constructs
highly compact models. We can safely draw the conclusion that the same model is difﬁcult to be obtained from
scratch. This coincides with architecture design researches
 that the model could be easier to train if there are
more channels in shallower layers. However, channel pruning favors shallower layers.
For from scratch (uniformed), the ﬁlters in each layers
is reduced by half (eg. reduce conv1_1 from 64 to 32).
We can observe that normal setting networks of the same
complexity couldn’t reach same accuracy either. This consolidates our idea that there’s much redundancy in networks
while training.
However, redundancy can be opt out at
inference-time. This maybe an advantage of inference-time
acceleration approaches over training-based approaches.
Notice that there’s a 0.6% gap between the from scratch
model and uniformed one, which indicates that there’s room
for model exploration.
Adopting our approach is much
faster than training a model from scratch, even for a thinner one. Further researches could alleviate our approach to
do thin model exploring.
Acceleration for Detection
VGG-16 is popular among object detection tasks . We evaluate transfer learning ability of our 2×/4×
pruned VGG-16, for Faster R-CNN object detections.
PASCAL VOC 2007 object detection benchmark contains 5k trainval images and 5k test images. The performance is evaluated by mean Average Precision (mAP) and
mmAP (primary challenge metric of COCO ). In our
experiments, we ﬁrst perform channel pruning for VGG-16
on the ImageNet. Then we use the pruned model as the
pre-trained model for Faster R-CNN.
The actual running time of Faster R-CNN is 220ms / image. The convolutional layers contributes about 64%. We
got actual time of 94ms for 4× acceleration.
From Table 5, we observe 0.4% mAP drops of our 2× model, which
is not harmful for practice consideration. Observed from
mmAP, For higher localization requirements our speedup
model does not suffer from large degradation.
Increased err.
GPU time/ms
VGG-16 (4×)
Jaderberg et al. ( ’s impl.)
8.051 (1.01×)
Asym. 
5.244 (1.55×)
Asym. 3D 
8.503 (0.96×)
Asym. 3D (ﬁne-tuned) 
8.503 (0.96×)
Ours (ﬁne-tuned)
3.264 (2.50×)
Table 3. GPU acceleration comparison. We measure forward-pass time per image. Our approach generalizes well on GPU (smaller is
Original (acc. 89.9%)
Top-5 err.
Increased err.
From scratch
From scratch (uniformed)
Ours (ﬁne-tuned)
Table 4. Comparisons with training from scratch, under 4× acceleration. Our ﬁne-tuned model outperforms scratch trained counterparts (smaller is better).
Table 5. 2×, 4× acceleration for Faster R-CNN detection. mmAP
is AP at IoU=.50:.05:.95 (primary challenge metric of COCO
Increased err.
(enhanced)
(enhanced, ﬁne-tuned)
Table 6. 2× acceleration for ResNet-50 on ImageNet, the baseline network’s top-5 accuracy is 92.2% (one view). We improve
performance with multi-branch enhancement (Sec. 3.3, smaller is
4.2. Experiments with Residual Architecture Nets
For Multi-path networks , we further explore
the popular ResNet and latest Xception , on ImageNet and CIFAR-10. Pruning residual architecture nets is
more challenging. These networks are designed for both ef-
ﬁciency and high accuracy. Tensor factorization algorithms
 have difﬁcult to accelerate these model. Spatially,
1 × 1 convolution is favored, which could hardly be factorized.
ResNet Pruning
ResNet complexity uniformly drops on each residual block.
Guided by single layer experiments (Sec. 4.1.1), we still
prefer reducing shallower layers heavier than deeper ones.
Increased err.
Filter pruning (our impl.)
Filter pruning 
(ﬁne-tuned, our impl.)
Ours (ﬁne-tuned)
Table 7. Comparisons for Xception-50, under 2× acceleration ratio. The baseline network’s top-5 accuracy is 92.8%. Our approach outperforms previous approaches. Most structured simpliﬁcation methods are not effective on Xception architecture
(smaller is better).
Following similar setting as Filter pruning , we
keep 70% channels for sensitive residual blocks (res5
and blocks close to the position where spatial size
change, e.g. res3a,res3d).
As for other blocks,
we keep 30% channels.
With multi-branch enhancement, we prune branch2a more aggressively within
each residual block.
The preserving channels ratios for
branch2a,branch2b,branch2c is 2 : 4 : 3 (e.g.,
Given 30%, we keep 40%, 80%, 60% respectively).
We evaluate performance of multi-branch variants of our
approach (Sec.
From Table 6, we improve 4.0%
with our multi-branch enhancement. This is because we
accounted the accumulated error from shortcut connection
which could broadcast to every layer after it. And the large
input feature map width at the entry of each residual block
is well reduced by our feature map sampling.
Xception Pruning
Since computational complexity becomes important in
model design, separable convolution has been payed much
attention . Xception is already spatially optimized
and tensor factorization on 1 × 1 convolutional layer is destructive. Thanks to our approach, it could still be accelerated with graceful degradation. For the ease of comparison,
we adopt Xception convolution on ResNet-50, denoted by
Xception-50. Based on ResNet-50, we swap all convolutional layers with spatial conv blocks. To keep the same
computational complexity, we increase the input channels
of all branch2b layers by 2×. The baseline Xception-
50 has a top-5 accuracy of 92.8% and complexity of 4450
Increased err.
Filter pruning 
(ﬁne-tuned, our impl.)
From scratch
Ours (ﬁne-tuned)
Table 8. 2× speed-up comparisons for ResNet-56 on CIFAR-10,
the baseline accuracy is 92.8% (one view). We outperforms previous approaches and scratch trained counterpart (smaller is better).
We apply multi-branch variants of our approach as described in Sec. 3.3, and adopt the same pruning ratio setting
as ResNet in previous section. Maybe because of Xception block is unstable, Batch Normalization layers must be
maintained during pruning. Otherwise it becomes nontrivial
to ﬁne-tune the pruned model.
Shown in Table 7, after ﬁne-tuning, we only suffer 1.0%
increase of error under 2×. Filter pruning could also
apply on Xception, though it is designed for small speedup ratio. Without ﬁne-tuning, top-5 error is 100%. After
training 20 epochs which is like training from scratch, increased error reach 4.3%. Our results for Xception-50 are
not as graceful as results for VGG-16, since modern networks tend to have less redundancy by design.
Experiments on CIFAR-10
Even though our approach is designed for large datasets, it
could generalize well on small datasets. We perform experiments on CIFAR-10 dataset , which is favored by
many acceleration researches. It consists of 50k images for
training and 10k for testing in 10 classes.
We reproduce ResNet-56, which has accuracy of 92.8%
(Serve as a reference, the ofﬁcial ResNet-56 has accuracy of 93.0%). For 2× acceleration, we follow similar
setting as Sec. 4.2.1 (keep the ﬁnal stage unchanged, where
the spatial size is 8 × 8). Shown in Table 8, our approach
is competitive with scratch trained one, without ﬁne-tuning,
under 2× speed-up. After ﬁne-tuning, our result is significantly better than Filter pruning and scratch trained
5. Conclusion
To conclude, current deep CNNs are accurate with high
inference costs.
In this paper, we have presented an
inference-time channel pruning method for very deep networks. The reduced CNNs are inference efﬁcient networks
while maintaining accuracy, and only require off-the-shelf
libraries. Compelling speed-ups and accuracy are demonstrated for both VGG Net and ResNet-like networks on ImageNet, CIFAR-10 and PASCAL VOC.
In the future, we plan to involve our approaches into
training time, instead of inference time only, which may
also accelerate training procedure.