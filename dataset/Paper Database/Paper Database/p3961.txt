Lecture Notes in Statistics
Edited by J. Berger, S. Fienberg, J. Gani, K. Krickeberg,
I. OIkin, and B. Singer
Peter Spirtes
Clark Glymour
Richard Scheines
Causation, Prediction,
and Search
Springer-Verlag
New York Berlin Heidelberg London Paris
Tokyo Hong Kong Barcelona Budapest
Peter Spirtes
Clark Glymour
Richard Scheines
Department of Philosophy
Carnegie Mellon University
Pittsburgh, P A 15213
Mathematics Subject Classification: 62-07
library of Congress Cataloging-in-Publication Data
Spirtes, Peter
Causation, prediction, and search I Peter Spirtes, Clarie Glymour
Richard Scheines
(Lecture notes in statistics ; 81)
ISBN-13:978-1-4612-7650-0
1. Mathematical statistics
I. Glymour, Clarie N. ll. Scheines,
In. Title.
IV. Series: Lecture notes in statistics
(Springer-Verlag) : v. 81.
519.5--dc20
Printed on acid-free paper.
© 1993 Springer-Verlag New Yorlc, Inc.
All rights reserved. This worie may not be translated or copied in whole orin part without the written
permission of the publisher (Springer-Verlag New Yorie, Inc., 175 Fifth Avenue, New Yorie, NY
10010, USA), except for brief excetpts in connection with reviews or scholarly analysis. Use in
connection with any foon of infonnation storage and retrieval, electronic adaptation, computer
software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.
The use of general descriptive names, trade names, trademaries, etc., in this publication, even if the
fonner are not especially identified, is not to be taken as a sign that such names, as understood by the
Trade Maries and Merchandise Maries Act, may accordingly be used freely by anyone.
Camera ready copy provided by the authors.
9 8 765 432 1
ISBN-13:978-1-4612-7650-0
e-ISBN-13:978-1-4612-2748-9
DOl: 10.1007/978-1-4612-2748-9
To my parents, Morris and Cecile Spirtes - P.S.
In memory of Lucille Lynch Schwartz Watkins Speede Tindall Preston - C. O.
To Martha, for her support and love - R.S.
It is with data affected by numerous causes that Statistics is
mainly concerned. Experiment seeks to disentangle a complex of
causes by removing all but one of them, or rather by
concentrating on the study of one and reducing the others, as far
as circumstances permit, to comparatively small residium.
Statistics, denied this resource, must accept for analysis data
subject to the influence of a host of causes, and must try to
discover from the data themselves which causes are the
important ones and how much of the observed effect is due to the
operation of each.
--G. U. Yule and M. G. Kendall 1950
The Theory of Estimation discusses the principles upon which
observational data may be used to estimate, or to throw light
upon the values of theoretical quantities, not known numerically,
which enter into our specification of the causal system
operating.
-- Sir Ronald Fisher, 1956
George Box has [almost] said "The only way to find out what will
happen when a complex system is disturbed is to disturb the
system, not merely to observe it passively." These words of
caution about "natural experiments" are uncomfortably strong.
Yet in today's world we see no alternative to accepting them as,
if anything, too weak.
--G. Mosteller and J. Tukey, 1977
Causal inference is one of the most important, most subtle, and
most neglected of all the problems of Statistics.
-- P. Dawid, 1979
This book is intended for anyone, regardless of discipline, who is interested in the use of
statistical methods to help obtain scientific explanations or to predict the outcomes of actions,
experiments or policies.
Much of G. Udny Yule's work illustrates a vision of statistics whose goal is to investigate
when and how causal influences may be reliably inferred, and their comparative strengths
estimated, from statistical samples. Yule's enterprise has been largely replaced by Ronald
Fisher's conception, in which there is a fundamental cleavage between experimental and nonexperimental inquiry, and statistics is largely unable to aid in causal inference without
randomized experimental trials. Every now and then members of the statistical community
express misgivings about this turn of events, and, in our view, rightly so. Our work represents
a return to something like Yule's conception of the enterprise of theoretical statistics and its
potential practical benefits.
If intellectual history in the 20th century had gone otherwise, there might have been a
discipline to which our work belongs. As it happens, there is not. We develop material that
belongs to statistics, to computer science, and to philosophy; the combination may not be
entirely satisfactory for specialists in any of these subjects. We hope it is nonetheless
satisfactory for its purpose. We are not statisticians by training or by association, and perhaps
for that reason we tend to look at issues differently, and, from the perspective common in the
discipline, no doubt oddly. We are struck by the fact that in the social and behavioral sciences,
epidemiology, economics, market research, engineering, and even applied physics, statistical
methods are routinely used to justify causal inferences from data not obtained from
randomized experiments, and sample statistics are used to predict the effects of policies,
manipulations or experiments. Without these uses the profession of statistics would be a far
smaller business. It may not strike many professional statisticians as particularly odd that the
discipline thriving from such uses assures its audience that they are unwarranted, but it strikes
us as very odd indeed. From our perspective outside the discipline, the most urgent questions
about the application of statistics to such ends concern the conditions under which causal
inferences and predictions of the effects of manipulations can and cannot reliably be made,
and the most urgent need is a principled, rigorous theory with which to address these
Causation, Prediction, and Search
problems. To judge from the testimony of their books, a good many statisticians think any
such theory is impossible. We think the common arguments against the possibility of inferring
causes from statistics outside of experimental trials are unsound, and radical separations of the
principles of experimental and observational study designs are unwise. Experimental and
observational design may not always permit the same inferences, but they are subject to
uniform principles.
The theory we develop follows necessarily from assumptions laid down in the statistical
community over the last fifteen years. The underlying structure of the theory is essentially
axiomatic. We will give two independent axioms on the relation between causal structures
and probability distributions and deduce from them features of causal relationships and
predictions that can and that cannot be reliably inferred from statistical constraints under a
variety of background assumptions. Versions of all of the axioms can be found in papers by
Lauritzen, Wermuth, Speed, Pearl, Rubin, Pratt, Schlaifer, and others. In most cases we will
develop the theory in terms of probability distributions that can be thought of loosely as
propensities that determine long run frequencies, but many of the probability distributions can
alternatively be understood as (normative) subjective degrees of belief, and we will
occasionally note Bayesian applications. From the axioms there follow a variety of theorems
concerning estimation, sampling, latent variable existence and structure, regression,
indistinguishability relations, experimental design, prediction, Simpson's paradox, and other
topics. Foremost among the "other topics" are the discovery that statistical methods
commonly used for causal inference are radically suboptimal, and that there exist
asymptotically reliable, computationally efficient search procedures that conjecture causal
relationships from the outcomes of statistical decisions made on the basis of sample data.
(The procedures we will describe require statistical decisions about the independence of
random variables; when we say such a procedure is "asymptotically reliable" we mean it
provides correct information if the outcome of each of the requisite statistical decisions is true
in the population under study.)
This much of the book is mathematics: where the axioms are accepted, so must the theorems
be, including the existence of search procedures. The procedures we describe are applicable
to both linear and discrete data and can be feasibly applied to a hundred or more variables so
long as the causal relations between the variables are sufficiently sparse and the sample
sufficiently large. These procedures have been implemented in a computer program,
TETRAD II, which at the time of writing is publicly available.!
The theorems concerning the existence and properties of reliable discovery procedures of
themselves tell us nothing about the reliabilities of the search procedures in the short run. The
methods we describe require an unpredictable sequence of statistical decisions, which we
have implemented as hypothesis tests. As is usual in such cases, in small samples the
conventional p values of the individual tests may not provide good estimates of type I error
probabilities for the search methods. We provide the results of extensive tests of various
procedures on simulated data using Monte Carlo methods, and these tests give considerable
evidence about reliability under the conditions of the simulations. The simulations illustrate
an easy method for estimating the probabilities of error for any of the search methods we
describe. The book also contains studies of one large pseudo-empirical data set--a body of
simulated data created by medical researchers to model emergency medicine diagnostic
indicators and their causes--and a great many empirical data sets, most of which have been
discussed by other authors in the context of specification searches.
A further aim of this work is to show that a proper understanding of the relationship between
causality and probability can help to clarify diverse topics in the statistical literature,
including the comparative power of experimentation versus observation, Simpson's paradox,
errors in regression models, retrospective versus prospective sampling, the perils of variable
selection, and other topics. There are a number of relevant topics we do not consider. They
include problems of estimation with discrete latent variables, optimizing statistical decisions,
many details of sampling designs, time series, and a full theory of "non-recursive" causal
structures--i.e., finite graphical representations of systems with feedback.
Causation, Prediction and Search is not intended to be a textbook, and it is not fitted out with
the associated paraphernalia. There are open problems but no exercises. In a textbook
everything ought to be presented as if it were complete and tidy, even if it isn't. We make no
such pretenses in this book, and the chapters are rich in unsolved problems and open
questions. Textbooks don't usually pause much to argue points of view; we pause quite a lot.
The various theorems in this book often have a graph theoretic character; many of them are
long, difficult case arguments of a kind quite unfamiliar in statistics. In order not to interrupt
! To anyone with a workstation with a UNIX operating system, a PASCAL compiler and a network connection.
A less flexible version of the program is available for IBM compatible 80-386 and 80-486 personal computers.
Write Richard Scheines at .
Causation, Prediction, and Search
the flow of the discussion we have placed all proofs but one in a chapter at the end of the
book. In the few cases where detailed proofs are available in the published literature, we have
simply referred the reader to them. Where proofs of important results have not been published
or are not readily available we have given the demonstrations in some detail.
The structure of the book is as follows. Chapter I concerns the motivation for the book in the
context of current statistical practice and advertises some of the results. Chapter 2 introduces
the mathematical ideas necessary to the investigation, and Chapter 3 gives the formal
framework a causal interpretation, lays down the axioms, notes circumstances in which they
are likely to fail, and provides a few fundamental theorems. The next two chapters work out
the consequences of two of the axioms for some fundamental issues in contexts in which it is
known, or assumed, that there are no unmeasured common causes affecting measured
variables. In Chapter 4 we give graphical characterizations of necessary and sufficient
conditions for causal hypotheses to be statistically indistinguishable from one another in each
of several senses. In Chapter 5 we criticize features of model specification procedures
commonly recommended in statistics, and we describe feasible algorithms that from
properties of population distributions extract correct information about causal structure,
assuming the axioms apply and that no unmeasured common causes are at work. The
algorithms are illustrated for a variety of empirical and simulated samples. Chapter 6 extends
the analysis of Chapter 5 to contexts in which it cannot be assumed that no unmeasured
common causes act on measured variables. From both a theoretical and practical perspective,
this chapter and the next form the center of the book, but they are especially difficult. Chapter
7 addresses the fundamental issue of predicting the effects of manipulations, policies, or
experiments. As an easy corollary, the chapter unifies directed graphical models with Donald
Rubin's "counterfactual" framework for analyzing prediction. Chapter 8 applies the results of
the preceding chapters to the subject of regression. We argue that even when standard
statistical assumptions are satisfied multiple regression is a defective and unreliable way to
assess causal influence even in the large sample limit, and various automated regression
model specification searches only make matters worse. We show that the algorithms of
Chapter 6 are more reliable in principle, and we compare the performances of these
algorithms against various multiple regression procedures on a variety of simulated and
empirical data sets. Chapter 9 considers the design of empirical studies in the light of the
results of earlier chapters, including issues of retrospective and prospective sampling, the
comparative power of experimental and observational designs, selection of variables, and the
design of ethical clinical trials. The chapter concludes with a look back at some aspects of the
dispute over smoking and lung cancer. Chapters 10 and 11 further consider the linear case,
and analyze algorithms for discovering or elaborating causal relations among measured and
unmeasured variables in linear systems. Chapter 12 is a brief consideration of a variety of
open questions. Proofs are given in Chapter 13.
We have tried to make this work self-contained, but it is admittedly and unavoidably difficult
The reader will be aided by a previous reading of Pearl , Whittaker or
Neopolitan .
Acknowledgments
One source of the ideas in this book is in work we began ten years ago at the University of
Pittsburgh. We drew many ideas about causality, statistics and search from the psychometric,
economic and sociological literature, beginning with Charles Spearman's project at the tum of
the century and including the work of Herbert Simon, Hubert Blalock and Herbert Costner.
We obtained a new perspective on the enterprise from Judea Pearl's Probabilistic Reasoning
in Intelligent Systems, which appeared the next year. Although not principally concerned with
discovery, Pearl's book showed us how to connect conditional independence with causal
structure quite generally, and that connection proved essential to establishing general, relillble
discovery procedures. We have since profited from correspondence and conversation with
Pearl and with Dan Geiger and Thomas Verma, and from several of their papers. Pearl's work
drew on the papers of Wermuth , Kiiveri and Speed , Wermuth and Lauritzen
 , and Kiiveri, Speed and Carlin , which in the early 1980s had already provided
the foundations for a rigorous study of causal inference. Paul Holland introduced one of us to
the Rubin framework some years ago, but we only recently realized it's logical connections
with directed graphical models. We were further helped by J. Whittaker's excellent
account of the properties of undirected graphical models.
We have learned a great deal from Gregory Cooper at the University of Pittsburgh who
provided us with data, comments, Bayesian algorithms and the picture and description of the
ALARM network which we consider in several places. Over the years we have learned useful
things from Kenneth Bollen. Chris Meek provided essential help in obtaining an important
theorem that derives various claims made by Rubin, Pratt and Schlaifer from axioms on
directed graphical models.
Steve Fienberg and several students from Carnegie Mellon's department of statistics joined
with us in a seminar on graphical models from which we learned a great deal. We are
indebted to him for his openness, intelligence and helpfulness in our research, and to
Elizabeth Slate for guiding us through several papers in the Rubin framework. We are obliged
to Nancy Cartwright for her courteous but salient criticism of the approach taken in our
previous book and continued here. Her comments prompted our work on parameters in
Causation, Prediction, and Search
Chapter 4. We are indebted to Brian Skyrms for his interest and encouragement over many
years, and to Marek Druzdzel for helpful comments and encouragement. We have also been
helped by Linda Bouck, Ronald Christensen, Jan Callahan, David Papineau, John Earman,
Dan Hausman, Joe Hill, Michael Meyer, Teddy Seidenfeld, Dana Scott, Jay Kadane, Steven
Klepper, Herb Simon,Peter Slezak, Steve Sorensen, John Worrall and Andrea Woody. We are
indebted to Ernest Seneca for putting us in contact with Dr. Rick Linthurst, and we are
especially grateful to Dr. Linthurst for making his doctoral thesis available to us.
Our work has been supported by many institutions. They, and those who made decisions on
their behalf, deserve our thanks. They include Carnegie Mellon University, the National
Science Foundation programs in History and Philosophy of Science, in Economics, and in
Knowledge and Database Systems, the Office of Naval Research, the Navy Personnel
Research and Development Center, the John Simon Guggenheim Memorial Foundation,
Susan Chipman, Stanley Collyer, Helen Gigley, Peter Machamer, Steve Sorensen, Teddy
Seidenfeld and Ron Overmann. The Navy Personnel Research and Development Center
provided us the benefit of access to a number of challenging data analysis problems from
which we have learned a great deal.
Table of Contents
Preface ................................................................................................................................ vii
Acknowledgments .............................................................................................................. xiii
Notational Conventions ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• xxi
1. Introduction and Advertisement •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••.••..•••.••••.••• 1
1.1 The Issue ............................................................................................................ 1
1.2 Advertisements .................................................................................................. 10
1.2.1 Bayes Networks from the Data ........................................................ , .. 11
1.2.2 Structural Equation Models from the Data ......................................... 13
1.2.3 Selection of Regressors ....................................................................... 14
1.2.4 Causal Inference without Experiment ................................................ 17
1.2.5 The Structure of the Unobserved ........................................................ 19
1.3 Themes ............................................................................................................... 21
2. Formal Preliminaries ..................................................................................................... 25
2.1 Graphs ................................................................................................................ 25
2.2 Probability .......................................................................................................... 31
2.3 Graphs and Probability Distributions ................................................................ 32
2.3.1 Directed Acyclic Graphs ..................................................................... 32
2.3.2 Directed Independence Graphs ........................................................... 34
2.3.3 Faithfulness ......................................................................................... 35
2.3.4 d-separation ......................................................................................... 36
2.3.5 Linear Structures ................................................................................. 36
2.4 Undirected Independence Graphs ...................................................................... 37
2.5 Deterministic and Pseudo-Indeterministic Systems .......................................... 38
2.6 Background Notes ............................................................................................. 39
3. Causation and Prediction: Axioms and Explications ................................................ .41
3.1 Conditionals ....................................................................................................... 41
3.2 Causation ........................................................................................................... 42
3.2.1 Direct vs. Indirect Causation ............................................................. .42
3.2.2 Events and Variables .......................................................................... 43
3.2.3 Examples ............................................................................................. 45
3.2.4 Representing Causal Relations with Directed Graphs ........................ 47
Causation, Prediction, and Search
3.3 Causality and Probability ................................................................................... 49
3.3.1 Deterministic Causal Structures ........................................................ .49
3.3.2 Pseudo-Indeterministic and Indetenninistic Causal Structures .......... 51
3.4 The Axioms ....................................................................................................... 53
3.4.1 The Causal Markov Condition ............................................................ 53
3.4.2 The Causal Minimality Condition ...................................................... 55
3.4.3 The Faithfulness Condition ................................................................. 56
3.5 Discussion of the Conditions ............................................................................. 57
3.5.1 The Causal Markov and Minimality Conditions ................................ 57
3.5.2 Faithfulness and Simpson's Paradox ................................................... 64
3.6 Bayesian Interpretations .................................................................................... 70
3.7 Consequences of The Axioms ........................................................................... 71
3.7.1 d-Separation ........................................................................................ 71
3.7.2 The Manipulation Theorem ................................................................ 75
3.8 Detenninism ...................................................................................................... 81
3.9 Background Notes ............................................................................................. 86
4. Statistical Indistinguishability ...................................................................................... 87
4.1 Strong Statistical Indistinguishability ................................................................ 88
4.2 Faithful Indistinguishability ............................................................................... 89
4.3 Weak Statistical Indistinguishability ................................................................. 90
4.4 Rigid Indistinguishability .................................................................................. 93
4.5 The Linear Case ................................................................................................. 94
4.6 Redefining Variables ......................................................................................... 99
4.7 Background Notes ............................................................................................. 101
5. Discovery Algorithms for CausaUy Sufficient Structures •••••••••••••••••••••••••••••••••••••••••. 103
5.1 Discovery Problems ........................................................................................... 103
5.2 Search Strategies in Statistics ............................................................................ 104
5.2.1 The Wrong Hypothesis Space ............................................................ 105
5.2.2 Computational and Statistical Limitations .......................................... 107
5.2.3 Generating a Single Hypothesis .......................................................... 108
5.2.4 Other Approaches ............................................................................... 109
5.2.5 Bayesian Methods ............................................................................... 109
5.3 The Wermuth-Lauritzen Algorithm ................................................................... 111
5.4 New ~gorithms ................................................................................................. 112
5.4.1 The SGS Algorithm ............................................................................ 114
5.4.2 The PC Algorithm ............................................................................... 116
5.4.3 The IG (Independence Graph) Algorithm .......................................... 124
5.4.4 Variable Selection ............................................................................... 125
5.4.5 Incorporating Background Knowledge ............................................... 127
5.5 Statistical Decisions ........................................................................................... 128
5.6 Reliability and Probabilities of Error ................................................................. 130
5.7 Estimation .......................................................................................................... 132
5.8 Examples and Applications ............................................................................... 132
5.8.1 The Causes of Publishing Productivity ............................................... 133
5.8.2 Education and Fertility ....................................................................... 139
5.8.3 The Female Orgasm ............................................................................ 140
5.8.4 The American Occupational Structure ............................................... 142
5.8.5 The ALARM Network ........................................................................ 145
5.8.6 Virginity .............................................................................................. 147
5.8.7 The Leading Crowd ............................................................................ 147
5.8.8 Influences on College Plans ................................................................ 149
5.8.9 Abortion Opinions .............................................................................. 150
5.8.10 Simulation Tests with Random Graphs ........................................... 152
5.9 Conclusion ......................................................................................................... 161
5.10 Background Notes ........................................................................................... 162
6. Discovery Algorithms without Causal Sufficiency ..................................................... 163
6.1 Introduction ........................................................................................................ 163
6.2 The PC Algorithm and Latent Variables ........................................................... 165
6.3 Mistakes ............................................................................................................. 168
6.4 Inducing Paths ................................................................................................... 173
6.5 Inducing Path Graphs ........................................................................................ 174
6.6 Partially Oriented Inducing Path Graphs ........................................................... 177
6.7 Algorithms for Causal Inference with Latent Common Causes ........................ 181
6.8 Theorems on Detectable Causal Influence ........................................................ 190
6.9 Non-Independence Constraints ........................................................................... 191
6.10 Generalized Statistical Indistinguishability and Linearity ............................... 193
6.11 The Tetrad Representation Theorem ............................................................... 196
6.12 An Example: Math Marks and Causal Interpretation ...................................... 197
6.13 Background Notes ........................................................................................... 200
7. Prediction ........................................................................................................................ 201
7.1 Introduction ........................................................................................................ 201
7.2 Prediction Problems ........................................................................................... 202
Causation, Prediction, and Search
7.3 Rubin-Holland-Pratt-Schlaifer Theory .............................................................. 203
7.4 Prediction with Causal Sufficiency ................................................................... 213
7.5 Prediction without Causal Sufficiency .............................................................. 216
7.6 Examples ............................................................................................................ 227
7.7 Conclusion ......................................................................................................... 237
7.8 Background Notes ............................................................................................ 237
8. Regression, Causation and Prediction ••••••••••••••••••••••••••••••••••••••••••.•••••••••••.•••••••••••••••••• 238
8.1 When Regression Fails to Measure Influence ................................................... 238
8.2 A Solution and Its Application .......................................................................... 242
8.2.1 Components of the Armed Forces Qualification Test ........................ 243
8.2.2 The Causes of Spartina Biomass ........................................................ 244
8.2.3 The Effects of Foreign Investment on Political Repression ............... 248
8.2.4 More Simulation Studies .................................................................... 250
8.3 Error Probabilities for Specification Searches ................................................... 252
8.4 Conclusion ......................................................................................................... 257
9. The Design of Empirical Studies ••••••••••••••...••••••.••••••••••.••••••••••••••••.•••••••••••.•.••••••••. _ •.••• 259
9.1 Observational or Experimental Study? .............................................................. 259
9.2 Selecting Variables ............................................................................................ 271
9.3 Sampling ............................................................................................................ 272
9.4 Ethical Issues in Experimental Design .............................................................. 276
9.4.1 The Kadane/SedranskiSeidenfeld Design ........................................... 277
9.4.2 Causal Reasoning in the Experimental Design ................................... 280
9.4.3 Towards Ethical Trials ........................................................................ 286
9.5 An Example: Smoking and Lung Cancer .......................................................... 291
9.6 Appendix ............................................................................................................ 302
10. The Structure of the Unobserved ............................................................................... 306
10.1 Introduction ...................................................................................................... 306
10.2 An Outline of the Algorithm ............................................................................ 307
10.3 Finding Almost Pure Measurement Models .................................................... 310
10.3.1 Intra-Construct Foursomes ............................................................... 310
10.3.2 Cross-Construct Foursomes ............................................... , .............. 311
10.4 Facts about the Unobserved Determined by the Observed .............................. 315
10.5 Unifying the Pieces .......................................................................................... 316
10.6 Simulation Tests .............................................................................................. 320
10.7 Conclusion ....................................................................................................... 322
11. Elaborating Linear Tbeories witb Unmeasured Variables •••••••••••••••••••••••••••••••••••••. 323
11.1 Introduction ...................................................................................................... 323
11.2 The Procedure ................................................................................................. 324
11.2.1 Scoring .............................................................................................. 324
11.2.2 Search ............................................................................................... 327
11.3 The LISREL and EQS Procedures ................................................................. 329
11.3.1 Input and Output ............................................................................... 329
11.3.2 Scoring .............................................................................................. 330
11.3.3 The LISREL VI Search .................................................................... 331
11.3.4 The EQS Search ................................................................................ 331
11.4 The Primary Study .......................................................................................... 332
11.4.1 The Design of Comparative Simulation Studies .............................. 332
11.4.2 Study Design ..................................................................................... 333
11.5 Results .............................................................................................................. 343
11.6 Reliability and Infonnativeness ....................................................................... 346
11. 7 Using LISREL and EQS as Adjuncts to Search .............................................. 349
11.8 Limitations of the TETRAD IT Elaboration Search ......................................... 351
11.9 Some Morals for Statistical Search .................................................................. 352
12. Open Problems ...................................................................... ,' •••••••••••••••••••••••••••••••••••••• 354
12.1 Feedback, Reciprocal Causation, and Cyclic Graphs ...................................... 354
12.1.1 Mason's Theorem .............................................................................. 355
12.1.2 Time Series and Cyclic Graphs ......................................................... 356
12.1.3 The Markov Condition, Factorizability and FaithfuIness ................. 359
12.1.4 Discovery Procedures ....................................................................... 360
12.2 Indistinguishability Relations .......................................................................... 361
12.3 Time series and Granger Causality .................................................................. 363
12.4 Model Specification and Parameter Estimation from the Same Data Base ..... 365
12.5 Conditional Independence Tests ...................................................................... 366
13. Proofs of Tbeorems ••••••••••••••••••••••••••••••••••••.••••.••••••••••••••••••••.••••••••••••••••••••••••••••••••••••••• 367
13.1 Theorem 2.1 ..................................................................................................... 367
13.2 Theorem 3.1 ..................................................................................................... 367
13.3 Theorem 3.2 ..................................................................................................... 374
13.4 Theorem 3.3 ..................................................................................................... 376
13.5 Theorem 3.4 ..................................................................................................... 385
13.6 Theorem 3.5 ..................................................................................................... 386
13.7 Theorem 3.6 (Manipulation Theorem) ............................................................ 395
Causation, Prediction, and Search
13.8 Theorem 3.7 ..................................................................................................... 398
13.9 Theorem 4.1 ..................................................................................................... 401
13.10 Theorem 4.2 ................................................................................................... 403
13.11 Theorem 4.3 ................................................................................................... 403
13.12 Theorem 4.4 ................................................................................................... 404
13.13 Theorem 4.5 ................................................................................................... 404
13.14 Theorem 4.6 ................................................................................................... 405
13.15 Theorem 5.1 ................................................................................................... 405
13.16 Theorem 6.1 ................................................................................................... 408
13.17 Theorem 6.2 ................................................................................................... 411
13.18 Theorem 6.3 ................................................................................................... 414
13.19 Theorem 6.4 ................................................................................................... 417
13.20 Theorem 6.5 ................................................................................................... 418
13.21 Theorem 6.6 ................................................................................................... 419
13.22 Theorem 6.7 ................................................................................................... 424
13.23 Theorem 6.8 ................................................................................................... 425
13.24 Theorem 6.9 ................................................................................................... 425
13.25 Theorem 6.10 (Tetrad Representation Theorem) .......................................... 426
13.26 Theorem 6.11 ................................................................................................. 460
13.27 Theorem 7.1 ................................................................................................... 460
13.28 Theorem 7.2 ................................................................................................... 462
13.29 Theorem 7.3 ................................................................................................... 463
13.30 Theorem 7.4 ................................................................................................... 470
13.31 Theorem 7.5 ................................................................................................... 471
13.32 Theorem 9.1 ................................................................................................... 472
13.33 Theorem 9.2 ................................................................................................... 472
13.34 Theorem 10.1 ................................................................................................. 473
13.35 Theorem 10.2 ................................................................................................. 476
13.36 Theorem 11.1 ................................................................................................. 479
Glossary .............................................................................................................................. 481
Bibliography ....................................................................................................................... 495
Index .................................................................................................................................... 517
Notational Conventions
In the text, each technical term is written in boldface where it is defined.
Variables:
Values of variables:
Values of sets of variables:
Members of X that are not members of Y:
Error variables:
Independence of X and Y:
Independence of X and Y conditional on Z:
Covariance of X and Y:
Correlation of X and Y :
Sample correlation of X and Y :
Partial Correlation of X and Y,
controlling for all members of set Z:
capitalized, and in italics, e.g, X
lower case, and in italics, e.g. X = x
capitalized, and in boldface, e.g, V
lower case, and in boldface, e.g. V = v
COV(X,Y) or'}'xy
In all of the graphs that we consider, the vertices are random variables. Hence we use the
terms "variables in a graph" and "vertices in a graph" interchangeably.
Figure numbers occur centered just below a figure, starting at I within each chapter. Where
necessary, we distinguish between measured and unmeasured variables by boxing measured
variables and circling unmeasured variables (except for error terms). Variables beginning with
e, e, or {; are understood to be "error," or "disturbance" variables. For example, in the figure
below, X and Yare measured, T is not, and e is an error term.
Causation, Prediction, and Search
We will neither box nor circle variables in graphs in which no distinction need be made
between measured and unmeasured variables, e.g., figure 2.
........ X ....-
For simplicity, we state and prove our results for probability distributions over discrete
random variables. However, under suitable integrability conditions, the results can be easily
generalized to continuous distributions that have density functions by replacing the discrete
variables by continuous variables, probability distributions by density functions, and
summations by integrals.
If a description of a set of variables is a function of a graph G and variables in G, then we
make G an optional argument to the function. For example, Parents(G,x) denotes the set of
variables that are parents of X in graph G; if the context makes clear which graph is being
referred to we will simply write Parents(X).
If a distribution is defined over a set of random variables 0 then we refer to the distribution as
P(O). An equation between distributions over random variables is understood to be true for
all values of the random variables for which all of the distributions in the equation are
defined. For example if X and Y each take the values 0 or 1 and P(X = 0) * 0 and P(){ = 1) * 0
then P(yIX) = P(y) means P(Y = OIX= 0) = P(y = 0), P(y = OIX= I) = P(Y = 0), P(y = llX= 0)
= P(Y = I), and P(Y = llX= 1) = P(Y = I).
Notational Conventions
We sometimes use a special summation symbol, L ' which has the following properties:
(i) when sets of random variables are written beneath the special summation symbol, it is
understood that the summation is to be taken over sets of values of the random variables,
not the random variables themselves,
(ii) if a conditional probability distribution appears in the scope of such a summation
symbol, the summation is to be taken only over values of the random variables for which
the conditional probability distributions are defined,
(iii) if there are no values of the random variables under the special summation symbol for
which the conditional probability distributions in the scope of the symbol are defined,
then the summation is equal to zero.
For example, suppose that X, Y, and Z can each take on the values 0 or 1. Then if
P(Y=O;Z=O) *" 0
LP(XIY = O,Z = 0) = P(X = OIY = O,Z = 0) + P(X = IIY = O,Z = 0)
However, if p(Y=o,z=o) = 0, then P(X=OIY=<i,z=O) and P(X=lIY=O,z=o) are not defined, so
LP(XIY = O,Z = 0) = 0
We will adopt the following conventions for empty sets of variables. If Y = 0 then
(i) P(XIY) means P(X).
(ii) fJXZ.Y means fJXZ.
(iii) A Jl DIY means A Jl D.
(iv) A Jl Y is always true.