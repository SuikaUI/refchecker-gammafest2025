Densely Connected Pyramid Dehazing Network
Vishal M. Patel
Department of Electrical and Computer Engineering
Rutgers University, Piscataway, NJ 08854
{he.zhang92,vishal.m.patel}@rutgers.edu
We propose a new end-to-end single image dehazing
method, called Densely Connected Pyramid Dehazing Network (DCPDN), which can jointly learn the transmission
map, atmospheric light and dehazing all together. The endto-end learning is achieved by directly embedding the atmospheric scattering model into the network, thereby ensuring that the proposed method strictly follows the physicsdriven scattering model for dehazing.
Inspired by the
dense network that can maximize the information ﬂow
along features from different levels, we propose a new
edge-preserving densely connected encoder-decoder structure with multi-level pyramid pooling module for estimating the transmission map. This network is optimized using a newly introduced edge-preserving loss function. To
further incorporate the mutual structural information between the estimated transmission map and the dehazed result, we propose a joint-discriminator based on generative adversarial network framework to decide whether the
corresponding dehazed image and the estimated transmission map are real or fake.
An ablation study is conducted to demonstrate the effectiveness of each module
evaluated at both estimated transmission map and dehazed
Extensive experiments demonstrate that the proposed method achieves signiﬁcant improvements over the
state-of-the-art methods. Code will be made available at:
 
1. Introduction
Under severe hazy conditions, ﬂoating particles in the atmosphere such as dusk and smoke greatly absorb and scatter the light, resulting in degradations in the image quality.
These degradations in turn may affect the performance of
many computer vision systems such as classiﬁcation and
detection. To overcome the degradations caused by haze,
image and video-based haze removal algorithms have been
proposed in the literature .
Sample image dehazing result using the proposed
DCPDN method. Left: Input hazy image. Right: Dehazed result.
The image degradation (atmospheric scattering model)
due to the presence of haze is mathematically formulated as
I(z) = J(z)t(z) + A(z)(1 −t(z)),
where I is the observed hazy image, J is the true scene
radiance, A is the global atmospheric light, indicating the
intensity of the ambient light, t is the transmission map
and z is the pixel location.
Transmission map is the
distance-dependent factor that affects the fraction of light
that reaches the camera sensor. When the atmospheric light
A is homogeneous, the transmission map can be expressed
as t(z) = e−βd(z), where β represents attenuation coefﬁcient of the atmosphere and d is the scene depth. In single
image dehazing, given I, the goal is to estimate J.
It can be observed from Eq. 1 that there exists two important aspects in the dehazing process: (1) accurate estimation of transmission map, and (2) accurate estimation of
atmospheric light. Apart from several works that focus on
estimating the atmospheric light , most of the other
algorithms concentrate more on the accurate estimation of
the transmission map and they leverage empirical rule in
estimating the atmospheric light . This is
mainly due to the common belief that good estimation of
transmission map will lead to better dehazing. These methods can be broadly divided into two main groups: priorbased methods and learning-based methods.
Prior-based
methods often leverage different priors in characterizing
the transmission map such as dark-channel prior , contrast color-lines and haze-line prior , while learningbased methods, such as those based on convolutional neural
networks (CNNs), attempt to learn the transmission map di-
 
Figure 2: An overview of the proposed DCPDN image dehazing
method. DCPDN consists of four modules: 1. Pyramid densely
connected transmission map estimation net. 2. Atmospheric light
estimation net. 3. Dehazing via Eq2. 4. Joint discriminator. We
ﬁrst estimate the transmission map using the proposed pyramid
densely-connected transmission estimation net, followed by prediction of atmospheric light using the U-net structure. Finally, using the estimated transmission map and the atmospheric light we
estimate the dehazed image via Eq. 2.
rectly from the training data . Once the
transmission map and the atmospheric light are estimated,
the dehazed image can be recovered as follows
ˆJ(z) = I(z) −ˆA(z)(1 −ˆt(z))
Though tremendous improvements have been made by
the learning-based methods, several factors hinder the performance of these methods and the results are far from optimal. This is mainly because: 1. Inaccuracies in the estimation of transmission map translates to low quality dehazed result. 2. Existing methods do not leverage end-toend learning and are unable to capture the inherent relation
among transmission map, atmospheric light and dehazed
image. The disjoint optimization may hinder the overall dehazing performance. Most recently, a method was proposed
in to jointly optimize the whole dehazing network.
This was achieved by leveraging a linear transformation to
embed both the transmission map and the atmospheric light
into one variable and then learning a light-weight CNN to
recover the clean image.
In this paper, we take a different approach in addressing the end-to-end learning for image dehazing. In particular, we propose a new image dehazing architecture, called
Densely Connected Pyramid Dehazing Network (DCPDN),
that can be jointly optimized to estimate transmission map,
atmospheric light and also image dehazing simultaneously
by following the image degradation model Eq. 1 (see
Fig. 2). In other words, the end-to-end learning is achieved
by embedding Eq. 1 directly into the network via the math
operation modules provided by the deep learning framework.
However, training such a complex network (with
three different tasks) is very challenging. To ease the training process and accelerate the network convergence, we
leverage a stage-wise learning technique in which we ﬁrst
progressively optimize each part of the network and then
jointly optimize the entire network.
To make sure that
the estimated transmission map preserves sharp edges and
avoids halo artifacts when dehazing, a new edge-preserving
loss function is proposed in this paper based on the observation that gradient operators and ﬁrst several layers of a CNN
structure can function as edge extractors.
Furthermore,
a densely connected encoder-decoder network with multilevel pooling modules is proposed to leverage features from
different levels for estimating the transmission map. To exploit the structural relationship between the transmission
map and the dehazed image, a joint discriminator-based
generative adversarial network (GAN) is proposed.
joint discriminator distinguishes whether a pair of estimated
transmission map and dehazed image is a real or fake pair.
To guarantee that the atmospheric light can also be optimized within the whole structure, a U-net is adopted to
estimate the homogeneous atmospheric light map. Shown
in Fig. 1 is a sample dehazed image using the proposed
This paper makes the following contributions:
• A novel end-to-end jointly optimizable dehazing network is proposed. This is enabled by embedding Eq. 1
directly into the optimization framework via math operation modules. Thus, it allows the network to estimate the transmission map, atmospheric light and dehazed image jointly. The entire network is trained by a
stage-wise learning method.
edge-preserving
encoder-decoder network is proposed for accurately
estimating the transmission map.
Further, it is optimized via a newly proposed edge-preserving loss
• As the structure of the estimated transmission map and
the dehazed image are highly correlated, we leverage a
joint discriminator within the GAN framework to determine whether the paired samples (i.e. transmission
map and dehazed image) are from the data distribution
• Extensive experiments are conducted on two synthetic
datasets and one real-world image dataset. In addition, comparisons are performed against several recent state-of-the-art approaches. Furthermore, an ablation study is conducted to demonstrate the improvements obtained by different modules in the proposed
2. Related Work
Single Image Dehazing. Single image dehazing is a highly
ill-posed problem.
Various handcrafted prior-based and
learning-based methods have been developed to tackle this
Handcrafted Prior-based: Fattal proposed a physicallygrounded method by estimating the albedo of the scene.
As the images captured from the hazy conditions always
lack color contrast, Tan et al. proposed a patch-based
contrast-maximization method. In , Kratz and Nishino
proposed a factorial MRF model to estimate the albedo
and depths ﬁled. Inspired by the observations that outdoor
objects in clear weather have at least one color channel
that is signiﬁcantly dark, He. et al. in proposed a
dark-channel model to estimate the transmission map.
More recently, Fattal proposed a color-line method
based on the observation that small image patches typically
exhibit a one-dimensional distribution in the RGB color
space. Similarly, Berman et al. proposed a non-local
patch prior to characterize the clean images.
Learning-based:
Unlike some of the above mentioned
methods that use different priors to estimate the transmission map, Cai et al.
 introduce an end-to-end CNN
network for estimating the transmission with a novel
BReLU unit.
More recently, Ren et al.
 proposed
a multi-scale deep neural network to estimate the transmission map. One of the limitations of these methods is
that they limit their capabilities by only considering the
transmission map in their CNN frameworks. To address
this issue, Li. et al proposed an all-in-one dehazing
network, where a linear transformation is leveraged to
encode the transmission map and the atmospheric light into
one variable.
Most recently, several benchmark datasets
of both synthetic and real-world hazy images for dehazing
problems are introduced to the community .
Generative Adversarial Networks (GANs). The notion
of GAN was ﬁrst proposed by Goodfellow et al. in 
to synthesize realistic images by effectively learning the
distribution of the training images via a game theoretic
min-max optimization framework. The success of GANs
in synthesizing realistic images has led researchers to explore the adversarial loss for various low-level vision applications such as text-to-image synthesis ,
image-image translation and other applications
 . Inspired by the success of
these methods in generating high-quality images with ﬁne
details, we propose a joint discriminator-based GAN to re-
ﬁne the estimated transmission map and dehazed image.
3. Proposed Method
The proposed DCPDN network architecture is illustrated
in Fig. 2 which consists of the following four modules: 1)
Pyramid densely connected transmission map estimation
net, 2) Atmosphere light estimation net, 3) Dehazing via
Eq. 2, and 4) Joint discriminator.
In what follows, we
explain these modules in detail.
Pyramid Densely Connected Transmission Map Estimation Network. Inspired by the previous methods that use
multi-level features for estimating the transmission map
 , we propose a densely connected encoderdecoder structure that makes use of the features from multiple layers of a CNN, where the dense block is used as
the basic structure. The reason to use dense block lies in
that it can maximize the information ﬂow along those features and guarantee better convergence via connecting all
In addition, a multi-level pyramid pooling module is adopted to reﬁne the learned features by considering the ‘global’ structural information into the optimization
 . To leverage the pre-deﬁned weights of the dense-net
 , we adopt the ﬁrst Conv layer and the ﬁrst three Dense-
Blocks with their corresponding down-sampling operations
Transition-Blocks from a pre-trained dense-net121 as our
encoder structure. The feature size at end of the encoding
part is 1/32 of the input size. To reconstruct the transmission
map into the original resolution, we stack ﬁve dense blocks
with the reﬁned up-sampling Transition-Blocks as
the decoding module. In addition, concatenations are employed with the features corresponding to the same dimension.
Figure 3: An overview of the proposed pyramid densely connected transmission map estimation network.
Even though the proposed densely connected encoderdecoder structure combines different features within the
network, the result from just densely connected structure
still lack of the ‘global’ structural information of objects
with different scales.
One possible reason is that the
features from different scales are not used to directly
estimate the ﬁnal transmission map. To efﬁciently address
this issue, a multi-level pyramid pooling block is adopted to
make sure that features from different scales are embedded
in the ﬁnal result. This is inspired by the use of global
context information in classiﬁcation and segmentation tasks
 . Rather than taking very large pooling size to
capture more global context information between different
objects , more ‘local’ information to characterize the
‘global’ structure of each object is needed.
four-level pooling operation with pooling sizes 1/32, 1/16,
Figure 4: Left: a dehazed image. Right: The transmission map
used to produce a hazy image from which the dehazed image on
the left was obtained.
1/8 and 1/4 is adopted. Then, all four level features are
up-sampling to original feature size and are concatenated
back with the original feature before the ﬁnal estimation.
Fig 3 gives an overview of the proposed pyramid densely
connected transmission map estimation network.
Atmospheric Light Estimation Network.
the image degradation model Eq.; 1, we assume that the
atmospheric light map A is homogeneous . Similar
to previous works, the predicted atmospheric light A is
uniform for a given image. In other words, the predicted
A is a 2D-map, where each pixel A(z) has the same value
(eg. A(z) = c, c
is a constant). As a result, the ground
truth A is of the same feature size as the input image
and the pixels in A are ﬁlled with the same value.
estimate the atmospheric light, we adopt a 8-block U-net
 structure, where the encoder is composed of four
Conv-BN-Relu blocks and the decoder is composed of
symmetric Dconv-BN-Relu block 1.
Dehazing via Eq. 2.
To bridge the relation among the
transmission map, the atmospheric light and the dehazed
image and to make sure that the whole network structure is
jointly optimized for all three tasks, we directly embed (2)
into the overall optimization framework. An overview of
the entire DCPDN structure is shown in Fig 1.
3.1. Joint Discriminator Learning
Let Gt and Gd denote the networks that generate the
transmission map and the dehazed result, respectively. To
reﬁne the output and to make sure that the estimated transmission map Gt(I) and the dehazed image Gd(I) are indistinguishable from their corresponding ground truths t and
J, respectively, we make use of a GAN with novel joint
discriminator.
It can be observed from (1) and also Fig. 4 that the
structural information between the estimated transmission
1Con: Convolution, BN: Batch-normalization and Dconv: Deconvolution (transpose convolution).
Figure 5: Feature visualization for gradient operator and low-level
features. (a) Input transmission map. (b) Horizontal gradient output. (c) Vertical gradient output. (d) and (e) are visualization of
two feature maps from relu1 2 of VGG-16 .
map ˆt = Gt(I) and the dehazed image ˆJ are highly correlated.
Hence, in order to leverage the dependency in
structural information between these two modalities, we
introduce a joint discriminator to learn a joint distribution to decide whether the corresponding pairs (transmission map, dehazed image) are real or fake. By leveraging the joint distribution optimization, the structural correlation between them can be better exploited. Similar to
previous works, the predicted air-light A is uniform for a
given image. In other words, the predicted air-light A is
a 2D-map, where each pixel A(z) has the same value (eg.
A(z) = c, c
is a constant). We propose the following
joint-discriminator based optimization
EI∼pdata(I)[log(1 −Djoint(Gt(I)))]+
EI∼pdata(I)[log(1 −Djoint(Gd(I)))]+
Et,J∼pdata(t,J)[log Djoint(t, J))].
In practice, we concatenate the dehazed image with the
estimated transmission map as a pair sample and then feed
it into the discriminator.
3.2. Edge-preserving Loss
It is commonly acknowledged that the Euclidean loss
(L2 loss) tends to blur the ﬁnal result. Hence, inaccurate
estimation of the transmission map with just the L2 loss
may result in the loss of details, leading to the halo artifacts
in the dehazed image . To efﬁciently address this issue,
a new edge-preserving loss is proposed, which is motivated
by the following two observations. 1) Edges corresponds
to the discontinuities in the image intensities, hence it can
be characterized by the image gradients. 2) It is known that
low-level features such as edges and contours can be captured in the shallow (ﬁrst several) layers of a CNN structure
 . In other words, the ﬁrst few layers function as an edge
detector in a deep network. For example, if the transmission map is fed into a pre-deﬁned VGG-16 model and
then certain features from the output of layer relu1 2 are
visualized, it can be clearly observed that the edge information being preserved in the corresponding feature maps (see
SSIM:0.9272
SSIM:0.9524
SSIM:0.9671
SSIM:0.9703
SSIM:0.9735
SSIM:0.8882
SSIM:0.9119
SSIM:0.9201
SSIM:0.9213
SSIM:0.9283
Figure 6: Transmission map estimation results using different modules. (a) DED; (b). DED-MLP; (c).DED-MLP-GRA; (d). DED-MLP-
EP; (e). DCPDN; (f) Target. It can be observed that the multi-level pooling module is able to reﬁne better global structure of objects in
the image (observed from (a) and (b) ), the edge-preserving loss can preserve much sharper edges (comparing (b), (c) and (d)) and the ﬁnal
joint-discriminator can better reﬁne the detail for small objects (comparing (d) and (e)).
Based on these observations and inspired by the gradient
loss used in depth estimation as well as the use of
perceptual loss in low-level vision tasks , we propose a new edge-preserving loss function that is composed
of three different parts: L2 loss, two-directional gradient
loss, and feature edge loss, deﬁned as follows
LE = λE,l2LE,l2 + λE,gLE,g + λE,fLE,f,
where LE indicates the overall edge-preserving loss, LE,l2
indicates the L2 loss, LE,g indicates the two-directional
(horizontal and vertical) gradient loss and LE,f is the feature loss. LE,g is deﬁned as follows
∥(Hx(Gt(I)))w,h −(Hx(t))w,h∥2
+ ∥(Hy(Gt(I)))w,h −(Hy(t))w,h∥2,
where Hx and Hy are operators that compute image gradients along rows (horizontal) and columns (vertical), respectively and w×h indicates the width and height of the output
feature map. The feature loss is deﬁned as
∥(V1(Gt(I)))c1,w1,h1 −(V1(t))c1,w1,h1∥2
∥(V2(Gt(I)))c2,w2,h2 −(V2(t))c2,w2,h2∥2,
where Vi represents a CNN structure and ci, wi, hi are the
dimensions of the corresponding low-level feature in Vi. As
the edge information is preserved in the low-level features,
we adopt the layers before relu1-1 and relu2-1 of VGG-16
 as the edge extractors V1 and V2, respectively. Here,
λE,l2, λE,g, and λE,f are weights to balance the loss function.
3.3. Overall Loss Function
The proposed DCPDN architecture is trained using the
following four loss functions
L = Lt + La + Ld + λjLj,
where Lt is composed of the edge-preserving loss LE, La
is composed of the traditional L2 loss in predicting the atmospheric light and Ld represents the dehazing loss, which
is also composed of the L2 loss only. Lj, which is denoted
as the joint discriminator loss 2, is deﬁned as follows
Lj = −log(Djoint(Gt(I)) −log(Djoint(Gd(I)).
Here λj is a constant.
3.4. Stage-wise Learning
During experiments, we found that directly training the
whole network from scratch with the complex loss Eq. 7 is
difﬁcult and the network converges very slowly. A possible reason may be due to the gradient diffusion caused by
different tasks. For example, gradients from the de-hazed
image loss may ‘distract’ the gradients from the loss of the
transmission map initially, resulting in the slower convergence. To address this issue and to speed up the training, a
stage-wise learning strategy is introduced, which has been
2To address the vanishing gradients problem for the generator, we also
minimize (8) rather than the ﬁrst two rows in (3) .
SSIM: 0.7654
SSIM: 0.9382
SSIM: 0.8637
SSIM: 0.9005
SSIM: 0.8683
SSIM: 0.9200
SSIM: 0.9777
SSIM: 0.6642
SSIM: 0.8371
He et al. 
SSIM: 0.8117
Zhu et al. 
SSIM: 0.8364
Ren et al. 
SSIM: 0.8575
Berman et al.
SSIM: 0.7691
Li et al. 
SSIM: 0.9325
Figure 7: Dehazing results from the synthetic test datasets TestA (ﬁrst row) and TestB (second row).
used in different applications such as multi-model recognition and feature learning . Hence, the information
in the training data is presented to the network gradually.
In other words, different tasks are learned progressively.
Firstly, we optimize each task separately by not updating
the other task simultaneously. After the ‘initialization’ for
each task, we ﬁne-tune the whole network all together by
optimizing all three tasks jointly.
4. Experimental Results
In this section, we demonstrate the effectiveness of the
proposed approach by conducting various experiments on
two synthetic datasets and a real-world dataset. All the results are compared with ﬁve state-of-the-art methods: He et
al. (CVPR’09) , Zhu et al (TIP’15) , Ren et al. 
(ECCV’16), Berman et al. (CVPR’16 and ICCP’17)
and Li et al. (ICCV’17). In addition, we conduct an ablation study to demonstrate the effectiveness of each module
of our network.
4.1. Datasets
Similar to the existing deep learning-based dehazing
methods , we synthesize the training samples
{Hazy /Clean /Transmission Map /Atmosphere Light} based
on (1). During synthesis, four atmospheric light conditions
A ∈[0.5, 1] and the scattering coefﬁcient β ∈[0.4, 1.6] are
randomly sampled to generate their corresponding hazy images, transmission maps and atmospheric light maps. A random set of 1000 images are selected from the NYU-depth2
dataset to generate the training set. Hence, there are in
total 4000 training images, denoted as TrainA. Similarly,
a test dataset TestA consisting of 400 (100×4) images also
from the NYU-depth2 are obtained. We ensure that none of
the testing images are in the training set. To demonstrate
the generalization ability of our network to other datasets,
we synthesize 200 {Hazy /Clean /Transmission Map /Atmosphere Light} images from both the Middlebury stereo
Table 1: Quantitative SSIM results for ablation study evaluated on
synthetic TestA and TestB datasets.
DED-MLP-GRA
DED-MLP-EP
Transmission
Transmission
Table 2: Quantitative SSIM results on the synthetic TestA dataset.
He. et al. 
Zhu. et al. 
Ren. et al. 
Berman. et al. 
Li. et al. 
Transmission
Table 3: Quantitative SSIM results on the synthetic TestB dataset.
He. et al. 
Zhu. et al. 
Ren. et al. 
Berman. et al. 
Li. et al. 
Transmission
database (40) and also the Sun3D dataset (160) as
the TestB set.
4.2. Training Details
We choose λE,l2 = 1, λE,g = 0.5, λE,f = 0.8 for the
loss in estimating the transmission map and λj = 0.25 for
optimizing the joint discriminator. During training, we use
ADAM as the optimization algorithm with learning rate of
2 × 10−3 for both generator and discriminator and batch
size of 1. All the training samples are resized to 512 ×
512. We trained the network for 400000 iterations. All the
parameters are chosen via cross-validation.
4.3. Ablation Study
In order to demonstrate the improvements obtained by
each module introduced in the proposed network, we perform an ablation study involving the following ﬁve exper-
He. et al.
(CVPR’09) 
Zhu. et al.
(TIP’15) 
Ren. et al.
(ECCV’16) 
Berman. et al.
(CVPR’16) 
Li. et al.
(ICCV’17) 
Figure 8: Dehazing results evaluated on real-world images released by the authors of previous methods.
iments: 1) Densely connected encoder decoder structure
(DED), 2) Densely connected encoder decoder structure
with multi-level pyramid pooling (DED-MLP), 3) Densely
connected encoder decoder structure with multi-level pyramid pooling using L2 loss and gradient loss (DED-MLP-
GRA), 4) Densely connected encoder decoder structure
with multi-level pyramid pooling using edge-preserving
loss (DED-MLP-EP), 5) The proposed DCPDN that is
composed of densely connected encoder decoder structure
with multi-level pyramid pooling using edge-preserving
loss and joint discriminator (DCPDN). 3
The evaluation is performed on the synthesized TestA
and TestB datasets. The SSIM results averaged on both estimated transmission maps and dehazed images for the various conﬁgurations are tabulated in Table 1. Visual comparisons are shown in the Fig 6. From Fig 6, we make the following observations: 1) The proposed multi-level pooling
module is able to better preserve the ‘global’ structural for
objects with relatively larger scale, compared with (a) and
(b). 2) The use of edge-preserving loss is able to better re-
ﬁne the edges in the estimated transmission map, compared
with (b), (c) and (d). 3) The ﬁnal joint-discriminator can
further enhance the estimated transmission map by ensuring that the ﬁne structural details are captured in the results,
such as details of the small objects on the table shown in the
3The conﬁguration 1) DED and 2) DED-MLP are optimized only with
second row in (e). The quantitative performance evaluated
on both TestA and TestB also demonstrate the effectiveness
of each module.
4.4. Comparison with state-of-the-art Methods
To demonstrate the improvements achieved by the proposed method, it is compared against the recent state-ofthe-art methods . on both synthetic and
real datasets.
Evaluation on synthetic dataset: The proposed network is
evaluated on two synthetic datasets TestA and TestB. Since
the datasets are synthesized, the ground truth images and the
transmission maps are available, enabling us to evaluate the
performance qualitatively as well as quantitatively. Sample
results for the proposed method and ﬁve recent state-of-theart methods, on two sample images from the test datasets
are shown in Fig. 7. It can be observed that even though
previous methods are able to remove haze from the input
image, they tend to either over dehaze or under dehaze the
image making the result darker or leaving some haze in the
result. In contrast, it can be observed from our results that
they preserve sharper contours with less color distortion and
are more visually closer to the ground-truth. The quantitative results, tabulated in Table 2 and Table 3 4, evaluated on
both TestA and TestB also demonstrate the effectiveness of
the proposed method.
4N/A: Code released is unable to estimate the transmission map.
He. et al.
(CVPR’09) 
Zhu. et al.
(TIP’15) 
Ren. et al.
(ECCV’16) 
Berman et al.
(CVPR’16) 
Li. et al.
(ICCV’17) 
Figure 9: Dehazing results evaluated on real-world images downloaded from the Internet.
Evaluation on a real dataset: To demonstrate the generalization ability of the proposed method, we evaluate the
proposed method on several real-world hazy images provided by previous methods and other challenging hazy images downloaded from the Internet.
Results for four sample images obtained from the previous methods are shown in Fig. 8. As revealed in
Fig. 8, methods of He et al. and Ren et al. (observed on the fourth row) tend to leave haze in the results
and methods of Zhu et al. and Li et al. (shown
on the second row) tend to darken some regions (notice the
background wall). Methods from Berman et al. and
our method have the most competitive visual results. However, by looking closer, we observe that Berman et al. 
produce unrealistic color shifts such as the building color
in the fourth row. In contrast, our method is able to generate realistic colors while better removing haze. This can be
seen by comparing the ﬁrst and the second row.
We also evaluate on several hazy images downloaded
from the Internet. The dehazed results are shown in Fig. 9.
It can be seen from these results that outputs from He et
al. and Berman et al. suffer from color distortions, as shown in the second and third rows. In contrast,
our method is able to achieve better dehazing with visually
appealing results.
5. Conclusion
We presented a new end-to-end deep learning-based dehazing method that can jointly optimize transmission map,
atmospheric light and dehazed image.
This is achieved
via directly embedding the atmospheric image degradation
model into the overall optimization framework.
To efﬁciently estimate the transmission map, a novel densely connected encoder-decoder structure with multi-level pooling
module is proposed and this network is optimized by a new
edge-preserving loss. In addition, to reﬁne the details and to
leverage the mutual structural correlation between the dehazed image and the estimated transmission map, a jointdiscriminator based GAN framework is introduced in the
proposed method. Various experiments were conducted to
show the signiﬁcance of the proposed method.
Acknowledgement
This work was supported by an ARO grant W911NF-16-