NormFace: L2 Hypersphere Embedding for Face Verification
Feng Wang∗
University of Electronic Science and Technology of China
2006 Xiyuan Ave.
Chengdu, Sichuan 611731
 
Xiang Xiang
Johns Hopkins University
3400 N. Charles St.
Baltimore, Maryland 21218
 
Jian Cheng
University of Electronic Science and Technology of China
2006 Xiyuan Ave.
Chengdu, Sichuan 611731
 
Alan L. Yuille
Johns Hopkins University
3400 N. Charles St.
Baltimore, Maryland 21218
 
Thanks to the recent developments of Convolutional Neural Networks, the performance of face veriﬁcation methods has increased
rapidly. In a typical face veriﬁcation method, feature normalization
is a critical step for boosting performance. This motivates us to
introduce and study the eﬀect of normalization during training.
But we ﬁnd this is non-trivial, despite normalization being diﬀerentiable. We identify and study four issues related to normalization
through mathematical analysis, which yields understanding and
helps with parameter settings. Based on this analysis we propose
two strategies for training using normalized features. The ﬁrst is
a modiﬁcation of softmax loss, which optimizes cosine similarity
instead of inner-product. The second is a reformulation of metric
learning by introducing an agent vector for each class. We show
that both strategies, and small variants, consistently improve performance by between 0.2% to 0.4% on the LFW dataset based on
two models. This is signiﬁcant because the performance of the two
models on LFW dataset is close to saturation at over 98%.
CCS CONCEPTS
• Computing methodologies →Object identiﬁcation; Supervised learning by classiﬁcation; Neural networks; Regularization;
Face Veriﬁcation, Metric Learning, Feature Normalization
INTRODUCTION
In recent years, Convolutional neural networks (CNNs) achieve
state-of-the-art performance for various computer vision tasks,
such as object recognition , detection , segmentation
∗Alan L. Yuille’s visiting student.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from .
MM ’17, October 23–27, 2017, Mountain View, CA, USA.
© 2017 ACM. ISBN 978-1-4503-4906-2/17/10...$15.00
DOI: 
 and so on. In the ﬁeld of face veriﬁcation, CNNs have already
surpassed humans’ abilities on several benchmarks .
The most common pipeline for a face veriﬁcation application
involves face detection, facial landmark detection, face alignment,
feature extraction, and ﬁnally feature comparison. In the feature
comparison step, the cosine similarity or equivalently L2 normalized
Euclidean distance is used to measure the similarities between
features. The cosine similarity
∥·∥∥·∥is a similarity measure which
is independent of magnitude. It can be seen as the normalized
version of inner-product of two vectors. But in practice the inner
product without normalization is the most widely-used similarity
measure when training a CNN classiﬁcation models . In
other words, the similarity or distance metric used during training is
diﬀerent from that used in the testing phase. To our knowledge, no
researcher in the face veriﬁcation community has clearly explained
why the features should be normalized to calculate the similarity
in the testing phase. Feature normalization is treated only as a trick
to promote the performance during testing.
To illustrate this, we performed an experiment which compared
the face features without normalization, i.e. using the unnormalized
inner-product or Euclidean distance as the similarity measurement.
The features were extracted from an online available model 1.
We followed the standard protocol of unrestricted with labeled outside data and test the model on the Labeled Faces in the Wild
(LFW) dataset . The results are listed in Table 1.
Table 1: Eﬀect of Feature Normalization
Similarity
Before Normalization
After Normalization
Inner-Product
As shown in the table, feature normalization promoted the performance by about 0.6% ∼0.7%, which is a signiﬁcant improvement
since the accuracies are already above 98%. Feature normalization
seems to be a crucial step to get good performance during testing.
Noting that the normalization operation is diﬀerentiable, there is no
reason that stops us importing this operation into the CNN model
to perform end-to-end training.
1 
 
innerproduct
classification
thresholding
Figure 1: Pipeline of face veriﬁcation model training and
testing using a classiﬁcation loss function. Previous works
did not use the normalization after feature extraction during training. But in the testing phase, all methods used a
normalized similarity, e.g. cosine, to compare two features.
Some previous works successfully trained CNN models
with the features being normalized in an end-to-end fashion. However, both of them used the triplet loss, which needs to sample
triplets of face images during training. It is diﬃcult to train because we usually need to implement hard mining algorithms to ﬁnd
non-trivial triplets . Another route is to train a classiﬁcation
network using softmax loss and regularizations to limit the
intra-class variance . Furthermore, some works combine the
classiﬁcation and metric learning loss functions together to train
CNN models . All these methods that used classiﬁcation loss
functions, e.g. softmax loss, did not apply feature normalization,
even though they all used normalized similarity measure, e.g. cosine similarity, to get the conﬁdence of judging two samples being
of the same identity at testing phase(Figure 1).
We did an experiment by normalizing both the features and the
weights of the last inner-product layer to build a cosine layer in
an ordinary CNN model. After suﬃcient iterations, the network
still did not converge. After observing this phenomenon, we deeply
dig into this problem. In this paper, we will ﬁnd out the reason and
propose methods to enable us to train the normalized features.
To sum up, in this work, we analyze and answer the questions
mentioned above about the feature normalization and the model
(1) Why is feature normalization so eﬃcient when comparing the
CNN features trained by classiﬁcation loss, especially for softmax loss?
(2) Why does directly optimizing the cosine similarity using softmax loss cause the network to fail to converge?
(3) How to optimize a cosine similarity when using softmax loss?
(4) Since models with softmax loss fail to converge after normalization, are there any other loss functions suitable for normalized
For the ﬁrst question, we explain it through a property of softmax
loss in Section 3.1. For the second and third questions, we provide
a bound to describe the diﬃculty of using softmax loss to optimize
a cosine similarity and propose using the scaled cosine similarity
in Section 3.3. For the fourth question, we reformulate a set of loss
functions in metric learning, such as contrastive loss and triplet
loss to perform the classiﬁcation task by introducing an ‘agent’
strategy (Section 4). Utilizing the ‘agent’ strategy, there is no need
to sample pairs and triplets of samples nor to implement the hard
mining algorithm.
We also propose two tricks to improve performance for both
static and video face veriﬁcation. The ﬁrst is to merge features extracted from both original image and mirror image by summation,
while previous works usually merge the features by concatenation . The second is to use histogram of face similarities between
video pairs instead of the mean or max similarity when
making classiﬁcation.
Finally, by experiments, we show that normalization during
training can promote the accuracies of two publicly available stateof-the-art models by 0.2 ∼0.4% on LFW and about 0.6% on
RELATED WORKS
Normalization in Neural Network. Normalization is a common
operation in modern neural network models. Local Response Normalization and Local Contrast Normalization are studied in the
AlexNet model , even though these techniques are no longer
common in modern models. Batch normalization is widely used
to accelerate the speed of neural network convergence by reducing
the internal covariate shift of intermediate features. Weight normalization was proposed to normalize the weights of convolution
layers and inner-product layers, and also lead to faster convergence
speed. Layer normalization tried to solve the batch size dependent problem of batch normalization, and works well on Recurrent
Neural Networks.
Face Veriﬁcation. Face veriﬁcation is to decide whether two images containing faces represent the same person or two diﬀerent
people, and thus is important for access control or re-identiﬁcation
tasks. Face veriﬁcation using deep learning techniques achieved a
series of breakthroughs in recent years . There
are mainly two types of methods according to their loss functions.
One type uses metric learning loss functions, such as contrastive
loss and triplet loss . The other type uses softmax loss and treats the problem as a classiﬁcation task, but also
constrains the intra-class variance to get better generalization for
comparing face features . Some works also combine both
kinds of loss functions .
Metric Learning. Metric learning tries to learn semantic
distance measures and embeddings such that similar samples are
nearer and diﬀerent samples are further apart from each other on a
manifold. With the help of neural networks’ enormous ability of
representation learning, deep metric learning can do even
better than the traditional methods. Recently, more complicated loss
functions were proposed to get better local embedding structures[8,
Recent Works on Normalization. Recently, cosine similarity
 was used instead of the inner-product for training a CNN
for person recognition, which is quite similar with face veriﬁcation.
The Cosine Loss proposed in is quite similar with the one
described in Section 3.3, normalizing both the features and weights.
L2-softmax shares a similar analysis about the convergence
problem described in Section 3.3. In , the authors also propose
to add a scale parameter after normalization, but they only normalize the features. SphereFace improves the performance of Large
Figure 2: Lef: The optimized 2-dimensional feature distribution using softmax loss on MNIST dataset. Note that the
Euclidean distance between f1 and f2 is much smaller than
the distance between f2 and f3, even though f2 and f3 are from
the same class. Right: The softmax probability for class 0 on
the 2-dimension plane. Best viewed in color.
Margin Softmax by normalizing the weights of the last innerproduct layer only. Von Mises-Fisher Mixture Model(vMFMM) 
interprets the hypersphere embedding as a mixture of von Mises-
Fisher distributions. To sum up, the Cosine Loss , vMFMM 
and our proposed loss functions optimize both features and weights,
while the L2-softmax normalizes the features only and the
SphereFace normalizes the weights only.
L2 NORMALIZATION LAYER
In this section, we answer the question why we should normalize
the features when the loss function is softmax loss and why the
network does not converge if we directly put a softmax loss on the
normalized features.
Necessity of Normalization
In order to give an intuitive feeling about the softmax loss, we
did a toy experiment of training a deeper LeNet model on
the MNIST dataset . We reduced the number of the feature
dimension to 2 and plot 10,000 2-dimensional features from the
training set on a plane in Figure 2. From the ﬁgure, we ﬁnd that
f2 can be much closer to f1 than to f3 if we use Euclidean distance
as the metric. Hence directly using the features for comparison
may lead to bad performance. At the same time, we ﬁnd that the
angles between feature vectors seem to be a good metric compared
with Euclidean distance or inner-product operations. Actually, most
previous work takes the cosine of the angle between feature vectors
as the similarity , even though they all use softmax loss
to train the network. Since the most common similarity metric for
softmax loss is the inner-product with unnormalized features, there
is a gap between the metrics used in the training and testing phases.
The reason why the softmax loss tends to create a ‘radial’ feature
distribution (Figure 2) is that the softmax loss actually acts as the
soft version of max operator. Scaling the feature vectors’ magnitude
does not aﬀect the assignment of its class. Formally speaking, we
recall the deﬁnition of the softmax loss,
where m is the number of training samples, n is the number of
classes, fi is the feature of the i-th sample, yi is the corresponding
Figure 3: Two selected scatter diagrams when bias term is
added after inner-product operation. Please note that there
are one or two clusters that are located near the zero point. If
we normalize the features of the center clusters, they would
spread everywhere on the unit circle, which would cause
misclassiﬁcation. Best viewed in color.
label in range [1,n], W and b are the weight matrix and the bias
vector of the last inner-product layer before the softmax loss, Wj is
the j-th column of W , which is corresponding to the j-th class. In
the testing phase, we classify a sample by
Class(f) = i = arg max
i f + bi).
In this case, we can infer that (Wif +bi)−(Wjf +bj) ≥0, ∀j ∈[1,n].
Using this inequality, we obtain the following proposition.
Proposition 1. For the softmax loss with no-bias inner-product
similarity as its metric, let Pi(f) =
j f denote the probability
of x being classiﬁed as class i. For any given scale s > 1, if i =
arg maxj (W T
j f), then Pi(sf) ≥Pi(f) always holds.
The proof is given in Appendix 8.1. This proposition implies that
softmax loss always encourages well-separated features to have
bigger magnitudes. This is the reason why the feature distribution
of softmax is ‘radial’. However, we may not need this property as
shown in Figure2. By normalization, we can eliminate its eﬀect.
Thus, we usually use the cosine of two feature vectors to measure
the similarity of two samples.
However, Proposition 1 does not hold if a bias term is added
after the inner-product operation. In fact, the weight vector of
the two classes could be the same and the model still could make
a decision via the biases. We found this kind of case during the
MNIST experiments and the scatters are shown in Figure 3. It can be
discovered from the ﬁgure that the points of some classes all locate
around the zero point, and after normalization the points from each
of these classes may be spread out on the unit circle, overlapping
with other classes. In these cases, feature normalization may destroy
the discrimination ability of the speciﬁc classes. To avoid this kind
of risk, we do not add the bias term before the softmax loss in this
work, even though it is commonly used for classiﬁcation tasks.
Layer Deﬁnition
In this paper, we deﬁne ∥x∥2 =
i + ϵ, where ϵ is a small
positive value to prevent dividing zero. For an input vector x ∈Rn,
Figure 4: Lef: The normalization operation and its gradient
in 2-dimensional space. Please note that ∥x+α ∂L
∂x ∥is always
bigger than ∥x∥for all α > 0 because of the Pythagoras theorem. Right: An example of the gradients w.r.t. the weight
vector. All the gradients are in the tangent space of the unit
sphere (denoted as the blue plane). The red, yellow and green
points are normalized features from 3 diﬀerent classes. The
blue point is the normalized weight corresponding to the red
class. Here we assume that the model tries to make features
get close to their corresponding classes and away from other
classes. Even though we illustrate the gradients applied on
the normalized weight only, please note that opposite gradients are also applied on the normalized features (red, yellow, green points). Finally, all the gradients are accumulated
together to decide which direction the weight should be updated. Best viewed in color, zoomed in.
an L2 normalization layer outputs the normalized vector,
Here x can be either the feature vector f or one column of the
weight matrix Wi. In backward propagation, the gradient w.r.t. x
can be obtained by the chain-rule,
It is noteworthy that vector x and ∂L
∂x are orthogonal with each
other, i.e. ⟨x, ∂L
∂x ⟩= 0. From a geometric perspective, the gradient
∂x is the projection of ∂L
∂˜x onto the tangent space of the unit
hypersphere at normal vector ˜x (see Figure 4). From Figure 4 left, it
can be inferred that after update, ∥x∥2 always increases. In order
to prevent ∥x∥2 growing inﬁnitely, weight decay is necessary on
Reformulating Softmax Loss
Using the normalization layer, we can directly optimize the cosine
similarity,
d(f, Wi) =
where f is the feature and Wi represents the i-th column of the
weight matrix of the inner-product layer before softmax loss layer.
However, after normalization, the network fails to converge. The
loss only decreases a little and then converges to a very big value
within a few thousands of iterations. After that the loss does not
decrease no matter how many iterations we train and how small
the learning rate is.
This is mainly because the range of d(f, Wi) is only [−1, 1] after
normalization, while it is usually between (−20, 20) and (−80, 80)
when we use an inner-product layer and softmax loss. This low
range problem may prevent the probability Pyi (f; W) =
where yi is f’s label, from getting close to 1 even when the samples
are well-separated. In the extreme case,
e1+(n−1)e−1 is very small
(0.45 when n = 10; 0.007 when n = 1000), even though in this
condition the samples of all other classes are on the other side of
the unit hypersphere. Since the gradient of softmax loss w.r.t. the
ground truth label is 1 −Pyi , the model will always try to give large
gradients to the well separated samples, while the harder samples
may not get suﬃcient gradients.
To better understand this problem, we give a bound to clarify
how small the softmax loss can be in the best case.
Proposition 2. (Softmax Loss Bound After Normalization) Assume
that every class has the same number of samples, and all the samples
are well-separated, i.e. each sample’s feature is exactly same with its
corresponding class’s weight. If we normalize both the features and
every column of the weights to have a norm of ℓ, the softmax loss will
have a lower bound, log
1 + (n −1)e−n
, where n is the class
The proof is given in Appendix 8.2. Even though reading the
proof need patience, we still encourage readers to read it because
you may get better understanding about the hypersphere manifold
This bound implies that if we just normalize the features and
weights to 1, the softmax loss will be trapped at a very high value on
training set, even if no regularization is applied. For a real example,
if we train the model on the CASIA-Webface dataset (n = 10575),
the loss will decrease from about 9.27 to about 8.50. The bound for
this condition is 8.27, which is very close to the real value. This
suggests that our bound is very tight. To give an intuition for the
bound, we also plot the curve of the bound as a function of the
norm ℓin Figure 5.
After we obtain the bound, the solution to the convergence
problem is clear. By normalizing the features and columns of weight
to a bigger value ℓinstead of 1, the softmax loss can continue to
decrease. In practice, we may implement this by directly appending
a scale layer after the cosine layer. The scale layer has only one
learnable parameter s = ℓ2. We may also ﬁx it to a value that is large
enough referring to Figure 5, say 20 or 30 for diﬀerent class number.
However, we prefer to make the parameter automatically learned
by back-propagation instead of introducing a new hyper-parameter
for elegance. Finally, the softmax loss with cosine distance is deﬁned
j=1 es ˜W T
where ˜x is the normalized x.
Squared Norm `2
Loss Bound
n = 10,000
n = 100,000
Figure 5: The softmax loss’ lower bound as a function of features and weights’ norm. Note that the x axis is the squared
norm ℓ2 because we add the scale parameter directly on the
cosine distance in practice.
REFORMULATING METRIC LEARNING
Metric Learning, or speciﬁcally deep metric learning in this work,
usually takes pairs or triplets of samples as input, and outputs the
distance between them. In deep metric models, it is a common
strategy to normalize the ﬁnal features . It seems that
normalization does not cause any problems for metric learning loss
functions. However, metric learning is more diﬃcult to train than
classiﬁcation because the possible input pairs or triplets in metric
learning models are very large, namely O(N 2) combinations for
pairs and O(N 3) combinations for triplets, where N is the amount
of training samples. It is almost impossible to deal with all possible combinations during training, so sampling and hard mining
algorithms are usually necessary , which are tricky and timeconsuming. By contrast, in a classiﬁcation task, we usually feed the
data iteratively into the model, namely the input data is in order
of O(N). In this section, we attempt to reformulate some metric
learning loss functions to do the classiﬁcation task, while keeping
their compatibility with the normalized features.
The most widely used metric learning methods in the face veriﬁcation community are the contrastive loss ,
 ∥˜fi −˜fj ∥2
max(0,m −∥˜fi −˜fj ∥2
and the triplet loss ,
LT = max(0,m + ∥˜fi −˜fj ∥2
2 −∥˜fi −˜fk ∥2
ci = cj,ci , ck, (8)
where the two m’s are the margins. Both of the two loss functions
optimize the normalized Euclidean distance between feature pairs.
Note that after normalization, the reformulated softmax loss can
: class center
: gradient
Figure 6: Illustration of how the C-contrastive loss works
with two classes on a 3-d sphere (projected on a 2-d plane).
Lef: The special case of m = 0. In this case, the agents
are only inﬂuenced by features from their own classes. The
agents will ﬁnally converge to the centers of their corresponding classes. Right: Normal case of m = 1. In this case,
the agents are inﬂuenced by all the features in the same
classes and other classes’ features in the margin. Hence
the agents are shifted away from the boundary of the two
classes. The features will follow their agents through the
intra-class term ∥˜fi −˜Wj ∥2
2,ci = j as the gradients shown in
the ﬁgure. Best viewed in color.
also be seen as optimizing the normalized Euclidean distance,
j=1 es ˜W T
2 ∥˜fi−˜Wyi ∥2
2 ∥˜fi−˜Wj ∥2
because ∥˜x −˜y∥2
2 = 2 −2˜xT ˜y. Inspired by this formulation, we
modify one of the features to be one column of a weight matrix
W ∈Rd×n, where d is the dimension of the feature and n is the
class number. We call column Wi as the ‘agent’ of the i-th class.
The weight matrixW can be learned through back-propagation just
as the inner-product layer. In this way, we can get a classiﬁcation
version of the contrastive loss,
 ∥˜fi −˜Wj ∥2
max(0,m −∥˜fi −˜Wj ∥2
and the triplet loss,
LT′ = max(0,m + ∥˜fi −˜Wj ∥2
2 −∥˜fi −˜Wk ∥2
ci = j,ci , k. (11)
To distinguish these two loss functions from their metric learning
versions, we call them C-contrastive loss and C-triplet loss respectively, denoting that these loss functions are designed for classiﬁcation.
Intuitively, Wj acts as a summarizer of the features in j-th class.
If all classes are well-separated by the margin, theWj’s will roughly
correspond to the means of features in each class (Figure 6 left).
In more complicated tasks, features of diﬀerent classes may be
overlapped with each other. Then the Wj’s will be shifted away
from the boundaries. The marginal features (hard examples) are
contrastive loss:
triplet loss:
: minimize
: maximize
Figure 7: Classiﬁcation version of contrastive loss (Left) and
triplet loss (Right). The shadowed points are the marginal
features that got omitted due to the ‘agent’ strategy. In the
original version of the two losses, the shadowed points are
also optimized. Best viewed in color.
guided to have bigger gradients in this case (Figure 6 right), which
means they move further than easier samples during update.
However, there are some side eﬀect of the agent strategy. After
reformulation, some of the marginal features may not be optimized
if we still use the same margin as the original version (Figure 7).
Thus, we need larger margins to make more features get optimized.
Mathematically, the error caused by the agent approximation is
given by the following proposition.
Proposition 3. Using an agent for each class instead of a speciﬁc
sample would cause a distortion of
 d(f0, fj) −d(f0,Wi)2,
where Wi is the agent of the ith-class. The distortion is bounded by
j ∈Ci d(fj,Wi)2.
The proof is given in Appendix 8.3. This bound gives us a theoretical guidance of setting the margins. We can compute it on-the-ﬂy
during training using moving-average and display it to get better
feelings about the progress. Empirically, the bound
j ∈Ci d(fj,Wi)2
is usually 0.5 ∼0.6. The recommendation value of the margins of
the modiﬁed contrastive loss and triplet loss is 1 and 0.8 respectively.
Note that setting the margin used to be complicated work .
Following their work, we have to suspend training and search for a
new margin for every several epochs. However, we no longer need
to perform such a searching algorithm after applying normalization.
Through normalization, the scale of features’ magnitude is ﬁxed,
which makes it possible to ﬁx the margin, too. In this strategy, we
will not try to train models using the C-contrastive loss or the
C-triplet loss without normalization because this is diﬃcult.
EXPERIMENT
In this section, we ﬁrst describe the experiment settings in Section
5.1. Then we evaluate our method on two diﬀerent datasets with
two diﬀerent models in Section 5.2 and 5.3. Codes and models are
released at 
Implementation Details
Baseline works. To verify our algorithm’s universality, we choose
two works as our baseline, Wu et. al.’s model 2 (Wu’s model,
2 
for short) and Wen et. al.’s model 3 (Wen’s model, for short).
Wu’s model is a 10-layer plain CNN with Maxout activation unit.
Wen’s model is a 28-layer ResNet trained with both softmax
loss and center loss. Neither of these two models apply feature
normalization or weight normalization. We strictly follow all the
experimental settings as their papers, including the datasets4, the
image resolution, the pre-processing methods and the evaluation
Training. The proposed loss functions are appended after the feature layer, i.e. the second last inner-product layer. The features and
columns of weight matrix are normalized to make their L2 norm
to be 1. Then the features and columns of the weight matrix are
sent into a pairwise distance layer, i.e. inner-product layer to produce a cosine similarity or Euclidean distance layer to produce a
normalized Euclidean distance. After calculating all the similarities
or distances between each feature and each column, the proposed
loss functions will give the ﬁnal loss and gradients to the distances.
The whole network models are trained end to end. To speed up the
training procedure, we ﬁne-tune the networks from the baseline
models. Thus, a relatively small learning rate, say 1e-4 for Wu’s
model and 1e-3 for Wen’s model, are applied to update the network
through stochastic gradient descent (SGD) with momentum of 0.9.
Evaluation. Two datasets are utilized to evaluate the performance,
one is Labeled Face in the Wild (LFW) and another is Youtube
Face (YTF) . 10-fold validation is used to evaluate the performance for both datasets. After the training models converge, we
continue to train them for 5, 000 iterations5, during which we save
a snapshot for every 1, 000 iterations. Then we run the evaluation
codes on the ﬁve saved snapshots separately and calculate an average score to reduce disturbance. We extract features from both
the frontal face and its mirror image and merge the two features
by element-wise summation. Principle Component Analysis (PCA)
is then applied on the training subset of the evaluation dataset to
ﬁt the features to the target domain. Similarity score is computed
by the cosine distance of two sample’s features after PCA. All the
evaluations are based on the similarity scores of image pairs.
Experiments on LFW
The LFW dataset contains 13, 233 images from 5, 749 identities, with large variations in pose, expression and illumination. All
the images are collected from Internet. We evaluate our methods
through two diﬀerent protocols on LFW, one is the standard unrestricted with labeled outside data , which is evaluated on 6, 000
image pairs, and another is BLUFR which utilize all 13, 233
images. It is noteworthy that there are three same identities in
CASIA-Webface and LFW . We delete them during training
to build a complete open-set validation.
We carefully test almost all combinations of the loss functions
on the standard unrestricted with labeled outside data protocol. The
results are listed in Table 2. Cosine similarity is used by softmax +
any loss functions. The distance used by C-contrastive and C-triplet
loss is the squared normalized Euclidean distance. The C-triplet
3 
4 Since the identity label of the Celebrity+ dataset is not publicly available, we
follow Wen’s released model which is trained on CASIA-Webface only. Wu’s
model is also trained on CASIA-Webface only.
5In each iteration we train 256 samples, i.e. the batch size is 256.
softmax + C-contrastive
softmax + center
softmax only
C-contrastive only
Figure 8: LFW accuracies as a function of the loss weight of
C-contrastive loss or center loss with error bars. All these
methods use the normalization strategy except for the baseline.
Table 2: Results on LFW 6,000 pairs using Wen’s model 
loss function
Normalization
softmax + dropout
softmax + center 
feature only
weight only
99.16% ± 0.025%
softmax + center
99.17% ± 0.017%
C-contrasitve
99.15% ± 0.017%
99.11% ± 0.008%
C-triplet + center
99.13% ± 0.017%
softmax + C-contrastive
99.19% ± 0.008%
+ center loss is implemented by forcing to optimize ∥xi −Wj ∥2
even if m + ∥xi −Wj ∥2
2 −∥xi −Wk ∥2
2 is less than 0. From Table 2
we can conclude that the loss functions have minor inﬂuence on
the accuracy, and the normalization is the key factor to promote
the performance. When combining the softmax loss with the Ccontrastive loss or center loss, we need to add a hyper-parameter
to make balance between the two losses. The highest accuracy,
99.2167%, is obtained by softmax + 0.01 ∗C-contrastive. However,
pure softmax with normalization already works reasonably well.
We have also designed two ablation experiments of normalizing
the features only or normalizing the columns of weight matrix only.
During experiments we ﬁnd that the scale parameter is necessary
when normalizing the feature, while normalizing the weight does
not need it. We cannot explain it so far. This is tricky but the network
will collapse if the scale parameter is not properly added. From Table
2 we can conclude that normalizing the feature causes performance
degradation, while normalizing the weight has little inﬂuence on
the accuracy. Note that these two special cases of softmax loss are
also ﬁne-tuned based on Wen’s model. When training from scratch,
faces in video 1
faces in video 2
histogramming
same identity
different identity
face similarities
histogram feature
Figure 9: (a): Illustration of how to generate a histogram
feature for a pair of videos. We ﬁrstly create a pairwise
score matrix by computing the cosine similarity between
two face images from diﬀerent video sequences. Then we accumulate all the scores in the matrix to create a histogram.
(b): Visualization of histogram features extracted from 200
video pairs with both same identities and diﬀerent identities. After collecting all histogram features, support vector
machine (SVM) using histogram intersection kernel(HIK) is
utilized to make a binary classiﬁcation.
normalizing the weights only will cause the network collapse, while
normalizing the features only will lead to a worse accuracy, 98.45%,
which is better than the conventional softmax loss, but much worse
than state-of-the-art loss functions.
In Figure 8, we show the eﬀect of the loss weights when using
two loss functions. As shown in the ﬁgure, the C-contrastive loss
is more robust to the loss weight. This is not surprising because Ccontrastive loss can train a model by itself only, while the center loss,
which only optimizes the intra-class variance, should be trained
with other supervised losses together.
To make our experiment more convincing, we also train some
of the loss functions on Wu’s model . The results are listed in
Table 4. Note that in , Wu et. al. did not perform face mirroring
when they evaluated their methods. In Table 4, we also present
the result of their model after face mirroring and feature merging.
As is shown in the table, the normalization operation still gives a
signiﬁcant boost to the performance.
On BLUFR protocol, the normalization technique works even
better. Here we only compare some of the models with the baseline
(Table 3). From Table 3 we can see that normalization could boost
the performance signiﬁcantly, which reveals that normalization
technique could perform much better when the false alarm rate
(FAR) is low.
Table 3: Results on LFW BLUFR protocol
loss function
Normalization
TPR@FAR=0.1%
DIR@FAR=1%
softmax + center 
C-triplet + center
softmax + C-contrastive
softmax 
C-contrastive
Table 4: Results on LFW 6,000 pairs using Wu’s model 
loss function
Normalization
softmax + mirror
98.75% ± 0.008%
C-contrastive
98.78% ± 0.017%
softmax + C-contrastive
98.71% ± 0.017%
Table 5: Results on YTF with Wen’s model 
loss function
Normalization
softmax + center 
softmax + HIK-SVM
C-triplet + center
C-triplet + center + HIK-SVM
softmax + C-contrastive
softmax + C-contrastive + HIK-SVM
Experiments on YTF
The YTF dataset consists of 3,425 videos of 1,595 diﬀerent people, with an average of 2.15 videos per person. We follow the unrestricted with labeled outside data protocol, which takes 5, 000 video
pairs to evaluate the performance.
Previous works usually extract face features from all frames or
some selected frames in a video. Then two videos can construct a
conﬁdence matrixC in which each elementCij is the cosine distance
of face features extracted from the i-th frame of the ﬁrst video and
j-th frame of the second video. The ﬁnal score is computed by the
average of all all elements in C. The one dimension score is then
used to train a classiﬁer, say SVM, to get the threshold of same
identity or diﬀerent identity.
Here we propose to use the histogram of elements in C as the
feature to train the classiﬁer. The bin of the histogram is set to
100 (Figure 9(a)). Then SVM with histogram intersection kernel
(HIK-SVM) is utilized to make a two-class classiﬁcation (Figure
9(b)). This method encodes more information compared to the one
dimensional mean value, and leads to better performance on video
face veriﬁcation.
The results are listed in Table 5. The models that perform better
on LFW also show superior performance on YTF. Moreover, the
newly proposed score histogram technique (HIK-SVM in the table)
can improve the accuracy further by a signiﬁcant gap.
CONCLUSION AND FUTURE WORK
In this paper, we propose to apply L2 normalization operation on
the features and the weight of the last inner-product layer when
training a classiﬁcation model. We explain the necessity of the normalization operation from both analytic and geometric perspective.
Two kinds of loss functions are proposed to eﬀectively train the
normalized feature. One is a reformulated softmax loss with a scale
layer inserted between the cosine score and the loss. Another is
designed inspired by metric learning. We introduce an agent strategy to avoid the need of hard sample mining, which is a tricky and
time-consuming work. Experiments on two diﬀerent models both
show superior performance over models without normalization.
From three theoretical propositions, we also provide some guidance
on the hyper-parameter setting, such as the bias term (Proposition
1), the scale parameter (Proposition 2) and the margin (Proposition
Currently we can only ﬁne-tune the network with normalization
techniques based on other models. If we train a model with Ccontrastive loss function, the ﬁnal result is just as good as center
loss . But if we ﬁne-tune a model, either Wen’s model or
Wu’s model , the performance could be further improved as
shown in Table 2 and Table 4. More eﬀorts are needed to ﬁnd a way
to train a model from scratch, while preserving at least a similar
performance as ﬁne-tuning.
Our methods and analysis in this paper are general. They can be
used in other metric learning tasks, such as person re-identiﬁcation
or image retrieval. We will apply the proposed methods on these
tasks in the future.
ACKNOWLEDGEMENT
This paper is funded by Oﬃce of Naval Research (N00014-15-1-
2356), National Science Foundation (CCF-1317376), the National
Natural Science Foundation of China (61671125, 61201271, 61301269)
and the State Key Laboratory of Synthetical Automation for Process
Industries (NO. PAL-N201401).
We thank Chenxu Luo and Hao Zhu for their assistance in formula derivation.