Emergence of Scenario-Appropriate Collaborative Behaviors
for Teams of Robotic Bodyguards
Hassam Ullah Sheikh
University of Central Florida
Orlando, Florida
 
Ladislau Bölöni
University of Central Florida
Orlando, Florida
 
We are considering the problem of controlling a team of robotic
bodyguards protecting a VIP from physical assault in the presence
of neutral and/or adversarial bystanders in variety of scenarios. This
problem is challenging due to the large number of active entities
with different agendas and dynamic movement patterns, the need of
cooperation between the robots as well as the requirement to take
into consideration criteria such as social norms in addition to the
main goal of VIP safety.
In this paper we show how a multi-agent reinforcement learning
approach can evolve behavior policies for teams of robotic bodyguards that outperform hand-engineered approaches. Furthermore,
we propose a novel multi-agent reinforcement learning algorithm
inspired by universal value function approximators that can learn
policies which exhibit appropriate, distinct behavior in environments
with different requirements.
Multi-Agent Reinforcement Learning; Robot Team Formation; Multi-
Robot Systems
INTRODUCTION
Recent progress in the field of autonomous robotics makes it feasible
for robots to interact with multiple humans in public spaces. In this
paper, we are considering a practical problem where a human VIP
moving in various crowded scenarios is protected from physical
assault by a team of robotics bodyguards. This problem has been
previously explored in where explicitly programmed behaviors
of robots were used to carry out the task.
With the recent advancements in the single agent Deep RL , there has been a renewed interest in multi-agent reinforcement
learning (MARL) . Despite having outstanding performance
in multiplayer games like Dota 2 and Quake III Capture-the-
Flag , MARL algorithms have failed to learn policies that can
work in different scenarios .
Providing physical protection to a VIP through robot bodyguards
is a complex task where the robots must take into account the position and movement of the VIP, the bystanders and other robots.
The variety of environments and scenarios in which the bodyguards
need to act presents another challenge. We aim to solve the VIP
Proc. of the 18th International Conference on Autonomous Agents and Multiagent
Systems , N. Agmon, M. E. Taylor, E. Elkind, M. Veloso (eds.), May 2019,
Montreal, Canada
© 2019 International Foundation for Autonomous Agents and Multiagent Systems
(www.ifaamas.org). All rights reserved.
 
protection problem through multi-agent deep reinforcement learning while simultaneously learning to communicate and coordinate
between the robots. We propose a novel general purpose technique
that allows multi-agent learners to learn distributed policies not only
over the state space but also over a set of scenarios. We show that
our solution outperforms a custom designed behavior, the quadrant
load balancing method .
THE VIP PROTECTION PROBLEM
We are considering a VIP moving in a crowd of bystanders B =
{b1,b2, . . . ,bm} protected from assault by a team of robot bodyguards R = {r1,r2, . . . ,rn}. To be able to reason about this problem,
we need to quantify the threat to the VIP at a given moment - the
aim of the bodyguards is to reduce this value.
Using the threat model defined in , the residual threat RT is
defined as the threat to the VIP at time t from bystanders B. The
cumulative residual threat to the VIP from bystanders B in the
presence of bodyguards R over the time period [0,T] is defined as:
(1 −RT (VIP,bi,R))dt
Our end goal is to minimize CRT through multi-agent reinforcement
learning. Moreover eq. (1) also forms the basis of our reward function
for the VIP protection problem.
MULTI-AGENT UNIVERSAL POLICY
To solve the VIP protection problem under various scenarios, we
propose multi-agent universal policy gradient: a multi-agent deep
reinforcement learning algorithm that learns distributed policies not
only over state space but also over a set of scenarios.
Our approach uses Universal Value Function Approximators 
to train policies and value functions that take a state-scenario pair
as input. The outcome are a universal multi-agent policies that are
able to perform on multiple scenarios as compared to policies that
are trained and tested separately.
The main idea is to represent the different value function approximators for each agent i by a single unified value function approximator that generalizes of over both state space and the scenarios. For
agent i we consider Vi (s,д;ϕ) ≈V ∗
iд (s) or Qi (s,a,д;ϕ) ≈Q∗
that approximate the optimal unified value functions over multiple scenarios and a large state space. These value functions can
be used to extract policies implicitly or as critics for policy gradient methods. We extend the idea of MADDPG with universal
functional approximator, specifically we augment the centralized
critic with scenario. Concretely, consider N agents with policies
 
AAMAS’19, May 2019, Montreal, Canada
Hassam Ullah Sheikh and Ladislau Bölöni
π1, . . . ,πN
πN } parameterized by θθθ = {θ1
θ1, . . . ,θN
θN } learning polices over GGG scenarios the multi-agent universal policy gradient for
agent i can written as
∇Jθi = Es,a,д∼D
∇θi πi (ai |oi,д) ∇aiQπ
i (s,a1, . . . ,aN ,д)
where s = (o1, . . . ,oN ), Qπ
i (s,a1, . . . ,aN ,д) is a centralized actionvalue function that takes the actions of all the agents, the state of
the environment and the scenario to estimate the Q-value for agent i,
ai = πi (oi,д) is action from agent i following policy πi in scenario
д and D is the experience replay buffer.
EXPERIMENTS
To investigate the effective of our proposed solution, we designed
four scenarios inspired from possible real world situations of VIP
protection and implemented them as behaviors in the Multi-Agent
Particle Environment . In each scenario, the participants are the
VIP, four robot bodyguards and one or more classes of bystanders.
The scenario description contains a number of landmarks, points
on a 2D space that serve as starting point and destinations for the
goal-directed movement by the agents. For each scenario, the VIP
(brown disk) starts from the starting point and moves towards the
destination landmark (green disk). The VIP exhibits a simple path
following behavior, augmented with a simple social skill metric: it
is about to enter the personal space of a bystander, it will slow down
or come to a halt.
(A) Random Landmark: In this scenario, landmarks are placed
randomly in the area. The bystanders are performing random
waypoint navigation: they pick a random landmark, move
towards it, and when they reached it, they choose a new
destination.
(B) Shopping Mall: In this scenario, landmarks are placed in
fixed position on the periphery of the area, representing shops
in a market. The bystanders visit randomly selected shops.
(C) Street: The bystanders are moving towards waypoints that
are outside the current area. However, due to their proximity
to each other, the position of the other bystanders influence
their movement described by laws of particles motion .
(D) Pie-in-the-Face: In this “red carpet” scenario the one bystander take an active interest in the VIP. The Unruly bystander break the limit imposed by the line and try to approach
the VIP (presumably, to throw a pie in his/her face).
The observation of each agent is the physical state of the closest
five bystanders in the environment and verbal utterances of all the
agents oi =
xj,...5,ck,...N
∈Oi where xj is the observation of the
entity j from the perspective of agent i and ck is the verbal utterance
of the agent k. The scenario д is represented as a one hot vector.
In order to verify the claim that MARL algorithms trained on specific scenario fail to generalize over different scenarios, we evaluate
policies trained via MADDPG on specific scenario and tested them
on different scenarios.
Figure 1: A confusion matrix representing the average residual
threat values of MADDPG policies trained on specific scenario
when tested on different scenarios over 100 episodes.
From the results shown in Figure 1 we can see that MADDPG
policies trained on specific scenarios performed poorly when tested
on different scenarios as compared to when tested on same scenario
with different seeds. In order to tackle the generalization problem,
we train the agents using multi-agent universal policy gradient and
compare with the results of scenario-dependant MADDPG policies
and quadrant load balancing(QLB): a hand engineered technique to
solve the VIP protection problem.The results can be seen in Figure 2.
Random Landmarks
Shopping Mall
Pie-in-the-face
Residual Threat
Quadrant Load Balancing
Figure 2: Comparing the average residual threat values for universal policy agents with MADDPG and QLB agents
CONCLUSIONS
In this paper, we highlighted the generalization problem faced by
multi-agent reinforcement learning across different scenarios. To
solve that problem we presented a novel algorithm that not only
generalizes over state space but also over different scenarios. Using
our solution, we solved the problem of providing physical protection
to a VIP moving in a crowded space that outperforms another stateof-the-art multi-agent reinforcement learning algorithm as well as
quadrant load-balancing: a hand engineered technique to solve the
VIP protection problem.
Acknowledgement: This research was sponsored by the Army
Research Laboratory and was accomplished under Cooperative
Agreement Number W911NF-10-2-0016. The views and conclusions contained in this document are those of the authors only.
Collaborative Behaviors for Teams of Robotic Bodyguards
AAMAS’19, May 2019, Montreal, Canada