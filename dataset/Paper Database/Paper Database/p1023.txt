Training Hierarchical Feed-Forward Visual Recognition
Models Using Transfer Learning from Pseudo-Tasks
Amr Ahmed1,⋆, Kai Yu2, Wei Xu2, Yihong Gong2, and Eric Xing1
1 School of Computer Science, Carnegie Mellon University
{amahmed,epxing}@cs.cmu.edu
2 NEC Labs America, 10080 N Wolfe Road, Cupertino, CA 95014
{kyu,xw,ygong}@sv.nec-labs.com
Abstract. Building visual recognition models that adapt across different domains is a challenging task for computer vision. While feature-learning machines
in the form of hierarchial feed-forward models (e.g., convolutional neural networks) showed promise in this direction, they are still difﬁcult to train especially
when few training examples are available. In this paper, we present a framework
for training hierarchical feed-forward models for visual recognition, using transfer learning from pseudo tasks. These pseudo tasks are automatically constructed
from data without supervision and comprise a set of simple pattern-matching operations. We show that these pseudo tasks induce an informative inverse-Wishart
prior on the functional behavior of the network, offering an effective way to incorporate useful prior knowledge into the network training. In addition to being
extremely simple to implement, and adaptable across different domains with little
or no extra tuning, our approach achieves promising results on challenging visual
recognition tasks, including object recognition, gender recognition, and ethnicity
recognition.
Introduction
Visual recognition has proven to be a challenging task for computer vision. This dif-
ﬁculty stems from the large pattern variations under which an automatic recognition
system must operate. Surprisingly, this task is extremely easy for humans, even when
very few examples are available to the learner. This superior performance is in fact due
to the expressive hierarchical representation employed by human visual cortex. Therefore, it has been widely believed that building robust invariant feature representation is
a key step toward solving visual recognition problems.
In the past years, researchers have designed various features that capture different
invariant aspects in the image, to name a few: shape descriptors , appearance descriptors like SIFT features and their variants , etc. A classiﬁer is then feed with
this representation to learn the decision boundaries between the object classes. On the
other hand, many efforts have been put toward building trainable vision systems in
the form of hierarchical feed-forward models that learn the feature extractors and the
classiﬁcation model simultaneously. This approach emulates processing in the visual
⋆Work mainly done while the author was interning at NEC labs.
D. Forsyth, P. Torr, and A. Zisserman (Eds.): ECCV 2008, Part III, LNCS 5304, pp. 69–82, 2008.
c⃝Springer-Verlag Berlin Heidelberg 2008
A. Ahmed et al.
cortex and is reminiscent of the Hubel-Wiesel architecture . While we concede that
given enough time and proper understanding of a particular visual recognition problem, researchers can devise ingenious feature extractors that would achieve excellent
classiﬁcation performance especially when the learner is faced with few examples, we
believe that it is hard to devise a single set of features that are universally suitable for all
recognition problems. Therefore, it is believed that learning the features automatically
via biologically inspired models will open the door for more robust methods with wider
applications.
In this paper, we focus on Convolutional Neural Networks (CNNs) as an example of
trainable hierarchical feed-forward models . CNNs have been successfully applied
to a wide range of applications, including character recognition, pose estimation, face
detection, and recently generic object recognitions. The model is very efﬁcient in the
recognition phase because of their feed-forward nature. However, this generality and
capacity of handling a wide variety of domains comes with a price: the model needs a
very large number of labeled examples per class for training. To solve this problem, recently an approach has been proposed that utilizes unlabeled data in the training process
 . Even though the method improves the performance of the model, to date, the best
reported recognition accuracy on popular benchmarks like Caltech101 by hierarchical
feed-forward models are yet unsatisfactory .
In this paper, we present a framework for training hierarchical feed-forward models
by leveraging knowledge via transfer learning from a set of pseudo tasks which are automatically constructed from data without supervision . We show that these auxiliary
tasks induce a data-dependent inverse-Wishart prior on the parameters of the model.
The resulting framework is extremely simple to implement, in fact, nothing is required
beyond the ability to train a hierarchical feed-forward model via backpropagation. We
show the adaptability and effectiveness of our approach on various challenging benchmarks that include the standard object recognition datasets Caltech101, gender classiﬁcation, and ethnic origin recognition on face databases FERET and FRGC . Overall,
our approach, with minimal across-domain extra tuning, exhibits excellent classiﬁcation accuracy on all of these tasks, outperforming other feed-forward models and being
comparable to other state-of-the-art methods. Our results indicate that:
– Incorporation of prior knowledge via transfer learning can boost the performance
of CNNs by a large margin.
– Trainable hierarchical feedforward models, have the ﬂexibility to handle various
visual recognition tasks of different nature with excellent performance.
Related Work
Various techniques have been proposed that exploit locally invariant feature descriptors, to name a few: appearance descriptors based on SIFT features and their derivatives , shape descriptors , etc. Based on these feature descriptors, a similarity
measure is induced over images, either in the bag of word representation , or in a
multi-resolution representation . This similarity measure is then then used to train
a discriminative classiﬁer. While these approaches achieve excellent performance, we
Training CNN Using Transfer Learning from Pseudo-Tasks
believe that it is hard to devise a single set of features that are universally suitable for
all visual recognition problems.
Motivated by the excellent performance and speed of the human visual recognition
system, researchers explored the possibility of learning the features automatically via
hierarchical feedforward models that emulate processing in the visual cortex. These approaches are reminiscent of multi-stage Hubel-Wiesel architectures that use alternating
layers of convolutional feature detectors (simple cells) and local pooling and subsampling (complex cells) . Examples of this generic architecture include: , , 
in addition to Convolutional Neural Networks (CNN) (see Fig. 2). Several approaches have been proposed to train these models. In and the ﬁrst layer is
hard-wired with Gabor ﬁlters, and then large number of image patches are sampled
from the second layer and used as the basis of the representation which is then used to
train a discriminative classiﬁer. In CNN all the layers, including a ﬁnal layer for classiﬁcation, are jointly trained using the standard backpropagation algorithm . While
this approach makes CNN powerful machines with a capacity to adapt to various tasks,
it also means that large number of training examples are required to prevent overﬁtting.
Recently proposed a layer-wise greedy algorithm that utilizes unlabeled data for
pre-training CNNs. More recently, in , the authors proposed to train a feed-forward
model jointly with an unsupervised embedding task, which also leads to improved results. Though using unlabeled data too, our work differs from the previous work at the
more emphasis on leveraging the prior knowledge which suggests that our work can be
combined with those approaches to further enhance the training of feed-forward models
in general and CNN in particular, as we will discuss in section 4.
Finally, our work is also related to a generictransfer learning framework , which
uses auxiliary tasks to learn a linear feature mapping. The work here is motivated differently and aims toward learning complex nonlinear visual feature maps as we will
discuss in section 3.3. Moreover, in object recognition, transfer learning has been studied in the context of probabilistic generative models and boosting . In this paper
our focus is on using transfer learning to train hierarchical feedforward models by leveraging information from unlabeled data.
Transfer Learning
Transfer learning, also known as multi-task learning , is a mechanism that improves
generalization by leveraging shared domain-speciﬁc information contained in related
tasks. In the setting considered in this paper, all tasks share the same input space (X) and
each task m can be viewed as a function fm that maps between this space to an output
space: fm : X →Y . Intuitively, if the tasks are truly related, then there is a shared
structure between all of all them that can be leveraged by learning them in parallel. For
example, Fig 1-a depicts few tasks. In this ﬁgure it is clear that input points a and b1
have similar values across all of these tasks, and thus one can conclude that these two
input points are semantically similar, and therefore should be assigned similar values
1 Please note that the order of points along the x-axis does not necessarily encode similarity.
A. Ahmed et al.
under other related tasks. When the input space X represents images, the inclusion of
related tasks would help induce similarity measures between images that enhances the
generalization of the main task being learned. The nature of this similarity measure
depends on the architecture of the learning system. For instance, in a feed-forward
Neural Network (NN) with one hidden layer, all tasks would share the same hidden
representation (feature space) Φ(x) (see Fig. 1-b) and thus the inclusion of pseudo tasks
in this architecture would hopefully result in constraining the model to map semantically
similar points like a and b ,from the input space, to nearby positions in the feature space.
Problem Formulation
Since in this paper we mainly focus on feed-forward models, we will formulate our
transfer learning problem using a generic neural network learning architecture as in
Fig. 1-b. Let N be the number of input examples, and assume that the main task to be
learnt has index m with training examples Dm = {(xn, ymn)} . A neural network has
a natural architecture to tackle this learning problem by minimizing:
l (Dm, θ) + γΩ(θ)
where l (Dm, θ) amounts to an empirical loss
mΦ(xn; θ)) + α∥wm∥2
Ω(θ) is a regularization term on the parameters of the feature extractors Φ(x; θ) =
[φ1(x; θ) . . . φJ(x; θ)]T – this feature extractor, i.e. the hidden layer of the network,
maps from the input space to the feature space. Moreover, ℓm(·, ·) is the cost function
for the target task. Unlike the usual practice in neural networks where the regularization
on θ is similar to the one on wm, we adopt a more informative Ω(θ) by additionally
introducing Λ pseudo auxiliary tasks, each represented by learning the input-output
pairs: Dλ = {(xn, yλn)}N
n=1, where yλn = gλ(xn) are a set of real-valued functions
automatically constructed from the input data. As depicted in Fig. 1.b, all the tasks share
the hidden layer feature mapping. Moreover, we hypothesis that each pseudo auxiliary
function, gλ(xn), is linearly related to Φ(xn; θ) via the projection weights wλ. Then the
regularization term Ω(θ) becomes:
Training the network in 1.b to realize the objective function in (1) is extremely simple because nothing beyond the standard back-propagation algorithm is needed. By
constructing meaningful pseudo functions from input data, the model is equipped with
extensive ﬂexibilities to incorporate our prior knowledge. Furthermore, there is no restriction on the parametric form of Φ(x; θ), which allows us to apply learning problem
(1) to more complex models (e.g., the CNN shown in Fig. 2.a). Our experiments will
demonstrate that these advantages can greatly boost the performance of CNNs for visual
recognition.
Training CNN Using Transfer Learning from Pseudo-Tasks
Fig. 1. Illustrating the mechanism of transfer learning. (a) Functional view: tasks represented as
functional mapping share stochastic characteristics. (b) Transfer learning in neural networks, the
hidden layer represents the level of sharing between all the task.
A Bayesian Perspective
In this section we give a Bayesian perspective to the transfer learning problem formulated in Section 3.2. While (1, 2) are all what is needed to implement the proposed
approach, the sole purpose of this section is to give more insight to the role of the
pseudo tasks and to formalize the claims we made near the end of Section 3.1.
In Section 3.2, we hypothesized that the pseudo tasks are realizable as a linear projection from the feature mapping layer output, Φ(x; θ), that is:
λ Φ(x; θ) + e
where e ∼N(0, β−1). The intuition behind (3) is to limit the capacity of this mapping
so that the constraints imposed by the pseudo tasks can only be satisﬁed by proper adjustments of the feature extraction layer paraments, θ. To make this point more clear,
consider Fig. 1.a, and consider points like a and b which are assigned similar values
under many pseudo tasks. Under the restriction that the pseudo auxiliary tasks are realizable as a linear projection from the feature extraction layer output, and given an
appropriate number of such pseudo tasks, the only way that the NN can satisfy these
requirements, is to map points like a and b to nearby position in the feature space.
Therefore, the kernel induced by the NN, K(xi, xj; θ), via its feature mapping function
Φ(.; θ), is constrained to be similar to the kernel induced by the pseudo tasks, where the
degree of similarity is controlled via the parameter γ in (1). Below we will make this
intuition explicit.
We ﬁrst begin by writing the empirical loss due to the pseudo auxiliary tasks,
L({Dλ}, θ, {wλ}), where we make the dependency on {wλ} explicit, as follows:
L({Dλ}, θ, {wλ}) =
If we assumer that wλ ∼N(0, I), and that e ∼N(0, β−1), then it is clear that (4)
is the negative log-likelihood of {Dλ} under these mild Gaussian noise assumptions.
A. Ahmed et al.
In Section 3.2, we decided to minimize this loss over {wλ}, which gives rise to the
regularizer term, Ω(θ). Here, we will take another approach, and rather integrate out
{wλ} from (4), which results in the following fully Bayesian regularizer, ΩB(θ):
2 log det(ΦTΦ + β−1I) + Λ
(ΦTΦ + β−1I)−1KΛ
where KΛ =
and Kλ = [gλ(xi)gλ(xj)]N
i,j=1. If we let K(θ) denotes the Kernel induced by the NN feature mapping layer, where K(xi, xj, θ)=⟨Φ(xi; θ), Φ(xj; θ)⟩
+ δijβ−1 , then (5) can be written as:
2 log det(K(θ)) + Λ
It is quite easy to show that (6) is equivalent to a loss term due to an inverse-wishart
prior, IW(Λ, KΛ), placed over K(θ). Put it another way, (6) is the KL-divergence between two multivariate normal distributions with zero means and covariance matrices
given by K(θ) and KΛ. Therefore, in order to minimize this loss term the leaner is
biased to make the kernel induced by the NN,K(θ), as similar as possible to the kernel induced by the pseudo-tasks,KΛ, and this helps regularize the functional behavior
of the network, especially when there are few training examples available. In Section
3.2, we choose to use the regularizer, Ω(θ) as a proxy for ΩB(θ) for efﬁciency as it
is amenable to efﬁcient integration with the online stochastic gradient descent algorithm used to train the NN, whereas ΩB(θ) requires gradient computations over the
whole pseudo auxiliary task data sets, for every step of the online stochastic gradient
algorithm. This decision turns out to be a sensible one, and results in an excellent performance as will be demonstrated in Section 6.
Transfer Learning in CNNs
There are no constraints on the form of the feature extractors Φ(.; θ) nor on how they
are parameterized given θ, therefore, our approach is applicable to any feed-forward
architecture as long as Φ(.; θ) is differentiable, which is required to train the whole
model via backpropagation. A popular architecture that showed excellent performance
for visual recognition is the CNN architecture, see Fig. 2.a, which is an instance of
multi-stage Hubel-Wiesel architectures , . The model includes alternating layers
of convolutional feature detectors (C layers), and local pooling of feature maps using
a max or an averaging operation (P layers), and a ﬁnal classiﬁcation layer. Detailed
descriptions of CNNs can be found in . Applying the transfer learning framework
described in Section 3 to CNNs results in the architecture in Fig. 2-a. The pseudo tasks
are extracted as described in Section 5 and the whole resulting architecture is then
trained using standard backpropagation to minimize the the objective function in (1).
Throughout the experiments of this paper, we applied CNNs with the following architecture: (1) Input: 140x140 pixel images, including R/G/B channels and additionally
two channels Dx and Dy, which are the horizontal and vertical gradients of gray intensities; (2) C1 layer: 16 ﬁlters of size 16 × 16; (3) P1 layer: max pooling over each
Training CNN Using Transfer Learning from Pseudo-Tasks
Fig. 2. Joint training using transfer-learning from pseudo-tasks
5 × 5 neighborhood; (4) C2 layer: 256 ﬁlters of size 6 × 6, connections with sparsity2
0.5 between the 16 dimensions of P1 layer and the 256 dimensions of C2 layer; (5) P2
layer: max pooling over each 5 × 5 neighborhood; (6) output layer: full connections
between 256 × 4 × 4 P2 features and outputs. Moreover, we used least square loss for
pseudo tasks and hinge loss for classiﬁcation tasks. Every convolution ﬁlter is a linear
function followed by a sigmoid transformation (see for more details).
It is interesting to contrast our approach with the layer-wise training one in .
In , each feature extraction layer is trained to model its input in a layer-wise fashion: the ﬁrst layer is trained on the raw images and then used to produce the input to
the second feature extraction layer. The whole resulting architecture is then used as a
multilayered feature extractor over labeled data, and the resulting representation is then
used to feed an SVM classiﬁer. On contrast, in our approach, we jointly train the classi-
ﬁer and the feature extraction layers, thus the feature extraction layer training is guided
by the pseudo-tasks as well as the labeled information simultaneously. Moreover, we
believe that the two approaches are orthogonal as we might ﬁrst pre-train the network
using the method in , and then use the result as a starting point for our method. We
leave this exploration for future work.
Generating Pseudo Tasks
We use a set of pseudo tasks to incorporate prior knowledge into the training of recognition models. Therefore, these tasks need to be 1) automatically computable based
on unlabeled images, and 2) relevant to the speciﬁc recognition task at hand, in other
words, it is highly likely that two semantically similar images would be assigned similar
outputs under a pseudo task.
A simple approach to construct pseudo tasks is depicted in Fig. 4. In this ﬁgure, the
pseudo-task is constructed by sampling a random 2D patch and using it as a template
to form a local 2D ﬁlter that operates on every training image. The value assigned to an
image under this task is taken to be the maximum over the result of this 2D convolution
operation. Following this method, one can construct as many pseudo-tasks as required.
2 In other words, on average, each ﬁlter in C2 is connected to a randomly chosen 8 dimensions
(ﬁlter maps) from P1.
A. Ahmed et al.
(a)Caltech101
(b)FRGC 2.0
Fig. 4. Simple pseudo task generation
Moreover, this construction satisﬁes condition (2) above as semantically similar images
are likely to have similar appearance. Unfortunately, this simple construction is brittle
with respect to scale, translation, and slight intensity variations, due to operating directly on the pixel-level of the image. Below, we show how to generalize this simple
approach to achieve mild local-invariance with respect to scale, translation and slight
intensity variations.
First, we processed all the images using a set of Gabor ﬁlters with 4 orientations
and 16 scales. This step aims toward focusing the pseudo-tasks on interesting parts of
the images by using our prior knowledge in the form of a set of Gabor ﬁlters. Then a
max-pooling operation, across scale and space, is employed to achieve mild scale and
translation-invariance. We then apply the simple method detailed above to this representation. It is interesting to note that this construction is similar in part to which
used random patches as the parameters of feed-forward ﬁlters which is later used as
the basis for the representation. The detailed procedure is as follows, assuming each
image is a 140 × 140 gray image: (1) Applying Gabor ﬁlters result in 64 feature maps
of size 104 × 104 for each image; (2) Max-pooling operation is performed ﬁrst within
each non-overlapping 4 × 4 neighborhood and then within each band of two successive
scales resulting in 32 feature maps of size 26×26 for each image; (3) An set of K RBF
ﬁlter of size 7 × 7 with 4 orientations are then sampled and used as the parameters of
the pseudo-tasks. To generate the actual values of a given pseudo-task, we ﬁrst process
each training image as above, and then convolve the resulting representation with this
pseudo-task’s RBF ﬁlter. This results in 8 feature maps of size 20 × 20; Finally, max
pooling is performed on the result across all the scales and within every non-overlapping
10 × 10 neighborhood, giving a 2 × 2 feature map which constitutes the value of this
image under this pseudo-task. Note that in the last step instead of using a global maxpooling operator over the whole image, we maintained some 2D spatial information by
this local max operator, which means that the pseudo-tasks are 4-dimensional vectorvalued functions, or equivalently, we obtained 4 ∗K pseudo-tasks (K actual random
patches, each operating at a different quadrant of the image).
These pseudo-tasks encode our prior knowledge that a similarity matching between
an image and a spatial pattern should tolerate a small change of scale and translation
as well as slight intensity variation. Thus, we can use these functions as pseudo tasks
to train our recognition models. We note that the framework can generally beneﬁt from
Training CNN Using Transfer Learning from Pseudo-Tasks
all kinds of pseudo task constructions that comply with our prior knowledge for the
recognition task at hand. We have tried other ways like using histogram features of
spatial pyramid based on SIFT descriptors and achieved a similar level of accuracy.
Due to space limitation, we only report the results using the method detailed in this
Experimental Results
To demonstrate the ability of our framework to adapt across domains with little tuning, ﬁrst, we ﬁxed the architecture of CNN as descried in Section 4. Second, we ﬁxed
the number of pseudo tasks K = 1024. To speed up the training phase, we apply PCA
to reduce these resulting pseudo-tasks to 300 ones. Moreover, in order to ensure that
the neural network is trained with balanced outputs, we further project these 300 dimensions using a random set of 300 orthonormal bases and scale each of the response
dimensions to have a unitary variance.
Object Recognition
We conducted experiments on the Caltech-101 database, which contains 102 categories
(including 101 object categories plus a background category) of object images, with
from 31 to 800 images per category. We chose Caltech-101, because the data set is considered one of the most diverse object databases available today, and more importantly,
is probably the most commonly tested benchmark in the literature of object recognition, which makes our results directly comparable with those of others. We follow the
standard setting in the literature, namely, train on 15/30 images per class and test on
the rest. For efﬁciency, we limit the number of test images to 30 per class. Note that,
because some categories are very small, we may end up with less than 30 test images.
To reduce the overweight of popular categories, we ﬁrst compute the accuracy within
each category and then compute the average over all the categories. All the experiments
were randomly repeated for 5 trails.
Table 1. Categorization accuracy of different hierarchical feed-forward models on
Caltech-101
Training Size
HMAX-1 
HMAX-2 
CNN + Pretraining 
23.9% 25.1%
CNN+Transfer
58.1% 67.2%
1 shows the comparison of our
results with those reported in the literature
using similar hierarchical feed-forward models on the same settings of experiments. The
baseline method “CNN”, i.e., CNN trained
without pseudo tasks, presented very poor accuracy, which is close to the phenomenon observed in . The “CNN+Pretraining” approach made a signiﬁcant improvement by
ﬁrst training a encoder-decoder architecture
with unlabeled data, and then feeding the result of applying the encoder on labeled data
to an SVM classifer . The idea was inspired by that suggested an unsupervised
layer-wise training to improve the performance of deep belief networks. Our strategy
“CNN+Pseudo Tasks” also improved the baseline CNN by a large margin, and achieved
A. Ahmed et al.
the best results of hierarchical feedforward architectures on the Caltech 101 data set. To
better understand the difference made by transfer learning with pseudo tasks, we visualize the learnt ﬁrst-layer ﬁlters of CNNs in Fig. 5 (a) and (b). Due to lacking of sufﬁcient
supervision in such a high-complexity learning task, a bit surprisingly, CNN cannot
learn any meaningful ﬁlters. In contrast, thanks to the additional bits of information offered by pseudo tasks, CNN ends up with much better ﬁlters. Our result is comparable
to the state-of-the-art accuracy, i.e., 64.6% ∼67.6% in the case of 30 training images
per class, achieved by the spatial pyramid matching (SPM) kernel based on SIFT features . However, the feedforward architecture of CNN can be more efﬁcient in
recognition phase. In our experiments, it takes in average 0.18 second in a PC with
2.66 GHz CPU, to process one 140 × 140 color image, including feature extraction and
classiﬁcation.
Gender and Ethnicity Recognition
In this section we work on gender and ethnicity recognitions based on facial appearance.
We use the FRGC 2.0 (Face Recognition Grand Challenge ) data set, which contains
568 individuals’ face images under various lighting conditions and backgrounds, presenting in total 14714 face images. Beside person identities, each image is annotated
with gender, age, race, as well as positions of eyes and nose. Each face image is aligned
based on the location of eyes, and normalized to be with zero mean and unitary length.
We note that the data set is not suitable for research on age prediction, because majority
of individuals are young students.
We built models for binary gender classiﬁcation and 3-class ethnicity recognition,
i.e., classifying images into “white”, “asian”, and “other”. For comparison, we implemented two state-of-the-art algorithms that both utilize holistic facial information: one
is “SVM+SPM”, namely, the SVM classiﬁer using SPM kernels based on dense SIFT
descriptors, as described by ; the other is “SVM+RBF”, namely, the SVM classiﬁer
using radius basis function (RBF) kernels operating directly on the aligned face images.
The second approach has demonstrated state-of-the-art accuracy for gender recognition
 . We ﬁx 114 persons’ 3014 images (randomly chosen) as the testing set, and train
the recognition models with various randomly selected 5%, 10%, 20%, 50%, and “All”
of the remaining data, in order to examine the model’s performance given different
training sizes. Note that we strictly ensure that a particular individual appear only in the
test set or training set. For each training size, we randomize the training data 5 times
and report the average error rate as well as the standard deviation. The results are shown
in Table 2 and Table 3.
Table 2. Error of gender recognition on the FRGC data set
Training Size
16.7 ± 2.4% 13.4 ± 2.4% 11.3 ± 1.0% 9.1 ± 0.5% 8.6%
15.3 ± 2.9% 12.3 ± 1.1% 11.1 ± 0.6% 10.3 ± 0.8% 8.7%
61.5 ± 7.3% 17.2 ± 4.3% 8.4 ± 0.5%
6.6 ± 0.3% 5.9%
CNN+Transfer 16.9 ± 2.0% 7.6 ± 1.1%
5.8 ± 0.3%
5.1 ± 0.2% 4.6%
Training CNN Using Transfer Learning from Pseudo-Tasks
Table 3. Error of ethnicity recognition on the FRGC data set
Training Size
22.9 ± 4.7% 16.9 ± 2.3% 14.1 ± 2.2% 11.3 ± 1.0% 10.2%
23.7 ± 3.2% 22.7 ± 3.6% 18.0 ± 3.6% 15.8 ± 0.7% 14.1%
30.0 ± 5.1% 13.9 ± 2.4% 10.0 ± 1.0% 8.2 ± 0.6% 6.3%
CNN+Transfer 16.0 ± 1.7% 9.2 ± 0.6%
7.9 ± 0.4%
6.4 ± 0.3% 6.1%
Fig. 5. First-layer ﬁlters on the B channel, learnt from both supervised CNN and CNN with
transfer Learning. top: ﬁlters learnt from supervised CNN. bottom: ﬁlters learnt using transfer
learning from pseudo-tasks. ﬁrst column: Caltech-101 (30 examples per class); second column:
FRGC-gender; and third column: FRGC-Race.
From Table 2 and 3 we have the following observations: (1) The two competitor
methods resulted in comparable results for gender classiﬁcation, while for ethnicity
recognition SVM+RBF is more accurate than SVM+SPM; (2) In general, CNN models outperformed the two competitors for both gender and ethnicity recognition, especially when sufﬁcient training data were given; (3) CNN without transfer learning
produced very poor results when only 5% of the total training data were provided; (4)
“CNN+Transfer” signiﬁcantly boosted the recognition accuracy in nearly all the cases.
In cases of small training sets, the improvement was dramatic. In the end, our methods
achieved 4.6% error rate for gender recognition and 6.1% for ethnicity recognition.
Interestingly, although CNN and “CNN+Transfer” resulted in close performances
when all the training data were employed, the ﬁlters learnt by CNN+Transfer (visualized in Fig. 5.d appear to be much smoother than those learnt by CNN (shown in
Fig. 5.c3 Moreover, as indicated by Fig. 6, we also found that “CNN+Transfer” converged much faster than CNN during the stochastic gradient training, indicating another
advantage of our approach.
We note that the best performances our method achieved here are not directly comparable to those reported in , because their results are based on the FERET data
3 To save space, here we only show the ﬁlters of one channel for gender and ethnicity recognition. However the same phenomenon was observed for ﬁlters of other channels.
A. Ahmed et al.
Table 4. Error of gender recognition on the FERET data set
RBF+SVM Boosting CNN CNN+Transfer
5.6% 2.3%
Fig. 6. Number of errors on test data over epochs, where dashed lines are results of CNN with
transfer learning, solid lines are CNN without transfer learning: (a) gender recognition; (b) ethnic
recognition
set4, which contains face images under highly controlled lighting conditions and simpler backgrounds.More importantly, as recently pointed by , their experiments mixed
up faces of same individuals across training and test sets, which made the results not
truly measuring the generalization performance of handling new individuals. To make
a direct comparison possible, we followed the experimental setting of as much as
possible, and conducted experiments on the FERET data for gender recognition, where
no individual is allowed to appear in the training and test simultaneously. The results
are summarized in Table 4, showing that“CNN+Transfer” achieved the best accuracy
on the FERET data set.
A Further Understanding of Our Approach
In the previous two subsections we showed that our framework, with little tuning, can
adapt across different domains with favorable performance. It is interesting to isolate
the source of this success. Is it only because of the informativeness of the pseudo-tasks
used? And if not, then is there a simpler way of combining the information from the
pseudo-tasks with its equivalent from a supervised CNN trained only on labeled data?
To answer the ﬁrst question, as we mentioned in Section 5, our pseudo-task construction overlaps with the the system in , 5, however, our results in Table 1 indicates
signiﬁcant improvement over these baseline. To answer the second question, we did an
additional experiment on Caltech101, using 30 training examples per category, to train
an SVM on the features produced by the pseudo-tasks alone or on the combined features
produced by the pseudo-tasks and the features from the last layer of a CNN trained via
purely supervised learning. The results were 49.6% and 50.6% respectively. This shows
that the gain from using the features from a supervised CNN was minimal. On the other
4 Available at 
5 In fact, the system in and its successor has other major features like inhibition,etc.
Training CNN Using Transfer Learning from Pseudo-Tasks
hand, our approach which involves joint-training of the whole CNN inherits the knowledge from the pseudo-tasks in the form of its induced kernel, as explained in Section
3.3, but is also supervised by labeled data and thus has the ability to further adapts its
induced kernel, K(θ), to better suit the task at hand.
Moreover, our approach results in an efﬁcient model at prediction time. In fact, the
pseudo-task extraction phase is computationally expensive and it took around 29 times
longer to process one image than a feedforward pass over the ﬁnal trained CNN. In
other words, we paid some overhead in the training phase to compute these pseudotasks once, but created a fast, compact, and accurate model for prediction.
Discussion, Conclusion, and Future Work
Beneﬁting from a deep understanding of a problem, hand-engineered features usually
demonstrate excellent performances. This success is in a large sense due to the fact that
the features are learnt by the smartest computational units – brains of researchers. In
this sense, hand-craft designing and automatic learning of visual features do not have
fundamental differences. An important indication of this paper is that, it is generally
hard to build a set of features that are universally suitable for all different tasks. For
example, the SPM kernel based on SIFT is excellent for object recognition, but may not
be good for gender and ethnicity recognition. Interestingly, an automatically learnable
architecture like CNN can adapt itself to a range of situations and learn signiﬁcantly
different features for object recognition and gender recognition (if comparing Fig. 5
(b) and (d)). We believe that given a sufﬁcient amount of time, very likely researchers
can come up with even better features for any visual recognition task. However, a completely trainable architecture can hopefully achieve good results for a less well-studied
task with minimum human efforts.
In this paper, we empirically observed that training a hierarchical feedforward architecture was extremely difﬁcult. We conjecture that the poor performance of CNN
on Caltech 101 is due to the lack of training data, given the large variation of object
patterns. In the tasks of gender and ethnicity recognitions, where we have sufﬁcient
data, CNNs in fact produced poor results on small training sets but excellent results
given enough training data (see Table 2 and Table 3). Therefore, when insufﬁcient
labeled examples are present, it is essential to use additional information to supervise
the network training.
We proposed using transfer learning to improve the training of hierarchical feedforward models. The approach has been implemented on CNNs, and demonstrated excellent performances on a range of visual recognition tasks. Our experiments showed
that transfer learning with pseudo tasks substantially improves the quality of CNNs
by incorporating useful prior knowledge. Our approach can be combined with the pretraining strategy , which remains an interesting future work.
Very recently, showed that detecting region of interest (ROI) can greatly boost the
performance of SPM kernel on Caltech 101. Our work is at the level of that builds
classiﬁer based on the whole image. In the future, it is highly interesting to develop a
mechanism of attention in CNNs that can automatically focus on the most interesting
region of images.
A. Ahmed et al.