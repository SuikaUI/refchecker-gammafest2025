Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3132–3142,
Hong Kong, China, November 3–7, 2019. c⃝2019 Association for Computational Linguistics
An End-to-End Generative Architecture for Paraphrase Generation
Qian Yang1∗, Zhouyuan Huo2, Dinghan Shen1
Yong Cheng3, Wenlin Wang1, Guoyin Wang1, Lawrence Carin1
1 Duke University
2 University of Pittsburgh
3 Google AI
 
Generating high-quality paraphrases is a fundamental yet challenging natural language
processing task. Despite the effectiveness of
previous work based on generative models,
there remain problems with exposure bias in
recurrent neural networks, and often a failure
to generate realistic sentences. To overcome
these challenges, we propose the ﬁrst endto-end conditional generative architecture for
generating paraphrases via adversarial training, which does not depend on extra linguistic information.
Extensive experiments on
four public datasets demonstrate the proposed
method achieves state-of-the-art results, outperforming previous generative architectures
on both automatic metrics (BLEU, METEOR,
and TER) and human evaluations.
Introduction
Paraphrases convey the same meaning as the original sentences or text, but with different expressions in the same language. Paraphrase generation aims to synthesize paraphrases of a given sentence automatically. This is a fundamental natural language processing (NLP) task, and it is important for many downstream applications . For example, paraphrases
can help diversify the response of chatbot engines
 , strengthen question answering
 , augment relation
extraction , and extend the
coverage of semantic parsers exploit how linguis-
∗Corresponding author
tic knowledge can improve the quality of generated paraphrases, including shallow linguistic features , and syntactic and semantic information . However, they are often domainspeciﬁc and hard to scale, or yield inferior results.
With the help of growing large data and neural network models, recent studies have shown
promising results. Three families of deep learning architectures have been investigated with the
goal of generating high-quality paraphrases. The
ﬁrst is to formulate paraphrase generation as a
sequence-to-sequence problem, following experience from machine translation . Prakash et al. 
proposes stacked residual long short-term memory networks (LSTMs), while Cao et al. 
makes use of the gated recurrent unit (GRU) as
the recurrent unit.
Shortly afterwards, several
works have focused on providing extra information to enhance the encoder-decoder model. Ma
et al. adds distributed word representations,
Huang et al. considers a paraphrased dictionary and
Iyyer et al. utilizes a syntactic parser.
Recently Wang et al. utilizes a semantic
augmented Transformer ;
however, its accuracy largely depends on extra semantic information. The second family of models
employs reinforcement learning .
The third family is the generative architecture that
we focus on.
Speciﬁcally, Gupta et al. 
applies the conditional variational autoencoders
(CVAEs) with LSTM encoderdecoders in generating paraphrases, expecting
RNN’s advantage of modeling local dependencies
to be a good complement to the CVAE’s power at
learning global representations. This simple combination, however, has two major challenges.
First, CVAE may fail to generate realistic sen-
How do I improve my English speaking ?
paraphrased
How do I speak English ﬂuently ?
How do I I to ?
What is the easiest way to lose weight faster?
paraphrased
What is the best way to loose weight quickly ?
How can I learn lose weight ?
Examples for generating paraphrases by
CVAE, from which it fails to generate realistic sentences. Original: source sentence. Paraphrased: target
sentence. Generated: the sentence generated by CVAE.
tences. When generating synthetic sentences by
decoding random samples from the latent space,
most regions of the latent space do not necessarily
decode to realistic sentences; the example in Table
1 reﬂects this challenge. In ,
the authors attempted to utilize RNN-based VAE
to generate more diverse sentences. However, this
ignores the fundamental problem of the posterior
distribution over latent variables not covering the
latent space appropriately. Second, when learning,
the ground-truth words are used to decode, while
during inference, the RNN generates words in sequence from previously generated words. Bengio
et al. called this phenomenon “Exposure
Bias” and tried to solve it by a scheduled sampling method. However, in practice, it produced
unstable results, because it is fundamentally an inconsistent training strategy .
We propose the ﬁrst paraphrase generative
model via adversarial training to tackle the above
problems, which we believe is a natural answer to
the aforementioned challenges. Speciﬁcally, we
formalize CVAE as a generator of the Generative
Adversarial Network (GAN) and tailor a discriminator to CVAE. By introducing an adversarial game between a generator
and a discriminator, GAN matches the distribution
of synthetic data with that of real data. The generator of GAN seeks to map samples from a given
prior distribution to realistic synthetic data. The
discriminator of GAN compares the entire real and
synthetic sentences, instead of individual words,
which should in principle alleviates the exposurebias issue . The intuition behind
our model can be interpreted as using CVAE to
generate similar sentences and enhancing CVAE
with an extended discriminator, so that the generator and discriminator work effectively together;
they provide feedback signals to each other, resulting in a mutual adversarial framework. Overall,
our contributions are as follows.
• To the best of our knowledge, this work represents the ﬁrst to propose an end-to-end
paraphrase generation architecture via adversarial training, which does not require extra
linguistic information.
• We take advantage of GAN to help choose
a better latent-variable distribution. By this,
we not only utilize the better latent variables
but also strengthen the expressiveness of the
generative model.
• Our experiments show that the proposed
model is capable of generating plausible
paraphrased sentences and outperforms competitive baseline models, with state-of-the-art
Preliminaries
We ﬁrst provide preliminaries of variational
auto-encoders and conditional variational autoencoders.
variational
auto-encoder
 is
a directed graphical model with latent variables.
The generative process of VAE employs an
• (Encoder) generating a set of latent variable
z from the prior distribution pθ(z), where θ
is the generative parameters;
• (Decoder) then generating the data x from
the generative distribution pθ(x|z) conditioned on z.
Although due to intractable posterior inference,
parameter estimation of directed graphical models is generally challenging, VAE parameters can
be estimated efﬁciently by a stochastic gradient
variational bayes (SGVB) estimator , and can be optimized straightforwardly using standard stochastic gradient techniques.
SGVB treats the variational lower bound of the
log-likelihood as a surrogate objective function,
which can be written as:
−KL(qφ(z|x) ∥pθ(z))
+KL(qφ(z|x) ∥pθ(z|x))
+Eqφ(z|x)[log pθ(x|z)]
−KL(qφ(z|x) ∥pθ(z))
+Eqφ(z|x)[log pθ(x|z)],
where the inequality follows because the second
term KL(qφ(z|x) ∥pθ(z|x)) in (1) is nonnegative.
In practice,
we can approximate the second term by drawing latent variable samples
{z1, z2, ..., zL} following qφ(z|x), where φ is the
variational parameters, and the empirical objective
of VAE with Gaussian latent variables can be represented as:
˜LV AE(x; θ, φ) = −KL(qφ(z|x) ∥pθ(z))
log pθ(x|zl),
where zl = gφ(x, ϵl), and ϵl ∼N(0, I). That
means qφ(z|x) is reparameterized with a differentiable unbiased function gφ(x, ϵl), where x is
the data and ϵ is the noise variable. VAE can be
trained efﬁciently by stochastic gradient descent
(SGD) because the reparameterization trick allows
error backpropagation through the Gaussian latent
variables.
CVAE. Sohn et al. develops a deep conditional generative model (CVAE) for structured
output prediction using Gaussian latent variables.
Different from the normal VAE, CVAE consists of
three variables: input variables x, output variables
y, and latent variables z, and its prior distribution
is pθ(z|x). CVAE is trained to maximize the conditional log-likelihood log pθ(y|x), and again it is
formulated in the framework of SGVB. The variational lower bound of the model is:
log pθ(y|x)
−KL(qφ(z|x, y) ∥pθ(z|x))
+Eqφ(z|x,y)[log pθ(y|x, z)](3)
and the empirical lower bound is represented as:
˜LCV AE(x, y; θ, φ)
−KL(qφ(z|x, y) ∥pθ(z|x))
log pθ(y|x, zl),
where zl = gφ(x, y, ϵl), ϵ ∼N(0, I), and L is
the number of samples.
Reparameterization
z ∼(μ, σ)
Figure 1: Our generative model. The red arrows denote the process for generating sentences and the green
one denotes the transmission of target paraphrased sentences.
Our new model, which we call GAP for Generative Adversarial Paraphrase model, targets the
goal of generating plausible paraphrased sentences
conditioned on the original sentences, without any
extra linguistic information.
We ﬁrst propose
our end-to-end conditional generative architecture
for generating paraphrase via adversarial training.
Two training techniques are then described for the
proposed model.
We consider a dataset with pairs of the original
paraphrased
k=1. For a given sentence s, let wt denote the t-th word. Each word wt is embedded
into a d-dimensional vector xt = We[wt], where
We ∈Rd×V is a learned word embedding matrix,
V is the vocabulary size, and the υ-th column of
We is We[υ]. All columns of We are normalized
to have unit ℓ2-norm, i.e., ∥We[υ]∥2 = 1, ∀υ,
by dividing each column with its ℓ2-norm. After
embedding, a sentence of length T (padded when
necessary) is represented as X ∈Rd×T , by simply
concatenating its word embeddings, i.e., xt is the
t-th column of X.
The training set is S = {(so
k=1 of pairwise data, where so
k ∈So denotes the original
sentence sequence, and sp
k ∈Sp denotes the reference paraphrased sentence sequence. The goal of
our model is to learn a function f: So 7→Sp. The
overall model structure of the proposed method is
shown in Figure 1. The components can be divided into three modules: encoder in yellow, decoder in blue (which is also the generator), and the
discriminator in purple. The connections between
them are drawn in arrows.
Encoder E. The encoder E consists of two LSTM
networks E1 and E2, and E generates latent variable z from the input sentences. There are two
different paths generating latent variable z. In the
ﬁrst path, we input sampled so and sp into E1 and
E2, respectively, and generate sufﬁcient statistics
of a Gaussian distribution, mean µ and standard
deviation σ. Therefore, the distribution of the latent variable z can be represented as q(z|so, sp).
However, we cannot know the paraphrased sentence sp when we evaluate or apply the trained
model in real applications. The training framework is not ﬁt for the circumstance at testing time,
and only E1 is used during the inference. Thus, we
also compute the other set of sufﬁcient statistics,
mean µ′ and standard deviation σ′ by inputting so
into encoder E1. The sampled latent variable z is
only dependent on so, and we can represent the
distribution of z as p(z|so).
Decoder/Generator G. We again combine two
LSTM decoders G1 and G2 as the generator G. At
ﬁrst, we feed the original sentence so into decoder
G1 and obtain ﬁnal state cG1
The paraphrased sentence ˜sp is predicted word by word. At
each step, we concatenate the latent variable z and
the previously predicted (ground truth) word, and
then input it into LSTM decoder G2 with hidden
state from the last step. At timestep 0, the input
word is BOS (“begin of sentence” symbol), and
the hidden state is the output from G1. Therefore,
the probability of the predicted paraphrased sentence ˜sp, given the encoded feature vector z, is
deﬁned as:
p(˜sp|z, so)
<t, z, so),
t is the t-th generated token, < t ≜
{0, · · · , t −1}. wp
0 denotes BOS. All the words
in ˜sp are generated sequentially using the LSTM,
based on previously generated words, until the
end-of-sentence symbol is generated.
t is generated as ˜wp
= argmax(Vht).
Note that the hidden units ht are updated through
t−1, ht−1, z), where E denotes the transition
function in the second LSTM cell related to the updates. The transition function E(·) is implemented
with an LSTM. V is a weight matrix used to compute a distribution over words.
t−1 is the embedding vector of the previously generated word
t−1, i.e.,
t−1 = We[ ˜wp
and is the input for t-th step. Consequently, the
generated sentence ˜sp = [ ˜wp
1, · · · , ˜wp
T ] is obtained
given z, by simply concatenating the generated
Two-Path Loss. Thus far, we can compute the
loss from two paths of the encoder. In the ﬁrst
path, we minimize the KL loss and MLE loss:
KL(q(z|so, sp)||p(z|so))
−Eq(z|so,sp) [log p(˜sp|z, so)] .
In the second path, the loss is presented as follows:
−Eq(z|so) [log p(˜sp|z, so)] .
The proposed two-path reconstruction loss diminishes the gap between prediction pipelines at training and testing, and helps to generate diverse but
realistic predictions.
The objective function of optimizing E and G
can be written as follows:
λ1KL(q(z|so, sp)||p(z|so))
−λ1Eq(z|so,sp) [log p(˜sp|z, so)]
−λ2Eq(z|so) [log p(˜sp|z, so)]
−λ3E˜sp∼p(˜sp|z′,so) log(D(˜sp)),
where D denotes the discriminator (described below) and z′ ∼q(z|so). We use λ1, λ2 and λ3 to
balance the weights of the different losses.
Discriminator D. We use one LSTM as the discriminator.
The goal of the discriminator is to
distinguish real paraphrased sentences from generated sentences. Given the embeddings of true
paraphrased sentence sp and the fake generated
sentence ˜sp, the discriminator loss is deﬁned as:
−E˜sp∼p(˜sp|z′,so) log(1 −D(˜sp))
−Esp∼p(sp)[log D(sp)],
where z′ ∼q(z|so).
Algorithm 1 Training Pipeline
Initialize: Gradients of corresponding parameters: Encoder E parameters gE = [gE1, gE2],
Decoder/Generator G parameters gG
[gG1, gG2], Discriminator D parameters gD;
1: while G has not converged do
Sample a batch of {so, sp} from dataset;
z1 = E(so, sp);
LKL = KL(q(z1|so, sp)||p(z|so));
z2 = E1(so);
Predict ˜sp
1 following: ˜sp
1 = G(so, z1);
Predict ˜sp
2 following: ˜sp
2 = G(so, z2);
Lrec1 = −log p(˜sp
1|z1, so);
Lrec2 = −log p(˜sp
2|z2, so);
Input sp and ˜sp
2 into Discriminator D;
LD = −log(1 −D(˜sp
2)) −log D(sp);
LDG = −log(D(˜sp
gE1 = ∇E1(λ1LKL+λ1Lrec1+λ2Lrec2+
gE2 = ∇E2(LKL + Lrec1);
gG = ∇G(λ1Lrec1 + λ2Lrec2 + λ3LDG);
Update model parameters using Adam optimizer with gradients gE1, gE2, gG and gD.
18: end while
Training Techniques
We summarize the training pipeline in Algorithm
1. At each iteration, a pair of original and paraphrased sentences are fed into two paths for sentence reconstruction. A discriminator is also used
to distinguish between real and generated sentences, which is helpful to the exposure bias problem.
Policy Gradient. Backpropagation of gradients
from the discriminative model to the generative
model is difﬁcult for sequence generative models. Following Yu et al. , we use the RE-
INFORCE algorithm to approximate gradients with respect to the generator and
encoder. In the experiments, we regard the generator G as the stochastic policy and the output of
discriminator D as its reward. In this way, we can
propagate the gradients from the discriminator to
both the generator models and encoder models.
Warm-up Training. It is difﬁcult to train GAN
using gradient-based methods . Previous generative models often pre-train the generator using a supervised model, like an auto-encoder. In this paper,
we propose a warm-up technique to train our generative model.
Following the notation in Algorithm 1, we suppose a warm-up step twp. In the
training process, we gradually increase the weight
of LDG to a ﬁxed value λ3. From step t = 0 to
twp, the update of λt
3 can be represented as
When t ≥twp, we ﬁx λt
3 = λ3 as a constant.
We also need to balance the losses between (7)
and (8). The second loss (8) represents that during
inference, z is generated only depending on xo.
We let λ1 = 1 −λt
2 and increase the value of λt
gradually until twp. Similarly, λt
2 is updated from
t = 0 to twp through:
If t ≥twp, we ﬁx λt
Experiments
We assess the performance of our GAP model and
compare it with previous methods. We ﬁrst describe the datasets we use, then present the details
of experimental setup, and ﬁnally analyze both
quantitative and qualitative results.
Following previous work ,
we conduct experiments on the same four standard
datasets that are used widely for paraphrase generation. Their content is speciﬁed below.
MSCOCO. MSCOCO was originally an imagecaptions dataset, containing over 120K images,
with ﬁve different captions from ﬁve different
annotators per image.
All the annotations toward one image describe the most prominent object or action in this image, which is suitable
for the paraphrase generation task. Speciﬁcally,
the dataset has two divisions:
training dataset
(“Train2014”, over 82K images) and validation
dataset (“Val2014”, over 40K images). We follow the operations of previous papers, randomly
choosing four captions out of ﬁve as two sourcereference pairs and limit the length of sentence to
be 15 words (removing the words beyond the ﬁrst
15), so that we can compare our results with published work.
Quora. Quora consists of over 400K lines of potential question pairs which are duplicate to each
other if a particular position of this line is annotated with 1. Again, we follow the operations of
previous work and ﬁlter out those pairs annotated
with 1. There are 155K such question pairs in total, among which three sub-datasets are created,
i.e., training dataset 50K, 100K, 150K and test
dataset 4K. The goal of using three sub-datasets
is to show how the size of dataset can affect the
results of paraphrase generation.
Experimental Setup
Encoders E1, E2 and generators G1, G2 are constructed using 2-layer LSTMs . For discriminator D, it is
also 2-layer LSTM. At the beginning, we map
each word to a 300-dimensional feature vector and
it is initialized with 300-dimensional GloVe vectors . Therefore, a sentence of length N can be represented by a matrix
of size N × 300. Before inputting the embedding
vector into LSTM models, we pre-process these
vectors using a two-layer highway network . We set the dimension of latent variables z to be 300. To balance these losses,
we set λ2 = 0.5 and λ3 = 0.01. The warm-up
step is 1 × 104 in all experiments.
We use Adam optimizer 
with learning rate 1 × 10−4 and other parameters
as default, for all models. Following , we clip gradients of the encoder and
generator parameters if the norm of the parameter
vector exceeds 10, and we clip the gradients of the
discriminator if the norm of the parameter vector
exceeds 5. Greedy search is employed to generate
paraphrases in the experiment.
Automatic Evaluation
We follow previous papers to choose the same
three well-known automatic evaluation metrics:
BLEU , METEOR , and TER . As studied by , human judgments on generated paraphrases correlate
well with these metrics. We then compare our results on these metrics with previous baselines below.
Baselines. Our GAP model contains LSTM and
VAE components, so we compare it with (i)
the basic attentive sequence-to-sequence model
from Machine Translation (seq2seq) , (ii) the stacked residual LSTM
BLEU↑METEOR↑TER↓
Seq2Seq + Att
Residual LSTM
Transformer
Transformer-PB
Table 2: Test accuracy on MSCOCO dataset, in percentage. VAE∗is our implementation of VAE. “best-
B”, “best-M” and “best-T” represent the scores with
the best BLEU, METEOR and TER respectively. “avg”
denotes an average over “best-B”, “best-M”, and “best-
T”. All other results are directly cited from the respective papers. For different model variants exist in one
paper, we only show the one with highest scores here.
For the arrows ↑of BLEU and TER, a higher score is
better. ↓of TER represents a lower score is better. The
best results are in boldface.
model doing paraphrase generation (Residual
LSTM) , and (iii) a VAE
model based on LSTM together with its variants
(VAE-S, VAE-SVG, VAE-SVG-eq) , semantic augmented Transformer which requires extra linguistic information (Transformer-
PB) , and a deep reinforcement
learning approach (RbM-SL) .
Results and Analysis. The results on MSCOCO
are shown in Table 2. We ﬁnd that our GAP model
outperforms the baseline models on all metrics.
For example, comparing our scores on MSCOCO
dataset when we achieve the best BLEU with
the ones from VAE-SVG, we improve the results
about 4 BLEU, 5 METEOR, and 4 TER, respectively.
Table 3 shows the performance on Quora
We notice that our accuracy increases
with the increasing size of data.
Meanwhile,
our model is more robust for a relatively smaller
dataset such as Quora-50K, leveraging its advantage in learning with fewer data. For example, we
achieve much better results than VAE and its variants on the smaller Quora-50k.
Ablation Study. VAE∗in Table 3 extends the pre-
Quora-100K
Quora-150K
METEOR↑TER↓
Seq2Seq + Att
Residual LSTM
VAE-SVG-eq
VAE-SVG-eq
Table 3: Test accuracy on Quora dataset, in percentage. VAE∗is our implementation of VAE. “best-B”, “best-M”
and “best-T” represent the scores with the best BLEU, METEOR and TER respectively. “avg” denotes an average
over “best-B”, “best-M”, and “best-T”. All other results are directly cited from the respective papers, and “-” means
no such results reported. For the arrows ↑of BLEU and TER, a higher score is better. ↓of TER represents a lower
score is better. The best results are in boldface.
vious VAE-SVG in by using
our proposed two-path loss. We observe that our
VAE∗usually outperforms previous VAE-based
models. It demonstrates the proposed two-path reconstruction loss can improve the quality of generated paraphrases. Moreover, our proposed method
via adversary training, denoted as “Ours”, also has
superior performance than VAE∗. Therefore, our
proposed adversarial training and two-path loss
take an apparently positive effect on alleviating the
exposure bias problem and generating diverse but
realistic predictions.
Human Evaluation
Data Preparation.
The accurate evaluation of
paraphrases is an open problem. We believe that
automatic evaluation is not enough for evaluating paraphrases from a ﬁne-grained perspective,
in terms of three attributes:
grammar correctness (plausibility), equivalence to the original sentence (equivalence), diversity expression (diversity).
For both MSCOCO and Quora datasets,
we randomly sample 100 sentence pairs (original
sentence, paraphrased sentence) from the test corpus, and apply the generative architectures, including both the VAE∗model and our GAP model,
to generating paraphrases. Thus, we obtain three
different sentence pairs: (original sentence, “generated” sentence) by the reference, (original sentence, generated paraphrase) by VAE∗, and by our
GAP model. To make the analysis fair, we randomly shufﬂed all of them. We then partitioned
them into ten buckets.
Process. We set up an Amazon Mechanical Turk
experiment; ten human judges are asked to evaluate the quality of paraphrases.
We hope our
judges play similar roles to the discriminator in
our model, to make a true/fake judgment. It is easier for them to make a binary choice than score
how good the paraphrased sentences are from a
wide score range.
These ten human judges are
ﬁnally involved in evaluating the total 600 sentence pairs. Each pair is judged by two different
judges, and the average score is the ﬁnal judgment. The agreement between judges is moderate (kappa=0.42). These ten human judges con-
ﬁrmed that they were proﬁcient in English and
they had understood the goals of the annotation
process very well. They were trained by means
of instructions and examples. If the meaning of
a generated sentence contains a grammatical error
or does not express the same meaning as the original sentence, or lacks diversity, we asked the annotators to score 0 for the corresponding attributes,
and 1 otherwise.
Results and Analysis. We report the results in
Table 4. Our model generates paraphrases with
higher scores in terms of plausibility, equivalence,
diversity than the previous VAE model, and their
differences are statistically signiﬁcant (paired ttest, p-value < 0.01). The failure cases we observed include implausible sentences, inequiva-
Plausibility
Equivalence
Plausibility
Equivalence
Table 4: Human judgments for paraphrase generation on different models.
a group of kids are eating pizza and
drinking soda
paraphrased
a person sits at a table eating pizza
generated-VAE
a man sitting at a table with a pizza
generated-Ours
a man sitting at a table eating
a plate of pizza and soda
What is the fastest possible way to
(Quora-50K)
lose weight?
paraphrased
What are some ways to lose weight fast?
generated-VAE
How can I lose weight weight?
generate-Ours
How can I lose weight in a month?
What were the immediate and most
(Quora-100K)
important causes that led to World War 1?
paraphrased
What were the causes of World War I?
generated-VAE
What were the main causes of World War I?
generated-Ours
What were the direct and main causes
of World War 1?
What could be the reason behind Arnab
(Quora-150K)
Goswami quitting Times Now?
paraphrased
Is Arnab Goswami quitting from Times now?
generated-VAE
Why did Arnab Goswami quit?
generated-Ours
Why did Arnab Goswami resign from Times
Samples of generated paraphrases from
MSCOCO and Quora. Note that for the last two examples, our generated result is even better than the ground
truth where our model paraphrasing “immediate” using
“direct”, and paraphrasing “quit” using “resign”.
lent, and inaccurate expressions. It is believed that
the reason for this is related to the input training
data. It contains noise caused by the length limitation of ≤15 words. But note that even for the
reference paraphrase, the accuracy cannot be as
high as 100%. Here is an example from the real
data: for the original sentence “children are playing soccer on a ﬁeld with several adults observing nearby”, the reference paraphrase is “soccer
player hits another player in the face”. Considering this, the results show that our GAP generates
relatively more plausible and diverse paraphrase
sentence, compared to the baseline model.
Case Study. In Table 5, we show examples sampled from both MSCOCO and Quora. We observe
that our model generates paraphrases with higher
diversity than VAE, with no loss of information.
What’s more, we discover some results where our
model is even better than the ground truth, which
are shown in the last two examples in the Table 5.
Related Work
Generative models or conditional generative models have experienced remarkable progress in the
visual domain, such as VAEs in and InfoGAN . Recent work also considers combining autoencoders or variational autoencoders with GAN to
demonstrate superior performance on image generation. In the area of NLP, generation can be summarized from two perspectives: text generation
with the goal of yielding diverse and plausible sentences given an original sentence, and conditional
text generation aiming to generate new sentences
conditioned on an original sentence. Considering
the latter, examples include generating a new sentence similar to an original sentence (paraphrase
generation), in a different style from the original
sentence (style transfer), or dependent on dialogue
history (dialogue generation). Attempts at using
VAEs ,
GANs , and
both have been made to address
generic text generation. However, all of them are
not suitable for conditional text generation. Recent work in tries to handle
paraphrase generation using VAEs. However, it
suffers from challenges common to VAEs. Our
method addresses these challenges with the help
of GAN. To the best of our knowledge, this is
the ﬁrst work on using GAN with VAEs for paraphrase generation task.
Conclusions
We propose the ﬁrst deep conditional generative
architecture for generating paraphrases via adversarial training, in the hope of combining advantages of CVAE to generate similar distributions
with the advantage of GAN to generate plausible
sentences. Experimental results evaluated on automatic metrics demonstrate the advantages of our
model, with human evaluations also verifying effectiveness. In future work, we intend to accelerate the training of our encoders and decoders with
the techniques in and apply our
architecture and training techniques to other NLP
tasks. Overall, we believe that our research makes
an important step for using generative models in
NLP, especially in conditional text generation.
Acknowledgments
This research was supported in part by DARPA,
DOE, NIH, ONR and NSF.