Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 169–214
Copenhagen, Denmark, September 711, 2017. c⃝2017 Association for Computational Linguistics
Findings of the 2017 Conference on Machine Translation (WMT17)
Ondˇrej Bojar
Charles University
Rajen Chatterjee
Christian Federmann
Microsoft Research
Yvette Graham
Dublin City University
Barry Haddow
Univ. of Edinburgh
Shujian Huang
Nanjing University
Matthias Huck
LMU Munich
Philipp Koehn
JHU / Edinburgh
Dublin City University
Varvara Logacheva
MIPT Moscow
Christof Monz
Univ. of Amsterdam
Matteo Negri
Johns Hopkins Univ.
Raphael Rubino
DFKI & Saarland Univ.
Lucia Specia
Univ. of Shefﬁeld
Marco Turchi
This paper presents the results of the
WMT17 shared tasks, which included
three machine translation (MT) tasks
(news, biomedical, and multimodal), two
evaluation tasks (metrics and run-time estimation of MT quality), an automatic
post-editing task, a neural MT training
task, and a bandit learning task.
Introduction
We present the results of the shared tasks of the
Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT
as workshops and conference 
• quality estimation (Section 4)
• automatic post-editing (Section 5)
The conference featured additional shared tasks
that are described in separate papers in these proceedings:
• metrics 
• multimodal machine translation and multilingual image description 
• biomedical translation 
• bandit learning 
In the news translation task (Section 2), participants were asked to translate a shared test set,
optionally restricting themselves to the provided
training data (constraint condition). We held 14
translation tasks this year, between English and
each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian
is a lesser resourced data condition on challenging
language pair. Chinese allowed us to co-operate
with an ongoing evaluation campaign on Asian
languages organized alongside the Chinese Workshop on Machine Translation (CWMT).1 System
outputs for each task were evaluated both automatically and manually.
The human evaluation (Section 3) involves asking human judges to score sentences output by
anonymized systems. We obtained large numbers
of assessments from researchers who contributed
evaluations proportional to the number of tasks
they entered.
In addition, we used Mechanical
Turk to collect further evaluations. This year, the
ofﬁcial manual evaluation metric is based on judgments of adequacy on a 100-point scale, a method
we explored last year with convincing results in
terms of the trade-off between annotation effort
and reliable distinctions between systems.
The quality estimation task (Section 4) this year
included three subtasks: sentence-level prediction
of post-editing effort scores, word and phraselevel prediction of good/bad labels.
1 
were released with English→German IT translations and German→English Pharmaceutical translations for all subtasks.
The automatic post-editing task (Section 5)
examined automatic methods for correcting errors produced by an unknown machine translation system. Participants were provided with training triples containing source, target and human
post-edits, and were asked to return automatic
post-edits for unseen (source, target) pairs.
this third round, the task focused on correcting
English→German translations in the IT domain
and German→English translations in the Pharmaceutical domain.
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to disseminate common test sets and public training data
with published performance numbers, and to re-
ﬁne evaluation and estimation methodologies for
machine translation. As before, all of the data,
translations, and collected human judgments are
publicly available.2 We hope these datasets serve
as a valuable resource for research into statistical machine translation, automatic evaluation, or
prediction of translation quality.
News translations are also available for interactive visualization
and comparison of differences between systems at
 using MT-ComparEval
 .
News Translation Task
The recurring WMT task examines translation between English and other languages in the news domain. As in the previous years, we include German, Czech, Russian, Finnish, and Turkish. New
languages this years are Latvian and Chinese.
We created a test set for each language pair by
translating newspaper articles and provided training data.
The test data for this year’s task was selected from
online sources, as before. We took about 1500 English sentences and translated them into the other
5 languages, and then additional 1500 sentences
from each of the other languages and translated
them into English. This gave us test sets of about
3000 sentences for our English-X language pairs,
which have been either originally written in English and translated into X, or vice versa.
2 
composition of the test documents is shown in Table 1.
The stories were translated by professional
translators, funded by the EU Horizon 2020
projects CRACKER and QT21 (German, Czech,
Latvian), by Yandex3, a Russian search engine
company (Turkish, Russian), and by BAULT, a research community on building and using language
technology funded by the University of Helsinki
(Finnish). The Chinese–English task was sponsored by Nanjing University, Xiamen University,
the Institutes of Computing Technology and of
Automation, Chinese Academy of Science, Northeastern University (China) and Datum Data Co.,
Ltd. All of the translations were done directly, and
not via an intermediate language.
For Latvian, the test set size was 2000 sentences, and an additional 2000 sentences were released as development set.
Training data
As in past years we provided parallel corpora to
train translation models, monolingual corpora to
train language models, and development sets to
tune system parameters. Some training corpora
were identical from last year (Europarl4, Common
Crawl, SETIMES2 , Russian-English parallel data
provided by Yandex, Wikipedia Headlines provided by CMU) and some were updated , News
Commentary v12, monolingual news data). A new
corpis is the EU Press Release parallel corpus for
German, Finnish, and Latvian.
For Latvian and Chinese a number of new corpora were released. For Latvian this data was prepared by the University of Latvia and Tilde, the
Chinese corpora were prepared by the Institutes of
Computing Technology and of Automation, Chinese Academy of Science, Northeastern University (China) and Datum Data Co., Ltd.
Some statistics about the training materials are
given in Figure 1.
Submitted systems
We received 103 submissions from 31 institutions. The participating institutions and their entry
names are listed in Table 2; each system did not
necessarily appear in all translation tasks. We also
3 
4As of Fall 2011, the proceedings of the European Parliament are no longer translated into all ofﬁcial languages.
Europarl Parallel Corpus
German ↔English
Czech ↔English
Finnish ↔English
Latvian ↔English
50,486,398 53,008,851 14,946,399 17,376,433 37,814,266 52,723,296 11,957,078 15,412,186
Distinct words
News Commentary Parallel Corpus
German ↔English
Czech ↔English
Russian ↔English
Chinese ↔English
Distinct words
Common Crawl Parallel Corpus
German ↔English
Czech ↔English
Russian ↔English
54,575,405 58,870,638 3,529,783 3,927,378 21,018,793 21,535,122
Distinct words
EU Press Release Parallel Corpus
German ↔English
Finnish ↔English
Latvian ↔English
22,078,112
22,998,930
10,063,161
Distinct words
Latvian Parallel Corpora
Online Books
Corpus of Eu. Parliament
Latvian ↔English
Latvian ↔English
Latvian ↔English
30,177,230
37,158,634
Distinct words
Chinese Parallel Corpora
casict2011
casict2015
Words (en)
20,571,578
34,866,598
22,802,353
24,632,984
25,182,185
29,696,442
Distinct words (en)
Yandex 1M Parallel Corpus
Russian ↔English
24,121,459
26,107,293
CzEng Parallel Corpus
Czech ↔English
62,493,539
611,094,888
688,534,994
Wiki Headlines Parallel Corpus
Russian ↔English
Finnish ↔English
United Nations Parallel Corpus
Russian ↔English
Chinese ↔English
23,239,280
15,886,041
482,966,738
524,719,646
372,612,596
Figure 1: Statistics for the training sets used in the translation task. The number of words and the number of distinct words
(case-insensitive) is based on the provided tokenizer.
Sources (Number of Documents)
ABC News (1), BBC (9), Brisbane Times (1), CBS News (5), CNN (1), Daily Mail (10), Euronews (1),
Fox News (2), Globe and Mail (1), Guardian (3), Independent (2), Los Angeles Times (1), Novinte (1),
New York Times (8), Reuters (4), Russia Today (3), Scotsman (1), Sydney Morning Herald (4), Telegraph (1), The Local (1), UPI (4)
Ifeng (82), People Daily (14), Sina (14), Xinhua (8)
aktu´alnˇe.cz (10), blesk.cz (4), blisty.cz (1), den´ık.cz (1), iDNES.cz (14), ihned.cz (4), lidovky.cz (8),
Novinky.cz (5), Reﬂex (1), tyden.cz (4), ZDN (2)
Abendzeitung M¨unchen (1), Abendzeitung N¨urnberg (1), ARD (1), Augsburger Allgemeine (1),
Bergedorfer Zeitung (1), Braunschweiger Zeitung (1), Der Standard (2), Deutsche Welle (1),
D¨ulmener Zeitung (1), Euronews (1), Frankfurter Rundschau (2), Generalanzeiger Bonn (1), G¨ottinger
Tageblatt (1), Handelsblatt (4), In Franken (4), In S¨udth¨uringen (1), Kieler Nachrichten (2),
Kreisanzeiger (1), Kreiszeitung (3), Krone (1), K¨olner Stadt Anzeiger (2), Merkur (1), Morgenpost (3),
Neue Presse Coburg (1), Nordbayerischer Kurier (1), oe24 (1), Potzdamer Neueste Nachrichten (1),
Passauer Neue Presse (1), Pforzheimer Zeitung (1), Rheinzeitung (1), Rundschau (1), Schwarzw¨alder
Bote (2), S¨udkurier (1), S¨uddeutsche Zeitung (1), Usinger Anzeiger (1), Westf¨alischer Anzeiger (1),
Westf¨alische Nachrichten (3), Westdeutsche Zeitung (4), Zeit (1), Waiblinger Kreiszeitung (4).
Etel¨a-Saimaa (2), Etel¨a-Suomen Sanomat (1), Helsingin Sanomat (14), Ilkka (10), Iltalehti (16), Ilta-
Sanomat (16), Kaleva (9), Kansan Uutiset (3), Karjalainen (10), Kouvolan Sanomat (2), Loimaan
Lehti (1).
Dienas Bizness (3), Delﬁ(11), Diena (13), grenet.lv (1), LSM (10), NRA (9), Talsu Vestis (1), TV
aif (), dp.ru (2), eg-online.ru (2), gazeta.ru (5), gzt-sv.ru (1), Izvestiya (7), Kommersant (16), Lenta (17),
lgng (5), MK RU (4), nov-pravda.ru (1), Novaya Gazeta (3), pnp.ru (4), rg.ru (1), rusplit.ru (1), Vedomosti (1), Versia (2), Vesti (3), VM News (1), zr.ru (3)
Sabah (96), S¨ozc¨u (19)
Table 1: Composition of the test set. For more details see the XML test ﬁles. The docid tag gives the source and the date for
each document in the test set, and the origlang tag indicates the original source language.
Europarl Language Model Data
59,848,044
53,534,167
14,946,399
39,511,068
12,092,389
Distinct words
News Language Model Data
166,127,560
221,793,141
59,184,372
31,285,072
10,938,701
3,816,723,867
3,938,344,482
974,167,234
572,672,132
137,162,922
Distinct words
17,824,672
Common Crawl Language Model Data
3,074,921,453
2,872,785,485
333,498,145
1,168,529,851
157,264,161
288,806,234
511,196,951
Words 65,128,419,540 65,154,042,103 6,694,811,063 23,313,060,950 2,935,402,545 8,140,378,873 11,882,126,872
342,760,462
339,983,035
50,162,437
101,436,673
47,083,545
37,846,546
88,463,295
German ↔EN
Finnish ↔EN
Latvian ↔EN
Sentences.
Distinct words
Russian ↔EN
Turkish ↔EN
Chinese ↔EN
Sentences.
Distinct words
Figure 2: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct
words (case-insensitive) is based on the provided tokenizer.
Institution
Aalto University 
AFRL-MITLL
Air Force Research Lab / MIT Lincoln Lab 
Apertium / Helsinki University 
Tartu-Riga-Z¨urich 
CASICT-DCU
Chinese Academy of Sciences / Dublin City University
 
CU-CHIMERA
Charles University 
Fondazione Bruno Kessler 
Hunter College, City University of New York 
Helsinki University 
Japan Advanced Institute of Science and Technology 
Johns Hopkins University 
Karlsruhe Institute of Technology 
LIMSI 
University of Le Mans / Universitat Autonoma de Barcelona
 
LMU Munich 
NMT-AVE-MULTI-CS
National Research Council, Canada
Orgon State University
Polish-Japanese Academy of Information 
PROMT Rule-Based System
QT21 project system combination 
University of Rochester 
RWTH Aachen 
Sogou Inc. 
Systran 
TALP, Technical University of Catalonia 
Tilde 
University of Edinburgh 
University of Shefﬁeld
Uppsala University
Xiamen University 
Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the
commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore
anonymized in a fashion consistent with previous years of the workshop.
included 39 online statistical MT systems (originating from 4 services), which we anonymized as
ONLINE-A,B,F,G.
For presentation of the results, systems are
treated as either constrained or unconstrained, depending on whether their models were trained only
on the provided data. Since we do not know how
they were built, these online and commercial systems are treated as unconstrained during the automatic and human evaluations.
Human Evaluation
A human evaluation campaign is run each year to
assess translation quality and to determine the ﬁnal
ranking of systems taking part in the competition.
This section describes how preparation of evaluation data, collection of human assessments, and
computation of the ofﬁcial results of the shared
task is carried out this year.
In previous years, we asked human annotators to rank the outputs of ﬁve systems.
these rankings, we produced pairwise translation
comparisons, and applied the TrueSkill algorithm
 to
produce system rankings.
We refer to this approach as the relative ranking (RR) approach, so
named because the pairwise comparisons denote
only relative ability between a pair of systems, and
cannot be used to infer absolute quality. For example, RR can be used to discover which systems
perform better than others, but RR does not provide any information about the absolute quality of
system translations, i.e. it provides no information
about how far a given system is from producing
perfect output according to a human user.
Work on evaluation over the past few years has
provided fresh insight into ways to collect direct
assessments (DA) of machine translation quality
 , and last year’s
evaluation campaign included parallel assessment
of a subset of News task language pairs evaluated
with RR and DA. DA has some clear advantages
over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing.
As established last year ,
DA results (via crowd-sourcing) and RR results
(produced by researchers) correlate strongly, with
Pearson correlation ranging from 0.920 to 0.997
across several source languages into English and
at 0.975 for English-to-Russian (the only pair evaluated out-of-English). This year, we thus employ
DA only. Where possible, we collect DA judgments via the crowd-sourcing platform, Amazon’s
Mechanical Turk, and as in previous year’s we
ask participating teams to provide manual evaluation of system outputs via Appraise with a
new implementation of DA. Researcher involvement is needed particularly for translations out-of-
Human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation (i.e.
no bilingual speakers are needed) on an analogue
scale, which corresponds to an underlying absolute 0–100 rating scale. Since DA involves evaluation of a single translation per screen, this allows
the sentence length restriction usually applied during manual evaluation to be removed for both researchers and crowd-sourced workers.5 Figure 3
shows one DA screen as completed by researchers
on Appraise, while Figure 4 provides a screenshot
of DA shown to crowd-sourced workers on Amazon’s Mechanical Turk.
The annotation is organized into “HITs” (following the Mechanical Turk’s term “human intelligence task”), each containing 100 such screens
and requiring about half an hour to ﬁnish. Appraise users were allowed to pause their annotation at any time, Amazon interface did not allow
any pauses. More details of composition of HITs
are given in Section 3.3 and details on time spent
in Section 3.6 below.
Evaluation Campaign Overview
In terms of the News translation task manual evaluation, a total of 151 individual researcher accounts were involved, and 754 turker accounts.6
Researchers in the manual evaluation came from
29 different research groups and contributed judgments of 125,693 translations, while 237,200
translation assessment scores were submitted in
total by the crowd.7
Under ordinary circumstances, each assessed
translation would correspond to a single individual scored segment. However, since many systems
5The maximum sentence length with RR was 30 in
6Numbers do not include the 954 workers on Mechanical
Turk who did not pass quality control.
7Numbers include quality control items for workers who
passed quality control but omit the additional 151,200 assessments collected on Mechanical Turk where a worker did not
pass quality control.
Figure 3: Screen shot of Direct Assessment in the Appraise interface used in the human evaluation campaign. The annotator
is presented with a reference translation and a single system output randomly selected from competing systems (anonymized),
and is asked to rate the translation on a sliding scale.
Figure 4: Screen shot of Direct Assessment as carried out by workers on Mechanical Turk.
can produce the same output for a particular input
sentence, we are often able to take advantage of
this and use a single assessment for multiple systems. This year we only combine human assessments in this way if the string of text belonging
to multiple systems is exactly identical. For example, even small differences in punctuation disqualify the potential combination of similar system outputs into a single human assessment, and
this is due to lack of evidence about what kinds of
minor differences might impact human evaluation.
Table 3 shows the numbers of segments for
which distinct MT systems participating in the
News task produced identical outputs. English to
Czech is the only language pair to include systems that do not belong to the news task, the additional NMT Training task systems, and we include
a breakdown of duplicate translations by each task
for that language pair in Table 3.
The biggest
saving in terms of exact duplicate translations for
multiple systems was made in the News task for
English to German.
Data Collection
The system ranking is produced from a large set of
human assessments, each of which indicates the
absolute quality of the output of a system. Annotations are collected in an evaluation campaign
that enlists participants in the shared task to help.
Each team is asked to contribute 8 hours anno-
Language Pair
# Total Segs
# Unique Segs
Overall Saving
Chinese→English
Czech→English
German→English
Finnish→English
Latvian→English
Russian→English
Turkish→English
English→Chinese
English→Czech
NMT Training
English→German
English→Finnish
English→Latvian
English→Russian
English→Turkish
Table 3: Total segments prior to sampling for manual evaluation and savings made by combining identical segments (Segs)
produced by multiple MT systems in the News (all language pairs) and NMT Training task (English→Czech only).
tation time, which we estimated this year at 16
100-translation HITs per primary system submitted. We continue to use the open-source Appraise8
 tool for our data collection, in
addition to Amazon Mechanical Turk.9 Table 4
shows total numbers of human assessments collected in WMT17 contributing to ﬁnal scores for
When summarizing and comparing annotation
times recorded on Appraise and Mechanical Turk,
both encounter possible challenges in terms of
idle times exaggerating summary statistics.
explore this issue in detail in Section 3.6, and
for the summary that follows, assessment times
for Appraise that appear to include very lengthy
idle times are each replaced with a realistic average time per assessment, as described in Section 3.6.
In total, our human annotators spent
nearly 24 days and 22 hours working on Appraise,
and 47 days and 23 hours annotating via crowdsourcing.10 This gives an average annotation time
of 4 hours per researcher using Appraise and 1
hour 32 minutes contribution by individual workers on Mechanical Turk.11 Compared to last year’s
8 
9 
10Numbers do not include the 2,106,918 seconds of annotation provided by workers who did not pass quality control.
11Times for Mechanical Turk workers do not include work-
RR evaluation, we see a reduction in average time
commitment per researcher, which was 6.4 hours
In this year’s evaluation, since it is the ﬁrst time
DA has been used with non-crowdsourced human
evaluators, estimates of expected assessment completion times were used to guess the required time
commitment by each participating team. Similar
to the previous campaigns, several of the Appraise
annotators passed the mark of required numbers
of annotations (the maximum number being 5,240
translation assessments) with the most patient annotator contributing close to 22.5 hours of work.
However, for one language pair, English to Latvian, insufﬁcient annotations were contributed by
researchers, which we suspect was caused by the
difﬁculty in sourcing Latvian speakers.
Nonetheless, the effort that goes into the manual
evaluation campaign each year is impressive, and
we are grateful to all participating individuals and
teams. We believe that human annotation provides
the best decision basis for evaluation of machine
translation output and it is great to see continued
contributions on this large scale.
ers who failed to pass quality control checks. Some but not
all of the HITs that do not pass quality control checks are rejected and therefore go unpaid. A portion of unusable data
is accepted and paid due to the possibility that some diligent
workers may simply lack the required literary skills to pass
quality control.
Language Pair
Assessments
Assess/Sys
Chinese→English
Czech→English
German→English
Finnish→English
Latvian→English
Russian→English
Turkish→English
English→Chinese
English→Czech
English→German
English→Finnish
English→Latvian
English→Russian
English→Turkish
Total Researcher
Total Crowd
Total WMT17
Table 4: Amount of data (assessments after removal of quality control items and “de-collapsing” multi-system outputs) collected in the WMT17 manual evaluation campaign. The ﬁnal six rows report summary information from previous years of the
workshop. Note how many rankings we get for Czech language pairs; these include systems from the NMT Training shared
Crowd Quality Control
Translations are arranged in sets of 100-translation
HITs as this allows a minimum number of pairs
of quality control translations to be collected from
each worker who participates, while at the same
time allowing sufﬁcient separation of assessment
of quality control translation pairs so that human
assessors are highly unlikely to simply remember
the score they assigned to the initial assessed translation. Details of the three kinds of quality control translation pairs employed by DA are provided
in Table 5: we repeat pairs (expecting a similar
judgement), damage MT outputs (expecting signiﬁcantly worse scores) and use references instead
of MT outputs (expecting high scores). Bad reference pairs are created automatically by replacing a
phrase within a given translation with a phrase of
the same length randomly selected from n-grams
extracted from the full test set of reference translations belonging to that language pair. This means
that the replacement phrase in itself will comprise
a ﬂuent sequence of words (making it difﬁcult to
tell that the sentence is low quality without reading
the entire sentence) while at the same time making
its presence highly likely to sufﬁciently change the
meaning of the MT output so that it causes a noticeable degradation. The length of the phrase to
be replaced is determined by the number of words
in the translation to be degraded, as follows:
Translation
# Words Replaced
Length (N)
in Translation
Repeat Pairs:
Original System output (10)
An exact repeat of it (10);
Bad Reference Pairs:
Original System output (10)
A degraded version of it (10);
Good Reference Pairs:
Original System output (10)
Its corresponding reference translation (10);
Table 5: Quality control translation pairs hidden within 100-translation HITs, numbers of items are provided in parentheses.
In total, 60 items in a 100-translation HIT serve
in quality control checks but 40 of those are regular judgements of MT system outputs (we exclude
assessments of bad references and ordinary reference translations when calculating ﬁnal scores).
The effort wasted for the sake of quality control is
Annotator Agreement
When an analogue (or 0–100 point, in practice)
scale is employed, agreement cannot be measured
using the conventional Kappa coefﬁcient, ordinarily applied to evaluation of human assessment
where judgments are discrete categories or preferences. Instead, we ﬁlter crowd-sourced human
assessors by how consistently they rate translations of known distinct quality using bad reference
pairs described previously.
Quality ﬁltering via
bad reference pairs is especially important for the
crowd-sourced portion of the manual evaluation.
Due to the anonymous nature of crowd-sourcing,
when collecting assessments of translations it is
likely to encounter workers who attempt to game
the service, as well as submission of inconsistent
and even robotic HITs. We therefore employ DA’s
quality control mechanism, facilitated by the use
of DA’s analogue rating scale.
Assessments belonging to a given crowdsourced worker who has not demonstrated that
they can reliably score bad reference translations
signiﬁcantly lower than corresponding genuine
system output translations are ﬁltered out.
p-value produced in a paired signiﬁcance test of
bad reference pair score distributions is used as
an estimate of human assessor reliability.
Assessments of workers whose p-value does not fall
below the conventional 0.05 threshold are omitted from the evaluation of systems, since they do
not reliably score degraded translations lower than
corresponding MT output translations.
Table 6 shows the number of unique workers
who evaluated MT output on Mechanical Turk via
DA, those who met our ﬁltering requirement by
showing a signiﬁcantly lower score for bad reference items, and the proportion of those workers who simultaneously showed no signiﬁcant difference between scores they attributed in repeat
assessment of identical translations.
is that the repeated input should receive a very
similar score.
Assuming that annotators do not
remember their previous assessment for the repeated sentence, the “Exact Rep.” corresponds to
intra-annotator agreement and it reaches very high
scores of 97–100%.
We also see in Table 6 that the number of excluded Mechanical Turk workers can be high for
many languages, between 42 and 58% for English HITs, 72% for Russian and 81% for Chinese.
The variance in English annotations for different
source languages are consistent with previous DA
evaluations and we do not believe this is caused in
any signiﬁcant way by the source language. With
respect to the choice of target language, however,
in general DA evaluation for languages with fewer
speakers on Mechanical Turk, such as Russian and
Chinese, do tend to encounter higher rates of gaming. Since HITs are slower to complete, due to
fewer workers with that language, HITs are live
for a longer duration on the service and gamertype workers have a greater opportunity to attempt
payment for them.
This year, bad reference items were only collected for crowd-sourced assessments. For information on quality control statistics for non crowdsourced workers see this year’s human evaluation
of the APE task, Section 5.5, where student volunteers were employed and although only 11 annotators were involved in total, 100% of those passed
DA’s quality control ﬁlter.
Producing the Human Ranking
All research and crowd data that passed quality control were combined to produce the overall
shared task results.
In order to iron out differences in scoring strategies of distinct human assessors, human assessment scores for translations
were ﬁrst standardized according to each individual human assessor’s overall mean and standard
deviation score, for both researchers and crowd.
Average standardized scores for individual segments belonging to a given system are then computed, before the ﬁnal overall DA score for that
(A) & No Sig.
Exact Rep.
Czech→English
German→English
Finnish→English
Latvian→English
Russian→English
Turkish→English
Chinese→English
English→Russian
English→Chinese
Table 6: Number of unique Mechanical Turk workers, (A) those whose scores for bad reference pairs were signiﬁcantly
different and numbers of unique human assessors in (A) whose scores for exact repeat assessments also showed no signiﬁcant
difference.
system is computed as the average of its segment
scores (Ave z in Table 7). Results are also reported
for average scores for systems, computed in the
same way but without any score standardization
applied (Ave % in Table 7).
Table 7 includes ﬁnal DA scores for all systems
participating in WMT17 translation task. Clusters
are identiﬁed by grouping systems together according to which systems signiﬁcantly outperform
all others in lower ranking clusters, according to
Wilcoxon rank-sum test. Figure 5 shows the underlying head-to-head signiﬁcance test results for
all pairs of systems.
Crowd versus Researcher Results
Comparison
Finally, although we have combined all data collected via crowd-sourcing and researchers to produce the overall results of the shared task, sufﬁcient assessments were collected to produce system scores independently in both set-ups for three
language pairs. Table 8 shows the Pearson correlation between DA scores for systems when evaluated by researchers with scores produced via
crowd-sourcing, showing high levels of agreement
reached overall for all language pairs as correlations range from 0.98 to 0.997.
In terms on annotation times, some differences
in the way HIT durations are recorded within Appraise and Mechanical Turk make a comparison
of annotation times for researchers and crowdsourced workers not entirely straightforward. On
the one hand, it is possible for a Mechanical Turk
(Mturk) worker, attempting to game the system,
to leave the window idle in order to obscure a lack
of effort, while on Appraise, researcher annotation
times will naturally include idle times due to interruptions of some kind.
The degree to which annotation times can be
exaggerated for Mturk workers is quite limited,
however. Firstly, since we impose quality control
checks throughout Mturk HITs, it won’t be possible for many workers to meet the quality threshold
without genuinely spending a minimum amount
of time on assessments. Additionally, we impose
a hard time limit of 90 minutes duration to each
100-translation HIT on Mturk (this corresponds
to an average maximum completion time of 54
seconds per translation) which limits the amount
of exaggeration of completion times that can take
place. The situation on Appraise is quite different
however, and idle times could potentially severely
skew annotation time analysis.
Figure 6(a) shows annotation times recorded for
our HITs on Mechanical Turk and Figure 6(b)
shows equivalent times for Appraise, where both
sets of completion times have been sorted from
shortest to longest duration. Examining the y-axis
of the Appraise plot in Figure 6(b) shows the maximum completion time for a single translation to
be at a whopping 329,578 seconds (3.8 days), revealing the extent to which the inclusion of idle
times for Appraise runs the risk of exaggerating
annotation times for researchers, while on Mechanical Turk, Figure 6(a), the 90 minute HIT du-
Chinese→English
SogouKnowing-nmt
afrl-mitll-opennmt
CASICT-DCU NMT
Oregon-State-Uni-S
NMT-Ave-Multi-Cs
English→Chinese
SogouKnowing-nmt
CASICT-DCU NMT
Oregon-State-Uni-S
Czech→English
English→Czech
limsi-factored-norm
CU-Chimera
TT-ufal-8GB
TT-afrl-4GB
TT-base-8GB
TT-afrl-8GB
TT-ufal-4GB
TT-denisov-4GB
German→English
RWTH-nmt-ensemb
English→German
LMU-nmt-reranked
LMU-nmt-single
RWTH-nmt-ensemb
PROMT-Rule-based
fbk-nmt-comb
Finnish→English
English→Finnish
jhu-nmt-latt-resc
AaltoHnmtMultitask
AaltoHnmtFlatcat
Latvian→English
tilde-nc-nmt-smt
tilde-c-nmt-smt
English→Latvian
tilde-nc-nmt-smt
tilde-c-nmt-smt
limsi-fact-norm
usfd-cons-qt21
usfd-cons-kit
tilde-nc-smt
Russian→English
afrl-mitll-comb
afrl-mitll-opennmt
English→Russian
PROMT-Rule-based
afrl-mitll-backtra
Turkish→English
afrl-mitll-m2w
afrl-mitll-comb
English→Turkish
jhu-nmt-latt-resc
Table 7: Ofﬁcial results of WMT17 News translation task. Systems ordered by standardized mean DA score, though systems
within a cluster are considered tied. Lines between systems indicate clusters according to Wilcoxon rank-sum test at p-level
p ≤0.05. Systems with gray background indicate use of resources that fall outside the constraints provided for the shared task.
Chinese→English
English→Chinese
Czech→English
English→Czech
SogouKnowing.nmt
afrl.mitll.opennmt
CASICT.DCU.NMT
Oregon.State.University.S
NMT.Model.Average.Multi.Cards
NMT−Model−Average−Multi−Cards
Oregon−State−University−S
CASICT−DCU−NMT
afrl−mitll−opennmt
SogouKnowing−nmt
SogouKnowing.nmt
CASICT.DCU.NMT
Oregon.State.University.S
Oregon−State−University−S
CASICT−DCU−NMT
SogouKnowing−nmt
limsi.factored.norm
CU.Chimera
tuning.task.ufal_8gb
tuning.task.afrl_4gb
tuning.task.baseline_8gb
tuning.task.afrl_8gb
tuning.task.ufal_4gb
tuning.task.denisov_4gb
tuning−task−denisov_4gb
tuning−task−ufal_4gb
tuning−task−afrl_8gb
tuning−task−baseline_8gb
tuning−task−afrl_4gb
tuning−task−ufal_8gb
CU−Chimera
limsi−factored−norm
German→English
English→German
Finnish→English
English→Finnish
RWTH.nmt.ensemble
RWTH−nmt−ensemble
LMU.nmt.reranked
LMU.nmt.single
RWTH.nmt.ensemble
PROMT.Rule.based
fbk.nmt.combination
fbk−nmt−combination
PROMT−Rule−based
RWTH−nmt−ensemble
LMU−nmt−single
LMU−nmt−reranked
apertium.unconstrained
apertium−unconstrained
jhu.nmt.lattice.rescore
AaltoHnmtMultitask
AaltoHnmtFlatcat
apertium.unconstrained
apertium−unconstrained
AaltoHnmtFlatcat
AaltoHnmtMultitask
jhu−nmt−lattice−rescore
Latvian→English
English→Latvian
Russian→English
English→Russian
tilde.nc.nmt.smt.hybrid
tilde.c.nmt.smt.hybrid
tilde−c−nmt−smt−hybrid
tilde−nc−nmt−smt−hybrid
tilde.nc.nmt.smt.hybrid
tilde.c.nmt.smt.hybrid
limsi.factored.norm
usfd.consensus.qt21
QT21.System.Combination
usfd.consensus.kit
tilde.nc.smt
tilde−nc−smt
usfd−consensus−kit
QT21−System−Combination
usfd−consensus−qt21
limsi−factored−norm
tilde−c−nmt−smt−hybrid
tilde−nc−nmt−smt−hybrid
afrl.mitll.syscomb
afrl.mitll.opennmt
afrl−mitll−opennmt
afrl−mitll−syscomb
PROMT.Rule.based
afrl.mitll.backtrans
afrl−mitll−backtrans
PROMT−Rule−based
Turkish→English
English→Turkish
afrl.mitll.m2w.nr1
afrl.mitll.syscomb
afrl−mitll−syscomb
afrl−mitll−m2w−nr1
jhu.nmt.lattice.rescore
jhu−nmt−lattice−rescore
Figure 5: Wilcoxon rank-sum signiﬁcance test results for pairs of systems competing in the News translation task, where a
green cell denotes a signiﬁcant win for the system in a given row over the system in a given column, at p ≤0.05.
# Researcher
Czech→English
Finnish→English
English→Russian
Table 8: Pearson correlation (r) between overall DA standardized mean adequacy scores collected via crowd-sourcing (Mturk)
and from researchers participating in the shared task (Appraise), numbers of assessments per system (#) are also provided for
each set-up.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
(a) Mechanical Turk (< 54 secs)
Translation Assessment Time (sec)
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
(b) Appraise (no filter)
Translation Assessment Time (sec)
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
(c) Appraise (< 54 secs)
Translation Assessment Time (sec)
Figure 6: Comparison of Completion Times for (a) Mechanical Turk Assessments; (b) Appraise Assessments (unﬁltered); (c)
Appraise Assessments (with reasonable cut-off imposed)
ration constraint we impose means that the maximum annotation time per translation is just under
54 seconds.
Although it is possible that assessment times for
Mechanical Turk HITs in Figure 6(a) still contain
a degree of idle-time exaggeration themselves, the
extent to which they could possibly obscure assessment times is vastly less than that of Appraise.
Prior to analysis of assessment times, we therefore impose a reasonable limit on what could be
considered a realistic maximum annotation time
for assessment of a single translation with DA on
Just to remind ourselves, the assessment of a single translation on Appraise includes:
(i) reading a reference translation; (ii) reading the
MT output; (iii) considering how well the latter expresses the meaning of the former; (iv) assigning
a score via the analogue rating scale; (v) pressing the submit button. We apply the same maximum cut-off applied within Mturk assessments of
54 seconds per translation assessment to Appraise
annotation times analysis therefore, which is a reasonable maximum duration for a single translation
assessment.
Figure 6(c) shows a plot of sorted
assessment times for Appraise assessments when
this cut-off is applied.
Once overly lengthy idle times have been omitted, it is possible to compare the speed at which
researchers and crowd-sourced workers complete
DA assessments, in addition to comparing annotation times in this year’s DA evaluation with
WMT16’s RR evaluation as both were completed
by researchers. Table 9 shows average annotation
times for each human annotator type, and annotation scheme. Annotation times for DA in terms of
the average time taken to assess a single translation are straightforward to compute, since a single
Researcher
Researcher
Table 9: Average annotation time per translation (in seconds)
translation is assessed per screen. Each RR assessment is made up of a relative ranking of ﬁve MT
output translations, however. Therefore to compute average annotation times for a single translation with RR we simply divide the average time to
evaluate ﬁve translations by ﬁve.
Before comparing annotation times, it is important to note that we must take care comparing annotations times collected in two different
year’s evaluation campaigns, as for researchers,
the annotators involved in the evaluation will have
some overlap, this is less likely for crowd-sourced
workers and in both cases the data involved comes
from two different data sets. The evaluation produced by researchers in WMT16 and WMT17
does, however, provide the ﬁrst data enabling a
comparison of annotation speeds for researchers
employing DA and RR. Annotation times analysis should only provide an approximate indication
of speeds as opposed to tried and tested ﬁndings,
however, which we hope to provide in the future.
Table 9 shows the reduction in average annotation time resulting from DA’s simpler assessment
set-up for researchers, from 20.8 seconds per assessment with RR to 17.1 seconds with DA, an
approximate reduction of 18%.
annotation
crowdsourced workers evaluating with DA in both
WMT16 and WMT17, we also see a slight speed
up from 19.6 to 17.5 seconds.
It is difﬁcult to
conclude from a comparison of crowd-sourced
workers that this as a genuine speed up as it is
likely due at least in part to variance in annotation
styles of two different groups of workers drawn
from a very large crowd. For example, average
annotation times of crowd-sourced workers in
the APE task this year was 13.6 seconds with
DA where a distinct set of workers was also
In terms of researchers versus crowd-sourced
workers evaluating with DA, when we compare this year’s results, researchers appear to be
marginally quicker, on average approximately 0.4
seconds faster per translation assessment.
Although again, this comparison includes average
annotation times of crowd-sourced workers that
can naturally vary from one group to the next.
Finally, we include a brief comparison in terms
of projected time commitments required by participants in future evaluations when the methodology employed is DA rather than RR. In subsequent evaluations, since we have veriﬁed that
DA results produced by quality controlled crowdsourcing correspond very closely to researcher results, it should be possible to collect all to-English
evaluations via crowd-sourcing. This means that
the switch to DA may result in only requiring participants to make a time commitment in terms of
out-of-English language pairs. For some research
groups this will cut the required manual evaluation
time commitment in half.
Assuming a similar number of language pairs
as in WMT17 (14 language pairs), an RR manual evaluation, which in previous years required
manual evaluation of 100 HITs (each containing
15 translations), amounts to a commitment of assessment of 1,500 translations per submitted system.
Considering researchers took on average
20.8 seconds per translation, a team wishing to
participate in all language pairs would require a
total time commitment of approximately (1,500
x 20.8 seconds x 14 = 436,800 seconds) 121.3
hours. In comparison for DA, even if we stick with
the same number of translations per submission
(1,500), when we take into account the fact that
all of the to-English language pairs can be crowdsourced as well as the quicker annotation time for
DA, the time commitment for such a team would
be reduced by approximately 60% to (1,500 x 17.1
seconds x 7 = 179,550 seconds) 49.9 hours.
Quality Estimation Task
This shared task builds on its previous ﬁve editions
to further examine automatic methods for estimating the quality of machine translation output at
run-time, without the use of reference translations.
It includes the (sub)tasks of word-level, phraselevel and sentence-level estimation. In addition to
advancing the state of the art at all prediction levels, our goals include:
• To test the effectiveness of larger (domainspeciﬁc
professionally
annotated)
datasets. We do so by signiﬁcantly increasing the size of one of last year’s training
• To study the effect of language direction and
domain. We do so by providing two datasets
created in similar ways, but for different domains and language directions.
• To investigate the utility of detailed information logged during post-editing. We do so by
providing a score for perceived post-editing
effort, post-editing time, keystrokes, and actual edits.
• To measure progress over years at all prediction levels. We do so by using last year’s test
set for comparative experiments.
This year’s shared task provides new training
and test datasets for all tasks, and allows participants to explore any additional data and resources deemed relevant. All tasks make use of a
large dataset produced from post-editions by professional translators. The data is domain-speciﬁc
(IT and Pharmaceutical domains) and substantially larger than in previous years. An in-house,
in-domain SMT system was used to produce translations for all tasks. System-internal information
was made available under request.
The data is
publicly available but since it was provided by industry collaborators it is subject to speciﬁc terms
and conditions. However, these have no practical
implications on the use of this data for research
The three tasks are deﬁned as follows: Task 1 at
sentence level (Section 4.4), Task 2 at word level
(Section 4.5), and Task 3 at phrase level (Section
4.6). Two datasets are used for all tasks (Section
4.3): English-German and German-English SMT
translations labelled with task-speciﬁc labels. Participants were also provided with a baseline set of
features for each task, and a software package to
extract these and other quality estimation features
and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems
for each task. A discussion on the main goals and
ﬁndings from this year’s task is given in Section
Baseline systems
Sentence-level baseline system:
For Task 1,
QUEST++12 was used to extract 17 MT
system-independent features from the source and
translation (target) ﬁles and parallel corpora:
• Number of tokens in the source and target
sentences.
• Average source token length.
• Average number of occurrences of the target
word within the target sentence.
• Number of punctuation marks in source and
target sentences.
• Language model (LM) probability of source
and target sentences based on models built
using the source or target sides of the parallel
corpus used to train the SMT system.
• Average number of translations per source
word in the sentence as given by the IBM
model 1 extracted using the SMT parallel
corpus, and thresholded such that P(t|s) >
0.2 or P(t|s) > 0.01.
• Percentage of unigrams, bigrams and trigrams in frequency quartiles 1 (lower frequency words) and 4 (higher frequency
words) in the source language extracted from
the source side of the SMT parallel corpus.
• Percentage of unigrams in the source sentence seen in the source side of the SMT parallel corps.
These features were used to train a Support Vector Regression (SVR) algorithm using a Radial
Basis Function (RBF) kernel within the SCIKIT-
LEARN toolkit.13 The γ,  and C parameters were
optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01,  =
0.0825, C = 20. This baseline system has proved
robust across a range of language pairs, MT systems, and text domains for predicting various
12 
questplusplus
13 
forms of post-editing effort .
Word-level baseline system:
For Task 2, the
baseline features were extracted with the MAR-
MOT tool . These are 28
features that have been deemed the most informative in previous research on word-level QE. 22 of
them were taken from the feature set described in
 , and had also been used as a
baseline feature set at WMT16:
• Word count in the source and target sentences, and source and target token count ratio.
Although these features are sentencelevel (i.e. their values will be the same for all
words in a sentence), the length of a sentence
might inﬂuence the probability of a word being wrong.
• Target token, its left and right contexts of 1
• Source word aligned to the target token, its
left and right contexts of 1 word. The alignments were given by the SMT system that
produced the automatic translations.
• Boolean dictionary features: target token is a
stopword, a punctuation mark, a proper noun,
or a number.
• Target language model features:
– The order of the highest order ngram
which starts and end with the target token.
– The order of the highest order ngram
which starts and ends with the source token.
– The part-of-speech (POS) tags of the target and source tokens.
(ti−2, ti−1, ti),
(ti−1, ti, ti+1),
(ti, ti+1, ti+2), where ti is the target
token ).
In addition to that, 6 new features were included
which contain combinations of other features, and
which proved useful in :
• Target word + left context.
• Target word + right context.
• Target word + aligned source word.
• POS of target word + POS of aligned source
• Target word + left context + source word.
• Target word + right context + source word.
The baseline system models the task as a
sequence prediction problem using the Linear-
Chain Conditional Random Fields (CRF) algorithm within the CRFSuite tool .
The model was trained using passive-aggressive
optimisation algorithm.
We note that this baseline is different from the
one used last year. In Section 4.7 we present results comparing this against last year’s baseline.
Phrase-level baseline system:
The phrase-level
system is identical to the one used in last year’s
shared task. The phrase-level features were also
extracted with MARMOT, but they are different
from the word-level features. They are based on
the sentence-level features in QUEST++.14 These
are the so-called “black-box” features — features
that do not use the internal information from the
MT system. The baseline uses the following 72
• Source phrase frequency features:
– average frequency of ngrams (unigrams,
bigrams, trigrams) in different quartiles
of frequency (the low and high frequency ngrams) in the source side of the
SMT parallel corpus.
– percentage of distinct source ngrams
(unigrams, bigrams, trigrams) seen in
the source side of the SMT parallel corpus.
• Translation probability features:
– average number of translations per
source word in the phrase as given
by the IBM model 1 extracted using
the SMT parallel corpus (with different
translation probability thresholds: 0.01,
0.05, 0.1, 0.2, 0.5).
– average number of translations per
source word in the phrase as given
by the IBM model 1 extracted using
the SMT parallel corpus (with different
translation probability thresholds: 0.01,
0.05, 0.1, 0.2, 0.5) weighted by the frequency of each word in the source side
of the parallel SMT corpus.
14 
files/features_blackbox
• Punctuation features:
– difference between numbers of various
punctuation marks (periods, commas,
colons, semicolons, question and exclamation marks) in the source and the target phrases.
– difference between numbers of various
punctuation marks normalised by the
length of the target phrase.
– percentage of punctuation marks in the
target or source phrases.
• Language model features:
– log probability of the source or target
phrases based on models built using the
source or target sides of the parallel corpus used to train the SMT system.
– perplexity of the source and the target
phrases using the same models as above.
• Phrase statistics:
– lengths of the source or target phrases.
– ratio between the source and target
phrase lengths.
– average length of tokens in source or target phrases.
occurrence
within the target phrase.
• Alignment features:
– number of unaligned target words, using the word alignment provided by the
SMT decoder.
– number of target words aligned to more
than one source word.
– average number of alignments per word
in the target phrase.
• Part-of-speech features:
– percentage of content words in the
source or target phrases.
– percentage of words of a particular part
of speech tag (verb, noun, pronoun) in
the source or target phrases.
– ratio of numbers of words of a particular part of speech (verb, noun, pronoun)
between the source and target phrases.
– percentage of numbers and alphanumeric tokens in the source or target
– ratio between the percentage of numbers
and alphanumeric tokens in the source
and target phrases.
This feature set was designed for sentences. We
expect that phrases, being sequences of words of
varied length, are similar to sentences and can be
treated analogously in QE. On the other hand, unlike sentences, phrases are related to their neighbouring phrases, and in this respect they are similar to words. Therefore, analogously to the baseline word-level system, we treat phrase-level QE
as a sequence labelling task, and model it using Conditional Random Fields. The phrase-level
baseline system is trained with CRFSuite toolkit
using passive-aggressive optimisation algorithm.
Participants
Table 10 lists all participating teams submitting
systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation
in speciﬁc tasks is denoted by a task identiﬁer (T1
= task1, T2 = task 2, T3 = task 3).
CDACM (T2,
The submissions from
CDACM use a recurrent neural network
language model (RNN-LM) architecture for
word-level QE as described in , and explore the word-level predictions
for phrase-level QE. CDACM’s WMT16 submission was modiﬁed to add other RNN
variants, such as LSTMs, deep LSTMs and
Another difference with respect to
the WMT16 submission is the addition of the
predicted history (only previous prediction)
and characters of the word as additional features to the RNN model. This modiﬁed architecture predicts the label (OK/BAD) in a slot
rather than predicting the word as in the case
of standard RNN-LMs. The input to the system is a word sequence, similar to the standard RNN-LM. Bilingual models were also
used and performed better than monolingual
models. The code for these models is freely
available.15
DCU (T2): DCU’s submission is an ensemble
of neural MT systems with different input
factors, designed to jointly tackle both the
automatic post-editing and word-level QE.
15 
Word-level features which have proven effective for QE, such as part-of-speech tags
and dependency labels are included as input
factors to NMT systems. NMT systems using different input representations are ensembled together in a log-linear model which is
tuned for the F1-mult metric using MERT
 . The output of the ensemble is
a pseudo-reference that is then TER aligned
with the original MT to obtain OK/BAD tags
for each word in the MT hypothesis.
DFKI (T1): These submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the
HTER score, the systems use a single-layer
perceptron with four outputs that jointly predict the number of each of the four distinct
post-editing operations that are then used to
calculate the HTER score. This also gives
the possibility to correct invalid (e.g. negative) predicted values prior to the calculation
of the HTER score.
The two submissions
use the baseline features and the English-
German submission also uses features from
 .
JXNU (T1): The JXNU submissions use features
extracted from a neural network, including
embedding features and cross-entropy features of the source sentences and their machine translations. The sentence embedding
features are extracted through global average pooling from word embedding, which
are trained using the WORD2VEC toolkit.
The sentence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the
source sentences and translation outputs, as
well as different sizes of the training corpus.
The experimental results show that the neural
network features lead to signiﬁcant improvements over the baseline, and that combining
the neural network features with baseline features leads to further improvement.
POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE
tasks are based on predictor-estimator architecture , which is the two-stage end-to-end
Participating team
Centre for Development of Advanced Computing, India 
Dublin City University 
German Research Centre for Artiﬁcial Intelligence, Germany 
Jiangxi Normal University, China 
Pohang University of Science and Technology, Republic of Korea 
Referential Translation Machines, Turkey 
University of Shefﬁeld, UK 
University of Hamburg, Germany 
Unbabel, Portugal 
Table 10: Participants in the WMT17 quality estimation shared task.
neural QE model. The predictor-estimator architecture consists of two types of stacked
neural network models:
1) a word prediction model based on bidirectional and
bilingual recurrent neural network language
model trained on additional large-scale parallel corpora and 2) a neural quality estimation
model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage
model, a stack propagation method was applied . In addition,
a “multilevel model” was developed where a
task-speciﬁc predictor-estimator model was
trained using not only task-speciﬁc training
examples but also all the other training examples of QE subtasks.
All the submitted
runs are ensembles that combine a set of neural models, trained under different settings
of varying dimensionalities and shufﬂing of
training examples.
RTM (T1, T2, T3): The RTM systems are improved versions over WMT16’s RTM submissions which average prediction scores
from different models using weights based
on their training performance to improve
the overall test performance.
use new features representing substring distances,
punctuation tokens,
character ngrams, and alignment crossings.
SHEF (T1, T2, T3): The SHEF team participated
in all the three sub-tasks.
For task 1, two
types of systems were submitted: CNN and
QUEST-EMB. The CNN submissions are
based on convolutional neural networks. The
system ﬁrst transforms the source and target
sentences into sequences of character embeddings, and then passes them through a series of deep parallel stacked convolution/max
pooling layers.
The baseline features are
provided through a multi-layer perceptron,
and then concatenated with the characterlevel information.
Finally, the concatenation is passed onto another multi-layer perceptron and the very last layer outputs HTER
The two submissions differ in the
the use of standard (CNN+BASE-Single) and
multi-task learning (CNN+BASE-Multi) for
training. The QUEST-EMB submission follows the word embeddings approach used
by for document-level
QE. Here in-domain word embeddings are
used instead of embeddings obtained general purpose data (same as in task 2, below).
Word embeddings were averaged to generate
a single vector for each sentence. Source and
target word embeddings were then concatenated with the baseline features and given to
an SVM regressor for model building.
For the word-level task SHEF investigated
a new approach based on predicting the
strength of the lexical relationships between
the source and target sentences (BMAPS).
Following the work by , a bilinear model is trained from three
matrices corresponding to the training data,
the development set and a “truth” matrix between them, which is built from the word
alignments and the gold labels to indicate
which lexical items form a pair, and whether
or not their lexical relation is OK or BAD.
The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They
train their model over 100 iterations with
the l2 norm as regulariser and using the
forward-backward splitting algorithm (FO-
BOS) as optimisation method. They report results considering
the word and its context versus the word in
isolation, as well as variants with and without the gold labels at training time.
Finally, for the phrase-level task, SHEF made
use of predictions generated by BMAPS for
task 2 and the phrase labelling approaches in
 . These approaches use
the number of BAD word-level predictions
in a phrase: an optimistic version labels the
phrase as OK if at least half of the words
in it are predicted to be OK, and a superpessimistic version labels the phrase as BAD
if any word is in is predicted to be BAD.
UHH (T1): The UHH-STK submission is based
on sequence and tree kernels applied on the
source and target input data for predicting
the HTER score.
The kernels use a backtranslation of the MT output into the source
language as an additional input data representation. Further hand-crafted features were
deﬁned in the form of the scores of the kernel functions applied on the pair of source
and back-translation sentences. The submitted runs outperformed the baseline systems
for both language pairs.
Unbabel (T1, T2): For word level, the “stacked”
system stacks a linear and a neural model
similar to the ones submitted by Unbabel
at WMT16. The “full-stacked-src-mt” system incorporates the output of an APE system, converted to OK/BAD tags, as an additional feature, similar to their work in . The sentence-level submissions use and normalise the word-level
predictions as percentage of words edited to
generate an HTER score.
One of the main differences between this year’s
and previous years’ tasks is the considerably larger
size of human-labelled datasets made available to
participants for training. Whereas the last year we
released a corpus of 12, 000 instances (plus 1, 000
and 2, 000 for development and test, respectively),
this year this ﬁgure was doubled. In contrast to last
year, we also provide datasets for two language
The structure used for the data have been the
same since WMT15. Each data instance consists
of (i) a source sentence, (ii) its automatic translation into the target language, (iii) the manually
post-edited version of the automatic translation,
(iv) a free reference translation of the source sentence. Post-edits are used to extract labels for the
different levels of granularity, which allows using
the same datasets for all three QE tasks.
The ﬁrst dataset contains texts in IT domain
translated from English into German. This is a
superset of the last year’s data: 11, 000 sentences
from the same source were added to the training
set. Their translations were produced using the
same statistical MT system and post-edited by professional translators who are native speakers of
German. The dataset statistics are outlined in Table 11.
The second dataset belongs to pharmaceutical
domain and provides translations from German
into English.
It contains 25, 000 instances for
training. Analogously to the IT dataset, automatic
translations were generated with a statistical MT
system and post-edited by professional translators.
The dataset statistics are shown in Table 12. The
Table shows another feature of this dataset: it contains much fewer errors than the IT one.
Development
Table 11: Statistics of the English–German dataset.
Development
Table 12: Statistics of the German–English dataset.
Task 1: Predicting sentence-level quality
This task consists in scoring (and ranking) translation sentences according to the proportion of their
words that need to be ﬁxed. HTER is used as quality score, i.e. the minimum
edit distance between the machine translation and
its manually post-edited version.
HTER labels were computed using the
TERCOM tool16 with default settings (tokenised,
case insensitive, exact matching only), with scores
capped to 1.
16 
Evaluation
Evaluation was performed against
the true HTER label and/or ranking, using the following metrics:
• Scoring: Pearson’s r correlation score (primary metric, ofﬁcial score for ranking submissions), Mean Average Error (MAE) and
Root Mean Squared Error (RMSE).
• Ranking: Spearman’s ρ rank correlation and
Statistical signiﬁcance on Pearson r was computed using the William’s test.17
Tables 13 and 14 summarise the results for Task 1 on German–English and English–
German datasets, respectively, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation
scores should be used to rank systems for the ranking variant.
The top three systems are the same for both
datasets, and the ranking of systems according
to their performance is similar for both datasets.
They are all based on neural models that ﬁrst
model the problem of word-level prediction and
then somehow generalise such predictions for sentence level QE, either by using them directly (Unbabel) or building a model from word to sentencelevel prediction (POSTECH). We also note that
the majority of the systems perform better than the
baseline, although ﬁve submissions are not significantly different from it.
Task 2: Predicting word-level quality
This task evaluates the extent to which we can detect word-level errors in MT output. Often, the
overall quality of a translated segment is signiﬁcantly harmed by speciﬁc errors in a small proportion of the words. Various classes of errors can
be found in translations, but for this task we consider all error types together, aiming at making a
binary distinction between correct (OK) and incorrect (BAD) tokens.
The binary labels for the datasets (OK
and BAD) were derived automatically from the
TERCOM tool with default settings and disabled
shifts (option “-d 0”). We aligned automatically
translated sentences with their post-edited version
and labelled each word in the automatic translation
17 
with an edit operation: insertion, deletion, substitution or no edit (correct word). We mark each
edited word as BAD, and the remainingn as OK.
Evaluation
Analogously to the last year’s task,
the primary evaluation metric is the multiplication of F1-scores for the OK and BAD classes,
denoted as F1-mult. Unlike previously used F1-
BAD score this metric is not biased towards “pessimistic” labellings.
We also report F1-scores
for individual classes for completeness. We test
the signiﬁcance of the results using randomisation tests with Bonferroni correction
 .
The results for Task 2 are summarised in
Tables 15 and 16, ordered by the F1-mult metric.
The top two systems are the same as for the
sentence-level task. This is perhaps not surprising since these are essentially word-level predictors: POSTECH and Unbabel. These along with
DCU’s submissions (which were speciﬁcally designed for the English–German word-level task),
are all based on neural models.
Word-level predictions for
sentence-level QE
Given that some submissions to the sentence-level
task which were actually based on word-level predictions performed very well at sentence level,
here we study the performance of all teams participating in the word-level task for sentence-level
prediction. The percentage of words labelled as
BAD in a sentence can essentially be seen as a
sentence-level HTER score. Participants were also
invited to submit an additional word-level system
tuned to optimise sentence-level scores, but we are
not aware of systems that did so.
In order to obtain sentence-level scores from
word-level predictions we computed HTER for
each sentence in the test set as the percentage
of words classiﬁed as BAD. We then evaluated
the submissions in terms of sentence-level metrics: Pearson correlation, MAE, RMSE. Table 17
shows the performance of the word-level systems
on the sentence-level task for the German–English
dataset and their comparison with the participants
of the Task 1. It can be clearly seen that wordlevel predictions are very close to sentence-level
ones: systems of different levels are well distributed along the ranked list.
The submissions by POSTECH and Unbabel
show that word-level and sentence-level systems
Spearman ρ
• POSTECH/Combined-MultiLevel-Ensemble
POSTECH/SingleLevel-Ensemble
Unbabel/full-stacked-src-mt
RTM/RTM-MIX
RTM/RTM-TREE
Unbabel/stacked
SHEF/QUEST-EMB-SCALE
JXNU/Emb+RNNLM+QuEst+SVM
SHEF/CNN+BASE-Single
SHEF/CNN+BASE-Multi
Table 13: Ofﬁcial results of the WMT17 Quality Estimation Task 1 for the German–English dataset. The winning submission
is indicated by a • and is statistically signiﬁcantly different from all others. Submissions in the grey area are those which are
not signiﬁcantly different from the baseline.
Spearman ρ
• POSTECH/Combined-MultiLevel-Ensemble
POSTECH/SingleLevel-Ensemble
Unbabel/full-stacked-src-mt
Unbabel/stacked
JXNU/Emb+RNNLM+QuEst+SVM
SHEF/QUEST-EMB-SCALE
RTM-PLS-GBR
SHEF/CNN+BASE-Single
SHEF/CNN+BASE-Multi
Table 14: Ofﬁcial results of the WMT17 Quality Estimation Task 1 for the English–German dataset. The winning submission
is indicated by a • and is statistically signiﬁcantly different from all others. Submissions in the grey area are those which are
not signiﬁcantly different from the baseline.
• POSTECH/Combined-MultiLevel-Ensemble
• Unbabel/full-stacked-src
POSTECH/SingleLevel-Ensemble
Unbabel/stacked
RTM/s4-RTM-GLMd
SHEF/BMAPS-unigram
SHEF/BMAPS-nolabel-unigram
Table 15: Ofﬁcial results of the WMT17 Quality Estimation Task 2 for the German–English dataset. The winning submissions
are indicated by a • and are statistically signiﬁcantly different from all others. Submissions in the grey area are those which are
not signiﬁcantly different from the baseline.
• POSTECH/Combined-MultiLevel-Ensemble
• Unbabel/full-stacked-src-mt
• DCU/SRC-APE-QE-TUNED
• DCU/AVG-ALL
POSTECH/SingleLevel-Ensemble
Unbabel/stacked
RTM/s5-RTM-GLMd
RTM/s4-RTM-GLMd
SHEF/BMAPS-unigram
SHEF/BMAPS-nolabel-unigram
Table 16: Ofﬁcial results of the WMT17 Quality Estimation Task 2 for the English–German dataset. The winning submissions
are indicated by a • and are statistically signiﬁcantly different from all others. Submissions in the grey area are those which are
not signiﬁcantly different from the baseline.
trained on the same data using the same (or
similar) methods yield very close results:
POSTECH sentence-level systems occupy the ﬁrst
two positions in the list, while their word-level
systems follow.
The corresponding word-level
and sentence-level systems by Unbabel are even
closer, their differences are not statistically signiﬁcant. This is expected since Unbabel’s submission
to the sentence-level task was based on their predictions for the word-level task. Finally, the baselines for the two task do not show signiﬁcant differences in their performance either, although they
are based on very different features and models.
Overall, these results suggest that word-level
QE models can indeed be successfully used to predict sentence-level quality of translation. Additionally, sentence-level metrics proved suitable for
the evaluation of word-level QE models (the rankings of word-level submissions produced by F1mult and Pearson r metrics have correlation coef-
ﬁcient of 0.96). Results for the English–German
task show the same trend.
Task 3: Predicting phrase-level quality
This level of granularity was ﬁrst introduced in the
shared task at WMT16. The goal is to predict MT
quality at the level of phrases.
The phrase-level QE task requires segmenting training and test sentences into phrases.
We used the segmentation produced by the SMT
system which generated automatic translations for
the datasets.
The phrase-level labels were produced from binary word-level labels: we labelled
a phrase as OK if all words in it were correct (OK
words). Any phrase with one or more BAD words
was labelled as BAD.
Evaluation
In contrast to the last year’s phraselevel shared task, where we used word-level metrics to evaluate phrase-level submissions, this time
we resort to phrase-level F1 scores. The reason for
that is that the word-level metrics were unable to
differentiate between various systems. Therefore,
here our primary metric is the phrase-level version of F1-mult, and we also report phrase-level
F1-BAD and F1-OK. Statistical signiﬁcance was
computed using randomised test with Bonferroni
correction as in task 2.
The results of the phrase-level task are
represented in Tables 18 and 19.
These results follow from those for the word-level task,
with POSTECH showing signiﬁcantly better results overall.
Discussion
In what follows, we discuss the main ﬁndings of
this year’s shared task based on the goals we had
previously identiﬁed for it.
Larger training data
To test the effectiveness
of larger (domain-speciﬁc and professionally annotated) datasets, we increase the size of last
year’s training set for English–German. In order to
check if the increased training data size helps improve the systems’ performance we compare the
baseline systems for all tasks trained on last year’s
versus this year’s dataset, with parameters optimised on the same development sets.
• POSTECH/Combined-MultiLevel-Ensemble
POSTECH/SingleLevel-Ensemble
word POSTECH/Combined-MultiLevel-Ensemble
word POSTECH/SingleLevel-Ensemble
Unbabel/full-stacked-src-mt
word Unbabel/full-stacked-src-mt
RTM/RTM-MIX
RTM/RTM-TREE
Unbabel/stacked
word Unbabel/stacked
SHEF2/QUEST-EMB-SCALE
JXNU/Emb+RNNLM+QuEst+SVM
word BASELINE
word CDACM/RNN
word RTM/s4-RTM-GLMd
SHEF1/CNN+BASE-Single
SHEF1/CNN+BASE-Multi
word SHEF/BMAPS-nolabel-unigram
word SHEF/BMAPS-unigram
Table 17: Additional results of the WMT17 Quality Estimation Task 1 for the German–English dataset: using for the wordlevel predictions for sentence-level QE, evaluated for scoring. The winning submission is indicated by a • and is statistically
signiﬁcantly different from all others. Submissions in the grey area are those which are not signiﬁcantly different from the
baselines. The word-level systems are denoted with preﬁx word.
• POSTECH/PredictorEstimator-Combined-MultiLevel-Ensemble
POSTECH/PredictorEstimator-SingleLevel-Ensemble
RTM/s5-RTM-GLMd
RTM/s4-RTM-GLMd
SHEF/BMAPS-unigram-opti
SHEF/BMAPS-unigram-nolabel-opti
Table 18: Ofﬁcial results for the WMT17 Quality Estimation Task 3 for the German-English data. The winning submission is
indicated by a • and is statistically signiﬁcantly different from all others. The gray area indicates the submissions whose results
are not statistically different from the baseline.
• POSTECH/PredictorEstimator-Combined-MultiLevel-Ensemble
POSTECH/PredictorEstimator-SingleLevel-Ensemble
SHEF/BMAPS-unigram-opti
SHEF/BMAPS-unigram-nolabel-opti
Table 19: Ofﬁcial results for the WMT17 Quality Estimation Task 3 for the English–German data. The winning submission is
indicated by a • and is statistically signiﬁcantly different from all others. The gray area indicates the submissions whose results
are not statistically different from the baseline.
In Table 20 we show the performance of the
baseline systems for all tasks trained on the
WMT16 and WMT17 English–German datasets
and tested on the WMT16 test set.
The performance improves for all tasks when using the
WMT17 training set, which is much larger. However, the gain for the word-level and phrase-level
tasks is smaller than that for sentence level. For
the word-level task, we also include experiments
with the WMT16 baseline system, which was simpler than the WMT17 baseline system. We observe larger improvement from the new word-level
features which we included in this year’s baseline
system than from the larger training set. This suggests that better features/models can lead to larger
performance gains than more data, at least for the
word-level task.
2016 word-level baseline
Training set
2017 word-level baseline
Training set
Phrase-level baseline
Training set
Sentence-level baseline
Training set
Table 20: Comparison of baseline English–German systems trained on WMT16 and WMT17 datasets (tested on the
WMT16 test set) for all tasks.
Progress over years
Progress over years is a dif-
ﬁcult factor to measure. We attempted to do so this
year for the ﬁrst time given the similarity between
the tasks this and last year for the English–German
data. We do so by requesting participants in this
year’s task to submit results using their WMT17
systems on the WMT16 test sets. We note however that this comparison is also affected by the
increased size of the training set for this language
pair in the current edition of the task.
Therefore, the WMT17 systems may be better systems
because of better techniques but also because of
larger amounts of training data.
In Table 21 we compare the results from
WMT16 and WMT17 systems on the WMT16 test
set at sentence level, where WMT16 systems are
highlighted in cyan background. Overall, it can
be clearly seen that WMT17 systems perform better: last year’s top system is only the 4th best
compared to the WMT17 submissions, and half of
WMT16 participants are below this year’s baseline. It is important to note that the baseline performs much better than last year because of the
additional training data – as shown in Table 20 –
since the baseline system itself did not change.
Table 22 shows the results for word-level systems, which indicates a similar trend: systems also
improved from last year’s submissions, with last
year’s winner being outperformed by four other
systems, and the majority of WMT16 participants
performing closely to this year’s baseline (which
we note is a stronger model than last year’s baseline as previously discussed).
Finally, the same trend is observed when comparing phrase-level systems submitted to WMT16
and WMT17 in Table 23. The only difference is
that although the new data improved the performance of the phrase-level baseline system, this improvement did not change its position in the systems ranking.
Overall, the (Person r and F1-mult) scores of
the winning submissions this year is much higher
than in last year’s results, which we believe to be a
combination of better techniques as well as better
(larger) data.
The progress of state-of-the-art QE models can
also be tracked by the performance of recurring
participants: the results of systems by POSTECH
(tasks 1, 2, 3) and CDACM (task 2) teams are better this year.
We note the increasing popularity of neural networks and their improving performance for QE:
although some of the last year’s winners (e.g.
YSDA team which won the sentence-level task)
did not use neural networks, all WMT17 winners
and the majority of best-performing systems use
neural networks for model building.
Languages and domains
To study the effect
of language direction and domain, we provided
two datasets created in similar ways, but for different domains and language directions, as was
previously mentioned. The QE performance on
these datasets varies considerably, with German–
English showing higher scores for the sentence-
• POSTECH/PredictorEstimator-Combined-MultiLevel-Ensemble
POSTECH/PredictorEstimator-SingleLevel-Ensemble
JXNU/Emb+RNNLM+QuEst+SVM
• YSDA/SNTX+BLEU+SVM
SHEF/QUEST-EMB-SCALE
POSTECH/SENT-RNN-QV2
SHEF-LIUM/SVM-NN-emb-QuEst
POSTECH/SENT-RNN-QV3
SHEF-LIUM/SVM-NN-both-emb
SHEF/CNN+BASE-Single
UGENT-LT3/SCATE-SVM2
BASELINE 2017
SHEF/CNN+BASE-Multi
UFAL/MULTIVEC
RTM/RTM-FS-SVR
UGENT-LT3/SCATE-SVM1
RTM/RTM-SVR
BASELINE 2016
SHEF/SimpleNets-SRC
SHEF/SimpleNets-TGT
RTM-PLS-GBR
Table 21: Comparison of ofﬁcial results of WMT17 and WMT16 sentence-level QE task on the English–German WMT16 test
set. The winning submission is indicated by a • and is statistically signiﬁcantly different from all others. WMT16 systems are
highlighted with cyan.
level task, both in terms of the baseline systems
the winning submissions, and English–German
showing generally higher scores for the word and
phrase-level tasks (except for the baseline system
in the phrase-level task). Even though the performance scores may not be directly comparable, we
can make some interesting observations. We believe that the main reasons for these differences
are related to the general quality of the MT systems and – as a consequence – the distribution of
quality labels in the QE datasets, and – to a lesser
extent – the sizes of the QE training sets, which
are slightly different (see Tables 11 and 12).
The quality of the translations in each dataset is
very different. As shown in Tables 11 and 12, the
German–English dataset contains much fewer errors. Indeed, when building the SMT systems that
generated these translations, we observed very different BLEU scores: 35.9 for English–German (IT
domain), and 53.4 for German–English (Pharma
domain). This difference in quality is not due to
training settings, since these were the same for
both datasets, except that for English–German the
SMT training set was much larger (7.2 vs 2.09
million sentences). Details on the SMT models
and data used to build such models are given in
 .
In addition to the wellknown fact that translating into English normally
leads to better quality than translating from English, we hypothesise that this difference could be
due to higher token repetition rate in the German–
English dataset.
The difference in quality was
conﬁrmed by the average HTER score obtaining
from the post-editing of these test sets: 0.25 for
English–German and 0.19 for German–English.18.
The fact that the German–English dataset contains fewer errors makes it harder for the word
and phrase-level tasks to achieve high F1-mult as
18We note that these BLEU and HTER scores were measured on a superset of this data, as described in than for English–German
(0.628 vs 0.904, respectively – Table 16), showing that the systems tend to overpredict OK labels for German–English. The same applies to the
phrase-level task. For the sentence-level task, the
skewed distribution towards good quality translations does not have the same effect, perhaps due to
the prediction of an aggregated (HTER) score and
the metric used for evaluation.
Additional evidence
To investigate the utility of
detailed information logged during post-editing,
we offered to participants other sources of information: post-editing time, keystrokes, and actual
Surprisingly, no participating system requested these additional labels. The DFKI submission re-created some of this information by further
annotating words with the actual edit operations,
as obtained from the HTER alignments. Instead of
predicting the HTER score, the systems attempted
to predict the number of each of the four postediting operations (add, replace, shift, delete) at
the sentence level. However, this did not lead to
positive results. In future editions of the task, we
plan to make this detailed post-editing information
available again and suggest clear ways of using it.
Automatic Post-editing Task
The WMT shared task on MT automatic postediting (APE), this year at its third round at WMT,
aims to evaluate systems for the automatic correction of errors in a machine translated text. As
pointed out by , from the
application point of view the task is motivated by
its possible uses to:
• Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage;
• Cope with systematic errors of an MT system
whose decoding process is not accessible;
• Provide professional translators with improved MT output quality to reduce (human)
post-editing effort;
• Adapt the output of a general-purpose MT
system to the lexicon/style requested in a speciﬁc application domain.
The third round of the APE task proposed to
participants the same general evaluation framework of the previous ones . It consists in a “black box” scenario in
which the MT system that produced the translations is unknown to the participants and cannot be
This year the task has been extended by including German-English as a new language direction in addition to English-German, which
was the only language pair covered in the 2016
For both directions, participants operated with domain-speciﬁc data (information
technology for EN-DE and pharmacological
for DE-EN),19 with post-edits collected from professional translators.20 All data has been provided
by the European Project QT21.21
As in 2016, TER and BLEU computed between
automatic and human post-edits have been respectively used as primary and secondary evaluation
metrics. In continuity with the previous round, a
manual evaluation has also been carried out to gain
further insights on ﬁnal output quality. However,
while in 2016 Appraise22 was
employed for manual evaluation, this year the German to English evaluation was carried out via direct human assessment and
quality controlled crowd-sourcing on Amazon’s
Mechanical Turk23, while the English to German
evaluation was completed, again via direct assessment, but translation students were employed as
opposed to crowd-sourcing.
In terms of participants and submitted runs, this
year’s round replicated the success of the 2016
edition. On English-German we had 7 participants
 , with a total of 15 submitted runs. On German-English (a more challenging
direction due to a much higher quality of the original MT output), we had 2 participants, with a total
of 5 submitted runs.
Building on the recent success of neural ap-
19As opposed to the general news domain data used in the
ﬁrst round, which proved to be more difﬁcult to handle due
to scarce repetitiveness.
20As opposed to the less coherent crowdsourced material
used in the ﬁrst round.
21 
22 
23 
proaches to APE, this year all the submissions relied on neural end-to-end solutions. The adoption
of multi-source models (able to combine information from raw MT output and the original source
text) and the extensive use of available synthetic
data (to increase the size of the training set) are
other traits common to several systems.
On both directions, all participants managed to
beat the baseline, at least with their primary submission. Top results achieved impressive improvements up to -4.9 TER and +7.6 BLEU points on
English-German and smaller, but statistically signiﬁcant gains up to -0.25 TER and +0.3 BLEU
on German-English.
The manual evaluation of
participants’ primary submissions conﬁrmed the
jump in performance of this year’s systems in the
English-German task. Although all of them are
still below human quality, the gap has been reduced with respect to the 2016 round, three systems are almost on par in the top tier (last year
it was only one) and the improvements over the
baseline are signiﬁcantly better than the original
MT output prior to post-editing for all participants
(last year this was only true for the top submission).
Task description
Similar to previous years, participants were provided with training and development data consisting of (source, target, human post-edit) triplets,
and were asked to return automatic post-edits for
a test set of unseen (source, target) pairs.
Previous rounds of the APE task suggested and conﬁrmed 
the dependence of system results on data repetitiveness.
In the 2015 pilot task, dealing with
“general-domain” news data and crowdsourced
post-edits proved to be very difﬁcult due to data
sparsity issues that prevented participants to learn
from the training set useful correction patterns reapplicable to the test set.
In 2016, the switch
to more repetitive (in other terms, less sparse)
domain-speciﬁc data post-edited by professional
translators resulted in a higher applicability of the
learned correction patterns.
The effect of this
switch was made evident by ﬁnal results: while
none of the submitted runs was able to beat the
baseline in the pilot round, more than half of the
submissions signiﬁcantly outperformed it in 2016.
Based on these outcomes, and to give stability to
a relatively young task, also this year we opted for
the adoption of domain-speciﬁc data post-edited
by professionals for both language directions.
Training and development sets consist of
(source, target, human post-edit) triplets in which:
• The source (SRC) is a tokenized sentence
with length between 3 and 30 tokens;
• The target (TGT) is a tokenized translation of
the source. Translations were obtained from
statistical MT systems.24 This information,
however, was unknown to participants, for
which the MT system was a black-box.
• The human post-edit (PE) is a manuallyrevised version of the target, done by professional translators.
Test data consists of (source, target) pairs having similar characteristics of those in the training
set. Human post-edits of the test target instances
were left apart to measure system performance.
English-German
data were drawn from the
Information Technology (IT) domain. Training and test sets respectively contain 11,000 and
2,000 triplets.
The data released for the 2016
round of the task (15,000 instances) and the artiﬁcially generated post-editing triplets (4 million instances) used by last year’s winning system 
were also provided as additional training material.
German-English
data were drawn from the
Pharmacological domain. Training and development sets respectively contain 25,000 and 1,000
triplets, while the test set consists of 2,000 instances.
Table 24 provides some basic statistics about
the data (the same used for the sentence-level
quality estimation task), which has been released
by the European Project QT21 .25 In addition, Tables 25 and 26 provide
a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets released in the three rounds
24We used phrase-based MT systems trained with generic
leveraging
prereordering techniques , and taking advantage of POS and word class-based language models.
25For both language directions, the source sentences
translations
( 
Train (23,000)
Dev (1,000)
Test (2,000)
Train (25,000)
Dev (1,000)
Test (2,000)
Table 24: Data statistics.
of the WMT APE task. RR measures the repetitiveness inside a text by looking at the rate of
non-singleton n-gram types (n=1...4) and combining them using the geometric mean. Larger values
indicate a higher text repetitiveness and, as discussed in , suggest a higher
chance of learning from the training set correction patterns that are applicable also to the test set.
In we considered the large
differences in repetitiveness between APE15 and
APE16 data as a possible motivation for the significant baseline improvements achieved by participants in the second round of the task. As we will
see in Section 5.4, similar explanations hold for
this year’s results, in which the higher repetitiveness of English-German data likely contributed to
facilitate the task in comparison with the German-
English direction.
Table 26 shows, for the same data sets, the
Translation Error Rate (TER) and the BLEU score of the original target translations, computed
against the human post-edits.
In this case, numeric evidence of a higher quality of the original
translations can indicate a smaller room for improvement for APE systems (having, at the same
time, less to learn during training and less to correct at test stage). On one side, indeed, training on
good (or near-perfect) automatic translations can
drastically reduce the number of learned correction patterns. On the other side, testing on similarly good translations can drastically reduce the
number of corrections required and the applicability of the learned patterns, thus making the task
potentially more difﬁcult. Together with the lower
repetition rates observed, also the large differences
in translation quality between the two APE17 language directions (62.49 BLEU for APE17 EN-DE
vs 79.54 for APE17 DE-EN) suggest a higher dif-
ﬁculty for the German-English task. Further indications in this direction are provided by Figures 7
and 8, which plot the TER distribution for the test
items in the two data sets. As can be seen, the
quality of English-German data is much more balanced compared to German-English, with about
50% of the test items distributed over the ﬁrst
ﬁve bins. In particular, what makes a big difference between the two test sets is the proportion of
“perfect” test instances having TER=0 (i.e. items
that should not be modiﬁed by the APE systems).
While for English-German they represent 14.0%
of the total, for German-English they are about
45.0% of the test data. This means that, for almost
half of the German-English test set, any correction
made by the APE systems will be unnecessary and
penalized by automatic evaluation metrics. This
difﬁcult scenario calls for conservative and precise
systems able to properly ﬁx errors only in the remaining 50% of the data.
Evaluation metric
System performance was evaluated by computing
the distance between automatic and human postedits of the machine-translated sentences present
in the test set (i.e. for each of the 2, 000 target
test sentences). Similar to last year, this distance
was measured in terms of TER and BLEU (casesensitive).26 Systems were ranked based on the average TER calculated on the test set by using the
26In the case of TER, the baseline is computed by averaging the distances between each machine-translated sentence
and its human-revised version. The actual evaluation metric
is the human-targeted TER (HTER). For the sake of clarity,
since TER and HTER compute edit distance in the same way
(the only difference is in the origin of the correct sentence
used for comparison), henceforth we will use TER to refer to
both metrics.
APE17 EN-DE
APE17 DE-EN
Table 25: Repetition Rate (RR) of the WMT15 (English-Spanish, news domain, crowdsourced post-edits), WMT16 (English-
German, IT domain, professional post-editors), WMT17 EN-DE (English-German, IT domain, professional post-editors) and
WMT17 DE-EN (German-English, pharmacological domain, professional post-editors) APE task data.
APE17 EN-DE
APE17 DE-EN
Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16,
WMT17 EN-DE and WMT17 DE-EN data.
Figure 7: TER distribution over the EN-DE test
Figure 8: TER distribution over the DE-EN test
TERcom27 software: lower average TER scores
correspond to higher ranks. BLEU was computed
using the multi-bleu.perl package28 available in
Also this year, the ofﬁcial baseline results are the
TER and BLEU scores calculated by comparing
the raw MT output with the human post-edits.
In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions.
In continuity with the previous rounds, we
used as additional term of comparison a reimplementation of the method ﬁrstly proposed
by Simard et al. .
It relies on a phrasebased post-editing approach to the task, which represented the common backbone of APE systems
before the spread of neural solutions. The system
is based on Moses ; translation and reordering models were estimated following the Moses protocol with default setup using
27 
28 
blob/master/scripts/generic/multi-bleu.perl
MGIZA++ for word alignment. For language modeling we used the KenLM
toolkit for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training . The results of this additional term
of comparison are also reported in Tables 28-29.
For each submitted run, the statistical signiﬁcance of performance differences with respect to
the baseline and our re-implementation of Simard
et al. was calculated with the bootstrap
test .
Participants
Seven teams participated in the English-German
task by submitting a total of ﬁfteen runs. Two of
them also participated in the German-English task
with ﬁve submitted runs. Participants are listed in
Table 27, and a short description of their systems
is provided in the following.
Adam Mickiewicz University.
AMU’s (EN-
DE) participation explores and combines multiple neural architectures available in the Marian toolkit.29 They include single source 
Univerzita Karlova v Praze, Czech Republic 
Dublin City University, Ireland 
Fondazione Bruno Kessler, Italy 
Jiangxi Normal University, Nanchang, China 
University of Lille & University Grenoble, France 
Saarland University, Germany
Fondazione Bruno Kessler, Italy 
University of Lille & University Grenoble, France 
Table 27: Participants in the WMT17 Automatic Post-editing task.
src →pe or mt →pe) and multi-source models
({src, mt} →pe), the latter being able to combine information from raw MT output and original source language input.
Different attention
mechanisms are explored, including soft attention
(looking at information anywhere in the source sequence during decoding) and hard monotonic attention (looking at one encoder state at a time
from left to right, thus being more conservative
and faithful to the original input), which are combined in different ways in the case of multi-source
models. The artiﬁcial data provided by Junczys-
Dowmunt and Grundkiewicz are used to
boost performance by increasing the size of the
corpus used for training.
Univerzita Karlova v Praze.
CUNI’s (EN-DE)
system is based on the character-to-character neural network architecture described in . This architecture was compared with the
standard neural network architecture proposed by
Bahdanau et al. which uses byte-pair encoding for generating translation tokens. During the experiments, two setups
have been compared for each architecture: i) a single encoder with SRC and MT sentences concatenated, and ii) a two-encoder system, where each
SRC and MT sentence is fed to a separate encoder.
The submitted system uses the two-encoder architecture with a character-level encoder and decoder.
The initial state of the decoder is a weighted combination of the ﬁnal states of the encoders. Attention is computed separately over each encoder.
The model was trained using both the WMT17
training data and the artiﬁcial data provided by
Junczys-Dowmunt and Grundkiewicz . The
WMT17 training dataset was sampled to match the
size of the artiﬁcial data. The submitted primary
submission used beam-search for decoding while
greedy decoding was used for the contrastive submission.
Dublin City University.
DCU’s (EN-DE) submission is an ensemble of neural MT systems
with different input factors, designed to jointly
tackle both the APE task and the Word-Level QE
task. Word-Level features which have proven effective for QE, such as word-alignments, partof-speech tags, and dependency labels, are included as input factors to neural machine translation systems, which are trained to output Post-
Edited MT hypotheses.
Concatenated source +
MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided
by Junczys-Dowmunt and Grundkiewicz ,
as well as min-risk training for ﬁne-tuning . The neural systems, which use different input representations but share the same output
vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric
using MERT.
Fondazione Bruno Kessler.
FBK’s (EN-DE &
DE-EN) submission extends the existing NMT
implementation in the Nematus toolkit to train an ensemble of multi-source
neural APE systems. Building on previous participations based on the phrase-based paradigm
 , and similar to , such systems jointly learn
from source and target information in order to increase robustness and precision of the automatic
corrections. The n-best hypotheses produced by
this ensemble are further re-ranked using features
based on the edit distance between the original MT
output and each APE hypothesis, as well as other
statistical models (n-gram language model and operation sequence model).
For English-German,
generic models are trained using the ∼4M synthetic data provided by Junczys-Dowmunt and
Grundkiewicz , and then ﬁne-tuned with
in-domain data. Similarly, for German-English,
synthetic post-editing training data are created by
round-trip translation of a sub-set of parallel data
released in the medical task at WMT‘14 .
Jiangxi Normal University.
JXNU’s (EN-DE)
system contains three neural automatic postediting models:
npe baseline, npe minor and
npe single.
Based on Junczys-Dowmunt and
Grundkiewicz , the npe baseline model
is created and trained with the training set of-
ﬁcially released by the evaluation campaign.
The npe minor model is obtained by ﬁne-tuning
npe baseline with a triplets corpus including raw
machine translation outputs needing four or less
edit operations. The npe single model is obtained
by ﬁne-tuning npe baseline with a triplets corpus
containing machine translations needing at most
two edit operations.
The output of these three
systems is integrated into an n-best list of translations hypotheses, which are scored and ranked
by means of a sentence-level QE approach and a statistical language model
 . Since the raw machine translation
outputs can be classiﬁed into ﬁve grades according
to the above sentence-level QE score, the best output can be selected from the n-best list in accordance with the raw MT outputs’ grading. The features used by these models can mitigate the overcorrection problem emerged in previous rounds of
the APE task .
University of Lille & University of Grenoble.
LIG’s (EN-DE & DE-EN) submission is a neuralbased APE system that exploits the approach proposed by Libovick´y et al. : instead of predicting words, it predicts edit operations (keep,
delete, or insert a word). An advantage of this
approach, is that it is very easy to learn to replicate the (“do-nothing”) baseline, by just predicting keep operations. By contrast, it can be hard for
a classic NMT model to learn the identity function, in particular because of the unknown word
problem, and because of the limited amounts of
training data. LIG’s submission proposes a number of improvements over this method: the simplest model (‘Contrastive-Forced’) uses a taskspeciﬁc attention mechanism, which forces the decoder to look at the right word in the input (i.e.,
the word being post-edited). This simple approach
gives very good results on the English-German
task in limited data conditions.
Finally, they
also propose a chained architecture (‘Contrastive-
Chained’), which uses two different models (and
two different training objectives): a translation
model (src →mt), and a post-editing model
(mt →pe). The attention vectors over src learned
by the translation model are used by the postediting model to give additional contextual information (when predicting a new edit operation, it
can look at the mt word to post-edit, and at the
src words that are aligned to this word.) This approach is a way to incorporate the source sentence
into the proposed framework, and gives promising
results on the English-German task, when adding
more data (‘primary’ models).
Saarland University.
USAAR’s (EN-DE) submission combines a neural model and an operation
sequence (OSM) phrase-based 
model. The neural system is trained on a bidirectional (forward-backward) RNN-based encoderdecoder30 MT model 
trained for mt →pe translation. The network has
been trained for 5 days using a hyper-parameter
setting similar to .
data consists of WMT-2016, 2017 APE data (23K)
and 4.5M artiﬁcial APE data . The OSM phrase-based
system consists of three basic
components: corpus pre-processing, hybrid word
alignment and a vanilla setting
of a phrase-based MT system integrated with the
hybrid word alignment. The model used 23K (target, human post-edit) data for training. Experiments on the WMT-2017 test set using both the
neural and the OSM-based APE systems revealed
that the neural system provides better performance
for short sentences (less than 15 words) and the
OSM-based APE model performs better for the
longer ones. A manual inspection indicates that
the neural system suffers from a “lack of coverage” while translating longer sentences. There-
30The system used is GroundHog – 
com/lisa-groundhog/GroundHog.
fore, the ﬁnal submission was based on a mix
of neural translations for short test sentences and
OSM translations for the longer ones.
TER/BLEU results
Participants’ TER and BLEU results are shown
in Tables 28 (English-German) and 29 (German-
The submitted runs are ranked based
on the average TER (case-sensitive), which is
the APE task primary evaluation metric.
Overall, similar to last year, TER and BLEU rankings
do not show major differences. The main ones
can be found in the English-German task where:
i) two mid-ranked primary submissions (USAAR
and JXNU) are inversely ordered by the two metrics, and ii) the phrase-based APE (worse in terms
of TER) would outperform the “do-nothing” strategy by around 0.48 BLEU points. In the German-
English task, TER and BLEU rankings differ in
the ordering of a primary submission and the “donothing” baseline, but the negligible score differences are not signiﬁcant.
As we will see in
Section5.5, for English-German, the human evaluation based on direct assessment (DA) suggests a
third different ranking that is slightly closer to the
BLEU-based one (two primary submissions are
ranked in the same position, while with TER this
happens only in one case). On German-English, a
slight preference is conﬁrmed for the BLEU-based
ranking as shown by the small difference (0.1) in
average DA scores in favour of the “do-nothing”
baseline over the second-ranked primary submission. However, due to the small differences in systems’ architectures and results, it’s not surprising
that different metrics and evaluation criteria produce slightly different rankings. Also this year, it’s
hence difﬁcult to draw deﬁnite conclusions about
which automatic metric is more reliable.
English-German
Compared to previous rounds
of the APE task, the most noticeable aspect is that
this year, for the ﬁrst time, all participants managed to beat the MT baseline at least with their
primary submission.31 This steady improvement
has been mainly driven by the massive migration
to the neural approach, which in 2016 allowed the
winning system to achieve impressive results (-
3.24 TER, +5.54 BLEU with respect to the baseline). This year, the gains on English-German data
31In 2015, none of the submitted runs were able to consistently improve over the raw MT output. Last year, only half
of the runs outperformed this baseline.
are even larger, with the winning system scoring -
4.88 TER and +7.58 BLEU points better than the
MT baseline. The technology advancement is evident if we look at our second term of comparison: the re-implementation of the phrase-based
approach by Simard et al. . Last year, on
English-German, the results of this method were
better than the baseline and in a middle position in
the ofﬁcial participants’ ranking. This year, on the
same language direction, they are almost identical
to those achieved in 2016, but also: i) worse than
the baseline in terms of TER (+0.21), ii) slightly
better in terms of BLEU (+0.48) and iii) competitive only against the contrastive submission of one
participant. Considering the distance between the
same phrase-based approach and the baseline as
an indicator of the task difﬁculty across different rounds of the task, we hypothesize that the
good results achieved by this year’s participants
are mainly due to improved techniques rather than
“easier” test data.
Indeed, for English-German
where a comparison with last year is possible, the
close repetition rate and BLEU scores reported in
Tables 25 and 26 reveal a similar level of difﬁculty
for the APE16 and APE 17 test data.
FBK Primary
AMU Primary
AMU Contrastive
DCU Primary
DCU Contrastive
FBK Contrastive
FBK USAAR Contr.
USAAR Primary
LIG Primary
JXNU Primary
LIG Contrastive-Forced
LIG Contrastive-Chained
CUNI Primary
USAAR Contrastive
 
CUNI Contrastive
Table 28: Results for the WMT17 APE EN-DE task – average TER (↓), BLEU score (↑). The † indicates a difference
from the MT baseline that is not statistically signiﬁcant.
German-English
On German-English, the improvements of the top submission over the baseline are smaller (-0.26 TER, +0.28 BLEU) but still
statistically signiﬁcant. Such smaller gains, ob-
FBK Primary
FBK Contrastive
LIG Primary
LIG Contrastive-Forced
LIG Contrastive-Chained
 
Table 29: Results for the WMT17 APE DE-EN task – average TER (↓), BLEU score (↑). The † indicates differences
from the MT baseline that are not statistically signiﬁcant.
tained by systems based on the same approaches
adopted for the English-German task, conﬁrm our
initial expectations about the different level of dif-
ﬁculty of the two language directions. The interaction between low repetition rates and high translation quality, which certainly played a role in reducing the gap between the primary submissions
and the “do-nothing” MT baseline, is hence an
interesting aspect for more thorough explorations
in future rounds of the APE task.
Also in this
case, however, the lowest results achieved by the
phrase-based APE baseline (with both metrics)
conﬁrm that the switch to neural methods represents a technology advancement in the right direction.
System/performance analysis
Although all participants built their systems under
the same general neural paradigm, results’ distribution in a 4.5 TER (and 6.5 BLEU) points interval suggests differences in systems’ behaviour that
it is worth to explore further. To this aim, and as
a complement to global TER/BLEU scores, also
this year we performed a more ﬁne-grained analysis of the changes made by each system to the test
instances.
Macro indicators: modiﬁed, improved
and deteriorated sentences
Tables 30 and 31 show the number of modiﬁed,
improved and deteriorated sentences, respectively
for the English-German and the German-English
tasks. It’s worth noting that, as in the previous
rounds and for both language directions, the number of sentences modiﬁed by each system is higher
than the sum of the improved and the deteriorated
ones. This difference is represented by modiﬁed
sentences for which the corrections do not yield
TER variations. This grey area, for which quality improvement/degradation can not be automatically assessed, contributes to motivate the human
evaluation discussed in Section 5.5
English-German.
As expected, differently from
last year where the amount of test sentences modiﬁed by the participants had a much larger variance due to the different approaches applied, this
year the top English-German systems show a quite
homogeneous behaviour. In 2016, out of 11 submitted runs, the number of sentences modiﬁed by
the top 3 primary submissions (the best one being
neural and the others being phrase-based) ranged
between 421 and 1,613 (respectively 21.0% and
80.6% of the total).
This year, out of 15 submitted runs (all neural-based), the top 3 primary
submissions have a number of modiﬁed sentences
that falls in a much smaller range between 1,583
and 1,607 (between 79.1% and 80.0% of the total). The same holds for systems’ precision (i.e.
the proportion of improved sentences out of the
total amount of modiﬁed test items). The top 3 primary submissions, indeed, have a precision ranging in a two points interval from 63.6% to 65.6%,
while last year the proportion for the top 3 primary runs was more spread in a 11 points interval from 57.9% to 68.8%. Overall, lower ranked
systems show a tendency to either modify less sentences (all submissions with less than 1,000 modiﬁed sentences are in the bottom half of the ranking), or to do it with lower precision (all submissions with less than 60.0% precision are in the bottom half of the ranking), or a combination of the
two, as in the case of the phrase-based approach
 , which is the second less aggressive method and by far the less precise one.
In general, looking at system precision numbers,
it’s worth noting that the close results between the
top submissions still leave large room for improvement. Indeed, in the case of the best systems, more
than 30 points in precision represent a huge gap to
be ﬁlled before considering APE a solved problem.
German-English.
In this case, the higher dif-
ﬁculty of the task (due to lower repetition rate
and higher translation quality, as discussed in Section 5.1.1) changes the global picture provided by
our macro indicators.
Although the two participating systems were developed under the neural paradigm, their different behaviour is evident
from the amount of modiﬁed sentences: the two
primary submissions respectively modiﬁed 270
Deteriorated
FBK Primary
1,607 (80.3%)
1,035 (64.4%)
334 (20.7%)
AMU Primary
1,583 (79.1%)
1,040 (65.6%)
322 (20.3%)
AMU Contrastive
1,583 (79.1%)
1,044 (65.9%)
326 (20.5%)
DCU Primary
1,592 (79.6%)
1,014 (63.6%)
361 (22.6%)
DCU Contrastive
1,558 (77.9%)
1,012 (64.9%)
329 (21.1%)
FBK Contrastive
1,597 (79.8%)
996 (62.3%)
344 (21.5%)
FBK USAAR Contrastive
1,675 (83.7%)
920 (55.0%)
482 (28.7%)
USAAR Primary
744 (37.2%)
461 (61.9%)
160 (21.5%)
LIG Primary
1,168 (58.4%)
629 (53.8%)
306 (26.1%)
JXNU Primary
1,385 (69.2%)
678 (48.9%)
404 (29.1%)
LIG Contrastive-Forced
719 (35.9%)
412 (57.3%)
166 (23.1%)
LIG Contrastive-Chained
814 (40.7%)
422 (51.8%)
217 (26.6%)
CUNI Primary
1,513 (75.6%)
713 (47.1%)
515 (34.0%)
USAAR Contrastive
306 (15.3%)
179 (58.4%)
76 (24.8%)
 
571 (28.5%)
211 (36.9%))
244 (42.7%)
CUNI Contrastive
1577 (78.8%)
644 (40.8%)
663 (42.0%)
Table 30: Number of test sentences modiﬁed, improved and deteriorated by each run submitted to the EN-DE task.
Deteriorated
FBK Primary
270 (13.5%)
108 (40.0%)
78 (28.9%)
FBK Contrastive
364 (18.2)
135 (37.0%
118 (32.4%)
LIG Primary
27 (42.1%)
24 (37.5%)
LIG Contrastive-Forced
13 (27.6%)
21 (44.7%)
LIG Contrastive-Chained
27 (42.1%)
46 (71.9%)
 
139 (6.9%)
30 (21.6%)
69 (49.6%)
Table 31: Number of test sentences modiﬁed, improved and deteriorated by each run submitted to the DE-EN task.
System behaviour (primary submissions) for EN-DE – TER(MT, APE)
Figure 10: System behaviour (primary submissions) for DE-EN – TER(MT, APE)
(13.5%) and 64 (3.2%) test items. On one side,
the small number of modiﬁed sentences compared
to English-German indicates systems’ ability to
keep under control the number of unnecessary corrections. If we consider that almost half of the
test items are “perfect” translations that should be
kept unchanged (see Table26), a rather conservative approach is indeed a desired behaviour. On
the other side, however, precision scores are much
lower compared to those observed in the English-
German task. Even for an “easy” target language
like English, coping with data featuring low repe-
tition rates and high translation quality is hence a
still open challenge.
Micro indicators: edit operations
Also this year we performed a more ﬁne-grained
analysis of systems’ behaviour in order to discover
possible differences in the way they correct the test
set instances. To this aim, we looked at the distribution of the edit operations done by each system (insertions, deletions, substitutions and shifts)
by computing the TER between the original MT
output and the output of each system taken as reference (only for the primary submissions). The
outcomes of this analysis are shown in Figures 9
(English-German) and 10 (German-English).
English-German.
As expected, compared to
last year, the plot in Figure 9 does not show large
differences between similar neural-based submissions. All of them are characterized by a rather homogeneous distribution of the types of correction
patterns applied, with a slight dominance of substitutions for the top submissions (between 37.0%
and 40.0%) and a slight dominance of deletions
for the others (between 34.5% and 42.1%). Another quite visible correlation is the one between
shift operations and performance results, which
tend to decrease for systems that perform less reordering (also last year, the winning neural system
had a signiﬁcantly larger amount of shifts compared to the others). Interestingly, also in this case
the phrase-based baseline (the weakest APE system in terms of results) is a clear outlier. It performs the lowest number of shifts (2.2% vs 9.7%
of the top submission), the lowest number of insertions (7.1% vs 19.5%) and the largest number
of deletions (50.2% vs 32.1%). This indicates a
scarce capability of the phrase-based approach to
learn reordering rules and its tendency to replace
them with more radical deletion operations.
German-English.
As shown by Figure 10, the
two primary submissions for this task have a quite
different behaviour. In addition to the large differences in the number of modiﬁed, improved and
deteriorated sentences (see Table 31), the distribution of the edit operations performed on test
data indicates opposite strategies.
Also in this
case, the distribution is more homogeneous for the
best performing system, with a dominance of substitutions and around 4.0% of shifts (though less
than in the English-German task, where they were
around 10.0%). The second system has a much
more unbalanced distribution, with lots of insertions and no shifts in the few sentence corrections
it returned. The distribution for the phrase-based
APE baseline is more similar to the best system
but, as shown in Table 31, its corrections are by
far the less reliable ones. Apart from these considerations, it is hard to draw clear conclusions
since the different correction strategies of the three
methods result in close ﬁnal scores. Indeed, as
shown in Table 29, only 0.24 TER and 0.33 BLEU
points separate the two primary systems, while
0.45 TER and 0.54 BLEU points separate the best
system from the phrase-based baseline. The small
improvements of the primary submissions over
the “do-nothing” MT baseline suggest that, independently from the different correction strategies
applied, both primary submissions deﬁnitely suffered from the large amount of “perfect” translation in the test set (around 45.0%). However, while
automatic evaluation metrics like TER and BLEU
always penalize unnecessary corrections of good
translations, there is a chance that some of these
corrections are acceptable paraphrases rather than
sentence deteriorations. One of the objectives of
the human evaluation discussed in the next section
is to check if this phenomenon has a visible impact
on performance.
Human evaluation
To assess the quality of the output of the APE
systems and produce a ranking based on human
judgment, as well as analyze how humans perceive
TER/BLEU performance differences between the
submitted systems, a human evaluation of the
quality of automatic post-edits was carried out using Direct Assessment (DA) produced by multiple APE systems.
(0–100), where human assessors are asked to rate
how adequately the APE system output expresses
the meaning of the human reference translation.
DA scores for systems and segments have been
shown to be highly repeatable in self-replication
experiments .
overcomes the previous challenges associated with
lack of reliability of human assessment of MT.
Since we also have a human post-edit available
for each MT output in the test set, to make DA
outcomes more informative we also included the
human post-edits as a hidden system in the evaluation, which will provide some insight into an
achievable DA score for a potential system that
achieved human-quality post-editing.
Additionally, we included the original MT output without
any post-editing as a hidden system to discover the
baseline DA score for each language pair.
When running the APE manual evaluation, it
was possible in many cases to take advantage of
the fact that multiple systems can produce identical outputs, as was begun in evaluation of the
News task in WMT15 . Table
32 shows numbers of translations in total for all
APE systems, as well as savings in terms of annotation effort that was gained by combining identical system outputs prior to running the evaluation,
where, as expected, a substantial saving was made
due to the fact that the systems quite often produced the same output. In terms of human effort
involved in carrying out the manual evaluation, Table 33 shows numbers of judgments collected in
total for each language pair and number of assessments contributing to the ﬁnal DA score for APE
systems on average.
When carrying out a manual evaluation of any
kind, it is important to consider the consistency
of annotators with the aim of estimating, where
the evaluation to be repeated, how likely it would
be that the same conclusions would be drawn.
Assess/Sys
Table 33: Amount of data (assessments after “de-collapsing”
multi-system outputs) collected in the WMT17 APE manual
evaluation campaign and numbers of assessments per system.
When an analogue scale is employed for human
assessment, consistency of human assessors cannot be evaluated in the usual way, such as the
Kappa coefﬁcient, commonly employed for evaluating the consistency of human assessors when
discrete quality judgments or relative preference
judgments are collected.
Instead, for analogue
scale data, we examine the consistency of individual human assessors according to their ability to
discriminate between the quality of pairs of known
worse quality translations, known as bad reference
pairs, where original translations produced by the
APE systems are degraded automatically. In addition, repeat assessments of the same translation
are given to human assessors to see how reliably
they assign similar scores to similar quality translations. Hiding bad reference and repeat translation pairs within hits allows a signiﬁcance test to
be carried out for each human judge investigating
if their score distributions show a signiﬁcant difference where there should be one, and another
test to check that no signiﬁcant difference shows
up for repeated assessment of the same translation.
As such, proportions of human assessors and
whether they discriminate between the quality of
bad reference pairs and repeat translations are
shown in Table 34.
Notably, all of the student
translators (for EN-DE) passed the DA’s quality control mechanism by assigning signiﬁcantly
lower scores to degraded translations, while 54%,
a usual number of crowd-sourced workers (for
DE-EN), passed quality control.
Proportions of workers showing a non significant difference in repeat items at ﬁrst appears
lower than usual for DA, at 91% for EN-DE and
93% for DE-EN, as this proportion has been between 97 and 100% for DA in past evaluations.
However, on closer inspection, the total number of assessors showing a signiﬁcant difference
for repeat items is as low as three assessors and
proportions are therefore exaggerated due to the
low number of workers involved in the evaluation
(A) & No Sig.
Exact Rep.
Table 34: Number of unique Mechanical Turk workers, (A)
those whose scores for bad reference pairs were signiﬁcantly
different and numbers of unique human assessors in (A)
whose scores for exact repeat assessments also showed no
signiﬁcant difference.
Prior to computing ﬁnal DA scores for systems,
in order to iron out differences in scoring strategies of distinct human assessors, human assessment scores for translations were ﬁrst standardized according to each individual human assessor’s overall mean and standard deviation score.
Average standardized scores for individual segments belonging to a given system are then computed, before the ﬁnal overall DA score for that
system is computed as the average of its segment
Human evaluation results
Table 35 includes DA results for English-German
and Table 36 shows results for German-English
APE systems. Clusters are identiﬁed by grouping
systems together according to which systems signiﬁcantly outperform all others in lower ranking
clusters, according to Wilcoxon rank-sum test.
HUMAN POST EDIT
NO POST EDIT
Table 35: EN-DE DA Human evaluation results showing
average raw DA scores (Ave %) and average standardized
scores (Ave z), lines between systems indicate clusters according to Wilcoxon rank-sum test at p-level p ≤0.05.
Figures 11 and 12 show head to head signiﬁcance test results for English-German and
German-English systems participating in the APE
task, as well as the two additional “systems” where
either no post-editing or human post-editing was
HUMAN POST EDIT
NO POST EDIT
Table 36: DE-EN DA Human evaluation results showing
average raw DA scores (Ave %) and average standardized
scores (Ave z), lines between systems indicate clusters according to Wilcoxon rank-sum test at p-level p ≤0.05.
HUMAN.POSTEDIT
NO.POSTEDIT
NO−POSTEDIT
HUMAN−POSTEDIT
Figure 11: EN-DE Wilcoxon rank-sum signiﬁcance test results for pairs of systems competing in the APE task, where a
green cell denotes a signiﬁcant win for the system in a given
row over the system in a given column, at p ≤0.05.
carried out, where a darker shade of green signi-
ﬁes a lower p-value and a conclusion made with
more certainty.
English-German.
For this language direction,
the ranking produced by DA is slightly different
from those based on TER/BLEU. This is not surprising if we consider the close performance results measured with automatic metrics. With primary submissions compressed in a relatively small
TER/BLEU interval, different system orders are
in fact likely to emerge also from manual evalua-
HUMAN.POSTEDIT
NO.POSTEDIT
NO−POSTEDIT
HUMAN−POSTEDIT
Figure 12: DE-EN Wilcoxon rank-sum signiﬁcance test results for pairs of systems competing in the APE task, where a
green cell denotes a signiﬁcant win for the system in a given
row over the system in a given column, at p ≤0.05.
tion. Overall, as shown in Table 35, three systems
emerge as signiﬁcantly better than the others. This
ranking is comparable to the one obtained with
automatic metrics, although the top two systems
(FBK and AMU) are switched, but this is in-line
with the human evaluation that showed no signiﬁcant difference between the two. This is also
in-line with TER/BLEU rankings, for which the
three systems are the only primary systems with
TER<20.00 and BLEU>69.00.
In agreement
with the BLEU-based ranking, the JXNU submission ranks in fourth position in its own cluster.
This represents the main difference with the TERbased ranking (in which it occupies the 6th place),
which suggests a higher agreement between DA
and BLEU. The remaining three systems, which
feature rather close TER/BLEU scores, are positioned in the same lower cluster, though in a different order, again with small raw DA score differences.
Apart from these general considerations, which
are difﬁcult to project into conclusive indications
about the reliability of our two automatic metrics, two major outcomes are evident. First, the
technology advancement with respect to the 2016
round is also conﬁrmed by DA scores, which indicate that all the systems are signiﬁcantly better
than the “do-nothing” baseline (NO POST EDIT).
Last year, in contrast, all participants but one were
in the same cluster of the baseline. The downside
is that, despite the signiﬁcant progress made, APE
systems are still far from human quality. Average
DA scores indicate that the distance between the
top primary submissions and human post-edits is
in fact similar to the distance that separates them
from the primary submissions in the bottom cluster.
German-English.
Also DA scores conﬁrm the
higher difﬁculty of the German-English task. As
expected, also in this case human quality is much
higher, with a gap that is even larger compared
to the distance observed in Figure 35. Moreover,
while in terms of automatic metrics the improvement over the baseline for the top ranked system
was statistically signiﬁcant, the DA-based ranking
places the two primary systems in the same cluster
of the baseline.
Lessons learned and outlook
The third round of the APE task has marked a
further step forward from the previous ones both
in terms of participants 
and, most importantly, in terms of the deployed
technology. Concerning the latter aspect, the wide
adoption of neural approaches has led, for the ﬁrst
time, to signiﬁcant improvements over the baselines for all participants. On English-German data
we observed the largest gains, which are up to -4.9
TER and +7.6 BLEU points for the top submission. On German-English, a more difﬁcult task
due to lower repetition rate and higher translation
quality of the test data, the improvements of the
top submission over the baseline are smaller (-0.26
TER, +0.28 BLEU) but still statistically signiﬁcant. With respect to previous years, similar design and training choices (e.g. the use of multisource solutions and additional synthetic training
data), produced a more compact ranking of the
participating systems but, at the same time, resulted in submissions that still feature different behaviour that deserve closer inspection in future.
Despite the technology improvement, some major challenges are still open. The main one is how
to better handle the difﬁcult case in which an automatic translation is already (or near-) perfect and
APE systems should abstain from performing useless (or risky) corrections. Another limitation of
current solutions is their inefﬁcacy in generalizing the learned correction patterns, so that training
data featuring low repetitiveness can be better exploited to learn useful correction patterns.
From the performance evaluation standpoint,
the selection of the best metric is still debatable.
TER (the ofﬁcial one in all the APE rounds so
far) and BLEU produce slightly different rankings,
which both differ from those produced by human
evaluation with direct assessment. The comparison with DA indicates a small preference for the
BLEU-based ranking, but drawing deﬁnite conclusions about the suitability of the two metrics
is difﬁcult due to the small performance differences observed. Most likely, future rounds of the
task will hence keep the the evaluation setting unaltered, possibly focusing on the aforementioned
challenges to increase the level of difﬁculty and
further raise the interest on the APE problem.
Acknowledgments
This work was supported in parts by the QT21,
funded by the European Commission (7th Framework Programme and H2020). Further datasets
were donated by University of Helsinki and Yandex. The APE task organizers would also like to
thank Text&Form for producing the manual postedits and the annotators involved in the manual
evaluation.