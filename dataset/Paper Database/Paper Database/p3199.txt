Contrasts and Classical Inference
Jean-Baptiste Poline, Ferath Kherif and Will Penny
Introduction
Some general remarks
Constructing models
What should be included in the model ? . . . . . . . . . . . . . .
Modelling the baseline . . . . . . . . . . . . . . . . . . . . . . . .
Extending our ﬁrst model . . . . . . . . . . . . . . . . . . . . . .
Constructing and testing contrasts
Parameter estimation
. . . . . . . . . . . . . . . . . . . . . . . .
Estimability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Constructing and testing t-contrasts . . . . . . . . . . . . . . . .
Computing t-statistics . . . . . . . . . . . . . . . . . . . . . . . .
Constructing and testing F-contrasts
Interpretations of F-contrasts . . . . . . . . . . . . . . . . . . . .
Correlation between regressors and other issues
Moving the variance across correlated regressors
. . . . . . . . .
Contrasts and reparametrised models
. . . . . . . . . . . . . . .
The estimation-detection dilemma
. . . . . . . . . . . . . . . . .
FIR and random eﬀects analyses . . . . . . . . . . . . . . . . . .
A Notation
B Subspaces
C Orthogonal projection
Introduction
The General Linear Model (GLM) characterises postulated relationships between our experimental manipulations and the observed data. These relations
may consist of multiple eﬀects all of which are contained within a speciﬁed design matrix. To test for a speciﬁc eﬀect we use a ‘contrast’ which allows us
to focus in on a particular characteristic of the data. The application of many
diﬀerent contrast vectors to the same design matrix allows us to test for multiple eﬀects without having to re-ﬁt the model. This is important in functional
imaging because model ﬁtting is computationally demanding.
There are often several ways to mathematically model an experimental paradigm.
For example, in a functional imaging experiment the baseline condition can be
modelled explicitly or not at all. This sort of issue generalises to more complex
Contrast speciﬁcation and the interpretation of results are entirely
dependent on the model speciﬁcation which in turn depends on the design of
the experiment. The most important step is clearly the speciﬁcation of the experimental paradigm since if a design is clearly thought through, the questions
asked of the data are generally easily speciﬁed.
In general, it is not very useful to know simply that a speciﬁc brain area
was more active during one condition than another. We wish to know whether
this diﬀerence is statistically signiﬁcant. The deﬁnition of contrasts is therefore
intimately related to statistical tests. We will therefore review the aspects of
hypothesis testing that relate directly to the speciﬁcation of contrasts.
This chapter is organised as follow. Section 2 reviews the basics beginning
with some general comments about speciﬁcation of contrasts and design matrices. In section 3, we review the raw material for the construction of contrasts
namely the parameters of the linear model.
In section 4, we describe some
rules for constructing contrasts based on the t-test.
In section 5 we discuss
F-contrasts and in section 6 discuss the important issue of correlation between
regressors.
Some general remarks
Thinking about which contrasts will be used should start before acquiring the
data. Indeed, most of the problems concerning contrast speciﬁcation come from
poor design speciﬁcation. Poor designs may be unclear about what the objective
is or may try to answer too many questions in a single model. This often leads
to a compromise in which it becomes diﬃcult to provide clear answers to the
questions of interest. This may seem obvious but still seems to be one of the
main source of problems in functional imaging data analysis.
This previous remark does not completely preclude the use of a complex
paradigm, in the sense that many conditions can and often should be included
in the design. The process of recruiting subjects and acquiring the data is long
and costly and therefore it is only natural that one would like to answer as
many questions as possible with the same data. However, this requires careful
thinking about which contrasts will then be speciﬁed and whether they actually
answer the question of interest.
Complex designs may also lead to testing many hypotheses at each voxel.
The fact that this will increase the risk of false positives is an issue that is more
often than not overlooked. Indeed, it is somewhat surprising that the problem
of multiple comparisons across voxels has received a large amount of attention
during the last ten years, while the problem of multiple comparisons across
contrasts has not yet been addressed in the brain imaging literature. If there is
no correction for the number of contrasts tested, results should be considered
as exploratory (with an uncontrolled risk of error). Alternatively, Bonferroni
correction can be applied to set a conservative bound on the risk of error.
One of the diﬃculties in brain imaging is that the form of the signal of
interest is not precisely known because the haemodynamic response varies across
subjects and brain regions. We therefore face a double task : estimation (what
has happened ?) and detection (has anything happened ?).
Constructing models
What should be included in the model ?
Again, it is generally a good idea to think about how the experiment is going
to be modelled and which comparison we wish to make before acquiring the
data. We ﬁrst review some of the questions often asked at the modelling step
that have an impact on how the comparisons between conditions are going to
be performed.
For a given experiment, diﬀerent model parameterisations are usually possible and some of them may allow for an easier speciﬁcation of contrasts. The speciﬁed model represents the a priori ideas about how the experimental paradigm
inﬂuences the measured signal. The less prior information there is about the
form of the induced signal, the larger the number of regressors required by the
model, such that the combination of regressors can account for the possible
signal variation.
To make this point clear, we take the example of an fMRI experiment investigating signal variation in the motor cortex when a subject is asked to press a
device with four diﬀerent forces (the “press” condition) interleaved with some
“rest” periods.
The ﬁrst question to be answered is whether the rest period should be modelled or not. In general, there is no diﬀerence whether the rest period is explicitly modelled or not, the only diﬀerence may arise because of edge eﬀects
during the convolution with the expected haemodynamic response function (for
fMRI data). However, it has to be understood that the information contained
in the data corresponds eﬀectively to the diﬀerence between the conditions and
the rest period. Therefore it is generally advisable not to model implicit conditions. One may think instead in terms of modelling the diﬀerence between
two conditions. However, when the number of conditions is greater than 2, it
is often easier to model each condition separately and accept that there will
be redundancy in the model and that only some comparisons will be valid (see
section 4.2 on estimability below).
The second question that may arise is how to model the “press” condition.
Here the prior information on neural activity and its metabolic consequence is
essential. One may have very speciﬁc prior assumptions, for example, that the
response should be linear with the force developed by the subject. In this case,
one regressor representing this linear increase should be included in the model
with, for instance, a value of one for the smallest force and a value of four for
the greatest. In this design, we might then ask what value should this covariate
take during the “rest” periods ? If zeros are assumed during this period, there
is an explicit hypothesis in the model that the diﬀerence between the rest and
the ﬁrst force level is the same as the diﬀerence between the ﬁrst and the second
force level (or second and third, etc.). To relax this hypothesis, the diﬀerence
between the “press” conditions and the rest period must be modelled explicitly.
Modelling the baseline
Generally the constant oﬀset of the signal has to be modelled (as a column of
ones) since the measured signal has an oﬀset (it is not on average zero even
without stimuli or task).
Figure 1 shows this simple design, referred to as
‘Model-1’ which has three regressors1 This is a ‘linear parametric’ model. Some
questions that can then be put to the design would be
1. Is there a positive linear increase ? (testing if the parameter associated
with the ﬁrst regressor is signiﬁcantly greater than zero);
2. Is there a diﬀerence between the rest and the ﬁrst force level that is not
the same as the one between other force levels ? In other words, is there
an additive oﬀset for the “press” condition not accounted for by the linear
increase ? This would be tested using the second coeﬃcient.
3. Is the average value of the signal during the rest period diﬀerent from zero
(positive or negative) ?
Note that in this ﬁrst example the model could be re-parameterised, for
instance by removing the mean of the ﬁrst and second regressors to model
only the diﬀerence around the mean (in fact, this is done automatically in
SPM). In that case, the ﬁrst and second questions would still be valid and the
corresponding parameters be unchanged, but the interpretation of the third
parameter would diﬀer. It would take the value of the average of the measured
data, therefore including a potential increase due to the ﬁrst condition. On the
other hand, if there is no information on whether the “press” condition might
involve some negative response or not (this corresponds to the case where the
1Whilst it is usual to convolve regressors with an assumed haemodynamic response function, as described in Chapter 7, we have skipped this step so as to focus more keenly on design
only a priori information is the diﬀerence between the levels of force) then it is
more reasonable to remove the mean.
The above points are clariﬁed by the following example. Suppose that the
data y come from Model-1. We use parameters values [10 5 100] for the three
regressors respectively. Analysis with a model in which the ﬁrst two covariates
have been mean centred leads to an estimation of [10 5 115] in the absence of
noise. Clearly, the parameter estimates corresponding to the ﬁrst two conditions
of interest (now centred) are unchanged, but the parameter modelling the mean
is changed (from 100 to 115). The reason is that it now includes the average of
regressors one and two weighted by their respective parameter estimates.
Extending our ﬁrst model
The hypothesis that the response increases linearly is a rather strong one. There
are basically two ways to relax this assumption.
First, the linear increase modelled in the ﬁrst covariate can be developed in
a Taylor-like expansion, such that not only linear increases but also quadratic
or cubic increases can be modelled. This solution leads to the inclusion of a
new regressor that is simply constructed by the square of values of the linear
increase regressor. This new model, Model-2, is shown in Figure 2. This is a
‘quadratic-parametric model’, a type of parametric modulation that is described
further in Chapters 7 and 10.
The second solution is to have a non-parametric form, leaving the model
completely free to capture any diﬀerences between the four force levels. This is
achieved by representing each force level as a separate covariate. This example,
Model-3, is shown in ﬁgure 3. This is a ’non-parametric model’. Clearly, modelling the diﬀerence between each level and the rest period renders the modelling
of the average “press” versus rest condition redundant, since the average can be
formed from the sum of the diﬀerent force levels.
Note that what we want to model can be seen as two independent components; the diﬀerences between levels 1 and 2, levels 2 and 3, and levels 3 and
4 for one part, and the average activation over all levels of force (the main effect of force) for the other part. Note that the diﬀerence between levels 1 and
4 can be created with (1-2)+(2-3)+(3-4). Modelling diﬀerences between levels
is similar to modelling interactions in factorial designs (see chapter on experimental design). We therefore have the choice here to model the diﬀerence of
each level with the rest condition, or the main eﬀect and the interaction. The
questions that can be put to these two designs are exactly the same, they are
just “rephrased”. The two versions of this model, models 3 and 4, are shown in
ﬁgures 3 (diﬀerence of each level with rest) and 4 (main eﬀect and interactions).
The choice between parametric and non-parametric models often depends
on the number of parameters that need to be modelled. If this number is large
then parametric models might be preferred. A limited number of parameters
(compared to the number of data points) with little prior information would
generally lead to nonparametric models.
In each case, we may be unsure about how to test the eﬀect of the force
levels and which eﬀects to test. Before answering these question more formally
in the next sections, we brieﬂy describe the issues involved. For the parametric
models, we might be interested in the following questions
• Is there a linear increase or decrease in activation with force level (modelled
by the ﬁrst covariate) ?
• Is there a quadratic increase or decrease in addition to the linear variation
(modelled by the second covariate) ?
• Is there anything that is either linear or quadratic in the response to the
diﬀerent levels of force (the joint use of the ﬁrst and second covariate) ?
Should we in this instance centre the quadratic covariate or not ?
ﬁrst answer to this is that it generally makes no diﬀerence. In general, only
variations around the averaged signal over time are easily interpretable. Not
centering this covariate would only make a diﬀerence if one were interested
in the “mean” parameter.
Likewise, the quadratic increase shown in Figure
2 can be decomposed into a linear increase and a “pure” quadratic increase
(one decorrelated from a linear trend). Removing the component that can be
explained by the linear increase from the quadratic increase makes the “linear
increase” regressor parameter easier to interpret. But one has to be careful not
to over-interpret a signiﬁcant linear increase since even a signal equal to the
second regressor in ﬁgure 2 may have a signiﬁcant linear component 2.
For the nonparametric models, interesting questions might be
• Is there an overall diﬀerence between force levels and the rest condition
(average diﬀerence between force levels and rest) ? This would involve the
average of the ﬁrst four regressors in model 3 and the ﬁrst regressor in
• Are there diﬀerences between force conditions ? This is resolved by looking
conjointly at all diﬀerences in force levels versus rest in model 3 and at
regressors 2 to 4 in model 4.
• Would it be possible to test for a linear increase of the signal as a function
of the force level ? Because any diﬀerence between condition levels has
been modelled, it would not be easy to test for a speciﬁc linear increase.
However, one can inspect the shape of the increase post-hoc by displaying
the parameter estimates.
The re-parameterisation question is often framed in the following form.
Should conditions A and B be modelled separately, or should the common part
of A and B (A+B) be modelled as well as the diﬀerence (A-B) ? Note that if
2Since there is a common part to these two regressors, it is really a matter of interpretation
whether this common part should be attributed to one or the other component. See section
6 and for further discussion of this matter.
there is no third condition (or implicit condition as a null event or baseline)
only (A-B) can be estimated from the data.
Another example often considered is the case where the data acquired during
a period of time correspond to several factors in an experiment. For instance,
consider an experiment comprising two factors, for instance, a modality factor
where stimuli are presented either auditorily or visually, and a word factor where
stimuli are either names, verbs or non-words. Rather than trying to model the
factors (and possibly their interaction), it is often easier to model each level of
the factor (here 2 by 3, yielding six conditions). If there is no further implicit or
explicit rest (or baseline) then the questions of interest can be framed in terms
of diﬀerences between these conditions. We return to this example in section 5.
Constructing and testing contrasts
Parameter estimation
We now turn to the issue of parameter estimation. As thoroughly reviewed in
Chapter 7, the model considered is
Y = Xβ + ϵ
simply expressing that the data Y (here Y is any time series of length n at a
given location in the brain, see section A for notation), can be approximated
with a linear combination of time series in X.
The matrix X of dimension
(n, p), therefore, contains all eﬀects that may have an inﬂuence on the acquired
signal. The quantity ϵ is additive noise and has a normal distribution with zero
mean and covariance σ2Σi. The parameters β can then be estimated using least
The most important thing to realise about model (1) is that it states that
the expectation of the data Y is equal to Xβ. If this is not the case then the
model is not appropriate and statistical results are likely to be invalid. This
will occur if X does not contain all eﬀects inﬂuencing the data, or contains too
many regressors not related to the data.
A second important remark is that least squares estimation is a “good”
estimation only under the assumption of normally distributed noise. This means
that if there are outliers, the estimate ˆβ may be biased. The noise in functional
imaging seems however close to normal and many aspects of the data processing
stream, eg.
spatial smoothing, have “normalising” properties 3.
The (true)
parameters β are estimated from the data using
ˆβ = (XT X)−XT Y
where X−denotes the (Moore-Penrose) pseudo inverse of X.
data ˆY are deﬁned as
3If the original noise properties are well known the most eﬃcient way to analyse the data
is the maximum likelihood procedure that would whiten the noise.
and represent what is predicted by the model. The estimated noise is
Y −ˆY = RY = r
R = In −XX−
The noise variance is estimated with
bσ2 = Y T RY/tr
Looking at formula (2) we realise that
• Parameters are dependent on the scaling chosen for the regressors in X.
This scaling will not be important when the parameter estimate is compared to its standard deviation, but is important if regressors are entered
“by hand” and then compared to each other in the same design. When
entered through the dedicated interface in SPM , the regressors are appropriately scaled to yield sensible comparisons.
• Not all parameters may be estimable. This is the subject of the following
subsection.
Estimability
One can appreciate that not all parameters may be estimable by taking the
rather unlikely model that contains the same regressor twice, say x1 and x2 = x1
having corresponding parameters β1 and β2. Clearly, there is no information in
the data on which to base the choice of ˆβ1 compared to ˆβ2. In this speciﬁc case,
any solution of the form ˆβ1 + ˆβ2 = constant will provide the same ﬁtted data,
the same residuals, but an inﬁnity of possible ˆβ1 and ˆβ2.
To generalise this argument we consider linear functions of the parameter
λ1 ˆβ1 + λ2 ˆβ2 + ... = λT ˆβ
The constants λi are the coeﬃcients of a function that ‘contrasts’ the parameter
estimates. The vector λT = [λ1λ2...λp], where p is the number of parameters
in X, is referred to as the contrast vector. The word contrast is used for the
result of the operation λT ˆβ. A contrast is therefore a random variable, since ˆβ
is estimated from noisy data.
This situation generalises each time a regressor can be constructed with a
linear combination of the others. The matrix X is said to be rank deﬁcient or
degenerate if (some of) the parameter estimates are not unique and therefore
do not convey any meaning by themselves. At ﬁrst sight, this situation seems
unlikely. However, especially for PET data, most design models are degenerate.
This is because of the joint modelling of a constant term for the mean and of
all the diﬀerences between any condition and the remaining scans.
A contrast is estimable if (and only if) the contrast vector can be written as
a linear combination of the rows of X. This is because we get the information
about a contrast through combinations of the rows of Y . If no combination of
of rows of X is equal to λT , then the contrast is not estimable4.
In more technical terms, the contrast λ has to lie within the space of XT ,
denoted by λ ⊂C(XT ), or, equivalently, that λ is unchanged when projected
orthogonally onto the rows of X (ie, that PXT λ = λ with PXT being the ‘projector’ onto XT (see appendix C).
The SPM interface ensures that any speciﬁed contrast is estimable, hence
oﬀering a protection against contrasts that would not make sense in degenerate
designs. A further possible diﬃculty is that a contrast may be estimable but
may be misinterpreted.
One of the goals of this chapter is to improve the
interpretation of contrasts.
Constructing and testing t-contrasts
If it is clear what the parameter estimates represent, then speciﬁcation of contrasts is simple, especially in the case of t-contrasts. These contrasts are of the
form described above ie. univariate linear combinations of parameter estimates.
For instance, for model 1 we can ask if there is a linear increase by testing
β1 using the combination 1β1 + 0β2 + 0β3, that is, with the contrast vector
λT = [1 0 0]. A linear decrease can be tested with λT = [−1 0 0].
To test for the additive oﬀset of the “press” condition, not accounted for by
the linear increase, we use λT = [0 1 0]. Note here that the linear increase is
starting with a value of one for the ﬁrst force level, up to 4 for the fourth level
(see ﬁgure 1).
When testing for the second regressor, we are eﬀectively removing that part
of the signal that can be accounted for by the ﬁrst regressor. This means that
the second regressor is not giving the average value of the diﬀerence between
the “press” conditions and the rest condition. To obtain this, we would have
to construct a re-parameterisation of model 1 and replace the ﬁrst regressor
so that it models only diﬀerence of “force levels” around an average diﬀerence
between “press” and rest. This is achieved by orthogonalising the ﬁrst regressor
with respect to the second. This new model, model-5, is shown in ﬁgure 5. The
parameter estimates of this new model are [10 30 100] as compared to [10 5
100] for model 1. This issue is detailed in and the same eﬀect can be seen
in F-tests (see section 5). In other words, one should have clearly in mind not
only what is but also what is not tested by the constructed statistics.
Another solution (useful in neuroimaging where estimating the parameters
can be time consuming) is to work out the equivalent contrast (see section 6.2).
The contrast vector λT = [1 1 0] is valid but diﬃcult to interpret.
example, the individual eﬀects may be strong but because they can have diﬀerent
signs the overall eﬀect may be weak.
4Strictly, as we have described in Chapter 7, all contrasts are estimable by deﬁnition. If
a linear combination of parameter estimates is not estimable then that linear combination is
not a contrast. In this chapter, however, we often use the expression ’estimable contrast’ for
purposes of emphasis and because this term is used in the neuroimaging community.
For model 3 the average amplitude of the “press” condition compared to rest
would be tested with λT = [1 1 1 1 0]. For model 4 the same eﬀect can be tested
with λT = [1 0 0 0 0]. The two contrasts give exactly the same t-maps. Note
that in both cases, it is the average over levels that is tested, and this could be
signiﬁcant just because of the impact of one level.
An interesting question is whether we can easily test the linearity of the
response to the four levels in those models. For model 3 the intuitive contrast
to enter would be λT = [1 2 3 4 0]. This would indeed test for a linear increase
of the force level, but in a very unspeciﬁc manner, in the sense that the test
might be signiﬁcant in a situation where only the fourth condition has a greater
signal than in rest condition. This is because we are testing for the weighted
sum of the related parameters. The test is therefore valid, but certainly does
not ensure that the signal has a linear change with force levels. In other words,
the model is ﬂexible and we are testing a very restricted hypothesis, such that
the shape of the predicted signal may be far away from the shape of the tested
component.
Computing t-statistics
Whatever contrast is used, the contrast t-statistics are produced using 
tdf = λT ˆβ/SD(λT ˆβ)
where SD(z) denotes the standard deviation of z and is computed from the
= ˆσ2λT (XT X)−XT ΣiX(XT X)−λ
For Gaussian errors tdf follows approximately a Student distribution with degrees of freedom given by df = tr
. At the voxel level, the
value tdf is tested against the likeliness of this value under the null hypothesis.
The important point here is that the standard deviation of the contrast
of parameter estimates depends on the matrix X. More speciﬁcally, when regressors are correlated, the variance of the corresponding parameter estimates
increase. In other words, the stability of the estimation for one component is
greater when other components included in the model are decorrelated. The
dependence of the covariance of the estimated eﬀects and the correlation within
the model can be used, for instance, to optimise event related designs.
The test of tdf is one-tailed when testing for positivity only or negativity only
and two-tailed when jointly testing for positive or negative eﬀects (see section
Constructing and testing F-contrasts
In this section, we will consider an experiment with two event-related conditions
using the simple case of right and left motor responses. In this experiment, the
subject is asked to press a button with the right or left hand depending on a visual instruction (involving some attention). The events arrive pseudo-randomly
but with a long inter-stimulus interval. We are interested in ﬁnding the brain
regions that respond more to the right than to the left motor movement.
Our ﬁrst model supposes that the shape of the haemodynamic response
function (HRF) can be modelled by a ‘canonical HRF’ (see Chapter 10 for
details). This model is shown in ﬁgure 6. To ﬁnd the brain regions responding
more to the left than to the right motor responses we can use λT = [1
1 0]. Application of this contrast produces the SPM-t map shown in ﬁgure 7.
This clearly shows activation of contralateral motor cortex plus other expected
regions such as ipsilateral cerebellum.
Because there is an implicit baseline, the parameters are also interpretable
on their own and when tested (SPM t-maps not shown), they show responses
not only for the motor regions but also the expected visual regions 5. Instead of
having the two regressors being one for the left response and one for the right
response, an equivalent model would have two regressors, the ﬁrst modelling
the response common to right and left and the second modelling the diﬀerence
between these responses.
The fact that the HRF varies across brain regions and subjects can be accommodated as follows. A simple extension of the model of ﬁgure 6 is presented
in ﬁgure 9, for which each response is modelled with three basis functions.
These functions are able to model small variations in the delay and dispersion
of the HRF, as described in Chapter 10. They are mean centred, so the mean
parameter will represent the overall average of the data.
For this new model, how do we test for the eﬀects of, for instance, the right
motor response ? The most reasonable approach in the ﬁrst instance is to test
for all regressors modelling this response.
This does not mean the sum (or
average) of the parameter estimates since the sign of those parameter estimates
is not interpretable, but rather the (weighted) sum of squares of those parameter
estimates. The appropriate F-contrast is shown in ﬁgure 10.
One interpretation of the F-contrast is of the speciﬁcation of a series of one
dimensional contrasts, each of them testing against the null hypothesis that the
parameter is zero. Because parameters are tested against zero one would have
to reconstruct the ﬁtted data and check the positive or negative aspects of the
To test for the overall diﬀerence between right and the left responses we use
the contrast shown in ﬁgure 11. Note that multiplying the F-contrast coeﬃcients by -1 does not change the value of the test. To see if the tested diﬀerence
is “positive” or “negative” (if this makes sense since the modelled diﬀerence
could be partly positive and partly negative) one has to look at the ﬁtted signal
corresponding to the extra sum of squares tested. The F-test image corresponding to this contrast is shown in ﬁgure 12. This image is very similar to the
corresponding image for the simpler model (ﬁgure 8). Finally, ﬁgure 13 shows
5Interestingly, there is some ipsilateral activation in the motor cortex such that the “leftright” contrast is slightly less signiﬁcant in the motor regions than the “left” [1 0 0] contrast.
that the more complex model provides a better ﬁt to the data.
To conclude this section, we give a few more examples using the design
described at the end of section 3. We suppose a 2 by 3 factorial design consisting
of words presented either visually (V) or aurally (A) and belonging to 3 diﬀerent
categories (C1, C2, C3). The way the design is constructed is to model all the 6
event types in the following order in the design matrix; V-C1 (presented visually
and in category one), V-C2, V-C3, A-C1, A-C2, A-C3. We can then test for
the interaction between the modality and category factors. We suppose that
the experiment is a rapid event related design with no implicit baseline, such
that only comparisons between diﬀerent kinds of event are meaningful (and not
events in themselves). In a ﬁrst example the events are modelled using only one
basis function. A test for the main eﬀect of modality would be the one presented
in ﬁgure 14(a). Figure 14(b) shows the test for the main eﬀect of categories.
Note that because there is no implicit baseline here, the main eﬀects of factors
are diﬀerences between the levels. Finally, the interaction term would be tested
for as in ﬁgure 14(c).
The number of rows in an interaction contrast (without implicit baseline) is
where N is the number of factors and li the number of levels of factor i.
Interpretations of F-contrasts
There are two equivalent ways of thinking about F-contrasts. For example, we
can think about the F-contrast in ﬁgure 10 as ﬁtting a reduced model that does
not contain the “right motor response” regressors. This reduced model would
have a design matrix X0 with zero entries where the “right motor response”
regressors were in the ‘full’ design matrix X. The test then looks at the variance
of the residuals (see section 4.1) as compared to that of the full model X. The
F-test simply computes the extra sum of squares that can be accounted for by
the inclusion in the model of the three “right hand” regressors. Following any
statistical textbook (e.g. ) and the work of , this is expressed by testing
the following quantity :
Fdf1,df2 = (Y T (I −PX0)Y −Y T (I −PX)Y )/ν1
Y T (I −PX)Y/ν2
tr ((R0 −R)Σi)
tr ((R0 −R)Σi(R0 −R)Σi)/tr((R0 −R)Σi)2
tr(RΣiRΣi)/tr (RΣi)2
where R0 is the projector onto the residual space of X0 and PX is the orthogonal
projector onto X (see appendix for a deﬁnition of a ‘projector’.) We also have
Fdf1,df2 ∼F(df1, df2)
Such a test can be implemented by specifying the columns of the design matrix
that should be kept for the reduced model.
The second interpretation of the F-test is of the speciﬁcation of a series of
one dimensional contrasts, each of them testing against the null hypothesis that
the parameter is zero. Note that in this case, parameters are tested against zero
and therefore, to interpret this test one would have to reconstruct the ﬁtted
data and check the positive or negative aspects of the response.
We now formally show how the two interpretations of the F-test are linked.
The model in equation (1), Y = Xβ + ϵ, is restricted by the test cT β = 0 where
c is now a ‘contrast matrix’. If c yields an estimable function then we can deﬁne
a matrix H such that c = HT X. Therefore, HT Xβ = 0 which, together with
equation (1), is equivalent to Y ⊂C(X) and Y ⊂C(H⊥), the space orthogonal
to H. It can be shown that the reduced model corresponding to this test can
be chosen to be X0 = PX −PH. This is a valid choice if and only if the space
spanned by X0 is the space deﬁned by C(H)⊥T C(X) and it is easy to show
that it is indeed the case.
If C(H) ⊂C(X), the numerator of equation 11 can be rewritten as
Y T (R0 −R)Y = Y T (X0 −R)Y = Y T (PX −X0)Y = Y T (PH)Y
We choose H such that it satisﬁes the condition above with H = (XT )−c, which
Y T X(XT X)−XT H(HT H)−HT X(XT X)−XT Y
ˆβT c(HT H)−cT ˆβ
The above rewriting of the F-test is important for several reasons. First it
makes the speciﬁcation and computation of F-tests feasible in the context of
large data sets. Specifying a reduced model and computing the extra sum of
squares using equation 11 would be too computationally demanding. Second,
it helps to make the link between a t-test and the test of a reduced model and
therefore helps to recall that what is tested is only the “extra” variability that
cannot be explained by the reduced model. Third, it makes the test of complex
interactions using F-tests more intuitive.
The F-contrast that looks at the total contribution of all the ‘right regressors’
is, however, quite a nonspeciﬁc test. One may have a speciﬁc hypothesis about
the magnitude or the delay of the response and would like to test speciﬁcally
for this. In the ﬁrst instance, it can be thought that a reasonable test would be
to use a t-test with contrast [0 0 0 1 0 0 0 0], testing for a positive parameter on
the regressor modelling the standard HRF. This is perfectly valid, but it has to
6This is an approximate result with good properties when the number of points is not too
small or if Σi is close to the identity matrix.
be remembered that what is tested here is the amount of adequacy of the BOLD
response with this regressor, not the magnitude of the response. This means, for
instance, that if the response has the shape of the one supposed by the standard
model but is signiﬁcantly delayed, the test might produce poor results even if
the delay is appropriately taken into account by the other regressors. This might
be quite important when comparing the magnitude of responses between two
conditions : if this magnitude is the same but the delays are diﬀerent across
conditions, the test comparing simply the standard response regressors might be
misinterpreted : a diﬀerence of delay might appear as a diﬀerence of magnitude
even if the basis function is decorrelated or orthogonal. A related problem, the
estimation of the delay of the HRF has been considered in an earlier chapter.
Note that the simplest F-contrasts are uni-dimensional, in which case the
F-statistic is simply the square of the corresponding t-statistic.
To visually
diﬀerentiate between unidimensional F-contrasts and t-contrasts in the SPM
interface, the former are displayed in terms of images and the latter as bars.
An important remark is that generally speaking, if we are conﬁdent about
the shape of the expected response, F-tests are often less sensitive than t-tests.
The reason is that the greater the ﬂexibility of the tested space, the greater
is the possibility of ﬁnding a signal that can have an important part of its
variability explained by the model, and there is an implicit correction for this
in the numerator of equation 11.
Correlation between regressors and other issues
Correlation between model regressors makes the tests more diﬃcult to interpret.
Unfortunately, such correlation is often imposed by the brain’s dynamics, experimental design or external measurements. The risks of mis-interpretation have
been extensively discussed in . To summarise, one could miss activations
when testing for a given contrast of parameters if there is signiﬁcant correlation
with the rest of the design. An important example of this situation is when the
response to a stimulus is highly correlated with a motor response.
If one believes that a region’s activity will not be inﬂuenced by the motor
response, then it is advisable to test this speciﬁc region by ﬁrst removing from
the motor response regressor all that can be explained by the stimulus. This is
of course a “dangerous” action since if in fact the motor response does inﬂuence
the signal in this region, then the test result will be overly signiﬁcant. That is,
experimental variance will be wrongly attributed to the stimulus.
Moving the variance across correlated regressors
If one decides that a regressor, or indeed several regressors or combination of
those, should be orthogonalised with respect to some part of the design matrix
before performing a test, it is not necessary to reparametrise and ﬁt the model
again. Once the model has been ﬁtted, all the information needed to test some
eﬀects can be found in the ﬁtted parameter estimates. For instance, instead of
testing the additional variance explained by a regressor, one may wish to test
for all the variance that can be explained by this regressor. Using the deﬁnitions
given in section 5.1, if c is the contrast testing for the extra sum of squares, it
is easy to show that the contrast matrix
cF ull space = XT Xc
tests for all the variance explained by the subspace of X deﬁned by Xc since
we then have H = Xc.
Contrasts and reparametrised models
The above principle can be generalised as follows. If the design matrix contains
three subspaces say (S1, S2, S3), one may wish to test for what is in S1, having
removed what could be explained by S2 (but not by S3). Other examples are
conjunction analyses in which a series of contrasts can be modiﬁed such that the
eﬀects they test are orthogonal. This involves orthogonalising the subsequent
subspaces tested. Therefore, results may diﬀer depending on the order in which
these contrasts are entered.
The principle to compute the contrast testing for various subspaces from
parametrised versions of the model is simple. If X and Xp are two diﬀerently
parametrised versions of the same model then we can deﬁne a matrix T such
that Xp = XT. If cp is a test expressed in Xp while the data have been ﬁtted
using X, the equivalent of cp using the parameter estimates of X is
c = cp(T T XT XT)−T T XT X
One should be careful using this sort of transformation, for instance, not putting
variance of “no interest” in the space tested. Such transformations are often
very useful, however, as the models do not require re-ﬁtting. See for further
The estimation-detection dilemma
A crucial point when analysing functional imaging data (PET and fMRI alike)
is that in general the model is not well known. The larger the model, the better
in general would be the estimation of the signal, as long as the model is not
starting to capture noise components. This often leads to less speciﬁc questions
and to less sensitive tests compared to situations where the diﬀerence is known
with better precision. There are two extreme choices:
• the use of a simple model with the danger of not having modelled some
eﬀects properly, a situation that may lead to biased results
• the use of a very ﬂexible model with less sensitive tests and diﬃculties in
the interpretation of the results
In other words, it is diﬃcult to estimate the signal and at the same time test
for this signal. A possible strategy would consist in using part of the data in
an estimation phase that is separate from a testing phase. This will, however,
involve ”losing” some data. For an instance of such a strategy see .
FIR and random eﬀects analyses
A typical example of a ﬂexible model is Finite Impulse Response (FIR) modelling of event-related fMRI data, as described in Chapter 10. The model in
this case is as ﬂexible as possible since the haemodynamic response function
is allowed to take any shape. A classic diﬃculty with this approach, however,
is how to implement a random eﬀects analysis (see Chapter 12).
The diﬃculty arises because, usually, one takes a single contrast image per subject up
to the second-level. With an FIR characterisation, however, one has multiple
parameter estimates per subject and one must therefore take into account the
covariance structure between parameters. Whilst this was prohibited in SPM99
it is now possible in SPM2, by making use of the ‘non-sphericity’ options as
alluded to in Chapters 9, 12 and 17.
In a functional imaging experiment it is often the case that one is interested in
many sorts of eﬀects eg. the main eﬀects of various conditions and the possible
interactions between them. To investigate each of these eﬀects one could ﬁt
several diﬀerent GLMs and test hypotheses by looking at individual parameter
estimates.
Because functional imaging data sets are so large, however, this
approach is impractical. A more expedient approach is to ﬁt larger models and
test for speciﬁc eﬀects using speciﬁc contrasts.
In this chapter we have shown how speciﬁcation of the design matrix is intimately related to the speciﬁcation of contrasts. For example, it is often the case
that main eﬀects and interactions can be set up using parametric or nonparametric designs. These diﬀerent designs lead to the use of diﬀerent contrasts.
Parametric approaches are favoured for factorial designs with many levels per
factor. For contrasts to be interpretable they must be estimable and we have
described the conditions for estimability.
In fMRI one can model haemodynamic responses using the canonical HRF.
This allows one to test for activations using t-contrasts. To account for the
variability in haemodynamic response across subjects and brain regions one
can model the HRF using the canonical HRF plus its derivatives with respect
to time and dispersion. Inferences about diﬀerences in activation can then be
made using F-contrasts. We have shown that there are two equivalent ways of
interpreting F-contrasts, one employing the extra-sum-of-squares principle to
compare the model and a reduced model and one specifying a series of onedimensional contrasts. Designs with correlation between regressors are less eﬃcient and correlation can be removed by orthogonalising one eﬀect with respect
to other eﬀects.
Finally, we have shown how such orthogonalisation can be
applied retrospectively, ie. without having to reﬁt the models.
The (n, 1) time series, where n is the number of time points or
weights of the parameter estimates used to form the (numerator)
of the statistics
or design model
the (n, p) matrix of regressors .
parameters
The true (unobservable) coeﬃcients such that the weighted sum
of the regressors is the expectation of our data (if X is correct)
Parameter estimates
The computed estimation of the β using the data Y :
forming matrix
Given a model X, the residual forming matrix R = In −XX−
transforms the data Y into the residuals r = RY .
scan (time) covariance
This (n, n) matrix describes the (noise) covariance between scans
Let us consider a set of p vectors xi of dimension (n, 1) (with p < n), such
as regressors in fMRI. The space spanned by this set of vectors is formed of
all possible vectors (say u) that can be expressed as a linear combination of
the xi : u = α1x1 + α2x2 + ...αpxp. If the matrix X is formed with the xi :
X = [x1x2...xp], we note this space as C(X).
Not all the xi may be necessary to form C(X). The minimal number needed
is called the rank of the matrix X. If only a subset of the xi are selected, say
that they form the smaller matrix X0, the space spanned by X0, C(X0) is called
a subspace of X. A contrast deﬁnes two subspaces of the design matrix X : one
that is tested and one of “no interest”, corresponding to the reduced model.
Orthogonal projection
The orthogonal projection of a vector x onto the space of a matrix A is the
vector (for instance a time series) that is the closest to what can be predicted
by linear combinations of the columns of A. The closest here is in the sense of
a minimal sum of square errors. The projector onto A, denoted PA, is unique
and can be computed with PA = AA−, with A−denoting the Moore-Penrose
pseudo inverse7 of A. For instance, in section 4.1, the ﬁtted data ˆY can be
computed with
ˆY = PXY = XX−Y = X(XT X)−XY = X ˆβ
7Any generalised inverse could be used.
Most of the operations needed when working with linear models only involve
computations in the parameter space, as is shown in equation 17. For a further
gain, if the design is degenerate, one can work in an orthonormal basis of the
space of X. This is how the SPM
code is implemented.