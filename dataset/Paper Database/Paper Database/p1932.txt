Quantum agents in the Gym:
a variational quantum algorithm for deep Q-learning
Andrea Skolik1,2, Soﬁene Jerbi3, and Vedran Dunjko1
1Leiden University, Niels Bohrweg 1, 2333 CA Leiden, The Netherlands
2Volkswagen Data:Lab, Ungererstraße 69, 80805 Munich, Germany
3Institute for Theoretical Physics, University of Innsbruck, Technikerstr. 21a, A-6020 Innsbruck, Austria
Quantum machine learning (QML) has
been identiﬁed as one of the key ﬁelds
that could reap advantages from nearterm quantum devices, next to optimization and quantum chemistry. Research in
this area has focused primarily on variational quantum algorithms (VQAs), and
several proposals to enhance supervised,
unsupervised and reinforcement learning
(RL) algorithms with VQAs have been
put forward.
Out of the three, RL is
the least studied and it is still an open
question whether VQAs can be competitive with state-of-the-art classical algorithms based on neural networks (NNs)
even on simple benchmark tasks. In this
we introduce a training method
for parametrized quantum circuits (PQCs)
that can be used to solve RL tasks for discrete and continuous state spaces based on
the deep Q-learning algorithm. We investigate which architectural choices for quantum Q-learning agents are most important
for successfully solving certain types of environments by performing ablation studies for a number of diﬀerent data encoding and readout strategies. We provide insight into why the performance of a VQAbased Q-learning algorithm crucially depends on the observables of the quantum
model and show how to choose suitable
observables based on the learning task at
hand. To compare our model against the
classical DQN algorithm, we perform an
extensive hyperparameter search of PQCs
and NNs with varying numbers of parameters.
We conﬁrm that similar to results in classical literature, the architectural choices and hyperparameters contribute more to the agents’ success in a
RL setting than the number of parameters
used in the model. Finally, we show when
recent separation results between classical
and quantum agents for policy gradient
RL can be extended to inferring optimal
Q-values in restricted families of environments. This work paves the way towards
new ideas on how a quantum advantage
may be obtained for real-world problems
in the future.
Introduction
Variational quantum algorithms are among the
most promising candidates to show quantum advantage in the near-term.
Quantum machine
learning has emerged as one ﬁeld that is amenable
to applications of VQAs on noisy intermediatescale quantum (NISQ) devices .
proposals for QML algorithms have been made
in supervised and unsupervised
 learning. In contrast, RL is a
subﬁeld of machine learning that has received less
attention in the QML community , and especially proposals for VQA-based approaches are
only now emerging . RL is essentially a way to solve the problem of optimal
control. In a RL task, an agent is not given a
ﬁxed set of training data, but learns from interaction with an environment. Environments are
deﬁned by a space of states they can be in, and
a space of actions that an agent uses to alter
the environment’s state. The agent chooses its
next action based on a policy (probability distribution over actions given states) and receives
a reward at each step, and the goal is to learn
an optimal policy that maximizes the long-term
reward the agent gets in the environment. State
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
 
and action spaces can be arbitrarily complex, and
it’s an open question which types of models are
best suited for these learning tasks. In classical
RL, using NNs as function approximators for the
agents’ policy has received increased interest in
the past decade. As opposed to learning exact
functions to model agent behavior which is infeasible in large state and action spaces, this method
of RL only approximates the optimal function.
These types of RL algorithms have been shown to
play Atari arcade games as well as human players
 , and even reach super-human levels of performance on games as complex as Go , Dota
 and StarCraft .
RL algorithms can be
divided into policy-based and value-based methods.
While policy-based algorithms seek to directly optimize a parametrized probability distribution that represents the policy, a value-based
algorithm learns a so-called value function which
then gives rise to a policy. These two methods
constitute related but fundamentally diﬀerent approaches to solve RL tasks, and both have their
own (dis-)advantages as we will explain in detail
in section 2.1. Interestingly, these two methods
can also be combined in a so-called actor-critic
setting which leverages the strengths of both approaches .
Actor-critic methods are among
the state-of-the-art in current RL literature ,
and therefore both value-based and policy-based
algorithms are areas of active research.
One classical value-based RL algorithm that
has gained much popularity is called Q-learning
 , where the goal is to learn Q-values, numerical values which for a given state-action pair
represent the expected future reward. Based on
these values, a Q-learning agent chooses the appropriate action in a given state, where a higher
Q-value corresponds to a higher expected reward. The NN’s role in a Q-learning algorithm
is to serve as a function approximator for the Qfunction. It is thus natural to ask whether RL
algorithms can be adapted to work with a VQA
approach, and whether such algorithms could offer an advantage over their classical counterparts.
RL is one of the hardest modes of learning in current ML research, and is known to require careful
tuning of model architectures and hyperparameters to perform well. For NN-based approaches,
one unfavorable hyperparameter setting can lead
to complete failure of the learning algorithm on
a speciﬁc task. Additionally, these hyperparameters and architectures are highly task dependent
and there is no a-priori way to know which settings are best. Well-performing settings are found
by experts via trial-and-error, and the ability to
quickly ﬁnd these settings is considered a “black
art that requires years of experience to acquire”
 . Thus a whole ﬁeld of heuristics and numerical studies has formed on ﬁnding good sets of hyperparameters like NN architectures ,
activation functions , or learning rates
and batch sizes . An increasingly investigated branch of research focuses on methods to
automate the whole process of ﬁnding good architectures and hyperparameters, among which
there is neural architecture search and automated machine learning .
It is thus to be expected that quantum models
in a VQA-based RL setting also need to be selected carefully. Even more so, it is still an open
question whether VQAs are suitable for function
approximation in RL at all. This question is directly related to choices made when deﬁning an
architecture for a VQA. There are three important factors to consider: the structure (or ansatz)
of the model, the data-encoding technique, and
the readout operators. For the choice of structure, there is a trade-oﬀbetween the expressivity
and trainability of a model, as certain structures
are subject to the so-called barren plateau phenomenon .
This phenomenon prevents successful training of models with a large number of
qubits and layers for highly expressive structures
like random circuits. On the other hand, overparametrization has been observed to simplify
optimization landscapes and lead to faster convergence for certain VQAs . Apart from
that, the choice of structure is also limited by
hardware constraints like the topology of a certain quantum device. While the model structure
is an important factor in training VQAs that has
received much attention in the QML community
 , the authors of 
have shown that the technique used to encode
data into the model plays an equally important
role, and that even highly expressive structures
fail to ﬁt simple functions with an insuﬃcient
data-encoding strategy.
A less explored architectural choice in the context of QML is that of the observables used to
read out information from the quantum model.
Considering that the readout operator of a quan-
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
tum model ﬁxes the range of values it can produce, this choice is especially important for tasks
where the goal is to ﬁt a real-valued function with
a given range, as is the case in many RL algorithms. This is in contrast to NNs, which have
no restriction on the range of output values and
can even change this range dynamically during
In Q-learning, the goal is to approximate the real-valued optimal Q-function, which
can have an arbitrary range based on the environment. Crucially, this range can change depending
on the performance of the agent in the environment, which is an impediment for quantum models with a ﬁxed range of output values.
A ﬁrst step to study the inﬂuence of architectural choices on PQCs for policy-based RL algorithms has been made in , who point out
that data-encoding and readout strategies play a
crucial role in these types of RL tasks, though
they leave the open question if similar architectural choices are also required in a value-based
setting. Previous work on Q-learning with PQCs
has addressed certain other fundamental questions about the applicability of VQAs in a valuebased context. A VQA for Q-learning in discrete
state spaces was introduced in , where the
quantum model’s output is followed by a layer
of additive weights, and it has been shown that
the model successfully solves two discrete-state
environments. A VQA for Q-learning in environments with continuous and discrete state spaces
has been proposed in , who simplify the continuous environments’ potentially inﬁnite range
of input values to a restricted encoding into angles of one initial layer of rotation gates, and
use measurements in the Z-basis to represent Qvalues. Notably, none of the models in that
were run for the continuous state-space environment Cart Pole reach a performance that is considered to be solving the environment according
to its original speciﬁcation , so it remains an
open question whether a value-based algorithm
that utilizes a PQC as the function approximator
can solve this type of learning task.
These initial works prompt a number of vital
follow-up questions related to the architectural
choices that are required to succeed in arbitrary
RL environments with a quantum Q-learning
We address these questions in form of
our main contributions as follows: ﬁrst, we propose a VQA which can encode states of discrete
and continuous RL environments and explain the
intricate relationship between the environment’s
speciﬁcation and the requirements on the readout operators of the quantum model. We show
how a quantum Q-learning agent only succeeds
if these requirements are met. Second, to enable
the model to match the environment’s requirements on the range of output values, we make this
range itself trainable by introducing additional
weights on the model outputs. We show how the
necessity of these weights can be inferred from the
range that the optimal Q-values take in an environment. Third, we study the performance of our
model on two benchmark environments from the
OpenAI Gym , Frozen Lake and Cart Pole.
For the continuous-state Cart Pole environment,
we also study a number of data encoding methods and illustrate the beneﬁt of previously introduced techniques to increase quantum model expressivity, like data re-uploading or trainable
weights on the input data . Additionally,
the state space dimension of both environments
is small enough so that inputs can be directly encoded into the quantum model without the use
of a dimensionality reduction technique.
makes it possible to directly compare our model
to a NN performing the same type of Q-learning
algorithm to evaluate its performance.
Speciﬁcally, we perform an in-depth comparison of the
performance of PQCs and NNs with varying numbers of parameters on the Cart Pole environment.
We show that recent results in classical deep Qlearning also apply to the case when a PQC is
used as the function approximator, namely that
increasing the number of parameters is only beneﬁcial up to some point . After this, learning
becomes increasingly unstable for both PQCs and
NNs. As an empirical comparison between PQCs
and NNs can only give us insight into model performance on the speciﬁc environments we study,
we also explain when recent separation results for
policy gradient RL between classical and quantum agents also hold in the Q-learning setting
for restricted families of environments.
The remainder of this paper is structured as
follows: in section 2 we give an introduction to
RL, followed by a description of our quantum RL
model in section 3. We show when recent results
for a separation between classical and quantum
algorithms for policy-based learning also apply in
the case of Q-learning in section 4. In section 5
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
we numerically evaluate the performance of our
algorithm and compare it to a classical approach,
and ﬁnally discuss our ﬁndings in section 6. The
full code that was used to perform the numerical
experiments in this work can be found on Github
Reinforcement learning
In RL, an agent does not learn from a ﬁxed data
set as in other types of learning, but by making
observations on and interacting with an environment . This distinguishes it from the other
two main branches of ML, supervised and unsupervised learning, and each of the three comes
with its individual challenges.
In a supervised
setting, an agent is given a ﬁxed set of training data that is provided with the correct labels,
where diﬃculties arise mainly in creating models that do not overﬁt the training data and keep
their performance high on unseen samples. In unsupervised learning, training data is not labeled
and the model needs to discover the underlying
structure of a given data set, and the challenge
lies in ﬁnding suitable loss functions and training methods that enable this.
RL also comes
with a number of challenges: there is no ﬁxed
set of training data, but the agent generates its
own samples by interacting with an environment.
These samples are not labeled, but only come
with feedback in form of a reward. Additionally,
the training data keeps changing throughout the
learning process, as the agent constantly receives
feedback from interacting with its environment.
An environment consists of a set of possible
states S that it can take, and a set of actions
A which the agent can perform to alter the environment’s state. Both state and action spaces
can be continuous or discrete. An agent interacts
with an environment by performing an action at
at time step t in state st, upon which it receives
a reward rt+1. A tuple (s, a, r, s′) of these four
quantities is called a transition, and transition
probabilities from state s to s′ after performing
action a in a given environment are represented
by the transition function P a
ss′ = P(s′|s, a).
The reward function is designed to evaluate the
quality of the agent’s actions on the environment based on the learning task at hand, and
the agent’s goal is to maximize its total reward
over a sequence of time steps starting at t, called
the return Gt
where γ ∈ is a discount factor introduced
to prevent divergence of the inﬁnite sum.
return Gt should be viewed as the agent’s expected reward when starting from time step t and
summing the discounted rewards of potentially
inﬁnitely many future time steps, where maximizing the return at step t implies also maximizing the return of future time steps.
the task is to maximize an expected value, and
that the reward rt in eq. (2), and therefore Gt are
random variables. Environments often naturally
break down into so-called episodes, where the sum
in eq. (2) is not inﬁnite, but only runs over a ﬁxed
number of steps called horizon H. An example of
this are environments based on games, where one
episode comprises one game played and an agent
learns by playing a number of games in series.
Value-based and policy-based learning
RL algorithms can be categorized into value-based
and policy-based learning methods . Both approaches aim to maximize the return as explained
above, but use diﬀerent ﬁgures of merit to achieve
this. Both approaches also have their disadvantages as we will see below, and which type of
algorithm should be used depends on the environment at hand.
In both cases, the function
that models the agent’s behavior in the environment is called the policy π(a|s), which gives the
probability of taking action a in a given state s.
The main diﬀerence between the two approaches
is how the policy is realized. In general, performance is evaluated based on a state-value function (or an action-value function, as we will see
in section 2.2) Vπ(s),
Vπ(s) = Eπ[Gt|st = s],
which is the expected return when following policy
π starting from state s at initial time step t, and
the goal of a RL algorithm is to learn the optimal
policy π∗which maximizes the expected return
for each state.
A policy-based algorithm seeks to learn an optimal policy directly, that is, learn a probability distribution of actions given states.
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
setting, the policy is implemented in form of a
parametrized conditional probability distribution
π(a|s; θ), and the goal of the algorithm is to ﬁnd
parameters θ such that the resulting policy is optimal. The ﬁgure of merit in this setting is some
performance measure J(θ) that we seek to maximize. This performance measure can include a
value function as in eq. (3), however, once the
policy has been learned J(θ) is not required for
action generation.
Typically, these algorithms
perform gradient ascent on an approximation of
the gradient of the performance measure ∇J(θ),
which is obtained by Monte Carlo samples of
policy rollouts (i.e., a set of observed interactions with the environment performed under the
given policy), and are hence called policy gradient
This approach produces smooth updates on the policy (as opposed to value-based algorithms, where a small change in the value function can drastically alter the policy) that enable
proofs of convergence to locally optimal policies
 . However, it also suﬀers from high variance
as updates are purely based on Monte Carlo samples of interactions with the environment . A
number of methods to reduce this variance have
been developed, like adding a value-based component as described below to a policy-based learner
in the so-called actor-critic method .
In a value-based algorithm, a value function as
in eq. (3) is learned instead of the policy. The
policy is then implicitly given by the value function: an agent will pick the action which yields
the highest expected return according to Vπ(s). A
concrete example of value-based learning is given
in section 2.2, where we introduce the algorithm
that we focus on in this work. While value-based
algorithms do not suﬀer from high training variance as policy gradient learning does, they often
require more episodes to converge. They also result in deterministic policies, as the agent always
picks the action that corresponds to the highest expected reward, so this approach will fail
when the optimal policy is stochastic and posttraining action selection is performed according
to the argmax policy.1 Additionally, the policy
resulting from a parametrized value function can
change substantially after a single parameter up-
1Consider for example a game of poker where bluﬀing is a valid action to scare other players into folding,
but quickly becomes obvious when greedily done in every
date (i.e., a very small change in the value function can lead to picking a diﬀerent action after
an update). This results in theoretical diﬃculties
to prove convergence when a function approximator is used to parametrize the value function,
hence there are even fewer theoretical guarantees
for this approach than for policy gradient methods.
On the other hand, it was the advent of
deep Q-learning that made it possible to solve
extremely complex problems such as Go with a
reinforcement learning approach .
Both approaches have their own (dis-) advantages, and while the popularity of either method
has surpassed the other at some point in the
last decades, there is no clear winner. As mentioned above, an actor-critic approach combines a
policy-based and value-based learner to leverage
the advantages of both while alleviating the disadvantages, and this method is among the stat-ofthe-art in classical RL literature . Additionally, it can be easier to learn either the policy or
the value function depending on a given environment. For this reason, both approaches are worth
being studied independently.
In the quantum
setting, VQAs for policy gradient learning have
been investigated in .
Using PQCs for
value-based learning has been explored in ,
and a speciﬁc value-based RL algorithm called Qlearning is the focus of this work.
Q-learning
In Q-learning, we are not interested in the statevalue function as shown in eq. (3), but in the
closely related action-value function Qπ(s, a),
Qπ(s, a) = Eπ[Gt|st = s, at = a],
which also gives us the expected return assuming we follow a policy π, but now additionally
conditioned on an action a. We call the optimal
Q-function Q∗(s, a) = maxπ Qπ(s, a), and an optimal policy can be easily derived from the optimal values by taking the highest-valued action in
each step, as
π∗(a|s) = argmax
The goal in Q-learning is to learn an estimate,
Q(s, a), of the optimal Q-function. In its original
form, Q-learning is a tabular learning algorithm,
where a so-called Q-table stores Q-values for each
possible state-action pair . When interacting
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
with an environment, an agent chooses its next
action depending on the Q-values as
at = argmax
where a higher value designates a higher expected
reward when action a is taken in state st as opposed to the other available actions. When we
consider learning by interaction with an environment, it is important that the agent is exposed
to a variety of transitions to suﬃciently explore
the state and action space. Intuitively, this provides the agent with enough information to tell
apart good and bad actions given certain states.
Theoretically, visiting all state-action transitions
inﬁnitely often is one of the conditions that are
required to hold for convergence proofs of tabular Q-learning to an optimal policy . Clearly,
if we always follow an argmax policy, the agent
only gets access to a limited part of the state and
action space. To ensure suﬃcient exploration in
a Q-learning setting, a so-called ϵ-greedy policy is
used. That is, with probability ϵ a random action
is performed and with probability 1 −ϵ the agent
chooses the action which corresponds to the highest Q-value for the given state as in eq. (6). Note
that the ϵ-greedy policy is only used to introduce
randomness to the actions picked by the agent
during training, but once training is ﬁnished, a
deterministic argmax policy is followed.
The Q-values are updated with observations
made in the environment by the following update
Q(st, at) ←Q(st, at) + α[rt+1 + γ · max
Q(st+1, a)
−Q(st, at)],
where α is a learning rate, rt+1 is the reward
at time t + 1, and γ is a discount factor. Intuitively, this update rule provides direct feedback
from the environment in form of the observed
reward, while simultaneously incorporating the
agent’s own expectation of future rewards at the
present time step via the maximum achievable
expected return in state st+1. In the limit of visiting all (s, a) pairs inﬁnitely often, this update
rule is proven to converge to the optimal Q-values
in the tabular case .
Q-values can take an arbitrary range, which
is determined by the environment’s reward function and the discount factor γ, which controls
how strongly expected future rewards inﬂuence
the agent’s decisions. Depending on γ, the optimal Q-values for the same environment can
take highly varying values, and can therefore be
viewed as diﬀerent learning environments themselves.
In practice, it is not necessary that an
agent learns the optimal Q-values exactly.
the next action at step t is chosen according to
eq. (6), it is suﬃcient that the action with the
highest expected reward has the highest Q-value
for the sake of solving an environment presuming
a deterministic policy. In other words, for solving
an environment only the order of Q-values is important, and the task is to learn this correct order by observing rewards from the environment
through interaction.
As our goal in this work
is to solve certain environments with Q-learning,
learning the correct order of Q-values is suﬃcient
and we restrict our attention to this case throughout our numerical studies.
Obviously, the tabular approach is intractable
for large state and action spaces. For this reason,
the Q-table was replaced in subsequent work by
a Q-function approximator which does not store
all Q-values individually .
In the seminal work , the authors use a NN as the Qfunction approximator which they call deep Qnetwork (DQN) and the resulting algorithm the
DQN algorithm, and demonstrate that this algorithm achieves human-level performance on a
number of arcade games. In this work, the agent
chooses actions based on an ϵ-greedy policy as described above. Typically ϵ is chosen large in the
beginning and then decayed in subsequent iterations, to ensure that the agent can suﬃciently explore the environment at early stages of training
by being exposed to a variety of states. The authors of also utilize two other improvements
over previous approaches which we adopt: (i) experience replay: past transitions and their outcomes are stored in a memory, and the batches
of these transitions that are used to compute parameter updates are sampled at random from this
memory to remove temporal correlations between
transitions, (ii) adding a second so-called target
network to compute the expected Q-values for
the update rule, where the target network has an
identical structure as the DQN, but is not trained
and only sporadically updated with a copy of
the parameters of the DQN to increase stability
during training. We refer to the original paper
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
for further details on the necessity of these techniques to stabilize training of the DQN algorithm.
The DQN is then trained almost in a supervised fashion, where the training data and labels are generated by the DQN itself through
interaction with the environment.
At each update step, a batch B of previous transitions
(st, at, rt+1, st+1) is chosen from the replay memory.
To perform a model update, we need to
compute max
Q(st+1, a).
When we use a target network, this value is not computed by the
DQN, but by the target network ˆQ.
training more eﬃcient, in practice the Q-function
approximator is redeﬁned as a function of a state
parametrized by θ, Qθ(s) = q, which returns a
vector of Q-values for all possible actions instead
of computing each Q(s, a) individually. We now
want to perform a supervised update of Qθ, where
the label is obtained by applying the update rule
in eq. (8) to the DQN’s output. To compute the
label for a state s that we have taken action a on
in the past, we take a copy of Qθ(s) which we call
qδ, and only the ith entry of qδ is altered where i
corresponds to the index of the action a, and all
other values remain unchanged. The estimated
maximum Q-value for the following state st+1 is
computed by ˆQθδ, and the update rule for the
i-th entry in qδ takes the following form
qδi = rt+1 + γ · max
ˆQθδ(st+1, a),
where θδ is a periodically updated copy of θ. The
loss function L is the mean squared error (MSE)
between q and qδ on a batch of sample transitions
L(q, qδ) = 1
(qb −qδb)2.
Note that because qδ is a copy of q where only
the i-th element is altered via the update rule in
eq. (8), the diﬀerence between all other entries
in those two vectors is zero. As Q-values are de-
ﬁned in terms of (s, a)-pairs, this approach does
not naturally apply to environments with continuous action spaces. In this case, the continuous
action space has to be binned into a discrete representation.
Quantum Q-learning
In this work, we adapt the DQN algorithm to use
a PQC as its Q-function approximator instead of
Figure 1: PQC architecture used in this work. Each layer
consists of a parametrized rotation along the Y and Z
axes on each qubit, and a daisy chain of CZ gates. The
green boxes correspond to data encoding gates that encode data as parameters of X rotations. When data reuploading is used, the whole circuit pictured is repeated
in each layer, without data re-uploading only the variational part without the initial X rotations is repeated.
a NN. For this, we use a hardware-eﬃcient ansatz
 as shown in ﬁg. 1. This ansatz is known to be
highly expressive, and is susceptible to the barren
plateau phenomenon for a large number of qubits
and layers, although this is not an issue for the
small state and action spaces we consider here.
All other aspects of the Q-learning algorithm described in section 2.2 stay the same: we use a
target network, an ϵ-greedy policy to determine
the agent’s next action, and experience replay to
draw samples for training the Q-network PQC.
Our Q-network PQC is then Uθ(s) parametrized
by θ and the target network PQC is ˆUθδ(s), where
θδ is a snapshot of the parameters θ which is
taken after ﬁxed intervals of episodes δ and the
circuit is otherwise identical to that of Uθ(s). We
now explain how environment states are encoded
into our quantum model, and how measurements
are performed to obtain Q-values.
Encoding environment states
Depending on the state space of the environment,
we distinguish between two diﬀerent types of encoding in this work:
mapped to bitstrings and then input into the
model, where on an all-zero state the bits corresponding to ones in the input state are ﬂipped.
Continuous state space: For continuous input
states, we scale each component x of an input
state vector x to x′ = arctan(x) ∈[−π/2, π/2]
and then perform a variational encoding, which
consists of rotations in the X direction by the
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
angles x′.
As shown in , when data is encoded into a
PQC by local rotation gates along the X-axis, the
PQC can only model simple sine functions of its
input. To further increase the expressivity of the
circuit, the data encoding can be repeated in two
ways: either in parallel by increasing the number
of qubits and duplicating the data encoding on
them, or in sequence in an alternating fashion
with the variational layers of the circuit.
latter is also referred to as data re-uploading in
 . Where needed, we will introduce data reuploading to our model in section 5.
The formalism introduced in establishes a
connection between PQCs and partial Fourier series by showing that the functions a given PQC
can model can be represented as a Fourier series,
where the accessible frequency spectrum depends
on the eigenvalues of the data encoding gates, and
the coeﬃcients depend on the architecture of the
variational part of the PQC and the observable
that deﬁnes the readout operation. They show
that in models as ours, where data is encoded in
form of Pauli rotations, only Fourier series up to
a certain degree can be learned, where the degree
depends on the number of times the encoding gate
is repeated. Additionally, the scale of the input
data must match the scale of the frequencies of
the modeled function for the model to ﬁt the target function exactly. Making the scaling of input
data itself trainable to increase a PQC’s expressivity has been suggested in , which we
will also use by introducing a weight wd on the
input data. The input value x′
i then becomes:
i = arctan(xi · wdi) ,
where wdi is the weight for input xi.
illustrate the advantage of these enhanced dataencoding strategies numerically in section 5.
Computing Q-values
The Q-values of our quantum agent are computed
as the expectation values of a PQC that is fed a
state s as
Q(s, a) = ⟨0⊗n| U†
θ(s)OaUθ(s) |0⊗n⟩,
where Oa is an observable and n the number of
qubits, and our model outputs a vector including Q-values for each possible Oa as described in
section 2.2. The type of measurements we perform to estimate Q-values will be described in
more detail in section 5 for each environment.
Before that, we want to highlight why the way Qvalues are read out from the PQC is an important
factor that determines the success at solving the
environment at hand. A key diﬀerence between
PQCs and NNs is that a PQC has a ﬁxed range
of output deﬁned by its measurements, while a
NN’s range of output values can change arbitrarily during training depending on its weights and
activation function. To understand why this is
an important diﬀerence in a RL setting, we need
to recall that Q-values are an estimate of the expected return
Qπ(s, a) = Eπ[Gt|st = s, at = a]
γkrt+k+1|st = s, at = a
This quantity is directly linked to the performance of the agent in a given environment, so
the model needs to have the ability to match the
range of optimal Q-values in order to approximate the optimal Q-function. This means that
the observables in a PQC-based Q-learning agent
need to be chosen with care, and highly depend
on the speciﬁc environment. To provide a simple
example where an insuﬃcient range prevents an
agent from solving an environment, consider tabular learning in an environment that consists of
a single state s and two actions a1 and a2, where
the agent should learn to always pick a1.
episode has a maximum length of H = 10 when
the agent picks a1 in each time step, and otherwise terminates when the agents picks action a2.
We consider a modiﬁcation where the values in
the Q-table are capped at 1, i.e., Q-values can
not become larger than one, and both Q-values
are initialized at zero. The environment is such
that the reward for each action is 1 and the Qvalue corresponding to the optimal action is > 1.
For simplicity we set α = 1 and γ = 1, which
gives us an optimal value Q∗(s, a1) = 10.
now perform an update on both Q-values according to the update rule in eq. (7),
Q(st, at) ←rt+1 + argmaxaQ(st+1, a).
For action a2, the transition from s leads to
episode termination, so the update rule yields
Q(s, a2) = rt+1 = 1.
For action a1, we get
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
Q(s, a1) = 2, however, due to the capped Q-table,
we also get Q(s, a1) = 1 for this state-action pair.
We see that after a single update according to this
update rule, both Q-values will be one and due to
the capped range of the Q-table the Q-values are
already saturated. No further update can change
the Q-values, which means that the agent can do
no better than random guessing hereafter. This
simple example illustrates why it is essential in a
tabular Q-learning setting that the range of values in the Q-table accommodates the magnitude
of optimal Q-values. Updates in the function approximation case like in the gradient-based DQN
algorithm are more complex due to the regression
task that the agent solves to perform parameter
updates, however, a similar saturation can still
occur as the update rule for Q-values is the same
(see eq. (8)).
We have seen that it is crucial for a PQCbased Q-learning agent to have an output range
that matches that of the optimal Q-values that
it seeks to approximate. There are two ways to
approach this issue: (i) multiply PQC outputs
by a ﬁxed factor to increase their range in a way
that accommodates the theoretical maximum Qvalue, (ii) make the output range itself a trainable model parameter. Multiplying the outputs
of the PQC by a ﬁxed factor increases the range
of output values, but at the cost of potentially
being close to the estimated maximum from the
beginning, which makes this approach more sensitive to randomness in model parameter initialization. In particular, as Q-values are initialized
randomly depending on the initial parameters of
the PQC, the Q-values for actions of a speciﬁc
state might have large diﬀerences. Considering
that the reward which controls the magnitude of
change given by the Q-value updates in eq. (8) is
comparatively small and actions are picked based
on the argmax policy argmaxaQ(s, a), it may take
a long time before subsequent updates of Q-values
will lead to the agent picking the right actions.
Even if we consider models that are initialized
such that all Q-values are close to zero in the
beginning, the actual changes in the rotation angles that the PQC needs to perform for Q-values
of large ranges can become very small. Especially
on NISQ devices, these changes might be impractically small to be reliably performed and measured on hardware. For these reasons, we focus
on option (ii). We add a trainable weight wo ∈R
to each readout operation, so that the output Qvalue Q(s, a) becomes
Q(s, a) = ⟨0⊗n| Uθ(s)†OaUθ(s) |0⊗n⟩· woa, (12)
and each action has a separate weight woa. We
make the weights multiplicative in analogy to
weights in a NN. This gives the model the possibility to ﬂexibly increase the magnitude of Qvalues to match the given environment. Notably,
the number of actions in an environment is usually small compared to the number of parameters
in the model, so adding one extra weight corresponding to each action does not designate a large
In section 5.2, we numerically show
that the approach of using a trainable weight
on the output value outperforms multiplying the
model output by a ﬁxed factor that is motivated
by the range of optimal Q-values.
Separation between quantum and
classical Q-learning in restricted environments
In this section, we make formal statements about
a separation between quantum and classical models for Q-learning in a restricted family of environments. These statements are based on recent results in supervised and policy gradient based
reinforcement learning . The latter work constructs families of environments that are proven
to be hard for any classical learner, but can be
solved in polynomial time by a quantum learner
in a policy learning setting. Learning policies is
closely related to learning Q-values, however, Qvalues contain more information about the environment per deﬁnition as they cover the whole
state-action space.
This means that it is not
straightforward to generalize the results from 
to a Q-learning setting. In this section, we will
show under which conditions optimal Q-values
can be inferred from optimal policies, so that
the separation results in also apply to the
Q-learning case. The environments constructed
in are based on the supervised learning task
introduced in , which are proven to be classically hard assuming the widely-believed hardness of the discrete logarithm problem, but can
be solved by a quantum learner in polynomial
time. To understand how a separation in supervised learning can be generalized to a RL setting,
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
it is important to state that any classiﬁcation task
can be turned into an environment for RL. To do
this, rewards in the environment are assigned according to the prediction the agent makes. First
examples of this were introduced in for cases
where the environment allows quantum access to
its states. A classiﬁcation task like the one proposed in can be turned into a RL task by
simply assigning a reward of 1 (-1) for a correct
(incorrect) classiﬁcation, and deﬁning an episode
as being presented with a set of training samples.
In this section, we will brieﬂy revise the separation results for supervised learning given in 
and those for policy gradient RL given in ,
before we move on to characterize the types of
environments that allow a generalization of the
results in to a Q-learning setting.
A classiﬁcation task based on the discrete
logarithm problem
The authors of construct a classiﬁcation task
that is intractable for any classical learner, but
can be solved by a quantum learner in polynomial time. The classiﬁcation task is based on the
discrete logarithm problem (DLP), and the separation relies on the the quantum learner’s ability
to perform the algorithm provided by Shor in 
to solve the DLP eﬃciently.
Deﬁnition 1 (Discrete logarithm problem). Let
p = {1, 2, . . . , p −1} be the cyclic multiplicative
group of integers modulo p for a large prime p,
and g a generator of this group.
The DLP is
deﬁned as computing logg x for an input x ∈Z∗
It is widely believed that no classical algorithm can solve the DLP eﬃciently, however, it is
proven that the algorithm provided by Shor can
solve DLP in poly(n) time for n = ⌈log2 p⌉ .
Based on this, construct a classiﬁcation task
with a concept class C = {fs}s∈Z∗p and data points
deﬁned over the data space X = Z∗
p ⊆{0, 1}n as
if logg x ∈[s, s + p−3
otherwise,
where each concept fs : Z∗
p →{−1, 1} maps one
half of the elements in Z∗
p to 1 and the other half
to −1, which yields a linearly separable set of data
points in log-space. A quantum learner can make
use of the algorithm from to compute the
discrete logarithm and solve the resulting trivial learning task. However, if a classical learner
could solve the above learning task this would
imply that there exists an eﬃcient classical algorithm that solves the DLP. This is contrary to the
widely believed conjecture that no eﬃcient classical algorithm can solve the DLP, and proves
that no classical learner can do better than random guessing.
To connect these results to the RL setting,
it is useful to be a bit more precise and deﬁne
some terminology. The learning task is deﬁned as
ﬁnding a decision rule f∗, which assigns a label
y ∈{−1, 1} to data point x ∈X 2. f∗is learned
on a set of labeled examples S = {xi, yi}i=1,...,m
generated by the unknown decision rule, or concept, f.
An eﬃcient learner needs to compute
f∗in time polynomial in n that agrees with the
labeling given by f with high probability, or in
other words reaches a high test accuracy on unseen samples,
accf(f∗) = Pr
x∈X[f(x) = f∗(x)].
The authors of prove that no eﬃcient classical
learner can achieve
accf(f∗) = 1
unless an eﬃcient classical algorithm that solves
the DLP exists, while there exists a quantum
learner that achieves close to perfect accuracy
with high probability in polynomial time.
Learning optimal policies in environments
based on the DLP classiﬁcation task
After stating the classiﬁcation task based on the
DLP in the previous section, we now brieﬂy review how the authors of construct families
of environments based on the DLP classiﬁcation
task to transfer the separation results to RL.
They show that (i) solving these environments
2Note that we are adhering to the notation given in
 , where the asterisk stands for the learned decision
rule and the function without an asterisk stands for the
decision rule we seek to learn. This is the opposite of the
notation used in Q-learning literature where Q∗stands for
the optimal Q-values, which we have followed in previous
sections. The authors of have also adopted the latter
notation in their paper to describe the DLP classiﬁcation
task. We will stick to denoting the learned decision rule
with an asterisk in this section.
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
is classically hard for any learner unless there exists an eﬃcient classical algorithm that solves the
DLP, (ii) there exists a quantum learner that can
solve these environments in polynomial time. To
understand how the DLP classiﬁcation task can
be used to construct a classically hard to solve RL
environment, it is important to note again that
any classiﬁcation task can be trivially turned into
a RL task by letting each data point x ∈X denote
a state in the environment, and giving rewards
to the agent depending on whether it correctly
assigns a state to its predeﬁned label y. The rewards for the DLP classiﬁcation task are 1 (−1)
for a correct (false) classiﬁcation. While are
interested in achieving a high test accuracy, in a
RL setting we want to ﬁnd an agent with closeto-optimal performance in the given environment.
The authors of measure this performance in
terms of a value function Vπ(s) for policy π and
Vπ(s) = Eπ
γtrt|st = s
which is the expected reward for following policy
π for an episode of length H in state s. Based
on the DLP classiﬁcation task from , the authors of deﬁne three diﬀerent environments
that are classically hard to learn, where the value
function of each of these environments is closely
related to the accuracy in eq. (14) of the policy
on the classiﬁcation task.
This allows them to
get bounds on the value function as a function of
bounds on the accuracy. Roughly speaking, by
Theorem 1 of no classical learner can achieve
performance better than that of random guessing in poly(n) time on those environments, unless
an eﬃcient classical algorithm to solve the DLP
exists. We will brieﬂy explain the set-up of the
quantum learner in , before going into more
detail on one of the families of environments they
construct to show a separation between classical
and quantum learners for policy learning.
A RL agent can be trivially constructed from
the classiﬁer in , which is based on a classical support vector machine (SVM) that takes the
samples that have been “decrypted" by a quantum feature map as an input. (This type of classi-
ﬁer is also referred to as an implicit SVM). However, to get a learner that more closely matches
the parametrized training of a quantum learner
done in , they use a model where the feature embedding and classiﬁcation task are both
solved by a PQC. This method is referred to as
an explicit SVM. The explicit SVM comprises
a feature-encoding unitary U(x) applied on the
all zero state, which they refer to as |φ(x)⟩=
U(x) |0⊗n⟩, a variational part V (θ) with parameters θ, and an observable O.
The featureencoding unitary for the DLP task is the same
as used in so that feature states take the following form for k = n −t log n for a constant t
related to noisy classiﬁcation (we refer the reader
to for a detailed description of classiﬁcation
under noise),
These states can be eﬃciently prepared on a
fault-tolerant quantum computer by a circuit that
uses the algorithm proposed by Shor in as a
subroutine. It was proven in that for all concepts fs the data points with labels 1 and −1,
respectively, can be separated by a hyperplane
with a large margin, and that this hyperplane always exists. The learning task of the PQC V (θ)
is then to ﬁnd this hyperplane. The hyperplanes
are normal to states of the form
for s′ ∈Z∗
A classiﬁer hs′(x) for these data
points can then be deﬁned as
if | ⟨φ(x)|φs′⟩|2/∆≥1/2
otherwise,
p−1 is the largest value the inner
product | ⟨φ(x)|φs′⟩|2 takes and is used to renormalize it to . The variational circuit is de-
ﬁned as V (θ) = ˆV (s′) which is similar in implementation to U(xi) with xi = gs′ and k ≈n/2,
and a measurement operator O = |0⊗n⟩⟨0⊗n|.
The simplest way of turning the DLP classiﬁcation task into an environment is to deﬁne one
episode as the agent being in a randomly chosen state corresponding to a training sample, performing an action which assigns the predicted label, and giving a reward of 1 (-1) for a correct
(incorrect) classiﬁcation. This family of environments is referred to as SL-DLP in . While the
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
family of SL-DLP environments is a straightforward way to generalize the results from to
policy learning, it lacks the characteristics typically associated with RL, namely a temporal
structure in the state transitions, such that these
depend on the actions taken by the agent.
construct a family of environments based on the
DLP which includes this kind of structure, 
introduce the family of Cliﬀwalk-DLP environments, inspired by the textbook Cliﬀwalk environment from . Here, the goal is still to assign correct labels to given states, but now these
states follow a randomly assigned but ﬁxed order.
The agent has to “walk along the edge of a cliﬀ",
where this edge is represented by the sequence
of ordered states the environment takes. A correct classiﬁcation leads to the next state in the
sequence, while an incorrect classiﬁcation leads
to “falling oﬀthe cliﬀ" and immediate episode
termination. The authors of show that the
quantum learnability results of the SL-DLP environment also hold for the family of Cliﬀwalk-
DLP environments. In the following section we
will generalize these results to Q-values by giving
a deﬁnition of the types of environments where
knowledge of an optimal policy lets us infer optimal Q-values.
Estimating optimal Q-values from optimal
In section 4.2, we revised how construct an
eﬃcient quantum agent that can achieve close-tooptimal policies in families of environments based
on the DLP. Now, we turn to generalizing their
results to the Q-learning setting.
The classical
hardness of the environment still holds irrespective of the learner that is used. The remaining
question is now whether there exists an eﬃcient
algorithm to obtain optimal Q-values, given we
have access to an optimal policy. Concretely, our
goal is to compute optimal Q-values Q∗(s, a) for
state-action pairs from an environment, where s
is given by the environment and a is determined
by the optimal policy.
One could imagine that Q∗(s, a) can be easily
estimated using Monte Carlo sampling since the
deﬁnition involves only the use of the optimal policy after the move (s, a) (cf. eq. (4)). However,
in general it is not possible for an agent to get to
arbitrary states s in poly time. We circumvent
this problem by considering special cases of environments that are classically hard, where there
are only two actions {a, a′}, and where the analytic values of Q∗(s, a) and Q∗(s, a′) are known.
The only unknown is which action a or a′ is the
optimal one. In this case it is clear that access to
the optimal policy resolves the question.
As an example of such an environment, consider the SL-DLP family of environments from
 . In each episode, the agent needs to classify
one random sample from a set of samples corresponding to the DLP classiﬁcation task from
section 4.1, where a correct (incorrect) classiﬁcation yields a reward of 1 (-1). If we set γ = 0,
the two possible Q-values for a given state and
the two possible actions are simply the rewards
corresponding to the result of the classiﬁcation.
To get the Q-value Q∗(s, a), we query the policy π∗(a|s) for the optimal action and assign the
reward for a correct classiﬁcation to the corresponding Q-value. (Note that we can also directly
infer Q∗(s, a′) for the wrong action a′ from this,
as there are only two distinct Q-values.)
can also be trivially extended to episodes with a
horizon greater than one and γ > 0. After querying the policy for the optimal action given the
initial state of the episode, the expected return is
computed directly assuming optimal actions until
the end of the episode is reached. I.e., we simply
Q∗(st, at) = Eπ∗
γkrt+k+1|st = s, at = a
for at given by the optimal policy, where all rewards are one from time step t onward. (For more
details on settings with longer horizon and a discount factor larger than zero, and an analytic expression of the Q-values in these cases, see ).
In more general cases, the issue of approximation reduces to the problem of reaching the desired state s eﬃciently. When this is possible (i.e.,
it is possible to construct environments which allow this without becoming easy to learn), then so
is estimating Q-values given an optimal policy.
Note that for all of the above, the same caveat
as in applies, namely that this method of
obtaining optimal Q-values does not resemble Qlearning in the sense that we use a tabular or
DQN-type approach as shown in section 2.2, and
it is still an open question whether a rigorous
quantum advantage can be shown in these settings for either policy-based RL or Q-learning.
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
cart position
pole angle
Figure 2: Gym environments solved by the quantum
a) Frozen Lake environment, where an agent
needs to learn to navigate from the top left of a grid to
retrieve the Frisbee at the bottom right without falling
into any of the holes (dark squares), b) Cart Pole environment, which consists of learning to balance a pole on
a cart which moves left and right on a frictionless track.
Numerical results
In this section, we present results for our PQC
model on two benchmark RL tasks from the OpenAI Gym , Frozen Lake v0 and Cart Pole
v0 (see ﬁg. 2).
We ran an extensive hyperparameter search for both environments, and
present our results for the best sets of hyperparameters. A detailed description of the hyperparameters we tuned and their best values can be
found in appendix B. Our experiments were run
with TensorFlow Quantum and Cirq , the
full code can be found on Github .
Frozen Lake
The Frozen Lake (FL) environment serves as an
example for environments with a simple, discrete
state space and with a reward structure that allows us to use an agent which performs measurements in the Z-basis to compute Q-values without
the need for trainable weights to scale the output range. It consists of a 4x4 grid representing
a frozen surface, where the agent can choose to
move one step up, down, left or right. The goal
is to cross the lake from the top left corner to the
bottom right corner where the goal is located.
However, some of the grid positions correspond
to holes in the ice, and when the agent steps on
them the episode terminates and it has to start
again from the initial state. In each episode, the
agent is allowed to take a maximum number of
steps mmax.
The episode terminates if one of
the following conditions is met: the agent performs mmax = 200 steps, reaches the goal, or
falls into a hole. For each episode in which the
goal is reached the agent receives a reward of 1,
and a reward of 0 otherwise. The environment
is considered solved when the agent reaches the
goal for 100 contiguous episodes. (See for full
environment speciﬁcation.)
As the FL environment is discrete and the dimensions of the state and action spaces are small,
there is no true notion of generalization in this environment, as all distinct state-action pairs are
likely observed during training.
On the other
hand, generalization to unseen state-action pairs
is one of the key reasons why function approximation was introduced to Q-learning. For this
reason, environments like Frozen Lake are not a
natural ﬁt for these types of algorithms and we
refrain from comparing to a classical function approximator. Note that we also refrain from comparing to the tabular approach, as this is (i) guaranteed to converge and (ii) not interesting beyond
environments with very limited state and action
spaces. However, this environment is interesting
from another perspective: there are only 64 Qvalues which we can compute exactly, and therefore we can directly compare the Q-values learned
by our model to the optimal Q-values Q∗, which
is not possible for the continuous-state Cart Pole
environment that we study in section 5.2.
show the diﬀerence between our agents’ Q-values
and the optimal Q-values during the course of
training in ﬁg. 3 b).
Additionally, the FL environment serves as a nice example for environments where a PQC with simple measurements
in the Z-basis can be used to solve a RL task,
without requiring additional post-processing, as
we describe below.
The FL environment has 16 states (one for
each square on the grid) of which four are holes
(marked as darker squares in ﬁg. 2 a), and 4 actions (top, down, left, right).
We encode each
position on the grid as one of the computational
basis states of a 4-qubit system, without use of
trainable input data weights or data re-uploading.
The optimal Q-values for each state-action pair
can be computed as Q∗(s, a) = γβ (cf. eq. (4)),
where β is the number of steps following the
shortest path to the goal from the state s′ that
the agent is in after the transition (s, a). We will
now motivate our choice of observables for the FL
agent by studying the range the optimal Q-values
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
(a) average scores
MAE(Q, Q * )
(b) mean absolute error with optimal Q-values
Figure 3: Agents with varying depth playing the Frozen Lake environment, and their closeness to the optimal Qvalues. The environment is solved when the agent reaches the goal (receives a score of 1) for 100 contiguous episodes.
a) Average score over 10 agents for circuits of depth 5, 10, and 15, respectively. All agents manage to solve the
environment, higher circuit depth leads to lower time to convergence. Shaded area shows standard deviation from
the mean. b) Mean absolute error between agents’ Q-values and the optimal Q-values Q∗for all (s, a) pairs over
time steps in episodes, where one time step corresponds to one transition in the environment. Shaded region shows
standard error of the mean.
can take. Note that these optimal Q-values are
deﬁned for the tabular case only, and serve as a
reference for the Q-values we want our Q-function
approximator to model. We know that only one
transition, that from state 14 to the goal state
15, is rewarded. This corresponds to a Q-value
Q∗(14, R) = γ. As the only other state adjacent
to the goal (state 11) is a hole, no other transition in this environment is rewarded. Through
the recursive Q-value update rule (see eq. (7)),
all other Q-values depend on Q∗(14, R), and are
smaller due to the discount factor and the zero
reward of all other transitions. In case of a function approximator, the Q-values may not be the
same as the optimal values, but the relationship
between Q(14, R) and all other Q-values still applies as the update rule in eq. (7) changes values
according to the observed reward and discounted
expected reward. That is, if the function approximator outputs values that match the range of
optimal Q-values and is not fundamentally limited in the updates that can be performed to it,
the relationship above can be replicated.
means that we have an upper bound on the range
of Q-values that we want to model which only
depends on γ ≤1 and stays constant over all
Therefore we do not expect that Qvalues need to become larger than γ for our agent
to solve the environment, and only become larger
in practice if the initialization of our model happens to yield higher values for some state-action
Motivated by this, we represent the Qvalues for the four actions as the expectation values of a measurement with the operator Zi for
each of the four qubits i ∈{1, . . . , 4}, which we
scale to lie between instead of [−1, 1]. Note
that even when parameter initialization yields Qvalues higher than the largest optimal Q-value,
they will still be close to this value as both optimal Q-values and those of our model are upperbounded by 1.
Figure 3 a) shows the average
scores of ten agents, each conﬁguration trained
with a circuit depth of 5, 10, and 15 layers, respectively. All agents manage to solve the environment, and the time to convergence decreases
as the number of layers increases.
Figure 3 b)
shows the averaged mean absolute error (MAE)
between the optimal Q-values and the Q-values
produced by the agents at each time step during
training. The agents trained on circuits of depth
15 reach the lowest values and converge earlier to
an average MAE that is roughly 0.05 lower than
that of the agents trained on a circuit of depth
5. This illustrates that as we increase the complexity of the function approximator, the optimal
Q-values can be more accurately modelled. How-
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
ever, the improvement between 10 and 15 layers
is relatively small compared to that between 5
and 10 layers, similar to a saturation in performance w.r.t. number of parameters found in classical deep RL .
We will study this type of
scaling behaviour more in-depth and compare it
to that of NNs in section 5.2. At the same time,
we see that producing optimal Q-values is not
necessary to solve an environment, as we argue
in section 2.2. In the following section, we study
an environment where we are not able to compute the optimal Q-values analytically due to the
continuous state space, but where we compare to
a classical approach to assess the quality of our
solution instead.
In the previous section, we have seen that for an
environment with discrete state space and a reward function that results in an upper bound of
Q-values of one, a simple PQC without enhanced
data encoding our readout strategies suﬃces to
solve the environment. Now we turn to an environment that is slightly more complex: the continuous state space necessitates a more evolved
data encoding strategy, while the reward function results in Q-values that far exceed the range
of a Z-basis measurement. In the Cart Pole v0 environment, an agent needs to learn to balance a
pole upright on a cart that moves on a frictionless
track. The action space consists of two actions:
moving the cart left and right. Its state space is
continuous and consists of the following variables:
cart position, cart velocity, pole angle, and pole
velocity at the tip. The cart position is bounded
between ±2.4, where values outside of this range
mean leaving the space that the environment is
deﬁned in and terminating the episode. The pole
angle is bounded between ±41.8°. The other two
variables can take inﬁnite values, but are bounded
in practice by how episode termination is deﬁned.
An episode terminates if the pole angle is outside
of ±12°, the cart position is outside of ±2.4, or
the agent reaches the maximum steps per episode
mmax = 200. For each step of the episode (including the terminal step) the agent receives a
reward of one. At the beginning of each episode,
the four variables of the environment state are
randomly initialized in a stable state within the
range [-0.05, 0.05]. The episode score is computed
as the cumulative reward of all steps taken in the
episode. The environment is solved when the average score of the last 100 episodes is ≥195. (See
 for full environment speciﬁcation.)
As in section 5.1, we now motive our choice
of observables depending on how rewards are received in this environment.
For this, we recall
that a Q-value gives us the expected return for a
given state-action pair,
Qπ(s, a) =
Cart Pole is an episodic environment with a maximum number of time steps H = 200 in the version
of the environment we study here, so the Q-value
following optimal policy π∗from a stable state s
Q∗(s, a) =
When following an arbitrary policy π and starting
in a random stable state of the environment, the
Q-value is
Qπ(s, a) =
where h ≤H is the length of the episode which is
determined by the policy. The longer the agent
balances the pole, the higher h, with h = H
the maximum number of steps allowed in an
episode. When not considering random actions
taken by the ϵ-greedy policy, h depends solely
on the performance of the agent, which changes
as the agent gets better at balancing the pole.
Consequently, the Q-values we want to approximate are lower bounded by the minimum number of steps it takes to make the episode terminate when always picking the wrong action
(i.e., the pole doesn’t immediately fall by taking
one false action alone), and upper bounded by
the Q-values assuming the optimal policy, where
h = H. We stress that this upper bound applies
to the optimal policy in one episode only, and
that in practice the upper bound of the magnitude of Q-values during training depends on the
performance of the agent as well as the number
of episodes played. Compared to the range of expectation values of computational basis measurements these values can become very high, e.g. for
γ = 0.99 we get max Q∗(s, a) ≈86. Even when
considering that Q-values need not necessarily be
close to the optimal values to solve an environment, the range given by computational basis
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
w/o data re-uploading
w/o trainable scaling
data re-uploading and
trainable scaling
(a) average scores with varying data encoding strategies
for best set of hyperparameters
w/o data re-uploading
w/o trainable scaling
data re-uploading and
trainable scaling
(b) average scores with varying data encoding strategies
for sub-optimal set of hyperparameters
Figure 4: Comparison of data-encoding strategies for the optimal and one sub-optimal set of hyperparameters for
agents training in the Cart Pole environment. The environment is solved when an agent has an average reward ≥195
for the past 100 episodes, after which training is stopped. Results are averaged over 10 agents each, where each
agent consists of 5 layers of the circuit architecture depicted in ﬁg. 1.
measurements is clearly too small compared to
the frequency with which rewards are given and
the number of episodes needed until convergence.
To give the agent the possibility to ﬂexibly adjust it’s output range, we add trainable weights
on the output values as described in section 3.2.
The Q-values now take the form
Q(s, a) = ⟨0⊗4| Uθ(s)†OaUθ(s) |0⊗4⟩+ 1
where Oa=L = Z1Z2 and Oa=R = Z3Z4 are
Pauli-ZZ operators on qubits (1, 2) and (3, 4) respectively, corresponding to actions left and right.
To further improve performance, we also use data
re-uploading and add trainable weights on the input values as described in section 3.1.
Comparison of data encoding and readout
strategies
To illustrate the eﬀect of data re-uploading and
trainable weights on the input and output values, we perform an ablation study and assess the
impact of each of these enhancements on learning performance. To illustrate that our proposed
architecture (i) performs better overall, and (ii)
is less sensitive to changes in hyperparameters,
we show results for the best set of hyperparameters that were found for a circuit of depth ﬁve,
as well as a sub-optimal set of hyperparameters
with which it is less easy for the agents to solve
the Cart Pole environment. The hyperparameters
we optimize over are: batch size, learning rates
and update frequencies of the Q-value-generating
model and the target model (cf. section 2.2) (see
appendix B for a detailed list of hyperparameter
settings). Otherwise, we only vary the hyperparameters of the enhancements we want to study.
The average performance of ten randomly initialized agents for each conﬁguration is presented in
ﬁg. 4 and ﬁg. 5. Once an agent solves the environment, we stop training and in the ﬁgures show
the last encountered score for each agent in the
averages (i.e., to form averages over equal lengths
of episodes, we assume that each agent continues
scoring the same value as it did in its last interaction with the environment).
Figure 4 a) and b) show the eﬀects of varying data encoding strategies.
While both data
re-uploading and trainable weights on the input values alone do not produce agents that
solve the environment in up to 5000 episodes
for both the best and sub-optimal set of hyperparameters, combining both of these enhancements yields agents that solve Cart Pole in 3000
and 600 episodes at most on average, respectively.
The fact that agents with trainable input weights and data re-uploading perform much
better than those without, emphasizes the importance of matching the PQC’s expressivity to the
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
trainable output weights
fixed range 
fixed range 
fixed range 
(a) average scores with varying output ranges for best set
of hyperparameters
trainable output weights
fixed range 
fixed range 
fixed range 
(b) average scores with varying output ranges for suboptimal set of hyperparameters
Figure 5: Comparison of diﬀerent readout strategies of the same agents as in ﬁg. 4 with the optimal and one
sub-optimal set of hyperparameters.
learning task at hand, as described in .
ﬁg. 5 a) and b), we compare agents with varying
output ranges.
Again, the green curves represent agents that are enhanced with a trainable
weight corresponding to each Q-value that lets
them ﬂexibly adjust their output range during
training, and these agents succeed with both sets
of the remaining hyperparameters.
The purple
curves show agents with a ﬁxed range of outputs
of , all of which stay at an extremely low
score during all 5000 episodes, as they fail to ﬁt a
good Q-function approximation regardless of hyperparameters.
The yellow curves show agents
with a ﬁxed output range of maximally 90, which
is motivated by the range of optimal Q-values.
These agents also solve the environment on average, however, they are much more sensitive to
parameter initialization and the remaining hyperparameters than agents with a trainable output
range. The low ﬁnal value of the yellow agents in
ﬁg. 5 a) is due to their last interaction with the
environment achieving a relatively low score on
As described above, the magnitude of Q-values
crucially depends on the agent’s ability to balance
the pole in each episode, and as a general trend it
will increase over the course of training for agents
that perform well. How large the ﬁnal Q-values
of a solving agent are therefore also depends on
the number of episodes it requires until convergence, so a range which is upper bounded by 90
presumes agents that converge relatively quickly.
Considering the range of ﬁnal Q-values of agents
in the green curves, they can become as high as
approximately 176 for agents that converge late.
However, as we see for agents with a ﬁxed output range of (magenta curves), increasing
the range to accommodate agents that converge
later can lead to complete failure depending on
the remaining hyperparameters.
Comparison to the classical DQN algorithm
In addition to investigating the eﬀects of varying
data encoding and readout strategies, we compare the performance of our PQC model to that
of the standard DQN algorithm that uses a NN
as a function approximator. We do this for varying numbers of parameters for both the PQC and
NN, and study how performance changes as the
number of parameters increases. Note that because environments are strictly deﬁned with a
ﬁxed number of input state variables, we cannot change the number of qubits arbitrarily for
a certain environment. Studying varying system
sizes in terms of qubits requires either artiﬁcially
adjusting the data encoding to ﬁt a certain number of qubits, or studying completely diﬀerent environments all together. Therefore we focus on
studying diﬀerent model sizes in terms of number of parameters here. Additionally, the standard approach to increase model performance in
supervised and unsupervised learning in the classical and quantum literature alike is often to add
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
(a) PQCs, labels show: #layers (#parameters)
(10, 10), 182
(15, 15), 347
(20, 20), 562
(24, 24), 770
(30, 30), 1142
(64, 64), 4610
(b) NNs, labels show: (#units in hidden layer 1, 2), #parameters
Figure 6: Comparison of classical and quantum agents with varying numbers of parameters in the Cart Pole environment. Each sub-ﬁgure contains results averaged over ten agents, and the vertical dashed line marks the average
number of episodes until solving the environment. We performed a hyperparameter optimization for each parameter conﬁguration separately, and show the best setting for each. (See appendix B for all settings and a list of
hyperparameters that were searched over.)
more parameters.
However, it has been shown
that this strategy does often not lead to success
in classical deep RL due to the instability of training larger networks . Instead, it is much more
important to ﬁnd good settings of hyperparameters (including the random initialization of model
parameters), and it is preferable to use models
which are less sensitive to changes in these settings.
To study whether this eﬀect is also present
when the function approximator is a PQC, we
compare agents with up to 30 layers of the hardware eﬃcient ansatz depicted in ﬁg. 1. All agents
use the enhancements which have shown to yield
good performance in ﬁg. 4 and ﬁg. 5, namely
data re-uploading and trainable input and output
weights. The other hyperparameters that yield
to the best performance for each depth are found
through an extensive hyperparameter search and
include the three diﬀerent learning rates (Qnetwork, input and output weights), batch size,
and update frequency of the Q-network and target network (see appendix B for detailed settings).
Figure 6 a) shows the average performance over 10 quantum agents of each conﬁguration. We indeed observe that increasing the number of parameters is only eﬃcient up to a certain
point, after which additional layers lead to slower
convergence. The best-performing conﬁguration
on average is a PQC with 25 layers and 302 parameters, which takes 500 episodes on average to
solve the Cart Pole environment.
To investigate the performance of the classical
DQN algorithm which uses a NN as the function
approximator, we compare NNs with two hidden
layers with varying numbers of units.
As simply increasing the depth of the NNs has not been
beneﬁcial in a RL setting, it has been proposed
to use shallow networks with increased width instead . Therefore we keep the depth of our
NNs ﬁxed at two, and vary the width by changing the number of units in each hidden layer. This
conﬁguration is also inspired by well-performing
agents on the oﬃcial OpenAi Gym leaderboard
 .3 We make the same observation for the NNs
in ﬁg. 6 b) as we did for the PQCs – increasing
the number of parameters does not necessarily
improve performance. The best-performing NN
is one with 20 units in each of its hidden layers,
which yields a network with 562 parameters overall that solves the Cart Pole environment in 250
episodes on average. Comparing the conﬁgurations of PQC and NN that perform best on aver-
3However, we note that it is hard to ﬁnd reliable benchmarks on the Cart Pole environment in classical literature,
as it was already too small to be considered in state-of-theart deep learning when the DQN algorithm was introduced
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
age, the best NN conﬁguration takes roughly half
as many episodes on average to solve Cart Pole
than the best PQC, and does this with roughly
twice as many parameters. Notably, the PQCs
seem to suﬀer more from an instability during
training as the number of parameters is increased
than the NNs do. We also show a comparison of
the best individual (not averaged) PQC and NN
agents in ﬁg. 7. Here, the gap is relatively small:
the best PQC (5 layers, 62 parameters) takes 206
episodes to solve Cart Pole, while the best NN (2
hidden layers with 30 units each, 1142 parameters) takes 186 episodes.
NN, (30, 30), 1142
PQC, 5 (62)
Figure 7: Best PQC and NN from the conﬁgurations
we study in ﬁg. 6. The best PQC (orange, 5 layers, 62
parameters) takes 20 episodes longer to solve Cart Pole
than the best NN (blue, two hidden layers with 30 units
each, 1142 parameters).
Finally, we note that unlike for the Frozen Lake
environment, it is not straightforward to compute
optimal Q-values for Cart Pole as its state space
is continuous. A trained model that is known to
implement the optimal policy (i.e., correct ordering of Q-values for all (s, a)-pairs) could be used
as a baseline to compare other models to, but
the magnitudes of Q-values can highly vary even
among agents that solve the environment so this
comparison will not provide much insight, which
is why we refrain from including it here. Nonetheless, we provide a visualization of the Q-values
learned by one of our best-performing quantum
models in appendix A. We observe that these Qvalues have a maximum value close to what we
expected from an optimal agent (i.e., 86).
Conclusion
In this work, we have proposed a quantum model
for deep Q-learning which can encode states of
environments with discrete and continuous state
We have illustrated the importance of
picking the observables of a quantum model such
that it can represent the range of optimal Qvalues that this algorithm should learn to approximate. One crucial diﬀerence between PQCs and
classical methods based on NNs, namely the former’s restricted range of output values deﬁned
by its measurement operators, was identiﬁed as
a major impediment to successfully perform Qlearning in certain types of environments. Based
on the range of optimal Q-values, we illustrate
how an informed choice can be made for the
quantum model’s observables. We also introduce
trainable weights on the observables of our model
to achieve a ﬂexible range of output values as
given by a NN and empirically show the bene-
ﬁt of this strategy on the Cart Pole environment
by performing ablation studies. Our results show
that a trainable output range can lead to better performance as well as lower sensitivity to
the choice of hyperparameters and random initialization of parameters of the model. We also
perform ablation studies on a number of data encoding techniques which enhance the expressivity of PQCs, namely data re-uploading and
trainable weights on the input . We show
the beneﬁt of combining both approaches in the
Cart Pole environment, where any of the two encoding strategies on its own does not suﬃce to
reliably solve the environment. Our results illustrate the importance of architectural choices for
QML models, especially for a RL algorithm as
Q-learning that has very speciﬁc demands on the
range of output values the model can produce.
Additionally, we investigate whether recent results in classical deep Q-learning also hold for
PQC-based Q-learning, namely that increasing
the number of parameters in a model might lead
to lower performance due to instability in training. To evaluate the performance of our model
compared to the classical approach where the
same DQN algorithm is used with a NN as the Qfunction approximator, we study the performance
of a number of classical and quantum models with
increasing numbers of parameters.
Our results
conﬁrm that PQC-based agents behave similarly
to their NN counterparts as the number of param-
Accepted in Quantum 2022-05-13, click title to verify. Published under CC-BY 4.0.
eters increases. Performance only increases up to
a certain point and then declines afterward. We
ﬁnd that in both cases, the hyperparameter settings (and in case of the PQC data encoding and
readout strategies) are the determining factors for
a model’s success much more than the number of
parameters. This is in contrast to previous results
for training PQCs on supervised and unsupervised learning tasks, where additional layers are
likely to increase performance . The
eﬀect that an increased number of parameters
hampers performance in Q-learning also seems to
be more prominent in PQCs than in NNs, which
raises the question whether we need additional
mechanisms to increase learning stability in this
setting than the ones from classical literature.
In addition to our numerical studies, we also
investigated whether a recent proof of quantum
advantage for policy gradient RL agents implies a separation of classical and quantum Qlearning agents as well.
We show how optimal
Q-values for state-action pairs can be eﬃciently
computed given access to an optimal policy in the
SL-DLP family of environments from .
explain additional requirements on the structure
of states in a given environment that need to be
fulﬁlled to allow eﬃciently inferring optimal Qvalues from optimal policies in more general environments. However, the separation results in
 only guarantee that quantum learners can be
constructed in general, and not that the optimal
policy can be learned by policy gradient methods
directly. It is an interesting open question if a
separation between classical and quantum agents
can also be proven for learning algorithms that
use policy gradient or Q-value updates as shown
in eq. (7). This opens up the path to future investigations of possible quantum advantages of these
types of quantum agents in relevant settings.
Acknowledgements
AS thanks Casper Gyurik for many valuable discussions over the course of this project. This work
was in part supported by the Dutch Research
Council (NWO/OCW), as part of the Quantum
Software Consortium program (project number
024.003.037). VD also acknowledges the support
by the project NEASQC funded from the European Union’s Horizon 2020 research and innovation programme (grant agreement No 951821),
and partial funding by an unrestricted gift from
Google Quantum AI. SJ acknowledges support
from the Austrian Science Fund (FWF) through
the projects DK-ALM:W1259-N27 and SFB BeyondC F7102. SJ also acknowledges the Austrian
Academy of Sciences as a recipient of the DOC
Fellowship.
AS acknowledges funding from the
European Union’s Horizon 2020 research and innovation programme under the Grant Agreement
No. 828826.