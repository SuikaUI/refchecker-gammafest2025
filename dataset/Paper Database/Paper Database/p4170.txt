Mach Learn 79: 151–175
DOI 10.1007/s10994-009-5152-4
A theory of learning from different domains
Shai Ben-David · John Blitzer · Koby Crammer ·
Alex Kulesza · Fernando Pereira ·
Jennifer Wortman Vaughan
Received: 28 February 2009 / Revised: 12 September 2009 / Accepted: 18 September 2009 /
Published online: 23 October 2009
© The Author(s) 2009. This article is published with open access at Springerlink.com
Abstract Discriminative learning methods for classiﬁcation perform well when training
and test data are drawn from the same distribution. Often, however, we have plentiful labeled
training data from a source domain but wish to learn a classiﬁer which performs well on
a target domain with a different distribution and little or no labeled training data. In this
work we investigate two questions. First, under what conditions can a classiﬁer trained from
source data be expected to perform well on target data? Second, given a small amount of
labeled target data, how should we combine it during training with the large amount of
labeled source data to achieve the lowest target error at test time?
Editors: Nicolo Cesa-Bianchi, David R. Hardoon, and Gayle Leen.
Preliminary versions of the work contained in this article appeared in Advances in Neural Information
Processing Systems .
S. Ben-David
David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada
e-mail: 
J. Blitzer ()
Department of Computer Science, UC Berkeley, Berkeley, CA, USA
e-mail: 
K. Crammer
Department of Electrical Engineering, The Technion, Haifa, Israel
e-mail: 
A. Kulesza
Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, USA
e-mail: 
F. Pereira
Google Research, Mountain View, CA, USA
e-mail: 
J.W. Vaughan
School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA
e-mail: 
Mach Learn 79: 151–175
We address the ﬁrst question by bounding a classiﬁer’s target error in terms of its source
error and the divergence between the two domains. We give a classiﬁer-induced divergence
measure that can be estimated from ﬁnite, unlabeled samples from the domains. Under the
assumption that there exists some hypothesis that performs well in both domains, we show
that this quantity together with the empirical source error characterize the target error of a
source-trained classiﬁer.
We answer the second question by bounding the target error of a model which minimizes
a convex combination of the empirical source and target errors. Previous theoretical work
has considered minimizing just the source error, just the target error, or weighting instances
from the two domains equally. We show how to choose the optimal combination of source
and target error as a function of the divergence, the sample sizes of both domains, and the
complexity of the hypothesis class. The resulting bound generalizes the previously studied
cases and is always at least as tight as a bound which considers minimizing only the target
error or an equal weighting of source and target errors.
Keywords Domain adaptation · Transfer learning · Learning theory · Sample-selection
1 Introduction
Most research in machine learning, both theoretical and empirical, assumes that models are
trained and tested using data drawn from some ﬁxed distribution. This single domain setting
has been well studied, and uniform convergence theory guarantees that a model’s empirical
training error is close to its true error under such assumptions. In many practical cases,
however, we wish to train a model in one or more source domains and then apply it to a
different target domain. For example, we might have a spam ﬁlter trained from a large email
collection received by a group of current users (the source domain) and wish to adapt it for
a new user (the target domain). Intuitively this should improve ﬁltering performance for the
new user, under the assumption that users generally agree on what is spam and what is not.
The challenge is that each user receives a unique distribution of email.
Many other examples arise in natural language processing. In general, labeled data for
tasks like part-of-speech tagging , parsing , information
extraction , and sentiment analysis are drawn from
a limited set of document types and genres in a given language due to availability, cost,
and speciﬁc goals of the project. However, useful applications for the trained systems may
involve documents of different types or genres. We can hope to successfully adapt the systems in these cases since parts-of-speech, syntactic structure, entity mentions, and positive
or negative sentiment are to a large extent stable across different domains, as they depend
on general properties of language.
In this work we investigate the problem of domain adaptation. We analyze a setting
in which we have plentiful labeled training data drawn from one or more source distributions but little or no labeled training data drawn from the target distribution of interest.
This work answers two main questions. First, under what conditions on the source and
target distributions can we expect to learn well? We give a bound on a classiﬁer’s target
domain error in terms of its source domain error and a divergence measure between the
two domains. In a distribution-free setting, we cannot obtain accurate estimates of common measures of divergence such as L1 or Kullback-Leibler from ﬁnite samples. Instead,
we show that when learning a hypothesis from a class of ﬁnite complexity, it is sufﬁcient to use a classiﬁer-induced divergence we call the HH-divergence 79: 151–175
Ben-David et al. 2006). Finite sample estimates of the HH-divergence converge uniformly
to the true HH-divergence, allowing us to estimate the domain divergence from unlabeled
data in both domains. Our ﬁnal bound on the target error is in terms of the empirical source
error, the empirical HH-divergence between unlabeled samples from the domains, and the
combined error of the best single hypothesis for both domains.
A second important question is how to learn when the large quantity of labeled source
data is augmented with a small amount of labeled target data, for example, when our new
email user has begun to manually mark a few received messages as spam. Given a source
domain S and a target domain T , we consider hypotheses h which minimize a convex combination of empirical source and target error (ˆϵT (h) and ˆϵS(h), respectively), which we refer
to as the empirical α-error:
αˆϵT (h) + (1 −α)ˆϵS(h).
Setting α involves trading off the ideal but small target dataset against the large (but less
relevant) source dataset. Baseline choices for α include α = 0 (using only source data) , α = 1 (using only target data), and the equal weighting of source and
target instances , setting α to the fraction of the instances that are from
the target domain. We give a bound on a classiﬁer’s target error in terms of its empirical α
error. The α that minimizes the bound depends on the divergence between the domains as
well as the size of the source and target training datasets. The optimal bound is always at
least as tight as the bounds using only source, only target, or equally-weighted source and
target instances. We show that for a real-world problem of sentiment classiﬁcation, nontrivial settings of α perform better than the three baseline settings.
In the next section, we give a brief overview of related work. We then specify precisely
our model of domain adaptation. Section 4 shows how to bound the target error of a hypothesis in terms of its source error and the source-target divergence. Section 5 gives our main
result, a bound on the target error of a classiﬁer which minimizes a convex combination of
empirical errors on the two domains, and in Sect. 6 we investigate the properties of the best
convex combination of that bound. In Sect. 7, we illustrate experimentally the above bounds
on sentiment classiﬁcation data. Section 8 describes how to extend the previous results to
the case of multiple data sources. Finally, we conclude with a brief discussion of future
directions for research in Sect. 9.
2 Related work
Crammer et al. introduced a PAC-style model of learning from multiple sources
in which the distribution over input points is assumed to be the same across sources but
each source may have its own deterministic labeling function. They derive bounds on the
target error of the function that minimizes the empirical error on (uniformly weighted) data
from any subset of the sources. As discussed in Sect. 8.2, the bounds that they derive are
equivalent to ours in certain restricted settings, but their theory is signiﬁcantly less general.
Daumé and Finkel suggest an empirically successful method for domain
adaptation based on multi-task learning. The crucial difference between our domain adaptation setting and analyses of multi-task methods is that multi-task bounds require labeled
data from each task, and make no attempt to exploit unlabeled data. Although these bounds
have a more limited scope than ours, they can sometimes yield useful results even when the
optimal predictors for each task are quite different .
Mach Learn 79: 151–175
Li and Bilmes give PAC-Bayesian learning bounds for adaptation using “divergence priors.” In particular, they place a source-centered prior on the parameters of a model
learned in the target domain. Like our model, the divergence prior emphasizes the tradeoff between source hypotheses trained on large (but biased) data sets and target hypotheses
trained from small (but unbiased) data sets. In our model, however, we measure the divergence (and consequently the bias) of the source domain from unlabeled data. This allows us
to choose a tradeoff parameter for source and target labeled data before training begins.
More recently, Mansour et al. introduced a theoretical model for the
“multiple source adaptation problem.” This model operates under assumptions very similar
to our multiple source analysis (Sect. 8), and we address their work in more detail there.
Finally, domain adaptation is closely related to the setting of sample selection bias . A well-studied variant of this is covariate shift, which has seen signiﬁcant work
in recent years . This line of
work leads to algorithms based on instance weighting, which have also been explored empirically in the machine learning and natural language processing communities . Our work differs from covariate shift primarily in two ways.
First, we do not assume the labeling rule is identical for the source and target data (although
there must exist some good labeling rule for both in order to achieve low error). Second,
our HH-divergence can be computed from ﬁnite samples of unlabeled data, allowing us
to directly estimate the error of a source-trained classiﬁer on the target domain.
A point of general contrast is that we work in an agnostic setting in which we do not
make strong assumptions about the data generation model, such as a speciﬁc relationship
between the source and target data distributions, which would be needed to obtain absolute
error bounds. Instead, we assume only that the samples from each of the two domains are
generated i.i.d. according to the respective data distributions, and as a result our bounds
must be relative to the error of some benchmark predictor rather than absolute, speciﬁcally,
relative to the combined error on both domains of an optimal joint predictor.
3 A rigorous model of domain adaptation
We formalize the problem of domain adaptation for binary classiﬁcation as follows. We
deﬁne a domain1 as a pair consisting of a distribution D on inputs X and a labeling function
f : X → , which can have a fractional (expected) value when labeling occurs nondeterministically. Initially, we consider two domains, a source domain and a target domain.
We denote by ⟨DS,fS⟩the source domain and ⟨DT ,fT ⟩the target domain.
A hypothesis is a function h : X →{0,1}. The probability according to the distribution
DS that a hypothesis h disagrees with a labeling function f (which can also be a hypothesis)
is deﬁned as
ϵS(h,f ) = Ex∼DS
|h(x) −f (x)|
When we want to refer to the source error (sometimes called risk) of a hypothesis, we use
the shorthand ϵS(h) = ϵS(h,fS). We write the empirical source error as ˆϵS(h). We use the
parallel notation ϵT (h,f ), ϵT (h), and ˆϵT (h) for the target domain.
1Note that this notion of domain is not the domain of a function. We always mean a speciﬁc distribution and
function pair when we say “domain.”
Mach Learn 79: 151–175
4 A bound relating the source and target error
We now proceed to develop bounds on the target domain generalization performance of a
classiﬁer trained in the source domain. We ﬁrst show how to bound the target error in terms
of the source error, the difference between labeling functions fS and fT , and the divergence
between the distributions DS and DT . Since we expect the labeling function difference to
be small in practice, we focus here on measuring distribution divergence, and especially on
how to estimate it with ﬁnite samples of unlabeled data from DS and DT . That is the role of
the H-divergence introduced in Sect. 4.1.
A natural measure of divergence for distributions is the L1 or variation divergence
d1(D,D′) = 2 sup
|PrD [B] −PrD′ [B]|,
where B is the set of measurable subsets under D and D′. We make use of this measure to
state an initial bound on the target error of a classiﬁer.
Theorem 1 For a hypothesis h,
ϵT (h) ≤ϵS(h) + d1(DS,DT )
|fS(x) −fT (x)|
|fS(x) −fT (x)|
Proof See Appendix.
In this bound, the ﬁrst term is the source error, which a training algorithm might seek
to minimize, and the third is the difference in labeling functions across the two domains,
which we expect to be small. The problem is the remaining term. Bounding the error in
terms of the L1 divergence between distributions has two disadvantages. First, it cannot be
accurately estimated from ﬁnite samples of arbitrary distributions and therefore has limited usefulness in practice. Second, for our purposes the L1
divergence is an overly strict measure that unnecessarily inﬂates the bound, since it involves
a supremum over all measurable subsets. We are only interested in the error of a hypothesis
from some class of ﬁnite complexity, thus we can restrict our attentions to the subsets on
which this type of hypothesis can commit errors. The divergence measure introduced in the
next section addresses both of these concerns.
4.1 The H-divergence
Deﬁnition 1 Given a domain X with D and D′ probability
distributions over X, let H be a hypothesis class on X and denote by I(h) the set for which
h ∈H is the characteristic function; that is, x ∈I(h) ⇔h(x) = 1. The H-divergence between D and D′ is
dH(D,D′) = 2 sup
|PrD [I(h)] −PrD′ [I(h)]|.
The H-divergence resolves both problems associated with the L1 divergence. First, for hypothesis classes H of ﬁnite VC dimension, the H-divergence can be estimated from ﬁnite
samples (see Lemma 1 below). Second, the H-divergence for any class H is never larger
than the L1 divergence, and is in general smaller when H has ﬁnite VC dimension.
Since it plays an important role in the rest of this work, we now state a slight modiﬁcation
of Theorem 3.4 of Kifer et al. as a lemma.
Mach Learn 79: 151–175
Lemma 1 Let H be a hypothesis space on X with VC dimension d. If U and U′ are samples
of size m from D and D′ respectively and ˆdH(U,U′) is the empirical H-divergence between
samples, then for any δ ∈(0,1), with probability at least 1 −δ,
dH(D,D′) ≤ˆdH(U,U′) + 4
d log(2m) + log( 2
Lemma 1 shows that the empirical H-divergence between two samples from distributions
D and D′ converges uniformly to the true H-divergence for hypothesis classes H of ﬁnite
VC dimension.
The next lemma shows that we can compute the H-divergence by ﬁnding a classiﬁer
which attempts to separate one domain from the other. Our basic plan of attack will be
as follows: Label each unlabeled source instance with 0 and unlabeled target instance as 1.
Then train a classiﬁer to discriminate between source and target instances. The H-divergence
is immediately computable from the error.
Lemma 2 For a symmetric hypothesis class H (one where for every h ∈H, the inverse
hypothesis 1 −h is also in H) and samples U, U′ of size m
ˆdH(U,U′) = 2
I [x ∈U] + 1
where I[x ∈U] is the binary indicator variable which is 1 when x ∈U.
Proof See Appendix.
This lemma leads directly to a procedure for computing the H-divergence. We ﬁrst ﬁnd
a hypothesis in H which has minimum error for the binary classiﬁcation problem of distinguishing source from target instances. The error of this hypothesis is related to the Hdivergence by Lemma 2. Of course, minimizing error for most reasonable hypothesis classes
is a computationally intractable problem. Nonetheless, as we shall see in Sect. 7, the error of
hypotheses trained to minimize convex upper bounds on error are useful in approximating
the H-divergence.
4.2 Bounding the difference in error using the H-divergence
The H-divergence allows us to estimate divergence from unlabeled data, but in order to use
it in a bound we must have tools to represent error relative to other hypotheses in our class.
We introduce two new deﬁnitions.
Deﬁnition 2 The ideal joint hypothesis is the hypothesis which minimizes the combined
h∗= argmin
ϵS(h) + ϵT (h).
We denote the combined error of the ideal hypothesis by
λ = ϵS(h∗) + ϵT (h∗).
Mach Learn 79: 151–175
The ideal joint hypothesis explicitly embodies our notion of adaptability. When this hypothesis performs poorly, we cannot expect to learn a good target classiﬁer by minimizing
source error. On the other hand, we will show that if the ideal joint hypothesis performs
well, we can measure adaptability of a source-trained classiﬁer by using the H-divergence
between the marginal distributions DS and DT .
Next we deﬁne the symmetric difference hypothesis space HH for a hypothesis
space H, which will be very useful in reasoning about error.
Deﬁnition 3 For a hypothesis space H, the symmetric difference hypothesis space HH is
the set of hypotheses
g(x) = h(x) ⊕h′(x)
for some h,h′ ∈H,
where ⊕is the XOR function. In words, every hypothesis g ∈HH is the set of disagreements between two hypotheses in H.
The following simple lemma shows how we can make use of the HH-divergence in
bounding the error of our hypothesis.
Lemma 3 For any hypotheses h,h′ ∈H,
|ϵS(h,h′) −ϵT (h,h′)| ≤1
2dHH(DS,DT ).
Proof By the deﬁnition of HH-distance,
dHH(DS,DT ) = 2 sup
h(x) ̸= h′(x)
h(x) ̸= h′(x)
|ϵS(h,h′) −ϵT (h,h′)| ≥2|ϵS(h,h′) −ϵT (h,h′)|.
We are now ready to give a bound on target error in terms of the new divergence measure
we have deﬁned.
Theorem 2 Let H be a hypothesis space of VC dimension d. If US, UT are unlabeled samples of size m′ each, drawn from DS and DT respectively, then for any δ ∈(0,1), with
probability at least 1 −δ (over the choice of the samples), for every h ∈H:
ϵT (h) ≤ϵS(h) + 1
ˆdHH(US,UT ) + 4
2d log(2m′) + log , which implies that for any labeling functions f1,
f2, and f3, we have ϵ(f1,f2) ≤ϵ(f1,f3) + ϵ(f2,f3). Then
ϵT (h) ≤ϵT (h∗) + ϵT (h,h∗)
≤ϵT (h∗) + ϵS(h,h∗) +
ϵT (h,h∗) −ϵS(h,h∗)
≤ϵT (h∗) + ϵS(h,h∗) + 1
2dHH(DS,DT )
Mach Learn 79: 151–175
≤ϵT (h∗) + ϵS(h) + ϵS(h∗) + 1
2dHH(DS,DT )
= ϵS(h) + 1
2dHH(DS,DT ) + λ
≤ϵS(h) + 1
ˆdHH(US,UT ) + 4
2d log(2m′) + log( 2
The last step is an application of Lemma 1, together with the observation that since we can
represent every g ∈HH as a linear threshold network of depth 2 with 2 hidden units, the
VC dimension of HH is at most twice the VC dimension of H (Anthony and Bartlett
The bound in Theorem 2 is relative to λ, and we brieﬂy comment that the form
λ = ϵS(h∗) + ϵT (h∗) comes from the use of the triangle inequality for classiﬁcation error.
Other losses result in other forms for this bound . When the combined
error of the ideal joint hypothesis is large, then there is no classiﬁer that performs well on
both the source and target domains, so we cannot hope to ﬁnd a good target hypothesis by
training only on the source domain. On the other hand, for small λ (the most relevant case
for domain adaptation), the bound shows that source error and unlabeled HH-divergence
are important quantities in computing the target error.
5 A learning bound combining source and target training data
Theorem 2 shows how to relate source and target error. We now proceed to give a learning
bound for empirical risk minimization using combined source and target training data.
At train time a learner receives a sample S = (ST ,SS) of m instances, where ST consists
of βm instances drawn independently from DT and SS consists of (1−β)m instances drawn
independently from DS. The goal of a learner is to ﬁnd a hypothesis that minimizes target
error ϵT (h). When β is small, as in domain adaptation, minimizing empirical target error
may not be the best choice. We analyze learners that instead minimize a convex combination
of empirical source and target error,
ˆϵα(h) = αˆϵT (h) + (1 −α)ˆϵS(h),
for some α ∈ . We denote as ϵα(h) the corresponding weighted combination of true
source and target errors, measured with respect to DS and DT .
We bound the target error of a domain adaptation algorithm that minimizes ˆϵα(h). The
proof of the bound has two main components, which we state as lemmas below. First we
bound the difference between the target error ϵT (h) and weighted error ϵα(h). Then we
bound the difference between the true and empirical weighted errors ϵα(h) and ˆϵα(h).
Lemma 4 Let h be a hypothesis in class H. Then
|ϵα(h) −ϵT (h)| ≤(1 −α)
2dHH(DS,DT ) + λ
Proof See Appendix.
Mach Learn 79: 151–175
The lemma shows that as α approaches 1, we rely increasingly on the target data, and
the distance between domains matters less and less. The uniform convergence bound on the
α-error is nearly identical to the standard uniform convergence bound for hypothesis classes
of ﬁnite VC dimension , only with target and
source errors weighted differently. The key part of the proof relies on a slight modiﬁcation
of Hoeffding’s inequality for our setup, which we state here:
Lemma 5 For a ﬁxed hypothesis h, if a random labeled sample of size m is generated by
drawing βm points from DT and (1 −β)m points from DS, and labeling them according
to fS and fT respectively, then for any δ ∈(0,1), with probability at least 1 −δ (over the
choice of the samples),
|ˆϵα(h) −ϵα(h)| ≥ϵ
β + (1−α)2
Before giving the proof, we ﬁrst restate Hoeffding’s inequality for completeness.
Proposition 1 (Hoeffding’s inequality) If X1,...,Xn are independent random variables
with ai ≤Xi ≤bi for all i, then for any ϵ > 0,
| ¯X −E[ ¯X]| ≥ϵ
≤2e−2n2ϵ2/
i=1(bi−ai)2,
where ¯X = (X1 + ··· + Xn)/n.
We are now ready to prove the lemma.
Proof (Lemma 5) Let X1,...,Xβm be random variables that take on the values
β |h(x) −fT (x)|
for the βm instances x ∈ST . Similarly, let Xβm+1,...,Xm be random variables that take on
the values
1 −β |h(x) −fS(x)|
for the (1 −β)m instances x ∈SS. Note that X1,...,Xβm ∈[0,α/β] and Xβm+1,...,Xm ∈
[0,(1 −α)/(1 −β)]. Then
ˆϵα(h) = αˆϵT (h) + (1 −α)ˆϵS(h)
|h(x) −fT (x)| + (1 −α)
|h(x) −fS(x)| = 1
Furthermore, by linearity of expectations
E[ˆϵα(h)] = 1
β ϵT (h) + (1 −β)m1 −α
1 −β ϵS(h))
= αϵT (h) + (1 −α)ϵS(h) = ϵα(h).
Mach Learn 79: 151–175
So by Hoeffding’s inequality the following holds for every h.
|ˆϵα(h) −ϵα(h)| ≥ϵ
i=1 range2(Xi)
β )2 + (1 −β)m( 1−α
β + (1−α)2
This lemma shows that as α moves away from β (where each instance is weighted
equally), our ﬁnite sample approximation to ϵα(h) becomes less reliable. We can now move
on to the main theorem of this section.
Theorem 3 Let H be a hypothesis space of VC dimension d. Let US and UT be unlabeled
samples of size m′ each, drawn from DS and DT respectively. Let S be a labeled sample of
size m generated by drawing βm points from DT and (1 −β)m points from DS and labeling
them according to fS and fT , respectively. If ˆh ∈H is the empirical minimizer of ˆϵα(h)
on S and h∗
T = minh∈H ϵT (h) is the target error minimizer, then for any δ ∈(0,1), with
probability at least 1 −δ (over the choice of the samples),
ϵT (ˆh) ≤ϵT (h∗
β + (1 −α)2
2d log(2(m + 1)) + 2log( 8
ˆdHH(US,UT ) + 4
2d log(2m′) + log , using Lemma 4 to bound the difference between target and weighted errors
and Lemma 5 for the uniform convergence of empirical and true weighted errors. The full
proof is in Appendix.
When α = 0 (that is, we ignore target data), the bound is identical to that of Theorem 2,
but with an empirical estimate for the source error. Similarly when α = 1 (that is, we use
only target data), the bound is the standard learning bound using only target data. At the
optimal α (which minimizes the right hand side), the bound is always at least as tight as
either of these two settings. Finally note that by choosing different values of α, the bound
allows us to effectively trade off the small amount of target data against the large amount of
less relevant source data.
We remark that when it is known that λ = 0, the dependence on m in Theorem 3 can be
improved; this corresponds to the restricted or realizable setting.
6 Optimal mixing value
We examine now the bound of Theorem 3 in more detail to illustrate some interesting properties. Writing the bound as a function of α and omitting additive constants, we obtain
f (α) = 2B
β + (1 −α)2
+ 2(1 −α)A,
Mach Learn 79: 151–175
ˆdHH(US,UT ) + 4
2d log(2m′) + log( 4
is the total divergence between source and target, and
2d log(2(m + 1)) + 2log( 8
is the complexity term, which is approximately √d/m. The optimal value α∗is a function of
the number of target examples mT = βm, the number of source examples mS = (1 −β)m,
and the ratio D =
α∗(mT ,mS;D) =
D2(mS + mT ) −mSmT
Several observations follow from this analysis. First, if mT = 0 (β = 0) then α∗= 0 and
if mS = 0 (β = 1) then α∗= 1. That is, if we have only source or only target data, the best
combination is to use exactly what we have. Second, if we are certain that the source and
target are the same, that is if A = 0 (or D →∞), then α∗= β, that is, the optimal combination is to use the training data with uniform weighting of the examples across all examples,
as in Crammer et al. , who always enforce such a uniform weighing. Finally, two
phase transitions occur in the value of α∗. First, if there are enough target data (speciﬁcally,
if mT ≥D2 = d/A2) then no source data are needed, and in fact using any source data will
yield suboptimal performance. This is because the possible reduction in error due to additional source data is always less than the increase in error caused by the source data being
too far from the target data. Second, even if there are few target examples, it might be the
case that we do not have enough source data to justify using it, and this small amount of
source data should be ignored. Once we have enough source data then we get a non-trivial
value for α∗.
These two phase transitions are illustrated in Fig. 1. The intensity of a point reﬂects the
value α∗and ranges from 0 (white) to 1 (black). In this plot α∗is a function of mS (x axis)
and mT (y axis), and we ﬁx the complexity to d = 1,601 and the divergence between source
and target to A = 0.715. We chose these values to correspond more closely to real data (see
Sect. 7). Observe ﬁrst that D2 = 1,601/(0.715)2 ≈3,130. When mT ≥D2, the ﬁrst case of
(2) predicts that α∗= 1 for all values of mS, which is illustrated by the black region above
the line mT = 3,130. Furthermore, ﬁxing the value of mT ≤3,130, the second case of (2)
predicts that α∗will be either one (1) if mS is small enough, or go smoothly to zero as mS
increases. This is illustrated by any horizontal line with mT ≤3,130. Each such line is black
for small values of mS and then gradually becomes white as mS increases (left to right).
7 Results on sentiment classiﬁcation
In this section we illustrate our theory on the natural language processing task of sentiment
classiﬁcation . The point of these experiments is not to instantiate the
Mach Learn 79: 151–175
Fig. 1 An illustration of the phase transition in the balance between source and target training data. The
value of α minimizing the bound is indicated by the intensity, where black means α = 1. We ﬁx d = 1,601
and A = 0.715, approximating the empirical setup in Fig. 3. The x-axis shows the number of source instances (log-scale). The y-axis shows the number of target instances. A phase transition occurs at 3,130
target instances. With more target instances than this, it is more effective to ignore even an inﬁnite amount of
source data
bound from Theorem 3 directly, since the amount of data we use here is much too small
for the bound to yield meaningful numerical results. Instead, we show how the two main
principles of our theory from Sects. 4 and 5 can be applied on a real-world problem. First,
we show that an approximation to the H-distance, obtained by training a linear model to discriminate between instances from different domains, correlates well with the loss incurred
by training in one domain and testing in another. Second, we investigate minimizing the
α-error as suggested by Theorem 3. We show that the optimal value of α for a given amount
of source and target data is closely related to our approximate H-distance.
The next subsection describes the problem of sentiment classiﬁcation, along with our
dataset, features, and learning algorithms. Then we show experimentally how our approximate H-distance is related to the adaptation performance and the optimal value of α.
7.1 Sentiment classiﬁcation
Given a piece of text (usually a review or essay), automatic sentiment classiﬁcation is the
task of determining whether the sentiment expressed by the text is positive or negative . While movie reviews are the most commonly studied domain,
sentiment analysis has been extended to a number of new domains, ranging from stock
message boards to congressional ﬂoor debates .
Research results have been deployed industrially in systems that gauge market reaction and
summarize opinion from Web pages, discussion boards, and blogs.
We used the publicly available data set from to examine our theory.2
The data set consists of reviews from the Amazon website for several different types of
products. We chose reviews from the domains apparel, books, DVDs, kitchen & housewares,
and electronics. Each review consists of a rating (1–5 stars), a title, and review text. We
created a binary classiﬁcation problem by binning reviews with 1–2 stars as “negative” and
4–5 stars as “positive”. Reviews with 3 stars were discarded.
Classifying product reviews as having either positive or negative sentiment ﬁts well into
our theory of domain adaptation. We note that reviews for different products have widely
2Available at 
Mach Learn 79: 151–175
Positive books review
Negative books review
Title: A great ﬁnd during an annual summer shopping trip
Review: I found this novel at a bookstore on the boardwalk I visit every summer....The narrative was brilliantly told,
the dialogue completely believable and the
plot totally heartwrenching. If I had made
it to the end without some tears, I would
believe myself made of stone!
Title: The Hard Way
Review: I am not sure whatever possessed me to buy this book. Honestly,
it was a complete waste of my time. To
quote a friend, it was not the best use
of my entertainment dollar. If you are
a fan of pedestrian writing, lack-luster
plots and hackneyed character development, this is your book.
Positive kitchen & housewares review
Negative kitchen & housewares review
Title: no more doggy feuds with neighbor
Review: i absolutely love this product. my
neighbor has four little yippers and my
shepard/chow mix was antagonized by the
yipping on our side of the fence. I hung
the device on my side of the fence and the
noise keeps the neighbors dog from picking “arguments” with my dog. all barking
and ﬁghting has ceased.
Title: cooks great, lid does not work
Review: I Love the way the Tefal deep
fryer cooks, however, I am returning my
second one due to a defective lid closure. The lid may close initially, but after a few uses it no longer stays closed.
Since I have small children in my home,
I will not be purchasing this one again.
Fig. 2 Some sample product reviews for sentiment classiﬁcation. The top row shows reviews from the books
domain. The bottom row shows reviews from kitchen & housewares
different vocabularies, so classiﬁers trained on one domain are likely to miss out on important lexical cues in a different domain. On the other hand, a single good universal sentiment
classiﬁer is likely to exist—namely the classiﬁer that assigns high positive weight to all positive words and high negative weight to all negative words, regardless of product type. We
illustrate the type of text in this dataset in Fig. 2, which shows one positive and one negative
review each from the domains books and kitchen & housewares.
For each domain, the data set contains 1,600 labeled documents and between 5,000 and
6,000 unlabeled documents. We follow Pang et al. and represent each instance (review) by a sparse vector containing the counts of its unigrams and bigrams, and we normalize the vectors in L1. Finally, we discard all but the most frequent 1,600 unigrams and bigrams from each data set. In all of the learning problems of the next section, including those
that require us to estimate an approximate H-distance, we use signed linear classiﬁers. To
estimate the parameters of these classiﬁers, we minimize a Huber loss with stochastic gradient descent .
7.2 Experiments
We explore Theorem 3 further by comparing its predictions to the predictions of an approximation that can be computed from ﬁnite labeled source and unlabeled source and target
samples. As we shall see, our approximation is a ﬁnite-sample analog of (1). We ﬁrst address λ, the error of the ideal hypothesis. Unfortunately, in general we cannot assume any
relationship between the labeling functions fS and fT . Thus in order to estimate λ, we must
Mach Learn 79: 151–175
estimate ϵT (h∗) independently of the source data. If we had enough target data to do this
accurately, we would not need to adapt a source classiﬁer in the ﬁrst place. For our sentiment task, however, λ is small enough to be a negligible term in the bound. Thus we ignore
We approximate the divergence between two domains by training a linear classiﬁer to
discriminate between unlabeled instances from the source and target domains. Then we apply Lemma 2 to get an estimate of ˆdH that we denote by ζ(US,UT ). ζ(US,UT ) is a lower
bound on ˆdH, which is in turn a lower bound on ˆdHH. For Theorem 3 to be valid, we
need an upper bound on ˆdHH. Unfortunately, this is computationally intractable for linear
threshold classiﬁers, since ﬁnding a minimum error classiﬁer is hard in general . We chose our ζ(US,UT ) estimate because it requires no new machinery beyond
an algorithm for empirical risk minimization on H. Finally, we note that the unlabeled sample size m′ is large, so we leave out the ﬁnite sample error term for the HH-divergence.
We set C to be 1,601, the VC dimension of a 1,600-dimensional linear classiﬁer and ignore
the logm term in the numerator of the bound. The complete approximation to the bound is
β + (1 −α)2
+ (1 −α)ζ(US,UT ).
m in (3) corresponds to B from (1), and ζ(US,UT ) is a ﬁnite sample approximation to A when λ is negligible and we have large unlabeled samples from both the source
and target domains.
We compare (3) to experimental results for the sentiment classiﬁcation task. All of our
experiments use the apparel domain as the target. We obtain empirical curves for the error
Fig. 3 Comparing the bound from Theorem 3 with test error for sentiment classiﬁcation. Each column varies
one component of the bound. For all plots, the y-axis shows the error and the x-axis shows α. Plots on the top
row show the value given by our approximation to the bound, and plots on the bottom row show the empirical
test set error. Column (a) depicts different distances among domains. Column (b) depicts different numbers
of target instances, and column (c) represents different numbers of source instances
Mach Learn 79: 151–175
as a function of α by training a classiﬁer using a weighted hinge loss. Suppose the target
domain has weight α and there are βm target training instances. Then we scale the loss of
target training instance by α/β and the loss of a source training instance by (1−α)/(1−β).
Figure 3 shows a series of plots of (3) (top row) coupled with corresponding plots of test
error (bottom row) as a function of α for different amounts of source and target data and
different distances between domains. In each column, a single parameter (distance, number
of target instances mT , or number of source instances mS) is varied while the other two are
held constant. Note that β = mT /(mT +mS). The plots on the top row of Fig. 3 are not meant
to be numerical proxies for the true error. (For the source domains “books” and “dvd”, the
distance alone is well above 1/2.) However, they illustrate that the bound is similar in shape
to the true error curve and that important relationships are preserved.
Note that in every pair of plots, the empirical error curves, like the bounds, have an
essentially convex shape. Furthermore, the value of α that minimizes the bound also yields
low empirical error in each case. This suggests that choosing α to minimize the bound of
Theorem 3 and subsequently training a classiﬁer to minimize the empirical error ˆϵα(h) can
work well in practice, provided we have a reasonable measure of complexity and λ is small.
Column (a) shows that more distant source domains result in higher target error. Column (b)
illustrates that for more target data, we have not only lower error in general, but also a higher
minimizing α. Finally, column (c) demonstrates the limitations of distant source data.
When enough labeled target data exists, we always prefer to use only the target data,
no matter how much source data is available. Intuitively this is because any biased source
domain cannot help to reduce error beyond some positive constant. When the target data
alone is sufﬁcient to surpass this level of performance, the source data ceases to be useful.
Thus column (c) illustrates empirically one phase transition we discuss in Sect. 6.
8 Combining data from multiple sources
We now explore an extension of our theory to the case of multiple source domains. In
this setting, the learner is presented with data from N distinct sources. Each source Sj is
associated with an unknown distribution Dj over input points and an unknown labeling
function fj. The learner receives a total of m labeled samples, with mj = βjm from each
source Sj, and the objective is to use these samples to train a model to perform well on
a target domain ⟨DT ,fT ⟩, which may or may not be one of the sources. This setting is
motivated by several domain adaptation algorithms that weigh the loss from training instances depending on how “far” they are from the target domain. That is, each training instance is its own
source domain.
As before, we examine algorithms that minimize convex combinations of training error
over the labeled examples from each source domain. Given a vector α = (α1,...,αN) of
domain weights with
j=1 αj = 1, we deﬁne the empirical α-weighted error of function h
αj ˆϵj(h) =
|h(x) −fj(x)|.
The true α-weighted error ϵα(h) is deﬁned analogously. We use Dα to denote the mixture of
the N source distributions with mixing weights equal to the components of α.
We present in turn two alternative generalizations of the bounds in Sect. 5. The ﬁrst
bound considers the quality and quantity of data available from each source individually,
Mach Learn 79: 151–175
ignoring the relationships between sources. In contrast, the second bound depends directly
on the HH-distance between the target domain and the weighted combination of source
domains. This dependence allows us to achieve signiﬁcantly tighter bounds when there exists a mixture of sources that approximates the target better than any single source. Both
results require the derivation of uniform convergence bounds for the empirical α-error. We
begin with those.
8.1 Uniform convergence
The following lemma provides a uniform convergence bound for the empirical α-error.
For each j ∈{1,...,N}, let Sj be a labeled sample of size βjm generated by
drawing βjm points from Dj and labeling them according to fj. For any ﬁxed weight vector
α, let ˆϵα(h) be the empirical α-weighted error of some ﬁxed hypothesis h on this sample,
and let ϵα(h) be the true α-weighted error. Then for any δ ∈(0,1), with probability at least
|ˆϵα(h) −ϵα(h)| ≥ϵ
Proof See Appendix.
Note that this bound is minimized when αj = βj for all j. In other words, convergence
is fastest when all data instances are weighted equally.
8.2 A bound using pairwise divergence
The ﬁrst bound we present considers the pairwise HH-distance between each source and
the target, and illustrates the trade-off that exists between minimizing the average divergence
of the training data from the target and weighting all points equally to encourage faster
convergence. The term
j=1 αjλj that appears in this bound plays a role corresponding to
λ in the previous section. Somewhat surprisingly, this term can be small even when there is
not a single hypothesis that works well for all heavily weighted sources.
Theorem 4 Let H be a hypothesis space of VC dimension d. For each j ∈{1,...,N},
let Sj be a labeled sample of size βjm generated by drawing βjm points from Dj and
labeling them according to fj. If ˆh ∈H is the empirical minimizer of ˆϵα(h) for a ﬁxed
weight vector α on these samples and h∗
T = minh∈H ϵT (h) is the target error minimizer, then
for any δ ∈(0,1), with probability at least 1 −δ,
ϵT (ˆh) ≤ϵT (h∗
d log(2m) −log(δ)
2λj + dHH(Dj,DT )
where λj = minh∈H{ϵT (h) + ϵj(h)}.
Mach Learn 79: 151–175
Proof See Appendix.
In the special case where the HH-divergence between each source and the target is 0
and all data instances are weighted equally, the bound in Theorem 4 becomes
ϵT (ˆh) ≤ϵT (h∗
2d log(2(m + 1)) + 2log . Aside from the constants in the complexity term, the only difference is that the quantity λi that appears here is replaced by an alternate measure of the label
error between source Sj and the target. Furthermore, these measures are equivalent when
the true target function is a member of H. However, the bound of Crammer et al. 
is less general. In particular, it does not handle positive HH-divergence or non-uniform
weighting of the data.
8.3 A bound using combined divergence
In the previous bound, divergence between domains is measured only on pairs, so it is not
necessary to have a single hypothesis that is good for every source domain. However, this
bound does not give us the ﬂexibility to take advantage of domain structure when calculating unlabeled divergence. The alternate bound given in Theorem 5 allows us to effectively
alter the source distribution by changing α. This has two consequences. First, we must now
demand that there exists a hypothesis h∗which has low error on both the α-weighted convex combination of sources and the target domain. Second, we measure HH-divergence
between the target and a mixture of sources, rather than between the target and each single
Theorem 5 Let H be a hypothesis space of VC dimension d. For each j ∈{1,...,N},
let Sj be a labeled sample of size βjm generated by drawing βjm points from Dj and
labeling them according to fj. If ˆh ∈H is the empirical minimizer of ˆϵα(h) for a ﬁxed
weight vector α on these samples and h∗
T = minh∈H ϵT (h) is the target error minimizer, then
for any δ ∈(0,1), with probability at least 1 −δ,
ϵT (ˆh) ≤ϵT (h∗
d log(2m) −log(δ)
+ 2γα + dHH(Dα,DT ),
where γα = minh{ϵT (h) + ϵα(h)} = minh{ϵT (h) +
j=1 αjϵj(h)}.
Proof See Appendix.
Theorem 5 reduces to Theorem 3 when N = 2 and one of the two source domains is the
target domain (that is, we have some small number of target instances).
Mach Learn 79: 151–175
8.4 Discussion
One might ask whether there exist settings where a non-uniform weighting can lead to a
signiﬁcantly lower value of the bound than a uniform weighting. Indeed, this can happen
if some non-uniform weighting of sources accurately approximates the target distribution.
This is true, for example, in the setting studied by Mansour et al. , who derive
results for combining pre-computed hypotheses. In particular, they show that for arbitrary
convex losses, if the Rényi divergence between the target and a mixture of sources is small,
it is possible to combine low-error source hypotheses to create a low-error target hypothesis.
They then show that if for each domain j there exists a hypothesis hj with error less than ϵ,
it is possible to achieve an error less than ϵ on the target by weighting the predictions of
h1,...,hN appropriately.
The Rényi divergence is not directly comparable to the HH-divergence in general;
however it is possible to exhibit source and target distributions which have low HHdivergence and high (even inﬁnite) Rényi divergence. For example, the Rényi divergence
is inﬁnite when the source and target distributions do not share support, but the HHdivergence is only large when these regions of differing support also coincide with classiﬁer
disagreement regions. On the other hand, we require that a single hypothesis be trained on
the mixture of sources. Mansour et al. give algorithms which do not require
the original training data at all, but only a single hypothesis from each source.
9 Conclusion
We presented a theoretical investigation of the task of domain adaptation, a task in which
we have a large amount of training data from a source domain, but we wish to apply a
model in a target domain with a much smaller amount of training data. Our main result is a
uniform convergence learning bound for algorithms which minimize convex combinations
of empirical source and target error. Our bound reﬂects the trade-off between the size of the
source data and the accuracy of the target data, and we give a simple approximation to it that
is computable from ﬁnite labeled and unlabeled samples. This approximation makes correct
predictions about model test error for a sentiment classiﬁcation task. Our theory also extends
in a straightforward manner to a multi-source setting, which we believe helps to explain the
success of recent empirical work in domain adaptation.
There are two interesting open problems that deserve future exploration. First, our bounds
on the divergence between source and target distribution are in terms of VC dimension. We
do not yet know whether our divergence measure admits tighter data-dependent bounds
 , or if there are other, more appropriate
divergence measures which do. Second, it would be interesting to investigate algorithms
that choose a convex combination of multiple sources to minimize the bound in Theorem 5
as possible approaches to adaptation from multiple sources.
Acknowledgements
This material is based upon work partially supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. NBCHD030010 (CALO), by the National Science
Foundation under grants ITR 0428193 and RI 0803256, and by a gift from Google, Inc. to the University of
Pennsylvania. Koby Crammer is a Horev fellow, supported by the Taub Foundations. Any opinions, ﬁndings,
and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views of the DARPA, Department of Interior-National Business Center (DOI-NBC), NSF, the
Taub Foundations, or Google, Inc.
Mach Learn 79: 151–175
Open Access
This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided
the original author(s) and source are credited.
Appendix: Proofs
Theorem 1 For a hypothesis h,
ϵT (h) ≤ϵS(h) + d1(DS,DT )
|fS(x) −fT (x)|
|fS(x) −fT (x)|
Proof Recall that ϵT (h) = ϵT (h,fT ) and ϵS(h) = ϵS(h,fS). Let φS and φT be the density
functions of DS and DT respectively.
ϵT (h) = ϵT (h) + ϵS(h) −ϵS(h) + ϵS(h,fT ) −ϵS(h,fT )
≤ϵS(h) + |ϵS(h,fT ) −ϵS(h,fS)| + |ϵT (h,fT ) −ϵS(h,fT )|
≤ϵS(h) + EDS
|fS(x) −fT (x)|
+ |ϵT (h,fT ) −ϵS(h,fT )|
≤ϵS(h) + EDS
|fS(x) −fT (x)|
|φS(x) −φT (x)||h(x) −fT (x)|dx
≤ϵS(h) + EDS
|fS(x) −fT (x)|
+ d1(DS,DT ).
In the ﬁrst line, we could instead choose to add and subtract ϵT (h,fS) rather than ϵS(h,fT ),
which would result in the same bound only with the expectation taken with respect to DT
instead of DS. Choosing the smaller of the two gives us the bound.
Lemma 2 For a symmetric hypothesis class H (one where for every h ∈H, the inverse
hypothesis 1 −h is also in H) and samples U, U′ of size m, the empirical H-distance is
dH(U,U′) = 2
I[x ∈U] + 1
where I[x ∈U] is the binary indicator variable which is 1 when x ∈U.
Proof We will show that for any hypothesis h and corresponding set I(h) of positively
labeled instances,
I[x ∈U] + 1
= PrU [I(h)] −PrU′ [I(h)].
I [x ∈U] +
I [x ∈U] + I
I [x ∈U] + I
Mach Learn 79: 151–175
I [x ∈U] +
I [x ∈U] −I
2 (1 −PrU′ [I(h)] −(1 −PrU [I(h)])) + 1
2 (PrU [I(h)] −PrU′ [I(h)])
= PrU [I(h)] −PrU′ [I(h)].
The absolute value in the statement of the lemma follows from the symmetry of H.
Lemma 4 Let h be a hypothesis in class H. Then
|ϵα(h) −ϵT (h)| ≤(1 −α)
2dHH(DS,DT ) + λ
Proof Similarly to the proof of Theorem 2, this proof relies heavily on the triangle inequality
for classiﬁcation error.
|ϵα(h) −ϵT (h)|
= (1 −α)|ϵS(h) −ϵT (h)|
|ϵS(h) −ϵS(h,h∗)| + |ϵS(h,h∗) −ϵT (h,h∗)| + |ϵT (h,h∗) −ϵT (h)|
ϵS(h∗) + |ϵS(h,h∗) −ϵT (h,h∗)| + ϵT (h∗)
2dHH(DS,DT ) + λ
Theorem 3 Let H be a hypothesis space of VC dimension d. Let US and UT be unlabeled
samples of size m′ each, drawn from DS and DT respectively. Let S be a labeled sample of
size m generated by drawing βm points from DT and (1 −β)m points from DS, labeling
them according to fS and fT , respectively. If ˆh ∈H is the empirical minimizer of ˆϵα(h)
on S and h∗
T = minh∈H ϵT (h) is the target error minimizer, then for any δ ∈(0,1), with
probability at least 1 −δ (over the choice of the samples),
ϵT (ˆh) ≤ϵT (h∗
β + (1 −α)2
2d log(2(m + 1)) + 2log
ˆdHH(US,UT ) + 4
2d log(2m′) + log 79: 151–175
and Bartlett 1999).
ϵT (ˆh) ≤ϵα(ˆh) + (1 −α)
2dHH(DS,DT ) + λ
≤ˆϵα(ˆh) + 2
β + (1 −α)2
2d log(2(m + 1)) + 2log( 8
2dHH(DS,DT ) + λ
β + (1 −α)2
2d log(2(m + 1)) + 2log( 8
2dHH(DS,DT ) + λ
ˆh = argmin
h∈H ˆϵα(h)
β + (1 −α)2
2d log(2(m + 1)) + 2log( 8
2dHH(DS,DT ) + λ
β + (1 −α)2
2d log(2(m + 1)) + 2log( 8
2dHH(DS,DT ) + λ
β + (1 −α)2
2d log(2(m + 1)) + 2log( 8
ˆdHH(US,UT ) + 4
2d log(2m′) + log( 4
Lemma 6 For each j ∈{1,...,N}, let Sj be a labeled sample of size βjm generated by
drawing βjm points from Dj and labeling them according to fj. For any ﬁxed weight vector
α, let ˆϵα(h) be the empirical α-weighted error of some ﬁxed hypothesis h on this sample,
and let ϵα(h) be the true α-weighted error. Then for any δ ∈(0,1), with probability at least
|ˆϵα(h) −ϵα(h)| ≥ϵ
Proof Due to its similarity to the proof of Lemma 5, we omit some details of this proof, and
concentrate only on the parts that differ.
For each source j, let Xj,1,...,Xj,βj m be random variables that take on the values
(αj/βj)|h(x)−fj(x)| for the βjm instances x ∈Sj. Note that Xj,1,...,Xj,βj m ∈[0,αj/βj].
Mach Learn 79: 151–175
αj ˆϵj(h) =
|h(x) −fj(x)| = 1
By linearity of expectations, we have that E[ˆϵα(h)] = ϵα(h), and so by Hoeffding’s inequality, for every h ∈H,
|ˆϵα(h) −ϵα(h)| ≥ϵ
i=1 range2(Xj,i)
Theorem 4 Let H be a hypothesis space of VC dimension d. For each j ∈{1,...,N},
let Sj be a labeled sample of size βjm generated by drawing βjm points from Dj and
labeling them according to fj. If ˆh ∈H is the empirical minimizer of ˆϵα(h) for a ﬁxed
weight vector α on these samples and h∗
T = minh∈H ϵT (h) is the target error minimizer, then
for any δ ∈(0,1), with probability at least 1 −δ,
ϵT (ˆh) ≤ϵT (h∗
2d log(2(m + 1)) + log( 4
2λj + dHH(Dj,DT )
where λj = minh∈H{ϵT (h) + ϵj(h)}.
Proof Let h∗
j = argminh{ϵT (h) + ϵj(h)}. Then
|ϵα(h) −ϵT (h)|
αjϵj(h) −ϵT (h)
ϵj(h) −ϵT (h)
ϵj(h) −ϵj(h,h∗
j) −ϵT (h,h∗
j) −ϵT (h)
j) −ϵT (h,h∗
2dHH(Dj,DT )
The third line follows from the triangle inequality. The last line follows from the deﬁnition
of λj and Lemma 3. Putting this together with Lemma 6, we ﬁnd that for any δ ∈(0,1),
Mach Learn 79: 151–175
with probability 1 −δ,
ϵT (ˆh) ≤ϵα(ˆh) +
2dHH(Dj,DT )
≤ˆϵα(ˆh) + 2
2d log(2(m + 1)) + log( 4
2dHH(Dj,DT )
2d log(2(m + 1)) + log( 4
2dHH(Dj,DT )
2d log(2(m + 1)) + log( 4
2dHH(Dj,DT )
2d log(2(m + 1)) + log( 4
2λj + dHH(Dj,DT )
Theorem 5 Let H be a hypothesis space of VC dimension d. For each j ∈{1,...,N},
let Sj be a labeled sample of size βjm generated by drawing βjm points from Dj and
labeling them according to fj. If ˆh ∈H is the empirical minimizer of ˆϵα(h) for a ﬁxed
weight vector α on these samples and h∗
T = minh∈H ϵT (h) is the target error minimizer, then
for any δ ∈(0,1), with probability at least 1 −δ,
ϵT (ˆh) ≤ϵT (h∗
2d log(2(m + 1)) + log( 4
2dHH(Dα,DT )
where γα = minh{ϵT (h) + ϵα(h)} = minh{ϵT (h) +
j=1 αjϵj(h)}.
Proof The proof is almost identical to that of Theorem 4 with minor modiﬁcations to the
derivation of the bound on |ϵα(h)−ϵT (h)|. Let h∗= argminh{ϵT (h)+ϵα(h)}. By the triangle
Mach Learn 79: 151–175
inequality and Lemma 3,
|ϵα(h) −ϵT (h)| ≤
ϵα(h) −ϵα(h,h∗)
ϵα(h,h∗) −ϵT (h,h∗)
ϵT (h,h∗) −ϵT (h)
ϵα(h,h∗) −ϵT (h,h∗)
2dHH(Dα,DT ).
The remainder of the proof is unchanged.