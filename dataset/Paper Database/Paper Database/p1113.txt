Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs
Xiaolong Wang∗
Abhinav Gupta
The Robotics Institute, Carnegie Mellon University
We consider the problem of zero-shot recognition: learning a visual classiﬁer for a category with zero training examples, just using the word embedding of the category and
its relationship to other categories, which visual data are
provided. The key to dealing with the unfamiliar or novel
category is to transfer knowledge obtained from familiar
classes to describe the unfamiliar class. In this paper, we
build upon the recently introduced Graph Convolutional
Network (GCN) and propose an approach that uses both
semantic embeddings and the categorical relationships to
predict the classiﬁers. Given a learned knowledge graph
(KG), our approach takes as input semantic embeddings for
each node (representing visual category). After a series of
graph convolutions, we predict the visual classiﬁer for each
category. During training, the visual classiﬁers for a few
categories are given to learn the GCN parameters. At test
time, these ﬁlters are used to predict the visual classiﬁers of
unseen categories. We show that our approach is robust to
noise in the KG. More importantly, our approach provides
signiﬁcant improvement in performance compared to the current state-of-the-art results (from 2 ∼3% on some metrics
to whopping 20% on a few).
1. Introduction
Consider the animal category “okapi”. Even though we
might have never heard of this category or seen visual examples in the past, we can still learn a good visual classi-
ﬁer based on the following description: ”zebra-striped four
legged animal with a brown torso and a deer-like face” (Test
yourself on ﬁgure 1). On the other hand, our current recognition algorithms still operate in closed world conditions: that
is, they can only recognize the categories they are trained
with. Adding a new category requires collecting thousands
of training examples and then retraining the classiﬁers. To
tackle this problem, zero-shot learning is often used.
The key to dealing with the unfamiliar or novel category
is to transfer knowledge obtained from familiar classes to describe the unfamiliar classes (generalization). There are two
∗Indicates equal contribution.
Figure 1. Can you ﬁnd “okapi” in these images? Okapi is ” zebrastriped four legged animal with a brown torso and a deer-like
face”. In this paper, we focus on the problem of zero-shot learning
where visual classiﬁers are learned from semantic embeddings and
relationships to other categories.
paradigms of transferring knowledge. The ﬁrst paradigm
is to use implicit knowledge representations, i.e. semantic
embeddings. In this approach, one learns a vector representation of different categories using text data and then learns
a mapping between the vector representation to visual classiﬁer directly . However, these methods are limited
by the generalization power of the semantic models and the
mapping models themselves. It is also hard to learn semantic
embeddings from structured information.
The alternative and less-explored paradigm for zero-shot
learning is to use explicit knowledge bases or knowledge
graphs. In this paradigm, one explicitly represents the knowledge as rules or relationships between objects. These relationships can then be used to learn zero-shot classiﬁers for
new categories. The simplest example would be to learn
visual classiﬁers of compositional categories. Given classiﬁers of primitive visual concepts as inputs, applies
a simple composition rule to generate classiﬁers for new
complex concepts. However, in the general form, the relationships can be more complex than simple compositionality.
An interesting question we want to explore is if we can use
structured information and complex relationships to learn
visual classiﬁers without seeing any examples.
In this paper, we propose to distill both the implicit knowledge representations (i.e. word embedding) and explicit
relationships (i.e. knowledge graph) for learning visual classiﬁers of novel classes. We build a knowledge graph where
each node corresponds to a semantic category. These nodes
are linked via relationship edges. The input to each node of
the graph is the vector representation (semantic embedding)
of each category. We then use Graph Convolutional Network
 
(GCN) to transfer information (message-passing) between different categories. Speciﬁcally, we train a 6-layer
deep GCN that outputs the classiﬁers of different categories.
We focus on the task of image classiﬁcation. We consider
both of the test settings: (a) ﬁnal test classes being only zeroshot classes (without training classes at test time); (b) at test
time the labels can be either the seen or the unseen classes,
namely “generalized zero-shot setting” . We
show surprisingly powerful results and huge improvements
over classical baselines such as DeVise , ConSE 
,and current state-of-the-art . For example, on standard
ImageNet with 2-hop setting, 43.7% of the images retrieved
by in top-10 are correct. Our approach retrieves 62.4%
images correctly. That is a whopping 18.7% improvement
over the current state-of-the-art. More interestingly, we
show that our approach scales amazingly well and giving
a signiﬁcant improvement as we increase the size of the
knowledge graph even if the graph is noisy.
2. Related Work
With recent success of large-scale recognition systems , the focus has now shifted to scaling these systems in terms of categories. As more realistic and practical
settings are considered, the need for zero-shot recognition
– training visual classiﬁers without any examples – has increased. Speciﬁcally, the problem of mapping text to visual
classiﬁers is very interesting.
Early work on zero-shot learning used attributes to represent categories as vector indicating presence/absence of attributes. This vector representation can
then be mapped to learn visual classiﬁers. Instead of using
manually deﬁned attribute-class relationships, Rohrbach et
al. mined these associations from different internet
sources. Akata et al. used attributes as side-information to
learn a semantic embedding which helps in zero-shot recognition. Recently, there have been approaches such as 
which trys to match Wikipedia text to images by modeling
noise in the text description.
With the advancement of deep learning, most recent approaches can be mapped into two main research directions.
The ﬁrst approach is to use semantic embeddings (implicit
representations). The core idea is to represent each category
with learned vector representations that can be mapped to
visual classiﬁers . Socher et al. proposed training two different neural
networks for image and language in an unsupervised manner,
and then learning a linear mapping between image representations and word embeddings. Motivated by this work,
Frome et al. proposed a system called DeViSE to train a
mapping from image to word embeddings using a ConvNet
and a transformation layer. By using the predicted embedding to perform nearest neighbor search, DeViSE scales up
the zero-shot recognition to thousands of classes. Instead of
training a ConvNet to predict the word embedding directly,
Norouzi et al. proposed another system named ConSE
which constructs the image embedding by combining an
existing image classiﬁcation ConvNet and word embedding
model. Recently, Changpinyo et al proposed an approach
to align semantic and visual manifolds via use of ‘phantom’
classes. They report state-of-the-art results on ImageNet
dataset using this approach. One strong shortcoming of
these approaches is they do not use any explicit relationships between classes but rather use semantic-embeddings
to represent relationships.
The second popular way to distill the knowledge is to
use knowledge graph (explicit knowledge representations).
Researchers have proposed several approaches on how to
use knowledge graphs for object recognition . For example, Salakhutdinov et
al. used WordNet to share the representations among
different object classiﬁers so that objects with few training
examples can borrow statistical strength from related objects.
On the other hand, the knowledge graph can also be used to
model the mutual exclusion among different classes. Deng
et al. applied these exclusion rules as a constraint in the
loss for training object classiﬁers (e.g. an object will not be
a dog and a cat at the same time). They have also shown
zero-shot applications by adding object-attribute relations
into the graph. In contrast to these methods of using graph as
constraints, our approach used the graph to directly generate
novel object classiﬁers .
In our work, we propose to distill information both via
semantic embeddings and knowledge graphs. Speciﬁcally,
given a word embedding of an unseen category and the
knowledge graph that encodes explicit relationships, our approach predicts the visual classiﬁers of unseen categories. To
model the knowledge graph, our work builds upon the Graph
Convolutional Networks . It was originally proposed for
semi-supervised learning in language processing. We extend
it to our zero-short learning problem by changing the model
architecture and training loss.
3. Approach
Our goal is to distill information from both implicit (wordembeddings) and explicit (knowledge-graph) representations
for zero-shot recognition. But what is the right way to extract
information? We build upon the recent work on Graph Convolutional Network (GCN) to learn visual classiﬁers. In
the following, we will ﬁrst introduce how the GCN is applied
in natural language processing for classiﬁcation tasks, and
then we will go into details about our approach: applying
the GCN with a regression loss for zero-shot learning.
3.1. Preliminaries: Graph Convolutional Network
Graph Convolutional Network (GCN) was introduced
in to perform semi-supervised entity classiﬁcation.
𝑊.(𝑐./$×𝐷)
Inputs: Word Embeddings 𝒳
(𝑘dimensions )
Outputs: Object classifiers 𝒲3
(𝐷 dimensions )
Hidden states
(𝑐$ dimensions)
Figure 2. An example of our Graph Convolutional Network. It takes word embeddings as inputs and outputs the object classiﬁers. The
supervision comes from the ground-truth classiﬁers w2 and w3 highlighted by green. During testing, we input the same word embeddings
and obtain classiﬁer for x1 as ˆw1. This classiﬁer will be multiplied with the image features to produce classiﬁcation scores.
Given object entities, represented by word embeddings or
text features, the task is to perform classiﬁcation. For example, entities such as “dog” and “cat” will be labeled as
“mammal”; “chair” and “couch” will be labeled “furniture”.
We also assume that there is a graph where nodes are entities
and the edges represent relationships between entities.
Formally, given a dataset with n entities (X, Y ) =
{(xi, yi)}n
i=1 where xi represents the word embedding for
entity i and yi ∈{1, ..., C} represents its label. In semisupervised setting, we know the ground-truth labels for the
ﬁrst m entities. Our goal is to infer yi for the remaining
n −m entities, which do not have labels, using the word
embedding and the relationship graph. In the relationship
graph, each node is an entity and two nodes are linked if they
have a relationship in between.
We use a function F(·) to represent the Graph Convolutional Network. It takes all the entity word embeddings X
as inputs at one time and outputs the SoftMax classiﬁcation
results for all of them as F(X). For simplicity, we denote
the output for the ith entity as Fi(X), which is a C dimension SoftMax probability vector. In training time, we apply
the SoftMax loss on the ﬁrst m entities, which have labels as
Lsoftmax(Fi(X), yi).
The weights of F(·) are trained via back-propagation with
this loss. During testing time, we use the learned weights
to obtain the labels for the n −m entities with Fi(X), i ∈
{m + 1, ..., n}.
Unlike standard convolutions that operate on local region
in an image, in GCN the convolutional operations compute
the response at a node based on the neighboring nodes de-
ﬁned by the adjacency graph. Mathematically, the convolutional operations for each layer in the network F(·) is
represented as
where ˆA is a normalized version of the binary adjacency
matrix A of the graph, with n × n dimensions. X′ is the
input n × k feature matrix from the former layer. W is
the weight matrix of the layer with dimension k × c, where
c is the output channel number. Therefore, the input to a
convolutional layer is n × k ,and the output is a n × c matrix
Z. These convolution operations can be stacked one after
another. A non-linear operation (ReLU) is also applied after
each convolutional layer before the features are forwarded to
the next layer. For the ﬁnal convolutional layer, the number
of output channels is the number of label classes (c = C).
For more details, please refer to .
3.2. GCN for Zero-shot Learning
Our model builds upon the Graph Convolutional Network.
However, instead of entity classiﬁcation, we apply it to the
zero-shot recognition with a regression loss. The input of our
framework is the set of categories and their corresponding
semantic-embedding vectors (represented by X = {xi}n
For the output, we want to predict the visual classiﬁer for
each input category (represented by W = {wi}n
Speciﬁcally, the visual classiﬁer we want the GCN to
predict is a logistic regression model on the ﬁxed pre-trained
ConvNet features. If the dimensionality of visual-feature
vector is D, each classiﬁer wi for category i is also a Ddimensional vector. Thus the output of each node in the
GCN is D dimensions, instead of C dimensions. In the
zero-shot setting, we assume that the ﬁrst m categories in
the total n classes have enough visual examples to estimate
their weight vectors. For the remaining n−m categories, we
want to estimate their corresponding weight vectors given
their embedding vectors as inputs.
One way is to train a neural network (multi-layer perceptron) which takes xi as an input and learns to predict wi as
an output. The parameters of the network can be estimated
using m training pairs. However, generally m is small (in
the order of a few hundreds) and therefore, we want to use
the explicit structure of the visual world or the relationships
between categories to constrain the problem. We represent
these relationships as the knowledge-graph (KG). Each node
in the KG represents a semantic category. Since we have a
total of n categories, there are n nodes in the graph. Two
nodes are linked to each other if there is a relationship between them. The graph structure is represented by the n × n
adjacency matrix, A. Instead of building a bipartite graph
as , we replace all directed edges in the KG by undirected edges, which leads to a symmetric adjacency matrix.
As Fig. 2 shows, we use a 6-layer GCN where each layer
l takes as input the feature representation from previous layer
(Zl−1) and outputs a new feature representation (Zl). For
the ﬁrst layer the input is X which is an n × k matrix (k is
the dimensionality of the word-embedding vector). For the
ﬁnal-layer the output feature-vector is ˆ
W which has the size
of n × D; D being the dimensionality of the classiﬁer or
visual feature vector.
Loss-function: For the ﬁrst m categories, we have predicted
classiﬁer weights ˆ
W1...m and ground-truth classiﬁer weights
learned from training images W1...m. We use the meansquare error as the loss function between the predicted and
the ground truth classiﬁers.
Lmse( ˆwi, wi).
During training, we use the loss from the m seen categories to estimate the parameters for the GCN. Using the
estimated parameters, we obtain the classiﬁer weights for
the zero-shot categories. At test time, we ﬁrst extract the
image feature representations via the pre-trained ConvNet
and use these generated classiﬁers to perform classiﬁcation
on the extracted features.
3.3. Implementation Details
Our GCN is composed of 6 convolutional layers with output channel numbers as 2048 →2048 →1024 →1024 →
512 →D, where D represents the dimension of the object
classiﬁer. Unlike the 2-layer network presented in , our
network is much deeper. As shown in ablative studies, we
ﬁnd that making the network deep is essential in generating the classiﬁer weights. For activation functions, instead
of using ReLU after each convolutional layer, we apply
LeakyReLU with the negative slope of 0.2. Empirically, we ﬁnd that LeakyReLU leads to faster convergence
for our regression problem.
While training our GCN, we perform L2-Normalization
on the outputs of the networks and the ground-truth classiﬁers. During testing, the generated classiﬁers of unseen
classes are also L2-Normalized. We ﬁnd adding this constraint important, as it regularizes the weights of all the
classiﬁers into similar magnitudes. In practice, we also ﬁnd
that the last layer classiﬁers of the ImageNet pre-trained
networks are naturally normalized. That is, if we perform
L2-Normalization on each of the last layer classiﬁers during
testing, the performance on the ImageNet 2012 1K-class
validation set changes marginally (< 1%).
To obtain the word embeddings for GCN inputs, we use
the GloVe text model trained on the Wikipedia dataset,
which leads to 300-d vectors. For the classes whose names
contain multiple words, we match all the words in the trained
model and ﬁnd their embeddings. By averaging these word
embeddings, we obtain the class embedding.
4. Experiment
We now perform experiments to showcase that our approach: (a) improves the state-of-the-art by a signiﬁcant
margin; (b) is robust to different pre-trained ConvNets and
noise in the KG. We use two datasets in our experiments. The
ﬁrst dataset we use is constructed from publicly-available
knowledge bases. The dataset consists of relationships and
graph from Never-Ending Language Learning (NELL) 
and images from Never-Ending Image Learning (NEIL) .
This is an ideal dataset for: (a) demonstrating that our approach is robust even with automatically learned (and noisy)
KG; (b) ablative studies since the KG in this domain is rich,
and we can perform ablations on KG as well.
Our ﬁnal experiments are shown on the standard ImageNet dataset. We use the same settings as the baseline approaches together with the WordNet knowledge graph. We show that our approach surpasses the stateof-the-art methods by a signiﬁcant margin.
4.1. Experiments on NELL and NEIL
Dataset settings. For this experiment, we construct a new
knowledge graph based on the NELL and NEIL 
datasets. Speciﬁcally, the object nodes in NEIL correspond
to the nodes in NELL. The NEIL dataset offers the sources
of images and the NELL dataset offers the common sense
knowledge rules. However, the NELL graph is incredibly
large 1: it contains roughly 1.7M types of object entities and
around 2.4M edges representing the relationships between
every two objects. Furthermore, since NELL is constructed
automatically, there are noisy edges in the graph. Therefore,
we create sub-graphs for our experiments.
The process of constructing this sub-graph is straightforward. We perform Breadth-ﬁrst search (BFS) starting from
the NEIL nodes. We discover paths with maximum length
K hops such that the ﬁrst and last node in the path are NEIL
nodes. We add all the nodes and edges in these paths into our
sub-graph. We set K = 7 during BFS because we discover
a path longer than 7 hops will cause the connection between
two objects noisy and unreasonable. For example, “jeep”
can be connected to “deer” in a long path but they are hardly
semantically related.
Note that each edge in NELL has a conﬁdence value that
is usually larger than 0.9. For our experiments, we create two
1 
NEIL Nodes
(Train/Test)
High Value Edges
Table 1. Dataset Statistics: Two different sizes of knowledge graphs
in our experiment.
different versions of sub-graphs. The ﬁrst smaller version is
a graph with high value edges (larger than 0.999), and the
second one used all the edges regardless of their conﬁdence
values. The statistics of the two sub-graphs are summarized
in Table 1. For the larger sub-graph, we have 14K object
nodes. Among these nodes, 704 of them have corresponding
images in the NEIL database. We use 616 classes for training
our GCN and leave 88 classes for testing. Note that these
88 testing classes are randomly selected among the classes
that have no overlap with the 1000 classes in the standard
ImageNet classiﬁcation dataset. The smaller knowledge
graph is around half the size of the larger one. We use the
same 88 testing classes in both settings
Training details. For training the ConvNet on NEIL images,
we use the 310K images associated with the 616 training
classes. The evaluation is performed on the randomly selected 12K images associated with the 88 testing classes,
i.e. all images from the training classes are excluded during
testing. We ﬁne-tune the ImageNet pre-trained VGGM 
network architecture with relatively small fc7 outputs (128dimension). Thus the object classiﬁer dimension in fc8 is
128. For training our GCN, we use the ADAM optimizer with learning rate 0.001 and weight decay 0.0005. We
train our GCN for 300 epochs for every experiment.
Baseline method. We compare our method with one of the
state-of-the-art methods, ConSE , which shows slightly
better performance than DeViSE in ImageNet. As a
brief introduction, ConSE ﬁrst feedforwards the test image
into a ConvNet that is trained only on the training classes.
With the output probabilities, ConSE selects top T predictions {pi}T
i=1 and the word embeddings {xi}T
i=1 of
these classes. It then generates a new word embedding by
weighted averaging the T embeddings with the probability
i=1 pixi. This new embedding is applied to perform
nearest neighbors in the word embeddings of the testing
classes. The top retrieved classes are selected as the ﬁnal
result. We enumerate different values of T for evaluations.
Quantitative Results. We perform evaluations on the task
of 88 unseen categories classiﬁcation. Our metric is based
on the percentage of correctly retrieved test data (out of top k
retrievals) for a given zero-shot class. The results are shown
in Table 2. We evaluate our method on two different sizes
of knowledge graphs. We use “High Value Edges” to denote
the knowledge graph constructed based on high conﬁdence
edges. “All Edges” represents the graph constructed with all
the edges. We denote the baseline as “ConSE(T)” where
High Value
ConSE(431)
ConSE(616)
Table 2. Top-k accuracy for different models in different settings.
Figure 3. We randomly drop 5% to 50% of the edges in the “All
Edges” graph and show the top-1, top-5 and top-10 accuracies.
we set T to be 5, 10 and the number of training classes.
Our method outperforms the ConSE baseline by a large
margin. In the “All Edges” dataset, our method outperforms
ConSE 3.6% in top-1 accuracy. More impressively, the accuracy of our method is almost 2 times as that of ConSE
in top-2 metric and even more than 2 times in top-5 and
top-10 accuracies. These results show that using knowledge graph with word embeddings in our method leads to
much better result than the state-of-the-art results with word
embeddings only.
From small to larger graph. In addition to improving performance in zero-shot recognition, our method obtains more
performance gain as our graph size increases. As shown in
Table 2, our method performs better by switching from the
small to larger graph. Our approach has obtained 2 ∼3% improvements in all the metrics. On the other hand, there is little to no improvements in ConSE performance. It also shows
that the KG does not need to be hand-crafted or cleaned. Our
approach is able to robustly handle the errors in the graph
structure.
Resilience to Missing Edges We explore how the performance of our model changes if we randomly drop 5% to
50% of the edges in the “All Edges” graph. As Fig. 3 shows,
by dropping from 5% to 10% of edges, the performance of
our model changes negligibly. This is mainly because the
Figure 4. We compute the minimum Euclidean distances between
predicted and training classiﬁers. The distances are plotted by
sorting them from small to large.
knowledge graph can have redundant information with 14K
nodes and 97K edges connecting them. This again implies
that our model is robust to small noisy changes in the graph.
As we start deleting more than 30% of the edges, the accuracies drop drastically. This indicates that the performance of
our model is highly correlated to the size of the knowledge
Random Graph? It is clear that our approach can handle
noise in the graph. But does any random graph work? To
demonstrate that the structure of the graph is still critical
we also created some trivial graphs: (i) star model: we
create a graph with one single root node and only have edges
connecting object nodes to the root node; (ii) random graph:
all nodes in the graph are randomly connected. Table 3
shows the results. It is clear that all the numbers are close to
random guessing, which means a reasonable graph plays an
important role and a random graph can have negative effects
on the model.
Trivial KG
Star Model
Random Graph
Table 3. Top-k accuracy on trivial knowledge graphs we create.
How important is the depth of GCN? We show that making the Graph Convolutional Network deep is critical in our
problem. We show the performance of using different numbers of layers for our model on the “All Edges” knowledge
graph shown in Table 4. For the 2-layer model we use 512
hidden neurons, and the 4-layer model has output channel
numbers as 2048 →1024 →512 →128. We show that
the performance keeps increasing as we make the model
deeper from 2-layer to 6-layer. The reason is that increasing
the times of convolutions is essentially increasing the times
of message passing between nodes in the graph. However,
we do not observe much gain by adding more layers above
Ours (2-layer)
Ours (4-layer)
Ours (6-layer)
Table 4. Top-k accuracy with different depths of our model.
the 6-layer model. One potential reason might be that the
optimization becomes harder as the network goes deeper.
Is our network just copying classiﬁers as outputs? Even
though we show our method is better than ConSE baseline, is
it possible that it learns to selectively copy the nearby classi-
ﬁers? To show our method is not learning this trivial solution,
we compute the Euclidean distance between our generated
classiﬁers and the training classiﬁers. More speciﬁcally, for
a generated classiﬁer, we compare it with the classiﬁers from
the training classes that are at most 3-hops away. We calculate the minimum distance between each generated classiﬁer
and its neighbors. We sort the distances for all 88 classi-
ﬁers and plot Fig. 4. As for reference, the distance between
“wooden spoon” and “spoon” classiﬁers in the training set is
0.26 and the distance between “wooden spoon” and “optimus prime” is 0.78. We can see that our predicted classiﬁers
Figure 5. t-SNE visualizations for our word embeddings and GCN
output visual classiﬁers in the “All Edges” dataset. The test classes
are shown in red.
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
(a) Top-k accuracy for different models when testing on only unseen
DeViSE 
ConSE 
Inception-v1
Inception-v1
DeViSE 
ConSE 
Inception-v1
Inception-v1
DeViSE 
ConSE 
Inception-v1
Inception-v1
(b) Top-k accuracy for different models when testing on both seen and
unseen classes (a more practical and generalized setting).
Table 5. Results on ImageNet. We test our model on 2 different settings over 3 different datasets.
are quite different from its neighbors.
Are the outputs only relying on the word embeddings?
We perform t-SNE visualizations to show that our output classiﬁers are not just derived from the word embeddings.
We show the t-SNE plots of both the word embeddings
and the classiﬁers of the seen and unseen classes in the “All
Edges” dataset. As Fig. 5 shows, we have very different clustering results between the word embeddings and the object
classiﬁers, which indicates that our GCN is not just learning
a direct projection from word embeddings to classiﬁers.
4.2. Experiments on WordNet and ImageNet
We now perform our experiments on a much larger-scale
ImageNet dataset. We adopt the same train/test split
settings as . More speciﬁcally, we report our results
on 3 different test datasets: “2-hops”, “3-hops” and the
whole “All” ImageNet set. These datasets are constructed
according to how similar the classes are related to the classes
in the ImageNet 2012 1K dataset. For example, “2-hops”
dataset (around 1.5K classes) includes the classes from the
ImageNet 2011 21K set which are semantically very similar
to the ImageNet 2012 1K classes. “3-hops” dataset (around
7.8K classes) includes the classes that are within 3 hops of
the ImageNet 2012 1K classes, and the “All” dataset includes
all the labels in ImageNet 2011 21K. There are no common
labels between the ImageNet 1K class and the classes in
these 3-dataset. It is also obvious to see that as the number
of class increases, the task becomes more challenging.
As for knowledge graph, we use the sub-graph of the
WordNet , which includes around 30K object nodes2.
Training details. Note that to perform testing on 3 differ-
2 
ent test sets, we only need to train one set of ConvNet and
GCN. We use two different types of ConvNets as the base
network for computing visual features: Inception-v1 
and ResNet-50 . Both networks are pre-trained using
the ImageNet 2012 1K dataset and no ﬁne-tuning is required.
For Inception-v1, the output feature of the second to the
last layer has 1024 dimensions, which leads to D = 1024
object classiﬁers in the last layer. For ResNet-50, we have
D = 2048. Except for the changes of output targets, other
settings of training GCN remain the same as those of the previous experiments on NELL and NEIL. It is worthy to note
that our GCN model is robust to different sizes of outputs.
The model shows consistently better results as the representation (features) improves from Inception-v1 (68.7% top-1
accuracy in ImageNet 1K val set) to ResNet-50 (75.3%).
We evaluate our method with the same metric as the
previous experiments: the percentage of hitting the groundtruth labels among the top k predictions. However, instead
of only testing with the unseen object classiﬁers, we include
both training and the predicted classiﬁers during testing, as
suggested by . Note that in these two settings of
experiments, we still perform testing on the same set of
images associated with unseen classes only.
Testing without considering the training labels. We ﬁrst
perform experiments excluding the classiﬁers belonging to
the training classes during testing. We report our results in
Table. 5a. We compare our results to the recent state-of-theart methods SYNC and EXEM . We show experiments
with the same pre-trained ConvNets (Inception-v1) as .
Due to unavailability of their word embeddings for all the
nodes in KG, we use a different set of word embeddings
(GloVe) ,which is publicly available.
GoogleNews
GoogleNews
Table 6. Results with different word embeddings on ImageNet (2
hops), corresponding to the experiments in Table 5a.
Therefore, we ﬁrst investigate if the change of wordembedding is crucial. We show this via the ConSE baseline.
Our re-implementation of ConSE, shown as “ConSE(us)”
in the table, uses the GloVe whereas the ConSE method
implemented in uses their own word embedding. We
see that both approaches have similar performance. Ours is
slightly better in top-1 accuracy while the one in is
better in top-20 accuracy. Thus, with respect to zero-shot
learning, both word-embeddings seem equally powerful.
We then compare our results with SYNC and
EXEM . With the same pre-trained ConvNet Inceptionv1, our method outperforms almost all the other methods on
all the datasets and metrics. On the “2-hops” dataset, our approach outperforms all methods with a large margin: around
6% on top-1 accuracy and 17% on top-5 accuracy. On the
“3-hops” dataset, our approach is consistently better than
EXEM around 2 ∼3% from top-5 to top-20 metrics.
By replacing the Inception-v1 with the ResNet-50, we
obtain another performance boost in all metrics. For the
top-5 metric, our ﬁnal model outperforms the state-of-the-art
method EXEM by a whooping 20.9% in the “2-hops”
dataset, 3.5% in the “3-hops” dataset and 1% in the “All”
dataset. Note that the gain is diminishing because the task
increases in difﬁculty as the number of unseen classes increases.
Sensitivity to word embeddings. Is our method sensitive
to word embeddings? What will happen if we use different
word embeddings as inputs? We investigate 3 different word
embeddings including GloVe (which is used in the other
experiments in the paper), FastText and word2vec 
trained with GoogleNews. As for comparisons, we have
also implemented the method in which trains a direct
mapping from word embeddings to visual features without
knowledge graphs. We use the Inception-v1 ConvNet to extract visual features. We show the results on ImageNet (with
the 2-hops setting same as Table 5a). We can see that 
highly relies on the quality of the word embeddings (top-5
results range from 17.2% to 33.5%). On the other hand, our
top-5 results are stably around 50% and are much higher
than . With the GloVe word embeddings, our approach
has a relative improvement of almost 200% over .
This again shows graph convolutions with knowledge graphs
Test Image
ConSE (10)
panthera tigris(train)
tiger cat (train)
felis onca (train)
leopard (train)
tiger shark (train)
tigress (test)
bengal tiger (test)
panthera tigris (train)
tiger cub (test)
tiger cat (train)
rock beauty (train)
ringlet (train)
flagpole (train)
large slipper (test)
yellow slipper (train)
butterfly fish (test)
rock beauty (train)
damselfish (test)
atoll (test)
barrier reef (test)
tractor (train)
reaper (train)
thresher (train)
trailer truck (train)
motortruck (test)
tracked vehicle (test)
tractor (train)
propelled vehicle (test)
reaper (train)
forklift (train)
Figure 6. Visualization of top 5 prediction results for 3 different
images. The correct prediction results are highlighted by red bold
characters. The unseen classes are marked with a red “test” in the
bracket. Previously seen classes have a plain “train” in the bracket.
play a signiﬁcant role in improving zero-shot recognition.
Testing with the training classiﬁers. Following the suggestions in , a more practical setting for zero-shot
recognition is to include both seen and unseen category classiﬁers during testing. We test our method in this generalized
setting. Since there are very few baselines available for
this setting of experiment, we can only compare the results
with ConSE and DeViSE. We have also re-implemented the
ConSE baselines with both Inception-v1 and ResNet-50 pretrained networks. As Table 5b shows our method almost
doubles the performance compared to the baselines on every metric and all 3-datasets. Moreover, we can still see
the boost in of performance by switching the pre-trained
Inception-v1 network to ResNet-50.
Visualizations. We ﬁnally perform visualizations using our
model and ConSE with T = 10 in Fig. 6 (Top-5 prediction
results). We can see that our method signiﬁcantly outperforms ConSE(10) in these examples. Although ConSE(10)
still gives reasonable results in most cases, the output labels
are biased to be within the training labels. On the other hand,
our method outputs the unseen classes as well.
5. Conclusion
We have presented an approach for zero-shot recognition using the semantic embeddings of a category and the
knowledge graph that encodes the relationship of the novel
category to familiar categories. Our work also shows that a
knowledge graph provides supervision to learn meaningful
classiﬁers on top of semantic embeddings. Our results indicate a signiﬁcant improvement over current state-of-the-art.
Acknowledgement:
This work was supported by ONR MURI
N000141612007, Sloan, Okawa Fellowship to AG and NVIDIA Fellowship
to XW. We would also like to thank Xinlei Chen, Senthil Purushwalkam,
Zhilin Yang and Abulhair Saparov for many helpful discussions.