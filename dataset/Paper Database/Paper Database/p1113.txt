Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs
Xiaolong Wang‚àó
Abhinav Gupta
The Robotics Institute, Carnegie Mellon University
We consider the problem of zero-shot recognition: learning a visual classiÔ¨Åer for a category with zero training examples, just using the word embedding of the category and
its relationship to other categories, which visual data are
provided. The key to dealing with the unfamiliar or novel
category is to transfer knowledge obtained from familiar
classes to describe the unfamiliar class. In this paper, we
build upon the recently introduced Graph Convolutional
Network (GCN) and propose an approach that uses both
semantic embeddings and the categorical relationships to
predict the classiÔ¨Åers. Given a learned knowledge graph
(KG), our approach takes as input semantic embeddings for
each node (representing visual category). After a series of
graph convolutions, we predict the visual classiÔ¨Åer for each
category. During training, the visual classiÔ¨Åers for a few
categories are given to learn the GCN parameters. At test
time, these Ô¨Ålters are used to predict the visual classiÔ¨Åers of
unseen categories. We show that our approach is robust to
noise in the KG. More importantly, our approach provides
signiÔ¨Åcant improvement in performance compared to the current state-of-the-art results (from 2 ‚àº3% on some metrics
to whopping 20% on a few).
1. Introduction
Consider the animal category ‚Äúokapi‚Äù. Even though we
might have never heard of this category or seen visual examples in the past, we can still learn a good visual classi-
Ô¨Åer based on the following description: ‚Äùzebra-striped four
legged animal with a brown torso and a deer-like face‚Äù (Test
yourself on Ô¨Ågure 1). On the other hand, our current recognition algorithms still operate in closed world conditions: that
is, they can only recognize the categories they are trained
with. Adding a new category requires collecting thousands
of training examples and then retraining the classiÔ¨Åers. To
tackle this problem, zero-shot learning is often used.
The key to dealing with the unfamiliar or novel category
is to transfer knowledge obtained from familiar classes to describe the unfamiliar classes (generalization). There are two
‚àóIndicates equal contribution.
Figure 1. Can you Ô¨Ånd ‚Äúokapi‚Äù in these images? Okapi is ‚Äù zebrastriped four legged animal with a brown torso and a deer-like
face‚Äù. In this paper, we focus on the problem of zero-shot learning
where visual classiÔ¨Åers are learned from semantic embeddings and
relationships to other categories.
paradigms of transferring knowledge. The Ô¨Årst paradigm
is to use implicit knowledge representations, i.e. semantic
embeddings. In this approach, one learns a vector representation of different categories using text data and then learns
a mapping between the vector representation to visual classiÔ¨Åer directly . However, these methods are limited
by the generalization power of the semantic models and the
mapping models themselves. It is also hard to learn semantic
embeddings from structured information.
The alternative and less-explored paradigm for zero-shot
learning is to use explicit knowledge bases or knowledge
graphs. In this paradigm, one explicitly represents the knowledge as rules or relationships between objects. These relationships can then be used to learn zero-shot classiÔ¨Åers for
new categories. The simplest example would be to learn
visual classiÔ¨Åers of compositional categories. Given classiÔ¨Åers of primitive visual concepts as inputs, applies
a simple composition rule to generate classiÔ¨Åers for new
complex concepts. However, in the general form, the relationships can be more complex than simple compositionality.
An interesting question we want to explore is if we can use
structured information and complex relationships to learn
visual classiÔ¨Åers without seeing any examples.
In this paper, we propose to distill both the implicit knowledge representations (i.e. word embedding) and explicit
relationships (i.e. knowledge graph) for learning visual classiÔ¨Åers of novel classes. We build a knowledge graph where
each node corresponds to a semantic category. These nodes
are linked via relationship edges. The input to each node of
the graph is the vector representation (semantic embedding)
of each category. We then use Graph Convolutional Network
 
(GCN) to transfer information (message-passing) between different categories. SpeciÔ¨Åcally, we train a 6-layer
deep GCN that outputs the classiÔ¨Åers of different categories.
We focus on the task of image classiÔ¨Åcation. We consider
both of the test settings: (a) Ô¨Ånal test classes being only zeroshot classes (without training classes at test time); (b) at test
time the labels can be either the seen or the unseen classes,
namely ‚Äúgeneralized zero-shot setting‚Äù . We
show surprisingly powerful results and huge improvements
over classical baselines such as DeVise , ConSE 
,and current state-of-the-art . For example, on standard
ImageNet with 2-hop setting, 43.7% of the images retrieved
by in top-10 are correct. Our approach retrieves 62.4%
images correctly. That is a whopping 18.7% improvement
over the current state-of-the-art. More interestingly, we
show that our approach scales amazingly well and giving
a signiÔ¨Åcant improvement as we increase the size of the
knowledge graph even if the graph is noisy.
2. Related Work
With recent success of large-scale recognition systems , the focus has now shifted to scaling these systems in terms of categories. As more realistic and practical
settings are considered, the need for zero-shot recognition
‚Äì training visual classiÔ¨Åers without any examples ‚Äì has increased. SpeciÔ¨Åcally, the problem of mapping text to visual
classiÔ¨Åers is very interesting.
Early work on zero-shot learning used attributes to represent categories as vector indicating presence/absence of attributes. This vector representation can
then be mapped to learn visual classiÔ¨Åers. Instead of using
manually deÔ¨Åned attribute-class relationships, Rohrbach et
al. mined these associations from different internet
sources. Akata et al. used attributes as side-information to
learn a semantic embedding which helps in zero-shot recognition. Recently, there have been approaches such as 
which trys to match Wikipedia text to images by modeling
noise in the text description.
With the advancement of deep learning, most recent approaches can be mapped into two main research directions.
The Ô¨Årst approach is to use semantic embeddings (implicit
representations). The core idea is to represent each category
with learned vector representations that can be mapped to
visual classiÔ¨Åers . Socher et al. proposed training two different neural
networks for image and language in an unsupervised manner,
and then learning a linear mapping between image representations and word embeddings. Motivated by this work,
Frome et al. proposed a system called DeViSE to train a
mapping from image to word embeddings using a ConvNet
and a transformation layer. By using the predicted embedding to perform nearest neighbor search, DeViSE scales up
the zero-shot recognition to thousands of classes. Instead of
training a ConvNet to predict the word embedding directly,
Norouzi et al. proposed another system named ConSE
which constructs the image embedding by combining an
existing image classiÔ¨Åcation ConvNet and word embedding
model. Recently, Changpinyo et al proposed an approach
to align semantic and visual manifolds via use of ‚Äòphantom‚Äô
classes. They report state-of-the-art results on ImageNet
dataset using this approach. One strong shortcoming of
these approaches is they do not use any explicit relationships between classes but rather use semantic-embeddings
to represent relationships.
The second popular way to distill the knowledge is to
use knowledge graph (explicit knowledge representations).
Researchers have proposed several approaches on how to
use knowledge graphs for object recognition . For example, Salakhutdinov et
al. used WordNet to share the representations among
different object classiÔ¨Åers so that objects with few training
examples can borrow statistical strength from related objects.
On the other hand, the knowledge graph can also be used to
model the mutual exclusion among different classes. Deng
et al. applied these exclusion rules as a constraint in the
loss for training object classiÔ¨Åers (e.g. an object will not be
a dog and a cat at the same time). They have also shown
zero-shot applications by adding object-attribute relations
into the graph. In contrast to these methods of using graph as
constraints, our approach used the graph to directly generate
novel object classiÔ¨Åers .
In our work, we propose to distill information both via
semantic embeddings and knowledge graphs. SpeciÔ¨Åcally,
given a word embedding of an unseen category and the
knowledge graph that encodes explicit relationships, our approach predicts the visual classiÔ¨Åers of unseen categories. To
model the knowledge graph, our work builds upon the Graph
Convolutional Networks . It was originally proposed for
semi-supervised learning in language processing. We extend
it to our zero-short learning problem by changing the model
architecture and training loss.
3. Approach
Our goal is to distill information from both implicit (wordembeddings) and explicit (knowledge-graph) representations
for zero-shot recognition. But what is the right way to extract
information? We build upon the recent work on Graph Convolutional Network (GCN) to learn visual classiÔ¨Åers. In
the following, we will Ô¨Årst introduce how the GCN is applied
in natural language processing for classiÔ¨Åcation tasks, and
then we will go into details about our approach: applying
the GCN with a regression loss for zero-shot learning.
3.1. Preliminaries: Graph Convolutional Network
Graph Convolutional Network (GCN) was introduced
in to perform semi-supervised entity classiÔ¨Åcation.
ùëä.(ùëê./$√óùê∑)
Inputs: Word Embeddings ùí≥
(ùëòdimensions )
Outputs: Object classifiers ùí≤3
(ùê∑ dimensions )
Hidden states
(ùëê$ dimensions)
Figure 2. An example of our Graph Convolutional Network. It takes word embeddings as inputs and outputs the object classiÔ¨Åers. The
supervision comes from the ground-truth classiÔ¨Åers w2 and w3 highlighted by green. During testing, we input the same word embeddings
and obtain classiÔ¨Åer for x1 as ÀÜw1. This classiÔ¨Åer will be multiplied with the image features to produce classiÔ¨Åcation scores.
Given object entities, represented by word embeddings or
text features, the task is to perform classiÔ¨Åcation. For example, entities such as ‚Äúdog‚Äù and ‚Äúcat‚Äù will be labeled as
‚Äúmammal‚Äù; ‚Äúchair‚Äù and ‚Äúcouch‚Äù will be labeled ‚Äúfurniture‚Äù.
We also assume that there is a graph where nodes are entities
and the edges represent relationships between entities.
Formally, given a dataset with n entities (X, Y ) =
{(xi, yi)}n
i=1 where xi represents the word embedding for
entity i and yi ‚àà{1, ..., C} represents its label. In semisupervised setting, we know the ground-truth labels for the
Ô¨Årst m entities. Our goal is to infer yi for the remaining
n ‚àím entities, which do not have labels, using the word
embedding and the relationship graph. In the relationship
graph, each node is an entity and two nodes are linked if they
have a relationship in between.
We use a function F(¬∑) to represent the Graph Convolutional Network. It takes all the entity word embeddings X
as inputs at one time and outputs the SoftMax classiÔ¨Åcation
results for all of them as F(X). For simplicity, we denote
the output for the ith entity as Fi(X), which is a C dimension SoftMax probability vector. In training time, we apply
the SoftMax loss on the Ô¨Årst m entities, which have labels as
Lsoftmax(Fi(X), yi).
The weights of F(¬∑) are trained via back-propagation with
this loss. During testing time, we use the learned weights
to obtain the labels for the n ‚àím entities with Fi(X), i ‚àà
{m + 1, ..., n}.
Unlike standard convolutions that operate on local region
in an image, in GCN the convolutional operations compute
the response at a node based on the neighboring nodes de-
Ô¨Åned by the adjacency graph. Mathematically, the convolutional operations for each layer in the network F(¬∑) is
represented as
where ÀÜA is a normalized version of the binary adjacency
matrix A of the graph, with n √ó n dimensions. X‚Ä≤ is the
input n √ó k feature matrix from the former layer. W is
the weight matrix of the layer with dimension k √ó c, where
c is the output channel number. Therefore, the input to a
convolutional layer is n √ó k ,and the output is a n √ó c matrix
Z. These convolution operations can be stacked one after
another. A non-linear operation (ReLU) is also applied after
each convolutional layer before the features are forwarded to
the next layer. For the Ô¨Ånal convolutional layer, the number
of output channels is the number of label classes (c = C).
For more details, please refer to .
3.2. GCN for Zero-shot Learning
Our model builds upon the Graph Convolutional Network.
However, instead of entity classiÔ¨Åcation, we apply it to the
zero-shot recognition with a regression loss. The input of our
framework is the set of categories and their corresponding
semantic-embedding vectors (represented by X = {xi}n
For the output, we want to predict the visual classiÔ¨Åer for
each input category (represented by W = {wi}n
SpeciÔ¨Åcally, the visual classiÔ¨Åer we want the GCN to
predict is a logistic regression model on the Ô¨Åxed pre-trained
ConvNet features. If the dimensionality of visual-feature
vector is D, each classiÔ¨Åer wi for category i is also a Ddimensional vector. Thus the output of each node in the
GCN is D dimensions, instead of C dimensions. In the
zero-shot setting, we assume that the Ô¨Årst m categories in
the total n classes have enough visual examples to estimate
their weight vectors. For the remaining n‚àím categories, we
want to estimate their corresponding weight vectors given
their embedding vectors as inputs.
One way is to train a neural network (multi-layer perceptron) which takes xi as an input and learns to predict wi as
an output. The parameters of the network can be estimated
using m training pairs. However, generally m is small (in
the order of a few hundreds) and therefore, we want to use
the explicit structure of the visual world or the relationships
between categories to constrain the problem. We represent
these relationships as the knowledge-graph (KG). Each node
in the KG represents a semantic category. Since we have a
total of n categories, there are n nodes in the graph. Two
nodes are linked to each other if there is a relationship between them. The graph structure is represented by the n √ó n
adjacency matrix, A. Instead of building a bipartite graph
as , we replace all directed edges in the KG by undirected edges, which leads to a symmetric adjacency matrix.
As Fig. 2 shows, we use a 6-layer GCN where each layer
l takes as input the feature representation from previous layer
(Zl‚àí1) and outputs a new feature representation (Zl). For
the Ô¨Årst layer the input is X which is an n √ó k matrix (k is
the dimensionality of the word-embedding vector). For the
Ô¨Ånal-layer the output feature-vector is ÀÜ
W which has the size
of n √ó D; D being the dimensionality of the classiÔ¨Åer or
visual feature vector.
Loss-function: For the Ô¨Årst m categories, we have predicted
classiÔ¨Åer weights ÀÜ
W1...m and ground-truth classiÔ¨Åer weights
learned from training images W1...m. We use the meansquare error as the loss function between the predicted and
the ground truth classiÔ¨Åers.
Lmse( ÀÜwi, wi).
During training, we use the loss from the m seen categories to estimate the parameters for the GCN. Using the
estimated parameters, we obtain the classiÔ¨Åer weights for
the zero-shot categories. At test time, we Ô¨Årst extract the
image feature representations via the pre-trained ConvNet
and use these generated classiÔ¨Åers to perform classiÔ¨Åcation
on the extracted features.
3.3. Implementation Details
Our GCN is composed of 6 convolutional layers with output channel numbers as 2048 ‚Üí2048 ‚Üí1024 ‚Üí1024 ‚Üí
512 ‚ÜíD, where D represents the dimension of the object
classiÔ¨Åer. Unlike the 2-layer network presented in , our
network is much deeper. As shown in ablative studies, we
Ô¨Ånd that making the network deep is essential in generating the classiÔ¨Åer weights. For activation functions, instead
of using ReLU after each convolutional layer, we apply
LeakyReLU with the negative slope of 0.2. Empirically, we Ô¨Ånd that LeakyReLU leads to faster convergence
for our regression problem.
While training our GCN, we perform L2-Normalization
on the outputs of the networks and the ground-truth classiÔ¨Åers. During testing, the generated classiÔ¨Åers of unseen
classes are also L2-Normalized. We Ô¨Ånd adding this constraint important, as it regularizes the weights of all the
classiÔ¨Åers into similar magnitudes. In practice, we also Ô¨Ånd
that the last layer classiÔ¨Åers of the ImageNet pre-trained
networks are naturally normalized. That is, if we perform
L2-Normalization on each of the last layer classiÔ¨Åers during
testing, the performance on the ImageNet 2012 1K-class
validation set changes marginally (< 1%).
To obtain the word embeddings for GCN inputs, we use
the GloVe text model trained on the Wikipedia dataset,
which leads to 300-d vectors. For the classes whose names
contain multiple words, we match all the words in the trained
model and Ô¨Ånd their embeddings. By averaging these word
embeddings, we obtain the class embedding.
4. Experiment
We now perform experiments to showcase that our approach: (a) improves the state-of-the-art by a signiÔ¨Åcant
margin; (b) is robust to different pre-trained ConvNets and
noise in the KG. We use two datasets in our experiments. The
Ô¨Årst dataset we use is constructed from publicly-available
knowledge bases. The dataset consists of relationships and
graph from Never-Ending Language Learning (NELL) 
and images from Never-Ending Image Learning (NEIL) .
This is an ideal dataset for: (a) demonstrating that our approach is robust even with automatically learned (and noisy)
KG; (b) ablative studies since the KG in this domain is rich,
and we can perform ablations on KG as well.
Our Ô¨Ånal experiments are shown on the standard ImageNet dataset. We use the same settings as the baseline approaches together with the WordNet knowledge graph. We show that our approach surpasses the stateof-the-art methods by a signiÔ¨Åcant margin.
4.1. Experiments on NELL and NEIL
Dataset settings. For this experiment, we construct a new
knowledge graph based on the NELL and NEIL 
datasets. SpeciÔ¨Åcally, the object nodes in NEIL correspond
to the nodes in NELL. The NEIL dataset offers the sources
of images and the NELL dataset offers the common sense
knowledge rules. However, the NELL graph is incredibly
large 1: it contains roughly 1.7M types of object entities and
around 2.4M edges representing the relationships between
every two objects. Furthermore, since NELL is constructed
automatically, there are noisy edges in the graph. Therefore,
we create sub-graphs for our experiments.
The process of constructing this sub-graph is straightforward. We perform Breadth-Ô¨Årst search (BFS) starting from
the NEIL nodes. We discover paths with maximum length
K hops such that the Ô¨Årst and last node in the path are NEIL
nodes. We add all the nodes and edges in these paths into our
sub-graph. We set K = 7 during BFS because we discover
a path longer than 7 hops will cause the connection between
two objects noisy and unreasonable. For example, ‚Äújeep‚Äù
can be connected to ‚Äúdeer‚Äù in a long path but they are hardly
semantically related.
Note that each edge in NELL has a conÔ¨Ådence value that
is usually larger than 0.9. For our experiments, we create two
1 
NEIL Nodes
(Train/Test)
High Value Edges
Table 1. Dataset Statistics: Two different sizes of knowledge graphs
in our experiment.
different versions of sub-graphs. The Ô¨Årst smaller version is
a graph with high value edges (larger than 0.999), and the
second one used all the edges regardless of their conÔ¨Ådence
values. The statistics of the two sub-graphs are summarized
in Table 1. For the larger sub-graph, we have 14K object
nodes. Among these nodes, 704 of them have corresponding
images in the NEIL database. We use 616 classes for training
our GCN and leave 88 classes for testing. Note that these
88 testing classes are randomly selected among the classes
that have no overlap with the 1000 classes in the standard
ImageNet classiÔ¨Åcation dataset. The smaller knowledge
graph is around half the size of the larger one. We use the
same 88 testing classes in both settings
Training details. For training the ConvNet on NEIL images,
we use the 310K images associated with the 616 training
classes. The evaluation is performed on the randomly selected 12K images associated with the 88 testing classes,
i.e. all images from the training classes are excluded during
testing. We Ô¨Åne-tune the ImageNet pre-trained VGGM 
network architecture with relatively small fc7 outputs (128dimension). Thus the object classiÔ¨Åer dimension in fc8 is
128. For training our GCN, we use the ADAM optimizer with learning rate 0.001 and weight decay 0.0005. We
train our GCN for 300 epochs for every experiment.
Baseline method. We compare our method with one of the
state-of-the-art methods, ConSE , which shows slightly
better performance than DeViSE in ImageNet. As a
brief introduction, ConSE Ô¨Årst feedforwards the test image
into a ConvNet that is trained only on the training classes.
With the output probabilities, ConSE selects top T predictions {pi}T
i=1 and the word embeddings {xi}T
i=1 of
these classes. It then generates a new word embedding by
weighted averaging the T embeddings with the probability
i=1 pixi. This new embedding is applied to perform
nearest neighbors in the word embeddings of the testing
classes. The top retrieved classes are selected as the Ô¨Ånal
result. We enumerate different values of T for evaluations.
Quantitative Results. We perform evaluations on the task
of 88 unseen categories classiÔ¨Åcation. Our metric is based
on the percentage of correctly retrieved test data (out of top k
retrievals) for a given zero-shot class. The results are shown
in Table 2. We evaluate our method on two different sizes
of knowledge graphs. We use ‚ÄúHigh Value Edges‚Äù to denote
the knowledge graph constructed based on high conÔ¨Ådence
edges. ‚ÄúAll Edges‚Äù represents the graph constructed with all
the edges. We denote the baseline as ‚ÄúConSE(T)‚Äù where
High Value
ConSE(431)
ConSE(616)
Table 2. Top-k accuracy for different models in different settings.
Figure 3. We randomly drop 5% to 50% of the edges in the ‚ÄúAll
Edges‚Äù graph and show the top-1, top-5 and top-10 accuracies.
we set T to be 5, 10 and the number of training classes.
Our method outperforms the ConSE baseline by a large
margin. In the ‚ÄúAll Edges‚Äù dataset, our method outperforms
ConSE 3.6% in top-1 accuracy. More impressively, the accuracy of our method is almost 2 times as that of ConSE
in top-2 metric and even more than 2 times in top-5 and
top-10 accuracies. These results show that using knowledge graph with word embeddings in our method leads to
much better result than the state-of-the-art results with word
embeddings only.
From small to larger graph. In addition to improving performance in zero-shot recognition, our method obtains more
performance gain as our graph size increases. As shown in
Table 2, our method performs better by switching from the
small to larger graph. Our approach has obtained 2 ‚àº3% improvements in all the metrics. On the other hand, there is little to no improvements in ConSE performance. It also shows
that the KG does not need to be hand-crafted or cleaned. Our
approach is able to robustly handle the errors in the graph
structure.
Resilience to Missing Edges We explore how the performance of our model changes if we randomly drop 5% to
50% of the edges in the ‚ÄúAll Edges‚Äù graph. As Fig. 3 shows,
by dropping from 5% to 10% of edges, the performance of
our model changes negligibly. This is mainly because the
Figure 4. We compute the minimum Euclidean distances between
predicted and training classiÔ¨Åers. The distances are plotted by
sorting them from small to large.
knowledge graph can have redundant information with 14K
nodes and 97K edges connecting them. This again implies
that our model is robust to small noisy changes in the graph.
As we start deleting more than 30% of the edges, the accuracies drop drastically. This indicates that the performance of
our model is highly correlated to the size of the knowledge
Random Graph? It is clear that our approach can handle
noise in the graph. But does any random graph work? To
demonstrate that the structure of the graph is still critical
we also created some trivial graphs: (i) star model: we
create a graph with one single root node and only have edges
connecting object nodes to the root node; (ii) random graph:
all nodes in the graph are randomly connected. Table 3
shows the results. It is clear that all the numbers are close to
random guessing, which means a reasonable graph plays an
important role and a random graph can have negative effects
on the model.
Trivial KG
Star Model
Random Graph
Table 3. Top-k accuracy on trivial knowledge graphs we create.
How important is the depth of GCN? We show that making the Graph Convolutional Network deep is critical in our
problem. We show the performance of using different numbers of layers for our model on the ‚ÄúAll Edges‚Äù knowledge
graph shown in Table 4. For the 2-layer model we use 512
hidden neurons, and the 4-layer model has output channel
numbers as 2048 ‚Üí1024 ‚Üí512 ‚Üí128. We show that
the performance keeps increasing as we make the model
deeper from 2-layer to 6-layer. The reason is that increasing
the times of convolutions is essentially increasing the times
of message passing between nodes in the graph. However,
we do not observe much gain by adding more layers above
Ours (2-layer)
Ours (4-layer)
Ours (6-layer)
Table 4. Top-k accuracy with different depths of our model.
the 6-layer model. One potential reason might be that the
optimization becomes harder as the network goes deeper.
Is our network just copying classiÔ¨Åers as outputs? Even
though we show our method is better than ConSE baseline, is
it possible that it learns to selectively copy the nearby classi-
Ô¨Åers? To show our method is not learning this trivial solution,
we compute the Euclidean distance between our generated
classiÔ¨Åers and the training classiÔ¨Åers. More speciÔ¨Åcally, for
a generated classiÔ¨Åer, we compare it with the classiÔ¨Åers from
the training classes that are at most 3-hops away. We calculate the minimum distance between each generated classiÔ¨Åer
and its neighbors. We sort the distances for all 88 classi-
Ô¨Åers and plot Fig. 4. As for reference, the distance between
‚Äúwooden spoon‚Äù and ‚Äúspoon‚Äù classiÔ¨Åers in the training set is
0.26 and the distance between ‚Äúwooden spoon‚Äù and ‚Äúoptimus prime‚Äù is 0.78. We can see that our predicted classiÔ¨Åers
Figure 5. t-SNE visualizations for our word embeddings and GCN
output visual classiÔ¨Åers in the ‚ÄúAll Edges‚Äù dataset. The test classes
are shown in red.
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
Inception-v1
(a) Top-k accuracy for different models when testing on only unseen
DeViSE 
ConSE 
Inception-v1
Inception-v1
DeViSE 
ConSE 
Inception-v1
Inception-v1
DeViSE 
ConSE 
Inception-v1
Inception-v1
(b) Top-k accuracy for different models when testing on both seen and
unseen classes (a more practical and generalized setting).
Table 5. Results on ImageNet. We test our model on 2 different settings over 3 different datasets.
are quite different from its neighbors.
Are the outputs only relying on the word embeddings?
We perform t-SNE visualizations to show that our output classiÔ¨Åers are not just derived from the word embeddings.
We show the t-SNE plots of both the word embeddings
and the classiÔ¨Åers of the seen and unseen classes in the ‚ÄúAll
Edges‚Äù dataset. As Fig. 5 shows, we have very different clustering results between the word embeddings and the object
classiÔ¨Åers, which indicates that our GCN is not just learning
a direct projection from word embeddings to classiÔ¨Åers.
4.2. Experiments on WordNet and ImageNet
We now perform our experiments on a much larger-scale
ImageNet dataset. We adopt the same train/test split
settings as . More speciÔ¨Åcally, we report our results
on 3 different test datasets: ‚Äú2-hops‚Äù, ‚Äú3-hops‚Äù and the
whole ‚ÄúAll‚Äù ImageNet set. These datasets are constructed
according to how similar the classes are related to the classes
in the ImageNet 2012 1K dataset. For example, ‚Äú2-hops‚Äù
dataset (around 1.5K classes) includes the classes from the
ImageNet 2011 21K set which are semantically very similar
to the ImageNet 2012 1K classes. ‚Äú3-hops‚Äù dataset (around
7.8K classes) includes the classes that are within 3 hops of
the ImageNet 2012 1K classes, and the ‚ÄúAll‚Äù dataset includes
all the labels in ImageNet 2011 21K. There are no common
labels between the ImageNet 1K class and the classes in
these 3-dataset. It is also obvious to see that as the number
of class increases, the task becomes more challenging.
As for knowledge graph, we use the sub-graph of the
WordNet , which includes around 30K object nodes2.
Training details. Note that to perform testing on 3 differ-
2 
ent test sets, we only need to train one set of ConvNet and
GCN. We use two different types of ConvNets as the base
network for computing visual features: Inception-v1 
and ResNet-50 . Both networks are pre-trained using
the ImageNet 2012 1K dataset and no Ô¨Åne-tuning is required.
For Inception-v1, the output feature of the second to the
last layer has 1024 dimensions, which leads to D = 1024
object classiÔ¨Åers in the last layer. For ResNet-50, we have
D = 2048. Except for the changes of output targets, other
settings of training GCN remain the same as those of the previous experiments on NELL and NEIL. It is worthy to note
that our GCN model is robust to different sizes of outputs.
The model shows consistently better results as the representation (features) improves from Inception-v1 (68.7% top-1
accuracy in ImageNet 1K val set) to ResNet-50 (75.3%).
We evaluate our method with the same metric as the
previous experiments: the percentage of hitting the groundtruth labels among the top k predictions. However, instead
of only testing with the unseen object classiÔ¨Åers, we include
both training and the predicted classiÔ¨Åers during testing, as
suggested by . Note that in these two settings of
experiments, we still perform testing on the same set of
images associated with unseen classes only.
Testing without considering the training labels. We Ô¨Årst
perform experiments excluding the classiÔ¨Åers belonging to
the training classes during testing. We report our results in
Table. 5a. We compare our results to the recent state-of-theart methods SYNC and EXEM . We show experiments
with the same pre-trained ConvNets (Inception-v1) as .
Due to unavailability of their word embeddings for all the
nodes in KG, we use a different set of word embeddings
(GloVe) ,which is publicly available.
GoogleNews
GoogleNews
Table 6. Results with different word embeddings on ImageNet (2
hops), corresponding to the experiments in Table 5a.
Therefore, we Ô¨Årst investigate if the change of wordembedding is crucial. We show this via the ConSE baseline.
Our re-implementation of ConSE, shown as ‚ÄúConSE(us)‚Äù
in the table, uses the GloVe whereas the ConSE method
implemented in uses their own word embedding. We
see that both approaches have similar performance. Ours is
slightly better in top-1 accuracy while the one in is
better in top-20 accuracy. Thus, with respect to zero-shot
learning, both word-embeddings seem equally powerful.
We then compare our results with SYNC and
EXEM . With the same pre-trained ConvNet Inceptionv1, our method outperforms almost all the other methods on
all the datasets and metrics. On the ‚Äú2-hops‚Äù dataset, our approach outperforms all methods with a large margin: around
6% on top-1 accuracy and 17% on top-5 accuracy. On the
‚Äú3-hops‚Äù dataset, our approach is consistently better than
EXEM around 2 ‚àº3% from top-5 to top-20 metrics.
By replacing the Inception-v1 with the ResNet-50, we
obtain another performance boost in all metrics. For the
top-5 metric, our Ô¨Ånal model outperforms the state-of-the-art
method EXEM by a whooping 20.9% in the ‚Äú2-hops‚Äù
dataset, 3.5% in the ‚Äú3-hops‚Äù dataset and 1% in the ‚ÄúAll‚Äù
dataset. Note that the gain is diminishing because the task
increases in difÔ¨Åculty as the number of unseen classes increases.
Sensitivity to word embeddings. Is our method sensitive
to word embeddings? What will happen if we use different
word embeddings as inputs? We investigate 3 different word
embeddings including GloVe (which is used in the other
experiments in the paper), FastText and word2vec 
trained with GoogleNews. As for comparisons, we have
also implemented the method in which trains a direct
mapping from word embeddings to visual features without
knowledge graphs. We use the Inception-v1 ConvNet to extract visual features. We show the results on ImageNet (with
the 2-hops setting same as Table 5a). We can see that 
highly relies on the quality of the word embeddings (top-5
results range from 17.2% to 33.5%). On the other hand, our
top-5 results are stably around 50% and are much higher
than . With the GloVe word embeddings, our approach
has a relative improvement of almost 200% over .
This again shows graph convolutions with knowledge graphs
Test Image
ConSE (10)
panthera tigris(train)
tiger cat (train)
felis onca (train)
leopard (train)
tiger shark (train)
tigress (test)
bengal tiger (test)
panthera tigris (train)
tiger cub (test)
tiger cat (train)
rock beauty (train)
ringlet (train)
flagpole (train)
large slipper (test)
yellow slipper (train)
butterfly fish (test)
rock beauty (train)
damselfish (test)
atoll (test)
barrier reef (test)
tractor (train)
reaper (train)
thresher (train)
trailer truck (train)
motortruck (test)
tracked vehicle (test)
tractor (train)
propelled vehicle (test)
reaper (train)
forklift (train)
Figure 6. Visualization of top 5 prediction results for 3 different
images. The correct prediction results are highlighted by red bold
characters. The unseen classes are marked with a red ‚Äútest‚Äù in the
bracket. Previously seen classes have a plain ‚Äútrain‚Äù in the bracket.
play a signiÔ¨Åcant role in improving zero-shot recognition.
Testing with the training classiÔ¨Åers. Following the suggestions in , a more practical setting for zero-shot
recognition is to include both seen and unseen category classiÔ¨Åers during testing. We test our method in this generalized
setting. Since there are very few baselines available for
this setting of experiment, we can only compare the results
with ConSE and DeViSE. We have also re-implemented the
ConSE baselines with both Inception-v1 and ResNet-50 pretrained networks. As Table 5b shows our method almost
doubles the performance compared to the baselines on every metric and all 3-datasets. Moreover, we can still see
the boost in of performance by switching the pre-trained
Inception-v1 network to ResNet-50.
Visualizations. We Ô¨Ånally perform visualizations using our
model and ConSE with T = 10 in Fig. 6 (Top-5 prediction
results). We can see that our method signiÔ¨Åcantly outperforms ConSE(10) in these examples. Although ConSE(10)
still gives reasonable results in most cases, the output labels
are biased to be within the training labels. On the other hand,
our method outputs the unseen classes as well.
5. Conclusion
We have presented an approach for zero-shot recognition using the semantic embeddings of a category and the
knowledge graph that encodes the relationship of the novel
category to familiar categories. Our work also shows that a
knowledge graph provides supervision to learn meaningful
classiÔ¨Åers on top of semantic embeddings. Our results indicate a signiÔ¨Åcant improvement over current state-of-the-art.
Acknowledgement:
This work was supported by ONR MURI
N000141612007, Sloan, Okawa Fellowship to AG and NVIDIA Fellowship
to XW. We would also like to thank Xinlei Chen, Senthil Purushwalkam,
Zhilin Yang and Abulhair Saparov for many helpful discussions.