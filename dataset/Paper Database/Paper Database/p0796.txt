Identifying Stable Patterns over Time for
Emotion Recognition from EEG
Wei-Long Zheng, Jia-Yi Zhu, and Bao-Liang Lu*, Senior Member, IEEE
Abstract—In this paper, we investigate stable patterns of electroencephalogram (EEG) over time for emotion recognition using
a machine learning approach. Up to now, various ﬁndings of activated patterns associated with different emotions have been
reported. However, their stability over time has not been fully investigated yet. In this paper, we focus on identifying EEG stability
in emotion recognition. To validate the efﬁciency of the machine learning algorithms used in this study, we systematically evaluate
the performance of various popular feature extraction, feature selection, feature smoothing and pattern classiﬁcation methods
with the DEAP dataset and a newly developed dataset for this study. The experimental results indicate that stable patterns
exhibit consistency across sessions; the lateral temporal areas activate more for positive emotion than negative one in beta and
gamma bands; the neural patterns of neutral emotion have higher alpha responses at parietal and occipital sites; and for negative
emotion, the neural patterns have signiﬁcant higher delta responses at parietal and occipital sites and higher gamma responses
at prefrontal sites. The performance of our emotion recognition system shows that the neural patterns are relatively stable within
and between sessions.
Index Terms—Affective Computing, Affective Brain-Computer Interaction, Emotion Recognition, EEG, Stable EEG Patterns,
Machine Learning
INTRODUCTION
MOTION plays an important role in human
communication and decision making. Although
it seems natural to us with emotions in our daily
life, we have little knowledge on the mechanisms of
emotional function of the brain and modeling human
emotion. In recent years, the research on emotion
recognition from EEG has attracted great interest from
a vast amount of interdisciplinary ﬁelds from psychology to engineering, including basic studies on
emotion theories and applications to affective Brain-
Computer Interaction (aBCI) , which enhances the
BCI systems with the ability to detect, process, and
respond to users affective states using physiological
Although many progresses in theories, methods
and experiments that support affective computing
have been made in the past several years, the problem
of detecting and modeling human emotions in aBCI
remains largely unexplored . Emotion recognition
is critical because computers can never respond to
users’ emotional states without recognizing human
emotions. However, emotion recognition from EEG
is very challenging due to the fuzzy boundaries and
• Wei-Long Zheng, Jia-Yi Zhu and Bao-Liang Lu are with the Center
for Brain-Like Computing and Machine Intelligence, Department of
Computer Science and Engineering, Shanghai Jiao Tong University
and Key Laboratory of Shanghai Education Commission for Intelligent
Interaction and Cognitive Engineering, Shanghai Jiao Tong University,
800 Dong Chuan Road, Shanghai 200240, China.
E-mail: {weilonglive,zhujiayi1991}@gmail.com, .
*Corresponding author
individual different variations of emotion. In addition,
we can’t obtain the ‘ground truth’ of human emotions
in theory, that is, the true label of EEG corresponding
different emotional states. The reason is that emotion
is considered as a function of time, context, space,
language, culture, and races .
Many previous studies focus on subject-dependent
and subject-independent patterns and evaluations for
emotion recognition. However, the stable patterns
and performance of models over time are not fully
exploited, which are very important for real world
applications. Stable EEG patterns are considered as
neural activities such as critical brain areas and critical
frequency bands that share commonality across individuals and sessions under different emotional states.
Although task-related EEG is sensitive to change due
to different cognitive states and environmental variables , we intuitively think that the stable patterns
for speciﬁc tasks should exhibit consistency among
repeated sessions of the same subjects. In this paper, we focus on the following issues of EEG-based
emotion recognition: What is the capability of EEG
signals for discrimination of different emotions? Are
there any stable EEG patterns of neural oscillations or
brain regions for representing emotions? What is the
performance of the models based on machine learning
approach from day to day?
The main contributions of this paper to emotion
recognition from EEG can be summarized as follows:
1) We have developed a novel emotion EEG dataset
as a subset of SEED (SJTU Emotion EEG Dataset),
that will be publicly available for research to evaluate
stable patterns across subjects and sessions. To the
 
best of our knowledge, there is no available public
EEG dataset for the analysis of stability of neural
patterns regarding emotion recognition. 2) We carry
out a systematic comparison and a qualitative evaluation of different feature extraction, feature selection,
feature smoothing and pattern classiﬁcation methods
on a public available EEG dataset, DEAP, and our
own dataset, SEED. 3) We adopt discriminative Graph
regularized Extreme Learning Machine (GELM) to
identify the stable patterns over time and evaluate the
stability of our emotion recognition model with crosssession schemes. 4) Our experiment results reveal that
neural signatures for three emotions (positive, neutral
and negative) do exist and the EEG patterns at critical frequency bands and brain regions are relatively
stable within and between sessions.
The layout of the paper is as follows. In Section 2,
we give a brief overview of related work on emotion
recognition from EEG, as well as the ﬁndings of stable
patterns for different emotions. A systematic description of brain signal analysis methods and classiﬁcation procedure for feature extraction, dimensionality
reduction and classiﬁers is given in Section 3. Section 4
presents the motivation and rationale for our emotion
experimental setting. A detailed explanation of all
the materials and protocol we used is described. A
systematic evaluation on different methods is conducted on the DEAP dataset and our SEED dataset.
We use time-frequency analysis to ﬁnd the neural
signatures and stable patterns for different emotions,
and evaluate the stability of our emotion recognition
models over time. In Section 5, we make a conclusion
about our work.
RELATED WORK
Emotion Recognition Methods
In the ﬁeld of affective computing, a vast amount of
studies has been conducted toward emotion recognition based on different signals. Many efforts have
been made to recognize emotions using external emotion expression, such as facial expression , speech
 , and gestures . However, sometimes the emotional states remain internal and cannot be detected by
external expression. Considering extreme cases where
people do not say anything but actually they are angry, even smile during negative emotional states due
to the social masking . In these cases, the external
emotional expression can be controlled subjectively
and these external cues for emotion recognition may
be inadequate.
Emotion is an experience that is associated with a
particular pattern of physiological activity including
central nervous system and autonomic nervous system. Contrary to audiovisual expression, it is more objective and straightforward to recognize emotions using physiological activities. These physiological activities can be recorded by noninvasive sensors, mostly
as electrical signals. These measures include skin conductivity, electrocardiogram, electromyogram, electrooculogram, and EEG. A detailed review of emotion
recognition methods can be found in .
With the fast development of micro-nano technologies and embedded systems, it is now conceivable
to port aBCI systems from laboratory to real-world
environments. Many advanced dry electrodes and
embedded systems are developed to handle the wearability, portability, and practical use of these systems
in real world applications , . Various studies in
affective computing community try to build computational models to estimate emotional states based on
EEG features. In short, a brief summary of emotion
recognition using EEG is presented in Table 1. These
studies show the efﬁciency and feasibility of building
computational models of emotion recognition using
EEG. In these studies, the stimuli used in emotion
recognition experiments contains still images, music
and videos and emotions evaluated in most studies
are discrete.
EEG Patterns Associated with Emotions
One of the goals in affective neuroscience is to examine whether patterns of brain activity for speciﬁc emotions exist, and whether these patterns are to some
extent common across individuals. Various studies
have examined the neural correlations of emotions. It
seems that there don’t exist any processing modules
for speciﬁc emotion. However, there may exist neural
signatures of speciﬁc emotion, as a distributed pattern
of brain activity . Mauss and Robinson proposed that emotional state is likely to involve circuits
rather than any brain region considered in isolation.
To AC researchers, to identify neural patterns that
are both common across subjects and stable across
sessions can provide valuable information for emotion
recognition from EEG.
Cortical activity in response to emotional cues was
related to the lateralization effect. Muller et al. 
reported increased gamma (30-50Hz) power for negative valence over left temporal region. Davidson et al.
 , showed that frontal EEG asymmetry is hypothesized to relate to approach and withdrawal emotions, with heightened approach tendencies reﬂected
in left frontal activity and heightened withdrawal
tendencies reﬂected in relative right-frontal activity.
Nie et al. reported that the subject-independent
features associated with positive and negative emotions are mainly in the right occipital lobe and parietal
lobe for the alpha band, the central site for beta band,
and the left frontal lobe and right temporal lobe for
gamma band. Balconi et al. found that frequency
band modulations are effected by valence and arousal
rating, with an increased response for high arousing
and negative or positive stimuli in comparison with
low arousing and neutral stimuli.
Various studies on emotion classiﬁcation using EEG and the best performance reported in each study
Method Description
Emotion states
Power of alpha and beta, then PCA, 5
subjects, classiﬁcation with FDA
Valence and
Valence: 92.3%,
arousal: 92.3%
Amplitudes of four frequency bands,
17 subjects, evaluated KNN, Bagging
Valence (12),
arousal (12) and
dominance (12)
Valence: 74%,
arousal: 74%, and
dominance: 75%
Wavelet features of alpha, beta and
gamma, 20 subjects, classiﬁcation with
KNN and LDA
disgust, happy,
surprise, fear and
Power spectral density and asymmetry
features of ﬁve frequency bands, 26
subjects, evaluated SVM
Joy, anger, sadness,
and pleasure
Spectral power features, 11 subjects,
Positive, negative
and neutral
Asymmetry index of alpha and beta
power, 16 subjects, SVM
Four quadrants of
the valence-arousal
94.4% (subjectdependent),
62.58% (subjectindependent)
Spectral power features of ﬁve
frequency bands, 32 subjects, Gaussian
naive Bayes classiﬁer
Valence (2), arousal
(2) and liking (2)
Valence: 57.6%,
arousal: 62% and
liking: 55.4%
Time-frequency (TF) analysis, 9
subjects, KNN, QDA and SVM
Like and dislike
Power spectral density features of ﬁve
frequency bands, modality fusion with
eye track, 24 subjects, SVM
Valence (3) and
arousal (3)
Valence: 68.5%,
arousal: 76.4%
Power spectrum features, wavelet
features, nonlinear dynamical features,
6 subjects, SVM
Positive and
Higher Order Crossings, Higher Order
Spectra and Hilbert-Huang Spectrum
features, 16 subjects, QDA
Happy, curious,
angry, sad, quiet
IAPS and IADS denotes the International Affective Picture System and the International Affective Digital Sounds, respectively.
The numbers given in parenthesis denote the numbers of categories for each dimension. Pattern study denotes revealing
neural activities (critical brain areas and critical frequency bands) that share commonality across subjects or sessions. Classiﬁers
include K Nearest Neighbors (KNN), Fisher’s Discriminant Analysis (FDA), Linear Discriminant Analysis (LDA), Quadratic
Discriminant Analysis (QDA), Bagging, and Support Vector Machine (SVM).
recognition,
subjectdependent
subject-independent
evaluating
performance
emotion recognition systems. As shown in Table 1,
some ﬁndings of activated patterns such as critical
channels and oscillations associated with different
emotions have been proposed. However, a major
limitation is that they extract activated patterns only
across subject but do not consider the time factor.
The studies of internal consistency and test-retest
stability of EEG can be dated back to many years ago
 , , , especially for clinical applications .
McEvoy et al. proposed that under appropriate
conditions, task-related EEG has sufﬁcient retest reliability for use in assessing clinical changes. However,
these previous studies investigated the stability of
EEG features under different conditions, for example,
a working memory task . Moreover, in these studies, stability and reliability are often quantiﬁed using
statistical parameters such as intraclass correlation
coefﬁcients , instead of the performance of pattern
classiﬁers.
So far, a few preliminary studies on stability and
reliability of neural patterns for emotion recognition
have been conducted. Lan et al. presented a pilot
study of stability of features in emotion recognition
algorithms. However, in their stability assessment, the
same features derived from the same channel from the
same emotion class of the same subject were grouped
together to compute the correlation coefﬁcients. Furthermore, their experiments were conducted on a
small group of subjects with 14-channel EEG signals.
They investigated the stability of each feature instead
of neural patterns that we focus on in this paper.
Up to now, there is no systematic evaluation about
the stability for activated patterns over time in previous studies. The performance of emotion recognition
systems over time is still an unsolved problem for
developing real-world application systems. Therefore,
our major aim in this paper is to investigate the stable
EEG patterns over time using time frequency analysis
and machine learning approaches. Here, we want to
emphasize that we do not study neural patterns under
emotion regulation , but for speciﬁc emotional
states during different times.
To investigate various critical problems of emotion
recognition from EEG, we face a serious lack of publicly available emotional EEG datasets. To the best of
our knowledge, the only publicly available emotional
EEG datasets are MAHNOB HCI and DEAP .
The ﬁrst one includes EEG, physiological signals, eye
gaze, audio, and facial expressions of 30 people when
watching 20 emotional videos. The DEAP dataset
includes the EEG and peripheral physiological signals of 32 participants when watching 40 one-minute
music videos. It also contains participants’ rate of
each video in terms of the levels of arousal, valence,
like/dislike, dominance, and familiarity. However,
these datasets do not contain EEG data from different
sessions for the same subject, which can not be used
for investigating the stable patterns over time. Since
there is no available published EEG dataset for the
analysis of stability of neural patterns for emotion
recognition, we develop a new emotional EEG dataset
for this study as a subset of SEED1.
EMOTION EXPERIMENT DESIGN
In order to investigate neural signatures of different emotions and stable patterns over time, we design a new emotion experiment to collect EEG data,
which is different from other existing publicly available datasets. In our experiments, the same subject
performs the emotion experiments three times with
an interval of one week or longer. We choose ﬁlm
clips as emotion elicitation materials. These emotional
ﬁlms contain both scene and audio, which can expose
subjects to more real-life scenarios and elicit strong
subjective and physiological changes.
In our emotion experiments, Chinese ﬁlm clips are
used considering that native culture factors may affect
elicitation in emotion experiments . In the preliminary study, we selected a pool of emotional ﬁlm
clips from famous Chinese ﬁlms manually. Twenty
participants were asked to assess their emotions when
watching the selected ﬁlm clips by scores (1-5) and
keywords (positive, neutral and negative). The criteria
for selecting ﬁlm clips are as follows: (a) the length
of the whole experiment should not be too long in
case it will make subjects visual fatigue; (b) the videos
should be understood without explanation; and (c) the
videos should elicit a single desired target emotion.
At last, 15 Chinese ﬁlm clips for positive, neutral
and negative emotions were chosen from the pool of
materials, which received 3 or higher sores of mean
ratings from the participants. Each emotion has ﬁve
1. 
ﬁlm clips in one experiment. The duration of each ﬁlm
clip is about 4 minutes. Each ﬁlm clip is well edited
to create coherent emotion eliciting. The details of the
ﬁlm clips used in the experiments are listed in Table 2.
Details of ﬁlm clips used in our emotion experiment
No. Labels
Film clips sources
negative Tangshan Earthquake
negative Back to 1942
positive Lost in Thailand
positive Flirting Scholar
positive Just Another Pandora’s Box
World Heritage in China
Fifteen subjects (7 males and 8 females; mean: 23.27,
std: 2.37) who are different from those in the preliminary study, participate in the experiments. In order
to investigate neural signatures and stable patterns
across sessions and individuals, each subject is required to perform the experiments for three sessions.
The time interval between two sessions is one week
or longer. All participants are native Chinese students
from Shanghai Jiao Tong University with self-reported
normal or corrected-to-normal vision and normal
hearing. Before the experiments, the participants are
informed about the experiment and instructed to sit
comfortably, watch the forthcoming movie clips attentively without diverting their attention from the
screen, and refrain as much as possible from overt
movements.
Facial videos and EEG data are recorded simultaneously. EEG is recorded using an ESI NeuroScan
System2 at a sampling rate of 1000 Hz from 62-channel
active AgCl electrode cap according to the international 10-20 system. The layout of EEG electrodes on
the cap is shown in Fig. 1. The impendence of each
electrode has to be less than 5 kΩ. The frontal face
videos are recorded from the camera mounted in front
of the subjects. Facial videos are encoded into AVI
format with the frame rate of 30 frames per second
and the resolution of 160 × 120.
There are totally 15 trials for each experiment.
There is a 15s hint before each clips and 10s feedback after each clip. For the feedback, participants
are told to report their emotional reactions to each
ﬁlm clip by completing the questionnaire immediately
after watching each clip. The questions are following
Philippot : (1) what they had actually felt in
response to viewing the ﬁlm clip; (2) how they felt
at the speciﬁc time they were watching the ﬁlm clips;
(3) have they watched this movie before; (4) have they
understood the ﬁlm clips. They also rate the intensity
of subjective emotional arousal using a 5-point scale
2. 
FC 4 FC 6 FT8
PO 4 PO 6 PO 8
P2 P4 P6 P8
CP 2 CP 4 CP 6 TP 8
Fig. 1. The EEG cap layout for 62 channels
Fig. 2. The protocol used in our emotion experiment
according to what they actually felt during the task
 . Fig. 2 shows the detailed protocol. For EEG signal
processing, the raw EEG data are ﬁrst downsampled
to 200Hz sampling rate. In order to ﬁlter the noise and
remove the artifacts, the EEG data are then processed
with a bandpass ﬁlter between 0.5Hz to 70Hz.
METHODOLOGY
Feature Extraction
From our previous work , , we have found
that the following six different features and electrode
combinations are efﬁcient for EEG-based emotion
recognition: power spectral density (PSD), differential entropy (DE), differential asymmetry (DASM),
rational asymmetry (RASM), asymmetry (ASM) and
differential caudality (DCAU) features from EEG. As
a result, we use these six different features in this
study. According to ﬁve frequency bands: delta (1-
3Hz); theta (4-7Hz); alpha (8-13Hz); beta (14-30Hz);
and gamma (31-50Hz), we compute the traditional
PSD features using Short Time Fourier Transform
(STFT) with a 1s long window and no overlapping
Hanning window. The differential entropy feature is
deﬁned as follows ,
2πσ2 exp (x −µ)2
exp (x −µ)2
2 log 2πeσ2,
where X submits the Gauss distribution N(µ, σ2), x
is a variable, and π and e are constants. According
to , in a certain band, DE is equivalent to the
logarithmic power spectral density for a ﬁxed length
EEG sequence.
Because many evidences show that the lateralization between the left and right hemisphere is associated with emotions , we investigate asymmetry
features. We compute differential asymmetry (DASM)
and rational asymmetry (RASM) features as the differences and ratios between the DE features of 27
pairs of hemispheric asymmetry electrodes (Fp1-Fp2,
F7-F8, F3-F4, FT7-FT8, FC3-FC4, T7-T8, P7-P8, C3-C4,
TP7-TP8, CP3-CP4, P3-P4, O1-O2, AF3-AF4, F5-F6, F7-
F8, FC5-FC6, FC1-FC2, C5-C6, C1-C2, CP5-CP6, CP1-
CP2, P5-P6, P1-P2, PO7-PO8, PO5-PO6, PO3-PO4, and
CB1-CB2) . DASM and RASM can be expressed,
respectively, as
DASM = DE(Xleft) −DE(Xright)
RASM = DE(Xleft)/DE(Xright).
ASM features are the direct concatenation of DASM
and RASM features for comparison. In the literature, the patterns of spectral differences along frontal
and posterior brain regions have also been explored
 . To characterize the spectral-band asymmetry in
respect of caudality (in frontal-posterior direction),
we deﬁne DCAU features as the differences between
DE features of 23 pairs of frontal-posterior electrodes
(FT7-TP7, FC5-CP5, FC3-CP3, FC1-CP1, FCZ-CPZ,
FC2-CP2, FC4-CP4, FC6-CP6, FT8-TP8, F7-P7, F5-P5,
F3-P3, F1-P1, FZ-PZ, F2-P2, F4-P4, F6-P6, F8-P8, FP1-
O1, FP2-O2, FPZ-OZ, AF3-CB1, and AF4-CB2). DCAU
is deﬁned as
DCAU = DE(Xfrontal) −DE(Xposterior).
The dimensions of PSD, DE, DASM, RASM, ASM
and DCAU are 310 (62 electrodes × 5 bands), 310
(62 electrodes × 5 bands), 135 (27 electrode pairs ×
5 bands), 135 (27 electrode pairs × 5 bands), 270 (54
electrode pairs × 5 bands), and 115 (23 electrode pairs
× 5 bands), respectively.
Feature Smoothing
Most of the existing approaches to emotion recognition from EEG may be suboptimal because they map
EEG signals to static discrete emotional states and do
not take temporal dynamics of the emotional state
into account. However, in general, emotion should
not be considered as a discrete psychophysiological
variable . Here, we assume that the emotional
state is deﬁned in a continuous space and emotional
states change gradually. Our approach focuses on
tracking the change of the emotional state over time
from EEG. In our approach, we introduce the dynamic characteristics of emotional changes into emotion recognition and investigate how observed EEG
is generated from a hidden emotional state. We apply
the linear dynamic system (LDS) approach to ﬁlter out
components which are not associated with emotional
states , . For comparison, we also evaluate the
performance of conventional moving average method.
To make use of the time dependency of emotion
changes and further reduce the inﬂuence of emotionunrelated EEG, we introduce the LDS approach to
smooth features. A linear dynamic system can be
expressed as follows,
xt = zt + wt,
zt = Azt−1 + vt,
where xt denotes observed variables, zt denotes the
hidden emotion variables, A is a transition matrix,
wt is a Gaussian noise with mean ¯w and variable
Q, and vt is a Gaussian noise with mean ¯v and
variable R. These equations can also be expressed in
an equivalent form in terms of Gaussian conditional
distributions,
p(xt|zt) = N(xt|zt + ¯w, Q),
p(zt|zt−1) = N(zt|Azt−1 + ¯v, R).
The initial state is assumed to be
p(z1) = N(z1|π0, S0).
parameterized
{A, Q, R, ¯w, ¯v, π0, S0}. θ can be determined using maximum likelihood through the EM algorithm based
on the observation sequence xt. To inference the latent states zt from the observation sequence xt, the
marginal distribution, p(zt|X), must be calculated.
The latent state can be expressed as
zt = E(zt|X),
where E means expectation. This marginal distribution can be achieved by using the messages propagation method . We use cross validation to estimate
the prior parameters.
Dimensionality Reduction
We compute the initial EEG features based on the
signal analysis. However, the features extracted may
be uncorrelated with emotion states, and lead to performance degradation of classiﬁers. What’s more, the
high dimensionality of features may make classiﬁers
suffer from the ‘curse of dimensionality’ . For realworld applications, dimensionality reduction could
help to increase the speed and stability of the classiﬁer. Hence, in this study, we compare two popular
approaches: principal component analysis (PCA) algorithm and minimal redundancy maximal relevance
(MRMR) algorithm .
Principal component analysis (PCA) algorithm uses
orthogonal transformation to project high-dimension
data to a low-dimension space with a minimal loss of
information. This transformation is deﬁned in such a
way that the ﬁrst principal component has the largest
possible variance, and each succeeding component
in turn has the highest variance possible under the
constraint that it is orthogonal to the preceding components.
Although PCA can reduce the feature dimension, it
cannot preserve the original domain information such
as channel and frequency after the transformation.
Hence, we also choose the minimal redundancy maximal relevance (MRMR) algorithm to select a feature
subset from an initial feature set. The MRMR algorithm uses mutual information as relevance measure
with max-dependency criterion and minimal redundancy criterion. Max-Relevance is to search features
satisfying (11) with the mean value of all mutual
information values between individual feature xd and
max D(S, c), D = 1
where S represents the feature subset to select. When
two features highly depend on each other, the respective class-discriminative power would not change
much if one of them is removed. Therefore, the following minimal redundancy condition can be added
to select mutually exclusive features,
min R(S), R =
I(xdi, xdj).
The criterion, combining the above two constraints,
is minimal-redundancy-maximal-relevance (MRMR),
which can be expressed as
max ϕ(D, R), ϕ = D −R.
In practice, an incremental search method is used to
ﬁnd the near optimal K features.
Classiﬁcation
The extracted features are further fed to three conventional pattern classiﬁers, i.e., k nearest neighbors
(KNN) , logistic regression (LR) , and support
vector machine (SVM) and a newly developed
pattern classiﬁer, discriminative Graph regularized
Extreme Learning Machine (GELM) , to build
emotion recognition systems. For the KNN classiﬁer,
the Euclidean distance is selected as the distance
metric and the number of nearest neighbors is set to
5 using cross validation. For LR, the parameters are
computed by maximal likelihood estimation. We use
LIBLINEAR software to build the SVM classiﬁer
with linear kernel. The soft margin parameter is selected using cross-validation.
Extreme Learning Machine (ELM) is a single hidden
layer feed forward neural network (SLFN) , while
learning with local consistency of data has drawn
much attention to improve the performance of the
existing machine learning models in recent years.
Peng et al. proposed a discriminative Graph
regularized Extreme Learning Machine (GELM) based
on the idea that similar samples should share similar
properties. GELM obtains much better performance
in comparison with other models for face recognition
 and emotion classiﬁcation .
Given a training data set,
L = {(xi, ti)|xi ∈Rd, ti ∈Rm},
where xi = (xi1, xi2, · · · , xid)T and ti = (ti1, ti2, · ·
·, xim)T . In GELM, the adjacent W is deﬁned as
if hi and hj belong to the tth class
otherwise;
(g1(xi), · · · , gK(xi))T
(g1(xj), · · · , gK(xj))T are hidden layer outputs for
two input samples xi and xj . Then we can compute
the graph Laplacian L = D −W , where D is a diagonal matrix and each of the entries in D is the column
sums of W. Therefore, GELM can incorporate two
regularization terms into conventional ELM model.
The objective function of GELM is deﬁned as follows,
β ||Hβ −T||2
2 + λ1Tr(HβLβT HT ) + λ2||β||2
where Tr(HβLβT HT ) is the graph regularization
term, ||β||2 is the l2-norm regularization term, and λ1
and λ2 are regularization parameters to balance two
By setting the differentiate of the objective function
(16) with respect to β as zero, we have
β = (HHT + λ1HLHT + λ2I)−1HT
In GELM, the constraint imposed on output weights
enforces the outputs of samples from the same class
to be similar. The constraint can be formulated as a
regularization term to the objective function of basic
ELM, which also makes the output weight matrix
calculated directly.
EXPERIMENT RESULTS
Experiment Results on DEAP Data
In this section, to validate the efﬁciency of the machine learning algorithms used in this study, we ﬁrst
evaluate these algorithms with the publicly available
emotion dataset, the DEAP dataset 3 and compare
the performance of our models with other methods
used in the existing studies on the same emotion EEG
The DEAP dataset consists of EEG and peripheral physiological signals of 32 participants as each
watched 40 excerpts of one-minute duration music
videos. The EEG signals were recorded from 32 active electrodes (channels) according to the international 10-20 system, whereas peripheral physiological
signals (8 channels) include galvanic skin response,
skin temperature, blood volume pressure, respiration
rate, electromyogram and electrooculogram (horizontal and vertical). The participants rated each video in
terms of the levels of arousal, valence, like/dislike,
dominance, and familiarity. More details about the
DEAP dataset are given in .
In this experiment, we adopt an emotion representation model based on the valence-arousal model.
Each dimension has value ranging from 1 to 9. Further, we segment the four quadrants of the valencearousal (VA) space according to the ratings. LALV,
HALV, LAHV, and HAHV denote low arousal/low
valence, high arousal/low valence, low arousal/high
valence, and high arousal/high valence, respectively.
Considering the fuzzy boundary of emotions and the
variations of participants’ ratings possibly associated
with individual difference in rating scale, we add an
gap to segment the quadrants of VA space to ensure
the correct ratings of participants’ true self-elicitation
emotion and discard the EEG data whose ratings of
arousal and valence are between 4.8 and 5.2. The numbers of instances for LALV, HALV, LAHV, and HAHV
3. 
Fig. 3. The rating distribution of DEAP on the arousalvalence plane (VA plane) for the four conditions (LALV,
HALV, LAHV, and HAHV)
are 12474, 16128, 10962 and 21420, respectively. The
rating distribution of DEAP on the arousal-valence
plane (VA plane) for the four conditions is shown
in Fig. 3. We can see that the ratings are distributed
approximately uniformly . We label the EEG data
according to the participants’ ratings of valence and
We ﬁrstly extracted the PSD, DE, DASM, RASM,
ASM and DCAU features of the 32-channel EEG data.
The original EEG data that we got from DEAP dataset
are preprocessed with a downsample to 128 Hz and
a bandpass frequency ﬁlter from 4.0-45.0Hz and EOG
artifacts are removed. So we extract the features in
the four frequency bands: theta: 4-7Hz, alpha: 8-13Hz,
beta: 14-30Hz, and gamma: 31-45Hz. The features
are further smoothed with the linear dynamic system approach. Then we select SVM and GELM as
classiﬁers. In this study, we use SVM classiﬁer with
linear kernel and the number of hidden layer neurons
for GELM is ﬁxed as 10 times of the dimensions
of input. To use the entire data set for training and
testing the classiﬁers, a 5-fold cross-validation scheme
is adopted. All experiments here are performed with
5-fold cross-validation and classiﬁcation performance
is evaluated through the classiﬁcation accuracy rate.
The mean accuracy rates (%) of SVM and GELM
classiﬁers for different features obtained from
separate and total frequency bands.
Feature Classiﬁer
Table 3 shows the mean accuracy rates of SVM
and GELM classiﬁers for different features obtained
from various frequency bands (theta, alpha, beta and
gamma) and total frequency bands. It should be noted
that ‘Total’ in Table 3 represents the direct concatenation of all features from four frequency bands. Since
the EEG data of DEAP are preprocessed with a bandpass frequency ﬁlter from 4.0-45.0Hz, the results of
delta frequency bands are not included. The average
accuracies (%) are 61.46, 69.67, 52.54, 52.70, 51.82 and
55.26 for PSD, DE, DASM, RASM, ASM and DCAU
features from the total frequency bands, respectively.
The best accuracy of GELM classiﬁer is 69.67% using
DE features of total frequency bands while the best
accuracy of SVM classiﬁer is 54.34%. We also evaluate
the performance of KNN, logistic regression and SVM
with RBF kernel on DEAP, which achieve the accuracies (%) and standard deviations (%) of 35.50/14.50,
40.86/16.87, and 39.21/15.87, respectively, using the
DE features of total frequency bands. We perform one
way analysis of variance (ANOVA) to study the statistical signiﬁcance. The DE features outperform the PSD
features signiﬁcantly (p < 0.01) and for classiﬁers,
GELM performs much better than SVM (p < 0.01).
As we can also see from Table 3, the diversity of
classiﬁcation accuracy for different frequency bands
is not signiﬁcant for the DEAP dataset (p > 0.95). The
results here do not show speciﬁc frequency bands for
the quadrants of VA space. We can also see that the
DCAU features achieve comparable accuracies. These
results indicate that there exists some kind asymmetry
which has discriminative information for the four
affect elicitation conditions (LALV, HALV, LAHV, and
HAHV), as discussed in Section 2.2.
Comparison of various studies using EEG in the
DEAP dataset. Our method achieves the best
performance among these approaches.
66.6%, 66.4% for valence and arousal (2
classes), 53.4%, 51.0% for valence and
arousal (3 classes) with all 32 participants.
Koelstra et
62.0%, 57.6% for valence and arousal (2
classes) with all 32 participants.
Liu et al.
63.04% for arousal-dominance recognition
(4 classes) with selected 10 participants.
75.19 % and 81.74 % on valence and
arousal (2 classes) with selected eight participants.
69.67% for quadrants of VA space (4
classes) with all 32 participants.
The recognition accuracy comparison of various
systems using EEG signals in DEAP dataset is presented in Table 4. Here single modality signal (EEG) is
used rather than in a combined modality fusion way.
Chung et al. deﬁned a weighted-log-posterior
function for the Bayes classiﬁer and evaluated the
method with DEAP dataset. Their accuracies for valence and arousal classiﬁcation are 66.6% and 66.4%
for two classes and 53.4% and 51.0% for three classes,
respectively. Koelstra et al. developed the DEAP
dataset and they obtained an average accuracy of
62.0%, 57.6% for valence and arousal (2 classes), respectively. Liu et al. proposed a real-time fractal
dimension (FD) based valence level recognition algorithm from EEG signals and got a mean accuracy of
63.04% for arousal-dominance recognition (4 classes)
with selected 10 participants. Zhang et al. described an ontological model for representation and
integration of EEG data and their model reached an
average recognition ratio of 75.19% on valence and
81.74% on arousal for eight participants. Although
their accuracies were relatively high, the categories
of each dimension are only two and these results
were achieved with a subset of the original dataset. In
contrast, our method achieves an average accuracy of
69.67% on the same data set for quadrants of VA space
(LALV, HALV, LAHV, and HAHV) with PSD features
from theta frequency band for all 32 participants.
Experiment Results on SEED data
In this section, we present the results of our approaches evaluated with our SEED dataset. One very
important difference between SEED and DEAP is that
SEED contains three sessions at the time interval of
one week or longer for the same subject.
Performance of Emotion Recognition Models
We ﬁrst compare six different features, namely PSD,
DE, DASM, RASM, ASM and DCAU, from total
frequency bands. Here we use GELM as classiﬁer
and the number of hidden layer neurons is ﬁxed
as 10 times of the dimensions of input. We adopt
a 5-fold cross-validation scheme. From Table 5, we
can see that DE features have the highest accuracy
and the lowest standard deviation than traditional
PSD features, which imply that DE features are more
suitable for EEG-based emotion recognition than other
ﬁve different features. For the asymmetry features,
although they have fewer dimensions than PSD features, they can achieve signiﬁcant better performance
than PSD, which means that brain processing about
positive, neutral and negative emotion has asymmetrical characteristics.
The means and standard deviations of accuracies in
percentage (%) for PSD, DE, DASM, RASM, ASM and
DCAU features from total frequency bands.
We also evaluate the performance of two different feature smoothing algorithms. Here, we compare
the linear dynamic system (LDS) approach with the
conventional moving average algorithm. The size of
moving windows is ﬁve in this study. The means and
standard deviations of accuracies in percentage (%)
for without smoothing, moving average, and the LDS
approach are 70.82/9.17, 76.07/8.86 and 91.07/7.54,
respectively. We can see that the LDS approach signiﬁcantly outperforms the moving average method
(p < 0.01), which achieves 14.41% higher for accuracy.
And the results also demonstrate that feature smoothing plays a signiﬁcant role in EEG-based emotion
recognition.
We compare the performance of four different classiﬁers, KNN, Logistic Regression, SVM and GELM. In
this evaluation, DE features of 310 dimensions were
used as the inputs of classiﬁers. The parameter K
of KNN was ﬁxed constant ﬁve. For LR and linear
SVM, grid search with cross-validation was used to
tune the parameters. The mean accuracies and standard deviations in percentage (%) of KNN, LR, SVM
with RBF kernel, SVM with linear kernel and GELM
are 70.43/12.73, 84.08/8.77, 78.21/9.72, 83.26/9.08 and
91.07/7.54, respectively. From the above results, we
Frequency Bands
Accuracy (%)
Fig. 4. The average accuracies of GELM using different features obtained from ﬁve frequency bands and in
a fusion method.
Fig. 5. The spectrogram of one electrode position T7
in one experiment. As different emotions elicited, we
can see that the spectrogram has different patterns.
can see that GELM outperforms other classiﬁers with
higher accuracies and lower standard deviations,
which imply that GELM is more suited for EEGbased emotion recognition. In GELM, the constraint
imposed on output weights enforces the output of
samples from the same class to be similar.
Neural Signatures and Stable Patterns
Fig. 4 presents the average accuracies of GELM classiﬁer with the six different features extracted from
ﬁve frequency bands (Delta, Theta, Alpha, Beta and
Gamma) and the direct concatenation of these ﬁve
frequency bands. The results in Fig. 4 indicate that
features obtained from gamma and beta frequency
bands perform better than other frequency bands,
which imply that beta and gamma oscillation of brain
activity are more related with the processing of these
three emotional states than other frequency oscillation
as described in – .
Fig. 5 shows the spectrogram of one electrode position T7 in a experiment and Fig. 6 shows the average
spectrogram over subjects for each session at some
electrodes (FPZ, FT7, F7, FT8, T7, C3, CZ, C4, T8, P7,
PZ, P8 and OZ). As we can see from Figs. 5 and 6,
the spectrograms have different patterns for different
elicited emotions. The dynamics of higher frequency
oscillations are more related to positive/negative
emotions, especially for the temporal lobes. Moreover,
the neural patterns over time are relatively stable for
each session. To see the neural patterns associated
with emotion processing, we project the DE features
to the scalp to ﬁnd temporal dynamics of frequency
oscillations and stable patterns across subjects.
Fig. 7 depicts the average neural patterns for
positive, neutral, and negative emotion. The results
demonstrate that neural signatures associated with
positive, neutral, and negative emotions do exist.
The lateral temporal areas activate more for positive emotion than negative one in beta and gamma
bands, while the energy of the prefrontal area enhances for negative emotion over positive one in
beta and gamma bands. While the neural patterns
of neutral emotion are similar to negative emotion,
which both have less activation in temporal areas,
the neural patterns of neutral emotion have higher
alpha responses at parietal and occipital sites. For
negative emotion, the neural patterns have signiﬁcant
higher delta responses at parietal and occipital sites
and signiﬁcant higher gamma responses at prefrontal
sites. The existing studies , have shown that
EEG alpha activity reﬂects attentional processing and
beta activity reﬂects emotional and cognitive processes. When participants watched neutral stimuli,
they tended to be more relaxed and less attentional,
which evoked alpha responses. And when processing
positive emotion processing, the energy of beta and
gamma response enhanced. Our results are consistent
with the ﬁndings of the existing work , , .
Dimensionality Reduction
As discussed above, the brain activities of emotion
processing have critical frequency bands and brain areas, which imply that there must be a low-dimension
manifold structure for emotion related EEG signals.
Therefore, we investigate how the dimension of features will affect the performance of emotion recognition. Here, we compare two dimensionality reduction
algorithms, the principle component analysis (PCA)
algorithm and the minimal redundancy maximal relevance (MRMR) algorithm, with DE features of 310
dimensions as inputs and GELM as a classiﬁer.
Fig. 8 shows the results of dimensionality reduction.
We ﬁnd that dimensionality reduction does not affect
the performance of our model very much. For PCA
algorithm, when the dimension is reduced to 210,
the accuracy drops from 91.07% to 88.46% and then
reached a local maximum value 89.57% at dimension
of 160. For MRMR algorithms, the accuracies vary
slightly with lower dimension features.
From the performance comparison between PCA
and MRMR, we see that it is better to apply MRMR
algorithm in EEG-based emotion recognition. Because
MRMR algorithm ﬁnds the best emotion-relevant and
minimal redundancy features. It also preserves original domain information such as channel and frequency bands, which have most discriminative information for emotion recognition after the transformation. This discovery helps us to reduce the
computations of features and the complexity of the
computational models.
Fig. 9 presents the distribution of the top 20 subjectindependent features selected by correlation coefﬁcient. The top 20 features were selected from alpha
frequency bands at electrode locations (FT8), beta
frequency bands at electrode locations (AF4, F6, F8,
FT7, FC5, FC6, FT8, T7, TP7) and gamma frequency
band at electrode locations (FP2, AF4, F4, F6, F8, FT7,
FC5, FC6, T7, C5). These selected features are mostly
Accuracy (%)
Fig. 8. The results of dimensionality reduction using
PCA and MRMR
Fig. 6. The average spectrogram over subjects for each session at some electrodes, which shows the stable
neural patterns over time in temporal lobes and high frequency bands. (Red color indicates high amplitude.)
Fig. 7. The average neural patterns over all subjects and sessions for different emotions, which shows that neural
signatures associated with positive, neutral and negative emotions do exist. The lateral temporal areas activate
more for positive emotion than negative one in beta and gamma bands. While the neural patterns of neutral
emotion are similar to that of negative emotion, which both have less activation in temporal areas, the neural
patterns of neutral emotion have higher alpha responses at parietal and occipital sites. The negative emotion
patterns have signiﬁcant higher delta responses at parietal and occipital sites and higher gamma responses at
prefrontal sites.
from beta and gamma frequency bands and at lateral
temporal and frontal brain areas, which is consistent
with the above ﬁndings of time-frequency analysis.
Stability of Emotion Recognition Model over
It should be noted that SEED consists of 15 participants and each one performed the experiments three
times. The interval of two sessions is one week or
longer. This is the novelty of SEED we developed in
comparison with the existing emotion EEG datasets.
By using SEED, we evaluate whether the performance
of our emotion recognition model is stable with the
passage of time. We split the data from different
sessions of one participant to training data and testing
data and trained the model with GELM. The features
employed here are the DE features extracted from
(a) Beta Band
(b) Gamma Band
One electrode location “FT8” is from alpha frequency band
Fig. 9. Distribution of the top 20 subject-independent
features selected by correlation coefﬁcient.
The classiﬁcation accuracies (%) of training and test data from different sessions using GELM.
’1st’, ’2nd’, and ’3rd’ mean the data obtained from the ﬁrst, second, and third experiments of one participant, respectively.
total frequency bands after LDS smoothing.
The average accuracies (%) of our emotion model
across sessions.
The results are presented in Tables 6 and 7. From
the mean accuracies and standard deviations, we ﬁnd
that the accuracies with training set and test set from
the same sessions are much higher than those from
different sessions. The performance of the emotion
recognition model is better with train data and test
data obtained from sessions performed in nearer time.
A comparative mean classiﬁcation accuracy of 79.28%
is achieved using our emotion recognition model with
training and testing datasets from different sessions.
This result implies that the relation between the variation of emotional states and the EEG signal is stable
for one person in a period of time. With the passage
of time, the performance of the model may become
worse. So the adaption of the computational model
should be further studied as future work.
Now we consider the situation of cross-subject and
examine the subject-independent emotion recognition
model. Here, we employ a leave-one-out cross validation to investigate the classiﬁcation performance in
a subject-independent approach and use linear SVM
classiﬁer with DE features from ﬁve frequency bands
as inputs. The average accuracy and standard deviation with subject-independent features reach 60.93%
and 13.95%, respectively. These results indicate that
the subject-independent features are relatively stable
and it is possible to build a common emotion recognition model. But on the other hand, the factors of
individual difference should be considered to build a
more robust affective computing model.
We have investigated how stable our emotion
recognition model both across subjects and sessions,
and we ﬁnd that the performance of the model across
subjects and sessions are worse than that on single
experiment. In general, we want to train the model on
the EEG data from a set of subjects or sessions and
make inference on the new data from other unseen
subjects or sessions. However, it is technically difﬁcult
due to individual differences across subjects with the
inherent variability of the EEG measurements such
as environmental variables . Although different
emotions share some commonalities of neural patterns
as we report above, they still contains some individual differences for different subjects and different
sessions, which may lead to the changes of underlying
probability distribution from subject to subject or from
session to session. This is why the average accuracy
of the classiﬁers trained and tested on each individual
subject or session is much higher than that of a
classiﬁer trained on a set of subjects or sessions and
tested on other subjects or sessions.
CONCLUSIONS AND FUTURE WORK
In this paper, we have systematically evaluated the
performance of different popular feature extraction,
feature selection, feature smoothing and pattern classiﬁcation methods for emotion recognition on both
our SEED dataset and the public DEAP dataset. From
the experimental results, we have found that GELM
with the differential entropy features outperforms
other methods. For DEAP dataset, the best average
classiﬁcation accuracy of 69.67 percent for quadrants
of VA space with 32 participants is obtained using
5-fold cross-validations. For our SEED dataset, the
best average classiﬁcation accuracy of 91.07 percent
from 45 experiments is obtained using 5-fold crossvalidations. The comparative classiﬁcation accuracies
achieved show the reliability and superior performance of our machine learning methods in comparison with the existing approaches. We have utilized
these methods to investigate the stability of neural
patterns over time.
On our SEED dataset, an average classiﬁcation
accuracy of 79.28% is achieved with training and
testing datasets from different sessions. The experimental results indicate that neural signatures and
stable EEG patterns associated with positive, neutral
and negative emotions do exist. We have found that
the lateral temporal areas activate more for positive emotion than negative one in beta and gamma
bands; the neural patterns of neutral emotion have
higher alpha responses at parietal and occipital sites;
and the negative emotion patterns have signiﬁcant
higher delta responses at parietal and occipital sites
and higher gamma responses at prefrontal sites. The
experiment results also indicate that the stable EEG
patterns across sessions exhibit consistency among
repeated EEG measurements of the same subject.
In this study, we investigate the stable neural patterns of three emotions, positive, neutral and negative.
For the future work, more categories of emotions will
be studied and the generalization of our proposed
approach extended to more categories of emotions
will be evaluated. Moreover, different factors such as
gender, age, and race should be considered. To make
automatical emotion recognition models adaptable,
the factors like individual difference and temporal
evolution should be considered. One possible way
to dealing with these problems is to adopt transfer
learning techniques – .
ACKNOWLEDGMENTS
The authors wish to thank all the participants in
the emotion experiments and thank the Center for
Brain-Like Computing and Machine Intelligence for
providing the platform for EEG experiments. This
work was supported in part by the grants from the
National Natural Science Foundation of China (Grant
No. 61272248), the National Basic Research Program
of China (Grant No. 2013CB329401), and the Science
and Technology Commission of Shanghai Municipality (Grant No.13511500200).