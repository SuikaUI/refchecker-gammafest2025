The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)
Do Not Have Enough Data? Deep Learning to the Rescue!
Ateret Anaby-Tavor,1 Boaz Carmeli,1,∗Esther Goldbraich,1 Amir Kantor,1 George Kour,1,2,∗
Segev Shlomov,1,3,∗Naama Tepper,1 Naama Zwerdling1
1IBM Research AI, 2University of Haifa, Israel, 3Technion - Israel Institute of Technology
{atereta, boazc, esthergold, amirka, naama.tepper, naamaz}@il.ibm.com, , 
Based on recent advances in natural language modeling and
those in text generation capabilities, we propose a novel data
augmentation method for text classiﬁcation tasks. We use a
powerful pre-trained neural network model to artiﬁcially synthesize new labeled data for supervised learning. We mainly
focus on cases with scarce labeled data. Our method, referred to as language-model-based data augmentation (LAM-
BADA), involves ﬁne-tuning a state-of-the-art language generator to a speciﬁc task through an initial training phase on
the existing (usually small) labeled data. Using the ﬁne-tuned
model and given a class label, new sentences for the class
are generated. Our process then ﬁlters these new sentences
by using a classiﬁer trained on the original data. In a series
of experiments, we show that LAMBADA improves classi-
ﬁers’ performance on a variety of datasets. Moreover, LAM-
BADA signiﬁcantly improves upon the state-of-the-art techniques for data augmentation, speciﬁcally those applicable to
text classiﬁcation tasks with little data.
Introduction
Text classiﬁcation , such as classiﬁcation
of emails into spam and not-spam , is a
fundamental research area in machine learning and natural language processing. It encompasses a variety of other
tasks such as intent classiﬁcation , sentiment analysis , topic classiﬁcation
 , and relation classiﬁcation .
Depending upon the problem at hand, getting a good
ﬁt for a classiﬁer model may require abundant labeled
data . However, in many cases, and especially
when developing AI systems for speciﬁc applications, labeled data is scarce and costly to obtain.
One example of text classiﬁcation is intent classiﬁcation
in the growing market of automated chatbot platforms . A developer of an intent classiﬁer for a new chatbot may start with a dataset containing two, three, or ﬁve samples per class, and in some
cases no data at all.
∗Equally main contributors
Copyright c⃝2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Data augmentation is a common strategy for handling scarce data situations. It works by synthesizing new data from existing training data, with the objective of improving the performance of the downstream model.
This strategy has been a key factor in the performance improvement of various neural network models, mainly in the
domains of computer vision and speech recognition. Specifically, for these domains there exist well-established methods for synthesizing labeled-data to improve classiﬁcation
tasks. The simpler methods also apply transformations on
existing training examples, such as cropping, padding, ﬂipping, and shifting along time and space dimensions, as these
transformations are usually class preserving .
However, in the case of textual data, such transformations
usually invalidate and distort the text, making it grammatically and semantically incorrect. This makes data augmentation more challenging. In fact, textual augmentation could
even do more harm than good, since it is not an easy task to
synthesize good artiﬁcial textual data. Thus, data augmentation methods for text usually involve replacing a single word
with a synonym, deleting a word, or changing the word order, as suggested by .
Recent advances in text generation models facilitate an innovative approach for handling scarce data situations. Although improving text classiﬁcation in these situations by
using deep learning methods seems like an oxymoron, pretrained models
 are opening new ways to address this
In this paper, we present a novel method, referred to
as language-model-based data augmentation (LAMBADA),
for synthesizing labeled data to improve text classiﬁcation
tasks. LAMBADA is especially useful when only a small
amount of labeled data is available, where its results go
beyond state-of-the-art performance. Models trained with
LAMBADA exhibit increased performance compared to:
1) The baseline model, trained only on the existing data
2) Models trained on augmented corpora generated by the
state-of-the-art techniques in textual data augmentation.
LAMBADA’s data augmentation pipeline builds upon a
powerful language model: the generative pre-training (GPT)
model . This neural-network model is
pre-trained on huge bodies of text. As such, it captures the
structure of natural language to a great extent, producing
deeply coherent sentences and paragraphs. We adapt GPT
to our needs by ﬁne-tuning it on the existing, small data.
We then use the ﬁne-tuned model to synthesize new labeled
sentences. Independently, we train a classiﬁer on the same
original small dataset and use it to ﬁlter the synthesized
data corpus, retaining only data that appears to be qualitative enough. We then re-train the task classiﬁer on both the
existing and the synthesized data.
We compare LAMBADA to other data augmentation
methods and ﬁnd it statistically better along several datasets
and classiﬁcation algorithms. We mainly focus on small
datasets, e.g., containing ﬁve examples per class, and show
that LAMBADA signiﬁcantly improves the baseline in such
scenarios.
In summary, LAMBADA contributes along three main
1. Statistically improves classiﬁers’ accuracy.
2. Outperforms state-of-the-art data augmentation methods
in scarce-data situations.
3. Suggests a compelling alternative to semi-supervised
techniques when unlabeled data does not exist.
The rest of this paper is structured as follows: In Section 2, we present related work for the state-of-the-art in textual data augmentation techniques and the recent advances
in generative pre-trained modeling. In Section 3, we deﬁne
the problem of data augmentation for text classiﬁcation, and
in Section 4, we detail our LAMBADA method solution. In
Section 5, we describe the experiments and results we conducted to analyze LAMBADA performance and to support
the paper’s main claims. We conclude with a discussion in
Section 6.
Related Work
Previous textual data augmentation approaches focus on
sample alteration , in which a single sentence is
altered in one way or another, to generate a new sentence
while preserving the original class. One set of these approaches make local changes only within a given sentence,
primarily by synonym replacement of a word or multiple
words. One of the recent methods in this category is easy
data augmentation (EDA) , which uses
simple operations such as synonym replacement and random
swap . Another method, conditional BERT
contextual augmentation recently introduced in , proposes ﬁne-tuned BERT for
data augmentation by carrying out a masked prediction of
words, while conditioning on the class label. Presumably,
methods that make only local changes will produce sentences with a structure similar to the original ones, thus
yielding low corpus-level variability.
Other recent possible approaches to textual data augmentation generate whole sentences rather than making a
few local changes. The approaches include using variational autoencoding (VAE) ,
round-trip translation , paraphrasing , and methods based on generative adversarial networks . They also include data noising techniques, such as altering words in
the input of self-encoder networks in order to generate
a different sentence , or introducing noise on the word-embedding
level. These methods were analyzed in . Although a viable option when no access to a formal
synonym model exists, they require abundant training data.
Last year, several exciting deep learning methods
 pushed the boundaries of natural language technology. They introduced new neural architectures and highly effective transfer learning techniques that
dramatically improve natural language processing. These
methods enable the development of new high-performance
deep learning models such as ELMO ,
GPT , BERT ,
and GPT-2 . Common to these models is a pre-train phase, in which the models are trained on
enormous bodies of publicly available text, and a ﬁne-tuned
phase, in which they are further trained on task-speciﬁc data
and loss functions.
When introduced, these models processed natural language better than ever, breaking records in a variety of
benchmark tasks related to natural language processing and
understanding, as well as tasks involving text generation.
For example, when GPT was ﬁrst introduced , it improved the state-of-the-art in 12 benchmark
tasks, including textual entailment, semantic similarity, sentiment analysis, and commonsense reasoning. These models
can produce high-quality sentences even when ﬁne-tuned on
small training data. Table 1, shows an example of a few generated sentences based on a small dataset consisting of ﬁve
sentences per class.
Class label
Flight time
what time is the last ﬂight from san
francisco to washington dc on continental
show me all the types of aircraft used
ﬂying from atl to dallas
show me the cities served by canadian
Table 1: Examples of generated sentences conditioned on
the class label. The generative model was trained on a small
dataset consisting of only ﬁve sentences per class.
These results suggest a counter-intuitive text classiﬁcation
approach: is it possible to ﬁne-tune a pre-trained model and
use it to generate new high-quality sentences that will improve the performance of a text classiﬁer?
Problem Deﬁnition
Text classiﬁcation is an instance of the supervised learning problem over textual data.
In this context, we are given a training dataset Dtrain =
{(xi, yi)}n
i=1 containing n labeled sentences. Each xi is a
string of text or, speciﬁcally, a sequence of tokens (roughly,
i . . . xli
i . The label yi ∈{1, . . . , q} indicates the
class of xi among a set of q classes. Each xi is drawn independently from the entire set of strings X (that is, xi ∈
X), according to an unknown distribution on X, denoted
by PX. Moreover, we assume there is an unknown function
f : X →{1, . . . , q}, and that in Dtrain, yi = f(xi) for all
i = 1, . . . , n.
The objective of a supervised learning problem is to approximate f on the entire X, given only the dataset Dtrain.
In short, we are generalizing from the domain of Dtrain
to the entire X. Formally, a classiﬁcation algorithm A receives the training dataset Dtrain, and after a training period, it outputs a classiﬁer function h = A(Dtrain), where
h : X →{1, . . . , q} is also known as a hypothesis. To estimate the extent to which h approximates f on X, it is customary to initially leave out both a training dataset Dtrain
and a test dataset Dtest = {(ˆxi, ˆyi)}ˆn
i=1. The test dataset is
chosen randomly and has the same structure as Dtrain. Both
parts are usually drawn from a single, extensive, dataset.
There are different ways of measuring the quality of classiﬁer h as an approximation to f using Dtest. The most
straightforward way measures the accuracy:
δ(h(ˆxi), ˆyi),
where δ(·, ·) is the Kronecker delta (equals 1 when both arguments are equal, or 0 otherwise), and ˆxi, ˆyi are drawn
from the test set. When the test set is large, accuracy approximates the probability of having h(x) equal f(x), namely,
PX(h(x) = f(x)). We use accuracy as an estimate of classiﬁer performance.
Regardless of how we measure performance, if the train
set Dtrain is small, it will dramatically affect the performance of the algorithm A. Data augmentation tries to solve
this problem by synthesizing additional training pairs that,
together with the existing dataset, better reﬂect the underlying distribution of the data while refraining from introducing
too much noise.
Our work does not focus on the classiﬁcation algorithm
per se. Rather, given a training dataset Dtrain and an algorithm A, we are interested in a general method for synthesizing an artiﬁcial dataset, Dsynthesized. We aim to apply algorithm A on Dtrain ∪Dsynthesized, denoted by ¯h =
A(Dtrain ∪Dsynthesized), to yield a relatively good classi-
ﬁer that outperforms the baseline classiﬁer h.
In the following section, we describe our method,
LAMBADA, and exactly how we obtain Dsynthesized
from Dtrain and A. LAMBADA is speciﬁcally tailored to
the case of small training sets, even miniscule ones, with
only a few examples per class.
LAMBADA Method
We introduce a novel method for improving the performance
of textual classiﬁcation. Named LAMBADA for its use of
Language Model Based Data Augmentation, this method
adds more synthesized, weakly-labeled data samples to a
given dataset. We deﬁne the method in Algorithm 1 and
elaborate on its steps in the following section. LAMBADA
has two key ingredients: 1) model ﬁne-tuning (step 2), which
synthesizes labeled data and 2) data ﬁltering (step 4), which
retains only high-quality sentences.
Algorithm 1: LAMBADA
Input: Training dataset Dtrain
Classiﬁcation algorithm A
Language model G
Number to synthesize per class N1, . . . , Nq
1 Train a baseline classiﬁer h from Dtrain using A
2 Fine-tune G using Dtrain to obtain Gtuned
3 Synthesize a set of labeled sentences D∗using Gtuned
4 Filter D∗using classiﬁer h to obtain Dsynthesized
5 return Dsynthesized
The main input to LAMBADA is a training
dataset Dtrain, which we would like to augment with synthesized data. Dtrain contains a set of sentences, each labeled with a class. To train a classiﬁer, we use a training algorithm A. As far as the LAMBADA method is concerned,
A is arbitrary. However, LAMBADA synthesizes data for
the algorithm A, and this is given as a second input to LAM-
BADA. This is a distinctive feature of our method. We describe both Dtrain and A in Section 3.
LAMBADA uses a pre-trained language model G to
synthesize new data. A language model provides an estimate for the probability that a token (word) will appear, in accordance with a given distribution of text Ptext, conditioned on the preceding and/or
succeeding tokens. More formally, given a token w, and the
preceding k tokens (or less) w1, . . . , wk, one would like
G(w|w1, . . . , wk) to approximate the conditional probability Ptext(w|w1, . . . , wk) of the appearance of w in accordance with Ptext. G is usually calculated using a concrete
corpus of text U, sampled from distribution Ptext.
In contrast to A, G is far from being arbitrary. We use
GPT-2, a recent pre-trained neural-network model (see Section 2), and show that our method outperforms state-of-theart classiﬁers in our main use case, where Dtrain is scarce.
GPT-2 is pre-trained on an enormous body of text available on the web. The corpus is organized as a long sequence
of tokens, denoted by U = w1 · · · wj · · ·. GPT-2, like GPT,
is a right-to-left model based on the transformer architecture
 . It is pre-trained on U with loss deﬁned
log Pθ(wj|wj−k, . . . , wj−1)
where θ is the set of learnable parameters in the neural network of GTP2, and Pθ is the trained language model: an
estimate of the conditional probability distribution on the
set of tokens, as calculated by the network. Speciﬁcally, we
take G = Pθ∗, where θ∗indicates the state of the learnable
parameters after pre-training. Nonetheless, from a technical
standpoint, the language model and its underlying technology can differ to a great extent, and it is thus presented as a
third input to LAMBADA. As ﬁnal input in addition to the
training set, classiﬁcation set, and language model LAM-
BADA is given the number of labeled sentences to synthesize per class N1, . . . , Nq.
Step 1: Train baseline classiﬁer
We train a baseline classiﬁer h = A(Dtrain) using the existing data Dtrain. This
classiﬁer will be used for ﬁltering in Step 4.
Step 2: Fine-tune language model
Independently of
Step 1, we ﬁne-tune the language model G to the task of
synthesizing labeled sentences, to obtain the ﬁne-tuned language model Gtuned. Here, G is speciﬁcally ﬁne-tuned to the
linguistic domain of Dtrain (that is, the sentences, vocabulary, style, etc.), as well as the particular classes in Dtrain.
Generally speaking, we would like to use Gtuned to generate
a sentence set of any size, and each sentence labeled with a
In our case, G is the neural model of GPT-2. We ﬁne-tune
GPT-2 by training it with the data in Dtrain = {(xi, yi)}n
We concatenate the sentences in Dtrain in order to
form U ∗, in the following way:
U ∗= y1SEPx1EOSy2SEPx2EOSy3 · · · ynSEPxnEOS (2)
Here, the auxiliary token SEP separates between a class label
and a corresponding sentence, while token EOS terminates
a sentence, and separates it from the label that follows. We
further train the learnable parameters of GPT-2 to predict the
next token in the exact same way GPT-2 was pre-trained –
using the loss function in Equation 1 (with the same training
procedure and hyperparameters). However, we use U ∗instead of U, and the learnable parameters are already initialized. The resulting language model is referred to as Gtuned.
Step 3: Synthesize labeled data
Given Gtuned, new labeled sentences can be synthesized. For any class label
y ∈{1, . . . , q}, we can use the adapted language model to
predict the continuation of the sequence “y SEP” until EOS,
which terminates the generated sentence. This way, for each
class, any number of sentences may be synthesized. For example, this allows us to balance between the classes or otherwise control the ratio of generated sentences per class. Creating a more balanced training set can improve classiﬁcation
performance, especially in the case of classiﬁers that are sensitive to unbalanced classes.
In this step, we synthesize a set of labeled sentences,
which is denoted by D∗= {(x′
i=1. We use a simple
and rather crude heuristic, where we generate for each class
y, 10 times more sentences than we wish to add to the class
(i.e., 10Ny). Accordingly, the total number of generated sentences is N = 10 q
y=1 Ny. Of course, more sophisticated
heuristics can also be examined.
GPT-2 generates labeled sentences that are typically both
high quality and diverse, facilitating the relative success of
our method. This is also where the power of GPT-2 comes
into play.
Step 4: Filter synthesized data
One obstacle in using
synthesized text is the noise and error it may introduce. In
the last step, we ﬁlter the data in D∗, which was synthesized
by Gtuned in Step 3, leaving only the instances of the highest
quality. We do this using the classiﬁer h that was trained in
For each class y, we take the top Ny sentences from D∗
that are labeled by y, as follows: Given a synthesized sentence (x, y) ∈D∗, we ﬁrst verify that h(x) = y, and then
use h conﬁdence score (see below) as a rank for (x, y). That
is, we take the top ranked Ny sentences for class y. This results in a synthesized dataset Dsynthesized ⊆D∗, consisting
of labeled sentences and with the same structure as Dtrain.
This is the outcome of LAMBADA.
The conﬁdence score given to a data instance by h can be
regarded as the extent the instance is conservative with respect to h. In turn, h takes into account both Dtrain and
the algorithm A that is to be used with the augmented
dataset. This approach is borrowed from semi-supervised
learning , where it is used to classify and ﬁlter unlabeled data in a conservative manner. Note, however,
that Gtuned generates sentences conditioned on a class label. In our case, this means we have a type of double voting
mechanism.
While not addressed in this paper, the process described
could generally be repeated by applying LAMBADA further on Dtrain ∪Dsynthesized to obtain D′
synthesized,
synthesized, and so on.
Experimental Results
We tested our method with three different classiﬁers (BERT,
SVM and LSTM) on three distinct datasets (ATIS, TREC,
and WVA) by running multiple experiments in which we
varied the amount of training samples per class. Next, we
compared LAMBADA to other data augmentation methods
(CVAE, EDA, and CBERT) by using the above-mentioned
classiﬁers and datasets. We statistically validated our results
with the McNemar test .
Table 2 presents a description of the datasets we used in our
experiments.
Flight reservations
Open-domain questions
Telco Customer support
Table 2: Datasets.
• Airline Travel Information Systems (ATIS)1 A dataset
providing queries on ﬂight-related information widely
used in language understanding research. ATIS is characterized as an imbalanced dataset, as most of the data
belongs to the ﬂight class.
• Text Retrieval Conference (TREC)2 A well-known
dataset in the information retrieval community for question classiﬁcation consisting of open-domain, fact-based
questions, divided into broad semantic categories.
• IBM Watson Virtual Assistant (WVA) A commercial
dataset used for intent classiﬁcation, comprising data for
training telco customer support chatbot systems.
We mainly focus on topic classiﬁcation datasets with the
task of classifying a sentence, not an entire document. Notably, classiﬁcation of shorter text is considered a more dif-
ﬁcult task. We randomly split each dataset into train, validation, and test sets (80%, 10%, 10%). We then randomly
chose from the training set a subset including 5, 10, 20, 50,
or 100 samples per class, which we used in each experiment for training. Once determined, we used the same subset
throughout all experiments.
Classiﬁers
We demonstrated that our augmentation approach is independent of the classiﬁcation algorithm by inspecting three
different classiﬁers, representing three text classiﬁcation
“generations”.
Support Vector Machine classiﬁers were already
commonly used before the deep neural network era. We
employ a commercial SVM classiﬁer (IBm Watson Natural
Language Classiﬁer) dedicated to natural language processing, which handles both the feature extraction process and
the training of the classiﬁer. While recent models are based
on neural networks, in the context of our problem, SVM may
have an advantage, since unlike neural-network-based models, it performs well even for relatively small datasets.
Long Short Term Memory represents the type of
classiﬁers that emerged after the advances in training recurrent neural networks, and the introduction of word embeddings , LSTMs are commonly used for
sequential and textual data classiﬁcation. We implemented
a sequence-to-vector model based on an LSTM component
followed by two fully connected layers and a softmax layer.
For word embedding, we employed GLoVe of 100 dimensions. An LSTM
classiﬁer usually requires a large amount of data for training.
Bidirectional Encoder Representations from Transformers is a relatively new family of classiﬁers. Based on the
transformer architecture, BERT is pre-trained using two unsupervised tasks: masked language model and next-sentence
prediction, on the “BooksCorpus” (800 million words) and has proven state-of-the-art performance on
several text classiﬁcation tasks. Therefore, BERT, like GPT-
2, leverages large amounts of data that were used as part of
1www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk
2 
its pre-training phase, in order to perform well, even on relatively small datasets.
Generative Models
We compared LAMBADA’s synthetic corpus quality to synthetic corpora generated by various other generative models.
Similar to our selection of classiﬁers, we selected generators of various types representing different generation approaches. For a fair comparison, we mainly considered conditional generative models that allow generating samples
conditioned on the class label. This enabled the creation of
balanced synthetic corpora, an important feature for some
classiﬁcation models. In the following we provide a brief
description of these generators.
Easy Data Augmentation . This
is a recent but simple rule-based data augmentation framework for text. It includes synonym replacement, random insertion, random swap, and random deletion. These methods
were found beneﬁcial, especially for small training set sizes.
Conditional Variational Autoencoder . This generative model assumes a prior distribution over a latent space and uses deep neural networks
to predict its parameters. It is an extension of the Variational
Autoencoder model, enabling the conditional generation of
an output sentence given a latent vector and the target class.
We used a standard CVAE model with RNN-based encoder
and decoder for generating sentences.
Conditional Bidirectional Encoder Representations from Transformers . As a recent augmentation method for labeled sentences based on BERT, this
model operates by randomly replacing words with more varied substitutions predicted by the language model. CBERT is
pre-trained on a large corpus in an unsupervised setting, allowing it to adapt to speciﬁc domains even when ﬁne-tuned
through relatively small datasets.
Table 3 describes the attributes of the three generative
models mentioned above, including the GPT-2 model.
External Dataset
Rule-Based
Autoencoder
Language Model
Wiki & Book corpus
Language Model
Table 3: Description of the different text generation models.
We conducted comprehensive experiments, testing LAM-
BADA’s quality from various aspects. We statistically validated all our results with McNemar’s test.
Number of Samples and Classiﬁers
We compared the
LAMBADA approach with the baseline using three different classiﬁers over varied numbers of trained samples: 5, 10,
20, 50, and 100 for each class. We used the ATIS dataset to
discover for which sample size our approach is beneﬁcial.
Figure 1: Accuracy for each sample size over ATIS
Figure 1 clearly demonstrates the superiority of our LAM-
BADA approach over the baseline throughout all classi-
ﬁers and all sample sizes that are smaller than or equal to
50. Larger amounts of data do not beneﬁt as much from
data augmentation and therefore, in the case of 100 samples for each class, the accuracy of LSTM and SVM does
not improve. Figure 1 also nicely demonstrates the differences between the classiﬁers after training them on similar datasets with various sizes. BERT, which is a pre-trained
model, is signiﬁcantly better than SVM and LSTM throughout all sample sizes. However, the gap between the accuracy
of BERT and the other classiﬁers is more predominant in
smaller sample sizes. SVM handles smaller data sizes better
than LSTM, as expected. Notably, our approach was even
able to improve BERT, which is state-of-the-art for text classiﬁcation and already pre-trained on vast amount of data.
We substantiate previous results by comparing
the baseline to our LAMBADA approach over three datasets
using ﬁve samples for each class. Table 4 shows that our approach signiﬁcantly improves all classiﬁers over all datasets.
% improvement
% improvement
% improvement
Table 4: Accuracy of LAMBADA vs. baseline over all
datasets and classiﬁers. Signiﬁcant improvement over all
datasets and all classiﬁers (McNemar, p−value< 0.01).
Similarly to ATIS dataset, TREC and WVA datasets also
demonstrate the dominance of BERT over SVM and LSTM.
LSTM achieves poor results when using a small number
of samples, as expected. Interestingly, on the ATIS dataset,
with BERT and SVM classiﬁers, the percentage of improvement is far greater than on the other datasets. We believe
that this improvement is due to ATIS’ imbalanced characteristics and our ability to generate additional data for the
under-represented classes.
Comparison of Generative Models
We compared our approach to other leading text generator approaches. Table 5
shows that our approach is statistically superior to all other
generation algorithms in the ATIS and WVA datasets over
all classiﬁers. In the TREC dataset, the results for BERT
are signiﬁcantly better than all other methods. On the TREC
dataset with SVM classiﬁer, our method is on par with EDA.
Table 5: Accuracy of LAMBADA vs. other generative approaches over all datasets and classiﬁers. LAMBADA is statistically (* McNemar, p−value< 0.01) superior to all models on each classiﬁer and each dataset (on par to EDA with
SVM on TREC).
augmentation
framework does not require additional unlabeled data. As
such, it can be applied when unlabeled data is unavailable
or costly. To test the expected LAMBADA performance in
such a scenario, we compared it to a semi-supervised approach that uses unlabeled data. Table 6 compares between different experimental settings on
ATIS using three classiﬁers and ﬁve samples per class.
Table 6: Accuracy of LAMBADA with or without label
vs. unlabeled data for ATIS dataset with 5 samples per
class. Signiﬁcant improvement for BERT and SVM classi-
ﬁers (*McNemar, p−value< 0.01).
To create an unlabeled dataset, we randomly selected
samples from the original dataset while ignoring their labels.
Next, following a simple weak labeling approach, we classiﬁed the samples with one of the classiﬁers after training it
on the labeled dataset. We compared LAMBADA’s classiﬁcation results with the results we obtained from this classi-
ﬁer. These results appear in the LAMBADA and Unlabeled
data columns of Table 6. Surprisingly, for most classiﬁers,
LAMBADA achieves better accuracy compared to a simple
weak labeling approach. Clearly, the generated dataset contributes more to improving the accuracy of the classiﬁer than
the unlabeled samples taken from the original dataset.
We may attribute this increased performance to two main
1. LAMBADA uses its “generated” labels, which signiﬁcantly improve performance.
2. LAMBADA allows us to control the number of samples per class by investing more effort in generating samples for classes that are under-represented in the original
We further assessed the importance of the “generated”
labels by removing them from LAMBADA’s synthesized
dataset. We provide the results for this experiment under the
GPT Unlabeled column in Table 6. In future work, we plan
to use various data balancing approaches on the unlabeled
dataset to assess the importance of the second factor above.
Discussion and Future Work
We introduce LAMBADA for improving classiﬁers’ performance. It involves ﬁne-tuning a language model, generating new labeled-condition sentences and a ﬁltering phase.
We showed that our method statically improves classiﬁers’
performance on small data sets. In addition, we showed
that LAMBADA beats the state-of-the-art techniques in data
augmentation.
Generative vs. Discriminative
Generally speaking, training a generative model requires more data than training
a discriminative model . This is attributed mainly to the fact that discriminative models aim
at estimating the class boundaries, while generative models
approximate the probability distribution of the samples in
each class. Therefore, prima facie, it is counter-intuitive to
employ a generative model to improve discriminative classi-
ﬁer accuracy. All the more so, when both models are trained
on the same dataset. However, unlike discriminative models,
generative models may exploit unsupervised data to compensate for the inherent higher sample complexity. Consequently, and given the available abundant amount of unlabeled textual data, language models, pre-trained on huge
corpora, have recently become state-of-the-art. Fine-tuning
these generative models requires an extremely small amount
of labeled data, as we show in this work, and sampling from
them is straightforward.
Filtering Approach
LAMBADA synthesizes data in two
steps. It ﬁrst generates a large number of sentences per class
and then ﬁlters them by multiple conditions. In this work,
we employ a simple ﬁltering heuristic, inspired by the semisupervised paradigm that takes into account: 1) the class label of the generated sentence 2) the class label as given by
the ﬁltering classiﬁer, together with its conﬁdence score and
3) the number of sentences per class. We plan to further investigate other ﬁltering heuristics and approaches in future
Weak Labeling and Self-Supervision
LAMBADA synthesizes corpora of weakly labeled data by conditionally
generating sentences on a given class’ label. Thus, one may
incorporate a LAMBADA synthesized corpus within any
weak labeling or semi-supervised framework such as one of
these suggested by . Moreover, one
may use a synthesized corpus in situations where unlabeled
data is not available and still expect comparable results.
Zero-shot Learning
Most textual datasets contain class
names with semantic meaning. LAMBADA, an approach
based on a language model, utilizes this class label meaning in its generation process. Consequently, it enables synthesizing samples for any meaningful, domain-related, class
name. It thus potentially allows the generation of samples
for unseen classes, a method also known as zero-shot learning . We plan to investigate this idea in
future research.
Iterative Training Process
While a single step of the augmentation process may sufﬁciently improve the classiﬁer, as
shown in this paper, there is no real impediment to repeat the
process by running several iterations of Algorithm 1. One of
the possible hazards that the repetition of this process may
cause is data drifting, in which biased synthesized samples
gain domination over the training dataset.