Online Learning for Ofﬂoading and
Autoscaling in Renewable-Powered Mobile
Edge Computing
Jie Xu, University of Miami
Shaolei Ren, University of California, Riverside
Mobile edge computing (a.k.a. fog computing) has recently emerged to enable in-situ processing
of delay-sensitive applications at the edge of mobile networks. Providing grid power supply in support
of mobile edge computing, however, is costly and even infeasible (in certain rugged or under-developed
areas), thus mandating on-site renewable energy as a major or even sole power supply in increasingly
many scenarios. Nonetheless, the high intermittency and unpredictability of renewable energy make
it very challenging to deliver a high quality of service to users in renewable-powered mobile edge
computing systems. In this paper, we address the challenge of incorporating renewables into mobile
edge computing and propose an efﬁcient reinforcement learning-based resource management algorithm,
which learns on-the-ﬂy the optimal policy of dynamic workload ofﬂoading (to centralized cloud) and
edge server provisioning to minimize the long-term system cost (including both service delay and
operational cost). Our online learning algorithm uses a decomposition of the (ofﬂine) value iteration
and (online) reinforcement learning, thus achieving a signiﬁcant improvement of learning rate and runtime performance when compared to standard reinforcement learning algorithms such as Q-learning.
I. INTRODUCTION
In the era of mobile computing and Internet of Things, a tremendous amount of data is
generated from massively distributed sources, requiring timely processing to extract its maximum
value. Further, many emerging applications, such as mobile gaming and augmented reality, are
delay sensitive and have resulted in an increasingly high computing demand that frequently
exceeds what mobile devices can deliver. Although cloud computing enables convenient access
to a centralized pool of conﬁgurable computing resources, moving all the distributed data and
computing-intensive applications to clouds (which are often physically located in remote megascale data centers) is simply out of the question, as it would not only pose an extremely heavy
burden on today’s already-congested backbone networks but also result in (sometimes intolerable)
large transmission latencies that degrade the quality of service.
As a remedy to the above limitations, mobile edge computing (a.k.a., fog computing) has
recently emerged to enable in-situ processing of (some) workloads locally at the network edge
without moving them to the cloud , . In mobile edge computing, network edge devices,
such as base stations, access points and routers, are empowered with computing and storage
capabilities to serve users’ requests as a substitute of clouds, while signiﬁcantly reducing the
transmission latency as they are placed in the proximity of end users. In this paper, we consider
(marco) base station as the default edge device and refer to the combination of an edge device
and the associated edge servers as an edge system.
In increasingly many scenarios, edge systems are primarily powered by renewable green energy
(e.g. solar and wind), rather than the conventional electric grid, due to various reasons such as
location, reliability, carbon footprint and cost. For instance, in many developing countries, the
majority of base stations have to be powered by continuously operating diesel generators because
the electric grid is too unreliable . Even if power line extension is technically feasible, grid-tied
edge systems can violate environmental quality regulations in rural areas that are ecologically
sensitive. Thus, in view of the signiﬁcant carbon footprint of grid power as well as soaring
electricity prices, renewable energy is embraced as a major energy source. Despite the clear
advantages, a distinct feature of renewable energy is that it can vary drastically over time and is
highly unpredictable. Although batteries are often installed as an energy buffer, the computing
capacity of an edge system is still signiﬁcantly limited at any moment in time. As a result,
although edge processing reduces the transmission latency, a considerable processing time may
occur when little power supply is available. This gives rise to an important trade-off between
transmission delay and processing delay, which is jointly determined by the edge system’s
ofﬂoading policy (i.e. how much workload is ofﬂoaded to the cloud) and autoscaling policy (i.e.
how many servers are dynamically provisioned or activated). The problem is further complicated
due to the temporal correlation — provisioning more servers and processing more workloads
at the edge system in the current time means that fewer servers can be provisioned and fewer
workloads can be processed locally in the future due to the limited and time-varying renewable
energy supply. Figure 1 illustrates the considered system architecture.
Architecture of a renewable-powered edge computing system. The photo shows a solar- and wind-powered system
deployed by Alcatel Lucent in Turkey. (Source: 
In this paper, we address the challenge of incorporating renewables into mobile edge computing
and propose an efﬁcient reinforcement learning-based resource management algorithm, which
learns on-the-ﬂy the optimal policy of dynamic workload ofﬂoading (to centralized cloud) and
edge server provisioning to minimize the long-term system cost (including both service delay
and operational cost). The problem is formulated as a Markov decision process (MDP) by taking
into account various unique aspects of the considered mobile edge system. A novel post-decision
state (PDS) based algorithm that learns the optimal joint ofﬂoading and autoscaling policy onthe-ﬂy is developed. Compared with conventional online reinforcement learning algorithms, e.g.
Q-learning, the proposed PDS based learning algorithm signiﬁcantly improves the convergence
speed and run-time performance by exploiting the special structure of the considered problem.
Based on extensive simulations and numerical results, we show that our algorithm can signiﬁcantly improve the performance of the green mobile edge computing system.
II. RELATED WORK
Mobile edge computing has received an increasing amount of attention in recent years. In
particular, a central theme of many prior studies is ofﬂoading policy on the user side, i.e.,
what/when/how to ofﬂoad a user’s workload from its device to the edge system or cloud (see ,
 and references therein). Our work on edge-side ofﬂoading and autoscaling is complementary
to these studies on user-side ofﬂoading.
Our study is also relevant to the rich literature on power management in wireless networks
 – and data centers , especially in the context of renewable-powered systems , ,
 . Nonetheless, our study differs from these works as we jointly optimize the ofﬂoading and
autoscaling decisions at the edge system, whereas prior research on edge device (base station) or
data center power management typically only considers one of the two decisions. For example,
autoscaling (a.k.a., right-sizing) in data centers dynamically controls the number of active
servers, but the control knob of ofﬂoading to the cloud is not available in the context of data
centers. While some studies on base station power management considers trafﬁc ofﬂoading to
small cells and/or other base stations , we study an orthogonal type of ofﬂoading — from
edge servers to the cloud — which requires different models and techniques (see Section III-D).
Further, in contrast with these studies , – , we propose a novel solution technique based
on reinforcement learning to incorporate intermittent and unpredictable renewables into mobile
edge systems. Finally, we note that the most relevant study to our work is , which also
studies workload allocation/ofﬂoading in a cloud-fog computing system. However, unlike our
renewable-powered edge system, this paper considers a grid-powered system and focuses on a
one-shot static optimization without addressing the temporal correlation among the ofﬂoading
decisions across time (due to intermittent renewables and limited battery).
III. SYSTEM MODEL
As a major deployment method of mobile edge computing , we consider an edge system
consisting of a base station and a set of edge servers, which are physically co-located and share
the same power supply in the cell site.
A. Workload model
We consider a discrete-time model by dividing the operating period into time slots of equal
length indexed by t = 0, 1, ..., each of which has a duration that matches the timescale at which
the edge device can adjust its computing capacity (i.e. number of active servers). We use x ∈L
to represent a location coordinate in the service area L. Let λ(x, t) represent the workload arrival
rate in location x, and θ(x, t) be the wireless transmission rate between the base station and
location x. Thus λ(t) = P
x∈L λ(x, t) ∈[0, λmax] is the total workload arrival rate at the edge
system, where λmax is the maximum possible arrival rate. The system decides the amount of
workload µ(t) ≤λ(t) that will be processed locally. The remaining workload ν(t) ≜λ(t)−µ(t)
will be ofﬂoaded to the cloud for processing. The edge system also decides at the beginning
of the time slot the number of active servers, denoted by m(t) ∈[0, M] ≜M. These servers
are used to serve the local workload µ(t). Since changing the number of servers during job
execution are difﬁcult and in many cases impossible, we only allow determining the number of
servers at the beginning of each time slot but not during the slot.
B. Power model
We interchangeably use power and energy, since energy consumption during each time slot
is the product of (average) power and the duration of each time slot that is held constant in our
model. The total power demand of the edge system in a time slot consists of two parts: ﬁrst,
basic operation and transmission power demand by edge devices (base station in our study); and
second, computing power demand by edge servers. The ﬁrst part is independent of the ofﬂoading
or the autoscaling policy, which is modeled as dop(λ(t)) = dsta + ddyn(λ(t)) where dsta is the
static power consumption and ddyn(λ(t)) is the dynamic power consumption depending on the
amount of total workload. The computing power demand depends on the number of active servers
as well as the locally processed workload. We use a generic function dcom(m(t), µ(t)), which is
increasing in m(t) and µ(t), to denote the computing power demand. The total power demand
is therefore
d(λ(t), m(t), µ(t)) = dop(λ(t)) + dcom(m(t), µ(t))
To model the uncertainty of the green power supply, we assume that the green power budget,
denoted by g(t), is realized after the ofﬂoading and autoscaling decisions are made. Therefore,
the decisions cannot utilize the exact information of g(t). However, we assume that there is an
environment state e(t) which the system can observe and it encodes valuable information of how
much green energy budget is anticipated in the current time slot. For instance, daytime in a good
weather usually implies high solar power budget. Speciﬁcally, we model g(t) as an i.i.d. random
variable given e(t), which obeys a conditional probability distribution Pg(g(t)|e(t)). Note that
the environment state e(t) itself may not be i.i.d.
C. Battery model
Batteries are used to balance the power supply and demand. In a solar+wind system, photovoltaic modules and wind turbines can combine their output to power the edge system and
charge the batteries. When their combined efforts are insufﬁcient, batteries take over to ensure
steady operation of the edge system. We denote the battery state at the beginning of time
slot t by b(t) ∈[0, B] ≜B (in units of power) where B is the battery capacity. For system
protection reasons, the battery unit has to be disconnected from the load once its terminal
voltage is below a certain threshold for charging. We map b(t) = 0 to this threshold voltage
to ensure basic operation of the system. Since green power budget is unpredictable and hence
unknown at the beginning of time slot t, the edge system uses a conservative policy which
satisﬁes dcom(m(t), µ(t)) ≤max{b(t) −dop(λ(t)), 0}. It instructs the edge system to ofﬂoad all
workload to the cloud if the existing battery level cannot even support the basic operation and
transmission in the current slot. When dop(λ(t)) ≥b(t), the backup power supply (e.g. diesel
generator) will be used to maintain basic operation for the slot. The cost due to activating the
backup power supply is cbak(t) = φ · dop(λ(t)) where φ > 0 is a large constant representing the
large cost due to using the backup power supply. The next time slot battery state then evolves to
b(t+1) = b(t)+g(t). When dop(λ(t)) ≤b(t), the edge system may process part of the workload
µ(t) ≤λ(t) at the local servers. Depending on the realized green power g(t) and the computing
power demand dcom(λ(t), m(t), µ(t)), the battery is recharged or discharged accordingly:
• If g(t) ≥d(λ(t), m(t), µ(t)), then the surplus g(t) −d(λ(t), m(t), µ(t)) is stored in the
battery until reaching its capacity B.
• If g(t) < d(λ(t), m(t), µ(t)), then the battery is discharged to cover the deﬁcit d(λ(t), m(t), µ(t))−
For simplicity, we will assume that there is no power loss either in recharging or discharging
the batteries, noting that this can be easily generalized. We also assume that the batteries are
not leaky. We model the battery depreciation cost in a time slot, denoted by cbattery(t), using
the amount of discharged power in this time slot since the lifetime discharging is often limited.
Speciﬁcally,
cbattery(t) = ω · max{d(λ(t), m(t), µ(t)) −g(t), 0}
where ω > 0 is the normalized unit depreciation cost.
D. Delay cost model
The average utilization of the base station is ρ(t) = P
x λ(x, t)/θ(x, t), which results in a total
wireless access and transmission delay of cwi(t) = P
x λ(x, t)/[θ(x, t)(1−ρ(t)] by following the
literature and modeling the base station as a queueing system . Next we model the workload
processing delay incurred at the edge servers.
For the local processed workload, the delay cost clo(t) is mainly processing delay due to the
limited computing capacity at the local edge servers. The transmission delay from the edge device
to the local servers is negligible due to physical co-location. To quantify the delay performance
of services, such as average delay and tail delay (e.g. 95th-percentile latency), without restricting
our model to any particular performance metric, we use the general notion of clo(m(t), µ(t))
to represent the delay performance of interest during time slot t. As a concrete example, we
can model the service process at a server instance as an M/G/1 queue and use the average
response time (multiplied by the arrival rate) to represent the delay cost, which can be expressed
as clo(m(t), µ(t)) =
m(t)−µ(t).
For the ofﬂoaded workload, the delay cost coff(t) is mainly transmission delay due to network
round trip time (RTT), which varies depending on the network congestion state. For modeling
simplicity, the service delay at the cloud side is also absorbed into the network congestion state.
Thus, we model the network congestion state, denoted by h(t), as an exogenous parameter and
express it in terms of the RTT (plus cloud service delay) for simplicity. The delay cost is thus
coff(h(t), λ(t), µ(t)) = (λ(t) −µ(t)) max{h(t) −d0, 0}. The total delay cost is therefore
cdelay(h(t), λ(t), m(t), µ(t))
=clo(m(t), µ(t)) + coff(h(t), λ(t), µ(t)) + cwi(λ(t))
IV. PROBLEM FORMULATION
In this section, we formulate the dynamic ofﬂoading and autoscaling problem as an online
learning problem, in order to minimize the system cost. The system system is described by
a tuple s(t) ≜(λ(t), e(t), h(t), b(t)), which is observable at the beginning of the time slot.
Among the four state elements, λ(t), e(t), h(t) are exogenous states which are independent of
the ofﬂoading and autoscaling actions. To make the stochastic control problem tractable, they
are assumed to have ﬁnite value spaces and evolve as ﬁnite-state Markov chains. Speciﬁcally,
let Pλ(λ(t + 1)|λ(t)), Pe(e(t + 1)|e(t)) and Ph(h(t + 1)|h(t)) denote the transition matrices for
λ(t), e(t) and h(t), respectively. Similar assumptions have been made in existing literature, e.g.
 . Importantly, all these probability distributions are unknown a priori to the edge system.
The stochastic control problem now can be cast into an MDP, which consists of four elements:
the state space S, the action space A, the state transition probabilities Ps(s(t+1)|s(t), a(t)), ∀s, s′ ∈
S, a ∈A, and the cost function c(s, a), ∀s, a. We have already deﬁned the state space. Next we
introduce the other elements as follows.
Actions. Although the actual actions taken by the edge system are ν(t) (ofﬂoading) and m(t)
(autoscaling) in each time slot t, we will consider an intermediate action in the MDP formulation,
which is the computing power demand in each time slot t, denoted by a(t) ∈A where A is
a ﬁnite value space. We will see in a moment how to determine the optimal ofﬂoading and
autoscaling actions based on this. As mentioned before, to maintain basic operation in the worst
case, we require that a(t) ≤max{b(t) −dop(λ(t)), 0}.
State transition probability. Given the current state s(t), the computing power demand a(t)
and the realized green power budget g(t), the buffer state in the next time slot is
b(t + 1) = [b(t) + g(t)]B
0 , if dop(λ(t)) > b(t)
b(t + 1) = [b(t) −P1(λ(t)) −a(t) + g(t)]B
0 , otherwise
where [·]B
0 denotes max{min{·, B}, 0}. The system then evolves into the next time slot t + 1
with the new state s(t + 1). The transition probability from s(t) to s(t + 1), given a(t), can be
expressed as follows
P(s(t + 1)|s(t), a(t))
=Pλ(λ(t + 1)|λ(t))Pe(e(t + 1)|e(t))Ph(h(t + 1)|h(t))
Pg(g(t)|e(t))1{ζ(t)}
where 1{·} is the indicator function and ζ(t) denotes the event deﬁned by (3). Notice that the
state transition only depends on a(t) but not the ofﬂoading or the autoscaling action. This is
why we can focus on the computing power demand action a(t) for the foresighted optimization
Cost function. The total system cost is the sum of the delay cost, the battery depreciation
cost and the backup power supply cost. If dop(λ(t)) > b(t), then the cost is simply
˜c(s(t), a(t)) = cdelay(h(t), λ(t), 0, 0) + cbak(λ(t))
since we must have m(t) = 0 and µ(t) = 0. Otherwise, the realized cost given the realized green
power budget g(t) is
˜c(t) = cdelay(h(t), λ(t), m(t), µ(t)) + ω · [a(t) −g(t)]∞
Since the state transition does not depend on µ(t) or m(t), they can be optimized given s(t)
and a(t) by solving the following myopic optimization problem
cdelay(h, λ, m, µ) s.t. P(m, µ) = a
Let m∗(s, a) and µ∗(s, a) denote the optimal solution and c∗
delay(s, a) the optimal value given s
and a. Therefore, the minimum cost in time slot t given s and a is
˜c(s(t), a(t), g(t)) = c∗
delay(s(t), a(t)) + ω · [a(t) −g(t)]∞
The expected cost is thus
c(s(t), a(t)) = c∗
delay(s(t), a(t)) + Eg(t)|e(t)ω · [a(t) −g(t)]∞
Policy. The edge system’s computing power demand policy (which implies the joint ofﬂoading
and autoscaling policy) in the MDP is a mapping π : Λ × E × H × B →A. We focus
on optimizing the policy to minimize the edge system’s expected long-term cost, which is
deﬁned as the expectation of the discounted sum of the edge device’s one-slot cost: Cπ(s(0)) =
δtc(s(t), a(t))|s(0)
where δ < 1 is a constant discount factor, which models the fact
that a higher weight is put on the current cost than the future cost. The expectation is taken over
the distribution of the green power budget, the workload arrival, the environment state and the
network congestion state. It is known that in MDP, this problem is equivalent to the following
optimization: minπ Cπ(s), ∀s ∈S. Let C∗(s) be the optimal discounted sum cost starting with
state s. It is well-known that π∗and C∗(s) can be obtained by recursively solving the following
set of Bellman equations
C∗(s) = min
c(s, a) + δ
P(s′|s, a)C∗(s′)
In the next section, we solve this problem using the idea of dynamic programming and online
V. POST-DECISION STATE BASED ONLINE LEARNING
If all the probability distributions were known a priori, then the optimal policy could be
solved using traditional algorithms for solving Bellman equations, e.g. the value iteration and
the policy iteration , in an ofﬂine manner. In the considered problem, all these probability
distributions are unknown a priori and hence, these algorithms are not feasible. In this section,
we propose an online reinforcement learning algorithm to derive the optimal policy π∗on-the-ﬂy.
Our solution is based on the idea of post-decision state (PDS), which exploits the partially known
information about the system dynamics and allows the edge system to integrate this information
into its learning process to speed up learning. Compared with conventional online reinforcement
learning algorithms, e.g. Q-learning, the proposed PDS based learning algorithm signiﬁcantly
improves its convergence speed and run-time performance.
A. Post-Decision State
We ﬁrst introduce the notion of PDS, which is the most critical idea of our proposed algorithm.
In our problem, PDS is the intermediate system state after the edge system takes the computing
power demand action a(t) but before the green power budget g(t) is realized. Speciﬁcally, the
PDS in time slot t, denoted by ˜s(t) ≜(˜λ(t), ˜e(t), ˜h(t),˜b(t)), is deﬁned as
˜λ(t) = λ(t),
˜e(t) = e(t),
˜h(t) = h(t)
˜b(t) = b(t), if dop(λ(t)) > b(t)
˜b(t) = max{b(t) −dop(λ(t)) −a(t), 0}, otherwise
As we can see, the post-decision workload state ˜λ(t), environment state ˜e(t) and network
congestion state ˜h(t) remain the same because the computing power demand action a(t) does not
have a direct impact on these elements of the system state. The only element of the system state
that may change is the battery state b(t). However, it is important to notice that the post-decision
battery state ˜b(t) is only a virtual state but not the real battery state. Given the deﬁnition of PDS,
we further deﬁne the post-decision value function V ∗(˜s) as follows:
P(s′|˜s)U∗(s′)
where the transition P(s′|˜s) is now independent of the action,
˜P(s|˜s) =Pλ(λ|˜λ)Pe(e|˜e)Ph(h|˜h)
Pg(g|˜e)1{b = min{˜b + g, B}}
For better exposition, we refer to s as the “normal” state and C∗(s) as the “normal” value (cost)
function, in order to differentiate with their post-decision counterparts. It is obvious that C∗(s)
and V ∗(˜s) are also related through:
C∗(s) = min
a∈A(c(s, a) + δV ∗(˜s))
The advantages of using the PDS and post-decision value function is summarized as follows.
(1) In the PDS based Bellman equations, the expectation operation is separated from the
minimization operation. If we can learn and approximate the post-decision value function V ∗(˜s),
then the minimization can be solved without any prior knowledge of the system dynamics.
(2) Given a, the PDS decomposes the system dynamics into an a priori unknown component,
i.e. λ, e, h and g whose evolution is independent of a, and an a priori known component,
i.e. the battery state evolution is partially determined by a. Importantly, λ, e, h and g are also
independent of the battery state b. This fact enables us to develop a batch update scheme on
the post-decision value functions, which can signiﬁcantly improve the convergence speed of the
proposed PDS based reinforcement learning.
B. The algorithm
The algorithm maintains and updates a set of variables in each time slot. These variables are
• The one slot cost estimate ˆct(s, a), ∀(s, a) ∈S × A.
• The post-decision value function estimate ˆV t(˜s), ∀˜s ∈˜S.
• The normal value function estimates ˆCt(s), ∀s ∈S.
The superscript t is used to denote the estimates at the beginning of the time slot t. If these
estimates are accurate, i.e. ˆct(s, a) = c(s, a), ˆV t(˜s) = V ∗(˜s) and ˆCt(s) = C∗(s), then the optimal
power demand policy is readily obtained by solving (13). Our goal is to learn these variables
over time using the realizations of the system states and costs. The algorithm works as follows:
(In each time slot t)
Step 1: Determine the optimal computing power demand a(t) = mina(ˆct(s(t), a)+δ ˆV t(˜s(t)))
where for each a, ˜s(t) is the corresponding PDS. Given this power demand, the corresponding
optimal ofﬂoading and autoscaling actions are determined as µ(t) = µ∗(s(t), a(t)) and m(t) =
m∗(s(t), a(t)) based on the solution of (6).
After the green power budget g(t) and hence the current slot cost ˜c(t) is realized, the battery
state evolves to b(t + 1) according to (3). The following steps update the estimates.
Step 2: Batch update ˆct(s, a) for any action a and any state s = (λ, e, h, b) such that e = e(t)
using the realized green power budget g(t) according to
ˆct+1(s, a) = (1 −ρt)ˆct(s, a) + ρtc(s, a, g(t))
where ρt is the learning rate factor that satisﬁes P∞
t=0 ρt = ∞and
(ρt)2 < ∞. For all other
action-state pair, ˆct+1(s, a) = ˆct(s, a). We can do this batch update because the green power
budget g(t) depends only on the environment state e(t) but not on other states or actions.
Step 3: Batch update the normal value function estimate for any state s = (λ, e, h, b) such
that e = e(t) according to
ˆCt+1(s) = min
a∈A(ˆct+1(s, a) + δ ˆV t(˜s))
The normal value function estimates for the remaining states are unchanged.
Step 4: Batch update the post-decision value function estimate for any ˜s ∈˜S such that
˜λ = ˜λ(t), ˜e = ˜e(t) and ˜h = ˜h(t) according to
ˆV t+1(˜s) = (1 −αt) ˆV t(˜s) + αt ˆCt+1(s)
where s = (λ, e, h, b) satisﬁes λ = λ(t+1), e = e(t+1), h = h(t+1) and b = min{˜b+g(t), B}.
In this way, we update not only the currently visited PDS ˜s(t) but all PDS with common ˜λ(t),
˜e(t) and ˜h(t). This is because the temporal transition of λ, e, h is independent of of the battery
state b and the green power budget realization follows the same distribution since the environment
state e is the same for these states.
C. Convergence of the PDS learning algorithm
Theorem 1. The PDS based online learning algorithm converges to the optimal post-decision
value function V ∗(˜s), ∀˜s when the sequence of learning rates αt satisﬁes P∞
t=0 αt = ∞and
(αt)2 < ∞.
Because C∗(s), ∀s is a deterministic function of V ∗(˜s), ∀˜s, it is straightforward that the PDS
based online learning algorithm also converges to C∗(s), ∀s. Therefore, we prove that the edge
system is able to learn the optimal power demand policy and hence the optimal ofﬂoading and
autoscaling policies using the proposed algorithm.
VI. SIMULATION
We consider each time slot as 15 minutes. The workload value space is Λ={10 unites/sec,
20 units/sec, 30 units/sec}. The environment state space is E={Low, Medium, High}. For each
environment state, the green power will be realized according to a normal distribution with
different means. The network congestion state space is H={50ms, 200ms, 800ms}. The battery
Average Cost
PDS learning (proposed)
Q-learning
Myopic optimization
Fixed 0.4kW
Fixed 1.6kW
Fig. 2. Run-time performance comparison
Battery level (kWh)
Computing power demand (kW)
PDS policy (λ = 10, h = 50 ms, e = Low)
PDS policy (λ = 20, h = 100 ms, e = Medium)
Myopic policy (λ = 10, h = 50 ms, e = Low)
Myopic policy (λ = 20, h = 100 ms, e = Medium)
Learned computing power demand policy.
Battery level (kWh)
Distribution
PDS policy (proposed)
Myopic optimization
Fixed (0.4 kW)
Fixed (1 kW)
Fixed (1.6 kW)
Insufficient
Fig. 4. Battery state distributions.
capacity is B=1kWh. The base station power consumption is 800W and the power consumption
of each edge server is 200W. The maximum service rate of each server is 10 units/second. We
set d0 = 30ms, ω = 0.2 and φ = 10. Three benchmark schemes are used: Fixed power. Fixed
computing power is used whenever possible in each slot. Myopic optimization. This scheme
ignores the temporal correlation between the system states and the decisions and minimizes the
current slot cost. Q-learning. This is a widely-used reinforcement learning algorithm for solving
MDP problems.
(1) Figure 2 illustrates the run-time performance. Each curve is generated by averaging 30
simulation runs. Firstly, the proposed PDS-based learning algorithm incurs a signiﬁcantly lower
cost than all benchmark schemes. At time slot 1000, the cost reduction exceeds 25% compared
to the second-best scheme. Secondly, the ﬁxed power schemes result in tremendously different
performance, which implies that they are sensitive to system parameters. Since the system
dynamics are unknown a priori and may change over time, using a ﬁxed computing power
scheme will cause signiﬁcant performance loss. Thirdly, the performance of Q-learning is much
worse. This is because Q-learning converges very slowly (as can be seen from the ﬁgure, there
is a declining trend) due to the large state space. (2) Figure 3 explains why the proposed
algorithm outperforms the myopic solution by showing the learned policies. When the workload
demand is low and the network is not congested, the policy learned by the proposed algorithm
is very conservative in using local computing power. In this way, more power can be saved for
future when the workload is high and the network congestion state degrades, thereby reducing
the system cost in the long term. On the other hand, the myopic policy ignores this temporal
correlation. It activates local servers to process workload even if the battery level is not so high.
As a result, even though it achieves slight improvement in the current slot, it wastes power
for potentially reducing signiﬁcant cost in the future. (3) Figure 4 show the distribution of the
battery state over 1000 slots in one representative simulation run for the various schemes. If a
too small ﬁxed power demand is used, the battery is in the high state most of the time, implying
that much of the green power is wasted due to the battery capacity constraint. If a too large
ﬁxed power demand is used, the battery tends to be in the low state and hence, it is not able to
support sufﬁciently many servers for processing a large amount of workload locally. Although
a proper ﬁxed power demand is able to strike a decent balance, it does not adapt well to the
changing system dynamics. The proposed PDS-based learning algorithm achieves the highest
harvesting efﬁciency.
VII. CONCLUSION
In this paper, we studied the joint ofﬂoading and autoscaling problem in mobile edge computing systems powered by renewable energy. We found that foresightedness and adaptivity are the
keys to reliable and efﬁcient operation of renewable-powered edge computing systems. To enable
fast learning in the presence of a priori unknown system parameters, a PDS-based reinforcement
learning algorithm was developed to learn the optimal policy by exploiting the special structure
of the considered problem.