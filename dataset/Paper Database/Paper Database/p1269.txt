Stacked Convolutional Auto-Encoders for
Hierarchical Feature Extraction
Jonathan Masci, Ueli Meier, Dan Cire¸san, and J¨urgen Schmidhuber
Istituto Dalle Molle di Studi sull’Intelligenza Artiﬁciale (IDSIA)
Lugano, Switzerland
{jonathan,ueli,dan,juergen}@idsia.ch
Abstract. We present a novel convolutional auto-encoder (CAE) for
unsupervised feature learning. A stack of CAEs forms a convolutional
neural network (CNN). Each CAE is trained using conventional on-line
gradient descent without additional regularization terms. A max-pooling
layer is essential to learn biologically plausible features consistent with
those found by previous approaches. Initializing a CNN with ﬁlters of a
trained CAE stack yields superior performance on a digit (MNIST) and
an object recognition (CIFAR10) benchmark.
Keywords: convolutional neural network, auto-encoder, unsupervised
learning, classiﬁcation.
Introduction
The main purpose of unsupervised learning methods is to extract generally useful features from unlabelled data, to detect and remove input redundancies, and
to preserve only essential aspects of the data in robust and discriminative representations. Unsupervised methods have been routinely used in many scientiﬁc
and industrial applications. In the context of neural network architectures, unsupervised layers can be stacked on top of each other to build deep hierarchies
 . Input layer activations are fed to the ﬁrst layer which feeds the next, and
so on, for all layers in the hierarchy. Deep architectures can be trained in an
unsupervised layer-wise fashion, and later ﬁne-tuned by back-propagation to become classiﬁers . Unsupervised initializations tend to avoid local minima and
increase the network’s performance stability .
Most methods are based on the encoder-decoder paradigm, e.g., . The input is ﬁrst transformed into a typically lower-dimensional space (encoder), and
then expanded to reproduce the initial data (decoder). Once a layer is trained,
its code is fed to the next, to better model highly non-linear dependencies in the
input. Methods using this paradigm include stacks of: Low-Complexity Coding
and Decoding machines (LOCOCODE) , Predictability Minimization layers , Restricted Boltzmann Machines (RBMs) , auto-encoders and
energy based models .
In visual object recognition, CNNs often excel. Unlike patchbased methods they preserve the input’s neighborhood relations and
T. Honkela et al. (Eds.): ICANN 2011, Part I, LNCS 6791, pp. 52–59, 2011.
⃝Springer-Verlag Berlin Heidelberg 2011
Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction
spatial locality in their latent higher-level feature representations. While the
common fully connected deep architectures do not scale well to realistic-sized
high-dimensional images in terms of computational complexity, CNNs do, since
the number of free parameters describing their shared weights does not depend
on the input dimensionality .
This paper introduces the Convolutional Auto-Encoder, a hierarchical unsupervised feature extractor that scales well to high-dimensional inputs. It learns
non-trivial features using plain stochastic gradient descent, and discovers good
CNNs initializations that avoid the numerous distinct local minima of highly
non-convex objective functions arising in virtually all deep learning problems.
Preliminaries
Auto-Encoder
We recall the basic principles of auto-encoder models, e.g., . An auto-encoder
takes an input x ∈Rd and ﬁrst maps it to the latent representation h ∈Rd′ using
a deterministic function of the type h = fθ = σ(Wx + b) with parameters θ =
{W, b}. This “code” is then used to reconstruct the input by a reverse mapping
of f: y = fθ′(h) = σ(W ′h + b′) with θ′ = {W ′, b′}. The two parameter sets
are usually constrained to be of the form W ′ = W T , using the same weights for
encoding the input and decoding the latent representation. Each training pattern
xi is then mapped onto its code hi and its reconstruction yi. The parameters
are optimized, minimizing an appropriate cost function over the training set
Dn = {(x0, t0), ..., (xn, tn)}.
Denoising Auto-Encoder
Without any additional constraints, conventional auto-encoders learn the identity mapping. This problem can be circumvented by using a probabilistic RBM
approach, or sparse coding, or denoising auto-encoders (DAs) trying to reconstruct noisy inputs . The latter performs as well as or even better than
RBMs . Training involves the reconstruction of a clean input from a partially
destroyed one. Input x becomes corrupted input ¯x by adding a variable amount v
of noise distributed according to the characteristics of the input image. Common
choices include binomial noise (switching pixels on or oﬀ) for black and white images, or uncorrelated Gaussian noise for color images. The parameter v represents
the percentage of permissible corruption. The auto-encoder is trained to denoise
the inputs by ﬁrst ﬁnding the latent representation h = fθ(¯x) = σ(W ¯x+b) from
which to reconstruct the original input y = fθ′(h) = σ(W ′h + b′).
Convolutional Neural Networks
CNNs are hierarchical models whose convolutional layers alternate with subsampling layers, reminiscent of simple and complex cells in the primary visual
cortex . The network architecture consists of three basic building blocks
J. Masci et al.
to be stacked and composed as needed. We have the convolutional layer, the
max-pooling layer and the classiﬁcation layer . CNNs are among the most
successful models for supervised image classiﬁcation and set the state-of-the-art
in many benchmarks .
Convolutional Auto-Encoder (CAE)
Fully connected AEs and DAEs both ignore the 2D image structure. This is not
only a problem when dealing with realistically sized inputs, but also introduces
redundancy in the parameters, forcing each feature to be global (i.e., to span the
entire visual ﬁeld). However, the trend in vision and object recognition adopted
by the most successful models is to discover localized features that repeat themselves all over the input. CAEs diﬀers from conventional AEs as their
weights are shared among all locations in the input, preserving spatial locality.
The reconstruction is hence due to a linear combination of basic image patches
based on the latent code.
The CAE architecture is intuitively similar to the one described in Sec. 2.2,
except that the weights are shared. For a mono-channel input x the latent representation of the k-th feature map is given by
hk = σ(x ∗Wk + bk)
where the bias is broadcasted to the whole map, σ is an activation function (we
used the scaled hyperbolic tangent in all our experiments), and ∗denotes the
2D convolution. A single bias per latent map is used, as we want each ﬁlter to
specialize on features of the whole input (one bias per pixel would introduce too
many degrees of freedom). The reconstruction is obtained using
hk ∗˜Wk + c)
where again there is one bias c per input channel. H identiﬁes the group of latent
feature maps; ˜W identiﬁes the ﬂip operation over both dimensions of the weights.
The 2D convolution in equation (1) and (2) is determined by context. The convolution of an m × m matrix with an n × n matrix may in fact result in an (m + n −
1)×(m+n−1) matrix (full convolution) or in an (m−n+1)×(m−n+1) (valid
convolution). The cost function to minimize is the mean squared error (MSE):
(xi −yi)2.
Just as for standard networks the backpropagation algorithm is applied to compute the gradient of the error function with respect to the parameters. This can
be easily obtained by convolution operations using the following formula:
∂W k = x ∗δhk + ˜hk ∗δy.
δh and δy are the deltas of the hidden states and the reconstruction, respectively.
The weights are then updated using stochastic gradient descent.
Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction
Max-Pooling
For hierarchical networks in general and CNNs in particular, a max-pooling layer
 is often introduced to obtain translation-invariant representations. Maxpooling down-samples the latent representation by a constant factor, usually
taking the maximum value over non overlapping sub-regions. This helps improving ﬁlter selectivity, as the activation of each neuron in the latent representation
is determined by the “match” between the feature and the input ﬁeld over the
region of interest. Max-pooling was originally intended for fully-supervised feedforward architectures only.
Here we introduce a max-pooling layer that introduces sparsity over the hidden representation by erasing all non-maximal values in non overlapping subregions. This forces the feature detectors to become more broadly applicable,
avoiding trivial solutions such as having only one weight “on” (identity function). During the reconstruction phase, such a sparse latent code decreases the
average number of ﬁlters contributing to the decoding of each pixel, forcing ﬁlters
to be more general. Consequently, with a max-pooling layer there is no obvious
need for L1 and/or L2 regularization over hidden units and/or weights.
Stacked Convolutional Auto-Encoders (CAES)
Several AEs can be stacked to form a deep hierarchy, e.g. . Each layer receives
its input from the latent representation of the layer below. As for deep belief
networks, unsupervised pre-training can be done in greedy, layer-wise fashion.
Afterwards the weights can be ﬁne-tuned using back-propagation, or the top
level activations can be used as feature vectors for SVMs or other classiﬁers.
Analogously, a CAE stack (CAES) can be used to initialize a CNN with identical
topology prior to a supervised training stage.
Experiments
We begin by visually inspecting the ﬁlters of various CAEs, trained in various
setups on a digit dataset (MNIST ) and on natural images (CIFAR10 ).
In Figure 1 we compare 20 7 × 7 ﬁlters (learned on MNIST) of four CAEs of
the same topology, but trained diﬀerently. The ﬁrst is trained on original digits
(a), the second on noisy inputs with 50% binomial noise added (b), the third
has an additional max-pooling layer of size 2 × 2 (c), and the fourth is trained
on noisy inputs (30% binomial noise) and has a max-pooling layer of size 2 × 2
(d). We add 30% noise in conjunction with max-pooling layers, to avoid loss of
too much relevant information. The CAE without any additional constraints (a)
learns trivial solutions. Interesting and biologically plausible ﬁlters only emerge
once the CAE is trained with a max-pooling layer. With additional noise the
ﬁlters become more localized. For this particular example, max-pooling yields
the visually nicest ﬁlters; those of the other approaches do not have a well-deﬁned
shape. A max-pooling layer is an elegant way of enforcing a sparse code required
to deal with the overcomplete representations of convolutional architectures.
J. Masci et al.
Fig. 1. A randomly selected subset of the ﬁrst layer’s ﬁlters learned on MNIST to
compare noise and pooling. (a) No max-pooling, 0% noise, (b) No max-pooling, 50%
noise, (c) Max-pooling of 2x2, (d) Max-pooling of 2x2, 30% noise.
Fig. 2. A randomly selected subset of the ﬁrst layer’s ﬁlters learned on CIFAR10 to
compare noise and pooling (best viewed in colours). (a) No pooling and 0% noise, (b)
No pooling and 50% noise, (c) Pooling of 2x2 and 0% noise, (d) Pooling of 2x2 and
50% noise.
Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction
When dealing with natural color images, Gaussian noise instead of binomial
noise is added to the input of a denoising CAE. We repeat the above experiment
on CIFAR10. The corresponding ﬁlters are shown in Figure 2. The impact of a
max-pooling layer is striking (c), whereas adding noise (b) has almost no visual
eﬀect except on the weight magnitudes (d). As for MNIST, only a max-pooling
layer guarantees convincing solutions, indicating that max-pooling is essential.
It seems to at least partially solve the problems that usually arise when training
auto-encoders by gradient descent. Another welcome aspect of our approach is
that except for the max-pooling kernel size, no additional parameters have to be
set by trial and error or time consuming cross-validation.
Initializing a CNN with Trained CAES Weights
The ﬁlters found in the previous section are not only interesting in themselves
but also biologically plausible. We now train a CAES and use it to initialize a
CNN with the same topology, to be ﬁne-tuned for classiﬁcation tasks. This has
already shown to alleviate common problems with training deep standard MLPs,
 . We investigate the beneﬁts of unsupervised pre-training through comparisons
with randomly initialized CNNs.
We begin with the well established MNIST benchmark to show the eﬀect
of pre-training for subsets of various sizes. Classiﬁcation results in Table 1 are
based on the complete test set and the speciﬁed numbers of training samples.
The network has 6 hidden layers: 1) convolutional layer with 100 5x5 ﬁlters per
input channel; 2) max-pooling layer of 2x2; 3) convolutional layer with 150 5x5
ﬁlters per map; 4) max-pooling layer of 2x2; 5) convolutional layer of 200 maps
of size 3x3; 6) a fully-connected layer of 300 hidden neurons. The output layer
has a softmax activation function with one neuron per class. The learning rate
is annealed during training. No deformations are applied to MNIST to increase
the “virtual” number of training samples, which would reduce the impact of
unsupervised pre-training for this problem that is already considered as good
as solved. We also test our model on CIFAR10. This dataset is challenging because little information is conveyed by its 32 by 32 pixel input patterns. Many
methods were tested on it. The most successful ones use normalization techniques to remove second order information among pixels , or deep CNNs
 . Our method provides good recognition rates even when trained on “raw”
Table 1. Classiﬁcation results on MNIST
using various subsets of the full data
7.23 1.88 0.71
7.63 2.21 0.79
K-means (4k feat) a
a We performed this experiment using
the code provide by the authors.
Table 2. Classiﬁcation results on CI-
FAR10 using various subsets of the full
data; comparison with other unsupervised
52.30 34.35 21.80
55.52 35.23 22.50
Mean-cov. RBM 
Conv. RBM 
K-means (4k feat) 
J. Masci et al.
pixel information only. We add 5% translations only for supervised ﬁne-tuning,
and re-use the MNIST CNN architecture, except that the input layer has three
maps, one for each color channel. Results are shown in Table 2. On CIFAR10 we
obtain, to our knowledge, the best result so far for any unsupervised architecture
trained on non-whitened data. Using raw data makes the system fully on-line
and, additionally, there is no need to gather statistics over the whole training set.
The performance improvement with respect to the randomly initialized CNN is
bigger than for MNIST because the problem is much harder and the network
proﬁts more from unsupervised pre-training.
Conclusion
We introduced the Convolutional Auto-Encoder, an unsupervised method for hierarchical feature extraction. It learns biologically plausible ﬁlters. A CNN can
be initialized by a CAE stack. While the CAE’s overcomplete hidden representation makes learning even harder than for standard auto-encoders, good ﬁlters
emerge if we use a max-pooling layer, an elegant way of enforcing sparse codes
without any regularization parameters to be set by trial and error. Pre-trained
CNNs tend to outperform randomly initialized nets slightly, but consistently.
Our CIFAR10 result is the best for any unsupervised method trained on the raw
data, and close to the best published result on this benchmark.