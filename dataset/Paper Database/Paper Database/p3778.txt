TrBagg: A Simple Transfer Learning Method
and Its Application to Personalization in Collaborative Tagging
Toshihiro Kamishima, Masahiro Hamasaki, and Shotaro Akaho
National Institute of Advanced Industrial Science and Technology (AIST)
AIST Tsukuba Central 2, Umezono 1–1–1, Tsukuba, Ibaraki, 305–8568 Japan
 ⟨ , 
Abstract—The aim of transfer learning is to improve prediction accuracy on a target task by exploiting the training
examples for tasks that are related to the target one. Transfer
learning has received more attention in recent years, because
this technique is considered to be helpful in reducing the
cost of labeling. In this paper, we propose a very simple
approach to transfer learning: TrBagg, which is the extension
of bagging. TrBagg is composed of two stages: Many weak
classiﬁers are ﬁrst generated as in standard bagging, and
these classiﬁers are then ﬁltered based on their usefulness
for the target task. This simplicity makes it easy to work
reasonably well without severe tuning of learning parameters.
Further, our algorithm equips an algorithmic scheme to avoid
negative transfer. We applied TrBagg to personalized tag
prediction tasks for social bookmarks Our approach has several
convenient characteristics for this task such as adaptation to
multiple tasks with low computational cost.
Keywords-transfer learning; bagging, ensemble learning, personalization, collaborative tagging
I. INTRODUCTION
In this paper, we propose a new transfer learning method,
TrBagg, which is a modiﬁcation of bagging . This method
is applied to a task for personalization of tags in collaborative tagging.
Transfer Learning has not been formally deﬁned, but
it refers broadly to “the problem of retaining and applying
the knowledge learned in one or more tasks to efﬁciently
develop an effective hypothesis for a new task1.” Though the
idea of transfer learning has been in existence since the late
1990s , research in this area has become active recently.
This is mainly because transfer learning is considered a
promising solution to the issue of labeling costs. Machine
learning algorithms generally assume that training and test
data are sampled from a distribution of the same domain.
Therefore, observed data for new tasks must be labeled to
apply machine learning algorithms. However, because data
are generally labeled by hand, labeling requires a lot of time
and effort. Further, the consistency of the labeling criteria
and the amount of labeled data greatly affect the predictive
performance of the learned rules of functions. For these
reasons, the cost of labeling data is a serious obstacle to
1From the announcement of the “NIPS 2005 Workshop, Inductive
Transfer: 10 Years Later”
applying machine learning techniques. One way to alleviate
this obstacle is to exploit existing data. Though labeled data
may not exist for the exact target task that we want to solve,
we are frequently able to access data for tasks related to the
target one. For example, in natural language processing, a
corpus of related domains is available , . Data for past
time periods can be used when analyzing the latest time
period . Transfer learning techniques enable us to exploit
these related data for solving a target task with far lower
labeling costs.
We here propose a simple algorithm, TrBagg. This algorithm is composed of two phases: learning and ﬁltering.
In the learning phase, as in standard bagging, training
data sets are bootstrap-sampled from the target and source
data, and weak classiﬁers are learned from these training
sets. Some of these training sets consist of data that are
largely useful for the target task, while others do not. If the
weak classiﬁers are learned from useful data for the target
task, they are selected in the ﬁltering phase. To achieve
this ﬁltering, we assume that some such classiﬁers would
accurately discriminate the given target data, while other
weak classiﬁers would be abandoned if their accuracies on
the target data are low. TrBagg possesses the following
valuable characteristics. Due to its simplicity, TrBagg works
reasonably well, if learning parameters are not severely
tuned. One serious issue in transfer learning is negative
transfer . In order to alleviate this issue, our TrBagg
is equipped with an algorithmic scheme that introduces a
fallback classiﬁer. When transferring source knowledge to
multiple domains, the computation in the learning phase can
be shared among these domains; thus, computational costs
are largely saved. Another merit of TrBagg is the capability
of learning weak classiﬁers in parallel. This is an enormously
helpful property for processing very large data sets. This
TrBagg is designed under the assumption that the target data
are consistently labeled based on the criterion of the target
task. Or, rather: some parts of the related source data are
labeled based on the target criterion, but the rest of the data
are not. That is to say, the source data set is a mixture of
data representing target and non-target concepts.
The prediction of the tag personalization in collaborative
tagging systems is one example of a task in which the above
2009 Ninth IEEE International Conference on Data Mining
1550-4786/09 $26.00 © 2009 IEEE
DOI 10.1109/ICDM.2009.9
assumption is reasonable. Collaborative tagging services
enable users to register their favorite Web pages. Users can
assign tags to these registered pages, and these pages and
tags can be shared among users of the service. With these
shared tags, users can search pages that other users have
registered, and they can ﬁnd like-minded users. Social tags
are labeled based on each user’s personal criterion; thus,
the semantics of social tags can vary greatly. Consequently,
tags labeled by one user may be inappropriate for another
user. When searching for documents with shared tags, users
might ﬁnd undesired documents or miss relevant pages. To
cope with this difﬁculty, tags are personalized for a speciﬁc
target user. By using tags labeled by the target user as
training examples, classiﬁcation techniques make it possible
to predict tags for new Web pages that are appropriate
from the user’s viewpoint. However, the number of tags
that are labeled by one user is generally too small for
training reliable classiﬁers. The transfer learning approach
is effective for addressing this problem, because it enables
the utilization of larger numbers of tags labeled by nontarget users. The target user’s tags and non-target users’
tags are treated as the target and source data, respectively.
Presumably, some of the non-target users would label Web
pages based on the same criterion as the target user’s, while
others would not. In short, this source data set would be a
mixture of useful and useless data. Consequently, we can
say that the assumption on which TrBagg is based would be
reasonable for this personalization task. We will later show
the experimental results of the application of our TrBagg to
this task.
Our contributions can be summarized as follows: We
have proposed a simple transfer learning algorithm, TrBagg,
which worked reasonably well without severe parameter
tuning and was less affected by the problem of negative
transfer than other transfer learning algorithms. Using this
algorithm, we have shown that tags can be personalized in
collaborative tagging services.
In section II, we introduce our TrBagg algorithm. Section III describes a tag personalization task in collaborative
ﬁltering. Section IV shows the experimental results on real
tagging data. After discussing the related work in section V,
we conclude in section VI.
II. TRANSFER LEARNING AND TRBAGG
Here we address the transfer learning problem and introduce the details of our TrBagg, which is developed
by modifying bagging so as to be applicable for transfer
A. Transfer Learning
We here state the transfer learning problem in the form
that is used in this paper. A more inclusive discussion of
transfer learning is postponed until section V.
An object is represented by a feature vector, x. The
variable c denotes the class in which the object should
be classiﬁed. The pair of a class and object, (ci, xi), is a
training example. The goal of classiﬁcation is to acquire
classiﬁers that can predict an appropriate class for any input
feature vector, x, from a set of training examples. A standard
classiﬁer is learned from only one homogeneous set of
training examples. These training examples are assumed to
be sampled from a distribution, P[c, x], which expresses
the target concept to be learned. On the other hand, in the
case of transfer learning, two types of training examples,
target and source, are given. Similar to the case of standard
classiﬁcation, target data are assumed to be sampled from a
distribution, P[c, x], that corresponds to the target concept.
This set of target data is denoted by DT = {(ci, xi)}NT
where NT = |DT |, and the source data set is denoted by
DS = {(ci, xi)}NS
i=1, where NS = |DS|. Regarding these
target and source data sets, we introduce three assumptions:
First, the source data are sampled from distributions that
express both consistent and irrelevant concepts. Second, the
number of examples of the target concept is as at least a
few times as large as NT , and the learner does not know
which examples express the target concept. Third, the size
of the source data set is much larger than that of the target
set, i.e., NS ≫NT .
These target and source data are mutually complementary.
That is to say, target data are consistent with the target
concept and are relatively few, while source data are less
reliable and more abundant. If we can extract information
about the target concept from a target data set and can exploit
the information along with information from the source data,
the learner can know more about the target concept. The goal
of transfer learning is to acquire classiﬁers that can more
accurately predict classes by exploiting the information in
source data.
B. Bagging
To solve the above problem, we developed a TrBagg algorithm (Transfer Bagging.) Before showing our TrBagg, we
brieﬂy describe the original bagging method; our algorithm
is a modiﬁed version of the original method . In the
bagging algorithm, the following steps are iterated for t =
1, . . . , T.
1) Obtain a training example set Dt by bootstrap sampling, that is, sampling with replacement, from a given
training example set D.
2) From this training set Dt, learn a weak classiﬁer ˆft
by using an arbitrary classiﬁcation algorithm.
By this procedure, a set of T weak classiﬁers, F
{ ˆf1, . . . , ˆfT }, can be acquired. To classify a new feature
vector, this vector is classiﬁed by these T classiﬁers, and
these classiﬁcation results are aggregated by majority voting.
Formally, the class to which the feature vector x should
belong is determined by
ˆc = arg max
I[c = ˆft(x)],
where I[cond] is an indicator function, which becomes 1 if
the condition cond is true and 0 otherwise, and C denotes
the domain of classes.
The reason why this bagging improves the prediction
accuracy has been explained simply based on the biasvariance theory , . According to the bias-variance
theory, a generalization error is divided into three factors:
the bias, which is derived from the choice of models, the
variance, which is derived from the variation in sampling of
training examples, and the intrinsic error, which cannot be
avoided. If a low-bias model, which can approximate various
forms of functions, is used for learning, the effect of the bias
factor can be lessened, while that of the variance factor has
a greater effect on the generalization error. Inversely, in the
case of a high-bias model, the degrees of effect caused by
the bias and variance factors are exchanged. In the process
of bagging, various training example sets are generated,
weak classiﬁers are learned from each example set, and
these classiﬁers are aggregated. Because this aggregated
classiﬁer has been trained using various training sets, the
variance factor can be lessened without increasing the error
caused by the bias factor. Therefore, if a low-bias model is
adopted, the bagging technique contributes to reducing the
generalization error. However, if a high-bias model such as
Fisher’s discriminant analysis is used, bagging fails to reduce
the prediction error, because the ratio of the error caused by
the variance factor is originally small.
We developed our TrBagg algorithm by modifying this
bagging procedure. In [8, section 7], an improvement technique for bagging is reported. In standard bagging, training
examples are bootstrap-sampled from a given example set,
and the sampled examples are fed to a weak learner without
modiﬁcation. Instead, by adding noise to the feature vectors
in the sampled examples, the prediction accuracy can be
improved. This is because more varied weak classiﬁers
can be learned by adding noise. An improvement in the
prediction performance by perturbing weak classiﬁers was
also reported in .
Our TrBagg algorithm is inspired by this technique. In
the process of TrBagg, training examples are sampled from
both source and target data. We expected that the variance
part of the error can be more drastically reduced, because
a source data set contains more diverse examples than a
target set. However, there is one difﬁculty. A source data
set contains examples of irrelevant concepts, which we
want to ignore, and we cannot discriminate which examples
represent the target concept. To avoid this difﬁculty, we
exploit target data that are consistent with the target concept.
Speciﬁcally, a weak classiﬁer is learned from both source
and target data, target data are classiﬁed by this weak
classiﬁer, and its empirical accuracy for the target data is
computed. If the accuracy is sufﬁciently high, we consider
that most of the examples that have been used to train
the weak classiﬁer are consistent with the target concept.
If the accuracy is not sufﬁcient, the acquired classiﬁer is
abandoned. By iterating these procedures, the learner can
obtain weak classiﬁers that are consistent with the target
concept and that additionally have wide diversity. In this
way, weak classiﬁers are acquired, and the ﬁnal class is
determined by majority voting, as in standard bagging. We
refer to the procedures of learning weak classiﬁers and the
selection of useful classiﬁers as the learning and ﬁltering
phases, respectively. We next describe the details of each
1) Learning Phase: In the learning phase, weak classi-
ﬁers are learned. Formally, we generate a merged data set,
D = DT ∪DS. As in standard bagging, a training set,
Dt, is generated by bootstrap-sampling from the merged
set, D. By using an arbitrary classiﬁcation algorithm, a
weak classiﬁer, ˆft, is learned from the training set, Dt.
By repeating these procedures for t = 1, . . . , T, T weak
classiﬁers, ˆf1, . . . , ˆfT , are acquired, where T is the initial
number of weak classiﬁers.
This procedure is almost the same as that of standard
bagging, but the size of the training set, |Dt|, differs. In the
case of bagging, the size of the training set of a weak learner
is the same as the size of the given training data set, i.e.,
|Dt| = |D|. In our TrBagg, the size of this set, Nt, should
be smaller than |D|. N ′
T denotes the number of data that are
consistent with the target concept in the source set. If N ′
is smaller than Nt, poor weak classiﬁers would be acquired
with high probability, because training sets would contain
less data that are consistent with the target concept. While
T is assumed to be unknown, it is also assumed to be a few
times as large as NT . Therefore, we set Nt to the size of the
target set, NT . If NT is too small, good weak classiﬁers are
acquired less frequently. In such cases, the larger training
set is preferred, or the sampling sizes themselves can be
changed randomly.
2) Filtering Phase: After the completion of the learning
phase, a set of T weak classiﬁers, F = { ˆf1, . . . , ˆfT }, is
available. In the ﬁltering phase, from F, a set of weak
classiﬁers, F∗
⊆F, is selected so that labels of the
target concept are accurately predicted. Once a set of weak
classiﬁers, F∗, is given, the classiﬁcation procedure of our
TrBagg is the same as that of standard bagging. Majority
voting in equation (1) is used to predict the classes for a
new vector, except that F∗is adopted instead of F.
We developed the following three ﬁltering approaches.
Majority voting on the target set: This ﬁrst approach
tries to ﬁnd a subset of F so as to minimize the empirical
errors on the target set derived by majority voting of
INPUT: F = { ˆf1, . . . , ˆfT } and DT
1: learn a classiﬁer, ˆf0, from the target set, DT ;
2: compute empirical errors on DT of each classiﬁer in F, and
sort classiﬁers in ascending of these errors: ⟨fi0, fi1, . . . , fiT ⟩
3: ϵ ←empirical error of fi0 on DT ; F′ ←{fi0}; F∗←{fi0}
4: for t = 1, 2, . . . T do
F′ ←F′ ∪{ ˆfit}
ϵ′ ←empirical error of a majority vote in F′ on DT
if ϵ′ ≤ϵ then F∗←F′; ϵ ←ϵ′
8: output F∗
Majority voting on the target set
classiﬁers in the subset of F . Due to the computational
cost of checking all subsets of F, we adopted a heuristic
procedure in Figure 1. In step 1, a classiﬁer, ˆf0, is learned
from the target set, and is added to F. We call
fallback classiﬁer, which is useful for avoiding negative
transfer as described later. Note that this fallback classiﬁer
is trained so as not to over-ﬁt to target training data. In
step 2, weak classiﬁers are sorted in ascending order of
their empirical errors on the target set. By this step, the
more promising classiﬁer is preferentially selected. After
initialization in step 3, the searching loop from step 4
starts. In this loop, one classiﬁer is iteratively added to the
candidate set, F′. Majority voting by classiﬁers in F′ is
performed for the target data, and the empirical error by this
voting is calculated. By iterating this loop, this algorithm
ﬁnds the subset, F∗, with a small empirical error on DT .
The time complexity of this algorithm is O((T + 1)NT ),
which is the required computational cost of evaluating all
target data by all weak classiﬁers, plus the cost of learning
one weak classiﬁer. We abbreviate this approach as MVT.
Majority voting on the validation set: Minimizing the
empirical error on the target may cause over-ﬁtting. To avoid
this problem, part of the target data is used as a validation
set. Empirical errors are evaluated for this validation set
using the algorithm in Figure 1. This approach is effective
if the target data are fully available and enough validation
data can be maintained. However, because the validation
data must be separated from the data for training weak
classiﬁers in the learning stage, a small target set damages
the prediction in this method more severely than in MVT.
We abbreviate this approach as MVV.
Simple comparison: This approach was adopted in our
preliminary studies published in workshops , . A
weak classiﬁer, ˆf0, is learned from the target set, DT . If a
weak classiﬁer in F has an empirical error on the DT that is
worse than that of ˆf0, it is rejected. If all classiﬁers in F are
rejected, the set that contains only ˆf0 is outputted. Contrary
to the above approaches, this approach ignores the potential
improvement by combining weak classiﬁers. We abbreviate
this approach as SC.
D. Useful Properties of TrBagg
TrBagg has the following desirable characteristics in
practical use.
Ease in tuning and implementation: The most prominent merit of TrBagg is its simplicity: It is easy to implement, and the number of learning parameters to tune
is very few. The learning parameters are those of weak
learners and optionally the size of the sampled training data.
Consequently, it is generally easy to tune this algorithm.
And, despite this ease, this algorithm works with reasonable
accuracy as shown in section IV. Though other transfer
learning methods might perform better if their learning
parameters are ﬁnely tuned, such tuning is can be difﬁcult.
Contrary to such methods, our TrBagg is off-the-shelf.
Addressing a negative transfer problem: In the ﬁltering
phase, we introduced a fallback classiﬁer, ˆf0, that is trained
from the target data set. This classiﬁer ensures output F∗=
{ ˆf0}, which contains only a fallback classiﬁer, if no useful
knowledge is found in the source data. Because the empirical
error of the classiﬁer, ˆf0, is guaranteed, negative transfer
 can be frequently avoided. While there is no theoretical
guarantee of its effectiveness, this simple trick worked well
in our experiments, shown later.
Multiple domain transfer: The learning phase is much
more time-consuming than the ﬁltering phase. We now
consider the situation in which the training sets of weak
learners consist of data from multiple domains, and must
be transferred to each of these domains. In this situation,
the generation of weak classiﬁers in the learning phase can
be shared; thus, most of the computational cost can be
saved. In the example case of tag personalization described
in the introduction, given the tags labeled by all users,
we wanted to personalize the results to each user. In the
learning stage, training data for weak learners are sampled
from tags labeled by all users. Then, these acquired weak
learners are delivered for each user. Each user can perform
the ﬁltering phase by referring only to his/her own tags,
and can successfully transfer other users’ knowledge from
received weak classiﬁers. We consider this characteristic to
be a strong point of our TrBagg.
Parallel computation: Recently developed CPUs have
many cores, and the construction cost for computer clusters
is coming down. As a consequence, the adaptability of
processes of parallel computation is becoming more attractive. In the time-consuming learning phase of TrBagg, weak
classiﬁers can be learned in parallel. This adaptability to
parallel computation is inherited from standard bagging
III. PERSONALIZATION IN COLLABORATIVE TAGGING
Collaborative tagging services such as delicious.com enable users to register their favorite Web pages. For these
registered pages, users can assign tags, which are words
that describe the contents, characteristics, or categories of
the tagged pages. These tags are useful for searching or
classifying their own registered pages. In addition, these
pages and tags can be shared among users of the service.
With these shared tags, users can search the pages that other
users have registered and ﬁnd like-minded users.
This social tagging process differs from a traditional
classiﬁcation scheme. In the case of a document repository
or library collection, the materials are classiﬁed under well
maintained and uniﬁed criteria. That is to say, the semantics
of the labels are strictly deﬁned. By contrast, social tags are
based on each user’s personal criterion. Users can freely
choose their favorite tags; thus, the semantics of social
tags can vary greatly. Golder and Huberman addressed such
inconsistency among collaborative tags . They discussed
the causes of variations in the semantics of the tags. One
variation involves the degree of speciﬁcity. For example,
the ofﬁcial page of the programming language python can
be tagged by either the speciﬁc word, “python,” or the
general one, “programming.” Another cause is polysemous
words having many related meanings. For example, one
user may consider “data mining” a statistical technique for
marketing research. But another user may use this term to
indicate techniques for analyzing massive data sets. Note
that these polysemous words are different from homonymous
words, which have multiple unrelated meanings. Further,
synonymous words or singular/plural forms give rise to
another type of inconsistency. As a consequence, the tags
of one user may be inappropriate for another user. When
searching for documents with shared tags, users might ﬁnd
undesired documents or miss relevant pages.
If we focus on a speciﬁc target user, the choice of tags
would be highly consistent, because users would presumably
follow their own preference pattern, which would be highly
self-consistent. We here perform a tag prediction task that
is personalized to the target user as follows. First, for each
candidate tag, we acquire a binary classiﬁer to discriminate
whether the target user will assign the candidate tag to Web
pages. To personalize this discrimination, this classiﬁer is
trained from Web pages that are tagged by the target user,
because such tags are considered to reﬂect the target user’s
concepts. Once such classiﬁers are learned, these can be used
for judging whether each candidate tag should be assigned to
a new Web page. For example, consider the case in which the
target user prefers the tag “python” to “programming.” The
classiﬁers for the tag “python” and “programming” would
return positive and negative outputs, respectively, and tags
that are appropriate for the target user could be estimated. Of
course, it is an inefﬁcient approach to learn binary classiﬁers
for all tags. We will tackle this problem by introducing
multi-label classiﬁers in future research.
However, we encountered one serious problem, which is
often referred to as the cold-start problem in a recommendation context . In order that Web pages are tagged
based on the concept of the target user, binary classiﬁers
are learned from a set of Web pages that have been already
tagged by the user. The number of such Web pages is
generally small, because it is difﬁcult for one user to tag
thousands of pages. In this case, the learned classiﬁers
cannot make precise predictions, due to the lack of training
Transfer learning can overcome this difﬁculty. We can
exploit the enormous number of Web pages that have been
tagged by non-target users. These Web pages cannot be
directly used for training classiﬁers, because most of them
are inconsistent with the target user’s concepts. We can
apply our TrBagg by treating the target user’s and the
non-target users’ tagging information as target and source
data, respectively. We expect that TrBagg will enable us
to learn more precise classiﬁers, because these source data
contain some useful information for learning the target user’s
concept. In particular, some users would surely select the
same level of speciﬁcity as that of the target user, and
some users would deﬁnitely use polysemous words in the
same sense as the target user. Furthermore, as described
in section II-D, TrBagg can transfer knowledge to multiple
domains with low computational cost. Once a set of weak
classiﬁers is learned, the personalization to each user is fast.
In addition, because only the user’s own tagging data are
required to perform the ﬁltering phase, users can personalize
without disclosing their own bookmarks.
IV. EXPERIMENTS
We next apply our TrBagg to the task of tag personalization in collaborative tagging. After introducing experimental
settings, we compare it with standard bagging and other
transfer learning methods, and we show the variation of the
performance in different settings.
A. Data Sets and Experimental Settings
We here describe our collaborative tagging data sets. We
obtained data from two social bookmark services: delicious
( and hatena ( In
both sites, many Web pages regarding technical topics are
bookmarked. The delicious site is a pioneer of the social
bookmarking service. We crawled this site in July, 2007.
The number of unique URLs, tags, and users were 762454,
172817, and 6488, respectively. We found 3198185 bookmarks, or pairs of one registered URL and one tag assigned
to the URL. The hatena site is a major bookmark site in
Japan. We crawled this site in November, 2006. The numbers
of unique URLs, tags, and users were 488978, 117264, and
15526, respectively. We found 6165052 bookmarks. Note
that the sizes of data sets are generally larger than those
of the delicious data, mainly because of the function that
suggests tags that have been already assigned.
We counted up the number of URLs to which each tag
was assigned, and selected the 20 most-assigned tags2. We
2The delicious tag “imported” was skipped, because the subsequent data
processing eliminated all the positive examples in the source data
THE SIZES OF COLLABORATIVE TAGGING DATA SETS
(a) delicious data set
target source
target source
1405 25353
5455 21857
6323 19512
3512 30264
6359 23335
6311 22914
1151 24288
programming
4498 25931
3472 18437
1291 31024
3518 28593
3493 23625
3218 22588
1870 30334
3509 23543
6258 16574
1098 25427
(b) hatena data set
target source
target source
5776 70531
8691 65493
8691 62146
5504 61914
7220 72804
1923 46284
4912 44920
1780 19201
3867 70390
2345 53362
7903 73400
1554 37171
2420 62345
11132 71846
4634 35245
3289 52738
4875 34566
5506 54355
2657 53533
programming
4823 28676
NOTE: The “tag name” columns show the words used as target
tags. Tags written in italics were originally in Japanese. Note
that neta means “something to talk about” and 2ch is the title
of a discussion board. The “target” and “source” columns show
the sizes of the corresponding target and source data sets.
focused on each of these tags, and call it a target tag. For
each target tag, we focused on the top user, who was the
user who assigned the target tag to the most URLs among all
users. This top user and the URLs tagged by this user were
treated as the target user and as the target data, respectively.
We tried a binary classiﬁcation to predict whether the target
tag would be assigned to a given URL or not. Among all
URLs tagged by the target user, the URLs with the target tag
were used as positive examples, and the rest of the URLs
were used as negative ones. As the source data set, we used
the URLs tagged by the second to twentieth top users of the
target tag. We refer to these nineteen users as source users.
For each of the twenty target tags, we generated data
sets. The sizes of target and source data are summarized in
Table I. Investigating the most popular tags would certainly
lead to the results that deviate from the standard tags. We
therefore generated shrunken data sets in which a part of
the target data was discarded. The data sets involving all the
target data are denoted by “ALL.” If 3/4 of target data were
discarded, the data set was denoted by “1/4.” For example,
in the case of the blog tag of the delicious site, the “All”
data set consisted of 603 target and 24201 source data. The
“1/2” data set still had 24201 source data, but the number
of target data were reduced to 302. For each target tag and
site, we generated “1/2” to “1/32” data sets.
PREDICTION ACCURACIES ON THE TARGET DATA OF THE HATENA SITE
NOTE: The win/loss tally is shown as in Table II.
We next turned to the features to represent URLs. As
features, we adopted the most popular 100 tags other than
the target tag. That is to say, the i-th element of the
feature was the number of users who assigned the i-th most
popular tag to the URL. We then abandoned the URLs
to which none of the top 100 tags was assigned. In the
context of recommendation, estimation methods based on
other users’ tags are referred to as collaborative ﬁltering. In
this setting, methods specialized for collaborative ﬁltering
would be straightforward. However, we plan to use text
features extracted from Web pages in future work. Using
text features enables us to recommend tags for Web pages
that are not tagged by any users, the existence of which
is one of weak points of the collaborative approach. The
reason why we use other users’ tags as features is to avoid
data cleansing. If in the present case we adopted text features
that needed elaborate data cleansing, we would not be able
to determine whether any potential failure of estimation was
caused by the failure of transfer or the poor data cleansing.
We performed a ﬁve-fold cross-validation test. The target
example set was divided into ﬁve blocks. One block was
respectively picked, and the examples in these blocks were
used for testing. The remaining four target blocks and all the
source examples were used as the target and source training
data, respectively. Note that original test data sizes were
kept even for the “1/2 – 1/32” data sets. For example, if
the original target size was 100, the training and test data
sizes of the “All” case became 80 and 20, respectively. In
the case of “1/2”, the training data were shrunk to 40, but
the test size was kept at 20.
Default experimental conditions were as follows: As weak
learners, we adopted a naive Bayes learner with multinomial
model , because we preferred a fast learner to deal with
data sets that are much larger than popular data sets, e.g.,
20Newsgroups or Reuters. The number of weak classiﬁers
was set to 100. The size of the sampled training data for a
weak learner, Dt, was the same as that of the target data,
NT . The ﬁltering of TrBagg was performed by the MVT
method. In the statistical tests, we used the Z-test to check
the difference between ratios at the signiﬁcance level of 1%.
B. Comparison with a Standard Bagging
We ﬁrst compared our TrBagg with a baseline method. As
described in section II-D, TrBagg was consistently superior
to the weak classiﬁer learned from the target data. Therefore,
as a baseline method, we chose a standard bagging method
in which weak classiﬁers were trained using the target data.
PREDICTION ACCURACIES ON THE TARGET DATA OF THE DELICIOUS SITE
size of target data sets
programming
NOTE: The column “tag name” shows the strings of the target tags. The column pairs “All”, “1/2 – 1/32”, indicate the sizes
of the target data sets. The left TBt and right BGt columns of each pair show the results derived by TrBagg and baseline
bagging, respectively. Each row shows the accuracies for the corresponding target tag. Bold face indicates the “winner,” i.e.,
the accuracy was statistically greater than its opponent. The last row “W/L” shows the number of target tags for which our
method won/lost against baseline bagging.
The accuracies on the target data sets are shown in
Tables II and III. Our TrBagg was equal to or superior
to standard bagging for all experiments on the delicious
data. Regarding the hatena data, though TrBagg lost against
bagging in several cases, TrBagg won much more frequently.
These results clearly show the effectiveness of TrBagg.
Further, the effectiveness of TrBagg became more signiﬁcant
as the size of the target sets decreased. Speciﬁcally, the
decrease of the accuracy with decreased size of target sets
was less using TrBagg than that using standard bagging.
This fact enhances the usefulness of our method, because
transfer learning is more useful when fewer target data are
available. Note that we also examined bagging in which
weak classiﬁers were trained from both the target and source
data, but the prediction accuracies on the target data were
worse than in the above bagging.
In terms of this comparison, we also performed experiments on delicious data using SVMs as weak learners. Note
that the hatena set was too large to apply SVM. We used the
kernlab package on R, and linear kernels were adopted due
to the fatal slowness of the other types of kernels. In this
experiment, no statistical differences were observed between
our TrBagg and standard bagging. However, even in this
case, the inference by the classiﬁers learned by TrBagg was
faster, because irrelevant weak classiﬁers were ﬁltered out.
In summary, TrBagg successfully acquired more accurate
classiﬁers than the baseline. It can be concluded that TrBagg
succeeded in learning better classiﬁers by exploiting useful
information involved in the source data.
C. Comparison with Other Transfer Learning Methods
Our TrBagg was compared with the following transfer
learning methods. First, naive Bayes was chosen as a baseline. The classiﬁer trained with the target set is denoted
by NBt, and that trained with both the target and source
data is denoted by NBm. The bagging classiﬁer used in
section IV-B is denoted by BGt. Further, that trained by both
the target and source data is denoted by BGm. TrAdaBoost
 is a transfer-learning version of the AdaBoost. In our
implementation, naive Bayes was used as a weak classiﬁer.
If the empirical error on the target set was larger than
1/2, positive and negative classes of weak classiﬁers were
exchanged. Further, if the empirical error was very small,
it was made a small constant to avoid zero-division. The
number of weak classiﬁers was changed to 10, 30, 50,
and 100, and the best classiﬁer was selected. The use of
independent validation sets for the evaluation is denoted
by TAv. As a validation set, one block in a training set
was used, and the training data set was reduced to three
blocks. The use of test sets for evaluation is denoted by TAt.
The performance of TAt was over-estimated, because this
accesses to test data. While the accuracies of TAv show the
level of performance when learning parameters are selected
COMPARISON WITH OTHER METHODS
(a) delicious
NBt NBm BGt BGm TAv TAt MX FE TBt
8/1 7/2 7/0
8/1 7/1 10/0 8/1
9/1 8/1 12/0 10/0 —
NBt NBm BGt BGm TAv TAt MX FE TBt
3/4 3/7 10/4 1/4 0/10
4/2 3/6 12/2 2/3 0/10
10/0 11/2 10/0
11/0 9/3 15/1 9/0
(b) hatena
NBt NBm BGt BGm TAv TAt MX FE TBt
8/7 3/10 11/3 14/4 0/2
5/9 13/3 14/3 0/2
3/8 12/3 14/4 —
3/8 12/2 14/4 1/11
6/6 14/1 16/3 2/9
11/1 17/0 9/2
11/1 13/3 9/6 18/1 17/3 —
NOTE: Each entry shows the counts of target tags the row
method won (left) or lost (right) against the column method.
The labels “All” and “1/6” indicate the size of the data sets.
using cross-validation, those of TAt show the potential
performance. We assumed that the target data were generated
from a single multinomial model and the source data were
generated from a mixture model of the target model and
another multinomial model. MX denotes a mixture model,
which implemented this assumption directly. While target
data are generated purely from the target distribution, source
data are generated from a mixture distribution of target
and non-target distributions. This can be considered as a
generative version of an approach in FE denotes the
frustratingly easy method in , which is as easy to
implement as our TrBagg. Finally, our TrBagg is denoted
by TBt. Note that for a few cases in which the target set
was too small to maintain the validation set, the results are
counted as a tie.
Overall, our TrBagg outperformed the other transfer learning methods as shown in Table IV. Regarding the baseline methods, i.e. single naive Bayes and bagging, TrBagg
almost always won. If knowledge was well transferred,
TrAdaBoost performed well, but the differences between
it and TrBagg were not signiﬁcant in almost all cases.
Inversely, because TrAdaBoost doesn’t have a method of
avoiding negative transfer, the prediction accuracies were
degraded if the source data contain no useful knowledge.
Additionally, TrAdaBoost was performed relatively well on
the hatena set, whose size is larger. TrAdaBoost determined
the sampling distribution based on accuracies on the target
data. To evaluate the accuracies precisely, the larger-size
target data sets are preferred. Therefore, our TrBagg was
advantageous for the relatively small target data set, as in this
experiment. Although the mixture model directly represented
COMPARISON WITH OTHER METHODS
CHANGING THE FILTERING APPROACHES
NOTE: Each cell shows the win/loss tally of the MVT approach
against MVV or SC.
TrBagg’s assumption, it performed poorly. We think that this
is mainly because the distribution of the source data was
rather complicated, and simple multinomial models failed
to ﬁt. We suppose that the reason why the frustratingly
easy method performed poorly was the mismatch of the
assumption. This method assumes that useful features are
different for each domain, but in this personalization task,
useful samples are different for each domain.
For comparison, we applied the above methods to the 20
Newsgroups corpus, which has been widely tested such as
in . Texts were processed as in , except that the most
frequent 5000 words were adopted as features because of the
limitation of our implementation. In addition to the original
setting, we tested the case in which the target data were
reduced to 1/16. According to the accuracies in Table V,
no clear improvement was observed. This would be because
the target data sets were larger in comparison with their
corresponding source sets. Again, the superiority of TAt to
TAv indicated the possibility of improvements by tuning
learning parameters. On the other hand, our TrBagg worked
reasonably well because of the simplicity in tuning and the
functionality of avoiding negative transfer.
In summary, our TrBagg was performed well compared
with other transfer learning methods especially when the
target data were relatively few in number.
D. Behaviors of TrBagg
Finally, we investigate the behaviors of TrBagg.
First, we changed the ﬁltering approaches in section II-C2.
The MVT method was compared with the other two methods. Though the MVV has the disadvantage that the number
of training data must be reduced to prepare independent
validation data, it also has the advantage that it can reduce
the expected generalization error directly. We therefore anticipated that MVV would perform well if a large target data
CHANGING THE NUMBER OF WEAK LEARNERS
delicious-All
delicious-1/16
hatena-All
hatena-1/16
NOTE: The win/loss tally compared with standard bagging
using the same number of weak learners.
set were given. The experimental results in Table VI show
that MVV is of comparative performance if target data are
abundant, but its performance is worsened by the reduction
of target data. Our old SC performed poorly for the small
target sets, too. This is because weak classiﬁers better than
ˆf0 are less frequently found, and the transfer of knowledge
Second, we changed the number of weak learners used in
TrBagg. We increased the size from 100 to 1000 as shown
in Table VII. Slight improvements were observed between
100 and 300 for two “All” data sets, but the accuracies
were almost saturated, and no remarkable improvement in
performance was observed. The most important thing in this
experiment was that no over-ﬁtting was observed.
Third, we changed the sizes of the training sets for weak
learners. So far, the size is ﬁxed to the size of the target data,
NT . We experimented with sampling sizes of 5% and 10%
of NT +NS. Overall, the larger size of sampling data seemed
to be preferable if NT was too small (data not shown).
V. RELATED WORK
The idea of using non-target data in bagging can be
found in , in which irrelevant data are exploited to
reduce the correlations among the errors of weak classiﬁers.
Several ensemble approaches for transfer learning have been
proposed. Given multiple weak classiﬁers that are trained in
different domains, these are combined with weights based on
relatedness to the target domains , . An AdaBoost
algorithm was modiﬁed so as to be better ﬁt for transfer
learning by less weighting of useless source data . While
boosting adjusts the weights of data or classiﬁers adaptively, bagging reduces variance by exploiting randomness
in sampling. This is the intrinsic difference between the
two approaches . The above methods basically follow a
boosting-like strategy; thus, they are highly effective if target
data are abundant and sufﬁcient information is available.
On the other hand, because our method is a non-adaptive
approach and depends on randomness, ours works when
fewer target data and less information is available.
Many transfer learning methods have been proposed .
As described in the introduction, it is difﬁcult to provide
a formal deﬁnition of transfer learning. This is mainly due
to the lack of an adequate deﬁnition of relatedness between
tasks, which remains the most important open problem in
the study of transfer learning . Different kinds of relatedness are assumed in different techniques. For example,
the sets of features that are useful for completing tasks
are slightly different , and the data distributions of the
features are different between the training and test data sets
 . We think that it would be almost impossible to give
a uniﬁed formal deﬁnition of such relatedness. Different
transfer learning algorithms are designed based on their own
assumptions about the relatedness. We call these algorithmspeciﬁc assumptions by transfer assumptions. If the involved
transfer assumption is reasonable, useful knowledge can
be transferred from the related source data. However, an
unreasonable assumption would cause negative transfer. The
most general form of transfer assumption is deﬁned by
the difference between distributions . However, if more
knowledge is available, a more speciﬁc assumption makes
it possible to transfer more knowledge. For example, positive and negative target data are generated from respective
Gaussians. The source data are generated from Gaussians of
which centers are the same as those of the target distributions
and of which variances are smaller than those of the target
distributions. If these conditions are known, the source data
can be exploited more effectively. We therefore conclude that
more new transfer learning methods should be developed for
speciﬁc transfer assumptions.
Given transfer assumptions must be manipulated using
mathematical models, which we here call transfer models.
Though one classiﬁcation scheme of these models was
proposed in , we offer a modiﬁed classiﬁcation scheme,
expressed in two axes. One axis designates whether the
transferred data knowledge is processed on the source side
or target side. While source data are transformed so as
to ﬁt the goal of the target domain, target models are
designed so as to accept untransformed source data. The
other axis represents the types of knowledge to transform.
Feature-based approaches transform the feature space so as
to be useful in the target domain. While feature spaces are
transformed on the source side , target side models in 
are designed to weigh features automatically. In the instancebased approach, each instance in data sets is weighted or
selected according to its relatedness to the target domain.
While the above ensemble techniques and covariate shifts
 , weigh the instances on the source side, migratorylogit adopts the target models to decide the importance
of instances automatically.
We ﬁnally discuss tagging on Web pages or blog posts. P-
TAG is a system that predicts appropriate tags for given
Web pages. Because this prediction is based on the user’s
personal repository, and other users’ tags are not referred
to, there is no need to consider any inconsistencies among
tagging criteria. Autotag , TagAssist , and a method
in are designed to predict proper tags for blog articles
based on similar blog posts. Though other users’ tags are
used, inconsistency in tags is not taken into account.
VI. CONCLUSION
We proposed a new transfer learning algorithm, TrBagg.
This algorithm is very simple: after generating many weak
classiﬁers from target and source data, these classiﬁers
are ﬁltered using the target data. This method has several
advantages in practical use. TrBagg worked reasonably well
in spite of its ease of tuning, and it could alleviate the
inﬂuence of negative transfer. We applied our TrBagg to
collaborative tagging data sets and showed its effectiveness.
In order to make our tag personalization method more
useful, we would like to introduce models that can deal
with multiple tags. To achieve this, we want to improve our
TrBagg so as to handle multi-label classiﬁcation problems
more elaborately. We also need to further enhance the
theoretical background of our method.
ACKNOWLEDGMENT
We wish to thank Dr. Yutaka Matuo and Dr. Atsushi Fujii
for their valuable advices. Thanks are due to Hottolink Inc.
for assistance in crawling.