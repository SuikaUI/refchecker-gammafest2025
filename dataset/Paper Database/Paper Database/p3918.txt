Learning Mid-level Filters for Person Re-identiﬁcation
Wanli Ouyang
Xiaogang Wang
Department of Electronic Engineering, the Chinese University of Hong Kong
{rzhao, wlouyang, xgwang}@ee.cuhk.edu.hk
In this paper, we propose a novel approach of learning
mid-level ﬁlters from automatically discovered patch clusters for person re-identiﬁcation. It is well motivated by our
study on what are good ﬁlters for person re-identiﬁcation.
Our mid-level ﬁlters are discriminatively learned for identifying speciﬁc visual patterns and distinguishing persons,
and have good cross-view invariance. First, local patches
are qualitatively measured and classiﬁed with their discriminative power. Discriminative and representative patches
are collected for ﬁlter learning. Second, patch clusters with
coherent appearance are obtained by pruning hierarchical clustering trees, and a simple but effective cross-view
training strategy is proposed to learn ﬁlters that are viewinvariant and discriminative. Third, ﬁlter responses are integrated with patch matching scores in RankSVM training.
The effectiveness of our approach is validated on the VIPeR
dataset and the CUHK01 dataset. The learned mid-level
features are complementary to existing handcrafted lowlevel features, and improve the best Rank-1 matching rate
on the VIPeR dataset by 14%.
1. Introduction
Person re-identiﬁcation is to match pedestrian images
observed from non-overlapping camera views based on appearance. It receives increasing attentions in video surveillance for its important applications in threat detection, human retrieval, and multi-camera tracking . It saves a
lot of human labor in exhaustively searching for a person
of interest from large amounts of video sequences.
Despite several years of research, person re-identiﬁcation is
still a very challenging task. A person observed in different camera views often undergoes signiﬁcant variations in
viewpoints, poses, and illumination. Background clutters
and occlusions introduce additional difﬁculties. Moreover,
since some persons share similar appearance, it is a big chal-
This work is supported by the General Research Fund sponsored by
the Research Grants Council of Hong Kong (Project No. CUHK 417110,
CUHK 417011, CUHK 429412).
(i) General
(iii) Effective
Occurrence
Image Index
Figure 1. Three types of patches with different discriminative and
generalization powers. Each dashed box indicates a patch cluster.
To make ﬁlters region-speciﬁc, we cluster patches within the same
horizontal stripe across different pedestrian images. See details in
the text of Section 1. Best viewed in color.
lenge to match a query pedestrian image with a large number of candidates from the gallery.
Feature extraction is the most important component for
the success of a person re-identiﬁcation system.
Different from existing approaches of using handcrafted features, which are not optimal for the task of person reidentiﬁcation, we propose to learn mid-level ﬁlters from automatically discovered clusters of patches. A ﬁlter captures
a visual pattern related to a particular body part. The whole
work is motivated by our study on what are good ﬁlters for
person re-identiﬁcation and what are good patch clusters to
train these ﬁlters, and how to quantify these observations
for guiding the learning process.
(1) A good mid-level ﬁlter should reach the balance between discriminative power and generalization ability. As
examples shown in Figure 1, we divide patches from training images into three categories. General patches - Patches
in the green dashed boxes appear frequently in a large portion of pedestrian images. Filters learned from this type of
patches are too general to discriminate pedestrian images.
Rare patches - Patches in the yellow dashed boxes indicate
patterns appearing in very few pedestrian images. Filters
learned from this type of patches have very low generalization ability on new test images. Including the ﬁlters learned
from these two types of patches in person re-identiﬁcation
increases computational cost. More importantly, they serve
as noise channels and deteriorate the identiﬁcation perfor-
Figure 2. Filter in (a1)(b1) is learned from a cluster with incoherent
appearance and generates scattered responses in the two images.
Filter in (a2)(b2) is learned from a cluster with coherent appearance. It generates compact responses. It also has view-invariance.
It matches (a2) and (b2) which are the same person in different
views, while distinguishes (b2) and (b′
2) which are different person
in the same view. Best viewed in color.
mance. The last category is Effective patches. Patches in
the red dashed boxes appear in an appropriate proportion of
pedestrian images. Filters learned from them are representative in describing common properties of a group of persons, and effective in discriminating identities. This study
will show how to quantify the discriminative and generalization powers and how to select effective patches as candidates for ﬁlter learning.
(2) A ﬁlter should be learned from a cluster of patches
with coherent appearance.
However, due to the imperfection of clustering algorithms, some patch clusters have
mixed visual patterns as shown in Figure 2 (a1)(b1). The
ﬁlter learned from this patch cluster cannot accurately locate a speciﬁc visual pattern, and therefore generates scattered ﬁlter responses. In contrast, the ﬁlter learned from
the patch cluster in Figure 2 (a2)(b2) generates compact responses and can accurately locate the target pattern.
(3) Human is a well-structured object with body parts
(e.g. head, and torso). We wish that the patches of a cluster
come from the same body part, such that the learned ﬁlter
can capture the visual pattern of a particular body part.
(4) The learned ﬁlters should be robust to cross-view
variations caused by body articulation, viewpoint and lighting changes. The ﬁlter responses in Figure 2 (a2)(b2) are
view-invariant. This ﬁlter well matches (a2) and (b2) which
are images of the same person in different views, while distinguishes (b2) and (b′
2) which are different persons in the
same view.
Based on above observations,
we propose a new
approach of learning mid-level ﬁlters for person reidentiﬁcation. Its contributions are in the following aspects:
(i) Based on observation (1), partial Area Under Curve
(pAUC) score is proposed to measure the discriminative
power of local patches. Discriminative and representative
patches are collected based on partial AUC quantization.
(ii) Based on observation (2), hierarchical clustering
trees are built to exploit visual patterns from local patches.
Through pruning the trees, we collect coherent cluster
nodes as primitives for ﬁlter learning.
(iii) Based on observation (3), all the patch matching,
patch clustering, and ﬁlter learning are done within the
same horizontal stripe as a spatial constraint. Moreover,
patches are clustered by including their locations as features, such that the clustered patches are spatially proximate
and region-speciﬁc.
(iv) Based on observation (4), a simple but effective
cross-view training strategy is proposed to learn SVM ﬁlters
that are view-invariant and discriminative. Filter responses
are sparsiﬁed to eliminate noise and redundancy.
(v) Finally, matching scores of ﬁlter responses are integrated with patch matching in RankSVM training.
The effectiveness of the learned mid-level ﬁlters is
shown through experiments on the VIPeR and CUHK01
 datasets. It achieves the state-of-the-art performance.
Our learned mid-level ﬁlters well complement to existing
handcrafted low-level features. By combining with LADF
 , it signiﬁcantly enhances the state-of-the-art by 14% on
Rank-1 matching rate on the VIPeR dataset.
2. Related Works
Existing person re-identiﬁcation approaches work on
three aspects: distance learning ,
feature design and selection , and
mid-level feature learning . A review can be
found in .
In distance learning, distance metrics are discriminatively optimized for matching persons. Zheng et al. 
introduced a Probabilistic Relative Distance Comparison
(PRDC) model to maximize likelihood of true matches having a relatively smaller distance than that of a wrong match
pair. Mignon and Jurie proposed Pairwise Constrained
Component Analysis (PCCA) to learn a projection from
high-dimensional input space into a low-dimensional space
where the distance between pairs of data points respects the
desired constraints. It exhibits good generalization properties in presence of high-dimensional data. Liu et al. 
presented a man-in-the-loop method to allow user quickly
reﬁne ranking performance, and achieved signiﬁcant improvement over other metric learning methods. Li et al. 
developed a Locally-Adaptive Decision Function (LADF)
that jointly models a distance metric and a locally adaptive
thresholding rule, and achieved good performance.
In feature design and selection, research works can be
further divided into unsupervised 
and supervised approaches. (i) Unsupervised approaches: Farenzena et al. proposed the Symmetry-
Driven Accumulation of Local Features (SDALF) by exploiting the symmetry property in pedestrian images to handle view variation. Cheng et al. utilized the Pictorial
Structures to estimate human body conﬁguration and also
computed visual features based on different body parts to
cope with pose variations. Liu et al. learned a bottomup feature importance to adaptively weight features of different individuals rather than using global weights. (ii) Supervised approaches: Gray et al. used boosting to select
a subset of optimal features for matching pedestrian images.
Prosser et al. formulated person re-identiﬁcation as a
ranking problem, and learned global feature weights based
on an ensemble of RankSVM.
Some research works on person re-identiﬁcation have
been done to learn reliable and effective mid-level features.
Layne et al. proposed to learn a selection and weighting
of mid-level semantic attributes to describe people. Song
et al. used human attributes to prune a topic model
and matched persons through Bayesian decision. However,
learning human attributes require attribute labels for pedestrian images which cost human labor.
It is much more
costly than labeling matched pedestrian pairs, since each
pedestrian image could have more than 50 attributes. Li et
al. proposed a deep learning framework to learn ﬁlter pairs, which encode photometric transforms across camera views for person re-identiﬁcation. However, it requires
larger scale training data. In this work, we automatically
learn discriminative mid-level features without annotation
of human attributes.
In a wider context, mid-level feature learning has been
exploited in recent works on several vision topics. Singh et
al. and Jain et al. learned mid-level features in scene
classiﬁcation and action recognition by patch clustering and
measuring of the purity and discriminativeness with detection scores. Different from these works, we use hierarchical
clustering and pruning to ﬁnd coherent patch clusters, and
jointly measures the representative and discriminative powers by proposed partial AUC quantization. Moreover, due
to the nature of re-identiﬁcation problem, our mid-level ﬁlter learning targets on cross-view invariance and considers
constraints of body parts through patch matching. This is
the ﬁrst study of importance of mid-level ﬁltering in person re-identiﬁcation. Existing works on mid-level feature
learning did not consider special challenges in person reidentiﬁcation.
3. Patch Classiﬁcation
Our mid-level ﬁlters are learned from local patches selected by their discriminative and representative powers at
different locations. The discriminative power of a patch is
quantiﬁed with its appearing frequency in pedestrian images. This is implemented by patch matching and computing partial Area Under Curve (pAUC) score. In this
section, we introduce how to build dense correspondence of
patches between images in different views, and how to perform partial AUC quantization based on matching results.
3.1. Dense Correspondence
Dense features. Local patches on a dense grid are extracted. The patch is of size 10 × 10 and the grid step is 5
pixels. 32-bin color histogram and 128-dimensional SIFT
features in each of LAB channels are computed for each
patch. To robustly capture the color information, color histograms are also computed on another two downsampled
scales for each patch with downsampling factors 0.5 and
0.75. The color histograms and SIFT features are normalized with L2 norm, and are concatenated to form the ﬁnal
672-dimensional dense local features.
Constrained Patch Matching. Dense local features for
an image are denoted by xA,u = {xA,u
m,n}, and xA,u
m,n represents the features of a local patch at the m-th row and
n-th column in the u-th image from camera view A, where
m = 1, . . . , M, n = 1, . . . , N, u = 1, . . . , U. When patch
m,n searches for its corresponding patch in the v-th image
from camera view B, i.e. xB,v = {xB,v
i,j }, v = 1, . . . , V ,
the constrained search set of xA,u
m,n in xB,v is
m,n, xB,v) = {xB,v
i,j | j = 1, . . . , N,
i = max(0, m −h), . . . , min(M, m + h)},
where h denotes the height of the search space. If all pedestrian images are well aligned and there is no vertical pose
variation, h should be zero. However, misalignment, camera view change, and vertical articulation result in vertical
movement of the human body in the image. Thus it is necessary to relax h to be greater than 0 for handling vertical
movement. We choose h = 2 in our experiment setting.
Patch matching is widely used, and many off-the-shelf
fast algorithms are available for faster speed. In this
work, we simply do a nearest-neighbor search for patch
m,n in its search set S(xA,u
m,n, xB,v).
For each patch
m,n, a nearest neighbor (NN) is sought from its search
set in every image from camera view B to build an NN set
m,n) ={x|x = argmin
i,j ∈S(xA,u
m,n, xB,v), v = 1, . . . , V },
where S(xA,u
m,n, xB,v) is the constrained search set deﬁned
in Eq.(1), and d(xA,u
i,j ) denotes the Euclidean distance
between xA,u
m,n and xB,v
3.2. Partial AUC quantization
Partial AUC Score.
To quantify the discriminative
and generalization power of local patches in discriminating
identities, we propose to compute pAUC score based on the
matching distances obtained in constrained patch matching.
Because the matching distances between a patch xA,u
its closer neighbors in XNN(xA,u
m,n) are more meaningful
to describe the ability of the patch in distinguishing similar
Image Index (after sorting for each patch)
Matching Distance
Figure 3. (a): Each curve represents the sorted distances between a patch and patches in its nearest-neighbor set, and pAUC score is
computed by accumulating patch matching distances with the Np closest nearest neighbors, as the black arrows indicated. Different color
indicates different pAUC level. (b): Example patches are randomly sampled from each pAUC level for illustration, patches in low pAUC
levels are monochromatic and frequently seen, while those in high pAUC levels are varicolored and less frequently appeared. Clearly the
examples show the effectiveness of the pAUC score in quantifying the discriminative power. Best viewed in color.
patches from other images, pAUC score is deﬁned as the
cumulation of distances between the patch xA,u
m,n and its K
nearest neighbors in XNN(xA,u
spAUC(xA,u
dk(XNN(xA,u
where dk denotes the distance between patch xA,u
m,n and its
k-th nearest neighbor in XNN(xA,u
m,n), and Np is the number
of nearest neighbors included in computing pAUC score.
We set Np = 0.3V in our experiments. Small spAUC implies that the patch xA,u
m,n has lots of similar patches from
camera view B, and it is too general to describe a speciﬁc
group of persons. Large spAUC implies that the patch xA,u
is dissimilar with most of the patches from another view,
and it can only describe few persons that have similar appearance. Median spAUC indicates that the patch xA,u
similar with a portion of patches from another view, and it
has the ability to describe the common properties of a group
of persons.
Quantization. To consider the patches in different body
parts separately, we ﬁrstly divide local patches into NY horizontal stripes as follows,
m,n | m = (y −1) ×
+ 1, . . . , y ×
n = 1, . . . , N, u = 1, . . . , U.}, y = 1, . . . , NY ,
y is the patch set in the y-th stripe, and NY is the
number of stripes. Then, we uniformly quantize the patches
within a stripe into NL pAUC levels according to their
pAUC scores as follows,
spAUC(x) ∈[sA
y,min + (l −1)ΔsA
y,min + l ΔsA
y,min = min{spAUC(x) | x ∈SA
max{spAUC(x) | x ∈SA
y,min, and
l = 1, . . . , NL. In our experiments, patches are quantized
into NY = 10 stripes and each stripe are quantized into
NL = 10 pAUC levels, as illustrated in Figure 3.
4. Learning Mid-level Filters
4.1. Hierarchical Patch Clustering
Although local patches are classiﬁed using partial AUC
quantization, patches with different visual information are
still mixed together. Therefore, clustering is performed to
group patches into subsets with coherent visual appearance.
In our task, patch features usually have high dimensions,
and the distributions of data are often in different densities,
sizes, and form manifold structures. Therefore, graph degree linkage (GDL) algorithm is employed for clustering patches since it can well handle these problems. An
ideal ﬁlter should be learned from a cluster of patches with
coherent appearance. However, due to the imperfection of
the clustering algorithm and the difﬁculty of determining
appropriate cluster granularities, some patch clusters have
mixed visual patterns. Filters learned from these patch clusters cannot accurately locate speciﬁc visual patterns. We
propose to build a hierarchical tree by clustering patches
from coarse to ﬁne granularities, and ﬁnd coherent patch
clusters through pruning the tree. Given a set of patches
y,l in Eq.(5), we build a hierarchical clustering tree with
order Ot and maximal depth Dt, i.e., each parent node in
the tree has Ot children and there are Dt layers of nodes.
We set Ot = 4 and Dt = 10 in experiments. The root node
contains all the patches in set SA
y,l, and other nodes in the
tree are patch clusters from coarse to ﬁne granularities. As
shown in Figure 4 (dashed box), shallow nodes (in black
color) are decomposed into deep nodes (in blue color) in
Hierarchical Tree Structure
Figure 4. Illustration of hierarchical clustering tree structure, and examples of cluster nodes. As shown in the dashed box, patches in a
parent node is divided into Ot = 4 children nodes, and shallow nodes (in black color) are decomposed into deep nodes (in blue color)
in hierarchical clustering. The shallow nodes represent coarse clusters while the deep nodes denote ﬁner clusters. Shallow nodes contain
patches with different color and texture patterns while the patch patterns in the deep nodes are more coherent. Best viewed in color.
hierarchical clustering. The shallow nodes represent coarse
clusters while the deep nodes denote ﬁner clusters. To learn
mid-level ﬁlters that can well describe a speciﬁc visual pattern and generate compact ﬁlter responses, we only retain
the deep nodes in the hierarchical clustering tree, i.e. nodes
are picked only when the number of patches in this node
is below a threshold Tmax. In addition, cluster nodes with
very few patches are pruned with threshold Tmin. Examples
of patch clusters are shown in Figure 4. We set Tmax = 300
and Tmin = 10 in experiments. The set of retained cluster
nodes are denoted by {Nodek}Nnode
k=1 , where Nnode is number of cluster nodes.
4.2. Learning Mid-level Filters
Given all the retained cluster nodes, we aim to learn ﬁlters that (1) are robust to appearance and lighting variations
caused by viewpoint change, and (2) have the ability of
distinguishing the most confusing images. We ﬁrstly perform an initial matching based on dense correspondence,
and then learn mid-level ﬁlters in a supervised cross-view
training strategy based on the initial matching.
Initial Matching. Since dense correspondence has been
built for images from two camera views, images can be initially matched based on the patch matching scores,
s0(xA,u, xB,v) = wT
psp(xA,u, xB,v),
p = [wp1, . . . , wpMN ], sT
p = [sp1, . . . , spMN ],
where pi and p′
i are indices of a pair of matched patches
in image xA,u and xB,v respectively, σp is a bandwidth
parameter, and wpi is the weight for similarity score spi.
wp will be automatically learned in an integrated matching model (see Section 4.3), and we initially set wpi =
ˆspAUC(xA,u
pi )ˆspAUC(xB,v
i ), where the similarity scores are
weighted by the consistence in normalized (unit-variance)
pAUC scores. The initial matching model will be used for
cross-view training of mid-level ﬁlters.
Cross-view Training. The learned ﬁlters should be robust to cross-view variations caused by body articulation,
viewpoint and lighting changes, and be discriminative to
identify the same person from different persons. We learn
a ﬁlter for each retained cluster node Nodek in a simple
but effective cross-view training scheme.
All patches in
Nodek are put in a positive set X+
k , and patches from the
other cluster nodes in the same stripe are randomly sampled to form a negative set X−
k . However, these are not
enough to learn a robust and discriminative ﬁlter. To make
sure that the learned ﬁlter robustly produce consistent responses in matched images from both views, for every patch
∈Nodek, its matched patch xB,u
in the matched image is added into an auxiliary positive patch set Xaux+
shown by the red solid arrow in Figure 5. In this way, the
learned ﬁlter can produce high ﬁlter responses in both views
and be robust to cross-view variations. In another aspect,
since initial matching model has some confusions in ﬁnding the true match from a portion of mismatched images,
extra negative patches can be mined from most confusing
mismatched images to avoid high ﬁlter responses on them,
i.e. we sample the matched patches of xA,u
in mismatched
images to build an auxiliary negative set Xaux−
for learning ﬁlters, as shown by the blue dashed arrows in Figure 5.
Since images in top ranks are more confusing, the sampling
is based on a decreasing probability distribution, as shown
in the bottom right of Figure 5.
After construction of positive and negative patch set,
we simply train a linear SVM ﬁlter {wk, bk} using the
train data {X+
} for each cluster node
Nodek. Because the patches in cluster node Nodek belong
to the yk-th stripe, the corresponding SVM ﬁlter is spatially
constrained within the stripe. With a set of SVM ﬁlters
{wk, bk, yk}Nnode
k=1 , ﬁlter responses are computed for each
image by max-pooling of detection scores within the yk-th
stripe. We denote the ﬁltering responses for the u-th image
in view A as f A,u = {f A,u
Normalization and Sparsity.
For each ﬁlter, its responses are ﬁrstly normalized with L2 norm along all images to ensure that it is equally active as other ﬁlters. Then,
for each image, responses of all ﬁlters are normalized with
We denote by ˆf A,u the normalized ﬁlter responses of u-th image in view A.
To suppress noise in
the ﬁlter responses, sparsity is then enforced on ˆf A,u by
∥ˆf A,u∥0 ≤Nsparse. As suggested by the evaluation result in 5.2, we set Nsparse = 0.5Nnode in experiments. The
sparsiﬁed ﬁlter response is denoted by ˆf A,u
. Similarly, ﬁlter
responses ˆf B,v
in camera view B can also be obtained.
4.3. Integrated Matching Scores
We integrate ﬁlter responses ˆf A,u
and ˆf B,v
initial matching scores in Eq.(6) into a uniﬁed matching
sint(xA,u, xB,v) = wTΦ(xA,u, xB,v, ˆf A,u
Φ(xA,u, xB,v)T = [sp(xA,u, xB,v)T, sf(ˆf A,u
) = [sf1, . . . , sfNnode ]T,
where sp(xA,u, xB,v) is patch matching scores deﬁned in
Eq.(6), sfk is the matching score between the k-th ﬁlter responses ˆf A,u
and ˆf B,v
k∗, σf is a bandwidth parameter, and
w is the uniﬁed weighting parameters which are learned by
RankSVM training .
5. Experimental Results
5.1. Datasets and Evaluation Protocol
We evaluate our approach on two public datasets, i.e.
the VIPeR dataset and the CUHK01 dataset . The
VIPeR dataset is the mostly used person re-identiﬁcation
dataset for evaluation, and the CUHK01 dataset contains
more images than VIPeR (3884 vs. 1264 speciﬁcally). Both
Sampling Probability
Figure 5. Scheme of learning view invariant and discriminative ﬁlters. Patches in red boxes are matched patches from images of
the same person, while those in blue boxes are matched patches in
most confusing images. Bottom right is the probability distribution for sampling auxiliary negative samples.
are very challenging datasets for person re-identiﬁcation because they show signiﬁcant variations in viewpoints, poses,
and illuminations, and their images are of low resolutions,
with occlusions and background clutters. All the quantitative results are reported in standard Cumulated Matching
Characteristics (CMC) curves .
VIPeR Dataset1 was captured from two hand-carried
cameras in outdoor academic environment.
placed at many different locations forming different view
pairs. It contains 632 pedestrian pairs, and each pair has
two images of the same person observed from different
camera views.
Most of the image pairs show viewpoint
change larger than 90 degrees. All images are normalized
to 128 × 48 for evaluations.
CUHK01 Dataset2 is also captured with two camera views in a campus environment, and it contains 971
persons, each of which has two images from two camera
views. Camera A captures the frontal view or back view of
pedestrians, while camera B captures the side view. All the
images are normalized to 160 × 60 for evaluations.
5.2. Evaluations and Analysis
Evaluation of Partial AUC Quantization. We investigate the inﬂuence of partial AUC quantization on the rank-1
matching rate in re-identiﬁcation. As shown by the results
in Figure 6(a), median pAUC levels have the highest Rank-
1 performance because patches in these pAUC levels are
representative in describing common properties of a group
of persons and effective in discriminating identities. Low
pAUC levels obtain lower performance than median levels
since patches with low pAUC score appear frequently in
pedestrian images and are too general to discriminate identities. High pAUC levels have the lowest performance because patches in these levels appear very few in pedestrian
images and have low generalization power.
1 
2 
identification.html
Partial AUC Score
Partial AUC Level
Rank−1 Matching Rate
(a) Evaluation of Partial AUC quantization
(b) Evaluation of cross-view training
Rank−1 Accuracy (%)
(c) Evaluation of sparsity
(d) Examples of positive training patches and high-score testing patches of high-weight and low-weight ﬁlters respectively
Figure 6. Evaluation and Analysis on the CUHK01 Dataset. (a): Average pAUC score and rank-1 matching rate in each pAUC level.
Gray bars show the average pAUC scores in 10 pAUC levels, and the blue line indicates the rank-1 matching rates in (%). (b):
Matching rate (%) in top ranks with different training strategies, i.e. S1:(X+, X−), S2:(X+, X−, Xaux+), S3:(X+, X−, Xaux−), and
S4:(X+, X−, Xaux+, Xaux−). (c): Rank-1 Performance of enforcing different sparsity in ﬁlter response. Larger sparsity indicates there
are more zero responses. (d): Example of learned ﬁlters. Each dashed box corresponds to a ﬁlter, and the contents in each dashed box are:
average spatial distribution and examples of positive training patches, and average spatial distribution and examples of high-score testing
patches. Examples in red dashes boxes (ﬁrst row) have high weights while those in blue dashed boxes (second row) have low weights.
Matching Rate (%)
17.40% AIR
18.78% MLA
21.20% ARLTM
29.11% Ours
Approaches on Mid−level Features
(a) VIPeR Dataset
Matching Rate (%)
19.87% SDALF
20.66% eBiCov
26.31% eSDC
15.66% PRDC
16.14% aPRDC
19.27% PCCA
19.60% KISSME
29.34% LADF
30.16% SalMatch
29.11% Ours
43.39% Ours+LADF
(b) VIPeR Dataset
Matching Rate (%)
10.33% L1−norm
9.84% L2−norm
9.90% SDALF
19.67% eSDC
13.45% LMNN
15.98% ITML
20.00% GenericMetric
28.45% SalMatch
34.30% Ours
(c) CUHK01 Dataset
Figure 7. CMC on the VIPeR dataset and the CUHK01 Dataset. Rank-1 matching rate is marked before the name of each approach.
Evaluation of Cross-view Training.
To validate
the effectiveness of cross-view training in Section 4,
we evaluate on the CUHK01 dataset with four controlled settings, i.e. S1:(X+, X−), S2:(X+, X−, Xaux+),
S3:(X+, X−, Xaux−), and S4:(X+, X−, Xaux+, Xaux−),
where X+ (Xaux+) represents ﬁlter learning using (auxilary) positive samples, similarily for X−(Xaux−).
shown in Figure 6(b), S4 has better performance because it
considers both view invariant property and ability in distinguishing confusing images. Thus, we adopt S4 in training.
Evaluation of Sparse Filtering. We also evaluate the
effectiveness of enforcing sparsity in ﬁlter response. As
shown in Figure 6(c), rank-1 performance varies as the sparsity (percentage of zeros in ﬁlter responses) changes, and
the performance is stable within [0, 0.5].
Evaluation of Learned Filters. The learned weighting
parameters w in Eq.(9) indicate the importance of response
for each ﬁlter. As shown in Figure 6(d), each dashed box
corresponds to a ﬁlter, and it contains examples of positive
training patches with their average spatial distribution, and
examples of high-score testing patches with their average
spatial distribution. As seen from the spatial distributions,
high-weight ﬁlters in red dashed boxes (ﬁrst row) have discriminative visual patterns and mostly focus on human body
part, while low-weight ﬁlters in blue dashed box (second
row) either locate in background or have less meaningful
visual patterns.
5.3. Comparison with State-of-the-Arts
Evaluation Protocol. Our experiments on both datasets
follow the evaluation protocol in , i.e. we randomly partition the dataset into two even parts, 50% for training and
50% for testing, without overlap on person identities. Images from camera A are used as probe and those from camera B as gallery. Each probe image is matched with every
image in gallery, and the rank of correct match is obtained.
Rank-k matching rate is the expectation of correct match
at rank k, and the cumulated values of recognition rate at
all ranks is recorded as one-trial CMC result. 10 trials of
evaluation are conducted to obtain stable statistics, and the
expectation is reported.
Result and Analysis. On the VIPeR dataset, we ﬁrstly
compare with approaches on learning mid-level features,
i.e. AIR , MLA , and ARLTM . As shown in
Figure 7(a), our approach signiﬁcantly outperform all other
methods in this category, which validates the effectiveness
of our mid-level ﬁlters. We also compare our approach with
benchmarking methods including SDALF , eBiCov ,
eSDC , PRDC , aPRDC , PCCA , KISSME
 , LF , SalMatch and LADF . As shown in
Figure 7(b), our approach achieves rank-1 accuracy 29.11%
and outperforms almost all the benchmarking methods. By
combining with the best performing LADF under the same
training / testing partitions, it signiﬁcantly enhances the
state-of-the-art by 14% on the rank-1 matching rate. On the
CUHK01 dataset, our approach is compared with L1-norm
distance, L2-norm distance, SDALF , eSDC , LMNN
 , ITML , GenericMetric , and SalMatch .
As the Figure 7(c) shows, our approach clearly outperform
all previous methods on this dataset. One possible reason
of the larger improvement compared with the results on the
VIPeR dataset is that images in the CUHK01 dataset are of
ﬁner resolution, in which ﬁlters are better learned.
6. Conclusion
In this paper, we propose to learn mid-level ﬁlters for person
re-identiﬁcation. We explore different discriminative abilities of
local patches by introducing pAUC score. Discriminative and
representative local patches are collected for learning ﬁlters. Coherent patch clusters are obtained by pruning hierarchical clustering trees, and a simple but effective cross-view training strategy is
propose to learn ﬁlters that are view invariant and discriminative
in distinguishing identities. Furthermore, matching scores of ﬁlter
responses are integrated with patch matching in RankSVM training. Experimental results show the learned mid-level ﬁlters greatly
improves the performance of person re-identiﬁcation.