Proceedings of NAACL-HLT 2015, pages 39–48,
Denver, Colorado, May 31 – June 5, 2015. c⃝2015 Association for Computational Linguistics
Relation Extraction:
Perspective from Convolutional Neural Networks
Thien Huu Nguyen
Computer Science Department
New York University
New York, NY 10003 USA
 
Ralph Grishman
Computer Science Department
New York University
New York, NY 10003 USA
 
Up to now, relation extraction systems have
made extensive use of features generated by
linguistic analysis modules.
Errors in these
features lead to errors of relation detection and
classiﬁcation. In this work, we depart from
these traditional approaches with complicated
feature engineering by introducing a convolutional neural network for relation extraction
that automatically learns features from sentences and minimizes the dependence on external toolkits and resources. Our model takes
advantages of multiple window sizes for ﬁlters and pre-trained word embeddings as an
initializer on a non-static architecture to improve the performance. We emphasize the relation extraction problem with an unbalanced
The experimental results show that
our system signiﬁcantly outperforms not only
the best baseline systems for relation extraction but also the state-of-the-art systems for
relation classiﬁcation.
Introduction
Learning to extract semantic relations between entity pairs from text plays a vital role in information
extraction, knowledge base population and question
answering, to name a few. The relation extraction
(RE) task can be divided into two steps: detecting
if a relation utterance corresponding to some entity
mention pair of interest in the same sentence represents some relation and classifying the detected
relation mentions into some predeﬁned classes. If
we only need to categorize the given relation mentions that are known to express some expected relation (perfect detection), we are left with the relation
classiﬁcation (RC) task. One variation of relation
classiﬁcation is that one might have non-relation examples in the dataset but the number of those is comparable to the number of the other examples. The
non-relation examples, therefore, can be treated as a
usual relation class. Relation extraction, on the other
hand, often comes with a tremendously unbalanced
dataset where the number of the non-relation examples far exceeds the others, making relation extraction more challenging but more practical than relation classiﬁcation. Our present work focuses on the
relation extraction task with an unbalanced corpus.
In the last decade, the relation extraction literature has been dominated by two methods, distinguished by the nature of the relation representation: the feature-based method 
and the kernel-based method . The common characteristic of these methods is the leverage of a large
body of linguistic analysis and knowledge resources
to transform relation mentions into some rich representation to be used by some statistical classiﬁer
such as Support Vector Machines (SVM) or Maximum Entropy (MaxEnt).
The linguistic analysis
pipeline which is hand-designed itself includes tokenization, part of speech tagging, chunking, name
tagging as well as parsing, often performed by existing natural language processing (NLP) modules.
While these methods allow the RE systems to inherit
the knowledge discovered by the NLP community
for the pre-processing tasks, they might be subject
to the error propagation introduced by the imperfect quality of the supervised NLP toolkits. For instance, all the tasks mentioned in the pipeline above
are known to suffer from a performance loss when
they are applied to out-of-domain data ,
causing the collapse of the RE systems based on
them. In this paper, we target an independent RE
system that both avoids complicated feature engineering and minimizes the reliance on the supervised NLP modules for features, potentially alleviating the error propagation and advancing our performance in this area.
To be concrete, our relation extraction system
is provided only with raw sentences marked with
the positions of the two entities of interest1. The
only elements we can derive from this structure
are the words, the n-grams and their positions in
the sentences, suggesting a paradigm in which relation mentions are represented by features that depend on these elements. Eventually, word embeddings that are capable of capturing latent semantic and syntactic properties of words and convolutional neural
networks (CNNs) that are able to recognize speciﬁc
classes of n-gram and induce more abstract representations are a natural
combination one should apply to obtain more effective representations for RE in this setting.
Convolutional neural networks are a type of feed-forward artiﬁcial neural
networks whose layers are formed by a convolution
operation followed by a pooling operation . Recently, with
the emerging interests of the community in deep
learning, CNNs have been revived and effectively
applied in various NLP tasks, including semantic
parsing , search query retrieval
 , sentence modeling and clas-
1For evaluation purpose, we assume the positions of the two
entities of interest in the sentence like most previous studies in
this area (listed above). These are the only external features we
need to achieve an end-to-end relation extractor.
siﬁcation ,
name tagging and semantic role labeling . For relation classiﬁcation and extraction, there are two very recent works on CNNs for
relation classiﬁcation 2 and ; however, to the best of our knowledge,
there has been no work on employing CNNs for relation extraction so far. This paper is the ﬁrst attempt
to ﬁll in that gap and serves as a baseline for future
research in this area.
Our convolutional neural network is built upon
that of Kalchbrenner et al. and Kim 
which are originally proposed for sentence classiﬁcation and modeling. We adapt the network for relation extraction by introducing the position embeddings to encode the relative distances of the words
in the sentence to the two entities of interest. Compared to the models in Liu et al. and Zeng
et al. for relation classiﬁcation that apply a
single window size, our model for relation extraction incorporates various window sizes for convolutional ﬁlters, allowing the network to capture wider
ranges of n-grams to be helpful for relation extraction. In addition, rather than initializing the word
embeddings randomly as do Liu et al. and
ﬁxing the randomly generated position embeddings
during training as do Zeng et al. , we use pretrained word embeddings for initialization and optimize both word embeddings and position embeddings as model parameters. More importantly, rather
than using exterior features (either from human annotation or other pre-processing modules) to enrich
the representation as do Liu et al. and Zeng et
al. , our model (adapted for RC where entity
heads are given) avoids usage of manual linguistic
resources and supervised NLP toolkits constructed
externally, utilizing word embeddings that can be
trained automatically in an unsupervised framework
as the only external resource for the whole system.
We explore different model architectures systematically and demonstrate that the best model performance is achieved when multiple window sizes are
implemented and the word embeddings, once initialized by some “universal” embeddings, are allowed
to vary during the optimization process to reach an
2The title of the paper on relation extraction is misleading since the authors actually do relation classiﬁcation, according to the experimental description.
effective state for relation extraction. We evaluate
our models on both relation classiﬁcation and relation extraction tasks. For relation classiﬁcation,
experiments show that our model (without any external features and resources) outperforms the stateof-the-art models whether the external features are
included in these models or not. For relation extraction, our model is signiﬁcantly better than the baseline models that use the words and the embeddings
themselves as the features. In the following, we discuss related work in Section 2 and present our model
in Section 3. We detail an extensive evaluation in
Section 4 and ﬁnally conclude in Section 5.
Related Work
As our present work focuses on the supervised
framework for relation extraction, we concentrate on
the supervised systems in this section. Besides the
supervised systems (either feature-based or kernelbased) mentioned above, some recent systems have
employed the distant supervision (DS) approach for
relation extraction. This approach is essentially similar to the traditional systems in representing relation
mentions but attempts to generate training data automatically by leveraging large knowledge bases of
facts and corpus .
Regarding neural networks, their ﬁrst application
to NLP is language modeling which has been useful to learn distributed representations (embeddings)
for words .
These word embeddings have opened a new direction for many other NLP tasks grounded on neural networks. Some of them are mentioned above.
Other than that, a class of recursive neural networks
(RNNs) and neural tensor networks are proposed for
paraphrase detection , parsing
 , sentiment analysis , knowledge base completion , question answering 
etc. Among these RNN systems, the study that is
most related to our relation extraction problem is
Socher et al. that learns compositional vector
representations for phrases and sentences through
syntactic parse trees and applies these representations for relation classiﬁcation.
However, this
method inherently requires syntactic parse trees in
contrast to our target of avoiding use of any external
features and resources for RC.
Convolutional Neural Network for
Relation Extraction
Our convolutional neural network for relation extraction consists of four main layers: (i) the look-up
tables to encode words in sentences by real-valued
vectors, (ii) the convolutional layer to recognize ngrams, (iii) the pooling layer to determine the most
relevant features and (iv) a logistic regression layer
(a fully connected neural network with a softmax at
the end) to perform classiﬁcation . Figure 1 gives an overview of the network.
Word Representation
The input to the CNN for relation extraction consists
of sentences marked with the two entity mentions of
interest. As CNNs can only work with ﬁxed length
inputs, we compute the maximal separation between
entity mentions linked by a relation and choose an
input width greater than this distance. We insure
that every input (relation mention) has this length
by trimming longer sentences and padding shorter
sentences with a special token.
Let n be the length of the relation mentions and
[x1, x2, . . . , xn] be some relation mention
where xi is the i-th word in the mention. Also, let
xi1 and xi2 be the two heads of the two entity mentions of interest . Before entering the network, each
word xi is ﬁrst transformed into a vector ei by looking up the word embedding table W that can be initialized either by a random process or by some pretrained word embeddings. Besides, in order to embed the positions of the two entity heads as well as
the other words in the relation mention into the representation, for each word xi, its relative distances to
the two entity heads i−i1 and i−i2 are also mapped
into real-value vectors di1 and di2 respectively using
a position embedding table D (initialized randomly)
 . Note that the relative distances only range
from −n + 1 to n −1 so the position embedding
matrix D has size (2n −1) × md (md is a hyperparameter indicating the dimensionality of the position
embedding vectors). Finally, the word embeddings
In the morning, the <e1>President</e1> traveled to <e2>Detroit</e2>
input sentence with marked entities
word embedding matrix
position embeddings matrix
table look-up
Convolutional layer
with multiple window sizes
for filters
Max pooling
Fully connected layer
with dropout and
softmax output
Look-up tables
Figure 1: Convolutional Neural Network for Relation Extraction.
ei and the position embeddings d1 and d2 are concatenated into a single vector xi = [ei, di1, di2]⊤to
represent the word xi. As a result, the original sentence x can now be viewed as a matrix x of size
(me + 2md) × n where me is the dimensionality of
the word embedding vectors.
x = [x1, x2, . . . , xn]
Convolution
In the next step, the matrix x representing the input relation mention is fed into the convolutional
layer to extract higher level features.
widow size w, a ﬁlter is seen as a weight matrix
f = [f1, f2, . . . , fw] (fi is a column vector of size
me + 2md). The core of this layer is obtained from
the application of the convolutional operator on the
two matrices x and f to produce a score sequence
s = [s1, s2, . . . , sn−w+1]:
where b is a bias term and g is some non-linear
function. This process can then be replicated for various ﬁlters with different window sizes to increase
the n-gram coverage of the model.
For relation extraction, we call the n-grams accompanied with relative positions of its words the
augmented n-grams. It is instructive to think about
the ﬁlter f as representing some hidden class of the
augmented n-grams and the scores si as measuring
the possibility the augmented n-gram at position i
belongs to the corresponding hidden class (although
these scores are not probabilities at all). The trained
weights of the ﬁlter f would then amount to a feature
detector that learns to recognize the hidden class of
the augmented n-grams .
The rationale of the pooling layer is to further abstract the features generated from the convolutional
layer by aggregating the scores for each ﬁlter to introduce the invariance to the absolute positions but
preserve the relative positions of the n-grams between themselves and the entity heads at the same
time. The popular aggregating function is max as
it bears responsibility for identifying the most important or relevant features from the score sequence.
Concretely, for each ﬁlter f, its score sequence s is
passed through the max function to produce a single
number: pf = max{s} = max{s1, s2, . . . sn−w+1}
which can be interpreted as estimating the possibility some augmented n-gram of the hidden class of f
appears in the context.
Regularization and Classiﬁcation
In the ﬁnal step, the pooling scores for every ﬁlter
are concatenated into a single feature vector z =
[p1, p2, . . . , pm] to represent the relation mention.
Here, m is the number of ﬁlters in the model and
pi is the pooling score of the i-th ﬁlter. Before actually applying this feature vector, following , we execute a dropout for
regularization by randomly setting to zero a proportion ρ of the elements of the feature vector3 z to produce the vector zd. The dropout vector zd is then
fed into a fully connected layer of standard neural
networks followed by a softmax layer in the end to
perform classiﬁcation. The fully connected layer induces a weight matrix C as model parameters. At
test time, the unseen relation mentions are scored
using the feature vectors that are not dropped out.
We also rescale the weights whose l2-norms exceed
a hyperparameter as Kim .
Overall, the parameters for the presented CNN
the word embedding matrix W, the position embedding matrix D, the m ﬁlter matrices,
the weight matrix C for the fully connected layer.
The gradients are computed using back-propagation
while training is done via stochastic gradient descent
with shufﬂed mini-batches and the AdaDelta update
rule .
Experiments
Hyperparameters and Resources
For all the experiments below, we use: tanh for
the non-linear function, 150 ﬁlters for each window
size in the model and position embedding vectors
with dimensionality of md = 504. Regarding the
other parameters, we use the same values as do Kim
 , i.e, the dropout rate ρ = 0.5, the mini-batch
size of 50, the hyperparameter for the l2 of 3.
Finally, we utilize the pre-trained word embeddings word2vec from Mikolov et al. which
have dimensionality of me = 300 and are trained on
100 billion words of Google News using the continuous bag-of-words architecture. These embeddings
are publicly available here5. Vectors for the words
not included in the pre-trained embeddings are initialized randomly.
Besides the word embeddings
word2vec, the model does not use any other NLP
toolkits or resources.
We evaluate our models on two datasets:
SemEval-2010 Task 8 dataset for relation classiﬁcation and the ACE 2005
3Following the Bernoulli distribution
4These values produce the best performance during our experimental process.
5 
dataset for relation extraction.
ACE 2005 (87,512)
SemEval 2010 (10,717)
Cause-Effect
Component-Whole
Entity-Destination
PART-WHOLE
Entity-Origin
Product-Producer
Member-Collection
Message-Topic
Content-Container
Instrument-Agency
Table 1: ACE 2005 and SemEval 2010 Relation Class
Distributions
The SemEval dataset can be downloaded here6
and contains 10,717 annotated examples, including
8,000 examples for training and 2,717 examples for
testing. Each example is a sentence annotated for
a pair of entities of interest and the corresponding
relation class for this entity pair.
There are 9
ordered relationships (with two directions) and
an undirected Other class, resulting in 19 classes.
A pair is counted as correct if the order of the
entities in the relationship is correct. For the ACE
2005 dataset, documents are annotated for 6 major
relation classes and 7 entity types.
In order to
generate the non-relation examples or the examples
for the Other class, we collect every pair of entity
mentions within a single sentence and not included
in the annotated relation set. To reduce the noise,
we truncate the generated dataset by removing all
the examples whose distances between the two
entity heads are greater than 15. This results in a
considerably unbalanced dataset of 8,365 positive
examples of the 6 annotated relation classes and
79,147 negative examples of the class Other. The
distributions of the relation classes on the two
datasets are shown in Table 1.
As we can see,
the ACE dataset is much more biased toward the
Other class than the SemEval dataset and thus more
appropriate for relation extraction experiments.
Evaluation of Model Architectures
We investigate the effectiveness of different window sizes of ﬁlters by running the proposed CNN
6 36c28v9pmw
nonstatic.rand
static.word2vec
nonstatic.word2vec
window sizes
Table 2: System Performance on various window size combinations and architectures
model on window sizes of 2, 3, 4 and 5. To understand the behavior of the model on multiple window sizes, we further test it on the following window size combinations: (4,5), (3,4,5) and (2,3,4,5).
In each of these window size conﬁgurations, we
evaluate the system on three different scenarios:
(i) the word embeddings and the position embeddings are randomly initialized and optimized during the training process (denoted by nonstatic.rand),
(ii) the word embeddings are initialized by the pretrained word embeddings; the position embeddings
are initialized randomly and the two embeddings
are kept unchanged during the training (denoted by
static.word2vec), (iii) the two embeddings are initialized as in case (ii) but they are optimized as
model parameters when the model is trained (denoted by nonstatic.word2vec). These experiments
are carried out for relation extraction on the ACE
2005 dataset via 5-fold cross validation.
presents the system performance on Precision (P),
Recall (R) and F1 score (F).
The key observations from the table are7:
(i) From rows 1, 2, 3, 4, we see that evaluating
window sizes individually is quite intricate. It is unclear which window size is the best size for CNNs
on relation extraction.
For instance, on the nonstatic.rand mode, the window size 4 seems to outperform the others while on the other modes, the
window sizes 3 and 5 turn out to be better. Besides,
the performance gaps between the window sizes are
small, making it hard to draw a conclusive judgement. In any case, the window size 2 seems to be
the worst, suggesting that the 2-grams might be less
informative than the others on representing relation
mentions for CNNs on this dataset.
7The statements at points (ii) and (iii) are signiﬁcant at con-
ﬁdence levels ≥95%.
(ii) While the results on evaluating single window
sizes are hard to analyze, the results for multiple
window sizes are quite clear and conclusive. Moving from single window sizes of 2, 3, 4 or 5 (rows 1,
2, 3 and 4 respectively) to the conﬁguration with two
window sizes 4 and 5 (row 5) gives us consistent improvements on all the model architectures. The performance is then consistently enhanced when more
window sizes are included, resulting in the best performance when all the window sizes 2, 3, 4 and 5 are
employed. This demonstrates the advantages of the
models with multiple window sizes over the single
window size models in Liu et al. and Zeng et
al. .
(iii) Regarding different model architectures, the
picture is even clearer. No matter which window
size conﬁguration is applied, we constantly see the
nonstatic.word2vec architecture performs most effectively, followed by the static.word2vect setting
which is in turn followed by the nonstatic.rand
This suggests the undeniable beneﬁts of
initializing the word embeddings by some “universal” pre-trained values and updating the embeddings
to reﬂex RE speciﬁc embeddings when training the
models .
the next experiments, we always use all the window
sizes 2, 3, 4 and 5 with the nonstatic.word2vec architecture.
Relation Extraction Experiment
We compare our system with the traditional featurebased relation extraction systems when these system
are only allowed to use the same information and resources as our systems, i.e, the words in the relation
mentions, the positions of the two entity heads and
the word embeddings. Given the sentences and the
positions of the two entity heads, the features that
the state-of-the-art feature-based systems extract in-
clude: the heads of the two entity mentions; the
words in the context before mention 1; after mention 2 and between two mentions; the bigrams, the
word sequences between two entities, the order of
two mentions, the number of words between two
mentions . The feature-based system using
this feature set is called Words.
Armed with the
word embeddings, one can further introduce these
embeddings into the head words or the words in the
context as additional features . We call the system Words augmented
with the embeddings for the two heads Words-HM-
Wed and Words augmented with the embeddings for
words in the contexts Words-WC-Wed.
the MaxEnt framework with L2 regularization in the
Mallet toolkit8 to train these feature-based models
 ). Table 3 shows the performance of the three baseline systems and our proposed CNN via 5-fold cross validation on the ACE
2005 dataset.
Words-WC-Wed
Words-HM-Wed
Table 3: Performance of Relation Extraction Systems
The ﬁrst observation is that adding the word embeddings to the words in the context hurt the performance of the feature-based systems while augmenting the heads of the entities with word embeddings signiﬁcantly improves the feature-based systems. This is consistent with the results reported
by Nguyen and Grishman and demonstrates
that the ability to wisely pick the words for embeddings and avoid embeddings on speciﬁc locations
is crucial to the feature-based systems. More importantly, our proposed CNN signiﬁcantly outperforms all the baseline models at the conﬁdence levels ≥95%, an improvement of 4.96% over the best
feature-based system Words-HM-Wed . This result indicates that CNNs
are a better way to employ word embeddings for relation extraction.
8 
Remember that although the traditional systems
can achieve a performance greater than 72% on the
ACE dataset ,
they come at the expense of elaborate feature engineering as well as much more expensive feature
extraction.
In particular, the feature extractors of
these feature-based systems require: (i) the perfect
entity and mention type information hand-labeled
laboriously by human annotators; (ii) the extensive
usage of the existing supervised NLP toolkits and
resources (constituent and dependency parsers, dictionaries, gazetteers etc) which might be unavailable for various domains in reality.
The absence
of the perfect (hand-annotated) entity and mention
type information (i.e point (i) above) greatly impairs
these feature-based systems’ performance. For instance, both Plank and Moschitti and Nguyen
and Grishman report a performance less than
60% on the ACE 2005 dataset when the perfect
entity type and mention type features are not employed although the other features with extensive
feature engineering (i.e point (ii) above) are still
As a result, in a more realistic setting
where hand-annotated features are prohibitive, the
proposed CNN requires much less feature engineering and resources but still performs better than the
traditional feature-based systems.
Relation Classiﬁcation Experiment
In order to further verify the effectiveness of the system, we test the system on the relation classiﬁcation task with the SemEval 2010 dataset and compare the results with the state-of-the-art systems in
this area. Table 4 describes the performance of various traditional systems that are based on classiﬁers
such as MaxEnt and SVM with series of supervised
and manual features9 as well
as the more recent systems based on convolutional
neural networks (O-CNN), recursive neural networks (RNN), matrix-vector recursive neural networks (MVRNN) or log-quadratic factor-based compositional
embedding model (FCM) 10.
As we can see, among the systems not using any
9i.e the features extracted from supervised pre-processing
NLP modules and manual resources
10These are the macro-averaged F1-scores, computed by the
ofﬁcially provided scorer.
Feature Sets
POS, WordNet, morphological
features, thesauri, Google ngrams
POS, WordNet, morphological
features, noun compound system, thesauri, Google n-grams
POS, WordNet,
preﬁxes and
morphological
dependency
NomLex-Plus,
Google n-grams, paraphrases,
TextRunner
POS, name tagging, WordNet
POS, name tagging, WordNet
dependency parse, name tagging
Table 4: Performance of Relation Classiﬁcation Systems
supervised and manual features (i.e, POS, WordNet,
name tagging, dependency parse, patterns etc), our
system signiﬁcantly outperforms the state-of-the-art
system FCM (80.6%) with an improvement of 2.2%. More interestingly, even without supervised and manual features, our system can
still work comparably to the other systems utilizing
these features as the vital components. For instance,
the supervised features (dependency parse and name
tagging) are crucial to FCM to signiﬁcantly improve its performance. We attribute our
performance advantage over the closely-related system O-CNN to the multiple window sizes, the optimization of the position embeddings during training and possibly the superiority of
the embeddings word2vec we use.
Impact of Unbalanced Dataset
Shifting from relation classiﬁcation to relation extraction with an unbalanced corpus, we witness a
large performance gap as described above. In this
section, we study the impact of the unbalanced corpus on the performance of relation extractors for
both convolutional neural networks and traditional
feature-based approaches (Words and Words-HM-
1 + log10( #positive
#negative)
Words-HM-Wed
Figure 2: F measures vs positive/negative ratios
Wed). In particular, we vary the ratio of positive (true
relations) and negative (the class Other) examples in
the ACE 2005 dataset and see how the system performance responds to this variation. Figure 2 shows
the curves. This is a 5-fold cross validation experiment and all the comparisons are signiﬁcant at con-
ﬁdence levels ≥95%.
From the ﬁgure, we see that all the models improve constantly with the increase of the ratio of the
positive and negative examples. The performance
peaks with an improvement of about 20% for all
models when the number of examples of the class
Other is small relative to the others. In other words,
the systems attain their best performance when relation extraction is reduced to the relation classiﬁcation problem, suggesting that relation extraction is
much more challenging than relation classiﬁcation.
Finally, for all the ratio values, we consistently see
that the convolutional neural network is superior to
the others, once again conﬁrming its advantages.
Conclusion
We present a CNN for relation extraction that emphasizes an unbalanced corpus and minimizes usage of external supervised NLP toolkits for features.
The network uses multiple window sizes for ﬁlters,
position embeddings for encoding relative distances
and pre-trained word embeddings for initialization
in a non-static architecture. The experimental results
demonstrate the effectiveness of the proposed CNN
on both RC and RE. Our future work includes: (i)
to enrich the representation of CNNs with more features for RE, (ii) to study the applications of CNNs
on other related tasks, and (iii) to examine other neural network models for RE.