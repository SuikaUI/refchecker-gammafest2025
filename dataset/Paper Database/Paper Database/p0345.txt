Residual Attention Network for Image Classiﬁcation
Fei Wang1, Mengqing Jiang2, Chen Qian1, Shuo Yang3, Cheng Li1,
Honggang Zhang4, Xiaogang Wang3, Xiaoou Tang3
1SenseTime Group Limited, 2Tsinghua University,
3The Chinese University of Hong Kong, 4Beijing University of Posts and Telecommunications
1{wangfei, qianchen, chengli}@sensetime.com,
 
3{ys014, xtang}@ie.cuhk.edu.hk, ,
 
In this work, we propose “Residual Attention Network”,
a convolutional neural network using attention mechanism
which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion. Our
Residual Attention Network is built by stacking Attention
Modules which generate attention-aware features.
attention-aware features from different modules change
adaptively as layers going deeper. Inside each Attention
Module, bottom-up top-down feedforward structure is used
to unfold the feedforward and feedback attention process
into a single feedforward process. Importantly, we propose
attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds
of layers.
Extensive analyses are conducted on CIFAR-10 and
CIFAR-100 datasets to verify the effectiveness of every module mentioned above.
Our Residual Attention Network
achieves state-of-the-art object recognition performance on
three benchmark datasets including CIFAR-10 (3.90% error), CIFAR-100 (20.45% error) and ImageNet (4.8% single
model and single crop, top-5 error). Note that, our method
achieves 0.6% top-1 accuracy improvement with 46% trunk
depth and 69% forward FLOPs comparing to ResNet-200.
The experiment also demonstrates that our network is robust against noisy labels.
1. Introduction
Not only a friendly face but also red color will draw our
attention. The mixed nature of attention has been studied
extensively in the previous literatures . Attention not only serves to select a focused location but also
enhances different representations of objects at that location. Previous works formulate attention drift as a sequential process to capture different attended aspects. However,
as far as we know, no attention mechanism has been applied
to feedforward network structure to achieve state-of-art results in image classiﬁcation task. Recent advances of image
classiﬁcation focus on training feedforward convolutional
neural networks using “very deep” structure .
Inspired by the attention mechanism and recent advances
in the deep neural network, we propose Residual Attention
Network, a convolutional network that adopts mixed attention mechanism in “very deep” structure. The Residual Attention Network is composed of multiple Attention Modules which generate attention-aware features. The attentionaware features from different modules change adaptively as
layers going deeper.
Apart from more discriminative feature representation
brought by the attention mechanism, our model also exhibits following appealing properties:
(1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. Fig.1 shows an example of different types
of attentions for a hot air balloon image. The sky attention
mask diminishes background responses while the balloon
instance mask highlighting the bottom part of the balloon.
(2) It is able to incorporate with state-of-the-art deep network structures in an end-to-end training fashion. Specifically, the depth of our network can be easily extended to
hundreds of layers. Our Residual Attention Network outperforms state-of-the-art residual networks on CIFAR-10,
CIFAR-100 and challenging ImageNet image classiﬁcation dataset with signiﬁcant reduction of computation (69%
forward FLOPs).
All of the aforementioned properties, which are challenging to achieve with previous approaches, are made possible with following contributions:
(1) Stacked network structure: Our Residual Attention Network is constructed by stacking multiple Attention Modules. The stacked structure is the basic application of mixed
attention mechanism. Thus, different types of attention are
able to be captured in different Attention Modules.
 
Origin image
before mask
Soft attention
after mask
before mask
after mask
Low-level color feature
High-level part feature
Balloon instance mask
Classification
Attention mechanism
Soft attention
Figure 1: Left: an example shows the interaction between features and attention masks. Right: example images illustrating
that different features have different corresponding attention masks in our network. The sky mask diminishes low-level
background blue color features. The balloon instance mask highlights high-level balloon bottom part features.
(2) Attention Residual Learning: Stacking Attention Modules directly would lead to the obvious performance drop.
Therefore, we propose attention residual learning mechanism to optimize very deep Residual Attention Network
with hundreds of layers.
(3) Bottom-up top-down feedforward attention: Bottom-up
top-down feedforward structure has been successfully applied to human pose estimation and image segmentation . We use such structure as part of Attention
Module to add soft weights on features. This structure can
mimic bottom-up fast feedforward process and top-down
attention feedback in a single feedforward process which
allows us to develop an end-to-end trainable network with
top-down attention. The bottom-up top-down structure in
our work differs from stacked hourglass network in its
intention of guiding feature learning.
2. Related Work
Evidence from human perception process shows the
importance of attention mechanism, which uses top information to guide bottom-up feedforward process. Recently,
tentative efforts have been made towards applying attention into deep neural network. Deep Boltzmann Machine
(DBM) contains top-down attention by its reconstruction process in the training stage.
Attention mechanism
has also been widely applied to recurrent neural networks
(RNN) and long short term memory (LSTM) to tackle
sequential decision tasks . Top information
is gathered sequentially and decides where to attend for the
next feature learning steps.
Residual learning is proposed to learn residual of
identity mapping.
This technique greatly increases the
depth of feedforward neuron network. Similar to our work,
 use residual learning with attention mechanism to beneﬁt from residual learning. Two information
sources (query and query context) are captured using attention mechanism to assist each other in their work. While in
our work, a single information source (image) is split into
two different ones and combined repeatedly. And residual
learning is applied to alleviate the problem brought by repeated splitting and combining.
In image classiﬁcation, top-down attention mechanism
has been applied using different methods: sequential process, region proposal and control gates.
Sequential process models image classiﬁcation as a sequential decision. Thus attention can be applied similarly
with above. This formulation allows end-to-end optimization using RNN and LSTM and can capture different kinds
of attention in a goal-driven way.
Region proposal has been successfully
adopted in image detection task. In image classiﬁcation,
an additional region proposal stage is added before feedforward classiﬁcation. The proposed regions contain top
information and are used for feature learning in the second stage. Unlike image detection whose region proposals rely on large amount of supervision, e.g. the ground
truth bounding boxes or detailed segmentation masks ,
unsupervised learning is usually used to generate region proposals for image classiﬁcation.
Control gates have been extensively used in LSTM. In
image classiﬁcation with attention, control gates for neurones are updated with top information and have inﬂuence
on the feedforward process during training . However, a new process, reinforcement learning or optimization is involved during the training step. Highway
Network extends control gate to solve gradient degradation problem for deep convolutional neural network.
However, recent advances of image classiﬁcation focus
on training feedforward convolutional neural networks using “very deep” structure . The feedforward
convolutional network mimics the bottom-up paths of human cortex.
Various approaches have been proposed to
further improve the discriminative ability of deep convolutional neural network. VGG , Inception and residual learning are proposed to train very deep neural
networks. Stochastic depth , Batch Normalization 
and Dropout exploit regularization for convergence and
avoiding overﬁtting and degradation.
Soft attention developed in recent work can be
trained end-to-end for convolutional network. Our Residual Attention Network incorporates the soft attention in
fast developing feedforward network structure in an innovative way. Recent proposed spatial transformer module 
achieves state-of-the-art results on house number recognition task. A deep network module capturing top information is used to generate afﬁne transformation. The afﬁne
transformation is applied to the input image to get attended
region and then feed to another deep network module. The
whole process can be trained end-to-end by using differentiable network layer which performs spatial transformation.
Attention to scale uses soft attention as a scale selection
mechanism and gets state-of-the-art results in image segmentation task.
The design of soft attention structure in our Residual Attention Network is inspired by recent development of localization oriented task, i.e. segmentation and human pose estimation . These tasks motivate researchers
to explore structure with ﬁned-grained feature maps. The
frameworks tend to cascade a bottom-up and a top-down
structure. The bottom-up feedforward structure produces
low resolution feature maps with strong semantic information. After that, a top-down network produces dense features to inference on each pixel. Skip connection is employed between bottom and top feature maps and achieved
state-of-the-art result on image segmentation. The recent
stacked hourglass network fuses information from multiple scales to predict human pose, and beneﬁts from encoding both global and local information.
3. Residual Attention Network
Our Residual Attention Network is constructed by stacking multiple Attention Modules.
Each Attention Module is divided into two branches: mask branch and trunk
branch. The trunk branch performs feature processing and
can be adapted to any state-of-the-art network structures.
In this work, we use pre-activation Residual Unit ,
ResNeXt and Inception as our Residual Attention
Networks basic unit to construct Attention Module. Given
trunk branch output T(x) with input x, the mask branch
uses bottom-up top-down structure to learn
same size mask M(x) that softly weight output features
T(x). The bottom-up top-down structure mimics the fast
feedforward and feedback attention process.
The output
mask is used as control gates for neurons of trunk branch
similar to Highway Network . The output of Attention
Module H is:
Hi,c(x) = Mi,c(x) ∗Ti,c(x)
where i ranges over all spatial positions and c ∈{1, ..., C}
is the index of the channel. The whole structure can be
trained end-to-end.
In Attention Modules, the attention mask can not only
serve as a feature selector during forward inference, but also
as a gradient update ﬁlter during back propagation. In the
soft mask branch, the gradient of mask for input feature is:
∂M(x, θ)T(x, φ)
= M(x, θ)∂T(x, φ)
where the θ are the mask branch parameters and the φ are
the trunk branch parameters. This property makes Attention
Modules robust to noisy labels. Mask branches can prevent
wrong gradients (from noisy labels) to update trunk parameters. Experiment in Sec.4.1 shows the robustness of our
Residual Attention Network against noisy labels.
Instead of stacking Attention Modules in our design, a
simple approach would be using a single network branch
to generate soft weight mask, similar to spatial transformer
layer . However, these methods have several drawbacks
on challenging datasets such as ImageNet. First, images
with clutter background, complex scenes, and large appearance variations need to be modeled by different types of
attentions. In this case, features from different layers need
to be modeled by different attention masks. Using a single
mask branch would require exponential number of channels
to capture all combinations of different factors. Second, a
single Attention Module only modify the features once. If
the modiﬁcation fails on some parts of the image, the following network modules do not get a second chance.
The Residual Attention Network alleviates above problems. In Attention Module, each trunk branch has its own
mask branch to learn attention that is specialized for its features. As shown in Fig.1, in hot air balloon images, blue
color features from bottom layer have corresponding sky
mask to eliminate background, while part features from top
layer are reﬁned by balloon instance mask. Besides, the incremental nature of stacked network structure can gradually
reﬁne attention for complex images.
max pooling
residual unit
max pooling
interpolation
residual unit
interpolation
residual unit
residual unit
residual unit
residual unit
residual unit
residual unit
residual unit
Attention Module
Attention Module
Attention Module
Soft Mask Branch
down sample
residual unit
sigmoid function
element-wise
element-wise
convolution
Figure 2: Example architecture of the proposed network for ImageNet. We use three hyper-parameters for the design of
Attention Module: p, t and r. The hyper-parameter p denotes the number of pre-processing Residual Units before splitting
into trunk branch and mask branch. t denotes the number of Residual Units in trunk branch. r denotes the number of Residual
Units between adjacent pooling layer in the mask branch. In our experiments, we use the following hyper-parameters setting:
{p = 1, t = 2, r = 1}. The number of channels in the soft mask Residual Unit and corresponding trunk branches is the
3.1. Attention Residual Learning
However, naive stacking Attention Modules leads to the
obvious performance drop. First, dot production with mask
range from zero to one repeatedly will degrade the value of
features in deep layers. Second, soft mask can potentially
break good property of trunk branch, for example, the identical mapping of Residual Unit.
We propose attention residual learning to ease the above
problems. Similar to ideas in residual learning, if soft mask
unit can be constructed as identical mapping, the performances should be no worse than its counterpart without attention. Thus we modify output H of Attention Module as
Hi,c(x) = (1 + Mi,c(x)) ∗Fi,c(x)
M(x) ranges from , with M(x) approximating 0,
H(x) will approximate original features F(x). We call this
method attention residual learning.
Our stacked attention residual learning is different from
residual learning. In the origin ResNet, residual learning is
formulated as Hi,c(x) = x+Fi,c(x), where Fi,c(x) approximates the residual function. In our formulation, Fi,c(x)
indicates the features generated by deep convolutional networks. The key lies on our mask branches M(x). They
work as feature selectors which enhance good features and
suppress noises from trunk features.
In addition, stacking Attention Modules backs up attention residual learning by its incremental nature. Attention
residual learning can keep good properties of original features, but also gives them the ability to bypass soft mask
branch and forward to top layers to weaken mask branch’s
feature selection ability. Stacked Attention Modules can
gradually reﬁne the feature maps. As show in Fig.1, features become much clearer as depth going deeper. By using
attention residual learning, increasing depth of the proposed
Residual Attention Network can improve performance consistently. As shown in the experiment section, the depth of
Residual Attention Network is increased up to 452 whose
performance surpasses ResNet-1001 by a large margin on
CIFAR dataset.
3.2. Soft Mask Branch
DBN , our mask branch contains fast feed-forward
sweep and top-down feedback steps. The former operation
quickly collects global information of the whole image, the
latter operation combines global information with original
feature maps.
In convolutional neural network, the two
steps unfold into bottom-up top-down fully convolutional
structure.
From input, max pooling are performed several times to
increase the receptive ﬁeld rapidly after a small number of
Residual Units. After reaching the lowest resolution, the
global information is then expanded by a symmetrical topdown architecture to guide input features in each position.
Linear interpolation up sample the output after some Residual Units. The number of bilinear interpolation is the same
as max pooling to keep the output size the same as the input
feature map. Then a sigmoid layer normalizes the output
down sample
down sample
convolution
receptive field
Soft Mask Branch
Trunk Branch
Figure 3: The receptive ﬁeld comparison between mask
branch and trunk branch.
range to after two consecutive 1 × 1 convolution layers. We also added skip connections between bottom-up
and top-down parts to capture information from different
scales. The full module is illustrated in Fig.2.
The bottom-up top-down structure has been applied to
image segmentation and human pose estimation. However,
the difference between our structure and the previous one
lies in its intention. Our mask branch aims at improving
trunk branch features rather than solving a complex problem directly. Experiment in Sec.4.1 is conducted to verify
above arguments.
3.3. Spatial Attention and Channel Attention
In our work, attention provided by mask branch changes
adaptably with trunk branch features. However, constrains
to attention can still be added to mask branch by changing
normalization step in activation function before soft mask
output. We use three types of activation functions corresponding to mixed attention, channel attention and spatial
attention. Mixed attention f1 without additional restriction
use simple sigmoid for each channel and spatial position.
Channel attention f2 performs L2 normalization within all
channels for each spatial position to remove spatial information. Spatial attention f3 performs normalization within
feature map from each channel and then sigmoid to get soft
mask related to spatial information only.
f1(xi,c) =
1 + exp(−xi,c)
f2(xi,c) = xi,c
f3(xi,c) =
1 + exp(−(xi,c −meanc)/stdc)
Where i ranges over all spatial positions and c ranges over
all channels. meanc and stdc denotes the mean value and
standard deviation of feature map from c-th channel. xi
denotes the feature vector at the ith spatial position.
Activation Function
Attention Type
Top-1 err. (%)
Mixed Attention
Channel Attention
Spatial Attention
Table 1: The test error (%) on CIFAR-10 of Attention-56
network with different activation functions.
Output Size
Attention-56
Attention-92
7 × 7, 64, stride 2
Max pooling
3 × 3 stride 2
Residual Unit
1 × 1, 256
Attention Module
Attention ×1
Attention ×1
Residual Unit
1 × 1, 128
3 × 3, 128
1 × 1, 512
Attention Module
Attention ×1
Attention ×2
Residual Unit
1 × 1, 256
3 × 3, 256
1 × 1, 1024
Attention Module
Attention ×1
Attention ×3
Residual Unit
1 × 1, 512
3 × 3, 512
1 × 1, 2048
Average pooling
7 × 7 stride 1
FC,Softmax
params×106
Trunk depth
Table 2: Residual Attention Network architecture details
for ImageNet. Attention structure is described in Fig. 2.
We make the size of the smallest output map in each mask
branch 7×7 to be consistent with the smallest trunk output
map size. Thus 3,2,1 max-pooling layers are used in mask
branch with input size 56×56, 28×28, 14×14 respectively.
The Attention Module is built by pre-activation Residual
Unit with the number of channels in each stage is the
same as ResNet .
The experiment results are shown in Table 1, the mixed
attention has the best performance. Previous works normally focus on only one type of attention, for example scale
attention or spatial attention , which puts additional
constrain on soft mask by weight sharing or normalization.
However, as supported by our experiments, making attention change adaptively with features without additional constraint leads to the best performance.
4. Experiments
In this section, we evaluate the performance of proposed Residual Attention Network on a series of benchmark datasets including CIFAR-10, CIFAR-100 , and
ImageNet . Our experiments contain two parts. In the
ﬁrst part, we analyze the effectiveness of each component in
the Residual Attention Network including attention residual
learning mechanism and different architectures of soft mask
branch in the Attention Module. After that, we explore the
noise resistance property. Given limited computation resources, we choose CIFAR-10 and CIFAR-100 dataset to
conduct these experiments. Finally, we compare our network with state-of-the-art results in CIFAR dataset. In the
second part, we replace the Residual Unit with Inception
Module and ResNeXt to demonstrate our Residual Attention Network surpasses origin networks both in parameter
efﬁciency and ﬁnal performance. We also compare image
classiﬁcation performance with state-of-the-art ResNet and
Inception on ImageNet dataset.
4.1. CIFAR and Analysis
Implementation.
datasets consist of 60, 000 32 × 32 color images of 10 and
100 classes respectively, with 50, 000 training images and
10, 000 test images. The broadly applied state-of-the-art
network structure ResNet is used as baseline method. To
conduct fair comparison, we keep most of the settings same
as ResNet paper . The image is padded by 4 pixels on
each side, ﬁlled with 0 value resulting in 40 × 40 image.
A 32 × 32 crop is randomly sampled from an image or
its horizontal ﬂip, with the per-pixel RGB mean value
subtracted. We adopt the same weight initialization method
following previous study and train Residual Attention
Network using nesterov SGD with a mini-batch size of 64.
We use a weight decay of 0.0001 with a momentum of 0.9
and set the initial learning rate to 0.1. The learning rate
is divided by 10 at 64k and 96k iterations. We terminate
training at 160k iterations.
The overall network architecture and the hyper parameters setting are described in Fig.2. The network consists of
3 stages and similar to ResNet , equal number of Attention Modules are stacked in each stage. Additionally,
we add two Residual Units at each stage. The number of
weighted layers in trunk branch is 36m+20 where m is the
number of Attention Module in one stage. We use original
32 × 32 image for testing.
Attention Residual Learning.
In this experiment, we
evaluate the effectiveness of attention residual learning
mechanism. Since the notion of attention residual learning (ARL) is new, no suitable previous methods are comparable therefore we use “naive attention learning” (NAL)
as baseline.
Speciﬁcally, “naive attention learning” uses
Attention Module where features are directly dot product
by soft mask without attention residual learning. We set
the number of Attention Module in each stage m = {1, 2,
For Attention Module, this leads to Attention-56
(named by trunk layer depth), Attention-92, Attention-128
and Attention-164 respectively.
We train these networks using different mechanisms and
ARL (Top-1 err. %)
NAL (Top-1 err.%)
Attention-56
Attention-92
Attention-128
Attention-164
Table 3: Classiﬁcation error (%) on CIAFR-10.
ResNet-164
Relative Mean Response
Figure 4: The mean absolute response of output features in
each stage.
summarize the results in the Table 3.
As shown in Table 3, the networks trained using attention residual learning technique consistently outperform the networks trained
with baseline method which proves the effectiveness of our
method. The performance increases with the number of Attention Module when applying attention residual learning.
In contrast, the performance of networks trained with “naive
attention learning” method suffers obvious degradation with
increased number of Attention Module.
To understand the beneﬁt of attention residual learning,
we calculate mean absolute response value of output layers
for each stage. We use Attention-164 to conduct this experiment. As shown in the Fig. 4, the response generated by the
network trained using naive attention learning quickly vanishes in the stage 2 after four Attention Modules compared
with network trained using attention residual learning. The
Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask. However, repeated dot product will lead
to severe degradation of both useful and useless information
in this process. The attention residual learning can relieve
signal attenuation using identical mapping, which enhances
the feature contrast. Therefore, it gains beneﬁts from noise
reduction without signiﬁcant information loss, which makes
optimization much easier while improving the discrimination of represented features. In the rest of the experiments,
we apply this technique to train our networks.
Comparison of different mask structures.
We conduct experiments to validate the effectiveness of encoderdecoder structure by comparing with local convolutions
without any down sampling or up sampling.
convolutions soft mask consists of three Residual Units using the same number of FLOPs. The Attention-56 is used
to construct Attention-Encoder-Decoder-56 and Attention-
Local-Conv-56 respectively. Results are shown in Table 4.
The Attention-Encoder-Decoder-56 network achieves lower
test error 5.52% compared with Attention-Local-Conv-56
network 6.48% with a considerable margin 0.94%. The result suggests that the soft attention optimization process will
beneﬁt from multi-scale information.
Attention Type
Top-1 err. (%)
Local Convolutions
Local Attention
Encoder and Decoder
Mixed Attention
Table 4: Test error (%) on CIFAR-10 using different mask
structures.
Noisy Label Robustness.
In this experiment, we show
our Residual Attention Network enjoys noise resistant property on CIFAR-10 dataset following the setting of paper . The confusion matrix Q in our experiment is set
as follows:
where r denotes the clean label ratio for the whole dataset.
We compare ResNet-164 network with Attention-92 network under different noise levels. The Table 5 shows the results. The test error of Attention-92 network is signiﬁcantly
lower than ResNet-164 network with the same noise level.
In addition, when we increase the ratio of noise, test error of Attenion-92 declines slowly compared with ResNet-
164 network. These results suggest that our Residual Attention Network can perform well even trained with high level
noise data. When the label is noisy, the corresponding mask
can prevent gradient caused by label error to update trunk
branch parameters in the network. In this way, only the
trunk branch is learning the wrong supervision information
and soft mask branch masks the wrong label.
Comparisons with state-of-the-art methods.
We compare our Residual Attention Network with state-of-the-art
methods including ResNet and Wide ResNet on
Noise Level
ResNet-164 err. (%)
Attention-92 err. (%)
Table 5: Test error (%) on CIFAR-10 with label noises.
params×106
ResNet-164 
ResNet-1001 
WRN-16-8 
WRN-28-10 
Attention-92
Attention-236
Attention-452†
Comparisons with state-of-the-art methods on
CIFAR-10/100. †: the Attention-452 consists of Attention
Module with hyper-parameters setting: {p = 2, t = 4,
r = 3} and 6 Attention Modules per stage.
CIFAR-10 and CIFAR-100 datasets. The results are shown
in Table 6. Our Attention-452 outperforms all the baseline
methods on CIFAR-10 and CIFAR-100 datasets. Note that
Attention-92 network achieves 4.99% test error on CIFAR-
10 and 21.71% test error on CIFAR-100 compared with
5.46% and 24.33% test error on CIFAR-10 and CIFAR-
100 for ResNet-164 network under similar parameter size.
In addition, Attention-236 outperforms ResNet-1001 using
only half of the parameters. It suggests that our Attention
Module and attention residual learning scheme can effectively reduce the number of parameters in the network while
improving the classiﬁcation performance.
4.2. ImageNet Classiﬁcation
In this section, we conduct experiments using ImageNet
LSVRC 2012 dataset , which contains 1, 000 classes
with 1.2 million training images, 50, 000 validation images,
and 100, 000 test images. The evaluation is measured on the
non-blacklist images of the ImageNet LSVRC 2012 validation set. We use Attention-56 and Attention-92 to conduct
the experiments. The network structures and hyper parameters can be found in the Table 2.
Implementation.
Our implementation generally follows
the practice in the previous study .
We apply scale
and aspect ratio augmentation to the original image.
A 224 × 224 crop is randomly sampled from an augment
image or its horizontal ﬂip, with the per-pixel RGB scale
to and mean value subtracted and standard variance
divided. We adopt standard color augmentation . The
network is trained using SGD with a momentum of 0.9. We
set initial learning rate to 0.1. The learning rate is divided
by 10 at 200k, 400k, 500k iterations. We terminate training
at 530k iterations.
Mask Inﬂuence.
In this experiment, we explore the efﬁciency of proposed Residual Attention Network. We compare Attention-56 with ResNet-152 . The ResNet-152
has 50 trunk Residual Units and 60.2×106 parameters com-
params×106
Top-1 err. (%)
Top-5 err. (%)
ResNet-152 
Attention-56
ResNeXt-101 
AttentionNeXt-56
Inception-ResNet-v1 
AttentionInception-56
ResNet-200 
Inception-ResNet-v2
Attention-92
Table 7: Single crop validation error on ImageNet.
pared with 18 trunk Residual Units and 31.9×106 parameters in Attention-56. We evaluate our model using single
crop scheme on the ImageNet validation set and show results in Table 7.
The Attention-56 network outperforms
ResNet-152 by a large margin with a 0.4% reduction on
top-1 error and a 0.26% reduction on top-5 error. More
importantly, Attention-56 network achieves better performance with only 52% parameters and 56% FLOPs compared with ResNet-152, which suggests that the proposed
attention mechanism can signiﬁcantly improve network performance while reducing the model complexity.
Different Basic Units.
In this experiment, we show
Residual Attention Network can generalize well using different basic unit. We apply three popular basic units: Residual Unit, ResNeXt , and Inception to construct our
Residual Attention Networks. To keep the number of parameters and FLOPs in the same scale, we simplify the Inception. Results are shown in Table 7.
When the basic unit is ResNeXt, the AttentionNeXt-56
network performance is the same as ResNeXt-101 while
the parameters and FLOPs are signiﬁcantly fewer than
ResNeXt-101.
For Inception, The AttentionIncepiton-56
outperforms Inception-ResNet-v1 by a margin with a
0.94% reduction on top-1 error and a 0.21% reduction on
top-5 error. The results show that our method can be applied on different network structures.
Comparisons with State-of-the-art Methods.
We compare our Attention-92 evaluated using single crop on the
ILSVRC 2012 validation set with state-of-the-art algorithms. Table 7 shows the results. Our Attention-92 outperforms ResNet-200 with a large margin. The reduction on
top-1 error is 0.6%. Note that the ResNet-200 network contains 32% more parameters than Attention-92. The computational complexity of Attention-92 shown in the Table 7
suggests that our network reduces nearly half training time
comparing with ResNet-200 by adding attention mechanism and reducing trunk depth. Above results suggest that
our model enjoys high efﬁciency and good performance.
5. Discussion
We propose a Residual Attention Network which stacks
multiple Attention Modules. The beneﬁts of our network
are in two folds: it can capture mixed attention and is an extensible convolutional neural network. The ﬁrst beneﬁt lies
in that different Attention Modules capture different types
of attention to guide feature learning. Our experiments on
the forms of activation function also validate this point:
free form mixed attention will have better performance than
constrained (including single) attention. The second beneﬁt comes from encoding top-down attention mechanism
into bottom-up top-down feedforward convolutional structure in each Attention Module. Thus, the basic Attention
Modules can be combined to form larger network structure.
Moreover, residual attention learning allows training very
deep Residual Attention Network. The performance of our
model surpasses state-of-the-art image classiﬁcation methods, i.e. ResNet on CIFAR-10 (3.90% error), CIFAR-100
(20.67% error), and challenging ImageNet dataset (0.6%
top-1 accuracy improvement) with only 46% trunk depth
and 69% forward FLOPs (comparing with ResNet-200). In
the future, we will exploit different applications of deep
Residual Attention Network such as detection and segmentation to better explore mixed attention mechanism for speciﬁc tasks.