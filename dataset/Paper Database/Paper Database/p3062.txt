Int J Comput Vis 123:32–73
DOI 10.1007/s11263-016-0981-7
Visual Genome: Connecting Language and Vision Using
Crowdsourced Dense Image Annotations
Ranjay Krishna1
· Yuke Zhu1 · Oliver Groth2 · Justin Johnson1 · Kenji Hata1 ·
Joshua Kravitz1 · Stephanie Chen1 · Yannis Kalantidis3 · Li-Jia Li4 ·
David A. Shamma5 · Michael S. Bernstein1 · Li Fei-Fei1
Received: 23 February 2016 / Accepted: 12 September 2016 / Published online: 6 February 2017
© The Author(s) 2017. This article is published with open access at Springerlink.com
Abstract Despiteprogressinperceptualtaskssuchasimage
classiﬁcation, computers still perform poorly on cognitive
taskssuchasimagedescriptionandquestionanswering.Cognition is core to tasks that involve not just recognizing, but
reasoning about our visual world. However, models used
to tackle the rich content in images for cognitive tasks are
still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models
need to understand the interactions and relationships between
objects in an image. When asked “What vehicle is the person riding?”, computers will need to identify the objects in
an image as well as the relationships riding(man, carriage)
and pulling(horse, carriage) to answer correctly that “the
person is riding a horse-drawn carriage.” In this paper, we
present the Visual Genome dataset to enable the modeling of
such relationships. We collect dense annotations of objects,
attributes, and relationships within each image to learn these
models. Speciﬁcally, our dataset contains over 108K images
where each image has an average of 35 objects, 26 attributes,
and 21 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases
in region descriptions and questions answer pairs to Word-
Net synsets. Together, these annotations represent the densest
Communicated by Margaret Mitchell, John Platt, and Kate Saenko.
B Ranjay Krishna
 
Stanford University, Stanford, CA, USA
Dresden University of Technology, Dresden, Germany
Yahoo Inc., San Francisco, CA, USA
Snapchat Inc., Los Angeles, CA, USA
Centrum Wiskunde & Informatica (CWI), Amsterdam,
The Netherlands
and largest dataset of image descriptions, objects, attributes,
relationships, and question answer pairs.
Keywords Computer vision · Dataset · Image · Scene graph ·
Question answering · Objects · Attributes · Relationships ·
Knowledge · Language · Crowdsourcing
1 Introduction
A holy grail of computer vision is the complete understanding of visual scenes: a model that is able to name and
detect objects, describe their attributes, and recognize their
relationships. Understanding scenes would enable important applications such as image search, question answering,
and robotic interactions. Much progress has been made in
recent years towards this goal, including image classiﬁcation and object detection . An important contributing factor
is the availability of a large amount of data that drives the
statistical models that underpin today’s advances in computational visual understanding. While the progress is exciting,
we are still far from reaching the goal of comprehensive scene
understanding. As Fig. 1 shows, existing models would be
able to detect discrete objects in a photo but would not be
able to explain their interactions or the relationships between
them. Such explanations tend to be cognitive in nature, integrating perceptual information into conclusions about the
relationships between objects in a scene . A cognitive understanding of our
visual world thus requires that we complement computers’ ability to detect objects with abilities to describe those
Int J Comput Vis 123:32–73
Fig. 1 An overview of the data needed to move from perceptual awareness to cognitive understanding of images. We present a dataset of
images densely annotated with numerous region descriptions, objects,
attributes, and relationships. Some examples of region descriptions (e.g.
“girl feeding large elephant” and “a man taking a picture behind girl”)
are shown (top). The objects (e.g. elephant), attributes (e.g. large)
and relationships (e.g. feeding) are shown (bottom). Our dataset also
contains image related question answer pairs (not shown)
objects and understand their interactions
within a scene .
There is an increasing effort to put together the next generation of datasets to serve as training and benchmarking
datasets for these deeper, cognitive scene understanding and
reasoning tasks, the most notable being MS-COCO and VQA . The MS-COCO dataset
consists of 300K real-world photos collected from Flickr. For
each image, there is pixel-level segmentation of 80 object
classes (when present) and 5 independent, user-generated
sentences describing the scene. VQA adds to this a set of
614K question answer pairs related to the visual contents of
each image (see more details in Sect. 3.1). With this information, MS-COCO and VQA provide a fertile training and
testing ground for models aimed at tasks for accurate object
detection, segmentation, and summary-level image captioning as well as basic QA .
For example, a state-of-the-art model provides a description of one MS-COCO image in
Fig. 1 as “two men are standing next to an elephant.” But
what is missing is the further understanding of where each
object is, what each person is doing, what the relationship
between the person and elephant is, etc. Without such relationships, these models fail to differentiate this image from
other images of people next to elephants.
To understand images thoroughly, we believe three key
elements need to be added to existing datasets: a grounding of visual concepts to language ,
a more complete set of descriptions and QAs for each
image based on multiple image regions ,
and a formalized representation of the components of an
image . In the spirit of mapping out this complete information of the visual world, we introduce the Visual
Genome dataset. The ﬁrst release of the Visual Genome
dataset uses 108,077 images from the intersection of the
YFCC100M and MS-COCO . Section 5 provides a more detailed description of the
dataset. We highlight below the motivation and contributions
of the three key elements that set Visual Genome apart from
existing datasets.
Int J Comput Vis 123:32–73
The Visual Genome dataset regards relationships and
attributes as ﬁrst-class citizens of the annotation space, in
addition to the traditional focus on objects. Recognition of
relationships and attributes is an important part of the complete understanding of the visual scene, and in many cases,
these elements are key to the story of a scene (e.g., the difference between “a dog chasing a man” versus “a man chasing
a dog”). The Visual Genome dataset is among the ﬁrst to provide a detailed labeling of object interactions and attributes,
grounding visual concepts to language.1
An image is often a rich scenery that cannot be fully
described in one summarizing sentence. The scene in Fig. 1
contains multiple “stories”: “a man taking a photo of elephants,” “a woman feeding an elephant,” “a river in the
background of lush grounds,” etc. Existing datasets such as
Flickr 30K and MS-COCO focus on high-level descriptions of an image.2 Instead,
for each image in the Visual Genome dataset, we collect more
than 50 descriptions for different regions in the image, providing a much denser and more complete set of descriptions
ofthescene.Inaddition,inspiredbyVQA ,
we also collect an average of 17 question answer pairs based
on the descriptions for each image. Region-based question
answers can be used to jointly develop NLP and vision models that can answer questions from either the description or
the image, or both of them.
With a set of dense descriptions of an image and the
explicit correspondences between visual pixels (i.e. bounding boxes of objects) and textual descriptors (i.e. relationships, attributes), the Visual Genome dataset is poised to be
the ﬁrst image dataset that is capable of providing a structured formalized representation of an image, in the form
that is widely used in knowledge base representations in
NLP .Forexample,inFig.1,we
can formally express the relationship holding between the
woman and food as holding(woman, food). Putting together
all the objects and relations in a scene, we can represent each
image as a scene graph . The scene graph
representation has been shown to improve semantic image
retrieval and image
captioning . Furthermore, all objects, attributes and relationships in each image in the Visual Genome dataset are
canonicalized to its corresponding WordNet 
ID (called a synset ID). This mapping connects all images in
Visual Genome and provides an effective way to consistently
1 The Lotus Hill Dataset also provides a similar annotation of object relationships, see Sec 3.1.
2 COCO has multiple sentences generated independently by different
users, all focusing on providing an overall, one sentence description of
the scene.
query the same concept (object, attribute, or relationship)
in the dataset. It can also potentially help train models that
can learn from contextual information from multiple images
(Figs. 2, 3).
In this paper, we introduce the Visual Genome dataset with
the aim of training and benchmarking the next generation
of computer models for comprehensive scene understanding. The paper proceeds as follows: In Sect. 2, we provide
a detailed description of each component of the dataset.
Section 3 provides a literature review of related datasets as
well as related recognition tasks. Section 4 discusses the
crowdsourcing strategies we deployed in the ongoing effort
ofcollectingthisdataset.Section5isacollectionofdataanalysis statistics, showcasing the key properties of the Visual
Genome dataset. Last but not least, Sect. 6 provides a set of
experimental results that use Visual Genome as a benchmark.
Further visualizations, API, and additional information on
the Visual Genome dataset can be found online.3
2 Visual Genome Data Representation
The Visual Genome dataset consists of seven main components: region descriptions, objects, attributes, relationships,
region graphs, scene graphs, and question answer pairs.
Figure 4 shows examples of each component for one image.
To enable research on comprehensive understanding of
images, we begin by collecting descriptions and question
answers. These are raw texts without any restrictions on
length or vocabulary. Next, we extract objects, attributes
and relationships from our descriptions. Together, objects,
attributes and relationships comprise our scene graphs that
represent a formal representation of an image. In this section,
we break down Fig. 4 and explain each of the seven components. In Sect. 4, we will describe in more detail how data
from each component is collected through a crowdsourcing
2.1 Multiple Regions and Their Descriptions
In a real-world image, one simple summary sentence is
often insufﬁcient to describe all the contents of and interactions in an image. Instead, one natural way to extend
this might be a collection of descriptions based on different regions of a scene. In Visual Genome, we collect
diverse human-generated image region descriptions, with
each region localized by a bounding box. In Fig. 5, we show
three examples of region descriptions. Regions are allowed
to have a high degree of overlap with each other when the
descriptions differ. For example, “yellow ﬁre hydrant” and
“woman in shorts is standing behind the man” have very little
3 
Int J Comput Vis 123:32–73
Fig. 2 An example image from the Visual Genome dataset. We show
3 region descriptions and their corresponding region graphs. We also
show the connected scene graph collected by combining all of the
image’s region graphs. The top region description is “a man and a
woman sit on a park bench along a river.” It contains the objects:
man, woman, bench and river. The relationships that connect
these objects are: sits_on(man, bench), in_front_of (man, river), and
sits_on(woman, bench)
Int J Comput Vis 123:32–73
Fig. 3 An example image from our dataset along with its scene
graph representation. The scene graph contains objects (child,
instructor, helmet, etc.) that are localized in the image as bounding boxes (not shown). These objects also have attributes: large,
green, behind, etc. Finally, objects are connected to each other
through relationships: wears(child, helmet), wears(instructor, jacket),
Int J Comput Vis 123:32–73
Fig. 4 A representation of the Visual Genome dataset. Each image
contains region descriptions that describe a localized portion of the
image. We collect two types of question answer pairs (QAs): freeform
QAs and region-based QAs. Each region is converted to a region graph
representation of objects, attributes, and pairwise relationships. Finally,
each of these region graphs are combined to form a scene graph with
all the objects grounded to the image. Best viewed in color
Int J Comput Vis 123:32–73
Fig. 5 To describe all the contents of and interactions in an image,
the Visual Genome dataset includes multiple human-generated image
regions descriptions, with each region localized by a bounding box.
Here, we show three regions descriptions on various image regions:
“man jumping over a ﬁre hydrant,” “yellow ﬁre hydrant,” and “woman
in shorts is standing behind the man”
overlap, while “man jumping over ﬁre hydrant” has a very
high overlap with the other two regions. Our dataset contains
on average a total of 50 region descriptions per image. Each
description is a phrase ranging from 1 to 16 words in length
describing that region.
2.2 Multiple Objects and Their Bounding Boxes
Each image in our dataset consists of an average of 35
objects, each delineated by a tight bounding box (Fig. 6).
Furthermore, each object is canonicalized to a synset ID
in WordNet . For example, man would get
mapped to man.n.03 (the generic use of the
word to refer to any human being).Similarly,
person gets mapped to person.n.01 (a human
being). Afterwards, these two concepts can be joined to
person.n.01 since this is a hypernym of man.n.03. We
did not standardize synsets in our dataset. However, given
our canonicalization, this is easily possible leveraging the
WordNet ontology to avoid multiple names for one object
(e.g. man, person, human), and to connect information across
2.3 A Set of Attributes
Each image in Visual Genome has an average of 26
attributes. Objects can have zero or more attributes asso-
Fig. 6 From all of the region descriptions, we extract all objects mentioned. For example, from the region description “man jumping over a
ﬁre hydrant,” we extract man and fire hydrant
Fig. 7 Some descriptions also provide attributes for objects. For example, the region description “yellow ﬁre hydrant” adds that the fire
hydrant is yellow. Here we show two attributes: yellow and
ciated with them. Attributes can be color (e.g. yellow),
states (e.g. standing), etc. (Fig. 7). Just like we collect objects from region descriptions, we also collect the
attributes attached to these objects. In Fig. 7, from the
phrase“yellowﬁrehydrant,”weextracttheattribute yellow
for the fire hydrant. As with objects, we canonicalize all attributes to WordNet ; for example,
yellow is mapped to yellow.s.01 123:32–73
Fig. 8 Our dataset also captures the relationships and interactions
between objects in our images. In this example, we show the relationship jumping over between the objects man and fire hydrant
the color spectrum; of something
resembling the color of an egg yolk).
2.4 A Set of Relationships
Relationships connect two objects together. These relationships can be actions (e.g. jumping over), spatial (e.g. is
behind), descriptive verbs (e.g. wear), prepositions (e.g.
with), comparative (e.g. taller than), or prepositional
phrases (e.g. drive on). For example, from the region
description “man jumping over ﬁre hydrant,” we extract the
relationship jumping over between the objects man and
fire hydrant (Fig. 8). These relationships are directed
from one object, called the subject, to another, called the
object. In this case, the subject is the man, who is performing the relationship jumping over on the object fire
hydrant. Each relationship is canonicalized to a Word-
Net synset ID; i.e. jumping is canonicalized to jump.a.1 (move forward by leaps and
bounds). On average, each image in our dataset contains
21 relationships.
2.5 A Set of Region Graphs
Combining the objects, attributes, and relationships extracted
from region descriptions, we create a directed graph representation for each of the regions. Examples of region graphs
are shown in Fig. 4. Each region graph is a structured representation of a part of the image. The nodes in the graph
represent objects, attributes, and relationships. Objects are
linked to their respective attributes while relationships link
one object to another. The links connecting two objects in
Fig. 4 point from the subject to the relationship and from the
relationship to the other object.
2.6 One Scene Graph
While region graphs are localized representations of an
image, we also combine them into a single scene graph representing the entire image (Fig. 3). The scene graph is the
union of all region graphs and contains all objects, attributes,
and relationships from each region description. By doing so,
we are able to combine multiple levels of scene information
in a more coherent way. For example in Fig. 4, the leftmost
region description tells us that the “ﬁre hydrant is yellow,”
while the middle region description tells us that the “man is
jumping over the ﬁre hydrant.” Together, the two descriptions
tell us that the “man is jumping over a yellow ﬁre hydrant.”
2.7 A Set of Question Answer Pairs
We have two types of QA pairs associated with each image
in our dataset: freeform QAs, based on the entire image, and
region-based QAs, based on selected regions of the image.
We collect 6 different types of questions per image: what,
where,how,when,who,and why.InFig.4,“Q.Whatisthe
woman standing next to?; A. Her belongings” is a freeform
QA. Each image has at least one question of each type
listed above. Region-based QAs are collected by prompting workers with region descriptions. For example, we use
the region “yellow ﬁre hydrant” to collect the region-based
QA: “Q. What color is the ﬁre hydrant?; A. Yellow.” Region
based QAs are based on the description and allow us to
independently study how well models perform at answering questions using the image or the region description as
3 Related Work
We discuss existing datasets that have been released and used
by the vision community for classiﬁcation and object detection. We also mention work that has improved object and
attribute detection models. Then, we explore existing work
that has utilized representations similar to our relationships
between objects. In addition, we dive into literature related to
cognitive tasks like image description, question answering,
and knowledge representation.
3.1 Datasets
Datasets (Table 1) have been growing in size as researchers
havebeguntacklingincreasinglycomplicatedproblems.Caltech 101 was one of the ﬁrst datasets
hand-curated for image classiﬁcation, with 101 object cate-
Int J Comput Vis 123:32–73
Table 1 A comparison of existing datasets with Visual Genome
Descriptions
Total objects
categories
Objects per
# Attributes
categories
Attributes
# Relationship
categories
Relationships
YFCC100M 
100,000,000 –
Tiny images 
80,000,000
ImageNet 
14,197,122
14,197,122
ILSVRC detection 
 
MS-COCO 
Flickr 30K 
Caltech 101 
Caltech 256 
Caltech pedestrian 
Pascal detection 
Abstract scenes 
aPascal 
Animal attributes 
SUN attributes 
Caltech birds 
COCO actions 
Visual phrases –
VisKE 
DAQUAR 
COCO QA 
Baidu 
VQA 
Visual Genome
We show that Visual Genome has an order of magnitude more descriptions and question answers. It also has a more diverse set of object, attribute, and relationship classes. Additionally, Visual
Genome contains a higher density of these annotations per image. The number of distinct categories in Visual Genome are calculated by lower-casing and stemming names of objects, attributes
and relationships
Int J Comput Vis 123:32–73
gories and 15–30 examples per category. One of the biggest
criticisms of Caltech 101 was the lack of variability in its
examples. Caltech 256 increased the
number of categories to 256, while also addressing some
of the shortcomings of Caltech 101. However, it still had
only a handful of examples per category, and most of its
images contained only a single object. LabelMe introduced a dataset with multiple objects per
category. They also provided a web interface that experts
and novices could use to annotate additional images. This
web interface enabled images to be labeled with polygons,
helping create datasets for image segmentation. The Lotus
Hill dataset contains a hierarchical decomposition of objects (vehicles, man-made objects, animals,
etc.) along with segmentations. Only a small part of this
dataset is freely available. SUN , just like
LabelMe and Lotus Hill , was curated for object detection. Pushing the size
of datasets even further, 80 Million Tiny Images created a signiﬁcantly larger dataset than its
predecessors. It contains tiny (i.e. 32 × 32 pixels) images
that were collected using WordNet synsets as
queries. However, because the data in 80 Million Images
were not human-veriﬁed, they contain numerous errors.
YFCC100M is another large database of
100 million images that is still largely unexplored. It contains
human generated and machine generated tags.
Pascal VOC pushed research
from classiﬁcation to object detection with a dataset containing 20 semantic categories in 11, 000 images. ImageNet took WordNet synsets and crowdsourced a large dataset of 14 million images. They started the
ILSVRC challenge for a variety of
computer vision tasks. Together, ILSVRC and PASCAL provide a test bench for object detection, image classiﬁcation,
object segmentation, person layout, and action classiﬁcation.
MS-COCO recently released its dataset,
with over 328, 000 images with sentence descriptions and
segmentations of 80 object categories. The previous largest
dataset for image-based QA, VQA , contains 204,721 images annotated with three question answer
pairs. They collected a dataset of 614,163 freeform questions
with 6.1M ground truth answers (10 per question) and provided a baseline approach in answering questions using an
image and a textual question as the input.
Visual Genome aims to bridge the gap between all these
datasets, collecting not just annotations for a large number
of objects but also scene graphs, region descriptions, and
question answer pairs for image regions. Unlike previous
datasets, which were collected for a single task like image
classiﬁcation, the Visual Genome dataset was collected to be
a general-purpose representation of the visual world, without
bias toward a particular task. Our images contain an average
of 35 objects, which is almost an order of magnitude more
dense than any existing vision dataset. Similarly, we contain
an average of 26 attributes and 21 relationships per image.
We also have an order of magnitude more unique objects,
attributes, and relationships than any other dataset. Finally,
we have 1.7 million question answer pairs, also larger than
any other dataset for visual question answering.
3.2 Image Descriptions
One of the core contributions of Visual Genome is its descriptions for multiple regions in an image. As such, we mention
other image description datasets and models in this subsection. Most work related to describing images can be divided
into two categories: retrieval of human-generated captions
and generation of novel captions. Methods in the ﬁrst category use similarity metrics between image features from
predeﬁned models to retrieve similar sentences . Other methods map both sentences and their images to a common vector space or map them to a space of triples . Among those in the second category, a common theme
has been to use recurrent neural networks to produce novel
captions . More recently,
researchers have also used a visual attention model and MS-COCO , whose sentence desriptions tend to focus, somewhat redundantly, on
these salient parts. For example, “an elephant is seen wandering around on a sunny day,” “a large elephant in a tall
grass ﬁeld,” and “a very large elephant standing alone in
some brush” are 3 descriptions from the MS-COCO dataset,
and all of them focus on the salient elephant in the image
and ignore the other regions in the image. Many real-world
scenes are complex, with multiple objects and interactions
that are best described using multiple descriptions . Our dataset pushes
toward a more complete understanding of an image by collecting a dataset in which we capture not just scene-level
descriptions but also myriad of low-level descriptions, the
“grammar” of the scene.
3.3 Objects
Object detection is a fundamental task in computer vision,
with applications ranging from identiﬁcation of faces in
photo software to identiﬁcation of other cars by self-driving
cars on the road. It involves classifying an object into a dis-
Int J Comput Vis 123:32–73
tinct category and localizing the object in the image. Visual
Genome uses objects as a core component on which each
visual scene is built. Early datasets include the face detection and pedestrian datasets . The PASCAL VOC and ILSVRC’s detection dataset
pushed research in object detection. But the images in these
datasets are iconic and do not capture the settings in which
these objects usually co-occur. To remedy this problem, MS-
COCO annotated real-world scenes that
capture object contexts. However, MS-COCO was unable to
describe all the objects in its images, since they annotated
only 80 object categories. In the real world, there are many
more objects that the ones captured by existing datasets.
Visual Genome aims at collecting annotations for all visual
elements that occur in images, increasing the number of distinct categories to 33,877.
3.4 Attributes
The inclusion of attributes allows us to describe, compare,
and more easily categorize objects. Even if we haven’t seen
an object before, attributes allow us to infer something about
it; for example, “yellow and brown spotted with long neck”
likelyreferstoagiraffe.Initialworkinthisareainvolvedﬁnding objects with similar features 
using examplar SVMs. Next, textures were used to study
objects , while other methods learned to predict colors .
Finally, the study of attributes was explicitly demonstrated to
lead to improvements in object classiﬁcation . Attributes were deﬁned to be parts (e.g. “has legs”),
shapes (e.g. “spherical”), or materials (e.g. “furry”) and could
be used to classify new categories of objects. Attributes have
also played a large role in improving ﬁne-grained recognition on ﬁne-grained attribute datasets
like CUB-2011 . In Visual Genome, we
use a generalized formulation , but we
extend it such that attributes are not image-speciﬁc binaries but rather object-speciﬁc for each object in a real-world
scene. We also extend the types of attributes to include size
(e.g. “small”), pose (e.g. “bent”), state (e.g. “transparent”),
emotion (e.g. “happy”), and many more.
3.5 Relationships
Relationship extraction has been a traditional problem in
information extraction and in natural language processing. Syntactic features , dependency tree methods , and deep neural networks have been
employed to extract relationships between two entities in a
sentence. However, in computer vision, very little work has
gone into learning or predicting relationships. Instead, relationships have been implicitly used to improve other vision
tasks. Relative layouts between objects have improved scene
categorization , and 3D spatial geometry between objects has helped object detection (Choi et al.
213). Comparative adjectives and prepositions between pairs
of objects have been used to model visual relationships and
improved object localization .
Relationships have already shown their utility in improving visual cognitive tasks . A meaning space of relationships has improved the
mapping of images to sentences . Relationships in a structured representation with objects have
been deﬁned as a graph structure called a scene graph, where
the nodes are objects with attributes and edges are relationships between objects. This representation can be used to
generate indoor images from sentences and also to improve
image search . We
use a similar scene graph representation of an image that
generalizes across all these previous works . Recently, relationships have come into focus again in
the form of question answering about associations between
objects . These questions ask if a relationship, involving generally two objects, is true, e.g. “do
dogs eat ice cream?”. We believe that relationships will be
necessary for higher-level cognitive tasks , so we collect the largest corpus of
them in an attempt to improve tasks by actually understanding interactions between objects.
3.6 Question Answering
Visual question answering (QA) has been recently proposed
as a proxy task of evaluating a computer vision system’s
ability to understand an image beyond object recognition
and image captioning . Several visual QA benchmarks have been proposed in the last few months. The DAQUAR dataset was the ﬁrst toy-sized QA benchmark built upon indoor scene RGB-D images of NYU
Depth v2 . Most new
datasets have collected QA pairs on MS-COCO
images, either generated automatically by NLP tools or written by human workers .
In previous datasets, most questions concentrated on simple recognition-based questions about the salient objects,
and answers were often extremely short. For instance, 90%
of DAQUAR answers and
89% of VQA answers consist of singleword object names, attributes, and quantities. This limitation
bounds their diversity and fails to capture the long-tail details
Int J Comput Vis 123:32–73
of the images. Given the availability of new datasets, an array
of visual QA models have been proposed to tackle QA tasks.
The proposed models range from SVM classiﬁers and probabilistic inference to recurrent
neuralnetworks and convolutional networks .
Visual Genome aims to capture the details of the images with
diverse question types and long answers. These questions
should cover a wide range of visual tasks from basic perception to complex reasoning. Our QA dataset of 1.7 million
QAs is also larger than any currently existing dataset.
3.7 Knowledge Representation
A knowledge representation of the visual world is capable
of tackling an array of vision tasks, from action recognition to general question answering. However, it is difﬁcult to
answer “what is the minimal viable set of knowledge needed
to understand about the physical world?” . It
was later proposed that there be a certain plurality to concepts
and their related axioms . These efforts have
grown to model physical processes or to model
a series of actions as scripts for
stories—both of which are not depicted in a single static
image but which play roles in an image’s story . More recently, NELL 
learns probabilistic horn clauses by extracting information
from the web. DeepQA proposes a
probabilistic question answering architecture involving over
100 different techniques. Others have used Markov logic
networks as their representation to perform statistical inference for knowledge base
construction. Our work is most similar to that of those who
attempt to learn common-sense relationships from images.
Visual Genome scene graphs can also be considered a dense
knowledge representation for images. It is similar to the format used in knowledge bases in NLP.
4 Crowdsourcing Strategies
Visual Genome was collected and veriﬁed entirely by crowd
workers from Amazon Mechanical Turk. In this section, we
outline the pipeline employed in creating all the components
of the dataset. Each component (region descriptions, objects,
attributes, relationships, region graphs, scene graphs, questions and answers) involved multiple task stages. We mention
the different strategies used to make our data accurate and
to enforce diversity in each component. We also provide
background information about the workers who helped make
Visual Genome possible.
Table 2 Geographic distribution of countries from where crowd workers contributed to Visual Genome
Distribution (%)
United States
Philippines
4.1 Crowd Workers
We used Amazon Mechanical Turk (AMT) as our primary
source of annotations. Overall, a total of over 33, 000 unique
workers contributed to the dataset. The dataset was collected
over the course of 6 months after 15 months of experimentation and iteration on the data representation. Approximately
800, 000 Human Intelligence Tasks (HITs) were launched on
AMT, where each HIT involved creating descriptions, questions and answers, or region graphs. Each HIT was designed
such that workers manage to earn anywhere between $6-
$8 per hour if they work continuously, in line with ethical
research standards on Mechanical Turk .
Visual Genome HITs achieved a 94.1% retention rate, meaning that 94.1% of workers who completed one of our tasks
went ahead to do more. Table 2 outlines the percentage distribution of the locations of the workers. 93.02% of workers
contributed from the United States.
Figure 9a, b outline the demographic distribution of our
crowd workers. This data was collected using a survey HIT.
The majority of our workers were between the ages of 25 and
34 years old. Our youngest contributor was 18 years and the
oldest was 68 years old. We also had a near-balanced split of
54.15% male and 45.85% female workers.
4.2 Region Descriptions
Visual Genome’s main goal is to enable the study of cognitive
computer vision tasks. The next step towards understanding images requires studying relationships between objects
in scene graph representations of images. However, we
observed that collecting scene graphs directly from an image
leads to workers annotating easy, frequently-occurring relationships like wearing(man, shirt) instead of focusing on
salient parts of the image. This is evident from previous
datasets that contain
a large number of such relationships. After experimentation,
we observed that when asked to describe an image using natural language, crowd workers naturally start with the most
salient part of the image and then move to describing other
Int J Comput Vis 123:32–73
Fig. 9 a Age and b gender distribution of Visual Genome’s crowd workers
parts of the image one by one. Inspired by this ﬁnding, we
focused our attention towards collecting a dataset of region
descriptions that is diverse in content.
When a new image is added to the crowdsourcing pipeline
with no annotations, it is sent to a worker who is asked to
draw three bounding boxes and write three descriptions for
the region enclosed by each box. Next, the image is sent
to another worker along with the previously written descriptions. Workers are explicitly encouraged to write descriptions
that have not been written before. This process is repeated
until we have collected 50 region descriptions for each image.
To prevent workers from having to skim through a long list
of previously written descriptions, we only show them the
top seven most similar descriptions. We calculate these most
similar descriptions using BLEU-like 
(n-gram) scores between pairs of sentences. We deﬁne the
similarity score S between a description di and a previous
description d j to be:
Sn(di, d j) = b(di, d j) exp
log pn(di, d j)
where we enforce a brevity penalty using:
b(di, d j) =
if len(di) > len(d j)
and pn calculates the percentage of n-grams in di that match
n-grams in d j.
When a worker writes a new description, we programmatically enforce that it has not been repeated by using BLEU
score thresholds set to 0.7 to ensure that it is dissimilar to
descriptions from both of the following two lists:
1. Image-Speciﬁc Descriptions A list of all previously
written descriptions for that image.
2. Global Image Descriptions A list of the top 100 most
common written descriptions of all images in the dataset.
This prevents very common phrases like “sky is blue”
Fig. 10 Good (left) and bad (right) bounding boxes for the phrase “a
street with a red car parked on the side,” judged on coverage
from dominating the set of region descriptions. The list
of top 100 global descriptions is continuously updated as
more data comes in.
Finally, we ask workers to draw bounding boxes that satisfy one requirement: coverage. The bounding box must
cover all objects mentioned in the description. Figure 10
showsanexampleofagoodboxthatcoversboththe street
as well the car mentioned in the description, as well as an
example of a bad box.
4.3 Objects
Once 50 region descriptions are collected for an image,
we extract the visual objects from each description. Each
description is sent to one crowd worker, who extracts all the
objects from the description and grounds each object as a
bounding box in the image. For example, from Fig. 4, let’s
consider the description “woman in shorts is standing behind
the man.” A worker would extract three objects: woman,
shorts, and man. They would then draw a box around each
of the objects. We require each bounding box to be drawn
to satisfy two requirements: coverage and quality. Coverage has the same deﬁnition as described above in Sect. 4.2,
where we ask workers to make sure that the bounding box
covers the object completely (Fig. 11). Quality requires that
each bounding box be as tight as possible around its object
such that if the box’s length or height were decreased by one
pixel, it would no longer satisfy the coverage requirement.
Int J Comput Vis 123:32–73
Fig. 11 Good (left) and bad (right) bounding boxes for the object fox,
judged on both coverage as well as quality
Since a one pixel error can be physically impossible for most
workers, we relax the deﬁnition of quality to four pixels.
Multiple descriptions for an image might refer to the same
object, sometimes with different words. For example, a man
in one description might be referred to as person in another
description. We can thus use this crowdsourcing stage to
build these co-reference chains. With each region description
given to a worker to process, we include a list of previously
extracted objects as suggestions. This allows a worker to
choose a previously drawn box annotated as man instead of
redrawing a new box for person.
Finally, to increase the speed with which workers complete this task, we also use Stanford’s dependency parser
 toextractnounsautomaticallyandsend
themtotheworkersassuggestions.Whiletheparsermanages
to ﬁnd most of the nouns, it sometimes misses compound
nouns, so we avoided completely depending on this automated method. By combining the parser with crowdsourcing
tasks, we were able to speed up our object extraction process
without losing accuracy.
4.4 Attributes, Relationships, and Region Graphs
Once all objects have been extracted from each region
description, we can extract the attributes and relationships
described in the region. We present each worker with a region
description along with its extracted objects and ask them to
add attributes to objects or to connect pairs of objects with
relationships, based on the text of the description. From the
description “woman in shorts is standing behind the man”,
workers will extract the attribute standing for the woman
and the relationships in(woman, shorts) and behind(woman,
man). Together, objects, attributes, and relationships form
the region graph for a region description. Some descriptions
like “it is a sunny day” do not contain any objects and therefore have no region graphs associated with them. Workers
are asked to not generate any graphs for such descriptions.
We create scene graphs by combining all the region graphs
for an image by combining all the co-referenced objects from
different region graphs.
Fig. 12 Each object (fox) has only one bounding box referring to it
(left). Multiple boxes drawn for the same object (right) are combined
together if they have a minimum threshold of 0.9 intersection over union
4.5 Scene Graphs
The scene graph is the union of all region graphs extracted
from region descriptions. We merge nodes from region
graphs that correspond to the same object; for example, man
and person in two different region graphs might refer to
the same object in the image. We say that objects from different graphs refer to the same object if their bounding boxes
have an intersection over union of 0.9. However, this heuristic might contain false positives. So, before merging two
objects, we ask workers to conﬁrm that a pair of objects with
signiﬁcant overlap are indeed the same object. For example, in Fig. 12 (right), the fox might be extracted from
two different region descriptions. These boxes are then combined together (Fig. 12, left) when constructing the scene
4.6 Questions and Answers
To create question answer (QA) pairs, we ask the AMT workers to write pairs of questions and answers about an image. To
ensure quality, we instruct the workers to follow three rules:
1) start the questions with one of the “six Ws” (who, what,
where, when, why and how); 2) avoid ambiguous and speculative questions; 3) be precise and unique, and relate the
question to the image such that it is clearly answerable if and
only if the image is shown.
We collected two separate types of QAs: freeform QAs
and region-based QAs. In freeform QA, we ask a worker
to look at an image and write eight QA pairs about it. To
encourage diversity, we enforce that workers write at least
three different Ws out of the six in their eight pairs. In regionbased QA, we ask the workers to write a pair based on a given
region. We select the regions that have large areas (more
than 5k pixels) and long phrases (more than 4 words). This
enables us to collect around twenty region-based pairs at the
same cost of the eight freeform QAs. In general, freeform QA
tends to yield more diverse QA pairs that enrich the question
distribution; region-based QA tends to produce more factual
QA pairs at a lower cost.
Int J Comput Vis 123:32–73
4.7 Veriﬁcation
All Visual Genome data go through a veriﬁcation stage as
soon as they are annotated. This stage helps eliminate incorrectly labeled objects, attributes, and relationships. It also
helps remove region descriptions and questions and answers
that might be correct but are vague (“This person seems to
enjoy the sun.”), subjective (“room looks dirty”), or opinionated (“Being exposed to hot sun like this may cause cancer”).
Veriﬁcation is conducted using two separate strategies:
majority voting and rapid judgments
 . All components of the dataset except
objects are veriﬁed using majority voting. Majority voting involves three unique workers looking
at each annotation and voting on whether it is factually correct. An annotation is added to our dataset if at least two (a
majority) out of the three workers verify that it is correct.
We only use rapid judgments to speed up the veriﬁcation
of the objects in our dataset. Rapid judgments use an interface inspired by rapid serial visual processing that enable veriﬁcation of objects with an order of
magnitude increase in speed than majority voting.
4.8 Canonicalization
All the descriptions and QAs that we collect are freeform
worker-generated texts. They are not constrained by any limitations. For example, we do not force workers to refer to a
man in the image as a man. We allow them to choose to refer
to the man as person, boy, man, etc. This ambiguity makes
it difﬁcult to collect all instances of man from our dataset. In
order to reduce the ambiguity in the concepts of our dataset
andconnectittootherresourcesusedbytheresearchcommunity, we map all objects, attributes, relationships, and noun
phrases in region descriptions and QAs to synsets in Word-
Net . In the example above, person, boy,
and man would map to the synsets: person.n.01 (a
human being), male_child.n.01 (a youthful
male person)and man.n.03 (the generic use
of the word to refer to any human being)
respectively. Thanks to the WordNet hierarchy it is now possible to fuse those three expressions of the same concept into
person.n.01 (a human being), which is the lowest common ancestor node of all aforementioned synsets.
We use the Stanford NLP tools to
extract the noun phrases from the region descriptions and
QAs. Next, we map them to their most frequent matching
synset in WordNet according to WordNet lexeme counts. We
then reﬁne this simple heuristic by hand-crafting mapping
rules for the 30 most common failure cases. For example
according to WordNet’s lexeme counts the most common
semantic for “table” is table.n.01 (a set of data
arranged in rows and columns). However in our
data it is more likely to see pieces of furniture and therefore
bias the mapping towards table.n.02 (a piece of
furniture having a smooth flat top that
is usually supported by one or more
vertical legs). The objects in our scene graphs are
already noun phrases and are mapped to WordNet in the same
We normalize each attribute based on morphology (so
called “stemming”) and map them to the WordNet adjectives.
We include 15 hand-crafted rules to address common failure
cases, which typically occur when the concrete or spatial
sense of the word seen in an image is not the most common
overall sense. For example, the synset long.a.02 (of
relatively great or greater than average
spatial extension) is less common in WordNet than
long.a.01 (indicating a relatively great
or greater than average duration of
time), even though instances of the word “long” in our
images are much more likely to refer to that spatial sense.
For relationships, we ignore all prepositions as they are
not recognized by WordNet. Since the meanings of verbs are
highly dependent upon their morphology and syntactic placement (e.g. passive cases, prepositional phrases), we try to
ﬁnd WordNet synsets whose sentence frames match with the
context of the relationship. Sentence frames in WordNet are
formalizedsyntacticframesinwhichacertainsenseofaword
might appear; e.g. , play.v.01: participate in
games or sport occurs in the sentence frames “Somebody [play]s” and “Somebody [play]s something.” For each
verb-synset pair, we then consider the root hypernym of that
synset to reduce potential noise from WordNet’s ﬁne-grained
sense distinctions. The WordNet hierarchy for verbs is segmented and originates from over 100 root verbs. For example,
draw.v.01: cause to move by pulling traces
back to the root hypernym move.v.02: cause to
move or shift into a new position,
draw.v.02: get or derive traces to the root get.
v.01: come into the possession of some
thing concrete or abstract. We also include 20
hand-mapped rules, again to correct for WordNet’s lower
representation of concrete or spatial senses.
These mappings are not perfect and still contain some
ambiguity. Therefore, we send all our mappings along with
the top four alternative synsets for each term to AMT. We ask
workers to verify that our mapping was accurate and change
the mapping to an alternative one if it was a better ﬁt. We
present workers with the concept we want to canonicalize
along with our proposed corresponding synset with 4 additional options. To prevent workers from always defaulting to
the our proposed synset, we do not explicitly specify which
one of the 5 synsets presented is our proposed synset. Section 5.8 provides experimental precision and recall scores for
our canonicalization strategy.
Int J Comput Vis 123:32–73
5 Dataset Statistics and Analysis
In this section, we provide statistical insights and analysis for
each component of Visual Genome. Speciﬁcally, we examine the distribution of images (Sect. 5.1) and the collected
data for region descriptions (Sect. 5.2) and questions and
answers (Sect. 5.7). We analyze region graphs and scene
graphs together in one section (Sect. 5.6), but we also break
up these graph structures into their three constituent parts—
objects (Sect. 5.3), attributes (Sect. 5.4), and relationships
(Sect. 5.5)—and study each part individually. Finally, we
describe our canonicalization pipeline and results (Sect. 5.8).
5.1 Image Selection
The Visual Genome dataset consists of all 108,077 creative
commons images from the intersection of MS-COCO’s 328, 000 images and YFCC100M’s 100 million images. This allows Visual Genome
annotations to be utilized together with the YFCC tags and
MS-COCO’s segmentations and full image captions. These
images are real-world, non-iconic images that were uploaded
onto Flickr by users. The images range from as small as 72
pixels wide to as large as 1280 pixels wide, with an average width of 500 pixels. We collected the WordNet synsets
into which our 108,077 images can be categorized using the
same method as ImageNet . Visual Genome
images can be categorized into 972 ImageNet synsets. Note
that objects, attributes and relationships are categorized separately into more than 18K WordNet synsets (Sect. 5.8).
Figure 13 shows the top synsets to which our images belong.
“ski” is the most common synset, with 2612 images; it is
followed by “ballplayer” and “racket,” with all three synsets
referring to images of people playing sports. Our dataset is
somewhat biased towards images of people, as Fig. 13 shows;
however, they are quite diverse overall, as the top 25 synsets
each have over 800 images, while the top 50 synsets each
have over 500 examples.
5.2 Region Description Statistics
One of the primary components of Visual Genome is its
region descriptions. Every image includes an average of
50 regions with a bounding box and a descriptive phrase.
Figure 14 shows an example image from our dataset with its
50 region descriptions. We display bounding boxes for only
6 out of the 50 descriptions in the ﬁgure to avoid clutter.
These descriptions tend to be highly diverse and can focus
on a single object, like in “A bag,” or on multiple objects, like
in “Man taking a photo of the elephants.” They encompass
the most salient parts of the image, as in “An elephant taking
food from a woman,” while also capturing the background,
as in “Small buildings surrounded by trees.”
Fig. 13 A distribution of the top 25 image synsets in the Visual
Genome dataset. A variety of synsets are well represented in the dataset,
with the top 25 synsets having at least 800 example images each. Note
that an image synset is the label of the entire image according to the ImageNet ontology and are separate from the synsets for objects, attributes
and relationships
MS-COCO dataset is good at generating variations on a single scene-level descriptor. Consider
three sentences from MS-COCO dataset on a similar image:
“there is a person petting a very large elephant,” “a person touching an elephant in front of a wall,” and “a man
in white shirt petting the cheek of an elephant.” These three
sentences are single scene-level descriptions. In comparison,
Visual Genome descriptions emphasize different regions in
the image and thus are less semantically similar. To ensure
diversity in the descriptions, we use BLEU score thresholds between new descriptions and
all previously written descriptions. More information about
crowdsourcing can be found in Sect. 4.
Region descriptions must be speciﬁc enough in an image
to describe individual objects (e.g. “A bag”), but they must
also be general enough to describe high-level concepts in an
Int J Comput Vis 123:32–73
Fig. 14 a An example image from the dataset with its region descriptions. We only display localizations for 6 of the 50 descriptions to avoid
clutter; all 50 descriptions do have corresponding bounding boxes. b
All 50 region bounding boxes visualized on the image
image (e.g. “A man being chased by a bear”). Qualitatively,
we note that regions that cover large portions of the image
tend to be general descriptions of an image, while regions
that cover only a small fraction of the image tend to be more
speciﬁc. In Fig. 15a, we show the distribution of regions over
the width of the region normalized by the width of the image.
We see that the majority of our regions tend to be around 10
to 15% of the image width. We also note that there are a large
number of regions covering 100% of the image width. These
regions usually include elements like “sky,” “ocean,” “snow,”
“mountains,” etc. that cannot be bounded and thus span the
entireimagewidth.InFig.15b,weshowasimilardistribution
over the normalized height of the region. We see a similar
overall pattern, as most of our regions tend to be very speciﬁc
descriptions of about 10% to 15% of the image height. Unlike
the distribution over width, however, we do not see a increase
in the number of regions that span the entire height of the
image, as there are no common visual equivalents that span
images vertically. Out of all the descriptions gathered, only
one or two of them tend to be global scene descriptions that
are similar to MS-COCO (Fig. 17).
In Fig. 16, we show the distribution of the length (word
count) of these region descriptions. The average word count
for a description is 5 words, with a minimum of 1 and a maximum of 12 words. In Fig. 18a, we plot the most common
phrases occurring in our region descriptions, with common
stop words removed. Common visual elements like “green
grass,” “tree [in] distance,” and “blue sky” occur much more
often than other, more nuanced elements like “fresh strawberry.” We also study descriptions with ﬁner precision in
Fig. 18b, where we plot the most common words used in
descriptions. Again, we eliminate stop words from our study.
Colors like “white” and “black” are the most frequently used
words to describe visual concepts; we conduct a similar study
on other captioning datasets including MS-COCO and Flickr 30K and ﬁnd a similar
distribution with colors occurring most frequently. Besides
colors, we also see frequent occurrences of common objects
like “man” and “tree” and of universal visual elements like
Semantic Diversity We also study the actual semantic contents of the descriptions. We use an unsupervised approach to
analyze the semantics of these descriptions. Speciﬁcally, we
use word2vec’s pre-trained model on
Google news corpus to convert each word in a description to
a 300-dimensional vector. Next, we remove stop words and
average the remaining words to get a vector representation
of the whole region description. This pipeline is outlined
in Fig. 17. We use hierarchical agglomerative clustering
 on vector representations of each
region description and ﬁnd 71 semantic and syntactic groupings or “clusters.” Figure 19a shows four such example
clusters. One cluster contains all descriptions related to tennis, like “A man swings the racquet” and “White lines on
the ground of the tennis court,” while another cluster contains descriptions related to numbers, like “Three dogs on the
street” and “Two people inside the tent.” To quantitatively
measure the diversity of Visual Genome’s region descriptions, we calculate the number of clusters represented in a
single image’s region descriptions. We show the distribution
of the variety of descriptions for an image in Fig. 19b. We
ﬁnd that on average, each image contains descriptions from
17differentclusters.Theimagewiththeleastdiversedescriptions contains descriptions from 4 clusters, while the image
Int J Comput Vis 123:32–73
Fig. 15 a A distribution of the width of the bounding box of a region description normalized by the image width. b A distribution of the height of
the bounding box of a region description normalized by the image height
Fig. 16 A distribution of the number of words in a region description.
The average number of words in a region description is 5, with shortest
descriptions of 1 word and longest descriptions of 16 words
with the most diverse descriptions contains descriptions from
26 clusters.
Finally, we also compare the descriptions in Visual
Genome to the captions in MS-COCO. First we aggregate
all Visual Genome and MS-COCO descriptions and remove
all stop words. After removing stop words, the descriptions
from both datasets are roughly the same length. We conduct
a similar study, in which we vectorize the descriptions for
each image and calculate each dataset’s cluster diversity per
image. We ﬁnd that on average, 2 clusters are represented
in the captions for each image in MS-COCO, with very few
images in which 5 clusters are represented. Because each
image in MS-COCO only contains 5 captions, it is not a fair
comparison to compare the number of clusters represented
in all the region descriptions in the Visual Genome dataset.
We thus randomly sample 5 Visual Genome region descriptions per image and calculate the number of clusters in an
image. We ﬁnd that Visual Genome descriptions come from
4 or 5 clusters. We show our comparison results in Fig. 19c.
The difference between the semantic diversity between the
Fig. 17 The process used to convert a region description into a 300dimensional vectorized representation
two datasets is statistically signiﬁcant (t = −240, p < 0.01)
(Fig. 20).
5.3 Object Statistics
In comparison to related datasets, Visual Genome fares
well in terms of object density and diversity (Table 3).
Visual Genome contains approximately 35 objects per
image, exceeding ImageNet , PASCAL
 , MS-COCO , and
other datasets by large margins. As shown in Fig. 21, there
are more object categories represented in Visual Genome
than in any other dataset. This comparison is especially pertinent with regards to Microsoft MS-COCO ,
which uses the same images as Visual Genome. The lower
count of objects per category is a result of our higher number
of categories. For a fairer comparison with ILSVRC 2014
Detection , Visual Genome has
about 2239 objects per category when only the top 200 categories are considered, which is comparable to ILSVRC’s
2671.5 objects per category. For a fairer comparison with
MS-COCO, Visual Genome has about 3768 objects per cat-
Int J Comput Vis 123:32–73
Fig. 18 a A plot of the most common visual concepts or phrases that
occur in region descriptions. The most common phrases refer to universal visual concepts like “blue sky,” “green grass,” etc. b A plot of the
most frequently used words in region descriptions. Each word is treated
as an individual token regardless of which region description it came
from. Colors occur the most frequently, followed by common objects
like man and dog and universal visual concepts like “sky”
egory when only the top 80 categories are considered. This
is comparable to MS-COCO’s object distribution.
The 3,843,636 objects in Visual Genome come from a
variety of categories. As shown in Fig. 22 (b), objects related
to WordNet categories such as humans, animals, sports, and
Int J Comput Vis 123:32–73
Numbers Cluster
Two people inside the tent.
Many animals crossing the road.
Five ducks almost in a row.
The number four.
Three dogs on the street.
Two towels hanging on racks.
Tennis Cluster
White lines on the ground of the tennis court.
A pair of tennis shoes.
Metal fence securing the tennis court.
Navy blue shorts on tennis player.
The man swings the racquet.
Tennis player preparing a backhand swing.
Ocean Cluster
Ocean is blue and calm.
Rows of waves in front of surfer.
A group of men on a boat.
Surfboard on the beach.
Woman is surfing in the ocean.
Foam on water’s edge.
Transportation Cluster
Ladder folded on fire truck.
Dragon design on the motorcycle.
Tall windshield on bike.
Front wheels of the airplane.
A bus rear view mirror.
The front tire of the police car.
Fig. 19 a Example illustration showing four clusters of region descriptions and their overall themes. Other clusters not shown due to limited
space. b Distribution of images over number of clusters represented
in each image’s region descriptions. c We take Visual Genome with
5 random descriptions taken from each image and MS-COCO dataset
with all 5 sentence descriptions per image and compare how many clusters are represented in the descriptions. We show that Visual Genome’s
descriptions are more varied for a given image, with an average of 4
clusters per image, while MS-COCO’s images have an average of 2
clusters per image
scenery are most common; this is consistent with the general bias in image subject matter in our dataset. Common
objects like man, person, and woman occur especially
frequently with occurrences of 24K, 17K, and 11K. Other
objects that also occur in MS-COCO 
are also well represented with around 5000 instances on
average. Figure 22a shows some examples of objects in
images. Objects in Visual Genome span a diverse set of
Wordnet categories like food, animals, and man-made structures.
Int J Comput Vis 123:32–73
Fig. 20 a Distribution of the number of objects per region. Most
regions have between 0 and 2 objects. b Distribution of the number
of objects per image. Most images contain between 15 and 20 objects
It is important to look not only at what types of objects
we have but also at the distribution of objects in images and
regions. Figure 20a shows, as expected, that we have between
0 and 2 objects in each region on average. It is possible for
regions to contain no objects if their descriptions refer to no
explicit objects in the image. For example, a region described
as “it is dark outside” has no objects to extract. Regions with
only one object generally have descriptions that focus on the
attributes of a single object. On the other hand, regions with
two or more objects generally have descriptions that contain
Number of Categories
Instances per Category
ImageNet Detection
Visual Genome
(all objects)
PASCAL Detection
Zitnick Abstract Scenes
Caltech 101
Caltech 256
Caltech Pedestrian
Visual Genome
(top 80 objects)
Fig. 21 Comparison of object diversity between various datasets.
Visual Genome far surpasses other datasets in terms of number of categories. When considering only the top 80 object categories, it contains
a comparable number of objects as MS-COCO. The dashed line is a
visual aid connecting the two Visual Genome data points
both attributes of speciﬁc objects and relationships between
pairs of objects.
As shown in Fig. 20b, each image contains on average
around 35 distinct objects. Few images have an extremely
high number of objects (e.g. over 40). Due to the image biases
that exist in the dataset, we have twice as many annotations
for men than we do of women.
5.4 Attribute Statistics
Attributes allow for detailed description and disambiguation
ofobjectsinourdataset.Ourdatasetcontains2.8milliontotal
attributes with 68,111 unique attributes. Attributes include
colors (e.g. green), sizes (e.g. tall), continuous action
verbs (e.g. standing), materials (e.g. plastic), etc.
Each object can have multiple attributes.
On average, each image in Visual Genome contains 26
attributes (Fig. 23). Each region contains on average 1
attribute, though about 34% of regions contain no attribute at
all; this is primarily because many regions are relationshipfocused. Figure 24a shows the distribution of the most
common attributes in our dataset. Colors (e.g. white,
green) are by far the most frequent attributes. Also common are sizes (e.g. large) and materials (e.g. wooden).
Figure 24b shows the distribution of attributes describing
Table 3 Comparison of Visual Genome objects and categories to related datasets
ILSVRC det.
 
MS-COCO 
Caltech101
 
Abstract scenes
 
Total objects
Total categories 33,877
Objects per
Int J Comput Vis 123:32–73
Fig. 22 a Examples of objects in Visual Genome. Each object is localized in its image with a tightly drawn bounding box. b Plot of the most
frequently occurring objects in images. People are the most frequently
occurring objects in our dataset, followed by common objects and visual
elements like building, shirt, and sky
people (e.g. man, girls, and person). The most common
attributes describing people are intransitive verbs describing their states of motion (e.g. standing and walking).
Certain sports (e.g. skiing, surfboarding) are overrepresented due to an image bias towards these sports.
Attribute Graphs We also qualitatively analyze the attributes
in our dataset by constructing co-occurrence graphs, in which
nodes are unique attributes and edges connect those attributes
that describe the same object. For example, if an image contained a “large black dog” (large(dog), black(dog)) and
another image contained a “large yellow cat” (large(cat),
yellow(cat)), its attributes would form an incomplete
graph with edges (large, black) and (large, yellow).
We create two such graphs: one for both the total set of
attributes and a second where we consider only objects that
Int J Comput Vis 123:32–73
Fig. 23 Distribution of the number of attributes a per image, b per
region description, c per object
refer to people. A subgraph of the 16 most frequently connected (co-occurring) person-related attributes is shown in
Cliques in these graphs represent groups of attributes in
which at least one co-occurrence exists for each pair of
attributes. In the previous example, if a third image contained
a“blackandyellowtaxi”(black(taxi),yellow(taxi)),
the resulting third edge would create a clique between the
attributes black, large, and yellow. When calculated
across the entire Visual Genome dataset, these cliques provide insight into commonly perceived traits of different types
of objects. Figure 25b is a selected representation of three
example cliques and their overlaps. From just a clique of
attributes, we can predict what types of objects are usually
referenced. In Fig. 25b, we see that these cliques describe an
animal (left), water body (top right), and human hair (bottom
Other cliques (not shown) can also uniquely identify
object categories. In our set, one clique contains athletic,
young, fit, skateboarding, focused, teenager,
male, skinny, and happy, capturing some of the common
traits of skateboarders in our set. Another such clique
has shiny, small, metal, silver, rusty, parked,
and empty, most likely describing a subset of cars. From
these cliques, we can thus infer distinct objects and object
types based solely on their attributes, potentially allowing for
highly speciﬁc object identiﬁcation based on selected characteristics.
5.5 Relationship Statistics
Relationships are the core components that link objects in
our scene graphs. Relationships are directional, i.e. they
involve two objects, one acting as the subject and one as
the object of a predicate relationship. We denote all relationships in the form relationship(subject, object). For example,
if a man is swinging a bat, we write swinging(man,
bat). Relationships can be spatial (e.g. inside_of), action
(e.g. swinging), compositional (e.g. part_of), etc.
More complex relationships such as standing_on, which
includes both an action and a spatial aspect, are also represented. Relationships are extracted from region descriptions
by crowd workers, similarly to attributes and objects. Visual
Genome contains a total of 42,374 unique relationships, with
over 2,347,187 million total relationships.
Figure 26a shows the distribution of relationships per
region description. On average, we have 1 relationship per
region, with a maximum of 7. We also have some descriptions like “an old, tall man,” which have multiple attributes
associated with the man but no relationships. Figure 26b
is a distribution of relationships per image object. Finally,
Fig. 26c shows the distribution of relationships per image.
Each image has an average of 19 relationships, with a minimum of 1 relationship and with a maximum of over 80
relationships.
Top Relationship Distributions We display the most frequently occurring relationships in Fig. 27a. on is the
most common relationship in our dataset. This is primarily because of the ﬂexibility of the word on, which can
refer to spatial conﬁguration (on top of), attachment
(hanging on), etc. Other common relationships involve
actions like holding and wearing and spatial conﬁgu-
Int J Comput Vis 123:32–73
Fig. 24 a Distribution showing the most common attributes in the
dataset. Colors (e.g. white, red) and materials (e.g. wooden,
metal) are the most common. b Distribution showing the number of
attributes describing people. State-of-motion verbs (e.g. standing,
walking) are the most common, while certain sports (e.g. skiing,
surfing) are also highly represented due to an image source bias in
our image set
rations like behind, next to, and under. Figure 27b
shows a similar distribution but for relationships involving people. Here we notice more human-centric relationships or actions such as kissing, chatting with, and
talking to. The two distributions follow a Zipf distribution.
Understanding Affordances Relationships allow us to also
understand the affordances of objects. Figure 28a shows
the distribution for subjects while Fig. 28b shows a similar
distribution for objects. Comparing the two, we ﬁnd clear
patterns of people-like subject entities such as person,
man, policeman, boy, and skateboarder that can
Int J Comput Vis 123:32–73
light brown
Fig. 25 a Graph of the person-describing attributes with the most cooccurrences. Edge thickness represents the frequency of co-occurrence
of the two nodes. b A subgraph showing the co-occurrences and intersections of three cliques, which appear to describe water (top right), hair
(bottom right), and some type of animal (left). Edges between cliques
have been removed for clarity
ride other objects; the other distribution contains objects
that afford riding, such as horse, bike, elephant,
motorcycle, and skateboard. We can also learn speciﬁc common-sense knowledge, like that zebras eat hay
and grass while a person eats pizzas and burgers
and that couches usually have pillows on them.
Int J Comput Vis 123:32–73
Fig. 26 Distribution of relationships a per image region, b per image
object, c per image
Related Work Comparison It is also worth mentioning in
this section some prior work on relationships. The concept of visual relationships has already been explored in
Visual Phrases , who introduced a
dataset of 17 such relationships such as next_to(person, bike)
and riding(person, horse). However, their dataset is limited
to just these 17 relationships. Similarly, the MS-COCO-a
a scene graph dataset introduced 156 actions that humans performed in MS-COCO’s
dataset . They show that to exhaustively
describe “common” images involving humans, only a small
set of visual actions is needed. However, their dataset is
limited to just actions, while our relationships are more general and numerous, with over 42,374 unique relationships.
Finally, VisKE introduced 6500 relationships, but in a much smaller dataset of images than Visual
5.6 Region and Scene Graph Statistics
We introduce in this paper the largest dataset of scene graphs
to date. We use these graph representations of images as a
deeper understanding of the visual world. In this section, we
analyze the properties of these representations, both at the
region-level through region graphs and at the image level
through scene graphs. We also brieﬂy explore other datasets
with scene graphs and provide aggregate statistics on our
entire dataset.
In previous work, scene graphs have been collected by
asking humans to write a list of triples about an image
 . However, unlike them, we collect
graphs at a much more ﬁne-grained level: the region graph.
We obtained our graphs by asking workers to create them
from the descriptions we collected from our regions. Therefore, we end up with multiple graphs for an image, one for
every region description. Together, we can combine all the
individual region graphs to aggregate a scene graph for an
image. This scene graph is made up of all the individual
region graphs. In our scene graph representation, we merge
all the objects that referenced by multiple region graphs into
one node in the scene graph.
Each of our images has between 5 to 100 region graphs
per image, with an average of 50. Each image has exactly one
scene graph. Note that the number of region descriptions and
the number of region graphs for an image are not the same.
For example, consider the description “it is a sunny day”.
Such a description contains no objects, which are the building
blocks of a region graph. Therefore, such descriptions have
no region graphs associated with them.
Objects, attributes, and relationships occur as a normal
distribution in our data. Table 4 shows that in a region graph,
there are an average of 0.71 objects, 0.52 attributes, and 21
relationships.Eachscenegraphandconsequentlyeachimage
has average of 35 objects, 26 attributes, and 21 relationships.
5.7 Question Answering Statistics
We collected 1,773,258 question answering (QA) pairs on
the Visual Genome images. Each pair consists of a question
and its correct answer regarding the content of an image. On
average, every image has 17 QA pairs. Rather than collecting unconstrained QA pairs as previous work has done , each
Int J Comput Vis 123:32–73
Fig. 27 a A sample of the most frequent relationships in our dataset. In
general, the most common relationships are spatial (on top of, on
side of, etc.). b A sample of the most frequent relationships involving humans in our dataset. The relationships involving people tend to
be more action oriented (walk, speak, run, etc.)
question in Visual Genome starts with one of the six Ws –
what, where, when, who, why, and how. There are two major
beneﬁts to focusing on six types of questions. First, they
offer a considerable coverage of question types, ranging from
basicperceptualtasks(e.g.recognizingobjectsandscenes)to
complex common sense reasoning (e.g. inferring motivations
of people and causality of events). Second, these categories
present a natural and consistent stratiﬁcation of task difﬁculty, indicated by the baseline performance in Sect. 6.4. For
instance, why questions that involve complex reasoning lead
to the poorest performance (3.4% top-100 accuracy compared to 9.6% top-100 accuracy of the next lowest) of the six
categories. This enables us to obtain a better understanding
of the strengths and weaknesses of today’s computer vision
models, which sheds light on future directions in which to
Int J Comput Vis 123:32–73
Fig. 28 a Distribution of subjects for the relationship riding. b Distribution of objects for the relationship riding. Subjects comprise
of people-like entities like person, man, policeman, boy, and
skateboarder that can ride other objects. On the other hand, objects
like horse, bike, elephant and motorcycle are entities that can
afford riding
We now analyze the diversity and quality of our questions
and answers. Our goal is to construct a large-scale visual
question answering dataset that covers a diverse range of
question types, from basic cognition tasks to complex reasoning tasks. We demonstrate the richness and diversity of
our QA pairs by examining the distributions of questions and
answers in Fig. 29.
QuestionTypeDistributions Thequestionsnaturallyfallinto
the 6W categories via their interrogative words. Inside each
of the categories, the second and following words categorize the questions with increasing granularity. Inspired by
VQA , we show the distributions of the
questions by their ﬁrst three words in Fig. 30. We can see that
“what” is the most common of the six categories. A notable
Int J Comput Vis 123:32–73
Table 4 The average number of objects, attributes, and relationships
per region graph and per scene graph
Attributes
Relationships
Region graph
Scene graph
difference between our question distribution and VQA’s is
that we focus on ensuring that all six question categories
are adequately represented, while in VQA, 38.37% of the
questions are yes/no binary questions. As a result, a trivial
model can achieve a reasonable performance by just predicting “yes” or “no” as answers. We encourage more difﬁcult
QA pairs by ruling out binary questions.
Question and Answer Length Distributions We also analyze the question and answer lengths of each 6W category.
Figure 31 shows the average question and answer lengths
of each category. Overall, the average question and answer
lengths are 5.7 and 1.8 words respectively. In contrast to
the VQA dataset, where 89.32%, 6.91%, and 2.74% of the
answers consist of one, two, or three words, our answers
exhibit a long-tail distribution where 57.3%, 18.1%, and
15.7% of the answers have one, two, or three words respectively. We avoid verbosity by instructing the workers to
write answers as concisely as possible. The coverage of long
answers means that many answers contain a short description that contains more details than merely an object or
an attribute. It shows the richness and complexity of our
visual QA tasks beyond object-centric recognition tasks.
We foresee that these long-tail answers can motivate future
Fig. 30 Distribution of question types by starting words. This ﬁgure
shows the distribution of the questions by their ﬁrst three words. The
angles of the regions are proportional to the number of pairs from the
corresponding categories. We can see that “what” questions are the
largest category with nearly half of the QA pairs
research in common-sense reasoning and high-level image
understanding.
5.8 Canonicalization Statistics
In order to reduce the ambiguity in the concepts of our dataset
and connect it to other resources used by the research community,wecanonicalizethesemanticmeaningsofallobjects,
Fig. 29 Example QA pairs in the Visual Genome dataset. Our QA pairs cover a spectrum of visual tasks from recognition to high-level reasoning
Int J Comput Vis 123:32–73
Fig. 31 Question and answer lengths by question type. The bars show
the average question and answer lengths of each question type. The
whiskers show the standard deviations. The factual questions, such
as “what” and “how” questions, usually come with short answers of
a single object or a number. This is only because “how” questions
are disproportionately counting questions that start with “how many”.
Questions from the “where” and “why” categories usually have phrases
and sentences as answers
relationships, and attributes in Visual Genome. By “canonicalization,” we refer to word sense disambiguation (WSD)
by mapping the components in our dataset to their respective
synsetsintheWordNetontology(Miller1995).Thismapping
reduces the noise in the concepts contained in the dataset and
also facilitates the linkage between Visual Genome and other
data sources such as ImageNet , which is
built on top of the WordNet ontology.
Figure 32 shows an example image from the Visual
components
canonicalized.
For example, horse is canonicalized as horse.n.01:
solid-hoofed herbivorous quadruped
domesticated since prehistoric times.
attribute, clydesdale, is canonicalized as its breed
clydesdale.n.01: heavy feathered-legged
breed of draft horse originally from
Scotland. We also show an example of a QA from which
we extract the nouns shamrocks, symbol, and St.
Patrick’s day, all of which we canonicalize to Word-
Net as well.
Related Work Canonicalization, or WSD , has been used in numerous applications, including
machine translation, information retrieval, and information
extraction . In
English sentences, sentences like “He scored a goal” and “It
was his goal in life” carry different meanings for the word
“goal.” Understanding these differences is crucial for translating languages and for returning correct results for a query.
Similarly, in Visual Genome, we ensure that all our components are canonicalized to understand how different objects
are related to each other; for example, “person” is a hypernym
of “man” and “woman.” Most past canonicalization models
use precision, recall, and F1 score to evaluate on the Semeval
dataset . The current state-of-the-art
performance on Semeval is an F1 score of 75.8% . Since our canonicalization setup is different from the
Semeval benchmark (we have an open vocabulary and no
annotated ground truth for evaluation), our canonicalization
Fig. 32 An example image from the Visual Genome dataset with
its region descriptions, QA pairs, objects, attributes, and relationships canonicalized. The large text boxes are WordNet synsets referenced by this image. For example, the carriage is mapped
to carriage.n.02: a vehicle with wheels drawn by
one or more horses. We do not show the bounding boxes for
the objects in order to allow readers to see the image clearly. We also
only show a subset of the scene graph for this image to avoid cluttering
Int J Comput Vis 123:32–73
Table5 Precision,recall,andmappingaccuracypercentagesforobject,
attribute, and relationship canonicalization
Attributes
Relationships
method is not directly comparable to these existing methods.
We do however, achieve a similar precision and recall score
on a held-out test set described below (Table 5).
Region Descriptions and QAs We canonicalize all objects
mentioned in all region descriptions and QA pairs. Because
objects need to be extracted from the phrase text, we use
Stanford NLP tools to extract the
noun phrases in each region description and QA, resulting in 99% recall of noun phrases from a subset of 200
region descriptions we manually annotated. After obtaining
the noun phrases, we map each to its most frequent matching
synset (according to WordNet lexeme counts). This resulted
in an overall mapping accuracy of 88% and a recall of 98.5%
(Fig. 5). The most common synsets extracted from region
descriptions, QAs, and objects are shown in Fig. 33.
Attributes We canonicalize attributes from the crowd-extracted attributes present in our scene graphs. The “attribute”
designation encompasses a wide range of grammatical
parts of speech. Because part-of-speech taggers rely on
high-level syntax information and thus fail on the disjoint elements of our scene graphs, we normalize each
attribute based on morphology alone ). Then, as with objects, we map each
attribute phrase to the most frequent matching WordNet
synset. We include 15 hand-mapped rules to address common failure cases in which WordNet’s frequency counts
prefer abstract senses of words over the spatial senses
present in visual data, e.g. short.a.01: limited in
duration over short.a.02: lacking in length. For veriﬁcation, we randomly sample 200 attributes,
produce ground-truth mappings by hand, and compare them
to the results of our algorithm. This resulted in a recall of
95.9% and a mapping accuracy of 85.7%. The most common attribute synsets are shown in Fig. 34a.
Relationships As with attributes, we canonicalize the relationships isolated in our scene graphs. We exclude prepositions, which are not recognized in WordNet, leaving a
set primarily composed of verb relationships. Since the
meanings of verbs are highly dependent upon their morphology and syntactic placement (e.g. passive cases, prepositional phrases), we map the structure of each relationship to the appropriate WordNet sentence frame and only
consider those WordNet synsets with matching sentence
frames. For each verb-synset pair, we then consider the
root hypernym of that synset to reduce potential noise
from WordNet’s ﬁne-grained sense distinctions. We also
include 20 hand-mapped rules, again to correct for Word-
Net’s lower representation of concrete or spatial senses;
for example, the concrete hold.v.02: have or hold
in one’s hand or grip is less frequent in WordNet
than the abstract hold.v.01: cause to continue
in a certain state. For veriﬁcation, we again randomly sample 200 relationships and compare the results of
our canonicalization against ground-truth mappings. This
resulted in a recall of 88.5% and a mapping accuracy of
92.9%. While several datasets, such as VerbNet and FrameNet , include semantic
restrictions or frames to improve classiﬁcation, there is no
comprehensive method of mapping to those restrictions or
frames. The most common relationship synsets are shown in
6 Experiments
Thus far, we have presented the Visual Genome dataset and
analyzed its individual components. With such rich information provided, numerous perceptual and cognitive tasks can
be tackled. In this section, we aim to provide baseline experimental results using components of Visual Genome that have
not been extensively studied.
Object detection is already a well-studied problem . Similarly, region
graphs and scene graphs have been shown to improve semantic image retrieval . We therefore focus on the remaining components, i.e.
attributes, relationships, region descriptions, and question
answer pairs.
In Sect. 6.1, we present results for two experiments on
attribute prediction. In the ﬁrst, we treat attributes independently from objects and train a classiﬁer for each attribute,
i.e. a classiﬁer for red or a classiﬁer for old, as in Malisiewicz et al. , Varma and Zisserman ,Ferrari
and Zisserman , Farhadi et al. and Johnson
et al. . In the second experiment, we learn object and
attribute classiﬁers jointly and predict object-attribute pairs
(e.g. predicting that an apple is red), as in Sadeghi and
Farhadi .
In Sect. 6.2, we present two experiments on relationship prediction. In the ﬁrst, we aim to predict the predicate
between two objects, e.g. predicting the predicate kicking
or wearing between two objects. This experiment is synonymous with existing work in action recognition . In another experiment,
Int J Comput Vis 123:32–73
Fig. 33 Distribution of the 25 most common synsets mapped from the words and phrases extracted from region descriptions which represent
objects in a region descriptions and question answers and b objects
we study relationships by classifying jointly the objects and
the predicate (e.g. predicting kicking(man, ball)); we show
that this is a very difﬁcult task due to the high variability in
the appearance of a relationship (e.g. the ball might be on
the ground or in mid-air above the man). These experiments
are generalizations of tasks that study spatial relationships
between objects and ones that jointly reason about the interaction of humans with objects .
In Sect. 6.3 we present results for region captioning.
This task is closely related to image captioning ; however, results from the two are not directly
comparable, as region descriptions are short, incomplete
sentences. We train one of the top 16 state-of-the-art
image caption generators on
(1) our dataset to generate region descriptions and on
(2) Flickr30K to generate sentence
descriptions. To compare results between the two train-
Int J Comput Vis 123:32–73
Fig. 34 Distribution of the 25 most common synsets mapped from a attributes and b relationships
ing approaches, we use simple templates to convert region
descriptions into complete sentences. For a more robust evaluation, we validate the descriptions we generate using human
Finally, in Sect. 6.4, we experiment on visual question
answering, i.e. given an image and a question, we attempt
to provide an answer for the question. We report results on
the retrieval of the correct answer from a list of existing
6.1 Attribute Prediction
Attributes are becoming increasingly important in the ﬁeld
of computer vision, as they offer higher-level semantic cues
for various problems and lead to a deeper understanding
of images. We can express a wide variety of properties through attributes, such as form (sliced), function
(decorative), sentiment (angry), and even intention
(helping). Distinguishing between similar objects 123:32–73
et al. 2015) leads to ﬁner-grained classiﬁcation, while
describing a previously unseen class through attributes
shared with known classes can enable “zero-shot” learning . Visual Genome
is the largest dataset of attributes, with 26 attributes per image
for more than 2.8 million attributes.
Setup For both experiments, we focus on the 100 most common attributes in our dataset. We only use objects that occur
at least 100 times and are associated with one of the 100
attributes in at least one image. For both experiments, we
follow a similar data pre-processing pipeline. First, we lowercase, lemmatize , and strip excess whitespace
fromallattributes.Sincethenumberofexamplesperattribute
class varies, we randomly sample 500 attributes from each
category 
pre-trained no ImageNet and ﬁne-tune it for both of these
experiments using the 50, 000 attribute and 43, 000 objectattribute pair instances respectively. We modify the network
so that the learning rate of the ﬁnal fully-connected layer
is 10 times that of the other layers, as this improves convergence time. Convergence is measured as the performance
on the validation set. We use a base learning rate of 0.001,
which we scale by 0.1 every 200 iterations, and momentum
and weight decays of 0.9 and 0.0005 respectively. We use
the ﬁne-tuned features from the network and train 100 individual SVMs to predict each attribute.
We output multiple attributes for each bounding box input.
For the second experiment, we also output the object class.
Results Table 6 shows results for both experiments. For
the ﬁrst experiment on attribute prediction, we converge
after around 700 iterations with 18.97% top-one accuracy
and 43.11% top-ﬁve accuracy. Thus, attributes (like objects)
are visually distinguishable from each other. For the second experiment where we also predict the object class, we
converge after around 400 iterations with 43.17% top-one
accuracy and 71.97% top-ﬁve accuracy. Predicting objects
jointly with attributes increases the top-one accuracy from
18.97% to 43.17%. This implies that some attributes occur
exclusively with a small number of objects. Additionally, by
Table 6 (First row) Results for the attribute prediction task where we
only predict attributes for a given image crop. (Second row) Attributeobject prediction experiment where we predict both the attributes as
well as the object from a given crop of the image
Top-1 accuracy (%)
Top-5 accuracy (%)
Object-attribute
jointly learning attributes with objects, we increase the interclass variance, making the classiﬁcation process an easier
Figure35ashowsexamplepredictionsfortheﬁrstattribute
prediction experiment. In general, the model is good at associating objects with their most salient attributes, for example,
animal with stuffed and elephant with grazing.
However, the crowdsourced ground truth answers sometimes
do not contain all valid attributes, so the model is incorrectly
penalized for some accurate/true predictions. For example,
the white stuffed animal is correct but evaluated as incorrect.
Figure 35b shows example predictions for the second
experiment in which we also predict the object. While the
results in the second row might be considered correct, to
keep a consistent evaluation, we mark them as incorrect. For
example, the predicted “green grass” might be considered
subjectively correct even though it is annotated as “brown
grass”. For cases where the objects are not clearly visible but
are abstract outlines, our model is unable to predict attributes
or objects accurately. For example, it thinks that the “ﬂying
bird” is actually a “black jacket”.
The attribute clique graphs in Sect. 5.4 clearly show that
learning attributes can help us identify types of objects. This
experiment strengthens that insight. We learn that studying
attributes together with objects can improve attribute prediction.
6.2 Relationship Prediction
While objects are the core building blocks of an image,
relationships put them in context. These relationships help
distinguish between images that contain the same objects but
have different holistic interpretations. For example, an image
of “a man riding a bike” and “a man falling off a bike” both
contain man and bike, but the relationship (riding vs.
falling_off) changes how we perceive both situations.
Visual Genome is the largest known dataset of relationships,
with more than 2.3 million relationships and an average of
21 relationships per image.
Setup The setups of both experiments are similar to those of
the experiments we performed on attributes. We again focus
Int J Comput Vis 123:32–73
Fig. 35 a Example predictions from the attribute prediction experiment. Attributes in the ﬁrst row are predicted correctly, those in the
second row differ from the ground truth but still correctly classify
an attribute in the image, and those in the third row are classiﬁed
incorrectly. The model tends to associate objects with attributes (e.g.
elephant with grazing). b Example predictions from the joint
object-attribute prediction experiment
on the top 100 most frequent relationships. We lowercase,
lemmatize , and strip excess whitespace from all
relationships. We end up with around 34, 000 unique relationshiptypesand27, 000 uniquesubject-relationship-object
triples for training, validation, and testing. The input data to
the experiment is the image region containing the union of
the bounding boxes of the subject and object (essentially, the
bounding box containing the two object boxes). We ﬁne-tune
a 16-layer VGG network 
with the same learning rates mentioned in Sect. 6.1.
Results Overall, we ﬁnd that relationships are only slightly
visually distinct enough for our discriminative model to learn
effectively. Table 7 shows results for both experiments. For
relationship classiﬁcation, we converge after around 800 iterations with 8.74% top-one accuracy and 29.69% top-ﬁve
accuracy. Unlike attribute prediction, the accuracy results for
relationships are much lower because of the high intra-class
variability of most relationships. For the second experiment
jointly predicting the relationship and its two object classes,
we converge after around 450 iterations with 25.83% top-one
accuracy and 65.57% top-ﬁve accuracy. We notice that object
classiﬁcation aids relationship prediction. Some relationships occur with some objects and never others; for example,
the relationship drive only occurs with the object person
and never with any other objects (dog, chair, etc.).
Table 7 Results for relationship classiﬁcation (ﬁrst row) and joint classiﬁcation (second row) experiments
Top-1 accuracy (%)
Top-5 accuracy (%)
Relationship
Sub./Rel./Obj.
Figure 36a shows example predictions for the relationship
classiﬁcation experiment. In general, the model associates
object categories with certain relationships (e.g. animals with
eating or drinking, bikes with riding, and kids with
Figure 36b, structured as in Fig. 36a, shows example
predictions for the joint prediction of relationships with its
objects. The model is able to predict the salient features of the
image (e.g. “boat in water”) but fails to distinguish between
different objects (e.g. boy vs. woman and car vs. bus in
the bottom row).
6.3 Generating Region Descriptions
Generating sentence descriptions of images has gained popularity as a task in computer vision ;
however, current state-of-the-art models fail to describe all
Int J Comput Vis 123:32–73
Fig. 36 a Example predictions from the relationship prediction experiment. Relationships in the ﬁrst row are predicted correctly, those in
the second row differ from the ground truth but still correctly classify a
relationship in the image, and those in the third row are classiﬁed incorrectly. The model learns to associate animals leaning towards the ground
as eating or drinking and bikes with riding. b Example predictions from the relationship-objects prediction experiment. The ﬁgure is
organized in the same way as a. The model is able to predict the salient
features of the image but fails to distinguish between different objects
(e.g. boy and woman and car and bus in the bottom row)
the different events captured in an image and instead provide
only a high-level summary of the image. In this section, we
test how well state-of-the-art models can caption the details
of images. For both experiments, we use the NeuralTalk
model , since it not only provides state-of-the-art results but also is shown to be robust
enoughforpredictingshortdescriptions.WetrainNeuralTalk
on the Visual Genome dataset for region descriptions and on
Flickr30K for full sentence descriptions.
Asamodeltrainedonotherdatasetswouldgeneratecomplete
sentences and would not be comparable 
to our region descriptions, we convert all region descriptions
generated by our model into complete sentences using predeﬁned templates .
Setup Fortraining,webeginbypreprocessingregiondescriptions; we remove all non-alphanumeric characters and lowercase and strip excess whitespace from them. We have
5,406,939 region descriptions in total. We end up with
3, 784, 857 region descriptions for training – 811, 040 each
for validation and testing. Note that we ensure descriptions of regions from the same image are exclusively in the
training, validation, or testing set. We feed the bounding
boxes of the regions through the pretrained VGG 16-layer
network to get the 4096dimensional feature vectors of each region. We then use the
NeuralTalk model to train a
long short-term memory (LSTM) network to generate descriptions of regions. We
use a learning rate of 0.001 trained with rmsprop . The model converges after four days.
For testing, we crop the ground-truth region bounding
boxes of images and extract their 4096-dimensional 16-layer
VGG network features. We
then feed these vectors through the pretrained NeuralTalk
model to get predictions for region descriptions.
Results Table 8 shows the results for the experiment. We
calculate BLEU , CIDEr , and METEOR 
scores between the generated descriptions and their ground-truth descriptions. In all cases, the
model trained on VisualGenome performs better. Moreover,
we asked crowd workers to evaluate whether a generated
description was correct—we got 1.6 and 43.03% for models
trained on Flickr30K and on Visual Genome, respectively.
The large increase in accuracy when the model trained on
our data is due to the speciﬁcity of our dataset. Our region
Int J Comput Vis 123:32–73
Table 8 Results for the region
description generation
experiment
Scores in the ﬁrst row are for the region descriptions generated from the NeuralTalk model trained on Flickr8K,
and those in the second row are for those generated by the model trained on Visual Genome data. BLEU,
CIDEr, and METEOR scores all compare the predicted description to a ground truth in different ways
Fig. 37 Example predictions from the region description generation
experiment by a model trained on Visual Genome region descriptions.
Regions in the ﬁrst column (left) accurately describe the region, and
those in the second column (right) are incorrect and unrelated to the
corresponding region
descriptions are shorter and cover a smaller image area. In
comparison, the Flickr30K data are generic descriptions of
entire images with multiple events happening in different
regions of the image. The model trained on our data is able
to make predictions that are more likely to concentrate on the
speciﬁc part of the image it is looking at, instead of generating a summary description. The objectively low accuracy in
both cases illustrates that current models are unable to reason
about complex images.
Figure 37 shows examples of regions and their predicted
descriptions. Since many examples have short descriptions,
the predicted descriptions are also short as expected; however, this causes the model to fail to produce more descriptive
phrases for regions with multiple objects or with distinctive
objects (i.e. objects with many attributes). While we use templates to convert region descriptions into sentences, future
work can explore smarter approaches to combine region
descriptions and generate a paragraph connecting all the
regions into one coherent description.
6.4 Question Answering
Visual Genome is currently the largest dataset of visual question answers with more than 1.7 million question and answer
pairs. Each of our 108,077 images contains an average of 17
questionanswerpairs.Answeringquestionsrequiresadeeper
understanding of an image than generic image captioning.
Question answering can involve ﬁne-grained recognition
(e.g. “What is the breed of the dog?”), object detection
(e.g. “Where is the kite in the image?”), activity recognition
(e.g. “What is this man doing?”), knowledge base reasoning
(e.g. “Is this glass full?”), and common-sense reasoning (e.g.
“What street will we be on if we turn right?”).
By leveraging the detailed annotations in the scene graphs
in Visual Genome, we envision building smart models that
can answer a myriad of visual questions. While we encourage
the construction of smart models, in this paper, we provide
some baseline results to help others compare their models.
Setup We split the QA pairs into a training set (60%) and
a test set (40%). We ensure that all images are exclusive to
either the training set or the test set. We implement a simple
baseline model that relies on answer frequency. The model
counts the top k most frequent answers [similar to the ImageNet challenge ] in the training set
as the predictions for all the test questions, where k = 100,
500, and 1000. We let a model make k different predictions.
We say the model is correct on a QA if one of the k predictions matches exactly with the ground-truth answer. We
report the accuracy over all test questions. This evaluation
method works well when the answers are short, especially
for single-word answers. However, it causes problems when
the answers are long phrases and sentences. We also report
humans performance [similar to previous work ] on these questions by presenting them
withtheimageandthequestionalongwith10multiplechoice
answersoutofwhichoneofthemwasthegroundtruthandthe
other 9 were randomly chosen from the dataset. Other evaluation methods require word ontologies 123:32–73
Table 9 Baseline QA performances in the 6 different question types
We report human evaluation as well as a baseline method that predicts
the most frequently occurring answer in the dataset
tion is common in existing QA datasets as well . The top 100, 500, and
1000 most frequent answers only cover 41.1%, 57.3%, and
64.1% of the correct answers. In comparison, the corresponding sets of frequent answers in VQA cover
63%, 75%, and 80% of the test set answers. The “where”
and “why” questions, which tend to involve spatial and common sense reasoning, tend to have more diverse answers and
hence perform poorly, with performances of 9.6 and 3.4%
top-100 respectively. The top 1000 frequent answers cover
only 41.8 and 18.7% of the correct answers from these two
question types respectively. In comparison, humans perform
extremely well in all the questions types achieving an overall
accuracy of 96.6%.
7 Future Applications and Directions
We have analyzed the individual components of this dataset
and presented experiments with baseline results for tasks
such as attribute classiﬁcation, relationship classiﬁcation,
description generation, and question answering. There are,
however, more applications and experiments for which our
dataset can be used. In this section, we note a few potential
applications that our dataset can enable.
Dense Image Captioning We have seen numerous image
captioning papers that attempt to
describe an entire image with a single caption. However,
these captions do not exhaustively describe every part of
the scene. A natural extension to this application, which the
Visual Genome dataset enables, is the ability to create dense
captioning models that describe parts of the scene.
Visual Question Answering While visual question answering has been studied as a standalone task , we
introduce a dataset that combines all of our question answers
with descriptions and scene graphs. Future work can build
supervised models that utilize various components of Visual
Genome to tackle question answering.
Image Understanding While we have seen a surge of image
captioning and question answering models, there has been little work on creating more comprehensive evaluation metrics to measure how
well these models are performing. Such models are usually
evaluated using BLEU, CIDEr, or METEOR and other similar metrics that do not effectively measure how well these
models understand the image . The Visual
Genome scene graphs can be used as a measurement for
image understanding. Generated descriptions and answers
can be matched against the ground truth scene graph of an
image to evaluate its corresponding model.
Relationship Extraction Relationship extraction has been
extensively studied in information retrieval and natural language processing . Visual
Genome is the ﬁrst large-scale visual relationship dataset.
This dataset can be used to study the extraction of visual
relationships from images, and its interactions between objects can also be used to study action
recognition 
and spatial orientation between objects .
Semantic Image Retrieval Previous work has already shown
that scene graphs can be used to improve semantic image
search . Further
methods can be explored using our region descriptions combined with region graphs. Attention-based search methods
can also be explored where the area of interest speciﬁed by
a query is also localized in the retrieved images.
Completing the Set of Annotations While Visual Genome is
the most densely annotated visual dataset for cognitive image
understanding, it is still not complete. In most images, it is not
feasible to collect an exhaustive set of attributes and relationships for every object or pair of objects. This raises two new
research questions. In computer vision, we need to develop
new evaluation metrics that do not penalize models due to
a lack of a complete set of annotations. In human computer
interaction, we need to design new interfaces and workﬂows
that incentivize humans to annotate visual common sense.
8 Conclusion
Visual Genome provides a multi-layered understanding of
pictures. It allows for a multi-perspective study of an image,
Int J Comput Vis 123:32–73
frompixel-level informationlikeobjects, torelationships that
require further inference, and to even deeper cognitive tasks
like question answering. It is a comprehensive dataset for
training and benchmarking the next generation of computer
vision models. With Visual Genome, we expect these models to develop a broader understanding of our visual world,
complementing computers’ capacities to detect objects with
abilities to describe those objects and explain their interactions and relationships. Visual Genome is a large formalized
knowledge representation for visual understanding and a
more complete set of descriptions and question answers that
grounds visual concepts to language.
Acknowledgements We would like to start by thanking our sponsors:
Stanford Computer Science Department, Yahoo Labs, The Brown Institute for Media Innovation, Toyota, Adobe and ONR MURI. Next, we
specially thank Michael Stark, Yutian Li, Frederic Ren, Sherman Leung,
Michelle Guo and Gavin Mai for their contributions. We thank Carsten
Rother from the University of Dresden for facilitating Oliver Groth’s
involvement. We also thank all the thousands of crowd workers for their
diligent contribution to Visual Genome. Finally, we thank all members
of the Stanford Vision Lab and Stanford HCI Group for their useful
comments and discussions.
Open Access This article is distributed under the terms of the Creative
Commons Attribution 4.0 International License ( 
ons.org/licenses/by/4.0/), which permits unrestricted use, distribution,
and reproduction in any medium, provided you give appropriate credit
to the original author(s) and the source, provide a link to the Creative
Commons license, and indicate if changes were made.