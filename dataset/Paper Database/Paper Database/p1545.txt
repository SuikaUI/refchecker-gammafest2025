IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Normalizing Flows: An Introduction and Review
of Current Methods
Ivan Kobyzev, Simon J.D. Prince, and Marcus A. Brubaker. Member, IEEE
Abstract—Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation
can be efﬁcient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the
construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review
current state-of-the-art literature, and identify open questions and promising future directions.
Index Terms—Generative models, Normalizing ﬂows, Density estimation, Variational inference, Invertible neural networks.
INTRODUCTION
MAJOR goal of statistics and machine learning has
been to model a probability distribution given samples
drawn from that distribution. This is an example of unsupervised learning and is sometimes called generative modelling. Its importance derives from the relative abundance
of unlabelled data compared to labelled data. Applications
include density estimation, outlier detection, prior construction, and dataset summarization.
Many methods for generative modeling have been proposed. Direct analytic approaches approximate observed
data with a ﬁxed family of distributions. Variational approaches and expectation maximization introduce latent
variables to explain the observed data. They provide additional ﬂexibility but can increase the complexity of learning and inference. Graphical models [Koller and Friedman,
2009] explicitly model the conditional dependence between
random variables. Recently, generative neural approaches
have been proposed including generative adversarial networks (GANs) [Goodfellow et al., 2014] and variational
auto-encoders (VAEs) [Kingma and Welling, 2014].
GANs and VAEs have demonstrated impressive performance results on challenging tasks such as learning
distributions of natural images. However, several issues
limit their application in practice. Neither allows for exact evaluation of the probability density of new points.
Furthermore, training can be challenging due to a variety
of phenomena including mode collapse, posterior collapse,
vanishing gradients and training instability [Bowman et al.,
2015; Salimans et al., 2016].
Normalizing Flows (NF) are a family of generative models with tractable distributions where both sampling and
density evaluation can be efﬁcient and exact. Applications
include image generation [Ho et al., 2019; Kingma and
Dhariwal, 2018], noise modelling [Abdelhamed et al., 2019],
video generation [Kumar et al., 2019], audio generation [Esling et al., 2019; Kim et al., 2018; Prenger et al., 2019], graph
generation [Madhawa et al., 2019], reinforcement learning
Borealis AI, Canada.
 
 
 
[Mazoure et al., 2019; Nadeem Ward et al., 2019; Touati et al.,
2019], computer graphics [M¨uller et al., 2018], and physics
[Kanwar et al., 2020; K¨ohler et al., 2019; No´e et al., 2019;
Wirnsberger et al., 2020; Wong et al., 2020].
There are several survey papers for VAEs [Kingma and
Welling, 2019] and GANs [Creswell et al., 2018; Wang et al.,
2017]. This article aims to provide a comprehensive review
of the literature around Normalizing Flows for distribution learning. Our goals are to 1) provide context and
explanation to enable a reader to become familiar with
the basics, 2) review the current literature, and 3) identify
open questions and promising future directions. Since this
article was ﬁrst made public, an excellent complementary
treatment has been provided by Papamakarios et al. .
Their article is more tutorial in nature and provides many
details concerning implementation, whereas our treatment
is more formal and focuses mainly on the families of ﬂow
In Section 2, we introduce Normalizing Flows and describe how they are trained. In Section 3 we review constructions for Normalizing Flows. In Section 4 we describe
datasets for testing Normalizing Flows and discuss the
performance of different approaches. Finally, in Section 5
we discuss open problems and possible research directions.
BACKGROUND
Normalizing Flows were popularised by Rezende and Mohamed in the context of variational inference and
by Dinh et al. for density estimation. However, the
framework was previously deﬁned in Tabak and Vanden-
Eijnden and Tabak and Turner , and explored
for clustering and classiﬁcation [Agnelli et al., 2010], and
density estimation [Laurence et al., 2014; Rippel and Adams,
A Normalizing Flow is a transformation of a simple
probability distribution (e.g., a standard normal) into a more
complex distribution by a sequence of invertible and differentiable mappings. The density of a sample can be evaluated
by transforming it back to the original simple distribution
and then computing the product of i) the density of the
inverse-transformed sample under this distribution and ii)
 
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Fig. 1. Change of variables (Equation (1)). Top-left: the density of
the source pZ. Top-right: the density function of the target distribution
pY(y). There exists a bijective function g, such that pY = g∗pZ, with
inverse f. Bottom-left: the inverse function f. Bottom-right: the absolute
Jacobian (derivative) of f.
the associated change in volume induced by the sequence
of inverse transformations. The change in volume is the
product of the absolute values of the determinants of the
Jacobians for each transformation, as required by the change
of variables formula.
The result of this approach is a mechanism to construct
new families of distributions by choosing an initial density
and then chaining together some number of parameterized,
invertible and differentiable transformations. The new density can be sampled from (by sampling from the initial
density and applying the transformations) and the density
at a sample (i.e., the likelihood) can be computed as above.
Let Z ∈RD be a random variable with a known and
tractable probability density function pZ : RD →R. Let
g be an invertible function and Y = g(Z). Then using the
change of variables formula, one can compute the probability density function of the random variable Y:
pY(y) = pZ(f(y)) |det Df(y)|
= pZ(f(y)) |det Dg(f(y))|−1 ,
where f is the inverse of g, Df(y) =
∂y is the Jacobian of
f and Dg(z) =
∂z is the Jacobian of g. This new density
function pY(y) is called a pushforward of the density pZ by
the function g and denoted by g∗pZ (Figure 1).
In the context of generative models, the above function g
(a generator) “pushes forward” the base density pZ (sometimes referred to as the “noise”) to a more complex density.
This movement from base density to ﬁnal complicated density is the generative direction. Note that to generate a data
point y, one can sample z from the base distribution, and
then apply the generator: y = g(z).
The inverse function f moves (or “ﬂows”) in the opposite, normalizing direction: from a complicated and irregular
data distribution towards the simpler, more regular or “normal” form, of the base measure pZ. This view is what gives
rise to the name “normalizing ﬂows” as f is “normalizing”
the data distribution. This term is doubly accurate if the base
measure pZ is chosen as a Normal distribution as it often is
in practice.
Intuitively, if the transformation g can be arbitrarily
complex, one can generate any distribution pY from any
base distribution pZ under reasonable assumptions on the
two distributions. This has been proven formally [Bogachev
et al., 2005; Medvedev, 2008; Villani, 2003]. See Section 3.4.3.
Constructing arbitrarily complicated non-linear invertible functions (bijections) can be difﬁcult. By the term Normalizing Flows people mean bijections which are convenient
to compute, invert, and calculate the determinant of their
Jacobian. One approach to this is to note that the composition of invertible functions is itself invertible and the
determinant of its Jacobian has a speciﬁc form. In particular,
let g1, . . . , gN be a set of N bijective functions and deﬁne
g = gN ◦gN−1 ◦· · · ◦g1 to be the composition of the
functions. Then it can be shown that g is also bijective, with
f = f1 ◦· · · ◦fN−1 ◦fN,
and the determinant of the Jacobian is
det Df(y) =
det Df i(xi),
where Df i(y) =
∂x is the Jacobian of fi. We denote the
value of the i-th intermediate ﬂow as xi = gi ◦· · · ◦g1(z) =
fi+1 ◦· · ·◦fN(y) and so xN = y. Thus, a set of nonlinear bijective functions can be composed to construct successively
more complicated functions.
More formal construction
In this section we explain normalizing ﬂows from more
formal perspective. Readers unfamiliar with measure theory
can safely skip to Section 2.2. First, let us recall the general
deﬁnition of a pushforward.
Deﬁnition 1. If (Z, ΣZ), (Y, ΣY) are measurable spaces,
g is a measurable mapping between them, and µ is a
measure on Z, then one can deﬁne a measure on Y
(called the pushforward measure and denoted by g∗µ)
by the formula
g∗µ(U) = µ(g−1(U)),
for all U ∈ΣY.
This notion gives a general formulation of a generative
model. Data can be understood as a sample from a measured “data” space (Y, ΣY, ν), which we want to learn.
To do that one can introduce a simpler measured space
(Z, ΣZ, µ) and ﬁnd a function g : Z →Y, such that
ν = g∗µ. This function g can be interpreted as a “generator”,
and Z as a latent space. This view puts generative models
in the context of transportation theory [Villani, 2003].
In this survey we will assume that Z = RD, all sigmaalgebras are Borel, and all measures are absolutely continuous with respect to Lebesgue measure (i.e., µ = pZdz).
Deﬁnition 2. A function g : RD →RD is called a diffeomorphism, if it is bijective, differentiable, and its inverse is
differentiable as well.
The pushforward of an absolutely continuous measure
pZdz by a diffeomorphism g is also absolutely continuous
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
with a density function given by Equation (1). Note that this
more general approach is important for studying generative
models on non-Euclidean spaces (see Section 5.2).
Remark 3. It is common in the normalizing ﬂows literature
to simply refer to diffeomorphisms as “bijections” even
though this is formally incorrect. In general, it is not
necessary that g is everywhere differentiable; rather it
is sufﬁcient that it is differentiable only almost everywhere with respect to the Lebesgue measure on RD. This
allows, for instance, piecewise differentiable functions to
be used in the construction of g.
Applications
Density estimation and sampling
The natural and most obvious use of normalizing ﬂows is to
perform density estimation. For simplicity assume that only
a single ﬂow, g, is used and it is parameterized by the vector
θ. Further, assume that the base measure, pZ is given and is
parameterized by the vector φ. Given a set of data observed
from some complicated distribution, D = {y(i)}M
i=1, we can
then perform likelihood-based estimation of the parameters
Θ = (θ, φ). The data likelihood in this case simply becomes
log p(D|Θ)
log pY(y(i)|Θ)
log pZ(f(y(i)|θ)|φ) + log
det Df(y(i)|θ)
where the ﬁrst term is the log likelihood of the sample
under the base measure and the second term, sometimes
called the log-determinant or volume correction, accounts
for the change of volume induced by the transformation of
the normalizing ﬂows (see Equation (1)). During training,
the parameters of the ﬂow (θ) and of the base distribution
(φ) are adjusted to maximize the log-likelihood.
Note that evaluating the likelihood of a distribution
modelled by a normalizing ﬂow requires computing f (i.e.,
the normalizing direction), as well as its log determinant.
The efﬁciency of these operations is particularly important
during training where the likelihood is repeatedly computed. However, sampling from the distribution deﬁned
by the normalizing ﬂow requires evaluating the inverse g
(i.e., the generative direction). Thus sampling performance
is determined by the cost of the generative direction. Even
though a ﬂow must be theoretically invertible, computation
of the inverse may be difﬁcult in practice; hence, for density
estimation it is common to model a ﬂow in the normalizing
direction (i.e., f). 1
Finally, while maximum likelihood estimation is often
effective (and statistically efﬁcient under certain conditions)
other forms of estimation can and have been used with
normalizing ﬂows. In particular, adversarial losses can be
used with normalizing ﬂow models .
1. To ensure both efﬁcient density estimation and sampling, van den
Oord et al. proposed an approach called Probability Density
Distillation which trains the ﬂow f as normal and then uses this as
a teacher network to train a tractable student network g.
Variational Inference
Consider a latent variable model p(x) =
R p(x, y)dy where
x is an observed variable and y the latent variable. The
posterior distribution p(y|x) is used when estimating the
parameters of the model, but its computation is usually
intractable in practice. One approach is to use variational
inference and introduce the approximate posterior q(y|x, θ)
where θ are parameters of the variational distribution. Ideally this distribution should be as close to the real posterior
as possible. This is done by minimizing the KL divergence
DKL(q(y|x, θ)||p(y|x)), which is equivalent to maximizing
the evidence lower bound L(θ) = Eq(y|x,θ)[log(p(y, x)) −
log(q(y|x, θ))]. The latter optimization can be done with
gradient descent; however for that one needs to compute gradients of the form ∇θEq(y|x,θ)[h(y)], which is not
straightforward.
As was observed by Rezende and Mohamed , one
can reparametrize q(y|x, θ) = pY(y|θ) with normalizing
ﬂows. Assume for simplicity, that only a single ﬂow g with
parameters θ is used, y = g(z|θ) and the base distribution
pZ(z) does not depend on θ. Then
EpY(y|θ)[h(y)] = EpZ(z)[h(g(z|θ))],
and the gradient of the right hand side with respect to θ can
be computed. This approach generally to computing gradients of an expectation is often called the “reparameterization
In this scenario evaluating the likelihood is only required
at points which have been sampled. Here the sampling
performance and evaluation of the log determinant are the
only relevant metrics and computing the inverse of the
mapping may not be necessary. Indeed, the planar and
radial ﬂows introduced in Rezende and Mohamed 
are not easily invertible (see Section 3.3).
Normalizing Flows should satisfy several conditions in order to be practical. They should:
be invertible; for sampling we need g while for
computing likelihood we need f,
be sufﬁciently expressive to model the distribution of
be computationally efﬁcient, both in terms of computing f and g (depending on the application) but
also in terms of the calculation of the determinant of
the Jacobian.
In the following section, we describe different types of ﬂows
and comment on the above properties. An overview of the
methods discussed can be seen in ﬁgure 2.
Elementwise Flows
A basic form of bijective non-linearity can be constructed
given any bijective scalar function. That is, let h : R →R be
a scalar valued bijection. Then, if x = (x1, x2, . . . , xD)T ,
g(x) = (h(x1), h(x2), . . . , h(xD))T
is also a bijection whose inverse simply requires computing
h−1 and whose Jacobian is the product of the absolute
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Non-linear elementwise transform
Problem: no mixing of variables
Affine combination of variables
Problem: limited representational power
Non-linear transforms
Problem: hard to compute inverse
Architectures that allow invertible
non-linear transformations.
Continuous flows depending on ODEs or SDEs
Invertible residual networks
Fig. 2. Overview of ﬂows discussed in this review. We start with elementwise bijections, linear ﬂows, and planar and radial ﬂows. All of these
have drawbacks and are limited in utility. We then discuss two architectures (coupling ﬂows and autoregressive ﬂows) which support invertible
non-linear transformations. These both use a coupling function, and we
summarize the different coupling functions available. Finally, we discuss
residual ﬂows and their continuous extension inﬁnitesimal ﬂows.
values of the derivatives of h. This can be generalized by
allowing each element to have its own distinct bijective
function which might be useful if we wish to only modify
portions of our parameter vector. In deep learning terminology, h, could be viewed as an “activation function”. Note
that the most commonly used activation function ReLU is
not bijective and can not be directly applicable, however,
the (Parametric) Leaky ReLU [He et al., 2015; Maas et al.,
2013] can be used instead among others. Note that recently
spline-based activation functions have also been considered
[Durkan et al., 2019a,b] and will be discussed in Section
Linear Flows
Elementwise operations alone are insufﬁcient as they cannot
express any form of correlation between dimensions. Linear
mappings can express correlation between dimensions:
g(x) = Ax + b
where A ∈RD×D and b ∈RD are parameters. If A is an
invertible matrix, the function is invertible.
Linear ﬂows are limited in their expressiveness. Consider a Gaussian base distribution: pZ(z) = N(z, µ, Σ). After transformation by a linear ﬂow, the distribution remains
Gaussian with distribution pY = N(y, Aµ + b, AT ΣA).
More generally, a linear ﬂow of a distribution from the exponential family remains in the exponential family. However,
linear ﬂows are an important building block as they form
the basis of afﬁne coupling ﬂows (Section 3.4.4.1).
Note that the determinant of the Jacobian is simply
det(A), which can be computed in O(D3), as can the
inverse. Hence, using linear ﬂows can become expensive
for large D. By restricting the form of A we can avoid these
practical problems at the expense of expressive power. In
the following sections we discuss different ways of limiting
the form of linear transforms to make them more practical.
If A is diagonal with nonzero diagonal entries, then its
inverse can be computed in linear time and its determinant
is the product of the diagonal entries. However, the result is
an elementwise transformation and hence cannot express
correlation between dimensions. Nonetheless, a diagonal
linear ﬂow can still be useful for representing normalization transformations [Dinh et al., 2017] which have become
a ubiquitous part of modern neural networks [Ioffe and
Szegedy, 2015].
Triangular
The triangular matrix is a more expressive form of linear
transformation whose determinant is the product of its
diagonal. It is non-singular so long as its diagonal entries
are non-zero. Inversion is relatively inexpensive requiring a
single pass of back-substitution costing O(D2) operations.
Tomczak and Welling combined K triangular
matrices Ti, each with ones on the diagonal, and a Kdimensional probability vector ω to deﬁne a more general
linear ﬂow y = (PK
i=1 ωiTi)z. The determinant of this
bijection is one. However ﬁnding the inverse has O(D3)
complexity, if some of the matrices are upper- and some are
lower-triangular.
Permutation and Orthogonal
The expressiveness of triangular transformations is sensitive
to the ordering of dimensions. Reordering the dimensions
can be done easily using a permutation matrix which has
an absolute determinant of 1. Different strategies have been
tried, including reversing and a ﬁxed random permutation
[Dinh et al., 2017; Kingma and Dhariwal, 2018]. However,
the permutations cannot be directly optimized and so remain ﬁxed after initialization which may not be optimal.
A more general alternative is the use of orthogonal
transformations. The inverse and absolute determinant of an
orthogonal matrix are both trivial to compute which make
them efﬁcient. Tomczak and Welling used orthogonal
matrices parameterized by the Householder transform. The
idea is based on the fact from linear algebra that any
orthogonal matrix can be written as a product of reﬂections.
To parameterize a reﬂection matrix H in RD one ﬁxes a
nonzero vector v ∈RD, and then deﬁnes H = 1−
||v||2 vvT .
Factorizations
Instead of limiting the form of A, Kingma and Dhariwal
 proposed using the LU factorization:
g(x) = PLUx + b
where L is lower triangular with ones on the diagonal, U is
upper triangular with non-zero diagonal entries, and P is a
permutation matrix. The determinant is the product of the
diagonal entries of U which can be computed in O(D). The
inverse of the function g can be computed using two passes
of backward substitution in O(D2). However, the discrete
permutation P cannot be easily optimized. To avoid this, P
is randomly generated initially and then ﬁxed. Hoogeboom
et al. [2019a] noted that ﬁxing the permutation matrix limits
the ﬂexibility of the transformation, and proposed using the
QR decomposition instead where the orthogonal matrix Q
is described with Householder transforms.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Convolution
Another form of linear transformation is a convolution
which has been a core component of modern deep learning architectures. While convolutions are easy to compute
their inverse and determinant are non-obvious. Several
approaches have been considered. Kingma and Dhariwal
 restricted themselves to “1 × 1” convolutions for
ﬂows which are simply a full linear transformation but
applied only across channels. Zheng et al. used
1D convolutions (ConvFlow) and exploited the triangular
structure of the resulting transform to efﬁciently compute
the determinant. However Hoogeboom et al. [2019a] have
provided a more general solution for modelling d×d convolutions, either by stacking together masked autoregressive
convolutions (referred to as Emerging Convolutions) or by
exploiting the Fourier domain representation of convolution
to efﬁciently compute inverses and determinants (referred
to as Periodic Convolutions).
Planar and Radial Flows
Rezende and Mohamed introduced planar and radial
ﬂows. They are relatively simple, but their inverses aren’t
easily computed. These ﬂows are not widely used in practice, yet they are reviewed here for completeness.
Planar Flows
Planar ﬂows expand and contract the distribution along
certain speciﬁc directions and take the form
g(x) = x + uh(wT x + b),
where u, w ∈RD and b ∈R are parameters and h : R →R
is a smooth non-linearity. The Jacobian determinant for this
transformation is
det(1D + uh′(wT x + b)wT )
1 + h′(wT x + b)uT w,
where the last equality comes from the application of the
matrix determinant lemma. This can be computed in O(D)
time. The inversion of this ﬂow isn’t possible in closed form
and may not exist for certain choices of h(·) and certain
parameter settings [Rezende and Mohamed, 2015].
The term uh(wT x+b) can be interpreted as a multilayer
perceptron with a bottleneck hidden layer with a single
unit [Kingma et al., 2016]. This bottleneck means that one
needs to stack many planar ﬂows to get high expressivity.
Hasenclever et al. and van den Berg et al. 
introduced Sylvester ﬂows to resolve this problem:
g(x) = x + Uh(WT x + b),
where U and W are D × M matrices, b ∈RM and h :
RM →RM is an elementwise smooth nonlinearity, where
M ≤D is a hyperparameter to choose and which can be
interpreted as the dimension of a hidden layer. In this case
the Jacobian determinant is:
= det(1D + U diag(h′(WT x + b))WT )
= det(1M + diag(h′(WT x + b))WUT ), (13)
where the last equality is Sylvester’s determinant identity
(which gives these ﬂows their name). This can be computationally efﬁcient if M is small. Some sufﬁcient conditions for
the invertibility of Sylvester transformations are discussed
in Hasenclever et al. and van den Berg et al. .
Radial Flows
Radial ﬂows instead modify the distribution around a speciﬁc point so that
g(x) = x +
α + ∥x −x0∥(x −x0)
where x0 ∈RD is the point around which the distribution is
distorted, and α, β ∈R are parameters, α > 0. As for planar
ﬂows, the Jacobian determinant can be computed relatively
efﬁciently. The inverse of radial ﬂows cannot be given in
closed form but does exist under suitable constraints on the
parameters [Rezende and Mohamed, 2015].
Coupling and Autoregressive Flows
In this section we describe coupling and auto-regressive
ﬂows which are the two most widely used ﬂow architectures. We ﬁrst present them in the general form, and then in
Section 3.4.4 we give speciﬁc examples.
Coupling Flows
Dinh et al. introduced a coupling method to enable
highly expressive transformations for ﬂows (Figure 3a).
Consider a disjoint partition of the input x ∈RD into
two subspaces: (xA, xB) ∈Rd × RD−d and a bijection
h(· ; θ) : Rd →Rd, parameterized by θ. Then one can deﬁne
a function g : RD →RD by the formula:
yA = h(xA; Θ(xB))
where the parameters θ are deﬁned by any arbitrary function
Θ(xB) which only uses xB as input. This function is called
a conditioner. The bijection h is called a coupling function,
and the resulting function g is called a coupling ﬂow. A
coupling ﬂow is invertible if and only if h is invertible and
has inverse:
xA = h−1(yA; Θ(xB))
The Jacobian of g is a block triangular matrix where the
diagonal blocks are Dh and the identity matrix respectively.
Hence the determinant of the Jacobian of the coupling ﬂow
is simply the determinant of Dh.
Most coupling functions are applied to xA element-wise:
h(xA; θ) = (h1(xA
1 ; θ1), . . . , hd(xA
where each hi(·; θi) : R →R is a scalar bijection. In this
case a coupling ﬂow is a triangular transformation (i.e., has
a triangular Jacobian matrix). See Section 3.4.4 for examples.
The power of a coupling ﬂow resides in the ability of a
conditioner Θ(xB) to be arbitrarily complex. In practice it is
usually modelled as a neural network. For example, Kingma
and Dhariwal used a shallow ResNet architecture.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Fig. 3. Coupling architecture. a) A single coupling ﬂow described in
Equation (15). A coupling function h is applied to one part of the space,
while its parameters depend on the other part. b) Two subsequent multiscale ﬂows in the generative direction. A ﬂow is applied to a relatively low
dimensional vector z; its parameters no longer depend on the rest part
zaux. Then new dimensions are gradually introduced to the distribution.
Sometimes, however, the conditioner can be constant
(i.e., not depend on xB at all). This allows for the construction of a “multi-scale ﬂow” Dinh et al. which gradually
introduces dimensions to the distribution in the generative
direction (Figure 3b). In the normalizing direction, the dimension reduces by half after each iteration step, such that
most of semantic information is retained. This reduces the
computational costs of transforming high dimensional distributions and can capture the multi-scale structure inherent
in certain kinds of data like natural images.
The question remains of how to partition x. This is
often done by splitting the dimensions in half [Dinh et al.,
2015], potentially after a random permutation. However,
more structured partitioning has also been explored and
is common practice, particularly when modelling images.
For instance, Dinh et al. used “masked” ﬂows that
take alternating pixels or blocks of channels in the case
of an image in non-volume preserving ﬂows (RealNVP).
In place of permutation Kingma and Dhariwal used
1 × 1 convolution (Glow). For the partition for the multiscale ﬂow in the normalizing direction, Das et al. 
suggested selecting features at which the Jacobian of the
ﬂow has higher values for the propagated part.
Autoregressive Flows
Kingma et al. used autoregressive models as a form
of normalizing ﬂow. These are non-linear generalizations of
multiplication by a triangular matrix (Section 3.2.2).
Let h(· ; θ) : R →R be a bijection parameterized by θ.
Then an autoregressive model is a function g : RD →RD,
which outputs each entry of y = g(x) conditioned on the
previous entries of the input:
yt = h(xt; Θt(x1:t−1)),
where x1:t = (x1, . . . , xt). For t = 2, . . . , D we choose
arbitrary functions Θt(·) mapping Rt−1 to the set of all
parameters, and Θ1 is a constant. The functions Θt(·) are
called conditioners.
The Jacobian matrix of the autoregressive transformation
g is triangular. Each output yt only depends on x1:t, and so
the determinant is just a product of its diagonal entries:
det (Dg) =
In practice, it’s possible to efﬁciently compute all the entries
of the direct ﬂow (Equation (18)) in one pass using a single
network with appropriate masks [Germain et al., 2015].
This idea was used by Papamakarios et al. to create
masked autoregressive ﬂows (MAF).
However, the computation of the inverse is more challenging. Given the inverse of h, the inverse of g can be found
with recursion: we have x1 = h−1(y1; Θ1) and for any
t = 2, . . . , D, xt = h−1(yt; Θt(x1:t−1)). This computation is
inherently sequential which makes it difﬁcult to implement
efﬁciently on modern hardware as it cannot be parallelized.
Note that the functional form for the autoregressive
model is very similar to that for the coupling ﬂow. In both
cases a bijection h is used, which has as an input one part
of the space and which is parameterized conditioned on
the other part. We call this bijection a coupling function in
both cases. Note that Huang et al. used the name
“transformer” (which has nothing to do with transformers
Alternatively, Kingma et al. introduced the “inverse autoregressive ﬂow” (IAF), which outputs each entry
of y conditioned the previous entries of y (with respect to
the ﬁxed ordering). Formally,
yt = h(xt; θt(y1:t−1)).
One can see that the functional form of the inverse autoregressive ﬂow is the same as the form of the inverse of
the ﬂow in Equation (18), hence the name. Computation
of the IAF is sequential and expensive, but the inverse of
IAF (which is a direct autoregressive ﬂow) can be computed
relatively efﬁciently (Figure 4).
Fig. 4. Autoregressive ﬂows. On the left, is the direct autoregressive
ﬂow given in Equation (18). Each output depends on the current and
previous inputs and so this operation can be easily parallelized. On
the right, is the inverse autoregressive ﬂow from Equation (20). Each
output depends on the current input and the previous outputs and so
computation is inherently sequential and cannot be parallelized.
In Section 2.2.1 we noted that papers typically model
ﬂows in the “normalizing ﬂow” direction (i.e., in terms of f
from data to the base density) to enable efﬁcient evaluation
of the log-likelihood during training. In this context one can
think of IAF as a ﬂow in the generative direction: i.e.in terms
of g from base density to data. Hence Papamakarios et al.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
 noted that one should use IAFs if fast sampling is
needed (e.g., for stochastic variational inference), and MAFs
if fast density estimation is desirable. The two methods
are closely related and, under certain circumstances, are
theoretically equivalent [Papamakarios et al., 2017].
Universality
For several autoregressive ﬂows the universality property
has been proven [Huang et al., 2018; Jaini et al., 2019a]. Informally, universality means that the ﬂow can learn any target
density to any required precision given sufﬁcient capacity
and data. We will provide a formal proof of the universality
theorem following Jaini et al. [2019a]. This section requires
some knowledge of measure theory and functional analysis
and can be safely skipped.
First, recall that a mapping T = (T1, . . . , TD) : RD →
RD is called triangular if Ti is a function of x1:i for each
i = 1, . . . , D. Such a triangular map T is called increasing if
Ti is an increasing function of xi for each i.
Proposition 4 . If µ
and ν are absolutely continuous Borel probability measures on RD, then there exists an increasing triangular
transformation T : RD →RD, such that ν = T∗µ. This
transformation is unique up to null sets of µ. A similar
result holds for measures on D.
Proposition 5. If µ is an absolutely continuous Borel probability measures on RD and {Tn} is a sequence of maps
RD →RD which converges pointwise to a map T, then a
sequence of measures (Tn)∗µ weakly converges to T∗µ.
Proof See Huang et al. , Lemma 4. The result follows
from the dominated convergence theorem.
As a corollary, to claim that a class of autoregressive
ﬂows g(·, θ) : RD →RD is universal, it is enough to demonstrate that a family of coupling functions h used in the class
is dense in the set of all monotone functions in the pointwise
convergence topology. In particular, Huang et al. used
neural monotone networks for coupling functions, and Jaini
et al. [2019a] used monotone polynomials. Using the theory
outlined in this section, universality could also be proved
for spline ﬂows [Durkan et al., 2019a,b] with splines for
coupling functions (see Section 3.4.4.4).
Coupling Functions
As described in the previous sections, coupling ﬂows and
autoregressive ﬂows have a similar functional form and
both have coupling functions as building blocks. A coupling
function is a bijective differentiable function h(·, θ) : Rd →
Rd, parameterized by θ. In coupling ﬂows, these functions
are typically constructed by applying a scalar coupling function h(·, θ) : R →R elementwise. In autoregressive ﬂows,
d = 1 and hence they are also scalar valued. Note that scalar
coupling functions are necessarily (strictly) monotone. In
this section we describe the scalar coupling functions commonly used in the literature.
Afﬁne coupling: Two simple forms of coupling functions h : R →R were proposed by [Dinh et al.,
2015] in NICE (nonlinear independent component estimation). These were the additive coupling function:
h(x ; θ) = x + θ,
and the afﬁne coupling function:
h(x; θ) = θ1x + θ2,
θ1 ̸= 0, θ2 ∈R.
Afﬁne coupling functions are used for coupling ﬂows
in NICE [Dinh et al., 2015], RealNVP [Dinh et al.,
autoregressive architectures in IAF [Kingma et al., 2016]
and MAF [Papamakarios et al., 2017]. They are simple
and computation is efﬁcient. However, they are limited
in expressiveness and many ﬂows must be stacked to
represent complicated distributions.
Nonlinear squared ﬂow: Ziegler and Rush
 proposed an invertible non-linear squared transformation deﬁned by:
h(x ; θ) = ax + b +
1 + (dx + h)2 .
Under some constraints on parameters θ = [a, b, c, d, h] ∈
R5, the coupling function is invertible and its inverse is
analytically computable as a root of a cubic polynomial
Experiments
these coupling functions facilitate learning multimodal
distributions.
Continuous mixture CDFs: Ho et al. 
proposed the Flow++ model, which contained several improvements, including a more expressive coupling function.
The layer is almost like a linear transformation, but one also
applies a monotone function to x:
h(x; θ) = θ1F(x, θ3) + θ2,
where θ1 ̸= 0, θ2 ∈R and θ3 = [π, µ, s] ∈RK × RK × RK
The function F(x, π, µ, s) is the CDF of a mixture of K
logistics, postcomposed with an inverse sigmoid:
F(x, π, µ, s) = σ−1
Note, that the post-composition with σ−1 : →R is
used to ensure the right range for h. Computation of the
inverse is done numerically with the bisection algorithm.
The derivative of the transformation with respect to x
is expressed in terms of PDF of logistic mixture (i.e., a
linear combination of hyperbolic secant functions), and
its computation is not expensive. An ablation study
demonstrated that switching from an afﬁne coupling
performance
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Splines: A spline is a piecewise-polynomial
or a piecewise-rational function which is speciﬁed by K + 1
points (xi, yi)K
i=0, called knots, through which the spline
passes. To make a useful coupling function, the spline
should be monotone which will be the case if xi < xi+1
and yi < yi+1. Usually splines are considered on a compact
Piecewise-linear and piecewise-quadratic:
M¨uller et al. used linear splines for coupling
functions h : → . They divided the domain into K
equal bins. Instead of deﬁning increasing values for yi, they
modeled h as the integral of a positive piecewise-constant
h(x; θ) = αθb +
where θ ∈RK is a probability vector, b = ⌊Kx⌋(the bin that
contains x), and α = Kx−b (the position of x in bin b). This
map is invertible, if all θk > 0, with derivative: ∂h
M¨uller et al. also used a monotone quadratic
spline on the unit interval for a coupling function and
modeled this as the integral of a positive piecewise-linear
function. A monotone quadratic spline is invertible; ﬁnding
its inverse map requires solving a quadratic equation.
Cubic Splines: Durkan et al. [2019a] proposed using
monotone cubic splines for a coupling function. They do not
restrict the domain to the unit interval, but instead use the
form: h(·; θ) = σ−1(ˆh(σ(·); θ)), where ˆh(·; θ) : → 
is a monotone cubic spline and σ is a sigmoid. Steffen’s
method is used to construct the spline. Here, one speciﬁes
K + 1 knots of the spline and boundary derivatives ˆh′(0)
and ˆh′(1). These quantities are modelled as the output of a
neural network.
Computation of the derivative is easy as it is piecewisequadratic. A monotone cubic polynomial has only one real
root and for inversion, one can ﬁnd this either analytically
or numerically. However, the procedure is numerically unstable if not treated carefully. The ﬂow can be trained by
gradient descent by differentiating through the numerical
root ﬁnding method. However, Durkan et al. [2019b], noted
numerical difﬁculties when the sigmoid saturates for values
far from zero.
splines: Durkan et al. [2019b]
model a coupling function h(x ; θ) as a monotone rationalquadratic spline on an interval as the identity function otherwise. They deﬁne the spline using the method of Gregory
and Delbourgo , by specifying K +1 knots {h(xi)}K
and the derivatives at the inner points: {h′(xi)}K−1
i=1 . These
locations of the knots and their derivatives are modelled as
the output of a neural network.
The derivative with respect to x is a quotient derivative
and the function can be inverted by solving a quadratic
equation. Durkan et al. [2019b] used this coupling function
with both a coupling architecture RQ-NSF(C) and an
auto-regressive architecture RQ-NSF(AR).
Neural autoregressive ﬂow: Huang et al.
 introduced Neural Autoregressive Flows (NAF)
where a coupling function h(· ; θ) is modelled with a deep
neural network. Typically such a network is not invertible,
but they proved a sufﬁcient condition for it to be bijective:
Proposition 6. If NN(·) : R →R is a multilayer percepton,
such that all weights are positive and all activation
functions are strictly monotone, then NN(·) is a strictly
monotone function.
They proposed two forms of neural networks: the deep
sigmoidal coupling function (NAF-DSF) and deep dense
sigmoidal coupling function (NAF-DDSF). Both are MLPs
with layers of sigmoid and logit units and non-negative
weights; the former has a single hidden layer of sigmoid
units, whereas the latter is more general and does not have
this bottleneck. By Proposition 6, the resulting h(· ; θ) is a
strictly monotone function. They also proved that a DSF
network can approximate any strictly monotone univariate
function and so NAF-DSF is a universal ﬂow.
Wehenkel and Louppe noted that imposing
positivity of weights on a ﬂow makes training harder
and requires more complex conditioners. To mitigate this,
they introduced unconstrained monotonic neural networks
(UMNN). The idea is in order to model a strictly monotone
function, one can describe a strictly positive (or negative)
function with a neural network and then integrate it
numerically. They demonstrated that UMNN requires less
parameters than NAF to reach similar performance, and so
is more scalable for high-dimensional datasets.
Sum-of-Squares polynomial ﬂow: Jaini et al.
[2019a] modeled h(· ; θ) as a strictly increasing polynomial.
They proved such polynomials can approximate any strictly
monotonic univariate continuous function. Hence, the resulting ﬂow (SOS - sum of squares polynomial ﬂow) is a
universal ﬂow.
The authors observed that the derivative of an increasing
single-variable polynomial is a positive polynomial. Then
they used a classical result from algebra: all positive singlevariable polynomials are the sum of squares of polynomials.
To get the coupling function, one needs to integrate the sum
of squares:
h(x ; θ) = c +
where L and K are hyperparameters (and, as noted in the
paper, can be chosen to be 2).
SOS is easier to train than NAF, because there are no
restrictions on the parameters (like positivity of weights).
For L=0, SOS reduces to the afﬁne coupling function and so
it is a generalization of the basic afﬁne ﬂow.
Piecewise-bijective coupling:
Dinh et al.
 explore the idea that a coupling function does not
need to be bijective, but just piecewise-bijective (Figure
5). Formally, they consider a function h(· ; θ) : R →R
and a covering of the domain into K disjoint subsets:
i=1 Ai, such that the restriction of the function onto
each subset h(· ; θ)|Ai is injective.
Dinh et al. constructed a ﬂow f : RD →RD with a
coupling architecture and piecewise-bijective coupling function in the normalizing direction - from data distribution
to (simpler) base distribution. There is a covering of the
data domain, and each subset of this covering is separately
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Fig. 5. Piecewise bijective coupling. The target domain (right) is divided
into disjoint sections (colors) and each mapped by a monotone function
(center) to the base distribution (left). For inverting the function, one
samples a component of the base distribution using a gating network.
mapped to the base distribution. Each part of the base
distribution now receives contributions from each subset of
the data domain. For sampling, Dinh et al. proposed
a probabilistic mapping from the base to data domain.
More formally, denote the target y and base z, and
consider a lookup function φ : R →[K] = {1, . . . , K},
such that φ(y) = k, if y ∈Ak. One can deﬁne a new map
R →R × [K], given by the rule y 7→(h(y), φ(y)), and a
density on a target space pZ,[K](z, k) = p[K]|Z(k|z)pZ(z).
One can think of this as an unfolding of the non-injective
map h. In particular, for each point z one can ﬁnd its preimage by sampling from p[K]|Z, which is called a gating
network. Pushing forward along this unfolded map is now
well-deﬁned and one gets the formula for the density pY :
pY (y) = pZ,[K](h(y), φ(y))|Dh(y)|.
This real and discrete (RAD) ﬂow efﬁciently learns distributions with discrete structures (multimodal distributions,
distributions with holes, discrete symmetries etc).
Residual Flows
Residual networks [He et al., 2016] are compositions of the
function of the form
g(x) = x + F(x).
Such a function is called a residual connection, and here the
residual block F(·) is a feed-forward neural network of any
kind (a CNN in the original paper).
The ﬁrst attempts to build a reversible network architecture based on residual connections were made in RevNets
[Gomez et al., 2017] and iRevNets [Jacobsen et al., 2018].
Their main motivation was to save memory during training
and to stabilize computation. The central idea is a variation
of additive coupling functions: consider a disjoint partition
of RD = Rd × RD−d denoted by x = (xA, xB) for the input
and y = (yA, yB) for the output, and deﬁne a function:
yA = xA + F(xB)
yB = xB + G(yA),
where F : RD−d →Rd and G : Rd →RD−d are residual
blocks. This network is invertible (by re-arranging the equations in terms of xA and xB and reversing their order) but
computation of the Jacobian is inefﬁcient.
A different point of view on reversible networks comes
from a dynamical systems perspective via the observation
that a residual connection is a discretization of a ﬁrst
order ordinary differential equation (see Section 3.6 for
more details). Chang et al. proposed several
architectures, some of these networks were demonstrated to
be invertible. However, the Jacobian determinants of these
networks cannot be computed efﬁciently.
Other research has focused on making the residual
connection g(·) invertible. A sufﬁcient condition for the
invertibility was found in [Behrmann et al., 2019]. They
proved the following statement:
Proposition 7. A residual connection (29) is invertible, if the
Lipschitz constant of the residual block is Lip(F) < 1.
There is no analytically closed form for the inverse, but it can
be found numerically using ﬁxed-point iterations (which, by
the Banach theorem, converge if we assume Lip(F) < 1).
Controlling the Lipschitz constant of a neural network is
not simple. The speciﬁc architecture proposed by Behrmann
et al. , called iResNet, uses a convolutional network
for the residual block. It constrains the spectral radius of
each convolutional layer in this network to be less than one.
The Jacobian determinant of the iResNet cannot be computed directly, so the authors propose to use a (biased)
stochastic estimate. The Jacobian of the residual connection
g in Equation (29) is: Dg = I + DF. Because the function
F is assumed to be Lipschitz with Lip(F) < 1, one has:
|det(I + DF)| = det(I + DF). Using the linear algebra
identity, ln det A = Tr ln A we have:
ln |det Dg| = ln det(I + DF) = Tr(ln (I + DF)),
Then one considers a power series for the trace of the matrix
logarithm:
Tr(ln (I + DF)) =
(−1)k+1 Tr(DF)k
By truncating this series one can calculate an approximation
to the log Jacobian determinant of g. To efﬁciently compute
each member of the truncated series, the Hutchinson trick
was used. This trick provides a stochastic estimation of
of a matrix trace A ∈RD×D, using the relation: TrA =
Ep(v)[vT Av], where v ∈RD, E[v] = 0, and cov(v) = I.
Truncating the power series gives a biased estimate
of the log Jacobian determinant (the bias depends on the
truncation error). An unbiased stochastic estimator was
proposed by Chen et al. in a model they called a
Residual ﬂow. The authors used a Russian roulette estimator
instead of truncation. Informally, every time one adds the
next term an+1 to the partial sum Pn
i=1 ai while calculating
the series P∞
i=1 ai, one ﬂips a coin to decide if the calculation
should be continued or stopped. During this process one
needs to re-weight terms for an unbiased estimate.
Inﬁnitesimal (Continuous) Flows
The residual connections discussed in the previous section
can be viewed as discretizations of a ﬁrst order ordinary
differential equation (ODE) [E, 2017; Haber et al., 2018]:
dtx(t) = F(x(t), θ(t)),
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
where F : RD × Θ →RD is a function which determines
the dynamic (the evolution function), Θ is a set of parameters
and θ : R →Θ is a parameterization. The discretization of
this equation (Euler’s method) is
xn+1 −xn = εF(xn, θn),
and this is equivalent to a residual connection with a residual block εF(·, θn).
In this section we consider the case where we do not
discretize but try to learn the continuous dynamical system
instead. Such ﬂows are called inﬁnitesimal or continuous. We
consider two distinct types. The formulation of the ﬁrst
type comes from ordinary differential equations, and of the
second type from stochastic differential equations.
ODE-based methods
Consider an ODE as in Equation (33), where t ∈ .
Assuming uniform Lipschitz continuity in x and continuity
in t, the solution exists (at least, locally) and, given an initial
condition x(0) = z, is unique . We denote the solution at
each time t as Φt(z).
Remark 8. At each time t, Φt(·) : RD →RD is a diffeomorphism and satisﬁes the group law: Φt ◦Φs = Φt+s.
Mathematically speaking, an ODE (33) deﬁnes a oneparameter group of diffeomorphisms on RD. Such a
group is called a smooth ﬂow in dynamical systems
theory and differential geometry [Katok and Hasselblatt,
When t = 1, the diffeomorphism Φ1(·) is called a time
one map. The idea to model a normalizing ﬂow as a time
one map y = g(z) = Φ1(z) was presented by [Chen et al.,
2018a] under the name Neural ODE (NODE). From a deep
learning perspective this can be seen as an “inﬁnitely deep”
neural network with input z, output y and continuous
weights θ(t). The invertibility of such networks naturally
comes from the theorem of the existence and uniqueness of
the solution of the ODE.
Training these networks for a supervised downstream
task can be done by the adjoint sensitivity method which
is the continuous analog of backpropagation. It computes
the gradients of the loss function by solving a second
(augmented) ODE backwards in time. For loss L(x(t)), where
x(t) is a solution of ODE (33), its sensitivity or adjoint
dx(t). This is the analog of the derivative of
the loss with respect to the hidden layer. In a standard
neural network, the backpropagation formula computes this
derivative:
dhn . For “inﬁnitely deep” neural
network, this formula changes into an ODE:
= −a(t)dF(x(t), θ(t))
For density estimation learning, we do not have a loss,
but instead seek to maximize the log likelihood. For normalizing ﬂows, the change of variables formula is given by
another ODE:
dt log(p(x(t))) = −Tr
Note that we no longer need to compute the determinant. To
train the model and sample from pY we solve these ODEs,
which can be done with any numerical ODE solver.
Grathwohl et al. used the Hutchinson estimator
to calculate an unbiased stochastic estimate of the traceterm. This approach which they termed FFJORD reduces
the complexity even further. Finlay et al. added two
regularization terms into the loss function of FFJORD: the
ﬁrst term forces solution trajectories to follow straight lines
with constant speed, and the second term is the Frobenius
norm of the Jacobian. This regularization decreased the
training time signiﬁcantly and reduced the need for multiple
GPUs. An interesting side-effect of using continuous ODEtype ﬂows is that one needs fewer parameters to achieve the
similar performance. For example, Grathwohl et al. 
show that for the comparable performance on CIFAR10,
FFJORD uses less than 2% as many parameters as Glow.
Not all diffeomorphisms can be presented as a time one
map of an ODE . For example, one necessary condition
is that the map is orientation preserving which means that
the Jacobian determinant must be positive. This can be seen
because the solution Φt is a (continuous) path in the space
of diffeomorphisms from the identity map Φ0 = Id to
the time one map Φ1. Since the Jacobian determinant of a
diffeomorphism is nonzero, its sign cannot change along the
path. Hence, a time one map must have a positive Jacobian
determinant. For example, consider a map f : R →R, such
that f(x) = −x. It is obviously a diffeomorphism, but it can
not be presented as a time one map of any ODE, because it
is not orientation preserving.
Dupont et al. suggested how one can improve
Neural ODE in order to be able to represent a broader
class of diffeomorphisms. Their model is called Augmented
Neural ODE (ANODE). They add variables ˆx(t) ∈Rp and
consider a new ODE:
with initial conditions x(0) = z and ˆx(0) = 0. The addition of ˆx(t) in particular gives freedom for the Jacobian
determinant to remain positive. As was demonstrated in the
experiments, ANODE is capable of learning distributions
that the Neural ODE cannot, and the training time is shorter.
Zhang et al. proved that any diffeomorphism can be
represented as a time one map of ANODE and so this is a
universal ﬂow.
A similar ODE-base approach was taken by Salman
et al. in Deep Diffeomorphic Flows. In addition to
modelling a path Φt(·) in the space of all diffeomorphic
transformations, for t ∈ , they proposed geodesic
regularisation in which longer paths are punished.
SDE-based methods (Langevin ﬂows)
The idea of the Langevin ﬂow is simple; we start with a
complicated and irregular data distribution pY(y) on RD,
and then mix it to produce the simple base distribution
pZ(z). If this mixing obeys certain rules, then this procedure
can be invertible. This idea was explored by Chen et al.
[2018b]; Jankowiak and Obermeyer ; Rezende and
Mohamed ; Salimans et al. ; Sohl-Dickstein et al.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
 ; Suykens et al. ; Welling and Teh . We
provide a high-level overview of the method, including the
necessary mathematical background.
A stochastic differential equation (SDE) or Itˆo process
describes a change of a random variable x ∈RD as a
function of time t ∈R+:
dx(t) = b(x(t), t)dt + σ(x(t), t)dBt,
where b(x, t) ∈RD is the drift coefﬁcient, σ(x, t) ∈RD×D
is the diffusion coefﬁcient, and Bt is D-dimensional Brownian
motion. One can interpret the drift term as a deterministic
change and the diffusion term as providing the stochasticity
and mixing. Given some assumptions about these functions,
the solution exists and is unique [Oksendal, 1992].
Given a time-dependent random variable x(t) we can
consider its density function p(x, t) and this is also time
dependent. If x(t) is a solution of Equation (38), its density
function satisﬁes two partial differential equations describing the forward and backward evolution [Oksendal, 1992].
The forward evolution is given by Fokker-Plank equation or
Kolmogorov’s forward equation:
∂tp(x, t) = −∇x · (b(x, t)p(x, t)) + P
∂xi∂xj Dij(x, t)p(x, t),
where D = 1
2σσT , with the initial condition p(·, 0) = pY(·).
The reverse is given by Kolmogorov’s backward equation:
∂tp(x, t) = b(x, t) · ∇x(p(x, t)) + P
i,j Dij(x, t)
∂xi∂xj p(x, t),
where 0 < t < T, and the initial condition is p(·, T) = pZ(·).
Asymptotically the Langevin ﬂow can learn any distribution if one picks the drift and diffusion coefﬁcients
appropriately [Suykens et al., 1998]. However this result is
not very practical, because one needs to know the (unnormalized) density function of the data distribution.
One can see that if the diffusion coefﬁcient is zero, the Itˆo
process reduces to the ODE (33), and the Fokker-Plank equation becomes a Liouville’s equation, which is connected to
Equation (36) . It is also equivalent to
the form of the transport equation considered in Jankowiak
and Obermeyer for stochastic optimization.
Sohl-Dickstein et al. and Salimans et al. 
suggested using MCMC methods to model the diffusion.
They considered discrete time t = 0, . . . , T. For each time t,
xt is a random variable where x0 = y is the data point,
= z is the base point. The forward transition
probability q(xt|xt−1) is taken to be either normal or binomial distribution with trainable parameters. Kolmogorov’s
backward equation implies that the backward transition
p(xt−1|xt) must have the same functional form as the forward transition (i.e., be either normal or binomial). Denote:
q(x0) = pY(y), the data distribution, and p(xT ) = pZ(z),
the base distribution. Applying the backward transition to
the base distribution, one obtains a new density p(x0),
which one wants to match with q(x0). Hence, the optimization objective is the log likelihood L =
R dx0q(x0) log p(x0).
This is intractable, but one can ﬁnd a lower bound as in
variational inference.
Several papers have worked explicitly with the SDE
[Chen et al., 2018b; Li et al., 2020; Liutkus et al., 2019;
List of Normalizing Flows for which we show performance results.
Architecture
Coupling function
Coupling, 3.4.1
Afﬁne, 3.4.4.1
Mixture CDF, 3.4.4.3
Splines, 3.4.4.4
quadratic (C)
Piecewise Bijective, 3.4.4.7
Autoregressive, 3.4.2
Polynomial, 3.4.4.6
Neural Network, 3.4.4.5
quadratic (AR)
RQ-NSF(AR)
Residual, 3.5
Residual ﬂow
ODE, 3.6.1
Peluchetti and Favaro, 2019; Tzen and Raginsky, 2019].
Chen et al. [2018b] use SDEs to create an interesting posterior for variational inference. They sample a latent variable
z0 conditioned on the input x, and then evolve z0 with SDE.
In practice this evolution is computed by discretization. By
analogy to Neural ODEs, Neural Stochastic Differential
Equations were proposed [Peluchetti and Favaro, 2019;
Tzen and Raginsky, 2019]. In this approach coefﬁcients of
the SDE are modelled as neural networks, and black box
SDE solvers are used for inference. To train Neural SDE one
needs an analog of backpropagation, Tzen and Raginsky
 proposed the use of Kunita’s theory of stochastic
ﬂows. Following this, Li et al. derived the adjoint SDE
whose solution gives the gradient of the original Neural
Note, that even though Langevin ﬂows manifest nice
mathematical properties, they have not found practical applications. In particular, none of the methods has been tested
on baseline datasets for ﬂows.
DATASETS AND PERFORMANCE
In this section we discuss datasets commonly used for
training and testing normalizing ﬂows. We provide comparison tables of the results as they were presented in the
corresponding papers. The list of the ﬂows for which we
post the performance results is given in Table 1.
Tabular datasets
We describe datasets as they were preprocessed in Papamakarios et al. (Table 2)2. These datasets are relatively
small and so are a reasonable ﬁrst test of unconditional
density estimation models. All datasets were cleaned and
de-quantized by adding uniform noise, so they can be considered samples from an absolutely continuous distribution.
We use a collection of datasets from the UC Irvine
machine learning repository [Dua and Graff, 2017].
POWER: a collection of electric power consumption
measurements in one house over 47 months.
2. See 
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
GAS: a collection of measurements from chemical
sensors in several gas mixtures.
measurements
high-energy
physics experiments aiming to detect particles with
unknown mass.
MINIBOONE: measurements from MiniBooNE experiment for observing neutrino oscillations.
In addition we consider the Berkeley segmentation
dataset [Martin et al., 2001] which contains segmentations
of natural images. Papamakarios et al. extracted 8×8
random monochrome patches from it.
In Table 3 we compare performance of ﬂows for these
tabular datasets. For experimental details, see the following
papers: RealNVP [Dinh et al., 2017] and MAF [Papamakarios et al., 2017], Glow [Kingma and Dhariwal, 2018] and
FFJORD [Grathwohl et al., 2019], NAF [Huang et al., 2018],
UMNN [Wehenkel and Louppe, 2019], SOS [Jaini et al.,
2019a], Quadratic Spline ﬂow and RQ-NSF [Durkan et al.,
2019b], Cubic Spline Flow [Durkan et al., 2019a].
Table 3 shows that universal ﬂows (NAF, SOS, Splines)
demonstrate relatively better performance.
Image datasets
These datasets summarized in Table 4. They are of increasing complexity and are preprocessed as in Dinh et al. 
by dequantizing with uniform noise (except for Flow++).
Table 5 compares performance on the image datasets for
unconditional density estimation. For experimental details,
see: RealNVP for CIFAR-10 and ImageNet [Dinh et al., 2017],
Glow for CIFAR-10 and ImageNet [Kingma and Dhariwal,
2018], RealNVP and Glow for MNIST, MAF and FFJORD
[Grathwohl et al., 2019], SOS [Jaini et al., 2019a], RQ-NSF
[Durkan et al., 2019b], UMNN [Wehenkel and Louppe,
2019], iResNet [Behrmann et al., 2019], Residual Flow [Chen
et al., 2019], Flow++ [Ho et al., 2019].
As of this writing Flow++ [Ho et al., 2019] is the best performing approach. Besides using more expressive coupling
layers (see Section 3.4.4.3) and a different architecture for
the conditioner, variational dequantization was used instead
of uniform. An ablation study shows that the change in
dequantization approach gave the most signiﬁcant improvement.
DISCUSSION AND OPEN PROBLEMS
Inductive biases
Role of the base measure
The base measure of a normalizing ﬂow is generally assumed to be a simple distribution (e.g., uniform or Gaussian). However this doesn’t need to be the case. Any distribution where we can easily draw samples and compute the
log probability density function is possible and the parameters of this distribution can be learned during training.
Theoretically the base measure shouldn’t matter: any
distribution for which a CDF can be computed, can be
simulated by applying the inverse CDF to draw from the
uniform distribution. However in practice if structure is
provided in the base measure, the resulting transformations
may become easier to learn. In other words, the choice of
base measure can be viewed as a form of prior or inductive
bias on the distribution and may be useful in its own right.
For example, a trade-off between the complexity of the generative transformation and the form of base measure was
explored in [Jaini et al., 2019b] in the context of modelling
tail behaviour.
Form of diffeomorphisms
The majority of the ﬂows explored are triangular ﬂows
(either coupling or autoregressive architectures). Residual
networks and Neural ODEs are also being actively investigated and applied. A natural question to ask is: are there
other ways to model diffeomorphisms which are efﬁcient
for computation? What inductive bias does the architecture
impose? For instance, Spantini et al. investigate the
relation between the sparsity of the triangular ﬂow and
Markov property of the target distribution.
A related question concerns the best way to model
conditional normalizing ﬂows when one needs to learn
a conditional probability distribution. Trippe and Turner
 suggested using different ﬂows for each condition,
but this approach doesn’t leverage weight sharing, and so
is inefﬁcient in terms of memory and data usage. Atanov
et al. proposed using afﬁne coupling layers where
the parameters θ depend on the condition. Conditional distributions are useful in particular for time series modelling,
where one needs to ﬁnd p(yt|y<t) [Kumar et al., 2019].
Loss function
The majority of the existing ﬂows are trained by minimization of KL-divergence between source and the target distributions (or, equivalently, with log-likelihood maximization).
However, other losses could be used which would put
normalizing ﬂows in a broader context of optimal transport
theory [Villani, 2003]. Interesting work has been done in this
direction including Flow-GAN [Grover et al., 2018] and the
minimization of the Wasserstein distance as suggested by
[Arjovsky et al., 2017; Tolstikhin et al., 2018].
Generalisation to non-Euclidean spaces
Flows on manifolds.
Modelling probability distributions on manifolds has applications in many ﬁelds including robotics, molecular biology, optics, ﬂuid mechanics, and plasma physics [Gemici
et al., 2016; Rezende et al., 2020]. How best to construct
a normalizing ﬂow on a general differentiable manifold
remains an open question. One approach to applying the
normalizing ﬂow framework on manifolds, is to ﬁnd a base
distribution on the Euclidean space and transfer it to the
manifold of interest. There are two main approaches: 1)
embed the manifold in the Euclidean space and “restrict”
the measure, or 2) induce the measure from the tangent
space to the manifold. We will brieﬂy discuss each in turn.
One can also use differential structure to deﬁne measures on manifolds [Spivak, 1965]. Every differentiable and
orientable manifold M has a volume form ω, then for a Borel
subset U ⊂M one can deﬁne its measure as µω(U) =
A Riemannian manifold has a natural volume form given
by its metric tensor: ω =
|g|dx1 ∧· · · ∧dxD. Gemici
et al. explore this approach considering an immersion
of an D-dimensional manifold M into a Euclidean space:
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Tabular datasets: data dimensionality and number of training examples.
Average test log-likelihood (in nats) for density estimation on tabular datasets (higher the better). A number in parenthesis next to a ﬂow indicates
number of layers. MAF MoG is MAF with mixture of Gaussians as a base density.
-17.70±0.02
-11.75±0.44
155.69±0.28
10.08±0.02
-17.73±0.02
-12.24±0.45
154.93±0.28
-17.39±0.02
-11.68±0.44
156.36±0.28
RealNVP(5)
-0.02±0.01
-19.62±0.02
-13.55±0.49
152.97±0.28
RealNVP(10)
-18.71±0.02
-13.84±0.52
153.28±1.78
11.91±0.13
-15.09±0.40
-8.86±0.15
157.73±0.04
11.96±0.33
-15.32±0.23
-9.01±0.01
157.43±0.30
10.89±0.70
-13.99±0.21
-9.67±0.13
157.98±0.01
11.99±0.41
-15.15±0.10
-8.90±0.11
157.48±0.41
Quadratic Spline (C)
12.80±0.02
-15.35±0.02
-9.35±0.44
157.65±0.28
Quadratic Spline (AR)
12.91±0.02
-14.67±0.03
-9.72±0.47
157.42±0.28
Cubic Spline
13.14±0.02
-14.59±0.02
-9.06±0.48
157.24±0.07
13.09±0.02
-14.75±0.03
-9.67±0.47
157.54±0.28
RQ-NSF(AR)
13.09±0.02
-14.01±0.03
-9.22±0.48
157.31±0.28
Image datasets: data dimensionality and number of training examples
for MNIST, CIFAR-10, ImageNet32 and ImageNet64 datasets.
Average test negative log-likelihood (in bits per dimension) for density
estimation on image datasets (lower is better).
Residual Flow
φ : M →RN, where N ≥D. In this case, one pullsback a Euclidean metric, and locally a volume form on
det((Dφ)T Dφ)dx1 ∧· · · ∧dxD, where Dφ is
the Jacobian matrix of φ. Rezende et al. pointed out
that the realization of this method is computationally hard,
and proposed an alternative construction of ﬂows on tori
and spheres using diffeomorphisms of the one-dimensional
circle as building blocks.
As another option, one can consider exponential maps
expx : TxM →M, mapping a tangent space of a Riemannian manifold (at some point x) to the manifold itself.
If the manifold is geodesic complete, this map is globally
deﬁned, and locally is a diffeomorphism. A tangent space
has a structure of a vector space, so one can choose an
isomorphism TxM ∼= RD. Then for a base distribution with
the density pZ on RD, one can push it forward on M via
the exponential map. Additionally, applying a normalizing
ﬂow to a base measure before pushing it to M helps to
construct multimodal distributions on M. If the manifold
M is a hyberbolic space, the exponential map is a global
diffeomorphism and all the formulas could be written explicitly. Using this method, Ovinnikov introduced the
Gaussian reparameterization trick in a hyperbolic space and
Bose et al. constructed hyperbolic normalizing ﬂows.
Instead of a Riemannian structure, one can impose a Lie
group structure on a manifold G. In this case there also
exists an exponential map exp : g →G mapping a Lie
algebra to the Lie group and one can use it to construct a
normalizing ﬂow on G. Falorsi et al. introduced an
analog of the Gaussian reparameterization trick for a Lie
Discrete distributions
Modelling distributions over discrete spaces is important
in a range of problems, however the generalization of normalizing ﬂows to discrete distributions remains an open
problem in practice. Discrete latent variables were used by
Dinh et al. as an auxiliary tool to pushforward continuous random variables along piecewise-bijective maps (see
Section 3.4.4.7). However, can we deﬁne normalizing ﬂows
if one or both of our distributions are discrete? This could
be useful for many applications including natural language
modelling, graph generation and others.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
To this end Tran et al. model bijective functions
on a ﬁnite set and show that, in this case, the change of
variables is given by the formula: pY(y) = pZ(g−1(y)),
i.e., with no Jacobian term (compare with Deﬁnition 1). For
backpropagation of functions with discrete variables they
use the straight-through gradient estimator [Bengio et al.,
2013]. However this method is not scalable to distributions
with large numbers of elements.
Alternatively Hoogeboom et al. [2019b] models bijections on ZD directly with additive coupling layers. Other
approaches transform a discrete variable into a continuous
latent variable with a variational autoencoder, and then
apply normalizing ﬂows in the continuous latent space
[Wang and Wang, 2019; Ziegler and Rush, 2019].
A different approach is dequantization, (i.e., adding
noise to discrete data to make it continuous) which can be
used with ordinal variables, e.g., discretized pixel intensities.
The noise can be uniform but other forms are possible
and this dequantization can even be learned as a latent
variable model [Ho et al., 2019; Hoogeboom et al., 2020].
Hoogeboom et al. analyzed how different choices of
dequantization objectives and dequantization distributions
affect the performance.
ACKNOWLEDGMENTS
The authors would like to thank Matt Taylor and Kry Yik-
Chau Lui for their insightful comments.