Masini, Ricardo P.; Medeiros, Marcelo C.; Mendes, Eduardo F.
Working Paper
Machine learning advances for time series forecasting
Texto para discussão, No. 679
Provided in Cooperation with:
Departamento de Economia, Pontifícia Universidade Católica do Rio de Janeiro
Suggested Citation: Masini, Ricardo P.; Medeiros, Marcelo C.; Mendes, Eduardo F. : Machine
learning advances for time series forecasting, Texto para discussão, No. 679, Pontifícia Universidade
Católica do Rio de Janeiro (PUC-Rio), Departamento de Economia, Rio de Janeiro
This Version is available at:
 
Standard-Nutzungsbedingungen:
Die Dokumente auf EconStor dürfen zu eigenen wissenschaftlichen
Zwecken und zum Privatgebrauch gespeichert und kopiert werden.
Sie dürfen die Dokumente nicht für öffentliche oder kommerzielle
Zwecke vervielfältigen, öffentlich ausstellen, öffentlich zugänglich
machen, vertreiben oder anderweitig nutzen.
Sofern die Verfasser die Dokumente unter Open-Content-Lizenzen
(insbesondere CC-Lizenzen) zur Verfügung gestellt haben sollten,
gelten abweichend von diesen Nutzungsbedingungen die in der dort
genannten Lizenz gewährten Nutzungsrechte.
Terms of use:
Documents in EconStor may be saved and copied for your personal
and scholarly purposes.
You are not to copy documents for public or commercial purposes, to
exhibit the documents publicly, to make them publicly available on the
internet, or to distribute or otherwise use the documents in public.
If the documents have been made available under an Open Content
Licence (especially Creative Commons Licences), you may exercise
further usage rights as specified in the indicated licence.
Machine Learning Advances for Time
Series Forecasting
Ricardo P. Masini
Marcelo C. Medeiros
Eduardo F. Mendes
TEXTO PARA DISCUSSÃO
DEPARTAMENTO DE ECONOMIA
www.econ.puc-rio.br
Machine Learning Advances for Time Series Forecasting
Ricardo P. Masini
S˜ao Paulo School of Economics, Getulio Vargas Foundation
E-mail: 
Marcelo C. Medeiros
Department of Economics, Pontiﬁcal Catholic University of Rio de Janeiro
E-mail: 
Eduardo F. Mendes
School of Applied Mathematics, Getulio Vargas Foundation
E-mail: 
December 23, 2020
In this paper we survey the most recent advances in supervised machine learning and highdimensional models for time series forecasting. We consider both linear and nonlinear
alternatives. Among the linear methods we pay special attention to penalized regressions
and ensemble of models. The nonlinear methods considered in the paper include shallow
and deep neural networks, in their feed-forward and recurrent versions, and tree-based
methods, such as random forests and boosted trees.
We also consider ensemble and
hybrid models by combining ingredients from diﬀerent alternatives. Tests for superior
predictive ability are brieﬂy reviewed. Finally, we discuss application of machine learning
in economics and ﬁnance and provide an illustration with high-frequency ﬁnancial data.
JEL Codes:C22
Keywords: Machine learning, statistical learning theory, penalized regressions, regularization, sieve approximation, nonlinear models, neural networks, deep learning, regression
trees, random forests, boosting, bagging, forecasting.
Acknowledgements: We are very grateful for the insightful comments made by two
anonymous referees.
The second author gratefully acknowledges the partial ﬁnancial
support from CNPq.
Introduction
This paper surveys the recent developments in Machine Learning (ML) methods to economic
and ﬁnancial time series forecasting.
ML methods have become an important estimation,
model selection and forecasting tool for applied researchers in Economics and Finance. With
the availability of vast datasets in the era of Big Data, producing reliable and robust forecasts
is of great importance.1
However, what is Machine Learning? It is certainly a buzzword which has gained a lot of
popularity during the last few years. There are a myriad of deﬁnitions in the literature and
one of the most well established is from the artiﬁcial intelligence pioneer Arthur L. Samuel
who deﬁnes ML as the “the ﬁeld of study that gives computers the ability to learn without
being explicitly programmed.”2 We prefer a less vague deﬁnition where ML is the combination
of automated computer algorithms with powerful statistical methods to learn (discover) hidden patterns in rich datasets. In that sense, Statistical Learning Theory gives the statistical
foundation of ML. Therefore, this paper is about Statistical Learning developments and not
ML in general as we are going to focus on statistical models. ML methods can be divided
into three major groups: supervised, unsupervised, and reinforcement learning. This survey is
about supervised learning, where the task is to learn a function that maps an input (explanatory variables) to an output (dependent variable) based on data organized as input-output
pairs. Regression models, for example, belongs to this class. On the other hand, unsupervised
learning is a class of ML methods that uncover undetected patterns in a data set with no
pre-existing labels as, for example, cluster analysis or data compression algorithms. Finally,
in reinforcement learning, an agent learns to perform certain actions in an environment which
lead it to maximum reward. It does so by exploration and exploitation of knowledge it learns
by repeated trials of maximizing the reward. This is the core of several artiﬁcial intelligence
game players (AlfaGo, for instance) as well as in sequential treatments, like Bandit problems.
The supervised ML methods presented here can be roughly divided in two groups. The ﬁrst
one includes linear models and are discussed in Section 2. We focus mainly on speciﬁcations
estimated by regularization, also known as shrinkage.
Such methods date back at least to
Tikhonov . In Statistics and Econometrics, regularized estimators gained attention after
the seminal papers by Willard James and Charles Stein who popularized the bias-variance tradeoﬀin statistical estimation . We start by considering the
Ridge Regression estimator put forward by Hoerl and Kennard . After that, we present
the Least Absolute Shrinkage and Selection (LASSO) estimator of Tibshirani and its
many extensions. We also include a discussion of other penalties. Theoretical derivations and
inference for dependent data are also reviewed.
The second group of ML techniques focus on nonlinear models. We cover this topic in Section
3 and start by presenting an uniﬁed framework based on sieve semiparametric approximation
1More recently, ML for causal inference have started to receive a lot of attention. However, this survey will
not cover causal inference with ML methods.
2The original sentence is “Programming computers to learn from experience should eventually eliminate the
need for much of this detailed programming eﬀort.” See, Samuel .
as in Grenander .
We continue by analysing speciﬁc models as special cases of our
general setup. More speciﬁcally, we cover feedforward neural networks, both in their shallow
and deep versions and recurrent neural networks, and tree-based models such as random forests
and boosted trees. Neural Networks (NN) are probably one of the most popular ML methods.
The success is partly due to the, in our opinion, misguided analogy to the functioning of the
human brain. Contrary of what has been boasted in the early literature, the empirical success
of NN models comes from a mathematical fact that a linear combination of suﬃciently many
simple basis functions is able to approximate very complicated functions arbitrarily well in
some speciﬁc choice of metric. Regression trees only achieved popularity after the development
of algorithms to attenuate the instability of the estimated models. Algorithms like Random
Forests and Boosted Trees are now in the toolbox of applied economists.
In addition to the models mentioned above, we also include a survey on ensemble-based
methods such as Bagging Breiman and the Complete Subset Regression . Furthermore, we give a brief introduction to what we named “hybrid methods”,
where ideas from both linear and nonlinear models are combined to generate new ML forecasting
Before presenting an empirical illustration of the methods, we discuss tests of superior
predictive ability in the context of ML methods.
General Framework
A quick word on notation: an uppercase letter as in X denotes a random quantity as opposed
to a lowercase letter x which denotes a deterministic (non-random) quantity. Bold letters as in
X and x are reserved for multivariate objects such as vector and matrices. The symbol ∥· ∥q
for q ≥1 denotes the ℓq norm of a vector. For a set S we use |S| to denote its cardinality.
Given a sample with T realizations of the random vector (Yt, Z′
t)′, the goal is to predict
YT+h for horizons h = 1, . . . , H. Throughout the paper, we consider the following assumption:
Assumption 1 (DGP). Let {(Yt, Z′
t=1 be a covariance-stationary stochastic process taking
values on Rd+1.
Therefore, we are excluding important non-stationary processes that usually appear in timeseries applications. In particular unit-root and some types on long-memory process are excluded
by Assumption 1.
For (usually predetermined) integers p ≥1 and r ≥0 deﬁne the n-dimensional vector
of predictors Xt :=
 Yt−1, . . . , Yt−p, Z′
t, . . . , Z′
′ where n = p + d(r + 1) and consider the
following direct forecasting model:
Yt+h = fh(Xt) + Ut+h,
h = 1, . . . , H,
t = 1, . . . , T,
where fh : Rn →R is an unknown (measurable) function and Ut+h := Yt+h −fh(Xt) is assumed
to be zero mean and ﬁnite variance3.
3The zero mean condition can be always ensured by including an intercept in the model. Also the variance
The model fh could be the conditional expectation function, fh(x) = E(Yt+h|Xt = x), or
simply the best linear projection of Yt+h onto the space spanned by Xt. Regardless of the model
choice, our target becomes fh, for h = 1, . . . , H. As fh is unknown, it should be estimated from
data. The target function fh can be a single model or an ensemble of diﬀerent speciﬁcations
and it can also change substantially for each forecasting horizon.
Given an estimate bfh for fh, the next step is to evaluate the forecasting method by estimating
its prediction accuracy. Most measures of prediction accuracy derives from the random quantity
∆h(Xt) := | bfh(Xt)−fh(Xt)|. For instance the term prediction consistency refers to estimators
such that ∆h(Xt)
−→0 as T →∞where the probability is taken to be unconditional; as
opposed to its conditional counterpart which is given by ∆h(xt)
−→0, where the probability
law is conditional on Xt = xt. Clearly, if the latter holds for (almost) every xt then the former
holds by the law of iterated expectation.
Other measures of prediction accuracy can be derived from the Lq norm induced by either
the unconditional probability law E|∆h(Xt)|q or the conditional one E(|∆h(Xt)|q|Xt = xt) for
q ≥1. By far, the most used are the (conditional) mean absolutely prediction error (MAPE)
when q = 1 and (conditional) mean squared prediction error (MSPE) when q = 2 or the
(conditional) root mean squared prediction error (RMSPE) which is simply the square root
of MSPE. Those measures of prediction accuracy based on the Lq norms are stronger than
prediction consistency in the sense that the converge to zero as sample since increases of any
of those (q ≥1) implies prediction consistency by Markov’s inequality.
This approach stems from casting economic forecasting as a decision problem. Under the
choice of a loss function, the goal is to select fh from a family of candidate models that minimises
the the expected predictive loss or risk. Given a estimate bfh for fh, the next step is to evaluate
the forecasting method by estimating its risk. The most commonly used losses are the absolute
error and squared error, corresponding to L1 and L2 risk functions, respectively. See Granger
and Machina for references a detailed exposition of this topic, Elliott and Timmermann
 for a discussion of the role of loss function in forecasting, and Elliott and Timmermann
 for a more recent review.
Sumamry of the Paper
Apart form this brief introduction, the paper is organized as follows. Section 2 reviews penalized
linear regression models. Nonlinear ML models are discussed in Section 3. Ensemble and hybrid
methods are presented in Section 4. Section 5 brieﬂy discusses tests for superior predictive
ability. An empirical application is presented in Section 6. Finally, we conclude and discuss
some directions for future research in Section 7.
of f(Xt) to be ﬁnite suﬃces for the ﬁnite variance
Penalized Linear Models
We consider the family of linear models where f(x) = β′
0x in (1.1) for a vector of unknown
parameters β0 ∈Rn. Notice that we drop the subscript h for clarity. However, the model as
well as the parameter β0 have to be understood for particular value of the forecasting horizon
h. These models contemplate a series of well-known speciﬁcations in time series analysis, such
as predictive regressions, autoregressive models of order p, AR(p), autoregressive models with
exogenous variables, ARX(p), autoregressive models with dynamic lags ADL(p, r), among
many others . In particular, (1.1) becomes
0Xt + Ut+h,
h = 1, . . . , H,
t = 1, . . . , T,
where, under squared loss, β0 is identiﬁed by the best linear projection of Yt+h onto Xt which is
well deﬁned whenever Σ := E(XtX′
t) is non-singular. In that case, Ut+h is orthogonal to Xt by
construction and this property is exploited to derive estimation procedures such as the Ordinary
Least Squares (OLS). However, when n > T (and sometimes n ≫T) the OLS estimator is
not unique as the sample counterpart of Σ is rank deﬁcient. In fact, we can completely overﬁt
whenever n ≥T.
Penalized linear regression arises in the setting where the regression parameter is not
uniquely deﬁned.
It is usually the case when n is large, possibly larger than the number
of observations T, and/or when covariates are highly correlated. The general idea is to restrict
the solution of the OLS problem to a ball around the origin. It can be shown that, although biased, the restricted solution has smaller mean squared error, when compared to the unrestricted
OLS .
In penalized regressions the estimator bβ for the unknown parameter vector β0 minimizes
the Lagrangian form
(Yt+h −β′Xt)
= ∥Y −Xβ∥2
where Y := (Yh+1, . . . YT)′, X := (X1, . . . XT−h)′ and p(β) := p(β; λ, γ, Z) ≥0 is a penalty
function that depends on a tuning parameter λ ≥0, that controls the trade-oﬀbetween the
goodness of ﬁt and the regularization term. If λ = 0, we have an the classical unrestricted
regression, since p(β; 0, γ, X) = 0. The penalty function may also depend on a set of extra
hyper-parameters γ, as well as on the data X. Naturally, the estimator bβ also depends on the
choice of λ and γ. Diﬀerent choices for the penalty functions were considered in the literature
of penalized regression.
Ridge Regression
The ridge regression was proposed by Hoerl and Kennard as a way to ﬁght highly
correlated regressors and stabilize the solution of the linear regression problem. The idea was
to introduce a small bias but, in turn, reduce the variance of the estimator. The ridge regression
is also known as a particular case of Tikhonov Regularization , in which the scale matrix is diagonal with identical entries.
The ridge regression corresponds to penalizing the regression by the squared ℓ2 norm of the
parameter vector, i.e., the penalty in (2.2) is given by
Ridge regression has the advantage of having an easy to compute analytic solution, where
the coeﬃcients associated with the least relevant predictors are shrunk towards zero, but never
reaching exactly zero. Therefore, it cannot be used for selecting predictors, unless some truncation scheme is employed.
Least Absolute Shrinkage and Selection Operator (LASSO)
The LASSO was proposed by Tibshirani and Chen et al. as a method to regularize and perform variable selection at the same time. LASSO is one of the most popular
regularization methods and it is widely applied in data-rich environments where number of
features n is much larger than the number of the observations.
LASSO corresponds to penalizing the regression by the ℓ1 norm of the parameter vector,
i.e., the penalty in (2.2) is given by
|βi| = λ∥β∥1.
The solution of the LASSO is eﬃciently calculated by coordinate descent algorithms . The ℓ1 penalty is the smallest convex ℓp penalty norm that yields sparse
solutions. We say the solution is sparse if only a subset k < n coeﬃcients are non-zero. In
other words, only a subset of variables is selected by the method. Hence, LASSO is most useful
when the total number of regressors n ≫T and it is not feasible to test combination or models.
Despite attractive properties, there are still limitations to the LASSO. A large number of
alternative penalties have been proposed to keep its desired properties whilst overcoming its
limitations.
Adaptive LASSO
The adaptive LASSO (adaLASSO) was proposed by H. Zou and aimed to improve the
LASSO regression by introducing a weight parameter, coming from a ﬁrst step OLS regression.
It also has sparse solutions and eﬃcient estimation algorithm, but enjoys the oracle property,
meaning that it has the same asymptotic distribution as the OLS conditional on knowing the
variables that should enter the model.4
The adaLASSO penalty consists in using a weighted ℓ1 penalty:
where ωi = |β∗
i |−1 and β∗
i is the coeﬃcient from the ﬁrst-step estimation (any consistent estimator of β0) AdaLASSO can deal with many more variables than observations. Using LASSO
as the ﬁrst-step estimator can be regarded as the two-step implementation of the local linear
approximation in Fan et al. with a zero initial estimate.
Elastic net
The elastic-net (ElNet) was proposed by Zou and Hastie as a way of combining strengths
of LASSO and ridge regression. While the L1 part of the method performs variable selection,
the L2 part stabilizes the solution. This conclusion is even more accentuated when correlations among predictors become high. As a consequence, there is a signiﬁcant improvement in
prediction accuracy over the LASSO .
The elastic-net penalty is a convex combination of ℓ1 and ℓ2 penalties:
i + (1 −α)
2 + (1 −α)∥β∥1],
where α ∈ . The elastic net has both the LASSO and ridge regression as special cases.
Just like in the LASSO regression, the solution to the elastic-net problem is eﬃciently calculated by coordinate descent algorithms. Zou and Zhang proposes the adaptive elastic
net. The elastic-net and adaLASSO improve the LASSO in distinct directions: the adaLASSO
has the oracle property and the elastic net helps with the correlation among predictors. The
adaptive elastic-net combines the strengths of both methods. It is a combination of ridge and
adaLASSO, where the ﬁrst-step estimator come from the elastic-net.
Folded concave penalization
LASSO approaches became popular in sparse high-dimensional estimation problems largely
due their computational properties.
Another very popular approach is the folded concave
penalization of Fan and Li .
This approach covers a collection of penalty functions
satisfying a set of properties. The penalties aim to penalize more parameters close to zero than
those that are further away, improving performance of the method. In this way, penalties are
concave with respect to each |βi|.
One of the most popular formulations is the SCAD (smoothly clipped absolute deviation).
4The oracle property was ﬁrst described in Fan and Li in the context of non-concave penalized
estimation.
Note that unlike LASSO, the penalty may depend on λ in a nonlinear way. We set the penalty
in (2.2) as p(β) = Pn
i=1 ep(βi, λ, γ) where
ep(u, λ, γ) =
2γλ|u|−u2−λ2
if λ ≤|u| ≤γλ
if |u| > γλ
for γ > 2 and λ > 0. The SCAD penalty is identical to the LASSO penalty for small coeﬃcients,
but continuously relaxes the rate of penalization as the coeﬃcient departs from zero. Unlike
OLS or LASSO, we have to solve a non-convex optimization problem that may have multiple
minima and is computationaly more intensive than the LASSO. Nevertheless, Fan et al. 
showed how to calculate the oracle estimator using an iterative Local Linear Approximation
algorithm.
Other Penalties
Regularization imposes a restriction on the solution space, possibly imposing sparsity. In a
data-rich environment it is a desirable property as it is likely that many regressors are not
relevant to our prediction problem. The presentation above concentrates on the, possibly, most
used penalties in time series forecasting. Nevertheless, there are many alternative penalties
that can be used in regularized linear models.
The group LASSO, proposed by Yuan and Lin , penalizes the parameters in groups,
combining the ℓ1 and ℓ2 norms. It is motivated by the problem of identifying ”factors”, denoted by groups of regressors as, for instance, in regression with categorical variables that can
assume many values. Let G = {g1, ..., gM} denote a partition of {1, ..., n} and βgi = [βi : i ∈gi]
the corresponding regression sub-vector. The group lasso assign to (2.2) the penalty p(β) =
|gi|∥βgi∥2, where |gi| is the cardinality of set gi. The solution is eﬃciently estimated
using, for instance, the group-wise majorization-descent algorithm Yang and H. Zou .
Naturally, the adaptive group LASSO was also proposed aiming to improve some of the limitations present on the group LASSO algorithm Wang and Leng . In the group LASSO,
the groups enter or not in the regression. The sparse group LASSO recover sparse groups by
combining the group LASSO penalty with the L1 penalty on the parameter vector modify the adaptive lasso penalty to explicitly take into account lag
information. Konzen and Ziegelmann propose a small change in penalty and perform a
large simulation study to asses the performance of this penalty in distinct settings. They observe
that taking into account lag information improves model selection and forecasting performance
when compared to the LASSO and adaLASSO. They apply their method to forecasting inﬂation
and risk premium with satisfactory results.
There is a Bayesian interpretation to the regularization methods presented here. The ridge
regression can be also seen as a maximum a posteriori estimator of a Gaussian linear regression
with independent, equivariant, Gaussian priors. The LASSO replaces the Gaussian prior by
a Laplace prior . These methods fall within the area of
Bayesian Shrinkage methods, which is a very large and active research area, and it is beyond
the scope of this survey.
Theoretical properties
In this section we give an overview of the theoretical properties of penalized regression estimators previously discussed. Most results in high-dimensional time series estimation focus on
model selection consistency, oracle property and oracle bounds, for both the ﬁnite dimension
(n ﬁxed, but possibly larger than T) and high-dimension (n increases with T, usually faster).
More precisely, suppose there is a population, parameter vector β0 that minimizes equation
(2.1) over repeated samples. Suppose this parameter is sparse in a sense that only components
indexed by S0 ⊂{1, ..., n} are non-null.
Let bS0 := {j : bβj ̸= 0}.
We say a method is
model selection consistent if the index of non-zero estimated components converges to S0 in
probability.5
P( bS0 = S0) →1,
Consistency can also be stated in terms of how close the estimator is to true parameter for a
given norm. We say that the estimation method is Lq-consistent if for every ϵ > 0:
P(∥bβ0 −β0∥q > ϵ) →0,
It is important to note that model selection consistency does not imply, nor it is implied by,
Lq-consistency. As a matter of fact, one usually have to impose speciﬁc assumptions to achieve
each of those modes of convergence.
Model selection performance of a given estimation procedure can be further broke down in
terms of how many relevant variables j ∈S0 are included in the model (screening). Or how
many irrelevant variables j /∈S0 are excluded from the model. In terms of probability, model
screening consistency is deﬁned by P( bS0 ⊇S0) →1 and model exclusion consistency deﬁned
by P( bS0 ⊆S0) →1 as T →∞.
We say a penalized estimator has the oracle property if its asymptotic distribution is the
same as the unpenalized one only considering the S0 regressors. Finally, oracle risk bounds are
ﬁnite sample bounds on the estimation error of bβ that hold with high probability. These bounds
require relatively strong conditions on the curvature of objective function, which translates into
a bound on the minimum restricted eigenvalue of the covariance matrix among predictors for
linear models and a rate condition on λ that involves the number of non-zero parameters, |S0|.
The LASSO was originally developed in ﬁxed design with independent and identically distributed (IID) errors, but it has been extended and adapted to a large set of models and
5A more precise treatment would separate sign consistency from model selection consistency. Sign consistency
ﬁrst appeared in Zhao and Yu and also verify whether the the sign of estimated regression weights converge
to the population ones.
designs. Knight and Fu was probably the ﬁrst paper to consider the asymptotics of
the LASSO estimator. The authors consider ﬁxed design and ﬁxed n framework. From their
results, it is clear that the distribution of the parameters related to the irrelevant variables is
non-Gaussian. To our knowledge, the ﬁrst work expanding the results to a dependent setting
was Wang et al. , where the error term was allowed to follow an autoregressive process.
Authors show that LASSO is model selection consistent, whereas a modiﬁed LASSO, similar
to the adaLASSO, is both model selection consistent and has the oracle property. Nardi and
Rinaldo shows model selection consistency and prediction consistency for lag selection
in autoregressive models. K-.S.Chan and Chen shows oracle properties and model selection consistency for lag selection in ARMA models. Yoon et al. derives model selection
consistency and asymptotic distribution of the LASSO, adaLASSO and SCAD, for penalized
regressions with autoregressive error terms. Sang and Sun studies lag estimation of
autoregressive processes with long memory innovations using general penalties and show model
selection consistency and asymptotic distribution for the LASSO and SCAD as particular cases.
Kock shows model selection consistency and oracle property of adaLASSO for lag selection in stationary and integrated processes. All results above hold for the case of ﬁxed number
of regressors or relatively high-dimension, meaning that n/T →0.
In sparse, high-dimensional, stationary univariate time-series settings, where n →∞at
some rate faster than T, Medeiros and Mendes show model selection consistency
and oracle property of a large set of linear time series models with diﬀerence martingale, strong
mixing, and non-Gaussian innovations. It includes, predictive regressions, autoregressive models AR(p), autoregressive models with exogenous variables ARX(p), autoregressive models with
dynamic lags ADL(p, r), with possibly conditionally heteroscedastic errors. Xie et al. 
shows oracle bounds for ﬁxed design regression with β-mixing errors. Wu and Wu derive
oracle bounds for the LASSO on regression with ﬁxed design and weak dependent innovations,
in a sense of Wu , whereas Han and Tsay show model selection consistency for
linear regression with random design and weak sparsity6 under serially dependent errors and covariates, within the same weak dependence framework. Xue and Taniguchi show model
selection consistency and parameter consistency for a modiﬁed version of the LASSO in time
series regressions with long memory innovations.
Fan and Li shows model selection consistency and oracle property for the folded
concave penalty estimators in a ﬁxed dimensional setting. Kim et al. showed that the
SCAD also enjoys these properties in high-dimensions. In time-series settings,Uematsu and
Tanaka shows oracle properties and model selection consistency in time series models
with dependent regressors. Lederer et al. derived oracle prediction bounds for many
penalized regression problems. The authors conclude that generic high dimensional penalized
estimators provide consistent prediction with any design matrix. Although the results are not
directly focused on time series problems, they are general enough to hold in such setting.
Babii et al. proposed the sparse-group LASSO as an estimation technique when
6Weak sparsity generalizes sparsity by supposing that coeﬃcients are (very) small instead of exactly zero.
high-dimensional time series data are potentially sampled at diﬀerent frequencies. The authors
derived oracle inequalities for the sparse-group LASSO estimator within a framework where
distribution of the data may have heavy tails.
Two frameworks not directly considered in this survey but of great empirical relevance are
nonstationary environments and multivariate models. In sparse, high-dimensional, integrated
time series settings, Lee and Z. Shi and Koo et al. show model selection consistency and derive the asymptotic distributions of LASSO estimators and some variants. Smeeks
and Wijler proposed the Single-equation Penalized Error Correction Selector (SPECS),
which is an automated estimation procedure for dynamic single-equation models with a large
number of potentially co-integrated variables. In sparse multivariate time series, Hsu et al.
 shows model selection consistency in VAR models with white-noise shocks. Ren and
Zhang uses adaLASSO in a similar setting, showing both model selection consistency
and oracle property. Afterwards, Callot et al. show model selection consistency and
oracle property of the adaptive Group LASSO. In high dimensional settings, where the dimension of the series increase with the number of observations, Kock and Callot ; Basu and
Michailidis shows oracle bounds and model selection consistency for the LASSO in Gaussian V AR(p) models, extending previous works. Melnyk and Banerjee extended these
results for a large collection of penalties. Zhu derive oracle estimation bounds for folded
concave penalties for Gaussian V AR(p) models in high dimensions. More recently researchers
have departed from gaussianity and correct model speciﬁcation. Wong et al. derived
ﬁnite-sample guarantees for the LASSO in a misspeciﬁed VAR model involving β-mixing process with sub-Weibull marginal distributions. Masini et al. derive equation-wise error
bounds for the LASSO estimator of weakly sparse V AR(p) in mixingale dependence settings,
that include models with conditionally heteroscedastic innovations.
Although several papers derived the asymptotic properties of penalized estimators as well as the
oracle property, these results have been derived under the assumption that the true non-zero
coeﬃcients are large enough. This condition is known as the β-min restriction. Furthermore,
model selection, such as the choice of the penalty parameter, has not been taken into account.
Therefore, the true limit distribution, derived under uniform asymptotics and without the βmin restriction can bee very diﬀerent from Gaussian, being even bimodal; see, for instance,
Leeb and P¨otscher , Leeb and P¨otscher , and Belloni et al. for a detailed
discussion.
Inference after model selection is actually a very active area of research and a vast number
of papers have recently appeared in the literature. van de Geer et al. proposed the
desparsiﬁed LASSO in order to construct (asymptotically) a valid conﬁdence interval for each
βj,0 by modifying the original LASSO estimate bβ. Let Σ∗be an approximation for the inverse
of Σ := E(XtX′
t), then the desparsiﬁed LASSO is deﬁned as eβ := bβ + Σ∗(Y −Xbβ)/T. The
addition of this extra term to the LASSO estimator results in an unbiased estimator that no
longer estimate any coeﬃcient exactly as zero. More importantly, asymptotic normality can
be recover in the sense that
T(eβi −βi,0) converges in distribution to a Gaussian distribution
under appropriate regularity conditions. Not surprisingly, the most important condition is how
well Σ−1 can be approximated by Σ∗. In particular, the authors propose to run n LASSO
regressions of Xi onto X−i := (X1, . . . , Xi−1, Xi+1, . . . , Xn), for 1 ≤i ≤n. The authors named
this process as nodewide regressions, and use those estimates to construct Σ∗ for details).
Belloni et al. put forward the double-selection method in the context of on a linear
model in the form Yt = β01X(1)
+ Ut, where the interest lies on the the scalar
parameter β01 and X(2)
is a high-dimensional vector of control variables. The procedure consists
in obtaining an estimation of the active (relevant) regressors in the high-dimension auxiliary
regressions of Yt on X(2) and of X(1)
t , given by bS1 and bS2, respectively.7 This can be
obtained either by LASSO or any other estimation procedure. Once the set bS := bS1 ∪bS2 is
identiﬁed, the (a priori) estimated non-zero parameters can by estimated by a low-dimensional
regression Yt on X(1)
: i ∈bS}. The main result )
states conditions under which the estimator bβ01 of the parameter of interest properly studentized
is asymptotically normal. Therefore, uniformly valid asymptotic conﬁdence intervals for β01 can
be constructed in the usual fashion.
Similar to Taylor et al. and Lockhart et al. , Lee et al. put forward
general approach to valid inference after model selection.
The idea is to characterize the
distribution of a post-selection estimator conditioned on the selection event. More speciﬁcally,
the authors argue that the post-selection conﬁdence intervals for regression coeﬃcients should
have the correct coverage conditional on the selected model. The speciﬁc case of the LASSO
estimator is discussed in details. The main diﬀerence between Lee et al. and Taylor et al.
 and Lockhart et al. is that in the former, conﬁdence intervals can be formed at
any value of the LASSO penalty parameter and any coeﬃcient in the model. Finally, it is
important to stress that Lee et al. inference is carried on the coeﬃcients of the selected
model, while van de Geer et al. and Belloni et al. consider inference on the
coeﬃcients of the true model.
The above papers do not consider a time-series environment. To our knowledge, Hecq et al.
 is the ﬁrst paper which attempts to consider post-selection inference in a time-series
environment. The authors generalize the results in Belloni et al. to dependent processes.
However, their results are derived under a ﬁxed number of variables. More recently, Ad´amek
et al. give a full treatment of post-selection inference in high-dimensional time-series
models. Their results consist of a generalization of the seminal work of van de Geer et al.
 . More speciﬁcally, the authors extend the desparsiﬁed LASSO to a time-series setting
under near-epoch dependence assumptions, allowing for non-Gaussian, serially correlated and
heteroskedastic processes. Furthermore, the number of regressors can possibly grow faster than
the sample size.
7The relevant regressors are the ones associated with non-zero parameter estimates.
Babii et al. consider inference in time-series regression models under heteroskedastic and autocorrelated errors. More speciﬁcaly, the authors considered heteroskedaticity- and
autocorrelation-consistent (HAC) estimation with sparse group-LASSO. They proposed a debiased central limit theorem for low dimensional groups of regression coeﬃcients and study the
HAC estimator of the long-run variance based on the sparse-group LASSO residuals.
Nonlinear Models
The function fh appearing (1.1) is unknown and in several applications the linearity assumption is too restrictive and more ﬂexible forms must be considered. Assuming a quadratic loss
function, the estimation problem turns to be the minimization of the functional
[Yt+h −f(Xt)]2 ,
where f ∈G, a generic function space. However, the optimization problem stated in (3.1) is
infeasible when G is inﬁnite dimensional, as there is no eﬃcient technique to search over all
G. Of course, one solution is to restrict the function space, as for instance, imposing linearity
or speciﬁc forms of parametric nonlinear models as in, for example, Ter¨asvirta , Suarez-
Fari˜nas et al. or McAleer and Medeiros ; see also Ter¨asvirta et al. for a
recent review of such models.
Alternatively, we can replace G by simpler and ﬁnite dimensional GD. The idea is to consider
a sequence of ﬁnite dimensional spaces, the sieve spaces, GD, D = 1, 2, 3, . . . , that converges to
G in some norm. The approximating function gD(Xt) is written as
where gj(·) is the j-th basis function for GD and can be either fully known or indexed by a
vector of parameters, such that: gj(Xt) := g(Xt; θj). The number of basis functions J := JT
will depend on the sample size T. D is the dimension of the space and it also depends on the
sample size: D := DT. Therefore, the optimization problem is then modiﬁed to
bgD(Xt) = arg
[Yt+h −gD(Xt)]2 .
The sequence of approximating spaces GD is chosen by using the structure of the original
underlying space G and the fundamental concept of dense sets. If we have two sets A and B
∈X, X being a metric space, A is dense in B if for any ϵ > 0, ∈R and x ∈B there is a y ∈A
such that ∥x −y∥X < ϵ. This is called the method of sieves. For a comprehensive review of
the method for time-series data, see Chen .
For example, from the theory of approximating functions we know that the proper subset
P ⊂C of polynomials is dense in C, the space of continuous functions. The set of polynomials is
smaller and simpler than the set of all continuous functions. In this case, it is natural to deﬁne
the sequence of approximating spaces GD, D = 1, 2, 3, . . . by making GD the set of polynomials
of degree smaller or equal to D −1 (including a constant in the parameter space). Note that
dim(GD) = D < ∞. In the limit this sequence of ﬁnite dimensional spaces converges to the
inﬁnite dimensional space of polynomials, which on its turn is dense in C.
When the basis functions are all known (linear sieves), the problem is linear in the parameters and methods like ordinary least squares (when J ≪T) or penalized estimation as
previously described can be used.
For example, let p = 1 and pick a polynomial basis such that
gD(Xt) = β0 + β1Xt + β2X2
t + · · · + βJXJ
In this case, the dimension D of GD is J + 1, due to the presence of a constant term.
If J << T, the vector of parameters β = (β1, . . . , βJ)′ can be estimated by
where XJ is the T × (J + 1) design matrix and Y = (Y1, . . . , YT)′.
When the basis functions are also indexed by parameters (nonlinear sieves), nonlinear
least-squares methods should be used. In this paper we will focus on frequently used nonlinear
sieves: neural networks and regression trees.
Neural Networks
Shallow Neural Networks
Neural Networks (NN) is one of the most traditional nonlinear sieves. NN can be classiﬁed into
shallow or deep networks. We start describing the shallow NNs. The most common shallow
NN is the feedforward neural network where the the approximating function gD(Xt) is deﬁned
gD(Xt) := gD(Xt; θ) = β0 +
jXt + γ0,j),
In the above model, ˜Xt = (1, X′
t)′, Sj(·) is a basis function and the parameter vector to be
estimated is given by θ = (β0, . . . , βK, γ′
1, . . . , γ′
JT , γ0,1, . . . , γ0,JT )′, where ˜γj = (γ0,j, γ′
NN models form a very popular class of nonlinear sieves and have been used in many
applications of economic forecasting. Usually, the basis functions S(·) are called activation
functions and the parameters are called weights. The terms in the sum are called hidden-
Figure 1: Graphical representation of a single hidden layer neural network.
neurons as an unfortunate analogy to the human brain. Speciﬁcation (3.3) is also known as a
single hidden layer NN model as is usually represented in the graphical as in Figure 1. The green
circles in the ﬁgure represent the input layer which consists of the covariates of the model (Xt).
In the example in the ﬁgure there are four input variables. The blue and red circles indicate the
hidden and output layers, respectively. In the example, there are ﬁve elements (neurons) in the
hidden layer.The arrows from the green to the blue circles represent the linear combination of
inputs: γ′
jXt + γ0,j, j = 1, . . . , 5. Finally, the arrows from the blue to the red circles represent
the linear combination of outputs from the hidden layer: β0 + P5
j=1 βjS(γ′
jXt + γ0,j).
There are several possible choices for the activation functions. In the early days, S(·) was
chosen among the class of squashing functions as per the deﬁnition bellow.
Deﬁnition 1. A function S : R −→[a, b], a < b, is a squashing (sigmoid) function if it is
non-decreasing,
x−→∞S(x) = b and
x−→−∞S(x) = a.
Historically, the most popular choices are the logistic and hyperbolic tangent functions such
Logistic: S(x) =
1 + exp(−x)
Hyperbolic tangent: S(x) = exp(x) −exp(−x)
exp(x) + exp(−x).
The popularity of such functions was partially due to theoretical results on function approximation. Funahashi establishes that NN models as in (3.3) with generic squashing
functions are capable of approximating any continuous functions from one ﬁnite dimensional
space to another to any desired degree of accuracy, provided that JT is suﬃciently large. Cybenko and Hornik et al. simultaneously proved approximation capabilities of NN
models to any Borel measurable function and Hornik et al. extended the previous results
and showed the NN models are also capable to approximate the derivatives of the unknown
function. Barron relate previous results to the number of terms in the model.
Stinchcombe and White and Park and Sandberg derived the same results of
Cybenko and Hornik et al. but without requiring the activation function to be
sigmoid. While the former considered a very general class of functions, the later focused on
radial-basis functions (RBF) deﬁned as:
Radial Basis: S(x) = exp(−x2).
More recently, Yarotsky showed that the rectiﬁed linear units (ReLU) as
Rectiﬁed Linear Unit: S(x) = max(0, x),
are also universal approximators.
Model (3.3) can be written in matrix notation. Let Γ = (˜γ1, . . . , ˜γK),
, and O(XΓ) =
Therefore, by deﬁning β = (β0, β1, . . . , βK)′, the output of a feed-forward NN is given by:
hD(X, θ) = [hD(X1; θ), . . . , hD(XT; θ)]′
k=1 βkS(γ′
kX1 + γ0,k)
k=1 βkS(γ′
kXT + γ0,k)
The dimension of the parameter vector θ = [vec (Γ)′, β′]′ is k = (n + 1) × JT + (JT + 1) and
can easily get very large such that the unrestricted estimation problem deﬁned as
bθ = arg min
θ∈Rk∥Y −O(XΓ)β∥2
is unfeasible. A solution is to use regularization as in the case of linear models and consider
the minimization of the following function:
Q(θ) = ∥Y −O(XΓ)β∥2
where usually p(θ) = λθ′θ. Traditionally, the most common approach to minimze (3.5) is to
use Bayesian methods as in MacKay , MacKay , and Foresee and Hagan .
A more modern approach is to use a technique known as Dropout .
The key idea is to randomly drop neurons (along with their connections) from the neural network during estimation. A NN with JT neurons in the hidden layer can generate 2JT
possible “thinned” NN by just removing some neurons. Dropout samples from this 2JT different thinned NN and train the sampled NN. To predict the target variable, we use a single
Figure 2: Deep neural network architecture
unthinned network that has weights adjusted by the probability law induced by the random
drop. This procedure signiﬁcantly reduces overﬁtting and gives major improvements over other
regularization methods.
We modify equation (3.3) by
D(Xt) = β0 +
j [r ⊙Xt] + vjγ0,j),
where s, v, and r = (r1, . . . , rn) are independent Bernoulli random variables each with probability q of being equal to 1. The NN model is thus estimated by using g∗
D(Xt) instead of
gD(Xt) where, for each training example, the values of the entries of r are drawn from the
Bernoulli distribution. The ﬁnal estimates for βj, γj, and γo,j are multiplied by q.
Deep Neural Networks
A Deep Neural Network model is a straightforward generalization of speciﬁcation (3.3) where
more hidden layers are included in the model as represented in Figure 2. In the ﬁgure we
represent a Deep NN with two hidden layers with the same number of hidden units in each.
However, the number of hidden neurons can vary across layers.
As pointed out in Mhaska et al. , while the universal approximation property holds
for shallow NNs, deep networks can approximate the class of compositional functions as well
as shallow networks but with exponentially lower number of training parameters and sample
complexity.
Set Jℓas the number of hidden units in layer ℓ∈{1, . . . , L}. For each hidden layer ℓdeﬁne
Γℓ= (˜γ1ℓ, . . . , ˜γkℓℓ). Then the output Oℓof layer ℓis given recursively by
Oℓ(Oℓ−1(·)Γℓ)
1ℓO1ℓ−1(·))
kℓℓO1ℓ−1(·))
1ℓO2ℓ−1(·))
kℓℓO2ℓ−1(·))
1ℓOnℓ−1(·))
JℓℓOnℓ−1(·))
where Oo := X. Therefore, the output of the Deep NN is the composition
hD(X) = OL(· · · O3(O2(O1(XΓ1)Γ2)Γ3) · · · )ΓLβ.
The estimation of the parameters is usually carried out by stochastic gradient descend
methods with dropout to control the complexity of the model.
Recurrent Neural Networks
Broadly speaking, Recurrent Neural Networks (RNNs) are NNs that allow for feedback among
the hidden layers. RNNs can use their internal state (memory) to process sequences of inputs.
In the framework considered in this paper, a generic RNN could be written as
Ht = f(Ht−1, Xt),
bYt+h|t = g(Ht),
where bYt+h|t is the prediction of Yt+h given observations only up to time t, f and g are functions
to be deﬁned and Ht is what we call the (hidden) state. From a time-series perspective, RNNs
can be see as a kind of nonlinear state-space model.
RNNs can remember the order that the inputs appear through its hidden state (memory)
and they can also model sequences of data so that each sample can be assumed to be dependent
on previous ones, as in time series models. However, RNNs are hard to be estimated as they
suﬀer from the vanishing/exploding gradient problem. Set the cost function to be
Yt+h −bYt+h|t
where θ is the vector of parameters to be estimated. It is easy to show that the gradient
can be very small or diverge. Fortunately, there is a solution to the problem proposed
by Hochreiter and Schmidhuber . A variant of RNN which is called Long-Short-Term
Memory (LSTM) network . Figure 3 shows the architecture of a typical LSTM layer. A LSTM
network can be composed of several layers. In the ﬁgure, red circles indicate logistic activation
functions, while blue circles represent hyperbolic tangent activation. The symbols “X” and “+”
represent, respectively, the element-wise multiplication and sum operations. The RNN layer
is composed of several blocks: the cell state and the forget, input, and ouput gates. The cell
state introduces a bit of memory to the LSTM so it can “remember” the past. LSTM learns to
keep only relevant information to make predictions, and forget non relevant data. The forget
gate tells which information to throw away from the cell state. The output gate provides the
activation to the ﬁnal output of the LSTM block at time t. Usually, the dimension of the hidden
state (Ht) is associated with the number of hidden neurons.
Algorithm 1 describes analytically how the LSTM cell works. f t represents the output
of the forget gate. Note that it is a combination of the previous hidden-state (Ht−1) with
CELL STATE
FORGET GATE
INPUT GATE
OUTPUT GATE
Figure 3: Architecture of the Long-Short-Term Memory Cell (LSTM)
the new information (Xt). Note that f t ∈ and it will attenuate the signal coming com
ct−1. The input and output gates have the same structure. Their function is to ﬁlter the
“relevant” information from the previous time period as well as from the new input. pt scales
the combination of inputs and previous information. This signal will be then combined with
the output of the input gate (it). The new hidden state will be an attenuation of the signal
coming from the output gate. Finally, the prediction is a linear combination of hidden states.
Figure 4 illustrates how the information ﬂows in a LSTM cell.
Algorithm 1. Mathematically, RNNs can be deﬁned by the following algorithm:
1. Initiate with c0 = 0 and H0 = 0.
2. Given the input Xt, for t ∈{1, . . . , T}, do:
f t = Logistic(W fXt + U fHt−1 + bf)
it = Logistic(W iXt + U iHt−1 + bi)
ot = Logistic(W oXt + U oHt−1 + bo)
pt = Tanh(W cXt + U cHt−1 + bc)
ct = (f t ⊙ct−1) + (it ⊙pt)
ht = ot ⊙Tanh(ct)
bY t+h|t = W yht + by
Figure 4: Information ﬂow in a LTSM Cell
where U f, U i, U o ,U c ,U f, W f, W i, W o, W c, bf, bi, bo, and bc are parameters to be
estimated.
Regression Trees
A regression tree is a nonparametric model that approximates an unknown nonlinear function
fh(Xt) in (1.1) with local predictions using recursive partitioning of the space of the covariates.
A tree may be represented by a graph as in the left side of Figure 5, which is equivalent as the
partitioning in the right side of the ﬁgure for this bi-dimensional case. For example, suppose
that we want to predict the scores of basketball players based on their height and weight. The
ﬁrst node of the tree in the example splits the players taller than 1.85m from the shorter players.
The second node in the left takes the short players groups and split them by weights and the
second node in the right does the same with the taller players. The prediction for each group is
displayed in the terminal nodes and they are calculated as the average score in each group. To
grow a tree we must ﬁnd the optimal splitting point in each node, which consists of an optimal
variable and an optimal observation. In the same example, the optimal variable in the ﬁrst
node is height and the observation is 1.85m.
The idea of regression trees is to approximate fh(Xt) by
if Xt ∈Rj,
otherwise.
From the above expression, it becomes clear that the approximation of fh(·) is equivalent to a
linear regression on JT dummy variables, where Ij(Xt) is a product of indicator functions.
Let J := JT and N := NT be, respectively, the number of terminal nodes (regions, leaves)
Figure 5: Example of a simple tree.
and parent nodes. Diﬀerent regions are denoted as R1, . . . , RJ. The root node at position
0. The parent node at position j has two split (child) nodes at positions 2j + 1 and 2j + 2.
Each parent node has a threshold (split) variable associated, Xsjt, where sj ∈S = {1, 2, . . . , p}.
Deﬁne J and T as the sets of parent and terminal nodes, respectively. Figure 6 gives an example.
In the example, the parent nodes are J = {0, 2, 5} and the terminal nodes are T = {1, 6, 11, 12}.
Therefore, we can write the approximating model as
βiBJi (Xt; θi) ,
BJi (Xt; θi) =
I(Xsj,t; cj)
ni,j(1+ni,j)
1 −I(Xsj,t; cj)
(1−ni,j)(1+ni,j) ,
I(Xsj,t; cj) =
if Xsj,t ≤cj
otherwise,
if the path to leaf i does not include parent node j;
if the path to leaf i include the right-hand child of parent node j;
if the path to leaf i include the left-hand child of parent node j.
Ji: indexes of parent nodes included in the path to leaf i. θi = {ck} such that k ∈Ji, i ∈T
j∈J BJi (Xt; θj) = 1.
(Region 1)
(Region 2)
(Region 3)
(Region 4)
Figure 6: Example of tree with labels.
Random Forests
Random Forest (RF) is a collection of regression trees, each speciﬁed in a bootstrap sample of
the original data. The method was originally proposed by Breiman . Since we are dealing
with time series, we use a block bootstrap. Suppose there are B bootstrap samples. For each
sample b, b = 1, . . . , B, a tree with Kb regions is estimated for a randomly selected subset of
the original regressors. Kb is determined in order to leave a minimum number of observations
in each region. The ﬁnal forecast is the average of the forecasts of each tree applied to the
original data:
bYt+h|t = 1
bβi,bBJi,b(Xt; bθi,b)
The theory for RF models has been developed only to independent and identically distributed random variables. For instance, Scornet et al. proves consistency of the RF
approximation to the unknown function fh(Xt). More recently, Wager and Athey proved
consistency and asymptotic normality of the RF estimator.
Boosting Regression Trees
Boosting is another greedy method to approximate nonlinear functions that uses base learners
for a sequential approximation. The model we consider here, called Gradient Boosting, was
introduced by Friedman and can be seen as a Gradient Descendent method in functional
The study of statistical properties of the Gradient Boosting is well developed for independent
data. For example, for regression problems, Duﬀy and Helmbold derived bounds on
the convergence of boosting algorithms using assumptions on the performance of the base
learner. Zhang and Yu proves convergence, consistency and results on the speed of
convergence with mild assumptions on the base learners.
B¨uhlmann shows similar
results for consistency in the case of ℓ2 loss functions and three base models. Since boosting
indeﬁnitely leads to overﬁtting problems, some authors have demonstrated the consistency of
boosting with diﬀerent types of stopping rules, which are usually related to small step sizes, as
suggested by Friedman . Some of these works include boosting in classiﬁcation problems
and gradient boosting for both classiﬁcation and regression problems. See, for instance, Jiang
 ; Lugosi and Vayatis ; Bartlett and M. Traskin ; Zhang and Yu ;
B¨uhlmann ; B¨uhlmann .
Boosting is an iterative algorithm. The idea of boosted trees is to, at each iteration, sequentially reﬁt the gradient of the loss function by small trees. In the case of quadratic loss as
considered in this paper, the algorithm simply reﬁt the residuals from the previous iteration.
Algorithm (2) presents the simpliﬁed boosting procedure for a quadratic loss. It is recommended to use a shrinkage parameter v ∈(0, 1] to control the learning rate of the algorithm.
If v is close to 1, we have a faster convergence rate and a better in-sample ﬁt. However, we
are more likely to have over-ﬁtting and produce poor out-of-sample results. Additionally, the
derivative is highly aﬀected by over-ﬁtting, even if we look at in-sample estimates. A learning
rate between 0.1 and 0.2 is recommended to maintain a reasonable convergence ratio and to
limit over-ﬁtting problems.
Algorithm 2. The boosting algorithm is deﬁned as the following steps.
1. Initialize φi0 = ¯Y := 1
2. For m = 1, . . . , M:
(a) Make Utm = Yt −φtm−1
(b) Grow a (small) Tree model to ﬁt utm, butm = P
i∈Tm bβimBJmi(Xt; bθim)
(c) Make ρm = arg min
t=1[utm −ρbutm]2
(d) Update φtm = φtm−1 + vρmbutm
The ﬁnal ﬁtted value may be written as
bYt+h = ¯Y +
bβkmBJmk(Xt; bθkm)
Conducting inference in nonlinear ML methods is tricky. One possible way is to follow Medeiros
et al. , Medeiros and Veiga and Suarez-Fari˜nas et al. and interpret particular
nonlinear ML speciﬁcations as parametric models, as for example, general forms of smooth
transition regressions. However, this approach restricts the application of ML methods to very
speciﬁc settings. An alternative, is to consider models that can be cast in the sieves framework
as described earlier. This is the case of splines and feed-forward NNs, for example. In this setup,
Chen and Shen and Chen derived, under regularity conditions, the consistency
and asymptotically normality of the estimates of a semi-parametric sieve approximations. Their
setup is deﬁned as follows:
0Xt + f(Xt) + Ut+h,
where f(Xt) is a nonlinear function that is nonparametrically modeled by sieve approximations.
Chen and Shen and Chen consider both the estimation of the linear and nonlinear
components of the model. However, their results are derived under the case where the dimension
of Xt is ﬁxed.
Recently, Chernozhukov et al. and Chernozhukov et al. consider the case
where the number of covariates diverge as the sample size increases in a very general setup. In
this case the asymptotic results in Chen and Shen and Chen are not valid and
the authors put forward the so-called double ML methods as a nice generalization to the results
of Belloni et al. . Nevertheless, the results do not include the case of time-series models.
More speciﬁcally to the case of Random Forests, asymptotic and inferential results have
derived in Scornet et al. and Wager and Athey . As the papers above, the results
for Random Forests have been proved only for IID data.
Other Methods
The term bagging means Bootstrap Aggregating and was proposed by Breiman to reduce the variance of unstable predictors8. It was popularized in the time series literature by
Inoue and Kilian , who to construct forecasts from multiple regression models with localto-zero regression parameters and errors subject to possible serial correlation or conditional
heteroscedasticity.
Bagging is designed for situations in which the number of predictors is
moderately large relative to the sample size.
The bagging algorithm in time series settings have to take into account the time dependence
dimension when constructing the bootstrap samples.
Algorithm 3 (Bagging for Time-Series Models). The Bagging algorithm is deﬁned as follows.
1. Arrange the set of tuples (yt+h, x′
t), t = h + 1, . . . , T, in the form of a matrix V of
dimension (T −h) × n.
2. Construct (block) bootstrap samples of the form
1, . . . , B, by drawing blocks of M rows of V with replacement.
8An unstable predictor has large variance. Intuitively, small changes in the data yield large changes in the
predictive model
3. Compute the ith bootstrap forecast as
(i)t+h|t =
j| < c ∀j,
otherwise,
(i)t := S∗
(i)t and St is a diagonal selection matrix with jth diagonal element
I{|tj|>c} =
if |tj| > c,
otherwise,
c is a pre-speciﬁed critical value of the test. bλ
(i) is the OLS estimator at each bootstrap
repetition.
4. Compute the average forecasts over the bootstrap samples:
˜yt+h|t = 1
In algorithm 3, above, one requires that it is possible to estimate and conduct inference
in the linear model. This is certainly infeasible if the number of predictors is larger than the
sample size (n > T), which requires the algorithm to be modiﬁed. Garcia et al. and
Medeiros et al. adopt the following changes of the algorithm:
Algorithm 4 (Bagging for Time-Series Models and Many Regressors). The Bagging algorithm
is deﬁned as follows.
0. Run n univariate regressions of yt+h on each covariate in xt. Compute t-statistics and
keep only the ones that turn out to be signiﬁcant at a given pre-speciﬁed level. Call this
new set of regressors as ˇxt
1–4. Same as before but with xt replaced by ˇxt.
Complete Subset Regression
Complete Subset Regression (CSR) is a method for combining forecasts developed by Elliott
et al. . The motivation was that selecting the optimal subset of Xt to predict Yt+h
by testing all possible combinations of regressors is computationally very demanding and, in
most cases, unfeasible. For a given set of potential predictor variables, the idea is to combine
forecasts by averaging9 all possible linear regression models with ﬁxed number of predictors.
For example, with n possible predictors, there are n unique univariate models and
9It is possible to combine forecasts using any weighting scheme. However, it is diﬃcult to beat uniform
weighting Genre et al. .
diﬀerent k-variate models for k ≤K. The set of models for a ﬁxed value of k as is known as
the complete subset.
When the set of regressors is large the number of models to be estimated increases rapidly.
Moreover, it is likely that many potential predictors are irrelevant.
In these cases it was
suggested that one should include only a small, k, ﬁxed set of predictors, such as ﬁve or ten.
Nevertheless, the number of models still very large, for example, with n = 30 and k = 8,
there are 5, 852, 925 regression. An alternative solution is to follow Garcia et al. and
Medeiros et al. and adopt a similar strategy as in the case of Bagging high-dimensional
models. The idea is to start ﬁtting a regression of Yt+h on each of the candidate variables and
save the t-statistics of each variable. The t-statistics are ranked by absolute value, and we
select the ˜n variables that are more relevant in the ranking. The CSR forecast is calculated on
these variables for diﬀerent values of k. This approach is based on the the Sure Independence
Screening of Fan and Lv , extended to dependent by Yousuf , that aims to select
a superset of relevant predictors among a very large set.
Hybrid Methods
Recently, Medeiros and Mendes proposed the combination of LASSO-based estimation
and NN models.
The idea is to construct a feedforward single-hidden layer NN where the
parameters of the nonlinear terms (neurons) are randomly generated and the linear parameters
are estimated by LASSO (or one of its generalizations). Similar ideas were also considered by
Kock and Ter¨asvirta and Kock and Ter¨asvirta .
Trapletti et al. and Medeiros et al. proposed to augment a feedforward shallow
NN by a linear term. The motivation is that the nonlinear component should capture only the
nonlinear dependence, making the model more interpretable. This is in the same spirit of the
semi-parametric models considered in Chen .
Inspired by the above ideas, Medeiros et al. proposed combining random forests
with adaLASSO and OLS. The authors considered two speciﬁcations. In the ﬁrst one, called
RF/OLS, the idea is to use the variables selected by a Random Forest in a OLS regression.
The second approach, named adaLASSO/RF, works in the opposite direction.
First select
the variables by adaLASSO and than use them in a Random Forest model. The goal is to
disentangle the relative importance of variable selection and nonlinearity to forecast inﬂation.
Forecast Comparison
With the advances in the ML literature, the number of available forecasting models and methods
have been increasing at a fast pace. Consequently, it is very important to apply statistical tools
to compare diﬀerent models. The forecasting literature provides a number of tests since the
seminal paper by Diebold and Mariano that can be applied as well to the ML models
described in this survey.
In the Diebold and Mariano’s test, two competing methods have the same unconditional expected loss under the null hypothesis, and the test can be carried out using a simple
t-test. A small sample adjustment was developed by Harvey et al. . See also the recent
discussion in Diebold . One drawback of the Diebold and Mariano’s test is that its
statistic diverges under null when the competing models are nested. However, Giacomini and
White show that the test is valid if the forecasts are derived from models estimated in
a rolling window framework. Recently, McCracken shows that if the estimation window
is ﬁxed, the Diebold and Mariano’s statistic may diverge under the null. Therefore, it
is very important that the forecasts are computed in a rolling window scheme.
In order to accommodate cases where there are more than two competing models, an unconditional superior predictive ability (USPA) test was proposed by White . The null
hypothesis states that a benchmark method outperforms a set of competing alternatives. However, Hansen showed that White’s test can be very conservative when there are
competing methods that are inferior to the benchmark. Another important contribution to the
forecasting literature is the model conﬁdence set (MCS) proposed by Hansen et al. . A
MCS is a set of competing models that is built in a way to contain the best model with respect
to a certain loss function and with a given level of conﬁdence. The MCS acknowledges the
potential limitations of the dataset, such that uninformative data yield a MCS with a large
number models, whereas informative data yield a MCS with only a few models. Importantly,
the MCS procedure does not assume that a particular model is the true one.
Another extension of the Diebold and Mariano’s test is the conditional equal predictive ability (CEPA) test proposed by Giacomini and White . In practical applications, it
is important to know not only if a given model is superior but also when it is better than the alternatives. Recently, Li et al. proposed a very general framework to conduct conditional
predictive ability tests.
In summary, it is very important to compare the forecasts from diﬀerent ML methods and
the literature provides a number of tests that can be used.
Applications of Machine Learning Methods to Economic and Financial Forecasting
Linear Methods
Penalized regressions are now an important option in the toolkit of applied economists are there
is a vast literature considering the use of such techniques to economics and ﬁnancial forecasting.
Macroeconomic forecasting is certainly one of the most successful applications of penalized
regressions. Medeiros and Mendes applied the adaLASSO to forecasting US inﬂation
and showed that the method outperforms the linear autoregressive and factor models. Medeiros
and Vasconcelos show that high-dimensional linear models produce, on average, smaller
forecasting errors for macroeconomic variables when a large set of predictors is considered.
Their results also indicate that a good selection of the adaLASSO hyperparameters reduces
forecasting errors. Garcia et al. show that high-dimensional econometric models, such
as shrinkage and complete subset regression, perform very well in real time forecasting of Brazilian inﬂation in data-rich environments. The authors combine forecasts of diﬀerent alternatives
and show that model combination can achieve superior predictive performance. Smeeks and
Wijler consider an application to a large macroeconomic US dataset and demonstrate
that penalized regressions are very competitive. Medeiros et al. conduct a vast comparison of models to forecast US inﬂation and showed the penalized regressions were far superior
than several benchmarks, including factor models. Ardia et al. introduce a general text
sentiment framework that optimizes the design for forecasting purposes and apply it to forecasting economic growth in the US. The method includes the use of the elastic net for sparse
data-driven selection and the weighting of thousands of sentiment values. Tarassow 
consider penalized VARs to forecast six diﬀerent economic uncertainty variables for the growth
of the real M2 and real M4 Divisia money series for the US using monthly data. Uematsu and
Tanaka consider high-dimensional forecasting and variable selection via folded-concave
penalized regressions. The authors forecast quarterly US gross domestic product data using
a high-dimensional monthly data set and the mixed data sampling (MIDAS) framework with
penalization. See also Babii et al. and Babii et al. .
There is also a vast list of applications in empirical ﬁnance. Elliott et al. ﬁnd that
combinations of subset regressions can produce more accurate forecasts of the equity premium
than conventional approaches based on equal-weighted forecasts and other regularization techniques. Audrino and Knaus used LASSO-based methods to estimated forecasting models
for realized volatilities. Callot et al. consider modelling and forecasting large realized
covariance matrices of the 30 Dow Jones stocks by penalized vector autoregressive (VAR) models. The authors ﬁnd that penalized VARs outperform the benchmarks by a wide margin and
improve the portfolio construction of a mean-variance investor. Chinco et al. use the
LASSO to make 1-minute-ahead return forecasts for a vast set of stocks traded at the New
York Stock Exchange. The authors provide evidence that penalized regression estimated by
the LASSO boost out-of-sample predictive power by choosing predictors that trace out the
consequences of unexpected news announcements.
Nonlinear Methods
There are many papers on the application of nonlinear ML methods to economic and ﬁnancial forecasting. Most of the papers focus on NN methods, specially the ones from the early
literature.
With respect to the early papers, most of the models considered were nonlinear versions
of autoregressive models. At best, a small number of extra covariates were included. See, for
example, Ter¨asvirta et al. and the references therein. In the majority of the papers,
including Ter¨asvirta et al. , there was no strong evidence of the superiority of nonlinear
models as the diﬀerences in performance were marginal. Other examples from the early litera-
ture are Swanson and White , Swanson and White , Swanson and White ,
Balkin and Ord , Tkacz , Medeiros et al. , and Heravi et al. .
More recently, with the availability of large datasets, nonlinear models are back to the scene.
For example, Medeiros et al. show that, despite the skepticism of the previous literature
on inﬂation forecasting, ML models with a large number of covariates are systematically more
accurate than the benchmarks for several forecasting horizons and show that Random Forests
dominated all other models. The good performance of the Random Forest is due not only
to its speciﬁc method of variable selection but also the potential nonlinearities between past
key macroeconomic variables and inﬂation. Other successful example is Gu et al. . The
authors show large economic gains to investors using ML forecasts of future stock returns
based on a very large set of predictors. The best performing models are tree-based and neural
networks. Coulombe et al. show signiﬁcant gains when nonlinear ML methods are used
to forecast macroeconomic time series.
Empirical Illustration
In this section we illustrate the use of some of the methods reviewed in this paper to forecast
daily realized variance of the Brazilian Stock Market index (BOVESPA). We use as regressors
information from other major indexes, namely, the S&P500 (US), the FTSE100 (United Kingdom), DAX (Germany), Hang Seng (Hong Kong), and Nikkei (Japan). Our measure of realized
volatility is constructed by aggregating intraday returns sample at the 5-minute frequency. The
data were obtained from the Oxford-Man Realized Library at Oxford University.10
For each stock index, we deﬁne the realized variance as
where rst is the log return sampled at the ﬁve-minute frequency. S is the number of available
returns at day t.
The benchmark model is the Heterogeneous Autoregressive (HAR) model proposed by Corsi
log RVt+1 = β0 + β1 log RVt + β5 log RV5,t + β22 log RV22,t + Ut+1,
where RVt is daily realized variance of the BOVESPA index,
RV22,t = 1
As alternatives we consider a extended HAR model with additional regressors estimated by
10 
adaLASSO. We include as extra regressors the daily past volatility of the other ﬁve indexes
considered here. The model has a total of eight candidate predictors. Furthermore, we consider
two nonlinear alternatives using all predictors: a random forest and shallow and deep neural
The realized variances of the diﬀerent indexes are illustrated in Figure 7. The data starts in
February 2, 2000 and ends in May 21, 2020, a total of 4,200 observations. The sample includes
two periods of very high volatility, namely the ﬁnancial crisis of 2007-2008 and the Covid-19
pandemics of 2020. We consider a rolling window exercise, were we set 1,500 observations in
each window. The models are re-estimated every day.
Several other authors have estimated nonlinear and machine learning models to forecast
realized variances. McAleer and Medeiros considered a smooth transition version of the
HAR while Hillebrand and Medeiros considered the combination of smooth transitions,
long memory and neural network models. Hillebrand and Medeiros and McAleer and
Medeiros combined NN models with bagging and Scharth and Medeiros considered smooth transition regression trees. The use of LASSO and its generalizations to estimate
extensions of the HAR model was proposed by Audrino and Knaus .
Although the models are estimated in logarithms, we report the results in levels, which in
the end is the quantity of interest. We compare the models according to the Mean Squared
Error (MSE) and the QLIKE metric.
The results are shown in Table 1. The table reports for each model, the mean squared
error (MSE) and the QLIKE statistics as a ratio to the HAR benchmark. Values smaller than
one indicates that the model outperforms the HAR. The asterisks indicate the results of the
Diebold-Mariano test of equal forecasting performance. *,**, and ***, indicate rejection of the
null of equal forecasting ability at the 10%, 5% and 1%, respectively. We report results for
the full out-of-sample period, the ﬁnancial crisis years and the for 2020 as a way
to capture the eﬀects of the Covid-19 pandemics on the forecasting performance of diﬀerent
As we can see from the tables the ML methods considered here outperform the HAR benchmark. The winner model is deﬁnitely the HAR model with additional regressors and estimated
with adaLASSO. The performance improves during the high volatility periods and the gains
reach 10% during the Covid-19 pandemics. Random Forests do not perform well. On the other
hand NN models with diﬀerent number of hidden layers outperform the benchmark.
Conclusions and the Road Ahead
In this paper we present a non-exhaustive review of the most of the recent developments in
machine learning and high-dimensional statistics to time-series modeling and forecasting. We
presented both linear and nonlinear alternatives. Furthermore, we consider ensemble and hybrid
models. Finally, we brieﬂy discuss tests for superior predictive ability.
Among linear speciﬁcation, we pay special attention to penalized regression (Ridge, LASSO
Brazil (Bovespa)
Germany (DAX)
Hong Kong (Hang Seng)
Japan (Nikkei)
Figure 7: Realized variance of diﬀerent stock indexes
Table 1: Forecasting Results
The table reports for each model, the mean squared error (MSE) and the QLIKE statistics as a ratio to the HAR
benchmark. Values smaller than one indicates that the model outperforms the HAR. The asterisks indicate the
results of the Diebold-Mariano test of equal forecasting performance. *,**, and ***, indicate rejection of the
null of equal forecasting ability at the 10%, 5% and 1%, respectively.
Full Sample
HARX-LASSO
Random Forest
Neural Network (1)
Neural Network (3)
Neural Network (5)
and its generalizations, for example) and ensemble methods (Bagging and Complete Subset
Regression). Although, there has been major theoretical advances in the literature on penalized
linear regression models for dependent data, the same is not true for ensemble methods. The
theoretical results for Bagging are so far based on independent data and the results for complete
subset regression are quite limited.
With respect to nonlinear ML methods, we focused on neural networks and tree-based
methods. Theoretical results for random forests and boosted trees have been developed only
to independent and identically distributed data and in the case of a low dimensional set of
regressors. For shallow neural networks, Chen et al. and Chen provide some
theoretical results for dependent data in the low dimensional case. The behavior of such models
in high-dimensions is still under study. The same is true for deep neural networks.
Nevertheless, the recent empirical evidence shows that nonlinear machine learning models
combined with large datasets can be extremely useful for economic forecasting.
As a direction for further developments we list the following points:
1. Develop results for Bagging and Boosting for dependent data.
2. Show consistency and asymptotic normality of the random forecast estimator of the unknown function fh(Xt) when the data are dependent.
3. Derive a better understanding of the variable selection mechanism of nonlinear ML methods.
4. Develop inferential methods to access variable importance in nonlinear ML methods.
5. Develop models based on unstructured data, such as text data, to economic forecasting.
6. Evaluate ML models for nowcasting.
7. Evaluate ML in very unstable environments with many structural breaks.
Finally, we would like to point that we left a number of other interesting ML methods out of
this survey, such as, for example, Support Vector Regressions, autoenconders, nonlinear factor
models, and many more. However, we hope that the material presented here can be of value
to anyone interested of applying ML techniques to economic and/or ﬁnancial forecasting.