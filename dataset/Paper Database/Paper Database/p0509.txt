Scaling Inference for Markov Logic via Dual Decomposition
Christopher R´e
Jude Shavlik
Department of Computer Sciences
University of Wisconsin-Madison, USA
{leonn, czhang, chrisre, shavlik}@cs.wisc.edu
Abstract—Markov logic is a knowledge-representation language that allows one to specify large graphical models.
However, the resulting large graphical models can make inference for Markov logic a computationally challenging problem.
Recently, dual decomposition (DD) has become a popular
approach for scalable inference on graphical models. We study
how to apply DD to scale up inference in Markov logic.
A standard approach for DD ﬁrst partitions a graphical
model into multiple tree-structured subproblems. We apply this
approach to Markov logic and show that DD can outperform
prior inference approaches. Nevertheless, we observe that
the standard approach for DD is suboptimal as it does not
exploit the rich structure often present in the Markov logic
program. Thus, we describe a novel decomposition strategy
that partitions a Markov logic program into parts based on its
structure. A crucial advantage of our approach is that we can
use specialized algorithms for portions of the input problem –
some of which have been studied for decades, e.g., coreference
resolution. Empirically, we show that our program-level decomposition approach outperforms both non-decomposition and
graphical model-based decomposition approaches to Markov
logic inference on several data-mining tasks.
I. INTRODUCTION
Markov logic is a knowledge-representation language
that uses weighted ﬁrst-order logic to specify graphical
models. It has been applied to a wide range of applications,
including many in information extraction and data mining
 . The resulting graphical models can be huge (hundreds
of millions of nodes or more in our applications), and so a
key technical challenge is the scalability and performance
of Markov logic inference.
Semantically, a Markov logic program, or Markov logic
network (MLN), speciﬁes a graphical model called a Markov
random ﬁeld (MRF). Thus, one approach to improving the
scalability of MLNs is to apply inference techniques from
the graphical-model literature. One such technique is dual
decomposition , which has recently been applied to
MRF inference , , , . In addition to increased
scalability, dual decomposition also offers the possibility of
dual certiﬁcates1 that can bound the distance of a solution
from optimality, e.g., for maximum a posteriori (MAP)
inference.
There are two steps in applying dual decomposition: (1)
decompose the inference problem into multiple parts, and
1Namely, an lower (resp. upper) bound to a minimization (resp. maximization) problem.
(2) iteratively combine solutions from individual parts. The
intuition is that the individual parts will be more tractable
than the whole problem – so much so that the improved
performance of individual parts will compensate for the
overhead of many iterations of repeated inference. Following
the literature , , we ﬁrst implement MRF-level decomposition and compare it with state-of-the-art MLN inference algorithms. On simpler MLN-generated MRFs, MRFlevel decomposition can achieve competitive performance
compared to monolithic inference (i.e., running a generic
MLN inference algorithm without decomposition). On more
complex MLN-generated MRFs, the performance and quality of both monolithic inference and MRF decomposition
approaches may be suboptimal.
Our key observation is that MRF-level decomposition
strategies in step (1) ignore valuable structural hints that
often occur in MLN programs. For example, a large Markov
logic program may have several “subroutines” that each
perform a standard, well-studied task such as coreference
resolution or labeling, e.g., with a conditional random ﬁeld
(CRF) . In contrast to traditional approaches that either
use a generic MLN inference algorithm or decompose into
semantically meaningless parts like trees, our idea is to
exploit this information so that we may use existing, specialized algorithms for such individual subtasks. For example,
we may choose to solve a labeling task with a CRF and
so use the Viterbi algorithm . Importantly, even if we
use different algorithms for each part, dual decomposition
preserves the joint-inference property – i.e., different parts
are not solved in isolation. The hope is to achieve higher
efﬁciency and quality than monolithic approaches.
To illustrate this idea, we describe several such correspondences between MLN structure and specialized inference algorithms, including logistic regression and linearchain conditional random ﬁelds. A user may declare (or an
algorithm may detect) which of these specialized algorithms
to use on an individual part of the MLN program. We call
our prototype system FELIX.
Experimentally, we validate that our (MRF-level and
program-level) dual-decomposition approaches have superior performance than prior MLN inference approaches on
several data-mining tasks. Our main results are that (1) on
simple MLNs taken from the literature, MRF-level decomposition outperforms monolithic inference, and FELIX has
Appears in Proceedings of the IEEE International Conference on Data Mining (ICDM), 2012.
competitive efﬁciency and quality compared to monolithic
inference and MRF-level decomposition; and (2) on more
complex MLNs (also taken from the literature), FELIX
achieves substantially higher efﬁciency and quality than
monolithic inference and MRF-level decomposition.
II. RELATED WORK
Dual Decomposition: Dual decomposition is a classic
and general technique in optimization that decomposes an
objective function into multiple smaller subproblems; in
turn, these subproblems communicate to optimize a global
objective via Lagrange multipliers . Recently, dual decomposition has been applied to inference in graphical
models such as MRFs. In the master-slave scheme , ,
the MAP solution from each subproblem is communicated
and the Lagrange multipliers are updated with the projected
gradient method at each iteration. Our prototype implementation of FELIX uses the master-slave scheme. It is future
work to adapt the closely related tree-reweighted (TRW)
algorithms , that decompose an MRF into a convex
combination of spanning trees.
MLN Inference: There has been extensive research
interest in techniques to improve the scalability and performance of MLN inference. For example, we have studied how
to prune MRFs based on logical rules in MLNs , and
use relational databases to improve the speed and scalability
of constructing MRFs . Our implementation builds on the
open-source TUFFY system , which implements many of
the above techniques.
III. BACKGROUND
We ﬁrst walk through an MLN program that extracts
afﬁliation relationships between people and organizations
from text. We then brieﬂy review dual decomposition.
A. Markov Logic
An MLN consists of three parts: schema, evidence, and
rules. To tell an MLN inference system what data will be
provided or generated, the user provides a schema, i.e.,
deﬁnitions of a set of relations (or equivalently, predicates).
The truth values for some relations are given; such relations
are called the evidence. In the schema of Figure 1, the
ﬁrst three relations are evidence relations. In addition to
evidence, there are also relations whose content we do not
know; they are called query relations. A user may ask for
predictions on some or all query relations.
In addition to schema and evidence, we also provide a
set of MLN rules that encode our knowledge about the
correlations and constraints over the relations. An MLN
rule is a ﬁrst-order logic formula associated with a real
number (or inﬁnity) called a weight. Inﬁnite-weighted rules
are called hard rules, which means that they must hold in
any prediction that the MLN system makes. In contrast,
rules with ﬁnite weights are soft rules: a positive weight
indicates conﬁdence in the rule’s correctness.
Semantics: An MLN program deﬁnes a probability
distribution over possible worlds. Formally, we ﬁrst ﬁx a
schema σ (as in Figure 1) and a domain D. Given as input a
set of formulae ¯F = F1, . . . , FN with weights w1, . . . , wN,
they deﬁne a probability distribution over possible worlds
as follows. Given a formula Fk with free variables ¯x =
(x1, · · · , xm), for each ¯d 2 Dm we create a new formula
g ¯d called a ground formula where g ¯d denotes the result
of substituting each variable xi of Fk with di. We assign
the weight wk to g ¯d. Denote by G = (¯g, w) the set of
all such weighted ground formulae of ¯F. Essentially, G
forms an MRF over a set of Boolean random variables
(representing the truth value of each possible ground tuple).
Let w be a function that maps each ground formula to its
assigned weight. Fix an MLN ¯F, then for any possible
world (instance) I, we say a ground formula g is violated
if w(g) > 0 and g is false in I, or if w(g) < 0 and g is
true in I. We denote the set of ground formulae violated in
a world I as V (I). The cost of the world I is
costMLN(I) =
Through costMLN, an MLN deﬁnes a probability distribution over all instances:
Pr[I] = Z−1 exp {−costMLN(I)} ,
where Z is a normalizing constant.
Inference: We focus on MAP inference that ﬁnds a
most likely world, i.e., a world with the lowest cost. MAP
inference is essentially a mathematical optimization problem
that is intractable, and so existing MLN systems implement
generic algorithms for inference.2
B. Dual Decomposition
We illustrate the basic idea of dual decomposition with an
example. Consider the problem of minimizing a real-valued
function f(x1, x2, x3). Suppose that f can be written as
f(x1, x2, x3) = f1(x1, x2) + f2(x2, x3).
Further suppose that we have black boxes to solve f1 and f2
(plus linear terms). To apply these black boxes to minimize
f we need to cope with the fact that f1 and f2 share the
variable x2. Following dual decomposition, we can rewrite
minx1,x2,x3 f(x1, x2, x3) into the form
x1,x21,x22,x3 f1(x1, x21) + f2(x22, x3) s.t. x21 = x22,
where we essentially make two copies of x2 and enforce
that they are identical. The signiﬁcance of such rewriting
is that we can apply Lagrangian relaxation to the equality
constraint to decompose the formula into two independent
2FELIX also supports marginal inference using dual decomposition.
pSimHard(per1, per2)
homepage(per, page)
faculty(org, per)
⇤affil(per, org)
⇤pCoref(per1, per2)
faculty(‘MIT’, ‘Chomsky’)
homepage(‘Joe’, ‘Doc201’)
+1 pCoref(p, p)
+1 pCoref(p1, p2) => pCoref(p2, p1)
+1 pCoref(x, y), pCoref(y, z) => pCoref(x, z)(F3)
pSimHard(p1, p2) => pCoref(p1, p2)
faculty(o, p) => affil(p, o)
An example MLN program that performs two tasks: 1. discover afﬁliation relationships between people and organizations (affil), 2. resolve
coreference among people mentions (pCoref). The remaining three relations are evidence relations.
pieces. To do this, we introduce a scalar variable λ 2 R
(called a Lagrange multiplier) and deﬁne g(λ) =
x1,x21,x22,x3 f1(x1, x21) + f2(x22, x3) + λ(x21 −x22).
For any λ, we have g(λ) minx1,x2,x3 f(x1, x2, x3).3
Thus, the tightest bound is to maximize g(λ); the problem
maxλ g(λ) is a dual problem for the problem on f. If the
optimal solution of this dual problem is feasible (here, x21 =
x22), then the dual optimal solution is also an optimum of
the original program [15, p. 168].
The key beneﬁt of this relaxation is that, instead of a
single problem on f, we can now compute g(λ) by solving
two independent problems (each problem is grouped by
parentheses) that may be easier to solve:
g(λ) = min
x1,x21 (f1(x1, x21) + λx21)
x22,x3 (f2(x22, x3) −λx22) .
To compute maxλ g(λ), we can use standard techniques
such as projected subgradient [15, p. 174]. Notice that dual
decomposition can be used for MLN inference if xi are truth
values of ground tuples and one deﬁnes f to be costMLN(I).
Decomposition Choices: The dual decomposition technique leaves open the question of how to decompose a
function f. We need to answer this question if we want
to apply dual decomposition to MLNs.
IV. DUAL DECOMPOSITION FOR MLNS
The two approaches that we implement for dual decomposition work at different levels of abstraction: at the MRF
level or at the MLN-program level. Still, the two methods are
similar: and both pass messages in a master-slave scheme,
and produce a MAP solution after inference in a similar
way. As a result, we are able to implement both approaches
on top of the TUFFY system. We describe each step of
the process in turn: decomposition (Section IV-A), masterslave message passing (Section IV-B), and production of the
solution (Section IV-C).
3The search space of LHS is a superset of RHS, therefore we always have
LHS RHS. One can always take x21 = x22 = x3 in the minimization,
and the value of the two object functions are equal.
Task: Classification
Task: Generic
A program-level decomposition for Example 1. Shaded boxes
are evidence relations. Solid arrows are data ﬂow; dash arrows are control.
A. Decomposition
In decomposition, we partition the input structure and
set up auxiliary structures to support message passing.
For MRF-level decomposition, we partition an MRF into
multiple trees that are linked via auxiliary singleton factors;
for program-level decomposition, we partition an MLN into
subprograms that are linked via auxiliary singleton rules.
MRF-level Decomposition: The input to the decomposition algorithm is an MRF which is the result of grounding
an input MLN. The MRF is represented as a factor graph
(i.e., a bipartite graph between ground-tuple nodes and
ground-formula factors).4 Following Komodakis et al. ,
 and Wainwright et al. , we decompose the MRF
into a collection of trees (smaller factor graphs with disjoint
factors) that cover this factor graph, i.e., each node in the
factor graph is present in one or more trees.5 Nodes that
are in multiple trees may take conﬂicting values; to allow
messages to be passed in the next step, we create a special
singleton factor for each copy of a shared node.
Program-level Decomposition: In contrast, FELIX performs decomposition at the program-level: the input is
the (ﬁrst-order) rule set of an MLN program, and FELIX
partitions it into multiple tasks each of which can be solved
with different algorithms. In our program-level approach,
we share at the granularity of relations, i.e., an entire
relation is shared or not. This choice allows FELIX to use
a relational database management system (RDBMS) for all
data movement, which can be formulated as SQL queries; in
turn, this allows us to ignore low-level issues like memory
management. We illustrate the idea with an example.
Example 1 Consider a simple MLN which we call Γ:
GoodNews(p) => Happy(p)
BadNews(p) => Sad(p)
Happy(p) <=> ¬Sad(p)
4This representation allows for non-pairwise MRFs.
5We use a greedy algorithm to ﬁnd these trees. We describe more details
in the full version .
where GoodNews and BadNews are evidence and the other
two relations are queries. Consider the decomposition
Γ1 = {φ1} and Γ2 = {φ2, φ3}. Γ1 and Γ2 share the relation
Happy, so we create two copies of this relation: Happy1
and Happy2, one for each subprogram. We introduce
Lagrange multipliers λp, one for each possible ground tuple
Happy(p). We thereby obtain a new program Γλ:
GoodNews(p) => Happy1(p)
BadNews(p) => Sad(p)
Happy2(p) <=> ¬Sad(p)
where each 'i represents a set of singleton rules, one
for each value of p (i.e., for each speciﬁc person in a
given testbed). This program contains two subprograms,
1, '1} and Γλ
2 = {φ2, φ0
3, '2}, that can be solved
independently with any inference algorithm.
As illustrated in Figure 2, the output of our decomposition
method is a bipartite graph between a set of subprograms
and a set of relations. In a program-level approach, FELIX
attaches an inference algorithm to each subprogram; we
call this pairing of algorithm and subprogram a task. We
discuss how to select decompositions and assign algorithms
in Section V.
B. Message Passing
We apply the master-slave message passing scheme ,
 for both MRF-level and program-level decomposition.
The master-slave approach alternates between two steps: (1)
perform inference on each part independently to obtain each
part’s predictions on shared variables, and (2) a process
called the Master examines the (possibly conﬂicting) predictions and sends messages in the form of Lagrange multipliers
to each task. The reader can consult , for details.6
C. Producing the Final Solution
When inference stops, for some variables all of their
copies might not have the same value. The last step is
to choose a ﬁnal solution based on solutions from the
decomposed parts.
MRF-level Decomposition: To obtain a solution to the
original MLN from individual solutions on each tree, we use
the heuristic proposed by Komogorov et al. (and used in
Komodakis et al. ) that sequentially ﬁxes shared-variable
values based on max-product messages in individual trees.
Program-level Decomposition: If a shared relation is
not subject to any hard rules, FELIX takes majority votes
from the predictions of related tasks. (If all copies of this
relation have converged, the votes would be unanimous.)
To ensure that hard rules in the input MLN program are
6For MRF-level decomposition, we can perform exact inference by
running the max-product algorithm on each component.
not violated in the ﬁnal output, we insist that for any query
relation r, all hard rules involving r (if any) be assigned to
a single task, and that the ﬁnal value of r be taken from
this task.7 This guarantees that the ﬁnal output is a possible
world for Γ (provided that the hard rules are satisﬁable).
V. SPECIALIZED TASKS
With program-level decomposition, we can use different
algorithms for different subprograms. FELIX uses specialized algorithms to handle tasks, which can be more efﬁcient
than generic MLN inference. As an existence proof, we
describe several tasks that are common in text processing.8
Classiﬁcation: Classiﬁcation is a fundamental statistical problem and ubiquitous in applications; it arises in
Markov logic as a query predicate R(x, y) with hard rules
R(x, y1) ^ y1 6= y2 => ¬R(x, y2),
which mandates that each object (represented by a possible
value of x) can only be assigned at most one label (represented by a possible value of y).
If the only query relation in a subprogram Γi is R and
R is mentioned at most once in each rule in Γi (except
the rule above), then Γi is essentially a logistic regression
(LR) classiﬁcation model. The inference problem for LR
is trivial given model parameters (here rule weights) and
feature values (here ground formulae).
Suppose there are N objects and K labels. Then the
memory requirement for LR is O(NK). On the other hand,
it would require N
factors to represent the above rule
in an MRF. For tasks such as entity linking (e.g., mapping
textual mentions to Wikipedia entities), the value of K could
be in the millions.
Other Tasks: FELIX also supports other tasks, including
(1) Segmentation and (2) Coreference. Segmentation encodes linear-chain CRF models that contain both unigram
and bigram features. Coreference takes a set of N strings and
decides which strings represent the same real-world entity.
FELIX implements the Viterbi algorithm for segmentation and Singh et al.’s algorithm for coreference.9
VI. EXPERIMENTS
Our main hypotheses are that (1) MRF-level decomposition can outperform monolithic inference, and (2) FELIX
can outperform both MRF-level decomposition and monolithic inference. We also validate that specialized algorithms
indeed have higher efﬁciency and quality than generic MLN
inference algorithms or MRF decomposition.
7This policy might result in cascaded subtasks. More sophisticated
policies are an interesting future direction.
8We describe how FELIX automatically detects these tasks given an MLN
program in the full version .
9We will describe these tasks in more details in the full version .
# Relations
# MLN Rules
# Evidence
DATASET SIZES.
Datasets and MLNs: We use two publicly available
MLN testbeds from ALCHEMY’s website.10 In addition,
we create an MLN program for named-entity recognition
based on skip-chain CRFs , and an MLN program that
we developed for TAC-KBP, a knowledge-base population
challenge.11 Table I shows some statistics of these datasets.
We describe the MLN testbeds from ALCHEMY’s website:
(1) IE, where one performs segmentation on Cora citations
using unigram and bigram features (see the “Isolated” program ); (2) IERJ, where one performs joint segmentation
and entity resolution on Cora citations (see the “Jnt-Seg-
ER” program ). The last two are (3) NER, where one
performs named-entity recognition on a dataset with 10K
tokens using the skip-chain CRF model (encoded in
three MLN rules); and (4) KBP, which is an implementation
of the TAC-KBP (knowledge-base population) challenge
using an MLN that performs entity linking (mapping textual
mentions to Wikipedia entities), slot ﬁlling (mapping cooccurring mentions to a set of possible relationships), entitylevel knowledge-base population, and fact veriﬁcation from
an existing partial knowledge base.
Among those datasets, IE, and NER are simpler programs
as they only have unigram and sparse bigram rules and
classiﬁcation constraints with very few labels; IERJ, and
KBP are more complex programs as they involve transitivity
rules and classiﬁcation constraints with many labels. To test
the scalability of FELIX, we also run the KBP program on
a 1.8M-doc TAC-KBP corpus (“KBP+”).
Experimental Setup: We run TUFFY and ALCHEMY
as state-of-the-art monolithic MLN inference systems. We
implement MRF-level decomposition and program-level decomposition (i.e., FELIX) approaches on top of the opensource TUFFY system.12 As TUFFY has similar or superior
performance to ALCHEMY on each dataset, here we use
TUFFY as a representative for state-of-the-art MLN inference
and report ALCHEMY’s performance in the technical-report
version of this paper. We use the following labels for these
three approaches: (1) TUFFY, in which TUFFY performs
RDBMS-based grounding and uses WalkSAT as its inference
algorithm; (2) TREE, in which we replace TUFFY’s Walk-
SAT algorithm with tree-based MRF-level dual decomposition as described in Section IV; and (3) FELIX, in which we
implement program-level dual decomposition as described
10 
11 
12 
in Sections IV and V. Table II lists FELIX’s decomposition
scheme for each dataset.
NUMBER OF TASKS OF EACH TYPE IN THE DECOMPOSITIONS USED BY
All three approaches are implemented in Java and use
PostgreSQL 9.0.4 as the underlying database system. Unless
speciﬁed otherwise, all experiments are run on a RHEL 6.1
server with four 2.00GHz Intel Xeon CPUs (40 total physical
cores plus hyperthreading) and 256 GB of RAM.
A. Overall Efﬁciency and Quality
We validate that TREE can outperform TUFFY and that
FELIX in turn outperforms both TREE and TUFFY on
complex programs. To support these claims, we compare the
efﬁciency and quality of all three approaches on the datasets
listed above. We run each system on each dataset for 5000
seconds (except for the largest dataset KBP, for which we
run 5 hours), and plot the MLN cost against runtime.
From Figure 3 we see that, on the two simpler programs (i.e., IE and NER), all three approaches were able
to converge to about the same result quality. Although
TREE and TUFFY have similar performance on IE, on NER
the TREE approach obtains a low-cost solution within two
minutes whereas TUFFY takes more than one hour to reach
comparable quality. FELIX has slower convergence behavior
than TUFFY and TREE on IE, but it does converge to a
similar solution. We note that alternate step-size rules may
improve FELIX’s performance on these datasets.
On the more complex programs (i.e., IERJ and KBP),
FELIX achieves dramatically better performance compared
to TUFFY and TREE: while TUFFY and TREE fail to ﬁnd a
feasible solution (i.e., a solution with ﬁnite cost) after 5000
seconds on each of these datasets, FELIX converges to a
feasible solution within minutes. There are complex structures in these MLNs; e.g., transitivity for entity resolution
and uniqueness constraints for entity linking in KBP. TUFFY
and TREE were not able to ﬁnd feasible solutions that satisfy
such complex constraints, and the corresponding curves were
obtained by replacing hard rules with a “softened” version
with weight 100. Still, we see that the results from both
TUFFY and TREE are substantially worse than FELIX. We
conclude that overall the FELIX approach is able to achieve
signiﬁcantly higher efﬁciency and quality than TUFFY and
TREE approaches.
Scalability: To test scalability, we also run FELIX on
the large KBP+ dataset with a parallel RDBMS (from Greenplum Inc.). This MLN converges within a few iterations;
!me$(sec)$
!me$(sec)$
!me$(sec)$
!me$(sec)$
Tree ran out of memory
High-level performance results of various approaches to MLN inference. For each dataset and each system, we plot a time-cost curve. Labels
ending with an asterisk indicate that some points in the corresponding curves correspond to infeasible solutions (i.e., solutions with inﬁnite cost), and the
curves were obtained by “softening” hard rules to have weight 100.
PERFORMANCE AND QUALITY COMPARISON ON INDIVIDUAL TASKS.
“INITIAL” (RESP. “FINAL”) IS THE TIME WHEN A SYSTEM PRODUCED
THE FIRST (RESP. CONVERGED) RESULT. FOR TUFFY, THE HARD RULES
WERE SOFTENED BECAUSE OTHERWISE NO SOLUTION WAS FOUND.
an iteration takes about ﬁve hours in FELIX. TUFFY and
ALCHEMY failed to run on this dataset.
B. Specialized Tasks
We next validate that the ability to integrate specialized
tasks into MLN inference is key to FELIX’s higher efﬁciency and quality. To do this, we demonstrate that FELIX’s
specialized algorithms outperform generic MLN inference
algorithms in both quality and efﬁciency when solving
specialized tasks. To evaluate this claim, we run FELIX,
TUFFY, TREE, and ALCHEMY on three MLN programs
encoding a CRF task.13 We measure application quality (F1
scores) on a subset of the CoNLL 2000 chunking dataset.14
As shown in Table III, while it always takes less than a
minute for FELIX on CRF, the other approaches take much
longer. Moreover, FELIX has the best inference quality (i.e.,
cost) and application quality (i.e., F1).
VII. CONCLUSION
We study how to apply dual decomposition to Markov
logic inference. We ﬁnd that MRF-level decomposition
empirically outperforms traditional, monolithic approaches
to MLN inference on some programs. However, MRF-level
decomposition ignores valuable structural hints in Markov
logic programs. Thus, we propose an alternative decomposition strategy that partitions an MLN program into high-level
tasks (e.g., classiﬁcation) that can be solved with specialized algorithms. On several datasets, we empirically show
that our program-level decomposition approach outperforms
both monolithic inference and MRF-level decomposition
approaches to MLN inference.
13We leave similar experiments on LR and CC to the full version .
14 
VIII. ACKNOWLEDGEMENTS
We gratefully acknowledge the support of the DARPA
FA8750-09-C-0181. CR is also generously supported by
NSF CAREER award under IIS-1054009, ONR award
N000141210041, and gifts or research awards from Google,
Greenplum, and Oracle. The opinions and conclusions in
this paper are not necessarily those of our sponsors.