Published as a conference paper at ICLR 2018
MEASURING THE INTRINSIC DIMENSION
OF OBJECTIVE LANDSCAPES
Chunyuan Li ∗
Duke University
 
Heerad Farkhoor, Rosanne Liu, and Jason Yosinski
Uber AI Labs
{heerad,rosanne,yosinski}@uber.com
Many recently trained neural networks employ large numbers of parameters to
achieve good performance. One may intuitively use the number of parameters
required as a rough gauge of the difﬁculty of a problem. But how accurate are
such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter
space, but instead in a smaller, randomly oriented subspace. We slowly increase
the dimension of this subspace, note at which dimension solutions ﬁrst appear,
and deﬁne this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several
suggestive conclusions. Many problems have smaller intrinsic dimensions than
one might suspect, and the intrinsic dimension for a given dataset varies little
across a family of models with vastly different sizes. This latter result has the
profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution
manifold. Intrinsic dimension allows some quantitative comparison of problem
difﬁculty across supervised, reinforcement, and other types of learning where we
conclude, for example, that solving the inverted pendulum problem is 100 times
easier than classifying digits from MNIST, and playing Atari Pong from pixels
is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method
is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple
approach for compressing networks, in some cases by more than 100 times.
INTRODUCTION
Training a neural network to model a given dataset entails several steps. First, the network designer
chooses a loss function and a network architecture for a given dataset. The architecture is then initialized by populating its weights with random values drawn from some distribution. Finally, the
network is trained by adjusting its weights to produce a loss as low as possible. We can think of
the training procedure as traversing some path along an objective landscape. Note that as soon as
a dataset and network architecture are speciﬁed, the landscape in its entirety is completely determined. It is instantiated and frozen; all subsequent parameter initialization, forward and backward
propagation, and gradient steps taken by an optimizer are just details of how the frozen space is
Consider a network parameterized by D weights. We can picture its associated objective landscape
as a set of “hills and valleys” in D dimensions, where each point in RD corresponds to a value of the
loss, i.e., the elevation of the landscape. If D = 2, the map from two coordinates to one scalar loss
can be easily imagined and intuitively understood by those living in a three-dimensional world with
similar hills. However, in higher dimensions, our intuitions may not be so faithful, and generally we
must be careful, as extrapolating low-dimensional intuitions to higher dimensions can lead to unreliable conclusions. The difﬁculty of understanding high-dimensional landscapes notwithstanding,
it is the lot of neural network researchers to spend their efforts leading (or following?) networks
∗Work performed as an intern at Uber AI Labs.
 
Published as a conference paper at ICLR 2018
over these multi-dimensional surfaces. Therefore, any interpreted geography of these landscapes is
Several papers have shed valuable light on this landscape, particularly by pointing out ﬂaws in common extrapolation from low-dimensional reasoning. Dauphin et al. showed that, in contrast
to conventional thinking about getting stuck in local optima (as one might be stuck in a valley in
our familiar D = 2), local critical points in high dimension are almost never valleys but are instead
saddlepoints: structures which are “valleys” along a multitude of dimensions with “exits” in a multitude of other dimensions. The striking conclusion is that one has less to fear becoming hemmed in
on all sides by higher loss but more to fear being waylaid nearly indeﬁnitely by nearly ﬂat regions.
Goodfellow et al. showed another property: that paths directly from the initial point to the
ﬁnal point of optimization are often monotonically decreasing. Though dimension is high, the space
is in some sense simpler than we thought: rather than winding around hills and through long twisting
corridors, the walk could just as well have taken a straight line without encountering any obstacles,
if only the direction of the line could have been determined at the outset.
In this paper we seek further understanding of the structure of the objective landscape by restricting
training to random slices through it, allowing optimization to proceed in randomly generated subspaces of the full parameter space. Whereas standard neural network training involves computing
a gradient and taking a step in the full parameter space (RD above), we instead choose a random
d-dimensional subspace of RD, where generally d < D, and optimize directly in this subspace. By
performing experiments with gradually larger values of d, we can ﬁnd the subspace dimension at
which solutions ﬁrst appear, which we call the measured intrinsic dimension of a particular problem.
Examining intrinsic dimensions across a variety of problems leads to a few new intuitions about the
optimization problems that arise from neural network models.
We begin in Sec. 2 by deﬁning more precisely the notion of intrinsic dimension as a measure of
the difﬁculty of objective landscapes. In Sec. 3 we measure intrinsic dimension over a variety of
network types and datasets, including MNIST, CIFAR-10, ImageNet, and several RL tasks. Based
on these measurements, we draw a few insights on network behavior, and we conclude in Sec. 4.
DEFINING AND ESTIMATING INTRINSIC DIMENSION
We introduce the intrinsic dimension of an objective landscape with an illustrative toy problem.
Let θ(D) ∈RD be a parameter vector in a parameter space of dimension D, let θ(D)
be a randomly
chosen initial parameter vector, and let θ(D)
be the ﬁnal parameter vector arrived at via optimization.
Consider a toy optimization problem where D = 1000 and where θ(D) optimized to minimize a
squared error cost function that requires the ﬁrst 100 elements to sum to 1, the second 100 elements
to sum to 2, and so on until the vector has been divided into 10 groups with their requisite 10 sums.
We may start from a θ(D)
that is drawn from a Gaussian distribution and optimize in RD to ﬁnd a
that solves the problem with cost arbitrarily close to zero.
Solutions to this problem are highly redundant. With a little algebra, one can ﬁnd that the manifold
of solutions is a 990 dimensional hyperplane: from any point that has zero cost, there are 990
orthogonal directions one can move and remain at zero cost. Denoting as s the dimensionality of
the solution set, we deﬁne the intrinsic dimensionality dint of a solution as the codimension of the
solution set inside of RD:
D = dint + s
Here the intrinsic dimension dint is 10 (1000 = 10 + 990), with 10 corresponding intuitively to the
number of constraints placed on the parameter vector. Though the space is large (D = 1000), the
number of things one needs to get right is small (dint = 10).
MEASURING INTRINSIC DIMENSION VIA RANDOM SUBSPACE TRAINING
The above example had a simple enough form that we obtained dint = 10 by calculation. But
in general we desire a method to measure or approximate dint for more complicated problems,
including problems with data-dependent objective functions, e.g. neural network training. Random
subspace optimization provides such a method.
Published as a conference paper at ICLR 2018
Subspace dim d
Performance
dint90 = dint100 = 10
Figure 1: (left) Illustration of parameter vectors for direct optimization in the D = 3 case. (middle)
Illustration of parameter vectors and a possible random subspace for the D = 3, d = 2 case. (right)
Plot of performance vs. subspace dimension for the toy example of toy example of Sec. 2. The
problem becomes both 90% solvable and 100% solvable at random subspace dimension 10, so
dint90 and dint100 are 10.
Standard optimization, which we will refer to hereafter as the direct method of training, entails
evaluating the gradient of a loss with respect to θ(D) and taking steps directly in the space of θ(D).
To train in a random subspace, we instead deﬁne θ(D) in the following way:
θ(D) = θ(D)
where P is a randomly generated D × d projection matrix1 and θ(d) is a parameter vector in a generally smaller space Rd. θ(D)
and P are randomly generated and frozen (not trained), so the system
has only d degrees of freedom. We initialize θ(d) to a vector of all zeros, so initially θ(D) = θ(D)
This convention serves an important purpose for neural network training: it allows the network to
beneﬁt from beginning in a region of parameter space designed by any number of good initialization
schemes to be well-conditioned, such that gradient descent
via commonly used optimizers will tend to work well.2
Training proceeds by computing gradients with respect to θ(d) and taking steps in that space.
Columns of P are normalized to unit length, so steps of unit length in θ(d) chart out unit length
motions of θ(D). Columns of P may also be orthogonalized if desired, but in our experiments
we relied simply on the approximate orthogonality of high dimensional random vectors. By this
construction P forms an approximately orthonormal basis for a randomly oriented d dimensional
subspace of RD, with the origin of the new coordinate system at θ(D)
. Fig. 1 (left and middle) shows
an illustration of the related vectors.
Consider a few properties of this training approach. If d = D and P is a large identity matrix, we
recover exactly the direct optimization problem. If d = D but P is instead a random orthonormal
basis for all of RD (just a random rotation matrix), we recover a rotated version of the direct problem.
Note that for some “rotation-invariant” optimizers, such as SGD and SGD with momentum, rotating
the basis will not change the steps taken nor the solution found, but for optimizers with axis-aligned
assumptions, such as RMSProp and Adam , the
path taken through θ(D) space by an optimizer will depend on the rotation chosen. Finally, in the
general case where d < D and solutions exist in D, solutions will almost surely (with probability 1)
not be found if d is less than the codimension of the solution. On the other hand, when d ≥D−s, if
the solution set is a hyperplane, the solution will almost surely intersect the subspace, but for solution
sets of arbitrary topology, intersection is not guaranteed. Nonetheless, by iteratively increasing d,
re-running optimization, and checking for solutions, we obtain one estimate of dint. We try this
sweep of d for our toy problem laid out in the beginning of this section, measuring the positive performance (higher is better) instead of loss.3 As
expected, the solutions are ﬁrst found at d = 10 (see Fig. 1, right), conﬁrming our intuition that for
this problem, dint = 10.
DETAILS AND CONVENTIONS
In the rest of this paper, we measure intrinsic dimensions for particular neural network problems
and draw conclusions about the associated objective landscapes and solution sets. Because modeling real data is more complex than the above toy example, and losses are generally never exactly
zero, we ﬁrst choose a heuristic for classifying points on the objective landscape as solutions vs.
non-solutions. The heuristic we choose is to threshold network performance at some level relative to a baseline model, where generally we take as baseline the best directly trained model. In
supervised classiﬁcation settings, validation accuracy is used as the measure of performance, and
in reinforcement learning scenarios, the total reward (shifted up or down such that the minimum
reward is 0) is used. Accuracy and reward are preferred to loss to ensure results are grounded to
real-world performance and to allow comparison across models with differing scales of loss and
different amounts of regularization included in the loss.
We deﬁne dint100 as the intrinsic dimension of the “100%” solution: solutions whose performance
is statistically indistinguishable from baseline solutions. However, when attempting to measure
dint100, we observed it to vary widely, for a few confounding reasons: dint100 can be very high
— nearly as high as D — when the task requires matching a very well-tuned baseline model, but
can drop signiﬁcantly when the regularization effect of restricting parameters to a subspace boosts
performance by tiny amounts. While these are interesting effects, we primarily set out to measure
the basic difﬁculty of problems and the degrees of freedom needed to solve (or approximately solve)
them rather than these subtler effects.
Thus, we found it more practical and useful to deﬁne and measure dint90 as the intrinsic dimension
of the “90%” solution: solutions with performance at least 90% of the baseline.
We chose 90% after looking at a number of dimension vs. performance plots (e.g. Fig. 2) as a
reasonable trade off between wanting to guarantee solutions are as good as possible, but also wanting
measured dint values to be robust to small noise in measured performance. If too high a threshold
is used, then the dimension at which performance crosses the threshold changes a lot for only tiny
changes in accuracy, and we always observe tiny changes in accuracy due to training noise.
If a somewhat different (higher or lower) threshold were chosen, we expect most of conclusions in
the rest of the paper to remain qualitatively unchanged. In the future, researchers may ﬁnd it useful
to measure dint using higher or lower thresholds.
RESULTS AND DISCUSSION
We begin by analyzing a fully connected (FC) classiﬁer trained on MNIST. We choose a network
with layer sizes 784–200–200–10, i.e. a network with two hidden layers of width 200; this results
in a total number of parameters D = 199, 210. A series of experiments with gradually increasing
subspace dimension d produce monotonically increasing performances, as shown in Fig. 2 (left).
By checking the subspace dimension at which performance crosses the 90% mark, we measure this
network’s intrinsic dimension dint90 at about 750.
Some networks are very compressible.
A salient initial conclusion is that 750 is quite low. At
that subspace dimension, only 750 degrees of freedom (0.4%) are being used and 198,460 (99.6%)
unused to obtain 90% of the performance of the direct baseline model. A compelling corollary of
this result is a simple, new way of creating and training compressed networks, particularly networks
for applications in which the absolute best performance is not critical. To store this network, one
need only store a tuple of three items: (i) the random seed to generate the frozen θ(D)
, (ii) the
3For this toy problem we deﬁne performance = exp(−loss), bounding performance between 0 and 1,
with 1 being a perfect solution.
Published as a conference paper at ICLR 2018
Subspace dim d
Validation accuracy
90% baseline
Subspace dim d
Validation accuracy
90% baseline
Figure 2: Performance (validation accuracy) vs. subspace dimension d for two networks trained
on MNIST: (left) a 784–200–200–10 fully-connected (FC) network (D = 199,210) and (right) a
convolutional network, LeNet (D = 44,426). The solid line shows performance of a well-trained
direct (FC or conv) model, and the dashed line shows the 90% threshold we use to deﬁne dint90. The
standard derivation of validation accuracy and measured dint90 are visualized as the blue vertical and
red horizontal error bars. We oversample the region around the threshold to estimate the dimension
of crossing more exactly. We use one-run measurements for dint90 of 750 and 290, respectively.
random seed to generate P and (iii) the 750 ﬂoating point numbers in θ(d)
∗. It leads to compression
(assuming 32-bit ﬂoats) by a factor of 260× from 793kB to only 3.2kB, or 0.4% of the full parameter
size. Such compression could be very useful for scenarios where storage or bandwidth are limited,
e.g. including neural networks in downloaded mobile apps or on web pages.
This compression approach differs from other neural network compression methods in the following
aspects. (i) While it has previously been appreciated that large networks waste parameters and weights contain redundancy that can be exploited for posthoc compression , this paper’s method constitutes a much simpler approach to
compression, where training happens once, end-to-end, and where any parameterized model is an
allowable base model.
(ii) Unlike layerwise compression models , we operate in the entire parameter space, which could work better or worse, depending on
the network. (iii) Compared to methods like that of Louizos et al. , who take a Bayesian
perspective and consider redundancy on the level of groups of parameters (input weights to a single
neuron) by using group-sparsity-inducing hierarchical priors on the weights, our approach is simpler
but not likely to lead to compression as high as the levels they attain. (iv) Our approach only reduces
the number of degrees of freedom, not the number of bits required to store each degree of freedom,
e.g. as could be accomplished by quantizing weights . Both approaches could
be combined. (v) There is a beautiful array of papers on compressing networks such that they also
achieve computational savings during the forward pass ; subspace training does not speed up execution time during inference. (vi) Finally, note
the relationships between weight pruning, weight tying, and subspace training: weight pruning is
equivalent to ﬁnding, post-hoc, a subspace that is orthogonal to certain axes of the full parameter
space and that intersects those axes at the origin. Weight tying, e.g. by random hashing of weights
into buckets , is equivalent to subspace training where the subspace is restricted
to lie along the equidistant “diagonals” between any axes that are tied together.
Robustness of intrinsic dimension.
Next, we investigate how intrinsic dimension varies across
FC networks with a varying number of layers and varying layer width.4 We perform a grid sweep
of networks with number of hidden layers L chosen from {1, 2, 3, 4, 5} and width W chosen from
{50, 100, 200, 400}. Fig. S6 in the Supplementary Information shows performance vs. subspace
dimension plots in the style of Fig. 2 for all 20 networks, and Fig. 3 shows each network’s dint90
plotted against its native dimension D. As one can see, D changes by a factor of 24.1 between the
smallest and largest networks, but dint90 changes over this range by a factor of only 1.33, with much
of this possibly due to noise.
Thus it turns out that the intrinsic dimension changes little even as models grown in width or depth!
The striking conclusion is that every extra parameter added to the network — every extra dimension
added to D — just ends up adding one dimension to the redundancy of the solution, s.
4Note that here we used a global baseline of 100% accuracy to compare simply and fairly across all models.
See Sec. S5 for similar results obtained using instead 20 separate baselines for each of the 20 models.
Published as a conference paper at ICLR 2018
Figure 3: Measured intrinsic dimension dint90 vs number of parameters D for 20 FC models of
varying width (from 50 to 400) and depth (number of hidden layers from 1 to 5) trained on MNIST.
The red interval is the standard derivation of the measurement of dint90. Though the number of
native parameters D varies by a factor of 24.1, dint90 varies by only 1.33, with much of that factor
possibly due to noise, showing that dint90 is a fairly robust measure across a model family and that
each extra parameter ends up adding an extra dimension directly to the redundancy of the solution.
Standard deviation was estimated via bootstrap; see Sec. S5.1.
Number of trainable parameters
Validation accuracy
Number of trainable parameters
Validation accuracy
Figure 4: Performance vs. number of trainable parameters for (left) FC networks and (right) convolutional networks trained on MNIST. Randomly generated direct networks are shown (gray circles)
alongside all random subspace training results (blue circles) from the sweep shown in Fig. S6. FC
networks show a persistent gap in dimension, suggesting general parameter inefﬁciency of FC models. The parameter efﬁciency of convolutional networks varies, as the gray points can be signiﬁcantly
to the right of or close to the blue manifold.
Often the most accurate directly trained models for a problem have far more parameters than needed
 ; this may be because they are just easier to train, and our observation suggests
a reason why: with larger models, solutions have greater redundancy and in a sense “cover” more
of the space.5 To our knowledge, this is the ﬁrst time this phenomenon has been directly measured.
We should also be careful not to claim that all FC nets on MNIST will have an intrinsic dimension
of around 750; instead, we should just consider that we have found for this architecture/dataset
combination a wide plateau of hyperparamter space over which intrinsic dimension is approximately
Are random subspaces really more parameter-efﬁcient for FC nets?
One might wonder to
what extent claiming 750 parameters is meaningful given that performance achieved (90%) is far
worse than a state of the art network trained on MNIST. With such a low bar for performance, could
a directly trained network with a comparable number of trainable parameters be found that achieves
the same performance? We generated 1000 small networks (depth randomly chosen from {1, 2, 3,
4, 5}, layer width randomly from {2, 3, 5, 8, 10, 15, 20, 25}, seed set randomly) in an attempt to
5To be precise, we may not conclude “greater coverage” in terms of the volume of the solution set —
volumes are not comparable across spaces of different dimension, and our measurements have only estimated
the dimension of the solution set, not its volume. A conclusion we may make is that as extra parameters are
added, the ratio of solution dimension to total dimension, s/D, increases, approaching 1. Further research
could address other notions of coverage.
Published as a conference paper at ICLR 2018
ﬁnd high-performing, small FC networks, but as Fig. 4 (left) shows, a gap still exists between the
subspace dimension and the smallest direct FC network giving the same performance at most levels
of performance.
Measuring dint90 on a convolutional network.
Next we measure dint90 of a convolutional network, LeNet (D=44,426). Fig. 2 (right) shows validation accuracy vs. subspace dimension d, and
we ﬁnd dint90 = 290, or a compression rate of about 150× for this network. As with the FC case
above, we also do a sweep of random networks,
but notice that the performance gap of convnets between direct and subspace training methods becomes closer for ﬁxed budgets, i.e., the number of trainable parameters. Further, the performance
of direct training varies signiﬁcantly, depending on the extrinsic design of convet architectures. We
interpret these results in terms of the Minimum Description Length below.
Relationship between Intrinsic Dimension and Minimum Description Length (MDL).
As discussed earlier, the random subspace training method leads naturally to a compressed representation
of a network, where only d ﬂoating point numbers need to be stored. We can consider this d as an
upper bound on the MDL of the problem solution.6 We cannot yet conclude the extent to which this
bound is loose or tight, and tightness may vary by problem. However, to the extent that it is tighter
than previous bounds (e.g., just the number of parameters D) and to the extent that it is correlated
with the actual MDL, we can use this interpretation to judge which solutions are more well-suited
to the problem in a principled way. As developed by Rissanen and further by Hinton &
Van Camp , holding accuracy constant, the best model is the one with the shortest MDL.
Thus, there is some rigor behind our intuitive assumption that LeNet is a better model than an FC
network for MNIST image classiﬁcation, because its intrinsic dimension is lower (dint90 of 290 vs.
750). In this particular case we are lead to a predictable conclusion, but as models become larger,
more complex, and more heterogeneous, conclusions of this type will often not be obvious. Having
a simple method of approximating MDL may prove extremely useful for guiding model exploration,
for example, for the countless datasets less well-studied than MNIST and for models consisting of
separate sub-models that may be individually designed and evaluated . In this latter case, considering the MDL for a sub-model could provide a more detailed
view of that sub-model’s properties than would be available by just analyzing the system’s overall
validation performance.
Finally, note that although our approach is related to a rich body of work on estimating the “intrinsic
dimension of a dataset” , it differs in a few respects. Here we do not measure
the number of degrees of freedom necessary to represent a dataset (which requires representation
of a global p(X) and per-example properties and thus grows with the size of the dataset), but those
required to represent a model for part of the dataset (here p(y|X), which intuitively might saturate
at some complexity even as a dataset grows very large). That said, in the following section we do
show measurements for a corner case where the model must memorize per-example properties.
Are convnets always better on MNIST? Measuring dint90 on shufﬂed data.
Zhang et al. 
provocatively showed that large networks normally thought to generalize well can nearly as easily be
trained to memorize entire training sets with randomly assigned labels or with input pixels provided
in random order. Consider two identically sized networks: one trained on a real, non-shufﬂed dataset
and another trained with shufﬂed pixels or labels. As noted by Zhang et al. , externally the
networks are very similar, and the training loss may even be identical at the ﬁnal epoch. However,
the intrinsic dimension of each may be measured to expose the differences in problem difﬁculty.
When training on a dataset with shufﬂed pixels — pixels for each example in the dataset subject
to a random permutation, chosen once for the entire dataset — the intrinsic dimension of an FC
network remains the same at 750, because FC networks are invariant to input permutation. But the
intrinsic dimension of a convnet increases from 290 to 1400, even higher than an FC network. Thus
while convnets are better suited to classifying digits given images with local structure, when this
structure is removed, violating convolutional assumptions, our measure can clearly reveal that many
6We consider MDL in terms of number of degrees of freedom instead of bits. For degrees of freedom stored
with constant ﬁdelity (e.g. float32), these quantities are related by a constant factor (e.g. 32).
Published as a conference paper at ICLR 2018
Table 1: Measured dint90 on various supervised and reinforcement learning problems.
MNIST (Shuf Pixels)
MNIST (Shuf Labels)
Network Type
Parameter Dim. D
Intrinsic Dim. dint90
Inverted Pendulum
Atari Pong
SqueezeNet
more degrees of freedom are now required to model the underlying distribution. When training on
MNIST with shufﬂed labels — the label for each example is randomly chosen — we redeﬁne our
measure of dint90 relative to training accuracy (validation accuracy is always at chance). We ﬁnd
that memorizing random labels on the 50,000 example MNIST training set requires a very high
dimension, dint90 = 190, 000, or 3.8 ﬂoats per memorized label. Sec. S5.2 gives a few further
results, in particular that the more labels are memorized, the more efﬁcient memorization is in terms
of ﬂoats per label. Thus, while the network obviously does not generalize to an unseen validation
set, it would seem “generalization” within a training set may be occurring as the network builds a
shared infrastructure that makes it possible to more efﬁciently memorize labels.
CIFAR-10 AND IMAGENET
We scale to larger supervised classiﬁcation problems by considering CIFAR-10 and ImageNet . When scaling beyond MNIST-sized networks
with D on the order of 200k and d on the order of 1k, we ﬁnd it necessary to use more efﬁcient
methods of generating and projecting from random subspaces. This is particularly true in the case
of ImageNet, where the direct network can easily require millions of parameters. In Sec. S7, we
describe and characterize scaling properties of three methods of projection: dense matrix projection,
sparse matrix projection , and the remarkable Fastfood transform .
We generally use the sparse projection method to train networks on CIFAR-10 and the Fastfood
transform for ImageNet.
Measured dint90 values for CIFAR-10 and are ImageNet given in Table 1, next to all previous
MNIST results and RL results to come. For CIFAR-10 we ﬁnd qualitatively similar results to
MNIST, but with generally higher dimension (9k vs. 750 for FC and 2.9k vs. 290 for LeNet).
It is also interesting to observe the difference of dint90 across network architectures. For example,
to achieve a global >50% validation accuracy on CIFAR-10, FC, LeNet and ResNet approximately
requires dint90 = 9k, 2.9k and 1k, respectively, showing that ResNets are more efﬁcient. Full results
and experiment details are given in Sec. S8 and Sec. S9. Due to limited time and memory issues,
training on ImageNet has not yet given a reliable estimate for dint90 except that it is over 500k.
REINFORCEMENT LEARNING ENVIRONMENTS
Measuring intrinsic dimension allows us to perform some comparison across the divide between
supervised learning and reinforcement learning. In this section we measure the intrinsic dimension
of three control tasks of varying difﬁculties using both value-based and policy-based algorithms.
The value-based algorithm we evaluate is the Deep Q-Network (DQN) , and the
policy-based algorithm is Evolutionary Strategies (ES) . Training details are
given in Sec. S6.2. For all tasks, performance is deﬁned as the maximum-attained (over training
iterations) mean evaluation reward (averaged over 30 evaluations for a given parameter setting). In
Fig. 5, we show results of ES on three tasks: InvertedPendulum−v1, Humanoid−v1 in Mu-
JoCo , and Pong−v0 in Atari. Dots in each plot correspond to the (noisy)
median of observed performance values across many runs for each given d, and the vertical uncertainty bar shows the maximum and minimum observed performance values. The dotted horizontal
line corresponds to the usual 90% baseline derived from the best directly-trained network 
InvertedPendulum−v1, (middle column) Humanoid−v1, and (right column) Pong−v0. The
intrinsic dimensions found are 4, 700, and 6k. This places the walking humanoid task on a similar
level of difﬁculty as modeling MNIST with a FC network (far less than modeling CIFAR-10 with a
convnet), and Pong on the same order of modeling CIFAR-10.
horizontal line). A dot is darkened signifying the ﬁrst d that allows a satisfactory performance. We
ﬁnd that the inverted pendulum task is surprisingly easy, with dint100 = dint90 = 4, meaning that
only four parameters are needed to perfectly solve the problem 
for a similarly small solution found via evolution). The walking humanoid task is more difﬁcult: solutions are found reliably by dimension 700, a similar complexity to that required to model MNIST
with an FC network, and far less than modeling CIFAR-10 with a convnet. Finally, to play Pong
on Atari (directly from pixels) requires a network trained in a 6k dimensional subspace, making it
on the same order of modeling CIFAR-10. For an easy side-by-side comparison we list all intrinsic
dimension values found for all problems in Table 1. For more complete ES results see Sec. S6.2,
and Sec. S6.1 for DQN results.
CONCLUSIONS AND FUTURE DIRECTIONS
In this paper, we have deﬁned the intrinsic dimension of objective landscapes and shown a simple
method — random subspace training — of approximating it for neural network modeling problems.
We use this approach to compare problem difﬁculty within and across domains. We ﬁnd in some
cases the intrinsic dimension is much lower than the direct parameter dimension, and hence enable
network compression, and in other cases the intrinsic dimension is similar to that of the best tuned
models, and suggesting those models are better suited to the problem.
Further work could also identify better ways of creating subspaces for reparameterization: here
we chose random linear subspaces, but one might carefully construct other linear or non-linear
subspaces to be even more likely to contain solutions. Finally, as the ﬁeld departs from single stackof-layers image classiﬁcation models toward larger and more heterogeneous networks often composed of many modules and trained by many losses, methods
like measuring intrinsic dimension that allow some automatic assessment of model components
might provide much-needed greater understanding of individual black-box module properties.
ACKNOWLEDGMENTS
The authors gratefully acknowledge Zoubin Ghahramani, Peter Dayan, Sam Greydanus, Jeff Clune,
and Ken Stanley for insightful discussions, Joel Lehman for initial idea validation, Felipe Such,
Edoardo Conti and Xingwen Zhang for helping scale the ES experiments to the cluster, Vashisht
Madhavan for insights on training Pong, Shrivastava Anshumali for conversations about random
projections, and Ozan Sener for discussion of second order methods. We are also grateful to Paul
Mikesell, Leon Rosenshein, Alex Sergeev and the entire OpusStack Team inside Uber for providing
our computing platform and for technical support.
Published as a conference paper at ICLR 2018