Kernel Methods on Riemannian Manifolds with
Gaussian RBF Kernels
Sadeep Jayasumana, Student Member, IEEE, Richard Hartley, Fellow, IEEE,
Mathieu Salzmann, Member, IEEE, Hongdong Li, Member, IEEE, and Mehrtash Harandi, Member, IEEE
Abstract—In this paper, we develop an approach to exploiting kernel methods with manifold-valued data. In many computer vision
problems, the data can be naturally represented as points on a Riemannian manifold. Due to the non-Euclidean geometry of
Riemannian manifolds, usual Euclidean computer vision and machine learning algorithms yield inferior results on such data. In this
paper, we deﬁne Gaussian radial basis function (RBF)-based positive deﬁnite kernels on manifolds that permit us to embed a given
manifold with a corresponding metric in a high dimensional reproducing kernel Hilbert space. These kernels make it possible to utilize
algorithms developed for linear spaces on nonlinear manifold-valued data. Since the Gaussian RBF deﬁned with any given metric is not
always positive deﬁnite, we present a uniﬁed framework for analyzing the positive deﬁniteness of the Gaussian RBF on a generic metric
space. We then use the proposed framework to identify positive deﬁnite kernels on two speciﬁc manifolds commonly encountered in
computer vision: the Riemannian manifold of symmetric positive deﬁnite matrices and the Grassmann manifold, i.e., the Riemannian
manifold of linear subspaces of a Euclidean space. We show that many popular algorithms designed for Euclidean spaces, such as
support vector machines, discriminant analysis and principal component analysis can be generalized to Riemannian manifolds with the
help of such positive deﬁnite Gaussian kernels.
Index Terms—Riemannian manifolds, Gaussian RBF kernels, Kernel methods, Positive deﬁnite kernels, Symmetric positive deﬁnite
matrices, Grassmann manifolds.
INTRODUCTION
ATHEMATICAL entities that do not form Euclidean
spaces but lie on nonlinear manifolds are often
encountered in computer vision. Examples include 3D
rotation matrices that form the Lie group SO(3), normalized histograms that form the unit n-sphere Sn,
symmetric positive deﬁnite (SPD) matrices and linear
subspaces of a Euclidean space. Recently, the latter
two manifolds have drawn signiﬁcant attention in the
computer vision community due to their widespread
applications. SPD matrices, which form a Riemannian
manifold when endowed with an appropriate metric ,
are encountered in computer vision in the forms of covariance region descriptors , , diffusion tensors ,
 and structure tensors , . Linear subspaces of a
Euclidean space, known to form a Riemannian manifold
named the Grassmann manifold, are commonly used to
model image sets , and videos .
Since manifolds lack a vector space structure and
other Euclidean structures such as norm and inner
product, many popular computer vision and machine
learning algorithms including support vector machines
(SVM), principal component analysis (PCA) and meanshift clustering cannot be applied in their original forms
on manifolds. One way of dealing with this difﬁculty
is to neglect the nonlinear geometry of manifold-valued
data and apply Euclidean methods directly. As intuition
suggests, this approach often yields poor accuracy and
undesirable effects, see, e.g., , for SPD matrices.
• The authors are with the College of Engineering and Computer Science,
Australian National University, Canberra and NICTA, Canberra.
Contact E-mail: 
• NICTA is funded by the Australian Government as represented by the
Department of Broadband, Communications and the Digital Economy
and the Australian Research Council (ARC) through the ICT Centre of
Excellence program.
When the manifold under consideration is Riemannian, another common approach used to cope with its
nonlinearity consists in approximating the manifoldvalued data with its projection to a tangent space at a
particular point on the manifold, for example, the mean
of the data. Such tangent space approximations are often
calculated successively as the algorithm proceeds .
However, mapping data to a tangent space only yields
a ﬁrst-order approximation of the data that can be distorted, especially in regions far from the origin of the
tangent space. Moreover, iteratively mapping back and
forth to the tangent spaces signiﬁcantly increases the
computational cost of the algorithm. It is also difﬁcult
to choose the origin of the tangent space, which heavily
affects the accuracy of this approximation.
In Euclidean spaces, the success of many computer
vision algorithms arises from the use of kernel methods , . Therefore, one could think of following
the idea of kernel methods in Rn and embed a manifold
in a high dimensional Reproducing Kernel Hilbert Space
(RKHS), where linear geometry applies. Many Euclidean
algorithms can be directly generalized to an RKHS,
which is a vector space that possesses an important
structure: the inner product. Such an embedding, however, requires a kernel function deﬁned on the manifold,
which, according to Mercer’s theorem , should be
positive deﬁnite. While many positive deﬁnite kernels
are known for Euclidean spaces, such knowledge remains limited for manifold-valued data.
The Gaussian radial basis function (RBF) kernel
exp(−γ∥x −y∥2) is perhaps the most popular and versatile kernel in Euclidean spaces. It would therefore
seem attractive to generalize this kernel to manifolds by
replacing the Euclidean distance in the RBF by a more
accurate nonlinear distance measure on the manifold,
 
such as the geodesic distance. However, a kernel formed
in this manner is not positive deﬁnite in general.
In this paper, we aim to generalize the successful and
powerful kernel methods to manifold-valued data. To
this end, we analyze the Gaussian RBF kernel on a
generic manifold and provide necessary and sufﬁcient
conditions for the Gaussian RBF kernel generated by
a distance function on any nonlinear manifold to be
positive deﬁnite. This lets us generalize kernel methods
to manifold-valued data while preserving the favorable
properties of the original algorithms.
We apply our theory to analyze the positive deﬁniteness of Gaussian kernels deﬁned on two speciﬁc manifolds: the Riemannian manifold of SPD matrices and
the Grassmann manifold. Given the resulting positive
deﬁnite kernels, we discuss different kernel methods on
these two manifolds including, kernel SVM, multiple
kernel learning (MKL) and kernel PCA. Our experiments
on a variety of computer vision tasks, such as pedestrian detection, segmentation, face recognition and action
recognition, show that our manifold kernel methods
outperform the corresponding Euclidean algorithms that
neglect the manifold geometry, as well as other state-ofthe-art techniques speciﬁcally designed for manifolds.
RELATED WORK
In this paper, we focus on the Riemannian manifold
of SPD matrices and on the Grassmann manifold. SPD
matrices ﬁnd a variety of applications in computer vision . For instance, covariance region descriptors are
used in object detection , texture classiﬁcation , ,
object tracking, action recognition and human recognition , . Diffusion Tensor Imaging (DTI) was one of
the pioneering ﬁelds for the development of non-linear
algorithms on SPD matrices , . In optical ﬂow
estimation and motion segmentation, structure tensors
are often employed to encode important image features,
such as texture and motion , . Structure tensors
have also been used in single image segmentation .
Grassmann manifolds are widely used to encode image sets and videos for face recognition , , activity recognition , , and motion grouping . In
image set based face recognition, a set of face images
of the same person is represented as a linear subspace,
hence a point on a Grassmann manifold. In activity
recognition and motion grouping, a subspace is formed
either directly from the sequence of images containing
a speciﬁc action, or from the parameters of a dynamic
model obtained from the sequence .
In recent years, several optimization algorithms have
been proposed for Riemannian manifolds. In particular,
LogitBoost for classiﬁcation on Riemannian manifolds
was introduced in . This algorithm has the drawbacks
of approximating the manifold by tangent spaces and
not scaling with the number of training samples due to
the heavy use of exponential/logarithmic maps to transit
between the manifold and the tangent space, as well
as of gradient descent based Karcher mean calculation.
Here, our positive deﬁnite kernels enable us to use
more efﬁcient and accurate classiﬁcation algorithms on
manifolds without requiring tangent space approximations. Furthermore, as shown in , , extending
existing kernel-free, manifold-based binary classiﬁers to
the multi-class case is not straightforward. In contrast,
the kernel-based classiﬁers on manifolds described in
this paper can readily be used in multi-class scenarios.
In , dimensionality reduction and clustering methods were extended to manifolds by designing Riemannian versions of Laplacian Eigenmaps (LE), Locally Linear Embedding (LLE) and Hessian LLE (HLLE). Clustering was performed after mapping to a low dimensional
space which does not necessarily preserve all the information in the original data. Instead, we use our kernels
to perform clustering in a higher dimensional RKHS that
embeds the manifold of interest.
The use of kernels on SPD matrices has previously
been advocated for locality preserving projections 
and sparse coding . In the ﬁrst case, the kernel,
derived from the afﬁne-invariant distance, is not positive
deﬁnite in general . In the second case, the kernel is
positive deﬁnite only for some values of the Gaussian
bandwidth parameter γ . For all kernel methods,
the optimal choice of γ largely depends on the data
distribution and hence constraints on γ are not desirable. Moreover, many popular automatic model selection
methods require γ to be continuously variable .
In , , the Projection kernel and its extensions
were introduced and employed for classiﬁcation on
Grassmann manifolds. While those kernels are analogous to the linear kernel in Euclidean spaces, our kernels
are analogous to the Gaussian RBF kernel. In Euclidean
spaces, the Gaussian kernel has proven more powerful
and versatile than the linear kernel. As shown in our
experiments, this also holds for kernels on manifolds.
Recently, mean-shift clustering with the heat kernel
on Riemannian manifolds was introduced . However, due to the mathematical complexity of the kernel
function, computing the exact kernel is not tractable and
hence only an approximation of the true kernel was
used. Parallel to our work, kernels on SPD matrices
and on Grassmann manifolds were used in , albeit
without explicit proof of their positive deﬁniteness. In
contrast, in this paper, we introduce a uniﬁed framework
for analyzing the positive deﬁniteness of the Gaussian
kernel deﬁned on any manifold and use this framework
to identify provably positive deﬁnite kernels on the manifold of SPD matrices and on the Grassmann manifold.
Other than for satisfying Mercer’s theorem to generate
a valid RKHS, positive deﬁniteness of the kernel is a
required condition for the convergence of many kernel based algorithms. For instance, the Support Vector
Machine (SVM) learning problem is convex only when
the kernel is positive deﬁnite . Similarly, positive
deﬁniteness of all participating kernels is required to
guarantee the convexity in Multiple Kernel Learning
(MKL) . Although theories have been proposed to
exploit non-positive deﬁnite kernels , , they have
not experienced widespread success. Many of these
methods ﬁrst enforce positive deﬁniteness of the kernel
matrix by ﬂipping or shifting its negative eigenvalues . As a consequence, they result in a distortion
of information and become inapplicable with large size
kernels, which are not uncommon in learning problems.
It is important to note the difference between this work
and manifold-learning methods such as . We work
with data sampled from a manifold whose geometry
is well known. In contrast, manifold-learning methods
attempt to learn the structure of an underlying unknown manifold from data samples. Furthermore, those
methods often assume that noise-free data samples lie
on a manifold from which noise push them away. In
our study, data points, regardless of their noise content,
always lie on the mathematically well-deﬁned manifold.
MANIFOLDS IN COMPUTER VISION
A topological manifold, generally referred to as simply a
manifold, is a topological space that is locally homeomorphic to the n-dimensional Euclidean space Rn, for some
n. Here, n is referred to as the dimensionality of the
manifold. A differentiable manifold is a topological manifold that has a globally deﬁned differential structure.
The tangent space at a given point on a differentiable
manifold is a vector space that consists of the tangent
vectors of all possible curves passing through the point.
A Riemannian manifold is a differentiable manifold
equipped with a smoothly varying inner product on
each tangent space. The family of inner products on
all tangent spaces is known as the Riemannian metric of
the manifold. It enables us to deﬁne various geometric
notions on the manifold such as the angle between two
curves and the length of a curve. The geodesic distance
between two points on the manifold is deﬁned as the
length of the shortest curve connecting the two points.
Such shortest curves are known as geodesics and are
analogous to straight lines in Rn.
The geodesic distance induced by the Riemannian
metric is the most natural measure of dissimilarity
between two points lying on a Riemannian manifold.
However, in practice, many other nonlinear distances or
metrics which do not necessarily arise from Riemannian
metrics can also be useful for measuring dissimilarity on
manifolds. It is worth noting that the term Riemannian
metric refers to a family of inner products while the term
metric refers to a distance function that satisﬁes the four
metric axioms. A nonempty set endowed with a metric
is known as a metric space which is a more abstract space
than a Riemannian manifold.
In the following, we discuss two important Riemannian manifolds commonly found in computer vision.
The Riemannian Manifold of SPD Matrices
A d × d, real Symmetric Positive Deﬁnite (SPD) matrix
S has the property: xT Sx > 0 for all nonzero x ∈Rd.
The space of d × d SPD matrices, which we denote
d , is clearly not a vector space since an SPD
matrix when multiplied by a negative scalar is no longer
SPD. Instead, Sym+
forms a convex cone in the d2dimensional Euclidean space.
The geometry of the space Sym+
d is best explained
with a Riemannian metric which induces an inﬁnite
distance between an SPD matrix and a non-SPD matrix , . Therefore, the geodesic distance induced
by such a Riemannian metric is a more accurate distance measure on Sym+
d than the Euclidean distance in
the d2-dimensional Euclidean space it is embedded in.
Two popular Riemannian metrics proposed on Sym+
are the afﬁne-invariant Riemannian metric and the
log-Euclidean Riemannian metric , . They result
in the afﬁne-invariant geodesic distance and the log-
Euclidean geodesic distance, respectively. These two distances are so far the most widely used metrics on Sym+
Apart from these two geodesic distances, a number of
other metrics have been proposed for Sym+
d to capture
its nonlinearity . For a detailed review of metrics on
d , we refer the reader to .
The Grassmann Manifold
A point on the (n, r) Grassmann manifold, where n > r,
is an r-dimensional linear subspace of the n-dimensional
Euclidean space. Such a point is generally represented by
an n × r matrix Y whose columns store an orthonormal
basis of the subspace. With this representation, the point
on the Grassmann manifold is the subspace spanned by
the columns of Y or span(Y ) which we denote by [Y ].
n to denote the (n, r) Grassmann manifold.
The set of n × r (n > r) matrices with orthonormal
columns forms a manifold known as the (n, r) Stiefel
manifold. From its embedding in the nr-dimensional Euclidean space, the (n, r) Stiefel manifold inherits a canonical Riemannian metric . Points on Gr
n are equivalence
classes of n × r matrices with orthonormal columns,
where two matrices are equivalent if their columns span
the same r-dimensional subspace. Thus, the orthogonal
group Or acts via isometries (change of orthogonal basis)
on the Stiefel manifold by multiplication on the right,
n can be identiﬁed as the set of orbits of this
action. Since this action is both free and proper, Gr
forms a manifold, and it is given a Riemannian structure
by equipping it with the standard normal Riemannian
metric derived from the metric on the Stiefel manifold.
A geodesic distance on the Grassmann manifold,
called the arc length distance, can be derived from
its canonical geometry described above. The arc length
distance between two points on the Grassmann manifold
turns out to be the l2 norm of the vector formed by
the principal angles between the two subspaces. Several
other metrics on this manifold can be derived from the
principal angles. We refer the reader to for more
details and properties of different Grassmann metrics.
HILBERT SPACE EMBEDDING
An inner product space is a vector space equipped with
an inner product. A Hilbert space is an (often inﬁnitedimensional) inner product space which is complete
with respect to the norm induced by the inner product.
A Reproducing Kernel Hilbert Space (RKHS) is a special
kind of Hilbert space of functions on some nonempty set
X in which all evaluation functionals are bounded and
hence continuous . The inner product of an RKHS of
functions on X can be deﬁned by a bivariate function on
X × X, known as the reproducing kernel of the RKHS.
Many useful computer vision and machine learning
algorithms developed for Euclidean spaces depend only
on the notion of inner product, which allows us to
measure angles and also distances. Therefore, such algorithms can be extended to Hilbert spaces without effort.
A notable special case arises with RKHSs where the inner
product of the Hilbert space can be evaluated using a
kernel function without computing the actual vectors.
This concept, known as the kernel trick, is commonly
utilized in machine learning in the following setting:
input data in some n-dimensional Euclidean space Rn
are mapped to a high dimensional RKHS where some
learning algorithm, which requires only the inner product, is applied. We never need to calculate actual vectors
in the RKHS since the learning algorithm only requires
the inner product of the RKHS, which can be calculated
by means of a kernel function deﬁned on Rn × Rn.
A variety of algorithms can be used with the kernel
trick, such as support vector machines (SVM), principal
component analysis (PCA), Fisher discriminant analysis
(FDA), k-means clustering and ridge regression.
Embedding lower dimensional data in a higher dimensional RKHS is commonly employed with data that
lies in a Euclidean space. The theoretical concepts of
such embeddings can directly be extended to manifolds.
Points on a manifold M are mapped to elements in a
high (possibly inﬁnite) dimensional Hilbert space H, a
subspace of the space spanned by real-valued functions1
on M. A kernel function k : (M × M) →R is used
to deﬁne the inner product on H, thus making it an
RKHS. The technical difﬁculty in utilizing Hilbert space
embeddings with manifold-valued data arises from the
fact that, according to Mercer’s theorem, the kernel must
be positive deﬁnite to deﬁne a valid RKHS. While many
positive deﬁnite kernel functions are known for Rn,
generalizing them to manifolds is not straightforward.
Identifying such positive deﬁnite kernel functions on
manifolds would, however, be greatly beneﬁcial. Indeed,
embedding a manifold in an RKHS has two major advantages: First, the mapping transforms the nonlinear
manifold into a (linear) Hilbert space, thus making it
1. We limit the discussion to real Hilbert spaces and real-valued
kernels, since they are the most useful kind in learning algorithms.
However, the theory holds for complex Hilbert spaces and complexvalued kernels as well.
possible to utilize algorithms designed for linear spaces
with manifold-valued data. Second, as evidenced by the
theory of kernel methods in Euclidean spaces, it yields
a much richer high-dimensional representation of the
original data, making tasks such as classiﬁcation easier.
In the following we build a framework that enables
us to deﬁne positive deﬁnite kernels on manifolds.
THEORY OF POSITIVE AND NEGATIVE DEFI-
NITE KERNELS
In this section, we present some general results on
positive and negative deﬁnite kernels. These results will
be useful for our derivations in later sections. We start
with the deﬁnition of real-valued positive and negative
deﬁnite kernels on a set . Note that by the term kernel
we mean a real-valued bivariate function hereafter.
Deﬁnition 5.1. Let X be a nonempty set. A kernel f : (X ×
X) →R is called positive deﬁnite if it is symmetric (i.e.,
f(x, y) = f(y, x) for all x, y ∈X) and
cicjf(xi, xj) ≥0
for all m ∈N, {x1, . . . , xm} ⊆X and {c1, ..., cm} ⊆R. The
kernel f is called negative deﬁnite if it is symmetric and
cicjf(xi, xj) ≤0
for all m ∈N, {x1, . . . , xm} ⊆X and {c1, ..., cm} ⊆R with
i=1 ci = 0.
It is important to note the additional constraint on
P ci for the negative deﬁnite case. Due to this constraint,
some authors refer to this latter kind as conditionally
negative deﬁnite. However, in this paper, we stick to the
most common terminology used in the literature.
We next present the following theorem which plays a
central role in this paper. It was introduced by Schoenberg in 1938 , well before the theory of Reproducing
Kernel Hilbert Spaces was established in 1950 .
Theorem 5.2. Let X be a nonempty set and f : (X ×X) →R
be a kernel. The kernel exp(−γ f(x, y)) is positive deﬁnite for
all γ > 0 if and only if f is negative deﬁnite.
Proof: We refer the reader to Theorem 3.2.2 of 
for a detailed proof of this theorem.
Note that this theorem describes Gaussian RBF–like
exponential kernels that are positive deﬁnite for all
γ > 0. One might also be interested in exponential
kernels that are positive deﬁnite for only some values of
γ. Such kernels, for instance, were exploited in . To
our knowledge, there is no general result characterizing
this kind of kernels. However, the following result can
be obtained from the above theorem.
Theorem 5.3. Let X be a nonempty set and f : (X ×X) →R
be a kernel. If the kernel exp(−γ f(x, y)) is positive deﬁnite
for all γ ∈(0, δ) for some δ > 0, then it is positive deﬁnite
for all γ > 0.
Proof: If exp(−γ f(x, y)) is positive deﬁnite for all
γ ∈(0, δ), it directly follows from Deﬁnition 5.1 that
1 −exp(−γ f(x, y)) is negative deﬁnite for all γ ∈(0, δ).
Therefore, the pointwise limit
1 −exp(−γ f(x, y))
is also negative deﬁnite. Now, since f(x, y) is negative
deﬁnite, it follows from Theorem 5.2 that exp(−γ f(x, y))
is positive deﬁnite for all γ > 0.
Next, we highlight an interesting property of negative deﬁnite kernels. It is well known that a positive deﬁnite kernel represents the inner product of an
RKHS . Similarly, a negative deﬁnite kernel represents the squared norm of a Hilbert space under some
conditions stated by the following theorem.
Theorem 5.4. Let X be a nonempty set and f(x, y) : (X ×
X) →R be a negative deﬁnite kernel. Then, there exists a
Hilbert space H and a mapping ψ : X →H such that,
f(x, y) = ∥ψ(x) −ψ(y)∥2 + h(x) + h(y)
where h : X →R is a function which is nonnegative whenever
f is. Furthermore, if f(x, x) = 0 for all x ∈X, then h = 0.
Proof: The proof for a more general version of this
theorem can be found in Proposition 3.3.2 of .
Now, we state and prove a lemma that will be useful
for the proof of our main theorem.
Lemma 5.5. Let X be a nonempty set, V be an inner product
space, and ψ : X →V be a function. Then, f : (X ×X) →R
deﬁned by f(x, y) := ∥ψ(x) −ψ(y)∥2
V is negative deﬁnite.
Proof: The kernel f is obviously symmetric. Based
i,j=1 cicjf(xi, xj) ≤0 for all m ∈N, {x1, . . . , xm} ⊆X
and {c1, ..., cm} ⊆R with Pm
i=1 ci = 0. Now,
cicjf(xi, xj) =
ψ(xi) −ψ(xj)
ψ(xi) −ψ(xj), ψ(xi) −ψ(xj)
ψ(xi), ψ(xi)
ψ(xi), ψ(xj)
ψ(xj), ψ(xj)
ψ(xi), ψ(xj)
Test for the Negative Deﬁniteness of a Kernel
In linear algebra, an m × m real symmetric matrix
M is called positive semi-deﬁnite if cT Mc ≥0 for all
c ∈Rm and negative semi-deﬁnite if cT Mc ≤0 for all
c ∈Rm. These conditions are respectively equivalent
to M having non-negative eigenvalues and non-positive
eigenvalues. Furthermore, a matrix M which has the
property ˆcT Mˆc ≤0 for all ˆc ∈Rm with P ˆci = 0,
where ˆcis are the components of the vector ˆc, is termed
conditionally negative semi-deﬁnite.
Positive (resp. negative) deﬁniteness of a given kernel–
a bivariate function–is usually tested by evaluating the
positive semi-deﬁniteness (resp. conditionally negative
semi-deﬁniteness) of kernel matrices generated with the
kernel2. Although such a test is not always conclusive if
it passes, it is particularly useful to identify non-positive
deﬁnite and non-negative deﬁnite kernels. Given a set of
points {xi}m
i=1 ⊆X and a kernel k : (X × X) →R, the
kernel matrix K of the given points has entries Kij =
k(xi, xj). Any kernel matrix generated by a positive
(resp. negative) deﬁnite kernel must be positive semideﬁnite (resp. conditionally negative semi-deﬁnite). As
noted above, it is straightforward to check the positive
semi-deﬁniteness of a kernel matrix K by checking its
eigenvalues. However, due to the additional constraint
that P ˆci = 0, checking the conditionally negative semideﬁniteness is not straightforward. We therefore suggest
the following procedure.
m, where Im is the m × m
identity matrix and 1m is the m-vector of ones. Any
given ˆc ∈Rm with P ˆci = 0 can be written as ˆc = Pc
for some c ∈Rm. Therefore, the condition ˆcT Mˆc ≤0
is equivalent to cT PMPc ≤0. Hence, we conclude
that an m × m matrix M is conditionally negative semideﬁnite if and only if PMP is negative semi-deﬁnite (i.e.,
has non-positive eigenvalues). This gives a convenient
test for the conditionally negative semi-deﬁniteness of a
matrix, which in turn is useful to evaluate the negative
deﬁniteness of a given kernel.
KERNELS ON MANIFOLDS
A number of well-known kernels exist for Rn including
the linear kernel, polynomial kernels and the Gaussian
RBF kernel. The key challenge in generalizing kernel
methods from Euclidean spaces to manifolds lies in
deﬁning appropriate positive deﬁnite kernels on the
manifold. There is no straightforward way to generalize
Euclidean kernels such as the linear kernel and polynomial kernels to nonlinear manifolds, since these kernels
depend on the linear geometry of Rn. However, we show
that the popular Gaussian RBF kernel can be generalized
to manifolds under certain conditions.
2. There is an unfortunate confusion of terminology here. A matrix
generated by a positive deﬁnite kernel is positive semi-deﬁnite while a
matrix generated by a negative deﬁnite kernel is conditionally negative
semi-deﬁnite.
In this section, we ﬁrst introduce a general theorem
that provides necessary and sufﬁcient conditions to de-
ﬁne a positive deﬁnite Gaussian RBF kernel on a given
manifold and then show that some popular metrics on
n yield positive deﬁnite Gaussian RBFs on
the respective manifolds.
The Gaussian RBF Kernel on Metric Spaces
The Gaussian RBF kernel has proven very effective in
Euclidean spaces for a variety of kernel-based algorithms. It maps the data points to an inﬁnite dimensional Hilbert space, which, intuitively, yields a very rich
representation of the data. In Rn, the Gaussian kernel
can be expressed as kG(x, y) := exp(−γ∥x −y∥2), which
makes use of the Euclidean distance between two data
points x and y. To deﬁne a kernel on a manifold, we
would like to replace the Euclidean distance by a more
accurate distance measure on the manifold. However,
not all geodesic distances yield positive deﬁnite kernels.
For example, in the case of the unit n-sphere embedded
in Rn+1, exp(−γ d2
g(x, y)), where dg is the usual geodesic
distance on the manifold, is not positive deﬁnite.
We now state our main theorem which provides the
necessary and sufﬁcient conditions to obtain a positive
deﬁnite Gaussian kernel from a given distance function
deﬁned on a generic space.
Theorem 6.1. Let (M, d) be a metric space and deﬁne k :
(M × M) →R by k(x, y) := exp(−γ d2(x, y)). Then, k is a
positive deﬁnite kernel for all γ > 0 if and only if there exists
an inner product space V and a function ψ : M →V such
that d(x, y) = ∥ψ(x) −ψ(y)∥V.
Proof: We ﬁrst note that positive deﬁniteness of
k(., .) for all γ and negative deﬁniteness of d2(., .) are
equivalent conditions according to Theorem 5.2.
To prove the forward direction of the present theorem,
let us ﬁrst assume that V and ψ exist such that d(x, y) =
∥ψ(x) −ψ(y)∥V. Then, from Lemma 5.5, d2 is negative
deﬁnite and hence k is positive deﬁnite for all γ.
On the other hand, if k is positive deﬁnite for all γ,
then d2 is negative deﬁnite. Furthermore, d(x, x) = 0 for
all x ∈M since d is a metric. Following Theorem 5.4,
then V and ψ exist such that d(x, y) = ∥ψ(x) −ψ(y)∥V.
Geodesic Distances and the Gaussian RBF
A Riemannian manifold, when considered with its
geodesic distance, forms a metric space. Given Theorem 6.1, it is then natural to wonder under which
conditions would a geodesic distance on a manifold
yield a Gaussian RBF kernel. We now present and prove
the following theorem, which answers this question for
complete Riemannian manifolds.
Theorem 6.2. Let M be a complete Riemannian manifold
and dg be the geodesic distance induced by its Riemannian
metric. The Gaussian RBF kernel kg : (M × M) →R :
kg(x, y) := exp(−γ d2
g(x, y)) is positive deﬁnite for all γ > 0
if and only if M is isometric (in the Riemannian sense) to
some Euclidean space Rn.
Proof: If M is isometric to some Rn, the geodesic
distance on M is simply the Euclidean distance in Rn,
which can be trivially shown to yield a positive deﬁnite
Gaussian RBF kernel by setting ψ in Theorem 6.1 to the
identity function.
On the other hand, if kg is positive deﬁnite, from
Theorem 6.1, there exists an inner product space Vg and
a function ψg : M →Vg such that dg(x, y) = ∥ψg(x) −
ψg(y)∥Vg. Let Hg be the completion of Vg. Therefore, Hg
is a Hilbert space, in which Vg is dense.
Now, take any two points x0, x1 in M. Since the
manifold is complete, from the Hopf-Rinow theorem,
there exists a geodesic δ(t) joining them with δ(0) = x0
and δ(1) = x1, and realizing the geodesic distance. By
deﬁnition, δ(t) has a constant speed dg(x0, x1). Therefore,
for all xt = δ(t) where t ∈ , the following equality
dg(x0, xt) + dg(xt, x1) = dg(x0, x1).
This must also be true for images ψ(xt) in Hg for
t ∈ . However, since Hg is a Hilbert space, this is
only possible if all the points ψ(xt) lie on a straight line
in Hg. Let ψ(M) be the range of ψ. From the previous
argument, for any two points in ψ(M) ⊆Hg, the straight
line segment joining them is also in ψ(M). Therefore,
ψ(M) is a convex set in Hg. Now, since M is complete,
any geodesic must be extensible indeﬁnitely. Therefore,
the corresponding line segment in ψ(M) must also be
extensible indeﬁnitely. This proves that ψ(M) is an afﬁne
subspace of Hg, which is isometric to Rn, for some n.
Since M is isometric to ψ(M), this proves that M is
isometric to the Euclidean space Rn.
According to Theorem 6.2, it is possible to obtain
a positive deﬁnite Gaussian kernel from the geodesic
distance on a Riemannian manifold only when the manifold is made essentially equivalent to some Rn by the
Riemannian metric that deﬁnes the geodesic distance.
Although this is possible for some Riemannian manifolds, such as the Riemannian manifold of SPD matrices,
for some others, it is theoretically impossible.
In particular, if the manifold is compact, it is impossible to ﬁnd an isometry between the manifold and Rn,
since Rn is not compact. Therefore, it is not possible
to obtain a positive deﬁnite Gaussian from the geodesic
distance of a compact manifold. In such cases, the best
hope is to ﬁnd a different non-geodesic distance on the
manifold that does not differ much from the geodesic
distance, but still satisﬁes the conditions of Theorem 6.1.
Kernels on Sym+
We now discuss different metrics on Sym+
be used to deﬁne positive deﬁnite Gaussian kernels.
Since Sym+
d is not compact, as explained in the previous
section, there is some hope in ﬁnding a geodesic distance
on it that also deﬁnes a positive deﬁnite Gaussian kernel.
In this section we show that the log-Euclidean distance,
which has been proved to be a geodesic distance on
d , is such a distance.
In the log-Euclidean framework, a geodesic connecting
S1, S2 ∈Sym+
d is deﬁned as γ(t) = exp((1 −t) log(S1) +
t log(S2)) for t ∈ . The log-Euclidean geodesic distance between S1 and S2 can be expressed as
dLE(S1, S2) = ∥log(S1) −log(S2)∥F ,
where ∥·∥F denotes the Frobenius matrix norm induced
by the Frobenius matrix inner product ⟨., .⟩F .
The log-Euclidean distance has proven an effective
distance measure on Sym+
d , . Furthermore, it
yields a positive deﬁnite Gaussian kernel as stated in
the following corollary to Theorem 6.1:
Corollary 6.3 (Theorem 6.1). The Log-Euclidean Gaussian
kLE(S1, S2) := exp(−γ d2
LE(S1, S2)), where dLE(S1, S2) is
the log-Euclidean distance between S1 and S2, is a positive
deﬁnite kernel for all γ > 0.
Proof: Directly follows from Theorem 6.1 with the
Frobenius matrix inner product.
A number of other metrics have been proposed for
d . The deﬁnitions and properties of these metrics are summarized in Table 1. Note that only some
of them were derived by considering the Riemannian
geometry of the manifold and hence deﬁne true geodesic
distances. Similarly to the log-Euclidean metric, from
Theorem 6.1, it directly follows that the Cholesky and
power-Euclidean metrics also deﬁne positive deﬁnite
Gaussian kernels for all values of γ. Note that some
metrics may yield a positive deﬁnite Gaussian kernel
for some value of γ only. This, for instance, was shown
in for the root Stein divergence metric. No such result is known for the afﬁne-invariant metric. Constraints
on γ are nonetheless undesirable, since one should be
able to freely tune γ to reﬂect the data distribution, and
automatic model selection algorithms require kernels to
be positive deﬁnite for continuous values of γ > 0 .
Kernels on Gr
Similarly to Sym+
d , different metrics can be deﬁned on
n. Many of these metrics are related to the principal
angles between two subspaces. Given two n × r matrices Y1 and Y2 with orthonormal columns, representing
two points on Gr
n, the principal angles between the
corresponding subspaces are obtained from the singular
value decomposition of Y T
1 Y2 . More speciﬁcally, if
USV T is the singular value decomposition of Y T
1 Y2, then
the entries of the diagonal matrix S are the cosines of
the principal angles between [Y1] and [Y2]. Let {θi}r
represent those principal angles. Then, the geodesic
distance derived from the canonical geometry of the
Grassmann manifold, called the arc length, is given by
i )1/2 . Unfortunately, as can be shown with
a counter-example, this distance, when squared, is not
negative deﬁnite and hence does not yield a positive
deﬁnite Gaussian for all γ > 0. Given Theorem 6.2 and
the discussion that followed, this not a surprising result:
Since the Grassmann manifold is a compact manifold, it
is not possible to ﬁnd a geodesic distance that also yields
a positive deﬁnite Gaussian for all γ > 0.
Nevertheless, there exists another widely used metric on the Grassmann manifold, namely the projection
metric, which gives rise to a positive deﬁnite Gaussian
kernel. The projection distance between two subspaces
[Y1], [Y2] is given by
dP ([Y1], [Y2]) = 2−1/2∥Y1Y T
We now formally introduce this Gaussian RBF kernel
on the Grassmann manifold.
Corollary 6.4 (Theorem 6.1). The Projection Gaussian
kP ([Y1], [Y2])
P ([Y1], [Y2])), where dP ([Y1], [Y2]) is the projection
distance between [Y1] and [Y2], is a positive deﬁnite kernel for
all γ > 0.
Proof: Follows from Theorem 6.1 with the Frobenius
matrix inner product.
As shown in Table 2, none of the other popular metrics
on Grassmann manifolds have this property. Counterexamples exist for these metrics to support our claims.
Calculation of the Projection Gaussian Kernel
To calculate the kernel introduced in Corollary 6.4,
one needs to calculate the squared projection distance given by d2
P ([Y1], [Y2]) = 2−1∥Y1Y T
P ([Y1], [Y2]) = (P
i sin2 θi)1/2 where both Y1 and Y2 are
n × r matrices. The second formula requires a singular
value decomposition to ﬁnd the θis, which we would like
to avoid due to its computational complexity. The ﬁrst
formula requires calculating the Frobenius l2 distance
between Y1Y T
2 , both of which are n × n
matrices. For many applications, n is quite large, and
therefore, the direct computation of ∥Y1Y T
is inefﬁcient. As a consequence, the cost of computing
P can be reduced with the following equation, which
makes use of some properties of the matrix trace and of
the fact that Y T
1 Y1 = Y T
2 Y2 = Ir:
P ([Y1], [Y2]) = 2−1∥Y1Y T
F = r −∥Y T
This implies that it is sufﬁcient to compute only the
Frobenius norm of Y T
1 Y2, an r × r matrix, to calculate
P ([Y1], [Y2]).
KERNEL-BASED ALGORITHMS
A major advantage of being able to compute positive
deﬁnite kernels on manifolds is that it directly allows
us to make use of algorithms developed for Rn, while
still accounting for the geometry of the manifold. In
this section, we discuss the use of ﬁve kernel-based
Metric Name
Geodesic Distance
Positive Deﬁnite Gaussian
Kernel for all γ > 0
Log-Euclidean
∥log(S1) −log(S2)∥F
Afﬁne-Invariant
∥log(S−1/2
∥chol(S1) −chol(S2)∥F
Power-Euclidean
Root Stein Divergence
2 log det(S1S2)
TABLE 1: Properties of different metrics on Sym+
d . We analyze the positive deﬁniteness of Gaussian kernels generated by
different metrics. Theorem 6.1 applies to the metrics claimed to generate positive deﬁnite Gaussian kernels. For the other metrics,
examples of non-positive deﬁnite Gaussian kernels exist.
Metric Name
Geodesic Distance
Positive Deﬁnite Gaussian
Kernel for all γ > 0
Projection
2−1/2∥Y1Y T
i sin2 θi)1/2
Arc length
Fubini-Study
arccos | det(Y T
1 Y2)| = arccos(Q
Chordal 2-norm
∥Y1U −Y2V ∥2 = 2 maxi sin 1
Chordal F-norm
∥Y1U −Y2V ∥F = 2(P
TABLE 2: Properties of different metrics on Gr
n. Here, USV T is the singular value decomposition of Y T
1 Y2, whereas θis are
the the principal angles between the two subspaces [Y1] and [Y2]. Our claims on positive deﬁniteness are supported by either
Theorem 6.1, or counter-examples.
algorithms on manifolds. The resulting algorithms can
be thought of as generalizations of the original Euclidean
kernel methods to manifolds. In the following, we use
M, k(., .), H and φ(x) to denote a manifold, a positive
deﬁnite kernel deﬁned on M × M, the RKHS generated
by k, and the feature vector in H to which x ∈M is
mapped, respectively. Although we use φ(x) for explanation purposes, following the kernel trick, it never needs
to be explicitly computed in any of the algorithms.
Classiﬁcation on Manifolds
We ﬁrst consider the binary classiﬁcation problem on a
manifold. To this end, we propose to extend the popular
Euclidean kernel SVM algorithm to manifold-valued
data. Given a set of training examples {(xi, yi)}m
∈M and the label yi
∈{−1, 1}, kernel
SVM searches for a hyperplane in H that separates the
feature vectors of the positive and negative classes with
maximum margin. The class of a test point x ∈M is
determined by the position of the feature vector φ(x) in
H relative to the separating hyperplane. Classiﬁcation
with kernel SVM can be done very fast, since it only
requires to evaluate the kernel at the support vectors.
The above procedure is equivalent to solving the standard kernel SVM problem with kernel matrix generated
by k. Thus, any existing SVM software package can be
utilized for training and classiﬁcation. Convergence of
standard SVM optimization algorithms is guaranteed,
since k is positive deﬁnite.
Kernel SVM on manifolds is much simpler to implement and less computationally demanding in both
training and testing phases than the current state-ofthe-art binary classiﬁcation algorithms on manifolds,
such as LogitBoost on a manifold , which involves
iteratively combining weak learners on different tangent
spaces. Weighted mean calculation in LogitBoost on a
manifold involves an extremely expensive gradient descent procedure at each boosting iteration, which makes
the algorithm scale poorly with the number of training
samples. Furthermore, while LogitBoost learns classiﬁers
on tangent spaces used as ﬁrst order Euclidean approximations to the manifold, our approach works in a rich
high dimensional feature space. As will be shown in our
experiments, this yields better classiﬁcation results.
With manifold-valued data, extending the current
state-of-the-art binary classiﬁcation methods to multiclass classiﬁcation is not straight-forward , . By
contrast, our manifold kernel SVM classiﬁcation method
can easily be extended to the multi-class case with
standard one-vs-one or one-vs-all procedures.
Feature Selection on Manifolds
multiple manifold-valued descriptors via a Multiple Kernel
Learning (MKL) approach. The core idea of MKL is to
combine kernels computed from different descriptors
(e.g., image features) to obtain a kernel that optimally
separates two classes for a given classiﬁer. Here, we
follow the formulation of and make use of an
SVM classiﬁer. As a feature selection method, MKL has
proven more effective than conventional techniques such
as wrappers, ﬁlters and boosting .
More speciﬁcally, given training examples {(xi, yi)}m
where xi ∈X (some nonempty set), yi ∈{−1, 1}, and
a set of descriptor generating functions {gj}N
→M, we seek to learn a binary classiﬁer
f : X →{−1, 1} by selecting and optimally combining
the different descriptors generated by g1, . . . , gN. Let
K(j) be the kernel matrix generated by gj and k as
= k(gj(xp), gj(xq)). The combined kernel can be
expressed as K∗= P
j λjK(j), where λj ≥0 for all j
guarantees the positive deﬁniteness of K∗. The weights
λ can be learned using a min-max optimization procedure with an l1 regularizer on λ to obtain a sparse
combination of kernels. The algorithm has two steps
in each iteration: First it solves a conventional SVM
problem with λ, hence K∗, ﬁxed. Then it updates λ with
the SVM parameters ﬁxed. These two steps are repeated
until convergence. For more details, we refer the reader
to and . Note that convergence of MKL is only
guaranteed if all the kernels are positive deﬁnite, which
is satisﬁed in this setup since k is positive deﬁnite.
We also note that MKL on manifolds gives a convenient method to combine manifold-valued descriptors
with Euclidean descriptors which is otherwise a difﬁcult
task due to their different geometries.
Dimensionality Reduction on Manifolds
We next study the extension of kernel PCA to nonlinear
dimensionality reduction on manifolds. The usual Euclidean kernel PCA has proven successful in many applications , . On a manifold, kernel PCA proceeds
as follows: All points xi ∈M of a given dataset {xi}m
are mapped to feature vectors in H, thus yielding the
transformed set {φ(xi)}m
i=1. The covariance matrix of this
transformed set is then computed, which really amounts
to computing the kernel matrix of the original data using
the function k. An l-dimensional representation of the
data is obtained by computing the eigenvectors of the
kernel matrix. This representation can be thought of as a
Euclidean representation of the original manifold-valued
data. However, owing to our kernel, it was obtained by
accounting for the geometry of M.
Once the kernel matrix is calculated with k, implementation details of the algorithm are similar to that of
the Euclidean kernel PCA algorithm. We refer the reader
to for more details on the implementation.
Clustering on Manifolds
For clustering problems on manifolds, we propose to
make use of kernel k-means. Kernel k-means maps
points to a high-dimensional Hilbert space and performs
k-means clustering in the resulting feature space . In
a manifold setting, a given dataset {xi}m
i=1, with each
xi ∈M, is clustered into a pre-deﬁned number of groups
in H, such that the sum of the squared distances from
each φ(xi) to the nearest cluster center is minimized. The
resulting clusters can then act as classes for the {xi}m
The unsupervised clustering method on Riemannian
manifolds proposed in clusters points in a lowdimensional space after dimensionality reduction on the
manifold. In contrast, our method performs clustering
in a high-dimensional RKHS which, intuitively, better
represents the data distribution.
Discriminant Analysis on Manifolds
The kernelized version of linear discriminant analysis,
known as Kernel Fisher Discriminant Analysis (Kernel
False Positives Per Window (FPPW)
Proposed Method (MKL on Manifold)
MKL with Euclidean Kernel
LogitBoost on Manifold
HOG Kernel SVM
HOG Linear SVM
Fig. 1: Pedestrian detection. Detection-Error tradeoff curves
for the proposed manifold MKL approach and state-of-theart methods on the INRIA dataset. Our method outperforms
existing manifold methods and Euclidean kernel methods. The
curves for the baselines were reproduced from .
FDA), can also be extended to manifolds on which a positive deﬁnite kernel can be deﬁned. Given {(xi, yi)}m
with each xi ∈M having class label yi, manifold kernel
FDA maps each point xi on the manifold to a feature
vector in H and ﬁnds a new basis in H where the class
separation is maximized. The output of the algorithm
is a Euclidean representation of the original manifoldvalued data, but with a larger separation between class
means and a smaller within-class variance. Up to (l −1)
dimensions can be extracted via kernel FDA where l
is the number of classes. We refer the reader to ,
 for implementation details. In Euclidean spaces,
kernel FDA has become an effective pre-processing step
to perform nearest-neighbor classiﬁcation in the highly
discriminative, reduced dimensional space.
APPLICATIONS AND EXPERIMENTS
We now present two series of experiments on Sym+
n using the positive deﬁnite kernels introduced in
Section 6 and the algorithms described in Section 7.
Experiments on Sym+
In the following, we use the log-Euclidean Gaussian
kernel deﬁned in Corollary 6.3 to apply different kernel
methods to Sym+
d . We compare our kernel methods on
d to other state-of-the-art algorithms on Riemannian
manifolds and to kernel methods on Sym+
d with the
usual Euclidean Gaussian kernel that does not account
for the nonlinearity of the manifold.
Pedestrian Detection
We ﬁrst demonstrate the use of our log-Euclidean
Gaussian kernel for the task of pedestrian detection
with kernel SVM and MKL on Sym+
d . Let {(Wi, yi)}m
be the training set, where each Wi
Rh×w is an
image window and yi
{−1, 1} is the class label
(background or person) of Wi. Following , we use
covariance descriptors computed from the feature vector
x, y, |Ix|, |Iy|,
I2x + I2y, |Ixx|, |Iyy|, arctan
where x, y are pixel locations and Ix, Iy, . . . are intensity
derivatives. The covariance matrix for an image patch
of arbitrary size therefore is an 8 × 8 SPD matrix. In
an h × w window W, a large number of covariance
descriptors can be computed from subwindows with
different sizes and positions sampled from W. We
subwindows
from h/5 × w/5 to h × w, positioned at all possible
locations. The covariance descriptor of each subwindow
is normalized using the covariance descriptor of the
full window to improve robustness against illumination
changes. Such covariance descriptors can be computed
efﬁciently using integral images .
8 denote the covariance descriptor of
the jth subwindow of Wi. To reduce this large number
of descriptors, we pick the best 100 subwindows that
do not mutually overlap by more than 75%, by ranking
them according to their variance across all training samples. The rationale behind this ranking is that a good
descriptor should have a low variance across all given
positive detection windows. Since the descriptors lie on
a manifold, for each X(j), we compute a variance-like
statistic var(X(j)) across all positive training samples as
var(X(j)) =
where m+ is the number of positive training samples
¯X is the Karcher mean of {Xi}i:yi=1 given by
i:yi=1 log(Xi)
under the log-Euclidean
metric. We set p = 1 in Eq.(3) to make the statistic less
sensitive to outliers. We then use the SVM-MKL framework described in Section 7.2 to learn the ﬁnal classiﬁer,
where each kernel is deﬁned on one of the 100 selected
subwindows. At test time, detection is achieved in a
sliding window manner followed by a non-maximum
suppression step.
To evaluate our approach, we made use of the INRIA
person dataset . Its training set consists of 2,416
positive windows and 1,280 person-free negative images,
and its test set of 1,237 positive windows and 453
negative images. Negative windows are generated by
sampling negative images . We ﬁrst used all positive samples and 12,800 negative samples (10 random
windows from each negative image) to train an initial
classiﬁer. We used this classiﬁer to ﬁnd hard negative
examples in the training images, and re-trained the
classiﬁer by adding these hard examples to the training
set. Cross validation was used to determine the hyperparameters including the parameter γ of the kernels. We
used the evaluation methodology of .
In Fig. 1, we compare the detection-error tradeoff
(DET) curves of our approach and state-of-the-art methods. The curve for our method was generated by continuously varying the decision threshold of the ﬁnal MKL
classiﬁer. We also evaluated our MKL framework with
the Euclidean Gaussian kernel. Note that the proposed
MKL method with our Riemannian kernel outperforms
MKL with the Euclidean kernel, as well as LogitBoost
on the manifold. This demonstrates the importance of
accounting for the nonlinearity of the manifold using an
appropriate positive deﬁnite kernel. It is also worth noting that LogitBoost on the manifold is signiﬁcantly more
complex and harder to implement than our method.
Visual Object Categorization
We next tackle the problem of unsupervised object categorization. To this end, we used the ETH-80 dataset 
which contains 8 categories with 10 objects each and 41
images per object. We used 21 randomly chosen images
from each object to compute the parameter γ and the rest
to evaluate clustering accuracy. For each image, we used
a single 5 × 5 covariance descriptor calculated from the
features [x, y, I , |Ix| , |Iy|], where x, y are pixel locations
and I, Ix, Iy are intensity and derivatives. To obtain
object categories, the kernel k-means algorithm on Sym+
described in Section 7.4 was employed.
One drawback of k-means and its kernel counterpart
is their sensitivity to initialization. To overcome this, we
ran each algorithm 20 times with random initializations
and picked the iteration that converged to the minimum
sum of point-to-centroid squared distances. For kernel kmeans on Sym+
5 , distances in the RKHS were used. We
assumed the number of clusters to be known.
To set a benchmark, we evaluated the performance
of both k-means and kernel k-means on Sym+
different metrics that generate positive deﬁnite Gaussian
kernels (see Table 1). For the power-Euclidean metric,
we used α = 0.5, which, as in the non-kernel method
of , we found to yield the best results. For all
non-Euclidean metrics with (non-kernel) k-means, the
Karcher mean was used to compute the centroid.
The results of the different methods are summarized in
Table 3. Manifold kernel k-means with the log-Euclidean
Gaussian kernel performs signiﬁcantly better than all
other methods in all test cases. These results also outperform the results with the heat kernel reported in .
Note, however, that only considered 3 and 4 classes
without mentioning which classes were used.
Texture Recognition
We then utilized our log-Euclidean Gaussian kernel to
demonstrate the effectiveness of manifold kernel PCA
on texture recognition. To this end, we used the Brodatz
dataset , which consists of 111 different 640 × 640
texture images. Each image was divided into four subimages of equal size, two of which were used for training
and the other two for testing.
For each training image, covariance descriptors of 50
randomly chosen 128 × 128 windows were computed
from the feature vector [I, |Ix|, |Iy|, |Ixx| , |Iyy|] . Kernel PCA on Sym+
with our Riemannian kernel was
Power-Euclidean
Log-Euclidean
TABLE 3: Object categorization. Sample images and percentages of correct clustering on the ETH-80 dataset using k-means
(KM) and kernel k-means (KKM) with different metrics on Sym+
d . The proposed KKM method with the log-Euclidean metric
achieves the best results in all the tests.
Classiﬁcation Accuracy
Log-Euclidean
TABLE 4: Texture recognition. Recognition accuracies on the
Brodatz dataset with k-NN in an l-dimensional Euclidean
space obtained by kernel PCA. The log-Euclidean Gaussian
kernel introduced in this paper captures information more
effectively than the usual Euclidean Gaussian kernel.
then used to extract the top l principal directions in the
RKHS, and project the training data along those directions. Given a test image, we computed 100 covariance
descriptors from random windows and projected them
to the l principal directions obtained during training.
Each such projection was classiﬁed using a majority vote
over its 5 nearest-neighbors. The class of the test image
was then decided by majority voting among the 100
descriptors. Cross validation on the training set was used
to determine γ. For comparison purposes, we repeated
the same procedure with the Euclidean Gaussian kernel.
Results obtained for these kernels and different values
of l are presented in Table 4. The better recognition
accuracy indicates that kernel PCA with the Riemannian
kernel more effectively captures the information of the
manifold-valued descriptors than the Euclidean kernel.
Segmentation
We now illustrate the use of our kernel to segment
different types of images. First, we consider DTI segmentation, which is a key application area of algorithms
d . We utilized kernel k-means on Sym+
3 with our
Riemannian kernel to segment a real DTI image of the
human brain. Each pixel of the input DTI image is a 3×3
SPD matrix, which can thus directly be used as input to
the algorithm. The k clusters obtained by the algorithm
act as classes, thus yielding a segmentation of the image.
Fig. 2 depicts the resulting segmentation along with
the ellipsoid and fractional anisotropy representations
of the original DTI image. We also show the results
obtained by replacing the Riemannian kernel with the
Euclidean one. Note that, up to some noise due to the
lack of spatial smoothing, Riemannian kernel k-means
was able to correctly segment the corpus callosum from
the rest of the image.
We then followed the same approach to perform 2D
motion segmentation. To this end, we used a spatio-
Ellipsoids
Fractional Anisotropy
Riemannian kernel
Euclidean kernel
Fig. 2: DTI segmentation. Segmentation of the corpus callosum with kernel k-means on Sym+
3 . The proposed kernel yields
a cleaner segmentation.
temporal structure tensor directly computed on image intensities (i.e., without extracting features such as
optical ﬂow). The spatio-temporal structure tensor for
each pixel is computed as T = K ∗(∇I∇IT ), where
∇I = (Ix, Iy, It) and K∗indicates convolution with
the regular Gaussian kernel for smoothing purposes.
Each pixel is thus represented as a 3 × 3 SPD matrix T
and segmentation can be performed by clustering these
matrices using kernel k-means on Sym+
We applied this strategy to two images taken from the
Hamburg Taxi sequence. Fig. 3 compares the results of
kernel k-means with our log-Euclidean Gaussian kernel
with the results of obtained by ﬁrst performing LLE,
LE, or HLLE on Sym+
3 and then clustering in the low dimensional space. Note that our approach yields a much
cleaner segmentation than the baselines. This could be
attributed to the fact that we perform clustering in a high
dimensional feature space, whereas the baselines work
in a reduced dimensional space.
Experiments on Gr
We now present our experimental evaluation of the
proposed kernel methods on Gr
n with the projection
Gaussian kernel introduced in Corollary 6.4. We compare
our results to those obtained with state-of-the-art classi-
ﬁcation and clustering methods on Gr
n, and show that we
achieve signiﬁcantly better classiﬁcation and clustering
accuracies in a number of different tasks.
In each of the following experiments, we model an
image set with the linear subspace spanned by its principal components. More speciﬁcally, let {fi}p
i=1, with each
KKM on Sym+
LLE on Sym+
LE on Sym+
HLLE on Sym+
Fig. 3: 2D motion segmentation. Comparison of the segmentations obtained with kernel k-means with our Riemannian kernel (KKM), LLE, LE and HLLE on Sym+
3 . Our KKM algorithm
yields a much cleaner segmentation. The baseline results were
reproduced from .
fi ∈Rn, be a set of descriptors each representing one
image in a set of p images. The image set can then
be represented by the linear subspace spanned by r
(r < p, n) principal components of {fi}p
i=1. Limiting r
helps reducing noise and other ﬁne variations within
the image set, which are not useful for classiﬁcation or
clustering. The image set descriptors obtained in this
manner are r-dimensional linear subspaces of the ndimensional Euclidean space, which lie on the (n, r)
Grassmann manifold Gr
n. We note that the r-principal
components of {fi}p
i=1 can be efﬁciently obtained by
performing a Singular Value Decomposition (SVD) on
the n × p matrix F having the fis as columns: If USV T
is the singular value decomposition of F, the columns
of U corresponding to the largest r singular values give
the r principal components of {fi}p
Video Based Face Recognition
Face recognition from video, which uses an image set for
identiﬁcation of a person, is a rapidly developing area
in computer vision. For this task, the videos are often
modeled as linear subspaces, which lie on a Grassmann
manifold , . To demonstrate the use of our projection Gaussian kernel in video based face recognition, we
used the YouTube Celebrity dataset , which contains
1910 video clips of 47 different people. Face recognition
on this dataset is challenging since the videos are highly
compressed and most of them have low resolution.
We used the cascaded face detector of to extract
face regions from videos and resized them to have a
common size of 96 × 96. Each face image was then
represented by a histogram of Local Binary Patterns 
having 232 equally-spaced bins. We next represented
each image set corresponding to a single video clip by
a linear subspace of order 5. We randomly chose 70%
of the dataset for training and the remaining 30% for
testing. We report the classiﬁcation accuracy averaged
over 10 different random splits.
We employed both kernel SVM on a manifold and
kernel FDA on a manifold with our projection Gaussian
kernel. With kernel SVM, a one-vs-all procedure was
Face Recognition
Action Recognition
60.21 ± 2.9
41.95 ± 9.6
67.49 ± 3.5
70.05 ± 0.9
58.72 ± 3.0
67.33 ± 1.1
61.05 ± 2.2
73.54 ± 2.0
Linear SVM
64.76 ± 2.1
74.66 ± 1.2
Manifold Kernel FDA
65.32 ± 1.4
76.35 ± 1.0
Manifold Kernel SVM
71.78 ± 2.4
76.95 ± 0.9
TABLE 5: Face and action recognition. Recognition accuracies
on the YouTube Celebrity and Ballet datasets. Our manifold
kernel SVM method achieves the best results.
used for multiclass classiﬁcation. For kernel FDA, the
training data was projected to an (l −1) dimensional
space, where l = 47 is the number of classes, and
we used a 1-nearest-neighbor method to predict the
class of a test sample projected to the same space. We
determined the hyperparameters of both methods using
cross-validation on the training data.
We compared our approach with several state-of-theart image set classiﬁcation methods: Discriminant analysis of Canonical Correlations (DCC) , Kernel Afﬁne
Hull Method (KAHM) , Grassmann Discriminant
Analysis (GDA) , and Graph-embedding Grassmann
Discriminant Analysis (GGDA) . As shown in Table 5,
manifold kernel SVM achieves the best accuracy. GDA
uses the Grassmann projection kernel which corresponds
to the linear kernel with FDA in a Euclidean space.
Therefore, the results of GDA and Manifold Kernel FDA
in Table 5 also provide a nice comparison between the
linear kernel and our projection Gaussian kernel. To obtain a similar comparison with SVMs, we also performed
one-vs-all SVM classiﬁcation with the projection kernel,
which really amounts to linear SVM in the Projective
space. This method is denoted by Linear SVM in Table 5.
With both FDA and SVM, our Gaussian kernel performs
better than the projection kernel, thus agreeing with the
observation in Euclidean spaces that the Gaussian kernel
performs better than the linear kernel.
Action Recognition
We next demonstrate the beneﬁts of our projection
Gaussian kernel on action recognition. To this end, we
used the Ballet dataset , which contains 44 videos
of 8 actions performed by 3 different actors. Each video
contains different actions performed by the same actor.
Action recognition on this dataset is challenging due to
large intra-class variations in clothing and movement.
We grouped every 6 subsequent frames containing the
same action, which resulted in 2338 image sets. Each
frame was described by a Histogram of Oriented Gradients (HOG) descriptor , and 4 principal components
were used to represent each image set. The samples
were randomly split into two equally-sized sets to obtain
training and test data. We report the average accuracy
over 10 different splits.
As in the previous experiment, we used kernel FDA
and one-vs-all SVM with the projection Gaussian kernel
and compared our methods with DCC, KAHM, GDA,
GGDA, and Linear SVM. As evidenced by the results
in Table 5, Manifold Kernel FDA and Manifold Kernel
SVM both achieve superior performance compared to
the state-of-the-art algorithms. Out of the two methods
proposed in this paper, Manifold Kernel SVM achieves
the highest accuracy.
Pose Categorization
Finally, we demonstrate the use of our projection Gaussian kernel in clustering on the Grassmann manifold.
To this end, we used the well-known CMU-PIE face
dataset , which contains face images of 67 subjects
with 13 different poses and 21 different illuminations.
Images were closely cropped to enclose the face region
followed by a resizing to 32×32. The vectorized intensity
values were directly used to describe each image. We
used images of the same subject with the same pose
but different illuminations to form an image set, which
was then represented by a linear subspace of order 6.
This resulted in a total of 67 × 13 = 871 Grassmann
descriptors, each lying on G6
The goal of the experiment was to cluster together
image sets having the same pose. We randomly divided
the images of the same subject with the same pose into
two equally sized sets to obtain two collections of 871
image sets. The optimum value for γ was determined
with the ﬁrst collection and the results are reported
on the second one. We compare our Manifold Kernel
k-means (MKKM) with two other algorithms on the
same data. The ﬁrst algorithm, proposed in , is the
conventional k-means with the arc length distance on
n and the corresponding Karcher mean (KM-AL). The
publicly available code of was used to obtain the
results with KM-AL. Since the Karcher mean with the arc
length distance does not have a closed-form solution and
has to be calculated using a gradient descent procedure,
KM-AL tends to slow down with the dimensionality of
the Grassmann descriptors and the number of samples.
The second baseline was k-means with the projection
metric and the corresponding Karcher mean (KM-PM).
Although the Karcher mean can be calculated in closedform in this case, the algorithm becomes slow when
working with large matrices. In our setup, since the
projection space consisted of symmetric matrices of size
1024×1024, projection k-means boils down to performing
k-means in a 1024 × (1024 + 1)/2 = 524, 800 dimensional
The results of the three clustering algorithms for different numbers of clusters are given in Table 6. As in the
experiment on Sym+
d , each clustering algorithm was run
20 times with different random initializations, and the
iteration which converged to the minimum energy was
picked. Note that our MKKM algorithm yields the best
performance in all scenarios. Performance gain with the
MKKM algorithm becomes more signiﬁcant when the
problem gets harder with more clusters.
Nb. of Clusters
Clustering Accuracy
TABLE 6: Pose grouping. Clustering accuracies on the CMU-
PIE dataset. The proposed kernel k-means method yields the
best results in all the test cases.
CONCLUSION
In this paper, we have introduced a uniﬁed framework
to analyze the positive deﬁniteness of the Gaussian RBF
kernel deﬁned on a manifold or a more general metric
space. We have then used the same framework to derive
provably positive deﬁnite kernels on the Riemannian
manifold of SPD matrices and on the Grassmann manifold. These kernels were then utilized to extend popular
learning algorithms designed for Euclidean spaces, such
as SVM and FDA, to manifolds. Our experimental evaluation on several challenging computer vision tasks, such
as pedestrian detection, object categorization, segmentation, action recognition and face recognition, has evidenced that identifying positive deﬁnite kernel functions
on manifolds can be greatly beneﬁcial when working
with manifold-valued data. In the future, we intend to
study this problem for other nonlinear manifolds, as well
as to extend our theory to more general kernels than the
Gaussian RBF kernel.
ACKNOWLEDGEMENTS
This work was supported in part by an ARC grant.
The authors would like thank Bob Williamson for useful
discussions.