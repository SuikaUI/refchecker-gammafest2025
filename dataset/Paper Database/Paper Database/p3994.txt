Multi-ﬁdelity optimization via
surrogate modelling
BY ALEXANDER I. J. FORRESTER*, ANDRA´ S SO´ BESTER AND
ANDY J. KEANE
Computational Engineering and Design Group, School of Engineering Sciences,
University of Southampton, Southampton SO17 1BJ, UK
This paper demonstrates the application of correlated Gaussian process based
approximations to optimization where multiple levels of analysis are available, using an
extension to the geostatistical method of co-kriging. An exchange algorithm is used to
choose which points of the search space to sample within each level of analysis. The
derivation of the co-kriging equations is presented in an intuitive manner, along with a new
variance estimator to account for varying degrees of computational ‘noise’ in the multiple
levels of analysis. A multi-ﬁdelity wing optimization is used to demonstrate the
methodology.
Keywords: co-kriging; kriging; noise; subset selection; wing design
1. Introduction
Let f(x) denote an objective function that maps a vector x of length k to some
scalar measure of merit. x is sought, such that f(x) is the maximum or
minimum of f(x). Such optimization problems (of which this is the simplest form)
are encountered in most branches of science. Of interest to us in this paper is the
class of optimization problems where f(x) is expensive to obtain or we know its
value only at a limited number of x values. Thus, efﬁciency, in terms of the
number of x values that must be mapped to their objective function values before
we get to within a reasonable distance of x, is the key feature of the optimization
algorithms considered.
While there is often no getting around the fact that a thorough search of a
highly nonlinear, multi-dimensional f landscape will require sampling at a large
number of sites, a useful shortcut is often employed. Based on a relatively small
number of measurements, we can build a statistical approximation of the
objective landscape, which, provided f is smooth and continuous and the
measurements are reasonably uniformly spread, will be accurate enough to guide
the search towards promising areas of the landscape.
If, at the same time, this surrogate of the expensive function is very cheap to
evaluate, we can be as thorough as we like in searching it. Thus, most of the
computational effort will be concentrated on what looks likely to be the
Proc. R. Soc. A 463, 3251–3269
doi:10.1098/rspa.2007.1900
Published online 2 October 2007
* Author for correspondence ( ).
Received 20 March 2007
Accepted 6 September 2007
This journal is q 2007 The Royal Society
neighbourhood of the global optimum. An approximation technique that has
received much attention in recent years is kriging1—we shall delve into the
details of this later on, as our chosen multi-ﬁdelity optimization technique is an
extension of kriging. But what exactly do we mean by multi-ﬁdelity
optimization?
The objective function evaluation system may feature lower-ﬁdelity, cheaper
models (in addition to the main, high-ﬁdelity calculation of f ). In most cases, the
fast (but less trustworthy) and the slow (but more accurate) objective values can
be obtained independently. Thus, we can learn more about the objective function
by additionally measuring the cheap function(s) on a large number of x values.
This process is usually referred to as multi-ﬁdelity optimization.
The use of secondary, correlated quantities to improve the accuracy of the
model of the primary objective is not a new concept. For example, Hevesi et al.
 predict average annual precipitation values near a potential nuclear waste
disposal site using a sparse set of precipitation measurements from the region
along with the correlated and more easily obtainable elevation map of the area.
Kennedy & O’Hagan approach the subject from the perspective of model
building using objectives resulting from computational simulations of varying
ﬁdelities and costs. This is also the context of the work presented here;
additionally, we introduce a method of optimization using correlated models of
different costs and ﬁdelities. We focus on combining co-kriging (the multiresponse extension to kriging alluded to earlier) with a Bayesian model update
criterion designed to balance the exploration of the search space and the quick
exploitation of promising basins of attraction on the f landscape. The criterion is
based on a newly derived error estimate that reﬂects the presence of noise in the
observed data.
After a brief overview of kriging in §2, we discuss co-kriging in §3, followed by
considerations on how to create sampling plans for multiple levels of analyses
(§4). We put these pieces together in §5, where we describe an optimization
strategy designed for multi-ﬁdelity inputs. An aircraft wing design problem is
used to demonstrate and compare the co-kriging approach and other multi-level
strategies in §6. Section 7 discusses the remaining issues related to the use of
correlated surrogates and summarizes our conclusions.
2. Kriging
We will consider co-kriging as a natural extension to the popular method of
kriging and hence begin with a very brief introduction to the kriging method in
order to introduce concepts and notation that follow through to co-kriging.
Equations are shown without derivations, which are very similar to those
presented later for co-kriging; the reader may wish to consult Jones or
Forrester et al. for more information on kriging.
As with all surrogate-based methods, to approximate a function f we start with
a set of sample data—usually computed at a set of points in the domain of
interest determined by a sampling plan (which will be discussed further in §4).
1 Originally called krigeage by Matheron after D. G. Krige—a South African mining
engineer who pioneered the method in the 1950s for determining ore-grade distributions based on
core samples .
A. I. J. Forrester et al.
Proc. R. Soc. A 
The kriging prediction of f is built from a mean base term, ^m (the circumﬂex
denotes a maximum likelihood estimate, MLE) plus a stationary Gaussian
process, Z(x), representing the local features of f around the n sample points,
XZfxð1Þ; .; xðnÞgT, x 2 Rk. Z(x) has zero mean and covariance
cov½ZðxðnC1ÞÞ; ZðxðiÞÞ Z s2jðiÞ;
where jðiÞ are correlations between a random variable Y(x) at the point to be
predicted (x(nC1)) and at the sample data points
jðiÞ Z corr½YðxðnC1ÞÞ; YðxðiÞÞ Z exp K
^qjkxðnC1Þ
The hyper-parameter, pj, can be thought of as determining the smoothness of the
function approximation. In many situations, we can assume that there will not be
any discontinuities and use pjZ2 rather than using an MLE. This means that the
basis function is inﬁnitely differentiable through a sample point (when
kxðnC1ÞKxðiÞkZ0) and that the function is in the same form as a Gaussian
distribution with variance 1=^qj. ^q therefore can be thought of as determining how
quickly the function changes as x(nC1) moves away from x(i ), with high and
low ^qj values indicating an active or inactive function along dimension j,
respectively. The kriging prediction is found as the value at x(nC1) that,
maximizes the likelihood, given the sample data and MLEs of the hyperparameters, and is given by
^yðxðnC1ÞÞ Z ^m C
bðiÞjðiÞðxðnC1Þ; xðiÞÞ:
The constants bi are given by the column vector bZJK1ðy K1^mÞ, where J is an
n!n symmetric matrix of correlations between the sample data; y is a column
vector of responses, fyðxð1ÞÞ; .; yðxðnÞÞgT; 1 is an n!1 column vector of ones;
and the MLE ^mZ1TJK1y=1TJK11.
3. Co-kriging
We now consider how to build an approximation of a function that is expensive
to evaluate which is enhanced by data from cheaper analyses of the function.
Combining multiple sets of data naturally leads to a complex notation and we
will try to simplify this by limiting ourselves to two datasets:2 the most accurate
expensive data with values ye at points Xe and the less accurate cheap data yc at
points Xc (Xe3Xc).3 These data are concatenated to give the combined set
c ; .; xðncÞ
e ; .; xðneÞ
2 Our methods can be extended to multiple code levels following the notation used by Kennedy &
O’Hagan .
3 We require partially collocated points in our estimation of a scaling parameter between the data.
It is possible to construct the presented co-kriging model with completely non-collocated points by
using kriging estimates ^ycðXeÞ.
Multi-ﬁdelity optimization
Proc. R. Soc. A 
As with kriging, the value at a point in X is treated as if it were the realization of
a Gaussian random variable. For co-kriging, we therefore have the random ﬁeld
Z ðYcðxð1Þ
c Þ; .; Ycðxðn cÞ
Þ; Yeðxð1Þ
e Þ; .; Yeðxðn eÞ
We follow the auto-regressive model of Kennedy & O’Hagan , which
assumes that cov Yeðxði ÞÞ; YcðxÞjYcðxði ÞÞ
Z0, cx sxði Þ. This means that no
more can be learnt about Yeðxði ÞÞ from the cheaper code if the value of the
expensive function at x(i ) is known—a Markov property (i.e. we assume that the
expensive simulation is correct and any inaccuracies lie wholly in the cheaper
simulation). Gaussian processes Zc($) and Ze($) represent the local features of the
cheap and expensive codes. Using the auto-regressive model, we are essentially
approximating the expensive code as the cheap code multiplied by a scaling
factor r plus a Gaussian process Zd($) that represents the difference between
rZc($) and Ze($)
ZeðxÞ Z rZcðxÞ CZdðxÞ:
Whereas in kriging we have a covariance matrix covfYðXÞ; YðXÞgZ
s2JðX; XÞ, we now have a more complex covariance matrix constructed as
covfY cðXcÞ; Y cðXcÞg
Z covfZcðXcÞ; ZcðXcÞg
cJcðXc; XcÞ;
covfY eðXeÞ; Y cðXcÞg
Z covfrZcðXcÞ CZdðXcÞ; ZcðXeÞg
cJcðXc; XeÞ;
covfY eðXeÞ; Y eðXeÞg
Z covfrZcðXeÞ CZdðXeÞ; rZcðXeÞ CZdðXeÞg
Z r2covfZcðXeÞ; ZcðXeÞg CcovfZdðXeÞ; ZdðXeÞg
cJcðXe; XeÞ Cs2
dJdðXe; XeÞ:
The notation JcðXe; XcÞ, for example, denotes a matrix of correlations of the
form jc between the data Xe and Xc. Our complete covariance matrix is thus
cJcðXc; XcÞ
cJcðXc; XeÞ
cJcðXe; XcÞ
cJcðXe; XeÞ Cs2
dJdðXe; XeÞ
The correlations are of the same form as equation (2.2), but there are two
correlations, jc and jd, and we therefore have more hyper-parameters to
estimate: qc; qd; pc; pd; and the scaling parameter r. Our cheap data are
considered to be independent of the expensive data and we can ﬁnd MLEs for mc,
c, qc and pc by maximizing the ln-likelihood (ignoring constant terms)
2 lnjdetðJcðXc; XcÞÞjK ðyc K1mcÞTJcðXc; XcÞK1ðyc K1mcÞ
A. I. J. Forrester et al.
Proc. R. Soc. A 
By setting the derivatives of equation (3.3) w.r.t. mc and s2
c to 0 and solving, we ﬁnd
^mc Z 1TJcðXc; XcÞK1yc=1TJcðXc; XcÞK11
c Z ðyc K1^mcÞTJcðXc; XcÞK1ðyc K1^mcÞ=nc:
Substituting equations (3.4) and (3.5) into (3.3) yields the concentrated ln-likelihood
2 lnjdetðJcðXc; XcÞÞj
and ^qc and ^pc (if not set at 2) are found by maximizing this equation.
To estimate the mean, variance, hyper-parameters and scaling parameter of
the difference model (md, s2
d, qd, pd and r), we ﬁrst deﬁne
d Z yeKrycðXeÞ;
where yc(Xe) are the values of yc at locations common to those of Xe (the
Markov property implies that we need to consider only this data). If yc is not
available at Xe, we may estimate r at little additional cost by using kriging
estimates ^ycðXeÞ, found from equation (2.3), using the already determined
hyper-parameters ^qc and ^pc. The ln-likelihood of d, given yc, is now
2 lnjdetðJdðXe; XeÞÞj
K ðd K1mdÞTJdðXe; XeÞK1ðd K1mdÞ
yielding MLEs of
^md Z 1TJdðXe; XeÞK1d=1TJdðXe; XeÞK11
d Z ðd K1^mdÞTJdðXe; XeÞK1ðd K1^mdÞ=ne;
with ^qd, ^pd (again, if not set at 2) and ^r found by maximizing
2 lnjdetðJdðXe; XeÞÞj:
Equations (3.6) and (3.10) must be maximized numerically using a suitable
global search routine such as a genetic algorithm (GA). Depending upon the cost
of evaluating the cheap and expensive functions fc and fe, for very high
dimensional problems, the multiple matrix inversions involved in the likelihood
maximization may render the use of the co-kriging model impractical (the size of
the matrices depends directly on the quantities of data available, and the number
of search steps needed in the MLE process is linked to the number of hyperparameters being tuned). Typically, a statistical model used as a surrogate will
be tuned many fewer times than the number of evaluations of fe required by a
direct search. The cost of tuning the model can therefore be allowed to exceed the
cost of computing fe and still provide signiﬁcant speed-up. For large k and n,
Multi-ﬁdelity optimization
Proc. R. Soc. A 
the time required to ﬁnd MLEs can be reduced by using a constant qc,j and qd,j
for all elements of qc and qd to simplify the maximization, though this may affect
the accuracy of the approximation.
With the hyper-parameters estimated, the co-kriging prediction of the
expensive function is given by
^yeðxðneC1ÞÞ Z ^m CcTC K1ðy K1^mÞ;
cjc Xc; xðnC1Þ
cjc Xe; xðnC1Þ
djd Xe; xðnC1Þ
and ^mZ1TC K1y=1TC K11. The notation jc Xe; xðnC1Þ
, for example, denotes a
column vector of correlations of the form jc between the data Xe and the new
point x(nC1). The derivation of equation (3.11) is given in appendix A. If we
make a prediction at one of our expensive points, xðnC1ÞZxðiÞ
and c is the
n cCith column of C, then cTC K1 is the n cCith unit vector and ^yeðxðiÞ
^mCyðncCiÞK ^mZyðiÞ
e . We see, therefore, that equation (3.11) is an interpolator of
the expensive data. If we make a prediction at one of the cheap points,
xðnC1ÞZxðiÞ
c , c is not a column of C and the predictor will in some sense regress yc
unless it coincides with ye.
The estimated mean-squared error in the predictor is given as follows:
s2ðxÞ Z ^r2^s2
dKcTC K1c:
Again, a derivation is given in appendix A. Since the co-kriging model is an
interpolator of ye, we expect the error to be zero at the expensive sample points. For
xðneC1ÞZxðiÞ
e , cTC K1 is the n cCith unit vector, cTC K1cZcðncCiÞZr2
hence s2(x) is indeed 0. For Xc\Xe, s2(x)s0 unless yeZyc(Xe). The error at these
points is determined by the character of Yd. If the difference between rYc(Xe) and
Ye(Xe) is simple (characterized by low qd,j values), the error will be low, whereas a
more complex difference (high qd,j values) will lead to high error estimates.
(a ) One-variable demonstration
We will now look at how co-kriging behaves using an example of a simple
one-variable function. Imagine that our expensive to compute data are
calculated by the function feðxÞZð6xK2Þ2 sinð12xK4Þ, x 2 f0; 1g, and a
cheaper estimate of this data is given by fcðxÞZAfeCBðxK0:5ÞKC. We
extensively
XcZf0; 0:1; 0:2; 0:3; 0:4; 0:5; 0:6; 0:7; 0:8; 0:9; 1g, but run the expensive function
only at four of these points, XeZf0; 0:4; 0:6; 1g.
Figure 1 shows the functions fe and fc with AZ0.5, BZ10 and CZK5. A
kriging prediction through ye gives a poor approximation to the deliberately
deceptive function, but the co-kriging prediction lies very close to fe, being better
than both the standard kriging model and the cheap data. Despite the
considerable differences between fe and fc, a simple relationship has been found
between the expensive and the cheap data and the estimated error reduces
almost to 0 at Xc (ﬁgure 2).
A. I. J. Forrester et al.
Proc. R. Soc. A 
We have chosen the relationship between our low- and high-ﬁdelity test
functions in order to show how the hyper-parameters of the co-kriging model
behave. The hyper-parameters pertaining to the cheap data, qc and pc, are affected
only by this data and behave as described in §2. Moving on to the scaling parameter
r, if our cheap model parameter A (the multiplying term linking the cheap and
expensive functions) is varied such that 1=A 2 fK10; 10g, we obtain the values for
^r shown in ﬁgure 3 and see that ^rx1=A. Similar trials show that the parameters B
and C have no effect on r; thus we see that r is purely a scaling parameter. Note that
^r is only an indicator of the scaling, since this value is estimated based on the data
Figure 2. Estimated error in the co-kriging prediction in ﬁgure 1. The simple relationship between
the data results in low error estimates at Xc as well as Xe.
kriging through
co-kriging
Figure 1. A one-variable co-kriging example. The kriging approximation using four expensive data
points (ye) has been signiﬁcantly improved using extensive sampling from the cheap function (yc).
Multi-ﬁdelity optimization
Proc. R. Soc. A 
available. For the data in ﬁgure 1, ^rZ1:87 (close to the true value of 2), but for
small samples of ye the MLE may be misleading. The slight deviations of the data in
ﬁgure 3 from ^rZ1=A (shown as a dashed line), the most signiﬁcant of which is at
1/AZ6, are where our GA search has not found the true MLE.
Recall that dZyeKrycðXeÞ (equation (3.7)) and hence, with ^rzye=yc, d
represents the difference in trends between the cheap and the expensive data.
Thus, for our one-variable example, when B, CZ0, ^md; ^s2
d/0 for all values of A,
if ^r is estimated accurately. Figure 4 shows how ^s2
d varies for B 2 fK10; 10g and
we see that as B/0 and therefore d/0, ^s2
d also approaches 0. Note that ^qe and
^pe will not be affected since the correlation in equation (2.2) is unaffected by the
scaling of the objective data (it is, however, affected by the scaling of X).
Our choice of cheap function for the above example is somewhat contrived,
but this has allowed us to show that the co-kriging model and its parameters are
behaving as we would expect. For our test function, the correction process Zd($)
is linear. Co-kriging will work effectively for more complex correction processes
with the proviso that Zd($) must be simpler to model than Ze($). In §6 we
demonstrate the beneﬁts of co-kriging using an engineering design problem.
4. Sampling plans
In §3 we have shown how to build the co-kriging model based on a set of sample
data. We now consider how to choose the points Xc and Xe to give us the best
prediction ^ye.
Figure 3. This plot of ^r versus 1/A shows that the MLE for r is a scaling factor between Zc($) and
Ze($), following the formulation in equation (3.1). There is a singularity at 1/AZ0, hence we have
used 1/AZ0.01 at this point.
A. I. J. Forrester et al.
Proc. R. Soc. A 
The results of experiments, whether physical or computational, are corrupted by
experimental error, that is, they deviate from the ‘true’ result. This can be due to
human error, systematic deviations (due to a ﬂaw or an inevitable physical or
numerical limitation of the experiment) and, only in physical experiments, random
error (usually linked to the limited accuracy of the instruments). When deciding on
the sampling plan, that is, when choosing X, there is nothing we can do about the
ﬁrst two error components and, while physical sampling plans often include
replicates to reduce the random error, computer experiment plans are simply
designed to cover the parameter space reasonably uniformly. The goal is, of course,
to enable the model ﬁtted to these points to give an accurate global approximation
of the unknown objective function landscape.
The deﬁnition of ‘uniform coverage’ is by no means obvious and a substantial
body of literature exists on the subject—here we work with the optimality
criterion of Morris & Mitchell . According to this, the plan with the best
space-ﬁlling properties is that which maximizes the smallest distance between
any pair of points within the sample (several additional ‘tie-breaker’ criteria are
given in case of multiple optima). Additionally, we restrict our search to a class
of plans known as Latin Hypercubes , which are built to
ensure uniformly spread projections of all points on all axes (ﬁgure 5a shows a
random LH). We thus generate an initial Morris–Mitchell optimal LH plan Xc.
A more interesting question is how do we select an ne-element subset Xe of Xc,
where the expensive simulations are to be run? Again, we wish to cover the
parameter space evenly and hence turn to the Morris–Mitchell criterion, but this
time we are dealing with a limited, discrete parameter space and thus the problem
becomes a combinatorial one. Since selecting the subset that satisﬁes this is
an NP-complete problem and an exhaustive search would have to examine
ncCneZnc!=ne!ðncKneÞ! subsets (clearly infeasible for all but very moderate
cardinalities), here we use an exchange algorithm to select Xe .
Figure 4. Variance ^s2
d as the cheap function coefﬁcient B is altered. ^s2
d; reduces to 0 as the difference
between fe and fc can be modelled purely by the scaling parameter r.
Multi-ﬁdelity optimization
Proc. R. Soc. A 
We start from a randomly selected subset Xe and calculate the Morris–Mitchell
criterion. We then exchange the ﬁrst point xð1Þ
with each of the remaining points in
Xc\Xe and retain the exchange that gives the best Morris–Mitchell criterion. This
process is repeated for each remaining point xð2Þ
. A number of restarts from
different initial subsets can be employed to avoid local optima. Figure 5b shows a
Morris–Mitchell optimal LH with a subset chosen using this exchange algorithm.
A rule of thumb for the number of points that should be used in the sampling
plan is nZ10k. When using a particularly cheap analysis, nc may be rather
greater than this, allowing us to build a more accurate model, and if the
relationship between fc and fe is simple, ne may be somewhat fewer—the
advantage of the co-kriging method. We will return to this subject in §7.
5. Co-kriging based optimization
In order to conﬁrm and enhance the predictions of a surrogate model, it is usual
to update the model with new evaluations in promising areas. Co-kriging (and
kriging) treats the value of the function at x as if it were the realization of a
Gaussian random variable Y(x), with a probability density function
YeðxÞK^yeðxÞ
with mean given by the predictor, ^yeðxÞ (equation (3.11)) and variance, s2(x)
(equation (3.12)). This allows us to model our uncertainty about the predictions we
make. The most plausible value at x is ^yeðxÞ, with theprobability decreasing as Ye(x)
moves away from ^yeðxÞ. Since there is an uncertainty in the value of ^yeðxÞ we can
calculate the expectation of it being an improvement, I ZminfyegKYðxÞ, on the
best value calculated so far as
KNmaxfminfyegK YeðxÞ; 0gfðYeðxÞÞ dYe
ðminfyegK^yeðxÞÞF
minfyegK^yeðxÞ
minfyegK^yeðxÞ
Figure 5. Two 20-point sampling plans: (a) a random LH and (b) a Morris–Mitchell optimal LH (plus
symbol) with a 5-point subset found using the exchange algorithm (circle).
A. I. J. Forrester et al.
Proc. R. Soc. A 
where F($) and f($) are the normal cumulative distribution function and probability
density function, respectively. By maximizing E[I(x)] we can ﬁnd the best new point
at which to sample the design space. Note that E[I(x)]Z0 when s(x)Z0 so that there
is no expectation of improvement at a point that has already been sampled and
therefore no possibility of re-sampling, which is a necessary characteristic of an
updating criterion when using deterministic computer experiments, and guarantees
global convergence; given that there is no possibility of re-sampling, as the number of
updates based on the maximum E[I(x)] increases, the design space will become
densely populated and so the global optimum will be found .
(a ) Co-kriging regression
When using multi-ﬁdelity analyses, we can modify the interpolating co-kriging
formulation in §3 such that each analysis can be regressed appropriately to ﬁlter
any noise present in the data. Different levels of ﬁltering may be required for each
analysis. For example, data from an empirical low-ﬁdelity computer code may be
smooth and require no regression, but could be coupled to a discretized high-
ﬁdelity analysis that displays noise that must be ﬁltered using regression.
Conversely, a low-ﬁdelity model may be discretized on a much coarser mesh,
requiring a higher degree of regression than the high-ﬁdelity code. In our
regressing co-kriging formulation, we therefore employ two regression constants,
lc and le. These are added to the leading diagonal of the correlation matrices to
give the covariance matrix CCl,
cfJcðXc; XcÞ CI ðnc!ncÞlcg
cfJcðXc; XeÞ C
0ðncKne!neÞ
cfJcðXe; XcÞ
cfJcðXe; XeÞ CI ðne!neÞlcg
C 0ðne!ncKneÞI ðne!neÞ
dfJdðXe; XeÞ CI ðne!neÞleg
where I is an identity matrix and 0 a zero matrix. The values of lc and le are
found along with the other hyper-parameters by maximizing equations (3.3) and
(3.8). The regressing predictor is now
^ye;rðxðneC1ÞÞ Z ^m CcTðC ClÞK1ðy K1^mÞ;
(subscript r denotes regression) with an estimated mean-squared error
rðxÞ Z ^r2^s2
c^lc C ^s2
d^leKcTðC ClÞK1c:
Unlike equation (3.12), the regression error does not fall to 0 at sample points,
and therefore Locatelli’s proof of convergence is invalid because max{E[I(x)]}
can occur at a previously sampled point. This is satisfactory when fc and/or fe are
evaluated through physical experiments, since repeated experiments may be
required to reduce measurement error. If fc and fe are found using deterministic
computer experiments, re-sampling should be avoided and the convergence
properties of max{E[I(x)]} (or other error-based sampling strategies) can be
restored by modifying the estimated error to eliminate the error due to the noise
ﬁltering, since this is the error associated with the data rather than the quality of
the model. This is achieved by ﬁtting a kriging interpolation through regressed
data, re-interpolation.
Multi-ﬁdelity optimization
Proc. R. Soc. A 
(b ) Co-kriging re-interpolation
We now show the derivation of a co-kriging error estimate that reﬂects the
uncertainty in predicting the underlying trend of data, rather than the overall
uncertainty, which includes any error in noisy data. The derivation, although
naturally complicated by multiple sets of data and regression constants, follows
the same theme as for kriging re-interpolation . As
mentioned above, to eliminate the error in the noisy data, we need to calculate
the error of an interpolation through a regressed set of data. The column vectors
of regressed cheap and expensive data are found from equations (2.3) (with the
addition of the regression constant) and (5.2), and can be expressed as
^yc;r Z 1^mc CJcðJc C ^lcIÞK1ðyc K1^mcÞ
^ye;r Z 1ðne!1Þ^m Cfcðxð1Þ
e ÞT; .; cðxðneÞ
ÞTgTðC ClÞK1ðyK1ðn!1Þ^mÞ;
where the MLEs for the hyper-parameters are those found for the co-kriging model.
Note that only the s2 terms in equation (3.12) depend on y. We therefore need to
substitute ^yc;r into equation (3.5) and ^ye;r into equation (3.9). Beginning with ^s2
c;ri Z ðyc K1^mcÞðJc ClcIÞK1JcðJc ClcIÞK1ðyc K1^mcÞ
(subscript ri denotes re-interpolation). Note that if lcZ0, then ^s2
d;ri, we substitute ^ye;r and ^yc;r into equation (3.7) to give
dr Z 1ðne!1Þ^m Cfcðxð1Þ
e ÞT; .; cðxðneÞ
ÞTgTðC ClÞK1ðyK1ðn!1Þ^mÞ
K ^r 1^mc CJcðXeÞðJcðXeÞ ClcIÞK1ðycðXeÞ K1^mcÞ
This is substituted into equation (3.9) in place of d (again, note that if leZ0
d). The expressions for ^s2
c;ri and ^s2
d;ri can now be substituted
into equation (3.12) to ﬁnd the re-interpolation error estimate that reduces to 0
at Xe. With the errors due to noisy data removed, the estimated error of the
predicted smooth trend is typically very small when compared with an interpolating
model. This can lead to problems with ﬂoating-point underﬂow when calculating
E[I(x)]. A scheme for avoiding this problem is presented in appendix B.
6. Example problem
We will demonstrate the beneﬁts of co-kriging, and in particular regressing
co-kriging, through the optimization of a generic transonic civil aircraft wing.
The wing is deﬁned by 11 variables: area; aspect ratio; kink position; sweep;
inboard and outboard taper ratios; root, kink and tip thickness to chord ratio; tip
washout; and kink washout. Our cheap analysis is in the form of an empirical
drag estimation code, Tadpole . This returns a drag
value based on curve ﬁts to previously analysed wings in approximately 0.6 s.
Our ‘expensive’ code is the linearized potential method VSaero 
A. I. J. Forrester et al.
Proc. R. Soc. A 
with viscous coupling (approx. 2 min per drag evaluation). We aim to minimize
drag/dynamic pressure (D/q) for a ﬁxed lift (determined by a wing weight
calculated by Tadpole).
To allow visualization of the design landscape, we limit the search to the four
variables that have the most impact on drag: area, S; aspect ratio,
; sweep, L;
and inboard taper ratio, Tin ). The Tadpole and VSaero design landscapes are shown using
hierarchical axis plots in ﬁgures 6 and 7. With the fast empirical drag estimation
provided by Tadpole, we have been able to build a high-resolution plot from
114Z14 641 runs of the code in 2.3 hours, while using the slower physics-based
VSaero we have produced a low-resolution plot using 32!112Z1089 evaluations
in 34.5 hours.4 Each tile of the plots shows D/q for S2{150, 250}m2 and
2{6, 12} for a ﬁxed L and Tin. L and Tin vary from tile to tile, with the value
at the lower left corner of the tile representing the value for the entire tile. The
blank portions of ﬁgure 7 are regions where VSaero has failed to return a result
for unusual geometries that lead to extreme ﬂow regimes.5 It is seen that the two
codes produce results that follow the same general trend of lower D/q for higher
and Tin, but the global optimum of the VSaero landscape goes against this
general trend, with the lowest D/q at TinZ0.55. Zd will not, therefore, be a trivial
function as in our one-variable example but, since the cheap and expensive
landscapes have the same general trend, should be simpler than Ze.
For this problem, we start with an initial Morris–Mitchell sampling plan for
Xc of 100 points (note n cO10k), to which a kriging model is ﬁtted. A comparison
of 100 further Tadpole calculations with predictions from the kriging model
yields a high correlation coefﬁcient of 0.96, indicating that the cheap code is
approximated well. A subset of neZ20 points (note ne!10k) is selected (using
the exchange algorithm) from which to build the initial co-kriging model. Xc and
Xe are shown in ﬁgure 6. Despite the apparent sparseness of the Xe data, a good
initial co-kriging prediction of the VSaero landscape is obtained, with a
correlation coefﬁcient of 0.96 when compared with the 112!32 VSaero dataset.
We follow an iterative process of updating the co-kriging model with new VSaero
and Tadpole data at max{E[I(x)]} and retuning hyper-parameters until the
optimum is found. Since nc is large and the correlation with independent data is
high, we do not expect the Tadpole landscape to be altered unduly by updates.
The GA-based tuning of qc and lc requires a large number of computationally
intensive inversions of the large n c!nc covariance matrix. We therefore save
time by limiting our re-tuning of the cheap hyper-parameters qc and lc to every
10 updates, while we re-tune qe and le at each step.
The co-kriging method is compared with a max{E[I(x)]} search of a kriging
model built using only the VSaero data. A D/q that improves on the minimum
from the previously computed 32!112 set of data plus one standard deviation of
the noise exhibited by the VSaero results (found from 10 VSaero evaluations of the
same design with small perturbations), D/qZ2.57C0.027, is used as a stopping
4 Generating such plots would, of course, not be possible in most cases due to the high
computational cost. Here we have computed this large quantity of data for illustrative purposes.
5 In our co-kriging and kriging-based searches, we have directed the search away from regions of
failure by inputing penalized values of ^yeðxeÞCs2ðxeÞ when VSaero fails to return a result
 .
Multi-ﬁdelity optimization
Proc. R. Soc. A 
sweep (deg.)
inboard taper ratio
Figure 6. Tadpole calculated D/q with a typical sampling plan. Xc (Tadpole evaluations) are shown as
dots and are circled at locations of Xe (VSaero evaluations). Red circles, failed VSaero simulations.
sweep (deg.)
inboard taper ratio
Figure 7. VSaero calculated D/q with co-kriging (dots) and kriging (crosses) updates from a typical
search (the search that yielded values closest to the mean results in table 1).
A. I. J. Forrester et al.
Proc. R. Soc. A 
criterion in both cases, i.e. similar quality answers are achieved from both the
searches, allowing a direct comparison of the effort involved. We also limit the size
of the kriging and co-kriging models to a maximum of 200 VSaero calculations.
Beyond this, the time required for tuning the model becomes equivalent to the
time required to run VSaero. Results, averaged over ﬁve searches from sampling
plans produced using different random number seeds, are shown in table 1.
The co-kriging based search consistently outperforms the kriging-based
search; ﬁnding better optima for reduced numbers of VSaero evaluations and
with fewer failed simulations. The initial prediction of the kriging model is almost
as accurate as the co-kriging model (see correlation and RMSE in table 1), but
the greater coverage of data in the co-kriging model leads to a better selection of
successful update points in promising regions, as seen in ﬁgure 7, which shows the
distribution of update evaluations for a typical search. Moreover, the co-kriging
updates are concentrated in regions of good designs, while the kriging updates are
more widespread because, as there is a sparsity of data, there is high error and
therefore high expectations of improvement in many areas (i.e. there is an
emphasis on exploration over exploitation).
Only discrete values of L and Tin can be displayed in ﬁgure 7 and these design
variables are rounded to the nearest 28 and 0.03 respectively, for the purpose of
visualization. It should be borne in mind that there is, in fact, a distribution of
designs between tiles. The reader should also appreciate that, although the major
axis is much larger than the axes of the individual tiles, they both represent the
same variation across a variable. Thus, although there seems to be a wide
distribution of update points across L and Tin, for the co-kriging updates these
points represent tight clusters in regions of optimal feasible designs.
In §3 we discussed the use of two regression constants lc and ld in the
co-kriging formulation. The ﬁnal MLEs of these hyper-parameters for the search
in ﬁgure 7 were ^lcZ1:2!10K6 and ^ldZ6:5!10K3. These values agree with our
intuition that data from an empirical code (Tadpole) will be smooth and require
little regression, i.e. a very small ^lc, and data from a dicretized physics-based
code (VSaero) will be noisy and require smoothing, i.e. a larger ^ld.
7. Discussion
Our results demonstrate that correlating analyses at multiple levels of ﬁdelity can
enhance the accuracy of a surrogate model of the highest level of analysis. This
correlated model can be used to ﬁnd optimal solutions more quickly. We have
Table 1. Performance comparison for the four-variable transonic wing problem. (Averaged over
ﬁve searches.)
initial model
best D/q (m2)
function evaluations
succeeded VSaero
failed VSaero
co-kriging
Multi-ﬁdelity optimization
Proc. R. Soc. A 
presented a global optimization strategy using a co-kriging based method that allows
for varying levels of noise ﬁltering across multi-ﬁdelity analyses and which converges
towards the global optimum using expected improvement maximization.
We have shown a co-kriging based optimization using two levels of CFD code
ﬁdelity. The methods are extendable to multiple levels and there are many other
examples of analyses that may be coupled. Here, both codes are predicting the same
quantity (drag) but, as long as there is a useful correlation, the analyses may be
related to completely different quantities as is the case with the work of Hevesi et al.
 , which correlated precipitation with elevation: two different but obviously
correlatedquantities. Oftenonlyoneformofanalysisis available,although this may
come in varying levels of ﬁdelity. For example, Kennedy & O’ Hagan used
different ﬁnite-element mesh resolutions to produce cheap and expensive results.
Forrester et al. showed that partially converged CFD simulations correlate
well with their converged counterparts. The convergence could be stopped at any
number of points, thus providing many levels of ﬁdelity.
In §4 we discussed the choice of n c and ne based on using more or fewer points
than an nZ10k rule of thumb. In the example problem, we checked that the model
built using our choices of nc and ne was accurate by assessing the correlation with a
second set of data. Naturally the number of points required is problem dependent
and additional validation data may be too expensive to compute. A possible way
to reduce computational effort in situations when the cost of fc is considerable is to
start with a very low n c and add data at points where the estimated error is
maximized until an accurate cheap model is obtained. Model accuracy may be
assessed using a leave-one-out cross-validation procedure. This method will
produce a space-ﬁlling design with nc controlled by the desired model accuracy.
Maximum expected improvement updates could then proceed starting from an
initial sample as small as neZ2, though it may be advantageous to follow a
maximum error update strategy until an accurate model is produced.
In our example problem we limited ourselves to four variables. Increasing the
number of variables naturally results in a more difﬁcult optimization problem,
requiring larger initial sampling plans and more update evaluations, and so the
beneﬁts of the co-kriging method are likely to increase. However, since we have noted
that co-kriging relies on the relationship between analyses being simpler to model
than the analyses themselves, we can assume that for additional variables which
have little impact on the design objective the co-kriging model will not offer a
signiﬁcant speed-up. Thus, while we expect co-kriging to perform well in high
dimensions, we should still try to isolate the most important variables, either
through prior knowledge or through the analysis of elementary effects .
We limited the total sample size (including updates) to neZ200 in the example
problem due to the expense of tuning the hyper-parameters for large quantities of
data overtaking the expense of the CFD simulations. This effectively puts an
upper limit on the number of variables that can be considered, since more
variables require more points. However, in such cases where fe is quick to
evaluate when compared with the hyper-parameter tuning we can choose to
simplify the correlation, and hence the tuning time, by using the same qc,j and
qe,j across all dimensions. The extension of surrogate-based techniques to higher
dimensions is a particularly active area of research and further developments in
this area would be welcome.
A. I. J. Forrester et al.
Proc. R. Soc. A 
This work has been supported by EPSRC grant code GR/T19209/01.
Appendix A
To derive the co-kriging predictor, we follow a method similar to that for ordinary
kriging. The basis of this method is that we wish our prediction of a new expensive
point to be consistent with the observed data and the MLEs for the hyperparameters. We therefore augment the observed data with a predicted value and
maximize the likelihood of this augmented dataset by varying our prediction while
keeping the hyper-parameters ﬁxed. This gives us an MLE ^ye xðneC1Þ
e xðneC1ÞTgT
yðneC1ÞgT, with covariance matrix ~C given by
cJcðXc;XcÞ
cJcðXc;XeÞ
cjcðXc;xðneC1ÞÞ
cJcðXe;XcÞ
cJcðXe;XeÞC^s2
dJdðXe;XeÞ
cjc Xe;xðneC1Þ
djd Xe;xðneC1Þ
cjcðXc;xðneC1ÞÞT r^s2
cjcðXe;xðneC1ÞÞT C^s2
djdðXe;xðneC1ÞÞT
which, deﬁning c as a column vector of the covariance of X and xðneC1Þ, can be
expressed as
In equations (3.3) and (3.8) it is seen that only the last term of the ln-likelihood
contains the sample data, and hence to ﬁnd an MLE ^yeðxðneC1ÞÞ we need to
2ð~y K1mÞT ~C
K1ð~y K1mÞ;
which may be expressed as
^yeðxðneC1ÞÞK ^m
^yeðxðneC1ÞÞK ^m
The inverse of the augmented covariance matrix
K1 is found using the
partitioned inverse formula 
C K1CC K1cðr2^s2
dKcTC K1cÞK1cTC K1 KC K1cðr2^s2
dKcTC K1cÞK1
dKcTC K1cÞK1cTC K1
dKcTC K1cÞK1
Substituting (A 3) into (A 2) and ignoring terms without ^yeðxðneC1ÞÞ we obtain
dKcTC K1cÞ
ð^yeðxðneC1ÞÞK ^mÞ2
cTC K1ðy K1^mÞ
dKcTC K1cÞ
ð^yeðxðneC1ÞÞK ^mÞ:
Multi-ﬁdelity optimization
Proc. R. Soc. A 
This expression is maximized by taking the derivative with respect to ^ye xðneC1Þ
and setting it to 0,
ð^yeðxðneC1ÞÞK ^mÞC
cTC K1ðy K1^mÞ
dKcTC K1cÞ
Solving for ^yeðxðneC1ÞÞ now gives
^yeðxðneC1ÞÞZ ^mCcTC K1ðy K1^mÞ:
For the estimated mean-squared error in this prediction, we again turn to
Jones for our method of derivation. It is argued that our MLE ^ye xðneC1Þ
is more accurate if the likelihood has a deﬁnite maximum, i.e. a high curvature.
Conversely, if different values of ^ye xðneC1Þ
have similar likelihoods the MLE is
less accurate. The error in the predictor is therefore inversely related to the
curvature (the second derivative with respect to ^ye xðneC1Þ
of the augmented
likelihood. We have already found the ﬁrst derivative in equation (A 5) and
taking the derivative and inverse of this equation we obtain
s2ðxÞzr2^s2
dKcTC K1c:
This equation is not precisely the classically derived formula, which includes an
extra term ð1K1TC K1cÞ2=cTC K1c. However, this extra term is so small that it
can safely be neglected.
Appendix B
The expected improvement (equation (5.1)) is typically calculated as
ðminfyegK^yeðxÞÞ 1
minfyegK^yeðxÞ
exp K ðminfyegK^yeðxÞÞ2
When using re-interpolation, s(x) is typically small and so erf($)/K1 and
exp($)/0. This often leads to ﬂoating-point underﬂow and E[I(x)]Z0. Using the
substitution aZðminfyegK^yeðxÞÞ=
sðxÞ, when a/K1, erf(a) can be expressed
using a Maclaurin series expansion and the ﬁrst term of (B 1) becomes
ðminfyegK^yeðxÞÞ
ðK1Þnð2n K1Þ!!
Note that exp(Ka2) appears in the second term in (B 1) and therefore E[I(x)] can
now be expressed as
ðminfyegK^yeðxÞÞ
ðK1Þnð2n K1Þ!!
aKð2nC1Þ C sðxÞ
!exp K ðminfyegK^yeðxÞÞ2
A. I. J. Forrester et al.
Proc. R. Soc. A 
We can now take natural logarithms and ln E[I(x)] can be searched by the
optimizer without problems with ﬂoating-point underﬂow.