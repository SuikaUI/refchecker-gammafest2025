Proxy Tasks and Subjective Measures Can Be Misleading in
Evaluating Explainable AI Systems
Zana Buc¸inca*
Harvard University
33 Oxford St
Cambridge, Massachusets 02138
 
Phoebe Lin*
Harvard University
33 Oxford St
Cambridge, Massachusets 02138
 
Krzysztof Z. Gajos
Harvard University
33 Oxford St
Cambridge, Massachusets 02138
 
Elena L. Glassman
Harvard University
33 Oxford St
Cambridge, Massachusets 02138
 
Explainable artiﬁcially intelligent (XAI) systems form part of sociotechnical systems, e.g., human+AI teams tasked with making
decisions. Yet, current XAI systems are rarely evaluated by measuring the performance of human+AI teams on actual decision-making
tasks. We conducted two online experiments and one in-person
think-aloud study to evaluate two currently common techniques for
evaluating XAI systems: (1) using proxy, artiﬁcial tasks such as how
well humans predict the AIﬁs decision from the given explanations,
and (2) using subjective measures of trust and preference as predictors of actual performance. Te results of our experiments demonstrate that evaluations with proxy tasks did not predict the results of
the evaluations with the actual decision-making tasks. Further, the
subjective measures on evaluations with actual decision-making
tasks did not predict the objective performance on those same tasks.
Our results suggest that by employing misleading evaluation methods, our ﬁeld may be inadvertently slowing its progress toward
developing human+AI teams that can reliably perform beter than
humans or AIs alone.
CCS CONCEPTS
•Human-centered computing →Interaction design; Empirical studies in interaction design;
explanations, artiﬁcial intelligence, trust
ACM Reference format:
Zana Buc¸inca*, Phoebe Lin*, Krzysztof Z. Gajos, and Elena L. Glassman.
2020. Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating
* equal contribution.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permited. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from .
IUI ’20, Cagliari, Italy
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
978-1-4503-7118-6/20/03...$15.00
DOI: 10.1145/3377325.3377498
Explainable AI Systems. In Proceedings of 25th International Conference on
Intelligent User Interfaces, Cagliari, Italy, March 17–20, 2020 (IUI ’20), 11 pages.
DOI: 10.1145/3377325.3377498
INTRODUCTION
Because people and AI-powered systems have complementary
strengths, many expected that human+AI teams would perform
beter on decision-making tasks than either people or AIs alone . However, there is mounting evidence that human+AI teams
ofen perform worse than AIs alone .
We hypothesize that this mismatch between our ﬁeld’s aspirations and the current reality can be atributed, in part, to several
pragmatic decisions we frequently make in our research practice.
Speciﬁcally, although our aspiration is formulated at the level of
sociotechnical systems , i.e., human+AI teams working together to
make complex decisions, we ofen make one of two possible critical
mistakes: (1) Rather than evaluating how well the human+AI team
performs together on a decision-making task, we evaluate by using
proxy tasks, how accurately a human can predict the decision or
decision boundaries of the AI . (2) We rely on subjective measures of trust and preference, e.g., , instead of
objective measures of performance. We consider each of these two
concerns in turn.
First, evaluations that use proxy tasks force study participants
to pay atention to the AI and the accompanying explanations—
something that they are unlikely to do when performing a realistic
decision-making task. Cognitive science provides compelling evidence that people treat cognition like any other form of labor 
and favor less demanding forms of cognition, i.e., heuristics over
analytical thinking, even in high stakes contexts like medical diagnosis . Terefore, we hypothesize that user performance and
preference on proxy tasks may not accurately predict their performance and preference on the actual decision-making tasks where
their cognitive focus is elsewhere and they can choose whether and
how much to atend to the AI.
Second, subjective measures such as trust and preference have
been embraced as the focal point for the evaluation of explainable
systems , but we hypothesize that subjective measures
may also be poor predictors of the ultimate performance of people
 
IUI ’20, March 17–20, 2020, Cagliari, Italy
Zana Buc¸inca*, Phoebe Lin*, Krzysztof Z. Gajos, and Elena L. Glassman
performing realistic decision-making tasks while supported by explainable AI-powered systems. Preference and trust are important
facets of explainable AI systems: they may predict users’ intent to
atend to the AI and its explanations in realistic tasks setings and
adhere to the systemﬁs recommendations. However, the goal of
explainable interfaces should be instilling in users the right amount
of trust . Tis remains a remarkable challenge, as on one
end of the trust spectrum users might over-rely on the system and
remain oblivious of its errors, whereas on the other end they might
exhibit self-reliance and ignore the systemﬁs correct recommendations. Furthermore, evaluating an AI’s decision, its explanation of
that decision, and incorporating that information into the decisionmaking process requires cognitive eﬀort and the existing evidence
suggests that preference does not predict performance on cognitive
tasks .
To evaluate these two hypotheses, we conducted two online
experiments and one in-person study of an AI-powered decision
support system for a nutrition-related decision-making task. In
one online study we used a proxy task, in which participants were
asked to predict the AI’s recommendations given the explanations
produced by the explainable AI system. In the second online study,
participants completed an actual decision-making task: actually
making decisions assisted by the same explainable AI system as in
the ﬁrst study. In both studies, we measured participants’ objective
performance and collected subjective measures of trust, preference,
mental demand, and understanding. In the in-person study, we used
a think-aloud method to gain insights into how people reason while
making decisions assisted by an explainable AI system. In each
study, we presented participants with two substantially distinct
explanation types eliciting either deductive or inductive reasoning.
Te results of these studies indicate that (1) subjective measures
from the proxy task do not generalize to the actual decision-making
task, and (2) when using actual decision-making tasks, subjective
results do not predict objective performance results. Speciﬁcally,
participants trusted and preferred inductive explanations in the
proxy task, whereas they trusted and preferred the deductive explanations in the actual task. Second, in the actual decision-making
task, participants recognized AI errors beter with inductive explanations, yet they preferred and trusted the deductive explanations
more. Te in-person think-aloud study revealed insights about
why participants preferred and trusted one explanation type over
another, but we found that by thinking aloud during an actual
decision-making task, participants may be induced to exert additional cognitive eﬀort, and behave diﬀerently than they would
during an actual decision-making task when they are, more realistically, not thinking aloud.
In summary, we show that the results of evaluating explainable AI systems using proxy tasks may not predict the results of
evaluations using actual decision-making tasks. Users also do not
necessarily perform beter with systems that they prefer and trust
more. To draw correct conclusions from empirical studies, explainable AI researchers should be wary of evaluation pitfalls, such as
proxy tasks and subjective measures. Tus, as we recognize that
explainable AI technology forms part of sociotechnical systems,
and as we increasingly use these technologies in high-stakes scenarios, our evaluation methodologies need to reliably demonstrate
how the entire sociotechnical systems (i.e., human+AI teams) will
perform on real tasks.
RELATED WORK
Decision-making and Decision Support
Decision-making is a fundamental cognitive process that allows
humans to choose one option or course of action from among a set
of alternatives . Since it is an undertaking that requires
cognitive eﬀort, people ofen employ mental shortcuts, or heuristics,
when making decisions . Tese heuristics save time and eﬀort,
and frequently lead to good outcomes, but in some situations they
result in cognitive biases that systematically lead to poor decisions
(see, e.g., ).
To help people make good decision reliably, computer-based Decision Support Systems (DSS) have been used across numerous disciplines (e.g., management , medicine , justice ). While
DSS have been around for a long time, they are now increasingly
being deployed because the recent advancements in AI enabled
these systems to achieve high accuracy. But since humans are the
ﬁnal arbiters in decisions made with DSS, the overall sociotechincal
system’s accuracy depends both on the system’s accuracy and on
the humans and their underlying cognitive processes. Research
shows that even when supported by a DSS, people are prone to
insert bias into the decision-making process .
One approach for mitigating cognitive biases in decision-making
is to use cognitive forcing strategies, which introduce self-awareness
and self-monitoring of decision-making . Although not universally eﬀective , these strategies have shown promising results as
they improve decision-making performance, both if the human is assisted or is not assisted by a DSS . To illustrate, Green &
Chen showed that across diﬀerent AI-assisted decision-making
treatments, humans performed best when they had to make the
preliminary decision on their own ﬁrst before being shown the
system recommendation (which forced them to engage analytically
with the system’s recommendation and explanation if their own
preliminary decision diﬀered from that oﬀered by the system). Even
though conceptual frameworks that consider cognitive processes
in decision-making with DSS have been proposed recently ,
further research is needed to thoroughly investigate how to incorporate DSS into human decision-making and the eﬀect of cognitive
processes while making system-assisted decisions.
Evaluating AI-Powered Decision Support
Motivated by the growing number of studies in interpretable and
explainable AI-powered decision support systems, researchers have
called for more rigorous evaluation of explainable systems . Notably, Doshi-Velez & Kim proposed a taxonomy for
evaluation of explainable AI systems, composed of the following
categories: application grounded evaluation (i.e., domain experts
evaluated on actual tasks), human grounded evaluation (i.e., lay
humans evaluated on simpliﬁed tasks) and functionally grounded
evaluation (i.e., no humans, proxy tasks). To put our work into context, our deﬁnition of the actual task falls into application grounded
Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems IUI ’20, March 17–20, 2020, Cagliari, Italy
evaluation, where people for whom the system is intended (i.e., not
necessarily experts) are evaluated on the intended task. Whereas,
the the proxy task is closer to human grounded evaluation but addresses both domain experts and lay people evaluated on simpliﬁed
tasks, such as the simulation of model’s prediction given an input
and an explanation.
Studies using actual tasks evaluate the performance of human
and the system, as a whole, on the decision-making task . In these studies, participants are told to focus on making
good decisions and it is up to them to decide whether and how to
use the AI’s assistance to accomplish the task. In contrast, studies
that use proxy tasks evaluate how well users are able to simulate
the model’s decisions or decision boundaries . In
such studies, participants are speciﬁcally instructed to pay atention
to the AI. Tese studies evaluate the human’s mental model of
the system when the human is actively atending to the system’s
predictions and explanations, but do not necessarily evaluate how
well the human is able to perform real decision-making tasks with
the system. For example, to identify which factors make a model
more interpretable, Lage et al. ask participants to simulate the
interpretable model’s predictions .
In addition to the evaluation task, the choice of evaluation metrics is a critical one for the correct evaluation of intelligent systems . In explainable AI literature, subjective measures, such
as user trust and experience, have been largely embraced as the
focal point for the evaluation of explainable systems .
Hoﬀman et al. proposed metrics for explainable systems that
are grounded in the subjective evaluation of a system (e.g., user
satisfaction, trust, and understanding). Tese may take the form
of questionnaires on atitude and conﬁdence in the system 
and helpfulness of the system . However, while these measures are informative, evidence suggests they do not necessarily
predict user’s performance with the system. For example, Green
& Chen discovered that self-reported measures could be misleading, since participant’s conﬁdence in their performance was
negatively associated with their actual performance. Similarly, Lai
& Tan found that humans cannot accurately estimate their
own performance. More closely related to our ﬁndings, Poursabzi-
Sangdeh et al. observed that even though participants were
signiﬁcantly more conﬁdent on the predictions of one model over
the other, their decisions did not reﬂect the stated conﬁdence. Furthermore, Lakkaraju & Bastani demonstrated that participants
trusted the same underlying biased model almost 10 times more
when they were presented with misleading explanations compared
to the truthful explanations that revealed the model’s bias. Tese
ﬁndings indicate that not only are subjective measures poor predictors of performance, but they can easily be manipulated and lead
users to adhere to biased or malicious systems.
EXPERIMENTS
We conducted experiments with two diﬀerent evaluation tasks and
explanation designs to test the following hypotheses:
H1: Results of widely accepted proxy tasks, where the user is asked
to explicitly engage with the explanations, may not predict the
results of realistic setings where the user’s focus is on the actual
decision-making task.
H2: Subjective measures, such as self-reported trust and preference
with respect to diﬀerent explanation designs, may not predict the
ultimate human+AI performance.
Proxy Task
Task Description. We designed the task around nutrition
because it is generally accessible and plausibly useful in explainable
AI applications for a general audience. Participants were shown a
series of 24 images of diﬀerent plates of food. Te ground truth of
the percent fat content was also shown to them as a fact. Participants were then asked: “What will the AI decide?” given that the
AI must decide “Is X% or more of the nutrients on this plate fat?”.
As illustrated in Figure 1, each image was accompanied by explanations generated by the simulated AI. Te participants chose which
decision they thought the AI would make given the explanations
and the ground truth.
We designed two types of explanations, eliciting either inductive or deductive reasoning. In inductive reasoning, one infers
general paters from speciﬁc observations. Tus, for the inductive
explanations, we created example-based explanations that required
participants to recognize the ingredients that contributed to fat
content and draw their own conclusion about the given image. As
shown in Figures 1a, the inductive explanations began with “Here
are examples of plates that the AI knows the fat content of and
categorizes as similar to the one above.” Participants then saw four
additional images of plates of food. In deductive reasoning, in contrast, one starts with general rules and reaches a conclusion with
respect to a speciﬁc situation. Tus, for the deductive explanations,
we provided the general rules that the simulated AI applied to generate its recommendations. For example, in Figure 1b, the deductive
explanation begins with “Here are ingredients the AI knows the fat
content of and recognized as main nutrients:” followed by a list of
ingredients.
We chose a within-subjects study design, where for one half of
the study session, participants saw inductive explanations and, for
the other half of the study session, they saw deductive explanations.
Te order in which the two types of explanations were seen was
counterbalanced. Each AI had an overall accuracy of 75%, which
meant that in 25% of the cases the simulated AI misclassiﬁed the
image or misrecognized ingredients (e.g., Figure 1b). Te order
of the speciﬁc food images was randomized, but all participants
encountered the AI errors at the same positions. We ﬁxed the errors
at questions 4, 7, 11, 16, 22 and 23, though which food the error
was associated to was randomized. We included the ground truth
of the fat content of plates of food, because the main aim of the
proxy task was to measure whether the user builds correct mental
models of the AI and not to assess the actual nutrition expertise of
the participant.
Procedure. Tis study was conducted online, using Amazon Mechanical Turk. Participants were ﬁrst presented with brief
information about the study and an informed consent form. Next,
participants completed the main part of the study, in which they
answered 24 nutrition-related questions, divided into two blocks of
12 questions. Tey saw inductive explanations in one block and the
deductive explanations in the other. Te order of explanations was
randomized across participants. Participants completed mid-study
IUI ’20, March 17–20, 2020, Cagliari, Italy
Zana Buc¸inca*, Phoebe Lin*, Krzysztof Z. Gajos, and Elena L. Glassman
Figure 1: Te proxy task. Illustration of the simulated AI system participants interacted with: (a) is an example of an inductive
explanation with appropriate examples. (b) is an example of a deductive explanation with misrecognized ingredients, where
the simulated AI misrecognized apples and beets as avocados and bacon.
and end-study questionnaires so that they would provide a separate
assessment for each of the two explanation types. Tey were also
asked to directly compare their experiences with the two simulated
AIs in a questionnaire at the end of the study.
Participants. We recruited 200 participants via Amazon
Mechanical Turk (AMT). Participation was limited to adults in the
US. Of the total 200 participants, 183 were retained for ﬁnal analyses,
while 17 were excluded based on their answers to two commonsense questions included in the questionnaires (i.e., What color is
the sky?). Te study lasted 7 minutes on average. Each worker was
paid 2 USD.
Design and Analysis. Tis was a within-subjects design.
Te within-subjects factor was explanation type — inductive or
deductive.
We collected the following measures:
• Performance: Percentage of correct predictions of AI’s
• Appropriateness: Participants responded to the statement
“Te AI based its decision on appropriate examples/ingredients.”
with either 0=No or 1=Yes (afer every question)
• Trust: Participants responded to the statement “I trust this
AI to assess the fat content of food.” on a 5-point Likert scale
from 1=Strongly disagree to 5=Strongly agree (at the end
of each block)
• Mental demand: Participants answered the question “How
mentally demanding was understanding how this AI makes
decisions?” on a 5-point Likert scale from 1=Very low to
5=Very high (every four questions)
• Comparison between the two explanation types: Participants were asked at the end of the study to choose one AI
over another on trust, preference, and mental demand.
We used repeated measures ANOVA for within-subjects analyses
and the binomial test for the comparison questions.
Actual Decision-making Task
Task description. Te actual decision-making task had
a similar set up to the proxy task. Participants were shown the
same series of 24 images of diﬀerent plates of food, but were asked
their own decision whether the percent fat content of nutrients
on the plate is higher than a certain percentage. As illustrated in
Figure 2, each image was accompanied by an answer recommended
by a simulated AI, and an explanation provided by that AI. We
introduced two more conditions to serve as baselines in the actual
decision-making task depicted in Figure 3.
Tere were three between-subjects conditions in this study: 1.
the no-AI baseline (where no recommendations or explanations
were provided), 2. the no-explanation baseline (where a recommendation was provided by a simulated AI, but no explanation
was given), and 3. the main condition in which both recommendations and explanations were provided. In this last condition, two
within-subjects sub-conditions were present: for one half of the
study participants saw inductive explanations and for the other
they saw deductive explanations. Te order in which the two types
of explanations were seen was counterbalanced. In the no-AI baseline, participants were not asked any of the questions relating to
the performance of the AI.
Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems IUI ’20, March 17–20, 2020, Cagliari, Italy
Figure 2: Te actual task. Illustration of the simulated AI system participants interacted with. (a) is an example of incorrect
recommendations with inductive explanations. Contrasting the query image with the explanations reveals that the simulated
AI misrecognized churros with chocolate as sweet potato fries with BBQ sauce. (b) is an example of correct recommendation
with deductive explanations.
Figure 3: Te baseline conditions. (a) no AI (b) no explanations
Te explanations in this task diﬀered only slightly from the
explanations in the proxy task, because they indicated the AI’s
recommendation. Inductive explanations started with: “Here are
examples of plates that the AI categorizes as similar to the one
above and do (not) have X% or more fat.” followed by four examples
of images. Similarly, deductive explanations stated: “Here are ingredients the AI recognized as main nutrients which do (not) make
up X% or more fat on this plate:” followed by a list of ingredients.
Procedure. Te procedure was the same as for the proxy
task. Te study was conducted online, using the Amazon Mechanical Turk. Participants were ﬁrst presented with a brief information
about the study and an informed consent form. Next, participants
completed the main part of the study, in which they answered 24
nutrition-related questions, divided into two blocks of 12 questions.
All participants also completed a questionnaire at the end of the
study, providing subjective assessments of the system they interacted with. Participants who were presented with AI-generated
recommendations accompanied by explanations also completed a
mid-study questionnaire (so that they would provide separate assessment for each of the two explanation types) and they were also
asked to directly compare their experiences with the two simulated
AIs at the end of the study.
Participants. We recruited 113 participants via Amazon
Mechanical Turk (AMT). Participation was limited to adults in
the US. Of the total 113 participants, 102 were retained for ﬁnal
analyses, while 11 were excluded based on their answers to two
common-sense questions included in the pre-activity and postactivity questionnaires (i.e., “What color is the sky?”). Te task
lasted 10 minutes on average. Each worker was paid 5 USD per
Design and Analysis. Tis was a mixed between- and
within-subjects design. As stated before, the three between-subjects
conditions were: 1. the no-AI baseline; 2. the no-explanation baseline, in which the AI-generated recommendations were provided
but no explanations; 3. the main condition, in which both the AIgenerated recommendations and explanations were provided. Te
IUI ’20, March 17–20, 2020, Cagliari, Italy
Zana Buc¸inca*, Phoebe Lin*, Krzysztof Z. Gajos, and Elena L. Glassman
within-subjects factor was explanation type (inductive or deductive) and it was applied only for participants who were presented
with AI-generated recommendations with explanations.
We collected the following measures:
• Performance: Percentage of correct answers (overall for
each AI, and speciﬁcally for questions when AI presented
incorrect explanations)
• Understanding: Participants responded to the statement “I
understand how the AI made this recommendation.” on a 5point Likert scale from 1=Strongly disagree to 5=Strongly
agree (afer every question)
• Trust: Participants responded to the statement “I trust this
AI to assess the fat content of food.” on a 5-point Likert scale
from 1=Strongly disagree to 5=Strongly agree (every four
questions)
• Helpfulness: Participants responded to the statement “Tis
AI helped me assess the percent fat content.” on a 5-point
Likert scale from 1=Strongly disagree to 5=Strongly agree
(at the end of each block)
• Comparison between the two explanation types: Participants were asked at the end of the study to choose one
AI over another on trust, preference, understanding and
helpfulness.
We used analysis of variance (ANOVA) for between-subjects
analyses and repeated measures ANOVA for within-subjects analyses. We used the binomial test for the comparison questions.
Proxy Task Results
Te explanation type had a signiﬁcant eﬀect on participants’ trust
and preference in the system. Participants trusted the AI more
when presented with inductive explanations (M = 3.55), rather
than deductive explanations (M = 3.40, F1,182 = 5.37,p = .02).
Asked to compare the two AIs, most of the participants stated they
trusted more the inductive AI (58%,p = .04). When asked the
hypothetical question: “If you were asked to evaluate fat content of
plates of food, which AI would you prefer to interact with more?”,
again most of the participants (62%) chose the inductive AI over
the deductive AI (p = .001).
Te inductive AI was also rated signiﬁcantly higher (M = 0.83)
than the deductive AI (M = 0.79) in terms of the appropriateness
of examples (ingredients for the deductive condition) on which the
AI based its decision (F(1, 182) = 13.68,p = 0.0003). When the AI
presented incorrect examples/ingredients, there was no signiﬁcant
diﬀerence among the inductive (M = 0.47) and deductive (M = 0.50)
conditions (F(1, 182) = 1.02,p = .31,n.s.).
We observed no signiﬁcant diﬀerence in overall performance
when participants were presented with inductive (M = 0.64) or
deductive explanations (M = 0.64, F(1, 182) = 0.0009,n.s.). When
either AI presented incorrect explanations, although the average
performance dropped for both inductive (M = 0.40) and deductive
(M = 0.41) conditions, there was also no signiﬁcant diﬀerence
among them (F(1, 182) = .03,n.s.).
In terms of mental demand, there was a signiﬁcant eﬀect of the
explanation type. Participants rated the deductive AI (M = 2.94)
as more mentally demanding than the inductive AI (M = 2.79,
F(1, 182) = 7.75,p = .0006). Te eﬀect was noticed also when
they were asked: “Which AI required more thinking while choosing
which decision it would make?”, with 61% of participants choosing
deductive over inductive (p = .005).
Actual Decision-making Task Results
18 participants were randomized into the no-AI condition, 19 into
the AI with no explanation condition, and 65 were presented with
AI recommendations supported by explanations.
We observed a signiﬁcant main eﬀect of the presence of explanations on participants’ trust in the AI’s ability to assess the fat content
of food. Participants who saw either kind of explanation, trusted the
AI more (M = 3.56) than those who received AI recommendations,
but no explanations (M = 3.17, F1,483 = 11.28,p = .0008). Further, there was a signiﬁcant main eﬀect of the explanation type on
participants’ trust: participants trusted the AI when they received
deductive explanations more (M = 3.68) than when they received
inductive explanations (M = 3.44, F1,64 = 5.96,p = .01). When
asked which of the two AIs they trusted more, most participants
(65%) said that they trusted the AI that provided deductive explanations more than the one that provided inductive explanations
(p = .02).
Participants also found the AI signiﬁcantly more helpful when
explanations were present (M = 3.78) than when no explanations
were oﬀered (M = 3.26, F1,147 = 4.88,p = .03). Further, participants reported that they found deductive explanations more helpful
(M = 3.92) than inductive ones (M = 3.65) and this diﬀerence was
marginally signiﬁcant (F1,64 = 3.66,p = .06). When asked which of
the two AIs they found more helpful, most participants (68%) chose
the AI that provided deductive explanations (p = .006).
Participants also reported that they understood how the AI made
its recommendations beter when explanations were present (M =
3.84) than when no explanations were provided . Tere was no diﬀerence in the perceived level of
understanding between the two explanation types (F1,64 = 0.44,p =
Asked about their overall preference, most participants (63%)
preferred the AI that provided deductive explanations over the AI
that provided inductive explanations (p = .05).
In terms of actual performance on the task, participants who
received AI recommendations (with or without explanations) provided a signiﬁcantly larger fraction of accurate answers (M = 0.72)
than those who did not receive AI recommendations (M = 0.46,
F1,2446 = 118.07,p < .0001). Explanations further improved overall performance: participants who saw explanations of AI recommendations had a signiﬁcantly higher proportion of correct answers (M = 0.74) than participants who did not receive explanations of AI recommendations 
(depicted in Figure 4a). Tere was no signiﬁcant diﬀerence between the two explanation types in terms of overall performance
(F1,64 = 0.44,n.s.). However, we observed a signiﬁcant interaction
between explanation type and the correctness of AI recommendations . When the AI made correct
recommendations, participants performed similarly when they
saw inductive (M = .78) and deductive (M = .81) explanations
(F1,64 = 1.13,n.s.). When the AI made incorrect recommendations,
Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems IUI ’20, March 17–20, 2020, Cagliari, Italy
performance
no explanations
with explanations
performance
non-erroneous
Figure 4: Performance in the actual decision-making task. (a) depicts the mean of performance among no-AI, no-Explanations
and with-Explanations (overall) conditions. (b) depicts the mean of performance among inductive and deductive conditions,
when the AI recommendation is correct and erroneous. Error bars indicate one standard error.
however, participants were signiﬁcantly more accurate when they
saw inductive (M = 0.63) than deductive (M = 0.48) explanation
(F1,64 = 7.02,p = .01) (depicted in Figure 4b).
To ensure the results of our studies were not random, we replicated both experiments with almost identical setup and obtained
the same main results (in terms of signiﬁcance) reported in this
QUALITATIVE STUDY
Trough the qualitative study, we explored the user reasoning and
sought to gain insight into the discrepancy between subjective
measures and performance. We asked participants to think aloud
during an in-person study in order to understand how and why
people perceive AI the way they do, in addition to what factors go
into making decisions when assisted by an AI.
Te same task design was used in this study as in the actual decisionmaking task, except that all participants were presented with the
main condition (where both recommendations and explanations
were provided). As in the actual decision-making task, each participant saw both inductive and deductive explanations.
Upon arriving to the lab, participants were presented with an informed consent form, including agreeing to being screen- and audiorecorded, and instructions on the task. Aferwards, the steps in
this study were similar to those in the actual decision-making task,
except that we added the think-aloud method : as participants
completed the task, they were asked to verbalize their thought
process as they made each decision. At the end of the task, there
was a semi-structured interview, during which participants brieﬂy
discussed how they believed the two AIs were making their recommendations and why they did or did not trust them. Participants
also discussed if and why they preferred one AI over the other.
Participants
We recruited 11 participants via community-wide emailing lists (8
female, 3 male, age range 23–29, M = 24.86, SD = 2.11). Participants
were primarily graduate students with backgrounds from design,
biomedical engineering, and education. Participants had varying
levels of experience with AI and machine learning, ranging from
0–5 years of experience.
Design and Analysis
We transcribed the think-aloud comments and the post-task interviews. Transcripts were coded and analyzed for paterns using an
inductive approach . We focused on comments about (1) how
the AI made its recommendations; (2) trust in the AI; (3) erroneous
recommendations; (4) why people preferred one explanation type
over the other. From a careful reading of the transcripts, we discuss
some of the themes and trends that emerged from the data.
Preference of one explanation type over another. Eight out
of the 11 participants preferred the inductive explanations. Participants who preferred inductive explanations perceived the four
images as data. One participant stated that “Because [the AI] showed
similar pictures, I knew that it had data backing it up” (P3). On
the other hand, participants who preferred deductive explanations
perceived the listing of ingredients to be reliable, and that “if the AI
recognized that it’s steak, then I would think, Oh the AI knows more
IUI ’20, March 17–20, 2020, Cagliari, Italy
Zana Buc¸inca*, Phoebe Lin*, Krzysztof Z. Gajos, and Elena L. Glassman
Figure 5: Subjective evaluations in terms of trust and preference of the two AIs. Red and blue depict the percent of
participants that chose inductive and deductive AI, respectively. (a) proxy task (b) actual decision-making task.
about steak fat than I do, so I’m going to trust that since it identiﬁed
the object correctly.” (P6).
In our observations, we found that the way participants used
the explanations was diﬀerent depending on the explanation type.
With inductive explanations, one participant ofen ﬁrst made their
own judgement before looking at the recommendation, and then
used the recommendation to conﬁrm their own judgement. In
a cake example, one participant said, “So I feel it probably does
have more than 30% because it’s cake, and that’s cream cheese. But
these are all similar to that, and the AI also says that it does have
more than 30% fat, so I agree” (P2). With deductive explanations,
participants evaluated the explanations and recommendation more
before making any decision. In the same cake example, a diﬀerent
participant said, “Tere are [the AI recognizes] nuts, cream cheese,
and cake. Tat seems to make sense. Nuts are high in fat, so is dairy,
so I agree with that.” (P6).
Cognitive Demand. At the end of the study, participants were
asked which AI was easier to understand. Ten out of 11 participants felt the inductive explanations were easier to understand
than the deductive explanations. Several participants stated that
the deductive explanations forced them to think more, and that
generally they spent more time making a decision with deductive
explanations. One participant said, for example, “I feel like with this
one I have to think a bit more and and rely on my own experiences
with food to see or understand to gauge what’s faty.” (P2).
Errors and Over-reliance. Nine out of 11 participants claimed
to trust the inductive explanations more. We intentionally introduced erroneous recommendations because we expected participants to utilize them to calibrate their mental model of the AI.
When participants understood the error and believed the error was
reasonable for an AI to make, they expressed less distrust in subsequent questions. However, when participants perceived the error
to be inconsistent with other errors, their trust in subsequent recommendations was hurt much more. For example, one participant
stated, “I think the AI makes the recommendation based on shape
and color. But in some other dessert examples, it was able to identify
the dessert as a dessert. So I wasn’t sure why it was so diﬃcult to
understand this particular item” (P5).
We found that there was also some observable correlation between explanation type and trust. Many participants claimed it
was easier to identify errors from the inductive explanations, yet
agreed with erroneous recommendations from inductive explanations more. In some of those instances, participants either did not
realize the main food image was diﬀerent from the other four or felt
the main food image was similar enough though not exact. Lastly,
one participant stated the inductive explanations were easier to
understand because “you can visually see exactly why it would come
to its decision,”, but for deductive explanations “you can see what
it’s detecting but not why” (P8), and yet this participant also stated
that the deductive explanations seemed more trustworthy.
Impact of the Tink-Aloud method on participant behavior. In this study, we asked participants to perform the actual
decision-making task and we expected to observe similar results to
those obtained in the previous experiment when using the actual
tasks. Yet, in this study, 8 out of the 11 participants preferred the inductive explanations and 10 out of 11 participants felt the inductive
explanations were easier to understand than the deductive explanations. Tese results are comparable to the results we obtained in
the previous experiment when we used the proxy task rather than
the actual task.
We believe that the use of the think-aloud method may have
impacted participants’ behavior in this study. Speciﬁcally, because
participants were instructed to verbalize their thoughts, they were
more likely to engage in analytical thinking when considering
the AI recommendations and explanations than they were in the
previous experiment with the actual tasks, where their focus was
primarily on making decisions.
It is possible that while the think-aloud method is part of standard research practice for evaluating interfaces, it is itself a form
of cognitive forcing intervention , which impacts how people
perform on cognitively-demanding tasks such as interacting with
an explainable AI system on decision-making tasks. Te act of
talking about the explanations led participants to devote more of
Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems IUI ’20, March 17–20, 2020, Cagliari, Italy
their atention and cognition to the explanations, and thus made
them behave more similarly to participants in working with the
proxy task rather than those working with the actual task.
DISCUSSION
In this study, we investigated two hypotheses regarding the evaluation of AI-powered explainable systems:
• H1: Results of widely accepted proxy tasks, where the user
is asked to explicitly engage with the explanations, may
not predict the results of realistic setings where the user’s
focus is on the actual decision-making task.
• H2: Subjective measures, such as self-reported trust and
preference with respect to diﬀerent explanation designs,
may not predict the ultimate human+AI performance.
We examined these hypotheses in the context of a nutrition-related
decision-making task, by designing two distinct evaluation tasks
and two distinct explanation designs. Te ﬁrst task was a proxy
task, where the users had to simulate the AI’s decision by examining the explanations. Te second task was the more realistic,
actual decision-making task, where the user had to make their
own decisions about the nutritional content of meals assisted by
AI-generated recommendations and explanations. Each of the tasks
had two parts, where users interacted with substantially diﬀerent
explanation styles—inductive and deductive.
In the experiment with the proxy task, participants preferred and
trusted the AI that used inductive explanations signiﬁcantly more.
Tey also reported that the AI that used inductive explanations
based its decision on more accurate examples on average than the
AI that used deductive explanations. When asked “If you were asked
to evaluate fat content of plates of food, which AI would you prefer to
interact with more?”), the majority of participants chose the AI that
provided inductive explanations.
In contrast with the proxy task experiment, in the experiment
with the actual decision-making task, participants rated the AI
with deductive explanations as their preferred AI, and viewed it as
more trustworthy and more helpful compared to the AI that used
inductive explanations.
Te contrast in terms of performance measures was less pronounced. When atempting proxy tasks, participants demonstrated
nearly identical accuracy regardless of explanation type. However, when atempting actual decision-making tasks and the AI
provided an incorrect recommendation, participants ignored that
incorrect recommendation and provided the correct answer signiﬁcantly more ofen when they had access to inductive, not deductive,
explanations for the AI’s recommendation.
Tese contradictory results produced by the two experiments
indicate that results of evaluations that use proxy tasks may not
correspond to results on actual tasks, thus supporting H1. Tis
may be because in the proxy task the users cannot complete the
task without engaging analytically with the explanations. Whereas,
in the actual decision-making task, the user’s primary goal is to
make the most accurate decisions about the nutritional content of
meals; she chooses whether and how deeply she engages with the
AI’s recommendations and explanations.
Tis ﬁnding has implications for the explainable AI community,
as there is a current trend to use proxy tasks to evaluate user mental
models of the AI-powered systems, with the implicit assumption
that the results will translate to the realistic setings where users
make decisions about an actual task while assisted by an AI.
We tested H2 on the actual decision-making task. Te results
show that participants preferred, trusted and found the AI with
deductive explanations more helpful than the AI that used inductive
explanations. Yet, they performed signiﬁcantly beter with the AI
that used inductive explanations when the AI made erroneous
recommendations. Terefore, H2 is also supported. Tis ﬁnding
suggests that the design decisions for explainable interfaces should
not be made by relying solely on user experience and subjective
measures. Subjective measures of trust and preference are, of course,
valuable and informative, but they should be used to complement
rather than replace performance measures.
Our research demonstrated that results from studies that use
proxy tasks may not predict results from studies that use realistic
tasks. Our results also demonstrated that user preference may not
predict their performance. However, we recognize that evaluating
novel AI advances through human subjects experiments that involve realistic tasks is expensive in terms of time and resources, and
may negatively impact the pace of innovation in the ﬁeld. Terefore,
future research needs to uncover why these diﬀerences exist so that
we can develop low burden evaluation techniques that correctly
predict the outcomes of deploying a system in a realistic seting.
We believe that the reason why explainable AI systems are sensitive to the diﬀerence between proxy task and actual task evaluation
designs is that diﬀerent AI explanation strategies require diﬀerent
kinds and amounts of cognition from the users (like our inductive
and deductive explanations). However, people are reluctant to exert
cognitive eﬀort unless they are motivated or forced to do
so. Tey also make substantially diﬀerent decisions depending on
whether they choose to exert cognitive eﬀort or not . In actual decision-making situations, people ofen choose not to engage
in eﬀortful analytical thinking, even in high-stakes situations like
medical diagnosis . Meanwhile, proxy tasks force participants
to explicitly pay atention to the behavior of the AI and the explanations produced. Tus, results observed when participants interact
with proxy tasks do not accurately predict people’s behavior in
many realistic setings. In our study, participants who interacted
with the proxy task felt that the deductive explanations required
signiﬁcantly more thinking than the inductive explanations. Terefore, in the proxy task where the participants were obliged to exert
cognitive eﬀort to evaluate the explanations, they said they preferred and trusted the less cognitively demanding explanations
more, the inductive explanations. In contrast, in the actual task the
participants could complete the task even without engaging with
the explanations. Tus, we suspect that in the deductive condition
participants perceived the explanations as too mentally demanding,
and chose to over-rely on the AI’s recommendation, just to avoid
cognitive eﬀort of examining those explanations. Tey also might
have perceived the AI that provided deductive explanations as more
competent because it required more thinking.
One implication of our analysis is that the eﬀectiveness of explainable AI systems can be substantially impacted by the design
of the interaction (rather than just the algorithms or explanations).
For example, a recent study showed that a simple cognitive forcing
strategy resulted in much higher accuracy of the ﬁnal decisions made by human+AI teams than any
strategy that did not involve cognitive forcing .
Inadvertently, we uncovered an additional potential pitfall for
evaluating explainable AI systems. As the results of our qualitative
study demonstrated, the use of the think-aloud method—a standard
technique for evaluating interactive systems—can also substantially
impact how participants allocate their mental eﬀort. Because participants were asked to think aloud, we suspect that they exerted
additional cognitive eﬀort to engage with the explanations and
analyze their reasoning behind their decisions.
Together, these results indicate that cognitive eﬀort is an important aspect of explanation design and its evaluation. Explanations
high in cognitive demand might be ignored by the users while simple explanations might not convey the appropriate amount of evidence that is needed to make informed decisions. At the same time,
traditional methods of probing users’ minds while using explainable
interfaces should also be re-evaluated. By taking into account the
cognitive eﬀort and cognitive processes that are employed during
the evaluation of the explanations, we might generate explainable
interfaces that optimize the performance of the sociotechnical (human+AI) system as a whole. Such interfaces would instill trust, and
make the user aware of the system’s errors.
CONCLUSION
To achieve the aspiration of human+AI teams that complement oneanother and perform beter than either the human or the AI alone,
researchers need to be cautious about their pragmatic decisions.
In this study, through online experiments and an in-person study,
we showed how several assumptions researchers make about the
evaluation of the explainable AI systems for decision-making tasks
could lead to misleading results.
First, choosing proxy tasks for the evaluation of explainable
AI systems shifs the user’s focus toward the AI, so the obtained
results might not correspond to results of the user completing the
actual decision-making task while assisted by the AI. In fact, our
results indicate that users trust and prefer one explanation design
(i.e. inductive) more in the proxy task, while they trust and prefer
the other explanation design (i.e. deductive) more in the actual
decision-making task.
Second, the subjective evaluation of explainable systems with
measures such as trust and preference may not correspond to the
ultimate user performance with the system. We found that people
trusted and preferred the AI with deductive explanations more, but
recognized AI errors beter with the inductive explanations.
Lastly, our results suggest that think-aloud studies may not convey how people make decisions with explainable systems in realistic
setings. Te results from the think-aloud in-person study, which
used the actual task design, aligned more with the results we obtained in the proxy task.
Tese ﬁndings suggest that to draw correct conclusions about
their experiments, explainable AI researchers should be wary of the
explainable systems’ evaluation pitfalls and design their evaluation
accordingly. Particularly, the correct and holistic evaluation of
explainable AI interfaces as sociotechnical systems is of paramount
importance, as they are increasingly being deployed in critical
decision-making domains with grave repercussions.
Acknowledgements. We would like to thank Tianyi Zhang and
Isaac Lage for helpful feedback.