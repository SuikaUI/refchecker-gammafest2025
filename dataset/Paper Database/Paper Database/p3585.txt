San Jose State University
San Jose State University
SJSU ScholarWorks
SJSU ScholarWorks
Master's Projects
Master's Theses and Graduate Research
Structural Entropy and Metamorphic Malware
Structural Entropy and Metamorphic Malware
Donabelle Baysa
San Jose State University
Follow this and additional works at: 
Part of the Computer Sciences Commons
Recommended Citation
Recommended Citation
Baysa, Donabelle, "Structural Entropy and Metamorphic Malware" . Master's Projects. 283.
DOI: 
 
This Master's Project is brought to you for free and open access by the Master's Theses and Graduate Research at
SJSU ScholarWorks. It has been accepted for inclusion in Master's Projects by an authorized administrator of SJSU
ScholarWorks. For more information, please contact .
Structural Entropy and Metamorphic Malware
Presented to
The Faculty of the Department of Computer Science
San Jose State University
In Partial Fulﬁllment
of the Requirements for the Degree
Master of Science
Donabelle Baysa
Donabelle Baysa
ALL RIGHTS RESERVED
The Designated Project Committee Approves the Project Titled
Structural Entropy and Metamorphic Malware
Donabelle Baysa
APPROVED FOR THE DEPARTMENTS OF COMPUTER SCIENCE
SAN JOSE STATE UNIVERSITY
Dr. Mark Stamp
Department of Computer Science
Dr. Robert Chun
Department of Computer Science
Dr. Richard Low
Department of Mathematics
Structural Entropy and Metamorphic Malware
by Donabelle Baysa
Metamorphic malware is capable of changing its internal structure without altering its functionality. A common signature is nonexistent in highly metamorphic
malware. Consequently, such malware may remain undetected even under emulation
and signature scanning combined.
In this project, we use the concept of structural entropy to analyze variations in
the complexity of data within a ﬁle. The process consists of two stages, namely, ﬁle
segmentation and sequence comparison. In the ﬁle segmentation stage, we use entropy
measurements and wavelet analysis to segment a ﬁle. The second stage measures the
similarity of ﬁles by computing the edit distance between sequence segments. We
apply this technique to the metamorphic detection problem and show that we can
obtain strong results in certain challenging cases.
ACKNOWLEDGMENTS
My sincere appreciation is due to my advisor, Dr. Mark Stamp, for his guidance
and encouragement throughout the project. I consider it an honor to have worked
with a professor who possesses true passion for teaching.
I would like to thank my committee members, Dr. Robert Chun and Dr. Richard
Low, for their contribution in the completion of this project.
I would like to express my earnest love and gratitude to my husband, Mark, for
his unconditional support and understanding through the duration of my studies. A
special recognition goes to my 2-year old daughter, Olivia, who keeps my work-life
balance in-check. Finally, I would like to thank my mom, Cynthia, for her continued
support and praises.
TABLE OF CONTENTS
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Malware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Viruses and worms . . . . . . . . . . . . . . . . . . . . . .
Obfuscation techniques . . . . . . . . . . . . . . . . . . . .
File similarity methods . . . . . . . . . . . . . . . . . . . . . . . .
HMM-based detection
. . . . . . . . . . . . . . . . . . . .
Similarity index . . . . . . . . . . . . . . . . . . . . . . . .
Opcode graph-based similarity . . . . . . . . . . . . . . . .
Structural entropy . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
File segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . .
Wavelet transform analysis . . . . . . . . . . . . . . . . . .
Segmentation using wavelet transform . . . . . . . . . . . .
Sequence comparison . . . . . . . . . . . . . . . . . . . . . . . . .
Levenshtein distance
. . . . . . . . . . . . . . . . . . . . .
Sequence alignment using Levenshtein distance . . . . . . .
Similarity calculation . . . . . . . . . . . . . . . . . . . . .
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Test data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Conclusion and Future Work
. . . . . . . . . . . . . . . . . . . . .
A MWOR Results
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B NGVCK Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
C ROC Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
LIST OF TABLES
An example of instruction transposition . . . . . . . . . . . . . . . . .
Edit matrix for strings “eleven” and “elevated”
. . . . . . . . . . . .
Cygwin ﬁles from ./cygutils
. . . . . . . . . . . . . . . . . . . . . . .
Linux system ﬁles
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
MWOR 0.5 similarity statistics
. . . . . . . . . . . . . . . . . . . . .
MWOR similarity statistics
. . . . . . . . . . . . . . . . . . . . . . .
AUC of NGVCK
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
LIST OF FIGURES
“Do-nothing” instructions . . . . . . . . . . . . . . . . . . . . . .
Generic Hidden Markov Model . . . . . . . . . . . . . . . . . . .
Similarity index 
. . . . . . . . . . . . . . . . . . . . . . . . . . .
Transform assembly instructions to weighted directed opcode graph 11
Basic executable ﬁle format
. . . . . . . . . . . . . . . . . . . . . . .
Wavelet and signal under investigation 
. . . . . . . . . . . . . . .
File segmentation process
. . . . . . . . . . . . . . . . . . . . . . . .
Entropy plot of a sample ﬁle . . . . . . . . . . . . . . . . . . . . . . .
Wavelet transform of a sample ﬁle . . . . . . . . . . . . . . . . . . . .
Penalty cost for the entropy diﬀerence between two segments . . . . .
Sequence comparison process . . . . . . . . . . . . . . . . . . . . . . .
G2 similarity
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
MWOR 0.5 similarity . . . . . . . . . . . . . . . . . . . . . . . . . . .
NGVCK (4 KB) similarity . . . . . . . . . . . . . . . . . . . . . . . .
NGVCK (8 KB) similarity . . . . . . . . . . . . . . . . . . . . . . . .
NGVCK (4 KB and 8 KB) similarity . . . . . . . . . . . . . . . . . .
NGVCK ROC curves . . . . . . . . . . . . . . . . . . . . . . . . . . .
AUC for NGVCK with various parameter values . . . . . . . . . . . .
File processing time . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Introduction
Metamorphism is a technique applied to computer programs in order to change
its internal structure while maintaining its functionality. It is popular among malware
writers for its eﬀectiveness at evading detection by traditional signature-based antivirus software.
As a result, well-written metamorphic viruses have the potential
to remain undetected. Therefore, static methods for measuring ﬁle similarity is of
Previous research projects aimed at metamorphic detection based on
static analysis of ﬁles have shown promising results against metamorphic malware,
including the highly metamorphic family of viruses produced by the NGVCK (Next
Generation Virus Creation Kit) generator.
This research project introduces a ﬁle similarity method based on the paper ,
which utilizes static analysis to measure the similarity of executable ﬁles. The technique uses the concept of structural entropy to analyze the complexity of a ﬁle’s data
order. This method consists of two stages, namely, ﬁle segmentation and sequence
comparison. In the ﬁle segmentation stage, we use entropy measure and wavelet analysis to segment a ﬁle. The second stage estimates the similarity of ﬁles by using the
edit distance between sequence segments. We apply this approach to identify whether
a given ﬁle belongs to a known metamorphic malware family.
The information in this report is presented as follows. Section 2 includes discussion of previous research projects related to ﬁle similarity measure. We review a
solution that uses Hidden Markov Models , a method based on graph analysis ,
and a similarity index technique. We also present background information on malware and brieﬂy introduce the concept of structural entropy, which is the basis of our
similarity measure. Section 3 describes the design of our method. In Section 4, we
show the results of our solution when applied to the metamorphic malware detection
problem. Finally, in Section 5 we convey our conclusions and propose direction for
future work.
Background
Development of methods for estimating ﬁle similarity is an important topic in
virus detection. Advanced anti-virus (AV) schemes such as emulation are often employed in conjunction with static signature-based detection to scan for more sophisticated viruses that employ obfuscation techniques such as encryption, polymorphism,
and metamorphism. However, the overhead (e.g., ﬁle storage spaces and resources)
and the complexity of these techniques pose challenges to AV vendors. Additionally,
as we will discuss in Section 2.1.1, metamorphic malware may remain undetected
even under the scrutiny of code emulation and signature-based detection. One solution to these problems is static analysis of ﬁles. It deﬁnes ﬁle characteristics needed
for comparison without examining its functionality, thereby eliminating the emulation process and in turn, reduces implementation diﬃculty. Also, it is purely based
on the analysis of diﬀerent areas of a ﬁle which makes it resilient to techniques used
by malware writers to protect their code.
First, we provide background information on malware including the methods it
uses to avoid detection. Then, we review other ﬁle similarity measures which were
applied to the metamorphic malware detection problem. Then we brieﬂy discuss the
concept of structural entropy and describe how we apply it to our similarity method.
Malware is a computer program designed to cause harm to computer systems.
It interrupts normal computer operation, destroys ﬁles by infection or deletion, steals
user information, and damages system resources such as the hard disk . It enters
the system without user consent.
Malware comes in various forms such as virus,
worm, trojan horse, and other programs created with malicious intent. In this paper,
we only consider viruses and worms.
Viruses and worms
A computer virus is a malware that executes itself by embedding its code to
another executable program. When the infected program executes, it, in turn, infects
other ﬁles, corrupting the host system. Thus, viruses are self-replicating programs
that cause destruction within the host machine .
In contrast to viruses, worms do not require other executable ﬁles to cause intrusion. That is, they are standalone alone programs. A worm spreads itself from one
system to another on the network . Worms not only have the ability to damage
its host system but also cause disruption to the computer networks.
Viruses and worms use a number of obfuscation techniques to avoid signaturebased detection. The aim is to make itself look diﬀerent at each replication, making
it diﬃcult for a scanner to ﬁnd commonalities between the variants.
To achieve
this, malware writers use methods such as encryption, polymorphism, and metamorphism . In the next sections, our discussion will focus on viruses. However, these
topics also apply to computer worms.
Obfuscation techniques
Encryption is the simplest form of code obfuscation. The virus body is encrypted
using a key. Given that a diﬀerent encryption key is used in each generation, the
virus variants will not share a common pattern necessary for signature scanning,
thereby evading detection. The virus also includes a decryptor module which decrypts
the virus body at execution. The decryptor is usually a simple, decrypted piece of
code that remain constant across replications . Therefore, the decryptor code is
susceptible to detection.
Polymorphism is a step up to encryption, which possesses vulnerability to detection. That is, an encrypted virus carries a detectable decryptor code. A polymorphic
virus not only uses encryption to hide its virus body, it also mutates its decryptor
so that each virus variant looks diﬀerent from one another . Therefore, no
common pattern exists that will allow signature scanners to use for identiﬁcation.
However, just as malware writers use sophisticated methods to evade detection, AV
software vendors develop other means to defeat them. In conjunction with signature
scanning, AV vendors use code emulation to allow potential viruses to execute in a
protected environment. Once the virus decrypts itself within this virtual environment,
it exposes its virus body, which stays constant, to detection.
Metamorphic virus goes even further and mutates its entire virus body, making
it resistant to emulation. A virus using metamorphism changes its internal structure
in each replication while maintaining its functionality . A highly metamorphic
virus does not necessarily need encryption since each generation will produce a structurally diﬀerent code and retrieval of a common signature will be highly unlikely .
Therefore, encryption adds no beneﬁt to the virus creation. As such, a well developed
metamorphic virus has the potential to remain undetected.
There are a number of techniques to create mutated virus programs. One simple method is to apply register swapping. In this approach, the code remains unchanged except for the registers. For example, ADD EDX,0088h can be converted to
ADD EAX,0088h, i.e., the register EDX is swapped for EAX. However, register swapping
can typically be exploited by a wildcard string .
Another prevalent method used in metamorphism is equivalent instruction substitution. An instruction or a set of instructions is exchanged for another yielding
the same result. For example, the sequence of instructions PUSH R1; MOV R1,R2 can
be substituted for PUSH R1; PUSH R2; POP R1, which results in the same functionality .
In some instances, the order of instructions can be rearranged without losing the
functionality if the instructions have no dependency with each other. This technique
is called instruction transposition; it enables viruses avoid signature-based detection
since the order of bytes deviates in each replication . A simple example of this
method is shown in Table 1.
Table 1: An example of instruction transposition
Original Instructions
Transposed Instructions
SUB R1, R2
SUB R3, R4
SUB R3, R4
SUB R1, R2
Subroutine permutation is another technique for manipulating the internal structure of a virus without changing the functionality. With n subroutines in a virus,
n! variants of the virus can be generated.
A virus with 10 subroutines, like the
Win32/Ghost virus, can easily generate 10! or 3, 628, 800 versions of itself . However, since the subroutines do not change, using this method alone can expose the
virus to detection by searching for common short string patterns .
Garbage instruction insertion is an eﬀective metamorphic technique. Garbage
instructions are those that are either executed without impacting the virus behavior,
or skipped all together. The former type of garbage instructions are often referred
as “do-nothing” code, and the latter are “dead-code” instructions. An example of a
“do-nothing” code sequence is shown in Figure 1.
Figure 1: “Do-nothing” instructions 
File similarity methods
In this section, we review previous research projects designed for estimating ﬁle
similarity between samples from a number of metamorphic family of viruses.
HMM-based detection
A hidden Markov model (HMM) represents a system having a Markov process in
which the states are non-observable . In other words, the process of transitioning
from one state to the next is dependent only on the current state and each state in
the transition sequence is hidden. What is visible, however, is the series of output
observed during the transition process .
Figure 2 shows a generic HMM.
Here, the Markov process consists of Xt hidden states which are “driven” by the
state transition probabilities A. Each hidden state uncovers some information, i.e.,
observation Ot, over a probability distribution B. Therefore, given a suﬃcient sample
of observations, we can produce a model that represent the data sample maximally.
Figure 2: Generic Hidden Markov Model 
We can then determine if another given set of data is related to the observed data
represented by the model. Generating a system that models a sample data set and
utilizing it to expose data relationships are two of the three fundamental problems
that can be solved using HMMs . We can also discover the unknown states of the
HMM given the model and a set of data. For more details on HMM, refer to .
The paper presents a detector based on hidden Markov models (HMM) and
shows superiority over popular commercial virus scanners on a set of test ﬁles. The
HMM method eﬀectively detects metamorphic viruses produced by various metamorphic generators including NGVCK, which generates highly morped versions of a given
virus. The technique involves training and detection phases. The training phase generates a hidden Markov model on a given ﬁle set of a metamorphic virus family (e.g.,
NGVCK). The model is trained on the assembly opcode sequences of the viruses.
The process starts by disassembling the virus executable ﬁles, producing assembly
opcodes, which are then concatenated into a long sequence of opcodes that is used
to build the model. The result is a model that represents the statistical proﬁle of
the virus family. In order to classify a given ﬁle as being malicious or benign, the
resulting HMM is used to measure the log likelihood of the virus ﬁles in the test set,
which belongs to the same virus family used in the training phase. The expectation is
that the model would give high likelihood scores to these ﬁles. The same calculation
is performed on another set consisting of normal ﬁles and virus ﬁles from another
virus family. The goal is to clearly set distinctions between “family viruses”, “nonfamily viruses”, and “normal ﬁles”. The results show that the HMM method is highly
eﬀective at detecting family viruses, overpowering some commercial virus scanners.
Similarity index
The same paper presents another similarity measure based on similarity
index. Although the technique is simpler, it is equally eﬀective at detecting metamorphic viruses. In fact, the experiment resulted in 100% detection rate and 0%
false positive rate. Similar to the HMM approach, this method analyses the opcode
sequences of ﬁles. However, here it directly compares the opcode sequences of two
ﬁles by matching all three consecutive opcode subsequences from each, in any order. For example, an opcode sequence of [add, sub, mov] from one ﬁle matches
the following sequences from another ﬁle: [sub, mov, add], [mov, add, sub], and
[sub, add, mov]. The matching subsequences are then plotted and adjusted, based
on a threshold, to cut down random matches. By reducing noise, the graph shows
a more accurate representation of the matching sequences. Figure 3 illustrates this
process. The similarity score is calculated based on the average of real matches. The
ﬁle being examined is categorized as “non-family” if it has no similarity to a randomly
chosen virus ﬁle from a known virus family. Otherwise, additional comparisons are
required between the chosen virus ﬁle and other ﬁles from the same virus family to
set a threshold. If the similarity score between the given ﬁle and the originally chosen
virus ﬁle is above the threshold, the ﬁle is deemed a “family” virus.
Figure 3: Similarity index 
Opcode graph-based similarity
In a more recent work, a technique developed using opcode graph outperforms the HMM-based detection under certain scenarios. This method takes an executable ﬁle, disassembles it, and extracts the opcode sequence. The sequence is then
transformed into a weighted directed graph. The graph is constructed as follows. The
nodes represent all the distinct opcodes. An edge is added from each opcode node
to all of its successor opcode nodes. The probability of the successor opcode node is
assigned as the weight of the corresponding edge. Figure 4 shows an example of a
weighted opcode graph of a sample assembly instructions.
To compare two ﬁles, the opcode graph is built for each ﬁle. The generated
opcode graphs are compared directly via a scoring function which takes the corresponding edge weights (probabilities of two opcodes or itself occurring in sequence)
from the two opcode diagrams and calculates a similarity score. This process is performed on pairs of virus ﬁles from a metamorphic virus family. The expectation is to
get scores with little to no variations from these family viruses. Similarly, the score
Figure 4: Transform assembly instructions to weighted directed opcode graph 
calculation is performed on ﬁle pairs involving one virus ﬁle from the same metamorphic family used in the earlier computation, and one benign ﬁle from a sample set of
normal ﬁles. A threshold is set based on these score calculations.
Finally, in order to detect a given ﬁle, its opcode graph is compared against
an opcode graph from a metamorphic virus family used in the above calculations.
Depending on where the score falls with respect to the threshold, it is classiﬁed as a
member of the metamorphic virus family or benign.
Structural entropy
Entropy is “a statistical measure of the disorder of a closed system” .
quantiﬁes uncertainty of systems where outcomes are not equally likely .
solution focuses on the order of code and data areas of a ﬁle, characterizing it by the
peculiarity of its byte sequences as well as its length. We use entropy measurement to
ﬁnd distinctions of bytes within the ﬁle. This technique is derived from the paper 
which produced eﬃcient and favorable experimental results on polymorphic malware
samples. In this project, we apply it to the metamorphic detection problem.
We use the concept of structural entropy to analyze the complexity of a ﬁle’s data
order. It consists of two stages, namely, ﬁle segmentation and sequence comparison.
In the ﬁle segmentation stage, we use entropy measurement and wavelet analysis to
segment a ﬁle. The second stage measures the similarity of ﬁles by using the edit
distance between sequence segments.
As previously cited, the similarity method we consider here is derived from the
paper . It is based upon static analysis of ﬁles using structural entropy to measure
similarity between two ﬁles. In contrast to , this solution examines executable
ﬁles without code disassembly. Instead of analyzing opcode sequences of ﬁles, we
scan the binaries directly and observe the distinctions of each byte with respect to
one another for diﬀerent areas within the ﬁles. In other words, we are only interested
in the structure, speciﬁcally, entropy and size characteristics of a ﬁle.
The proposed solution compares two given ﬁles and produces a similarity measure. It starts by splitting each ﬁle into sequence of segments of diﬀerent entropy
levels using entropy and wavelet analysis, followed by aligning the two sequences
by calculating the edit distance between the sequence segments. The result is the
similarity between the two ﬁles, expressed as a percentage.
File segmentation
The key in properly segmenting a ﬁle is to locate the areas where signiﬁcant
changes occur. In our method, we are looking directly into the sequence of data bytes
within an executable ﬁle. The standard structure of an executable includes information about its code and data fragments. Figure 5 shows a basic format of a Windows
portable executable ﬁle, organized into various sections . The format starts
with structures of diﬀerent headers followed by the code and data regions. Each of
these ﬁle fragments can be characterized by the nature of information it contains. For
instance, the header sections include pointers to other areas within the ﬁle, and the
Figure 5: Basic executable ﬁle format
text and data sections contain the code and initialized data (i.e., initialized global
and static variables), respectively .
The method we use here is based
on the notion that variants from the same malware family will share similar code
patterns and data structures in order to preserve its malicious intent. Therefore, we
can examine not only the similarity within the headers, but more importantly, the
distinctive byte sequences within the code and data sections. We can use entropy
and size to represent each of these byte patterns and use them as bases for our ﬁle
segmentation process. Consequently, our goal is to ﬁnd the borders where these areas
occur and use it to segment a ﬁle. We achieve this by using wavelet analysis on a ﬁle
represented as a series of entropy measure.
Wavelet transform analysis
Wavelet analysis is a process of transforming a signal (i.e., a data set) into a more
revealing and useful form. It uses wavelets, which are wavelike functions, to analyze
the raw data in diﬀerent locations and for diﬀerent wavelet scales . Figure 6
illustrates the signal and the analyzing wavelet. The transformation at a given scale
is determined based on the signal approximation and detail at the previous scale.
Figure 6: Wavelet and signal under investigation 
Scaling the wavelet smooths out the high frequency information present in the original
data. The transformed data is referred as wavelet transform, which represents the
relationship of the data and the analyzing wavelet. Mathematically, it is a convolution
of the signal and the wavelet function for a range of locations and scales .
Segmentation using wavelet transform
We use wavelet analysis to trace those areas in the ﬁle where signiﬁcant changes
The process is as follows.
First, we apply the sliding window method to
represent the ﬁle as a series of entropy measures Y = {yi : i = 1, ..., N}, where N
is the number of windows and yi is the entropy of each window.
The entropy is
calculated using Shannon’s formula 
p(j) log2 p(j),
where p(j) is the frequency of byte j within window i, and m is a number of distinct
bytes in the window. Second, the resulting entropy series Y is fed into wavelet analysis
using discrete wavelet transform
where a is a scaling parameter, b is a shifting parameter of the analyzing wavelet, yi
is the entropy of window i, N is the number of windows in the ﬁle, and ψHAAR is the
Haar wavelet function, which is deﬁned by
ψHAAR(t) =
0 ≤t < 1/2,
1/2 ≤t < 1,
t < 0, t ≥1.
The wavelet transform formula in (2) requires the scaling parameter a to be of
power of 2, i.e., an = 2n where n is the maximum scale index . For example, if
we want to transform the raw data to the maximum scale of 16, equation (2) will be
iterated four times for a = 2, 4, 8, and 16 and for diﬀerent locations, b, on the data.
The ﬁle segments are determined by the wavelet coeﬃcients at the maximum scale,
where boundaries of these segments are deﬁned by the local extrema based on a set
threshold. The ﬁle segmentation process is summarized in Figure 7.
Figure 7: File segmentation process
To illustrate our ﬁle segmentation method, consider the diagram in Figure 8,
which shows the entropy plot of a 4 KB size sample ﬁle. This entropy series is calculated using a window size and window slide size of 64 bytes and 32 bytes, respectively.
Figure 8: Entropy plot of a sample ﬁle
Using 32 bytes for the slide size ensures the entropy map size to be of power of 2,
which is 128 in this example. The wavelet transform of this sample ﬁle is shown in
Figure 9 with maximum scale of 16, i.e., a4 = 24. On the lower transformation scales,
Figure 9: Wavelet transform of a sample ﬁle
detail of changes in source data is evident, while insigniﬁcant changes are ignored at
the higher scales. Therefore, the segments are determined by the coeﬃcients at scale
index 4. Using a threshold, we can locate the segment borders by ﬁnding peaks that
are greater than the threshold in height.
Sequence comparison
Once each ﬁle has been transformed into a sequence of segments, we compare the
two sequences and determine the degree of similarity between them. The comparison
process is as follows. First, we align the two sequences by evaluating the diﬀerences of
their respective elements. We achieve this by using an algorithm based on the Levenshtein distance. The result is a measure of the diﬀerence between two sequences .
We then use this result to estimate the similarity between the two ﬁles.
In this section, we ﬁrst provide background information on the Levenshtein distance. Then we illustrate how it is used in our sequence alignment algorithm. Finally,
we discuss our method for evaluating the similarity measure.
Levenshtein distance
The Levenshtein distance, or edit distance, is a measure of the diﬀerence between
two sequences of data . It calculates the distance by tallying the minimum number
of edit operations required to transform the ﬁrst sequence into the other. The set
of edit operations are substitution, insertion, and deletion. Substitution replaces an
element from the ﬁrst sequence with an element in the second sequence, insertion
adds a element into the second sequence, and deletion removes an element from the
second sequence . Each required edit operation adds to the overall distance. In
other words, each edit translates into a penalty. Consequently, the more the edits,
the higher the diﬀerence is between two sequences.
Consider an example of comparing two sequences of characters where each substitution, insertion or deletion results to a penalty of 1. The number of edit operations
required to transform the string “eleven” to “elevated” is 3,
1. eleven →elevaen, insert ‘a’
2. elevaen →elevaten, insert ‘t’
3. elevaten →elevated, substitute ‘n’ for ‘d’.
Notice that the transformation cannot be completed in fewer than three edits, therefore the Levenshtein distance between these two strings is 3.
Equation 3 formulates the general deﬁnition of the Levenshtein distance D(i, j)
between sequences x[1 : m] and y[1 : n] where edit penalties are determined by the
cost function δ .
if i = 0 and j = 0
D(0, j −1) + δ(λ, yj)
if i = 0 and j > 0
D(i −1, 0) + δ(xi, λ)
if i > 0 and j = 0
D(i −1, j −1)
if xi = yj
D(i, j −1) + δ(λ, yj)
D(i −1, j) + δ(xi, λ)
D(i −1, j −1) + δ(xi, yj)
if xi ̸= yj.
Applying this deﬁnition to our previous example with δ = 1 (i.e., penalty is 1) for
each edit operation produces the edit matrix in Table 2. The last matrix element
represent the edit distance, which is 3 in our example.
Sequence alignment using Levenshtein distance
As discussed in the the previous section, the Levenshtein distance adds a penalty
of 1 on each edit operation applied on sequences of characters. In our method, we
Table 2: Edit matrix for strings “eleven” and “elevated”
are dealing with sequence elements that are characterized by size and entropy. Thus,
our cost function must penalize on the diﬀerence of two elements based on their sizes
and entropy values. As in , we choose the function for calculating the size penalty
costs(x, y) = |x.size −y.size|
x.size + y.size ,
where x and y are the two compared segments. If they are equal in size, the penalty
is 0. The maximum size penalty, in the case of absolute diﬀerence, is 1.
For evaluating the diﬀerence in entropy, we use the equivalent formula in 
based on a Sigmoid function
coste(x, y) =
1 + exp(−4 · |x.ent −y.ent| + 6.5) −0.001501,
where each segment is associated with an entropy value denoted by “ent”. Function (5) represents a sigmoid curve , as shown in Figure 10. The use of constants,
6.5 and 0.001501, in equation (5) forces the cost to equate to 0 when the entropy
diﬀerence of two segments is 0. As with the size, the maximum entropy penalty is 1.
Adding the size (4) and entropy (5) costs gives us the total penalty
cost(x, y) = costs(x, y) · PART SIZE + coste(x, y) · PART ENT,
Figure 10: Penalty cost for the entropy diﬀerence between two segments
where PART_SIZE and PART_ENT are used to allow the penalty fractions to be set
diﬀerently.
Now that we have determined a cost function (6), we apply it into our sequence
alignment algorithm based on Levenshtein distance, which builds a two-dimensional
edit array d using dynamic programming where each of its element is determined
by the preceding elements. At each step of ﬁlling in this array, we will use the cost
function (6), which penalizes based on the size and entropy diﬀerences, as well as the
logarithmic sizes of the compared segments. The resulting penalty for comparing two
sequences s1 and s2 is determined by the last element in the d matrix.
For the ﬁrst column of d, we use the formula
d[i] = d[i −1] + TAX · log10(s1[i −1].size), i = 1...length(s1),
which corresponds to deletion of elements from the ﬁrst sequence s1. The constant
TAX allows for adjustment to the penalty calculation. Similarly, we use the formula
for ﬁlling in the ﬁrst row
d [j] = d [j −1] + TAX · log10(s2[j −1].size), j = 1...length(s2),
which denotes insertion of elements from the second sequence s2. The rest of the
elements are set using the formula
d[i+1][j+1] = min
d[i][j] + cost(s1[i], s2[j]) · log10((s1[i].size + s2[j].size)/2)
d[i][j + 1] + TAX · log10(s1[i].size)
d[i + 1][j] + TAX · log10(s2[j].size).
For each remaining item of d, we calculate the three equations in (7) and select the
minimum result. The penalty for substituting an element from s1 to an element in s2
is given by the ﬁrst equation in (7). In this case, we take into account both the overall
cost (6) and the average size of the two segments. The second equation evaluates the
penalty for a deletion of an element in sequence s1. Lastly, an insertion of an element
into sequence s2 is speciﬁed by the third equation. Penalties from both the deletion
and insertion operations are based on the size of the segment.
Similarity calculation
Using the resulting edit distance (i.e., the last element of the edit matrix d) as
described in the previous section, we can then calculate the similarity percentage
between s1 and s2 sequences by the formula
similarity = 100 −d[length(s1)][length(s2)]
where costmax is the penalty in the worst case scenario where all elements in s1 are
deleted and all elements in s2 are inserted. In addition, we increase this penalty
based on the edit operation being performed. This maximum penalty is determined
as follows
costmax+ =
2 · TAX · (log10(s1[i].size) + log10(s2[j].size)), s1[i] substitute s2[j]
TAX · log10(s1[i].size), delete s1[i]
TAX · log10(s2[j].size), insert s2[j].
Calculating costmax is similar to penalty calculations for the ﬁrst column and ﬁrst
row of matrix d. The only exception is the doubling of the penalty when the element
from the sequence s1 is substituted for the element in the second sequence s2.
Figure 11 summarizes the comparison of two sequences of ﬁle segments.
Figure 11: Sequence comparison process
Experiments
We apply our method to the metamorphic virus detection problem using a process
consistent with with a slight alteration.
The diﬀerence is in the number of
comparisons, i.e., pairing of ﬁles. Instead of exclusive pairing , we compute the
similarity of each possible pair in the test set. This progression from makes for a
more complete experiment, hence produce a more accurate result.
In order to identify malicious programs, we deﬁne a similarity threshold that
separates them from the benign programs.
Our goal is to clearly distinguish the
viruses from ordinary ﬁles. To set the threshold, we perform the following steps:
1. Determine the similarity range for a metamorphic virus family by calculating
the degree of similarity for all pairs of virus ﬁles. For a test set of 50 ﬁles, as in
the case of the G2 and NGVCK family of viruses, we will perform
similarity
calculations, which is equivalent to 1,225 comparisons.
2. Determine the similarity range for the metamorphic ﬁles used in step 1 versus
a set of benign ﬁles by calculating the degree of similarity for all pairs. For
a set of 50 virus ﬁles and 16 benign ﬁles, this will generate (50 ∗16) = 800
comparisons.
3. Set the threshold by analyzing the values around the lower end of the range in
step 1 and higher end of step 2.
If a threshold can be set in step 3, then our method is capable of detecting a virus
from a known metamorphic virus family.
We use 50 virus ﬁles generated by G2 (Second Generation virus generator) and
50 ﬁles from the NGVCK virus family . For the comparison set (i.e., benign ﬁles),
we use 16 randomly selected Cygwin utilities ﬁles shown in Table 3. The Cygwin
ﬁles are chosen with the assumption that much of their functionality is comparable
with those of the virus programs without the malicious intent. The Cygwin utilities
ﬁles were also used for comparison in .
Table 3: Cygwin ﬁles from ./cygutils
Cygwin benign ﬁle
Cygwin benign ﬁle
msgtool.exe
banner.exe
putclip.exe
readshortcut.exe
cygdrop.exe
realpath.exe
cygstart.exe
getclip.exe
semstat.exe
semtool.exe
mkshortcut.exe
shmtool.exe
We also evaluate our method against metamorphic worms generated by
MWOR , which is a generator developed to evade statistical-based detection
such as the HMM-based detector we discussed in Section 2.2.1. One of the metamorphic techniques MWOR uses is dead code insertion. The amount of dead-code added
in each replication is determined by the dead-code to worm-code padding ratio. A
padding ratio of 2 means that the dead-code is twice as much as the worm instructions. Additionally, the instructions used for padding are retrieved from a subset of
the benign ﬁles in our test set. Insertion of dead-code allows the worm to assimilate
code patterns present in benign programs. Its intent is to resemble ordinary programs
to avoid detection.
In our experiments, we use seven sets of MWOR with increasing padding ratios
of 0.5, 1.0, 1.5, 2.0, 3.0, and 4.0 . With higher padding ratios, the expectation is
a high degree of similarity will result from worm and benign ﬁles comparison. Since
the MWOR ﬁles were generated on a Linux environment, for comparison, we use
a set of 30 Linux system programs shown in Table 4.
Table 4: Linux system ﬁles
Linux benign ﬁle
Linux benign ﬁle
usr/bin/kill
/bin/dmesg
usr/bin/killall
usr/bin/last
/bin/mknod
usr/bin/ld
/bin/mount
usr/bin/namei
usr/bin/nm
/bin/sleep
usr/bin/nm-tool
usr/bin/objdump
/bin/touch
usr/bin/oclock
/usr/bin/as
usr/bin/readelf
/usr/bin/at
usr/bin/rpl8
/usr/bin/ﬁle
usr/bin/shuf
/usr/bin/funzip
usr/bin/size
/usr/bin/dig
usr/bin/strip
/usr/bin/msgcat
usr/bin/sum
Parameters
Recall that our algorithm consists of two stages, namely, ﬁle segmentation and
sequence comparison. In the ﬁle segmentation phase, we build an entropy map of a
ﬁle using sliding window. We set the window size to 64 bytes for the G2 and NVGCK
ﬁles, and 256 bytes for the MWOR ﬁle sets. The slide size is set to approximately
half the window size, making certain that the entropy map is of size power of 2. This
size restriction is due to the segmentation algorithm, which successively splits the
input data in half at each transformation scale . We use lower values for G2 and
NGVCK due to their small ﬁle sizes. Higher parameter values generate insuﬃcient
number of segments necessary for sequence comparison.
For computing the total penalty cost in equation (6) in the sequence comparison phase, we set the coeﬃcients to 0.4 and 1.6 for PART_ENT and PART_SIZE,
respectively. By using such values, the size diﬀerence of the compared segments is
augmented. This increases the inﬂuence of the size penalty to the overall cost. The
constant TAX is set to 0.3, meaning the average penalty for a pair of mismatching
segments is 0.3.
First, we applied our similarity measure to a family of G2 viruses and a set of
Cygwin utility ﬁles. We used the process outlined at the start of this chapter. Figure 12 shows the results, where the red points correspond to the similarity percentages
between virus pairs and the virus-benign pairs are denoted by the green points. You
can see in this diagram that the G2 viruses are almost identical with one another.
The similarity percentages are all above 95%. Since found that the virus variants generated by G2 have the highest average similarity among other generators,
the result here is as expected. Also, notice the three bands among the virus-benign
pairs. These are formed due to the ﬁle size diﬀerence between the benign ﬁles and
the viruses. The higher the size diﬀerence, the lower the similarity is between them.
Evidently, our method is capable of detecting G2 viruses with false positive rate of
0%. We can easily use the lowest similarity score of a pair of viruses for setting the
Figure 12: G2 similarity
threshold, which is 95.61%.
Next, we assessed our method against the MWOR family of metamorphic worms.
We tested seven MWOR sets with diﬀering padding ratios. Each MWOR set was
compared against a set of benign programs consisting of Linux system ﬁles. Figure 13
displays the results of MWOR with 0.5 padding ratio. This ratio means that the worm
samples contain half as much dead-code as the actual worm instructions.
the similarity results between worms and benign ﬁles, particularly the low group of
points around 30% and 40%. Since diﬀerent variants of the same malicious program
are typically equal in size and our solution penalizes on the size diﬀerence, the
smallest and largest benign ﬁles scored the lowest when compared to the worm ﬁles.
Also, a subset of the benign ﬁles used for padding the worm code came close to
the average similarity of all worm and benign ﬁles comparisons, which is around 67%.
Figure 13: MWOR 0.5 similarity
The similarity percentages between the worm ﬁles are well above 90%; thus, we can
set the threshold to the lowest similarity score of 92.82% without any false positive
detection. The statistics of our results are summarized in Table 5.
Table 5: MWOR 0.5 similarity statistics
Comparison
Minimum (%)
Maximum (%)
worm vs. worm
worm vs. benign
We performed the same experiments on MWOR sets with higher padding ratios
and the results are consistent with the outcome from MWOR 0.5. Although the average similarity between worms decreases as the padding ratio increases, the separation
between the worms and benign ﬁles is evident in every MWOR set. Therefore, our
method is capable of detecting diﬀerent samples of MWOR ﬁles. The similarity plots
are shown in Appendix A. Table 6 summarizes the similarity statistics of the MWOR
Table 6: MWOR similarity statistics
Comparison
Minimum (%)
Maximum (%)
worm vs. worm
worm vs. benign
worm vs. worm
worm vs. benign
worm vs. worm
worm vs. benign
worm vs. worm
worm vs. benign
worm vs. worm
worm vs. benign
worm vs. worm
worm vs. benign
Lastly, we turn to the NGVCK virus family, which found to be the most
highly metamorphic. Our ﬁrst observation is that ﬁles in this sample diﬀer in size.
This is unlike the G2 and MWOR test data where all the ﬁles within each set have
the same size. Out of the 50 NGVCK ﬁles in our test set, 34 ﬁles are 4 kilobytes
in size, 13 ﬁles are 8 kilobytes, and the other three ﬁles are 16, 36, and 40 kilobytes
(KB). First, we ran a similarity test between the 34 NGVCK ﬁles (4 KB) and a set
of Cygwin ﬁles listed in Table 3. The plot is shown in Figure 14. While the average
similarity score of the virus pairs came out high at 90.31%, there are a number of
overlaps among the virus and benign ﬁle comparisons. If we take the minimum score
from the virus pair comparisons and set it as the threshold, 1.5% of the virus and
benign pairs score higher than the threshold.
We repeated the same test on the group of 13 NGVCK ﬁles having equal size of
Figure 14: NGVCK (4 KB) similarity
8 KB. Figure 15 displays the similarity plot. In this case, the threshold can be set
to the minimum virus pair comparison score with 0% false positive rate. However, if
we combine the 4 KB and 8 KB NGVCK ﬁles, a number of virus pair comparisons
result to a low degree of similarity and we are unable to set a concrete threshold.
This similarity plot is shown in Figure 16. Adding the other three larger NGVCK
ﬁles (16 KB, 36 KB, 40 KB) into the set produced similar result where a threshold
is not tangible. The plot is included in Appendix B. Statistics of NGVCK are also
presented in Appendix B.
We summarize all the results by plotting the ROC curves. An ROC (Receiver
Operating Characteristics) curve is a visual aid for performance analysis of a classiﬁer.
It illustrates the trade-oﬀbetween true positive rates and false positive rates for
varying thresholds . Figure 17 shows the ROC curves for the NGVCK results. The
curves for the 4 KB and 8 KB ﬁle sets are trivial, whereas curves for the combined
Figure 15: NGVCK (8 KB) similarity
Figure 16: NGVCK (4 KB and 8 KB) similarity
sets exhibit the eﬀect of the overlaps between viruses and benign ﬁles comparisons.
Figure 17: NGVCK ROC curves
We also calculated the area under the curve (AUC) for each of the ROC graphs.
The AUC measures the accuracy of a classiﬁer with values from 0 to 1.0, where 1.0
indicates faultless classiﬁcation . However, a usable classiﬁer should have an AUC
above 0.5. Otherwise, it would be no better than random guessing. Table 7 displays
the AUC for sets of NGVCK ﬁles. The ROC and AUC results for G2 and MWOR
Table 7: AUC of NGVCK
NGVCK File Set
4 KB (34 ﬁles)
8 KB (13 ﬁles)
4-8 KB (47 ﬁles)
All (50 ﬁles)
are in Appedix C.
Additionally, to analyze the impact of the cost parameters, we calculated the
AUC for diﬀerent values of PART_ENT and PART_SIZE on the complete NGVCK
For the entropy parameter, we used a range of values from 0 to 1 with
0.05 increments, and values from 0 to 2 with 0.10 increments for the size parameter. Figure 18 shows the results. Notice that for any values of PART_ENT where
PART_SIZE is below 0.80 and for any combination where PART_ENT is below 0.25,
our method’s accuracy is 0.92 at best, which is lower than our experimental result
found in Table 7 for all 50 NGVCK ﬁles. This conﬁrms that the parameter values
used in our experiments are adequate.
Figure 18: AUC for NGVCK with various parameter values
Finally, to determine the eﬃciency of our method, we processed various ﬁles with
sizes ranging from 10 KB to 4 MB. For ﬁles less than 100 KB in size, the processing
time is around 0.02 seconds. A 4 MB ﬁle took less than 1 second to complete. The
results are shown in Figure 19. Note that the largest malware ﬁle in our experiments
is 68 KB, so each of those samples took no longer than 0.02 seconds to score. These
processes times were measured on a 64-bit Windows 7 machine with Intel Core i5-
2540M processor at 2.60 GHz and 8 GB of memory.
Figure 19: File processing time
Conclusion and Future Work
The similarity method presented in this paper is based on static analysis of
ﬁles. Development of such measure is vital in classiﬁcation of highly metamorphic
malware since it uses obfuscation techniques designed to evade signature-based detection. We reviewed other static detection schemes, which analyze opcode sequences
of executable ﬁles. In contrast, our solution examines the binary ﬁles directly and
evaluates the distinction of bytes within the ﬁles. Therefore, our algorithm requires
no code disassembly, reducing computational overhead.
We applied our method to the metamorphic malware detection problem. The experiments show that our proposed solution is capable of identifying malware samples
generated by the G2 and MWOR metamorphic generators at a 100% rate. It appears
that the amount of dead-code applied by the MWOR generator in our test set did not
impact the structural entropy of the worms. However, it is valuable to mention that
the average similarity between MWOR ﬁles slowly decreased with increasing padding
ratio. Therefore, given a suﬃcient amount of dead-code may strengthen its ability
against static detection. Nevertheless, large blocks of instructions that never execute
may appear suspicious to modern anti-virus emulators.
We repeated the same experiment against NGVCK family of viruses. Immediately, we noticed the size diﬀerences of the NGVCK ﬁles, 34 of which are 4 KB
in size, 13 are 8 KB, and the other three ﬁles are 16 KB, 36 KB, and 40 KB. We
grouped these ﬁles based on their sizes and ran separate tests. We also ran additional
tests on combined ﬁles. Our technique was able to distinguish the viruses from the
benign programs with a tolerable 1.5% false match for the 4 KB ﬁles. The 8 KB
NGVCK ﬁles resulted in 100% detection rate without false positive. In the combined
test, however, a threshold was not viable. Our experiments resulted in a number of
overlaps between viruses and benign ﬁles. It appears that NGVCK not only has the
ability to change its structure at certain generations, it also makes some variants look
more like normal programs.
Overall, our results indicate that a similarity measure based on structural entropy
is potentially useful for classifying highly metamorphic malware. At the least, this
creates an additional hurdle that a malware writer must clear to create metamorphic
malware that is undetectable by static means.
For future work, it might be worthwhile to further study the NGVCK samples
and its morphing techniques. Also, it may be worth exploring the cost function used
this research. Balancing the inﬂuence of the entropy and size diﬀerences between
compared elements might improve the similarity score. Furthermore, consideration
of other algorithms for sequence comparison that employ local alignment may also
improve the results for sequences with diverse lengths, which is the case for the
NGVCK samples. Such alignment methods take into account only similar regions
rather than all elements in the sequences.
A well-known method based on local
sequence alignment is the Smith-Waterman algorithm, which compares all possible
sub-sequences and selects the optimal one .
LIST OF REFERENCES
 P. Addison. The Illustrated Wavelet Transform Handbook: Introductory Theory
and Applications in Science, Engineering, Medicine and Finance. Taylor and
Francis Group, 2002.
 A. Apostolico and Z. Galil. Pattern Matching Algorithms. Oxford University
Press, 1997.
 M. Borda. Fundamentals in Information Theory and Coding. Springer, 2011.
 Cygwin. Cygwin utility ﬁles.
 
 Dictionary, Entropy.
 
 T. Fawcett. ROC graphs: notes and practical considerations for researchers.
Bioinformatics and Computational Biology, George Mason University, 2004.
 
 R. Islam, A. W. Naji, A. A. Zaidan, and B. B. Zaidan. New system for secure
cover ﬁle of hidden data in the image page within executable ﬁle using statistical steganography techniques. International Journal of Computer Science and
Information Security (IJCSIS), Vol. 7, No. 1, pp. 273-279, January 2010.
 Karmeshu. Entropy Measures, Maximum Entropy Principle and Emerging Applications. Springer, 2003.
 S. Kazi, Hidden markov models for software piracy detection, Master’s report,
Department of Computer Science, San Jose State University, 2012.
 
 Wikipedia. Levenshtein distance, 2012.
 
 SearchSecurity. Metamorphic and polymorphic malware, 2010.
 
metamorphic-and-polymorphic-malware.
 M. Patel, Similarity tests for metamorphic virus detection, Master’s report, Department of Computer Science, San Jose State University, 2011.
 
 M. Pietrek. Peering inside the PE: a tour of the Win32 portable executable ﬁle
format. MSDN Magazine
 
 S. Robinson. Expert .NET 1.1 Programming. Apress, 2004.
 N. Runwal, R. Low, and M. Stamp, Opcode graph similarity and metamorphic
detection. Journal in Computer Virology, Vol. 8, No. 1-2, pp. 37-52, May 2012.
 A. Singh. Identifying Malicious Code Through Reverse Engineering. Springer,
 I. Sorokin. Comparing ﬁles using structural entropy. Journal in Computer Virology, Vol. 7, No. 4, pp. 259-265, November 2011.
 S. M. Sridhara. Metamorphic worm that carries its own morphing engine. Master’s report, Department of Computer Science, San Jose State University, 2012.
 
 S. M. Sridhara and M. Stamp. Metamorphic worm that carries its own morphing
engine. Journal in Computer Virology, October 2012.
 M. Stamp, A revealing introduction to hidden Markov models, 2012,
 
 Z. Struzik and A. Siebes. The haar wavelet transform in the time series similarity
paradigm. In Proceedings of the Third European Conference on Principles of
Data Mining and Knowledge Discovery (PKDD ’99). Springer-Verlag, London,
UK, 12-22.
 
 P. Van Fleet, The discrete haar wavelet transformation, Joint Mathematical
Meetings, Center for Applied Mathematics, University of St. Thomas, 2007,
 
NewOrleans07/HaarTransform.pdf.
 G. Verschuuren. Excel 2007 for Scientists and Engineers. Holy Macro! Books,
 Department of Computer Science, San Jose State University. Virus ﬁles, 2012.
 
 Symantec. Viruses, worms, and trojans, 2011.
 
 T. Vuorenmaa, The discrete wavelet transform with ﬁnancial time series
applications, Seminar on Learning Systems, University of Helsinki, 2003.
 
 R. A. Wagner and M. J. Fischer. The string-to-string correction problem. Journal
of the ACM (JACM), Vol. 21, No. 1, pp. 168-173, January 1974.
 W. Wong and M. Stamp, Hunting for metamorphic engines. Journal in Computer
Virology, Vol. 2, No. 3, pp. 211-229, December 2006.
 I. You and K. Yim, Malware obfuscation techniques: a brief survey. Broadband,
Wireless Computing, Communication and Applications (BWCCA), 2010 International Conference on, pp. 297-300, November 2010.
 M. Zvelebil and J. O. Baum. Understanding Bioinformatics. Garland Science,
Taylor & Francis Group, 2008.
APPENDIX A
MWOR Results
Figures A.20, A.21, A.22, A.23, A.24 and A.25 show the similarity plots of
MWOR family of worms with padding ratios 1.0, 1.5, 2.0, 2.5, 3.0, and 4.0, respectively. Each MWOR set was compared with a set of benign ﬁles listed in Table 4
of Section 4.1.
Figure A.20: MWOR 1.0 similarity
Figure A.21: MWOR 1.5 similarity
Figure A.22: MWOR 2.0 similarity
Figure A.23: MWOR 2.5 similarity
Figure A.24: MWOR 3.0 similarity
Figure A.25: MWOR 4.0 similarity
APPENDIX B
NGVCK Results
Figure B.26 shows the similarity plot for all 50 NGVCK ﬁles. The statistics for
the NGVCK sets are in Table B.8.
Figure B.26: NGVCK similarity
Table B.8: NGVCK similarity statistics
Comparison
Minimum (%)
Maximum (%)
worm vs. worm
(34 4KB ﬁles)
worm vs. benign
worm vs. worm
(13 8KB ﬁles)
worm vs. benign
worm vs. worm
(4KB + 8KB ﬁles)
worm vs. benign
worm vs. worm
(all 50 ﬁles)
worm vs. benign
APPENDIX C
ROC Curves
Figure C.27 shows the ROC curve for G2 with AUC of 1.0. The results from the
MWOR experiments all produced the same ROC curve, which is shown in Figure C.28
with AUC of 1.0.
Figure C.27: G2 ROC curve
Figure C.28: MWOR ROC curve (for all padding ratios)