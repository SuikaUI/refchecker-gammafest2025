Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 779–784
Brussels, Belgium, October 31 - November 4, 2018. c⃝2018 Association for Computational Linguistics
Neural Latent Extractive Document Summarization
Xingxing Zhang†, Mirella Lapata‡, Furu Wei† and Ming Zhou†
†Microsoft Research Asia, Beijing, China
‡Institute for Language, Cognition and Computation,
School of Informatics, University of Edinburgh, UK
{xizhang,fuwei,mingzhou}@microsoft.com, 
Extractive
summarization
sentence-level
created heuristically , two have consistently attracted attention: extractive approaches generate summaries
by copying parts of the source document (usually
whole sentences), while abstractive methods may
generate new words or phrases which are not in
the document.
A great deal of previous work has focused on
extractive summarization which is usually modeled as a sentence ranking or binary classiﬁcation problem (i.e., sentences which are top
ranked or predicted as True are selected as
summaries).
Early attempts mostly leverage
human-engineered features coupled with binary classi-
ﬁers , hidden Markov models
 , graph based methods
 , and integer linear programming
 .
The successful application of neural network
models to a variety of NLP tasks and the availability of large scale summarization datasets has
provided strong impetus to develop data-driven
approaches which take advantage of continuousspace representations. Cheng and Lapata 
propose a hierarchical long short-term memory
network to learn context dependent sentence representations for a document and then use yet another LSTM decoder to predict a binary label for
each sentence.
Nallapati et al. adopt a
similar approach, they differ in their neural architecture for sentence encoding and the features
used during label prediction, while Narayan et al.
 equip the same architecture with a training
algorithm based on reinforcement learning. Abstractive models are based on sequenceto-sequence learning , however, most of them underperform or are on par with the baseline of simply
selecting the leading sentences in the document as
summaries .
Although seemingly more successful than their
abstractive counterparts, extractive models require
sentence-level labels, which are not included in
most summarization datasets (only document and
gold summary pairs are available). Sentence labels are usually obtained by rule-based methods
 or by maximizing the
ROUGE score between a subset of sentences and the human written summaries .
These methods do not fully
exploit the human summaries, they only create
True/False labels which might be suboptimal.
Latent variable extractive summarization
model. senti is a sentence in a document and sum senti
is a sentence in a gold summary of the document.
In this paper we propose a latent variable extractive model and view labels of sentences in
a document as binary latent variables (i.e., zeros and ones). Instead of maximizing the likelihood of “gold” standard labels, the latent model
directly maximizes the likelihood of human summaries given selected sentences. Experiments on
the CNN/Dailymail dataset 
show that our latent extractive model improves
upon a strong extractive baseline trained on rulebased labels and also performs competitively to
several recent models.
We ﬁrst introduce the neural extractive summarization model upon which our latent model is
based on. We then describe a sentence compression model which is used in our latent model and
ﬁnally move on to present the latent model itself.
Neural Extractive Summarization
In extractive summarization, a subset of sentences
in a document is selected as its summary.
model this problem as an instance of sequence labeling. Speciﬁcally, a document is viewed as a sequence of sentences and the model is expected to
predict a True or False label for each sentence,
where True indicates that the sentence should be
included in the summary. It is assumed that during
training sentences and their labels in each document are given (methods for obtaining these labels
are discussed in Section 3).
As shown in the lower part of Figure 1, our
extractive model has three parts: a sentence encoder to convert each sentence into a vector, a
document encoder to learn sentence representations given surrounding sentences as context, and
a document decoder to predict sentence labels
based on representations learned by the document
Let D = (S1, S2, . . . , S|D|) denote a
document and Si = (wi
2, . . . , wi
|Si|) a sentence in D (where wi
j is a word in Si).
Y = (y1, . . . , y|D|) denote sentence labels. The
sentence encoder ﬁrst transforms Si into a list
of hidden states (hi
2, . . . , hi
|Si|) using a Bidirectional Long Short-Term Memory Network . Then, the sentence encoder
yields vi, the representation of Si, by averaging
these hidden states (also see Figure 1):
In analogy to the sentence encoder, the document encoder is another Bi-LSTM but applies
on the sentence level.
After running the Bi-
LSTM on a sequence of sentence representations
(v1, v2, . . . , v|D|), we obtain context dependent
sentence representations (hE
2 , . . . , hE
The document decoder is also an LSTM which
predicts sentence labels.
At each time step, it
takes the context dependent sentence representation of Si produced by the document encoder as
well as the prediction in the previous time step:
i = LSTM(hD
We e(yi−1)
where We ∈Rd×2 is the label embedding matrix (d is the hidden dimension for the document
decoder LSTM) and yi−1 is the prediction at time
step i−1; the predicted label distribution for yi is:
p(yi|y1:i−1, hD
i−1) = softmax except that
we use a word-level long short-term memory network coupled with mean pooling to learn sentence
representations, while they use convolutional neural network coupled with max pooling (Kim et al.,
Sentence Compression
We train a sentence compression model to map a
sentence selected by the extractive model to a sentence in the summary. The model can be used to
evaluate the quality of a selected sentence with respect to the summary (i.e., the degree to which it is
similar) or rewrite an extracted sentence according
to the style of the summary.
For our compression model we adopt a standard attention-based sequence-to-sequence architecture .
The training set for this model is generated from
the same summarization dataset used to train the
exractive model. Let D = (S1, S2, . . . , S|D|) denote a document and H = (H1, H2, . . . , H|H|)
its summary. We view each sentence Hi in the
summary as a target sentence and assume that
its corresponding source is a sentence in D most
similar to it. We measure the similarity between
source sentences and candidate targets using
ROUGE, i.e., Sj = argmaxSj ROUGE(Sj, Hi)
and ⟨Sj, Hi⟩is a training instance for the compression model. The probability of a sentence ˆ
Hi being the compression of ˆSj (i.e., ps2s( ˆ
Hi| ˆSj)) can
be estimated with a trained compression model.
Latent Extractive Summarization
Training the extractive model described in Section 2.1 requires sentence-level labels which are
obtained heuristically . Our latent variable model
views sentences in a document as binary variables
(i.e., zeros and ones) and uses sentences with activated latent variables (i.e., ones) to infer gold summaries.
The latent variables are predicted with
an extractive model and the loss during training
comes from gold summaries directly.
Let D = (S1, S2, . . . , S|D|) denote a document
and H = (H1, H2, . . . , H|H|) its human summary
(Hk is a sentence in H). We assume that there is a
latent variable zi ∈{0, 1} for each sentence Si indicating whether Si should be selected, and zi = 1
entails it should.
We use the extractive model
from Section 2.1 to produce probability distributions for latent variables (see Equation (3)) and obtain them by sampling zi ∼p(zi|z1:i−1, hD
Figure 1). C = {Si|zi = 1}, the set of sentences
whose latent variables equal to one, are our current
extractive summaries. Without loss of generality,
we denote C = (C1, . . . , C|C|). Then, we estimate
how likely it is to infer the human summary H
from C. We estimate the likelihood of summary
sentence Hl given document sentence Ck with the
compression model introduced in Section 2.2 and
calculate the normalized1 probability skl:
|Hl| log ps2s(Hl|Ck)
The score Rp measures the extent to which H can
be inferred from C:
Rp(C, H) = 1
For simplicity, we assume one document sentence
can only ﬁnd one summary sentence to explain it.
Therefore, for all Hl, we only retain the most evident skl. Rp(C, H) can be viewed as the “precision” of document sentences with regard to summary sentences. Analogously, we also deﬁne Rr,
which indicates the extent to which H can be covered by C:
Rr(C, H) =
Rr(C, H) can be viewed as the “recall” of document sentences with regard to summary sentences.
The ﬁnal score R(C, H) is the weighted sum of the
R(C, H) = α Rp(C, H) + (1 −α) Rr(C, H) (7)
Our use of the terms “precision” and “recall” is
reminiscent of relevance and coverage in other
summarization work .
We train the model by minimizing the negative
expected R(C, H):
L(θ) = −E(z1,...,z|D|)∼p(·|D)[R(C, H)]
where p(·|D) is the distribution produced by the
neural extractive model (see Equation (3)). Unfortunately, computing the expectation term is prohibitive, since the possible latent variable combinations are exponential. In practice, we approximate this expectation with a single sample from
1We also experimented with unnormalized probabilities
(i.e., excluding the exp in Equation (4)), however we obtained inferior results.
the distribution of p(·|D).
We use the REIN-
FORCE algorithm to approximate the gradient of L(θ):
i=1 ∇log p(zi|z1:i−1, hD
i−1)[R(C, H) −bi]
Note that the model described above can be
viewed as a reinforcement learning model, where
R(C, H) is the reward. To reduce the variance of
gradients, we also introduce a baseline linear regression2 model bi to estimate the expected value of R(C, H). To avoid
random label sequences during sampling, we use
a pre-trained extractive model to initialize our latent model.
Experiments
Dataset and Evaluation
We conducted experiments on the CNN/Dailymail dataset .
We followed
the same pre-processing steps as in See et al.
The resulting dataset contains 287,226
document-summary pairs for training, 13,368 for
validation and 11,490 for test.
To create sentence level labels, we used a strategy similar to
Nallapati et al. .
We label the subset of
sentences in a document that maximizes ROUGE
(against the human summary) as True and all
other sentences as False. Using the method described in Section 2.2, we created a compression
dataset with 1,045,492 sentence pairs for training,
53,434 for validation and 43,382 for testing. We
evaluated our models using full length F1 ROUGE
 and the ofﬁcial ROUGE-1.5.5.pl
We report ROUGE-1, ROUGE-2, and
Implementation
extractive
model on an Nvidia K80 GPU card with a batch
size of 32. Model parameters were uniformly initialized to [−1
√c] (c is the number of columns
in a weight matrix). We used Adam to optimize our models with a learning
rate of 0.001, β1 = 0.9, and β2 = 0.999. We
trained our extractive model for 10 epochs and
selected the model with the highest ROUGE on
the validation set. We rescaled the gradient when
its norm exceeded 5 and
2The linear regression model bt is trained by minimizing the mean squared error between the prediction of bt and
LEAD3 
pointer+coverage
abstract-RL
abstract-ML+RL
SummaRuNNer
EXTRACT-CNN
REFRESH 40.00
LATENT+COMPRESS
CNN/Dailymail test set using full-length F1 ROUGE-1
(R-1), ROUGE-2 (R-2), and ROUGE-L (R-L).
regularized all LSTMs with a dropout rate of 0.3
 . We
also applied word dropout at
rate 0.2. We set the hidden unit size d = 300 for
both word-level and sentence-level LSTMs and all
LSTMs had one layer. We used 300 dimensional
pre-trained FastText vectors 
to initialize our word embeddings.
The latent
model was initialized from the extractive model
(thus both models have the same size) and we
set the weight in Equation (7) to α = 0.5. The
latent model was trained with SGD, with learning
rate 0.01 for 5 epochs. During inference, for both
extractive and latent models, we rank sentences
with p(yi = True|y1:i−1, D) and select the top
three as summary (see also Equation (3)).
Comparison Systems
We compared our model
against LEAD3, which selects the ﬁrst three leading sentences in a document as the summary
and a variety of abstractive and extractive models.
Abstractive models include a sequence-tosequence architecture ; abstract), its pointer generator variant (See et al.
pointer+coverage), and two reinforcement learning-based models . We also compared our approach against an extractive model
based on hierarchical recurrent neural networks
 , the model
described in Section 2.1 (EXTRACT) which encodes sentences using LSTMs, a variant which
employs CNNs instead , as well as a similar system based
on reinforcement learning .
However, note
that SummaRuNNer generates anonymized summaries while our models
generate non-anonymized ones, and therefore the
results of EXTRACT and SummaRuNNer are not
strictly comparable (also note that LEAD3 results
are different in Table 1). Nevertheless, EXTRACT
and +0.57 in terms of ROUGE-L, while SummaRuNNer exceeds LEAD3 by +0.50 ROUGE-
2 points and is worse by −0.20 points in terms
of ROUGE-L. We thus conclude that EXTRACT
is better when evaluated with ROUGE-2 and
ROUGE-L. EXTRACT outperforms all abstractive
models except for abstract-RL. ROUGE-2 is lower
for abstract-RL which is more competitive when
evaluated against ROUGE-1 and ROUGE-L.
Our latent variable model (LATENT; Section
2.3) outperforms EXTRACT, despite being a strong
baseline, which indicates that training with a loss
directly based on gold summaries is useful. Differences among LEAD3, EXTRACT, and LATENT
are all signiﬁcant with a 0.95 conﬁdence interval (estimated with the ROUGE script).
Interestingly, when applying the compression model
from Section 2.2 to the output of our latent model
( LATENT+COMPRESS ), performance drops considerably.
This may be because the compression model is a sentence level model and it removes phrases that are important for creating the
document-level summaries.
Conclusions
We proposed a latent variable extractive summarization model which leverages human summaries
directly with the help of a sentence compression
model. Experimental results show that the proposed model can indeed improve over a strong extractive model while application of the compression model to the output of our extractive system
leads to inferior output. In the future, we plan to
explore ways to train compression models tailored
to our summarization task.
Acknowledgments
We thank the EMNLP reviewers for their valuable
feedback and Qingyu Zhou for preprocessing the
CNN/Dailymail dataset. We gratefully acknowledge the ﬁnancial support of the European Research Council (award number 681760; Lapata).