IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 12, DECEMBER 2015
Salient Object Detection: A Benchmark
Ali Borji, Ming–Ming Cheng, Huaizu Jiang and Jia Li
Abstract—We extensively compare, qualitatively and quantitatively, 41 state-of-the-art models (29 salient object detection, 10 ﬁxation prediction, 1 objectness, and 1 baseline)
over 7 challenging datasets for the purpose of benchmarking
salient object detection and segmentation methods. From the
results obtained so far, our evaluation shows a consistent rapid
progress over the last few years in terms of both accuracy
and running time. The top contenders in this benchmark
signiﬁcantly outperform the models identiﬁed as the best in the
previous benchmark conducted three years ago. We ﬁnd that
the models designed speciﬁcally for salient object detection
generally work better than models in closely related areas,
which in turn provides a precise deﬁnition and suggests an
appropriate treatment of this problem that distinguishes it
from other problems. In particular, we analyze the inﬂuences
of center bias and scene complexity in model performance,
which, along with the hard cases for state-of-the-art models,
provide useful hints towards constructing more challenging
large scale datasets and better saliency models. Finally, we
propose probable solutions for tackling several open problems
such as evaluation scores and dataset bias, which also suggest
future research directions in the rapidly-growing ﬁeld of
salient object detection.
Index Terms—Salient object detection, saliency, explicit
saliency, visual attention, regions of interest, objectness, segmentation, interestingness, importance, eye movements
I. INTRODUCTION
ISUAL attention, the astonishing capability of human
visual system to selectively process only the salient
visual stimuli in details, has been investigated by multiple
disciplines such as cognitive psychology, neuroscience,
and computer vision – . Following cognitive theories
(e.g., feature integration theory (FIT) , guided search
model , ) and early attention models (e.g., Koch and
Ullman and Itti et al. ), hundreds of computational
Manuscript received January 5, 2015; revised July 13, 2015 and September 19, 2015; accepted October 4, 2015. Date of publication October
7, 2015; date of current version October 23, 2015. The associate editor
coordinating the review of this manuscript and approving it for publication
was Prof. Christine Guillemot. (Ali Borji and Ming-Ming Cheng equally
contributed to this work.)
A. Borji is with the Computer Science Department, University of
Wisconsin, Milwaukee, WI 53211. E-mail: 
M.M Cheng (corresponding author) is with CCCE & CS, Nankai University, Jinnan, Tianjin, P.R.China, 300353. E-mail: 
H. Jiang is with the Institute of Artiﬁcial Intelligence and Robotics,
Xi’an Jiaotong University, China. E-mail: 
J. Li is with State Key Laboratory of Virtual Reality Technology
and Systems, School of Computer Science and Engineering, Beihang
University. He is also with the International Research Institute for Multidisciplinary Science (IRIMS) at Beihang University, Beijing, China. Email: 
An earlier version of this work has been published in ECCV 2012 .
Please contact Ming-Ming Cheng as corresponding author for benchmark updates at 
Color versions of one or more of the ﬁgures in this paper are available
online at 
Digital Object Identiﬁer 10.1109/TIP.2015.2487833
saliency models have been proposed to detect salient visual
subsets from images and videos.
Despite the psychological and neurobiological deﬁnitions, the concept of visual saliency is becoming vague in
the ﬁeld of computer vision. Some visual saliency models
(e.g., , – ) aimed to predict human ﬁxations as a
way to test their accuracy in saliency detection, while other
models – , which were often driven by computer
vision applications such as content-aware image resizing
and photo visualization , attempted to identify salient
regions/objects and used explicit saliency judgments for
evaluation . Although both types of saliency models are
expected to be applicable interchangeably, their generated
saliency maps actually demonstrate remarkably different
characteristics due to the distinct purposes in saliency
detection. For example, ﬁxation prediction models usually
pop-out sparse blob-like salient regions, while salient object
detection models often generate smooth connected areas.
On the one hand, detecting large salient areas often causes
severe false positives for ﬁxation prediction. On the other
hand, popping-out only sparse salient regions causes massive misses in detecting salient regions and objects.
To separate these two types of saliency models, in this
study we provide a precise deﬁnition and suggest an appropriate treatment of salient object detection. Generally, a
salient object detection model should, ﬁrst detect the salient
attention-grabbing objects in a scene, and second, segment
the entire objects. Usually, the output of the model is a
saliency map where the intensity of each pixel represents
its probability of belonging to salient objects. From this
deﬁnition, we can see that this problem in its essence
is a ﬁgure/ground segmentation problem, and the goal is
to only segment the salient foreground object from the
background. Note that it slightly differs from the traditional
image segmentation problem that aims to partition an image
into perceptually coherent regions.
The value of salient object detection models lies in their
applications in many areas such as computer vision, graphics, and robotics. For instance, these models have been successfully applied in many applications such as object detection and recognition – , image and video compression , , video summarization – , photo collage/media re-targeting/cropping/thumb-nailing , ,
 , image quality assessment – , image segmentation – , content-based image retrieval and image
collection browsing – , image editing and manipulating – , visual tracking – , object discovery , , and human-robot interaction – .
The ﬁeld of salient object detection develops very fast.
Many new models and benchmark datasets have been
proposed since our earlier benchmark conducted three years
 
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 12, DECEMBER 2015
ago . Yet, it is unclear how the new algorithms fare
against previous models and new datasets. Are there any
real improvements in this ﬁeld or we are just ﬁtting models
to datasets? It is also interesting to test the performance
of old high-performing models on the new benchmark
datasets. A recent exhaustive review of salient object detection models can be found in .
In this study, we compare and analyze models from
three categories: 1) salient object detection, 2) ﬁxation
prediction, and 3) object proposal generation1. The reason
to include the latter two types of models is to conduct
across-category comparison and to study whether models speciﬁcally designed for salient object detection show
actual advantage over models for ﬁxation prediction and
object proposal generation. This is particularly important
since these models have different objectives and generate
visually distinctive maps. We also include a baseline model
to study the effect of center bias in model comparison.
In summary, we hope that such a benchmark not only
allows researchers to compare their models with other
algorithms but also helps identify the chief factors affecting
the performance of salient object detection models.
II. SALIENT OBJECT DETECTION BENCHMARK
In this benchmarking, we focus on evaluating models
whose input is a single image. This is due to the fact that
salient object detection on a single input image is the main
research direction, while the comprehensive evaluation of
models working on multiple input images (e.g., co-salient
object detection and spatio-temporal saliency) lacks public
benchmarks.
A. Compared Models
In this study, we run 41 models in total (29 salient
object detection models, 10 ﬁxation prediction models, 1
objectness proposal model, and 1 baseline) whose codes or
executables were accessible (see Fig. 1 for a complete list).
The baseline model, denoted as “Average Annotation Map
(AAM),” is simply the average of ground-truth annotations
of all images on each dataset. Note that AAM often has a
larger activation at the image center (see Fig. 2), and we can
thus study the effect of center bias in model comparison.
B. Datasets
Since there exist many datasets that differ in number
of images, number of objects per image, image resolution
and annotation form (bounding box or accurate region
mask), it is likely that models may rank differently across
datasets. Hence, to come up with a fair comparison, it is
necessary to run models over multiple datasets so as to
draw objective conclusions. A good model should perform
well over almost all datasets. Toward this end, seven
1Object proposal generation is a recently emerging trend which attempts
to detect image regions that may contain objects from any object category
(a.k.a, category independent object proposals).
Salient Object Detection
Fixation Prediction
Compared salient object detection, ﬁxation prediction, object
proposal generation, and baseline models sorted by their publication year
{M= Matlab, C= C/C++, EXE = executable}. The average running time
is tested on MSRA10K dataset (typical image resolution 400×300) using
a desktop machine with Xeon E5645 2.4 GHz CPU and 8GB RAM. We
evaluate those models whose codes or executables are available.
datasets2 were chosen for model comparison, including: 1)
MSRA10K , 2) THUR15K , 3) ECSSD , 4)
JuddDB , 5) DUT-OMRON and 6) SED2 ,
 , and 7) PASCAL-S . These datasets were
selected based on the following four criteria: 1) being
widely-used, 2) containing a large number of images, 3)
having different biases (e.g., number of salient objects,
image clutter, center-bias), and 4) potential to be used as
benchmarks in the future research.
MSRA10K is a descendant of the MSRA dataset . It
contains 10,000 annotated images that covers all the 1,000
images in the popular ASD dataset . THUR15K and
DUT-OMRON are used to compare models on a large
scale. ECSSD contains a large number of semantically
meaningful but structurally complex natural images. The
reason to include JuddDB and PASCAL-S datasets was to
2To save space, we show some plots over the ECSSD dataset on our
online benchmark website.
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 12, DECEMBER 2015
(a) MSRA10K
(b) PASCAL-S
(c) THUR15K
(d) DUT-OMRON
(e) JuddDB
Average annotation maps of six datasets used in benchmarking.
assess performance of models over scenes with multiple
objects with high background clutter. Finally, we also
evaluate models over SED2 to check whether salient object
detection algorithms can perform well on images containing
more than one salient object (i.e., two in SED2). Fig. 2
shows the AAM model output of six benchmark datasets
to illustrate their different center biases. See Fig. 3 for
representative images and annotations from each dataset.
We illustrate in Fig. 4 the statistics of the seven chosen
datasets. In Fig. 4(a), we show the normalized distances
from the centroid of salient objects to the corresponding image centers. We can see that salient objects in ECCSD have
the shortest distance to image centers, while salient objects
in SED2 have the longest distances. This is reasonable since
images in SED2 usually have two objects aligned around
opposite image borders. Moreover, we can see that the
spatial distribution of salient objects in JuddDB has a larger
variety than other datasets, indicating that this dataset has
smaller positional bias (i.e., center-bias of salient objects
and border-bias of background regions).
In Fig. 4(b), we aim to show the complexity of images in
seven benchmark datasets. Toward this end, we apply the
segmentation algorithm by Felzenszwalb et al. to see
how many super-pixels (i.e., homogeneous regions) can be
obtained on average from salient objects and background
regions of each image, respectively. In this manner, we can
use this measure to reﬂect how challenging a benchmark
dataset is since massive super-pixels often indicate complex
foreground objects and cluttered background. From Fig.
4(b), we can see that JuddDB (followed by PASCAL-S)
is the most challenging benchmark since it has an average
number of 493 super-pixels from the background of each
image. On the contrary, SED2 contains fewer number of
super-pixels in foreground and background regions, indicating that images in this benchmark often contain uniform
regions and are relatively easier to process.
In Fig. 4(c), we demonstrate the average object sizes
of these benchmarks, while the size of each object is
normalized by the size of the corresponding image. We
can see that MSRA10K and ECCSD datasets have larger
objects while SED2 has smaller ones. In particular, we
can see that some benchmarks contain a limited number
(a) MSRA10K
(b) PASCAL-S
(c) JuddDB
(d) DUT-OMRON
(e) THUR15K
Fig. 3. Images and pixel-level annotations from six salient object datasets.
of image regions with large foreground objects. By jointly
considering the center-bias property, it becomes very easy
to achieve a high precision on these images.
C. Evaluation Measures
There are several ways to measure the agreement between model predictions and human annotations . Some
metrics evaluate the overlap between a tagged region and
and model predictions while others try to assess the accuracy of drawn shapes with object boundary. In addition,
some metrics have tried to consider both boundary and
shape .
Here, we use four universally-agreed, standard, and easyto-understand measures for evaluating a salient object detection model. The ﬁrst two evaluation metrics are based
on the overlapping area between subjective annotation and
saliency prediction, including the precision-recall (PR) and
the receiver operating characteristics (ROC). From these
two metrics, we also report the F-Measure, which jointly
considers recall and precision, and AUC, which is the
area under the ROC curve. The third measure directly
computes the mean absolute error (MAE) between the
estimated saliency map and ground-truth annotation. For the
sake of simpliﬁcation, we use S to represent the predicted
saliency map normalized to and G to represent the
ground-truth binary mask of salient objects. For a binary
mask, we use | · | to represent the number of non-zero
entries in the mask. Moreover, we also use the fourth
measure proposed by Margolin et al. which remedies
some problems with the classic F-measure for evaluating
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 12, DECEMBER 2015
Probability Density
(c) Normalized object size
(b) Number of regions
Probability Density
Probability Density
(a) Object to image center distance
Background area (solid lines)
Salient object area (dashed lines)
Statistics of the benchmark datasets. a) distribution of normalized object distance from image center, b) distribution of number of super-pixels
on salient objects and image background, and c) distribution of normalized object size. See text for precise deﬁnitions.
foreground-background maps obtained using segmentation
algorithms.
Precision-recall (PR). For a saliency map S, we can
convert it to a binary mask M and compute Precision
and Recall by comparing M with ground-truth G:
Precision = |M ∩G|
Recall = |M ∩G|
From this deﬁnition, we can see that the binarization
of S is the key step in the evaluation. Usually, there are
three popular ways to perform the binarization. In the ﬁrst
solution, Achanta et al. proposed the image-dependent
adaptive threshold for binarizing S, which is computed as
twice as the mean saliency of S:
y=1 S(x, y)
where W and H are the width and the height of the saliency
map S, respectively.
The second way to partition S is to use a ﬁxed threshold
which changes from 0 to 255. On each threshold, a pair
of precision/recall scores are computed, and are ﬁnally
combined to form a precision-recall (PR) curve to describe
the model performance at different situations.
The third way of binarization is to use the SaliencyCut
algorithm . In this solution, a loose threshold, which
typically results in good recall but relatively poor precision,
is used to generate the initial binary mask. Then the method
iteratively uses the GrabCut segmentation method to
gradually reﬁne the binary mask. The ﬁnal binary mask is
used to re-compute the precision-recall value.
F-measure. Usually, neither Precision nor Recall can
comprehensively evaluate the quality of a saliency map. To
this end, the F-measure is proposed as a weighted harmonic
mean of them with a non-negative weight β:
Fβ = (1 + β2)Precision × Recall
β2Precision + Recall
As suggested by many salient object detection works (e.g.,
 , , ), β2 is set to 0.3 to increase the importance
of the Precision value. The reason for weighting precision
more than recall is that recall rate is not as important as
precision (see also ). For instance, 100% recall can be
easily achieved by setting the whole region to foreground.
According to the different ways for saliency map binarization, there exist two ways to compute F-Measure. When
the adaptive threshold or GrabCut algorithm is used for the
binarization, we can generate a single Fβ for each image
and the ﬁnal F-Measure is computed as the average Fβ.
When using ﬁxed thresholding, the resulted PR curve can
be scored by its maximal Fβ, which is a good summary
of the detection performance (as suggested in ). As
deﬁned in (3), F-Measure is the weighted harmonic mean
of precision and recall, thus share the same value bounds
as precision and recall values, i.e. .
Receiver operating characteristics (ROC) curve. In addition to the Precision, Recall and Fβ, we can also
report the false positive rate (FPR) and true positive rate
(TPR) when binarizing the saliency map with a set of ﬁxed
thresholds:
TPR = |M ∩G|
FPR = |M ∩¯G|
M and ¯G denote the complement of the binary mask
M and ground-truth, respectively. The ROC curve is the
plot of TPR versus FPR by varying the threshold Tf.
Area under ROC curve (AUC) score. While ROC is a
two-dimensional representation of a model’s performance,
the AUC distills this information into a single scalar. As
the name implies, it is calculated as the area under the
ROC curve. A perfect model will score an AUC of 1, while
random guessing will score an AUC around 0.5.
Mean absolute error (MAE) score. The overlap-based
evaluation measures introduced above do not consider the
true negative saliency assignments, i.e., the pixels correctly
marked as non-salient. This favors methods that successfully assign saliency to salient pixels but fail to detect
non-salient regions over methods that successfully detect
non-salient pixels but make mistakes in determining the
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 12, DECEMBER 2015
Recall / false positive rate
Precision / true positive rate
BMS−PrecisionRecall
GB−PrecisionRecall
PR and ROC curves for BMS and GB over ECSSD.
salient ones , . Moreover, in some application
scenarios the quality of the weighted, continuous
saliency maps may be of higher importance than the binary
masks. For a more comprehensive comparison, we therefore
also evaluate the mean absolute error (MAE) between the
continuous saliency map ¯S and the binary ground truth ¯G,
both normalized in the range . The MAE score is
deﬁned as:
y=1 | ¯S(x, y) −¯G(x, y)| (5)
β -measure. Here, we adopt the technique proposed by
Margolin et al. for quantitative evaluation of models.
As an intuitive generalization of the Fβ-measure, the new
evaluation metric (F w
β -measure) provides reliable evaluation by i) extending the basic quantities (true-positive, truenegative, false-positive, and false negative) to non-binary
values, and ii) weighting errors according to their location
and their neighborhood. F w
β -measure offers uniﬁed solution
for evaluation of binary and non-binary maps.
Note that these scores sometimes do not agree with
each other. For example, Fig. 5 shows a comparison of
two models over the ECSSD dataset using PR and ROC
metrics. While there is not a big difference in ROC curves
(thus about the same AUC), one model clearly scores better
using the PR curve (thus having higher Fβ). Such disparity
between the ROC and PR measures has been extensively
studied in . Note that the number of negative examples (non-salient pixels) is typically much bigger than
the number of positive examples (salient object pixels)
in evaluating salient object detection models. Therefore,
PR curves are more informative than ROC curves and
can present an over optimistic view of an algorithm’s
performance . Thus we mainly base our conclusions
on the PR curves scores (i.e., F-Measure scores), and also
report other scores for comprehensive comparisons and for
facilitating speciﬁc application requirements. It is worth
mentioning that active research is ongoing to ﬁgure out
the better ways of evaluating salient object detection and
segmentation models (e.g. ).
D. Quantitative Comparison of Models
We evaluate saliency maps produced by different models
on seven datasets by using all evaluation metrics:
1) Fig. 6 and Fig. 7 show PR and ROC curves;
2) Fig. 8 and Fig. 9 demonstrate AUC and MAE scores;
3) Fig. 10 and Fig. 11 show F w
β and Fβ scores of all
models, respectively3.
In terms of both PR and ROC curves, DRFI model
surprisingly outperforms all other models on seven benchmark datasets with large margins. Besides, RBD, DSR and
MC (solid lines with blue, yellow, and magenta colors, respectively) achieve close performance and perform slightly
better than other models.
Using the F-measure (i.e., Fβ), the ﬁve best models are:
DRFI, MC, RBD, DSR, and GMR, where DRFI model
consistently wins over all the 5 datasets. MC ranks the
second best over 2 datasets and the third best over 2
datasets. SR and SIM models perform the worst.
With respect to the AUC score, DRFI again ranks the
best over all seven datasets. Following DRFI, DSR model
ranks the second over 4 datasets. RBD ranks the second
on 1 dataset and the third on 2 datasets. While PCA ranks
the third on 1 dataset in terms of AUC score, it is not on
the list of top three contenders using Fβ measure. IT, SR,
and SUN achieve the worst performance. It is worth being
mentioned that all the models perform well above chance
level (AUC = 0.5) on seven benchmark datasets.
Rankings of models using MAE are more diverse than
either Fβ or AUC scores. DSR, RBD and DRFI rank on
the top, but none of them are among top three models over
JuddDB. MC, which performs well in terms of Fβ and
AUC, is not included in the top three models on any dataset.
PCA performs the best on JuddDB but worse on others.
SIM and SVO models perform the worst.
Using the Fw
β -measure., RBD, DRFI, and ST rank at
the top. Other top contenders here are: DSR, QCUT, RC
and HS. RBD model ranks better using this score than the
other ones.
On average, the compared ﬁxation prediction and object
proposal generation models perform worse than salient
object detection models. As two outliers, COV and BMS
outperform several salient object detection models in terms
of all evaluation metrics, implying that they are suitable
for detecting salient proto objects. Additionally, Fig. 12
shows the distribution of Fβ, ROC and MAE scores of all
salient object detection models versus all ﬁxation prediction
models over all benchmark datasets. We can see a sharp
separation of models especially for the Fβ score, where
most of the top models are salient object detection models.
This result is consistent with the conclusion in that
ﬁxation prediction models perform lower than salient object
detection models. Though stemming from ﬁxation prediction, research in salient object detection shares its unique
3Three segmentation methods are used, including adaptive threshold,
ﬁxed threshold, and SaliencyCut algorithm. The inﬂuence of segmentation
methods will be discussed in Sect. III-A
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 12, DECEMBER 2015
(a) MSRA10K
(b) PASCAL-S
(c) JuddDB
(d) DUT-OMRON
(e) THUR15K
Precision (vertical axis) and recall (horizontal axis) curves of saliency methods on 6 popular benchmark datasets.
properties and has truly added to what traditional saliency
models focusing on ﬁxation prediction already offer.
In particular, most of the salient object detection models
outperform the baseline AAM model. Among these 29
models, AAM only outperforms 1 model over MSRA10K,
8 models over ECSSD, 3 on THUR15K, 11 on JuddDB,
9 on PASCAL-S and 3 on DUT-OMRON in terms of
Fβ (Fixed). Interestingly, AAM model does not outperform
any model over SED2, which means that indeed there is
less center bias in this dataset and salient object detection
models can detect off-center objects. Notice that AAM
ranks lowest on SED2 compared to other datasets. Please
notice that it does not necessarily mean that models below
AAM are not good, as taking advantage of the location
prior may further enhance their performance MSRA10K
(b) PASCAL-S
(c) JuddDB
(d) DUT-OMRON
(e) THUR15K
ROC curves of models on 6 benchmarks. False and true positive rates are shown in x and y axes, respectively.
A. Analysis of Segmentation Methods
In many computer vision and graphics applications, segmenting regions of interest is of great practical importance
 , , – , , . The simplest way of
segmenting a salient object is to binarize the saliency map
using a ﬁxed threshold, which might be hard to choose.
In this section, we extensively evaluate two additional
most commonly used salient object segmentation methods,
including adaptive threshold and SaliencyCut .
Average Fβ scores for salient object segmentation results
on seven benchmark datasets are shown in Fig. 11. Each
segmentation algorithm was fed with saliency maps produced by all 41 compared models.
Except JuddDB, PASCAL-S and SED2 datasets, best
segmentation results are all achieved via SaliencyCut
method combined with a sophisticated salient object detection model (e.g., DRFI, RBD, MNP). This suggests
that enforcing label consistency in terms of using graphbased segmentation and global appearance statistics beneﬁts
salient object segmentations. The default SaliencyCut 
program only outputs the most dominate salient object, This
causes results for SED2, PASCAL-S and JuddDB benchmarks to be less optimal, as images in these two datasets
(see Fig. 3) do not follow the “single none ambiguous
salient object assumption” made in .
As also observed by most works in image segmentation
literature, nearby pixels with similar appearance tend to
have similar object labels. To validate this, we demonstrated
in Fig. 14(a) some better segmentation results by further
enforcing label consistency among nearby and similar pixels. Enforcing such label consistency often helps improve
labeling pixels specially when the majority of the salient
object pixels have been highlighted in the detection phase.
Challenging examples might still exist, however, such as
complex object topology, spindle components, and similar
appearance with respect to image background. More results
of using the best combination, DRFI saliency maps and
SaliencyCut segmentation, are demonstrated for images
with various complexities, as shown in Fig. 14(b).
A failure case of SaliencyCut segmentation along with
intermediate results is also shown in the last row of Fig.
14(a). Due to the complex topology of the salient object, label consistency in a local range considered in the
SaliencyCut algorithm may not work well. Additionally,
the appearance of the object looks very distinct due to the
existence of shading and reﬂection, which makes the segmentation of the whole object very challenging. Therefore,
only a part of the object is ﬁnally segmented.
B. Analysis of Center Bias
In this section, we study the center-bias challenge since
it has caused a major problem in evaluating ﬁxation prediction and salient object detection models. Some studies
usually add a Gaussian center prior to models when comparing them. This might not be fair as several salient object
detection models already contain center-bias at different
levels. Alternatively, we randomly choose 1000 images with
no/less center bias from the MSRA10K dataset. First, the
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 12, DECEMBER 2015
AUC: area under ROC curve (Higher is better. The top three
models are highlighted in red, green and blue).
MAE: Mean Absolute Error (Smaller is better. The top three
models are highlighted in red, green and blue).
distance of salient object centroid to the image center is
computed for each image. Those images for which such
distance is bigger than a threshold are then chosen. Some
sample images with no/less center-bias, as well as an
illustration of the threshold of choosing images, are shown
in Fig. 15. The average annotation of less center-biased
images shows two peaks on the left and on the right of
the image, which is suitable for testing the performance of
salient object detection models on off-center images.
We evaluate all the compared 41 models on these 1000
images. PR and ROC curves, Fβ, AUC, and MAE scores
are all shown in Fig. 16. DRFI and DSR again perform
the best. Overall, most models’ performance decrease when
testing on no/less center biased images (e.g., the AUC score
of MC declines from 0.951 to 0.888), while a few others
show increase. For example, the AUC score of SVO raises
from 0.930 to 0.942 and it gets the second ranking. Some
models, e.g., HS (with the second ranking in terms of Fβ
score), performs better according to their rank changes w.r.t
the whole MSRA10K dataset. DRFI still wins over other
models here with a large margin. The difference in Fβ,
AUC, and MAE scores are not very large for this model
over all data and 1000 less center-biased images (difference
are 0.05, 0.05, and 0.009, respectively). This means that this
model is not taking advantage of center-bias much. In the
contrast, CB model uses a great deal of location prior and
that is why it’s performance drops heavily when applied
to the off-center images .
Additionally, it can be observed from Fig. 2(f), there is
less center bias over the SED2 dataset where there is less
activation in the center of its average annotation map. We
can therefore study the center bias on it. Similarly, DRFI
and DSR outperform other models in terms of Fβ, AUC,
and MAE scores, indicating they are more robust to the
location variations of salient objects. HS again ranks second
according to the Fβ score. Fig. 17 shows best and worst
off-centered stimuli for DRFI and DSR models.
Overall, all the models perform well above the chance
level over either the less center-biased subset of MSRA10K
or SED2. It is also worth noticing that the AAM model
performs signiﬁcantly worse on these two datasets, as well
as JuddDB, validating our motivation of studying center
bias on them.
C. Analysis of Salient Object Existence
The existence of a salient object in the image is somewhat neglected by the community. Almost all of existing
salient object detection models assume that there is at least
one salient object in the input image. This impractical
assumption might lead to less optimal performance on
“background images”, which do not contain any dominant
salient objects, as studied in . Just recently, Zhang et
al. introduced a fast method for a more challenging
task of counting (subitizing) salient objects in a scene.
We can see from Fig. 18 that no dominated salient object
exists in background images consisting of only textures
or cluttered backgrounds. A good model should generate
a dark (blank) saliency map on a background image, i.e.,
without any activation as there are no salient objects. Fig.
18 shows saliency maps using three top salient object
detection models and a classical ﬁxation prediction model
on background images. Top salient object detection models
like DRFI, DSR, and MC do not perform well and often
generate activations on the background images even though
only regular textures exist (the second and third rows of Fig.
18). This is reasonable as they always assume there exist
salient objects in the input image and will try their best
to ﬁnd one. These models can be distracted by the clutter
in the background since high contrast always exist on the
cluttered region. Most of existing salient object detection
models compute saliency based on contrast values. These
cluttered regions are thus more likely considered as salient.
It is worth pointing out that ground truth of eye ﬁxations
do exist on such background images.
In addition to salient object existence, quantitative evaluations of models on background images is an open problem
as well. Note that it is not feasible to calculate PR and
ROC curves (and thus Fβ and AUC scores) on background
images since the ground truth positive labeling is empty.
MAE score is not informative either as most salient object
detection methods explicitly normalize the saliency maps in
the range of as a post-processing step. By demonstrating qualitative results of salient object detection models
on some background images, we aim to motivate future
works focusing on salient object detection on background
D. Analysis of Worst and Best Cases for Top Models
To understand what are the challenges for existing salient
object detection models, we illustrate the three best and the
three worst cases for top models over all seven benchmark
datasets. The stimuli for 11 top models were sorted according to the Fβ scores. We only give a demonstration of
DRFI and MC models in Fig. 19 due to limited space. See
our online challenge website for additional illustrations.
It can be noticed from Fig. 19 that models share the
same easy and difﬁcult stimuli. Both DRFI and MC perform
substantially well on the cases where a dominated salient
object exists in a relatively clean background. Since most
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 12, DECEMBER 2015
Fixed AdpT SCut Fixed AdpT SCut Fixed AdpT SCut Fixed AdpT SCut Fixed AdpT SCut Fixed AdpT SCut Fixed AdpT SCut
.660 .601 .671 .631 .580 .648 .455 .394 .459 .631 .577 .635 .868 .825 .896 .752 .690 .777 .818 .805 .768
.695 .654 .613 .651 .625 .620 .509 .454 .480 .683 .647 .647 .874 .843 .843 .779 .738 .747 .810 .801 .672
.604 .572 .611 .602 .571 .636 .412 .378 .422 .609 .572 .643 .837 .807 .877 .705 .669 .740 .822 .802 .758
.652 .607 .667 .596 .566 .618 .457 .403 .461 .630 .580 .647 .856 .821 .884 .718 .680 .757 .837 .825 .750
.596 .508 .604 .551 .509 .546 .418 .338 .378 .599 .540 .580 .816 .770 .830 .664 .583 .677 .798 .753 .639
.522 .510 .630 .495 .523 .603 .367 .337 .405 .467 .486 .576 .668 .724 .822 .568 .555 .709 .621 .778 .765
.606 .554 .622 .579 .557 .610 .432 .385 .433 .545 .541 .593 .842 .806 .862 .701 .654 .739 .742 .781 .729
.661 .622 .670 .610 .603 .600 .460 .420 .434 .627 .603 .615 .847 .824 .855 .742 .704 .745 .779 .803 .630
.646 .619 .650 .611 .604 .597 .454 .421 .410 .626 .614 .593 .835 .824 .833 .737 .717 .703 .794 .821 .632
.631 .586 .634 .612 .591 .643 .417 .368 .424 .604 .586 .637 .825 .804 .857 .722 .684 .735 .750 .750 .658
.535 .472 .553 .533 .517 .497 .384 .321 .342 .535 .528 .506 .794 .777 .780 .641 .612 .593 .729 .730 .616
.538 .525 .629 .519 .534 .618 .371 .353 .416 .482 .504 .609 .696 .714 .857 .586 .563 .738 .692 .776 .764
.593 .567 .634 .544 .558 .601 .432 .404 .368 .554 .554 .624 .782 .782 .845 .646 .627 .720 .754 .796 .701
.679 .615 .690 .670 .607 .674 .475 .419 .447 .665 .605 .669 .881 .838 .905 .787 .733 .801 .831 .839 .702
.643 .607 .654 .597 .594 .579 .454 .409 .432 .610 .591 .591 .847 .825 .839 .740 .712 .736 .773 .789 .643
.637 .559 .647 .585 .549 .602 .442 .358 .428 .616 .565 .616 .845 .800 .870 .731 .659 .769 .811 .776 .713
LMLC .555 .505 .614 .540 .519 .588 .375 .302 .397 .521 .493 .551 .801 .772 .860 .659 .600 .735 .653 .712 .674
.544 .488 .461 .500 .495 .342 .373 .319 .219 .519 .512 .377 .779 .759 .573 .619 .576 .378 .764 .794 .509
.619 .605 .534 .547 .575 .426 .424 .411 .333 .520 .555 .380 .717 .753 .534 .645 .655 .467 .617 .785 .174
.623 .561 .636 .581 .556 .615 .444 .375 .435 .542 .534 .593 .815 .775 .857 .717 .656 .761 .730 .704 .657
.586 .361 .621 .554 .441 .609 .414 .279 .419 .557 .407 .609 .789 .585 .863 .639 .357 .737 .744 .667 .746
.577 .523 .642 .528 .560 .649 .434 .386 .454 .478 .506 .613 .689 .705 .871 .624 .549 .781 .548 .714 .737
.423 .383 .464 .386 .401 .436 .286 .257 .280 .382 .380 .435 .677 .663 .740 .460 .441 .499 .736 .759 .646
.466 .351 .470 .610 .586 .639 .431 .370 .425 .599 .578 .621 .844 .820 .875 .741 .701 .776 .774 .807 .649
.534 .344 .627 .500 .425 .580 .376 .268 .393 .516 .450 .562 .697 .585 .812 .568 .408 .715 .704 .640 .669
.503 .485 .399 .478 .490 .200 .341 .324 .089 .476 .490 .193 .696 .711 .362 .530 .536 .203 .743 .783 .298
.489 .472 .586 .458 .494 .557 .353 .330 .394 .435 .458 .532 .621 .679 .748 .515 .494 .625 .591 .737 .565
.408 .367 .357 .386 .400 .238 .278 .250 .132 .381 .388 .259 .635 .628 .472 .434 .431 .257 .715 .734 .436
.326 .279 .265 .382 .431 .068 .227 .199 .049 .354 .383 .040 .520 .566 .014 .411 .410 .038 .684 .729 .140
.544 .444 .596 .498 .482 .593 .368 .282 .413 .481 .445 .578 .718 .681 .840 .574 .456 .698 .685 .723 .731
.617 .596 .624 .568 .578 .594 .434 .404 .416 .573 .576 .580 .805 .798 .822 .683 .659 .690 .713 .760 .627
.589 .604 .535 .510 .587 .398 .429 .427 .315 .486 .579 .373 .667 .755 .394 .641 .677 .413 .518 .724 .212
.469 .451 .552 .415 .482 .523 .344 .321 .397 .396 .443 .502 .572 .642 .675 .467 .441 .574 .533 .696 .641
.434 .407 .599 .372 .429 .568 .295 .292 .384 .358 .402 .539 .498 .585 .794 .433 .391 .672 .498 .685 .725
.433 .406 .566 .374 .419 .536 .316 .285 .388 .385 .411 .532 .542 .607 .755 .419 .391 .596 .521 .714 .702
.359 .294 .467 .387 .432 .486 .303 .291 .285 .321 .360 .445 .505 .596 .670 .388 .376 .478 .504 .661 .613
.447 .442 .497 .374 .457 .002 .279 .270 .001 .298 .363 .000 .473 .569 .001 .381 .385 .001 .504 .700 .002
.581 .567 .651 .526 .571 .650 .419 .396 .455 .507 .548 .638 .688 .737 .837 .624 .613 .765 .571 .746 .695
.450 .375 .593 .427 .461 .559 .317 .260 .360 .361 .377 .495 .555 .575 .750 .449 .357 .571 .541 .718 .693
.414 .453 .255 .373 .437 .005 .297 .283 .000 .378 .449 .005 .471 .586 .158 .407 .414 .003 .579 .697 .008
.549 .536 .578 .458 .569 .620 .392 .367 .411 .406 .514 .534 .580 .692 .779 .597 .627 .756 .388 .524 .640
Fβ statistics on each dataset, using varying ﬁxed thresholds, adaptive threshold, and SaliencyCut (Higher is better. The top three models are
highlighted in red, green and blue).
Histogram of AUC, MAE, and Mean Fβ scores for salient object detection models (blue) versus ﬁxation prediction models (red) collapsed
over all datasets.
existing salient object detection models do not utilize any
high-level prior knowledge, they may fail when a complex
scene has a cluttered background or when the salient object
is semantically salient . Another reason causing poor saliency
detection is object size. Both DRFI and MC models have
difﬁculty in detecting small objects (See hard cases on
DUT-OMRON and JuddDB).
Particularly, since saliency cues adopted by DRFI are
mainly based on contrast, this model fails on scenes where
salient objects share close appearance with the background
(e.g., the hard cases of MSRA10K and ECSSD). Another
possible reason is related to the failure in segmenting the
image. MC relies on the pseudo-background prior that the
image border areas are background. That is why it fails on
scenes where the salient object touches the image border,
e.g., the gorilla image in MSRA10K dataset (4th row of
the right column of Fig. 19).
E. Runtime Analysis
Runtime of compared models are shown in Fig. 1 over
all 10K images of MSRA10K (typical image resolution of
400 × 300) using an Intel Xeon E5645 2.40GHz CPU with
8 GB RAM. A 2D scatter plot of F w
β scores versus running
time of different methods based on the quantitative results
(a) Left to right: image, saliency map, AdpT: Adaptive
Threshold, SCut: SaliencyCut and gTruth: Ground Truth.
(b) DRFI model output fed to the SaliencyCut algorithm.
Samples of salient object segmentation results.
Left: Histogram of object center over all images, threshold
(red line = 0.247), and annotation map over 1000 less center-biased
images from MSRA10K dataset. Right: Four less center-biased images.
The overlaid circle illustrates the center-bias threshold.
of MSRA10K dataset is shown in Fig. 20, which is helpful
to demonstrate the trade-off between efﬁcacy and efﬁciency
of compared models.
Of all compared methods, the HC model is the fastest
(about 0.017 seconds per image) followed by GC and SR
models. The best model in our benchmark (DRFI) needs
about 0.697 seconds to process one image. We can also
observe that RC, GMR, MC, and RBD share similar tradeoffs between F w
β scores and runtime.
IV. DISCUSSIONS AND CONCLUSIONS
From the results obtained so far, we summarize in Fig.
21 the rankings of models based on average performance
over datasets in terms of segmentation methods, center
bias, salient object existence, and run time4. Based on the
rankings, we conclude that:
4We have created a uniﬁed repository for sharing code and data where
researchers can run models with a single click or can add new models for
benchmarking purposes. All codes, data, and results are available in our
online benchmark website: 
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 12, DECEMBER 2015
ST QCUT HDCT RBD GR MNP UFO MC DSR CHM GC LBI PCA DRFI GMR HS LMLC SF
.811 .791 .661
.805 .764 .776
.746 .697 .685 .750
.747 .621 .693
.943 .925 .912
.929 .888 .938
.920 .860 .910 .928
.885 .839 .872
.106 .183 .188
.128 .171 .117
.138 .164 .197 .162
.150 .160 .207
Method SVO SWD HC
RC SEG MSS CA
AC OBJ BMS COV
SIM SeR SUN SR
.521 .700 .744 .629 .666 .620 .671 .521 .708 .739
.463 .571 .515 .546 .498 .444 .590 .540 .460
.813 .898 .855 .828 .868 .896 .843 .800 .915 .879
.805 .852 .858 .849 .795 .750 .850 .836 .655
.291 .176 .177 .300 .167 .199 .183 .177 .243 .146
.176 .225 .363 .273 .276 .184 .208 .265 .165
Results of center-bias analysis over 1000 less center-biased images chosen from the MSRA10K dataset. Top: ROC and PR curves, Bottom:
Max Fβ, AUC, and MAE scores for all models.
Top and Bottom rows for each model illustrate best and worst
cases in off-centered images.
“DRFI, QCUT, RBD, ST, DSR, and MC are the top 6
models for salient object detection.”
To gauge the progress in this ﬁeld, we show in Fig. 22,
the maximum and average AUC and Fβ scores of different
salient object detection methods versus their publication
years. We ﬁnd a continuous ascending success rate over
the last couple of years which raises the hope that even
better salient object detection models are possible in the
By investigating the performances and the design choices
of all compared models, our extensive evaluations do suggest some clear messages about commonly used design
choices, which could be valuable for developing future
algorithms. We refer readers to our recent survey for a
comprehensive review of different design choices adopted
for salient object detection.
Sample background-only images and prediction maps of DRFI,
DSR, MC, and IT models.
• From the elements perspective, top ﬁve models (except QCUT) are built upon superpixels (regions). On
the one hand, compared with pixels, more effective
features (e.g., color histogram) can be extracted from
regions. On the other hand, compared with patches,
the boundary of the salient object is better preserved
for region-based approaches, leading to more accurate
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 12, DECEMBER 2015
Best (1st rows for each model on a dataset) and worst (2nd rows) cases of DRFI and MC. Ground-truth object(s) is denoted by a red contour.
Run Time (log scale)
β scores versus (log scale of) runtime of different methods
based on the quantitative results of MSRA10K dataset.
detection performance. Moreover, since the number of
superpixels is far less than the number of pixels or
patches, region-based methods has the potential to run
• All the top six models explicitly consider the background prior, which assumes that the area in the
narrow border of the image belongs to the background.
Compared with the location prior of a salient object,
such a background prior performs more robust.
• The leading method in our benchmark (i.e., DRFI),
discriminatively trains a regression model to predict
region saliency according to a 93-dimensional feature
vector. Instead of purely relying on the cues extracted
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 12, DECEMBER 2015
Overall Rank
Overall Rank
Summary rankings of models under different evaluation metrics over all datasets (excluding SED2). The overall rankings of different methods
are computed based on the average (the higher the better) of AUC, (1-MAE), Max Fβ, AdpT, ScutT, and F w
β scores. The top three models under
each evaluation metric are highlighted in red, green and blue.
Published Years
Average AUC
Average Fβ
Maximum and average AUC and Fβ scores of different salient
object methods versus their publication years. Model accuracy shows an
increasing trend.
only from the input image, DRFI resorts to human annotations to automatically discover feature integration
rules. The high performance of this simple learningbased method encourages pursuing data-driven approaches for salient object detection.
However, even considering top performing models,
salient object detection still seems far from being solved. To
achieve more appealing results, three challenges should be
addressed. First, in our large-scale benchmark (see Sec. II),
all top performing algorithms use the location prior cues,
limiting their adaptation to general cases. Second, although
the ranking of top scoring models are quite consistent
across datasets, performance scores (Fβ and AUC) drop
signiﬁcantly from easier datasets to more difﬁcult ones.
The third challenge regards the run time of models. Some
models need around one minute to process a 400 × 300
image (e.g., CA: 40.9s, SVO: 56.5s, and LMLC 140s).
One area for future research would be designing scores
for tackling dataset biases and evaluation of saliency segmentation maps with respect to ground-truth annotations
similar to . In this benchmark, we only focused
on single-input scenarios. Although some RGBD datasets
exist , benchmark datasets for multiple input images (e.g., salient object detection on videos, co-salient
object detection ) are still lacking. Another future
direction will be following active segmentation algorithms
(e.g., , , ) by segmenting a salient object
from a seed point. For example, a simple model proposed
by Borji which segments the most salient object (at
the peak of a map generated by a ﬁxation prediction model
as the seed point) using superpixels outperforms several
salient object detection models on scenes with multiple
salient objects (JuddDB). This indicates that several models
are affected by a bias imposed by some former datasets
(i.e., ASD) which is the existence of only one object in
the image. Aggregation of saliency models for building a
strong prediction model (similar to , , , and
behavioral investigation of saliency judgments by humans
(e.g., , ) are two other interesting directions.
The relationship (similarity and difference) between salient
object detection and related ﬁelds such as object detection,
object proposals, general segmentation, and ﬁxation prediction5 and the ways these areas can beneﬁt from each other
still remains to be explored further.
Inspired by the overwhelming performance of deep
learning methods in other vision tasks like image classiﬁcation , and object detection , deep
convolutional neural networks (CNNs) are also studied
in recent works – . The leading performance of
DRFI demonstrates the effectiveness of data-driven feature
integration. Through deep architectures, more powerful
representations can be learned than hand-crafted features
for salient object detection tasks even if CNNs are trained
for image classiﬁcation. It indicates the promising direction
of investigating deep learning methods for salient object
detection in the future.
Saliency models play an important role in the way we represent
and understand scenes at the high level. Saliency models
continue to be useful in a variety of domains encompassing
human-robot interaction, image processing, and computer
vision. So far modeling effort has been focused on improving the performance of existing datasets. State of the
art models do very well even on large scale salient object
datasets. We believe that it is now the time to consider
how saliency detection can help other challenging tasks in
computer vision for problems such as describing a scene
(e.g., language and vision – ), scene understanding (e.g., , – ), and even object and scene
classiﬁcation (e.g., , , ).
Salient object detection is a very active research area in
computer vision with several papers emerging each year in
major conferences and journals. In fact, several models have
been introduced since the initial submission of this work.
Some, we have included in our benchmark6 during the
review process and some newer ones (such as – 
mainly based on the deep CNNs) will be considered in our
online saliency detection benchmark. We will extensively
review and discuss these models in our ongoing work .
ACKNOWLEDGMENT
Authors would like to thank anonymous reviewers for
their helpful comments on the paper. Ali Borji was supported by Defense Advanced Research Projects Agency
(NO. HR0011-10-C-0034), the National Science Foundation (CRCNS grant number BCS-0827764), the General
Motors Corporation, and the Army Research Ofﬁce (NO.
W911NF-08-1-0360). Ming-Ming Cheng is supported by
the grants from NSFC (NO. 61572264). Jia Li is supported
by the grants from NSFC (NO. 61370113), and Fundamental Research Funds for the Central Universities.