TIKHONOV REGULARIZATION AND TOTAL LEAST SQUARES∗
GENE H. GOLUB†, PER CHRISTIAN HANSEN‡, AND DIANNE P. O’LEARY§
SIAM J. MATRIX ANAL. APPL.
⃝1999 Society for Industrial and Applied Mathematics
Vol. 21, No. 1, pp. 185–194
Abstract. Discretizations of inverse problems lead to systems of linear equations with a highly
ill-conditioned coeﬃcient matrix, and in order to compute stable solutions to these systems it is
necessary to apply regularization methods. We show how Tikhonov’s regularization method, which
in its original formulation involves a least squares problem, can be recast in a total least squares
formulation suited for problems in which both the coeﬃcient matrix and the right-hand side are
known only approximately. We analyze the regularizing properties of this method and demonstrate
by a numerical example that, in certain cases with large perturbations, the new method is superior
to standard regularization methods.
Key words. total least squares, discrete ill-posed problems, regularization, bidiagonalization
AMS subject classiﬁcations. 65F20, 65F30
PII. S0895479897326432
1. Introduction. In this paper we study a class of methods for producing an
approximate solution to a linear system of equations A x ≈b, where A is m × n with
m ≥n. We assume that the elements of A and those of b are contaminated by some
noise. An appropriate statement of the problem in the case of noisy data is the total
least squares (TLS) formulation , [6, section 12.3], :
∥(A , b) −( ˜A , ˜b)∥F
subject to
˜b = ˜A x.
In contrast, if the elements of A are exact and only b contains noise, then the corresponding formulation is the least squares (LS) problem
Our focus in this work is on very ill-conditioned problems where the singular values
of A decay gradually to zero. Such problems arise, for example, from the discretization
of ill-posed problems such as integral equations of the ﬁrst kind. See, e.g., , , ,
 , and for examples and details.
In these problems, the solutions to the two formulations in (1) and (2) can be
hopelessly contaminated by the noise in directions corresponding to small singular
values of A or (A , b). Because of this, it is necessary to compute a regularized solution
in which the eﬀect of such noise is ﬁltered out. For LS problems, the ﬁltering is often
done by Tikhonov’s regularization method, where a damping is added to each singular
value decomposition (SVD) component of the solution, thus eﬀectively ﬁltering out
∗Received by the editors August 25, 1997; accepted for publication (in revised form) by J. Varah
October 30, 1998; published electronically August 26, 1999.
 
†Department of Computer Science, Stanford University, Stanford, CA 94305 (golub@sccm.
stanford.edu). The work of this author was supported by National Science Foundation grant CCR
‡Department of Mathematical Modelling, Technical University of Denmark, Building 321, DK-
2800 Lyngby, Denmark ( ).
§Department of Computer Science and Institute for Advanced Computer Studies, University of
Maryland, College Park, MD 20742 ( ). The work of this author was supported by
National Science Foundation grant CCR 95-03126 and by a General Research Board grant from the
Oﬃce of Graduate Studies and Research of the University of Maryland.
G. H. GOLUB, P. C. HANSEN, AND D. P. O’LEARY
the components corresponding to the small singular values. Alternatively, one can
truncate the SVD expansion of the solution, leaving out all the SVD components
corresponding to the small singular values.
For the TLS problem, the truncation approach has already been studied by Fierro
et al. . In the present work we focus on the Tikhonov regularization approach for
TLS. We thus arrive at a new regularization method in which stabilization enters the
formulation in a natural way and which is able to produce regularized solutions with
superior properties for certain problems in which the perturbations are large.
Our paper is organized as follows. In section 2 we introduce the regularized TLS
method and we study its regularizing properties. Computational aspects are described
in section 3, and we conclude the paper with a numerical example in section 4. A
preliminary report on this work appeared as .
2. The regularized TLS method. Our regularization of the TLS problem is
based on Tikhonov regularization. For the linear LS problem (2), a general version of
Tikhonov’s method takes the form
2 + λ ∥L x∥2
where λ is a positive constant chosen to control the size of the solution vector and
L is a matrix that deﬁnes a (semi)norm on the solution through which the “size”
is measured . Often, L represents the ﬁrst or second derivative operator. If L is
the identity matrix, then the Tikhonov problem is said to be in standard form. The
solution xλ to (3) solves the problem
(AT A + λ LT L)x = AT b.
As λ increases, the (semi)norm ∥L x∥2 of the solution vector decreases monotonically
while the residual ∥A x −b∥2 increases monotonically.
Tikhonov regularization has an important equivalent formulation as
min ∥A x −b∥2
subject to
∥L x∥2 ≤δ,
where δ is a positive constant. Problem (5) is an LS problem with a quadratic constraint and, using the Lagrange multiplier formulation,
L(x, λ) = ∥A x −b∥2
it can be shown that if δ ≤∥xLS∥2, where xLS is the LS solution to (2), then the
solution xδ to (5) is identical to the solution xλ to (3) for an appropriately chosen λ,
and there is a monotonic relation between the parameters δ and λ.
To carry this idea over to the TLS setting, we add an upper bound on the
(semi)norm ∥L x∥2 of the solution vector x in the TLS problem (1). The formulation of the regularized TLS (R-TLS) problem thus becomes
min ∥(A , b) −( ˜A , ˜b)∥F
subject to
˜b = ˜A x,
∥L x∥2 ≤δ,
and the corresponding Lagrange multiplier formulation is
ˆL( ˜A, x, µ) = ∥(A , b) −( ˜A , ˜A x)∥2
F + µ (∥L x∥2
where µ is the Lagrange multiplier, zero if the inequality constraint is inactive. The
solution ¯xδ to this problem is diﬀerent from the solution xTLS to (1) whenever δ is
TIKHONOV REGULARIZATION AND TOTAL LEAST SQUARES
less than ∥L xTLS∥2. The two solutions ¯xδ and xδ to the two regularized problems in
(5) and (7) have a surprising relationship, explained by the following theorem.
Theorem 2.1. The R-TLS solution ¯xδ to (7), with the inequality constraint replaced by equality, is a solution to the problem
(AT A + λIIn + λLLT L) x = AT b,
where the parameters λI and λL are given by
λI = −∥b −A x∥2
λL = µ (1 + ∥x∥2
and where µ is the Lagrange multiplier in (8). The two parameters are related by
λL δ2 = bT (b −A x) + λI.
Moreover, the TLS residual satisﬁes
∥(A , b) −( ˜A , ˜b)∥2
Proof. Deﬁne ˜r ≡b −˜b = b −˜A x. We characterize the solution to (7) by setting
the partial derivatives of the Lagrangian ˆL (8) to zero. Diﬀerentiation with respect to
the entries in ˜A yields
˜A −A −˜rxT = 0,
and diﬀerentiation with respect to the entries in x yields
−˜AT ˜r + µ LT L x = 0
( ˜AT ˜A + µ LT L) x = ˜AT b.
Since A = ˜A −˜rxT and ˜AT ˜r = µ LT L x, we see that
AT A = ( ˜AT −x˜rT )( ˜A −˜rxT )
= ˜AT ˜A −µ xxT LT L + x˜rT ˜rxT −µ LT L xxT
˜AT b = AT b + x˜rT b.
Gathering terms, we arrive at (9), with
λI = µ δ2 −∥˜r∥2
λL = µ (1 + ∥x∥2
To simplify the expression for λI, we ﬁrst rewrite ˜r as
˜r = b −˜A x = b −(A + ˜r xT ) x = b −A x −˜r xT x,
G. H. GOLUB, P. C. HANSEN, AND D. P. O’LEARY
from which we obtain the relation
˜r (1 + ∥x∥2
2) = b −A x
and, therefore, (1 + ∥x∥2
2 = (1 + ∥x∥2
2)−1∥b −A x∥2
2. In (14), multiplication from
the left by xT leads to
µ = xT ˜AT ˜r
xT LT Lx = bT ˜r −∥˜r∥2
Inserting (15) and (16) into the above expression for λI, we obtain (10). Equation
(12) is proved by multiplying λL by δ2 and inserting (15) and (16). Finally, to prove
the expression for the TLS residual, we use the relation
(A , b) −( ˜A , ˜b) = (A −˜A , b −˜b) = (−˜r xT , ˜r) = −˜r
Taking the Frobenius norm and using (15), we obtain (13).
This theorem characterizes the solution to the problem in (7) whenever δ is small
enough that the constraint on Lx is binding. When the solution ¯xδ to (7) satisﬁes
∥L¯xδ∥2 < δ(with L = 0 as a special case), then (7) is just the standard TLS problem.
Below, we discuss the implications of this theorem for both the standard-form
case L = In (the identity matrix) and the general-form case L ̸= In, both of which
are important in applications.
2.1. The standard-form case. In the standard-form case, the problems simplify considerably. The Tikhonov problem (5) becomes
min ∥A x −b∥2
subject to
with solution xδ satisfying
(AT A + λ In)xδ = AT b
for some value of λ ≥0. Similarly, the R-TLS problem takes the form
min ∥(A , b) −( ˜A , ˜b)∥F
subject to
˜b = ˜A x,
and (9) says that the solution ¯xδ satisﬁes
(AT A + λILIn) x = AT b
whenever ∥¯xδ∥2 > δ, with λIL = λI + λL. Clearly, these two solutions are closely
Theorem 2.2. Let L = In and let ¯σn+1 denote the smallest singular value of
(A , b). Then, for each value of δ, the resulting solutions ¯xδ and xδ are related as in
the following chart.
δ < ∥xLS∥2
δ = ∥xLS∥2
¯xδ = xδ = xLS
∥xLS∥2 < δ < ∥xTLS∥2
¯xδ ̸= xδ = xLS
0 > λIL > −¯σ2
δ ≥∥xTLS∥2
¯xδ = xTLS, xδ = xLS
λIL = −¯σ2
TIKHONOV REGULARIZATION AND TOTAL LEAST SQUARES
i−th component
Fig. 1. Regularized solutions to (3) for various values of the regularization parameter λ; see
Table 1 for details.
The solutions shown in Figure 1.
Solution norm
solid line
dashed line
dotted line
Proof. Consider the LS problem (17) ﬁrst. If δ ≥∥xLS∥, then the constraint is
not binding and the solution to the problem is xLS, with λ = 0. As δ is reduced
below ∥xLS∥, the value of λ increases monotonically, as can be seen by using the SVD
i in (18) to obtain
and then taking norms.
Similarly, as long as δ ≥∥xTLS∥, the constraint in (19) is not binding, and in
this case we have the standard TLS problem with λIL = −¯σ2
n+1 [17, Theorem 2.7].
As δ is reduced below ∥xTLS∥, (21) again shows that the λIL value must increase
monotonically in order to reduce the solution norm.
Noting that ∥xLS∥≤∥xTLS∥[17, Corollary 6.2], the theorem’s conclusion follows.
In regularization problems, the norm ∥xLS∥2 of the LS solution is normally very
large due to the errors; thus, one will choose δ ≤∥xLS∥2. When this is the case, we
conclude from Theorem 2.2 that R-TLS produces solutions that are identical to the
Tikhonov solutions. In other words, replacing the LS residual with the TLS residual
in the Tikhonov formulation has no eﬀect when L = In and δ ≤∥xLS∥2.
We remark that since ∥xTLS∥2 ≥∥xLS∥2, there is usually a nontrivial set of
“large” values of δ for which the multiplier λIL is negative. The corresponding R-TLS
solutions ¯xδ are distinctly diﬀerent from the Tikhonov solutions and can be expected
to be even more dominated by errors than the LS solution xLS.
We illustrate Theorem 2.2 with an example: discretization of a Fredholm integral
equation with the second derivative operator as kernel. The implementation is deriv2
from , the size of the matrix A is 64 × 32, and both A and b are perturbed by
G. H. GOLUB, P. C. HANSEN, AND D. P. O’LEARY
Gaussian noise with zero mean and standard deviation 10−5. We have ¯σ2
n+1 ≈2.38 ·
10−9. Figure 1 shows the solutions listed in Table 1. We see that both xLS and xTLS
have large oscillations (the oscillations of xTLS being larger than those of xLS), while
the regularized LS solution xλ corresponding to λ = 10−5 is much smoother. The
R-TLS solution, corresponding to λ = −1
n+1 = −1.19 · 10−9, has large oscillations
whose amplitude is between that of xTLS and xLS.
We conclude that if we use Tikhonov regularization with L = I in order to reduce
the norm of the solution below ∥xLS∥2, then the R-TLS formulation produces the
same solution as the regularized LS formulation. This is in contrast to the truncating
approach, since we have shown in that truncated TLS can be superior to truncated
2.2. The general-form case. In many applications, it is necessary to choose
a matrix L diﬀerent from the identity matrix; these issues are discussed, e.g., in [10,
section 4.3]. In this case, the R-TLS solution ¯xδ is diﬀerent from the Tikhonov solution
whenever the residual b−Ax is diﬀerent from zero, since both λI and λL are nonzero.
From Theorem 2.1 we notice that λL is always positive when δ < ∥xTLS∥2, because
the Lagrange parameter µ is positive for these values of λ. On the other hand, λI is
always negative and thus adds some deregularization to the solution. Statistical aspects
of a negative regularization parameter in Tikhonov’s method are discussed in .
For a given δ, there are usually several pairs of parameters λI and λL (and thus
several solutions x) that satisfy relations (9)–(11), but only one of these solves the
optimization problem (7). According to (13), this is the solution that corresponds to
the smallest value of |λI|. The following relations hold.
Theorem 2.3. For a given value of δ, the solution ¯xδ to the R-TLS problem is
related to the solution to the TLS problem xTLS as in the following chart.
δ < ∥L xTLS∥2
¯xδ ̸= xTLS
∂λI/∂δ > 0
δ ≥∥L xTLS∥2
¯xδ = xTLS
Proof. For δ < ∥L xTLS∥2, the inequality constraint is binding and the Lagrange
multiplier µ is positive, since this is a necessary condition for optimality; see, for
example, . Thus λL is positive. The residual (13) is monotonically decreasing as
δ increases, since optimal solutions for smaller values of δ are candidate solutions for
larger δs, so λI is monotonically increasing.
For δ = ∥L xTLS∥2, the Lagrange multiplier is zero, and the solution becomes the
unconstrained minimizer xTLS. The value −¯σ2
n+1 follows from [5, Theorem 4.1]. The
constraint is never again binding for larger δ, so the solution to the problem remains
unchanged.
We note that if the matrix λIIn + λLLT L is positive deﬁnite, then the R-TLS
solution corresponds to a Tikhonov solution for which the expression λ ∥L x∥2 in (3)
is replaced with the norm (λI∥x∥2
2 + λL∥L x∥2
2)1/2. If λIIn + λLLT L is indeﬁnite or
negative deﬁnite then there is no equivalent interpretation.
3. Computational aspects. To compute the R-TLS solutions for L ̸= In, we
have found it most convenient to avoid explicit use of δ; instead we use λL as the
free parameter, ﬁxing its value and then computing the value of λI that satisﬁes (10)
and is smallest in absolute value. The corresponding value of δ can then easily be
computed from relation (12).
TIKHONOV REGULARIZATION AND TOTAL LEAST SQUARES
We now discuss how to solve (9) eﬃciently for many values of λI and λL. First,
we note that the equation is equivalent to the augmented system
Our algorithm is based on this formulation.
We assume that the matrix L is a banded matrix, often the case when L represents
a derivative operator. The key to eﬃciency, then, is to reduce A to n × n bidiagonal
form B by means of orthogonal transformations: HT A K = B. The orthogonal right
transformations also should be applied to L, and simultaneously we should apply
orthogonal transformations to L from the left in order to maintain its banded form. It
is convenient to use sequences of Givens transformations to form J, H, and K, since
this gives us the most freedom to retain the banded form of C = JT L K.
Once B and C have been computed, we can recast the augmented system in (22)
in the following form:
Since λI changes more frequently than λL in our approach, we will now use Givens
rotations to annihilate λ1/2
L C using B by means of Elden’s algorithm [1, section 5.3.4],
which can be represented as
When we insert this G into the augmented system (23), it becomes
The middle block row is now decoupled, and we obtain
Finally, we apply a symmetric perfect shuﬄe reordering
n + 1, 1, n + 2, 2, n + 3, 3, . . . , n, 2n
to the rows and columns of the above matrix, to obtain a symmetric, tridiagonal,
indeﬁnite matrix of size 2n × 2n:
Up until now, we have used only orthogonal transformations in order to preserve
numerical stability. We can solve this permuted system in a stable way using a general
tridiagonal solver (e.g., Gauss elimination with partial pivoting).
G. H. GOLUB, P. C. HANSEN, AND D. P. O’LEARY
4. Numerical results. In this section we present a numerical example that
demonstrates the usefulness of the R-TLS method. Our computations are carried out
in MATLAB using the Regularization Tools package .
It is a generally accepted fact that, for small noise levels, we should not expect
the ordinary TLS solution to diﬀer much from the ordinary LS solution; see . The
same observation is made in for the T-TLS solution, and the numerical results
presented below also support this observation for the R-TLS method. We emphasize
that the precise meaning of “small” depends on the particular problem.
In several test problems from , the R-TLS algorithm was able to compute better
results than Tikhonov’s method. The test problem we have chosen to illustrate the
R-TLS algorithm is the one for which the improvement is most dramatic. This test
problem is a discretization by means of Gauss–Laguerre quadrature of the inverse
Laplace transform
exp(−s t) f(t) dt = 1
f(t) = 1 −exp(−t 4/25),
originating from and implemented in the function ilaplace(n,2) in , except that
the constant 4/25 was chosen instead of 1/2. The matrix L approximates the ﬁrstderivative operator. The dimensions are m = n = 16, the matrix A and the exact
solution x∗are scaled such that ∥A∥F = ∥A x∗∥2 = 1, and the perturbed right-hand
side is generated as
b = (A + σ∥E∥−1
F E)x∗+ σ∥e∥−1
where the elements of the perturbations E and e are from a normal distribution with
zero mean and unit standard deviation.
Figure 2 shows the relative errors ∥x∗−xδ∥2/∥x∗∥2 and ∥x∗−¯xδ∥2/∥x∗∥2 in the
Tikhonov and R-TLS solutions, respectively, for four values of the noise level:
σ = 0.001, 0.01, 0.1, 1.
We see that, for small values of σ, the two methods lead to almost the same minimum relative error, for almost the same value of λL. However, for larger values of σ,
the minimum relative error for the R-TLS method is clearly smaller than that for
Tikhonov’s method, and it occurs for a larger value of λL. This shows the potential
advantage of the R-TLS method, provided, of course, that a good estimate of the
optimal regularization parameter can be found. This topic is outside the scope of the
current paper.
In Figure 3 we have plotted the “optimal” Tikhonov and R-TLS solutions, deﬁned
as the solutions that correspond to the minima of the curves in Figure 2. In addition,
we have plotted the exact solution x∗. Clearly, the addition of the term λIIn in (9)
introduces a nonconstant component in the right part of the plot of the regularized
solution; it is precisely this component that improves the R-TLS error compared to
the Tikhonov error.
Conclusion. We have proved that the R-TLS solution to the regularized form
(7) of the TLS problem is closely related to the Tikhonov solution to (3). In the
standard-form case, L = In, the two solutions are identical, showing that Tikhonov
TIKHONOV REGULARIZATION AND TOTAL LEAST SQUARES
Relative errors
Relative errors
Relative errors
Relative errors
Fig. 2. Plots of the relative errors in the Tikhonov solutions (dashed lines) and R-TLS solutions
(solid lines) versus λL for four values of the noise level.
i−th component
λI = −1.7125e−09
λL = 0.0016156
i−th component
λI = −1.1075e−07
λL = 0.017783
i−th component
λI = −3.6062e−05
λL = 0.5109
i−th component
λI = −0.00037492
λL = 2.1544
Fig. 3. Plots of the “optimal” Tikhonov solutions (dashed lines) and R-TLS solutions (solid
lines) for four values of the noise level. Also shown is the exact solution (dotted lines).
G. H. GOLUB, P. C. HANSEN, AND D. P. O’LEARY
regularization in this case is also suited for problems with a perturbed coeﬃcient
matrix. For general problems with L ̸= In, the R-TLS solution is diﬀerent from the
corresponding Tikhonov solution in that a deregularizing term λIIn with a negative
parameter λI is added to the formulation in (4).