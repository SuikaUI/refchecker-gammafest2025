Proceedings of NAACL-HLT 2018, pages 114–120
New Orleans, Louisiana, June 1 - 6, 2018. c⃝2017 Association for Computational Linguistics
Pieces of Eight: 8-bit Neural Machine Translation∗
Jerry Quinn
Miguel Ballesteros
IBM Research,
1101 Kitchawan Road, Route 134 Yorktown Heights, NY 10598. U.S
 , 
Neural machine translation has achieved levels of ﬂuency and adequacy that would have
been surprising a short time ago. Output quality is extremely relevant for industry purposes,
however it is equally important to produce results in the shortest time possible, mainly for
latency-sensitive applications and to control
cloud hosting costs. In this paper we show the
effectiveness of translating with 8-bit quantization for models that have been trained using 32-bit ﬂoating point values. Results show
that 8-bit translation makes a non-negligible
impact in terms of speed with no degradation
in accuracy and adequacy.
Introduction
Neural machine translation (NMT) has recently
achieved remarkable performance improving ﬂuency and adequacy over phrase-based machine
translation and is being deployed in commercial
settings . However,
this comes at a cost of slow decoding speeds compared to phrase-based and syntax-based SMT (see
section 3).
NMT models are generally trained using 32-bit
ﬂoating point values. At training time, multiple
sentences can be processed in parallel leveraging
graphical processing units (GPUs) to good advantage since the data is processed in batches. This is
also true for decoding for non-interactive applications such as bulk document translation.
Why is fast execution on CPUs important?
First, CPUs are cheaper than GPUs. Fast CPU
computation will reduce commercial deployment
costs. Second, for low-latency applications such
as speech-to-speech translation , it is important to translate individual sentences quickly enough so that users can have an
application experience that responds seamlessly.
Translating individual sentences with NMT requires many memory bandwidth intensive matrixvector or matrix-narrow matrix multiplications
 . In addition, the batch
size is 1 and GPUs do not have a speed advantage over CPUs due to the lack of adequate parallel work ).
Others have successfully used low precision approximations to neural net models.
et al. explored 8-bit quantization for feedforward neural nets for speech recognition. Devlin
 explored 16-bit quantization for machine
translation. In this paper we show the effectiveness of 8-bit decoding for models that have been
trained using 32-bit ﬂoating point values. Results
show that 8-bit decoding does not hurt the ﬂuency
or adequacy of the output, while producing results
up to 4-6x times faster. In addition, implementation is straightforward and we can use the models
as is without altering training.
The paper is organized as follows: Section 2
reviews the attentional model of translation to be
sped up, Section 3 presents our 8-bit quantization
in our implementation, Section 4 presents automatic measurements of speed and translation quality plus human evaluations, Section 5 discusses the
results and some illustrative examples, Section 6
describes prior work, and Section 7 concludes the
The Attentional Model of Translation
Our translation system implements the attentional
model of translation consisting of an encoder-decoder network with an at-
tention mechanism.
The encoder uses a bidirectional GRU recurrent neural network to encode
a source sentence x = (x1, ..., xl), where xi is
the embedding vector for the ith word and l is the
sentence length. The encoded form is a sequence
of hidden states h = (h1, ..., hl) where each hi is
computed as follows
"←−f (xi, ←−h i+1)
−→f (xi, −→h i−1)
h0 = 0. Here ←−f and −→f are GRU
Given h, the decoder predicts the target translation y by computing the output token sequence
(y1, ...ym), where m is the length of the sequence.
At each time t, the probability of each token yt
from a target vocabulary is
p(yt|h, yt−1..y1) = g(st, yt−1, Ht),
where g is a two layer feed-forward network over
the embedding of the previous target word (yt−1),
the decoder hidden state (st), and the weighted
sum of encoder states h (Ht), followed by a softmax to predict the probability distribution over the
output vocabulary.
We compute st with a two layer GRU as
t = r . Ht is computed as
i=1(αt,i · ←−h i)
i=1(αt,i · −→h i)
where αt,i are the elements of αt which is the output vector of the attention model. This is computed with a two layer feed-forward network
t = v(tanh(w(hi) + u . Generally models are trained with batch
sizes ranging from 64 to 128 and unbiased Adam
stochastic optimizer . We
use an embedding size of 620 and hidden layer
sizes of 1000. We select model parameters according to the best BLEU score on a held-out development set over 10 epochs.
8-bit Translation
Our translation engine is a C++ implementation.
The engine is implemented using the Eigen matrix library, which provides efﬁcient matrix operations.
Each CPU core translates a single sentence at a time. The same engine supports both
batch and interactive applications, the latter making single-sentence translation latency important.
We report speed numbers as both words per second (WPS) and words per core second (WPCS),
which is WPS divided by the number of cores running. This gives us a measure of overall scaling
across many cores and memory buses as well as
the single-sentence speed.
Phrase-based SMT systems, such as , for English-German run at 170 words per
core second (3400 words per second) on a 20 core
Xeon 2690v2 system.
Similarly, syntax-based
SMT systems, such as , for the same language pair run at 21.5
words per core second (430 words per second).
In contrast, our NMT system (described in Section 2) with 32-bit decoding runs at 6.5 words per
core second (131 words per second). Our goal is
to increase decoding speed for the NMT system to
what can be achieved with phrase-based systems
while maintaining the levels of ﬂuency and adequacy that NMT offers.
Benchmarks of our NMT decoder unsurprisingly show matrix multiplication as the number
one source of compute cycles. In Table 1 we see
that more than 85% of computation is spent in
Eigen’s matrix and vector multiply routines (Eigen
matrix vector product and Eigen matrix multiply).
It dwarfs the costs of the transcendental function
computations as well as the bias additions.
Given this distribution of computing time, it
makes sense to try to accelerate the matrix operations as much as possible. One approach to increasing speed is to quantize matrix operations.
Replacing 32-bit ﬂoating point math operations
with 8-bit integer approximations in neural nets
has been shown to give speedups and similar ac-
Eigen matrix vector product
Eigen matrix multiply
NMT decoder layer
Eigen fast tanh
NMT tanh wrapper
Table 1: Proﬁle before 8-bit conversion. More than
85% is spent in Eigen matrix/vector multiply routines.
curacy . We chose to apply similar optimization to our translation system,
both to reduce memory trafﬁc as well as increase
parallelism in the CPU.
Our 8-bit matrix multiply routine uses a naive
implementation with no blocking or copy.
code is implemented using Intel SSE4 vector instructions and computes 4 rows at a time, similar
to . Simplicity led to implementing
8-bit matrix multiplication with the results being
placed into a 32-bit ﬂoating point result. This has
the advantage of not needing to know the scale of
the result. In addition, the output is a vector or
narrow matrix, so little extra memory bandwidth
is consumed.
Multilayer matrix multiply algorithms result in
signiﬁcantly faster performance than naive algorithms .
This is due to
the fact that there are O(N3) math operations on
O(N2) elements when multiplying NxN matrices, therefore it is worth signiﬁcant effort to minimize memory operations while maximizing math
operations. However, when multiplying an NxN
matrix by an NxP matrix where P is very small
(<10), memory operations dominate and performance does not beneﬁt from the complex algorithm. When decoding single sentences, we typically set our beam size to a value less than 8 following standard practice in this kind of systems
 . We actually ﬁnd that
at such small values of P, the naive algorithm is a
bit faster.
8-bit matrix multiply
Eigen fast tanh
NMT decoder layer
NMT tanh wrapper
Table 2: Proﬁle after 8-bit conversion. Matrix multiply
includes matrix-vector multiply. Matrix multiply is still
70% of computation. Tanh is larger but still relatively
Table 2 shows the proﬁle after converting
the matrix routines to 8-bit integer computation.
There is only one entry for matrix-matrix and
matrix-vector multiplies since they are handled by
the same routine. After conversion, tanh and sigmoid still consume less than 7% of CPU time. We
decided not to convert these operations to integer
in light of that fact.
It is possible to replace all the operations with
8-bit approximations , but this
makes implementation more complex, as the scale
of the result of a matrix multiplication must be
known to correctly output 8-bit numbers without
dangerous loss of precision.
1000x1000 with a range of values [−10, 10],
the individual dot products in the result could be
as large as 108. In practice with neural nets, the
scale of the result is similar to that of the input
matrices. So if we scale the result to [−127, 127]
assuming the worst case, the loss of precision will
give us a matrix full of zeros. The choices are to
either scale the result of the matrix multiplication
with a reasonable value, or to store the result as
ﬂoating point. We opted for the latter.
8-bit computation achieves 32.3 words per core
second (646 words per second), compared to the
6.5 words per core second (131 words per second)
of the 32-bit system (both systems load parameters
from the same model). This is even faster than
the syntax-based system that runs at 21.5 words
per core second (430 words per second). Table 3
summarizes running speeds for the phrase-based
SMT system, syntax-based system and NMT with
32-bit decoding and 8-bit decoding.
Phrase-based
Syntax-based
NMT 32-bit
Table 3: Running speed (in words per core second)
of the phrase-based SMT system, syntax-based system,
NMT with 32-bit decoding and NMT with 8-bit decoding.
Measurements
To demonstrate the effectiveness of approximating the ﬂoating point math with 8-bit integer computation, we show automatic evaluation results
on several models, as well as independent human
evaluations. We report results on Dutch-English,
English-Dutch, Russian-English, German-English
and English-German models. Table 4 shows training data sizes and vocabulary sizes. All models
have 620 dimension embeddings and 1000 dimension hidden states.
Vocabulary
Vocabulary
Table 4: Model training data and vocabulary sizes
Automatic results
Here we report automatic results comparing decoding results on 32-bit and 8-bit implementations. As others have found , 8-bit
implementations impact quality very little.
In Table 6, we compared automatic scores
and speeds for Dutch-English, English-Dutch,
Russian-English, German-English and English-
German models on news data.
The English-
German model was run with both a single model
(1x) and an ensemble of two models (2x) . Table 5 gives the number of sentences and average sentence length for the test sets
Table 5: Test data sizes and sentence lengths
Speed is reported in words per core second
(WPCS). This gives us a better sense of the speed
of individual engines when deployed on multicore systems with all cores performing translations. Total throughput is simply the product of
WPCS and the number of cores in the machine.
The reported speed is the median of 9 runs to ensure consistent numbers. The results show that we
see a 4-6x speedup over 32-bit ﬂoating point de-
Speed (WPSC)
Table 6: BLEU scores and speeds for 8-bit and 32bit versions of several models. Speeds are reported in
words per core second.
coding. German-English shows the largest deﬁcit
for the 8-bit mode versus the 32-bit mode. The
German-English test set only includes 168 sentences so this may be a spurious difference.
Human evaluation
These automatic results suggest that 8-bit quantization can be done without perceptible degradation. To conﬁrm this, we carried out a human evaluation experiment.
In Table 7, we show the results of performing
human evaluations on some of the same language
pairs in the previous section.
An independent
native speaker of the language being translated
to/from different than English (who is also pro-
ﬁcient in English) scored 100 randomly selected
sentences. The sentences were shufﬂed during the
evaluation to avoid evaluator bias towards different runs. We employ a scale from 0 to 5, with
0 being unintelligible and 5 being perfect translation.
Table 7: Human evaluation scores for 8-bit and 32-bit
systems. All tests are news domain.
The Table shows that the automatic scores
shown in the previous section are also sustained
Sie standen seit 1946 an der Parteispitze
They had been at the party leadership since 1946
They stood at the top of the party since 1946.
So erwarten die Experten f¨ur dieses Jahr lediglich einen Anstieg der Weltproduktion
von 3,7 statt der im Juni prognostizierten 3,9 Prozent. F¨ur 2009 sagt das Kieler
Institut sogar eine Abschw¨achung auf 3,3 statt 3,7 Prozent voraus.
For this year, the experts expect only an increase in world production of 3.7
instead of the 3.9 percent forecast in June. In 2009, the Kiel Institute
predictated a slowdown to 3.3 percent instead of 3.7 percent.
For this year, the experts expect only an increase in world production of 3.7
instead of the 3.9 percent forecast in June. In 2009, the Kiel Institute even
forecast a slowdown to 3.3% instead of 3.7 per cent.
Heftige Regenf¨alle wegen “Ike” werden m¨oglicherweise schwerere Sch¨aden anrichten
als seine Windb¨oen. Besonders gef¨ahrdet sind dicht besiedelte Gebiete im Tal des Rio
Grande, die noch immer unter den Folgen des Hurrikans “Dolly” im Juli leiden.
Heavy rainfall due to “Ike” may cause more severe damage than its gusts of wind,
particularly in densely populated areas in the Rio Grande valley, which are still
suffering from the consequences of the “dolly” hurricane in July.
Heavy rainfall due to “Ike” may cause heavier damage than its gusts of wind,
particularly in densely populated areas in the Rio Grande valley, which still
suffer from the consequences of the “dolly” hurricane in July.
Table 8: Examples of De-En news translation system comparing 32-bit and 8-bit decoding. Differences are in
boldface. Sentence times are average of 10 runs.
Het is tijd om de kloof te overbruggen.
It’s time to bridge the gap.
It is time to bridge the gap.
Niet dat Barientos met zijn vader van plaats zou willen wisselen.
Not that Barientos would want to change his father’s place.
Not that Barientos would like to switch places with his father.
Table 9: Examples of Nl-En news translation system comparing 32-bit and 8-bit decoding. Differences are in
boldface. Sentence times are average of 10 runs.
by humans. 8-bit decoding is as good as 32-bit
decoding according to the human evaluators.
Discussion
Having a faster NMT engine with no loss of accuracy is commercially useful. In our deployment
scenarios, it is the difference between an interactive user experience that is sluggish and one that
is not. Even in batch mode operation, the same
throughput can be delivered with 1/4 the hardware.
In addition, this speedup makes it practical to
deploy small ensembles of models.
above in the En-De model in Table 6, an ensemble can deliver higher accuracy at the cost of a 2x
slowdown. This work makes it possible to translate with higher quality while still being at least
twice as fast as the previous baseline.
As the numbers reported in Section 4 demonstrate, 8-bit and 32-bit decoding have similar average quality. As expected, the outputs produced
by the two decoders are not identical. In fact, on a
run of 166 sentences of De-En translation, only 51
were identical between the two. In addition, our
human evaluation results and the automatic scoring suggest that there is no speciﬁc degradation by
the 8-bit decoder compared to the 32-bit decoder.
In order to emphasize these claims, Table 8 shows
several examples of output from the two systems
for a German-English system. Table 9 shows 2
more examples from a Dutch-English system.
In general, there are minor differences without
any loss in adequacy or ﬂuency due to 8-bit decoding. Sentence 2 in Table 8 shows a spelling
error (“predictated”) in the 32-bit output due to re-
assembly of incorrect subword units.1
Related Work
Reducing the resources required for decoding neural nets in general and neural machine translation
in particular has been the focus of some attention
in recent years.
Vanhoucke et al. explored accelerating
convolutional neural nets with 8-bit integer decoding for speech recognition.
They demonstrated
that low precision computation could be used with
no signiﬁcant loss of accuracy. Han et al. 
investigated highly compressing image classiﬁcation neural networks using network pruning, quantization, and Huffman coding so as to ﬁt completely into on-chip cache, seeing signiﬁcant improvements in speed and energy efﬁciency while
keeping accuracy losses small.
Focusing on machine translation, Devlin 
implemented 16-bit ﬁxed-point integer math to
speed up matrix multiplication operations, seeing a 2.59x improvement.
They show competitive BLEU scores on WMT English-French
NewsTest2014 while offering signiﬁcant speedup.
Similarly, applies 8-bit end-toend quantization in translation models. They also
show that automatic metrics do not suffer as a result. In this work, quantization requires modiﬁcation to model training to limit the size of matrix
Conclusions and Future Work
In this paper, we show that 8-bit decoding for neural machine translation runs up to 4-6x times faster
than a similar optimized ﬂoating point implementation. We show that the quality of this approximation is similar to that of the 32-bit version. We also
show that it is unnecessary to modify the training
procedure to produce models compatible with 8bit decoding.
To conclude, this paper shows that 8-bit decoding is as good as 32-bit decoding both in automatic
measures and from a human perception perspective, while it improves latency substantially.
In the future we plan to implement a multilayered matrix multiplication that falls back to the
naive algorithm for matrix-panel multiplications.
This will provide speed for batch decoding for applications that can take advantage of it. We also
1In order to limit the vocabulary, we use BPE subword
units in all models.
plan to explore training with low precision for
faster experiment turnaround time.
Our results offer hints of improved accuracy
rather than just parity. Other work has used training as part of the compression process. We would
like to see if training quantized models changes
the results for better or worse.