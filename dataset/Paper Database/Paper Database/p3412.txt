Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2673‚Äì2679
Florence, Italy, July 28 - August 2, 2019. c‚Éù2019 Association for Computational Linguistics
A Simple Recipe towards Reducing Hallucination in
Neural Surface Realisation
Feng Nie1‚àóJin-Ge Yao2
Jinpeng Wang2
Chin-Yew Lin2
1Sun Yat-Sen University
2Microsoft Research Asia
 ,
 
2{jinge.yao, jinpwa, cyl}@microsoft.com
Recent neural language generation systems often hallucinate contents (i.e., producing irrelevant or contradicted facts), especially when
trained on loosely corresponding pairs of the
input structure and text. To mitigate this issue, we propose to integrate a language understanding module for data reÔ¨Ånement with selftraining iterations to effectively induce strong
equivalence between the input data and the
paired text.
Experiments on the E2E challenge dataset show that our proposed framework can reduce more than 50% relative unaligned noise from the original data-text pairs.
A vanilla sequence-to-sequence neural NLG
model trained on the reÔ¨Åned data has improved
on content correctness compared with the current state-of-the-art ensemble generator.
Introduction
Neural models for natural language generation
(NLG) based on the encoder-decoder framework
have become quite popular recently . Albeit being appealing for producing
Ô¨Çuent and diverse sentences, neural NLG models
often suffer from a severe issue of content hallucination , which refers to the problem
that the generated texts often contain information
that is irrelevant to or contradicted with the input.
Given that similar issues have been less reported
or noticed in the latest neural machine translation
systems, we believe that the origin of the issue for
neural NLG comes from the data side. Current
datasets used for training neural NLG systems often include instances that do not contain the same
amount of information from the input structure
and the output text . There is no exception for datasets
‚àóContribution during internship at Microsoft.
Golden Palace
5 out of 5
Reference: Golden Palace is a restaurant specializing
in breakfast in the low price range.
Table 1: A loosely corresponded MR-text pair. Bolded
phrases conforms to the MR, underlined words are
domain-speciÔ¨Åc additional information, and italic values in the MR are not realised in the reference.
originally intended for surface realisation (‚Äúhow to
say‚Äù) without focusing on content selection (‚Äúwhat
Table 1 depicts an example, where
the attribute Rating=5 out of 5 in the input meaning representation (MR) is not verbalised
in a reference text written by human, while the
word restaurant in the reference should refer to
an attribute value EatType=Restaurant not
contained in the MR. Without explicit alignments
in between MRs and the corresponding utterances
for guidance, neural systems trained on such data
often produce unexpected errors.
Previous work attempted at injecting indirect
semantic control over the encoder-decoder architecture or encouraging consistency during training ,
without essentially changing to the noisy training data. One exception is the Slug2Slug system
 , where the authors use an
aligner with manually written heuristic rules to Ô¨Ålter out unrealized attributes from data.
In this paper, we propose a simple, automatic
recipe towards reducing hallucination for neural
surface realisers by enhancing the semantic equivalence between pairs of MRs and utterances. The
steps include: (1) Build a language understanding
module (ideally well-calibrated) that tries to parse
the MR from an utterance; (2) Use it to reconstruct the correct attribute values revealed in the
reference texts; (3) With proper conÔ¨Ådence thresh-
olding, conduct self-training to iteratively recover
data pairs with identical or equivalent semantics.
Experiments on the E2E challenge benchmark
 show that our framework can reduce more than 50% relative unaligned
noise from original MR-text pairs, and a vanilla
sequence-to-sequence model trained on the reÔ¨Åned
data can improve content correctness in both human and automatic evaluations, when compared
with the current state-of-the-art neural ensemble
system .
Our proposed framework consists of a neural natural language understanding (NLU) module with
iterative data reÔ¨Ånement to induce semantically
equivalent MR-text pairs from a dataset containing a moderate level of noise.
Formally, given a corpus with paired meaning representations and text descriptions {(R, X)}N
the input MR R = (r1, . . . , rM) is a set of slotvalue pairs rj = (sj, vj), where each rj contains
a slot sj (e.g., rating) and a value vj (e.g., 5
out of 5). The corpus has M pre-deÔ¨Åned slots
, and each slot sj has Kj unique categorical values vj ‚àà(cj,1, . . . , cj,Kj). The corresponding utterance X = (x1, . . . , xT ) is a sequence of words
describing the MR.
Neural NLU Model
As shown in Figure 1, the NLU model consists of
a self-attentive encoder and an attentive scorer.
Self-Attentive Encoder.
The encoder produces
the vector representations of slot-value pairs in
MR and its paired utterance.
A slot-value pair
r can be treated as a short sequence W
(w1, . . . , wn) by concatenating words in its slot
and value. The word sequence W is Ô¨Årst represented as a sequence of word embedding vectors
(v1, . . . , vn) from a pre-trained embedding matrix
E, and then passed through a bidirectional LSTM
layer to yield the contextualized representations
Usv = to obtain the sentence vector cs, due to the effectiveness
of self-attention modules over variable-length sequences. Similarly, we obtain the contextualized
Self-attention
Slot-value pair ùëü
Utterance ùëã
Figure 1: The structure of the neural NLU model.
representations Uo = (uo
1, . . . , uo
T ) for the utterance X.
Attentive Scorer.
The scorer calculates the semantic similarity between a slot-value pair r (e.g.,
Price=Cheap) and the utterance X (e.g., reference in Table 1). Firstly, an attention layer is applied to select the most salient words in X related
to r, which yields the attentive representation d of
utterance X. Given the sentence vector cs of the
slot-value pair r and the attentive vector d of the
utterance X, the normalized semantic similarity is
deÔ¨Åned as:
p(r|X) = softmax(‚àí||d ‚àícs||2), where
t, with bt = softmax((uo
Model Inference.
Each utterance X will be
parsed to an MR Re = (re
1, . . . , re
M), with each
slot-value pair re
j = (sj, vj) determined by selecting the candidate value vj with the maximum semantic similarity for each slot sj:
vj = cj,k,
k = arg max
j = (sj, cj,k)|X),
where cj,k denotes the kth categorical value for jth
slot. Since an utterance may not describe any information about a speciÔ¨Åc slot s, we add a NONE
value as a candidate value of each slot.
Model Training.
The NLU model is optimized
by minimizing the cross-entropy loss:
log p(ri,j|Xi; Œ∏)
where Œ∏ denotes model parameters, and ri,j denotes the jth slot-value pair in the ith training MR.
Iterative Data ReÔ¨Ånement
The performance of NLU can be inaccurate when
trained on noisy data-text pairs. However, models trained on data with a moderate level of noise
could still be well-calibrated. This could enable
an iterative relabeling procedure, where we only
take MRs produced by NLU with high conÔ¨Ådence
together with their utterances as new training MRtext pairs to bootstrap the NLU training.
Algorithm 1 describes the training procedure.
We Ô¨Årst pre-train the NLU model using the original data-text pairs for Npre iterations. Then the
NLU model parses relevant MR for every utterance in training data, which can be used as new
training examples (Line 4). However, due to the
inaccuracy of the NLU results, we only use a small
portion (œÜ is set to 40% on validation) with high
conÔ¨Ådence. Moreover, as each MR consists of up
to M slots with some of them being unreliable, we
Ô¨Ålter the slot-value pairs with slot probability below average according to slot conÔ¨Ådence (Line 8 -
14). Finally, the NLU model is Ô¨Åne-tuned with the
new training corpus De. This process is repeated
for Ntune epochs. The Ô¨Ånal NLU model is leveraged to parse all utterances in the training corpus.
The resulting MRs paired with original utterances
form the reÔ¨Åned training corpus for NLG.
Experiments
Our experiments are conducted on E2E
challenge dataset, which
aims at verbalizing all information from the MR.
It has 42,061, 4,672 and 4,693 MR-text pairs for
training, validation and testing, respectively. Note
that every input MR in this dataset has 8.65 different references on average. The test set has 630
unique input MRs. We examine the effectiveness
of our proposed method in two aspects: 1) reducing the noise in data-text pairs (NLU), 2) reducing
hallucinated contents in surface realisation (NLG).
Automatic metrics.
The well-crafted rule-based
aligner built by Juraska et al. 1 is adopted
to approximately reÔ¨Çect the semantic correctness
of NLU and NLG models. The error rate is calculated by matching the slot values in output utterance: Err = M
N , where N is the total number
1 We use the public available evaluation script in
 aligner
/data analysis.py
Algorithm 1 Iterative Data ReÔ¨Ånement
Require MR-text pairs D = {(R, X)}N
1 , conÔ¨Ådence threshold œÜ, pre-training epochs Npre, tuning epochs Ntune,
1: Train Œ∏ with Eq. 3 on D for Npre iterations
2: for iter = 1 to Ntune do
Reset self-training corpus De = {}
Parse the MR Re
i,1, . . . , re
i,M) for every Xi using Eq. 2
Slot conÔ¨Åd. pj = PN
i,j|Xi) for sj
MR conÔ¨Åd. fi = PM
i,j|Xi) for Re
Sort {(Re, X)}N
1 by MR conÔ¨Ådence in reverse order
for i = 1 to ‚åäœÜ ¬∑ N‚åãdo
for j = 1 to M do
i,j|Xi) < pj/N then
i,j from Re
De ‚ÜêDe ‚à™ is also reported, although
currently neither BLEU nor any other automatic
metrics could be convincingly used for evaluating
language generation .
Human Evaluation.
We randomly sample 100
data-text pairs from test set and ask three crowd
workers to manually annotate missed (M), added
(A), and contradicted (C) slot values in NLG outputs with respect to the input MR, or exact match
(E) if all slot values have been realized in the
given utterance which contains no additional hallucinated information. When evaluating the NLU
systems, missed and added slots refer to the opposite directions, respectively.
Compared Systems.
Systems in comparison:
‚Ä¢ TGen :
a sequence-tosequence (Seq2Seq) model with reranking.
‚Ä¢ Slug2Slug :
current state-of-the-art method on E2E challenge
dataset. It is an ensemble model and uses a rule
based aligner for data cleaning and reranking.
‚Ä¢ Seq2Seq: a basic Seq2Seq model trained on
original MR-text pairs with the copy mechanism .
‚Ä¢ Seq2Seq+aug: Seq2Seq trained on the MRtext pairs reconstructed by pre-trained NLU.
‚Ä¢ Seq2Seq+aug+iter: Seq2Seq trained on
the MR-text pairs reconstructed by NLU model
with iterative data reÔ¨Ånement algorithm.
‚Ä¢ Seq2Seq+aligner:
Seq2Seq trained on
the MR-text pairs produced by the rule based
aligner .
Implementation Details.
For all models, we use
Ô¨Åxed pre-trained GloVe vectors and character embeddings .
The dimensions of trainable hidden units in LSTMs are all set to 400. The epochs
for pre-training Npre and bootstrapping Ntune are
all set to 5 on validation.
During training, we
regularize all layers with a dropout rate of 0.1.
We use stochastic gradient descent (SGD) for optimisation with learning rate 0.1.
The gradient
is truncated by 5.
For hyper-parameter œÜ, we
conduct experiments with different values (œÜ =
0.2, 0.4, 0.6, 0.8, 1.0), details in Appendix A.
Main Results
NLU Results.
One challenge in E2E dataset is
the need to account for the noise in the corpus
as some of the MR-text pairs are not semantically equivalent due to the data collection process . We examine the performance of the NLU module by comparing noise
reduction of the reconstructed MR-text pairs with
the original ones in both training and test sets.
Table 2 shows the automatic results.
our NLU model with iterative data reÔ¨Ånement,
the error rates of reÔ¨Åned MR-text pairs yields
23.33% absolute error reduction on test set. Human evaluation in Table 3 shows that our proposed
method achieves 16.69% improvement on information equivalence between MR-text pairs. These
results conÔ¨Årm the effectiveness of our method in
reducing the unaligned data noise, and the large
improvement (i.e, 15.09%) on exact match when
applying self-training algorithm suggests the importance of iterative data reÔ¨Ånement.
NLG Results.
Table 4 presents the automatic results of different neural NLG systems. We can
see that Seq2Seq+aug+iter achieves comparable BLEU score as Slug2Slug but with
4.44% error reduction on content correctness over
Train Err(%)
Test Err(%)
Original data
NLU reÔ¨Åned data
w/o self-training
Table 2: Automatic evaluation results of different NLU
models on both training and test sets
Original data
NLU reÔ¨Åned data
w/o self-training
Table 3: Human evaluation results for NLU on test set
(inter-annotator agreement: Fleiss‚Äô kappa = 0.855)
18.09 (114/630)
6.51 (41/630)
69.37 (374/630)
Seq2Seq+aug
28.89 (182/630)
Seq2Seq+aug+iter
2.07 (13/630)
Seq2Seq+aligner
1.75 (11/630)
Table 4: Automatic metrics for NLG
Seq2Seq+aug+iter
Table 5: Human evaluation results for NLG (interannotator agreement: Fleiss‚Äô kappa = 0.832)
Slug2Slug.
Seq2Seq+aug+iter largely
improves the content correctness over the baseline Seq2Seq with 67.3% error reduction. Besides, we also replace our NLU module with
the rule based aligner crafted by Juraska et al.
 for data reÔ¨Ånement to inspect the difference between our proposed method and manually designed rich heuristics. We can observe that
these two models (Seq2Seq+aug+iter and
Seq2Seq+aligner) achieve comparable performance, while our approach is fully automatic
and requires no domain knowledge.
The human evaluation results are shown in Table 5. We can Ô¨Ånd that Seq2Seq+aug+iter
improves 2.59% accuracy on exact match over
Slug2Slug.
SpeciÔ¨Åcally, Slug2Slug augments original training data by only deleting additional slot values not realized in the utterance
with an aligner, which is not capable of the situation where the given utterance contains incorrect or additional slot values and leads more con-
Utterance: Located in riverside, near Caf Sicilia,
is the Phoenix, a French pub that is family-friendly and
has average prices and an average rating.
Original MR: name[The Phoenix], eatType[pub],
food[French], priceRange , area[riverside],
customer rating[3 out of 5], familyFriendly[no],
near[Caf Sicilia]
ReÔ¨Åned MR: name[The Phoenix], eatType[pub],
food[French], priceRange[moderate], area[riverside],
customer rating[average], familyFriendly[yes],
near[Caf Sicilia]
Table 6: Example for data reÔ¨Ånement; The underscored
item is incorrect.
Name:[The Mill]; EatType:[pub];
Food:[Fast Food];PriceRange:[high];
FaimilyFriendly:[yes];Near:[Caf Sicilia];
Area:[riverside]; Rating:[average]
The Mill is a high priced family friendly
fast food pub located near Caf Sicilia
in the riverside area.
children friendly pub in the riverside
area near Caf Sicilia. It has a high
price range and a high customer rating
The Mill is a family friendly pub located
near Caf Sicilia.
The Mill is a children friendly fast food
pub near Caf Sicilia in the riverside area.
It has a high price range and an average
customer rating.
Table 7: Examples of different system outputs.
tradicted errors. Our method can complement and
correct original MR with additional slot values described in the paired texts to effectively alleviate
generating contradicted facts. However, due to the
imperfection of NLU model, our method may ignore part of slot values realized in utterances and
produce some additional errors.
Case Study
Example for reÔ¨Åned data.
Table 6 depicts a
case for one pair with originally inaccurate MR
while being corrected by NLU module and iterative reÔ¨Ånement. Our proposed method is capable
of reducing the unaligned noise for original data.
Example for NLG.
Table 7 shows the sentences
generated by different NLG systems. Seq2Seq
without any semantic control tends to generate
shorter descriptions. Slug2Slug and TGen with
reranker to control the content coverage can generate more input information, but still misses one
input information and Slug2Slug produces a
contradicted fact (i.e., customer rating). Our proposed method Seq2Seq+aug+iter trained on
reÔ¨Åned MR-text pairs, verbalises all the input information correctly, which shows the importance
of data quality in terms of strong equivalence between MR and utterance.
Discussion
In this paper, we present a simple recipe to reduce the hallucination problem in neural language
generation: introducing a language understanding
module to implement conÔ¨Ådence-based iterative
data reÔ¨Ånement. We Ô¨Ånd that our proposed method
can effectively reduce the noise in the original
MR-text pairs from the E2E dataset and improve
the content coverage for standard neural surface
realisation (no focus on content selection).
However, the currently presented approach still
has two clear limitations. One is that this simple
approach is implicitly built on an assumption of a
moderate level of noise in the original data, which
makes it possible to bootstrap a well-calibrated
NLU module.
We are still on the way to Ô¨Ånd
out solutions for cases with huge noise , where heavy manual intervention or external knowledge should be desperately needed.
The other limitation of this preliminary work is
that it currently overlooks the challenges of lexical
choices for quantities, degrees, temporal expressions, etc, which are rather difÔ¨Åcult to learn merely
from data and should require additional commonsense knowledge. An example case is in Table 6,
where the original priceRange=20-25 is re-
Ô¨Åned to be priceRange=moderate, which
enhances the correspondence between the MR and
the text but sidesteps the lexical choice for numbers which requires localised numerical commonsense.
Additional modules for lexical choices
should be expected for a reÔ¨Åned system.
Acknowledgement
We thank Zhirui Zhang, Shuangzhi Wu, and
the anonymous reviewers for helpful comments.
Feng Nie is partially supported by National
Key R&D Program of China (2018YFB1004404)
and Key R&D Program of Guangdong Province
(2018B010107005). The contact author of this paper, according to the meaning given to this role by
Sun Yat-Sen University, is Rong Pan.