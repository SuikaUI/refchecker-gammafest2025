Challenges in Data-to-Document Generation
Wiseman, Sam, Stuart M. Shieber and Alexander M. Rush. 2017. Challenges in
Data-to-Document Generation. In Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP2017), Copenhagen, Denmark, September 7-11,
2017: 2243–2253.
Published version
 
 
Terms of use
This article was downloaded from Harvard University’s DASH repository, and is made
available under the terms and conditions applicable to Open Access Policy Articles (OAP),
as set forth at
 
Accessibility
 
Share Your Story
The Harvard community has made this article openly available.
Please share how this access benefits you. Submit a story
Challenges in Data-to-Document Generation
Sam Wiseman and Stuart M. Shieber and Alexander M. Rush
School of Engineering and Applied Sciences
Harvard University
Cambridge, MA, USA
{swiseman,shieber,srush}@seas.harvard.edu
Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on
a small number of database records.
this work, we suggest a slightly more dif-
ﬁcult data-to-text generation task, and investigate how effective current approaches
are on this task. In particular, we introduce
a new, large-scale corpus of data records
paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural
generation methods.
Experiments show
that these models produce ﬂuent text, but
fail to convincingly approximate humangenerated documents.
Moreover, even
templated baselines exceed the performance of these neural models on some
metrics, though copy- and reconstructionbased extensions lead to noticeable improvements.
Introduction
Over the past several years, neural text generation systems have shown impressive performance
on tasks such as machine translation and summarization. As neural systems begin to move toward
generating longer outputs in response to longer
and more complicated inputs, however, the generated texts begin to display reference errors, intersentence incoherence, and a lack of ﬁdelity to
the source material. The goal of this paper is to
suggest a particular, long-form generation task in
which these challenges may be fruitfully explored,
to provide a publically available dataset for this
task, to suggest some automatic evaluation metrics, and ﬁnally to establish how current, neural
text generation methods perform on this task.
A classic problem in natural-language generation (NLG) involves taking structured
data, such as a table, as input, and producing text
that adequately and ﬂuently describes this data as
output. Unlike machine translation, which aims
for a complete transduction of the sentence to be
translated, this form of NLG is typically taken
to require addressing (at least) two separate challenges: what to say, the selection of an appropriate
subset of the input data to discuss, and how to say
it, the surface realization of a generation . Traditionally, these two challenges have been modularized and handled separately by generation systems. However, neural generation systems, which
are typically trained end-to-end as conditional language models , blur this distinction.
In this context, we believe the problem of
generating multi-sentence summaries of tables or
database records to be a reasonable next-problem
for neural techniques to tackle as they begin to
consider more difﬁcult NLG tasks.
In particular, we would like this generation task to have the
following two properties: (1) it is relatively easy
to obtain fairly clean summaries and their corresponding databases for dataset construction, and
(2) the summaries should be primarily focused on
conveying the information in the database. This
latter property ensures that the task is somewhat
congenial to a standard encoder-decoder approach,
and, more importantly, that it is reasonable to evaluate generations in terms of their ﬁdelity to the
One task that meets these criteria is that of generating summaries of sports games from associated box-score data, and there is indeed a long
history of NLG work that generates sports game
summaries . To this end, we make
the following contributions:
• We introduce a new large-scale corpus consisting of textual descriptions of basketball
games paired with extensive statistical tables.
This dataset is sufﬁciently large that fully
data-driven approaches might be sufﬁcient.
• We introduce a series of extractive evaluation models to automatically evaluate output
generation performance, exploiting the fact
that post-hoc information extraction is significantly easier than generation itself.
• We apply a series of state-of-the-art neural
methods, as well as a simple templated generation system, to our data-to-document generation task in order to establish baselines and
study their generations.
Our experiments indicate that neural systems
are quite good at producing ﬂuent outputs and
generally score well on standard word-match metrics, but perform quite poorly at content selection
and at capturing long-term structure. While the
use of copy-based models and additional reconstruction terms in the training loss can lead to improvements in BLEU and in our proposed extractive evaluations, current models are still quite far
from producing human-level output, and are signiﬁcantly worse than templated systems in terms
of content selection and realization. Overall, we
believe this problem of data-to-document generation highlights important remaining challenges in
neural generation systems, and the use of extractive evaluation reveals signiﬁcant issues hidden by
standard automatic metrics.
Data-to-Text Datasets
We consider the problem of generating descriptive
text from database records. Following the notation
in Liang et al. , let s = {rj}J
j=1 be a set of
records, where for each r ∈s we deﬁne r.t ∈T to
be the type of r, and we assume each r to be a binarized relation, where r.e and r.m are a record’s
entity and value, respectively.
For example, a
database recording statistics for a basketball game
might have a record r such that r.t = POINTS, r.e
= RUSSELL WESTBROOK, and r.m = 50.
this case, r.e gives the player in question, and r.m
gives the number of points the player scored. From
these records, we are interested in generating descriptive text, ˆy1:T = ˆy1, . . . , ˆyT of T words such
that ˆy1:T is an adequate and ﬂuent summary of s.
A dataset for training data-to-document systems
typically consists of (s, y1:T ) pairs, where y1:T is
a document consisting of a gold (i.e., human generated) summary for database s.
Several benchmark datasets have been used in
recent years for the text generation task, the most
popular of these being WEATHERGOV and ROBOCUP . Recently, neural generation systems have
show strong results on these datasets, with the system of Mei et al. achieving BLEU scores
in the 60s and 70s on WEATHERGOV, and BLEU
scores of almost 30 even on the smaller ROBOCUP
These results are quite promising, and
suggest that neural models are a good ﬁt for
text generation. However, the statistics of these
datasets, shown in Table 1, indicate that these
datasets use relatively simple language and record
structure. Furthermore, there is reason to believe
that WEATHERGOV is at least partially machinegenerated . More recently, Lebret
et al. introduced the WIKIBIO dataset,
which is at least an order of magnitude larger in
terms of number of tokens and record types. However, as shown in Table 1, this dataset too only
contains short (single-sentence) generations, and
relatively few records per generation. As such, we
believe that early success on these datasets is not
yet sufﬁcient for testing the desired linguistic capabilities of text generation at a document-scale.
With this challenge in mind, we introduce
a new dataset for data-to-document text generation, available at 
harvardnlp/boxscore-data. The dataset
is intended to be comparable to WEATHERGOV
in terms of token count, but to have signiﬁcantly
longer target texts, a larger vocabulary space, and
to require more difﬁcult content selection.
The dataset consists of two sources of articles summarizing NBA basketball games, paired
with their corresponding box- and line-score tables. The data statistics of these two sources, RO-
TOWIRE and SBNATION, are also shown in Table 1. The ﬁrst dataset, ROTOWIRE, uses professionally written, medium length game summaries
targeted at fantasy basketball fans. The writing
is colloquial, but relatively well structured, and
targets an audience primarily interested in game
CITY . . .
Tyler Johnson
Dwight Howard
Paul Millsap
Goran Dragic
Wayne Ellington
Dennis Schroder
Rodney McGruder
Thabo Sefolosha
Kyle Korver
The Atlanta Hawks defeated the Miami Heat
, 103 - 95 , at Philips Arena on Wednesday
. Atlanta was in desperate need of a win and
they were able to take care of a shorthanded
Miami team here . Defense was key for
the Hawks , as they held the Heat to 42
percent shooting and forced them to commit
16 turnovers . Atlanta also dominated in the
paint , winning the rebounding battle , 47
- 34 , and outscoring them in the paint 58
- 26.The Hawks shot 49 percent from the
ﬁeld and assisted on 27 of their 43 made
baskets . This was a near wire - to - wire
win for the Hawks , as Miami held just one
lead in the ﬁrst ﬁve minutes . Miami ( 7 -
15 ) are as beat - up as anyone right now
and it ’s taking a toll on the heavily used
starters . Hassan Whiteside really struggled
in this game , as he amassed eight points ,
12 rebounds and one blocks on 4 - of - 12
shooting ...
Figure 1: An example data-record and document pair from the ROTOWIRE dataset. We show a subset of the game’s records
(there are 628 in total), and a selection from the gold document. The document mentions only a select subset of the records, but
may express them in a complicated manner. In addition to capturing the writing style, a generation system should select similar
record content, express it clearly, and order it appropriately.
Rec. Types
Avg Records
Table 1: Vocabulary size, number of total tokens, number of
distinct examples, average generation length, total number of
record types, and average number of records per example for
the ROBOCUP (RC), WEATHERGOV (WG), WIKIBIO (WB),
ROTOWIRE (RW), and SBNATION (SBN) datasets.
statistics. The second dataset, SBNATION, uses
fan-written summaries targeted at other fans. This
dataset is signiﬁcantly larger, but also much more
challenging, as the language is very informal, and
often tangential to the statistics themselves. We
show some sample text from ROTOWIRE in Figure 1.
Our primary focus will be on the RO-
TOWIRE data.
Evaluating Document Generation
We begin by discussing the evaluation of generated documents, since both the task we introduce
and the evaluation methods we propose are motivated by some of the shortcomings of current approaches to evaluation. Text generation systems
are typically evaluated using a combination of automatic measures, such as BLEU , and human evaluation.
While BLEU is
perhaps a reasonably effective way of evaluating
short-form text generation, we found it to be unsatisfactory for document generation. In particular, we note that it primarily rewards ﬂuent text
generation, rather than generations that capture the
most important information in the database, or that
report the information in a particularly coherent
way. While human evaluation, on the other hand,
is likely ultimately necessary for evaluating generations , it is much
less convenient than using automatic metrics. Furthermore, we believe that current text generations
are sufﬁciently bad in sufﬁciently obvious ways
that automatic metrics can still be of use in evaluation, and we are not yet at the point of needing to
rely solely on human evaluators.
Extractive Evaluation
To address this evaluation challenge, we begin
with the intuition that assessing document quality
is easier than document generation. In particular,
it is much easier to automatically extract information from documents than to generate documents
that accurately convey desired information.
such, simple, high-precision information extraction models can serve as the basis for assessing
and better understanding the quality of automatic
generations. We emphasize that such an evaluation scheme is most appropriate when evaluating
generations (such as basketball game summaries)
that are primarily intended to summarize information. While many generation problems do not fall
into this category, we believe this to be an interesting category, and one worth focusing on because
it is amenable to this sort of evaluation.
To see how a simple information extraction system might work, consider the document in Figure 1.
We may ﬁrst extract candidate entity
(player, team, and city) and value (number and certain string) pairs r.e, r.m that appear in the text,
and then predict the type r.t (or none) of each candidate pair.
For example, we might extract the
entity-value pair (“Miami Heat”, “95”) from the
ﬁrst sentence in Figure 1, and then predict that the
type of this pair is POINTS, giving us an extracted
record r such that (r.e, r.m, r.t) = (MIAMI HEAT,
95, POINTS).
Indeed, many relation extraction
systems reduce relation extraction to multi-class
classiﬁcation precisely in this way .
More concretely, given a document ˆy1:T , we
consider all pairs of word-spans in each sentence
that represent possible entities e and values m.
We then model p(r.t | e, m; θ) for each pair, using r.t = ϵ to indicate unrelated pairs. We use architectures similar to those discussed in Collobert
et al. and dos Santos et al. to parameterize this probability; full details are given in the
Importantly, we note that the (s, y1:T ) pairs
typically used for training data-to-document systems are also sufﬁcient for training the information extraction model presented above, since we
can obtain (partial) supervision by simply checking whether a candidate record lexically matches
a record in s.1 However, since there may be multiple records r ∈s with the same e and m but with
different types r.t, we will not always be able to
determine the type of a given entity-value pair
found in the text.
We therefore train our classiﬁer to minimize a latent-variable loss: for all
document spans e and m, with observed types
t(e, m) = {r.t : r ∈s, r.e = e, r.m = m} (possibly {ϵ}), we minimize
p(r.t = t′ | e, m; θ).
We ﬁnd that this simple system trained in this way
is quite accurate at predicting relations. On the
1Alternative approaches explicitly align the document
with the table for this task .
ROTOWIRE data it achieves over 90% accuracy on
held-out data, and recalls approximately 60% of
the relations licensed by the records.
Comparing Generations
With a sufﬁciently precise relation extraction system, we can begin to evaluate how well an automatic generation ˆy1:T has captured the information
in a set of records s. In particular, since the predictions of a precise information extraction system
serve to align entity-mention pairs in the text with
database records, this alignment can be used both
to evaluate a generation’s content selection (“what
the generation says”), as well as content placement
(“how the generation says it”).
We consider in particular three induced metrics:
• Content Selection (CS): precision and recall of unique relations r extracted from
ˆy1:T that are also extracted from y1:T . This
measures how well the generated document
matches the gold document in terms of selecting which records to generate.
• Relation Generation (RG): precision and
number of unique relations r extracted from
ˆy1:T that also appear in s. This measures how
well the system is able to generate text containing factual (i.e., correct) records.
normalized
Damerau-Levenshtein
and Moore, 2000)2 between the sequences
of records extracted from y1:T
extracted from ˆy1:T . This measures how well
the system orders the records it chooses to
We note that CS primarily targets the “what to say”
aspect of evaluation, CO targets the “how to say it”
aspect, and RG targets both.
We conclude this section by contrasting the
automatic evaluation we have proposed with
recently proposed adversarial evaluation approaches, which also advocate automatic metrics
backed by classiﬁcation . Unlike adversarial evaluation, which uses a blackbox classiﬁer to determine the quality of a generation, our metrics are deﬁned with respect to the
2DLD is a variant of Levenshtein distance that allows
transpositions of elements; it is useful in comparing the ordering of sequences that may not be permutations of the same
set (which is a requirement for measures like Kendall’s Tau).
predictions of an information extraction system.
Accordingly, our metrics are quite interpretable,
since by construction it is always possible to determine which fact (i.e., entity-value pair) in the generation is determined by the extractor to not match
the database or the gold generation.
Neural Data-to-Document Models
In this section we brieﬂy describe the neural generation methods we apply to the proposed task. As a
base model we utilize the now standard attentionbased encoder-decoder model . We
also experiment with several recent extensions to
this model, including copy-based generation, and
training with a source reconstruction term in the
loss (in addition to the standard per-target-word
Base Model
For our base model, we map each
record r ∈s into a vector ˜r by ﬁrst embedding r.t
(e.g., POINTS), r.e (e.g., RUSSELL WESTBROOK),
and r.m (e.g., 50), and then applying a 1-layer
MLP ).3 Our source
data-records are then represented as ˜s = {˜rj}J
Given ˜s, we use an LSTM decoder with attention and input-feeding, in the style of Luong et al.
 , to compute the probability of each target
word, conditioned on the previous words and on
s. The model is trained end-to-end to minimize
the negative log-likelihood of the words in the gold
text y1:T given corresponding source material s.
There has been a surge of recent work
involving augmenting encoder-decoder models to
copy words directly from the source material on
which they condition . These models typically
introduce an additional binary variable zt into the
per-timestep target word distribution, which indicates whether the target word ˆyt is copied from the
source or generated:
p(ˆyt | ˆy1:t−1, s) =
p(ˆyt, zt = z | ˆy1:t−1, s).
In our case, we assume that target words are
copied from the value portion of a record r; that
is, a copy implies ˆyt = r.m for some r and t.
3We also include an additional feature for whether the
player is on the home- or away-team.
Joint Copy Model
The models of Gu et al.
 and Yang et al. parameterize the
joint distribution table over ˆyt and zt directly:
p(ˆyt, zt | ˆy1:t−1, s) ∝
copy(ˆyt, ˆy1:t−1, s)
zt = 1, ˆyt ∈s
zt = 1, ˆyt ̸∈s
gen(ˆyt, ˆy1:t−1, s)
where copy and gen are functions parameterized
in terms of the decoder RNN’s hidden state that assign scores to words, and where the notation ˆyt ∈s
indicates that ˆyt is equal to r.m for some r ∈s.
Conditional
G¨ulc¸ehre
 , on the other hand, decompose the joint
probability as:
p(ˆyt, zt | ˆy1:t−1, s) =
pcopy(ˆyt | zt, ˆy1:t−1, s) p(zt | ˆy1:t−1, s)
pgen(ˆyt | zt, ˆy1:t−1, s) p(zt | ˆy1:t−1, s)
where an MLP is used to model p(zt | ˆy1:t−1, s).
Models with copy-decoders may be trained to
minimize the negative log marginal probability,
marginalizing out the latent-variable zt . However, if it is known which target words yt are
copied, it is possible to train with a loss that does
not marginalize out the latent zt. G¨ulc¸ehre et al.
 , for instance, assume that any target word
yt that also appears in the source is copied, and
train to minimize the negative joint log-likelihood
of the yt and zt.
In applying such a loss in our case, we again
note that there may be multiple records r such
that r.m appears in ˆy1:T .
Accordingly, we
slightly modify the pcopy portion of the loss of
G¨ulc¸ehre et al. to sum over all matched
records. In particular, we model the probability
of relations r ∈s such that r.m = yt and r.e
is in the same sentence as r.m. Letting r(yt) =
{r ∈s : r.m = yt, same−sentence(r.e, r.m)},
pcopy(yt | zt, y1:t−1, s) =
p(r | zt, y1:t−1, s).
We note here that the key distinction for our purposes between the Joint Copy model and the Conditional Copy model is that the latter conditions on
whether there is a copy or not, and so in pcopy the
source records compete only with each other. In
the Joint Copy model, however, the source records
also compete with words that cannot be copied. As
a result, training the Conditional Copy model with
the supervised loss of G¨ulc¸ehre et al. can
be seen as training with a word-level reconstruction loss, where the decoder is trained to choose
the record in s that gives rise to yt.
Reconstruction
Reconstruction-based
techniques can also be applied at the documentor sentence-level during training.
One simple
approach to this problem is to utilize the hidden
states of the decoder to try to reconstruct the
A fully differentiable approach using
the decoder hidden states has recently been
successfully applied to neural machine translation
by Tu et al. . Unlike copying, this method
is applied only at training, and attempts to learn
decoder hidden states with broader coverage of
the input data.
In adopting this reconstruction approach we
segment the decoder hidden states ht into ⌈T
contiguous blocks of size at most B. Denoting a
single one of these hidden state blocks as bi, we
attempt to predict each ﬁeld value in some record
r ∈s from bi. We deﬁne p(r.e, r.m | bi), the probability of the entity and value in record r given bi,
to be softmax(f(bi)), where f is a parameterized
function of bi, which in our experiments utilize a
convolutional layer followed by an MLP; full details are given in the Appendix. We further extend
this idea and predict K records in s from bi, rather
than one. We can train with the following reconstruction loss for a particular bi:
r∈s log pk(r | bi; θ)
log pk(r.x | bi; θ),
where pk is the k’th predicted distribution over
records, and where we have modeled each component of r independently. This loss attempts to
make the most probable record in s given bi more
probable. We found that augmenting the above
loss with a term that penalizes the total variation
distance (TVD) between the pk to be helpful.4
4Penalizing the TVD between the pk might be useful if,
for instance, K is too large, and only a smaller number of
records can be predicted from bi. We also experimented with
Both L(θ) and the TVD term are simply added
to the standard negative log-likelihood objective at
training time.
Experimental Methods
In this section we highlight a few important details of our models and methods; full details are
in the Appendix. For our ROTOWIRE models, the
record encoder produces ˜rj in R600, and we use
a 2-layer LSTM decoder with hidden states of the
same size as the ˜rj, and dot-product attention and
input-feeding in the style of Luong et al. .
Unlike past work, we use two identically structured attention layers, one to compute the standard
generation probabilities (gen or pgen), and one to
produce the scores used in copy or pcopy.
We train the generation models using SGD and
truncated BPTT , as in language modeling. That is, we split
each y1:T into contiguous blocks of length 100,
and backprop both the gradients with respect to
the current block as well as with respect to the encoder parameters for each block.
Our extractive evaluator consists of an ensemble of 3 single-layer convolutional and 3 singlelayer bidirectional LSTM models. The convolutional models concatenate convolutions with kernel widths 2, 3, and 5, and 200 feature maps in the
style of . Both models are trained with
Templatized Generator
In addition to neural baselines, we also use a problem-speciﬁc,
template-based generator.
The template-based
generator ﬁrst emits a sentence about the teams
playing in the game, using a templatized sentence
taken from the training set:
The <team1> (<wins1>-<losses1>) defeated the <team2> (<wins2>-<losses2>)
<pts1>-<pts2>.
Then, 6 player-speciﬁc sentences of the following
form are emitted (again adapting a simple sentence
from the training set):
<player> scored <pts> points (<fgm>-
<tpm>-<tpa>
<fta> FT) to go with <reb> rebounds.
encouraging, rather than penalizing the TVD between the pk,
which might make sense if we were worried about ensuring
the pk captured different records.
The 6 highest-scoring players in the game are used
to ﬁll in the above template. Finally, a typical end
sentence is emitted:
The <team1>’ next game will be at home
Mavericks,
<team2> will travel to play the Bulls.
Code implementing all models can be found
 
data2text.
Our encoder-decoder models are
based on OpenNMT .
We found that all models performed quite poorly
on the SBNATION data, with the best model
achieving a validation perplexity of 33.34 and a
BLEU score of 1.78. This poor performance is
presumably attributable to the noisy quality of the
SBNATION data, and the fact that many documents in the dataset focus on information not in
the box- and line-scores. Accordingly, we focus
on ROTOWIRE in what follows.
The main results for the ROTOWIRE dataset are
shown in Table 2, which shows the performance
of the models in Section 4 in terms of the metrics
deﬁned in Section 3.2, as well as in terms of perplexity and BLEU.
Discussion
There are several interesting relationships in the
development portion of Table 2. First we note that
the Template model scores very poorly on BLEU,
but does quite well on the extractive metrics, providing an upper-bound for how domain knowledge could help content selection and generation.
All the neural models make signiﬁcant improvements in terms of BLEU score, with the conditional copying with beam search performing the
best, even though all the neural models achieve
roughly the same perplexity.
The extractive metrics provide further insight
into the behavior of the models.
We ﬁrst note
that on the gold documents y1:T , the extractive
model reaches 92% precision.
Using the Joint
Copy model, generation only has a record generation (RG) precision of 47% indicating that relationships are often generated incorrectly. The
best Conditional Copy system improves this value
to 71%, a signiﬁcant improvement and potentially
the cause of the improved BLEU score, but still far
below gold.
The Utah Jazz ( 38 - 26 ) defeated the Houston Rockets ( 38
- 26 ) 117 - 91 on Wednesday at Energy Solutions Arena in
Salt Lake City . The Jazz got out to a quick start in this one
, out - scoring the Rockets 31 - 15 in the ﬁrst quarter alone
. Along with the quick start , the Rockets were the superior
shooters in this game , going 54 percent from the ﬁeld and
43 percent from the three - point line , while the Jazz went
38 percent from the ﬂoor and a meager 19 percent from deep
. The Rockets were able to out - rebound the Rockets 49 -
49 , giving them just enough of an advantage to secure the
victory in front of their home crowd . The Jazz were led
by the duo of Derrick Favors and James Harden . Favors
went 2 - for - 6 from the ﬁeld and 0 - for - 1 from the three
- point line to score a game - high of 15 points , while also
adding four rebounds and four assists ....
Figure 2: Example document generated by the Conditional
Copy system with a beam of size 5. Text that accurately re-
ﬂects a record in the associated box- or line-score is highlighted in blue, and erroneous text is highlighted in red.
Notably, content selection (CS) and content ordering (CO) seem to have no correlation at all
with BLEU. There is some improvement with CS
for the conditional model or reconstruction loss,
but not much change as we move to beam search.
CO actually gets worse as beam search is utilized,
possibly a side effect of generating more records
(RG#). The fact that these scores are much worse
than the simple templated model indicates that further research is needed into better copying alone
for content selection and better long term content
ordering models.
Test results are consistent with development results, indicating that the Conditional Copy model
is most effective at BLEU, RG, and CS, and that
reconstruction is quite helpful for improving the
joint model.
Human Evaluation
We also undertook two human evaluation studies,
using Amazon Mechanical Turk. The ﬁrst study
attempted to determine whether generations considered to be more precise by our metrics were
also considered more precise by human raters. To
accomplish this, raters were presented with a particular NBA game’s box score and line score, as
well as with (randomly selected) sentences from
summaries generated by our different models for
those games. Raters were then asked to count how
many facts in each sentence were supported by
records in the box or line scores, and how many
were contradicted. We randomly selected 20 distinct games to present to raters, and a total of 20
generated sentences per game were evaluated by
raters. The left two columns of Table 3 contain the
Development
Joint Copy
Joint Copy + Rec
Joint Copy + Rec + TVD
Conditional Copy
Joint Copy
Joint Copy + Rec
Joint Copy + Rec + TVD
Conditional Copy
Joint Copy + Rec (B=5)
Joint Copy + Rec + TVD (B=1)
Conditional Copy (B=5)
Table 2: Performance of induced metrics on gold and system outputs of RotoWire development and test data. Columns indicate
Record Generation (RG) precision and count, Content Selection (CS) precision and recall, Count Ordering (CO) in normalized
Damerau-Levenshtein distance, perplexity, and BLEU. These ﬁrst three metrics are described in Section 3.2. Models compare Joint and Conditional Copy also with addition Reconstruction loss and Total Variation Distance extensions (described in
Section 4).
average numbers of supporting and contradicting
facts per sentence as determined by the raters, for
each model. We see that these results are generally
in line with the RG and CS metrics, with the Conditional Copy model having the highest number of
supporting facts, and the reconstruction terms signiﬁcantly improving the Joint Copy models.
Using a Tukey HSD post-hoc analysis of an
ANOVA with the number of contradicting facts as
the dependent variable and the generating model
and rater id as independent variables, we found
signiﬁcant (p < 0.01) pairwise differences in contradictory facts between the gold generations and
all models except “Copy+Rec+TVD,” as well as a
signiﬁcant difference between “Copy+Rec+TVD”
and “Copy”. We similarly found a signiﬁcant pairwise difference between “Copy+Rec+TVD” and
“Copy” for number of supporting facts.
Our second study attempted to determine
whether generated summaries differed in terms of
how natural their ordering of records (as captured,
for instance, by the DLD metric) is. To test this,
we presented raters with random summaries generated by our models and asked them to rate the
naturalness of the ordering of facts in the summaries on a 1-7 Likert scale.
30 random summaries were used in this experiment, each rated
3 times by distinct raters. The average Likert ratings are shown in the rightmost column of Table 3.
# Cont. Order Rat.
Joint Copy
Joint Copy + Rec
Joint Copy + Rec +TVD
Conditional Copy
Table 3: Average rater judgment of number of box score
ﬁelds supporting (left column) or contradicting (middle column) a generated sentence, and average rater Likert rating for
the naturalness of a summary’s ordering (right column). All
generations use B=1.
While it is encouraging that the gold summaries
received a higher average score than the generated summaries (and that the reconstruction term
again improved the Joint Copy model), a Tukey
HSD analysis similar to the one presented above
revealed no signiﬁcant pairwise differences.
Qualitative Example
Figure 2 shows a document generated by the Conditional Copy model, using a beam of size 5. This
particular generation evidently has several nice
properties: it nicely learns the colloquial style of
the text, correctly using idioms such as “19 percent from deep.” It is also partially accurate in its
use of the records; we highlight in blue when it
generates text that is licensed by a record in the
associated box- and line-scores.
At the same time, the generation also contains
major logical errors. First, there are basic copying mistakes, such as ﬂipping the teams’ win/loss
records. The system also makes obvious semantic errors; for instance, it generates the phrase
“the Rockets were able to out-rebound the Rockets.” Finally, we see the model hallucinates factual statements, such as “in front of their home
crowd,” which is presumably likely according to
the language model, but ultimately incorrect (and
not supported by anything in the box- or linescores). In practice, our proposed extractive evaluation will pick up on many errors in this passage. For instance, “four assists” is an RG error,
repeating the Rockets’ rebounds could manifest in
a lower CO score, and incorrectly indicating the
win/loss records is a CS error.
Related Work
In this section we note additional related work not
noted throughout.
Natural language generation
has been studied for decades , and generating summaries of sports games has been a topic of
interest for almost as long .
Historically, research has focused on both content selection (“what to say”) ,
and surface realization (“how to say it”) with earlier
work using (hand-built) grammars, and later work
using SMT-like approaches or generating from PCFGs 
or other formalisms . In the late 2000s and early
2010s, a number of systems were proposed that
did both .
Within the world of neural text generation,
some recent work has focused on conditioning
language models on tables ,
and generating short biographies from Wikipedia
Tables use a neural encoderdecoder approach on standard record-based generation datasets, obtaining impressive results, and
motivating the need for more challenging NLG
Conclusion and Future Work
This work explores the challenges facing neural
data-to-document generation by introducing a new
dataset, and proposing various metrics for automatically evaluating content selection, generation,
and ordering. We see that recent ideas in copying
and reconstruction lead to improvements on this
task, but that there is a signiﬁcant gap even between these neural models and templated systems.
We hope to motivate researchers to focus further
on generation problems that are relevant both to
content selection and surface realization, but may
not be reﬂected clearly in the model’s perplexity.
Future work on this task might include approaches that process or attend to the source
records in a more sophisticated way, generation
models that attempt to incorporate semantic or
reference-related constraints, and approaches to
conditioning on facts or records that are not as explicit in the box- and line-scores.
Acknowledgments
We gratefully acknowledge the support of a
Google Research Award.