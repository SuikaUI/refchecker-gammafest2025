Deep Convolutional Ranking for
Multilabel Image Annotation
Yunchao Gong
UNC Chapel Hill
 
Yangqing Jia
Google Research
 
Thomas K. Leung
Google Research
 
Alexander Toshev
Google Research
 
Sergey Ioffe
Google Research
 
Multilabel image annotation is one of the most important challenges in computer
vision with many real-world applications. While existing work usually use conventional visual features for multilabel annotation, features based on Deep Neural
Networks have shown potential to signiﬁcantly boost performance. In this work,
we propose to leverage the advantage of such features and analyze key components that lead to better performances. Speciﬁcally, we show that a signiﬁcant performance gain could be obtained by combining convolutional architectures with
approximate top-k ranking objectives, as thye naturally ﬁt the multilabel tagging
problem. Our experiments on the NUS-WIDE dataset outperforms the conventional visual features by about 10%, obtaining the best reported performance in
the literature.
Introduction
Multilabel image annotation is an important and challenging problem in computer vision.
Most existing work focus on single-label classiﬁcation problems , where each image is assumed to have only one class label. However, this is not necessarily true for real world applications,
as an image may be associated with multiple semantic tags (Figure 1). As a practical example,
images from Flickr are usually accompanied by several tags to describe the content of the image,
such as objects, activities, and scene descriptions. Images on the Internet, in general, are usually
associated with sentences or descriptions, instead of a single class label, which may be deemed as a
type of multitagging. Therefore, it is a practical and important problem to accurately assign multiple
labels to one image.
Single-label image classiﬁcation has been extensively studied in the vision community, the most
recent advances reported on the large-scale ImageNet database . Most existing work focus on
designing visual features for improving recognition accuracy. For example, sparse coding ,
Fisher vectors , and VLAD have been proposed to reduce the quantization error of “bag of
words”-type features. Spatial pyramid matching has been developed to encode spatial information for recognition. Very recently, deep convolutional neural networks (CNN) have demonstrated
promising results for single-label image classiﬁcation . Such algorithms have all focused on
learning a better feature representation for one-vs-rest classiﬁcation problems, and it is not yet clear
how to best train an architecture for multilabel annotation problems.
In this work, we are interested in leveraging the highly expressive convolutional network for the
problem of multilabel image annotation. We employed a similar network structure to , which
contains several convolutional and dense connected layers as the basic architecture. We studied
 
Tags: green,
flower sun,
flowers, zoo,
day, sunny,
Tags: art, girl,
woman, wow,
dance, jump,
Figure 1: Sample images from the NUS-WIDE dataset, where each image is annotated with several
and compared several other popular multilabel losses, such as the ranking loss that optimizes
the area under ROC curve (AUC), and the cross-entropy loss used in Tagprop . Speciﬁcally, we
propose to use the top-k ranking loss, inspired by , for embedding to train the network. Using the
largest publicly available multilabel dataset NUS-WIDE , we observe a signiﬁcant performance
boost over conventional features, reporting the best retrieval performance on the benchmark dataset
in the literature.
Previous Work
In this section we ﬁrst review related works on multilabel image annotation and then brieﬂy discuss
works on deep convolutional networks.
Modeling Internet images and their corresponding textural information (e.g., sentences, tags) have
been of great interest in the vision community . In this work, we focus on
the image annotation problem and summarize several important lines of related research. Early work
in this area was mostly devoted to annotation models inspired by machine translation techniques
 . The work by Barnard et al. applied machine translation methods to parse natural
images and tried to establish a relationship between image regions and words.
More recently, image annotation has been formulated as a classiﬁcation problem. Early works focused on generative model based tagging , centred upon the idea of learning a parametric
model to perform predictions. However, because image annotation is a highly nonlinear problem,
a parametric model might not be sufﬁcient to capture the complex distribution of the data. Several
recent works on image tagging have mostly focused on nonparametric nearest-neighbor methods,
which offer higher expressive power. The work by Makadia et al. , which proposed a simple nearest-neighbor-based tag transfer approach, achieved signiﬁcant improvement over previous
model-based methods. Recent improvements on the nonparametric approach include TagProp ,
which learns a discriminative metric for nearest neighbors to improve tagging.
Convolutional neural networks (CNNs) are a special type of neural network that
utilizes speciﬁc network structures, such as convolutions and spatial pooling, and have exhibited
good generalization power in image-related applications. Combined with recent techniques such
as Dropout and fast parallel training, CNN models have outperformed existing hancrafted features.
Krizhevsky et al. reported record-breaking results on ILSVRC 2012 that contains 1000 visualobject categories. However, this study was mostly concerned with single-label image classiﬁcation,
and the images in the dataset only contain one prominent object class. At a ﬁner scale, several methods focus on improving speciﬁc network designs. Notably, Zeiler et al. investigated different
pooling methods for training CNNs, and several different regularization methods, such as Dropout
 , DropConnect , and Maxout have been proposed to improve the robustness and representation power of the networks. In adition, Earlier studies have shown that CNN features are
suitable as a general feature for various tasks under the conventional classiﬁcation schemes, and our
work focuses on how to directly train a deep network from raw pixels, using multilabel ranking loss,
to address the multilabel annotation problem.
Multilabel Deep Convolutional Ranking Net
In our approach for multilabel image annotation, we adopted the architecture proposed in as
our basic framework and mainly focused on training the network with loss functions tailored for
multi-label prediction tasks.
Network Architecture
The basic architecture of the network we use is similar to the one used in . We use ﬁve convolutional layers and three densely connected layers. Before feeding the images to the convolutional
layers, each image is resized to 256×256. Next, 220×220 patches are extracted from the whole
image, at the center and the four corners to provide an augmentation of the dataset. Convolution
ﬁlter sizes are set to squares of size 11, 9, and 5 respectively for the different convolutional layers;
and max pooling layers are used in some of the convolutional layers to introduce invariance. Each
densely connected layer has output sizes of 4096. Dropout layers follow each of the densely connected layers with a dropout ratio of 0.6. For all the layers, we used rectiﬁed linear units (RELU) as
our nonlinear activation function.
The optimization of the whole network is achieved by asynchronized stochastic gradient descent
with a momentum term with weight 0.9, with mini-batch size of 32. The global learning rate for the
whole network is set to 0.002 at the beginning, and a staircase weight decay is applied after a few
epochs. The same optimization parameters and procedure are applied to all the different methods.
For our dataset with 150,000 training images, it usually takes one day to obtain a good model by
training on a cluster. Unlike previous work that usually used ImageNet to pre-train the network, we
train the whole network directly from the training images from the NUS-WIDE dataset for a fair
comparison with conventional vision baselines.
Multilabel Ranking Losses
We mainly focused on loss layer, which speciﬁes how the network training penalizes the deviation
between the predicted and true labels, and investigated several different multilabel loss functions for
training the network. The ﬁrst loss function was inspired by Tagprop , for which we minimized
the multilabel softmax regression loss. The second loss was a simple modiﬁcation of a pairwiseranking loss , which takes multiple labels into account. The third loss function was a multilabel
variant of the WARP loss , which uses a sampling trick to optimize top-k annotation accuracy.
For notations, assume that we have a set of images x and that we denote the convolutional network
by f(·) where the convolutional layers and dense connected layers ﬁlter the images. The output of
f(·) is a scoring function of the data point x, that produces a vector of activations. We assume there
are n image training data and c tags.
The softmax loss has been used for multilabel annotation in Tagprop , and is also used in singlelabel image classiﬁcation ; therefore, we adopted it in our context. The posterior probability of
an image xi and class j can be expressed as
exp(fj(xi))
k=1 exp(fk(xi)),
where fj(xi) means the activation value for image xi and class j. We then minimized the KL-
Divergence between the predictions and the ground-truth probabilities. Assuming that each image
has multiple labels, and that we can form a label vector y ∈R1×c where yj = 1 means the presence
of a label and yj = 0 means absence of a label for an image, we can obtain ground-truth probability
by normalizing y as y/∥y∥1. If the ground truth probability for image i and class j is deﬁned as
¯pij, the cost function to be minimized is
¯pij log(pij) = −1
where c+ denotes the number of positive labels for each image. For the ease of exposition and
without loss of generality, we set c+ to be the same for all images.
Pairwise Ranking
The second loss function we considered was the pairwise-ranking loss , which directly models
the annotation problem. In particular, we wanted to rank the positive labels to always have higher
scores than negative labels, which led to the following minimization problem:
max(0, 1 −fj(xi) + fk(xi)),
where c+ is the positive labels and c−is the negative labels. During the back-propagation, we
computed the sub-gradient of this loss function. One limitation of this loss is that it optimizes
the area under the ROC curve (AUC) but does not directly optimize the top-k annotation accuracy.
Because for image annotation problems we were mostly interested in top-k annotations, this pairwise
ranking loss did not best ﬁt our purpose.
Weighted Approximate Ranking (WARP)
The third loss we considered was the weighted approximate ranking (WARP), which was ﬁrst described in . It speciﬁcally optimizes the top-k accuracy for annotation by using a stochastic
sampling approach. Such an approach ﬁts the stochastic optimization framework of the deep architecture very well. It minimizes
L(rj) max(0, 1 −fj(xi) + fk(xi)).
where L(·) is a weighting function for different ranks, and rj is the rank for the jth class for image
i. The weighting function L(·) used in our work is deﬁned as:
αj, with α1 ≥α2 ≥. . . ≥0.
We deﬁned the αi as equal to 1/j, which is the same as . The weights deﬁned by L(·) control
the top-k of the optimization. In particular, if a positive label is ranked top in the label list, then L(·)
will assign a small weight to the loss and will not cost the loss too much. However, if a positive label
is not ranked top, L(·) will assign a much larger weight to the loss, which pushes the positive label
to the top. The last question was how to estimate the rank rj . We followed the sampling method
in : for a positive label, we continued to sample negative labels until we found a violation; then
we recorded the number of trials s we sampled for negative labels. The rank was estimated by the
following formulation
rj = ⌊c −1
for c classes and s sampling trials. We computed the sub-gradient for this layer during optimization.
As a minor noite, the approximate objective we optimize is a looser upper bound compared to the
original WARP loss proposed in . To see this, notice that in the original paper, it is assumed
that the probability of sampling a violator is p =
#Y −1 with a positive example (x, y) with rank
r, where #Y is the number of labels. Thus, there are r labels with higher scores than y. This is
true only if all these r labels are negative. However, in our case, since there may be other positive
labels having higher scores than y due to the multi-label nature of the problem, we effectively have
Visual Feature based Image Annotation Baslines
We used a set of 9 different visual features and combined them to serve as our baseline features.
Although such a set of features might not have been the best possible ones we could obtain, they
already serve as a very strong visual representation, and the computation of such features is nontrivial. On top of these features, we ran two simple but powerful classiﬁers (kNN and SVM) for
image annotation. We also experimented with Tagprop , but found it cannot easily scale to a
large training set because of the O(n2) time complexity. After using a small training set to train the
Tagprop model, we found the performance to be unsatisfactory and therefore do not compare it here.
Visual Features
GIST : We resized each image to 200×200 and used three different scales to ﬁlter each
RGB channel, resulting in 960-dimensional (320×3) GIST feature vectors.
SIFT: We used two different sampling methods and three different local descriptors to extract texture
features, which gave us a total of 6 different features. We used dense sampling and a Harris corner
detector as our patch-sampling methods. For local descriptors, we extracted SIFT , CSIFT ,
and RGBSIFT , and formed a codebook of size 1000 using kmeans clustering; then built a twolevel spatial pyramid that resulted in a 5000-dimensional vector for each image. We will refer
to these six features as D-SIFT, D-CSIFT, D-RGBSIFT, H-SIFT, H-CSIFT, and H-RGBSIFT.
HOG: To represent texture information at a larger scale, we used 2×2 overlapping HOG as described
in . We quantized the HOG features to a codebook of size 1000 and used the same spatial
pyramid scheme as above, which resulted in 5000-dimensional feature vectors.
Color: We used a joint RGB color histogram of 8 bins per dimension, for a 512-dimensional feature.
The same set of features were used in , and achieved state-of-the-art performance for image retrieval and annotation. The combination of this set of features has a total dimensionality of 36,472,
which makes learning very expensive. We followed to perform simple dimensionality reductions to reduce computation. In particular, we performed a kernel PCA (KPCA) separately on each
feature to reduce the dimensionality to 500. Then we concatenated all of the feature vectors to form
a 4500-dimensional global image feature vector and performed different learning algorithms on it.
Visual feature + kNN
The simplest baseline that remains very powerful involves directly applying a weighted kNN on the
visual feature vectors. kNN is a very strong baseline for image annotation, as suggested by Makadia
et al. , mainly because multilabel image annotation is a highly nonlinear problem and handling
the heavily tailed label distribution is usually very difﬁcult. By contrast, kNN is a highly nonlinear
and adaptive algorithm that better handles rare tags. For each test image, we found its k nearest
neighbors in the training set and computed the posterior probability p(c|i) as
k exp(−||xi −xj||2
where yik indexes the labels of training data, yik = 1 when there is one label for this image, and
yik = 0 when there is no label for this image. σ is the bandwidth that needs to be tuned. After
obtaining the prediction probabilities for each image, we sorted the scores and annotated each testing
image with the top-k tags.
Visual feature + SVM
Another way to perform image annotation is to treat each tag separately and to train c different onevs-all classiﬁers. We trained a linear SVM for each tag and used the output of the c different
SVMs to rank the tags. Because we had already performed nonlinear mapping to the data during the
KPCA stage, we found a linear SVM to be sufﬁcient. Thus we assigned top-k tags to one image,
based on the ranking of the output scores of the SVMs.
Experiments
We performed experiments on the largest publicly available multilabel dataset, NUS-WIDE . This
dataset contains 269,648 images downloaded from Flickr that have been manually annotated, with
several tags (2-5 on average) per image. After ignoring the small subset of the images that are not
annotated by any tag, we had a total of 209,347 images for training and testing. We used a subset
of 150,000 images for training and used the rest of the images for testing. The tag dictionary for the
images contains 81 different tags. Some sample images and annotations are shown in Figure 1.
method / metric
per-class recall
per-class precision
overall recall
overall precision
Upper bound
Visual Feature + kNN
Visual Feature + SVM
CNN + Softmax
CNN + Ranking
CNN + WARP
Table 1: Image annotation results on NUS-WIDE with k = 3 annotated tags per image. See text in
section 5.4 for the deﬁnition of “Upper bound”.
method / metric
per-class recall
per-class precision
overall recall
overall precision
Upper bound
Visual Feature + kNN
Visual Feature + SVM
CNN + Softmax
CNN + Ranking
CNN + WARP
Table 2: Image annotation results on NUS-WIDE with k = 5 annotated tags per image. See text in
section 5.4 for the deﬁnition of “Upper bound”.
Evaluation Protocols
We followed previous research in our use of the following protocols to evaluate different methods. For each image, we assigned k (e.g., k = 3, 5) highest-ranked tags to the image and compared
the assigned tags to the ground-truth tags. We computed the recall and precision for each tag separately, and then report the mean-per-class recall and mean-per-class precision:
per-class recall = 1
per-class precision = 1
where c is the number of tags, N c
i is the number of correctly annotated image for tag i, N g
the number of ground-truth tags for tag i, and N p
i is the number of predictions for tag i. The
above evaluations are biased toward infrequent tags, because making them correct would have a
very signiﬁcant impact on the ﬁnal accuracy. Therefore we also report the overall recall and overall
precision:
overall recall =
overall precision =
For the above two metrics, the frequent classes will be dominant and have a larger impact on ﬁnal
performance. Finally, we also report the percentage of recalled tag words out of all tag words as N+.
We believe that evaluating all of these metrics makes the evaluation unbiased.
Baseline Parameters
In our preliminary evaluation, we optimized the parameters for the visual-feature-based baseline
systems. For visual-feature dimensionality reduction, we followed the suggestions in Gong et al.
 to reduce the dimensionality of each feature to 500 and then concatenated the PCA-reduced
vectors into a 4500-dimensional global image descriptor, which worked as well as the original feature. For kNN, we set the bandwidth σ to 1 and k to 50, having found that these settings work best.
For SVM, we set the regularization parameter to C = 2, which works best for this dataset.
We ﬁrst report results with respect to the metrics introduced above. In particular, we vary the number
k of predicted keywords for each image and mainly consider k = 3 and k = 5. Before doing
so, however, we must deﬁne an upper bound for our evaluation. In the dataset, each image had
different numbers of ground-truth tags, which made it hard for us to precisely compute an upper
Tags (decreasing frequency)
Figure 2: Analysis of per-class recall of the 81 tags in NUS-WIDE dataset with k = 3.
Tags (decreasing frequency)
Figure 3: Analysis of per-class precision of the 81 tags in NUS-WIDE dataset with k = 3.
bound for performance with different k. For each image, when the number of ground-truth tags was
larger than k, we randomly chose k ground-truth tags and assigned them to that image; when the
number of ground-truth tags was smaller than k, we assigned all ground-truth tags to that image
and randomly chose other tags for that image. We believe this baseline represents the best possible
performance when the ground truth is known. The results for assigning 3 keywords per image are
reported in Table 1. The results indicate that the deep network achieves a substantial improvement
over existing visual-feature-based annotation methods. The CNN+Softmax method outperforms the
VisualFeature+SVM baseline by about 10%. Comparing the same CNN network with different loss
functions, results show that softmax already gives a very powerful baseline. Although using the
pairwise ranking loss does not improve softmax, by using the weighted approximated-ranking loss
(WARP) we were able to achieve a substantial improvement over softmax. This is probably because
pairwise-ranking is not directly optimizing the top-k accuracy, and because WARP pushes classes
that are not ranked top heavier, which boosts the performance of rare tags. From these results,
we can see that all loss functions achieved comparable overall-recall and overall-precision, but that
WARP loss achieved signiﬁcantly better per-class recall and per-class precision. Results for k = 5,
which are given in Table 2, show similar trends to k = 3.
We also provide a more detailed analysis of per-class recall and per-class precision. The recall for
each tags appears in Figure 2, and the precision for each tag in Figure 3. The results for different
tags are sorted by the frequency of each tag, in descending order. From these results, we see that the
accuracy for frequent tags greater than for infrequent tags. Different losses performed comparably
to each other for frequent classes, and WARP worked better than other loss functions for infrequent
classes. Finally, we show some image annotation examples in Figure 4. Even though some of the
predicted tags for these do not match the ground truth, they are still very meaningful.
Discussion and Future Work
In this work, we proposed to use ranking to train deep convolutional neural networks for multilabel image annotation problems. We investigated several different ranking-based loss functions for
training the CNN, and found that the weighted approximated-ranking loss works particularly well
for multilabel annotation problems. We performed experiments on the largest publicly available
multilabel image dataset NUS-WIDE, and demonstrated the effectiveness of using top-k ranking to
train the network. In the future, we would like to use very large amount of noisy-labeled multilabel
images from the Internet (e.g., from Flickr or image searches) to train the network.
Predic2ons
Predic2ons
Figure 4: Qualitative image annotation results obtained with WARP.