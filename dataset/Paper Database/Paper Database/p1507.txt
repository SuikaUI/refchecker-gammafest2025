Improving Object Detection With One Line of Code
Navaneeth Bodla*
Bharat Singh*
Rama Chellappa
Larry S. Davis
Center For Automation Research, University of Maryland, College Park
{nbodla,bharat,rama,lsd}@umiacs.umd.edu
Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on
the basis of their scores. The detection box M with the
maximum score is selected and all other detection boxes
with a signiﬁcant overlap (using a pre-deﬁned threshold)
with M are suppressed. This process is recursively applied
on the remaining boxes. As per the design of the algorithm,
if an object lies within the predeﬁned overlap threshold, it
leads to a miss. To this end, we propose Soft-NMS, an algorithm which decays the detection scores of all other objects
as a continuous function of their overlap with M. Hence,
no object is eliminated in this process. Soft-NMS obtains
consistent improvements for the coco-style mAP metric on
standard datasets like PASCAL VOC 2007 (1.7% for both R-
FCN and Faster-RCNN) and MS-COCO (1.3% for R-FCN
and 1.1% for Faster-RCNN) by just changing the NMS algorithm without any additional hyper-parameters. Using
Deformable-RFCN, Soft-NMS improves state-of-the-art in
object detection from 39.8% to 40.9% with a single model.
Further, the computational complexity of Soft-NMS is the
same as traditional NMS and hence it can be efﬁciently
implemented. Since Soft-NMS does not require any extra
training and is simple to implement, it can be easily integrated into any object detection pipeline. Code for Soft-
NMS is publicly available on GitHub 
1. Introduction
Object detection is a fundamental problem in computer
vision in which an algorithm generates bounding boxes for
speciﬁed object categories and assigns them classiﬁcation
scores. It has many practical applications in autonomous
driving , video/image indexing , surveillance
 etc. Hence, any new component proposed for the
object detection pipeline should not create a computational
*The ﬁrst two authors contributed equally to this paper.
Figure 1. This image has two conﬁdent horse detections (shown
in red and green) which have a score of 0.95 and 0.8 respectively.
The green detection box has a signiﬁcant overlap with the red one.
Is it better to suppress the green box altogether and assign it a score
of 0 or a slightly lower score of 0.4?
bottleneck, otherwise it will be conveniently “ignored’ in
practical implementations. Moreover, if a complex module
is introduced which requires re-training of models which
leads to a little improvement in performance, it will also be
ignored. However, if a simple module can improve performance without requiring any re-training of existing models,
it would be widely adopted. To this end, we present a soft
non-maximum suppression algorithm, as an alternative to
the traditional NMS algorithm in the current object detection pipeline.
Traditional object detection pipelines employ a
multi-scale sliding window based approach which assigns
foreground/background scores for each class on the basis
of features computed in each window. However, neighboring windows often have correlated scores (which increases
false positives), so non-maximum suppression is used as
a post-processing step to obtain ﬁnal detections. With the
advent of deep learning, the sliding window approach was
replaced with category independent region proposals generated using a convolutional neural network. In state-ofthe-art detectors, these proposals are input to a classiﬁcation sub-network which assigns them class speciﬁc scores
 
Input : B = {b1, .., bN}, S = {s1, .., sN}, Nt
B is the list of initial detection boxes
S contains corresponding detection scores
Nt is the NMS threshold
while B 6= empty do
m argmax S
D D S M; B B −M
for bi in B do
if iou(M, bi) ≥Nt then
B B −bi; S S −si
si sif(iou(M, bi))
return D, S
Figure 2. The pseudo code in red is replaced with the one in green
in Soft-NMS. We propose to revise the detection scores by scaling
them as a linear or Gaussian function of overlap.
 . Another parallel regression sub-network reﬁnes
the position of these proposals. This reﬁnement process improves localization for objects, but also leads to cluttered
detections as multiple proposals often get regressed to the
same region of interest (RoI). Hence, even in state-of-the-art
detectors, non-maximum suppression is used to obtain the
ﬁnal set of detections as it signiﬁcantly reduces the number
of false positives.
Non-maximum suppression starts with a list of detection
boxes B with scores S. After selecting the detection with
the maximum score M, it removes it from the set B and
appends it to the set of ﬁnal detections D. It also removes
any box which has an overlap greater than a threshold Nt
with M in the set B. This process is repeated for remaining boxes B. A major issue with non-maximum suppression is that it sets the score for neighboring detections to
zero. Thus, if an object was actually present in that overlap
threshold, it would be missed and this would lead to a drop
in average precision. However, if we lower the detection
scores as a function of its overlap with M, it would still
be in the ranked list, although with a lower conﬁdence. We
show an illustration of the problem in Fig 1.
Using this intuition, we propose a single line modiﬁcation to the traditional greedy NMS algorithm in which we
decrease the detection scores as an increasing function of
overlap instead of setting the score to zero as in NMS. Intuitively, if a bounding box has a very high overlap with M,
it should be assigned a very low score, while if it has a low
overlap, it can maintain its original detection score. This
Soft-NMS algorithm is shown in Figure 2. Soft-NMS leads
to noticeable improvements in average precision measured
over multiple overlap thresholds for state-of-the-object detectors on standard datasets like PASCAL VOC and MS-
COCO. Since Soft-NMS does not require any extra-training
and is simple to implement, it can be easily integrated in the
object detection pipeline.
2. Related Work
NMS has been an integral part of many detection algorithms in computer vision for almost 50 years. It was ﬁrst
employed in edge detection techniques . Subsequently,
it has been applied to multiple tasks like feature point detection , face detection and object detection
 . In edge detection, NMS performs edge thinning
to remove spurious responses . In feature point
detectors , NMS is effective in performing local thresholding to obtain unique feature point detections. In face detection , NMS is performed by partitioning boundingboxes into disjoint subsets using an overlap criterion. The
ﬁnal detections are obtained by averaging the co-ordinates
of the detection boxes in the set. For human detection, Dalal
and Triggs demonstrated that a greedy NMS algorithm,
where a bounding box with the maximum detection score
is selected and its neighboring boxes are suppressed using a pre-deﬁned overlap threshold improves performance
over the approach used for face detection . Since then,
greedy NMS has been the de-facto algorithm used in object
detection .
It is surprising that this component of the detection
pipeline has remained untouched for more than a decade.
Greedy NMS still obtains the best performance when average precision (AP) is used as an evaluation metric and is
therefore employed in state-of-the-art detectors . A
few learning-based methods have been proposed as an alternative to greedy NMS which obtain good performance for
object class detection . For example, ﬁrst
computes overlap between each pair of detection boxes. It
then performs afﬁnity propagation clustering to select exemplars for each cluster which represent the ﬁnal detection
boxes. A multi-class version of this algorithm is proposed in
 . However, object class detection is a different problem,
where object instances of all classes are evaluated simultaneously per image. Hence, we need to select a threshold for
all classes and generate a ﬁxed set of boxes. Since different thresholds may be suitable for different applications, in
generic object detection, average precision is computed using a ranked list of all object instances in a particular class.
Therefore, greedy NMS performs favourably to these algo-
Figure 3. In object detection, ﬁrst category independent region
proposals are generated. These region proposals are then assigned
a score for each class label using a classiﬁcation network and their
positions are updated slightly using a regression network. Finally,
non-maximum-suppression is applied to obtain detections.
rithms on generic object detection metrics.
In another line of work, for detecting salient objects, a
proposal subset optimization algorithm was proposed 
as an alternative to greedy NMS. It performs a MAP-based
subset optimization to jointly optimize the number and locations of detection windows. In salient object detection, the
algorithm is expected to only ﬁnd salient objects and not all
objects. So, this problem is also different from generic object detection and again greedy NMS performs favourably
when performance on object detection metrics is measured.
For special cases like pedestrian detection, a quadratic unconstrained binary optimization (QUBO) solution was proposed which uses detection scores as a unary potential and
overlap between detections as a pairwise potential to obtain the optimal subset of detection boxes . Like greedy
NMS, QUBO also applies a hard threshold to suppress detection boxes, which is different from Soft-NMS. In another learning-based framework for pedestrian detection, a
determinantal point process was combined with individualness prediction scores to optimally select ﬁnal detections
 . To the best of our knowledge, for generic object detection, greedy NMS is still the strongest baseline on challenging object detection datasets like PASCAL VOC and
3. Background
We brieﬂy describe the object-detection pipeline used in
state-of-the-art object detectors in this section. During inference, an object detection network performs a sequence
of convolution operations on an image using a deep convolutional neural network (CNN). The network bifurcates
into two branches at a layer L — one branch generates region proposals while the other performs classiﬁcation and
regression by pooling convolutional features inside RoIs
generated by the proposal network. The proposal network
generates classiﬁcation scores and regression offsets for anchor boxes of multiple scales and aspect ratios placed at
each pixel in the convolutional feature map . It then
ranks these anchor boxes and selects the top K (≈6000)
anchors to which the bounding box regression offsets are
added to obtain image level co-ordinates for each anchor.
Greedy non-maximum suppression is applied to top K anchors which eventually generates region proposals 1.
The classiﬁcation network generates classiﬁcation and
regression scores for each proposal generated by the proposal network. Since there is no constraint in the network
which forces it to generate a unique RoI for an object, multiple proposals may correspond to the same object. Hence,
other than the ﬁrst correct bounding-box, all other boxes on
the same object would generate false positives. To alleviate
this problem, non-maximum-suppression is performed on
detection boxes of each class independently, with a speciﬁed overlap threshold. Since the number of detections is
typically small and can be further reduced by pruning detections which fall below a very small threshold, applying
non-maximum suppression at this stage is not computationally expensive. We present an alternative approach to this
non-maximum suppression algorithm in the object detection pipeline. An overview of the object detection pipeline
is shown in Fig 3.
4. Soft-NMS
Current detection evaluation criteria emphasise precise
localization and measure average precision of detection
boxes at multiple overlap thresholds (ranging from 0.5 to
0.95). Therefore, applying NMS with a low threshold like
0.3 could lead to a drop in average precision when the overlap criterion during evaluation for a true positive is 0.7 (we
refer to the detection evaluation threshold as Ot from here
This is because, there could be a detection box bi
which is very close to an object (within 0.7 overlap), but
had a slightly lower score than M (M did not cover the
object), thus bi gets suppressed by a low Nt. The likelihood of such a case would increase as the overlap threshold
criterion is increased. Therefore, suppressing all nearby detection boxes with a low Nt would increase the miss-rate.
Also, using a high Nt like 0.7 would increase false positives when Ot is lower and would hence drop precision averaged over multiple thresholds. The increase in false positives would be much higher than the increase in true positives for this case because the number of objects is typically
1We do not replace this non-maximum suppression in the object detection pipeline.
much smaller than the number of RoIs generated by a detector. Therefore, using a high NMS threshold is also not
To overcome these difﬁculties, we revisit the NMS algorithm in greater detail. The pruning step in the NMS algorithm can be written as a re-scoring function as follows,
iou(M, bi) < Nt
iou(M, bi) ≥Nt
Hence, NMS sets a hard threshold while deciding what
should be kept or removed from the neighborhood of M.
Suppose, instead, we decay the classiﬁcation score of a box
bi which has a high overlap with M, rather than suppressing it altogether. If bi contains an object not covered by
M, it won’t lead to a miss at a lower detection threshold.
However, if bi does not cover any other object (while M
covers an object), and even after decaying its score it ranks
above true detections, it would still generate a false positive.
Therefore, NMS should take the following conditions into
• Score of neighboring detections should be decreased to
an extent that they have a smaller likelihood of increasing the false positive rate, while being above obvious
false positives in the ranked list of detections.
• Removing neighboring detections altogether with a
low NMS threshold would be sub-optimal and would
increase the miss-rate when evaluation is performed at
high overlap thresholds.
• Average precision measured over a range of overlap
thresholds would drop when a high NMS threshold is
We evaluate these conditions through experiments in
Section 6.3.
Rescoring Functions for Soft-NMS: Decaying the
scores of other detection boxes which have an overlap with
M seems to be a promising approach for improving NMS.
It is also clear that scores for detection boxes which have
a higher overlap with M should be decayed more, as they
have a higher likelihood of being false positives. Hence, we
propose to update the pruning step with the following rule,
iou(M, bi) < Nt
si(1 −iou(M, bi)),
iou(M, bi) ≥Nt
The above function would decay the scores of detections
above a threshold Nt as a linear function of overlap with
M. Hence, detection boxes which are far away from M
would not be affected and those which are very close would
be assigned a greater penalty.
However, it is not continuous in terms of overlap and
a sudden penalty is applied when a NMS threshold of Nt
is reached. It would be ideal if the penalty function was
continuous, otherwise it could lead to abrupt changes to
the ranked list of detections. A continuous penalty function should have no penalty when there is no overlap and
very high penalty at a high overlap. Also, when the overlap is low, it should increase the penalty gradually, as M
should not affect the scores of boxes which have a very low
overlap with it. However, when overlap of a box bi with
M becomes close to one, bi should be signiﬁcantly penalized. Taking this into consideration, we propose to update
the pruning step with a Gaussian penalty function as follows,
si = sie−iou(M,bi)2
This update rule is applied in each iteration and scores of
all remaining detection boxes are updated.
The Soft-NMS algorithm is formally described in Figure 2, where f(iou(M, bi))) is the overlap based weighting function. The computational complexity of each step
in Soft-NMS is O(N), where N is the number of detection boxes. This is because scores for all detection boxes
which have an overlap with M are updated. So, for N detection boxes, the computational complexity for Soft-NMS
is O(N 2), which is the same as traditional greedy-NMS.
Since NMS is not applied on all detection boxes (boxes with
a minimum threshold are pruned in each iteration), this step
is not computationally expensive and hence does not affect
the running time of current detectors.
Note that Soft-NMS is also a greedy algorithm and does
not ﬁnd the globally optimal re-scoring of detection boxes.
Re-scoring of detection boxes is performed in a greedy fashion and hence those detections which have a high local score
are not suppressed. However, Soft-NMS is a generalized
version of non-maximum suppression and traditional NMS
is a special case of it with a discontinuous binary weighting function. Apart from the two proposed functions, other
functions with more parameters can also be explored with
Soft-NMS which take overlap and detection scores into account. For example, instances of the generalized logistic
function like the Gompertz function can be used, but such
functions would increase the number of hyper-parameters.
5. Datasets and Evaluation
We perform experiments on two datasets, PASCAL VOC
 and MS-COCO . The Pascal dataset has 20 object
categories, while the MS-COCO dataset has 80 object categories. We choose the VOC 2007 test partition to measure
performance. For the MS-COCO dataset, sensitivity analysis is conducted on a publicly available minival set of 5,000
images. We also show results on the test-dev partition on
Training data
Testing data
AP 0.5:0.95
Recall @ 10
Recall @ 100
R-FCN 
train+val35k
R-FCN + S-NMS G
train+val35k
R-FCN + S-NMS L
train+val35k
F-RCNN 
train+val35k
F-RCNN + S-NMS G
train+val35k
F-RCNN + S-NMS L
train+val35k
D-RFCN 
D-RFCN S-NMS G
D-RFCN + MST
D-RFCN + MST + S-NMS G
Table 1. Results on MS-COCO test-dev set for R-FCN, D-RFCN and Faster-RCNN (F-RCNN) which use NMS as baseline and our
proposed Soft-NMS method. G denotes Gaussian weighting and L denotes linear weighting. MST denotes multi-scale testing.
 +S-NMS G
 +S-NMS L
 +S-NMS G
 +S-NMS L
Table 2. Results on Pascal VOC 2007 test set for off-the-shelf standard object detectors which use NMS as baseline and our proposed
Soft-NMS method. Note that COCO-style evaluation is used.
the MS-COCO dataset which consists of 20,288 images.
To evaluate our method, we experimented with three
state-of-the-art detectors, namely, Faster-RCNN , R-
FCN and Deformable-RFCN. For the PASCAL
dataset, we selected publicly available pre-trained models provided by the authors.
The Faster-RCNN detector was trained on VOC 2007 train set while the R-FCN
detector was trained on VOC 2007 and 2012.
COCO also, we use the publicly available model for Faster-
RCNN. However, since there was no publicly available
model trained on MS-COCO for R-FCN, we trained our
own model in Caffe starting from a ResNet-101 CNN
architecture . Simple modiﬁcations like 5 scales for
RPN anchors, a minimum image size of 800, 16 images
per minibatch and 256 ROIs per image were used. Training
was done on 8 GPUs in parallel. Note that our implementation obtains 1.9% better accuracy than that reported in 
without using multi-scale training or testing. Hence, this is
a strong baseline for R-FCN on MS-COCO. Both these detectors use a default NMS threshold of 0.3. In the sensitivity
analysis section, we also vary this parameter and show results. We also trained deformable R-FCN with the same settings. At a threshold of 10e-4, using 4 CPU threads, it takes
0.01s per image for 80 classes. After each iteration, detections which fall below the threshold are discarded. This
reduces computation time. At 10e-2, run time is 0.005 seconds on a single core. We set maximum detections per image to 400 on MS-COCO and the evaluation server selects
the top 100 detections per class for generating metrics . Setting maximum detections to 100 reduces coco-style AP by
6. Experiments
In this section, we show comparative results and perform
sensitivity analysis to show robustness of Soft-NMS compared to traditional NMS. We also conduct speciﬁc experiments to understand why and where does Soft-NMS perform better compared to traditional NMS.
6.1. Results
In Table 1 we compare R-FCN and Faster-RCNN with
traditional non-maximum suppression and Soft-NMS on
MS-COCO. We set Nt to 0.3 when using the linear weighting function and σ to 0.5 with the Gaussian weighting
function. It is clear that Soft-NMS (using both Gaussian
and linear weighting function) improves performance in all
cases, especially when AP is computed at multiple overlap thresholds and averaged. For example, we obtain an
improvement of 1.3% and 1.1% respectively for R-FCN
and Faster-RCNN, which is signiﬁcant for the MS-COCO
Note that we obtain this improvement by just
changing the NMS algorithm and hence it can be applied
easily on multiple detectors with minimal changes. We perform the same experiments on the PASCAL VOC 2007 test
set, shown in Table 1. We also report average precision
averaged over multiple overlap thresholds like MS-COCO.
Even on PASCAL VOC 2007, Soft-NMS obtains an improvement of 1.7% for both Faster-RCNN and R-FCN. For
detectors like SSD and YOLOv2 which are not
proposal based, with the linear function, Soft-NMS only
obtains an improvement of 0.5%. This is because proposal
Table 3. Sensitivity Analysis across multiple overlap thresholds Nt and parameters σ for NMS and Soft-NMS using R-FCN on coco
minival. Best performance at each Ot is marked in bold for each method.
AP [05:0.95]
Sensitivity to hyper parameter
Faster-RCNN σ
Faster-RCNN Nt
Figure 4. R-FCN Sensitivity to hyper parameters σ (Soft-NMS)
and Nt (NMS)
based detectors have higher recall and hence Soft-NMS has
more potential to improve recall at higher Ot.
From here on, in all experiments, when we refer to Soft-
NMS, it uses the Gaussian weighting function. In Fig 6,
we also show per-class improvement on MS-COCO. It is
interesting to observe that Soft-NMS when applied on R-
FCN improves maximum performance for animals which
are found in a herd like zebra, giraffe, sheep, elephant, horse
by 3-6%, while there is little gain for objects like toaster,
sports ball, hair drier which are less likely to co-occur in
the same image.
6.2. Sensitivity Analysis
Soft-NMS has a σ parameter and traditional NMS has
an overlap threshold parameter Nt. We vary these parameters and measure average precision on the minival set of
MS-COCO set for each detector, see Fig 4. Note that AP is
stable between 0.3 to 0.6 and drops signiﬁcantly outside this
range for both detectors. The variation in AP in this range is
around 0.25% for traditional NMS. Soft-NMS obtains better performance than NMS from a range between 0.1 to 0.7.
Its performance is stable from 0.4 to 0.7 and better by ∼1%
for each detector even on the best NMS threshold selected
by us on the coco-minival set. In all our experiments, we set
σ to 0.5, even though a σ value of 0.6 seems to give better
performance on the coco minival set. This is because we
conducted the sensitivity analysis experiments later on and
a difference of 0.1% was not signiﬁcant.
6.3. When does Soft-NMS work better?
Localization Performance Average precision alone
does not explain us clearly when Soft-NMS obtains signiﬁcant gains in performance. Hence, we present average
precision of NMS and Soft-NMS when measured at different overlap thresholds. We also vary the NMS and Soft-
NMS hyper-parameters to understand the characteristics of
both these algorithms. From Table 3, we can infer that average precision decreases as NMS threshold is increased.
Although it is the case that for a large Ot, a high Nt obtains
slightly better performance compared to a lower Nt — AP
does not drop signiﬁcantly when a lower Nt is used. On
the other hand, using a high Nt leads to signiﬁcant drop in
AP at lower Ot and hence when AP is averaged at multiple
thresholds, we observe a performance drop. Therefore, a
better performance using a higher Nt does not generalize to
lower values of Ot for traditional NMS.
However, when we vary σ for Soft-NMS, we observe
a different characteristic. Table 3 shows that even when
we obtain better performance at higher Ot, performance at
lower Ot does not drop. Further, we observe that Soft-NMS
performs signiﬁcantly better (∼2%) than traditional NMS
irrespective of the value of the selected Nt at higher Ot.
Also, the best AP for any hyper-parameter (Nt or σ) for a
selected Ot is always better for Soft-NMS. This comparison makes it very clear that across all parameter settings,
the best σ parameter for Soft-NMS performs better than
a hard threshold Nt selected in traditional NMS. Further,
when performance across all thresholds is averaged, since a
single parameter setting in Soft-NMS works well at multiple values of Ot, overall performance gain is ampliﬁed. As
expected, low values of σ perform better at lower Ot and
higher values of sigma perform better at higher Ot. Unlike
NMS, where higher values of Nt lead to very little improvement in AP, higher values of σ lead to signiﬁcant improvement in AP at a higher Ot. Therefore, a larger σ can be used
Precision vs Recall at Ot = 0.6
Precision vs Recall at Ot = 0.7
Precision vs Recall at Ot = 0.8
Figure 5. R-FCN : Precision vs Recall at multiple overlap thresholds Ot
to improve performance of the detector for better localization which is not the case with NMS, as a larger Nt obtains
very little improvement.
Precision vs Recall Finally, we would like to also know
at what recall values is Soft-NMS performing better than
NMS at different Ot. Note that we re-score the detection
scores and assign them lower scores, so we do not expect
precision to improve at a lower recall. However, as Ot and
recall is increased, Soft-NMS obtains signiﬁcant gains in
precision. This is because, traditional NMS assigns a zero
score to all boxes which have an overlap greater than Nt
with M. Hence, many boxes are missed and therefore precision does not increase at higher values of recall. Soft-
NMS re-scores neighboring boxes instead of suppressing
them altogether which leads to improvement in precision at
higher values of recall. Also, Soft-NMS obtains signiﬁcant
improvement even for lower values of recall at higher values of Ot because near misses are more likely to happen in
this setting.
6.4. Qualitative Results
We show a few qualitative results in Fig 7 using a detection threshold of 0.45 for images from the COCO-validation
set. The R-FCN detector was used to generate detections. It
is interesting to observe that Soft-NMS helps in cases when
bad detections (false positives) have a small overlap with a
good detection (true positive) and also when they have a low
overlap with a good detection. For example, in the street
image (No.8), a large wide bounding box spanning multiple people is suppressed because it had a small overlap with
multiple detection boxes with a higher score than it. Hence,
its score was reduced multiple times because of which it
was suppressed. We observe a similar behaviour in image
No.9. In the beach image (No.1), the score for the larger
bounding box near the woman’s handbag is suppressed below 0.45. We also see that a false positive near the bowl in
the kitchen image (No.4) is suppressed. In other cases, like
for zebra, horse and giraffe images (images 2,5,7 and 13),
the detection boxes get suppressed with NMS while Soft-
NMS assigns a slightly lower score for neighboring boxes
motorcycle
tennis racket
teddy bear
baseball bat
wine glass
skateboard
refrigerator
toothbrush
fire hydrant
dining table
traffic light
parking meter
potted plant
cell phone
baseball glove
sports ball
hair drier
motorcycle
tennis racket
teddy bear
baseball bat
skateboard
wine glass
refrigerator
parking meter
potted plant
fire hydrant
traffic light
dining table
toothbrush
baseball glove
cell phone
sports ball
hair drier
Figure 6. Per class improvement in AP for MS-COCO using Soft-
NMS for R-FCN is shown in the left and for Faster-RCNN is
shown on the right. Green bars indicate improvements beyond
because of which we are able to detect true positives above
a detection threshold of 0.45.