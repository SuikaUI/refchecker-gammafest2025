Global Context Enhanced Graph Neural Networks for
Session-based Recommendation
Ziyang Wang1, Wei Wei1,4, Gao Cong2, Xiao-Li Li3, Xian-Ling Mao4, Minghui Qiu5
1 Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and
Technology, Huazhong University of Science and Technology
2 School of Computer Engineering, Nanyang Technological University, Singapore
3 Institute for Infocomm Research, Singapore
4 School of Computer Science and Technology, Beijing Institute of Technology
5 Alibaba Group
1 {ziyang1997, weiw}@hust.edu.cn
2 
3 
4 
5 
Session-based recommendation (SBR) is a challenging task, which
aims at recommending items based on anonymous behavior sequences. Almost all the existing solutions for SBR model user preference only based on the current session without exploiting the
other sessions, which may contain both relevant and irrelevant
item-transitions to the current session. This paper proposes a novel
approach, called Global Context Enhanced Graph Neural Networks
(GCE-GNN) to exploit item transitions over all sessions in a more
subtle manner for better inferring the user preference of the current
session. Specifically, GCE-GNN learns two levels of item embeddings from session graph and global graph, respectively: (i) Session
graph, which is to learn the session-level item embedding by modeling pairwise item-transitions within the current session; and (ii)
Global graph, which is to learn the global-level item embedding
by modeling pairwise item-transitions over all sessions. In GCE-
GNN, we propose a novel global-level item representation learning
layer, which employs a session-aware attention mechanism to recursively incorporate the neighbors‚Äô embeddings of each node on
the global graph. We also design a session-level item representation
learning layer, which employs a GNN on the session graph to learn
session-level item embeddings within the current session. Moreover, GCE-GNN aggregates the learnt item representations in the
two levels with a soft attention mechanism. Experiments on three
benchmark datasets demonstrate that GCE-GNN outperforms the
state-of-the-art methods consistently.
CCS CONCEPTS
‚Ä¢ Information systems ‚ÜíRecommender systems.
4: Corresponding Author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from .
SIGIR ‚Äô20, July 25‚Äì30, 2020, Virtual Event, China
¬© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-8016-4/20/07...$15.00
 
Recommendation system; Session-based recommendation; Graph
neural network
ACM Reference Format:
Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, Minghui
Qiu. 2020. Global Context Enhanced Graph Neural Networks for Sessionbased Recommendation. In Proceedings of the 43rd International ACM SIGIR
Conference on Research and Development in Information Retrieval (SIGIR ‚Äô20),
July 25‚Äì30, 2020, Virtual Event, China. ACM, New York, NY, USA, 10 pages.
 
INTRODUCTION
Recommendation systems play critical roles on various on-line
platforms, due to their success in addressing information overload
problem by recommending useful content to users. Conventional
recommendation approaches (e.g., collaborative filtering ) usually rely on the availability of user profiles and long-term historical
interactions, and may perform poorly in many recent real-world
scenarios, e.g., mobile stream media like YouTube1 and Tiktok2,
when such information is unavailable (e.g., unlogged-in user) or
limited available (e.g., short-term historical interaction). Consequently, session-based recommendation has attracted extensive
attention recently, which predicts the next interested item based
on a given anonymous behavior sequence in chronological order.
Most of early studies on session-based recommendation fall into
two categories, i.e., similarity-based and chain-based . The
former heavily replies on the co-occurrence information of items
in the current session while neglecting the sequential behavior
patterns. The later infers all possible sequences of user choices over
all items, which may suffer from intractable computation problem
for real-world applications where the number of items is large. Recently, many deep learning based approaches are proposed for the
task, which make use of pairwise item-transition information to
model the user preference of a given session .
These approaches have achieved encouraging results, but they still
face the following issues. First, some of them infer the anonymous
user‚Äôs preference by sequentially extracting the session‚Äôs pairwise
item-transition information in chronological order using recurrent
1 
2 
 
Session 1:
Session 2:
Session 3:
Irrelevant
Figure 1: A toy example of global-level item transition modeling.
neural networks (RNN) (e.g., GRU4REC , NARM ) and memory networks (e.g., STAMP ). However, a session may contain
multiple user choices and even noise, and thus they may be insufficient in generating all correct dependencies, which suffer from
the inability of modeling the complicated inherent order of itemtransition patterns in embedding. Second, the others are based on
graph neural networks with self-attention mechanisms
such as SR-GNN . They learn the representation of the entire
session by calculating the relative importance based on the session‚Äôs pairwise item-transition between each item and the last one,
and the performance heavily rely on the relevance of the last item
to the user preference of the current session.
Furthermore, almost all the previous studies model user preference only based on the current session while ignoring the useful
item-transition patterns from other sessions. To the best of our
knowledge, CSRM is the only work incorporating collaborative
information from the latest ùëösessions to enrich the representation
of the current session in end-to-end manner. CSRM treats sessions
as the minimum granularity and measure similarities between the
current and the latest ùëösessions to extract collaborative information. However, it may unfortunately encode both relevant and
irrelevant information of the other sessions into the current session
embeddings, which may even deteriorate the performance . We
illustrate this with an example in Figure 1. Without loss of generality, suppose the current session is ‚ÄúSession 2‚Äù, and the session-based
recommendation aims to recommend the relevant accessories related to ‚ÄúIphone ‚Äù. From Figure 3, we observe that: (i) Utilizing the
item-transition of the other sessions might help model the user
preference of the current session. For example, we can find relevant
pairwise item-transition information for Session 2 from ‚ÄúSession
1‚Äù and ‚ÄúSession 3‚Äù, e.g., a new pairwise item-transition ‚Äú[Iphone,
Phone Case]‚Äù; and (ii) Directly utilizing the item-transition information of the entire other session may introduce noise when part
of the item-transition information encoded in such session is not
relevant to the current session. For instance, CSRM may also
consider to utilize ‚ÄúSession 3‚Äù to help modeling the user preference
of ‚ÄúSession 2‚Äù if ‚ÄúSession 3‚Äù is one of the latest ùëösessions, and it
will introduce the irrelevant items (i.e., ‚Äúclothes‚Äù and ‚Äútrousers‚Äù)
when learning ‚ÄúSession 2‚Äù‚Äôs embedding as it treats ‚ÄúSession 3‚Äù as a
whole without distinguishing relevant item-transition from irrelevant item-transition, which is challenging.
To this end, we propose a novel approach to exploit the itemtransitions over all sessions in a more subtle manner for better inferring the user preference of the current session for session-based
recommendation, which is named Global Context Enhanced Graph
Neural Networks (GCE-GNN). In GCE-GNN, we propose to learn
two levels of item embeddings from session graph and global graph,
respectively: (i) Session graph, which is to learn the session-level
item embedding by modeling pairwise item-transitions within the
current session; and (ii) Global graph, which is to learn the globallevel item embeddings by modeling pairwise item-transitions over
sessions (including the current session). In GCE-GNN, we propose a
novel global-level item representation learning layer, which employs
a session-aware attention mechanism to recursively incorporate
the neighbors‚Äô embeddings of each node on the global graph. We
also design a session-level item representation learning layer, which
employs a GNN on the session graph to learn session-level item
embeddings within the current session. Moreover, GCE-GNN aggregates the learnt item representations in the two levels with a
soft attention mechanism.
The main contributions of this work are summarized as follows:
‚Ä¢ To the best of our knowledge, this is the first work of exploiting global-level item-transitions over all sessions to learn
global-level contextual information for session-based recommendation.
‚Ä¢ We propose a unified model to improve the recommendation
performance of the current session by effectively leveraging
the pairwise item-transition information from two levels of
graph models, i.e., session graph and global graph.
‚Ä¢ We also propose a position-aware attention to incorporate
the reversed position information in item embedding, which
shows the superiority performance for session-based recommendation.
‚Ä¢ We conduct extensive experiments on three real-world datasets,
which demonstrate that GCE-GNN outperforms nine baselines including state-of-the-art methods.
RELATED WORK
Markov Chain-based SBR. Several traditional methods can be
employed for SBR although they are not originally designed for
SBR. For example, markov Chain-based methods map the current
session into a Markov chain, and then infer a user‚Äôs next action
based on the previous one. Rendle et al. propose FPMC to
capture both sequential patterns and long-term user preference
by a hybrid method based on the combination of matrix factorization and first-order Markov chain for recommendation. It can be
adapted for SBR by ignoring the user latent representation as it is
not available for anonymous SBR. However, MC-based methods
usually focus on modeling sequential transition of two adjacent
items. In contrast, our proposed model converts the sequentially
item-transitions into graph-structure data for capturing the inherent order of item-transition patterns for SBR.
Deep-learning based SBR. In recent years, neural network-based
methods that are capable of modeling sequential data have been
utilized for SBR. Hidasi et al. propose the first work called
GRU4REC to apply the RNN networks for SBR, which adopts a
multi-layer Gated Recurrent Unit (GRU) to model item interaction
sequences. Then, Tan et al. extend the method by introducing data augmentation. Li et al. propose NARM that incorporates
attention mechanism into stack GRU encoder to capture the more
representative item-transition information for SBR. Liu et al. 
propose an attention-based short-term memory networks (named
STAMP) to captures the user‚Äôs current interest without using RNN.
Both NARM and STAMP emphasize the importance of the last click
by using attention mechanism. Inspired byùëáùëüùëéùëõùë†ùëìùëúùëüùëöùëíùëü , SAS-
Rec stacks multiple layers to capture the relevance between
items. ISLF takes into account the user‚Äôs interest shift, and
employs variational auto-encoder (VAE) and RNN to capture the
user‚Äôs sequential behavior characteristics for SBR. MCPRN 
proposes to model the multi-purpose of a given session by using
a mixture-channel model for SBR. However, similar to MC-based
methods, RNN-based methods focus on modeling the sequential
transitions of adjacent items to infer user preference via the
chronology of the given sequence, and thus cannot model the complex item-transition patterns (e.g., non-adjacent item transitions).
Recently, several proposals employ GNN-based model on graph
built from the current session to learn item embeddings for SBR.
Wu et al. propose a gated GNN model (named SR-GNN) to
learn item embeddings on the session graph, and then obtain a
representative session embedding by integrating each learnt item
embedding with attentions, which is calculated according to the
relevance of each item to the last one. Following the success of
SR-GNN, some variants are also proposed for SBR, such as GC-SAN
 . Qiu et al. propose FGNN to learn each item representation by aggregating its neighbors‚Äô embeddings with multi-head
attention, and generate the final session representation by repeatedly combining each learnt embeddings with the relevance of each
time to the session. However, all these approaches only model the
item-transition information on the current session. In contrast, our
proposed model learns the item-transition information over all
sessions to enhance the learning from the current session.
Collaborative Filtering-based SBR. Although deep learning based
methods have achieved remarkable performance, collaborative filtering (CF) based methods can still provide competitive results.
Item-KNN can be extended for SBR by recommending items
that are most similar to the last item of the current session. KNN-
RNN makes use of GRU4REC and the co-occurrence-based
KNN model to extract the sequential patterns for SBR. Recently,
Wang et al. propose an end-to-end neural model named CSRM,
which achieves state-of-the-art performance. It first utilizes NARM
over item-transitions to encode each session, then enriches the
representation of the current session by exploring the latest ùëö
neighborhood sessions, and finally utilizes a fusion gating mechanism to learn to combine different sources of features. However, it
may suffer from noise when integrating other sessions‚Äô embeddings
for the current one. In contrast, our proposed method considers
the collaborative information in item-level: we use the item embeddings in other sessions to enrich the item embeddings of the current
session, and then integrate them into session representation for
PRELIMINARIES
In this section, we first present the problem statement, and then
introduce two types of graph models, i.e., session graph and global
graph, based on different levels of pair-wise item transitions over
sessions for learning item representations, in which we highlight
the modeling of global-level item transition information as it is the
basis of global graph construction.
Problem Statement
Let ùëâ= {ùë£1, ùë£2, ..., ùë£ùëö} be all of items. Each anonymous session,
which is denoted by ùëÜ= {ùë£ùë†
2, ..., ùë£ùë†
ùëô}, consists of a sequence of
interactions (i.e., items clicked by a user) in chronological order,
ùëñdenotes item ùë£ùëñclicked within session ùëÜ, and the length
Given a session ùëÜ, the problem of session-based recommendation
aims to recommend the top-ùëÅitems (1 ‚â§ùëÅ‚â§|ùëâ|) from ùëâthat are
most likely to be clicked by the user of the current session ùëÜ.
Graph Models: Session Graph and Global
In this subsection, we present two different graph models to capture
different levels of item transition information over all available
sessions for item representation learning.
Session Graph Model. Session-based graph aims to learn
the session-level item embedding by modeling sequential patterns
over pair-wise adjacent items in the current session. Inspired by
 , each session sequence is converted into a session graph for
learning the embeddings of items in the current session via GNN,
which is defined as follows, given session ùëÜ= {ùë£ùë†
2, ..., ùë£ùë†
Gùë†= (Vùë†, Eùë†) be the corresponding session graph, where Vùë†‚äÜùëâ
is the set of clicked items in ùëÜ, Eùë†= {ùëíùë†
ùëñùëó} denotes the edge set, in
which each edge indicates two adjacent items (ùë£ùë†
ùëó) in ùëÜ, which is
called session-level item-transition pattern. By following the work
 , each item is added a self loop (rf. Figure 2a).
Different from , our session graph has four types of edges
depending on the relationship between item ùëñand item ùëówhich
are denoted by ùëüùëñùëõ, ùëüùëúùë¢ùë°, ùëüùëñùëõ‚àíùëúùë¢ùë°and ùëüùë†ùëíùëôùëì. For edge (ùë£ùë†
indicates there is only transition from ùë£ùë†
ùëñ, ùëüùëúùë¢ùë°implies there
is only transition from ùë£ùë†
ùëó, and ùëüùëñùëõ‚àíùëúùë¢ùë°reveals there are both
transitions from ùë£ùë†
ùëñand from ùë£ùë†
ùëó; ùëüùë†ùëíùëôùëìrefers to the self
transition of an item.
Global Graph Model. Compared with traditional deep learningbased approaches (e.g., RNN-based ) that focus on modeling sequential patterns of the entire session, session graph can efficiently
capture complicated graph patterns of a session to learn sessionlevel item embeddings.
However, we also aim to capture item-transition information
from other sessions for learning representations of items, which is
called global-level item transition information.
Global-level Item Transition Modeling. Here, we take into account global-level item transitions for global-level item representation learning, via integrating all pairwise item transitions over
sessions. As such, we propose a novel global graph model for learning global-level item embeddings, which breaks down sequence
ùíóùüè‚Üíùíóùüê‚Üíùíóùüë‚Üíùíóùüê‚Üíùíóùüí
(a) Session Graph.
ùíîùíÜùíîùíîùíäùíêùíè ùüè: ùíóùüí‚Üíùíóùüè‚Üíùíóùüë‚Üíùíóùüê‚Üíùíóùüî‚Üíùíóùüë‚Üíùíóùüï
ùíîùíÜùíîùíîùíäùíêùíè ùüê: ùíóùüì‚Üíùíóùüë‚Üíùíóùüê‚Üíùíóùüî‚Üíùíóùüï
ùíîùíÜùíîùíîùíäùíêùíè ùüë: ùíóùüï‚Üíùíóùüê‚Üíùíóùüñ
Global-based Graph
ùíóùüê: {ùíóùüè, ùíóùüë, ùíóùüì, ùíóùüî, ùíóùüï, ùíóùüñ }
ùíóùüè ùíóùüê ùíóùüë ùíóùüí
ùíóùüê ùíóùüè ùíóùüë ùíóùüì ùíóùüî ùíóùüï ùíóùüñ
ùíóùüë ùíóùüè ùíóùüê ùíóùüí ùíóùüì ùíóùüî ùíóùüï
ùíóùüî ùíóùüê ùíóùüë ùíóùüï
ùíóùüï ùíóùüê ùíóùüë ùíóùüî ùíóùüñ
(b) Global Graph.
Figure 2: Illustrations of construction of session graph and
global graph.
independence assumption with linking all pairs of items based on
pairwise transitions over all sessions (including the current one).
Next, we firstly present a concept (i.e., ùúÄ-neighbor set) for modeling
global-level item transition, and then give the definition of global
Definition 1. ùúÄ-Neighbor Set (NùúÄ(ùë£)). For any item ùë£ùëù
ùëñin session ùëÜùëù, the ùúÄ-neighbor set of ùë£ùëù
ùëñindicates a set of items, each
element of which is defined as follows,
ùëñ‚Ä≤ ‚ààùëÜùëù‚à©ùëÜùëû; ùë£ùëù
ùëó‚ààùëÜùëû; ùëó‚àà[ùëñ
‚Ä≤ + ùúÄ];ùëÜùëù‚â†ùëÜùëû
‚Ä≤ is the order of item ùë£ùëù
ùëñin session ùëÜùëû, ùúÄis a hyperparameter
to control the scope of modeling of item-transition between ùë£ùëù
and the items in ùëÜùëû. Note that, parameter ùúÄfavors the modeling of
short-range item transitions over sessions, since it is helpless (even
noise, e.g., irrelevant dependence) for capturing the global-level
item transition information if beyond the scope (ùúÄ).
According to Definition 1, for each item ùë£ùëñ‚ààùëâ, global-level
item transition is defined as {(ùë£ùëñ, ùë£ùëó)|ùë£ùëñ, ùë£ùëó‚ààùëâ;ùë£ùëó‚ààNùúÄ(ùë£ùëñ)}.
Notably, we do not distinguish the direction of global-level item
transition information for efficiency.
Global Graph. Global graph aims to capture the global-level item
transition information, which will be used to learn item embeddings over all sessions. Specifically, the global graph is built based
on ùúÄ-neighbor sets of items in all sessions. Without loss of generality, global graph is defined as follows, let Gùëî= (Vùëî, Eùëî) be the
global graph, where Vùëîdenotes the graph node set that contains
all items in ùëâ, and Eùëî= {ùëíùëî
ùëñùëó|(ùë£ùëñ, ùë£ùëó)|ùë£ùëñ‚ààùëâ, ùë£ùëó‚ààNùúÄ(ùë£ùëñ)} indicates
the set of edges, each corresponding to two pairwise items from all
the sessions. Figure 2b shows an example of constructing the global
graph (ùúÄ= 2). Additionally, for each node ùë£ùëñ, we generate weight for
its adjacent edges to distinguish the importance of ùë£ùëñ‚Äôs neighbors
as follows: For each edge (ùë£ùëñ, ùë£ùëó) (ùë£ùëó‚ààNùëî
ùë£ùëñ), we use its frequency
over all the sessions as its weight of the corresponding edge; we
only keep top-ùëÅedges with the highest weights for each item ùë£ùëñon
graph Gùëîdue to efficiency consideration. Note that the definition
of the neighbors3 (i.e., Nùëî
ùë£) of item ùë£on graph Gùëîis same as NùúÄ(ùë£).
Hence, Gùëîis an undirected weighted graph as ùúÄ-neighbor set is undirected. During the testing phase, we do not dynamically update the
topological structure of global graph for efficiency consideration.
Remark. Each item in ùëâis encoded into an unified embedding
space at time-step ùë°, i.e., hùë°
ùëñ‚ààRùëë(ùëëindicates the dimension of item
embedding), which is feed with an initialization embedding h0
R|ùëâ|, here we use one-hot based embedding and it is transformed
into ùëë-dimensional latent vector space by using a trainable matrix
W0 ‚ààRùëë√ó|ùëâ|.
THE PROPOSED METHOD
We propose a novel Global Context Enhanced Graph Neural Networks
for Session-based Recommendation (GCE-GNN). GCE-GNN aims
to exploit both session-level and global-level pairwise item transitions for modeling the user preference of the current session for
recommendation. Figure 3 presents the architecture of GCE-GNN,
which comprises four main components: 1) global-level item representation learning layer. It learn global-level item embeddings over
all sessions by employing a session-aware attention mechanism to
recursively incorporate each node‚Äôs neighbors‚Äô embeddings based
on the global graph (Gùëî) structure; 2) session-level item representation learning layer. It employs a GNN model on session graph Gùë†to
learn session-level item embeddings within the current session; 3)
session representation learning layer It models the user preference of
the current session by aggregating the learnt item representations
in both session-level and global-level; 4) prediction layer. It outputs
the predicted probability of candidate items for recommendation.
We next present the four components in detail.
Global-level Item Representation Learning
We next present how to propagate features on global graph to
encode item-transition information from other sessions to help
recommendation.
Our layers are built based on the architecture of graph convolution network , and we generate the attention weights based
on the importance of each connection by exploiting the idea of
graph attention network . Here, we first describe a single layer,
which consists of two components: information propagation and
information aggregation, and then show how to generalize it to
multiple layers.
Information Propagation: An item may be involved in multiple
sessions, from which we can obtain useful item-transition information to effectively help current predictions.
To obtain the first-order neighbor‚Äôs features of itemùë£, one straightforward solution is to use mean pooling method . However, not
3We do not distinguish NùúÄ(ùë£) and Nùëî
ùë£when the context is clear and discriminative.
Session-level Item
Representation Learning Layer
Reversed Position Embedding
Prediction Layer
Attention Network
ùíóùüè‚Üíùíóùüê‚Üíùíóùüë‚Üíùíóùüê‚Üíùíóùüí
ùíóùüè‚Üíùíóùüì‚Üíùíóùüî‚Üíùíóùüí
Global-level Item Representation Learning Layer
Global-level Item Representation Learning Layer
Global Graph
Element-wise Sum
Element-wise Sum
Concatenation
Concatenation
Session Graph
Figure 3: An overview of the proposed framework. Firstly, a global graph is constructed based on all training session sequences.
Then for each session, a global feature encoder and local feature encoder will be used to extract node feature with global
context and local context. Then the model incorporates position information to learn the contribution of each item to the
next predicted item. Finally, candidate items will be scored.
all of items in ùë£‚Äôs ùúÄ-neighbor set are relevant to the user preference
of the current session, and thus we consider to utilize a sessionaware attention to distinguish the importance of items in (NùúÄ(ùë£)).
Therefore, each item in NùúÄ(ùë£) is linearly combined according to
the session-aware attention score,
ùúã(ùë£ùëñ, ùë£ùëó)hùë£ùëó,
where ùúã(ùë£ùëñ, ùë£ùëó) estimates the importance weight of different neighbors. Intuitively, the closer an item is to the preference of the current
session, the more important this item is to the recommendation.
Therefore we implement ùúã(ùë£ùëñ, ùë£ùëó) as follows:
ùúã(ùë£ùëñ, ùë£ùëó) = qùëá
1 LeakyRelu W1[(s ‚äôhùë£ùëó)‚à•ùë§ùëñùëó],
here we choose LeakyRelu as activation function, ‚äôindicates elementwise product, ‚à•indicates concatenation operation, ùë§ùëñùëó‚ààR1 is
the weight of edge (ùë£ùëñ, ùë£ùëó) in global graph, W1 ‚ààRùëë+1√óùëë+1 and
q1 ‚ààRùëë+1 are trainable parameters, s can be seen as the features
of current session, which is obtained by computing the average of
item representations of the current session,
Distinct from mean pooling, our approach makes the propagation
of information dependent on the affinity between ùëÜand ùë£ùëó, which
means neighbors that match the preference of current session will
be more favourable.
Then we normalize the coefficients across all neighbors connected with ùë£ùëñby adopting the softmax function:
ùúã(ùë£ùëñ, ùë£ùëó) =
exp  ùúã(ùë£ùëñ, ùë£ùëó)
ùë£ùëñexp  ùúã(ùë£ùëñ, ùë£ùëò) .
As a result, the final attention score is capable of suggesting which
neighbor nodes should be given more attention.
Information Aggregation: The final step is to aggregate the item
representation hùë£and its neighborhood representation ‚Ñéùëî
implement aggregator function agg as follows,
ùë£= relu W2[hùë£‚à•hNùëî
where we choose relu as the activation function and W2 ‚ààRùëë√ó2ùëë
is transformation weight.
Through a single aggregator layer, the representation of an item
is dependent on itself and its immediate neighbors. We could explore the high-order connectivity information through extending
aggregator from one layer to multiple layers, which allows more
information related to the current session to be incorporated into
the current representation. We formulate the representation of an
item in the ùëò-th steps as:
= agg h(ùëò‚àí1)
is representation of item ùë£which is generated from previous
information propagation steps, h(0)
is set as hùë£at the initial propagation iteration. In this way, the ùëò-order representation of an item
is a mixture of its initial representations and its neighbors up to ùëò
hops away. This enables more effective messages to be incorporated
into the representation of the current session.
Session-level Item Representation Learning
The session graph contains pairwise item-transitions within the
current session. We next present how to learn the session-level
item embedding.
As the neighbors of item in session graph have different importance to itself, we utilize attention mechanism to learn the weight
between different nodes. The attention coefficients can be computed
through element-wise product and non-linear transformation:
ùëíùëñùëó= LeakyReLU
where ùëíùëñùëóindicates the importance of node ùë£ùëó‚Äôs features to node ùë£ùëñ
and we choose LeakyReLU as activation function, ùëüùëñùëóis the relation
between ùë£ùëñand ùë£ùëóand a‚àó‚ààRùëëare weight vectors.
For different relations, we train four weight vectors, namely ùëéùëñùëõ,
ùëéùëúùë¢ùë°, ùëéùëñùëõ‚àíùëúùë¢ùë°and ùëéùë†ùëíùëôùëì. As not every two nodes are connected in
the graph, we only compute ùëíùëñùëófor nodes ùëó‚ààNùë†ùë£ùëñto inject the graph
structure into the model, where Nùë†ùë£ùëñis the first-order neighbors of
ùë£ùëñ. And to make coefficients comparable across different nodes, we
normalize the attention weights through softmax function:
ùë£ùëò‚ààNùë†ùë£ùëñexp  LeakyReLU  a‚ä§ùëüùëñùëò
In Eq. (8) the attention coefficients ùõºùëñùëóis asymmetric, as their neighbors are different, which means the contribution they make to each
other are unequal. Next we obtain the output features for each node
by computing a linear combination of the features corresponding
to the coefficients:
The item representations in session graph is aggregated by the features of item itself and its neighbor in the current session. Through
the attention mechanism, the impact of noise on the session-level
item representation learning is reduced.
Session Representation Learning Layer
For each item, we obtain its representations by incorporating both
global context and session context, and its final representation is
computed by sum pooling,
= dropout hùëî,(ùëò)
here we utilize dropout on global-level representation to avoid
overfitting.
Based on the learnt item representations, we now present how
to obtain the session representations. Different from previous work
 which mainly focus on the last item, in this paper we
propose a more comprehensive strategy to learn the contribution
of each part of the session for prediction.
In our method, a session representation is constructed based on
all the items involved in the session. Note that the contribution of
different items to the next prediction is not equal. Intuitively, the
items clicked later in the session are more representative of the
user‚Äôs current preferences, which shows their greater importance
for the recommendation. Moreover, it is important to find the main
purpose of the user and filter noise in current session . Hence we
incorporate reversed position information and session information
to make a better prediction.
After feeding a session sequence into graph neural networks,
we can obtain the representation of the items involved in the session, i.e., H =
2, ..., h‚Ä≤
. We also use a learnable position
embedding matrix P = [p1, p2, ..., pùëô], where pùëñ‚ààRùëëis a position vector for specific position ùëñand ùëôis the length of the current
session sequence. The position information is integrated through
concatenation and non-linear transformation:
where parameters W3 ‚ààRùëë√ó2ùëëand b3 ‚ààRùëëare trainable parameters. Here we choose the reversed position embedding because the length of the session sequence is not fixed. Comparing
to forward position information, the distance of the current item
from the predicted item contains more effective information, e.g.,
in the session {ùë£2 ‚Üíùë£3 ‚Üí?}, ùë£3 is the second in the sequence
and shows great influence to prediction, however in the session
{ùë£2 ‚Üíùë£3 ‚Üíùë£5 ‚Üíùë£6 ‚Üíùë£8 ‚Üí?}, the importance of ùë£3 would be
relatively small. Therefore the reversed position information can
more accurately suggest the importance of each item.
The session information is obtained by computing the average
of item representations of the session,
Next, we learn the corresponding weights through a soft-attention
mechanism:
2 ùúé W4zùëñ+ W5s‚Ä≤ + b4
where W4, W5 ‚ààRùëë√óùëëand q2, b4 ‚ààRùëëare learnable parameters.
Finally, the session representation can be obtained by linearly
combining the item representations:
The session representation S is constructed by all the items involved in the current session, where the contribution of each item
is determined not only by the information in the session graph, but
also by the chronological order in the sequence.
Prediction Layer
Based on the obtained session representations S, the final recommendation probability for each candidate item based on their initial
embeddings as well as current session representation, and we first
use dot product and then apply softmax function to obtain the
output ÀÜy:
ÀÜyùëñ= Softmax  S‚ä§hùë£ùëñ
where ÀÜyùëñ‚ààÀÜy denotes the probability of item ùë£ùëñappearing as the
next-click in the current session.
The loss function is defined as the cross-entropy of the prediction
results ÀÜy:
yùëñlog (ÀÜyùëñ) + (1 ‚àíyùëñ) log (1 ‚àíÀÜyùëñ) ,
where y denotes the one-hot encoding vector of the ground truth
EXPERIMENTS
We have conducted extensive experiments to evaluate the accuracy
of the proposed GCE-GNN method by answering the following five
key research questions:
‚Ä¢ RQ1: Does GCE-GNN outperform state-of-the-art SBR baselines in real world datasets?
‚Ä¢ RQ2: Does global graph and global-level encoder improve
the performance of GCE-GNN? How well does GCE-GNN
perform with different depth of receptive field ùëò?
Table 1: Statistics of the used datasets.
Diginetica
Nowplaying
‚Ä¢ RQ3: Is reversed position embedding useful?
‚Ä¢ RQ4: How well does GCE-GNN perform with different aggregation operations?
‚Ä¢ RQ5: How do different hyper-parameter settings (e.g., node
dropout) affect the GCE-GNN‚Äôs accuracy?
Datesets and Preprocessing
We employ three benchmark datasets, namely, ùê∑ùëñùëîùëñùëõùëíùë°ùëñùëêùëé4,ùëáùëöùëéùëôùëô5
and ùëÅùëúùë§ùëùùëôùëéùë¶ùëñùëõùëî6. Particularly, Diginetica dataset is from CIKM
Cup 2016, consisting of typical transaction data. Tmall dataset
comes from IJCAI-15 competition, which contains anonymized
user‚Äôs shopping logs on Tmall online shopping platform. Nowplaying dataset comes from , which describes the music listening
behavior of users.
Following , we conduct preprocessing step over the three
datasets. More specifically, sessions of length 1 and items appearing
less than 5 times were filtered across all the three datasets. Similar to
 , we set the sessions of last week (latest data) as the test data, and
the remaining historical data for training. Furthermore, for a session
ùëÜ= [ùë†1,ùë†2, ...,ùë†ùëõ], we generate sequences and corresponding labels
by a sequence splitting preprocessing, i.e., ([ùë†1] ,ùë†2), ([ùë†1,ùë†2] ,ùë†3),
..., ([ùë†1,ùë†2, ...,ùë†ùëõ‚àí1] ,ùë†ùëõ) for both training and testing across all the
three datasets. The statistics of datasets, after preprocessing, are
summarized in Table 1.
Evaluation Metrics
We adopt two widely used ranking based metrics: P@N and MRR@N
by following previous work .
Baseline Algorithms
We compare our method with classic methods as well as state-ofthe-art models. The following nine baseline models are evaluated.
POP: It recommends top-ùëÅfrequent items of the training set.
Item-KNN : It recommends items based on the similarity between items of the current session and items of other ones.
FPMC : It combines the matrix factorization and the first-order
Markov chain for capturing both sequential effects and user preferences. By following the previous work, we also ignore the user
latent representations when computing recommendation scores.
GRU4Rec7 : It is RNN-based model that uses Gated Recurrent
Unit (GRU) to model user sequences.
4 
5 
6 
7 
NARM8 : It improves over GRU4Rec by incorporating attentions into RNN for SBR.
STAMP9 : It employs attention layers to replace all RNN encoders in previous work by fully relying on the self-attention of
the last item in the current session to capture the user‚Äôs short-term
SR-GNN10 : It employs a gated GNN layer to obtain item embeddings, followed by a self-attention of the last item as STAMP 
does to compute the session level embeddings for session-based
recommendation.
CSRM11 : It utilizes the memory networks to investigate the
latest ùëösessions for better predicting the intent of the current
FGNN12 : It is recently proposed by designing a weighted attention graph layer to learn items embeddings, and the sessions for
the next item recommendation are learnt by a graph level feature
extractor.
Parameter Setup
Following previous methods , the dimension of the latent
vectors is fixed to 100, and the size for mini-batch is set to 100 for all
models. We keep the hyper-parameters of each model consistent for
a fair comparison. For CSRM, we set the memory size to 100 which
is consistent with the batch size. For FGNN, we set the number
of GNN layer to 3 and the number of heads is set to 8. For our
model, all parameters are initialized using a Gaussian distribution
with a mean of 0 and a standard deviation of 0.1. We use the Adam
optimizer with the initial learning rate 0.001, which will decay by
0.1 after every 3 epoch. The L2 penalty is set to 10‚àí5 and the dropout
ratio is searched in {0.1, 0.2, ..., 0.9} on a validation set which is a
random 10% subset of the training set. Moreover, we set the number
of neighbors and the maximum distance of adjacent items ùúÄto 12
and 3, respectively.
Overall Comparison (RQ1)
Table 2 reports the experimental results of the 9 baselines and
our proposed model on three real-world datasets, in which the
best result of each column is highlighted in boldface. It can be
observed that GCE-GNN achieves the best performance (statistically
significant) across all three datasets in terms of the two metrics
(with N=10, and 20) consistently, which ascertains the effectiveness
of our proposed method.
Among the traditional methods, POP‚Äôs performance is the worst,
as it only recommends top-ùëÅfrequent items. Comparing with POP,
FPMC shows its effectiveness on three datasets, which utilizes firstorder Markov chains and matrix factorization. Item-KNN achieves
the best results among the traditional methods on the Diginetica and
Nowplaying datasets. Note it only applies the similarity between
items and does not consider the chronological order of the items
8 
9 
10 
11 
12 
Table 2: Effectiveness comparison on three datasets.
Diginetica
Nowplaying
P@10 P@20 MRR@10 MRR@20 P@10 P@20 MRR@10 MRR@20 P@10 P@20 MRR@10 MRR@20
10.96 15.94
13.10 16.06
19.17 23.30
22.63 26.47
13.22 17.66
13.20 18.14
38.42 51.26
23.41 27.57
20.67 25.24
13.89 18.78
FGNN(reported)1
41.16 54.22
28.01 33.42
16.94 22.37
<0.001 <0.001
<0.001 <0.001
<0.001 <0.001
1 The codes of FGNN model released by the author are incomplete. For fairness, we compare our method with our re-implemented FGNN model as
well as the results reported in original paper.
in a session, and thus it cannot capture the sequential transitions
between items.
Compared with traditional methods, neural network based methods usually have better performance for session-based recommendation. In sprite of preforming worse than Item-KNN on Diginetica,
GRU4Rec, as the first RNN based method for SBR, still demonstrates
the capability of RNN in modeling sequences. However, RNN is
designed for sequence modeling, and session based recommendation problems are not merely a sequence modeling task because
the user‚Äôs preference may change within the session.
The subsequent methods, NARM and STAMP outperform GRU4REC
significantly. NARM combines RNN and attention mechanism,
which uses the last hidden state of RNN as the main preference
of user, this result indicates that directly using RNN to encode the
session sequence may not be sufficient for SBR as RNN only models one way item-transition between adjacent items in a session.
We also observe that STAMP, a complete attention-based method,
achieves better performance than NARM on Tmall, which incorporates a self-attention over the last item of a session to model the
short-term interest, this result demonstrates the effectiveness of
assigning different attention weights on different items for session
encoding. Compared with RNN, attention mechanism appears to be
a better option, although STAMP neglects the chronological order
of items in a session.
CSRM performs better than NARM and STAMP on Diginetica
and Tmall. It shows the effectiveness of using item transitions from
other sessions, and also shows the shortcomings of the memory
networks used by CSRM that have limited slots, additionally CSRM
treats other sessions as a whole one without distinguishing the
relevant item-transitions from the irrelevant ones encoded in other
Among all the baseline methods, the GNN-based methods perform better on the Diginetica and Nowplaying datasets. By modeling every session sequence as a subgraph and applying GNN to
encode items, SR-GNN and FGNN demonstrate the effectiveness
of applying GNN in session-based recommendation. This indicates
that the graph modeling would be more suitable than the sequence
modeling, RNN, or a set modeling, the attention modeling for SBR.
Our approach GCE-GNN outperforms SR-GNN and FGNN on all
the three datasets. Specifically, GCE-GNN outperforms the SR-GNN
by 6.86% on Diginetica, 16.34% on Tmall and 15.71% on Nowplaying on average. Different from SR-GNN and FGNN, our approach
integrates information from global context, i.e., other session, and
local context, i.e., the current session, and also incorporates relative
position information, leading to consistent better performance.
Impact of Global Feature Encoder (RQ2)
We next conduct experiments on three datasets to evaluate the effectiveness of global-level feature encoder and session-level feature
encoder. Specially, we design four contrast models:
‚Ä¢ GCE-GNN w/o global: GCE-GNN without global-level feature encoder and only with local feature
‚Ä¢ GCE-GNN w/o session: GCE-GNN without session-level
feature encoder and only with global feature
‚Ä¢ GCE-GNN-1-hop: GCE-GNN with global-level feature encoder, which sets the number of hop to 1.
‚Ä¢ GCE-GNN-2-hop: GCE-GNN with global-level feature encoder, which sets the number of hop to 2.
Table 3 shows the comparison between different contrast models. It
is clear that with global-level feature encoder, GCE-GNN achieves
better performance. Comparing with GCE-GNN w/o global context,
GCE-GNN with 1-hop and 2-hop global-level feature encoder can
explore item-transition information from other sessions, which
helps the model to make more accurate predictions. It can also be
observed that GCE-GNN with 2-hop performs better than GCE-
GNN with 1-hop on Diginetica, indicating that high-level exploring
might obtain more effective information from global graph. In addition, GCE-GNN with 1-hop performs better than GCE-GNN with
2-hop on Tmall, and this indicates that higher-level exploring might
also introduce noise.
Table 3: The performance of contrast models.
Diginetica
Nowplaying
P@20 MRR@20 P@20 MRR@20 P@20 MRR @20
w/o global
w/o session 51.46
Table 4: The performance of contrast models.
Diginetica
Nowplaying
P@20 MRR@20 P@20 MRR@20 P@20 MRR @20
GCE-GNN-NP 50.45
GCE-GNN-SA 51.68
Table 5: Effects of different aggregation operations.
Diginetica
Nowplaying
P@20 MRR@20 P@20 MRR@20 P@20 MRR @20
Gate Mechanism 53.84
Max Pooling
Concatenation
Sum Pooling
Impact of Position Vector (RQ3)
The position vector is used to drive GCE-GNN to learn the contribution of each part in the current session. Although SASRec 
has injected forward position vector into the model to improve performance, we argue that forward position vector has very limited
effect on the SBR task. To verify this and evaluate the effectiveness
of using the position vector in a reverse order, which is proposed
in GCE-GNN, we design a series of contrast models:
‚Ä¢ GCE-GNN-NP: GCE-GNN with forward position vector replacing the reverse order position vector.
‚Ä¢ GCE-GNN-SA: GCE-GNN with self attention function replacing the position-aware attention.
Table 4 shows the performance of different contrast models. We observe that our attention network with reversed position embedding
performs better than the other two variants.
GCE-GNN-NP does not perform well on all datasets. That is
because the model cannot capture the distance from each item to
the predicted item, which will mislead the model when training for
sessions of various lengths.
GCE-GNN-SA performs better than GCE-GNN-NP on three
datasets, indicating that the last item in a session contains the
most relevant information for recommendation. However, it does
not perform well on Tmall dataset, as it lacks a more comprehensive
judgment of the contribution of each item.
Comparing with the two variants, reversed position embedding
demonstrates its effectiveness. This confirms that the reversed position information can more accurately suggest the importance of
each item. Moreover, though the attention mechanism, we filter
the noise in the current session, which makes the model perform
Dropout Ratio
(a) Diginetica
Dropout Ratio
Figure 4: Impact of dropout ratio.
Impact of Aggregation Operations (RQ4)
As we use local feature encoder and global feature encoder, it is
meaningful to compare GCE-GNN with different aggregation operations, i.e.,, gating mechanism, max pooling and concatenation
mechanism.
For gating mechanism, we use a linear interpolation between
local feature representation ‚Ñéùëôand global feature representation
rùë£= ùúé(Wùë†hùë†
ùë£+ (1 ‚àírùë£)hùë†
where ùúéis the sigmoid activation function and rùë£is learned to
balance the importance of two features.
For max pooling, we take the maximum value of every dimension
for each feature, and the ùëñ‚àíth dimension of an item representation
ùë£ùëñis formulated as
ùë£ùëñ= max(hùëî
For the concatenation operation, the final representation is the
concatenation of vectors hùëî
where M ‚ààRùëë√ó2ùëëis the transformer weight.
Table 5 shows the performance of different aggregation operations on the three datasets. It can be observed that GCE-GNN with
sum pooling outperforms other aggregation operations on Diginetica and Tmall in terms of Recall@20 and MRR@20. Max pooling‚Äôs
performance is the worst on Diginetica but it performs better than
the other two aggregators on Tmall in terms of MRR@20. Despite of
using additional parameters, Gate mechanism and Concatenation‚Äôs
performance is also worse than sum pooling, possibly because too
many parameters may lead to overfitting.
Impact of Dropout Setting (RQ5)
To prevent GCE-GNN from overfitting, we employ dropout 
regularization techniques, which have been shown to be effective
in various neural network architectures including graph neural
networks . The key idea of dropout is to randomly drop
neurons with probability ùëùduring training, while using all neurons for testing. Figure 4 shows the impact of dropout in Equation
(10) on Diginetica and Tmall datasets. We can observe that when
dropout ratio is small, the model does not perform well on both
datasets, as it is easy to overfit. It achieves the best performance
when dropout ratio is set to 0.4 on Diginetica and 0.6 on Tmall.
However, when dropout ratio is big, the performance of the model
starts to deteriorate, as it is hard for the model to learn from data
with limited available neurons.
CONCLUSION
This paper studies the problem of session-based recommendation,
which is a challenging task as the user identities and historical
interactions are often unavailable due to privacy and data protection concern. It proposes a novel architecture for session-based
recommendation based on graph neural network. Specifically, it
first converts the session sequences into session graphs and construct a global graph. The local context information and global
context information are subsequently combined to enhance the
feature presentations of items. Finally, it incorporates the reversed
position vector and session information to empower the proposed
model to better learn the contribution of each item. Comprehensive
experiments demonstrate that the proposed method significantly
outperforms nine baselines over three benchmark datasets consistently.
ACKNOWLEDGMENTS
This work was supported in part by the National Natural Science Foundation of China under Grant No.61602197 and Grant
No.61772076, and in part by Equipment Pre-Research Fund for The
13th Five-year Plan under Grant No.41412050801.