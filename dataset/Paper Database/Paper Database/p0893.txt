Multiscale Combinatorial Grouping
Pablo Arbel´aez1,⇤Jordi Pont-Tuset2,⇤
Jonathan T. Barron1
Ferran Marques2
Jitendra Malik1
1University of California, Berkeley
Berkeley, CA 94720
{arbelaez,barron,malik}@eecs.berkeley.edu
2Universitat Polit`ecnica de Catalunya, BarcelonaTech
Barcelona, Spain
{jordi.pont,ferran.marques}@upc.edu
We propose a uniﬁed approach for bottom-up hierarchical image segmentation and object candidate generation
for recognition, called Multiscale Combinatorial Grouping
(MCG). For this purpose, we ﬁrst develop a fast normalized cuts algorithm. We then propose a high-performance
hierarchical segmenter that makes effective use of multiscale information. Finally, we propose a grouping strategy
that combines our multiscale regions into highly-accurate
object candidates by exploring efﬁciently their combinatorial space. We conduct extensive experiments on both the
BSDS500 and on the PASCAL 2012 segmentation datasets,
showing that MCG produces state-of-the-art contours, hierarchical regions and object candidates.
1. Introduction
Two paradigms have shaped the ﬁeld of object recognition in the last decade.
The ﬁrst one, popularized by
the Viola-Jones face detection algorithm , formulates
object localization as window classiﬁcation.
scanning-window architecture, relying on histograms of
gradients and linear support vector machines, was introduced by Dalal and Triggs in the context of pedestrian
detection and is still at the core of leading object detectors
on the PASCAL challenge such as Deformable Part Models
The second paradigm relies on perceptual grouping to
provide a limited number of high-quality and categoryindependent object candidates, which can then be described
with richer representations and used as input to more sophisticated learning methods.
Examples in this family
are . Recently, this approach has dominated the
PASCAL segmentation challenge , improved object
detection and proven competitive in large-scale classi-
ﬁcation .
⇤First two authors contributed equally
Figure 1. Top: original image, instance-level groundtruth from
PASCAL and our multiscale hierarchical segmentation. Bottom:
our best object candidates among 400.
Since the power of this second paradigm is critically dependent on the accuracy and the number of object candidates, an increasing body of research has delved into the
problem of their generation . However, those
approaches typically focus on learning generic properties of
objects from a set of examples, while reasoning on a ﬁxed
set of regions and contours produced by external bottom-up
segmenters such as .
In this paper, we propose a uniﬁed approach to multiscale hierarchical segmentation and object candidate generation called Multiscale Combinatorial Grouping (MCG).
Fig. 1 shows an example of our results and Fig. 2 an
overview of our pipeline. Our main contributions are:
• An efﬁcient normalized cuts algorithm, which in practice provides a 20⇥speed-up to the eigenvector computation required for contour globalization (Sect. 3.1).
• A state-of-the-art hierarchical segmenter that leverages multiscale information (Sect. 4).
• A grouping algorithm that produces accurate object
candidates by efﬁciently exploring the combinatorial space
of our multiscale regions (Sect. 6).
We conduct a comprehensive empirical validation. On the
BSDS500 (Sect. 5) we report the best results to date in
contour detection and hierarchical segmentation. On the
VOC2012 segmentation dataset (Sect. 7), our candidates
obtain overall state-of-the-art object-level accuracy. At a
regime of 1100 candidates per image (c/i), we report the
best results on 12/20 object categories and a relative improvement of +20% over Selective Search . At 100 c/i,
our candidates provide a relative improvement of +7.8%
over CPMC .
2. Related Work
For space reasons, we focus our review on recent normalized cut algorithms and object candidates for recognition.
Fast normalized cuts
The efﬁcient computation of
normalized-cuts eigenvectors has been the subject of recent
work, as it is often the computational bottleneck in grouping algorithms. Taylor presented a technique for using
a simple watershed oversegmentation to reduce the size of
the eigenvector problem, sacriﬁcing accuracy for speed. We
take a similar approach of solving the eigenvector problem
in a reduced space, though we use simple image-pyramid
operations on the afﬁnity matrix (instead of a separate segmentation algorithm) and we see no loss in performance
despite a 20⇥speed improvement. Maire and Yu presented a novel multigrid solver for producing eigenvectors
at multiple scales, which speeds up ﬁne-scale eigenvector computation by leveraging coarse-scale solutions. Our
technique also uses the scale-space structure of an image,
but instead of solving the problem at multiple scales, we
simply reduce the scale of the problem, solve it at a reduced
scale, and then upsample the solution while preserving the
structure of the image. As such, our technique is faster and
much simpler, requiring only a few lines of code wrapped
around a standard sparse eigensolver.
Object Candidates
Class-independent methods that generate object hypotheses can be divided into those whose output is an image window and those that generate segmented
candidates.
Among the former, Alexe et al. propose an objectness
measure to score randomly-sampled image windows based
on low-level features computed on the superpixels of .
Van de Sande et al. present a selective window search
based on segmentation. Starting with the superpixels of 
for a variety of color spaces, they produce a set of segmentation hierarchies by region merging, which are used to
produce a set of object candidate windows. While we also
take advantage of different hierarchies to gain diversity, we
leverage multiscale information rather than different color
spaces. Furthermore, in contrast to we focus on the
ﬁner-grained task of pixel-accurate object extraction, rather
than on window selection.
Among the methods that produce segmented candidates,
Carreira and Sminchisescu hypothesize a set of placements of fore- and background seeds and, for each con-
ﬁguration, solve a constrained parametric min-cut (CPMC)
problem to generate a pool of object hypotheses. Endres
and Hoiem base their category-independent object proposals on an iterative generation of a hierarchy of regions,
based on the contour detector of and occlusion boundaries of . Kim and Grauman propose to match parts
of the shape of exemplar objects, regardless of their class, to
detected contours by . They infer the presence and shape
of a candidate object by adapting the matched object to the
computed superpixels.
Recently, two works proposed to train a cascade of classiﬁers to learn which sets of regions should be merged to
form objects. Ren and Shankhnarovich produce full
region hierarchies by iteratively merging pairs of regions
and adapting the classiﬁers to different scales. Weiss and
Taskar specialize the classiﬁers also to size and class
of the annotated instances to produce object candidates.
Malisiewicz and Efros took one of the ﬁrst steps
towards combinatorial grouping, by running multiple segmenters with different parameters and merging up to three
adjacent regions. In , another step was taken by considering hierarchical segmentations at three different scales and
combining pairs and triplets of adjacent regions from the
two coarser scales to produce object candidates.
A substantial difference between our approach and previous work is that, instead of relying on pre-computed hierarchies or superpixels, we propose a uniﬁed approach that
produces and groups high-quality multiscale regions. With
respect to the combinatorial approaches of , our main
contribution is to develop efﬁcient algorithms to explore a
much larger combinatorial space by taking into account a
set of object examples, increasing the likelihood of having
complete objects in the pool of candidates. Our approach
has therefore the ﬂexibility to adapt to speciﬁc applications
and types of objects, and can produce candidates at any
trade-off between their number and their accuracy.
3. The Segmentation Algorithm
Consider a segmentation of the image into regions that
partition its domain S = {Si}i. A segmentation hierarchy is a family of partitions {S⇤, S1, ..., SL} such that: (1)
S⇤is the ﬁnest set of superpixels, (2) SL is the complete
domain, and (3) regions from coarse levels are unions of regions from ﬁne levels. A hierarchy where each level Si is
assigned a real-valued index λi can be represented by a dendrogram, a region tree where the height of each node is its
index. Furthermore, it can also be represented as an ultrametric contour map (UCM), an image obtained by weighting the boundary of each pair of adjacent regions in the hierarchy by the index at which they are merged . This
representation uniﬁes the problems of contour detection and
hierarchical image segmentation: a threshold at level λi in
the UCM produces the segmentation Si.
Fixed-Scale
Segmentation
Rescaling &
Combination
Resolution
Combinatorial
Image Pyramid
Segmentation Pyramid
Aligned Hierarchies
Candidates
Multiscale Hierarchy
Figure 2. Multiscale Combinatorial Grouping. Starting from a multiresolution image pyramid, we perform hierarchical segmentation
at each scale independently. We align these multiple hierarchies and combine them into a single multiscale segmentation hierarchy. Our
grouping component then produces a ranked list of object candidates by efﬁciently exploring the combinatorial space of these regions.
As an example, in the gPb-ucm algorithm of , brightness, color and texture gradients at three ﬁxed disk sizes are
ﬁrst computed. These local contour cues are globalized using spectral graph-partitioning, resulting in the gPb contour
detector. Hierarchical segmentation is then performed by
iteratively merging adjacent regions based on the average
gPb strength on their common boundary. This algorithm
produces therefore a tree of regions at multiple levels of homogeneity in brightness, color and texture, and the boundary strength of its UCM can be interpreted as a measure of
Coarse-to-ﬁne is a powerful processing strategy in computer vision. We exploit it in two different ways to develop
an efﬁcient, scalable and high-performance segmentation
algorithm: (1) To speed-up spectral graph partitioning and
(2) To create aligned segmentation hierarchies.
3.1. Fast Downsampled Eigenvector Computation
The normalized cuts criterion is a key globalization
mechanism of recent high-performance contour detectors
such as ; Although powerful, such spectral graph partitioning has a signiﬁcant computational cost and memory
footprint that limit its scalability. In this section, we present
an efﬁcient normalized cuts algorithm which in practice
preserves full performance for contour detection, has low
memory requirements and provides a 20⇥speed-up.
Given a symmetric afﬁnity matrix A, we would like to
compute the k smallest eigenvectors of the Laplacian of A.
Directly computing such eigenvectors can be very costly
even with sophisticated solvers, due to the large size of A.
We therefore present a technique for approximating them
much more efﬁciently by taking advantage of the multiscale
nature of our problem: A models afﬁnities between pixels
in an image, and images naturally lend themselves to multiscale or pyramid-like representations and algorithms.
Our algorithm is inspired by two observations: 1) if A
is bistochastic (the rows and columns of A sum to 1) then
the eigenvectors of the Laplacian A are equal to the eigenvectors of the Laplacian of A2, and 2) because of the scalesimilar nature of images, the eigenvectors of a “downsampled” version of A in which every other pixel has been removed should be similar to the eigenvectors of A. Let us
deﬁne pixel decimate (A), which takes an afﬁnity matrix A and returns the indices of rows/columns in A corresponding to a decimated version of the image from which
A was constructed. That is, if i = pixel decimate (A),
then A [i, i] is a decimated matrix in which alternating rows
and columns of the image have been removed. Computing
the eigenvectors of A [i, i] works poorly, as decimation disconnects pixels in the afﬁnity matrix, but the eigenvectors
of the decimated squared afﬁnity matrix A2 [i, i] are similar to those of A, because by squaring the matrix before
decimation we intuitively allow each pixel to propagate information to all of its neighbors in the graph, maintaining
connections even after decimation. Our algorithm works by
efﬁciently computing A2 [i, i] as A [:, i]T A [:, i] (the naive
approach of ﬁrst squaring A and then decimating it is intractable), computing the eigenvectors of A2 [i, i], and then
“upsampling” those eigenvectors back to the space of the
original image by multiplying by A [:, i]. This squaringand-decimation procedure can be applied recursively several times, improving efﬁciency while sacriﬁcing accuracy.
Pseudocode for our algorithm, which we call “DNCuts”
(downsampled normalized cuts) is given in Alg. 1, where A
is our afﬁnity matrix and D is the number of times that our
squaring-and-decimation operation is applied.
Our algorithm repeatedly applies our joint squaring-and-decimation
procedure, computes the smallest k eigenvectors of the
ﬁnal “downsampled” matrix AD by using a standard
sparse eigensolver ncuts(AD, K), and repeatedly “upsamples” those eigenvectors. Because our A is not bistochastic
and decimation is not an orthonormal operation, we must do
some normalization throughout the algorithm (line 5) and
whiten the resulting eigenvectors (line 10). We found that
Algorithm 1 dncuts(A, D, K)
2: for d = [1 : D] do
id pixel decimate (Ad−1)
Bd Ad−1 [ : , id ]
Cd diag(Bd~1)−1Bd
7: XD ncuts(AD, K)
8: for d = [D : −1 : 1] do
10: return whiten(X0)
values of K = 2 or K = 3 worked well in practice. Larger
values of K yielded little speed improvement (as much of
the cost is spent downsampling A0) and start hurting performance. Our technique is similar to Nystrom’s method
for computing the eigenvectors of a subset of A, but our
squaring-and-decimation procedure means that we do not
depend on long-range connections between pixels.
3.2. Aligning Segmentation Hierarchies
Spatially transforming an UCM is nontrivial because
its boundaries are one-dimensional entities whose topology
and strength determine the underlying hierarchy, and an error at a single pixel can have drastic effects. We therefore
opt for sampling uniformly K levels in the hierarchy, transforming them sequentially, and reconstructing from their
boundaries a transformed UCM.
We consider two different segmentations R = {Ri}i and
S = {Sj}j. We deﬁne the projection of the segmentation
R onto a region Sj 2 S as the majority label
⇡(R, Sj) = argmax
And the projection of R onto S as
⇡(R, S) = {⇡(R, Sj)}j.
In order to project an UCM onto a target segmentation S,
which we denote ⇡(UCM, S), we project each of the levels
in the hierarchy in turn.
In the next section, we will iterate this procedure, and
project an UCM recursively to a set of target segmentations
{S1⇤, ..., SN⇤}. However, note that the composition of two
such projections can be written as :
⇡(⇡(UCM, S1), S2) = ⇡(UCM, S1) ◦⇡(S1, S2).
In practice, this property means that successive projections
of the target segmentations can be pre-computed, the UCM
has to be projected only to the ﬁrst target segmentation, and
its ﬁnal labels are obtained by N −1 look-ups. This procedure is summarized in pseudo-code in Algorithm 21.
1Note that, by construction, the routines sampleHierarchy and
Algorithm 2 UCM Rescaling and Alignment
Require: An UCM and a set of levels [t1, ..., tK]
Require: A set of target segmentations {S1⇤, ..., SN⇤}
1: Pre-compute target projections:
2: ⇡(S1⇤, S2⇤), ⇡(⇡(S1⇤, S2⇤), S3⇤), ...
4: for t = [t1, ..., tK] do
S sampleHierarchy(UCM, t)
S rescaleSegmentation(S, S1⇤)
S ⇡(S, S1⇤)
S readLabels(S, {S1⇤, ..., SN⇤})
contours extractBoundary(S)
UCM⇡ max(UCM⇡, t ⇤contours)
11: return UCM⇡
4. Multiscale Hierarchical Segmentation
Single-scale segmentation
We consider as input the following local contour cues: (1) brightness, color and texture
differences in half-disks of three sizes , (2) sparse coding on patches , and (3) structured forest contours .
We globalize the contour cues independently using our fast
eigenvector gradients of Sect. 3.1, combine global and local cues linearly, and construct an UCM based on the mean
contour strength. We tried learning weights using gradient
ascent on the F-measure on the train set , but evaluating the ﬁnal hierarchies rather than open contours. We observed that this objective favors the quality of contours at
the expense of regions and obtained better overall results by
optimizing the Cover metric.
Hierarchy Alignment
We construct a multiresolution
pyramid with N scales by subsampling / supersampling
the original image and applying our single-scale segmenter.
In order to preserve thin structures and details, we declare
as set of possible boundary locations the ﬁnest superpixels
SN⇤in the highest-resolution. We extract the ﬁnest superpixels of each hierarchy, rescale them to the original image
resolution, pre-compute their successive projections to SN⇤
and then transfer recursively the strength of all the coarser
UCMs by applying Algorithm 2.
Multiscale Hierarchy
After alignment, we have a ﬁxed
set of boundary locations, and N strengths for each of them,
coming from the different scales. We formulate this as a binary boundary classiﬁcation problem and train a classiﬁer
that combines these N features into a single probability of
boundary estimation. We experimented with several learning strategies for combining UCM strengths: (a) Uniform
weights transformed into probabilities with Platt’s method.
extractBoundary are fast, as they involve only connected component
labeling and thresholding operations. The complexity is thus dominated
by the transformations in Steps 6 and 7, which are computed K times.
(b) SVMs and logistic regression, with both linear and additive kernels. (c) Random Forests. (d) The same algorithm as for single-scale.
We found the results with all
learning methods surprisingly similar, in agreement with
the observation reported by . This particular learning
problem, with only a handful of dimensions and millions
of data points, is relatively easy and performance is mainly
driven by our already high-performing and well calibrated
features. We therefore use the simplest option (a).
5. Experiments on the BSDS500
We conduct extensive experiments on the BSDS500
 , using the standard evaluation metrics and following
the best practice rules of that dataset. We also report results
with a recent evaluation metric Fop , Precision-Recall
for objects and parts, using the publicly-available code.
Single-scale Segmentation
Table 1-top shows the performance of our single-scale segmenter for different types of
input contours on the validation set of the BSDS500. We
obtain high-quality hierarchies for all the cues considered,
showing the generality of our approach. Furthermore, when
using them jointly (row ’single-combined’), our segmenter
outperforms the versions with individual cues, suggesting
its ability to leverage diversiﬁed inputs. In terms of efﬁciency, our fast normalized cuts algorithm provides an average 20⇥speed-up over , starting from the same local
cues, with no signiﬁcant loss in accuracy and with a low
memory footprint.
Multiscale Segmentation
Table 1-bottom evaluates our
full approach in the same experimental conditions as the
upper panel. We observe a consistent improvement in performance in all the metrics for all the inputs, which validates
our architecture for multiscale segmentation. We tested also
two degraded versions of our system (not shown in the table). For the ﬁrst one, we resized contours to the original image resolution, created UCMs and combined them.
For the second one, we transformed per-scale UCMs to the
original resolution, but ommited the strength transfer to the
ﬁnest superpixels. The ﬁrst ablated version produces interpolation artifacts and smooths away details, while the second one suffers from misalignment. Both fail to improve
the performance of the single-scale result, which provides
additional empirical support for our multiscale approach.
Since there are no major changes in our results when taking as input the different individual cues or their combination, in the sequel we use the version with structured forests
for efﬁciency reasons, which we denote Ours-multi.
Comparison with state-of-the-art.
Figure 3 compares
our multiscale hierarchical segmenter on the BSDS500 test
set against all the methods for which there is publicly available code. We also compare to the recent ISCRA hierarchies, provided precomputed by the authors. We obtain
Boundaries
Human [0.81-0.21]
NCuts [0.63]
GTH [0.96]
EGB [0.61]
Ours-multi [0.75]
MShift [0.60]
gPb-UCM [0.73]
Quadtree [0.41]
ISCRA [0.72]
Objects and Parts
Human [0.56-0.05]
GTH [0.64]
Ours-multi [0.38]
ISCRA [0.35]
gPb-UCM [0.35]
MShift [0.23]
NCuts [0.21]
EGB [0.16]
Quadtree [0.06]
Figure 3. BSDS500 test set. Precision-Recall curves for boundaries (left) and for objects and parts (right). The marker
on each curve is placed on the Optimal Dataset Scale (ODS). The
isolated red asterisks refer to the human performance assessed on
the same image and on a swapped image. In the legend, the F measure of the marked point on each curve is presented in brackets.
Single-Scale
Multiscale
Table 1. BSDS500 val set. Control experiments for single-scale
(top) and multiscale (bottom) hierarchical segmentation.
consistently the best results to date on the BSDS for all operating regimes, both in terms of boundary and region quality.
Note that the range of object scales in the BSDS500
is limited, which translates into modest absolute gains for
multiscale segmentation. However, we will observe more
substantial improvements when we move to PASCAL in
Section 7 (See Fig. 5).
Ground-Truth Hierarchy.
In order to gain further insights, we transfer the strength of each ground-truth segmentation to our highest-resolution superpixels SN⇤and
construct a combined hierarchy. This approximation to the
semantic hierarchy, “GTH” in Fig. 3, is an upper-bound for
our approach as both share the same boundary locations and
the only difference is their strength. Since the strength of
GTH is proportional to the number of subjects marking it,
it provides naturally the correct semantic ordering, where
outer object boundaries are stronger than internal parts.
Recently, Maire et al. developed an annotation tool
where the user encodes explicitly the “perceptual strength”
of each contour.
Our approach provides an alternative
where the semantic hierarchy is reconstructed by sampling
ﬂat annotations from multiple subjects.
6. Object Candidate Generation
Our proposal for object candidate generation is to create
a large-enough set of hypotheses with a very high achievable quality (Sect. 6.1) and then to learn to rank them using
low-level features (Sect. 6.2) to keep the maximum quality.
6.1. Combinatorial Grouping of Candidates
We consider the singletons, pairs, triplets, and 4-tuples of
regions from the three individual scales and the multiscale
hierarchy as 16 lists of ranked candidates. Since the full
joint set of candidates is very large and redundant, our goal
is to reduce it by keeping only the top Ni candidates from
each ranked list Li, resulting in a total number of candidates
At training time, we would like to ﬁnd, for different values of Nc, the number of candidates from each list N i such
that the joint pool of Nc candidates has the best achievable
quality. We frame this learning problem as a Pareto front
optimization with two conﬂicting objective functions:
number of candidates and achievable quality. At test time,
we select a working point on the Pareto front, represented
values, based either on the number of candidates Nc we can handle or on the minimum achievable
quality our application needs, and we combine the N i top
candidates from each ranked list.
Efﬁcient learning:
Formally, assuming R ranked lists Li,
an exhaustive learning algorithm would consider all possible values of the R-tuple {N1, . . . , NR}, where Ni 2
{0, . . . , |Li|}; adding up to QR
1 |Li| parameterizations to
try, which is intractable in our setting.
To reduce the dimensionality of the learning step, we
start by selecting two ranked lists L1, L2 and we sample the
list at S levels of number of candidates. We then scan the
full S2 different parameterizations to combine the candidates from both. Each of these sets is analyzed in the plane
of number of candidates-achievable quality, so the full combination can be assessed as S2 points in this plane.
The key step of the optimization consists in discarding
those parameterizations whose quality point is not in the
Pareto front. We sample the Pareto front to S points and we
iterate the process until all the ranked lists are combined.
Each point in the ﬁnal Pareto front corresponds to a particular parameterization {N1, . . . , NR}. At test time, we
choose a point on this curve, either at a given number of
candidates Nc or at the achievable quality we are interested
in, and combine the
N 1, . . . , N R
top candidates from
each ranked list. The number of sampled conﬁgurations using the proposed algorithm is (R−1)S2, that is, we have
reduced an exponential problem (SR) to a quadratic one.
6.2. Regressed Ranking of Candidates
The previous section tackles the reduction of candidates
from millions to thousands while keeping the achievable
quality as high as possible. To further reduce their number,
we train a regressor from low-level features, as in .
We focus on features that can be computed efﬁciently in
a bottom-up fashion. This way, we can precompute all the
features for the original regions and efﬁciently calculate the
features for the candidates in a few operations. We compute
the following features:
• Size and location: Area and perimeter of the candidate; area, position, and aspect ratio of the bounding box;
and the area balance between the regions in the candidate.
• Shape: Perimeter (and sum of contour strength) divided by the squared root of the area; and area of the region
divided by that of the bounding box.
• Contours: Sum of contour strength at the boundaries,
mean contour strength at the boundaries; minimum and
maximum UCM threshold of appearance and disappearance
of the regions forming the candidate.
We train a Random Forest using these features to regress the
object overlap with the ground truth, and diversify the ranking based on Maximum Marginal Relevance measures .
7. Experiments on PASCAL 2012
Evaluation Measures:
We assess the generation of candidates in terms of achievable quality with respect to the
number of candidates, that is, the quality we would have if
an oracle selected the best candidate among the pool. As a
measure of quality of a speciﬁc candidate with respect to an
annotated object, we will consider the Jaccard index, also
known as covering, or overlap, which is deﬁned as the intersection over the union of two sets.
When computing the overall quality for the whole
database, we will consider two metrics. First, we deﬁne the
Jaccard index at class level (Jc) as the mean over classes
of the covering of all pixels of each class (the segmentation accuracy of PASCAL). Second, to avoid the bias of Jc
towards methods focusing on large objects, we deﬁne the
Jaccard index at instance level (Ji) as the mean best overlap
for all the ground-truth instances in the database (also Best
Spatial Support score (BSS) ).
Learning Strategy Evaluation:
This section estimates
the loss in performance due to not sweeping all the possible
values of {N1, . . . , NR} via the greedy strategy proposed.
To do so, we will compare it with the full combination on
a reduced problem to make the latter feasible. Speciﬁcally,
we combine the 4 ranked lists coming from the singletons at
all scales, instead of the full 16 lists, and we limit the search
to 20 000 candidates.
In this situation, the mean loss in achievable quality
along the full curve of parameterization is Ji =0.0002, with
Number of candidates
Jaccard index at pixel level (Jp)
Pareto up to 4-tuples
Pareto up to triplets
Pareto up to pairs
Pareto only singletons
Raw Ours-multi singl.
Raw gPb-UCM singl.
Raw Quadtree singl.
Selected conﬁguration
Filtered candidates
Regressed ranking
Figure 4. VOC12 train set. Object candidates achievable quality
Instance level
Number of candidates
Jaccard index at instance level (Ji)
Class level
Number of candidates
Jaccard index at class level (Jc)
MCG-Our (34s/im)
SCG-Our (4.7s/im)
Categ. indep. 
Shape sharing 
Regions and parts 
Selective search 
Sel. search SP
Objectness 
Objectness SP
Ours-multi
ISCRA 
gPb-UCM 
MCG and CPMC
Figure 5. VOC12 val set. Object candidates achievable quality at instance and class level. We
also compare favorably to Scalpel on VOC10 val set.
Table 2. VOC12 val set. Per-class and global Jaccard index at instance level (Ji)
a maximum loss of Ji = 0.004 (0.74%). In exchange, our
proposed learning strategy on the full 16 ranked lists takes
about 1 minute to compute on the training set of PASCAL
2012, while the limited full combination takes 4 days
Combinatorial Grouping:
We extract the lists of candidates from the three scales and the multiscale hierarchy, for
singletons, pairs, triplets, and 4-tuples of regions, leading
to 16 lists, ranked by the minimum UCM strength of the
regions forming each candidate.
Figure 4 shows the Pareto front evolution of Jc with respect to the number of candidates for up to 1, 2, 3, and 4
regions per candidate (4, 8, 12, and 16 lists, respectively) at
training time. As baselines, we plot the raw singletons from
Ours-multi, gPb-UCM, and Quadtree.
The improvement of considering the combination of all
1-region candidates from the 3 scales and the Ours-multi
with respect to the raw Ours-multi is signiﬁcant, which corroborates the diversity obtained from hierarchies at different
scales. In turn, the addition of 2- and 3-region candidates
noticeably improves the achievable quality. The improvement when adding 4-tuples is marginal at the number of
candidates we are working.
The red asterisk ( ) marks the selected conﬁguration
N 1, . . . , N R
we choose, and the red plus sign ( ) represents the set of candidates after removing duplicates with
an overlap higher than 0.95. The candidates at this point
(5 038 per image in mean with Jc = 0.85) are the ones
that are ranked by the learnt regressor, the result of which is
plotted in black (on the training set).
In summary, the full set of candidates (i.e., combining
the full 16 lists) would contain millions of candidates per
image. In the validation set of PASCAL 2012, the multiscale combinatorial grouping allows us to reduce the number of candidates to 5 086 with a very high achievable Jc of
0.84. The regressed ranking allows us to further reduce the
number of candidates below this point.
Comparison with State of the Art:
We compare our results against , using the implementations
from the respective authors.
Figure 5 shows the achievable quality of all methods on
the validation set of PASCAL 2012. We plot the raw regions
of Ours-multi, ISCRA, gPb-UCM, and QuadTree as baselines. To compare more fairly with selective search and
objectness , we adapted their boxes to the superpixels of
our multiscale hierarchy, obtaining a small improvement.
At instance level (Ji), MCG candidates outperform the
state-of-the-art at all regimes. At class level (Jc), our candidates practically achieve the same quality than CPMC.
To evaluate their complementarity, we compute the Pareto
front of combining the two sets of ranked candidates; that is,
we evaluate the sets of candidates corresponding to combining some candidates from MCG and CPMC. The curve obtained (dashed magenta
), shows that MCG and CPMC
are very complementary: the combination of both methods
leads to an improvement of Jc = 0.03 at around 650 c/i.
We also present a faster single-scale version of MCG
(SCG), which takes the hierarchy at the native scale only
and combines up to three regions. We decrease the timing
one order of magnitude while keeping competitive results.
Table 2 shows the quality (Ji) on each of the 20 PAS-
CAL classes at two different number of candidates (100 and
1100), comparing MCG with the relevant state-of-the-art
techniques at that number of candidates. MCG outperforms
all techniques on the two regimes at the global level and the
majority of classes.
MCG and SCG Timing:
Table 3 shows the timings of
our approach from scratch, on a single core using less than
2Gb of RAM. Our full MCG takes about 25 s. per image
to compute the multiscale hierarchies, and 10 s. to generate
and rank 5 038 candidates on the validation set of PASCAL
2012. Our single-scale version produces a segmentation hierarchy of quality comparable to gPb-ucm in just 3 s.
Hierarchical
Segmentation
Generation
24.4 ± 3.6
34.3 ± 6.2
Table 3. Time in seconds per image of MCG and SCG
8. Conclusions
We proposed Multiscale Combinatorial Grouping, a uni-
ﬁed approach for bottom-up segmentation and object candidate generation. Our approach produces state-of-the-art
contours, hierarchical regions and object candidates. At its
core are a fast eigenvector computation for normalized-cut
segmentation and an efﬁcient algorithm for combinatorial
merging of hierarchical regions. In order to promote reproducible research on perceptual grouping, all the resources
of this project – code, results and evaluation protocols – are
publicly available.
Acknowledgements
This work was partially supported
by the ONR MURI N00014-10-10933, and the Spanish Ministerio de Ciencia e Innovaci´on, under project
TEC2010-18094 and FPU grant AP2008-01164.