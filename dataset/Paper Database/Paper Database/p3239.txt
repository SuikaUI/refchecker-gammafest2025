HAL Id: hal-01390134
 
Submitted on 29 Jun 2017
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Greedily Improving Our Own Closeness Centrality in a
Pierluigi Crescenzi, Gianlorenzo d’Angelo, Lorenzo Severini, Yllka Velaj
To cite this version:
Pierluigi Crescenzi, Gianlorenzo d’Angelo, Lorenzo Severini, Yllka Velaj. Greedily Improving Our
Own Closeness Centrality in a Network.
ACM Transactions on Knowledge Discovery from Data
(TKDD), 2016, 11 (1), pp.1-32. ￿10.1145/2953882￿. ￿hal-01390134￿
Greedily Improving Our Own Closeness Centrality in a Network
PIERLUIGI CRESCENZI, University of Florence
GIANLORENZO D’ANGELO, LORENZO SEVERINI, and YLLKA VELAJ,
Gran Sasso Science Institute
The closeness centrality is a well-known measure of importance of a vertex within a given complex network.
Having high closeness centrality can have positive impact on the vertex itself: hence, in this paper we consider
the optimization problem of determining how much a vertex can increase its centrality by creating a limited
amount of new edges incident to it. We will consider both the undirected and the directed graph cases. In both
cases, we ﬁrst prove that the optimization problem does not admit a polynomial-time approximation scheme
(unless P = NP), and then propose a greedy approximation algorithm (with an almost tight approximation
ratio), whose performance is then tested on synthetic graphs and real-world networks.
General Terms: Closeness Centrality, Collaboration Networks, Citation Networks
Additional Key Words and Phrases: Approximation algorithms, graph augmentation, greedy algorithm, large
1. INTRODUCTION
Looking for the most important vertices within a given complex network has always
been one of the main goals in the ﬁeld of real-world network analysis. Different measures of importance have been introduced in the literature, and several of them are
related to the notion of “centrality” of a vertex. This latter notion, in turn, has been explicitly formalized in different ways: one of the most popular is the closeness centrality
measure . This measure somehow evaluates
the efﬁciency of a vertex while spreading information to all other vertices in its connected component: more formally, the closeness centrality of u is equal to the sum of the
reciprocal of the distances to u from all other vertices. Computing closeness centrality,
however, is too time expensive, since it requires to run a breadth ﬁrst search (BFS)
Preliminary results about this work have been presented in the 14th International Symposium on Experimental Algorithms (SEA) [Crescenzi et al. 2015].
Authors’ addresses: G. D’Angelo, L. Severini, and Y. Velaj, Gran Sasso Science Institute (GSSI), Viale F.
Crispi, 7, I-67100 L’Aquila, Italy; emails: {gianlorenzo.dangelo, lorenzo.severini, yllka.velaj}@gssi.infn.it; P.
Crescenzi, Department of Information Engineering, University of Florence, Viale Morgagni, 65, I-50134
Florence, Italy; email: .
for each vertex, which is clearly infeasible for networks with millions of vertices and
edges (which is the “normal” size of many interesting real-world networks). For this
reason, several randomized and/or approximation algorithms have been proposed for
the computation of this centrality measure [Cohen et al. 2014].
In this paper, instead, we consider a different problem related to the closeness centrality, that is, the problem of identifying which “strategy” a vertex should adopt in
order to increase its own centrality value. Indeed, increasing its own ranking in terms
of centrality can have positive consequences for the vertex. For example, in the ﬁeld
of author citation networks closeness centrality seems to be signiﬁcantly correlated
with citation counts (as it has been already observed in the case of collaboration networks) [Yan and Ding 2009]. We then consider the optimization problem of efﬁciently
determining, for a given vertex u, the set of k edges incident to u that, when added
to the original graph, allows u to increase as much as possible its closeness centrality
and its ranking according to this measure. We will analyze both the undirected and
the directed graph cases. We ﬁrst prove that this problem is hard to be approximated
within an approximation factor greater than 1 −
15e in the undirected case (respectively, 1 −
3e in the directed case), and then show that a greedy approach yields a
e )-approximation algorithm in both undirected and directed cases. Successively,
we present several experiments that we have performed (i) in order to evaluate how
good is the approximation factor in the case of relatively small randomly generated
graphs, (ii) in order to apply the greedy approach to real-world collaboration, citation
and transportation networks, and (iii) in order to evaluate the actual improvement
in information spread. As a result of the ﬁrst set of experiments, we have that the
greedy algorithm seems to perform much better than the theoretical results, since it
often computes an optimal solution and, in any case, it achieves an approximation
factor signiﬁcantly larger than the theoretical one. By applying the greedy algorithm
to real-world networks, instead, we observe that by adding a very few edges a vertex
can drastically increase its centrality measure and, hence, its ranking. We note that
after adding a limited number of edges, the number of informed nodes in the network
highly increases.
The problem of adding edges to a graph in order to modify some general properties
has been widely studied. To the best of our knowledge, the problems that aim at
optimizing some property by adding a limited number of edges are: minimizing the
average shortest-path distance between all pair of nodes [Meyerson and Tagiku 2009;
Papagelis et al. 2011; Parotsidis et al. 2015], minimizing the average number of hops
in shortest paths of weighted graphs [Bauer et al. 2012], maximizing the leading
eigenvalue of the adjacency matrix [Saha et al. 2015; Tong et al. 2012], minimizing the
diameter [Bil`o et al. 2012; Frati et al. 2015], maximizing or minimizing the number of
triangles [Dehghani et al. 2015; Li and Yu 2015], minimizing the eccentricity [Perumal
et al. 2013], and minimizing the characteristic path length [Papagelis 2015].
The problem analyzed in this paper differs from above mentioned ones as it focuses
on improving the centrality of a predeﬁned vertex. As far as we know, our problem has
never been attacked before, even though similar problems have been studied for other
centrality measures, i.e., page-rank [Avrachenkov and Litvak 2006; Olsen and Viglas
2014], eccentricity [Demaine and Zadimoghaddam 2010], average distance [Meyerson
and Tagiku 2009], some measures related to the number of paths passing through a
given node [Ishakian et al. 2012], and betweenness centrality [Crescenzi et al. 2015;
D’Angelo et al. 2016]. Hence, we had no other algorithms to compare with. However, we
also consider other potential alternative algorithms and show that the greedy algorithm
signiﬁcantly outperforms them, whenever k > 1.
1.1. Motivating Applications
In this section we motivate our study by showing two applications in which improving
the centrality of a speciﬁc node by adding edges incident to it can give beneﬁts to the
node itself or to the whole network.
1.1.1. Increasing the Spreading of Information. Intuitively closeness centrality evaluates
the efﬁciency of a vertex while spreading information to all other vertices in its connected component. We show that solving our problem for a set of given vertices has
positive consequences for the spreading of information through the network. To this
aim, we consider the Linear Threshold Model which is a widely studied model in network analysis to represent the spread of information [Kempe et al. 2015]. In this model,
we can distinguish between active nodes (which spread the information) and inactive
ones. The idea is that a node becomes active if a large part of its neighbors are active.
In detail, each node u has a threshold a chosen uniformly at random in the interval
 . The threshold represents the fraction of neighbors of u that must become active
in order for u to become active. At the beginning of the process a small percentage of
nodes of the graph is set to active to let the information diffusion process start, these
nodes are called seeds. In subsequent steps of the process, a node becomes active if the
fraction of its active neighbors is greater than its threshold.
In our experimental study, we show that adding a small number of edges incident
to some randomly-chosen seeds highly increases the spreading of information in terms
of number of nodes that become active. Note that this represents an improvement of
the whole network in terms of the efﬁciency of propagating information. We performed
such experiments on both undirected and directed networks.
1.1.2. Link Recommendation. The link recommendation task consists in suggesting potential connections to social network users with the aim of increasing their social
circle. Link recommendations improve the user experience and at the same time help
to increase the connectivity inside the network and speed-up the network growth.
Most of the existing link recommendation methods focus on estimating the likelihood
that a link is adopted by users and recommend links that are likely to be established
[Backstrom and Leskovec 2011; Liben-Nowell and Kleinberg 2003; Popescul and Ungar
2003; Yin et al. 2010].
Recently, a new approach has been proposed whose aim is to recommend a set of
links that, when added to the network, increase the centrality of a user in a network.
In particular, suggesting links that minimize the expected average distance of a node
accurately predicts the links that will actually appear in the graph [Parotsidis et al.
2016]. An important step in this approach is to determine the set of links that, when
added to the network, maximize the speciﬁc centrality measure considered.1
Of particular interest in this context are the collaboration networks in which nodes
represent users and links represent collaboration between users (e.g., authors collaborating in the same papers or actors that acted in the same movie). The link recommendation problem in such a case consists in suggesting possible persons to whom request
for future collaboration. In our experiments we show that we are able to compute a
set of nodes that highly increase the closeness centrality in very large collaboration
networks such as those induced by the DBLP and IMDB databases [Ley; IMDB].
1.2. Structure of the Paper
The rest of the paper will be divided into two parts: the ﬁrst part is devoted to the
analysis of the undirected graph case, while the second one is devoted to the directed
1Note that the centrality measure used in Parotsidis et al. is the inverse of the arithmetic mean of
the distances to a node, while in this paper we consider the harmonic mean of the distances to a node.
graph case. Each part is in turn split into a theoretical part and experimental part.
In particular, in both parts, we ﬁrst deﬁne the optimization problem (see Sections 2.1
and 3.1), then prove the non-approximability result (see Sections 2.2 and 3.2), and
ﬁnally introduce and analyze the greedy approximation algorithm (see Sections 2.3
and 3.3). In the case of undirected graphs, we also show how the running time of
this algorithm can be improved by making use of dynamic algorithm techniques (see
Section 2.4). In the experimental sections of the two parts, instead, we ﬁrst compare
the greedy solution with the optimal one, then compare the performances of the greedy
algorithm with the ones of several alternative baseline strategies, and lastly analyze
the impact of adding edges on the performances of the information spreading process
(see Sections 2.5 and 3.4). Finally, we measure the improvement in the value of the
closeness of a node and in its closeness ranking within two large real-world undirected
networks (see Section 2.6) and one large real-world directed network (see Section 3.4).
2. THE UNDIRECTED GRAPH CASE
In this section we will focus on undirected graphs. After giving all necessary deﬁnitions and preliminary results, we will introduce the optimization problem that will be
considered, prove a non-approximability result, and then describe an approximation
algorithm. Finally, we will present the experiments that we have performed in order
to validate this algorithm and to apply it to two quite big collaboration networks.
2.1. The Maximum Closeness Improvement Problem
Let G = (V, E) be an undirected graph, where V denotes the set of nodes, and E denotes
the set of edges {u, v} with u, v ∈V . For each node u, Nu denotes the set of neighbors of
u, i.e., Nu = {v | {u, v} ∈E}. Given two vertices u and v, we denote by duv the distance
from u to v in G, that is, the number of edges in a shortest path from u to v (if there
is no path from u to v, we then set duv = ∞). For each node u, the closeness centrality
 of u is deﬁned as follows:
Given a set S of edges not in E, we denote by G(S) the graph augmented by adding
the edges in S to G, i.e., G(S) = (V, E ∪S). For a parameter x of G, we denote by x(S)
the same parameter computed in the augmented graph G(S) (for example, the distance
from u to v in G(S) is denoted as duv(S)).
The closeness centrality of a vertex clearly depends on the graph structure: if we
augment a graph by adding a set of edges S, then the centrality of a vertex might
change. Generally speaking, adding edges incident to some vertex u can only increase
the centrality of u. Given a graph G = (V, E), a vertex u ∈V , and an integer k, the
Maximum Closeness Improvement (MCI) problem consists in ﬁnding a set S of edges
incident to u not in E (that is, S ⊆{{u, v} : v ∈V \Nu}) such that |S| ≤k and cu(S) is
2.2. The Non-Approximability Result
In this section, in order to derive our approximation hardness result for the MCI
problem, we will make use of the Minimum Dominating Set (MDS) problem, which
is deﬁned as follows: given an undirected graph G = (V, E), ﬁnd a dominating set of
minimum cardinality, that is, a subset D of V such that V = D ∪
u∈D Nu. It is known
that for any r with 0 < r < 1, it cannot exist a (r ln |V |)-approximation algorithm
for the MDS problem, unless P = NP [Dinur and Steurer 2014]. We will now use
this result in order to show that the MCI problem does not admit a polynomial-time
The approximation algorithm for the MDS problem, given a γ -approximation algorithm A for the
MCI problem and a “guess” k for the optimal value of MDS.
approximation scheme. To this aim, we will design an algorithm A′ that, given an
undirected graph G = (V, E) and given the size k of the optimal dominating set of G, by
using an approximation algorithm A for the MCI problem will return a dominating set
of G whose approximation ratio is at most (r ln |V |). Clearly, we do not know the value
of k, but we know that this value must be at least 1 and at most |V |: hence, we run
algorithm A′ for each possible value of k, and return the smallest dominating set found.
Algorithm A′ will run the approximation algorithm A for the MCI problem multiple
times. Each time A will ﬁnd k nodes u ∈V which are the “new” neighbors of the node
whose centrality has to be increased: we then add these nodes to the dominating set and
create a smaller instance of the MCI problem (which will contain, among the others,
all the nodes in V not yet dominated). We continue until all nodes in V are dominated.
THEOREM 2.1. For each γ > 1 −
15e, there is no γ -approximation algorithm for the
MCI problem, unless P = NP.
PROOF. We will show that a γ -approximation algorithm Afor the MCI problem, with
15e, would imply a (r ln n)-approximation algorithm A′ for the MDS problem,
thus, proving the theorem. In particular, the algorithm A′ is speciﬁed in Figure 1, where
k denotes a “guess” of the size of an optimal solution for MDS with input the graph G.
In the following, ω will denote the number of times the while loop is executed. Since,
at each iteration of the loop, we include in the dominating set at most k nodes, at the
end of the execution of algorithm A′ the set D includes at most k· ω nodes. Hence, if k is
the correct guess of the value of the optimal solution for the MDS instance, then D is
a ω-approximate solution for the MDS problem (as we have already noticed, we don’t
know the correct value of k, but algorithm A′ can be executed for any possible value of
k, that is, for each k ∈[|V |]).
The ﬁrst instruction of the while loop of algorithm A′ computes a transformed graph
G′ (to be used as part of the new instance for MCI) starting from the current graph
G = (V, EV ), which is the subgraph of the original graph induced by the set {u1, . . . , un},
where n = |V |, of still not dominated nodes. This computation is done as follows (see
Figure 2). We add a new node z and two new nodes xi and yi, for each i with 1 ≤i ≤n.
Moreover, we add to EV the edges {z, yi}, {xi, yi}, and {xi, ui}, for each i with 1 ≤i ≤n.
As it is shown in the second line of the while loop, z is the node whose centrality cz
has to be increased by adding at most k edges: that is, the MCI instance is formed by
G′, z, and k. Observe that any solution for this instance that contains an edge {xi, z}
can be modiﬁed, without decreasing its measure, by substituting this edge with {ui, z}:
The reduction used in Theorem 2.1. The dashed edges between node z and nodes ui denote those
added in a solution to MCI.
hence, we can assume that the solution S computed at the second line of the while loop
of algorithm A′ contains only edges connecting z to nodes in V (which are shown by
dashed edges between node z and nodes ui in Figure 2).
First of all, note that, since k is (a guess of) the measure of an optimal solution D∗
for MDS with input G, we have that the measure c∗(G′, z, k) of an optimal solution S∗
for MCI with input G′ satisﬁes the following inequality:
c∗(G′, z, k) ≥k + 1
2(n −k) + 3
This is due to the fact that by connecting z to all the k nodes in D∗, in the worst case we
have that k nodes in G are at distance 1, n−k nodes in G are at distance 2 (since D∗is
a dominating set), the n nodes yi are at distance 1, and the n nodes xi are at distance 2
Given the solution S computed by the approximation algorithm A for MCI, let a and
b denote the number of nodes in G at distance 2 and 3, respectively, from z in G′(S).
Since all nodes in G′ are at distance at most 3 from z, we have that n = k + a + b (we
can assume, without loss of generality, that n ≥k): hence, a = n −b −k. Since A is a
γ -approximation algorithm for MCI, we have that cz(S) ≥γ c∗(G′, z, k). That is,
From this inequality, it follows that:
a ≥γ (k + 4n) −3n −2k −2
By using the fact that a = n −b −k, we have that
n −b −k ≥γ (k + 4n) −3n −2k −2
b ≤12(1 −γ )n + 3(1 −γ )k.
Since k ≤n, we then have that
b ≤15n(1 −γ ).
Assuming γ > 1−
15 (which implies 15(1−γ ) < 1), then after one iteration of the
while loop of algorithm A′, the number of nodes in G decreases by a factor 15(1 −γ ).
The greedy algorithm for undirected graphs.
Hence, after ω −1 iterations, the number n of nodes in the graph G is at most a fraction
[15(1 −γ )]ω−1 of the number N of nodes in the original graph. Since we can stop as
soon as n < k, we need to ﬁnd the maximum value of ω such that k ≤N[15(1 −γ )]ω−1.
By solving this inequality and by recalling that 15(1 −γ ) < 1, we obtain
ω −1 ≤log15(1−γ )
N ≤log15(1−γ )
One more iteration might be necessary to trivially deal with the remaining nodes,
which are less than k. Hence, the total number ω of iterations is at most
15(1−γ ) + 1.
If γ > 1 −
15e, we have that r′ =
15(1−γ ) < 1: as a consequence of the observation at
the beginning of the proof, the solution reported by algorithm A′ is an (r′ ln N + 1)approximate solution. Clearly, for any r with 0 < r′ < r < 1, there exists Nr sufﬁciently
large, such that for any N > Nr, r′ ln N + 1 ≤r ln N: hence, algorithm A′ would be
an r ln N-approximation algorithm for MDS, and, because of the result of Dinur and
Steurer , P would be equal to NP. Thus, we have that, if P ̸= NP, then γ has to
be not greater than 1 −
15e and the theorem is proved.
2.3. The Greedy Approximation Algorithm
Let us consider the following optimization problem. Given a set X and an integer k, ﬁnd
a subset Y of X of cardinality at most k that maximizes the value f (Y), where f : 2X →
N is a speciﬁc objective function. If f is monotone submodular, that is, if, for any pair
of sets S ⊆T ⊆X and for any element e ∈X\T , f (S ∪{e}) −f (S) ≥f (T ∪{e}) −f (T ),
then the following greedy algorithm approximates the above problem within a factor
e [Nemhauser et al. 1978]: start with the empty set, and repeatedly add an element
that gives the maximal marginal gain. In this section, we exploit this result by showing
that cu is monotone and submodular with respect to the possible set of edges incident
to u. Hence, the greedy algorithm GREEDYIMPROVEMENT (reported in Figure 3) provides
e )-approximation. Note that the computational complexity of such algorithm is
O(k· n· g(n, m+ k)), where g(n, m+ k) is the complexity of computing cu in a graph with
n nodes and m+ k edges.
THEOREM 2.2. For each vertex u, function cu is monotone and submodular with respect
to any feasible solution for MCI.
PROOF. To simplify the notation, in the following we will assume that
show that cu is monotone increasing, it is enough to observe that for each solution S to
MCI, for each edge {u, v} ̸∈E ∪S, and for each node x ∈V \{u}, dux(S ∪{{u, v}}) ≤dux(S)
(since adding an edge cannot increase the distance between two nodes) and, therefore,
dux(S∪{{u,v}}) ≥
dux(S). We now show that for each pair S and T of solutions for MCI such
that S ⊆T , and for each edge {u, v} ̸∈T ∪E,
cu(S ∪{{u, v}}) −cu(S) ≥cu(T ∪{{u, v}}) −cu(T ).
To this aim, we prove that each term of cu is submodular, i.e., that for each vertex
x ∈V \{u},
dux(S ∪{{u, v}}) −
dux(T ∪{{u, v}}) −
Let us consider the shortest paths from u to x in G(T ∪{{u, v}}), and let us distinguish
the following two cases:
(1) The ﬁrst edge of a shortest path from u to x in G(T ∪{{u, v}}) is {u, v} or belongs
to S ∪E. In this case, such a path is a shortest path also in G(S ∪{{u, v}}), as
it cannot contain edges in T \S (since these edges are all incident to u). Then,
dux(S ∪{{u, v}}) = dux(T ∪{{u, v}}) and
dux(S∪{{u,v}}) =
dux(T ∪{{u,v}}). Moreover, dux(S) ≥
dux(T ) (since S ⊆T ) and, therefore, −
(2) The ﬁrst edge of all shortest paths from u to x in G(T ∪{{u, v}}) belongs to T \S. In
this case, dux(T ) = dux(T ∪{{u, v}}) and, therefore,
dux(T ∪{{u,v}}) −
dux(T ) = 0. As
is monotone increasing, then
dux(S∪{{u,v}}) −
dux(S) ≥0.
In both cases, we have that inequality (1) is satisﬁed and, hence, the theorem follows.
COROLLARY 2.3. The MCI problem is approximable within a factor (1 −1
As it can be seen, there is quite a signiﬁcant gap between the non-approximability
result proved in Theorem 2.1 (that is, the upper bound equal to 1 −
15e ≈0.98), and
the approximability result of the above corollary (that is, the lower bound (1 −1
0.63). One of the main goals of the next experimental session is to analyze the “real”
performance, in terms of solution quality, of the greedy algorithm on relatively small
real-world and synthetic graphs.
2.4. Improving the Greedy Algorithm Running Time
In this section we show how to improve the running time of GREEDYIMPROVEMENT. This
algorithm requires O(k · n · g(n, m + k)) computational time, where g(n, m + k) is the
complexity of computing cu in a graph with n nodes and m + k edges. The classical
algorithm to compute cu consists in determining all the distances to u by running a
BFS starting from u. Therefore, with such an approach, GREEDYIMPROVEMENT requires
O(k · n · (n + m+ k)) in the worst case. In this section we provide a dynamic algorithm
to reduce the time required to compute cu.2 Furthermore, we show how to exploit the
submodularity of cu in order to reduce the running time of iterations i ≥2 of the for
loop at line 2 of GREEDYIMPROVEMENT.
2Note that the idea of incrementally updating the closeness centrality has been already explored in the
literature [Kas et al. 2013; Sariy¨uce et al. 2013]. However, in this paper we consider the harmonic mean to
compute the closeness centrality instead of the arithmetic mean that is used in other papers. The motivation
is that the harmonic mean has been shown to be more robust in the case of undirected disconnected networks
or directed not-strongly connected networks [Boldi and Vigna 2014]. Therefore, we cannot directly use the
algorithms in the literature and we devise a new dynamic algorithm.
Algorithm DynamicBFS.
Let us assume that we add an edge {u, v} ̸∈E ∪S to graph G(S). The dynamic
algorithm aims at computing only the distances between u and any other node that
change as a consequence of the addition of edge {u, v} (i.e., nodes w such that duw(S) ̸=
duw(S∪{u, v})) and keep the old distances to any other node in the graph. The algorithm
is based on the following observation: if we add an edge {u, v} to G(S), then duw(S) ̸=
duw(S∪{u, v}), for some w ∈V , only if the shortest path between u and w in G(S∪{u, v})
contains edge {u, v}. Therefore, we can determine the nodes that change their distance
to u by ﬁnding all the shortest paths passing through edge {u, v} in G(S ∪{u, v}). To
this aim, the dynamic algorithm executes a BFS starting from node v and prunes the
search as soon as a node that does not change its distance to u is extracted from the
queue. We report the dynamic algorithm DynamicBFS in Figure 4. In detail, Procedure
DynamicBFS returns the value Clo which corresponds to the increment to cu(S)
which is obtained by adding edge {u, v}. To compute Clo, the algorithm computes
the distances between u and any node y such that duy(S) ̸= duy(S ∪{u, v}). First, it
computes the distances of u and its neighbors (lines 3–5) and the initial increment
Clo that is equal to the difference between the reciprocal of the new distance and
that of the old distance (line 7). Then, it pushes in queue Q the neighbors of u (lines 8
and 9) and performs the BFS starting from v (lines 10–17). For each extracted node,
it updates Clo by subtracting the reciprocal of the old distance and adding the new
one (line 12). After that, it enqueues a neighbor y of the extracted node x only if the
old distance duy(S) is greater than the length of the path made of the shortest path
from u to x in G(S ∪{u, v}) and the edge {x, y} (that is dux(S ∪{u, v}) + 1, see the test
at line 14). Note that this condition is satisﬁed only if the shortest path between u
and y passes through edge {u, v}. The procedure repeats this process until the queue is
We give an example of execution of Algorithm DynamicBFS in Figure 5.
Example of execution of Algorithm DynamicBFS.
In order to analyze the computational complexity of Algorithm DynamicBFS, let us
deﬁne as γuv(S) as the set of nodes that change their distance to u as a consequence of
the addition of edge {u, v} to G(S), that is
γuv(S) = {w ∈V | dxu(S) ̸= dxu(S ∪{{u, v}})}.
Moreover, let uv(S) be the number of edges incident to nodes in γuv(S), that is uv(S) =
w∈γuv(S) |N(w)|. Parameters |γuv(S)| and uv(S) measure the minimal number of nodes
and edges, respectively, that must be visited in order to update all the distance to uafter
the addition of edge {u, v}. Note that uv(S) = O(m+ n) in the worst case; however, it is
much smaller than min many practical cases as shown in the next section. In Figure 5,
the nodes in γuv(S) are represented in gray, while the number of double edges is uv(S).
The next theorem gives the computational complexity of Algorithm DynamicBFS as a
function of O(uv(S)).
THEOREM 2.4. Algorithm DynamicBFS requires O(uv(S)) time.
PROOF. Lines 1–9 require O(Nv(S)) = O(uv(S)) time. In the loop at lines 10–17,
variable visited ensures that each node is inserted into Q at most once. Therefore, the
overall time requirement of such loop is equal to the sum of Nx(S), for all the nodes
x that are inserted into Q. Hence, to prove the statement, we show that all the nodes
inserted into Q belong to γuv(S). We ﬁrst show that for each x ∈γuv(S) all the distances dxu(S ∪{{u, v}}) between u and x in G(S ∪{{u, v}}) are correctly computed by
Algorithm DynamicBFS. By contradiction, suppose that the distance between some
node in γuv(S) and u is not correctly computed and consider a node y ∈γuv(S) having
minimal distance to u among such nodes. At the last iteration when y is inserted into
Q, there exists a node x ∈N(y) such that duy(S) > dux(S ∪{u, v}) + 1. It follows that
duy(S ∪{u, v}) = dux(S ∪{u, v}) + 1 (see the test at line 14). Since the distance between
y and u is minimal among those that are not correctly computed by the algorithm,
then dux(S ∪{u, v}) is correct. It follows that the distance between y and u is correctly
computed at line 15, a contradiction. By contradiction, suppose that some node not in
γuv(S) is inserted into Q and consider a node y ̸∈γuv(S) having minimal distance to u
among such nodes. Since y has minimal distance to u among the nodes not in γuv(S)
inserted into Q, then the node x for which the condition at line 14 is satisﬁed when y is
inserted into Q must belong to γuv(S). By the previous arguments, dux(S ∪{u, v}) is correctly computed by the algorithm and then duy(S ∪{u, v}) = dux(S ∪{u, v}) + 1 < duy(S),
a contradiction to the fact that y does not belong to γuv(S).
The new dynamic algorithm can now be obtained by the GREEDYIMPROVEMENT algorithm shown in Figure 3, by doing the following modiﬁcations:
—Before line 2, we compute cu in G.
—At line 4, we incrementally compute cu(S ∪{u, v}) by making use of algorithm
DynamicBFS instead of a full BFS.
Note that, for each v ∈V , uv(S) is maximized when S = ∅, then the algorithm requires
an overall O(k · n) computational time, where  = maxv∈V {uv(∅)}.
We now show how to exploit the deﬁnition of submodularity to reduce the running
time of iterations i ≥2 of the for loop at line 2 of GREEDYIMPROVEMENT. Let cu(S∪{{u, v}})
be the increment to the centrality of node u after adding the edge {u, v} to graph G(S).
Since cu is submodular, then cu(S ∪{{u, v}}) is monotonic non-increasing. It follows
that cu(S ∪{{u, v}}) is upper bounded by cu(S′ ∪{{u, v}}), where S′ ⊆S. We exploit
this observation in algorithm DYNAMICGREEDYIMPROVEMENT given in Figure 6.
First, we compute cu and initialize cu (lines 1–3). For each iteration i of the for
loop at line 6, we use variable LB (line 7) to maintain the maximum improvement to
closeness found so far, that is LB is a lower bound to the improvement that will be
found at the end of iteration i. If at iteration i ≥2, for some node v ∈V \Nu(S), we
have that LB ≥cu({S′ ∪{{u, v}}}) (line 9), where S′ is the value of S at iteration i −1,
then edge {u, v} cannot increase the value of cu more than the maximum found so far.
Therefore, in this case we prune the search. Otherwise, we compute cu(S ∪{{u, v}})
and check whether it improves LB or not (line 11). In the afﬁrmative case, we update
LB (line 12).
We can improve the performance of DYNAMICGREEDYIMPROVEMENT by means of two
further heuristics. First, we sort the nodes of Nv(S), for each v ∈V , in non-increasing
order of distance from u and we stop the for loop of line 13 of algorithm DynamicBFS
when a node y such that duy(S) ≤dux(S ∪{u, v}) + 1 is extracted. In fact, for any other
node adjacent to x with a distance to u greater than duy(S) the condition at line 14 is not
satisﬁed. Then, we can easily parallelize algorithm DYNAMICGREEDYIMPROVEMENT over p
processors since V \(Nv(S)) can be divided into sets of ⌊|V \(Nv(S))|
⌋nodes and the for loop
Algorithm DYNAMICGREEDYIMPROVEMENT.
at line 8 of algorithm DYNAMICGREEDYIMPROVEMENT can be executed in parallel for each
set. In this case, LB is given by the maximum over each subset.
2.5. The Experimental Study: Part I
In this section we analyze the greedy algorithm from an experimental point of view.
First, we compare the solution of the greedy algorithm with the optimal solution computed by using an integer program formulation of the MCI problem, in order to assess
its real performance in terms of solution quality. Then, we compare the greedy algorithm with several alternative baselines. Finally, we study how the spreading of
information increases as a consequence of the augmentation of the graph due to our
algorithm.
All our experiments have been performed on a computer equipped with two Intel
Xeon E5-2643 CPUs, each with 6 cores clocked at 3.4GHz and 128GB of main memory,
and our programs have been implemented in C++ (gcc compiler v4.8.2 with optimization
level O3).
2.5.1. Evaluating the Solution Quality. In this section we evaluate the quality of the solution produced by the greedy algorithm by measuring its approximation ratio on several, relatively small, randomly generated networks and on four real-world networks.
In particular, we considered four random graph generating models, that is, undirected
Preferential Attachment (PA) [Barabasi and Albert 1999], Erd˝os–R´enyi (ER) [Erd˝os and
R´enyi 1959], Conﬁguration Model (CM) [Molloy and Reed 1995; Bender and Canﬁeld
1978], and Watts–Strogatz model (WS) [Watts and Strogatz 1998]. The size of the generated graphs is reported in Table I. For each combination (n, m), we generated ﬁve
random undirected graphs. Moreover, we considered the four real-world graphs, whose
size is reported in Table II. The ﬁrst graph is the collaboration network between Jazz
musicians that have played together in a band, and it has been obtained from the
Konect database [Kunegis 2013], while the last three graphs have been downloaded
Table I. Comparison between the GREEDYIMPROVEMENT
Algorithm and the Optimum in Random Graphs. The
First Three Columns Report the Type and Size of
the Graphs, and the Fourth Column Reports the
Minimum Measured Approximation Ratio
Min Approx.
Table II. Comparison between the GREEDYIMPROVEMENT Algorithm
and the Optimum in Real World Graphs. The First Three
Columns Report the Name and Size of the Graphs,
and the Fourth Column Reports the Minimum
Measured Approximation Ratio
Min Approx.
celegans metabolic
from the Uri AlonLab database: in particular, s838_st is an electronic network, while
the other two graphs are biological networks.
For both random and real-world graphs we focused our attention on 20 vertices u,
which have been chosen on the basis of their original closeness ranking. In particular,
we have divided the list of vertices, sorted by their original ranking, in four intervals,
and chosen ﬁve random vertices uniformly at random in each interval: we denote by
uX% the average value of the vertices in the interval of the top Xth percentile. The value
of k ranged from 1 to 10. In the experiments, we measured the ratio between the value
of the solution found by the greedy algorithm and the optimal value computed by using
the integer program formulation of the MCI problem, deﬁned as follows:
dsu({u, v}) −1
subject to
for each s ∈V \{u}
for each s ∈V \{u}, v ∈V \Nu,
xv, ysv ∈{0, 1},
for each s ∈V \{u}, v ∈V \Nu.
The decision variables xv and ysv specify a solution S of the MCI problem as follows.
For any v ∈V \Nu,
if {u, v} ∈S,
otherwise,
and, for each s ∈V \{u} and v ∈V \Nu,
if a shortest path from s to u in G({u, v}) passes through edge {u, v},
otherwise.
The ﬁrst constraint of the integer program ensures that each node s can be covered
by at most one edge {u, v} and, hence, that the distance from s to u is counted only once
in the objective function, while the second constraint ensures that if ysv = 1, then xv = 1
and, hence, that the shortest path from s to u passing through {u, v} is considered only
if {u, v} ∈S. Finally, note that in the objective function, the value of
dsu({u,v}) and
be preprocessed, and that the term 
dsu is a constant.
We solved the above integer program by using the GLPK solver [GNU]. The results
are reported in Tables I and II where we show the minimum (i.e., worst-case) approximation ratio obtained by the greedy algorithm. The experiments clearly show that the
experimental approximation ratio is by far better than the theoretical one proven in
the previous section. In fact, in the worst case the ratio is 0.9798.
In Figure 7, instead, we plot the average closeness centrality and ranking of vertices
u as a function of k in a small real-world network, namely the s838_st electrical
network. We observe that the charts on the top, where the values are computed using
the GREEDYIMPROVEMENT algorithm, and the charts on the bottom, in which we used the
optimal algorithm, are almost identical. Indeed, the approximation ratio in the worst
case is 0.9862: that is, the GREEDYIMPROVEMENT algorithm performs very well in practice.
Finally, we tested our algorithm on several artiﬁcial instances generated by the
Erd˝os–R´enyi and the Watts–Strogatz models. In the former model we can choose appropriate values of the graph density, while in the latter one we can choose the clustering coefﬁcient. It turned out that the performance of our algorithm is not inﬂuenced by
these two factors. Indeed, the approximation ratio ranges in [0.9798, 1] and improves
a little when the density is very high (i.e., m > 0.5n2).
2.5.2. The Comparison with Alternative Baselines. In this section we compare our algorithm
with the following algorithms:
(1) The algorithm that connects u to a set of k nodes extracted uniformly at random
(2) The algorithm that connects uto a set of knodes having the highest degree (DEGREE).
(3) The algorithm that connects u to a set of k nodes having the highest harmonic
centrality (TOP-K).
(4) The algorithm that connects u to a set of k nodes that have the highest fractional
value when solving the linear relaxation of the integer program (ROUNDING).
(5) The algorithm that connects u to a set of k nodes computed with an approximation
algorithm for the k-median with penalties problem given in Meyerson and Tagiku
 (K-MEDIAN).
The ﬁrst two algorithms are easy to describe and implement efﬁciently. In what
follows we give more details on the implementation of the last three algorithms.
The TOP-K Algorithm. The classical algorithm to ﬁnd the k nodes having the highest
value of centrality, consists, for each node v, in determining all the distances to v by
running a BFS and computing cv. With such an approach computing the k nodes having
Closeness centrality and ranking of vertices in the four intervals u as a function of k in the network
s838_st. Comparison between the GREEDYIMPROVEMENT algorithm and the optimal one.
the highest value of centrality requires O(n · (n + m)). In Figures 8 and 9, we give an
algorithm that reduces the computation time by using a branch-and-bound technique
that prunes the unnecessary BFS by comparing the intermediate results of centrality
with a properly deﬁned upper bound.
Let Ck be the set of k nodes having the highest closeness centrality. We represent
Ck with a min-heap in order to ﬁnd the minimum in constant time. First of all, TOP-K
algorithm inserts the k nodes with highest degree in Ck and computes their centrality.
Then, it computes the closeness centrality of other nodes v by performing a BFS starting
at each v. The algorithm uses the minimum value of centrality in Ck as a lower bound
and prunes the BFS from v when such lower bound is greater than an upper bound (to
be deﬁned later) on cv. Such upper bound is computed every time a node is extracted
from the BFS queue. If the BFS is completed without any pruning, it removes the
minimum from Ck and it inserts the node v in it.
The upper bound estimates the value of the closeness centrality of a node v. The
main idea is that, at each BFS step, when we extract a node x at distance dvx from
v, we can maintain the exact number of nodes that are at distance dvx and that are
not visited yet. Moreover, we can upper bound the distance to any other node. When
x is extracted from the queue, let Vx be the set of nodes at distance dvx from the
source v that are not visited, visited be the set of nodes currently visited during the
BFS, and Currc be the value of the closeness centrality at the current step, that is
Algorithm PrunedBFS.
Algorithm TOP-K.
Table III. Real-World Graphs Used in the Comparison
between the GREEDYIMPROVEMENT Algorithm and the
Other Baselines. The Columns Report the Type
and Size of the Graphs
ca-AstroPh
ca-CondMat
dip20090126
Newman-Cond mat 95-99
PGPgiantcompo
dvy . Then, we have that |Vx| nodes are at distance dvx from v, while
the remaining |V | −|visited| −|Vx| nodes are at distance at most dvx + 1 from v. Hence
the upper bound is deﬁned as
UBv = Currc + |Vx| · 1
+ (|V | −|visited| −|Vx|) ·
The ROUNDING Algorithm. This approach consists in adding the edges which are
obtained by rounding to one k variables of the optimal solution to the linear relaxation
of the integer program for the MCI problem given in Section 2.5.1. In particular, we
connect u to the nodes corresponding to the k highest values in an optimal fractional
The K-MEDIAN Algorithm. This approach consists in connecting u with the k nodes
which are a solution of the k-median with penalties problem [Meyerson and Tagiku
2009]. In this problem, we are given a set of cities and a set of potential facility
locations. Each city has a demand that needs to be served by a facility. Each city also
has a penalty cost, which we pay if we refuse to serve the city. If we choose to serve
a city, we must pay the distance between the city and its assigned facility for each
unit demand. Our job is to ﬁnd a set of k facilities to open, a set of cities to be served,
and an assignment of cities to open facilities such that our total cost is minimized. We
implemented the approximation algorithm given in Meyerson and Tagiku that
is based on local search. We give an informal description of such algorithm and refer
to Meyerson and Tagiku for more details. The algorithm starts from a solution
S to the k-median with penalties problem and compute its objective function value.
Then, at each iteration it changes S by swapping a node in S with a node in V \S and
checking whether the objective function value is improved. In the afﬁrmative case, the
solution S is updated. The procedure is repeated by selecting another pair of nodes
in S × V \S. The algorithm stops when no swap can improve the value of the current
solution. In our implementation, at each iteration we select the pair in S × V \S that
maximizes the improvement of the objective function value.
Analysis of the Results. In order to compare the solution obtained by the GREEDY-
IMPROVEMENT algorithm with that obtained by using the other aforementioned approaches, we run all the algorithms on several real-world networks reported in
Tables II and III. We ﬁrst observe that the rounding and the k-median algorithms
cannot be executed on networks having more than a few hundred of nodes in reasonable computational time. Therefore, in what follows we ﬁrst compare GREEDYIMPROVE-
MENT with DEGREE, RANDOM, and TOP-K on network ca-HepPh, which is a well-known
Closeness centrality and ranking of vertex u as a function of k in network ca-HepPh.
collaboration network obtained from the SNAP database [Leskovec and Krevl 2014],
and then we compare GREEDYIMPROVEMENT with ROUNDING and K-MEDIAN on the network
jazz. The results for the other networks are similar (but for the networks in Table III
and the ROUNDING and K-MEDIAN algorithms for which we have no results due to the
high computational time).
Regarding the greedy algorithm, in Figure 10 we plot the closeness centrality and the
ranking of vertex u as a function of k on network ca-HepPh. We observe that any vertex
becomes central by adding just a few edges. In Figure 11, we compare the ranking
obtained with the solution given by our algorithm with that obtained with the solution
given by the other approaches on the same network. In particular, we show the average
relative ranking position (ARRP), that is,
ARRP = ru(SB) −ru(SGR)
where SGR and SB are the solutions given by our algorithm and one baseline algorithm,
respectively, and ru(S) denotes the closeness ranking of node u in G(S). The average
relative ranking position represents the gain of our algorithm over any other baseline
in terms of ranking position. Each curve represents the average relative ranking position in a given interval and the values are expressed in percentage. We observe that
the greedy algorithm signiﬁcantly outperforms RANDOM, DEGREE, and TOP-K, whenever
In Figure 12, we compare, the ranking of node uin the solution given by our algorithm
with that given by the ROUNDING and K-MEDIAN. We conﬁrm that our algorithm is by far
better than the other approaches.
We observe that similar results hold if we use the closeness value instead of the
ranking position to compare the greedy algorithm against the other baselines. In some
cases, ROUNDING gives better solutions in terms of objective function value, however,
such cases correspond to the instances in which the fractional solution is integral and
therefore is optimal for the problem. Note that computing such a solution requires
a long computational time and that we cannot apply such an approach for instances
having more than a few hundred nodes.
2.5.3. The Analysis of Information Spreading. In this section we analyze how adding a
limited number of edges incident to some randomly chosen seeds highly increase the
number of nodes that become active in the threshold model.
Average relative ranking position between the Ranking of the solution obtained by the
GREEDYIMPROVEMENT algorithm and the different baselines in network ca-HepPh.
Average relative ranking position between the Ranking of the solution obtained by the
GREEDYIMPROVEMENT algorithm and the different baselines in network jazz.
In the experiments we have chosen a number of seeds, that is, 2%, 4%, 6%, 8%,
and 10% of the number of nodes of the graph. The seeds are chosen uniformly at
random. We run different experiments where the threshold a is uniform, i.e., all the
nodes have the same threshold, or distributed uniformly at random. In detail we set
a uniform threshold equal to 0.2, 0.3, and 0.4 and a threshold uniformly distributed
in the intervals (0, 0.4], (0, 0.6], and (0, 0.8]. We measured the number of nodes that
become active at the end of the process in the graphs of Tables II and III.
In Figure 13, we plot the percentage of active nodes as a function of k for the coli1
undirected network for the case of uniform threshold. The value for k = 0 is the
percentage of active nodes in the original graph. The plots clearly show that the number
of active nodes highly increases even with the addition of a few edges and that the
percentage of active nodes tends to 100%. The results for the other networks in Tables II
and III and for non-uniform threshold are similar.
We run the same experiments by using the solutions obtained by RANDOM, DEGREE,
and TOP-K and observe that such solutions are competitive with that obtained by the
greedy algorithm if k is small (i.e., k ≤3). If k > 3, the percentage of informed nodes
is smaller than that obtained by using the greedy algorithm and the gap between the
baselines and the greedy algorithm increases with k. As already observed, algorithms
ROUNDING and K-MEDIAN can only be executed on small networks.
2.6. The Experimental Study: Part II
We also conducted a second type of experiment by measuring the improvement in
the value of closeness of u and in the closeness ranking of u within the network. In
particular, we studied two large real-world networks obtained from the DBLP [Ley]
and IMDB database [IMDB]. In such networks, the nodes are authors or actors and
there is an edge connecting vertex x and vertex y if the author, or actor, corresponding
to vertex x collaborated with y for writing a paper or for acting in the same movie.
For each graph, we used 20 vertices as u but, differently from the experiments on
random graphs, these vertices have been chosen on the basis of their degree ranking:
in particular, we divided the list of vertices sorted by their ranking in four parts and
chosen randomly ﬁve vertices for each interval. The value of k ranges from 1 to 10.
The analysis of these two large networks has been possible only by using the
DYNAMICGREEDYIMPROVEMENT algorithm, since this algorithm visits only a few edges
of the graph (as explained in the previous section): in particular, for all the iterations
i = 1,2, . . . , k the algorithm visits only 0.09% of the edges. The results for the DBLP network (n = 1,305,445, m = 6,108,727) are plotted in Figure 14, while the results for the
IMDB network (n = 1,797,446, m = 72,880,156) are similar and are shown in Figure 15.
In the chart on the left we plot the closeness centrality of vertex u as a function of k. We
observe that any vertex improves its closeness value by adding just a few edges. In the
right chart we plot the execution time of the algorithm DYNAMICGREEDYIMPROVEMENT.
We notice that the computational effort is high for k = 1 but then it is almost constant
for k > 1: this is due to the submodularity property.
The great difference in execution time between different nodes is due to the dynamic
algorithm: indeed, the most a node has a low degree, the most it is likely that adding
an edge incident to a central node u can affect the majority of the distances between u
and the other nodes (running the dynamic algorithms on nodes with small degree has
complexity O(uv(S)) ∼O(m+ n)). Due to the fact that the degree distribution in the
big networks analyzed is not uniform, the difference between the execution times of
u25% and u50%, u50% and u75%, and u75% and u100% is not proportional to the difference of
the positions in the initial ranking.
In order to test the scalability of the parallelized version of DYNAMICGREEDYIMPROVE-
MENT algorithm, we run the same set of experiments with different numbers of cores
Percentage of active nodes in coli1 network as a function of k in the case of uniform threshold.
Parameter a denotes the threshold and the percentage of seeds is equal to 2%, 4%, 6%, 8%, and 10%,
respectively.
and we measured the execution time and the speedup, i.e., the ratio between the execution time with one core and the execution time with p cores for p = 2, 4, 6, 8. The
results are reported in Table IV. We notice that the parallel algorithm shows a good
scalability in terms of execution time. Note that the small increase in the case of eight
Performance of DYNAMICGREEDYIMPROVEMENT algorithm on network DBLP.
Performance of DYNAMICGREEDYIMPROVEMENT algorithm on network IMDB.
Table IV. Execution Time and Speedup of DYNAMICGREEDYIMPROVEMENT
Algorithm on DBLP and IMDB Networks with Different Number of Cores
Execution Time
cores is due to the fact that in our machine, each CPU has six physical cores and hence
in the case of eight cores the computation is performed by two different processors.
3. THE DIRECTED GRAPH CASE
In this section we will focus on directed graphs. After giving all necessary deﬁnitions
and preliminary results, we will ﬁrst introduce the optimization problem that will be
considered, then prove a non-approximability result, and lastly describe an almost
optimal approximation algorithm. Finally, we will present the experiments that we
have performed in order to validate this algorithm and to apply it to a quite big citation
network and a web graph.
3.1. The Maximum Directed Closeness Improvement Problem
Let G = (V, A) be a directed graph, where V denotes the set of nodes, and Adenotes the
set of arcs (u, v) with u and v in V (note that (u, v) ∈A does not imply that (v, u) ∈A).
For each node u, Nu denotes the set of in-neighbors of u, i.e., Nu = {v | (v, u) ∈A}. Given
two vertices u and v, dvu is deﬁned as in the undirected graph case. Given a set S of
arcs not in A, we denote by G(S) the graph augmented by adding the arcs in S to G,
i.e., G(S) = (V, A∪S). Once again, for a parameter x of G, we denote by x(S) the same
parameter in graph G(S). For each node u, the closeness centrality of u is deﬁned as
Given a directed graph G = (V, A), a vertex u ∈V , and an integer k, the Maximum
Directed Closeness Improvement (MDCI) problem consists in ﬁnding a set S of arcs
entering u not in A (that is, S ⊆{(v, u) : v ∈V \Nu}) such that |S| ≤k and cu(S) is
We observe that the following results hold also for the related problem in which
the edges to be added to the graph are outgoing from u and the closeness centrality
considers distances duv instead of dvu.
3.2. The Non-Approximability Result
In this section, in order to derive our approximation hardness result for the MDCI
problem, we will make use of the Maximum Set Coverage (MSC) problem, which is
deﬁned as follows: given a set X, a collection F = {S1, S2, . . . , S|F|} of subsets of X, and
an integer k, ﬁnd a sub-collection F′ ⊆F such that |F′| ≤k and s(F′) = |∪Si∈F′ Si| is
maximized. It is known that the MSC problem cannot be approximated within a factor
greater than 1 −1
e , unless P = NP [Feige 1998]. We will now use this result in order to
show that the MDCI problem does not admit a polynomial-time approximation scheme.
THEOREM 3.1. The MDCI problem cannot be approximated within a factor greater
3e, unless P = NP.
We give an L-reduction with parameters a and b [Papadimitriou and
Yannakakis 1991]. In detail, we will give a polynomial-time algorithm that transforms any instance IMSC of MSC into an instance IMDCI of MDCI and a polynomial-time
algorithm that transforms any solution S for IMDCI into a solution F′ for IMSC such that
the following two conditions are satisﬁed for some values a and b:
OPT(IMDCI) ≤aOPT(IMSC)
OPT(IMSC) −s(F′) ≤b(OPT(IMDCI) −cu(S)),
where OPT denotes the optimal value of an instance of an optimization problem. If the
above conditions are satisﬁed and there exists a α-approximation algorithm for MDCI,
then there exists a (1 −ab(1 −α))-approximation algorithm for MSC [Papadimitriou
and Yannakakis 1991]. Since MSC is hard to approximate within a factor greater than
e , then 1 −ab(1 −α) < 1 −1
e , unless P = NP. This implies that α < 1 −
Given an instance IMSC = (X, F, k) of MSC, we deﬁne an instance IMCI = (G, u, k) of
MDCI as follows (see Figure 16): G = (V, A), where V = {u}∪{vxi | xi ∈X}∪{vSj | Sj ∈F}
and A = {(vxi, vSj) | xi ∈Sj}.
The reduction used in Theorem 2.1 (in this example, x1 ∈S1, x1 ∈S2, x2 ∈S1, and x2 ∈S|F|). The
dashed arcs denote those added in a solution.
Without loss of generality, we can assume that any solution S of MDCI contains only
arcs (vSj, u) for some Sj ∈F. In fact, if a solution does not satisfy this property, then
we can improve it in polynomial time by repeatedly applying the following rule: if S
contains an arc (vxi, u), for some xi ∈X, then exchange such arc with an arc (vSj, u) such
that (vSj, u) ̸∈S (note that such an arc must exist, since otherwise |F| ≤k and IMSC
could be easily solved). The above rule does not decrease the value of cu(S): indeed, if
we exchange an arc (vxi, u) with an arc (vSj, u) such that (vSj, u) ̸∈S, then the closeness
centrality of u decreases by either 1 or 1
2 (because of the deletion of (vxi, u)) but certainly
increases by 1 (because of the insertion of (vSj, u)).
Given a solution S of MDCI, let F′ be the solution of MSC such that Sj ∈F′ if and
only if (vSj, u) ∈S. We now show that cu(S) = 1
2s(F′) + k. To this aim, let us note that
the distance from a vertex vxi to u is equal to 2 if an arc (xSj, u) such that xi ∈Sj belongs
to S, and it is ∞otherwise. Similarly, the distance from a vertex vSj to u is equal to
1 if (xSj, u) ∈S, and it is ∞otherwise. Moreover, the set of elements xi of X such that
dvxi u(S) < ∞is equal to {xi | xi ∈Sj ∧(vSj, u) ∈S} = 
Sj∈F′ Sj. Therefore,
dvxi u(S)<∞
dvxi u(S) +
dvSj u(S)<∞
2|{xi ∈X | dvxi u(S) < ∞}| + |{Sj ∈F | dvSj u(S) < ∞}|
+ |{Sj | (vSj, u) ∈S}| = 1
2s(F′) + k.
It follows that Conditions (2) and (3) are satisﬁed for a =
2 and b = 2. Indeed,
OPT(IMDCI) = 1
2OPT(IMSC) + k ≤3
2OPT(IMSC), where the inequality is due to the fact
that OPT(IMSC) ≥k, since otherwise the greedy algorithm would ﬁnd an optimal solution for IMSC. Moreover, OPT(IMSC) −s(F′) = 2(OPT(IMDCI) −k) −2(cu(S) −k) =
2(OPT(IMDCI) −cu(S)). The theorem follows by plugging the values of a and b into
3.3. The Greedy Approximation Algorithm
As in the case of undirected graphs, we can show that for each vertex u, the function cu
is monotone and submodular with respect to any feasible solution for MDCI. Indeed,
the proof of Theorem 2.2 can be easily adapted to the directed graph case, and the
following result holds.
The greedy algorithm for directed graphs.
COROLLARY 3.2. The algorithm shown in Figure 17 is a (1 −1
e )-approximation algorithm for the MDCI problem.
Also in this case, there is a gap between the non-approximability result proved in
Theorem 3.1 (that is, the upper bound equal to 1 −1
3e ≈0.88), and the approximability
result of the above corollary (that is, the lower bound (1 −1
e ) ≈0.63). One of the main
goals of the next experimental session is to analyze the “real” performance, in terms of
solution quality, of the greedy algorithm on relatively small real-world and synthetic
3.4. Experimental Results
As in the undirected graph case in this section we analyze the greedy algorithm from
an experimental point of view. First, we compare the solution of the greedy algorithm
with the optimal solution in order to assess its real performance in terms of solution
quality. Then, we compare the greedy algorithm with the other approaches used in
the undirected cases, but K-MEDIAN because it cannot be applied in the directed case.
We adapted the DYNAMICGREEDYIMPROVEMENT algorithm used for the undirected case.
In particular, we run the (pruned) BFSs on the transpose graph of G to reduce the
time required to compute cu and to improve the computational complexity of GREEDY-
IMPROVEMENT. Moreover, we show the results of our experiments on a real-world graph
measuring the improvement in the value of closeness of u within the network. Finally,
we analyze how the information spread increases.
We measured the approximation ratio of the greedy algorithm on ﬁve types of
randomly generated directed networks, namely directed PA [Bollob´as et al. 2003],
ER [Erd˝os and R´enyi 1959], Copying (COPY) [Kumar et al. 2000], Compressible Web
(COMP) [Chierichetti et al. 2009] and Forest Fire (FF) [Leskovec et al. 2007]. The size
of the graphs is reported in Table V. For each combination (n, m), we generated ﬁve
random directed graphs and used 20 vertices as u. These vertices have been chosen on
the basis of their original closeness ranking as in the undirected case.
The results of the experiments are reported in Table V, where we show, similarly to
the undirected graph case, the minimum ratio obtained. The experiments show that in
the worst case the ratio is 0.9668.
For the comparison with the other approaches we used real-world citation networks
obtained from the Arnetminer database [Arnetminer] (see Table VI for details). In
Arnetminer’s networks, there is a vertex for each author and an arc from vertex x to
vertex y if the author corresponding to vertex x cited in his paper one paper written by
the author corresponding to y. We parsed the Arnetminer database in order to select
a sub-network induced by the authors that published at least a paper in one of the
Table V. Comparison between the DIRECTEDGREEDYIMPROVEMENT
Algorithm and the Optimum. The First Three Columns Report
the Type and Size of the Graphs, and the Fourth Column
Reports the Approximation Ratio
Min Approx. Ratio
Table VI. Collaboration Networks Obtained from Arnetminer Database
Software Engineering
Information Security
Computer Graphics Multimedia
Theoretical Computer Science
Artificial Intelligence
High-Performance Computing
Computer Networks
Interdisciplinary Studies
main conferences or journals. As in the previous experiment, for each graph, we used
20 vertices as u. The value of k ranges from 1 to 10.
The results for the citation network Information Security are plotted in Figure 18.
In the two charts we plot the closeness centrality and the ranking of vertex u as a
function of k. We observe that any vertex becomes central by adding just a few arcs. For
example, a vertex with the smallest closeness centrality which initially has closeness
0 and is ranked 509 improves its closeness and ranking to 213.32 and 1, respectively,
by adding only seven arcs.
In the chart in Figure 19 we compare the greedy algorithm with the other approaches.
We report the comparison of the average relative ranking position reached by the nodes.
We do not show the results for ROUNDING as such algorithm is not able to terminate
within a reasonable amount of time. The experiments clearly show that the greedy
Performance of the DIRECTEDGREEDYIMPROVEMENT algorithm on network Information Security.
Table VII. Execution Time and Speedup of DYNAMICGREEDYIMPROVEMENT Algorithm
on uk-2007 Network with Different Number of Cores
Execution Time
algorithm outperforms the other approaches. Also in this case, similar results hold
if we use the closeness value instead of the ranking position to compare the greedy
algorithm against the other approaches.
We further used the DYNAMICGREEDYIMPROVEMENT algorithm to analyze a web network [LAW]. The results for the web network uk-2007 (n = 100,000, m = 3,050,615)
are plotted in Figure 20. In the right chart, we report the execution time of the algorithm. The DYNAMICGREEDYIMPROVEMENT algorithm is up to 103 times faster than the
basic GREEDYIMPROVEMENT algorithm and for all the iteration it visits only the 0.18% of
the arcs of the graph: using the DYNAMICGREEDYIMPROVEMENT algorithm, it is possible to
solve the MCI problem on very large graphs where it is impossible to obtain a solution
using the GREEDYIMPROVEMENT algorithm.
Like in the undirected case, we run the same set of experiments with different
numbers of cores and we measured the execution time and the speedup, i.e., the ratio
between the execution time with one core and the execution time with p cores for p =
2, 4, 6, 8. The results are reported in Table VII. We notice that the parallel algorithm
shows a very good scalability in terms of execution time.
Finally, we measured the increase in information spreading when a few edges are
added to a small set of randomly-chosen seeds. We performed experiments similar to
those shown in the undirected case in the graphs of Table VI. In Figure 21, we plot the
results for the Information_security directed network in the case of uniform threshold.
We observe that also in this case the informed nodes percentage increases. The results
for the other networks in Table VI and for non-uniform threshold are similar. As for
the case of undirected graphs, we observe that when k is big enough, the percentage of
nodes that become informed by using algorithms RANDOM, DEGREE, and TOP-K is smaller
than that obtained by using the greedy algorithm and that the gap between the other
baselines and the greedy algorithm increases with k.
Average relative ranking position between the Ranking obtained by the DIRECTEDGREEDYIMPROVE-
MENT algorithm and the different baselines in the network Information_security.
Performance of the DYNAMICGREEDYIMPROVEMENT algorithm on network uk-2007.
Percentage of active nodes in Information_security network as a function of k in the case of
uniform threshold. Parameter a denotes the threshold and the percentage of seeds is equal to 2%, 4%, 6%,
8%, and 10%, respectively.
4. CONCLUSION AND FUTURE RESEARCH
We considered the problem of adding k edges in a (directed or undirected) graph in
order to maximize the closeness of a predeﬁned vertex. For undirected graphs, we have
shown that the problem cannot be approximated within a factor larger than 1 −
and we proposed a greedy algorithm that guarantees an approximation factor of 1 −1
For directed graphs, the problem cannot be approximated within a factor larger than
3e, while the greedy algorithm still guarantees an approximation factor of 1 −1
We experimentally evaluated such algorithms and showed that they often compute an
optimal solution and, in any case, they achieve an approximation factor signiﬁcantly
better than the theoretical one. Moreover, by adding a very few edges a vertex can
drastically increase its centrality measure and its ranking.
As future works, we plan to extend our work to further centrality measures such
as betweenness, to generalize the problem by allowing the addition of edges incident
to other nodes of the graph, and to maximize the ranking of a node instead of the
centrality value.