Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1192–1202,
Austin, Texas, November 1-5, 2016. c⃝2016 Association for Computational Linguistics
Deep Reinforcement Learning for Dialogue Generation
Jiwei Li1, Will Monroe1, Alan Ritter2, Michel Galley3, Jianfeng Gao3 and Dan Jurafsky1
1Stanford University, Stanford, CA, USA
2Ohio State University, OH, USA
3Microsoft Research, Redmond, WA, USA
{jiweil,wmonroe4,jurafsky}@stanford.edu, 
{mgalley,jfgao}@microsoft.com
Recent neural models of dialogue generation
offer great promise for generating responses
for conversational agents, but tend to be shortsighted, predicting utterances one at a time
while ignoring their inﬂuence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional
NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to
integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues
between two virtual agents, using policy gradient methods to reward sequences that display
three useful conversational properties: informativity, coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as
with human judges, showing that the proposed
algorithm generates more interactive responses
and manages to foster a more sustained conversation in dialogue simulation. This work marks
a ﬁrst step towards learning a neural conversational model based on the long-term success of
dialogues.
Introduction
Neural response generation is of growing interest. The LSTM sequence-to-sequence (SEQ2SEQ)
model is one type of neural
generation model that maximizes the probability of
generating a response given the previous dialogue
turn. This approach enables the incorporation of rich
context when mapping between consecutive dialogue
turns in a way not possible, for
example, with MT-based dialogue models .
Despite the success of SEQ2SEQ models in dialogue generation, two problems emerge:
SEQ2SEQ models are trained by predicting the next
dialogue turn in a given conversational context using
the maximum-likelihood estimation (MLE) objective
function. However, it is not clear how well MLE
approximates the real-world goal of chatbot development: teaching a machine to converse with humans,
while providing interesting, diverse, and informative
feedback that keeps users engaged. One concrete
example is that SEQ2SEQ models tend to generate
highly generic responses such as “I don’t know” regardless of the input . This can be ascribed
to the high frequency of generic responses found in
the training set and their compatibility with a diverse
range of conversational contexts. Yet “I don’t know”
is apparently not a good action to take, since it closes
the conversation down.
Another common problem, illustrated in the two
sample conversations on the left of Table 1, is that
the system becomes stuck in an inﬁnite loop of repetitive responses. This is due to MLE-based SEQ2SEQ
models’ inability to account for repetition. In example 2 (bottom left), the dialogue falls into an inﬁnite
loop after three turns, with both agents generating
dull, generic utterances like i don’t know what you
are talking about and you don’t know what you are
saying. Looking at the entire conversation, utterance
(4) turns out to be a bad action to take because it
offers no way of continuing the conversation.1
1A similar rule is often suggested in improvisational comedy:
 
Baseline mutual information model 
Proposed reinforcement learning model
A: Where are you going? (1)
A: Where are you going? (1)
B: I’m going to the restroom. (2)
B: I’m going to the police station. (2)
A: See you later. (3)
A: I’ll come with you. (3)
B: See you later. (4)
B: No, no, no, no, you’re not going anywhere. (4)
A: See you later. (5)
A: Why? (5)
B: See you later. (6)
B: I need you to stay here. (6)
A: I don’t know what you are talking about. (7)
A: how old are you? (1)
A: How old are you? (1)
B: I’m 16. (2)
B: I’m 16. Why are you asking? (2)
A: 16? (3)
A I thought you were 12. (3)
B: I don’t know what you are talking about. (4)
B: What made you think so? (4)
A: You don’t know what you are saying. (5)
A: I don’t know what you are talking about. (5)
B: I don’t know what you are talking about . (6)
B: You don’t know what you are saying. (6)
A: You don’t know what you are saying. (7)
Table 1: Left Column: Dialogue simulation between two agents using a 4-layer LSTM encoder-decoder
trained on the OpenSubtitles dataset. The ﬁrst turn (index 1) is input by the authors. Then the two agents
take turns conversing, taking as input the other agent’s prior generated turn. The output is generated using
the mutual information model in which an N-best list is ﬁrst obtained using beam search
based on p(t|s) and reranked by linearly combining the backward probability p(s|t), where t and s respectively
denote targets and sources. Right Column: Dialogue simulated using the proposed reinforcement learning
model. The new model has more forward-looking utterances (questions like “Why are you asking?” and
offers like “I’ll come with you”) and lasts longer before it falls into conversational black holes.
These challenges suggest we need a conversation framework that has the ability to (1) integrate
developer-deﬁned rewards that better mimic the true
goal of chatbot development and (2) model the longterm inﬂuence of a generated response in an ongoing
To achieve these goals, we draw on the insights of
reinforcement learning, which have been widely applied in MDP and POMDP dialogue systems (see Related Work section for details). We introduce a neural reinforcement learning (RL) generation method,
which can optimize long-term rewards designed by
system developers. Our model uses the encoderdecoder architecture as its backbone, and simulates
conversation between two virtual agents to explore
the space of possible actions while learning to maximize expected reward. We deﬁne simple heuristic approximations to rewards that characterize good conversations: good conversations are forward-looking
 or interactive (a turn suggests
a following turn), informative, and coherent. The parameters of an encoder-decoder RNN deﬁne a policy
over an inﬁnite action space consisting of all possible
utterances. The agent learns a policy by optimizing
the long-term developer-deﬁned reward from ongoing dialogue simulations using policy gradient methods , rather than the MLE objective
deﬁned in standard SEQ2SEQ models.
Our model thus integrates the power of SEQ2SEQ
systems to learn compositional semantic meanings of
utterances with the strengths of reinforcement learning in optimizing for long-term goals across a conversation. Experimental results (sampled results at the
right panel of Table 1) demonstrate that our approach
fosters a more sustained dialogue and manages to
produce more interactive responses than standard
SEQ2SEQ models trained using the MLE objective.
Related Work
Efforts to build statistical dialog systems fall into two
major categories.
The ﬁrst treats dialogue generation as a sourceto-target transduction problem and learns mapping
rules between input messages and responses from a
massive amount of training data. Ritter et al. 
frames the response generation problem as a statisti-
cal machine translation (SMT) problem. Sordoni et
al. improved Ritter et al.’s system by rescoring the outputs of a phrasal SMT-based conversation
system with a neural model that incorporates prior
context. Recent progress in SEQ2SEQ models inspire
several efforts to build endto-end conversational systems which ﬁrst apply an
encoder to map a message to a distributed vector representing its semantics and generate a response from
the message vector. Serban et al. propose
a hierarchical neural model that captures dependencies over an extended conversation history. Li et al.
 propose mutual information between message and response as an alternative objective function
in order to reduce the proportion of generic responses
produced by SEQ2SEQ systems.
The other line of statistical research focuses on
building task-oriented dialogue systems to solve
domain-speciﬁc tasks.
Efforts include statistical
models such as Markov Decision Processes (MDPs)
 , POMDP models, and models that statistically learn generation rules . This dialogue literature thus widely applies
reinforcement learning to train dialogue
policies. But task-oriented RL dialogue systems often rely on carefully limited dialogue parameters, or
hand-built templates with state, action and reward signals designed by humans for each new domain, making the paradigm difﬁcult to extend to open-domain
scenarios.
Also relevant is prior work on reinforcement learning for language understanding - including learning
from delayed reward signals by playing text-based
games ,
executing instructions for Windows help , or understanding dialogues that give
navigation directions .
Our goal is to integrate the SEQ2SEQ and reinforcement learning paradigms, drawing on the advantages of both. We are thus particularly inspired by
recent work that attempts to merge these paradigms,
including Wen et al. — training an end-to-end
task-oriented dialogue system that links input representations to slot-value pairs in a database— or Su
et al. , who combine reinforcement learning
with neural generation on tasks with real users, showing that reinforcement learning improves dialogue
performance.
Reinforcement Learning for
Open-Domain Dialogue
In this section, we describe in detail the components
of the proposed RL model.
The learning system consists of two agents. We
use p to denote sentences generated from the ﬁrst
agent and q to denote sentences from the second.
The two agents take turns talking with each other.
A dialogue can be represented as an alternating sequence of sentences generated by the two agents:
p1, q1, p2, q2, ..., pi, qi. We view the generated sentences as actions that are taken according to a policy
deﬁned by an encoder-decoder recurrent neural network language model.
The parameters of the network are optimized to
maximize the expected future reward using policy
search, as described in Section 4.3. Policy gradient methods are more appropriate for our scenario
than Q-learning , because we can
initialize the encoder-decoder RNN using MLE parameters that already produce plausible responses,
before changing the objective and tuning towards a
policy that maximizes long-term reward. Q-learning,
on the other hand, directly estimates the future expected reward of each action, which can differ from
the MLE objective by orders of magnitude, thus making MLE parameters inappropriate for initialization.
The components (states, actions, reward, etc.) of our
sequential decision problem are summarized in the
following sub-sections.
An action a is the dialogue utterance to generate.
The action space is inﬁnite since arbitrary-length sequences can be generated.
A state is denoted by the previous two dialogue turns
[pi, qi]. The dialogue history is further transformed
to a vector representation by feeding the concatenation of pi and qi into an LSTM encoder model as
described in Li et al. .
A policy takes the form of an LSTM encoder-decoder
(i.e., pRL(pi+1|pi, qi) ) and is deﬁned by its parameters. Note that we use a stochastic representation
of the policy (a probability distribution over actions
given states). A deterministic policy would result in
a discontinuous objective that is difﬁcult to optimize
using gradient-based methods.
r denotes the reward obtained for each action. In this
subsection, we discuss major factors that contribute
to the success of a dialogue and describe how approximations to these factors can be operationalized in
computable reward functions.
Ease of answering
A turn generated by a machine
should be easy to respond to. This aspect of a turn
is related to its forward-looking function: the constraints a turn places on the next turn . We propose to
measure the ease of answering a generated turn by
using the negative log likelihood of responding to
that utterance with a dull response. We manually constructed a list of dull responses S consisting 8 turns
such as “I don’t know what you are talking about”,
“I have no idea”, etc., that we and others have found
occur very frequently in SEQ2SEQ models of conversations. The reward function is given as follows:
log pseq2seq(s|a)
where NS denotes the cardinality of NS and Ns denotes the number of tokens in the dull response s.
Although of course there are more ways to generate
dull responses than the list can cover, many of these
responses are likely to fall into similar regions in the
vector space computed by the model. A system less
likely to generate utterances in the list is thus also
less likely to generate other dull responses.
represents the likelihood output by
SEQ2SEQ models. It is worth noting that pseq2seq
is different from the stochastic policy function
pRL(pi+1|pi, qi), since the former is learned based
on the MLE objective of the SEQ2SEQ model while
the latter is the policy optimized for long-term future
reward in the RL setting. r1 is further scaled by the
length of target S.
Information Flow
We want each agent to contribute new information at each turn to keep the dialogue moving and avoid repetitive sequences. We
therefore propose penalizing semantic similarity between consecutive turns from the same agent. Let
hpi and hpi+1 denote representations obtained from
the encoder for two consecutive turns pi and pi+1.
The reward is given by the negative log of the cosine
similarity between them:
r2 = −log cos(hpi, hpi+1) = −log
hpi · hpi+1
∥hpi∥∥hpi+1∥
Semantic Coherence
We also need to measure the
adequacy of responses to avoid situations in which
the generated replies are highly rewarded but are ungrammatical or not coherent. We therefore consider
the mutual information between the action a and previous turns in the history to ensure the generated
responses are coherent and appropriate:
log pseq2seq(a|qi, pi)+ 1
log pbackward
seq2seq (qi|a)
pseq2seq(a|pi, qi) denotes the probability of generating response a given the previous dialogue utterances
[pi, qi]. pbackward
seq2seq (qi|a) denotes the backward probability of generating the previous dialogue utterance
qi based on response a. pbackward
is trained in a similar way as standard SEQ2SEQ models with sources
and targets swapped. Again, to control the inﬂuence of target length, both log pseq2seq(a|qi, pi) and
log pbackward
seq2seq (qi|a) are scaled by the length of targets.
The ﬁnal reward for action a is a weighted sum of
the rewards discussed above:
r(a, [pi, qi]) = λ1r1 + λ2r2 + λ3r3
where λ1 + λ2 + λ3 = 1. We set λ1 = 0.25, λ2 =
0.25 and λ3 = 0.5. A reward is observed after the
agent reaches the end of each sentence.
Simulation
The central idea behind our approach is to simulate
the process of two virtual agents taking turns talking
with each other, through which we can explore the
state-action space and learn a policy pRL(pi+1|pi, qi)
that leads to the optimal expected reward. We adopt
an AlphaGo-style strategy by
initializing the RL system using a general response
generation policy which is learned from a fully supervised setting.
Supervised Learning
For the ﬁrst stage of training, we build on prior work
of predicting a generated target sequence given dialogue history using the supervised SEQ2SEQ model
 . Results from supervised
models will be later used for initialization.
We trained a SEQ2SEQ model with attention on the OpenSubtitles dataset,
which consists of roughly 80 million source-target
pairs. We treated each turn in the dataset as a target
and the concatenation of two previous sentences as
source inputs.
Mutual Information
Samples from SEQ2SEQ models are often times dull
and generic, e.g., “i don’t know” 
We thus do not want to initialize the policy model
using the pre-trained SEQ2SEQ models because this
will lead to a lack of diversity in the RL models’ experiences. Li et al. showed that modeling
mutual information between sources and targets will
signiﬁcantly decrease the chance of generating dull
responses and improve general response quality. We
now show how we can obtain an encoder-decoder
model which generates maximum mutual information responses.
As illustrated in Li et al. , direct decoding
from Eq 3 is infeasible since the second term requires
the target sentence to be completely generated. Inspired by recent work on sequence level learning
 , we treat the problem of generating maximum mutual information response as a
reinforcement learning problem in which a reward
of mutual information value is observed when the
model arrives at the end of a sequence.
Similar to Ranzato et al. , we use policy gradient methods 
for optimization. We initialize the policy model pRL
using a pre-trained pSEQ2SEQ(a|pi, qi) model. Given
an input source [pi, qi], we generate a candidate list
A = {ˆa|ˆa ∼pRL}.
For each generated candidate ˆa, we will obtain the mutual information score
m(ˆa, [pi, qi]) from the pre-trained pSEQ2SEQ(a|pi, qi)
and pbackward
SEQ2SEQ(qi|a). This mutual information score
will be used as a reward and back-propagated to the
encoder-decoder model, tailoring it to generate sequences with higher rewards. We refer the readers to
Zaremba and Sutskever and Williams 
for details. The expected reward for a sequence is
J(θ) = E[m(ˆa, [pi, qi])]
The gradient is estimated using the likelihood ratio
∇J(θ) = m(ˆa, [pi, qi])∇log pRL(ˆa|[pi, qi])
We update the parameters in the encoder-decoder
model using stochastic gradient descent. A curriculum learning strategy is adopted 
as in Ranzato et al. such that, for every sequence of length T we use the MLE loss for the ﬁrst
L tokens and the reinforcement algorithm for the
remaining T −L tokens. We gradually anneal the
value of L to zero. A baseline strategy is employed to
decrease the learning variance: an additional neural
model takes as inputs the generated target and the
initial source and outputs a baseline value, similar
to the strategy adopted by Zaremba and Sutskever
 . The ﬁnal gradient is thus:
∇J(θ) = ∇log pRL(ˆa|[pi, qi])[m(ˆa, [pi, qi]) −b]
Dialogue Simulation between Two Agents
We simulate conversations between the two virtual
agents and have them take turns talking with each
other. The simulation proceeds as follows: at the
initial step, a message from the training set is fed to
the ﬁrst agent. The agent encodes the input message
to a vector representation and starts decoding to generate a response output. Combining the immediate
output from the ﬁrst agent with the dialogue history,
the second agent updates the state by encoding the
dialogue history into a representation and uses the
decoder RNN to generate responses, which are subsequently fed back to the ﬁrst agent, and the process
is repeated.
How old are
I’m 16, why are
you asking?
Input Message
I thought you
Figure 1: Dialogue simulation between the two agents.
Optimization
We initialize the policy model pRL
with parameters from the mutual information model
described in the previous subsection. We then use
policy gradient methods to ﬁnd parameters that lead
to a larger expected reward. The objective to maximize is the expected future reward:
JRL(θ) = EpRL(a1:T )[
R(ai, [pi, qi])]
where R(ai, [pi, qi]) denotes the reward resulting
from action ai. We use the likelihood ratio trick
 for gradient updates:
∇log p(ai|pi, qi)
R(ai, [pi, qi])
We refer readers to Williams and Glynn
 for more details.
Curriculum Learning
A curriculum Learning strategy is again employed
in which we begin by simulating the dialogue for 2
turns, and gradually increase the number of simulated
turns. We generate 5 turns at most, as the number
of candidates to examine grows exponentially in the
size of candidate list. Five candidate responses are
generated at each step of the simulation.
Experimental Results
In this section, we describe experimental results
along with qualitative analysis. We evaluate dialogue
generation systems using both human judgments and
two automatic metrics: conversation length (number
of turns in the entire session) and diversity.
The dialogue simulation requires high-quality initial
inputs fed to the agent. For example, an initial input
of “why ?” is undesirable since it is unclear how
the dialogue could proceed. We take a subset of
10 million messages from the OpenSubtitles dataset
and extract 0.8 million sequences with the lowest
likelihood of generating the response “i don’t know
what you are taking about” to ensure initial inputs
are easy to respond to.
Automatic Evaluation
Evaluating dialogue systems is difﬁcult. Metrics such
as BLEU and perplexity have
been widely used for dialogue quality evaluation , but it is widely debated how well these automatic metrics are correlated with true response quality . Since the
goal of the proposed system is not to predict the
highest probability response, but rather the long-term
success of the dialogue, we do not employ BLEU or
perplexity for evaluation2.
2We found the RL model performs worse on BLEU score. On
a random sample of 2,500 conversational pairs, single reference
BLEU scores for RL models, mutual information models and
vanilla SEQ2SEQ models are respectively 1.28, 1.44 and 1.17.
BLEU is highly correlated with perplexity in generation tasks.
# of simulated turns
mutual information
Table 2: The average number of simulated turns
from standard SEQ2SEQ models, mutual information model and the proposed RL model.
Length of the dialogue
The ﬁrst metric we propose is the length of the simulated dialogue. We say
a dialogue ends when one of the agents starts generating dull responses such as “i don’t know” 3 or two
consecutive utterances from the same user are highly
overlapping4.
The test set consists of 1,000 input messages. To
reduce the risk of circular dialogues, we limit the
number of simulated turns to be less than 8. Results
are shown in Table 2. As can be seen, using mutual
information leads to more sustained conversations
between the two agents. The proposed RL model is
ﬁrst trained based on the mutual information objective and thus beneﬁts from it in addition to the RL
model. We observe that the RL model with dialogue
simulation achieves the best evaluation score.
We report degree of diversity by calculating the number of distinct unigrams and bigrams in
generated responses. The value is scaled by the total
number of generated tokens to avoid favoring long
sentences as described in Li et al. . The resulting metric is thus a type-token ratio for unigrams
and bigrams.
For both the standard SEQ2SEQ model and the proposed RL model, we use beam search with a beam
size 10 to generate a response to a given input message. For the mutual information model, we ﬁrst
generate n-best lists using pSEQ2SEQ(t|s) and then
linearly re-rank them using pSEQ2SEQ(s|t). Results
are presented in Table 4. We ﬁnd that the proposed
RL model generates more diverse outputs when com-
Since the RL model is trained based on future reward rather than
MLE, it is not surprising that the RL based models achieve lower
BLEU score.
3We use a simple rule matching method, with a list of 8
phrases that count as dull responses. Although this can lead
to both false-positives and -negatives, it works pretty well in
4Two utterances are considered to be repetitive if they share
more than 80 percent of their words.
pared against both the vanilla SEQ2SEQ model and
the mutual information model.
Human Evaluation
We explore three settings for
human evaluation: the ﬁrst setting is similar to what
was described in Li et al. , where we employ
crowdsourced judges to evaluate a random sample of
500 items. We present both an input message and the
generated outputs to 3 judges and ask them to decide
which of the two outputs is better (denoted as singleturn general quality). Ties are permitted. Identical
strings are assigned the same score. We measure
the improvement achieved by the RL model over the
mutual information model by the mean difference in
scores between the models.
For the second setting, judges are again presented
with input messages and system outputs, but are
asked to decide which of the two outputs is easier to
respond to (denoted as single-turn ease to answer).
Again we evaluate a random sample of 500 items,
each being assigned to 3 judges.
For the third setting, judges are presented with simulated conversations between the two agents (denoted
as multi-turn general quality). Each conversation
consists of 5 turns. We evaluate 200 simulated conversations, each being assigned to 3 judges, who are
asked to decide which of the simulated conversations
is of higher quality.
Results for human evaluation are shown in Table 5.
The proposed RL system does not introduce a signiﬁcant boost in single-turn response quality (winning
40 percent of time and losing 36 percent of time).
This is in line with our expectations, as the RL model
is not optimized to predict the next utterance, but
rather to increase long-term reward. The RL system produces responses that are signiﬁcantly easier
to answer than does the mutual information system,
as demonstrated by the single-turn ease to answer
setting (winning 52 percent of time and losing 23
percent of time), and also signiﬁcantly higher quality
multi-turn dialogues, as demonstrated by the multiturn general quality setting (winning 72 percent of
Qualitative Analysis and Discussion
random sample of generated responses in Table 3 and
simulated conversations in Table 1 at the beginning
of the paper. From Table 3, we can see that the
RL based agent indeed generates more interactive
Input Message
Mutual Information Model
How old are you?
I’m 16, why are you asking?
What’s your full name?
I have no idea.
What’s yours?
I don’t want to go home tonight.
Do you have any feelings for me?
I don’t know what you are talking about.
I don’t want to hurt your feelings.
How much time do you have here?
Not long enough. Sorry, sir.
Ten seconds.
Shall we get started?
Of course!
Yes. We’ve got a lot of work to do here.
Do you play football?
No, i don’t
Yes. I love football!
We’d have to talk to him.
I mean, he’s a good guy
About what ?
How come you never say it?
Because I don’t want to hurt you.
I don’t think it’s a good idea to say it.
Table 3: Sampled responses generated from the mutual information models and the proposed RL model.
mutual information
Table 4: Diversity scores (type-token ratios) for the
standard SEQ2SEQ model, mutual information model
and the proposed RL model.
single-turn general quality
single-turn ease to answer
multi-turn general quality
Table 5: RL gains over the mutual information system based on pairwise human judgments.
responses than the other baselines. We also ﬁnd that
the RL model has a tendency to end a sentence with
another question and hand the conversation over to
the user. From Table 1, we observe that the RL model
manages to produce more interactive and sustained
conversations than the mutual information model.
During error analysis, we found that although we
penalize repetitive utterances in consecutive turns,
the dialogue sometimes enters a cycle with length
greater than one, as shown in Table 6. This can be
ascribed to the limited amount of conversational history we consider. Another issue observed is that the
model sometimes starts a less relevant topic during
the conversation. There is a tradeoff between relevance and less repetitiveness, as manifested in the
reward function we deﬁne in Eq 4.
The fundamental problem, of course, is that the
manually deﬁned reward function can’t possibly
cover the crucial aspects that deﬁne an ideal conversation. While the heuristic rewards that we deﬁned are
amenable to automatic calculation, and do capture
A: What’s your name ?
B: Daniel.
A: How old are you ?
B. Twelve. What’s your name ?
A. Daniel.
B: How old are you ?
A: Twelve. What’s your name ?
B: Daniel.
A: How old are you ?
Table 6: An simulated dialogue with a cycle longer
some aspects of what makes a good conversation,
ideally the system would instead receive real rewards
from humans. Another problem with the current
model is that we can only afford to explore a very
small number of candidates and simulated turns since
the number of cases to consider grow exponentially.
Conclusion
We introduce a reinforcement learning framework for
neural response generation by simulating dialogues
between two agents, integrating the strengths of neural SEQ2SEQ systems and reinforcement learning
for dialogue. Like earlier neural SEQ2SEQ models,
our framework captures the compositional models
of the meaning of a dialogue turn and generates semantically appropriate responses. Like reinforcement learning dialogue systems, our framework is
able to generate utterances that optimize future reward, successfully capturing global properties of a
good conversation. Despite the fact that our model
uses very simple, operationable heuristics for capturing these global properties, the framework generates
more diverse, interactive responses that foster a more
sustained conversation.
Acknowledgement
We would like to thank Chris Brockett, Bill Dolan
and other members of the NLP group at Microsoft Research for insightful comments and suggestions. We
also want to thank Kelvin Guu, Percy Liang, Chris
Manning, Sida Wang, Ziang Xie and other members
of the Stanford NLP groups for useful discussions.
Jiwei Li is supported by the Facebook Fellowship, to
which we gratefully acknowledge. This work is partially supported by the NSF via Awards IIS-1514268,
IIS-1464128, and by the DARPA Communicating
with Computers (CwC) program under ARO prime
contract no. W911NF- 15-1-0462. Any opinions,
ﬁndings, and conclusions or recommendations expressed in this material are those of the authors and
do not necessarily reﬂect the views of NSF, DARPA,
or Facebook.