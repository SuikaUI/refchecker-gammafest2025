Mathematical Programming 27 107-120
North-Holland
DISTRIBUTED ASYNCHRONOUS
COMPUTATION
OF FIXED POINTS*
Dimitri P. BERTSEKAS
Laboratory for Information and Decision Systems, Massachusetts Institute of Technology,
Canibridge, MA 02139, U.S.A.
Received 2 October 1981
Revised manuscript received 20 August 1982
We present an algorithmic model for distribnted computation of fixed points whereby
several processors participate simultaneously in the calculations while exchanging information
via communication links. We place essentially no assumptions on the ordering of computation
and communication between processors thereby allowing for completely uncoordinated
execution. We provide a general convergence theorem for algorithms of this type, and
demonstrate its applicability to several classes of problems including the calculation of fixed
points of contraction and monotone mappings arising in linear and nonlinear systems of
equations, optimization problems, shortest path problems, and dynamic programming.
Key words: Fixed Points, Distributed Algorithm, Optimization, Shortest Path, Dynamic
Programming.
1. Introduction
There is presently a great deal of interest in distributed implementations of
various iterative algorithms whereby the computational load is shared by several
processors while coordination is maintained by information exchange via com-
munication links. In most of the work done in this area the starting point is some
iterative algorithm which is guaranteed to converge to the correct solution under
the usual circumstances of centralized computation in a single processor. The
computational load of the typical iteration is then divided in some way between
the available processors, and it is assumed that the processors exchange all
necessary information regarding the outcomes of the current iteration before a
new iteration can begin.
The mode of operation described above may be termed synchronous in the
sense that each processor must complete its assigned portion of an iteration and
communicate the results to every other processor before a new iteration can
begin. This assumption certainly enchances the orderly operation of the al-
gorithm and greatly simplifies the convergence analysis. On the other hand
synchronous distributed algorithms also have some obvious implementation
disadvantages such as the need for an algorithm initiation and iteration synch-
*This research was contacted at the M.I.T. Laboratory for Information and Decision Systems with
partial support provided by the Defense Advanced Research Projects Agency under Grant No.
ONR-N00014-75-C- I 183.
D.P. Bertsekas/ Distributed computation
ronization protocol. Furthermore the speed of computation is limited to that of
the slowest processor. It is thus interesting to consider algorithms that can
tolerate a more flexible ordering of computation and communication between
processors. Such algorithms have so far found applications in computer com-
munication networks such as the ARPANET where processor failures are
common and it is quite complicated to maintain synchronization between the
nodes of the entire network as they execute real-time network functions such as
ihe routing algorithm. They could also find application in special purpose
multiprocessors of the type that are currently being implemented by several
research groups.
Given a distributed algorithm it is natural to try to determine the minimum
degree of synchronization between computation and communication that is
necessary in order for the algorithm to work correctly. In this paper we consider
an extreme model of asynchronous distributed algorithms whereby computation
and communication is performed at the various processors completely in-
dependently of the progress in other processors. Perhaps somewhat surprisingly
we find that even under these potentially chaotic circumstances of un-
coordinated computation it is possible to solve correctly broad and significant
classes of fixed point problems by means of the natural distributed version of the
successive approximation method. A general convergence theorem is developed
for this purpose which delineates circumstances under which convergence is
guaranteed. The theorem is then applied to broad classes of fixed point problems
involving contraction and monotone mappings.
The nature of the distributed algorithm and the convergence result of this
paper can be illustrated by considering the following example involving itera-
tions of the Gauss-Seidel type for solving systems of nonlinear eqnations.
Fixed points of mappings
on R": Consider the problem of finding an n-
dimensional vector x* which is a fixed point of a mapping f : R" ~ R", i.e.
x* = f(x*).
Let x~ and fi, i = 1 ..... n denote the coordinates of x and f respectively, and
consider iterations of the form
x, +-'- f i(x,, .';'_ ..... X,,) ,
xz~f~_(x,,
x~_ ..... x,,),
x,,+---f,,(x~,xe ..... x,,).
In order to give precise meaning to iterations (1.1)-(1.n) we must specify the
order in which they are executed and the rule by which the values of the
coordinates x~ ..... x,, are chosen in the right side of each iteration. There are
several ways of doing this that lead to well-known algorithms. Assume for
example that all computations are done at a single central processor and at some
D.P. Bertsekas/ Distributed computation
time instant k the vector x has the value x k= (x~ ..... x~). A possible iteration
(the classical successive approximation method) is
x k§ = f(xk).
It corresponds to all iterations (I. I)-(l.n) being carried out "simultaneously', i.e.
without substituting in the right side of (1.1)-(l.n) the most recently computed
values of the coordinates of x. This can be contrasted with the following
Gauss-Seidel type iteration
k+, = fl(x], ,ok
x~ +' = f~(x~', x~ ..... x~;),
where the most recently computed values of the coordinates are being used.
Iterations (l.1)-(l.n) lend themselves well to distributed computation by n
processors each assigned the responsibility of executing only one of these
iterations and using communication links to exchange with other processors the
most recent result of their respective computations. Iteration (2) corresponds to
a synchronous model whereby each processor i executes its assigned iteration
(1.i) and then communicates the updated value x~ ~ ~ to all other processors. After
all updated values are exchanged the process is repeated. Iteration (3.1)-(3.n)
corresponds to a sequential model whereby processor 1 executes (1.1), transmits
x~ *~ to all other processors, then processor 2 executes (1.2), transmits x~' ~ to all
other processors and so on. One of the sharpest general convergence results
available for iterations (2) and (3.1)-(3.n) is that a sufficient condition for
convergence to the unique fixed point of f is that f be a P-contraction mapping
[11, p. 433]. By this we mean that there exists an n xn
matrix P with
nonnegative elements, with spectral radius strictly less than unity, and such that
If(x) -f(Y)l ~< PIx - yl
Vx, y ~ e"
where for any vector z =(zt ..... z,,) we denote by [zl the column vector with
coordinates the absolute values Iz~l ..... lz,,I, and the inequality in (4) is meant to
hold separately for each coordinate.
There is a variety of ways of executing iterations (1. l)-(1.n) other than (2) or
(3.1)-(3.n). For example the order in which the iterations are executed may
change as time progresses. An even bolder step is to assume that not only the
order of the iterations may be arbitra.rily changed, but also the values of the
coordinates in the right side of the iterations may be arbitrarily out-of-date. For
example (1.1) may be executed on the basis of the values of x~ ..... x,, most
recently computed but (1.2)may
be executed on the basis of values of
x~, x3 .... , x,, computed, say, one hundred computation 'cycles" ago. A surprising
fact--a consequence of the general convergence result of this paper--is that
D.P. Bertsekas] Distrihuted computation
even under these extreme circumstances the assumption (4) is still sufficient to
guarantee convergence of the resulting algorithm.
In order to prove results such as the one briefly described above it is
necessary to introduce a precise distributed computation model since the tradi-
tional concept of an iteration does not fully capture the essense of distributed
algorithms of tile type we are interested in. This is done in the next section. In
Section 3 we develop our main convergence result while in Section 4 we analyze
some important special cases.
2. A model for distributed asynchronous fixed point algorithms
The fixed point problem considered in this paper is defined in terms of a set X
and a function f : X --, X. We wish to find an element x* E X such that
x* =f(x*).
Each x @ X is defined in terms of "coordinates' x~, i E I where I is a possibly
infinite index set, i.e. we have x = {x, I i E I}. Each coordinate x~ is either a real
number or _+ ~c. Similarly f is defined in terms of its coordinate ftmctions f~, i E I
fi(x)~[-z,
equivalently written as
If 1 has n elements, I = {1,2 ..... n}, and x~ is a real number for each i, then the
problem is simply to find a fixed point of an n-dimensional mapping on a subset
X of the Euclidean space R"--the example considered in the previous section.
Evidently all problems of solving a system of n nonlinear equations with n
unknowns, as well as many problems of n-dimensional unconstrained and
constrained optimization can be posed in this manner. The case where x~ can
take the values +~ and -~
is genuinely interesting as it arises in dynamic
programming problems (see [4, 2, Chapters 6 and 7]). Despite the fact that in any
practical implementation of the algorithm of this paper the index set I must be
finite and the coordinates x~ must be real numbers (indeed bounded precision
rationals), it is useful for analytical purposes to consider the problem in the more
general framework described above.
An interesting example of problem (5) is the shortest path problem for which
the algorithm of this paper bears close relation with a routing algorithm ori-
~While this paper was under review the author became aware of considerable related work of
Chazan and Miranker [5l, Miellou , and Baudet on asynchronous relaxation methods. The
result just stated is proved in essence by these authors by different methods. The computation model
considered by these authors is similar but is less general and differs in essential details from ours.
The convergence result of this paper is much more general--for example it applies to dynamic
programming algorithms.
D.P. Bertsekas/ Distributed computation
ginally implemented in the ARPANET and subsequently in several other com-
puter networks .
Shortest path problem: Let (I, A) be a directed graph where I =(1 ..... n}
denotes the set of nodes and A denotes the set of arcs. Let N(i) denote the
downstream neighbors of node i, i.e., the set of nodes j for which (i, j) is an arc.
Assume that each arc (i, j) is assigned a positive scalar aii referred to as its
length. Assume also that there is a directed path to node 1 from every other
node. Then it is known that the shortest path distances x* to node 1 from
all other nodes i solve uniquely the equations
x* = min {aij + x*}
If we make the identifications
min {aii+ xi}
X = {x [ x~ = 0, x~ E [0, ~], i = 2 ..... n },
then we find that the fixed point problem (6) reduces to the shortest path
Actually the problem above is representative of a broad class of dynamic
programming problems which can be viewed as special cases of the fixed point
problem (6) and can be correctly solved by using the distributed algorithm of this
paper (see ).
Our algorithmic model can be described in terms of a collection of n com-
putation centers (or processors) referred to as nodes and denoted I, 2 ..... n. The
index set I is partitioned into n disjoint sets denoted I~ ..... I,,, i.e.
liNI,,,=o,
Each node j is assigned the responsibility of computing the coordinates x* of a
fixed point x* for all i E I i.
At each time instant, node j can be in one of three possible states compute,
transmit, or idle. In the compute state node j computes a new estimate xi for all
i E Ij. In the transmit state node j communicates the estimate obtained from its
own latest computation to one or more nodes re(m# j). In the idle state node j
does nothing related to the solution of the problem. It is assumed that a node can
receive a transmission from other nodes simultaneously with computing or
transmitting, but this is not a real restriction since, if needed, a time period in a
separate receive state can be lumped into a time period in the idle state.
We assume that computation and transmission for each node takes place in
uninterupted time intervals [tT, t2] with t~ < t,_, but do not exclude the possibility
D.P. Bertsekas/ Distributed ctmlputation
that a node may be simultaneously transmitting to more than one node nor do
we assume that the transmission intervals to these nodes have the same origin
and/or termination. We also make no assumptions on the length, timing and
sequencing of computation and transmission intervals other than the following:
Assumption A. For every node j and time t -> 0 there exists a time t' > t such that
[t, t'] contains at least one computation interval for j and at least one trans-
mission interval from j to each node m 4 j.
Assumption A is very natural. It states in essence that no node 'drops out of
the algorithm' permanently--perhaps due to a hardware failure. Without this
assumption there is hardly anything we can hope to prove.
Each node j has a buffer Bj,,, for each m~ j where it stores the latest
transmission from m, as well as a fubber Bij where it stores its own estimate of
the coordinates x~ of a solution for all i C lj. The contents for each buffer Bi,, at
time t are denoted x'(j, m). Thus x'(j, m) is, for every t, j and m a vector of
coordinate estimates {x~ ] i E I,,} available at node j at time t. It is important to
realize in what follows that the buffer contents x'(j, m) and xt(j ', m) at two
different nodes .i and j' need not coincide at all times. If j# m and j'~ m the
buffer contents x'(j, m) and x'(j', m) need not coincide at any time t. The vector
of all buffer contents of node j is denoted x'(j), i.e.,
x'(j) = {x'(j, m)lm
= 1 ..... n}.
The coordinates of x'(j) are denoted x'~(j), i E I, and the coordinates of x'(j, m)
are denoted x'~(j, m ), i C I ....
The rules according to which the buffer contents x'(j, m) are updated are as
(1) If [tL, t,] is a transmission interval from node m to node j, the contents of
the buffer B ..... at time t~ are transmitted and entered in the buffer Bj,,, at time t:,
x"(m, m) = x~'~(j, m).
(2) If Its, t2] is a computation interval for node j, the contents of the buffer Bii
at time t~ are replaced by L[x'~(j)]. i E I~, i.e.
x',2(j) = h[x"(j)]
(3) The contents of a buffer Bij can change only at the end of a computation
interval for node j. The contents of a buffer Bj,,,, jg: m can change only at the end
of a transmission interval from m to j.
Our objective is to derive conditions under which
limx](j)=x*
j=l ..... n,
where x* E X is a fixed point of f. This is the subject of the next section.
D.P. Berlsekas/ Distributed computation
3. A general convergence theorem
In our effort to develop a general convergence result for the distributed
algorithmic model of the previous section we draw motivation from existing
convergence theories for (centralized) iterative algorithms. There are several
theories of this type (Zangwill , Luenberger , Daniel , Ortega and
Rheinboldt [ll]--the most general are due to Poljak and Polak ). All
these theories have their origin in Lyapunov's stability theory for differential and
difference equations. The main idea is to consider a generalized distance
function (or Lyapunov function) of the typical iterate to the solution set. In
optimization methods the objective function is often suitable for this purpose
while in equation solving methods a norm of the difference between the current
iterate and the solution is usually employed. The idea is typically to show at each
iteration the value of the distance function is reduced and reaches its minimum
value in the limit.
Our result is based on a similar idea. However, instead of working with a
generalized distance function we prefer to work (essentially) with the level sets
of such a function.
We formulate the following assumption under which we will subsequently
prove convergence of the type indicated in (9). In what follows R denotes the
set of all vectors x={x~]x~E[-oc, oc], i~l},
denotes the Cartesian
product of X with itself, and fI~'=l X denotes the Cartesian product of X with
itself n times.
Assumption B. There exists a sequence X k of subsets of X with the following
properties:
(a) If {x k} is a sequence in X such that if x k E X k for all k, then
where x* • X is some fixed point of f.
(b) For all k=0,1 .... and j= 1 ..... n
x E Xk=:), f(x; j)@ X k
where f(. ; j) : X --~ )( is the mapping defined by
(c) For all k=0, l .... and j= 1 ..... n
x E X k, x' C X k ~ C(x, x'; j) E X k
D.P. Bertsekas[ Distributed computation
where C(., .:j):X x X~)(
is the mapping defined by
if x~! lj,
Ci(x,x';j)=
(d) For all k = 0, 1 ....
x I E X k, x 2 @ X k ..... x" ~ X k ~ F(xL, x 2 ..... x") ~ X k+l,
where F : FI;' i X ~ 2( is the mapping defined by
Fi(xl,x 2 ..... x")=fi(x i)
Vi C li, j= 1 ..... n.
Assumption B seems rather complicated so it may be worth providing some
motivation for introducing it. Property (a) specifies how the sets X k should
relate to a solution x*. Property (d) guarantees that if the functions in the buffers
of all nodes i= 1 .... , n belong to X k and a computation phase is carried out
simultaneously at all nodes followed by a communication phase from every node
to every other node, then the resulting function in the buffer of each node (which
will be the same for all nodes), will belong to X k+~. Properties (a) and (d)
alone guarantee that the algorithm will converge to a correct solution if executed
in a synchronous manner, i.e., a simultaneous computation phase at all nodes is
followed by a simultaneous communication phase from each node to all other
nodes and the process is repeated. Property (b) involves the mapping f(.: j)
which is related to a computation phase at node j (compare (8) with (12)), while
property (c) involves the mapping C(., .: j) which is related to a communication
phase from node j to some other node (compare (7) with (13), (14)). Basically
properties (b) and (c) guarantee that the sets X k are closed with respect to
individual node computation and communication. By this we mean that if all
buffer contents are within X k, then after a single node computation or com-
munication all buffer contents will remain in X k. The following proposition
asserts that when properties (b) and (c) hold in addition to (a) and (d), then the
algorithm converges to the correct solution when operated in a totally un-
coordinated manner.
Proposition. Let Assumptions
A and B hold, and assume that the initial barfer
contents x~
at each node j = l ..... n belong to x ~ Then
limx'~(j)=x*
V i ~ l, j = l ..... n
where x* ~ X is a fixed point of f and xl(j) is the ith coordinate of the buffer
content vector x'(j) of node j at time t.
Proof. We will show that for every k = O, 1 .... and t -> 0 the condition
D.P. Bertsekas/ Distributed computation
implies that there exists a time t~ > t such that
xr(j) C X k
V t' >- t, j =- 1 ..... n,
x"(j) EX k+~
Vt'>-t~,j=
l ..... n.
In view of condition (a) of Assumption B, this will suffice to prove the
proposition.
Assume that (18) holds for some k = 0, l .... and t ->0. Then (19) clearly holds
since, for t '> _ t, the buffer content xr(j) of node j at t' is obtained from the
buffer contents x'(m) of all nodes m = 1 ..... n at t via operations that (according
to conditions (b) and (c) of Assumption B) preserve membership in X k.
By assumption A there exists a scalar 6~ >0 such that [t, t + 6~] contains at
least one computation interval for each node j = 1 ..... n. Therefore, using (8), we
have that for each t'_> t + 61.
xl'(j)=fi[x;(j)]
V iEI i,j= l ..... n
is the buffer content of node j at some time tE [t, t + ~5~] (t depends
on j), and by (19)
Using again Assumption A we have that there exists a scalar 6~ > 0 such that
[t + 6~, t + 6~ + 6~] contains at least one communication interval from every node
to every other node. It follows that, for every t'>_t§ 6~ + 6_~, each buffer Bi,,.
contains a vector xr(i, m) such that (cf. (7), (21))
x~i(j, m) = x~(m, m) =fi[x;(m)]
V i E I,,., j, m = 1 ..... n,
where x;(m, m) is the content of buffer B ...... at node m at some time /'C
[t+6~,t+6~+62]
and x;(m) is the buffer content of node m at some time
[E It, t']. (Again here the times { and [ depend on j and m.)
Let t~ = t + 6t + 32. By using (22) and (19) we can assert that for each t'- > tt
and j = 1 ..... n there exist vectors s E X k, j = 1 ..... n such that
xl'(j)=f~(g j)
V i E Ij, j = l ..... n,
It follows from condition (d) of Assumption B (cf. (15), (16)) that
Vt'>-t~,j=
which is (20). This completes the proof of the proposition.
Note that (18) and (20) can form the basis for an estimate of the rate of
convergence of the algorithm. For example if there exists an index /~ such that
X k = X ~ = {x*} for all k -/~ (i.e. after some index the sets X k contain only one
element--a fixed point x* ~X),
then it follows from (18)-(20) that the dis-
tributed algorithm converges to the correct solution in a finite amount of time.
D.P. Berlsekas/ Distributed computation
This argument can, for example, be used to establish finite time convergence for
the distributed algorithm as applied to the shortest path problem of Section 2.
4. Special cases
In this section we verify that Assumption B of the previous section is satisfied
for some important classes of problems.
4.1. Contraction mappings with respect to sup-norms
Let J( be the vector space of all x={x~lxi~(-~-~c),i~l}
bounded in the sense that there exists M >0 such that ]x~[-< M for all i E I.
Consider a norm on X of the form
Ilxll = sup ~,lx, I
where {ai I i E l} is a set of scalars such that for some d > 0 and a > 0 we have
Assume that the set X either equals 2 or is a closed sphere centered at a fixed
point x* of f. Assume further that f is a contraction mapping on X with respect
to the norm (23) in the sense that, for some p < 1 we have
]If(x) -f(y)[] <- pllx - YI] V x, y ~ X.
Then, because 2 is a complete space and X is a closed subset of 2, x* is the
unique fixed point of f in X (cf. [151).
For q > 0 define
x k=Ix~2]llx-x*ll<_p"q},
k=O,l .....
It is evident that if Xc'C X, then the sequence {X k} satisfies conditions (a)-(d) of
Assumption B.
We note that the use of a sup-norm such as (23) is essential in order for
Assumption B to hold. If f is a contraction mapping with respect to some other
type of norm, Assumption B need not be satisfied.
4.2. P-contraction mappings
Let I = {l, 2 ..... n} and assume that X is a subset of R". Suppose that f is a
P-contraction mapping, i.e. satisfies the condition
]f(x)-f(y)]<-Plx-y]
where P is an n x n matrix with nonnegative elements and spectral radius
strictly less than unity, and for any z = (z~, z2 ..... z,,) we denote by Izl the column
D.P. Bertsekas/ Distributed computation
vector with coordinates Iz,I, ]z21 ..... Iz,,]. Condition (24) holds in particular if P is
a stochastic matrix (all elements of P are nonnegative and the sum of the
elements of each row of P is less than or equal to unity) and lim~.~P k =0.
Fixed point problems involving P-contraction mappings arise in dynamic pro-
gramming [2, p. 374], and solution of systems of nonlinear equations [11, Section
It has been shown in [1, p. 231] that if f is a P-contraction, then it is a
contraction mapping with respect to some norm of the form (23). We are
therefore reduced to the case examined earlier.
4.3. Monotone mappings
Assume that f has the monotonicity property
xi<_x~,ViEI=)fi(x)<-fi(x'),
Denote by fk the composition of f with itself k times and assume that there exist
two elements x and $ of X such that
{x Ix-~-<x~-<~i, ViEI}cX
and for all k = 0, 1 ....
I~ t._v)</~+~(~)_<
lim f~(._x) = lim f~(~) = x*
where x* ~ X is a fixed point of f.
As an example consider the shortest path problem in Section 2, and the
Vi= 1 ..... n,
It is easily verified that the corresponding function f satisfies (25) and that S, ~ as
defined above satisfy (26), (27), (28).
Define now for k = 0, 1 ....
X k = {x [ f~(~) -< .x~ -< f~(.r
Then it is easily seen that the sequence {X k} satisfies conditions (a)-(d) of
Assumption B.
Fixed point problems involving monotone mappings satisfying (25) arise in
dynamic programming and solution of systems of nonlinear equations
[11, Section 13.2].
D.P. Bertsekas/ Distributed COmlmtatiorl
4.4. Unconslrained
optimization
Consider the problem
subject to
where g : R" ~ R is a twice continuously differentiable convex function, with
Hessian matrix Veg(x) which is positive definite for all x.
The mapping that corresponds to Newton's method is given by
f(x) = x - [V2g(x)] Wg(x)
where Vg(x) denotes the gradient of g at x. Under the assumptions made earlier
a vector x* satisfying Vg(x*)= 0 is the unique globally optimal solution of
problem (29) and also the unique fixed point of the mapping f of (30). Suppose
there exists such a vector x*. Then it is a simple matter to verify that there
exists an open sphere centered at x* such that the mapping f of (30) is a
contraction mapping in X with respect to the norm Ilxll = max~ Ix~[. Therefore the
distributed version of Newton's method is convergent if the starting buffer
contents are sufficiently near x*. A similar fact can be shown if the inverse
Hessian [V2g(x)] ~ in (30) is replaced by a matrix H(x)
such that the difference
H(x)-[V2g(x)]
-~ has sufficiently small norm uniformly within X.
Consider next the mapping corresponding to the ordinary gradient method
f(x) = x - oeVg(x)
where ~ is a positive scalar stepsize. Again if x* is the unique optimal solution
of problem (29), then x* is the unique fixed point of f as given by (31). The
Jacobian matrix of f is given by
Of(x) = I - c~V2g(x)
where I is the n x n identity matrix. Using the mean value theorem we have for
all x, yR"
af~(x ~) r
f~(x) - f,(y) = ~ ~
,xj - y~) V i = 1 ..... n
where x ~ is a vector lying on the line segment joining x and y. From (33) we
Ifi(x)-f,(Y)[--<
Ix i - y/].
Denote by If(x)-f(y)l and Ix- y] the column vectors with coordinates ]/,(x)-
f~(y)] and ]x~- y~[ respectively. Assume that the stepsize c~ in (31) satisfies
Vi=I,2 ..... n,
D.P. Bertsekas/ Distributed computation
Then, with the aid of (32), we can write (34) as
If(x) -/(Y)I <- Fix - Y]
where F is the n x n matrix given by
F = I - aG
and G is given by
G = I(ax,)
and the derivatives in the ith row of the matrix above are evaluated at x ~.
It is now seen easily from (36) and (37) that f will be a P-contraction mapping
within an open sphere centered at x* provided the following two conditions
(a) The matrix G* is positive definite where G* is given by (38) with all partial
derivatives evaluated at x*.
(b) The stepsize a is sufficiently small so that (35) holds and the matrix
1 - cYG* (cf. (37)) is positive definite. Equivalently a should be smaller than the
inverses of the largest eigenvalue and the largest diagonal element of G*.
If the two conditions above are satisfied, then the distributed gradient al-
gorithm based on the mapping f of (31) is convergent to x* provided all buffer
contents are sufficiently close to x*.
Unfortunately it is not true that the matrix G* is always positive definite and
indeed examples can be constructed where the distributed gradient method can
fail to converge to the optimal solution x* regardless of the choice of the
stepsize or. Despite this fact we believe that the distributed gradient method is an
interesting algorithm. We will show in a forthcoming publication that it has
satisfactory convergence properties provided we impose certain mild restrictions
on the relative timing of computations and communications in place of Assump-
5. Conclusions
The analysis of this paper shows that broad classes of fixed point problems
can be solved by distributed algorithms that operate under very weak restric-
tions on the timing and ordering of processor computation and communication
phases. It is also interesting that the initial processor buffer contents need not be
identical and can vary within a broad range. This means that for problems that
are being solved continuously in real time it is not necessary to reset the initial
D.P. Bertsekas/ Distributed computation
conditions and resynchronize the algorithm each time the problem data changes.
As a result the potential for tracking slow changes in the solution function is
improved and algorithmic implementation is greatly simplified.