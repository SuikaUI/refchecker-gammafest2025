EfﬁcientDet: Scalable and Efﬁcient Object Detection
Mingxing Tan
Ruoming Pang
Quoc V. Le
Google Research, Brain Team
{tanmingxing, rpang, qvl}@google.com
Model efﬁciency has become increasingly important in
computer vision. In this paper, we systematically study neural network architecture design choices for object detection
and propose several key optimizations to improve efﬁciency.
First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and
width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family
of object detectors, called EfﬁcientDet, which consistently
achieve much better efﬁciency than prior art across a wide
spectrum of resource constraints. In particular, with singlemodel and single-scale, our EfﬁcientDet-D7 achieves stateof-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs1, being 4x – 9x smaller and using
13x – 42x fewer FLOPs than previous detectors. Code is
available at 
master/efficientdet.
1. Introduction
Tremendous progresses have been made in recent years
towards more accurate object detection; meanwhile, stateof-the-art object detectors also become increasingly more
expensive. For example, the latest AmoebaNet-based NAS-
FPN detector requires 167M parameters and 3045B
FLOPs (30x more than RetinaNet ) to achieve state-ofthe-art accuracy. The large model sizes and expensive computation costs deter their deployment in many real-world
applications such as robotics and self-driving cars where
model size and latency are highly constrained. Given these
real-world resource constraints, model efﬁciency becomes
increasingly important for object detection.
There have been many previous works aiming to develop more efﬁcient detector architectures, such as one-
1Similar to , FLOPs denotes number of multiply-adds.
FLOPs (Billions)
EfﬁcientDet-D7
Mask R-CNN
ResNet + NAS-FPN
AmoebaNet + NAS-FPN + AA
FLOPs (ratio)
EfﬁcientDet-D0
YOLOv3 
33.0 71B (28x)
EfﬁcientDet-D1
RetinaNet 
39.2 97B (16x)
EfﬁcientDet-D7x†
AmoebaNet+ NAS-FPN +AA † 50.7 3045B (13x)
†Not plotted.
Figure 1: Model FLOPs vs. COCO accuracy – All numbers are for single-model single-scale.
Our EfﬁcientDet
achieves new state-of-the-art 55.1% COCO AP with much
fewer parameters and FLOPs than previous detectors. More
studies on different backbones and FPN/NAS-FPN/BiFPN
are in Table 4 and 5. Complete results are in Table 2.
stage and anchor-free detectors ,
or compress existing models . Although these methods tend to achieve better efﬁciency, they usually sacriﬁce
accuracy. Moreover, most previous works only focus on a
speciﬁc or a small range of resource requirements, but the
variety of real-world applications, from mobile devices to
datacenters, often demand different resource constraints.
A natural question is: Is it possible to build a scalable detection architecture with both higher accuracy and
better efﬁciency across a wide spectrum of resource constraints (e.g., from 3B to 300B FLOPs)? This paper aims
to tackle this problem by systematically studying various
design choices of detector architectures. Based on the onestage detector paradigm, we examine the design choices for
backbone, feature fusion, and class/box network, and identify two main challenges:
Challenge 1: efﬁcient multi-scale feature fusion – Since
introduced in , FPN has been widely used for multi-
 
scale feature fusion. Recently, PANet , NAS-FPN ,
and other studies have developed more network
structures for cross-scale feature fusion. While fusing different input features, most previous works simply sum them
up without distinction; however, since these different input
features are at different resolutions, we observe they usually contribute to the fused output feature unequally. To
address this issue, we propose a simple yet highly effective
weighted bi-directional feature pyramid network (BiFPN),
which introduces learnable weights to learn the importance
of different input features, while repeatedly applying topdown and bottom-up multi-scale feature fusion.
Challenge 2: model scaling –
While previous works
mainly rely on bigger backbone networks or
larger input image sizes for higher accuracy, we observe that scaling up feature network and box/class prediction network is also critical when taking into account both
accuracy and efﬁciency. Inspired by recent works , we
propose a compound scaling method for object detectors,
which jointly scales up the resolution/depth/width for all
backbone, feature network, box/class prediction network.
Finally, we also observe that the recently introduced EfﬁcientNets achieve better efﬁciency than previous commonly used backbones. Combining EfﬁcientNet backbones
with our propose BiFPN and compound scaling, we have
developed a new family of object detectors, named EfﬁcientDet, which consistently achieve better accuracy with
much fewer parameters and FLOPs than previous object
detectors.
Figure 1 and Figure 4 show the performance
comparison on COCO dataset . Under similar accuracy constraint, our EfﬁcientDet uses 28x fewer FLOPs than
YOLOv3 , 30x fewer FLOPs than RetinaNet , and
19x fewer FLOPs than the recent ResNet based NAS-FPN
 . In particular, with single-model and single test-time
scale, our EfﬁcientDet-D7 achieves state-of-the-art 55.1 AP
with 77M parameters and 410B FLOPs, outperforming previous best detector by 4 AP while being 2.7x smaller
and using 7.4x fewer FLOPs. Our EfﬁcientDet is also up to
4x to 11x faster on GPU/CPU than previous detectors.
With simple modiﬁcations, we also demonstrate that
our single-model single-scale EfﬁcientDet achieves 81.74%
mIOU accuracy with 18B FLOPs on Pascal VOC 2012 semantic segmentation, outperforming DeepLabV3+ by
1.7% better accuracy with 9.8x fewer FLOPs.
2. Related Work
One-Stage Detectors:
Existing object detectors are
mostly categorized by whether they have a region-ofinterest proposal step (two-stage ) or not (onestage ). While two-stage detectors tend to be
more ﬂexible and more accurate, one-stage detectors are often considered to be simpler and more efﬁcient by leveraging predeﬁned anchors . Recently, one-stage detectors
have attracted substantial attention due to their efﬁciency
and simplicity . In this paper, we mainly follow
the one-stage detector design, and we show it is possible
to achieve both better efﬁciency and higher accuracy with
optimized network architectures.
Multi-Scale Feature Representations:
One of the main
difﬁculties in object detection is to effectively represent and
process multi-scale features. Earlier detectors often directly
perform predictions based on the pyramidal feature hierarchy extracted from backbone networks . As one
of the pioneering works, feature pyramid network (FPN)
 proposes a top-down pathway to combine multi-scale
Following this idea, PANet adds an extra
bottom-up path aggregation network on top of FPN; STDL
 proposes a scale-transfer module to exploit cross-scale
features; M2det proposes a U-shape module to fuse
multi-scale features, and G-FRNet introduces gate units
for controlling information ﬂow across features. More recently, NAS-FPN leverages neural architecture search
to automatically design feature network topology. Although
it achieves better performance, NAS-FPN requires thousands of GPU hours during search, and the resulting feature
network is irregular and thus difﬁcult to interpret. In this
paper, we aim to optimize multi-scale feature fusion with a
more intuitive and principled way.
Model Scaling:
In order to obtain better accuracy, it
is common to scale up a baseline detector by employing
bigger backbone networks (e.g., from mobile-size models
 and ResNet , to ResNeXt and AmoebaNet
 ), or increasing input image size (e.g., from 512x512
 to 1536x1536 ). Some recent works show
that increasing the channel size and repeating feature networks can also lead to higher accuracy.
These scaling
methods mostly focus on single or limited scaling dimensions. Recently, demonstrates remarkable model efﬁciency for image classiﬁcation by jointly scaling up network
width, depth, and resolution. Our proposed compound scaling method for object detection is mostly inspired by .
In this section, we ﬁrst formulate the multi-scale feature
fusion problem, and then introduce the main ideas for our
proposed BiFPN: efﬁcient bidirectional cross-scale connections and weighted feature fusion.
3.1. Problem Formulation
Multi-scale feature fusion aims to aggregate features at
different resolutions. Formally, given a list of multi-scale
features ⃗P in = (P in
l2 , ...), where P in
represents the
feature at level li, our goal is to ﬁnd a transformation f that
can effectively aggregate different features and output a list
of new features: ⃗P out = f(⃗P in). As a concrete example,
(c) NAS-FPN
repeated blocks
repeated blocks
Figure 2: Feature network design – (a) FPN introduces a top-down pathway to fuse multi-scale features from level 3 to
7 (P3 - P7); (b) PANet adds an additional bottom-up pathway on top of FPN; (c) NAS-FPN use neural architecture
search to ﬁnd an irregular feature network topology and then repeatedly apply the same block; (d) is our BiFPN with better
accuracy and efﬁciency trade-offs.
Figure 2(a) shows the conventional top-down FPN . It
takes level 3-7 input features ⃗P in = (P in
3 , ...P in
7 ), where
represents a feature level with resolution of 1/2i of the
input images. For instance, if input resolution is 640x640,
represents feature level 3 (640/23 = 80) with resolution 80x80, while P in
7 represents feature level 7 with resolution 5x5. The conventional FPN aggregates multi-scale
features in a top-down manner:
= Conv(P in
= Conv(P in
6 + Resize(P out
= Conv(P in
3 + Resize(P out
where Resize is usually a upsampling or downsampling
op for resolution matching, and Conv is usually a convolutional op for feature processing.
3.2. Cross-Scale Connections
Conventional top-down FPN is inherently limited by the
one-way information ﬂow. To address this issue, PANet
 adds an extra bottom-up path aggregation network, as
shown in Figure 2(b). Cross-scale connections are further
studied in . Recently, NAS-FPN employs
neural architecture search to search for better cross-scale
feature network topology, but it requires thousands of GPU
hours during search and the found network is irregular and
difﬁcult to interpret or modify, as shown in Figure 2(c).
By studying the performance and efﬁciency of these
three networks (Table 5), we observe that PANet achieves
better accuracy than FPN and NAS-FPN, but with the cost
of more parameters and computations. To improve model
efﬁciency, this paper proposes several optimizations for
cross-scale connections: First, we remove those nodes that
only have one input edge.
Our intuition is simple: if a
node has only one input edge with no feature fusion, then
it will have less contribution to feature network that aims
at fusing different features. This leads to a simpliﬁed bidirectional network; Second, we add an extra edge from the
original input to output node if they are at the same level,
in order to fuse more features without adding much cost;
Third, unlike PANet that only has one top-down and
one bottom-up path, we treat each bidirectional (top-down
& bottom-up) path as one feature network layer, and repeat
the same layer multiple times to enable more high-level feature fusion. Section 4.2 will discuss how to determine the
number of layers for different resource constraints using a
compound scaling method. With these optimizations, we
name the new feature network as bidirectional feature pyramid network (BiFPN), as shown in Figure 2 and 3.
3.3. Weighted Feature Fusion
When fusing features with different resolutions, a common way is to ﬁrst resize them to the same resolution and
then sum them up. Pyramid attention network introduces global self-attention upsampling to recover pixel localization, which is further studied in . All previous
methods treat all input features equally without distinction.
However, we observe that since different input features are
at different resolutions, they usually contribute to the output
feature unequally. To address this issue, we propose to add
an additional weight for each input, and let the network to
learn the importance of each input feature. Based on this
idea, we consider three weighted fusion approaches:
Unbounded fusion:
i wi · Ii, where wi is a
learnable weight that can be a scalar (per-feature), a vector (per-channel), or a multi-dimensional tensor (per-pixel).
We ﬁnd a scale can achieve comparable accuracy to other
approaches with minimal computational costs. However,
since the scalar weight is unbounded, it could potentially
cause training instability. Therefore, we resort to weight
normalization to bound the value range of each weight.
Softmax-based fusion: O = P
j ewj · Ii. An intuitive
idea is to apply softmax to each weight, such that all weights
are normalized to be a probability with value range from 0
to 1, representing the importance of each input. However,
as shown in our ablation study in section 6.3, the extra softmax leads to signiﬁcant slowdown on GPU hardware. To
minimize the extra latency cost, we further propose a fast
fusion approach.
Fast normalized fusion: O = P
· Ii, where
wi ≥0 is ensured by applying a Relu after each wi, and
ϵ = 0.0001 is a small value to avoid numerical instability.
Similarly, the value of each normalized weight also falls
between 0 and 1, but since there is no softmax operation
here, it is much more efﬁcient. Our ablation study shows
this fast fusion approach has very similar learning behavior
and accuracy as the softmax-based fusion, but runs up to
30% faster on GPUs (Table 6).
Our ﬁnal BiFPN integrates both the bidirectional crossscale connections and the fast normalized fusion. As a concrete example, here we describe the two fused features at
level 6 for BiFPN shown in Figure 2(d):
w1 · P in
6 + w2 · Resize(P in
w1 + w2 + ϵ
3 · Resize(P out
where P td
6 is the intermediate feature at level 6 on the topdown pathway, and P out
is the output feature at level 6 on
the bottom-up pathway. All other features are constructed
in a similar manner. Notably, to further improve the efﬁciency, we use depthwise separable convolution for
feature fusion, and add batch normalization and activation
after each convolution.
4. EfﬁcientDet
Based on our BiFPN, we have developed a new family
of detection models named EfﬁcientDet. In this section, we
will discuss the network architecture and a new compound
scaling method for EfﬁcientDet.
4.1. EfﬁcientDet Architecture
Figure 3 shows the overall architecture of EfﬁcientDet,
which largely follows the one-stage detectors paradigm
 .
We employ ImageNet-pretrained EfﬁcientNets as the backbone network. Our proposed BiFPN
serves as the feature network, which takes level 3-7 features
{P3, P4, P5, P6, P7} from the backbone network and repeatedly applies top-down and bottom-up bidirectional feature fusion. These fused features are fed to a class and box
network to produce object class and bounding box predictions respectively. Similar to , the class and box network weights are shared across all levels of features.
4.2. Compound Scaling
Aiming at optimizing both accuracy and efﬁciency, we
would like to develop a family of models that can meet
a wide spectrum of resource constraints. A key challenge
here is how to scale up a baseline EfﬁcientDet model.
Previous works mostly scale up a baseline detector by
employing bigger backbone networks (e.g., ResNeXt 
or AmoebaNet ), using larger input images, or stacking more FPN layers . These methods are usually ineffective since they only focus on a single or limited scaling dimensions. Recent work shows remarkable performance on image classiﬁcation by jointly scaling up all
dimensions of network width, depth, and input resolution.
Inspired by these works , we propose a new compound scaling method for object detection, which uses a
simple compound coefﬁcient φ to jointly scale up all dimensions of backbone , BiFPN, class/box network, and resolution. Unlike , object detectors have much more scaling
dimensions than image classiﬁcation models, so grid search
for all dimensions is prohibitive expensive. Therefore, we
use a heuristic-based scaling approach, but still follow the
main idea of jointly scaling up all dimensions.
Backbone network –
we reuse the same width/depth
scaling coefﬁcients of EfﬁcientNet-B0 to B6 such that
we can easily reuse their ImageNet-pretrained checkpoints.
BiFPN network –
we linearly increase BiFPN depth
Dbifpn (#layers) since depth needs to be rounded to small
integers. For BiFPN width Wbifpn (#channels), exponentially grow BiFPN width Wbifpn (#channels) as similar to
 . Speciﬁcally, we perform a grid search on a list of values {1.2, 1.25, 1.3, 1.35, 1.4, 1.45}, and pick the best value
1.35 as the BiFPN width scaling factor. Formally, BiFPN
width and depth are scaled with the following equation:
Wbifpn = 64 ·
Dbifpn = 3 + φ
Box/class prediction network –
we ﬁx their width to be
always the same as BiFPN (i.e., Wpred = Wbifpn), but lin-
EfficientNet backbone
BiFPN Layer
Class prediction net
Box prediction net
Figure 3: EfﬁcientDet architecture – It employs EfﬁcientNet as the backbone network, BiFPN as the feature network,
and shared class/box prediction network. Both BiFPN layers and class/box net layers are repeated multiple times based on
different resource constraints as shown in Table 1.
early increase the depth (#layers) using equation:
Dbox = Dclass = 3 + ⌊φ/3⌋
Input image resolution –
Since feature level 3-7 are used
in BiFPN, the input resolution must be dividable by 27 =
128, so we linearly increase resolutions using equation:
Rinput = 512 + φ · 128
Following Equations 1,2,3 with different φ, we have developed EfﬁcientDet-D0 (φ = 0) to D7 (φ = 7) as shown
in Table 1, where D7 and D7x have the same BiFPN and
head, but D7 uses higher resolution and D7x uses larger
backbone network and one more feature level (from P3 to
P8). Notably, our compound scaling is heuristic-based and
might not be optimal, but we will show that this simple scaling method can signiﬁcantly improve efﬁciency than other
single-dimension scaling methods in Figure 6.
5. Experiments
5.1. EfﬁcientDet for Object Detection
We evaluate EfﬁcientDet on COCO 2017 detection
datasets with 118K training images.
Each model
is trained using SGD optimizer with momentum 0.9 and
weight decay 4e-5. Learning rate is linearly increased from
0 to 0.16 in the ﬁrst training epoch and then annealed down
using cosine decay rule. Synchronized batch norm is added
after every convolution with batch norm decay 0.99 and epsilon 1e-3. Same as the , we use SiLU (Swish-1) activation and exponential moving average with
decay 0.9998. We also employ commonly-used focal loss
 with α = 0.25 and γ = 1.5, and aspect ratio {1/2, 1,
D0 (φ = 0)
D1 (φ = 1)
D2 (φ = 2)
D3 (φ = 3)
D4 (φ = 4)
D5 (φ = 5)
D6 (φ = 6)
D7 (φ = 7)
Table 1: Scaling conﬁgs for EfﬁcientDet D0-D6 – φ is
the compound coefﬁcient that controls all other scaling dimensions; BiFPN, box/class net, and input size are scaled
up using equation 1, 2, 3 respectively.
2}. During training, we apply horizontal ﬂipping and scale
jittering [0.1, 2.0], which randomly rsizes images between
0.1x and 2.0x of the original size before cropping. We apply soft-NMS for eval. For D0-D6, each model is trained
for 300 epochs with total batch size 128 on 32 TPUv3 cores,
but to push the envelope, we train D7/D7x for 600 epochs
on 128 TPUv3 cores.
Table 2 compares EfﬁcientDet with other object detectors, under the single-model single-scale settings with
no test-time augmentation.
We report accuracy for both
test-dev (20K test images with no public ground-truth)
and val with 5K validation images. Notably, model performance depends on both network architecture and trainning
settings (see appendix), but for simplicity, we only reproduce RetinaNet using our trainers and refer other models
from their papers. In general, our EfﬁcientDet achieves bet-
Latency (ms)
EfﬁcientDet-D0 (512)
YOLOv3 
EfﬁcientDet-D1 (640)
RetinaNet-R50 (640) 
RetinaNet-R101 (640) 
EfﬁcientDet-D2 (768)
Detectron2 Mask R-CNN R101-FPN 
Detectron2 Mask R-CNN X101-FPN 
EfﬁcientDet-D3 (896)
ResNet-50 + NAS-FPN (1024) 
ResNet-50 + NAS-FPN (1280) 
ResNet-50 + NAS-FPN (1280@384) 
EfﬁcientDet-D4 (1024)
AmoebaNet+ NAS-FPN +AA(1280) 
EfﬁcientDet-D5 (1280)
Detectron2 Mask R-CNN X152 
EfﬁcientDet-D6 (1280)
AmoebaNet+ NAS-FPN +AA(1536) 
EfﬁcientDet-D7 (1536)
EfﬁcientDet-D7x (1536)
We omit ensemble and test-time multi-scale results . RetinaNet APs are reproduced with our trainer and others are from papers.
‡Latency numbers with ‡ are from detectron2, and others are measured on the same machine (TensorFlow2.1 + CUDA10.1, no TensorRT).
Table 2: EfﬁcientDet performance on COCO – Results are for single-model single-scale. test-dev is the COCO
test set and val is the validation set. Params and FLOPs denote the number of parameters and multiply-adds. Latency is
for inference with batch size 1. AA denotes auto-augmentation . We group models together if they have similar accuracy,
and compare their model size, FLOPs, and latency in each group.
ter efﬁciency than previous detectors, being 4x – 9x smaller
and using 13x - 42x less FLOPs across a wide range of accuracy or resource constraints. On relatively low-accuracy
regime, our EfﬁcientDet-D0 achieves similar accuracy as
YOLOv3 with 28x fewer FLOPs. Compared to RetinaNet
 and Mask-RCNN , our EfﬁcientDet achieves similar accuracy with up to 8x fewer parameters and 21x fewer
On high-accuracy regime, our EfﬁcientDet also
consistently outperforms recent object detectors 
with much fewer parameters and FLOPs.
In particular,
our single-model single-scale EfﬁcientDet-D7x achieves a
new state-of-the-art 55.1 AP on test-dev, outperforming prior art by a large margin in both accuracy (+4 AP) and
efﬁciency (7x fewer FLOPs).
In addition, we have also compared the inference latency
on Titan-V FP32 , V100 GPU FP16, and single-thread CPU.
Notably, our V100 latency is end-to-end including preprocessing and NMS postprocessing. Figure 4 illustrates the
comparison on model size and GPU/CPU latency. For fair
comparison, these ﬁgures only include results that are measured on the same machine with the same settings. Compared to previous detectors, EfﬁcientDet models are up to
4.1x faster on GPU and 10.8x faster on CPU, suggesting
they are also efﬁcient on real-world hardware.
5.2. EfﬁcientDet for Semantic Segmentation
While our EfﬁcientDet models are mainly designed for
object detection, we are also interested in their performance
on other tasks such as semantic segmentation. Following
 , we modify our EfﬁcientDet model to keep feature
level {P2, P3, ..., P7} in BiFPN, but only use P2 for the
ﬁnal per-pixel classiﬁcation. For simplicity, here we only
evaluate a EfﬁcientDet-D4 based model, which uses a ImageNet pretrained EfﬁcientNet-B4 backbone (similar size to
ResNet-50). We set the channel size to 128 for BiFPN and
256 for classiﬁcation head. Both BiFPN and classiﬁcation
head are repeated by 3 times.
Table 3 shows the comparison between our models
and previous DeepLabV3+ on Pascal VOC 2012 .
Notably, we exclude those results with ensemble, testtime augmentation, or COCO pretraining.
same single-model single-scale settings, our model achieves
1.7% better accuracy with 9.8x fewer FLOPs than the prior
art of DeepLabV3+ . These results suggest that Efﬁcient-
Det is also quite promising for semantic segmentation.
Parameters (M)
Mask R-CNN
EfﬁcientDet-D6
ResNet + NAS-FPN
Params Ratio
EfﬁcientDet-D1
RetinaNet 
EfﬁcientDet-D3
ResNet + NASFPN 
EfﬁcientDet-D6
AmoebaNet + NAS-FPN 209M
(a) Model Size
GPU latency (s)
EfﬁcientDet-D6
ResNet + NAS-FPN
EfﬁcientDet-D1
RetinaNet 
EfﬁcientDet-D3
ResNet + NASFPN 
150ms 4.1x
EfﬁcientDet-D6
AmoebaNet + NAS-FPN 489ms 2.9x
(b) GPU Latency
CPU latency (s)
EfﬁcientDet-D6
ResNet + NAS-FPN
EfﬁcientDet-D1
RetinaNet 
EfﬁcientDet-D3
ResNet + NASFPN 
EfﬁcientDet-D6
AmoebaNet + NAS-FPN 
(c) CPU Latency
Figure 4: Model size and inference latency comparison – Latency is measured with batch size 1 on the same machine
equipped with a Titan V GPU and Xeon CPU. AN denotes AmoebaNet + NAS-FPN trained with auto-augmentation .
Our EfﬁcientDet models are 4x - 9x smaller, 2x - 4x faster on GPU, and 5x - 11x faster on CPU than other detectors.
DeepLabV3+ (ResNet-101) 
DeepLabV3+ (Xception) 
Our EfﬁcientDet†
†A modiﬁed version of EfﬁcientDet-D4.
Table 3: Performance comparison on Pascal VOC semantic segmentation.
6. Ablation Study
In this section, we ablate various design choices for our
proposed EfﬁcientDet. For simplicity, all accuracy results
here are for COCO validation set.
6.1. Disentangling Backbone and BiFPN
Since EfﬁcientDet uses both a powerful backbone and a
new BiFPN, we want to understand how much each of them
contributes to the accuracy and efﬁciency improvements.
Table 4 compares the impact of backbone and BiFPN using RetinaNet training settings. Starting from a RetinaNet
detector with ResNet-50 backbone and top-down
FPN , we ﬁrst replace the backbone with EfﬁcientNet-
B3, which improves accuracy by about 3 AP with slightly
less parameters and FLOPs. By further replacing FPN with
our proposed BiFPN, we achieve additional 4 AP gain with
much fewer parameters and FLOPs. These results suggest
that EfﬁcientNet backbones and BiFPN are both crucial for
our ﬁnal models.
6.2. BiFPN Cross-Scale Connections
Table 5 shows the accuracy and model complexity for
feature networks with different cross-scale connections
listed in Figure 2.
Notably, the original FPN and
PANet only have one top-down or bottom-up ﬂow, but
for fair comparison, here we repeat each of them multiple
Parameters
ResNet50 + FPN
EfﬁcientNet-B3 + FPN
EfﬁcientNet-B3 + BiFPN
Table 4: Disentangling backbone and BiFPN – Starting
from the standard RetinaNet (ResNet50+FPN), we ﬁrst replace the backbone with EfﬁcientNet-B3, and then replace
the baseline FPN with our proposed BiFPN.
times and replace all convs with depthwise separable convs,
which is the same as BiFPN. We use the same backbone and
class/box prediction network, and the same training settings
for all experiments. As we can see, the conventional topdown FPN is inherently limited by the one-way information ﬂow and thus has the lowest accuracy. While repeated
FPN+PANet achieves slightly better accuracy than NAS-
FPN , it also requires more parameters and FLOPs. Our
BiFPN achieves similar accuracy as repeated FPN+PANet,
but uses much less parameters and FLOPs. With the additional weighted feature fusion, our BiFPN further achieves
the best accuracy with fewer parameters and FLOPs.
6.3. Softmax vs Fast Normalized Fusion
As discussed in Section 3.3, we propose a fast normalized feature fusion approach to get ride of the expensive
softmax while retaining the beneﬁts of normalized weights.
Table 6 compares the softmax and fast normalized fusion
approaches in three detectors with different model sizes. As
shown in the results, our fast normalized fusion approach
achieves similar accuracy as the softmax-based fusion, but
runs 1.26x - 1.31x faster on GPUs.
In order to further understand the behavior of softmaxbased and fast normalized fusion, Figure 5 illustrates the
Input1 weight (%)
(a) Example Node 1
Input1 weight (%)
(b) Example Node 2
Input1 weight (%)
(c) Example Node 3
Figure 5: Softmax vs. fast normalized feature fusion – (a) - (c) shows normalized weights (i.e., importance) during training
for three representative nodes; each node has two inputs (input1 & input2) and their normalized weights always sum up to 1.
Repeated top-down FPN
Repeated FPN+PANet
Fully-Connected FPN
BiFPN (w/o weighted)
BiFPN (w/ weighted)
Table 5: Comparison of different feature networks – Our
weighted BiFPN achieves the best accuracy with fewer parameters and FLOPs.
Softmax Fusion
Fast Fusion
AP (delta)
33.85 (-0.11)
43.77 (-0.01)
48.74 (-0.05)
Table 6: Comparison of different feature fusion – Our
fast fusion achieves similar accuracy as softmax-based fusion, but runs 28% - 31% faster.
learned weights for three feature fusion nodes randomly selected from the BiFPN layers in EfﬁcientDet-D3. Notably,
the normalized weights (e.g., ewi/ P
j ewj for softmaxbased fusion, and wi/(ϵ + P
j wj) for fast normalized fusion) always sum up to 1 for all inputs. Interestingly, the
normalized weights change rapidly during training, suggesting different features contribute to the feature fusion
unequally. Despite the rapid change, our fast normalized
fusion approach always shows very similar learning behavior to the softmax-based fusion for all three nodes.
6.4. Compound Scaling
As discussed in section 4.2, we employ a compound
scaling method to jointly scale up all dimensions of
depth/width/resolution for backbone, BiFPN, and box/class
prediction networks.
Figure 6 compares our compound
scaling with other alternative methods that scale up a single dimension of resolution/depth/width. Although start-
Compound Scaling
Scale by image size
Scale by #channels
Scale by #BiFPN layers
Scale by #box/class layers
Figure 6: Comparison of different scaling methods –
compound scaling achieves better accuracy and efﬁciency.
ing from the same baseline detector, our compound scaling
method achieves better efﬁciency than other methods, suggesting the beneﬁts of jointly scaling by better balancing
difference architecture dimensions.
7. Conclusion
In this paper, we systematically study network architecture design choices for efﬁcient object detection, and propose a weighted bidirectional feature network and a customized compound scaling method, in order to improve accuracy and efﬁciency. Based on these optimizations, we develop a new family of detectors, named EfﬁcientDet, which
consistently achieve better accuracy and efﬁciency than the
prior art across a wide spectrum of resource constraints. In
particular, our scaled EfﬁcientDet achieves state-of-the-art
accuracy with much fewer parameters and FLOPs than previous object detection and semantic segmentation models.
Acknowledgements
Special thanks to Golnaz Ghiasi, Adams Yu, Daiyi
Peng for their help on infrastructure and discussion. We
also thank Adam Kraft, Barret Zoph, Ekin D. Cubuk,
Hongkun Yu, Jeff Dean, Pengchong Jin, Samy Bengio,
Reed Wanderman-Milne, Tsung-Yi Lin, Xianzhi Du, Xiaodan Song, Yunxing Dai, and the Google Brain team. We
thank the open source community for the contributions.