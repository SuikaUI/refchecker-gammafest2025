Context Dependent Recurrent Neural Network Language
Tomas Mikolov and Geoffrey Zweig
Microsoft Research Technical Report MSR-TR-2012-92
July 27th, 2012
Recurrent neural network language models (RNNLMs) have recently demonstrated state-of-the-art
performance across a variety of tasks. In this paper, we improve their performance by providing
a contextual real-valued input vector in association with each word. This vector is used to convey
contextual information about the sentence being modeled. By performing Latent Dirichlet Allocation
using a block of preceding text, we achieve a topic-conditioned RNNLM. This approach has the key
advantage of avoiding the data fragmentation associated with building multiple topic models on
different data subsets. We report perplexity results on the Penn Treebank data, where we achieve a
new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition task,
where we observe improvements in word-error-rate.
Key words: Recurrent Neural Network, Language Modeling, Topic Models, Latent Dirichlet Allocation
1 Introduction
Recurrent neural network language models (RNNLMs) have recently been shown to produce
state-of-the-art results in perplexity and word error rate across a variety of tasks . These networks differ from classical feed-forward neural network language models in that they
maintain a hidden-layer of neurons with recurrent connections to their own previous values. This
recurrent property gives a RNNLM the potential to model long span dependencies. However, theoretical analysis indicates that the gradient computation becomes increasingly ill-behaved the
farther back in time an error signal must be propagated, and that therefore learning arbitrarily longspan phenomena will not be possible. In practice, performance comparisons with very long-span
feed-forward neural networks indicate that the RNNLM is similar to a feedforward network
with eight or nine words of context.
Tomas Mikolov and Geoffrey Zweig
Microsoft Research, Redmond, WA., e-mail: {t-tomik,gzweig}@microsoft.com
Tomas Mikolov and Geoffrey ZweigMicrosoft Research Technical Report MSR-TR-2012-92July 27th, 2012
In the past, a a number of techniques have been used to bring long span and contextual information to bear in conventional N-gram language models. Perhaps the simplest of these is the cache
language model in which a language model score based on a model trained on the last K words is
interpolated with that from a general model. Similar in spirit to the cache based models are the latent
semantic analysis (LSA) based approaches of . These methods represent long-span history as
a vector in latent semantic space, and base LSA-estimated word probability on the cosine similarity
between a hypothesized word and the history. These similarity-based probabilities are then interpolated with N-gram probabilities. Topic-conditioned language models , e.g. , most frequently
work by partitioning the training data into subsets, with the goal of making subsets containing data
on only one topic. Separate language models are then trained, and at runtime the most appropriate
one (or combination) is chosen. In a voice-search application , long span context was used in a
maximum-entropy N-gram model by creating features to indicate when a hypothesized word
appeared in a user’s history. Finally, in whole-sentence language models , trigger features
based on the existence of widely separated word pairs also provides long-span information.
In this paper, we study the use of long-span context in RNNLMs. One approach to increasing the
effective context is to improve the learning algorithm to avoid the problem of vanishing gradients
identiﬁed in . This is exempliﬁed by recent work on Hessian-free optimization . Another
is to modify the model itself, as in the Long Short-Term Memory neural networks , which use
gating neurons to ”latch” onto the error signal for multiple timesteps. In contrast to these approaches,
we have chosen to explicitly compute a context vector based on the sentence history, and provide
it directly to the network as an additional input. This has the advantage of allowing us to bring
sophisticated and pre-existing topic modeling techniques to bear with little overhead, speciﬁcally
Latent Dirichlet Allocation (LDA) . Moreover, it does this in a way that in other applications
allows us to use context that is external to the text (e.g. a vector representing user-habits in voice
search). Chu and Mangu also recently used Latent Dirichlet Allocation to determine topics, but
performed a hard partitioning of the data and built a set of disjoint models.
This paper makes several contributions. First, we suggest the use of context vectors to improve the
performance of a RNNLM. Secondly, we demonstrate perplexity improvements over the previous
state-of-the-art for the Penn Treebank. Thirdly, we develop an efﬁcient method for computing context vectors when using a sliding window of context. Finally, we evaluate our models by rescoring
N-best lists from a speech recognizer and observe improvements there as well.
The remainder of this paper is structured as follows. Section 2 describes the augmented RNN
model we use. Section 3 describes our method of constructing context vectors based on Latent
Dirichlet Allocation. Sections 4 and 5 present perplexity results on the Penn Treebank and word
error rates on the Wall Street Journal speech recognition task. We provide some future directions
and concluding remarks in Section 6.
2 Model Structure
The simple recurrent neural network language model consists of an input layer, a hidden layer
with recurrent connections that propagate time-delayed signals, and an output layer, plus the corresponding weight matrices. The input vector w(t) represents input word at time t encoded using
1-of-N coding (also called one-hot coding), and the output layer produces a probability distribution
Context Dependent Recurrent Neural Network Language Model
over words. The hidden layer maintains a representation of the sentence history. We extend this basic
model with an additional feature layer f(t) that is connected to both the hidden and output layers,
as shown in Figure 1. The feature layer represents an external input vector that should contain complementary information to the input word vector w(t). In the rest of this paper, we will be using
features that represent topic information. Nevertheless, we note that the external features can be any
information source such as part of speech tags, morphological information about the current word
w(t), or speaker-speciﬁc information in the context of ASR.
There are several possible advantages to using topic information as additional input to the model,
instead of building many separate topic-speciﬁc submodels: mainly, the training data will be less
fragmented. Also, by providing the topic information directly at the input of the model, we elegantly
avoid the long-standing problem of training recurrent neural networks to remember longer-term
information (usually referred to as the vanishing gradient problem, and addressed in ).
The input vector w(t) and the output vector y(t) have dimensionality of the vocabulary (later denoted as V). After the network is trained using stochastic gradient descent, the vector y(t) represents
a probability distribution over words from the vocabulary given the previous word w(t), the context
vector s(t−1) and the feature vector f(t).
The values in the hidden and output layers are computed as follows:
s(t) = f (Uw(t)+Ws(t−1)+Ff(t))
y(t) = g(Vs(t)+Gf(t)),
The training of neural network language models consists of ﬁnding the weight matrices U,V,W,F
and G such that the likelihood of the training data is maximized. The reader is referred to for
further detail.
3 Latent Dirichlet Allocation for Context Modeling
We use Latent Dirichlet Allocation (LDA) to achieve a compact vector-space representation of
long span context. This procedure maps a bag-of-words representation of a document into a lowdimensional vector which is conventionally interpreted as a topic representation. For our purposes, a
document will consist of a sentence or block of contiguous words. Each induced topic has associated
with it a unigram distribution over words, and the collection of distributions is denoted β. LDA is a
generative model of text, and the generation process of a document goes as follows:
1. Decide on the document length N by sampling from a Poisson distribution: N ∼Poisson(ξ).
2. Decide on a multinomial distribution over topics for the document by sampling from a Dirichlet
distribution parameterized by α: Θ ∼Dir(α).
3. For each of the N words to be generated, ﬁrst decide on a topic to draw it from, and then on the
word itself:
• Choose the topic zn ∼Multinomial(Θ).
Tomas Mikolov and Geoffrey ZweigMicrosoft Research Technical Report MSR-TR-2012-92July 27th, 2012
Fig. 1 Recurrent neural network based language model, with the additional feature layer f(t) and the corresponding
weight matrices.
• Choose a word wn from the unigram distribution associated with the topic: p(wn|zn,β).
A key parameter in LDA is α, which controls the shape of the prior distribution over topics for
individual documents. As is common, we used a ﬁxed α across topics. When α is less than 1, the
prior distribution is peaked, with most topics receiving low probability in any given document. α = 1
represents a uniform prior, and α > 1 penalizes distributions that assign a high probability to any
one topic in a speciﬁc document. Blei et al. describe a method based on variational inference
for learning the model parameters from a collection of documents, and our experiments use their
implementation ( 
The result of LDA is a learned value for α, and the set of topic distributions β. An inference
procedure can then be used to obtain the expected number of words accounted for by each topic in
any given text, and thus the topic distribution of the text.
In our experiments, we used topic distributions computed from a ﬁxed-length block of words
preceding the current word. Thus, it is necessary to update the context vector after each word, which
is an expensive process to do exactly. Therefore, as described in the next section, we have developed
an efﬁcient alternative method for computing context vectors based on the β matrix output by LDA.
Note that this is not a modiﬁcation of the LDA training or inference procedures; instead, it is an
efﬁcient technique for computing context as the RNN is being run.
Context Dependent Recurrent Neural Network Language Model
3.1 Fast Approximate Topic Representations
Empirically, it has been observed that the runtime for LDA inference is O(kN2) where N is the
number of words, and k is the number of topics. Computing an LDA representation for each word
given its sentence preﬁx would thus require O(kN3) time, which is undesirably slow. The same holds
for computation over a sliding window of words. Therefore, we developed a more efﬁcient method
for computing context. In this computation, we make context vectors directly during the training of
the RNN language model, using only the β matrix computed by the LDA. From the β matrix, we
extract a continuous-space representation for each word by using the normalized column vectors.
Since the that the topics are about equally represented in the training data training data, this results
in a vector of entries t j
wi representing P(tj|wi).
We found that it is possible to compute a reasonable topic distribution for a block of words w
by multiplying individual distributions over topics for each word from w, and renormalizing the
resulting distribution:
where tw(t) is the vector that represents the LDA topic distribution for word w(t). For this approximation to work, it is important to smooth the β matrix by adding a small constant to avoid extremely
small probabilities.
As we see in Section 4, the procedure can be further improved by weighting more recent words
higher than those in the more distant past. To do this, we introduce features with an exponential
decay, where we compute the feature vector as
Z f(t −1)γt(1−γ)
where γ controls the rate at which the topic vector can change - values close to 1 will enforce the
feature vector to change slowly, while lower values will allow quick adaptation to topics.
While this procedure is not an approximation of the LDA inference procedure, we have found that
it nevertheless does a good job representing contextual history, and admits an incremental update,
reducing a factor of N2 from the runtime. An empirical comparison to the use of LDA topic posteriors
is found in Section 4.
4 Penn Treebank Results
To maintain comparability with the previous research, e.g. , we chose to perform experiments
on the well-known Penn Treebank (PTB) portion of the Wall Street Journal corpus. This choice
also allows the fast evaluation of different models and parameter settings. We used the same standard
preprocessing of the data as is common in previous research: all words outside the 10K vocabulary
are mapped to the <unk> token; sections 0-20 are used as the training set (930K tokens), sections
21-22 as the validation set (74K tokens) and sections 23-24 as the test set (82K tokens). Extensive
comparison of performance of advanced language modeling techniques on this setup is given in .
Tomas Mikolov and Geoffrey ZweigMicrosoft Research Technical Report MSR-TR-2012-92July 27th, 2012
Table 1 Perplexities on the Penn Treebank Corpus for RNN models with LDA-based features, using 40 topics and a
sliding window of 50 previous words.
Dev PPL Test PPL
Kneser-Ney 5-gram, no count cutoffs
10 neurons, no features
10 neurons, exact LDA features
10 neurons, approximate LDA features
100 neurons, no features
100 neurons, exact LDA features
100 neurons, approximate LDA features
Table 2 Perplexities on the Penn Treebank Corpus with exponentially decaying features.
Dev PPL Test PPL
10 neurons, no features
10 neurons, γ = 0
10 neurons, γ = 0.9
10 neurons, γ = 0.95
10 neurons, γ = 0.98
To obtain additional features at every word position, we trained a LDA model using documents
consisting of 10 sentence long non-overlapping blocks of text from the PTB training data. We explored several conﬁgurations, and found that good results can be obtained with between 10 and 40
topics. Once the LDA model is trained, we can compute the probability distribution over topics for
any new document. After some experimentation, we used a sliding window of the previous 50 words
to represent the history. While this often goes beyond sentence boundaries, it makes sense because
the PTB reproduces news stories, in which adjacent sentences are related to each other. The resulting
probability distribution over topics is used directly as an additional feature input for the RNNLM.
Our initial experiments were performed using small RNN models with only 10 neurons. For
reduction of computational complexity, we used a factorization of the output layer using 100 classes,
as described in . After tuning hyper-parameters such as the optimal number of topics and the
size of the sliding window, we ran the same experiments with RNN models having 100 neurons.
The results are summarized in Table 1. As can be seen, the perplexity is reduced very signiﬁcantly
for small models, and the improvements hold up with larger models. Moreover, the approximate
topic features of Section 3.1 work almost as well as the exact LDA features. Thus, in the subsequent
experiments on larger data sets we focused on the approximate features. Table 2 shows that for values
around γ = 0.95, the approximate features with exponential decay outperform those computed with
an equally weighted window of the last 50 words (Table 1).
4.1 State-of-the-art Comparisons and Model Combination
In this section, we show that the improvements of a context-sensitive RNNLM hold up even in combination with Kneser-Ney 5-grams, cache LMs, and other models. Moreover, in combination with
Context Dependent Recurrent Neural Network Language Model
Table 3 Perplexities on the Penn Treebank Corpus for various neural net models, interpolated with the baseline 5gram and 5-gram+cache models. The RNN-LDA LM has 300 neurons, and uses 40-dimensional features computed
on a 50-words history (sliding window).
Individual +KN5 +KN5+cache
KN5 + cache
Feedforward NNLM
Log-bilinear NNLM
Syntactical NNLM
Recurrent NNLM
RNN-LDA LM
Table 4 Perplexities on the Penn Treebank Corpus for model combination using linear interpolation.
Kneser-Ney 5-gram, no count cutoffs
Model combination 
Combination of RNN-LDA models
RNN-LDA models + KN5 + cache
Combination of ALL
our best previous results we advance the state-of-the-art by 6% relative. Table 3 presents results for
a RNN-LDA model with 300 neurons, and a set of previous models, both in isolation and when interpolated with a Kneser-Ney 5-gram model and a cache model. The RNN-LDA model outperforms
the other models signiﬁcantly, even after they are combined with the cache model. The description
of the compared models is given in .
Next, we combine the new RNN-LDA model with the previous state-of-the-art model combination on this task. This is important to establish that the RNN LDA model provides truly new
information. The previous model combination achieved a perplexity 78.8 by combining many different RNN LMs and other well-known language models such as a random forest LM , a structured
language model , a class-based LM and other models . For these combination experiments,
we trained 8 RNN-LDA models with different conﬁgurations (up to 400 neurons and 40 dimensional
LDA). The results are presented in Table 4. It can be seen that the ﬁnal combination is signiﬁcantly
better than the best previous result. In addition, when we examine the interpolation weights, the vast
majority of weight is assigned to the RNN-LDA models. The simple RNN models are the second
most important group, and small contribution still comes from the 5-gram KN model with cache.
Other techniques provide insigniﬁcant improvements.
5 Wall Street Journal ASR Results
In this section, we use a RNN-LDA model for N-best rescoring in a speech recognition setting. We
used the medium-sized Wall Street Journal automatic speech recognition task, with around 37M
tokens in the LM training data. To handle the computational complexity associated with applying
Tomas Mikolov and Geoffrey ZweigMicrosoft Research Technical Report MSR-TR-2012-92July 27th, 2012
RNNs to larger data sets, a number of computational efﬁciencies have been developed , which are
adopted here. The most useful techniques for complexity reduction are:
• factorization of the output layer using simple frequency-based classes to avoid expensive normalization over the full vocabulary
• training the neural net jointly with a simple maximum entropy model with N-gram features, to
avoid huge hidden layers (denoted as RNNME model)
For the following experiments, we used lattices generated with the Kaldi speech recognition
toolkit . To allow comparability with previous work, the 100-best lists used in this work are
the same as those used in . The triphone GMM acoustic models were trained on the SI-84 data
further described in . We tested with the 20k open-vocabulary task. Note that while we do not
use advanced acoustic modeling techniques such as SGMMs and speaker adaptation, the baseline system achieves comparable results as is common in the language modeling research and
is sufﬁcient for comparison of advanced language modeling techniques.
The ASR system produced lattices using a pruned trigram model with Good-Turing smoothing,
from which we generated the 100-best lists that are used in the rescoring. The baseline N-gram
language models used for rescoring are a modiﬁed Kneser-Ney 5-gram (KN5) (denoted as
KN5) with singleton cutoffs, and a KN5 model without any N-gram cutoffs.
Next, we trained a RNNME model with 10 hidden neurons and 1000M parameters for a concurrent hash-based ME model using 4-gram features. We used 200 classes in the output layer. Next, we
trained RNNME models with the same conﬁguration and with additional features that represent the
topic information in the current sentence and with exponential decay γ = 0.9. We reset the feature
vector at the beginning of each new sentence, thus the features represent only the topic of the current sentence. This places fewer constraints on the training and test phases (the order of sentences
can be random). Results are presented in Table 5, where it can be seen that the topic information is
very useful and leads to 0.4% - 0.6% WER reduction. Moreover, from a purely language modeling
perspective, we obtain perplexity improvements in this domain as well.
We trained additional RNNME models with 200 hidden neurons and with 40 LDA features, to
see the potential of topic information in larger models. As can be seen in Table 5, we obtain similar
reductions in perplexity as with the smaller models; nevertheless, it is harder to obtain improvements
in word-error-rate over the large RNNME-200 model. We have explored several scenarios including combination with the KN5 model, always obtaining improvements of 0.1% - 0.2%. Still, the
perplexity of the RNNME-200+LDA-40 model is by more than 5% lower than of the RNNME-200
model; thus in this task the improvements are sometimes small but always consistent across metrics
and setups.
6 Conclusion
In this paper, we introduced the use of context dependent recurrent neural network language models.
The main idea is to condition the hidden and output vectors on a continuous space representation of
the previous words and sentences. Using a representation based on Latent Dirichlet Allocation, we
are able to avoid the data fragmentation associated with the traditional process of building multiple
topic-speciﬁc language models. We further develop a fast approximate context-updating technique
Context Dependent Recurrent Neural Network Language Model
Table 5 Perplexities, and word error rates for WSJ 100-best list rescoring with RNNME and RNNME-LDA models.
12.5% 16.6%
KN5, no count cutoffs
RNNME-10+LDA-5
RNNME-10+LDA-20
RNNME-200+LDA-40
KN5+RNNME-200
KN5+RNNME-200+LDA-40
which allows us to efﬁciently compute context vectors with a sliding window. The use of these models results in the lowest published perplexity on the Penn Treebank data, and in WER improvements
for the Wall Street Journal task.
In the future, we are interested in applying our approach to leverage external information that
is not present in the text. For example, in the machine translation setting, to use a source-language
representation to condition the target-side language model, or in a voice-search setting to use a
contextual vector representing a users interests.