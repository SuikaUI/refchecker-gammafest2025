HAL Id: hal-01644645
 
Submitted on 3 Jan 2020
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Proximal Decomposition on the Graph of a Maximal
Monotone Operator
Philippe Mahey, Said Oualibouch, Tao Pham Dinh
To cite this version:
Philippe Mahey, Said Oualibouch, Tao Pham Dinh. Proximal Decomposition on the Graph of a Maximal Monotone Operator. SIAM Journal on Optimization, 1995, 5 (2), pp.454-466. ￿10.1137/0805023￿.
￿hal-01644645￿
PROXIMAL DECOMPOSITION ON THE GRAPH OF A MAXIMAL
MONOTONE OPERATOR*
PHILIPPE MAHEYt, SAID OUALIBOUCHt, AND PHAM DINH TAO§
Abstract. We present an algorithm to solve: Find (x,y) E A X A.L such that y E Tx, where
A is a subspace and T is a maximal monotone operator. The algorithm is based on the proximal
decomposition on the graph of a monotone operator and we show how to recover Spingarn's decom­
position method. We give a proof of convergence that does not use the concept of partial inverse and
show how to choose a scaling factor to accelerate the convergence in the strongly monotone case.
Numerical results performed on quadratic problems confirm the robust behaviour of the algorithm.
Key words. proximal point algorithm, partial inverse, convex programming
AMS subject classification. 90C25
1. Introduction. We consider in this paper the following constrained inclusion
problem: let X be a finite dimensional vector space and A a subspace of X. Let
us denote by B the orthogonal subspace of A, i.e., B = A.L. Let T be a maximal
monotone operator on X and denote its graph by Gr(T), i.e., Gr(T) = { (x, y) E
X x Xly E Tx }. Then, the problem is to find x E A and y E B such that y E Tx,
which can be written:
(P) Find (x, y) EX x X such that (x, y) E A x B n Gr(T).
A typical situation, which is easily shown to give the form (P), is the problem
of minimizing a convex lower semicontinuous function on a subspace. The particular
applications we have in mind are the decomposition methods for separable convex
programming. They have recently gained some new interest with the possibility of
implementing them on massively parallel architectures to solve very large problems
such as the ones that appear in network optimization or stochastic programming (see
 ). There are many different ways to transform a separable convex program in the
form (P), but the general idea is to represent the coupling between the subsystems
by a subspace of the product space of the copies of the primal and dual variables.
We are aiming here at the application of the Proximal Point Algorithm (PPA)
(cf. ) to problem (P). In 1983, Spingarn proposed a generalization of PPA to
solve (P) that was based on the notion of the Partial Inverse operator. If we denote
by x A the orthogonal projection of x on a subspace A, the graph of the partial inverse
operator T A is given by
Gr(TA) = { (xA + YB. YA + XB) I y E Tx }.
Applying the PPA to this operator leads to the Partial Inverse Method (PIM) which
we summarize here.
ALGORITHM 1 (PIM). At iteration k, (xk,Yk) E A X B. Then, find (xÂ,yÃ)
such that Xk + Yk = xÂ + yÃ and Ä (YDA + (YDB E T((xÂ)A + Ä (xÂ)B).
* Received by the editors March 26, 1993; accepted for publication (in revised form) February 14,
t ISIMA, B.P. 125,63173 Aubiere, Cedex, France (maheyGlflamengo.isima.fr).
t Institut d'lnformatique et d'lntelligence Artificielle, Monruz 36, CH-2000 Neuchatel, Switzer­
§ LMAI, INSA Rouen, Place Emile Blonde!, B.P. 8, 76131 Mont-Saint-Aignan, France.
Then, (xk+l , Yk+l ) = ((x)A , (YDB ) .
The main problem that arises with this algorithm is the difficulty of performing
the proximal step (1) when c =11 in most interesting situations including the decom­
position methods. When c = 1, then the proximal step is a proximal decomposition
on the graph of T and the subspaces A and B only appear in the projection step.
In §3 we present the resultant algorithm, indeed equivalent to PIM with c = 1. The
convergence is proved without the need to consider the Partial Inverse operator. The
iteration is now written in the following way.
Proximal decomposition. Find the unique (x , YD such that x + y = X k + Yk
and (x, YD E Gr(T) If (x , YD E A x B, then stop.
Else (xk+t. Yk+1 ) = (( x)A , (YDB) ·
The unique solution of the proximal decomposition step is given by
X= (I + T )-1(xk + Yk) ,
Y =(I + T-1 )-1(xk + Yk) ·
Of course, only one proximal calculus is needed as (I + r-1 )-1 = I- (I + T)-1 . We
propose then a modified proximal decomposition algorithm by introducing scaling
factors .X and p,. Indeed, problem (P) may be written in two ways :
y E T x  x + .Xy E (I + .XT ) x,
x E T-1y  y + p,x E (I + p,T-1 )y ,
which induces the following fixed point iteration, a natural scaled version of (1).
Modified proximal decomposition.
X= (I + .XT )-1(xk + AYk) ,
y =(I + J.LT-1 )-1(Yk + p,xk) .
If (x , y) E A x B, then stop.
Else (xk+t.Yk+1 ) = ((x)A ,(YDB) ·
It appears that the modified proximal step is uniquely determined and corresponds
to a proximal decomposition on the graph of .XT if .Xp, = 1. We recover then the
scaled version of PIM proposed by Spingarn in . It is mentioned in that the
performance of PIM is very sensitive to the scaling factor variations and we give an
explanation of this fact, allowing its adjustment to an optimal value in the strongly
monotone case.
In §4, we give some numerical results that confirm the accelerating properties of
the scaling parameter.
2. The proximal decomposition on the graph of a maximal monotone
operator. We recall here some known results on the " Prox" mapping (I + T)-1
associated to a maximal monotone operator T and focus on the properties of the
decomposition on the graph of T. More details on that subject can be found in (2]
Let T be a maximal monotone operator on a Hilbert space X. The graph of T,
denoted by Gr(T), is defined by
Gr(T) = {(x,y ) EX x Xiy E T x} .
Monotonicity implies that for all x, x' E X and for all y E Tx, for all y' E Tx',
(y-y', x-x') 2: 0. As T is maximal, its graph is not properly contained in the graph
of any other monotone operator.
If T is strongly monotone, then there exists a positive p such that
Vx,x'EX and
VyETx, Vy'ETx', (y-y',x-x')2:Piix-x'ii2.
We say that the operator T is Lipschitz with constant L if
Vx, x' EX and
Vy E Tx, Vy' E Tx', iiY-y'il ¿ Lilx-x'll·
For monotone operators that share both properties, we get the following explicit
Piix-x'll ¿ IIY-y'll ¿ Lllx-x'ii·
When T is a linear operator represented by a positive definite matrix T, the best
estimates for p and L are, respectively, the smallest and the largest eigenvalues ofT.
Of course, if T is maximal monotone, then for any >. > 0, >.T is maximal monotone
and if, moreover, T is strongly monotone with modulus p and Lipschitz with constant
L, then >.T is strongly monotone with modulus >.p and Lipschitz with constant >.L.
The resolvent associated with maximal monotone operator T is defined by (I +
T)-1. It is single-valued, defined on the whole space, and firmly nonexpansive, which
means that, if we let U = (I+ T)-1 and V = I-U, then,
or equivalently
Vx,x' EX, !lUx-Ux'ii2 + IIVx-Vx'ii2 ¿ llx-x'll2
IIUx-Ux'ii2 L (x-x', Ux-Ux').
Related interesting facts on this characteristic property of resolvents may be found
in theses by Martinet and Eckstein (see also ). Indeed, resolvents and maximal
firmly nonexpansive mappings coincide and, following , one-to-one correspondences
among these operators, maximal monotone, and maximal nonexpansive operators,
may be stated. This fact is explored further in the appendix.
We introduce now the proximal decomposition on the graph of a maximal mono­
tone operator.
Given a maximal monotone operator T and a vector (x,y) EX x X, there exists
a unique pair (u, v) EX x X called the proximal decomposition of (x, y) on the graph
of T such that
u+ v= x+ y and
(u,v)EGr(T).
The unicity is a direct consequence of the maximality ofT and we get
u = (I+ T)-1 (x + y),
v = (I+ r-1)-1 (x + y).
3. The proximal decomposition algorithm. We return now to problem (P),
which has been analyzed by Spingarn . Let T be a maximal monotone operator
on X. Let A be a subspace and B its orthogonal subspace. The problem is to find
(x, y) EX x X such that (x, y) E A x B n Gr(T).
This problem is a particular case of the general problem of finding a zero of the sum
of two maximal monotone operators. The algorithms we are aiming at are splitting
methods that alternate computations on each operator separately (see ). Indeed,
most large-scale optimization problems can be formulated as the problem of mini­
mizing a separable convex lower semicontinuous function on a very simple subspace
which represents the coupling between the subsystems.
We propose then a generic algorithm that alternates a proximal decomposition
on the graph of T with a projection on A x B. Before going on with the analysis of
the method, we observe that the other alternatives that come to mind to find a point
in the intersection of two sets are not suitable.
1. We can use the classical successive projections method on the two sets. The
problem is that Gr(T) is generally not convex in X x X.
2. We cannot use another proximal decomposition on A x B (which is indeed
the graph of the maximal monotone operator 8XA , the subdifferential of the indicator
function of the set A), because it would lead back to the original point! Indeed,
if (x,y) E A x B and (u,v) is the proximal decomposition of x +yon Gr(T), then
x = ( u + v) A and y = ( u + v) B, which means that ( x, y) is the proximal decomposition
of u +von the graph of 8XA·
The Algorithm PDG (proximal decomposition on the graph) is stated below.
ALGORITHM 2 (PDG). Let (x0,yo) E A x B. k = 0.
If (xk, Yk) E Gr(T), then stop: (xk, Yk) is a solution of (P).
Else compute the proximal decomposition (uk,vk) of Xk + Yk on the graph of T. If
(uk,vk) E A x B, stop: (uk,vk) is a solution of (P).
Else, Xk+l = (uk)A and Yk+l = (vk)B·
An iteration of the algorithm may be formally stated as
(x,y) E A x B Á C (x,y) = x+ y = z EXÁ (u,v) = :Fz Á PAxs(u,v) E A x B,
where L is isometric from X x X into X, :F is the proximal decomposition operator
from X into X x X, and PAxB is the projection on A x B. Let us denote the composed
mapping by
.J = PAxB o:FoC.
We verify now that any fixed point (x, y) of Algorithm PDG is a solution of (P).
Indeed, (x, y) = PAxs(u, v) and (u, v) = :Fz with z = x + y. If (u, v) E A x B, then
(x, y) is a solution of (P). Else, we have
(u-x, v-y) E L = { (a, b) EX x Xia + b = 0}.
But, as (x,y) = PAxs(u,v), we can state
(u-x,v-y)EB x A.
A and B being orthogonal subspace, the unique intersection of L and B x A is (0, 0).
Thus, (x,y) = (u,v) and (x,y) solves (P).
On the other hand, if (x, y) is a solution of (P), :F (x + y) = (x, y), and (x, y) E
A x B, which means that (x,y) is a fixed point of Algorithm PDG.
The PDG Algorithm is a particular instance of Spingarn's Partial Inverse Method
(12]. Indeed, when c
= 1, the proximal step on the Partial Inverse operator TA
becomes: Find (xÂ, YD such that : Xk + Yk = xÂ + yÃ and (YDA + (YOB E T( (xÂ)A +
(xDB), which means, of course, that (xÂ, YD is the proximal decomposition of (xk, Yk)
on the graph ofT. Thus, the convergence has been established by Spingarn who has
used the properties of the PPA applied to the partial inverse operator. However, here
we give a direct proof of this fact that does not use the concept of the Partial Inverse.
The main interest is that we shall obtain as a corollary the numerical analysis of the
scaled version of PDG in the strongly monotone case.
We prove first that the composed mapping :T associated with Algorithm PDG is
firmly nonexpansive. It can easily be seen that the mapping U = C o:J o£-1 is indeed
the proximal operator associated to the Partial Inverse of T, i.e., U = (I + TA)-1.
But, we do not use this fact to prove that :Tis firmly nonexpansive.
THEOREM 3.1. The mapping :T associated to Algorithm PDG is firmly nonex­
pansive if and only if T is monotone. Moreover, it is defined on the whole space A x B
if and only if T is maximal monotone.
Proof. Let (x,y) and (x',y') E A x B, z, z' E C (x,y),C (x',y') respectively, i.e.,
z = x + y and z' = x' + y', (u,v) E :F(z) and (u',v') E :F(z'), i.e., u + v = z,
u = (I+ T)-1z and u' + v' = z', u' = (I + T)-1z'. Finally, let (uA,VB) and
( uA., vk) E A x B be the respective projections of ( u, v) and ( u', v') on A x B.
It is clear that, as z E (I+T)u, dom(:F) = R(I+T), and dom(.J) = c-1(dom(:F)) =
{(zA, ZB) E A X Biz E dom(:F)}.
Now, :Tis firmly nonexpansive if and only if
't/ (x,y), (x',y') E dom(:T) and 't/ (uA,VB) E :T (x,y), (uA,,vk) E :T (x',y')
((x, y)- (x', y'), (uA, VB)- (uA,, vk)} 2: ll (uA, VB)- (uA,, vk)ll2•
But, we have
( (x,y)- (x',y'),.(uA,VB)- (uA,,vk)} = (x-x', (u-u')A} + (y-y', (v-v')B}
and, as x,x' E A and y,y' E B
(x-x', (u - · u')A} = (z- z', (u-u')A}
= ( (u + v-u'-v', (u-u')A}
= ( (u-u') + (v-v'), (u-u')A}
(y-y', (v-v')B} = (z- z', (v-v')B}
= ((u-u') + (v-v'), (v-v')B}·
Hence, inequality (6) becomes
((u-u') + (v-v'), (u-u')A}
+ ((u-u') + (v-v'), (v-v')B} 2: ll (u-u')AII2 + ll (v-v')BII2•
We can now use the orthogonal decomposition of u -u' and v -v' on the direct sum
A$B to get
'v'(u,v), (u',v') E Gr(T),
((u-u')A, (v-v')A) + ((u-u')B, (v-v')B) J 0.
Finally, remarking that
(u-u',v-v') = ( (u-u')A, (v-v')A) + ( (u-u')B, (v-v')B),
we can conclude that .7 is firmly nonexpansive if and only if T is monotone.
Moreover, as dom(.7) = {(x, y) E A x Blx + y E dom(F)}, we obtain
.7firmly nonexpansive } { Tmonotone
dom(.7) =A x B
maxtma mono one.
Assuming that (P) has a solution, the convergence of the algorithm follows directly
from Opial's lemma (see ), which states that, if a fixed point exists, a firmly
nonexpansive operator is asymptotically regular and generates a convergent sequence.
This is the very same idea as used by Martinet in the original proof for the PPA 
and developed further by Rockafellar who included approximate computations of the
proximal steps .
4. A scaled decomposition on the graph of T. We introduce now a scaled
version of the decomposition on the graph of a maximal monotone operator.
DEFINITION 4.1. Let (x,y) E X x X, T be a maximal monotone operator and>.
a positive number. Then, the scaled proximal decomposition of (x, y) on the graph of
T is the unique ( u, v) such that
u + >.v = x + >.y,
(u,v) E Gr(T).
Again, the existence and unicity of that new decomposition is a consequence of
T being maximal monotone. Indeed, if v E Tu, we can write
u + >.v E u + >.Tu
=> u = (I+ >.T)-1 (u + >.v)
= (I+ >.T)-1 (x + >.y)
v = .x-1 (x + >.y-u).
Observe that we can also write the following inclusions using the inverse operator r-1
for a given positive J.L:
v+ J.Lu E v+ J.LT-1v,
then v =(I+ J.LT-1)-1 (v + J.LU).
Now, if J.L satisfies J.L-l = >., we get v + J.LU = J.L(u + >.v) and, using the fact that
(J.LT)-1z = r-1 (J.L-1z), we obtain
v = >. -lu + J.LT-1 )-1 (u + >.v)
=(I+ J.LT-1)-1 (J.Lx + y).
Resuming, the scaled decomposition on the graph of T can be defined by
u = (I + >.T)-1(x + >.y ) ,
(I + J.t T-1)-1 (J.t X + y ) ,
which appears as a natural generalization of ( 1) . But, in fact, only one scaling factor
can be introduced to maintain the desired properties, this is why we must fix AJ.t = 1.
We can now describe the iteration of a scaled version of Algorithm PDG.
ALGORITHM 3 ( SPDG) . (xk,Yk) E A x B.
Compute the scaled decomposition of (xk, Yk) on the graph of T.
= (I + >.T)-1(xk + >.y k) ,
= >. -l(Xk + AYk- Uk) ·
If (uk, vk) E A x B, then stop. Else, Xk+l = (uk)A and Yk+l = (vk)B·
Observe that the scaled proximal decomposition can be stated in the following
Let w = >.v and r = >.y . Then, if ( u, v) is the scaled proximal decomposition
of (x, y ) on the graph of T, (u, w) is the proximal decomposition of (x, r) on the
graph of >.T. Hence, from the preceding section, we know that the sequence {(xk, rk) }
converges to a point in A x BnGr(>.T) . This fact implies that the sequence {(xk, Yk) }
converges to a solution of ( P) .
On the other hand, we can see that SPDG is equivalent to the scaled version of
the Partial Inverse Method (with c = 1) described by Spingarn in [13, Algorithm 2, p.
208) for the minimization of a convex function on a subspace. It reduces, of course,
to PDG, i.e., to PIM, when>.= 1. Again, as the decomposition on the graph ofT is
a proximal step, approximate rules for computations can be added as in (11) to get an
implementable algorithm. We prefer to omit these details to focus on the accelerating
properties of the scaling parameter, which constitute the main contribution of the
present work.
To analyze the influence of the scaling parameter on the speed ratio of convergence
of SPDG, we consider now the case where T is both strongly monotone and Lipschitz.
THEOREM 4.2. When Tis strongly monotone with modulus p and Lipschitz with
constant L, then the convergence of the sequence {(xk, rk) } genemted by SPDG with
rk = >.y k is linear with speed mtio
1 - ( 1 + >. L )2 .
Proof. If .J>. is the composed operator associated to the monotone operator >.T, we
define as in Theorem 3.1 (x, r) , (x ', r ') E A x B, z = x+r, z' = x ' +r', (u, w), (u', w') E
Gr(>.T) . Then, (uA,WB) E .J>.(x,r) and (uÀ,wB) E .J>.(x ',r ') .
The strong monotonicity of >.T implies that
Vw E Tu, w' E Tu', (w- w', u- u') ;:: >.piiu-u'll2
and, as z E (I + >.T)u and z '
E {I + >.T)u'
liz- z'll $ ( 1 + L)iiu-u'll·
From the composed nature of .:1>. and using the relations ( 8) and (9), we deduce the
following bounds:
ll (uA,WB)- (uA,w¾)ll2 ¿ llu-u'll2 + llw-w'll2
¿liz-z'll2-2(u-u',w-w'}
¿ liz-z'll2 -2pllu-u'll2
¿ ( 1- (1 
).:£)2) liz-z'll2
¿ (1- (1£)2) li (x,r)- (x',r')ll2·
Let (x*, r*) be the limit point of the sequence { (xk, rk) }. It is therefore a fixed point of
the mapping :1> ... Applying the above inequality to the pairs (xk+l. rk+l) and (x*, r*),
we obtain the desired result:
ll (xk+l>rk+l)- (x*,r*)ll2 ¿ (1- (1 ).:£)2) ll (xk,rk)- (x*,r*)ll2.
Observe that, as
L J p,r(>.) =
1- (1 + ).£)2 < 1.
We easily deduce the theoretical optimal value for >.:
"X= 1/L and
r("X) = J1- ;L.
When T is a linear positive definite operator, we observe that bad conditioning implies
a slowdown of the algorithm. The optimal value of the scaling parameter must be
chosen very small if L = J.Lmax, the largest eigenvalue of the associated matrix, is
very large. We may observe that the speed ratio obtained in Theorem 4.2 is the
same as the one given in for the Douglas-Rachford splitting algorithm. Indeed,
the connection between that algorithm and the Partial Inverse Method has been
established by Eckstein and we give its precise meaning in the Appendix.
The influence of the Lipschitz constant on the number of iterations has been
analyzed for quadratic convex functions that were minimized on a simple subspace.
The sensitivity to that parameter is shown on the five graphics of Fig. 1 and 2 for
different values of L, p, and the dimension of the space. These results are shown
in Table 1. The influence of the scaling parameter on the number of iterations is
illustrated by comparing columns iter(":\) ( number of iterations when ). = "X) and
iter(1) (number of iterations when ). = 1). The number of iterations corresponds to
the implementation of Algorithm PDG associated with the graph of >.T. We show
below why it is faster than the straightforward application of SPDG even if the primal
sequences {xk} coincide in both algorithms.
It is also interesting to analyze the behaviour of the sequence { (xk,Yk)} and to
look for some values of the scaling parameter such that, that sequence is mapped by
a contraction. To be more precise, let .1>. and 1-l>. be the maps associated with the
sequences { (xk, rk)} and { (xk, Yk)}, respectively. Then, if D>. is the mapping defined
D>. (x, y) = (x, >.y),
dimension = 1 0
'- --c +--1-----T--------
dimension = 1 o
0------------
FIG. 1. Number of iterations for dim=lO.
we can write the following correspondence:
rt>. = n;.1 o :1>. o D>..
As (xk, Yk) = D;.1 (xk, rk), we already know that the sequence { (xk, Yk)} converges
when { (xk, rk)} converges. Note that a direct proof of this fact seems rather hard to
state. The reason is that 1i>. is not necessarily a contractive map for any >.. We study
below the conditions on >. to get a contraction in the strongly monotone case. In the
strongly monotone and Lipschitz cases, we already know that 1i>. is a contraction for
>. = 1. The next theorem shows that this remains true if >. lies in a specific interval
containing one.
dimension -100
L·10,96807
p= 10,02107
····-····-····-····-··-
0 ¬---------------»
dimension = 1 00
p=O,l10S3S
L = 0,584036
......... -............ _________ ··--··-··-·
·-··-··-··-··-··--··-··-··-··-··-··--··-··-··-·
oT-------------------------
FIG. 2. Number of itemtions for dim=lOO.
THEOREM 4.3. Suppose that T is strongly monotone with modulus p and Lipschitz
with constant L. Then, if A E ( 1, p + y' 1 + p2), the mapping 'H>. is a contraction.
Proof Again let u = (I+ AT)-1 (x + AY) and u' = (I+ AT)-1 (x' + Ay'). We use
successively the nonexpansiveness of the projection and the firmly nonexpansiveness
of the resolvent to write
II'H>.(x,y) -'H>.(x',y')llk $\\ (u-u1,A-1 (x-x' + A (y-y')-u + u'))llk
$ A-211X-x'll2 + IIY-y'll2 + A2-p - 111u-u'll2•
Using the Lipschitz property, we obtain
Hence, a sufficient condition that ensures that 'H.>. is a contraction is A K 1 and
O (A) = A2-2Ap- 1 < 0. That condition does not depend on the Lipschitz constant
(indeed, this happens because 0 < p < L). We observe now that 0 (1) = -2p < 0 and
the desired interval must be : A E ( 1, p + y' 1 + p2).
The different behaviour of both sequences { (xk, Zk)} and { (xk, Yk)} is illustrated
in Fig. 3. For a small A, the second sequence (which is the one that will yield a solution
Numerical tests for quadmtic problems.
FIG. 3. Comparison of both sequences
for the original problem ( P) ) converges much slower even if it presents a monotonic
decrease toward the fixed point.
We conclude with the following observations on the choice of the scaling pa­
rameter: if bad conditioning is due to a too-small p, then we must accelerate the
convergence by choosing .X close to the optimal value 1/ L (if it is not too far from
1!). If bad conditioning is due to a too-large L, then we may choose >. close to 1 in
[l,p+ Jl + p2 ).
Appendix. The relation between the partial inverse and the Douglas-Rachford
splitting operator may be explained in the following way which is directly inspired by
the work of Lawrence and Spingarn . It was later derived by Eckstein and Bertsekas
We recall the one-to-one correspondences among maximal monotone operators,
maximal nonexpansive, and proximal operators as described in .
Let a : (x, y) Å--> (x, 2y-x) be the one-to-one correspondence of the class of
proximal operators onto the class of nonexpansive operators and let f3 : (x, y) B-->
(x + y,x-y) be the one-to-one correspondence of the class of monotone operators
onto the class of nonexpansive operators. Following , let us define two types of
composition operations.
Let p1 *P2 = a-1(a(p1) oa(p2 ) ) be the proximal operator obtained by composing
two proximal operators p1 and p2 through their associated respective nonexpansive
images (which give indeed another nonexpansive operator when composed). Likewise,
let T1 8 T2 = /3-1(/3(T1) o /3(T2 ) ) be the monotone operator obtained by composing
two monotone operators in the same way. A straightforward calculus shows that, if p1
and P2 are the resolvents of T1 and T2, respectively, then p = Pl * P2 is the resolvent
of T = T1 8 T2. As observed in , we have the following interpretation of the *
operation :
P1 * P2 = P1 ° (2p2 - I) + I- P2,
which is the operator associated to the fixed point iteration of the Douglas-Rachford
splitting method (see ). Observe that the nonexpansive operator a(p1) o a(p2 ) ) is
the operator associated with the Peaceman-Rachford iteration.
On the other side, it is shown in that, when T1 is the subdifferential mapping
of the indicator function of a subspace A, i.e., Gr(T1) = A x Al., then T1 8 T = TA,
the Partial Inverse ofT. Resuming these facts, we have the following proposition.
PROPOSITION. Let T1 and T2 be two maximal monotone operators on X. The
Douglas-Rachford splitting operator p = p1 o ( 2p2 - I) + I- p2 , where p1 = (I + >.T1 ) -1
and p2 = (I+ >.T2 )-1, is a proximal operator, indeed p = (I+ T)-1, where T =
>.T1 8 >.T2. Moreover, if Gr(T1 ) =A x Al. and T2 = T, then p = (I + (>.T )A)-1, the
resolvent of the partial inverse of >.T. Then, the Douglas-Rachford iteration applied to
problem (P) is the partial inverse method associated to >.T. SPDG is the corresponding
algorithm defined in the product space X x X.
Observation. Clearly (I+ (>.T )A)-1 f. (I+ >.TA)-1• This point is crucial because
the computation can only be performed in the first expression (this is then the SPDG
Algorithm) or in the second expression with >. = 1.