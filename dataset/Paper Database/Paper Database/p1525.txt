Machine Learning, 8, 341-362 
© 1992 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.
The Convergence of TD(X) for General X
PETER DAYAN
 
Centre for Cognitive Science & Department of Physics, University of Edinburgh, EH8 9LW, Scotland
Abstract. The method of temporal differences (TD) is one way of making consistent predictions about the futgre.
This paper uses some analysis of Watkins to extend a convergence theorem due to Sutton from
the case which only uses information from adjacent time steps to that involving information from arbitrary ones.
It also considers how this version of TD behaves in the face of linearly dependent representations for states--
demonstrating that it still converges, but to a different answer from the least mean squares algorithm. Finally
it adapts Watldns' theorem that Q-learning, his closely related prediction and action learning method, converges
with probability one, to demonstrate this strong form of convergence for a slightly modified version of TD.
Keywords. Reinforcement learning, temporal differences, asynchronous dynamic programming
I. Introduction
Many systems operate in temporally extended circumstances, for which whole sequences
of states rather than just individual ones are important. Such systems may frequently have
to predict some future outcome, based on a potentially stochastic relationship between it
and their current states. Furthermore, it is often important for them to be able to learn
these predictions based on experience.
Consider a simple version of this problem in which the task is to predict the expected
ultimate terminal values starting from each state of an absorbing Markov process, and for
which some further random processes generate these terminating values at the absorbing
states. One way to make these predictions is to learn the transition matrix of the chain
and the expected values from each absorbing state, and then solve a simultaneous equation
in one fell swoop. A simpler alternative is to learn the predictions directly, without first
learning the transitions.
The methods of temporal differences (TD), first defined as such by Sutton ,
fall into this simpler category. Given some parametric way of predicting the expected values
of states, they alter the parameters to reduce the inconsistency between the estimate from
one state and the estimates from the next state or states. This learning can happen incre-
mentally, as the system observes its states and terminal values. Sutton proved some
results about the convergence of a particular case of a TD method.
Many control problems can be formalized in terms of controlled, absorbing, Markov
processes, for which each policy, i.e., mapping from states to actions, defines an absorbing
Markov chain. The engineering method of dynamic programming (DP) uses the predictions of the expected terminal values as a way of judging and hence
improving policies, and TD methods can also be extended to accomplish this. As discussed
extensively by Watkins and Barto, Sutton and Watkins , TD is actually very
closely related to DP in ways that significantly illuminate its workings.
This paper uses Watkins' insights to extend Sutton's theorem from a special case of TD,
which considers inconsistencies only between adjacent states, to the general case in which
arbitrary states are important, weighted exponentially less according to their temporal dis-
tances. It also considers what TD converges to if the representation adopted for states is
linearly dependent, and proves that one version of TD prediction converges with probability
one, by casting it in the form of Q-learning.
Some of the earliest work in temporal difference methods was due to Samuel .
His checkers (draughts) playing program tried to learn a consistent function for evaluating
board positions, using the discrepancies between the predicted values at each state based
on limited depth games-tree searches, and the subsequently predicted values after those
numbers of moves had elapsed. Many other proposals along similar lines have been made:
Sutton acknowledged the influence of Klopf and in Sutton discussed
Holland's bucket brigade method for classifier systems , and a procedure
by Witten . Hampson presented empirical results for a quite similar
navigation task to the one described by Barto, Sutton and Watkins . Barto, Sutton
and Anderson described an early TD system which learns how to balance an upended
pole, a problem introduced in a further related paper by Michie and Chambers .
Watkins also gave further references.
The next section defines TD(X), shows how to use Watkins' analysis of its relationship with
DP to extend Sutton's theorem, and makes some comments about unhelpful state representa-
tions. Section 3 looks at Q-learning, and uses a version of Watkins' convergence theorem
to demonstrate in a particular case the strongest guarantee known for the behavior of TD(0).
Sutton developed the rationale behind TD methods for prediction, and proved that
TD(0), a special case with a time horizon of only one step, converges in the mean for
observations of an absorbing Markov chain. Although his theorem applies generally, he
illustrated the case in point with an example of the simple random walk shown in figure 1.
Here, the chain always starts in state D, and moves left or right with equal probabilities
from each state until it reaches the left absorbing barrier A or the right absorbing barrier
G. The problem facing TD is estimating the probability it absorbs at the right hand barrier
rather than the left hand one, given any of the states as a current location.
Figure 1. Sutton's Markov example. Transition probabilities given above (right to left) and below (left to right)
the arrows.
TD(X) FOR GENERAL X
The raw information available to the system is a collection of sequences of states and
terminal locations generated from the random walk--it initially has no knowledge of the
transition probabilities. Sutton described the supervised least mean squares (LMS) technique, which works by making the estimates of the probabilities for
each place visited on a sequence closer to 1 if that sequence ended up at the right hand
barrier, and closer to 0 if it ended up at the left hand one. He showed that this technique
is exactly TD(1), one special case of TD, and constrasted it with TD(R) and particularly
TD(0), which tries to make the estimate of probability from one state closer to the estimate
from the next, without waiting to see where the sequence might terminate. The discounting
parameter h in TD(R) determines exponentially the weights of future states based on their
temporal distance--smoothly interpolating between k = 0, for which only the next state
is relevant, and k = 1, the LMS case, for which all states are equally weighted. As described
in the introduction, it is its obeisance to the temporal order of the sequence that marks
The following subsections describe Sutton's results for TD(0) and separate out the
algorithm from the vector representation of states. They then show how Watkins' analysis
provides the wherewithal to extend it to TD(h) and finally re-incorporate the original
representation.
2.1. The convergence theorem
Following Sutton , consider the case of an absorbing Markov chain, defined by sets
and values:
qij ~ [0, 11
i~9z, jfi 9z tJ 5
terminal states
non-terminal states
transition probabilities
vectors representing non-terminal states
expected terminal values from state j
probabilities of starting at state i, where
The payoff structure of the chain shown in figure 1 is degenerate, in the sense that the
values of the terminal states A and G are deterministically 0 and 1 respectively. This makes
the expected value from any state just the probability of absorbing at G.
The estimation system is fed complete sequences xil, xi=, • • • Xim of observation vectors,
together with their scalar terminal value z. It has to generate for every non-terminal state
i ~ 0L a prediction of the expected value ~E[z I i1 for starting from that state. If the transition
matrix of the Markov chain were completely known, these predictions could be computed as:
Again, following Sutton, let [M]a~, denote the ab th entry of any matrix M, [U]a denote the
a ~ component of any vector u. Q denote the square matrix with components [Q]a~ = qa~,
a, b E 9Z, and h denote the vector whose components are [hi a = ~,~qab~b, for a E ff~.
Then from equation (1):
As Sutton showed, the existence of the limit in this equation follows from the fact that
Q is the transition matrix for the nonterminal states of an absorbing Markov chain, which,
with probability one will ultimately terminate.
During the learning phase, linear TD(X) generates successive vectors Wl x, w2 x, ... ,1
changing w x after each complete observation sequence. Define VX~(i) = w n x i as the pre-
diction of the terminal value starting from state i, at stage n in learning. Then, during one
such sequence, V~(it) are the intermediate predictions of these terminal values, and, abus-
ing notation somewhat, define also V~n(im+l) : z, the observed terminal value. Note that
in Sutton , Sutton used Pt ~ for VXn(it). TD(X) changes w x according to:
x+~(c~[V~(it+l)_VXn(iz)]2Xt-k V
Wn+ 1 = Wn
where c~ is the learning rate.
Sutton showed that TD(1) is just the normal LMS estimator ,
and also proved that the following theorem:
Theorem T For any absorbing Markov chain, for any distribution of starting probabilities
~i such that there are no inaccessible states, for any outcome distributions with finite ex-
pected values ~., and for any linearly independent set of observation vectors {xi[i E ffL},
there exists an e > 0 such that, for all positive c~ < e and for any initial weight vector,
the predictions of linear TD(k) (with weight updates after each sequence) converge in ex-
pected value to the ideal predictions (2); that is, "
If W n denotes the weight vector after n
sequences have been experienced, then
lim lE[Wn x" xil = IE[zIi]
is true in the case that 3, = 0. This paper proves theorem T for general 3`.
2.2. Localist representation
Equation (3) conflates two issues; the underlying TD(3`) algorithm and the representation
of the prediction functions Vn x. Even though these will remain tangled in the ultimate proof
of convergence, it is beneficial to separate them out, since it makes the operation of the
algorithm clearer.
TD(X) FOR GENERAL ~,
Consider representing V~ x as a look-up table, with one entry for each state. This is equiv-
alent to choosing a set of vectors xi for which just one component is 1 and all the others
are 0 for each state, and no two states have the same representation. These trivially satisfy
the conditions of Sutton's theorem, and also make the w, easy to interpret, as each com-
ponent is the prediction for just one state. Using them also prevents generalization. For
this representation, the terms Vw, VX~(i~) in the sum
just reduce to counting the number of times the chain has visited each state, exponentially
weighted in recency by X. In this case, as in the full linear case, these terms do not depend
on n, only on the states the chain visits. Define a characteristic function for state j:
and the prediction function VX~(i) as the entry in the look-up table for state i at stage n
during learning. Then equation (3) can be reduced to its elemental pieces
VnX+l (i) = VXn(i) +Z
°t[VXn(i'+l) - Van(it)]
in which the value for each state is updated separately.
To illustrate this process, take the punctate representation of the states B, C, D, E and
F in figure 1 to be: 2
If the observed sequence is D, C, D, E, F, E, F, G, then the sums
after each step are:
)k 2 + X 4
~k 3 + X 5
)k 4 + X 6
and component i in this sum is clearly
~] V-~x;(k)
and time t.
2.3. Contraction mappings
Watkins showed that a fruitful way of looking at TD estimators is through dynamic
programming and its associated contraction mappings. The method starts from the current
prediction function Vn(i), ¥i ~ 9Z, shows how to define a whole collection of statistically
better estimators Vn+l(i), ¥i fi ~ based on an observed sequence, and then uses for TD(X)
a linear combination of these estimators. Except where explicitly noted, this section follows
Watkins exactly--the equations developed have their exact analogues for the linear represen-
tation, as will be seen in section 2.5.
Imagine the chain starts at some state i0, and runs forward through states il, i2, ...,
ultimately absorbing. Define the r-step estimate of i0 as either the estimate Vn(ir) of state
ir, if the chain is not absorbed after r steps and so ir ~ 9Z, or the observed terminal value
z of the sequence, if the chain has absorbed before this time.
Formally, define the random variables
Vln,io = (~n(il)
if i r E ~
TD(X) FOR GENERAL X
where i~ is the first state accessed by the Markov chain in one particular sequence starting
from i0; i2 is the second, and so on, and z is the observed terminal value if the chain gets
absorbed before time step r is reached. These are random variables, since they depend
on the particular sequence of states which will be observed. Naturally, they also depend
on the values V~(i).
The point of these is that if the chain absorbs after completing s steps from i0, then each
of the V£,io, for r _> s will be based on the terminal value provided by the world, rather
than one derived from the 'bootstraps' Vn(i ). V~,io should therefore on average be more
accurate than V. and so can be used incrementally to improve it. This can be shown by
looking at the difference between IE[V£,io] and lE[zli0], the ideal predictions.
IE[12nr, io] =
qiot,~tl q- Z
qi~t2~h q- "'" +
qir-ltr~tr -~- Z
Qioi r n(lr)
whereas it can easily be shown that
lE[z[i0] =
qiot,~, + ~
Qio~, ~_a qilt2~t2 +
Or.:ltOlr_l Z
qir-ltrZ-tr + Z
ai~irlE[Zlir]
ir_ l E~,Y(.
Therefore,
IF.[ n,io] - IE[zli0] =Z
Qioir(n(lr)
IE[z I ir])
Watldns actually treated a slightly different case, in which the target values of the predictors
are based on discounted future values whose contribution diminishes exponentially with
the time until they happen. In this case it is easier to see how the reduction in error is
brought about. His analogue of equation (9) was:
IF.[ n.io] - IE[zIi0] =
Qiroir(Vn(ir) - lE[zlir])
where 3' < 1 is the discount factor. Since
Z Qiroir <- 1,
as Q is the matrix of a Markov chain, Watkins could guarantee that
max I IE[Vnr, io] - IE[z]i0] ] -< 3'r max I Vn(ir)
-- lE[zlir] I
which provides a (weak) guarantee that the error of V~ r will be less than that of V n.
The nondiscounted case, when 3' = 1, is somewhat different. Here, for some initial states
i0, there is a nonzero probability that the chain will absorb before finishing r steps. In
this case, the value of vr~,io, being z, will be unbiased, and so should provide for error
reduction. Even if the chain does not absorb, its value can be no farther from what it should
be than is the most inaccurate component of V n. Although there is no error reduction due
to % it is guaranteed that
Qiroir <- 1
with inequality for all those states from which it is possible to absorb within r steps.
This does not ensure that
max I lE[V~,io I - lE[zli0] [ < max I Vn(ir) -- IE[Z [ ir] I
since the maximum could be achieved, pathologically, at a state from which it is impossi-
ble to absorb in only r steps. However, the estimates for the states that are within r steps
of absorption will, on average, improve, and this should, again on average, filter back to
the other states.
Watkins demonstrated that TD(X) is based on a weighted average of the Vff, io. Consider
V x. = (1 - X) ~
which is also a valid estimator of the terminal value starting at i0. He points out that in
choosing the value of X, there is a tradeoff between the bias caused by the error in Vn,
and the variance of the real terminal value z. The higher X, the more significant are the
V,~ for higher values of r, and the more effect the unbiased terminal values will have. This
leads to higher variance and lower bias. Conversely, the lower ),, the less significant are
the contributions from higher values of r, and the less the effect of the unbiased terminal
values. This leads to smaller variance and greater bias.
It remains to be shown that TD()0 is indeed based on this combination estimator. Expand-
ing out the sum in equation (10).
Vn(io) = [Vn(il) -
Vn(io)] + X[Vn(i2) -
)t2[Vn(i3)
defining Vn(is) = z for s > max{tlit e 9Z}.
TD(k) FOR GENERAL k
The whole point of defining vY. is so that it can be used to make V more accurate.
The obvious incremental update rule to achieve this has
Vn+l(io) = V.(io) + o~[VnX, i0 - Vn(io)].
From equation (11) it is apparent that the changes to Vn(io) involve summing future values
of Vn(it+l) -
Vn(it) weighted by powers of X. Again following Watkins, these differences
can be calculated through an activity trace based on the characteristic functions xi(t) that
were defined earlier as a way of counting how often and how recently the chain has entered
particular states. Using index t for the members of the observed sequence, the on-line ver-
sion of the TDO~) rule has
Vt+l(i) = Vt(i) + c~[Vt(it+~) - Vt(it)] ~
kt-~xi(k).
For the problem that Sutton treated, the change to Vn is applied off-line, after a complete
sequence through the chain. Therefore, if the states through which the chain passes on one
sequence are io, i~, ..., ira-1 ~ 9~, and im ~ 3, it absorbs with terminal value Vn(im) =- z,
and Vn+~ is the new estimator after experiencing the sequence, then
Vn+l(io) = Vn(iO)
c~[Vn(i,+l) - I/.(i,)] ~
Xt-kXio(k )
l/'n+l(il) = Vn(il)
ot[Vn(it+l) -
xt-kxi,(k)
Vn+l(im_l) = Vn(im_l) +
Vn(im_l) ] ~ X t-k
Xirn_l ~ ),
summing over terms where i a =
i b (SO Xi a ~
Xib). Note that these expressions are exactly
the same as the TD(X) weight change formula in equation (4).
Thus, the actual TD(X) algorithm is based on the exponentially weighted sum defined
in equation (10) of the outcomes of the V/r random variables. The mean contraction proper-
ties of these variables will therefore determine the mean contraction properties of the overall
TD(X) estimator.
2.4. Linear representation
The previous subsection considered the TD(X) algorithm isolated from the representation
Sutton used. Although a number of different representations might be employed, the simplest
is the linear one he adopted. Identifying the vectors x with the states they represent gives
where wn is the weight vector at stage n of learning.
The basic algorithm is concerned with the V~ x predictor random variables rather than
how their values can be used to change the initial predictor V n. Under the new represen-
tation, equation (12) no longer makes sense since the states cannot be separated in the appro-
priate manner. Rather, the information about the error has to be used to update all the
weights on which it depends. The appropriate formula, derived from the delta-rule is
Wn+l = Wn + ot[VXn,io - Vn(io)] Vw. Vn(io)
weighting the error due to state i0 by the vector representation of i0. Then the equivalent
of equation 03) is just Sutton's main TD(X) equation (3).
More sophisticated representations such as kd-trees for a review)
or CMACs may lead to faster learning and better generalization, but each
requires a separate convergence proof. Dayan compares the qualities of certain different
representations for Barto, Sutton and Watkins' grid task .
2.5. The proof of theorem T
The strategy for proving theorem T is to follow Sutton in considering the expected
value of the new prediction weight vector given the observation of a complete sequence,
and to follow Watkins in splitting this change into the components due to the equivalents
of the V r random variables, and then summing them. Mean error reduction over iterations
will be assured by the equivalent of equation (9) for the linear representation.
Define the V~,. random variables as in equation (5) as
if Xir ~ ~
where x i are identified with the states in the observed sequence, w~ is the current weight
vector defining the estimated terminal values, and z is the actual value. Then, after observ-
r is updated as:
ing the whole sequence, w n
xifi 9"£ visited
V~(i)] Vw. V,,(i)
IVy, i -- W n • xi] x i.
An exact parallel of Sutton's proof procedure turns out to apply to w r. Define ~ij as the
number of times the s-step transition
X i "-'> Xkl "-'> Xk2
occurs, for any intermediate states xk, E OZ.
TD(X) FOR GENERAL X
The sum in equation (14) can be regrouped in terms of source and destination states of
the transitions:
Wnr+l = Wn r q- Od~
~rtWnr" X jr -- Wn r" Xi] X i
i~9Z jr~OZ
~--d ~rijr[ZJr- Wn r" Xi] X i
~rijr-l_l[ZJr_l -- w~r ° Xi] X i
i~)Z Jr_l~5
where zj indicates that the terminal value is generated from the distribution due to state
j, and the extra terms are generated by the possibility that, from visiting any x i ~ 9Z, the
chain absorbs before taking r further steps.
Taking expected values over sequences, for i ~ 9Z
lE[r/~] = ~
lE[r/ij ] = ~
i~dik ql:j
lE[r/~j] = diqij
for j fi 3
where d i is the expected number of times the Markov chain is in state i in one sequence.
For an absorbing Markov chain, it is known that the dependency of this on the probabilities
txi of starting in the various states is:
di =~a lzj(I- Q)j~l = [#r(i_ Q)-I]i
Substituting into equation (15), after taking expectations on both sides, noting that the
dependence of lE[w~+ 1 I Wn ~] on w~ r is linear, and using @ to denote expected values, a close
relation of equation (6) emerges for the linear representation:
@; "~ Od Z
Qijr( Jr " ~¢rn)
-- (Xi • @n r)
Qik qkj~ + ''"
ai~k-lqkjr~r q- Z
ik qkjr 1 :/'r-1 ~1- .
Define X to be the matrix whose columns are xi, so [X]a~ = [Xa] b, and D to be the
diagonal matrix [D]a b = 6abda, where 6ab is the Kronecker delta. Remembering that
hi = ~j~3 qij~, and converting to matrix form
Wn q_ o~XD[QrXT@~
+ ... + I)h]
airjr q- Z
airk-lqkjr -~- "'" ~- Z
as this covers all the possible options for r-step moves from state i.
Define the correct predictions [6"]i = IE[zl i]; then also, from equation (2),
6" = [lE[zli]]
= h + Qh + Q2h + ...
= (I + Q + Q2 + ... + Qr-1)h + Qr(i + Q + Q2 + ... + Qr-1)h + ...
[Qr]k(i + Q + Q2 + ... + Qr-1)h
= (I- Qr)-l(I + Q + Q2 + ... + Qr-1)h
where the sum converges since the chain is absorbing. This is another way of writing equa-
Multiplying equation (17) on the left by X T,
X Wn+ 1 = X w n + otXTXD[(I + Q + Q2 + ... + Qr-1)h + QrXTwn-r __ xT~,~]
= [I - o[.X T SD(I
Qr)]xT@~ + o~XTXD(I + Q + Q2 + ... + Qr-1)h
TD(X) FOR GENERAL X
Subtracting ~* from both sides of the equation, and noting that from equation (18)
Qr)~. = (I + Q + Q2 + ...
+ Qr-~)h ' this gives the update rule, which is the
equivalent of equation (9):
i*] = [I - otXTXD(I -
+ otXrXD(I -
Qr)][xT@~ -- I~*].
The Watkins construction of TD(X) developed in equation (10) in the previous section
reveals that, starting from wn = Wn, Yr,
= (1 - N) ~.j --r-1
Therefore, since for 0 < k < 1, (1 - k)~]'=~ )k r-1 = 1,
[x~,#x n -
XQ]-I)} [Xr~,~ - ~'1
where @x are the expected weights from the TD(X) procedure. The sum
(1 -- )k) ~
converges since 0 < ~ < 1.
o~TXD(I - (1 - X)Q[I - XQ] -1)
then the truth of theorem T will be shown if it can be demonstrated that 3e > 0 such that
for 0 < c~ < e, limn-~A~, = 0. For then [XT'~n ~ -- &] --~ 0 as n ~ ~, and all the esti-
mates will tend to be correct.
Almost all of Sutton's proof of this applies rnutatis mutandis to the case that X
~ 0, always provided the crucial condition holds that X has full rank. For completeness,
the entire proof is given in the appendix. Overall it implies that the expected values of
the estimates will converge to their desired values as more sequences are observed under
the conditions stated in theorem T.
2.6 Non-independence of the xi
In moving from Watkins' representation-free proof to Sutton's treatment of the linear case,
one assumption was that the xi, the vectors representing the states, were independent.
If they are not, so that matrix X does not have full rank, the proof breaks down.
D(I - (1 - X)Q[I - XQ]-1) is still positive, however XrXD(I - (1 - X)Q[I - XQ]-~)
will no longer have a full set of eigenvalues with positive real parts, since the null subspace
Y = {y]XD(I-
(1 - X)Q[I-
kQl-1)y = 0} ~{0}
is not empty. Any nonzero member of this is an eigenvector with eigenvalue 0 of XrXD(/
- X)Q[I- XQ]-~).
Saying what will happen to the expected values of the weights turns out to be easier than
understanding it. Choose a basis:
{b 1 .....
bp, bp+~ ....
, bn} for (R n,
with bi ( Y, for 1 _< i _< p being a basis for Y.
Then the proof in the appendix applies exactly to bp+ 1 .....
bn; that is there exists some
D < e < 1 such that:
lim [1 - ~XTXD(I - (1 - X)Q[I - XQ]-I)] n bi = 0, forp < i _< n, and 0 < a < e.
(1 - X)Q[I - XQ]-I)] n bi = hi, for 1 _< i _< p
by the definition of Y.
Qr)]n[Xrl~0 x
~ibi ' as n -~ ~
TD(X) FOR GENERAL k
)~Q]-I)[XT~n~ - 6*] ~ 0 as n ~
To help understand this result, consider the equivalent for the LMS rule, TD(1). There
XD[XT@In -- 6*] --~" 0 as n ~ ~a.
and so, since D is symmetric,
[xT,~n l _ 6,]TD[XS@~ _ 6"1 = X(D + DT)[xT~,rn 1 -- 6* 1
= 2XD[Xr~,~ -
~ 0 as n -~ 0%
by equation (21). For weights w, the square error for state i is I[XTw -- 6*] I/2, and the ex-
pected number of visits to i in one sequence is d i. Therefore the quadratic form
6*]~D[X~'w
is just the loaded square error between the predictions at each state and their desired values,
where the loading factors are just the expected frequencies with which the Markov chain
hits those states. The condition in equation (24) implies that the expected values of the
weights tend to be so as to minimize this error.
This does not happen in general for ~x ~ 1. Intuitively, bias has returned to haunt. For
the case where X is full rank, Sutton shows that it is harmless to use the inacurrate estimates
from the next state Xit+I°W to criticize the estimates for the current state Xit°W, Where X
is not full rank, these successive estimates become biased on account of what might be
deemed their 'shared' representation. The amount of extra bias is then related to the amount
of sharing and the frequency with which the transitions happen from one state to the next.
Formalizing this leads to a second issue; the interaction between the two statistical proc-
esses of calculating the mean weight and calculating the expected number of transitions.
Comparing equations (20) and (21), one might expect
[xT~,n x _ 6,]TD( / _ (1 -- X)Q[I-
XQ]-I)[xT@nX - 6*] = 0
However, the key step in proving equation (24) was the transition between equations (22)
and (23), which relied on the symmetry of D. Since Q is not in general symmetric, this
will not happen.
g(w') = ~ww [XTw -- ~*]TD(I -- (1 - X)Q[I - ~,Q]-I)[XTw' - ~*]
= XD(I - (1 -
)x)Q[I - )xQ]-l)[Xrw ' - ~'1
all that will actually happen is that g('~n x) -~ 0 as n ~ ~.
Although ~e behavior described by equation (25) is no more satisfactory than that de-
scribed by equation (26), it is reve~ing to consider what happens if one attempts to arrange
for it to hold. This can be achieved by 'completing' ~e derivative, i.e., by having a learn-
ing rule whose effect is
IX Wn+ I -
XQ) -~ + (I -
XQ~)-~QrD r
[Xr@~ - &]
The Q ~ te~ effectively arranges for backwards as well as forw~ds learning to occur, so
~at not o~y would state i t adjust its estimate to make it more like state it+a, but ~so state
it+ ~ would adjust its estimate to make it more like state i~.
Werbos and Sutton (personal co~unicafion) bo~ discussed this point in ~e con-
text of ~e gradient descent of TD(X) rather ~an its convergence for non-independent x i.
Wer~s presented an e~mple based on a lea~ng tec~ique ve~ s~lar to TD(0), ~ which
completing the derivative in ~is manner makes the rule converge away from the t~e solu-
tion. He faulted ~is proc~ure for introducing ~e u~elp~l co,elations be~een ~e lea~-
ing role ~d the random moves from one state to the next which were mentioned above.
He pointed out the convergence in terms of ~nctions g in equation (26) in which ~e w'
weights are fixed.
Sutton presented an e~ple
to help expl~n the result. At first sight, augmenting TD(X)
s~ms quite reaso~ble; after ~ it could q~te eas~y hap~n by random ch~ce of ~e ~ai~g
sequences that ~e predictions for one state are more accurate th~ ~e predictions for ~e
next at some point. Therefore, trai~ng ~e second to be more l~e ~e first would be help~l.
However, Suaon pointed out ~at time ~d choices always move forward, not backwards.
Consider the case shown in figure 2, where ~e numbers over the arrows represent the transi-
tion probab~ifies, ~d ~e n~bers at ~e te~
n~es represent ~
absorb~g v~ues.
Here, the value of state A is reasonably 1/2, as there is 50% probability of ending up
at eider Y or Z. The value of state B, ~ough, should be 1, as ~e chain is certain to end
up at E Training forwards will give ~is, but training backwards too will make ~e value
of B tend to 3/4. h Werbos' ~s,
~ere ~e co~ela~ons be~n
~e weigh~ ~d ~e possible
transitions that count against ~e augmented term. Incidentally, ~is result does not affect
TD(1), because ~e training v~ues, being just ~e ter~nal value for ~e sequence, be~
no relation to the transitions ~emselves, just the number of times each s~te is visited.
Co~ng back to the case where X is not ~11 ra~. TD(X) for k ~ will still converge,
but away from the 'best' value, to a degree that is determined by ~e matrix
(1 - ),)Q[I- XQ]-~).
TD(k) FOR GENERAL k
Figure 2. Didactic example of the pitfalls of backwards training. If Y and Z are terminal states with values 1
and 0 respectively, what values should be assigned to states A and B respectively.
3. Convergence with probability one
Sutton's proof and the proofs in the previous section accomplish only the nadir of stochastic
convergence, viz convergence of the mean, rather than the zenith, viz convergence with
probability one. Watkins proved convergence with probability one for a form of predic-
tion and action learning he called Q-learning. This section shows that this result can be
applied almost directly to the discounted predictive version of TD(0), albeit without the
linear representations, and so provides the first strong convergence proof for a temporal
difference method.
Like dynamic programming (DP), Q-learning combines prediction and control. Consider
a controlled, discounted, nonabsorbing Markov-process, i.e., one in which at each state
i fi 91; there is a finite set of possible actions a ~ 12. Taking one action leads to an immedi-
ate reward, which is a random variable ri(a ) whose distribution depends on both i and a,
and a stochastic transition according to a Markov matrix (Pij(a) for j ~ 9Z. If an agent
has some policy ~r(i) ~ 12, which determines which action it would perform at state i, then,
defining the value of i under 7r as F~(i), this satisfies:
V~(i) = IE[ri(Tr(i))] + 3' ~_~ (Pij(Tr(i))V~(J),
where 3' is the discount factor. Define the Q value of state i and action a under policy 7r as:
Qr(i, a) = lE[ri(a)] + "y Z
(PiJ(a)V~(j),
which is the value of taking action a at i followed by policy ~r thereafter. Then the theory
of DP implies that a policy which is at least as good as zr is
to take the action a* at state i where a* = argmaxb {Q'~(i, b)}, and to follow 7r in all other
states. In this fact lies the utility of the Q values. For discounted problems, it turns out
that there is at least one optimal policy 7r*; define Q*(i, a) = Q~*(i, a).
Q-learning is a method of determining Q*, and hence an optimal policy, based on explor-
ing the effects of actions at states. Consider a sequence of observations (in, an, in, Zn),
where the process at state in is probed with action a n, taking it to statejn and giving reward
Zn. Then define recursively:
- an)O,~(i, a) + o~n(z n + 3"Un(jn))
O,,n+ 1(i, a)
ifi = in anda = an,
for any starting values C%(i, a), where Un(L) = maxb {O~(jn, b)}. The % are a set of
learning rates that obey standard stochastic convergence criteria:
Olnk(i,a ) = 0~,
O~n (i,a) < 0~, ¥i E ~, a ~ ~
where n~(i, a) is the k th time that in = i and a n = a. Watkins proved that if, in
addition, the rewards are all bounded, then, with probability one:
lim O~(i, a) = O~*(i, a),
Consider a degenerate case of a controlled Markov process in which there is only one
action possible from every state. In that case, the O~ ~, V ~, and the (similarly defined) U ~
values are exactly the same and equal to 0~*, and equation (27) is exactly the on-line form
of TD(0) for the case of a nonabsorbing chain in which rewards (i.e., the terminal values
discussed above in the context of absorbing Markov chains) arrive from every state rather
than just some particular set of absorbing states. Therefore, under the conditions of Watkins'
theorem, the on-line version of TD(0) converges to the correct predictions, with probabil-
Although clearly a TD procedure, there are various differences between this and the
one described in the previous section. Here, learning is on-line, that is the V(= 0~) values
are changed for every observation. Also, learning need not proceed along an observed se-
quence-there is no requirement that jn = in+ l, and so uncoupled or disembodied moves
can be used? The conditions in equation (28) have as a consequence that every state must
be visited infinitely often. Also note that Sutton's proof, since it is confined to showing
convergence in the mean, works for a fixed learning rate ~, whereas Watkins', in common
with other stochastic convergence proofs, requires % to tend to 0.
Also, as stated, the O~-learning theorem only applies to discounted, nonabsorbing, Markov
chains, rather than the absorbing ones with 3`=1 of the previous section. 3' < 1 plays the
important rdle in Watkins' proof of bounding the effect of early O~n values. It is fairly easy
to modify his proof to the case of an absorbing Markov chain with 3"=1, as the ever increas-
ing probability of absorption achieves the same effect. Also, the conditions of Sutton's
theorem imply that every nonabsorbing state will be visited infinitely often, and so it suf-
fices to have one set of % that satisfy the conditions in (28) and apply them sequentially
for each visit to each state in the normal running of the chain.
TD(k) FOR GENERAL k
4, Conclusions
This paper has used Watkins' analysis of the relationship between temporal difference (TD)
estimation and dynamic programming to extend Sutton's theorem that TD(0) prediction
converges in the mean, to the case of theorem T; TD(X) for general k. It also demonstrated
that if the vectors representing the states are not linearly independent, then TD(X) for ~ ~ 1
converges to a different solution from the least mean squares algorithm.
Further, it has applied a special case of Watldns' theorem that Q-learning, his method
of incremental dynamic programming, converges with probability one, to show that TD(0)
using a localist state representation, also converges with probability one. This leaves open
the question of whether TD(k), with punctate or distributed representations, also converges
in this manner.
Appendix: Existence of appropriate ~
c~XTXD(I- (1 - X)Q[I-
it is necessary to show that there is some e such that for 0 < c~ < e, limn-~ A~ = 0.
In the case that X = 0 (for which this formula remains correct), and X has full rank, Sutton
proved this on pages 26-28 of , by showing successively that D(I - Q) is
positive, that XTXD(I - Q) has a full set of eigenvalues all of whose real parts are posi-
tive, and finally that c~ can thus be chose such that all eigenvalues of I - o~zXD(I - Q)
are less than 1 in modulus. This proof requires little alteration to the case that X ~ 0, and
its path will be followed exactly.
The equivalent ofD(I - Q) is D(I - (1 - ~,)Q[I - XQ]-~). This will be positive defi-
nite, according to a lemma by Varga and an observation by Sutton, if
XQ] -1) + {D(I-
(1 - X)Q[I-
can be shown to be strictly diagonally dominant with positive diagonal entries. This is the
part of the proof that differs from Sutton, but even here, its structure is rather similar.
Qr) + {D(I-
[Sr]ii = [D(I-
Qr)lii + [{D(I-
since Q is the matrix of an absorbing Markov chain, and so Q r has no diagonal elements
_> 1. Therefore Sr has positive diagonal elements.
Also, for i ~ j,
[Sr]ij = di[I
Qr]ij + dj[I - Qqji
- d~[Qr]jg
since all the elements of Q, and hence also those of Q~, are positive.
In this case, Sr will be strictly diagonally dominant if, and only if, Ej[Sr]ij >- O, with
strict inequality for some i.
~ ~s,.j,., = ~
or]ij + [dR/- Q~)]i
= d~. I1 _~
[ # T ( / _ j .
where equation (29) follows from equation (16), equation (30) holds since
Q)(I + Q + Q2 +
... + Q~-~)
and equation (31) holds since Ej[Qr]ij ~ 1, as the chain is absorbing, and [QS]ij ~ 0, vs.
Also, ~ere exists at least one i for which ~i > 0, and ~e inequality is strict for that i.
Since S~ is strictly diagon~ly dominant for all r ~ 1,
Sx = (1 - k) ~
TD0~) FOR GENERAL X
is strictly diagonally dominant too, and therefore D(I - (1 - X)Q[I - XQ] -~) is positive
The next stage is to show that XTXD(I - (1 - X)Q[I - XQ] -~) has a full set of eigen-
values all of whose real parts are positive, xTx, D and (I - (1 - X)Q[I - XQ]-1) are
all nonsingular, which ensures that the set is full. Let ff and u be any eigenvalue-eigenvector
pair, with u = a + bi and v = (XTX)-lu ~ 0, SO U = (xTX)v. Then
(1 - X)Q[I - XQl-~)u =
= kv*(UX)v
= ~(Xv)*Xv
where '*' indicates conjugate transpose. This implies that
(1 - X)Q[I- XQ]-~)u] = Re(~(Xv)*Xv)
or equivalently,
{(Xv)*Xv}Re[~l = aTD(I- (1 - X)Q[I- XQ]-l)a
+ bTD(I - (1 -
X)Q[I - XQl-1)b.
Since the right side (by positive definiteness) and (Xv)*Xv are both strictly positive, the
real part of k must be strictly positive too.
Furthermore, u must also be an eigenvector of
I - o~XTXD(I- (1 - X)Q[I -
o~XTXD(I- (1 - X)Q[I- )~Q]-llu = u - o~u
Therefore, all the eigenvalues of I - cOfTXD(/ -- (1 -- X)Q[I - XQ]-I) are of the form
1 - a~ where ~ = v + 4i has positive v. Take
0 < c ~ < - -
v~ + q,2 '
for all eigenvalues ~p, and then all the eigenvalues 1 - ~b of the iteration matrix are guar-
anteed to have modulus less than one. By another theorem of Varga 
lim [I - o~XTXD(I - (1 - X)Q[I -
XQ]-~)] n = 0.
Acknowledgments
This paper is based on a chapter of my thesis . I am very grateful to Andy Barto,
Steve Finch, Alex Lascarides, Satinder Singh, Chris Watldns, David Willshaw, the large num-
ber of people who read drafts of the thesis, and particularly Rich Sutton and two anonymous
reviewers for their helpful advice and comments. Support was from SERC. Peter Dayan's
current address is CNL, The Salk Institute, P.O. Box 85800, San Diego, CA 92186-5800.
1. Here and subsequently, a superscript k is used to indicate a TD(k)-based estimator.
2. States A and G are absorbing and so are not represented.
3. This was one of Watkins' main motivations, as it allows his system to learn about the effect of actions it believes
to be suboptimal.