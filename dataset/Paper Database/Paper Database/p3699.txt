The Astrophysical Journal, 638:L51–L54, 2006 February 20
 2006. The American Astronomical Society. All rights reserved. Printed in U.S.A.
A NESTED SAMPLING ALGORITHM FOR COSMOLOGICAL MODEL SELECTION
Pia Mukherjee, David Parkinson, and Andrew R. Liddle
Astronomy Centre, University of Sussex, Brighton BN1 9QH, UK
Received 2005 August 24; accepted 2006 January 10; published 2006 January 31
The abundance of cosmological data becoming available means that a wider range of cosmological models are
testable than ever before. However, an important distinction must be made between parameter ﬁtting and model
selection. While parameter ﬁtting simply determines how well a model ﬁts the data, model selection statistics, such
as the Bayesian evidence, are now necessary to choose between these different models, and in particular to assess the
need for new parameters. We implement a new evidence algorithm known as nested sampling, whichcombinesaccuracy,
generality of application, and computational feasibility, and we apply it to some cosmological data sets and models.
We ﬁnd that a ﬁve-parameter model with a Harrison-Zel’dovich initial spectrum is currently preferred.
Subject heading: cosmology: theory
1. INTRODUCTION
The higher level inference problem of allowing the data to
decide the set of parameters to be used in ﬁtting is known as
model selection. In the Bayesian framework, the key model
selection statistic is the Bayesian evidence , being the average likelihood of a model over
its prior parameter space. The evidence can be used to assign
probabilities to models and to robustly establish whether the
data require additional parameters.
While the use of Bayesian methods is common practice in
cosmological parameter estimation, its natural extension to
model selection has lagged behind. Work in this area has been
hampered by difﬁculties in calculating the evidence to high
enough accuracy to distinguish between the models of interest.
The issue of model selection has been raised in some recent
papers , and information criterion–based approximate methods have been introduced in Liddle .
Semianalytic approximations such as the Laplace approximation, which works well only for Gaussian likelihoods, and the
Savage-Dickey density ratio, which works for more general
likelihood functions but requires the models being compared
to be nested, are methods that were recently exploited by Trotta
 . A more general and accurate numerical method is thermodynamic integration, but Beltra´n et al. found that in
realistic applications, around 107 likelihood evaluations were
needed per model to obtain good accuracy, making it a supercomputer-class problem in cosmology where likelihood
evaluations are computationally costly.
In this Letter we present a new algorithm that, for the ﬁrst
time, combines accuracy, general applicability, and computational feasibility. It is based on the method of nested sampling,
proposed by Skilling , in which the multidimensional
integral of the likelihood of the data over parameter space is
performed using Monte Carlo sampling, working through the
prior volume to the regions of high likelihood.
2. BAYESIAN INFERENCE
Using Bayes’s theorem, the probability that a model (hypothesis: H) is true in light of observed data (D) is given by
P(DFH)P(H)
It shows how our prior knowledge
is modiﬁed in the
presence of data.
The posterior probability of the parameters (v) of a model
in light of data is given by
P(DFv, H)P(vFH)
P(vFD, H) p
is the likelihood of the data given the model
and its parameters, and
is the prior on the parameters.
This is the relevant quantity for parameter estimation within a
model, for which the denominator
is of no conse-
is, however, the evidence for the model H, the
key quantity of interest for the purpose of model selection
 . Normalizing
the posterior
marginalized over v to unity, it is given
E p P(DFH) p
dv P(DFv, H)P(vFH),
the prior also being normalized to unity.
The evidence for a given model is thus the normalizing
constant that sets the area under the posterior
unity. In the now-standard Markov Chain Monte Carlo method
for tracing the posterior , the posterior is reﬂected in the binned number density
of accumulated samples. The evidence found in this way would
not generally be accurate, as the algorithm would sample the
peaks of the probability distribution well, but would undersample the tails that might occupy a large volume of the prior.
When comparing two different models using Bayes’stheorem,
the ratio of posterior probabilities of the two models would be
the ratio of their evidences (called the Bayes factor) multiplied
by the ratio of any prior probabilities that we may wish to assign
to these models (eq. ). It can be seen from equation (3) that
while more complex models will generally result in better ﬁts
to the data, the evidence, being proportional to the volume occupied by the posterior relative to that occupied by the prior,
automatically implements Occam’s razor. It favors simpler models with greater predictive power provided they give a good ﬁt
to the data, quantifying the tension between model simplicity
and the ability to ﬁt to the data in the Bayesian sense. Jeffreys
MUKHERJEE, PARKINSON, & LIDDLE
Fig. 1.—The nested sampling algorithm integrates the likelihood over the
prior volume by peeling away thin isosurfaces of equal likelihood.
 provides a useful guide to what constitutes a signiﬁcant
difference between two models:
is substantial,
1 ! D ln E ! 2.5
is strong, and
is decisive. For ref-
2.5 ! D ln E ! 5
D ln E 1 5
of 2.5 corresponds to odds of about 1 in 13.
While for parameter ﬁtting the priors become irrelevant once
the data are good enough, for model selection some dependence
on the prior ranges always remains, no matter how good the
data. The dependence on prior parameter ranges is a part of
Bayesian reasoning, and priors should be chosen to reﬂect our
state of knowledge about the parameters before the data came
along. The Bayesian evidence is unbiased, as opposed to approximations such as the information criteria.
Perhaps the most important application of model selection is
in assessing the need for a new parameter, i.e., describing some
new physical effect proposed to inﬂuence the data. Frequentiststyle approaches are commonplace, where one accepts the parameter on the basis of a better ﬁt, corresponding to an improvement in
by some chosen threshold (leading to phrases
such as “2 j detection”). Such approaches are non-Bayesian: the
evidence shows that the size of the threshold depends both on
the properties of the data set and on the priors, and in fact the
more powerful the data set, the higher the threshold that must
be set . Furthermore, as the Bayesian evidence provides a rank-ordered list of models, the need to choose a threshold
is avoided (although one must still decide how large a difference
in evidence is needed for a robust conclusion).
The main purpose of this Letter is to present an algorithm
for evidence computation with widespread applications. However, as a speciﬁc application, we examine the need for extra
parameters against the simplest viable cosmological model, a
LCDM model with a Harrison-Zel’dovich initial spectrum,
whose ﬁve parameters are the baryon density
, the cold
dark matter density
, the Hubble parameter
km s1 Mpc1 (or the ratio V of the approximate sound horizon
at decoupling to its angular diameter distance), the optical depth
t, and the amplitude
of primordial perturbations. We study
the case for two additional parameters, the scalar spectral index
and the dark energy equation of state (assumed constant).
3. NESTED SAMPLING
3.1. Basic Method
Nested sampling is a scheme to trace the variation of the likelihood function with prior mass, with the effects
of topology, dimensionality, and everything else implicitly built
into it. It breaks up the prior volume into a large number of
“equal mass” points and orders them by likelihood. Rewriting
equation (3) in the notation of Skilling , with X as the
fraction of total prior mass such that
dX p P(v, H)dv
likelihood
, the equation for the evidence
P(DFv, H) p L(X)
Thus, the problem of calculating the evidence has become a
one-dimensional integral, in which the integrand is positive and
decreasing. Suppose we can evaluate the likelihood as L p
, where the
are a sequence of decreasing values, such
! … ! X ! X ! 1,
as shown schematically in Figure 1. Then the evidence can be
estimated by any numerical method, for example, the trapezoid
The nested sampling algorithm achieves the above summation in the following way:
1. Sample N points randomly from within the prior, and
evaluate their likelihoods. Initially we will have the full prior
range available, i.e.,
(0, X p 1)
2. Select the point with the lowest likelihood
. The prior
volume corresponding to this point,
, can be estimated
probabilistically. The average volume decrease is given as
, where t is the expectation value of the largest of
N random numbers from uniform (0, 1), which is
3. Increment the evidence by
4. Discard the lowest likelihood point and replace it with a
new point, which is uniformly distributed within the remaining
prior volume
. The new point must satisfy the hard con-
straint on likelihood of
5. Repeat steps 2–4, until the evidence has been estimated
to some desired accuracy.
The method thus works its way up the likelihood surface,
through nested surfaces of equal likelihood. After j steps, the
prior mass remaining shrinks to
. The process
X ∼[N/(N  1)]
is terminated when some stopping criterion is satisﬁed, and a
ﬁnal amount of
due to the
remaining sample points
is added to the thus far accumulated evidence.
Thus, a multidimensional integral is performed using Monte
Carlo sampling, imposing a hard constraint in likelihood on
samples that are uniformly distributed in the prior, implying a
regularized probabilistic progression through the prior volume.
Besides implementing and testing this scheme in the cosmological context, our main contribution lies in developing a general strategy to sample new points efﬁciently.
No. 2, 2006
BAYESIAN EVIDENCE FROM NESTED SAMPLING
Fig. 2.—Evidence against the number of sample points N for a ﬂat Harrison-
Zel’dovich model with a cosmological constant. Points are displaced slightly
horizontally for visual clarity.
Fig. 3.—(a) The accumulated evidence (dashed curve), the evidence contributed by the remaining points at each stage (dot-dashed curve), and their sum
(solid curve), shown against the prior volume remaining, for a ﬂat Harrison-
Zel’dovich model with a cosmological constant. The iteration proceeds from
right to left. (b) A later part of the solid curve shown against the
3.2. Details
The prior space to sample from reduces by a constant factor
every time the lowest likelihood point is dis-
carded. The most challenging task in implementing the algorithm is to sample uniformly from the remaining prior volume,
without creating too large an overhead in likelihood evaluations
even when the remaining volume of prior space may be very
small. The new point must be uncorrelated to the existing
points, but we can still use the set of existing points as a guide.
We ﬁnd the covariance of the live points, rotate our coordinates
to the principal axes, and create an ellipsoid that just touches
the maximum coordinate values of the existing points. To allow
for the isolikelihood contours not being exactly elliptical, as
well as to take in the edges, these limits are expanded by a
constant enlargement factor, aiming to include the full volume
with likelihood exceeding the current limit (if this is not done,
new points will be biased toward the center, thus overestimating
the evidence). New points are then selected uniformly within
the expanded ellipse until one has a likelihood exceeding the
old minimum, which then replaces the discarded lowest likelihood point.
The two algorithm parameters to be chosen are the number of
points N and the enlargement factor of the ellipsoid. Figure 2
shows evidence verses N, for a ﬂat Harrison-Zel’dovich model
with a cosmological constant. The mean evidence values and standard deviations obtained from four repetitions are shown. These
are shown for two different values of the enlargement factor.When
the enlargement factor is large enough (here 1.5), the evidence
obtained with 100 sample points agrees with that obtained with
500, while when the enlargement factor is not large enough, the
evidences obtained with small N are systematically biased high.
Similar tests done on multidimensional Gaussian likelihood functions, for which the expected evidence can be found analytically,
indicated the same. Based on such tests, we choose to work with
N of 300, and enlargement factors of 1.5 for the ﬁve-dimensional
model (this corresponds to a 50% increase in the range of each
parameter), 1.7 for the six-dimensional model, and 1.8 for the
seven-dimensional model. These choices are conservative(smaller
values would reduce the computing time) and were made in order
to ensure evidences that are accurate and free of systematics. In
our cosmological applications, we have also computed evidences
with larger enlargement factors and found that they remained unchanged. The typical acceptance rate in ﬁnding a new point during
the course of the algorithm was found to be roughly constant at
∼20%–25% for an enlargement factor of 1.5, and lower for larger
enlargement factors, after an initial period of almost 100% acceptance, as expected.
Figure 3a shows the accumulated evidence, the evidence
contributed by the remaining points at each stage, and their
sum, against the prior volume remaining, again for a ﬂat
Harrison-Zel’dovich model with a cosmological constant. The
X at which the calculation can be terminated will depend on
the details of the problem (e.g., dimensionality, priors, etc.).
We deﬁne a parameter “tol” as the maximum possible fractional amount that the remaining points could increase the
evidence by:
is the maximum
likelihood of the current set of sample points. Figure 3b zooms
into a late part of the solid curve, now plotting it against the
value of the parameter “tol.” The calculation need only be
carried out until the standard error on the mean evidence,
computed for a certain number of repetitions, drops below
the desired accuracy. An uncertainty in
of 0.1 would be
the highest conceivable accuracy one might wish, and with
eight repetitions this happens when
∼a few. This takes
us to quite small X, of order 106 to 107 in our actual cosmological simulations.
4. RESULTS
We have calculated the evidences of four different cosmological models: (1) a ﬂat, Harrison-Zel’dovich model with a cosmological constant (LCDMHZ), (2) the same as model 1,
except allowing the tilt of the primordial perturbation spectrum
to vary (LCDMns), (3) the same as model 1, except allowing
the equation of state of the dark energy to take alternative values
(wHZ), and ﬁnally (4) allowing both
vary (wns). The prior ranges for the other parameters were
0.018 ≤Q h ≤0.032 0.04 ≤Q
h ≤0.16 0.98 ≤
V ≤1.1 0 ≤t ≤0.5
2.6 ≤ln (A # 10 ) ≤4.2
The data sets we use are CMB TT and TE anisotropy power
spectrum data from the WMAP experiment , together with higher l
CMB temperature power spectrum data from VSA , CBI , and ACBAR (Kuo et
MUKHERJEE, PARKINSON, & LIDDLE
Parameter Ranges and Evidences for Various Cosmological Models
(wide prior)
ns .................
w .................
1/3 to 1
1/3 to 1
e.f. ...............
Nlike (#104) ......
..............
0.00  0.08
0.58  0.09
1.16  0.08
0.45  0.08
1.52  0.08
Note.—Other parameter ranges are given in the text.
al. 2004), the matter power spectrum data from SDSS and 2dFGRS , and
supernova apparent magnitude–redshift data from Riess et al.
Results are shown in Table 1. For the spectral tilt, evidences
have been found for two different prior ranges, as an additional
test of the method. For a prior range twice the size of the
original in
, the evidence is expected to change by
most, and that difference is recovered. The ﬁrst two rows show
the priors on the additional parameters of the model, or their
constant values if they were ﬁxed. The third row shows the
enlargement factor (e.f.) we used for the model. The fouth row
shows the total number of likelihood evaluations needed to
compute the mean
evidence to an accuracy ∼0.1, and the
ﬁfth row shows the mean
and the standard error in that
mean computed from eight repetitions of the calculation, normalized to the LCDMHZ evidence.
The LCDMHZ model has the highest evidence and, as
such, is the preferred ﬁt to the data. Hence, we do not ﬁnd any
indication of a need to introduce parameters beyond the base
set of 5, in agreement with the conclusions of Liddle 
and Trotta . However, the difference between the ln E
of the higher dimensional models and the base model is not
large enough to signiﬁcantly exclude any of those models at
5. CONCLUSIONS
We introduce the nested sampling algorithm for the computation of Bayesian evidences for cosmological model selection. We ﬁnd that this new algorithm uniquely combines accuracy, general applicability, and computational feasibility. It
is able to attain an accuracy (standard error in the mean ln
evidence) of 0.1 in
likelihood evaluations. It is therefore
much more efﬁcient than thermodynamic integration, which is
the only other method that shares the general applicability of
nested sampling. Nested sampling also leads to a good estimate
of the posterior probability density of the parameters of the
model for free, which we will discuss in a forthcoming paper.
We also plan to make a public release of the code in the near
Using nested sampling, we have computed the evidence for
the simplest cosmological model, with a base set of ﬁve parameters, which provides a good ﬁt to current cosmological
data. We have computed the evidence of models with additional
parameters—the scalar spectral tilt, a constant dark energy
equation-of-state parameter, and both of these together. We ﬁnd
that current data offer no indication of a need to add extra
parameters to the base model, which has the highest evidence
among the models considered.
The authors were supported by the PPARC. We thank Martin
Kunz, Sam Leach, Peter Thomas, and especially John Skilling
for helpful advice and comments. The analysis was performed
on the UK national cosmology supercomputer (COSMOS) in
Cambridge.