HAL Id: hal-01590922
 
Submitted on 20 Sep 2017
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Scalable Fine-Grained Proofs for Formula Processing
Haniel Barbosa, Jasmin Christian Blanchette, Pascal Fontaine
To cite this version:
Haniel Barbosa, Jasmin Christian Blanchette, Pascal Fontaine.
Scalable Fine-Grained Proofs for
Formula Processing. Proc. Conference on Automated Deduction (CADE), 2017, Gotenburg, Sweden.
pp.398 - 412, ￿10.1007/978-3-642-02959-2_10￿. ￿hal-01590922￿
Scalable Fine-Grained Proofs for Formula Processing
Haniel Barbosa1,2(B), Jasmin Christian Blanchette3,1,4, and Pascal Fontaine1
1 Université de Lorraine, CNRS, Inria, LORIA, Nancy, France
{haniel.barbosa,jasmin.blanchette,pascal.fontaine}@loria.fr
2 Universidade Federal do Rio Grande do Norte, Natal, Brazil
3 Vrije Universiteit Amsterdam, Amsterdam, The Netherlands
4 Max-Planck-Institut für Informatik, Saarbrücken, Germany
Abstract. We present a framework for processing formulas in automatic theorem provers, with generation of detailed proofs. The main components are a
generic contextual recursion algorithm and an extensible set of inference rules.
Clausiﬁcation, skolemization, theory-speciﬁc simpliﬁcations, and expansion of
‘let’ expressions are instances of this framework. With suitable data structures,
proof generation adds only a linear-time overhead, and proofs can be checked in
linear time. We implemented the approach in the SMT solver veriT. This allowed
us to dramatically simplify the code base while increasing the number of problems
for which detailed proofs can be produced, which is important for independent
checking and reconstruction in proof assistants.
Introduction
An increasing number of automatic theorem provers can generate certiﬁcates, or proofs,
that justify the formulas they derive. These proofs can be checked by other programs
and shared across reasoning systems. Some users will also want to inspect this output
to understand why a formula holds. Proof production is generally well understood for
the core proving methods and for many theories commonly used in satisﬁability modulo
theories (SMT). But most automatic provers also perform some formula processing or
preprocessing—such as clausiﬁcation and rewriting with theory-speciﬁc lemmas—and
proof production for this aspect is less mature.
For most provers, the code for processing formulas is lengthy and deals with a
multitude of cases, some of which are rarely executed. Although it is crucial for efﬁciency,
this code tends to be given much less attention than other aspects of provers. Developers
are reluctant to invest effort in producing detailed proofs for such processing, since this
requires adapting a lot of code. As a result, the granularity of inferences for formula
processing is often coarse. Sometimes, processing features are even disabled to avoid
gaps in proofs, at a high cost in proof search performance.
Fine-grained proofs are important for a variety of applications. We propose a framework to generate such proofs without slowing down proof search. Proofs are expressed
using an extensible set of inference rules (Sect. 2). The succedent of a rule is an equality
between the original term and the translated term. (It is convenient to consider formulas
a special case of terms.) The rules have a ﬁne granularity, making it possible to cleanly
separate theories. Clausiﬁcation, theory-speciﬁc simpliﬁcations, and expansion of ‘let’
expressions are instances of this framework. Skolemization may seem problematic, but
with the help of Hilbert’s choice operator, it can also be integrated into the framework.
Some provers provide very detailed proofs for parts of the solving, but we are not aware
of any publications about practical attempts to provide easily reconstructible proofs for
processing formulas containing quantiﬁers and ‘let’ expressions.
At the heart of the framework lies a generic contextual recursion algorithm that
traverses the terms to translate (Sect. 3). The context ﬁxes some variables, maintains
a substitution, and keeps track of polarities or other data. The transformation-speciﬁc
work, including the generation of proofs, is performed by plugin functions that are
given as parameters to the framework. The recursion algorithm, which is critical for
the performance and correctness of the generated proofs, needs to be implemented only
once. Another beneﬁt of the modular architecture is that we can easily combine several
transformations in a single pass, without complicating the code unduly or compromising
the level of detail of the proof output. For very large inputs, this can improve performance.
The inference rules and the contextual recursion algorithm enjoy many desirable
properties (Sect. 4). The rules are sound, and the treatment of binders is correct even in
the presence of name clashes. Moreover, with suitable data structures, proof generation
adds an overhead that is proportional to the time spent processing the terms. Checking
proofs represented as directed acyclic graphs (DAGs) can be performed with a time
complexity that is linear in their size. Detailed proofs of the metatheory are included in a
technical report , together with more explanations and examples.
We implemented the approach in veriT (Sect. 5), an SMT solver that is competitive on
problems combining equality, linear arithmetic, and quantiﬁers . Compared with other
SMT solvers, veriT is known for its very detailed proofs , which are reconstructed in
the proof assistants Coq and Isabelle/HOL and in the GAPT system . As a
proof of concept, we implemented a prototype checker in Isabelle/HOL.
By adopting the new framework, we were able to remove large amounts of complicated code in the solver, while enabling detailed proofs for more transformations than
before. The contextual recursion algorithm had to be implemented only once and is
more thoroughly tested than any of the monolithic transformations it subsumes. Our
empirical evaluation reveals that veriT is as fast as before even though it now generates
ﬁner-grained proofs.
Conventions Our setting is a many-sorted classical ﬁrst-order logic as deﬁned by the
SMT-LIB standard . A signature Σ = (S,F ) consists of a set S of sorts and a set F of
function symbols. Nullary function symbols are called constants. We assume that the
signature contains a Bool sort and constants true, false : Bool, a family (≃: σ×σ →
Bool)σ∈S of function symbols interpreted as equality, and the connectives ¬, ∧, ∨, and
→. Formulas are terms of type Bool, and equivalence is equality (≃) on Bool. Terms
are built over symbols from F and variables from a ﬁxed family of inﬁnite sets (Vσ)σ∈S.
In addition to ∀and ∃, we rely on two more binders: Hilbert’s choice operator εx.ϕ and
a ‘let’ construct, let ¯xn ≃¯sn in t, which simultaneously assigns n variables.
We use the symbol = for syntactic equality on terms. We reserve the names a,f,p,q
for function symbols; x,y for variables; r, s,t,u for terms (which may be formulas); ϕ,ψ
for formulas; and Q for quantiﬁers (∀and ∃). We use the notations ¯an and (ai)n
denote the tuple, or vector, (a1,...,an). We write [n] for {1,...,n}.
Given a term t, the set of its free variables is written FV(t). The notation t[¯xn] stands
for a term that may depend on ¯xn; t[¯sn] is the corresponding term where the terms ¯sn are
substituted for ¯xn. Bound variables in t are renamed to avoid capture. Following these
conventions, Hilbert choice and ‘let’ are characterized by
|= ∃x. ϕ[x] −■
|= (∀x. ϕ ≃ψ) −■
→(εx.ϕ) ≃(εx.ψ)
|= (let ¯xn ≃¯sn in t[¯xn]) ≃t[¯sn]
Substitutions ρ are functions from variables to terms such that ρ(xi) ̸= xi for at most
ﬁnitely many variables xi. We write them as {¯xn 7→¯sn}. The substitution ρ[¯xn 7→¯sn] maps
each variable xi to the term si and otherwise coincides with ρ. The application of a substitution ρ to a term t is denoted by ρ(t). It is capture-avoiding; bound variables in t are renamed as necessary. Composition ρ′ ◦ρ is deﬁned as for functions (i.e., ρ is applied ﬁrst).
Inference System
The inference rules used by our framework depend on a notion of context deﬁned
by the grammar Γ ::= ∅| Γ, x | Γ, ¯xn 7→¯sn. Each context entry either ﬁxes a variable x or deﬁnes a substitution {¯xn 7→¯sn}. If a context introduces the same variable
several times, the rightmost entry shadows the others. Abstractly, a context Γ ﬁxes
a set of variables and speciﬁes a substitution subst(Γ) deﬁned by subst(∅) = {},
subst(Γ, x) = subst(Γ)[x 7→x], and subst(Γ, ¯xn 7→¯tn) = subst(Γ) ◦{¯xn 7→¯tn}. In the
second equation, the [x 7→x] update shadows any replacement of x induced by Γ. We
write Γ(t) to abbreviate the capture-avoiding substitution subst(Γ)(t).
Transformations of terms (and formulas) are justiﬁed by judgments of the form
Γ ▷t ≃u, where Γ is a context, t is an unprocessed term, and u is the corresponding
processed term. The free variables in t and u must appear in the context Γ. Semantically,
the judgment expresses the equality of the terms Γ(t) and u for all variables ﬁxed by Γ.
Crucially, the substitution applies only on the left-hand side of the equality.
The inference rules for the transformations covered in this paper are presented below.
if |=T Γ(t) ≃u
if Γ(t) = t
 Γ ▷ti ≃ui
Γ ▷f(¯tn) ≃f(¯un)
Γ, y, x 7→y ▷ϕ ≃ψ
if y /∈FV(Qx.ϕ)
Γ ▷(Qx.ϕ) ≃(Qy.ψ)
Γ, x 7→(εx.ϕ) ▷ϕ ≃ψ
Γ ▷(∃x. ϕ) ≃ψ
Γ, x 7→(εx.¬ϕ) ▷ϕ ≃ψ
Γ ▷(∀x. ϕ) ≃ψ
 Γ ▷ri ≃si
Γ, ¯xn 7→¯sn ▷t ≃u
if Γ(si) = si for all i ∈[n]
Γ ▷(let ¯xn ≃¯rn in t) ≃u
• TAUTT relies on an oracle |=T to derive arbitrary lemmas in a theory T . In practice,
the oracle will produce some kind of certiﬁcate to justify the inference. An important
special case, for which we use the name REFL, is syntactic equality.
• TRANS needs the side condition because the term t appears both on the left-hand
side of ≃(where it is subject to Γ’s substitution) and on the right-hand side.
• CONG can be used for any function symbol f, including the logical connectives.
• BIND is a congruence rule for quantiﬁers. The rule also justiﬁes the renaming of the
bound variable. The side condition prevents an unwarranted variable capture. In the
antecedent, the renaming is expressed by a substitution in the context.
• SKO∃and SKO∀exploit (ε1) to replace a quantiﬁed variable with a suitable witness,
simulating skolemization. We can think of the ε expression in each rule abstractly
as a fresh function symbol that takes any ﬁxed variables it depends on as arguments.
• LET exploits (let) to expand a ‘let’ expression. The terms ¯rn assigned to the variables
¯xn can be transformed into terms ¯sn.
The antecedents of all the rules inspect subterms structurally, without modifying them.
Modiﬁcations to the term on the left-hand side are delayed; the substitution is applied
only in TAUT. This is crucial to obtain compact proofs that can be checked efﬁciently. By
systematically renaming variables in BIND, we can satisfy most side conditions trivially.
Judgments can be encoded into a well-understood theory of binders: the simply typed
λ-calculus. This provides a solid basis to reason about them, and to reconstruct proofs
expressed in the inference system. We refer to our technical report for details.
The set of rules can be extended to cater for arbitrary transformations that can be
expressed as equalities, using Hilbert choice to represent fresh symbols if necessary. The
usefulness of Hilbert choice for proof reconstruction is well known , but we push
the idea further and use it to simplify the inference system and make it more uniform.
Example 1. The following derivation tree justiﬁes the expansion of a ‘let’ expression:
x 7→a ▷x ≃a
x 7→a ▷x ≃a
x 7→a ▷p(x, x) ≃p(a,a)
▷(let x ≃a in p(x, x)) ≃p(a,a)
Skolemization can be applied regardless of polarity. Normally, we skolemize only
positive existential quantiﬁers and negative universal quantiﬁers. However, skolemizing
other quantiﬁers is sound in the context of proving. The trouble is that it is generally incomplete, if we introduce Skolem symbols and forget their deﬁnitions in terms of Hilbert
choice. To paraphrase Orwell, all quantiﬁers are skolemizable, but some quantiﬁers are
more skolemizable than others.
Contextual Recursion
We propose a generic algorithm for term transformations, based on structural recursion.
The algorithm is parameterized by a few simple plugin functions embodying the essence
of the transformation. By combining compatible plugin functions, we can perform
several transformations in one traversal. Transformations can depend on some context
that encapsulates relevant information, such as bound variables, variable substitutions,
and polarity. Each transformation can deﬁne its own notion of context.
The output is generated by a proof module that maintains a stack of derivation
trees. The procedure apply(R, n, Γ, t, u) pops n derivation trees ¯Dn from the stack and
pushes the tree of Γ ▷t ≃u obtained by applying rule R to ¯Dn. The plugin functions are
responsible for invoking apply as appropriate.
The Generic Algorithm The algorithm performs a depth-ﬁrst postorder contextual
recursion on the term to process. Subterms are processed ﬁrst; then an intermediate term
is built from the resulting subterms and is processed in turn. The context ∆is updated in a
transformation-speciﬁc way with each recursive call. It is abstract from the point of view
of the algorithm. The plugin functions are divided into two groups: ctx_let, ctx_quant,
and ctx_app update the context when entering the body of a binder or when moving from
a function symbol to one of its arguments; build_let, build_quant, build_app, and build_
var return the processed term and produce the corresponding proof as a side effect.
function process(∆, t)
return build_var(∆, x)
case f(¯tn):
n ←(ctx_app(∆, f, ¯tn, i))n
return build_app
n, f, ¯tn, (process(∆′
case Qx.ϕ:
∆′ ←ctx_quant(∆, Q, x, ϕ)
return build_quant(∆, ∆′, Q, x, ϕ, process(∆′, ϕ))
case let ¯xn ≃¯rn in t′:
∆′ ←ctx_let(∆, ¯xn, ¯rn, t′)
return build_let(∆, ∆′, ¯xn, ¯rn, t′, process(∆′, t′))
‘Let’ Expansion The ﬁrst instance of the contextual recursion algorithm expands ‘let’
expressions and renames bound variables systematically to avoid capture. Skolemization
and theory simpliﬁcation, presented below, assume that this transformation has been
performed. The context consists of a list of ﬁxed variables and variable substitutions, as
in Sect. 2. The plugin functions are as follows:
function ctx_let(Γ, ¯xn, ¯rn, t)
return Γ, ¯xn 7→(process(Γ, ri))n
function ctx_app(Γ, f, ¯tn, i)
function build_let(Γ, Γ′, ¯xn, ¯rn, t, u)
apply(LET, n+1, Γ, let ¯xn ≃¯rn in t, u)
function build_app(Γ, ¯Γ′
n, f, ¯tn, ¯un)
apply(CONG, n, Γ, f(¯tn), f(¯un))
return f(¯un)
function ctx_quant(Γ, Q, x, ϕ)
y ←fresh variable
return Γ, y, x 7→y
function build_quant(Γ, Γ′, Q, x, ϕ, ψ)
apply(BIND, 1, Γ, Qx.ϕ, Qy.ψ)
return Qy.ψ
function build_var(Γ, x)
apply(REFL, 0, Γ, x, Γ(x))
return Γ(x)
The ctx_let and build_let functions process ‘let’ expressions. In ctx_let, the substituted
terms are processed further before they are added to a substitution entry in the context. In
build_let, the LET rule is applied and the transformed term is returned. Analogously, the
ctx_quant and build_quant functions rename quantiﬁed variables systematically. This
ensures that any variables that arise in the range of the substitution speciﬁed by ctx_
let will resist capture when the substitution is applied. Finally, the ctx_app, build_app,
and build_var functions simply reproduce the term traversal in the generated proof; they
perform no transformation-speciﬁc work.
Example 2. Following up on Example 1, assume ϕ = let x ≃a in p(x, x). Given the above
plugin functions, process(∅, ϕ) returns p(a,a). It is instructive to study the evolution
of the stack during the execution of process. First, in ctx_let, the term a is processed
recursively; the call to build_app pushes a nullary CONG step with succedent ▷a ≃a
onto the stack. Then the term p(x, x) is processed. For each of the two occurrences of x,
build_var pushes a REFL step onto the stack. Next, build_app applies a CONG step to
justify rewriting under p: The two REFL steps are popped, and a binary CONG is pushed.
Finally, build_let performs a LET inference with succedent ▷ϕ ≃p(a,a) to complete
the proof: The two CONG steps on the stack are replaced by the LET step. The stack now
consists of a single item: the derivation tree of Example 1.
Skolemization Our second transformation, skolemization, assumes that ‘let’ expressions
have been expanded and bound variables have been renamed apart. The context is a pair
∆= (Γ, p), where Γ is as deﬁned in Sect. 2 and p is the polarity (+, −, or ?) of the term
being processed. The main plugin functions are those that manipulate quantiﬁers:
function ctx_quant((Γ, p), Q, x, ϕ)
if (Q, p)∈{(∃,+), (∀,−)} then
Γ′ ←Γ, x 7→sko_term(Γ, Q, x, ϕ)
return (Γ′, p)
The polarity is updated by ctx_app, which is not shown. For example, ctx_app((Γ, −), ¬,
ϕ, 1) returns (Γ, +), because if ¬ϕ occurs negatively in a larger formula, then ϕ occurs
positively. The plugin functions build_app and build_var are as for ‘let’ expansion.
Positive occurrences of ∃and negative occurrences of ∀are skolemized. All other
quantiﬁers are kept as they are. The sko_term function returns an applied Skolem function symbol following some reasonable scheme; for example, outer skolemization 
creates an application of a fresh function symbol to all variables ﬁxed in the context.
To comply with the inference system, the application of SKO∃or SKO∀in build_quant
instructs the proof module to systematically replace the Skolem term with the corresponding ε term when outputting the proof.
Theory Simpliﬁcation All kinds of theory simpliﬁcation can be performed on formulas.
We restrict our focus to a simple yet quite characteristic instance: the simpliﬁcation of
u+0 and 0+u to u. We assume that ‘let’ expressions have been expanded. The context
is a list of ﬁxed variables. The plugin functions ctx_app and build_var are as for ‘let’
expansion; the remaining ones are presented below.
function ctx_quant(Γ, Q, x, ϕ)
return Γ, x
function build_quant(Γ, Γ′, Q, x, ϕ, ψ)
apply(BIND, 1, Γ, Qx.ϕ, Qx.ψ)
return Qx.ψ
function build_app(Γ, ¯Γ′
n, f, ¯tn, ¯un)
apply(CONG, n, Γ, f(¯tn), f(¯un))
if f(¯un) has form u+0 or 0+u then
apply(TAUT+, 0, Γ, f(¯un), u)
apply(TRANS, 2, Γ, f(¯tn), u)
return f(¯un)
The quantiﬁer manipulation code, in ctx_quant and build_quant, is straightforward. The
interesting function is build_app. It ﬁrst applies the CONG rule to justify rewriting the
arguments. Then, if the resulting term f(¯un) can be simpliﬁed further into a term u, it
performs a transitive chain of reasoning: f(¯tn) ≃f(¯un) ≃u.
Combinations of Transformations Theory simpliﬁcation can be implemented as a
family of transformations, each member of which embodies its own set of theory-speciﬁc
rewrite rules. If the union of the rewrite rule sets is conﬂuent and terminating, a unifying
implementation of build_app can apply the rules in any order until a ﬁxpoint is reached.
Moreover, since theory simpliﬁcation modiﬁes terms independently of the context, it is
compatible with ‘let’ expansion and skolemization. This allows us to perform arithmetic
simpliﬁcation in the substituted terms of a ‘let’ expression in a single pass.
The combination of ‘let’ expansion and skolemization is less straightforward. Consider the formula ϕ = let y ≃∃x.p(x) in y →y. When processing the subformula ∃x.p(x),
we cannot (or at least should not) skolemize the quantiﬁer, because it has no unambiguous polarity; indeed, the variable y occurs both positively and negatively in the ‘let’
expression’s body. We can of course give up and perform two passes: The ﬁrst pass
expands ‘let’ expressions, and the second pass skolemizes and simpliﬁes terms. There
is also a way to perform all the transformations in a single instance of the framework,
described in our report .
Scope and Limitations Other possible instances of contextual recursion are the clause
normal form (CNF) transformation and the elimination of quantiﬁers using one-point
rules. CNF transformation is an instance of rewriting of Boolean formulas and can be
justiﬁed by a TAUTBool rule. Tseytin transformation can be supported by representing the
introduced constants by the formulas they represent, similarly to our treatment of Skolem
terms. One-point rules—e.g., the transformation of ∀x. x ≃a −■
→p(x) into p(a)—are
similar to ‘let’ expansion and can be represented in much the same way in our framework.
Some transformations, such as symmetry breaking and rewriting based on global
assumptions, require a global analysis of the problem that cannot be captured by local
substitution of equals for equals. They are beyond the scope of the framework. Other
transformations, such as simpliﬁcation based on associativity and commutativity of function symbols, require traversing the terms to be simpliﬁed when applying the rewriting.
Since process visits terms in postorder, the complexity of the simpliﬁcations would be
quadratic, while a processing that applies depth-ﬁrst preorder traversal can perform the
simpliﬁcations with a linear complexity. Hence, applying such transformations optimally
is also outside the scope of the framework.
Theoretical Properties
The ﬁrst two metatheoretical results below concern the soundness of the inference rules
and the correctness of the recursion algorithm that generates proofs in that system. The
other results have to do with the cost of proof generation and checking.
Theorem 1 (Soundness of Inferences). If judgment Γ ▷t ≃u is derivable using the
inference system with theories T1,...,Tn, then |=T1 ∪···∪Tn ∪≃∪ε∪let Γ(t) ≃u.
Theorem 2 (Total Correctness of Recursion). For the instances presented in Sect. 3,
the contextual recursion algorithm always produces correct proofs.
Observation 3 (Complexity of Recursion). For the instances presented in Sect. 3, the
‘process’ function is called at most once on every subterm of the input.
As a corollary, if all the operations performed in process excluding the recursive calls
can be accomplished in constant time, the algorithm has linear-time complexity with
respect to the input. There exist data structures for which the following operations take
constant time: extending the context with a ﬁxed variable or a substitution, accessing
direct subterms of a term, building a term from its direct subterms, choosing a fresh
variable, applying a context to a variable, checking if a term matches a simple template,
and associating the parameters of the template with the subterms. Thus, it is possible to
have a linear-time algorithm for ‘let’ expansion and simpliﬁcation. On the other hand,
skolemization is at best quadratic in the worst case.
Observation 4 (Overhead of Proof Generation). For the instances presented in Sect.3,
the number of ‘apply’ calls is proportional to the number of subterms in the input.
Notice that all arguments to apply must be computed regardless of the apply calls.
If an apply call takes constant time, the proof generation overhead is linear in the size
of the input. To achieve this performance, it is necessary to use sharing to represent
contexts and terms in the output.
Observation 5 (Cost of Proof Checking). Checking an inference step can be performed in constant time if checking the side condition takes constant time.
The above statement may appear weak, since checking the side conditions might
itself be linear, leading to a cost of proof checking that can be at least quadratic in the
size of the proof. Fortunately, most of the side conditions can be checked efﬁciently. For
example, simpliﬁcation proofs can be checked in linear time because subst(Γ) is always
the identity. Moreover, certifying a proof by checking each step locally is not the only
possibility. An alternative is to use an algorithm similar to the process function to check
a proof in the same way as it has been produced, exploiting sophisticated invariants.
Implementation
The ideas presented in this paper have been implemented in two tools. We implemented
the contextual recursion algorithm and the transformations described in Sect. 3 in the
SMT solver veriT , showing that replacing the previous ad hoc code with the generic
proof-producing framework had no signiﬁcant detrimental impact on the solving times.
In addition, we developed a prototypical proof checker for the inference system described
in Sect. 2 using Isabelle/HOL , to convince ourselves that veriT’s output can easily
be reconstructed.
Isabelle The Isabelle/HOL proof assistant is based on classical higher-order logic
(HOL), a variant of the simply typed λ-calculus. The proof checker is included in the
development version of Isabelle.1
Derivations are represented by a recursive datatype in Standard ML, Isabelle’s
primary implementation language. A derivation is a tree whose nodes are labeled by rule
names. Rule TAUTT also carries a theorem that represents the oracle |=T , and rules
TRANS and LET are labeled with the terms that occur only in the antecedent (t and ¯sn).
Judgments Γ ▷t ≃u are translated to HOL equalities t′ ≃u′, where t′ and u′ are HOL
terms in which the context Γ is encoded using λ-abstractions and (for substitutions)
applications. For example, the judgment x, y 7→g(x) ▷f(y) ≃f(g(x)) is represented by
the HOL equality (λx.(λy.f y)(g x)) ≃(λx.f (g x)).
Because reconstruction is not veriﬁed, there are no guarantees that it will always
succeed, but when it does, the result is certiﬁed by Isabelle’s LCF-style inference
kernel . We hard-coded a few dozen examples to test different cases, such as this
one: Given the HOL terms
t = ¬∀x. p ∧∃x. ∀x. q x x
u = ¬∀x. p ∧∃x. q(εx.¬q x x)(εx.¬q x x)
and the ML tree
N (Cong,[N (Bind,[N (Cong,[N (Reﬂ,[]),N (Bind,[N (Sko_All,[N (Reﬂ,[])])])])])]))
the reconstruction function returns the HOL theorem t ≃u.
veriT We implemented the contextual recursion framework in the SMT solver veriT,2
replacing large parts of the previous non-proof-producing, hard-to-maintain code. Even
though it offers more functionality (proof generation), the preprocessing module is about
20% smaller than before and consists of about 3000 lines of code. There are now only
two traversal functions instead of 10. This is, for us, a huge gain in maintainability.
We were able to reuse its existing proof module and proof format . A proof is a list
of inferences, each of which consists of an identiﬁer, the name of the rule, the identiﬁers
of the dependencies, and the derived clause. The use of identiﬁers makes it possible to
represent proofs as DAGs. We extended the format with the inference rules of Sect. 2.
The rules that augment the context take a sequence of inferences—a subproof—as a
justiﬁcation. The subproof occurs within the scope of the extended context.
In contrast with the abstract proof module described in Sect. 3, veriT leaves REFL
steps implicit for judgments of the form Γ ▷t ≃t. The other inference rules are generalized to cope with missing REFL judgments. In addition, when printing proofs, the proof
module can automatically replace terms in the inferences with some other terms. This is
necessary for transformations such as skolemization and ‘if–then–else’ elimination. We
1 
Preprocessing.thy
2 
must apply a substitution in the replaced term if the original term contains variables. In
veriT, efﬁcient data structures are available to perform this.
The implementation of contextual recursion uses a single global context, augmented
before processing a subterm and restored afterwards. The context consists of a set of
ﬁxed variables, a substitution, and a polarity. In our setting, the substitution satisﬁes
the side conditions by construction. If the context is empty, the result of processing a
subterm is cached. For skolemization, a separate cache is used for each polarity. No
caching is attempted under binders.
Invoking process on a term returns the identiﬁer of the inference at the root of its
transformation proof in addition to the processed term. These identiﬁers are threaded
through the recursion to connect the proof. The proofs produced by instances of contextual recursion are inserted into the larger resolution proof produced by veriT.
Transformations performing theory simpliﬁcation were straightforward to port to the
new framework: Their build_app functions simply apply rewrite rules until a ﬁxpoint
is reached. Porting transformations that interact with binders required special attention
in handling the context and producing proofs. Fortunately, most of these aspects are
captured by the inference system and the abstract contextual recursion framework, where
they can be studied independently of the implementation.
Some transformations are performed outside of the framework. Proofs of CNF transformation are expressed using the inference rules of veriT’s underlying SAT solver, so
that any tool that can reconstruct SAT proofs can also reconstruct these proofs. Simpliﬁcation based on associativity and commutativity of function symbols is implemented as a
dedicated procedure, for efﬁciency reasons. It currently produces coarse-grained proofs.
To evaluate the impact of the new contextual recursion algorithm and of producing
detailed proofs, we compare the performance of different conﬁgurations of veriT. Our
experimental data is available online.3 We distinguish three conﬁgurations. BASIC
only applies transformations for which the old code provided some (coarse-grained)
proofs. EXTENDED also applies transformations for which the old code did not provide
any proofs, whereas the new code provides detailed proofs. COMPLETE applies all
transformations available, regardless of whether they produce proofs.
More speciﬁcally, BASIC applies the transformations for ‘let’ expansion, skolemization, elimination of quantiﬁers based on one-point rules, elimination of ‘if–then–else’,
theory simpliﬁcation for rewriting n-ary symbols as binary, and elimination of equivalences and exclusive disjunctions with quantiﬁers in subterms. EXTENDED adds Boolean
and arithmetic simpliﬁcations to the transformations performed by BASIC. COMPLETE
performs global rewriting simpliﬁcations and symmetry breaking in addition to the
transformations in EXTENDED.
The evaluation relies on two main sets of benchmarks from SMT-LIB without
bit vectors and nonlinear arithmetic (currently not supported by veriT): the 20916
benchmarks in the quantiﬁer-free (QF) categories, and the 30250 benchmarks labeled
as unsatisﬁable in the non-QF categories. Our experiments were conducted on servers
equipped with two Intel Xeon E5-2630 v3 processors, with eight cores per processor,
and 126 GB of memory. Each run of the solver uses a single core. The time limit was set
to 30 s, a reasonable value for interactive use within a proof assistant.
3 
The table below shows the number of problems solved in total by each conﬁguration.
Without proofs
With proofs
Old code New code
Old code New code
These results indicate that the new generic contextual recursion algorithm and the
production of detailed proofs do not impact performance negatively compared with the
old code and coarse-grained proofs. Moreover, allowing Boolean and arithmetic simpli-
ﬁcations leads to some improvements. We expect that generating proofs for the global
transformations would lead to substantial improvements on quantiﬁer-free problems.
Related Work
Most automatic provers that support the TPTP syntax for problems generate proofs in
TSTP format . Like a veriT proof, a TSTP proof consists of a list of inferences. TSTP
does not mandate any inference system; the meaning of the rules and the granularity of
inferences vary across systems. For example, the E prover combines clausiﬁcation,
skolemization, and variable renaming into a single inference, whereas Vampire 
appears to cleanly separate preprocessing transformations. SPASS’s custom proof
format does not record preprocessing steps; reverse engineering is necessary to make
sense of its output, and optimizations ought to be disabled [6, Sect. 7.3].
Most SMT solvers can parse the SMT-LIB format, but each solver has its own
output syntax. Z3’s proofs can be quite detailed , but rewriting steps often combine
many rewrites rules. CVC4’s format is an instance of LF with Side Conditions
(LFSC) ; despite recent progress , neither skolemization nor quantiﬁer
instantiation are currently recorded in the proofs. Proof production in Fx7 is based
on an inference system whose formula processing fragment is subsumed by ours; for
example, skolemization is more ad hoc, and there is no explicit support for rewriting.
Conclusion
We presented a framework to represent and generate proofs of formula processing and
its implementation in veriT and Isabelle/HOL. The framework centralizes the delicate
issue of manipulating bound variables and substitutions soundly and efﬁciently, and it
is ﬂexible enough to accommodate many interesting transformations. Although it was
implemented in an SMT solver, there appears to be no intrinsic limitation that would
prevent its use in other kinds of ﬁrst-order, or even higher-order, automatic provers. The
framework covers many preprocessing techniques and can be part of a larger toolbox.
Detailed proofs have been a deﬁning feature of veriT for many years now. It now
produces more detailed justiﬁcations than ever, but there are still some global transformations for which the proofs are nonexistent or leave much to be desired. In particular,
supporting rewriting based on global assumptions would be essential for proof-producing
inprocessing, and symmetry breaking would be interesting in its own right.
Acknowledgment We thank Simon Cruanes for discussing many aspects of the framework with
us as it was emerging, and we thank Robert Lewis, Stephan Merz, Lawrence Paulson, Anders
Schlichtkrull, Mark Summerﬁeld, Sophie Tourret, and the anonymous reviewers for suggesting
many textual improvements. This research has been partially supported by the Agence nationale
de la recherche / Deutsche Forschungsgemeinschaft project SMArT (ANR-13-IS02-0001, STU
483/2-1) and by the European Union project SC2 (grant agreement No. 712689). The work has
also received funding from the European Research Council under the European Union’s Horizon
2020 research and innovation program (grant agreement No. 713999, Matryoshka). Experiments
presented in this paper were carried out using the Grid’5000 testbed ( 
fr/), supported by a scientiﬁc interest group hosted by Inria and including CNRS, RENATER,
and several universities as well as other organizations. A mirror of all the software and evaluation
data described in this paper is hosted by Zenodo.4