Proceedings of NAACL-HLT 2016, pages 332–342,
San Diego, California, June 12-17, 2016. c⃝2016 Association for Computational Linguistics
A Latent Variable Recurrent Neural Network
for Discourse Relation Language Models
Yangfeng Ji
Georgia Institute of Technology
Atlanta, GA 30308, USA
 
Gholamreza Haffari
Monash University
Clayton, VIC, Australia
gholamreza.haffari
@monash.edu
Jacob Eisenstein
Georgia Institute of Technology
Atlanta, GA 30308, USA
 
This paper presents a novel latent variable recurrent neural network architecture for jointly
modeling sequences of words and (possibly
latent) discourse relations between adjacent
sentences. A recurrent neural network generates individual words, thus reaping the beneﬁts of discriminatively-trained vector representations.
The discourse relations are represented with a latent variable, which can be
predicted or marginalized, depending on the
task. The resulting model can therefore employ a training objective that includes not only
discourse relation classiﬁcation, but also word
prediction. As a result, it outperforms state-ofthe-art alternatives for two tasks: implicit discourse relation classiﬁcation in the Penn Discourse Treebank, and dialog act classiﬁcation
in the Switchboard corpus. Furthermore, by
marginalizing over latent discourse relations
at test time, we obtain a discourse informed
language model, which improves over a strong
LSTM baseline.
Introduction
Natural language processing (NLP) has recently experienced a neural network “tsunami” . A key advantage of these neural architectures is that they employ discriminatively-trained
distributed representations, which can capture the
meaning of linguistic phenomena ranging from individual words to longer-range
linguistic contexts at the sentence level and beyond . Because they are discriminatively trained, these methods can learn representations that yield very accurate
predictive models .
However, in comparison with the probabilistic
graphical models that were previously the dominant
machine learning approach for NLP, neural architectures lack ﬂexibility. By treating linguistic annotations as random variables, probabilistic graphical models can marginalize over annotations that are
unavailable at test or training time, elegantly modeling multiple linguistic phenomena in a joint framework . But because these graphical models represent uncertainty for every element
in the model, adding too many layers of latent variables makes them difﬁcult to train.
In this paper, we present a hybrid architecture
that combines a recurrent neural network language
model with a latent variable model over shallow
discourse structure. In this way, the model learns
a discriminatively-trained distributed representation
of the local contextual features that drive word
choice at the intra-sentence level, using techniques
that are now state-of-the-art in language modeling .
However, the model
treats shallow discourse structure — speciﬁcally, the
relationships between pairs of adjacent sentences —
as a latent variable. As a result, the model can act
as both a discourse relation classiﬁer and a language
model. Speciﬁcally:
• If trained to maximize the conditional likelihood of the discourse relations, it outperforms state-of-the-art methods for both implicit discourse relation classiﬁcation in the
Penn Discourse Treebank and dialog act classiﬁcation in Switch-
board . The
model learns from both the discourse annotations as well as the language modeling objective, unlike previous recursive neural architectures that learn only from annotated discourse
relations .
• If the model is trained to maximize the joint
likelihood of the discourse relations and the
text, it is possible to marginalize over discourse
relations at test time, outperforming language
models that do not account for discourse structure.
In contrast to recent work on continuous latent
variables in recurrent neural networks , which require complex variational autoencoders to represent uncertainty over the latent variables, our model is simple to implement and train,
requiring only minimal modiﬁcations to existing recurrent neural network architectures that are implemented in commonly-used toolkits such as Theano,
Torch, and CNN.
We focus on a class of shallow discourse relations, which hold between pairs of adjacent sentences (or utterances). These relations describe how
the adjacent sentences are related: for example, they
may be in CONTRAST, or the latter sentence may offer an answer to a question posed by the previous
sentence. Shallow relations do not capture the full
range of discourse phenomena ,
but they account for two well-known problems: implicit discourse relation classiﬁcation in the Penn
Discourse Treebank, which was the 2015 CoNLL
shared task ; and dialog act classiﬁcation, which characterizes the structure of interpersonal communication in the Switchboard corpus , and is a key component of
contemporary dialog systems . Our model outperforms state-of-the-art alternatives for implicit discourse relation classiﬁcation
in the Penn Discourse Treebank, and for dialog act
classiﬁcation in the Switchboard corpus.
Background
Our model scaffolds on recurrent neural network
(RNN) language models , and
recent variants that exploit multiple levels of linguistic detail .
RNN Language Models
Let us denote token n in
a sentence t by yt,n ∈{1 . . . V }, and write yt =
{yt,n}n∈{1...Nt} to indicate the sequence of words in
sentence t. In an RNN language model, the probability of the sentence is decomposed as,
p(yt,n | yt,<n),
where the probability of each word yt,n is conditioned on the entire preceding sequence of words
yt,<n through the summary vector ht,n−1. This vector is computed recurrently from ht,n−2 and from
the embedding of the current word, Xyt,n−1, where
X ∈RK×V and K is the dimensionality of the word
embeddings. The language model can then be summarized as,
ht,n =f(Xyt,n, ht,n−1)
p(yt,n | yt,<n) =softmax (Woht,n−1 + bo) ,
where the matrix Wo ∈RV ×K deﬁnes the output
embeddings, and bo ∈RV is an offset. The function
f(·) is a deterministic non-linear transition function.
It typically takes an element-wise non-linear transformation (e.g., tanh) of a vector resulting from the
sum of the word embedding and a linear transformation of the previous hidden state.
The model as described thus far is identical to the
recurrent neural network language model (RNNLM)
of Mikolov et al. . In this paper, we replace
the above simple hidden state units with the more
complex Long Short-Term Memory units , which have consistently been shown to yield much stronger performance in language modeling .
For simplicity, we still use the term RNNLM in referring to this model.
Document Context Language Model
One drawback of the RNNLM is that it cannot propagate longrange information between the sentences. Even if
we remove sentence boundaries, long-range information will be attenuated by repeated application of
the non-linear transition function. Ji et al. 
propose the Document Context Language Model
(DCLM) to address this issue. The core idea is to
represent context with two vectors: ht,n, representing intra-sentence word-level context, and ct, representing inter-sentence context. These two vectors
yt−1, Nt−1−2
yt−1, Nt−1−1
yt−1, Nt−1
Figure 1: A fragment of our model with latent variable zt, which only illustrates discourse information ﬂow from sentence (t −1)
to t. The information from sentence (t −1) affects the distribution of zt and then the words prediction within sentence t.
p(yt,n+1 | zt, yt,<n, yt−1) = g
relation-speciﬁc
intra-sentential context
relation-speciﬁc
inter-sentential context
relation-speciﬁc
Figure 2: Per-token generative probabilities in the discourse relation language model
are then linearly combined in the generation function for word yt,n,
p(yt,n | yt,<n, y<t)
= softmax (Woht,n−1 + Wcct−1 + bo) ,
where ct−1 is set to the last hidden state of the previous sentence. Ji et al. show that this model
can improve language model perplexity.
Discourse Relation Language Models
We now present a probabilistic neural model over
sequences of words and shallow discourse relations.
Discourse relations zt are treated as latent variables,
which are linked with a recurrent neural network
over words in a latent variable recurrent neural network .
Our model (see Figure 1) is formulated as a two-step
generative story. In the ﬁrst step, context information from the sentence (t−1) is used to generate the
discourse relation between sentences (t −1) and t,
p(zt | yt−1) = softmax (Uct−1 + b) ,
where zt is a random variable capturing the discourse relation between the two sentences, and ct−1
is a vector summary of the contextual information
from sentence (t −1), just as in the DCLM (Equation 5). The model maintains a default context vector c0 for the ﬁrst sentences of documents, and treats
it as a parameter learned with other model parameters during training.
In the second step, the sentence yt is generated,
conditioning on the preceding sentence yt−1 and the
discourse relation zt:
p(yt | zt, yt−1) =
p(yt,n | yt,<n, yt−1, zt), (7)
The generative probability for the sentence yt decomposes across tokens as usual (Equation 7). The
per-token probabilities are shown in Equation 4, in
Figure 2. Discourse relations are incorporated by parameterizing the output matrices W(zt)
depending on the discourse relation that holds between (t −1) and t, these matrices will favor different parts of the embedding space. The bias term
is also parametrized by the discourse relation,
so that each relation can favor speciﬁc words.
Overall, the joint probability of the text and discourse relations is,
p(y1:T , z1:T ) =
p(zt | yt−1) × p(yt | zt, yt−1).
If the discourse relations zt are not observed, then
our model is a form of latent variable recurrent neural network (LVRNN). Connections to recent work
on LVRNNs are discussed in § 6; the key difference
is that the latent variables here correspond to linguistically meaningful elements, which we may wish to
predict or marginalize, depending on the situation.
Parameter Tying
As proposed, the Discourse Relation Language Model has a large number of parameters. Let K, H and V be the input dimension,
hidden dimension and the size of vocabulary in language modeling. The size of each prediction matrix
is V × H; there are two such matrices for each possible discourse relation. We reduce
the number of parameters by factoring each of these
matrices into two components:
= Wo · V(z),
= Wc · M(z),
where V(z) and M(z) are relation-speciﬁc components for intra-sentential and inter-sentential contexts; the size of these matrices is H × H, with
H ≪V . The larger V × H matrices Wo and Wc
are shared across all relations.
There are two possible inference scenarios:
inference over discourse relations, conditioning on
words; and inference over words, marginalizing over
discourse relations.
Inference over Discourse Relations
The probability of discourse relations given the sentences
p(z1:T | y1:T ) is decomposed into the product of
probabilities of individual discourse relations conditioned on the adjacent sentences Q
t p(zt | yt, yt−1).
These probabilities are computed by Bayes’ rule:
p(zt | yt, yt−1) =
p(yt | zt, yt−1) × p(zt | yt−1)
z′ p(yt | z′, yt−1) × p(z′ | yt−1).
The terms in each product are given in Equations 6
and 7. Normalizing involves only a sum over a small
ﬁnite number of discourse relations. Note that inference is easy in our case because all words are observed and there is no probabilistic coupling of the
discourse relations.
Inference over Words
In discourse-informed language modeling, we marginalize over discourse relations to compute the probability of a sequence of
sentence y1:T , which can be written as,
p(y1:T ) =
p(zt | yt−1) × p(yt | zt, yt−1),
because the word sequences are observed, decoupling each zt from its neighbors zt+1 and zt−1.
This decoupling ensures that we can compute the
overall marginal likelihood as a product over local
marginals.
The model can be trained in two ways: to maximize
the joint probability p(y1:T , z1:T ), or to maximize
the conditional probability p(z1:T | y1:T ). The joint
training objective is more suitable for language modeling scenarios, and the conditional objective is better for discourse relation prediction. We now describe each objective in detail.
Joint likelihood objective
The joint likelihood
objective function is directly adopted from the joint
probability deﬁned in Equation 8.
The objective
function for a single document with T sentences or
utterances is,
log p(zt | yt−1)
log p(yt,n | yt,<n, yt−1, zt),
where θ represents the collection of all model parameters, including the parameters in the LSTM
units and the word embeddings.
Maximizing the objective function ℓ(θ) will
jointly optimize the model on both language language and discourse relation prediction. As such,
it can be viewed as a form of multi-task learning , where we learn a shared representation that works well for discourse relation
prediction and for language modeling. However, in
practice, the large vocabulary size and number of tokens means that the language modeling part of the
objective function tends to dominate.
Conditional objective
This training objective is
speciﬁc to the discourse relation prediction task, and
based on Equation 10 can be written as:
log p(zt | yt−1) + log p(yt | zt, yt−1)
p(z′ | yt−1) × p(yt | z′, yt−1)
The ﬁrst line in Equation 13 is the same as ℓ(θ),
but the second line reﬂects the normalization over all
possible values of zt. This forces the objective function to attend speciﬁcally to the problem of maximizing the conditional likelihood of the discourse
relations and treat language modeling as an auxiliary task .
Modeling limitations
The discourse relation language model is carefully
designed to decouple the discourse relations from
each other, after conditioning on the words.
is clear that text documents and spoken dialogues
have sequential discourse structures, and it seems
likely that modeling this structure could improve
performance. In a traditional hidden Markov model
(HMM) generative approach ,
modeling sequential dependencies is not difﬁcult,
because training reduces to relative frequency estimation.
However, in the hybrid probabilisticneural architecture proposed here, training is already expensive, due to the large number of parameters that must be estimated.
Adding probabilistic couplings between adjacent discourse relations
⟨zt−1, zt⟩would require the use of dynamic programming for both training and inference, increasing time complexity by a factor that is quadratic in
the number of discourse relations. We did not attempt this in this paper; we do compare against a
conventional HMM on the dialogue act prediction
task in § 5.
Ji et al. propose an alternative form of
the document context language model, in which the
contextual information ct impacts the hidden state
ht+1, rather than going directly to the outputs yt+1.
They obtain slightly better perplexity with this approach, which has fewer trainable parameters. However, this model would couple zt with all subsequent
sentences y>t, making prediction and marginalization of discourse relations considerably more challenging. Sequential Monte Carlo algorithms offer a
possible solution ,
which may be considered in future work.
Data and Implementation
We evaluate our model on two benchmark datasets:
(1) the Penn Discourse Treebank , which is annotated on a corpus of
Wall Street Journal acticles; (2) the Switchboard dialogue act corpus ,
which is annotated on a collections of phone conversations. Both corpora contain annotations of discourse relations and dialogue relations that hold between adjacent spans of text.
The Penn Discourse Treebank (PDTB)
a low-level discourse annotation on written texts. In
the PDTB, each discourse relation is annotated between two argument spans, Arg1 and Arg2. There
are two types of relations: explicit and implicit.
Explicit relations are signalled by discourse markers (e.g., “however”, “moreover”), and the span of
Arg1 is almost totally unconstrained: it can range
from a single clause to an entire paragraph, and
need not be adjacent to either Arg2 nor the discourse marker. However, automatically classifying
these relations is considered to be relatively easy,
due to the constraints from the discourse marker itself . In addition, explicit relations are difﬁcult to incorporate into language models which must generate each word exactly once. On
the contrary, implicit discourse relations are annotated only between adjacent sentences, based on a
semantic understanding of the discourse arguments.
Automatically classifying these discourse relations
is a challenging task . We therefore focus on implicit discourse relations, leaving to the future work the question of
how to apply our modeling framework to explicit
discourse relations. During training, we collapse all
relation types other than implicit (explicit, ENTREL,
and NOREL) into a single dummy relation type,
which holds between all adjacent sentence pairs that
do not share an implicit relation.
As in the prior work on ﬁrst-level discourse relation identiﬁcation ,
we use sections 2-20 of the PDTB as the training
set, sections 0-1 as the development set for parameter tuning, and sections 21-22 for testing. For preprocessing, we lower-cased all tokens, and substituted all numbers with a special token “NUM”. To
build the vocabulary, we kept the 10,000 most frequent words from the training set, and replaced lowfrequency words with a special token “UNK”. In
prior work that focuses on detecting individual relations, balanced training sets are constructed so that
there are an equal number of instances with and
without each relation type .
In this paper, we target the more challenging multiway classiﬁcation problem, so this strategy is not applicable; in any case, since our method deals with
entire documents, it is not possible to balance the
training set in this way.
The Switchboard Dialog Act Corpus (SWDA)
is annotated on the Switchboard Corpus of humanhuman conversational telephone speech .
The annotations label each utterance
with one of 42 possible speech acts, such as AGREE,
HEDGE, and WH-QUESTION. Because these speech
acts form the structure of the dialogue, most of them
pertain to both the preceding and succeeding utterances (e.g., AGREE). The SWDA corpus includes
1155 ﬁve-minute conversations.
We adopted the
standard split from Stolcke et al. , using 1,115
conversations for training and nineteen conversations for test. For parameter tuning, we randomly
select nineteen conversations from the training set
as the development set. After parameter tuning, we
train the model on the full training set with the selected conﬁguration. We use the same preprocessing
techniques here as in the PDTB.
Implementation
We use a single-layer LSTM to build the recurrent architecture of our models, which we implement in the CNN package.1
Our implementation is available on 
jiyfeng/drlm. Some additional details follow.
Initialization
Following prior work on RNN initialization , all parameters except
the relation prediction parameters U and b are initialized with random values drawn from the range
6/(d1 + d2),
6/(d1 + d2)], where d1 and d2
are the input and output dimensions of the parameter matrix respectively. The matrix U is initialized
with random numbers from [−10−5, 10−5] and b is
initialized to 0.
Online learning was performed using
AdaGrad with initial learning
1 
rate λ = 0.1. To avoid the exploding gradient problem, we used norm clipping trick with a threshold of
τ = 5.0 . In addition, we used
value dropout with rate 0.5,
on the input X, context vector c and hidden state h,
similar to the architecture proposed by Pham et al.
 . The training procedure is monitored by the
performance on the development set. In our experiments, 4 to 5 epochs were enough.
Hyper-parameters
Our model includes two tunable hyper-parameters: the dimension of word representation K, the hidden dimension of LSTM unit
H. We consider the values {32, 48, 64, 96, 128} for
both K and H. For each corpus in experiments, the
best combination of K and H is selected via grid
search on the development set.
Experiments
Our main evaluation is discourse relation prediction,
using the PDTB and SWDA corpora. We also evaluate on language modeling, to determine whether
incorporating discourse annotations at training time
and then marginalizing them at test time can improve performance.
Implicit discourse relation prediction on
We ﬁrst evaluate our model with implicit discourse
relation prediction on the PDTB dataset. Most of the
prior work on ﬁrst-level discourse relation prediction focuses on the “one-versus-all” binary classiﬁcation setting, but we attack the more general fourway classiﬁcation problem, as performed by Rutherford and Xue . We compare against the following methods:
Rutherford and Xue build a set of featurerich classiﬁers on the PDTB, and then augment
these classiﬁers with additional automaticallylabeled training instances. We compare against
their published results, which are state-of-the-art.
Ji and Eisenstein employ a recursive neural
network architecture. Their experimental setting
is different, so we re-run their system using the
same setting as described in § 4.
1. Most common class
Prior work
2. 
3. 
with extra training data
4. 
Our work - DRLM
5. Joint training
6. Conditional training
∗signiﬁcantly better than lines 2 and 4 with p < 0.05
Table 1: Multiclass relation identiﬁcation on the ﬁrst-level
PDTB relations.
As shown in Table 1, the conditionallytrained discourse relation language models (DRLM)
outperforms all alternatives, on both metrics. While
the jointly-trained DRLM is at the same level as the
previous state-of-the-art, conditional training on the
same model provides a signiﬁcant additional advantage, indicated by a binomial test.
Dialogue Act tagging
Dialogue act tagging has been widely studied in both
NLP and speech communities. We follow the setup
used by Stolcke et al. to conduct experiments,
and adopt the following systems for comparison:
Stolcke et al. employ a hidden Markov
model, with each HMM state corresponding to a
dialogue act.
Kalchbrenner and Blunsom employ a
complex neural architecture, with a convolutional
network at each utterance and a recurrent network over the length of the dialog. To our knowledge, this model attains state-of-the-art accuracy
on this task, outperforming other prior work such
as .
As shown in Table 2, the conditionallytrained discourse relation language model (DRLM)
outperforms all competitive systems on this task. A
binomial test shows the result in line 6 is signiﬁcantly better than the previous state-of-the-art (line
4). All comparisons are against published results,
and Macro-F1 scores are not available. Accuracy
2. Most common class
Prior work
3. 
 , which was shown to outperform a Kneser-
Ney smoothed 5-gram model on modeling Wall
Street Journal text. Following Pham et al. ,
we replace the Sigmoid nonlinearity with a long
short-term memory (LSTM).
DCLM We compare against the Document Context
Language Model (DCLM) of Ji et al. . We
use the “context-to-output” variant, which is identical to the current modeling approach, except that
it is not parametrized by discourse relations. This
model achieves strong results on language modeling for small and medium-sized corpora, outperforming RNNLM+LSTM.
The perplexities of language modeling on
the PDTB and the SWDA are summarized in Table 3.
The comparison between line 1 and line
2 shows the beneﬁt of considering multi-sentence
context information on language modeling.
3 shows that adding discourse relation information
Table 3: Language model perplexities (PPLX), lower is better.
The model dimensions K and H that gave best performance on
the dev set are also shown.
yields further improvements for both datasets. We
emphasize that discourse relations in the test documents are marginalized out, so no annotations are
required for the test set; the improvements are due
to the disambiguating power of discourse relations
in the training set.
Because our training procedure requires discourse
annotations, this approach does not scale to the large
datasets typically used in language modeling. As a
consequence, the results obtained here are somewhat
academic, from the perspective of practical language
modeling. Nonetheless, the positive results here motivate the investigation of training procedures that
are also capable of marginalizing over discourse relations at training time.
Related Work
This paper draws on previous work in both discourse
modeling and language modeling.
Discourse and dialog modeling
Early work on
discourse relation classiﬁcation utilizes rich, handcrafted feature sets . Recent representation learning approaches attempt to learn good representations
jointly with discourse relation classiﬁers and discourse parsers . Of particular relevance are applications of
neural architectures to PDTB implicit discourse relation classiﬁcation . All of these
approaches are essentially classiﬁers, and take supervision only from the 16,000 annotated discourse
relations in the PDTB training set. In contrast, our
approach is a probabilistic model over the entire text.
Probabilistic models are frequently used in dialog act tagging, where hidden Markov models have
been a dominant approach . In
this work, the emission distribution is an n-gram
language model for each dialogue act; we use a
conditionally-trained recurrent neural network language model. An alternative neural approach for dialogue act tagging is the combined convolutionalrecurrent architecture of Kalchbrenner and Blunsom
 . Our modeling framework is simpler, relying on a latent variable parametrization of a purely
recurrent architecture.
Language modeling
There are an increasing
number of attempts to incorporate document-level
context information into language modeling. For example, Mikolov and Zweig introduce LDAstyle topics into RNN based language modeling.
Sordoni et al. use a convolutional structure
to summarize the context from previous two utterances as context vector for RNN based language
modeling. Our models in this paper provide a uni-
ﬁed framework to model the context and current sentence. Wang and Cho and Lin et al. 
construct bag-of-words representations of previous
sentences, which are then used to inform the RNN
language model that generates the current sentence.
The most relevant work is the Document Context
Language Model ; we describe the connection to this model in § 2. By adding
discourse information as a latent variable, we attain
better perplexity on held-out data.
Latent variable neural networks
Introducing latent variables to a neural network model increases
its representational capacity, which is the main goal
of prior efforts in this space . From this perspective,
our model with discourse relations as latent variables shares the same merit. Unlike this prior work,
in our approach, the latent variables carry a linguistic interpretation, and are at least partially observed. Also, these prior models employ continuous
latent variables, requiring complex inference techniques such as variational autoencoders . In contrast, the discrete latent variables in our
model are easy to sum and maximize over.
Conclusion
We have presented a probabilistic neural model
over sequences of words and shallow discourse relations between adjacent sequences.
This model
combines positive aspects of neural network architectures with probabilistic graphical models: it
can learn discriminatively-trained vector representations, while maintaining a probabilistic representation of the targeted linguistic element: in this case,
shallow discourse relations.
This method outperforms state-of-the-art systems in two discourse relation detection tasks, and can also be applied as a language model, marginalizing over discourse relations
on the test data. Future work will investigate the
possibility of learning from partially-labeled training data, which would have at least two potential advantages. First, it would enable the model to scale up
to the large datasets needed for competitive language
modeling. Second, by training on more data, the
resulting vector representations might support even
more accurate discourse relation prediction.
Acknowledgments
Thanks to Trevor Cohn, Chris Dyer, Lingpeng Kong,
and Quoc V. Le for helpful discussions, and to the
anonymous reviewers for their feedback. This work
was supported by a Google Faculty Research award
to the third author. It was partially performed during the 2015 Jelinek Memorial Summer Workshop
on Speech and Language Technologies at the University of Washington, Seattle, and was supported
by Johns Hopkins University via NSF Grant No IIS
1005411, DARPA LORELEI Contract No HR0011-
15-2-0027, and gifts from Google, Microsoft Research, Amazon and Mitsubishi Electric Research
Laboratory.