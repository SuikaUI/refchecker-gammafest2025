Int J Comput Vis 76: 53–69
DOI 10.1007/s11263-007-0071-y
3-D Depth Reconstruction from a Single Still Image
Ashutosh Saxena · Sung H. Chung · Andrew Y. Ng
Received: 1 November 2006 / Accepted: 6 June 2007 / Published online: 16 August 2007
© Springer Science+Business Media, LLC 2007
Abstract We consider the task of 3-d depth estimation
from a single still image. We take a supervised learning approach to this problem, in which we begin by collecting a
training set of monocular images (of unstructured indoor
and outdoor environments which include forests, sidewalks,
trees, buildings, etc.) and their corresponding ground-truth
depthmaps. Then, we apply supervised learning to predict
the value of the depthmap as a function of the image. Depth
estimation is a challenging problem, since local features
alone are insufﬁcient to estimate depth at a point, and one
needs to consider the global context of the image. Our
model uses a hierarchical, multiscale Markov Random Field
(MRF) that incorporates multiscale local- and global-image
features, and models the depths and the relation between
depths at different points in the image. We show that, even
on unstructured scenes, our algorithm is frequently able to
recover fairly accurate depthmaps. We further propose a
model that incorporates both monocular cues and stereo (triangulation) cues, to obtain signiﬁcantly more accurate depth
estimates than is possible using either monocular or stereo
cues alone.
Keywords Monocular vision · Learning depth · 3D
reconstruction · Dense reconstruction · Markov random
ﬁeld · Depth estimation · Monocular depth · Stereo vision ·
Hand-held camera · Visual modeling
A. Saxena () · S.H. Chung · A.Y. Ng
Computer Science Department, Stanford University, Stanford,
CA 94305, USA
e-mail: 
S.H. Chung
e-mail: 
e-mail: 
1 Introduction
Recovering 3-d depth from images is a basic problem in
computer vision, and has important applications in robotics, scene understanding and 3-d reconstruction. Most work
on visual 3-d reconstruction has focused on binocular vision (stereopsis) and on
other algorithms that require multiple images, such as structure from motion and depth
from defocus . These algorithms consider only the geometric (triangulation) differences. Beyond
stereo/triangulation cues, there are also numerous monocular cues—such as texture variations and gradients, defocus,
color/haze, etc.—that contain useful and important depth information. Even though humans perceive depth by seamlessly combining many of these stereo and monocular cues,
most work on depth estimation has focused on stereovision.
Depth estimation from a single still image is a difﬁcult
task, since depth typically remains ambiguous given only
local image features. Thus, our algorithms must take into account the global structure of the image, as well as use prior
knowledge about the scene. We also view depth estimation
as a small but crucial step towards the larger goal of image
understanding, in that it will help in tasks such as understanding the spatial layout of a scene, ﬁnding walkable areas in a scene, detecting objects, etc. In this paper, we apply
supervised learning to the problem of estimating depthmaps
(Fig. 1b) from a single still image (Fig. 1a) of a variety of
unstructured environments, both indoor and outdoor, containing forests, sidewalks, buildings, people, bushes, etc.
Our approach is based on modeling depths and relationships between depths at multiple spatial scales using a hierarchical, multiscale Markov Random Field (MRF). Taking a supervised learning approach to the problem of depth
estimation, we used a 3-d scanner to collect training data,
Int J Comput Vis 76: 53–69
Fig. 1 a A single still image, and b the corresponding (ground-truth)
depthmap. Colors in the depthmap indicate estimated distances from
the camera
which comprised a large set of images and their corresponding ground-truth depthmaps. (This data has been made publically available on the Internet.) Using this training set, we
model the conditional distribution of the depths given the
monocular image features. Though learning in our MRF
model is approximate, MAP inference is tractable via linear programming.
We further consider how monocular cues from a single
image can be incorporated into a stereo system. We believe that monocular cues and (purely geometric) stereo cues
give largely orthogonal, and therefore complementary, types
of information about depth. We show that combining both
monocular and stereo cues gives better depth estimates than
is obtained with either alone.
We also apply these ideas to autonomous obstacle avoidance. Using a simpliﬁed version of our algorithm, we drive
a small remote-controlled car at high speeds through various
unstructured outdoor environments containing both manmade and natural obstacles.
This paper is organized as follows. Section 2 gives an
overview of various methods used for 3-d depth reconstruction. Section 3 describes some of the visual cues used by
humans for depth perception, and Sect. 4 describes the image features used to capture monocular cues. We describe
our probabilistic model in Sect. 5. In Sect. 6.1, we describe
our setup for collecting aligned image and laser data. The
results of depth prediction on single images are presented
in Sect. 6.2. Section 6.2 also describes the use of a simpliﬁed version of our algorithm in driving a small remotecontrolled car autonomously. We describe how we incorporate monocular and stereo cues into our model in Sect. 7.
Finally, we conclude in Sect. 8.
2 Related Work
Although our work mainly focuses on depth estimation from
a single still image, there are many other 3-d reconstruction techniques, such as: explicit measurements with laser
or radar sensors , using two (or
more than two) images , and
using video sequences . Among the
vision-based approaches, most work has focused on stereovision ,
and on other algorithms that require multiple images, such
as optical ﬂow , structure from motion
 and depth from defocus . Frueh and Zakhor constructed 3d
city models by merging ground-based and airborne views.
A large class of algorithms reconstruct the 3-d shape of
known objects, such as human bodies, from images and
laser data .
Structured lighting offers another method for depth reconstruction.
There are some algorithms that can perform depth reconstruction from single images in very speciﬁc settings. Nagai
et al. performed surface reconstruction from single
images for known, ﬁxed, objects such as hands and faces.
Methods such as shape from shading and shape from texture generally assume uniform color and/or texture,1
and hence would perform very poorly on the complex, unconstrained, highly textured images that we consider. Hertzmann and Seitz reconstructed high quality 3-d models from several images, but they required that the images
also contain “assistant” objects of known shapes next to the
target object. Torresani and Hertzmann worked on reconstructing non-rigid surface shapes from video sequences.
Torralba and Oliva studied the Fourier spectrum of
the images to compute the mean depth of a scene. Michels
et al. used supervised learning to estimate 1-d distances to obstacles, for the application of autonomously
driving a small car. Delage et al. generated 3-d
models of indoor environments containing only walls and
ﬂoor, from single monocular images. Single view metrology assumes that vanishing lines and
points are known in a scene, and calculates angles between
parallel lines to infer 3-d structure from Manhattan images.
We presented a method for learning depths from a single image in and extended our method
to improve stereo vision using monocular cues in . In work that is contemporary to ours, Hoiem
et al. built a simple “pop-up” type 3-d model
from an image by classifying the image into ground, vertical
and sky. Their method, which assumes a simple “groundvertical” structure of the world, fails on many environments
1Also, most of these algorithms assume Lambertian surfaces, which
means the appearance of the surface does not change with viewpoint.
Int J Comput Vis 76: 53–69
that do not satisfy this assumption and also does not give accurate metric depthmaps. Building on these concepts of single image 3-d reconstruction, Hoiem et al. and Sudderth et al. integrated learning-based object recognition with 3-d scene representations. Saxena et al. 
extended these ideas to create 3-d models that are both visually pleasing as well as quantitatively accurate.
Our approach draws on a large number of ideas from
computer vision such as feature computation and multiscale
representation of images. A variety of image features and
representations have been used by other authors, such as
Gabor ﬁlters , wavelets , SIFT features , etc.
Many of these image features are used for purposes such as
recognizing objects ,
faces , facial expressions , grasps ; image segmentation
 , computing the visual gist of a
scene and computing sparse representations of natural images .
Stereo and monocular image features have been used together for object recognition and image segmentation .
Our approach is based on learning a Markov Random
Field (MRF) model. MRFs are a workhorse of machine
learning, and have been successfully applied to numerous problems in which local features were insufﬁcient and
more contextual information had to be used. Examples include image denoising , stereo vision and image segmentation ,
text segmentation , object classiﬁcation
 , and image labeling .
For the application of identifying man-made structures in
natural images, Kumar and Hebert used a discriminative random ﬁelds algorithm . Since MRF
learning is intractable in general, most of these models are
trained using pseudo-likelihood; sometimes the models’ parameters are also hand-tuned.
3 Visual Cues for Depth Perception
Humans use numerous visual cues to perceive depth. Such
cues are typically grouped into four distinct categories:
monocular, stereo, motion parallax, and focus cues . Humans combine these cues to understand the 3-d structure of the world . Below, we describe these cues in more detail. Our probabilistic
model will attempt to capture a number of monocular cues
(Sect. 5), as well as stereo triangulation cues (Sect. 7).
3.1 Monocular Cues
Humans use monocular cues such as texture variations, texture gradients, interposition, occlusion, known object sizes,
light and shading, haze, defocus, etc. 
For example, many objects’ texture will look different at different distances from the viewer. Texture gradients, which
capture the distribution of the direction of edges, also help
to indicate depth . For example,
a tiled ﬂoor with parallel lines will appear to have tilted lines
in an image. The distant patches will have larger variations
in the line orientations, and nearby patches with almost parallel lines will have smaller variations in line orientations.
Similarly, a grass ﬁeld when viewed at different distances
will have different texture gradient distributions. Haze is another depth cue, and is caused by atmospheric light scattering .
Many monocular cues are “contextual information”, in
the sense that they are global properties of an image and
cannot be inferred from small image patches. For example,
occlusion cannot be determined if we look at just a small
portion of an occluded object. Although local information
such as the texture and color of a patch can give some information about its depth, this is usually insufﬁcient to accurately determine its absolute depth. For another example,
if we take a patch of a clear blue sky, it is difﬁcult to tell if
this patch is inﬁnitely far away (sky), or if it is part of a blue
object. Due to ambiguities like these, one needs to look at
the overall organization of the image to determine depths.
3.2 Stereo Cues
Each eye receives a slightly different view of the world and
stereo vision combines the two views to perceive 3-d depth
 . An object is projected onto different locations on the two retinae (cameras in the case of a stereo
system), depending on the distance of the object. The retinal (stereo) disparity varies with object distance, and is inversely proportional to the distance of the object. Disparity
is typically not an effective cue for estimating small depth
variations of objects that are far away.
3.3 Motion Parallax and Focus Cues
As an observer moves, closer objects appear to move more
than further objects. By observing this phenomenon, called
motion parallax, one can estimate the relative distances in
a scene . Humans have the ability to
change the focal lengths of the eye lenses by controlling
the curvature of lens, thus helping them to focus on objects at different distances. The focus, or accommodation,
cue refers to the ability to estimate the distance of an object
from known eye lens conﬁguration and the sharpness of the
image of the object .
Int J Comput Vis 76: 53–69
Fig. 2 The convolutional ﬁlters used for texture energies and gradients. The ﬁrst nine are 3 × 3 Laws’ masks. The last six are the oriented edge
detectors spaced at 30° intervals. The nine Laws’ masks are used to perform local averaging, edge detection and spot detection
Fig. 3 The absolute depth
feature vector for a patch, which
includes features from its
immediate neighbors and its
more distant neighbors (at larger
scales). The relative depth
features for each patch use
histograms of the ﬁlter outputs
4 Feature Vector
In our approach, we divide the image into small rectangular
patches, and estimate a single depth value for each patch.
We use two types of features: absolute depth features—used
to estimate the absolute depth at a particular patch—and
relative features, which we use to estimate relative depths
(magnitude of the difference in depth between two patches).
These features try to capture two processes in the human
visual system: local feature processing (absolute features),
such as that the sky is far away; and continuity features
(relative features), a process by which humans understand
whether two adjacent patches are physically connected in
3-d and thus have similar depths.2
We chose features that capture three types of local cues:
texture variations, texture gradients, and color. Texture information is mostly contained within the image intensity
channel ,3 so we apply Laws’ masks to this channel to compute the
texture energy (Fig. 2). Haze is reﬂected in the low frequency information in the color channels, and we capture
this by applying a local averaging ﬁlter (the ﬁrst Laws’
mask) to the color channels. Lastly, to compute an estimate
2If two neighboring patches of an image display similar features, humans would often perceive them to be parts of the same object, and
therefore to have similar depth values.
3We represent each image in YCbCr color space, where Y is the intensity channel, and Cb and Cr are the color channels.
of texture gradient that is robust to noise, we convolve the
intensity channel with six oriented edge ﬁlters could also be included.
Similarly, one can also include features based on surfaceshading .
4.1 Features for Absolute Depth
We ﬁrst compute summary statistics of a patch i in the image I(x,y) as follows. We use the output of each of the 17
(9 Laws’ masks, 2 color channels and 6 texture gradients) ﬁlters Fn, n = 1,...,17 as: Ei(n) = 
(x,y)∈patch(i) |I ∗Fn|k,
where k ∈{1,2} give the sum absolute energy and sum
squared energy respectively.4 This gives us an initial feature
vector of dimension 34.
To estimate the absolute depth at a patch, local image
features centered on the patch are insufﬁcient, and one has
to use more global properties of the image. We attempt to
capture this information by using image features extracted
at multiple spatial scales (image resolutions).5 (See Fig. 3.)
4Our experiments using k ∈{1,2,4} did not improve performance noticeably.
5The patches at each spatial scale are arranged in a grid of equally sized
non-overlapping regions that cover the entire image. We use 3 scales
in our experiments.
Int J Comput Vis 76: 53–69
Objects at different depths exhibit very different behaviors
at different resolutions, and using multiscale features allows
us to capture these variations . For example, blue sky may appear similar at different scales, but textured grass would not. In addition to capturing more global
information, computing features at multiple spatial scales
also helps to account for different relative sizes of objects.
A closer object appears larger in the image, and hence will
be captured in the larger scale features. The same object
when far away will be small and hence be captured in the
small scale features. Features capturing the scale at which
an object appears may therefore give strong indicators of
To capture additional global features (e.g. occlusion relationships), the features used to predict the depth of a particular patch are computed from that patch as well as the
four neighboring patches. This is repeated at each of the
three scales, so that the feature vector at a patch includes
features of its immediate neighbors, its neighbors at a larger
spatial scale (thus capturing image features that are slightly
further away in the image plane), and again its neighbors
at an even larger spatial scale; this is illustrated in Fig. 3.
Lastly, many structures (such as trees and buildings) found
in outdoor scenes show vertical structure, in the sense that
they are vertically connected to themselves (things cannot
hang in empty air). Thus, we also add to the features of a
patch additional summary features of the column it lies in.
For each patch, after including features from itself and
its 4 neighbors at 3 scales, and summary features for its 4
column patches, our absolute depth feature vector x is 19 ∗
34 = 646 dimensional.
4.2 Features for Relative Depth
We use a different feature vector to learn the dependencies
between two neighboring patches. Speciﬁcally, we compute
a 10-bin histogram of each of the 17 ﬁlter outputs |I ∗Fn|,
giving us a total of 170 features yis for each patch i at
scale s. These features are used to estimate how the depths
at two different locations are related. We believe that learning these estimates requires less global information than predicting absolute depth, but more detail from the individual
patches. For example, given two adjacent patches of a distinctive, unique, color and texture, we may be able to safely
conclude that they are part of the same object, and thus that
their depths are close, even without more global features.
Hence, our relative depth features yijs for two neighboring
patches i and j at scale s will be the differences between
their histograms, i.e., yijs = yis −yjs.
5 Probabilistic Model
Since local images features are by themselves usually insufﬁcient for estimating depth, the model needs to reason
more globally about the spatial structure of the scene. We
capture the spatial structure of the image by modeling the
relationships between depths in different parts of the image.
Although the depth of a particular patch depends on the features of the patch, it is also related to the depths of other
parts of the image. For example, the depths of two adjacent patches lying in the same building will be highly correlated. We will use a hierarchical multiscale Markov Random
Field (MRF) to model the relationship between the depth of
a patch and the depths of its neighboring patches (Fig. 4). In
addition to the interactions with the immediately neighboring patches, there are sometimes also strong interactions between the depths of patches which are not immediate neighbors. For example, consider the depths of patches that lie
on a large building. All of these patches will be at similar
depths, even if there are small discontinuities (such as a window on the wall of a building). However, when viewed at the
smallest scale, some adjacent patches are difﬁcult to recognize as parts of the same object. Thus, we will also model
interactions between depths at multiple spatial scales.
5.1 Gaussian Model
Our ﬁrst model will be a jointly Gaussian Markov Random
Field (MRF) as shown in (1).
PG(d|X;θ,σ)
(di(1) −xT
(di(s) −dj(s))2
To capture the multiscale depth relations, we will model the
depths di(s) for multiple scales s = 1,2,3. In our experi-
Fig. 4 The multiscale MRF model for modeling relation between features and depths, relation between depths at same scale, and relation
between depths at different scales. (Only 2 out of 3 scales, and a subset
of the edges, are shown)
Int J Comput Vis 76: 53–69
ments, we enforce a hard constraint that depths at a higher
scale are the average of the depths at the lower scale.6 More
formally, we deﬁne di(s + 1) = (1/5)
j∈Ns(i)∪{i} dj(s).
Here, Ns(i) are the 4 neighbors of patch i at scale s.7
In (1), M is the total number of patches in the image (at
the lowest scale); Z is the normalization constant for the
model; xi is the absolute depth feature vector for patch i;
and θ and σ are parameters of the model. In detail, we use
different parameters (θr, σ1r, σ2r) for each row r in the image, because the images we consider are taken from a horizontally mounted camera, and thus different rows of the image have different statistical properties. For example, a blue
patch might represent sky if it is in upper part of image, and
might be more likely to be water if in the lower part of the
Our model is a conditionally trained MRF, in that its
model of the depths d is always conditioned on the image
features X; i.e., it models only P(d|X). We ﬁrst estimate
the parameters θr in (1) by maximizing the conditional log
likelihood ℓ(d) = logP(d|X;θr) of the training data. Since
the model is a multivariate Gaussian, the maximum likelihood estimate of parameters θr is obtained by solving a linear least squares problem.
The ﬁrst term in the exponent above models depth as a
function of multiscale features of a single patch i. The second term in the exponent places a soft “constraint” on the
depths to be smooth. If the variance term σ 2
2rs is a ﬁxed
constant, the effect of this term is that it tends to smooth
depth estimates across nearby patches. However, in practice
the dependencies between patches are not the same everywhere, and our expected value for (di −dj)2 may depend
on the features of the local patches.
Therefore, to improve accuracy we extend the model to
capture the “variance” term σ 2
2rs in the denominator of the
second term as a linear function of the patches i and j’s relative depth features yijs (discussed in Sect. 4.2). We model
the variance as σ 2
rs|yijs|. This helps determine which
neighboring patches are likely to have similar depths; for
example, the “smoothing” effect is much stronger if neighboring patches are similar. This idea is applied at multiple
scales, so that we learn different σ 2
2rs for the different scales
s (and rows r of the image). The parameters urs are learned
2rs to the expected value of (di(s) −dj(s))2, with
a constraint that urs ≥0 (to keep the estimated σ 2
2rs nonnegative), using a quadratic program (QP).
6One can instead have soft constraints relating the depths at higher
scale to depths at lower scale. One can also envision putting more constraints in the MRF, such as that points lying on a long straight edge in
an image should lie on a straight line in the 3-d model, etc.
7Our experiments using 8-connected neighbors instead of 4-connected
neighbors yielded minor improvements in accuracy at the cost of a
much longer inference time.
Similar to our discussion on σ 2
2rs, we also learn the variance parameter σ 2
r xi as a linear function of the features. Since the absolute depth features xi are non-negative,
the estimated σ 2
1r is also non-negative. The parameters vr
are chosen to ﬁt σ 2
1r to the expected value of (di(r)−θT
subject to vr ≥0. This σ 2
1r term gives a measure of the uncertainty in the ﬁrst term, and depends on the features. This
is motivated by the observation that in some cases, depth
cannot be reliably estimated from the local features. In this
case, one has to rely more on neighboring patches’ depths,
as modeled by the second term in the exponent.
After learning the parameters, given a new test-set image
we can ﬁnd the MAP estimate of the depths by maximizing
(1) in terms of d. Since (1) is Gaussian, logP(d|X;θ,σ)
is quadratic in d, and thus its maximum is easily found in
closed form (taking at most 1–2 seconds per image). More
details are given in Appendix 1.
5.2 Laplacian Model
We now present a second model (see (2)) that uses Laplacians instead of Gaussians to model the posterior distribution of the depths.
PL(d|X;θ,λ)
|di(1) −xT
|di(s) −dj(s)|
Our motivation for doing so is three-fold. First, a histogram
of the relative depths (di −dj) is empirically closer to Laplacian than Gaussian for more
details on depth statistics), which strongly suggests that it
is better modeled as one.8 Second, the Laplacian distribution has heavier tails, and is therefore more robust to outliers in the image features and to errors in the training-set
depthmaps (collected with a laser scanner; see Sect. 6.1).
Third, the Gaussian model was generally unable to give
depthmaps with sharp edges; in contrast, Laplacians tend to
model sharp transitions/outliers better.
This model is parametrized by θr (similar to (1)) and by
λ1r and λ2rs, the Laplacian spread parameters. Maximumlikelihood parameter estimation for the Laplacian model is
not tractable (since the partition function depends on θr).
However, by analogy to the Gaussian case, we approximate
8Although the Laplacian distribution ﬁts the log-histogram of multiscale relative depths reasonably well, there is an unmodeled peak near
zero. A more recent model attempts to model
this peak, which arises due to the fact that the neighboring depths at
the ﬁnest scale frequently lie on the same object.
Int J Comput Vis 76: 53–69
Fig. 5 The log-histogram of relative depths. Empirically, the distribution of relative depths is closer to Laplacian than Gaussian
this by solving a linear system of equations Xrθr ≈dr to
minimize L1 (instead of L2) error, i.e., minθr ||dr −Xrθr||1.
Here Xr is the matrix of absolute depth features. Following
the Gaussian model, we also learn the Laplacian spread parameters in the denominator in the same way, except that the
instead of estimating the expected values of (di −dj)2 and
r xi)2, we estimate the expected values of |di −dj|
and |di(r) −θT
r xi|, as a linear function of urs and vr respectively. This is done using a Linear Program (LP), with
urs ≥0 and vr ≥0.
Even though maximum likelihood (ML) parameter estimation for θr is intractable in the Laplacian model, given
a new test-set image, MAP inference for the depths d is
tractable and convex. Details on solving the inference problem as a Linear Program (LP) are given in Appendix 2.
Remark We can also extend these models to combine
Gaussian and Laplacian terms in the exponent, for example by using a L2 norm term for absolute depth, and a L1
norm term for the interaction terms. MAP inference remains
tractable in this setting, and can be solved using convex optimization as a QP (quadratic program).
6 Experiments
6.1 Data Collection
We used a 3-d laser scanner to collect images and their corresponding depthmaps (Fig. 7). The scanner uses a laser device (SICK LMS-291) which gives depth readings in a vertical column, with a 1.0◦resolution. To collect readings along
the other axis (left to right), the SICK laser was mounted
on a panning motor. The motor rotates after each vertical
scan to collect laser readings for another vertical column,
with a 0.5◦horizontal angular resolution. We reconstruct
the depthmap using the vertical laser scans, the motor readings and known relative position and pose of the laser device and the camera. We also collected data of stereo pairs
with corresponding depthmaps (Sect. 7), by mounting the
laser range ﬁnding equipment on a LAGR (Learning Applied to Ground Robotics) robot (Fig. 8). The LAGR vehicle
is equipped with sensors, an onboard computer, and Point
Grey Research Bumblebee stereo cameras, mounted with a
baseline distance of 11.7 cm .
We collected a total of 425 image+depthmap pairs, with
an image resolution of 1704 × 2272 and a depthmap resolution of 86 × 107. In the experimental results reported here,
75% of the images/depthmaps were used for training, and
the remaining 25% for hold-out testing. The images comprise a wide variety of scenes including natural environments (forests, trees, bushes, etc.), man-made environments
(buildings, roads, sidewalks, trees, grass, etc.), and purely
indoor environments (corridors, etc.). Due to limitations of
the laser, the depthmaps had a maximum range of 81m (the
maximum range of the laser scanner), and had minor additional errors due to reﬂections, missing laser scans, and
mobile objects. Prior to running our learning algorithms, we
transformed all the depths to a log scale so as to emphasize
multiplicative rather than additive errors in training. Data
used in the experiments is available at: 
~asaxena/learningdepth/.
6.2 Results
We tested our model on real-world test-set images of forests
(containing trees, bushes, etc.), campus areas (buildings,
people, and trees), and indoor scenes (such as corridors).
Table 1 shows the test-set results with different feature
combinations of scales, summary statistics, and neighbors,
on three classes of environments: forest, campus, and indoor.
The Baseline model is trained without any features, and predicts the mean value of depth in the training depthmaps. We
see that multiscale and column features improve the algorithm’s performance. Including features from neighboring
patches, which help capture more global information, reduces the error from 0.162 orders of magnitude to 0.133 orders of magnitude.9 We also note that the Laplacian model
performs better than the Gaussian one, reducing error to
0.084 orders of magnitude for indoor scenes, and 0.132
orders of magnitude when averaged over all scenes. Empirically, the Laplacian model does indeed give depthmaps
with signiﬁcantly sharper boundaries (as in our discussion
in Sect. 5.2; also see Fig. 6).
Figure 9 shows that modeling the spatial relationships in
the depths is important. Depths estimated without using the
second term in the exponent of (2), i.e., depths predicted using only image features with row-sensitive parameters θr,
9Errors are on a log10 scale. Thus, an error of ε means a multiplicative
error of 10ε in actual depth. E.g., 100.132 = 1.355, which thus represents an 35.5% multiplicative error.
Int J Comput Vis 76: 53–69
Fig. 6 Results for a varied set of environments, showing a original image, b ground truth depthmap, c predicted depthmap by Gaussian model,
d predicted depthmap by Laplacian model. (Best viewed in color)
Int J Comput Vis 76: 53–69
Fig. 7 The 3-d scanner used for collecting images and the corresponding depthmaps
are very noisy (Fig. 9d).10 Modeling the relations between
the neighboring depths at multiple scales through the second term in the exponent of (2) also gave better depthmaps
(Fig. 9e). Finally, Fig. 9c shows the model’s “prior” on
depths; the depthmap shown reﬂects our model’s use of
image-row sensitive parameters. In our experiments, we also
found that many features/cues were given large weights;
therefore, a model trained with only a few cues (e.g., the
top 50 chosen by a feature selection method) was not able to
predict reasonable depths.
Our algorithm works well in a varied set of environments,
as shown in Fig. 6 (last column). A number of vision algorithms based on “ground ﬁnding” appear to perform poorly when there are discontinuities or signiﬁcant luminance variations caused by shadows,
10This algorithm gave an overall error of 0.181, compared to our full
model’s error of 0.132.
Fig. 8 The custom built 3-d scanner for collecting depthmaps with
stereo image pairs, mounted on the LAGR robot
or when there are signiﬁcant changes in the ground texture.
In contrast, our algorithm appears to be robust to luminance
variations, such as shadows (Fig. 6, 4th row) and camera exposure (Fig. 6, 2nd and 5th rows).
Some of the errors of the algorithm can be attributed to
errors or limitations of the training set. For example, the
maximum value of the depths in the training and test set
is 81 m; therefore, far-away objects are all mapped to the
distance of 81 m. Further, laser readings are often incorrect
for reﬂective/transparent objects such as glass; therefore, our
algorithm also often estimates depths of such objects incorrectly. Quantitatively, our algorithm appears to incur the
largest errors on images which contain very irregular trees,
in which most of the 3-d structure in the image is dominated
by the shapes of the leaves and branches. However, arguably
even human-level performance would be poor on these images.
We note that monocular cues rely on prior knowledge,
learned from the training set, about the environment. This is
because monocular 3-d reconstruction is an inherently ambiguous problem. Thus, the monocular cues may not generalize well to images very different from ones in the training
set, such as underwater images or aerial photos.
To test the generalization capability of the algorithm,
we also estimated depthmaps of images downloaded from
the Internet (images for which camera parameters are not
known).11 The model (using monocular cues only) was able
11Since we do not have ground-truth depthmaps for images downloaded from the Internet, we are unable to give a quantitative comparisons on these images. Further, in the extreme case of orthogonal
cameras or very wide angle perspective cameras, our algorithm would
need to be modiﬁed to take into account the ﬁeld of view of the camera.
Int J Comput Vis 76: 53–69
Table 1 Effect of multiscale and column features on accuracy. The average absolute errors (RMS errors gave very similar trends) are on a log
scale (base 10). H1 and H2 represent summary statistics for k = 1,2. S1, S2 and S3 represent the 3 scales. C represents the column features.
Baseline is trained with only the bias term (no features)
Gaussian (S1,S2,S3, H1,H2, no neighbors)
Gaussian (S1, H1,H2)
Gaussian (S1,S2, H1,H2)
Gaussian (S1,S2,S3, H1,H2)
Gaussian (S1,S2,S3, C, H1)
Gaussian (S1,S2,S3, C, H1,H2)
Fig. 9 a original image, b ground truth depthmap, c “prior” depthmap (trained with no features), d features only (no MRF relations), e Full
Laplacian model. (Best viewed in color)
to produce reasonable depthmaps on most of the images
(Fig. 10). Informally, our algorithm appears to predict the
relative depths quite well (i.e., their relative distances to the
camera);12 even for scenes very different from the training
set, such as a sunﬂower ﬁeld, an oil-painting scene, mountains and lakes, a city skyline photographed from sea, a city
during snowfall, etc.
Car Driving Experiments
Michels et al. used a
simpliﬁed version of the monocular depth estimation algorithm to drive a remote-controlled car (Fig. 11a). The algorithm predicts (1-d) depths from single still images, captured from a web-camera with 320 × 240 pixel resolution.
The learning algorithm can be trained on either real camera images labeled with ground-truth ranges to the closest
obstacle in each direction, or on a training set consisting of
synthetic graphics images. The resulting algorithm, trained
on a combination of real and synthetic data, was able to
learn monocular visual cues that accurately estimate the relative depths of obstacles in a scene (Fig. 11b). We tested
the algorithm by driving the car at four different locations,
12For most applications such as object recognition using knowledge
of depths, robotic navigation, or 3-d reconstruction, relative depths
are sufﬁcient. The depths could be rescaled to give accurate absolute
depths, if the camera parameters are known or are estimated.
ranging from man-made environments with concrete tiles
and trees, to uneven ground in a forest environment with
rocks, trees and bushes where the car is almost never further
than 1 m from the nearest obstacle. The mean time before
crash ranged from 19 to more than 200 seconds, depending on the density of the obstacles .
The unstructured testing sites were limited to areas where
no training or development images were taken. Videos of
the algorithm driving the car autonomously are available at:
 
7 Improving Performance of Stereovision using
Monocular Cues
Consider the problem of estimating depth from two images
taken from a pair of stereo cameras (Fig. 12). The most
common approach for doing so is stereopsis (stereovision),
in which depths are estimated by triangulation using the
two images. Over the past few decades, researchers have
developed very good stereovision systems . Although these systems
work well in many environments, stereovision is fundamentally limited by the baseline distance between the two cameras. Speciﬁcally, their depth estimates tend to be inaccurate
when the distances considered are large 76: 53–69
Fig. 10 Typical examples of the predicted depthmaps for images downloaded from the Internet. (Best viewed in color)
Fig. 11 a The remote-controlled car driven autonomously in various
cluttered unconstrained environments, using our algorithm. b A view
from the car, with the chosen steering direction indicated by the red
square; the estimated distances to obstacles in the different directions
are shown by the bar graph below the image
small triangulation/angle estimation errors translate to very
large errors in distances). Further, stereovision also tends to
fail for textureless regions of images where correspondences
cannot be reliably found.
On the other hand, humans perceive depth by seamlessly
combining monocular cues with stereo cues. We believe
that monocular cues and (purely geometric) stereo cues give
largely orthogonal, and therefore complementary, types of
information about depth. Stereo cues are based on the difference between two images and do not depend on the content of the image. Even if the images are entirely random, it
would still generate a pattern of disparities . On the other hand, depth
estimates from monocular cues are entirely based on the evidence about the environment presented in a single image. In
this section, we investigate how monocular cues can be integrated with any reasonable stereo system, to obtain better
depth estimates than the stereo system alone.
7.1 Disparity from Stereo Correspondence
Depth estimation using stereovision from two images (taken
from two cameras separated by a baseline distance) involves
three steps: First, establish correspondences between the two
images. Then, calculate the relative displacements (called
“disparity”) between the features in each image. Finally, determine the 3-d depth of the feature relative to the cameras,
using knowledge of the camera geometry.
Int J Comput Vis 76: 53–69
Fig. 12 Two images taken from
a stereo pair of cameras, and the
depthmap calculated by a stereo
Stereo correspondences give reliable estimates of disparity, except when large portions of the image are featureless
(i.e., correspondences cannot be found). Further, for a given
baseline distance between cameras, the accuracy decreases
as the depth values increase. In the limit of very distant
objects, there is no observable disparity, and depth estimation generally fails. Empirically, depth estimates from stereo
tend to become unreliable when the depth exceeds a certain
Our stereo system ﬁnds good feature correspondences
between the two images by rejecting pixels with little texture, or where the correspondence is otherwise ambiguous.
More formally, we reject any feature where the best match
is not signiﬁcantly better than all other matches within the
search window. We use the sum-of-absolute-differences correlation as the metric score to ﬁnd correspondences . Our cameras (and algorithm) allow subpixel interpolation accuracy of 0.2 pixels of disparity. Even
though we use a fairly basic implementation of stereopsis,
the ideas in this paper can just as readily be applied together
with other, perhaps better, stereo systems.
7.2 Modeling Uncertainty in Stereo
The errors in disparity are often modeled as either Gaussian
 or via some other, heavier-tailed distribution . Speciﬁcally, the errors in disparity have two main causes: (a) Assuming unique/perfect
correspondence, the disparity has a small error due to image
noise (including aliasing/pixelization), which is well modeled by a Gaussian. (b) Occasional errors in correspondence
cause larger errors, which results in a heavy-tailed distribution for disparity .
If the standard deviation is σg in computing disparity g
from stereo images (because of image noise, etc.), then the
standard deviation of the depths13 will be σd,stereo ≈σg/g.
For our stereo system, we have that σg is about 0.2 pix-
13Using the delta rule from statistics: Var(f (x)) ≈(f ′(x))2Var(x), derived from a second order Taylor series approximation of f (x). The
depth d is related to disparity g as d = log(C/g), with camera parameters determining C.
els;14 this is then used to estimate σd,stereo. Note therefore
that σd,stereo is a function of the estimated depth, and specifically, it captures the fact that variance in depth estimates is
larger for distant objects than for closer ones.
7.3 Probabilistic Model
We use our Markov Random Field (MRF) model, which
models relations between depths at different points in the
image, to incorporate both monocular and stereo cues.
Therefore, the depth of a particular patch depends on the
monocular features of the patch, on the stereo disparity, and
is also related to the depths of other parts of the image.
PG(d|X;θ,σ)
(di(1) −di,stereo)2
+ (di(1) −xT
(di(s) −dj(s))2
PL(d|X;θ,λ)
|di(1) −di,stereo|
+ |di(1) −xT
|di(s) −dj(s)|
In our Gaussian and Laplacian MRFs (see (3) and (4)),
we now have an additional term di,stereo, which is the depth
estimate obtained from disparity.15 This term models the re-
14One can also envisage obtaining a better estimate of σg as a function of a match metric used during stereo correspondence , such as normalized sum of squared differences; or learning σg
as a function of disparity/texture based features.
15In this work, we directly use di,stereo as the stereo cue. In , we use a library of features created from stereo depths as
the cues for identifying a grasp point on objects.
Int J Comput Vis 76: 53–69
lation between the depth and the estimate from stereo disparity. The other terms in the models are similar to (1) and (2)
in Sect. 5.
7.4 Results on Stereo
For these experiments, we collected 257 stereo pairs +
depthmaps in a wide-variety of outdoor and indoor environments, with an image resolution of 1024 × 768 and a
depthmap resolution of 67 × 54. We used 75% of the images/depthmaps for training, and the remaining 25% for
hold-out testing.
We quantitatively compare the following classes of algorithms that use monocular and stereo cues in different ways:
(i) Baseline: This model, trained without any features,
predicts the mean value of depth in the training depthmaps.
(ii) Stereo: Raw stereo depth estimates, with the missing
values set to the mean value of depth in the training
depthmaps.
(iii) Stereo (smooth): This method performs interpolation
and region ﬁlling; using the Laplacian model without
the second term in the exponent in (4), and also without
using monocular cues to estimate λ2 as a function of the
(iv) Mono (Gaussian): Depth estimates using only monocular cues, without the ﬁrst term in the exponent of the
Gaussian model in (3).
(v) Mono (Lap): Depth estimates using only monocular
cues, without the ﬁrst term in the exponent of the
Laplacian model in (4).
(vi) Stereo+Mono: Depth estimates using the full model.
Table 2 shows that although the model is able to predict depths using monocular cues only (“Mono”), the performance is signiﬁcantly improved when we combine both
mono and stereo cues. The algorithm is able to estimate
depths with an error of 0.074 orders of magnitude, (i.e.,
18.6% of multiplicative error because 100.074 = 1.186)
which represents a signiﬁcant improvement over stereo
(smooth) performance of 0.088.
Figure 13 shows that the model is able to predict
depthmaps (column 5) in a variety of environments. It also
demonstrates how the model takes the best estimates from
both stereo and monocular cues to estimate more accurate
depthmaps. For example, in row 6 (Fig. 13), the depthmap
generated by stereo (column 3) is very inaccurate; however,
the monocular-only model predicts depths fairly accurately
(column 4). The combined model uses both sets of cues to
produce a better depthmap (column 5). In row 3, stereo cues
give a better estimate than monocular ones. We again see
that our combined MRF model, which uses both monocular and stereo cues, gives an accurate depthmap (column 5)
Table 2 The average errors (RMS errors gave very similar trends) for
various cues and models, on a log scale (base 10)
Stereo (smooth)
Mono (Gaussian)
Mono (Lap)
Stereo+Mono
correcting some mistakes of stereo, such as some far-away
regions that stereo predicted as close.
In Fig. 14, we study the behavior of the algorithm as a
function of the 3-d distance from the camera. At small distances, the algorithm relies more on stereo cues, which are
more accurate than the monocular cues in this regime. However, at larger distances, the performance of stereo degrades,
and the algorithm relies more on monocular cues. Since our
algorithm models uncertainties in both stereo and monocular cues, it is able to combine stereo and monocular cues
effectively.
We note that monocular cues rely on prior knowledge,
learned from the training set, about the environment. This is
because monocular 3-d reconstruction is an inherently ambiguous problem. In contrast, the stereopsis cues we used
are purely geometric, and therefore should work well even
on images taken from very different environments. For example, the monocular algorithm fails sometimes to predict
correct depths for objects which are only partially visible in
the image (e.g., Fig. 13, row 2: tree on the left). For a point
lying on such an object, most of the point’s neighbors lie
outside the image; hence the relations between neighboring
depths are less effective here than for objects lying in the
middle of an image. However, in many of these cases, the
stereo cues still allow an accurate depthmap to be estimated
(row 2, column 5).
8 Conclusions
Over the last few decades, stereo and other “triangulation” cues have been successfully applied to many important problems, including robot navigation, building 3-d models of urban environments, and object recognition. Unlike
triangulation-based algorithms such as stereopsis and structure from motion, we have developed a class of algorithms
that exploit a largely orthogonal set of monocular cues. We
presented a hierarchical, multiscale Markov Random Field
(MRF) learning model that uses such cues to estimate depth
from a single still image. These monocular cues can not only
Int J Comput Vis 76: 53–69
Fig. 13 Results for a varied set of environments, showing one image
of the stereo pairs (column 1), ground truth depthmap collected from
3-d laser scanner (column 2), depths calculated by stereo (column 3),
depths predicted by using monocular cues only (column 4), depths predicted by using both monocular and stereo cues (column 5). The bottom
row shows the color scale for representation of depths. Closest points
are 1.2 m, and farthest are 81 m. (Best viewed in color)
Int J Comput Vis 76: 53–69
Fig. 14 The average errors (on a log scale, base 10) as a function of
the distance from the camera
be combined with triangulation ones, but also scale better
than most triangulation-based cues to depth estimation at
large distances. Although our work has been limited to depth
estimation, we believe that these monocular depth and shape
cues also hold rich promise for many other applications in
Acknowledgements
We give warm thanks to Jamie Schulte, who
designed and built the 3-d scanner, and to Andrew Lookingbill, who
helped us with collecting the data used in this work. We also thank Jeff
Michels, Larry Jackel, Sebastian Thrun, Min Sun and Pieter Abbeel for
helpful discussions. This work was supported by the DARPA LAGR
program under contract number FA8650-04-C-7134.
Appendix 1: MAP Inference for Gaussian Model
We can rewrite (1) as a standard multivariate Gaussian,
PG(d|X;θ,σ)
2(d −Xaθr)T Σ−1
a (d −Xaθr)
where Xa = (Σ−1
1 X, with Σ1 and Σ2
representing the matrices of the variances σ 2
1,i and σ 2
2,i in the
ﬁrst and second terms in the exponent of (1) respectively.16
Q is a matrix such that rows of Qd give the differences of
the depths in the neighboring patches at multiple scales (as
in the second term in the exponent of (1)). Our MAP estimate of the depths is, therefore, d∗= Xaθr.
During learning, we iterate between learning θ and estimating σ. Empirically, σ1 ≪σ2, and Xa is very close to X;
therefore, the algorithm converges after 2–3 iterations.
16Note that if the variances at each point in the image are constant,
then Xa = (I + σ 2
2 QT Q)−1X. I.e., Xa is essentially a smoothed
version of X.
Appendix 2: MAP Inference for Laplacian Model
Exact MAP inference of the depths d ∈RM can be obtained
by maximizing logP(d|X;θ,λ) in terms of d (see (2)).
More formally,
d∗= argmax
logP(d|X;θ,λ)
1 |d −Xθr| + cT
where, c1 ∈RM with c1,i = 1/λ1,i, and c2 ∈R6M with
c2,i = 1/λ2,i. Our features are given by X ∈RMxk and the
learned parameters are θr ∈Rk, which give a naive estimate
˜d = Xθr ∈RM of the depths. Q is a matrix such that rows
of Qd give the differences of the depths in the neighboring patches at multiple scales (as in the second term in the
exponent of (2)).
We add auxiliary variables ξ1 and ξ2 to pose the problem
as a Linear Program (LP):
d∗= arg min
−ξ1 ≤d −˜d ≤ξ1
−ξ2 ≤Qd ≤ξ2.
In our experiments, MAP inference takes about 7–8 seconds
for an image.