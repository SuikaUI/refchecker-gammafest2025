Attributing Fake Images to GANs: Learning and Analyzing GAN Fingerprints
Ning Yu1,2
Larry Davis1
Mario Fritz3
1University of Maryland, College Park
2Max Planck Institute for Informatics
Saarland Informatics Campus, Germany
3CISPA Helmholtz Center for Information Security
Saarland Informatics Campus, Germany
 
 
 
Recent advances in Generative Adversarial Networks
(GANs) have shown increasing success in generating photorealistic images.
But they also raise challenges to visual forensics and model attribution. We present the ﬁrst
study of learning GAN ﬁngerprints towards image attribution and using them to classify an image as real or GANgenerated. For GAN-generated images, we further identify
their sources. Our experiments show that (1) GANs carry
distinct model ﬁngerprints and leave stable ﬁngerprints in
their generated images, which support image attribution;
(2) even minor differences in GAN training can result in
different ﬁngerprints, which enables ﬁne-grained model authentication; (3) ﬁngerprints persist across different image
frequencies and patches and are not biased by GAN artifacts; (4) ﬁngerprint ﬁnetuning is effective in immunizing
against ﬁve types of adversarial image perturbations; and
(5) comparisons also show our learned ﬁngerprints consistently outperform several baselines in a variety of setups 1.
1. Introduction
In the last two decades, photorealistic image generation
and manipulation techniques have rapidly evolved. Visual
contents can now be easily created and edited without leaving obvious perceptual traces . Recent breakthroughs
in generative adversarial networks (GANs) have further improved the quality and photorealism of generated images.
The adversarial framework of
GANs can also be used in conditional scenarios for image translation or manipulation in a given context , which diversiﬁes media synthesis.
1Code, data, models, and supplementary material are available at
Figure 1. A t-SNE visual comparison between our ﬁngerprint
features (right) and the baseline inception features (left) for
image attribution. Inception features are highly entangled, indicating the challenge to differentiate high-quality GAN-generated
images from real ones. However, our result shows any single difference in GAN architectures, training sets, or even initialization
seeds can result in distinct ﬁngerprint features for effective attribution.
At the same time, however, the success of GANs has
raised two challenges to the vision community:
forensics and intellectual property protection.
challenges
forensics.
widespread concern about the impact of this technology
when used maliciously. This issue has also received increasing public attention, in terms of disruptive consequences to visual security, laws, politics, and society in general . Therefore, it is critical to look into effective
visual forensics against threats from GANs.
While recent state-of-the-art visual forensics techniques
demonstrate impressive results for detecting fake visual media , they have
only focused on semantic, physical, or statistical inconsistency of speciﬁc forgery scenarios, e.g., copy-move
manipulations or face swapping . Forensics on
GAN-generated images shows good accuracy,
but each method operates on only one GAN architecture by
identifying its unique artifacts and results deteriorate when
the GAN architecture is changed. It is still an open question
of whether GANs leave stable marks that are commonly
 
shared by their generated images. That motivates us to investigate an effective feature representation that differentiates GAN-generated images from real ones.
GAN challenges to intellectual property protection.
Similar to other successful applications of deep learning
technology to image recognition or natural language
processing , building a product based on GANs is nontrivial . It requires a large amount of training data,
powerful computing resources, signiﬁcant machine learning
expertise, and numerous trial-and-error iterations for identifying optimal model architectures and their model hyperparameters.
As GAN services become widely deployed
with commercial potential, they will become increasingly
vulnerable to pirates. Such copyright plagiarism may jeopardize the intellectual property of model owners and take
future market share from them. Therefore, methods for attributing GAN-generated image origins are highly desirable
for protecting intellectual property.
Given the level of realism that GAN techniques already
achieve today, attribution by human inspection is no longer
feasible (see the mixture of Figure 4). The state-of-theart digital identiﬁcation techniques can be separated into
two categories: digital watermarking and digital ﬁngerprint
detection.
Neither of them is applicable to GAN attribution. Previous work on watermarking deep neural networks depends on an embedded security scheme
during “white-box” model training, requires control of the
input, and is impractical when only GAN-generated images
are accessible in a “black-box” scenario. Previous work on
digital ﬁngerprints is limited to device ﬁngerprints 
or in-camera post-processing ﬁngerprints , which cannot be easily adapted to GAN-generated images. That motivates us to investigate GAN ﬁngerprints that attribute different GAN-generated images to their sources.
We present the ﬁrst study addressing the two GAN challenges simultaneously by learning GAN ﬁngerprints for image attribution: We introduce GAN ﬁngerprints and use
them to classify an image as real or GAN-generated. For
GAN-generated images, we further identify their sources.
We approach this by training a neural network classiﬁer and
predicting the source of an image. Our experiments show
that GANs carry distinct model ﬁngerprints and leave stable
ﬁngerprints in their generated images, which support image
attribution.
We summarize our contributions as demonstrating the
existence, uniqueness, persistence, immunizability, and visualization of GAN ﬁngerprints. We address the following
questions:
Existence and uniqueness: Which GAN parameters differentiate image attribution?
We present experiments
on GAN parameters including architecture, training data, as
well as random initialization seed. We ﬁnd that a difference
in any one of these parameters results in a unique GAN ﬁngerprint for image attribution. See Figure 1, Section 3.1 and
Persistence: Which image components contain ﬁngerprints for attribution?
We investigate image components in different frequency bands and in different patch
sizes. In order to eliminate possible bias from GAN artifact components, we apply a perceptual similarity metric
to distill an artifact-free subset for attribution evaluation.
We ﬁnd that GAN ﬁngerprints are persistent across different frequencies and patch sizes, and are not dominated by
artifacts. See Section 3.2 and 4.3.
Immunizability: How robust is attribution to image perturbation attacks and how effective are the defenses?
We investigate common attacks that aim at destroying image ﬁngerprints. They include noise, blur, cropping, JPEG
compression, relighting, and random combinations of them.
We also defend against such attacks by ﬁnetuning our attribution classiﬁer. See Section 4.4.
Visualization: How to expose GAN ﬁngerprints?
propose an alternative classiﬁer variant to explicitly visualize GAN ﬁngerprints in the image domain, so as to better
interpret the effectiveness of attribution. See Section 3.3
Comparison to baselines.
In terms of attribution accuracy, our method consistently outperforms three baseline
methods (including a very recent one ) on two datasets
under a variety of experimental conditions.
In terms of
feature representation, our ﬁngerprints show superior distinguishability across image sources compared to inception
features .
2. Related work
Generative Adversarial Networks (GANs).
52, 10, 32, 38, 19] have shown improved photorealism in
image synthesis , translation , or manipulation . We focus on unconditional GANs as
the subject of our study. We choose the following four GAN
models as representative candidates of the current state of
the art: ProGAN , SNGAN , CramerGAN ,
and MMDGAN , considering their outstanding performances on face generation.
Visual forensics.
Visual forensics targets detecting statistical or physics-based artifacts and then recognizing the authenticity of visual media without evidence from an embedded security mechanism .
An example is a
steganalysis-based method , which uses hand-crafted
features plus a linear Support Vector Machine to detect forgeries. Recent CNN-based methods learn deep features and further improve tampering detection performance on images or videos. R¨ossler
et al. introduced a large-scale face manipulation
dataset to benchmark forensics classiﬁcation and segmentation tasks, and demonstrated superior performance when using additional domain-speciﬁc knowledge. For forensics on
GAN-generated images, several existing works 
show good accuracy. However, each method considers only
one GAN architecture and results do not generalize across
architectures.
Digital ﬁngerprints.
Prior digital ﬁngerprint techniques
focus on detecting hand-crafted features for either device
ﬁngerprints or postprocessing ﬁngerprints. The device ﬁngerprints rely on the fact that individual devices, due to
manufacturing imperfections, leave a unique and stable
mark on each acquired image, i.e., the photo-response nonuniformity (PRNU) pattern .
Likewise, postprocessing ﬁngerprints come from the speciﬁc in-camera postprocessing suite (demosaicking, compression, etc.) during
each image acquisition procedure . Recently, Marra et
al. visualize GAN ﬁngerprints based on PRNU, and
show their application to GAN source identiﬁcation. We
replace their hand-crafted ﬁngerprint formulation with a
learning-based one, decoupling model ﬁngerprint from image ﬁngerprint, and show superior performances in a variety
of experimental conditions.
Digital watermarking.
Digital watermarking is a complementary forensics technique for image authentication . It involves embedding artiﬁcial watermarks in images. It can be used to reveal image source
and ownership so as to protect their copyright. It has been
shown that neural networks can also be actively watermarked during training . In such models, a characteristic pattern can be built into the learned representation
but with a trade-off between watermarking accuracy and the
original performance. However, such watermarking has not
been studied for GANs. In contrast, we utilize inherent ﬁngerprints for image attribution without any extra embedding
burden or quality deterioration.
3. Fingerprint learning for image attribution
Inspired by the prior works on digital ﬁngerprints , we introduce the concepts of GAN model ﬁngerprint
and image ﬁngerprint.
Both are simultaneously learned
from an image attribution task.
Model ﬁngerprint.
Each GAN model is characterized by
many parameters: training dataset distribution, network architecture, loss design, optimization strategy, and hyperparameter settings. Because of the non-convexity of the
objective function and the instability of adversarial equilibrium between the generator and discriminator in GANs, the
values of model weights are sensitive to their random initializations and do not converge to the same values during each
training. This indicates that even though two well-trained
GAN models may perform equivalently, they generate highquality images differently. This suggests the existence and
uniqueness of GAN ﬁngerprints. We deﬁne the model ﬁngerprint per GAN instance as a reference vector, such that
it consistently interacts with all its generated images. In a
speciﬁcally designed case, the model ﬁngerprint can be an
RGB image the same size as its generated images. See Section 3.3.
Image ﬁngerprint.
GAN-generated images are the outcomes of a large number of ﬁxed ﬁltering and non-linear
processes, which generate common and stable patterns
within the same GAN instances but are distinct across different GAN instances. That suggests the existence of image
ﬁngerprints and attributability towards their GAN sources.
We introduce the ﬁngerprint per image as a feature vector
encoded from that image. In a speciﬁcally designed case,
an image ﬁngerprint can be an RGB image the same size as
the original image. See Section 3.3.
3.1. Attribution network
Similar to the authorship attribution task in natural language processing , we train an attribution classiﬁer
that can predict the source of an image: real or from a GAN
We approach this using a deep convolutional neural network supervised by image-source pairs {(I, y)} where I ∼
I is sampled from an image set and y ∈Y is the source
ground truth belonging to a ﬁnite set.
That set is composed of pre-trained GAN instances plus the real world.
Figure 2(a) depicts an overview of our attribution network.
We implicitly represent image ﬁngerprints as the ﬁnal
classiﬁer features (the 1 × 1 × 512 tensor before the ﬁnal fully connected layer) and represent GAN model ﬁngerprints as the corresponding classiﬁer parameters (the
1×1×512 weight tensor of the ﬁnal fully connected layer).
Why is it necessary to use such an external classiﬁer
when GAN training already provides a discriminator? The
discriminator learns a hyperplane in its own embedding
space to distinguish generated images from real ones. Different embedding spaces are not aligned. In contrast, the
proposed classiﬁer necessarily learns a uniﬁed embedding
space to distinguish generated images from different GAN
instances or from real images.
Note that our motivation to investigate “white-box”
GANs subject to known parameters is to validate the attributability along different GAN parameter dimensions. In
practice, our method also applies to “black-box” GAN API
services. The only required supervision is the source label
of an image. We can simply query different services, collect
their generated images, and label them by service indices.
Our classiﬁer would test image authenticity by predicting if
an image is sampled from the desired service. We also test
service authenticity by checking if most of their generated
images have the desired source prediction.
3.2. Component analysis networks
In order to analyze which image components contain ﬁngerprints, we propose three variants of the network.
Figure 2. Different attribution network architectures. Tensor representation is speciﬁed by two spatial dimensions followed by the
number of channels. The network is trained to minimize crossentropy classiﬁcation loss.
(a) Attribution network.
(b) Predownsampling network example that downsamples input image to
8 × 8 before convolution. (c) Pre-downsampling residual network
example that extracts the residual component between 16×16 and
8×8 resolutions. (d) Post-pooling network example that starts average pooling at 64 × 64 resolution.
Pre-downsampling network.
We propose to test whether
ﬁngerprints and attribution can be derived from different
frequency bands. We investigate attribution performance
w.r.t. downsampling factor. Figure 2(b) shows an architecture example that extracts low-frequency bands. We replace
the trainable convolution layers with our Gaussian downsampling layers from the input end and systematically control at which resolution we stop such replacement.
Pre-downsampling residual network.
Complementary
to extracting low-frequency bands, Figure 2(c) shows an architecture example that extracts a residual high-frequency
band between one resolution and its factor-2 downsampled
resolution. It is reminiscent of a Laplacian Pyramid .
We systematically vary the resolution at which we extract
such residual.
Post-pooling network.
We propose to test whether ﬁngerprints and attribution can be derived locally based on
patch statistics.
We investigate attribution performance
w.r.t. patch size. Figure 2(d) shows an architecture example.
Inspired by PatchGAN , we regard a “pixel” in a neural
tensor as the feature representation of a local image patch
covered by the receptive ﬁeld of that “pixel”. Therefore,
post-pooling operations count for patch-based neural statistics. Earlier post-pooling corresponds to a smaller patch
size. We systematically vary at which tensor resolution we
start this pooling in order to switch between more local and
more global patch statistics.
3.3. Fingerprint visualization
Alternatively to our attribution network in Section 3.1
where ﬁngerprints are implicitly represented in the feature
domain, we describe a model similar in spirit to Marra et
al. to explicitly represent them in the image domain.
But in contrast to their hand-crafted PRNU-based representation, we modify our attribution network architecture and
Figure 3. Fingerprint visualization diagram. We train an AutoEncoder and GAN ﬁngerprints end-to-end. ⊙indicates pixel-wise
multiplication of two normalized images.
learn ﬁngerprint images from image-source pairs ({I, y}).
We also decouple the representation of model ﬁngerprints
from image ﬁngerprints. Figure 3 depicts the ﬁngerprint visualization model.
Abstractly, we learn to map from input image to its ﬁngerprint image.
But without ﬁngerprint supervision, we
choose to ground the mapping based on a reconstruction
task with an AutoEncoder. We then deﬁne the reconstruction residual as the image ﬁngerprint. We simultaneously
learn a model ﬁngerprint for each source (each GAN instance plus the real world), such that the correlation index
between one image ﬁngerprint and each model ﬁngerprint
serves as softmax logit for classiﬁcation.
Mathematically, given an image-source pair (I, y) where
y ∈Y belongs to the ﬁnite set Y of GAN instances plus the
real world, we formulate a reconstruction mapping R from
I to R(I). We ground our reconstruction based on pixelwise L1 loss plus adversarial loss:
Lpix(I) = ||R(I) −I||1
Ladv(I) = Drec
 R(I), I|Drec
where Drec is an adversarially trained discriminator, and
GP(·) is the gradient penalty regularization term deﬁned
We then explicitly deﬁne image ﬁngerprint F I
reconstruction residual F I
im = R(I) −I.
We further explicitly deﬁne model ﬁngerprint F y
freely trainable parameters with the same size as F I
that corr(F I
mod), the correlation index between F I
mod, is maximized over Y. This can be formulated as the
softmax logit for the cross-entropy classiﬁcation loss supervised by the source ground truth:
Lcls(I, y) = −log
ˆy∈Y corr(F I
where corr(A, B) = ˆA ⊙ˆB, ˆA and ˆB are the zero-mean,
unit-norm, and vectorized version of images A and B, and
⊙is the inner product operation.
Our ﬁnal training objective is
{(I,y)} (λ1Lpix + λ2Ladv + λ3Lcls) (4)
(a) CelebA real data
(b) ProGAN
(d) CramerGAN
(e) MMDGAN
Figure 4. Face samples from difference sources.
where λ1 = 20.0, λ2 = 0.1, and λ3 = 1.0 are used to
balance the order of magnitude of each loss term, which are
not sensitive to dataset and are ﬁxed.
Note that this network variant is used to better visualize
and interpret the effectiveness of image attribution. However, it introduces extra training complexity and thus is not
used if we only focus on attribution.
4. Experiments
We discuss the experimental setup in Section 4.1. From
Section 4.2 to 4.5, we explore the four research questions
discussed in the Introduction.
4.1. Setup
. We employ CelebA human face dataset 
and LSUN bedroom scene dataset , both containing
20, 000 real-world RGB images.
GAN models.
We consider four recent state-of-the-art
GAN architectures: ProGAN , SNGAN , Cramer-
GAN , and MMDGAN . Each model is trained
from scratch with their default settings except we ﬁx the
number of training epochs to 240 and ﬁx the output size of
a generator to 128 × 128 × 3.
Baseline methods.
Given real-world datasets and four
pre-trained GAN models, we compare with three baseline
classiﬁcation methods: k-nearest-neighbor (kNN) on raw
pixels, Eigenface , and the very recent PRNU-based ﬁngerprint method from Marra et al. .
Evaluation.
We use classiﬁcation accuracy to evaluate
image attribution performance.
In addition, we use the ratio of inter-class and intra-class
Fr´echet Distance , denoted as FD ratio, to evaluate the
distinguishability of a feature representation across classes.
The larger the ratio, the more distinguishable the feature
representation across sources. See supplementary material
for more detail. We compare our ﬁngerprint features to image inception features . The FD of inception features
is also known as FID for GAN evaluation . Therefore,
the FD ratio of inception features can serve as a reference to
show how challenging it is to attribute high-quality GANgenerated images manually or without ﬁngerprint learning.
4.2. Existence and uniqueness: which GAN parameters differentiate image attribution?
We consider GAN architecture, training set, and initialization seed respectively by varying one type of parameter
and keeping the other two ﬁxed.
Different architectures.
First, we leverage all the real
images to train ProGAN, SNGAN, CramerGAN, and
MMDGAN separately. For the classiﬁcation task, we con-
ﬁgure training and testing sets with 5 classes: {real, Pro-
GAN, SNGAN, CramerGAN, MMDGAN}. We randomly
collect 100, 000 images from each source for classiﬁcation
training and another 10, 000 images from each source for
testing. We show face samples from each source in Figure 4 and bedroom samples in the supplementary material.
Table 1 shows that we can effectively differentiate GANgenerated images from real ones and attribute generated images to their sources, just using a regular CNN classiﬁer.
There do exist unique ﬁngerprints in images that differentiate GAN architectures, even though it is far more challenging to attribute those images manually or through inception
features .
Different GAN training sets.
We further narrow down
the investigation to GAN training sets. From now we only
focus on ProGAN plus real dataset. We ﬁrst randomly select a base real subset containing 100, 000 images, denoted
as real subset diff 0.
We then randomly select 10 other
real subsets also containing 100, 000 images, denoted as
real subset diff #i, where i ∈{1, 10, 100, 1000, 10000,
20000, 40000, 60000, 80000, 100000} indicates the number of images that are not from the base subset. We collect
such sets of datasets to explore the relationship between attribution performance and GAN training set overlaps.
For each real subset diff #i, we separately train a Pro-
GAN model and query 100, 000 images for classiﬁer
training and another 10, 000 images for testing, labeled
as ProGAN subset diff #i.
In this setup of {real, Pro-
GAN subset diff #i}, we show the performance evaluation
in Table 2. Surprisingly, we ﬁnd that attribution performance remains equally high regardless of the amount of
GAN training set overlap. Even GAN training sets that differ in just one image can lead to distinct GAN instances.
That indicates that one-image mismatch during GAN training results in a different optimization step in one iteration
Table 1. Evaluation on {real, ProGAN, SNGAN, CramerGAN,
MMDGAN}. The best performance is highlighted in bold.
Eigenface 
Inception 
Our ﬁngerprint
Table 2. Evaluation on {real, ProGAN subset diff #i}. The best
performance is highlighted in bold.
Eigenface 
Inception 
Our ﬁngerprint
Table 3. Evaluation on {real, ProGAN seed v#i}. The best performance is highlighted in bold. “Our visNet” row indicates our ﬁngerprint visualization network described in Section 3.3 and evaluated in Section 4.5.
Eigenface 
Our visNet
Inception 
Our ﬁngerprint
and ﬁnally results in distinct ﬁngerprints. That motivates
us to investigate the attribution performance among GAN
instances that were trained with identical architecture and
dataset but with different random initialization seeds.
Different initialization seeds.
We next investigate the
impact of GAN training initialization on image attributability.
We train 10 ProGAN instances with the entire real
dataset and with different initialization seeds.
We sample 100, 000 images for classiﬁer training and another
10, 000 images for testing.
In this setup of {real, Pro-
GAN seed v#i} where i ∈{1, ..., 10}, we show the performance evaluation in Table 3. We conclude that it is the difference in optimization (e.g., caused by different randomness) that leads to attributable ﬁngerprints. In order to verify our experimental setup, we ran sanity checks. For example, two identical ProGAN instances trained with the same
seed remain indistinguishable and result in random-chance
attribution performance.
Table 4. Classiﬁcation accuracy (%) of our network w.r.t. downsampling factor on low-frequency or high-frequency components
of {real, ProGAN seed v#i}.
“L-f” column indicates the lowfrequency components and represents the performances from the
pre-downsampling network.
“H-f” column indicates the highfrequency components and represents the performances from the
pre-downsampling residual network.
Downsample
Table 5. Classiﬁcation accuracy (%) of our network w.r.t. patch
size on {real, ProGAN seed v#i}.
Pooling starts at
Patch size
4.3. Persistence: which image components contain
ﬁngerprints for attribution?
We systematically explore attribution performance w.r.t.
image components in different frequency bands or with different patch sizes. We also investigate possible performance
bias from GAN artifacts.
Different frequencies.
We investigate if band-limited images carry effective ﬁngerprints for attribution. We separately apply the proposed pre-downsampling network and
pre-downsampling residual network for image attribution.
Given the setup of {real, ProGAN seed v#i}, Table 4 shows
the classiﬁcation accuracy w.r.t.
downsampling factors.
We conclude that (1) a wider frequency band carries more
ﬁngerprint information for image attribution, (2) the lowfrequency and high-frequency components (even at the resolution of 8×8) individually carry effective ﬁngerprints and
result in attribution performance better than random, and (3)
at the same resolution, high-frequency components carry
more ﬁngerprint information than low-frequency components.
Different local patch sizes.
We also investigate if local
image patches carry effective ﬁngerprints for attribution.
We apply the post-pooling network for image attribution.
Given the setup of {real, ProGAN seed v#i}, Table 5 shows
the classiﬁcation accuracy w.r.t. patch sizes. We conclude
that for CelebA face dataset a patch of size 24×24 or larger
carries sufﬁcient ﬁngerprint information for image attribution without deterioration; for LSUN, a patch of size 52×52
(a) Non-selected samples
(b) Selected samples
Figure 5. Visual comparisons between (a) arbitrary face samples
and (b) selected samples with top 10% Perceptual Similarity 
to CelebA real dataset. We notice the selected samples have higher
quality and fewer artifacts. They are also more similar to each
other, which challenge more on attribution.
Table 6. Evaluation on the 10% selected images of {real, Pro-
GAN seed v#i}. The best performance is highlighted in bold.
Eigenface 
Inception 
Our ﬁngerprint
or larger carries a sufﬁcient ﬁngerprint.
Artifact-free subset.
Throughout our experiments, the
state-of-the-art GAN approaches are capable of generating
high-quality images – but are also generating obvious artifacts in some cases. There is a concern that attribution
might be biased by such artifacts. In order to eliminate this
concern, we use Perceptual Similariy to measure the 1nearest-neighbor similarity between each testing generated
image and the real-world dataset, and then select the 10%
with the highest similarity for attribution. We compare face
samples between non-selected and selected sets in Figure 5
and compare bedroom samples in the supplementary material. We notice this metric is visually effective in selecting
samples of higher quality and with fewer artifacts.
GAN seed v#i},
we show the performance evaluation
in Table 6.
All the FD ratio measures consistently decreased compared to Table 3. This indicates our selection
also moves the image distributions from different GAN
instances closer to the real dataset and consequently
closer to each other. This makes the attribution task more
challenging.
Encouragingly, our classiﬁer, pre-trained
on non-selected images, can perform equally well on the
selected high-quality images and is hence not biased by
artifacts.
4.4. Immunizability: how robust is attribution to
image perturbation attacks and how effective
are the defenses?
We apply ﬁve types of attacks that perturb testing images : noise, blur, cropping, JPEG compression,
relighting, and random combination of them. The intention
is to confuse the attribution network by destroying image
ﬁngerprints. Examples of the perturbations on face images
are shown in Figure 6. Examples on bedroom images are
shown in the supplementary material.
Noise adds i.i.d. Gaussian noise to testing images. The
Gaussian variance is randomly sampled from U[5.0, 20.0].
Blur performs Gaussian ﬁltering on testing images with kernel size randomly picked from {1, 3, 5, 7, 9}. Cropping crops testing images with a random offset between 5%
and 20% of the image side lengths and then resizes back to
the original. JPEG compression performs JPEG compression processing with quality factor randomly sampled from
U . Relighting uses SfSNet to replace the current image lighting condition with another random one from
their lighting dataset. The combination performs each attack with a 50% probability in the order of relighting, cropping, blur, JPEG compression, and noise.
Given perturbed images and the setup of {real, Pro-
GAN seed v#i}, we show the pre-trained classiﬁer performances in the “Akt” columns in Table 7 and Table 8. All
performances decrease due to attacks. In detail, the classiﬁer completely fails to overcome noise and JPEG compression attacks. It still performs better than random when
facing the other four types of attacks. The relighting attack is the least effective one because it only perturbs lowfrequency image components. The barely unchanged ﬁngerprints in high-frequency components enables reasonable
attribution.
In order to immunize our classiﬁer against attacks, we ﬁnetune the classiﬁer under the assumption that
we know the attack category. Given perturbed images and
the setup of {real, ProGAN seed v#i}, we show the ﬁnetuned classiﬁer performance in the “Dfs” columns in Table 7 and Table 8. It turns out that the immunized classiﬁer
completely regains performance over blur, cropping and relighting attacks, and partially regains performance over the
others. However, the recovery from combination attack is
minimal due to its highest complexity.
In addition, our
method consistently outperforms the method of Marra et
al. under each attack after immunization, while theirs
does not effectively beneﬁt from such immunization.
4.5. Fingerprint visualization
Given the setup of {real, ProGAN seed v#i}, we alternatively apply the ﬁngerprint visualization network (Section 3.3) to attribute images. We show the attribution performance in the “Our visNet” row in Table 3, which are com-
(a) No attack
(d) Cropping
(e) Compression
(f) Relighting
(g) Combination
Figure 6. Image samples for the attacks and defenses of our attribution network.
Table 7. Classiﬁcation accuracy (%) of our network w.r.t. different perturbation attacks before or after immunization on CelebA {real,
ProGAN seed v#i}. The best performance is highlighted in bold.
Compression
Relighting
Combination
Table 8. Classiﬁcation accuracy (%) of our network w.r.t. different perturbation attacks before or after immunization on LSUN bedroom
{real, ProGAN seed v#i}. The best performance is highlighted in bold.
Compression
Relighting
Combination
Figure 7. Visualization of model and image ﬁngerprint samples.
Their pairwise interactions are shown as the confusion matrix.
parable to that of the attribution model. Figure 7 visualizes
face ﬁngerprints. Bedroom ﬁngerprints are shown in the
supplementary material. It turns out that image ﬁngerprints
maximize responses only to their own model ﬁngerprints,
which supports effective attribution. To attribute the realworld image, it is sufﬁcient for the ﬁngerprint to focus only
on the eyes. To attribute the other images, the ﬁngerprints
also consider clues from the background, which, compared
to foreground faces, is more variant and harder for GANs to
approximate realistically .
5. Conclusion
We have presented the ﬁrst study of learning GAN ﬁngerprints towards image attribution. Our experiments show
that even a small difference in GAN training (e.g., the difference in initialization) can leave a distinct ﬁngerprint that
commonly exists over all its generated images. That enables
ﬁne-grained image attribution and model attribution. Further encouragingly, ﬁngerprints are persistent across different frequencies and different patch sizes, and are not biased
by GAN artifacts. Even though ﬁngerprints can be deteriorated by several image perturbation attacks, they are effectively immunizable by simple ﬁnetuning. Comparisons
also show that, in a variety of conditions, our learned ﬁngerprints are consistently superior to the very recent baseline for attribution, and consistently outperform inception features for cross-source distinguishability.
Acknowledgement
This project was partially funded by DARPA MediFor
program under cooperative agreement FA87501620191.
We acknowledge the Maryland Advanced Research Computing Center for providing computing resources. We thank
Hao Zhou for helping with the relighting experiments. We
also thank Yaser Yacoob and Abhinav Shrivastava for constructive advice in general.