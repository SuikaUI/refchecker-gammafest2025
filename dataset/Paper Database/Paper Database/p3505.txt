End-to-End Object Detection with Transformers
Nicolas Carion⋆, Francisco Massa⋆, Gabriel Synnaeve, Nicolas Usunier,
Alexander Kirillov, and Sergey Zagoruyko
Facebook AI
Abstract. We present a new method that views object detection as a
direct set prediction problem. Our approach streamlines the detection
pipeline, eﬀectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation
that explicitly encode our prior knowledge about the task. The main
ingredients of the new framework, called DEtection TRansformer or
DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given
a ﬁxed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output
the ﬁnal set of predictions in parallel. The new model is conceptually
simple and does not require a specialized library, unlike many other
modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-
CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation
in a uniﬁed manner. We show that it signiﬁcantly outperforms competitive baselines. Training code and pretrained models are available at
 
Introduction
The goal of object detection is to predict a set of bounding boxes and category
labels for each object of interest. Modern detectors address this set prediction
task in an indirect way, by deﬁning surrogate regression and classiﬁcation problems on a large set of proposals , anchors , or window centers .
Their performances are signiﬁcantly inﬂuenced by postprocessing steps to collapse near-duplicate predictions, by the design of the anchor sets and by the
heuristics that assign target boxes to anchors . To simplify these pipelines,
we propose a direct set prediction approach to bypass the surrogate tasks. This
end-to-end philosophy has led to signiﬁcant advances in complex structured prediction tasks such as machine translation or speech recognition, but not yet in
object detection: previous attempts either add other forms of prior
knowledge, or have not proven to be competitive with strong baselines on challenging benchmarks. This paper aims to bridge this gap.
⋆Equal contribution
 
Carion et al.
transformer
encoderdecoder
set of box predictions
bipartite matching loss
no object (ø)
no object (ø)
set of image features
Fig. 1: DETR directly predicts (in parallel) the ﬁnal set of detections by combining
a common CNN with a transformer architecture. During training, bipartite matching
uniquely assigns predictions with ground truth boxes. Prediction with no match should
yield a “no object” (∅) class prediction.
We streamline the training pipeline by viewing object detection as a direct set
prediction problem. We adopt an encoder-decoder architecture based on transformers , a popular architecture for sequence prediction. The self-attention
mechanisms of transformers, which explicitly model all pairwise interactions between elements in a sequence, make these architectures particularly suitable for
speciﬁc constraints of set prediction such as removing duplicate predictions.
Our DEtection TRansformer (DETR, see Figure 1) predicts all objects at
once, and is trained end-to-end with a set loss function which performs bipartite matching between predicted and ground-truth objects. DETR simpliﬁes the
detection pipeline by dropping multiple hand-designed components that encode
prior knowledge, like spatial anchors or non-maximal suppression. Unlike most
existing detection methods, DETR doesn’t require any customized layers, and
thus can be reproduced easily in any framework that contains standard CNN
and transformer classes.1.
Compared to most previous work on direct set prediction, the main features of
DETR are the conjunction of the bipartite matching loss and transformers with
(non-autoregressive) parallel decoding . In contrast, previous work
focused on autoregressive decoding with RNNs . Our matching
loss function uniquely assigns a prediction to a ground truth object, and is
invariant to a permutation of predicted objects, so we can emit them in parallel.
We evaluate DETR on one of the most popular object detection datasets,
COCO , against a very competitive Faster R-CNN baseline . Faster R-
CNN has undergone many design iterations and its performance was greatly
improved since the original publication. Our experiments show that our new
model achieves comparable performances. More precisely, DETR demonstrates
signiﬁcantly better performance on large objects, a result likely enabled by the
non-local computations of the transformer. It obtains, however, lower performances on small objects. We expect that future work will improve this aspect
in the same way the development of FPN did for Faster R-CNN.
Training settings for DETR diﬀer from standard object detectors in multiple ways. The new model requires extra-long training schedule and beneﬁts
1 In our work we use standard implementations of Transformers and ResNet 
backbones from standard deep learning libraries.
End-to-End Object Detection with Transformers
from auxiliary decoding losses in the transformer. We thoroughly explore what
components are crucial for the demonstrated performance.
The design ethos of DETR easily extend to more complex tasks. In our
experiments, we show that a simple segmentation head trained on top of a pretrained DETR outperfoms competitive baselines on Panoptic Segmentation ,
a challenging pixel-level recognition task that has recently gained popularity.
Related work
Our work build on prior work in several domains: bipartite matching losses for
set prediction, encoder-decoder architectures based on the transformer, parallel
decoding, and object detection methods.
Set Prediction
There is no canonical deep learning model to directly predict sets. The basic set
prediction task is multilabel classiﬁcation (see e.g., for references in the
context of computer vision) for which the baseline approach, one-vs-rest, does
not apply to problems such as detection where there is an underlying structure
between elements (i.e., near-identical boxes). The ﬁrst diﬃculty in these tasks
is to avoid near-duplicates. Most current detectors use postprocessings such as
non-maximal suppression to address this issue, but direct set prediction are
postprocessing-free. They need global inference schemes that model interactions
between all predicted elements to avoid redundancy. For constant-size set prediction, dense fully connected networks are suﬃcient but costly. A general
approach is to use auto-regressive sequence models such as recurrent neural networks . In all cases, the loss function should be invariant by a permutation of
the predictions. The usual solution is to design a loss based on the Hungarian algorithm , to ﬁnd a bipartite matching between ground-truth and prediction.
This enforces permutation-invariance, and guarantees that each target element
has a unique match. We follow the bipartite matching loss approach. In contrast
to most prior work however, we step away from autoregressive models and use
transformers with parallel decoding, which we describe below.
Transformers and Parallel Decoding
Transformers were introduced by Vaswani et al. as a new attention-based
building block for machine translation. Attention mechanisms are neural network layers that aggregate information from the entire input sequence. Transformers introduced self-attention layers, which, similarly to Non-Local Neural
Networks , scan through each element of a sequence and update it by aggregating information from the whole sequence. One of the main advantages of
attention-based models is their global computations and perfect memory, which
makes them more suitable than RNNs on long sequences. Transformers are now
Carion et al.
replacing RNNs in many problems in natural language processing, speech processing and computer vision .
Transformers were ﬁrst used in auto-regressive models, following early sequenceto-sequence models , generating output tokens one by one. However, the prohibitive inference cost (proportional to output length, and hard to batch) lead
to the development of parallel sequence generation, in the domains of audio ,
machine translation , word representation learning , and more recently
speech recognition . We also combine transformers and parallel decoding for
their suitable trade-oﬀbetween computational cost and the ability to perform
the global computations required for set prediction.
Object detection
Most modern object detection methods make predictions relative to some initial guesses. Two-stage detectors predict boxes w.r.t. proposals, whereas
single-stage methods make predictions w.r.t. anchors or a grid of possible
object centers . Recent work demonstrate that the ﬁnal performance
of these systems heavily depends on the exact way these initial guesses are set.
In our model we are able to remove this hand-crafted process and streamline the
detection process by directly predicting the set of detections with absolute box
prediction w.r.t. the input image rather than an anchor.
Set-based loss. Several object detectors used the bipartite matching
loss. However, in these early deep learning models, the relation between diﬀerent
prediction was modeled with convolutional or fully-connected layers only and a
hand-designed NMS post-processing can improve their performance. More recent
detectors use non-unique assignment rules between ground truth and
predictions together with an NMS.
Learnable NMS methods and relation networks explicitly model
relations between diﬀerent predictions with attention. Using direct set losses,
they do not require any post-processing steps. However, these methods employ
additional hand-crafted context features like proposal box coordinates to model
relations between detections eﬃciently, while we look for solutions that reduce
the prior knowledge encoded in the model.
Recurrent detectors. Closest to our approach are end-to-end set predictions
for object detection and instance segmentation . Similarly to us,
they use bipartite-matching losses with encoder-decoder architectures based on
CNN activations to directly produce a set of bounding boxes. These approaches,
however, were only evaluated on small datasets and not against modern baselines.
In particular, they are based on autoregressive models (more precisely RNNs),
so they do not leverage the recent transformers with parallel decoding.
The DETR model
Two ingredients are essential for direct set predictions in detection: (1) a set
prediction loss that forces unique matching between predicted and ground truth
End-to-End Object Detection with Transformers
boxes; (2) an architecture that predicts (in a single pass) a set of objects and
models their relation. We describe our architecture in detail in Figure 2.
Object detection set prediction loss
DETR infers a ﬁxed-size set of N predictions, in a single pass through the
decoder, where N is set to be signiﬁcantly larger than the typical number of
objects in an image. One of the main diﬃculties of training is to score predicted
objects (class, position, size) with respect to the ground truth. Our loss produces
an optimal bipartite matching between predicted and ground truth objects, and
then optimize object-speciﬁc (bounding box) losses.
Let us denote by y the ground truth set of objects, and ˆy = {ˆyi}N
set of N predictions. Assuming N is larger than the number of objects in the
image, we consider y also as a set of size N padded with ∅(no object). To ﬁnd
a bipartite matching between these two sets we search for a permutation of N
elements σ ∈SN with the lowest cost:
ˆσ = arg min
Lmatch(yi, ˆyσ(i)),
where Lmatch(yi, ˆyσ(i)) is a pair-wise matching cost between ground truth yi and
a prediction with index σ(i). This optimal assignment is computed eﬃciently
with the Hungarian algorithm, following prior work (e.g. ).
The matching cost takes into account both the class prediction and the similarity of predicted and ground truth boxes. Each element i of the ground truth
set can be seen as a yi = (ci, bi) where ci is the target class label (which
may be ∅) and bi ∈ 4 is a vector that deﬁnes ground truth box center coordinates and its height and width relative to the image size. For the
prediction with index σ(i) we deﬁne probability of class ci as ˆpσ(i)(ci) and
the predicted box as ˆbσ(i). With these notations we deﬁne Lmatch(yi, ˆyσ(i)) as
−1{ci̸=∅}ˆpσ(i)(ci) + 1{ci̸=∅}Lbox(bi,ˆbσ(i)).
This procedure of ﬁnding matching plays the same role as the heuristic assignment rules used to match proposal or anchors to ground truth objects
in modern detectors. The main diﬀerence is that we need to ﬁnd one-to-one
matching for direct set prediction without duplicates.
The second step is to compute the loss function, the Hungarian loss for all
pairs matched in the previous step. We deﬁne the loss similarly to the losses of
common object detectors, i.e. a linear combination of a negative log-likelihood
for class prediction and a box loss deﬁned later:
LHungarian(y, ˆy) =
−log ˆpˆσ(i)(ci) + 1{ci̸=∅}Lbox(bi,ˆbˆσ(i))
where ˆσ is the optimal assignment computed in the ﬁrst step (1). In practice, we
down-weight the log-probability term when ci = ∅by a factor 10 to account for
Carion et al.
class imbalance. This is analogous to how Faster R-CNN training procedure balances positive/negative proposals by subsampling . Notice that the matching
cost between an object and ∅doesn’t depend on the prediction, which means
that in that case the cost is a constant. In the matching cost we use probabilities ˆpˆσ(i)(ci) instead of log-probabilities. This makes the class prediction term
commensurable to Lbox(·, ·) (described below), and we observed better empirical
performances.
Bounding box loss. The second part of the matching cost and the Hungarian
loss is Lbox(·) that scores the bounding boxes. Unlike many detectors that do box
predictions as a ∆w.r.t. some initial guesses, we make box predictions directly.
While such approach simplify the implementation it poses an issue with relative
scaling of the loss. The most commonly-used ℓ1 loss will have diﬀerent scales for
small and large boxes even if their relative errors are similar. To mitigate this
issue we use a linear combination of the ℓ1 loss and the generalized IoU loss 
Liou(·, ·) that is scale-invariant. Overall, our box loss is Lbox(bi,ˆbσ(i)) deﬁned as
λiouLiou(bi,ˆbσ(i)) + λL1||bi −ˆbσ(i)||1 where λiou, λL1 ∈R are hyperparameters.
These two losses are normalized by the number of objects inside the batch.
DETR architecture
The overall DETR architecture is surprisingly simple and depicted in Figure 2. It
contains three main components, which we describe below: a CNN backbone to
extract a compact feature representation, an encoder-decoder transformer, and
a simple feed forward network (FFN) that makes the ﬁnal detection prediction.
Unlike many modern detectors, DETR can be implemented in any deep learning framework that provides a common CNN backbone and a transformer architecture implementation with just a few hundred lines. Inference code for DETR
can be implemented in less than 50 lines in PyTorch . We hope that the simplicity of our method will attract new researchers to the detection community.
Backbone. Starting from the initial image ximg ∈R3×H0×W0 (with 3 color
channels2), a conventional CNN backbone generates a lower-resolution activation
map f ∈RC×H×W . Typical values we use are C = 2048 and H, W = H0
Transformer encoder. First, a 1x1 convolution reduces the channel dimension
of the high-level activation map f from C to a smaller dimension d. creating a
new feature map z0 ∈Rd×H×W . The encoder expects a sequence as input, hence
we collapse the spatial dimensions of z0 into one dimension, resulting in a d×HW
feature map. Each encoder layer has a standard architecture and consists of a
multi-head self-attention module and a feed forward network (FFN). Since the
transformer architecture is permutation-invariant, we supplement it with ﬁxed
positional encodings that are added to the input of each attention layer. We
defer to the supplementary material the detailed deﬁnition of the architecture,
which follows the one described in .
2 The input images are batched together, applying 0-padding adequately to ensure
they all have the same dimensions (H0, W0) as the largest image of the batch.
End-to-End Object Detection with Transformers
set of image features
transformer
positional encoding
transformer
object queries
prediction heads
Fig. 2: DETR uses a conventional CNN backbone to learn a 2D representation of an
input image. The model ﬂattens it and supplements it with a positional encoding before
passing it into a transformer encoder. A transformer decoder then takes as input a
small ﬁxed number of learned positional embeddings, which we call object queries, and
additionally attends to the encoder output. We pass each output embedding of the
decoder to a shared feed forward network (FFN) that predicts either a detection (class
and bounding box) or a “no object” class.
Transformer decoder. The decoder follows the standard architecture of the
transformer, transforming N embeddings of size d using multi-headed self- and
encoder-decoder attention mechanisms. The diﬀerence with the original transformer is that our model decodes the N objects in parallel at each decoder layer,
while Vaswani et al. use an autoregressive model that predicts the output
sequence one element at a time. We refer the reader unfamiliar with the concepts
to the supplementary material. Since the decoder is also permutation-invariant,
the N input embeddings must be diﬀerent to produce diﬀerent results. These input embeddings are learnt positional encodings that we refer to as object queries,
and similarly to the encoder, we add them to the input of each attention layer.
The N object queries are transformed into an output embedding by the decoder.
They are then independently decoded into box coordinates and class labels by
a feed forward network (described in the next subsection), resulting N ﬁnal
predictions. Using self- and encoder-decoder attention over these embeddings,
the model globally reasons about all objects together using pair-wise relations
between them, while being able to use the whole image as context.
Prediction feed-forward networks (FFNs). The ﬁnal prediction is computed by a 3-layer perceptron with ReLU activation function and hidden dimension d, and a linear projection layer. The FFN predicts the normalized center
coordinates, height and width of the box w.r.t. the input image, and the linear layer predicts the class label using a softmax function. Since we predict a
ﬁxed-size set of N bounding boxes, where N is usually much larger than the
actual number of objects of interest in an image, an additional special class label ∅is used to represent that no object is detected within a slot. This class
plays a similar role to the “background” class in the standard object detection
approaches.
Auxiliary decoding losses. We found helpful to use auxiliary losses in
decoder during training, especially to help the model output the correct number
Carion et al.
of objects of each class. We add prediction FFNs and Hungarian loss after each
decoder layer. All predictions FFNs share their parameters. We use an additional
shared layer-norm to normalize the input to the prediction FFNs from diﬀerent
decoder layers.
Experiments
We show that DETR achieves competitive results compared to Faster R-CNN
in quantitative evaluation on COCO. Then, we provide a detailed ablation
study of the architecture and loss, with insights and qualitative results. Finally, to show that DETR is a versatile and extensible model, we present results
on panoptic segmentation, training only a small extension on a ﬁxed DETR
model. We provide code and pretrained models to reproduce our experiments at
 
Dataset. We perform experiments on COCO 2017 detection and panoptic segmentation datasets , containing 118k training images and 5k validation
images. Each image is annotated with bounding boxes and panoptic segmentation. There are 7 instances per image on average, up to 63 instances in a single
image in training set, ranging from small to large on the same images. If not
speciﬁed, we report AP as bbox AP, the integral metric over multiple thresholds.
For comparison with Faster R-CNN we report validation AP at the last training
epoch, for ablations we report median over validation results from the last 10
Technical details. We train DETR with AdamW setting the initial transformer’s learning rate to 10−4, the backbone’s to 10−5, and weight decay to 10−4.
All transformer weights are initialized with Xavier init , and the backbone
is with ImageNet-pretrained ResNet model from torchvision with frozen
batchnorm layers. We report results with two diﬀerent backbones: a ResNet-
50 and a ResNet-101. The corresponding models are called respectively DETR
and DETR-R101. Following
 , we also increase the feature resolution by
adding a dilation to the last stage of the backbone and removing a stride from
the ﬁrst convolution of this stage. The corresponding models are called respectively DETR-DC5 and DETR-DC5-R101 (dilated C5 stage). This modiﬁcation
increases the resolution by a factor of two, thus improving performance for small
objects, at the cost of a 16x higher cost in the self-attentions of the encoder,
leading to an overall 2x increase in computational cost. A full comparison of
FLOPs of these models and Faster R-CNN is given in Table 1.
We use scale augmentation, resizing the input images such that the shortest
side is at least 480 and at most 800 pixels while the longest at most 1333 .
To help learning global relationships through the self-attention of the encoder,
we also apply random crop augmentations during training, improving the performance by approximately 1 AP. Speciﬁcally, a train image is cropped with
probability 0.5 to a random rectangular patch which is then resized again to
800-1333. The transformer is trained with default dropout of 0.1. At inference
End-to-End Object Detection with Transformers
Table 1: Comparison with Faster R-CNN with a ResNet-50 and ResNet-101 backbones
on the COCO validation set. The top section shows results for Faster R-CNN models
in Detectron2 , the middle section shows results for Faster R-CNN models with
GIoU , random crops train-time augmentation, and the long 9x training schedule.
DETR models achieve comparable results to heavily tuned Faster R-CNN baselines,
having lower APS but greatly improved APL. We use torchscript Faster R-CNN and
DETR models to measure FLOPS and FPS. Results without R101 in the name correspond to ResNet-50.
GFLOPS/FPS #params AP AP50 AP75 APS APM APL
Faster RCNN-DC5
42.3 21.4 43.5 52.5
Faster RCNN-FPN
43.8 24.2 43.5 52.0
Faster RCNN-R101-FPN
45.9 25.2 45.6 54.6
Faster RCNN-DC5+
44.3 22.9 45.9 55.0
Faster RCNN-FPN+
45.5 26.6 45.4 53.4
Faster RCNN-R101-FPN+
44.0 63.9 47.8 27.2 48.1 56.0
44.2 20.5 45.8 61.1
45.9 22.5 47.3 61.1
46.4 21.9 48.0 61.8
DETR-DC5-R101
44.9 64.7 47.7 23.7 49.5 62.3
time, some slots predict empty class. To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding
conﬁdence. This improves AP by 2 points compared to ﬁltering out empty slots.
Other training hyperparameters can be found in section A.4. For our ablation
experiments we use training schedule of 300 epochs with a learning rate drop
by a factor of 10 after 200 epochs, where a single epoch is a pass over all training images once. Training the baseline model for 300 epochs on 16 V100 GPUs
takes 3 days, with 4 images per GPU (hence a total batch size of 64). For the
longer schedule used to compare with Faster R-CNN we train for 500 epochs
with learning rate drop after 400 epochs. This schedule adds 1.5 AP compared
to the shorter schedule.
Comparison with Faster R-CNN
Transformers are typically trained with Adam or Adagrad optimizers with very
long training schedules and dropout, and this is true for DETR as well. Faster
R-CNN, however, is trained with SGD with minimal data augmentation and
we are not aware of successful applications of Adam or dropout. Despite these
diﬀerences we attempt to make a Faster R-CNN baseline stronger. To align it
with DETR, we add generalized IoU to the box loss, the same random
crop augmentation and long training known to improve results . Results
are presented in Table 1. In the top section we show Faster R-CNN results
from Detectron2 Model Zoo for models trained with the 3x schedule. In the
middle section we show results (with a “+”) for the same models but trained
Carion et al.
Table 2: Eﬀect of encoder size. Each row corresponds to a model with varied number
of encoder layers and ﬁxed number of decoder layers. Performance gradually improves
with more encoder layers.
GFLOPS/FPS
with the 9x schedule (109 epochs) and the described enhancements, which in
total adds 1-2 AP. In the last section of Table 1 we show the results for multiple
DETR models. To be comparable in the number of parameters we choose a
model with 6 transformer and 6 decoder layers of width 256 with 8 attention
heads. Like Faster R-CNN with FPN this model has 41.3M parameters, out of
which 23.5M are in ResNet-50, and 17.8M are in the transformer. Even though
both Faster R-CNN and DETR are still likely to further improve with longer
training, we can conclude that DETR can be competitive with Faster R-CNN
with the same number of parameters, achieving 42 AP on the COCO val subset.
The way DETR achieves this is by improving APL (+7.8), however note that the
model is still lagging behind in APS (-5.5). DETR-DC5 with the same number
of parameters and similar FLOP count has higher AP, but is still signiﬁcantly
behind in APS too. Faster R-CNN and DETR with ResNet-101 backbone show
comparable results as well.
Attention mechanisms in the transformer decoder are the key components which
model relations between feature representations of diﬀerent detections. In our
ablation analysis, we explore how other components of our architecture and loss
inﬂuence the ﬁnal performance. For the study we choose ResNet-50-based DETR
model with 6 encoder, 6 decoder layers and width 256. The model has 41.3M
parameters, achieves 40.6 and 42.0 AP on short and long schedules respectively,
and runs at 28 FPS, similarly to Faster R-CNN-FPN with the same backbone.
Number of encoder layers. We evaluate the importance of global imagelevel self-attention by changing the number of encoder layers (Table 2). Without
encoder layers, overall AP drops by 3.9 points, with a more signiﬁcant drop of
6.0 AP on large objects. We hypothesize that, by using global scene reasoning,
the encoder is important for disentangling objects. In Figure 3, we visualize the
attention maps of the last encoder layer of a trained model, focusing on a few
points in the image. The encoder seems to separate instances already, which
likely simpliﬁes object extraction and localization for the decoder.
Number of decoder layers. We apply auxiliary losses after each decoding
layer (see Section 3.2), hence, the prediction FFNs are trained by design to pre-
End-to-End Object Detection with Transformers
self-attention(430, 600)
self-attention(520, 450)
self-attention(450, 830)
self-attention(440, 1200)
Fig. 3: Encoder self-attention for a set of reference points. The encoder is able to separate individual instances. Predictions are made with baseline DETR model on a validation set image.
dict objects out of the outputs of every decoder layer. We analyze the importance
of each decoder layer by evaluating the objects that would be predicted at each
stage of the decoding (Fig. 4). Both AP and AP50 improve after every layer,
totalling into a very signiﬁcant +8.2/9.5 AP improvement between the ﬁrst and
the last layer. With its set-based loss, DETR does not need NMS by design. To
verify this we run a standard NMS procedure with default parameters for
the outputs after each decoder. NMS improves performance for the predictions
from the ﬁrst decoder. This can be explained by the fact that a single decoding
layer of the transformer is not able to compute any cross-correlations between
the output elements, and thus it is prone to making multiple predictions for the
same object. In the second and subsequent layers, the self-attention mechanism
over the activations allows the model to inhibit duplicate predictions. We observe that the improvement brought by NMS diminishes as depth increases. At
the last layers, we observe a small loss in AP as NMS incorrectly removes true
positive predictions.
Similarly to visualizing encoder attention, we visualize decoder attentions in
Fig. 6, coloring attention maps for each predicted object in diﬀerent colors. We
observe that decoder attention is fairly local, meaning that it mostly attends to
object extremities such as heads or legs. We hypothesise that after the encoder
has separated instances via global attention, the decoder only needs to attend
to the extremities to extract the class and object boundaries.
Importance of FFN. FFN inside tranformers can be seen as 1 × 1 convolutional layers, making encoder similar to attention augmented convolutional
networks . We attempt to remove it completely leaving only attention in the
transformer layers. By reducing the number of network parameters from 41.3M
to 28.7M, leaving only 10.8M in the transformer, performance drops by 2.3 AP,
we thus conclude that FFN are important for achieving good results.
Importance of positional encodings. There are two kinds of positional encodings in our model: spatial positional encodings and output positional encod-
Carion et al.
decoder layer
AP NMS=0.7
decoder layer
AP NMS=0.7
AP50 No NMS
AP50 NMS=0.7
Fig. 4: AP and AP50 performance after each decoder layer. A single long schedule baseline model
is evaluated. DETR does not need NMS by design, which is validated by this ﬁgure. NMS lowers
AP in the ﬁnal layers, removing TP predictions,
but improves AP in the ﬁrst decoder layers, removing double predictions, as there is no communication in the ﬁrst layer, and slightly improves
Fig. 5: Out of distribution generalization
Even though no image in the
training set has more than 13
giraﬀes, DETR has no diﬃculty generalizing to 24 and
more instances of the same
ings (object queries). We experiment with various combinations of ﬁxed and
learned encodings, results can be found in table 3. Output positional encodings
are required and cannot be removed, so we experiment with either passing them
once at decoder input or adding to queries at every decoder attention layer. In
the ﬁrst experiment we completely remove spatial positional encodings and pass
output positional encodings at input and, interestingly, the model still achieves
more than 32 AP, losing 7.8 AP to the baseline. Then, we pass ﬁxed sine spatial
positional encodings and the output encodings at input once, as in the original
transformer , and ﬁnd that this leads to 1.4 AP drop compared to passing
the positional encodings directly in attention. Learned spatial encodings passed
to the attentions give similar results. Surprisingly, we ﬁnd that not passing any
spatial encodings in the encoder only leads to a minor AP drop of 1.3 AP. When
we pass the encodings to the attentions, they are shared across all layers, and
the output encodings (object queries) are always learned.
Given these ablations, we conclude that transformer components: the global
self-attention in encoder, FFN, multiple decoder layers, and positional encodings,
all signiﬁcantly contribute to the ﬁnal object detection performance.
Loss ablations. To evaluate the importance of diﬀerent components of the
matching cost and the loss, we train several models turning them on and oﬀ.
There are three components to the loss: classiﬁcation loss, ℓ1 bounding box
distance loss, and GIoU loss. The classiﬁcation loss is essential for training
and cannot be turned oﬀ, so we train a model without bounding box distance
loss, and a model without the GIoU loss, and compare with baseline, trained with
all three losses. Results are presented in table 4. GIoU loss on its own accounts
End-to-End Object Detection with Transformers
Fig. 6: Visualizing decoder attention for every predicted object (images from COCO
val set). Predictions are made with DETR-DC5 model. Attention scores are coded with
diﬀerent colors for diﬀerent objects. Decoder typically attends to object extremities,
such as legs and heads. Best viewed in color.
Table 3: Results for diﬀerent positional encodings compared to the baseline (last row),
which has ﬁxed sine pos. encodings passed at every attention layer in both the encoder
and the decoder. Learned embeddings are shared between all layers. Not using spatial
positional encodings leads to a signiﬁcant drop in AP. Interestingly, passing them in
decoder only leads to a minor AP drop. All these models use learned output positional
encodings.
spatial pos. enc.
output pos. enc.
learned at input
sine at input
sine at input
learned at input
learned at attn.
learned at attn.
learned at attn.
sine at attn.
learned at attn.
sine at attn.
sine at attn.
learned at attn.
Table 4: Eﬀect of loss components on AP. We train two models turning oﬀℓ1 loss, and
GIoU loss, and observe that ℓ1 gives poor results on its own, but when combined with
GIoU improves APM and APL. Our baseline (last row) combines both losses.
for most of the model performance, losing only 0.7 AP to the baseline with
combined losses. Using ℓ1 without GIoU shows poor results. We only studied
Carion et al.
Fig. 7: Visualization of all box predictions on all images from COCO 2017 val set
for 20 out of total N = 100 prediction slots in DETR decoder. Each box prediction is
represented as a point with the coordinates of its center in the 1-by-1 square normalized
by each image size. The points are color-coded so that green color corresponds to small
boxes, red to large horizontal boxes and blue to large vertical boxes. We observe that
each slot learns to specialize on certain areas and box sizes with several operating
modes. We note that almost all slots have a mode of predicting large image-wide boxes
that are common in COCO dataset.
simple ablations of diﬀerent losses (using the same weighting every time), but
other means of combining them may achieve diﬀerent results.
Decoder output slot analysis In Fig. 7 we visualize the boxes predicted
by diﬀerent slots for all images in COCO 2017 val set. DETR learns diﬀerent
specialization for each query slot. We observe that each slot has several modes of
operation focusing on diﬀerent areas and box sizes. In particular, all slots have
the mode for predicting image-wide boxes (visible as the red dots aligned in the
middle of the plot). We hypothesize that this is related to the distribution of
objects in COCO.
Generalization to unseen numbers of instances. Some classes in COCO
are not well represented with many instances of the same class in the same
image. For example, there is no image with more than 13 giraﬀes in the training
set. We create a synthetic image3 to verify the generalization ability of DETR
(see Figure 5). Our model is able to ﬁnd all 24 giraﬀes on the image which
is clearly out of distribution. This experiment conﬁrms that there is no strong
class-specialization in each object query.
DETR for panoptic segmentation
Panoptic segmentation has recently attracted a lot of attention from the
computer vision community. Similarly to the extension of Faster R-CNN to
Mask R-CNN , DETR can be naturally extended by adding a mask head on
top of the decoder outputs. In this section we demonstrate that such a head can
be used to produce panoptic segmentation by treating stuﬀand thing classes
3 Base picture credit: 
End-to-End Object Detection with Transformers
Multi head attention
Input image
(3 x H x W)
Box embeddings
Encoded image
(d x H/32 x W/32)
Attention maps
(N x M x H/32 x W/32)
Masks logits
(N x H/4 x W/4)
Pixel-wise argmax
Concatenate
2 x (Conv 3x3 + GN + ReLU)
2x up + add
Conv 3x3 + GN + ReLU
2x up + add
Conv 3x3 + GN + ReLU
2x up + add
Conv 3x3 + GN + ReLU + Conv 3x3
FPN-style CNN
Resnet features
Fig. 8: Illustration of the panoptic head. A binary mask is generated in parallel for each
detected object, then the masks are merged using pixel-wise argmax.
Fig. 9: Qualitative results for panoptic segmentation generated by DETR-R101. DETR
produces aligned mask predictions in a uniﬁed manner for things and stuﬀ.
in a uniﬁed way. We perform our experiments on the panoptic annotations of the
COCO dataset that has 53 stuﬀcategories in addition to 80 things categories.
We train DETR to predict boxes around both stuﬀand things classes on
COCO, using the same recipe. Predicting boxes is required for the training to
be possible, since the Hungarian matching is computed using distances between
boxes. We also add a mask head which predicts a binary mask for each of the
predicted boxes, see Figure 8. It takes as input the output of transformer decoder
for each object and computes multi-head (with M heads) attention scores of this
embedding over the output of the encoder, generating M attention heatmaps
per object in a small resolution. To make the ﬁnal prediction and increase the
resolution, an FPN-like architecture is used. We describe the architecture in
more details in the supplement. The ﬁnal resolution of the masks has stride 4
and each mask is supervised independently using the DICE/F-1 loss and
Focal loss .
The mask head can be trained either jointly, or in a two steps process, where
we train DETR for boxes only, then freeze all the weights and train only the mask
head for 25 epochs. Experimentally, these two approaches give similar results, we
report results using the latter method since it results in a shorter total wall-clock
time training.
Carion et al.
Table 5: Comparison with the state-of-the-art methods UPSNet and Panoptic
FPN on the COCO val dataset We retrained PanopticFPN with the same dataaugmentation as DETR, on a 18x schedule for fair comparison. UPSNet uses the 1x
schedule, UPSNet-M is the version with multiscale test-time augmentations.
PQth SQth RQth PQst SQst RQst
PanopticFPN++
PanopticFPN++
37.3 78.7 46.5
45.1 79.9 55.5
To predict the ﬁnal panoptic segmentation we simply use an argmax over
the mask scores at each pixel, and assign the corresponding categories to the
resulting masks. This procedure guarantees that the ﬁnal masks have no overlaps
and, therefore, DETR does not require a heuristic that is often used to align
diﬀerent masks.
Training details. We train DETR, DETR-DC5 and DETR-R101 models following the recipe for bounding box detection to predict boxes around stuﬀand
things classes in COCO dataset. The new mask head is trained for 25 epochs
(see supplementary for details). During inference we ﬁrst ﬁlter out the detection
with a conﬁdence below 85%, then compute the per-pixel argmax to determine
in which mask each pixel belongs. We then collapse diﬀerent mask predictions
of the same stuﬀcategory in one, and ﬁlter the empty ones (less than 4 pixels).
Main results. Qualitative results are shown in Figure 9. In table 5 we compare
our uniﬁed panoptic segmenation approach with several established methods
that treat things and stuﬀdiﬀerently. We report the Panoptic Quality (PQ) and
the break-down on things (PQth) and stuﬀ(PQst). We also report the mask
AP (computed on the things classes), before any panoptic post-treatment (in
our case, before taking the pixel-wise argmax). We show that DETR outperforms published results on COCO-val 2017, as well as our strong PanopticFPN
baseline (trained with same data-augmentation as DETR, for fair comparison).
The result break-down shows that DETR is especially dominant on stuﬀclasses,
and we hypothesize that the global reasoning allowed by the encoder attention
is the key element to this result. For things class, despite a severe deﬁcit of
up to 8 mAP compared to the baselines on the mask AP computation, DETR
obtains competitive PQth. We also evaluated our method on the test set of the
COCO dataset, and obtained 46 PQ. We hope that our approach will inspire the
exploration of fully uniﬁed models for panoptic segmentation in future work.
End-to-End Object Detection with Transformers
Conclusion
We presented DETR, a new design for object detection systems based on transformers and bipartite matching loss for direct set prediction. The approach
achieves comparable results to an optimized Faster R-CNN baseline on the challenging COCO dataset. DETR is straightforward to implement and has a ﬂexible
architecture that is easily extensible to panoptic segmentation, with competitive
results. In addition, it achieves signiﬁcantly better performance on large objects
than Faster R-CNN, likely thanks to the processing of global information performed by the self-attention.
This new design for detectors also comes with new challenges, in particular
regarding training, optimization and performances on small objects. Current
detectors required several years of improvements to cope with similar issues,
and we expect future work to successfully address them for DETR.
Acknowledgements
We thank Sainbayar Sukhbaatar, Piotr Bojanowski, Natalia Neverova, David
Lopez-Paz, Guillaume Lample, Danielle Rothermel, Kaiming He, Ross Girshick,
Xinlei Chen and the whole Facebook AI Research Paris team for discussions and
advices without which this work would not be possible.