Proceedings of the First Workshop on Neural Machine Translation, pages 28–39,
Vancouver, Canada, August 4, 2017. c⃝2017 Association for Computational Linguistics
Six Challenges for Neural Machine Translation
Philipp Koehn
Computer Science Department
Johns Hopkins University
 
Rebecca Knowles
Computer Science Department
Johns Hopkins University
 
We explore six challenges for neural
machine translation:
domain mismatch,
amount of training data, rare words, long
sentences, word alignment, and beam
We show both deﬁciencies and
improvements over the quality of phrasebased statistical machine translation.
Introduction
Neural machine translation has emerged as the
most promising machine translation approach in
recent years, showing superior performance on
public benchmarks and rapid
adoption in deployments by, e.g., Google , Systran , and
WIPO . But there
have also been reports of poor performance, such
as the systems built under low-resource conditions
in the DARPA LORELEI program.1
In this paper, we examine a number of challenges to neural machine translation (NMT) and
give empirical results on how well the technology
currently holds up, compared to traditional statistical machine translation (SMT).
We ﬁnd that:
1. NMT systems have lower quality out of domain, to the point that they completely sacri-
ﬁce adequacy for the sake of ﬂuency.
2. NMT systems have a steeper learning curve
with respect to the amount of training data,
resulting in worse quality in low-resource
settings, but better performance in highresource settings.
1 
3. NMT systems that operate at the sub-word
level (e.g. with byte-pair encoding) perform
better than SMT systems on extremely lowfrequency words, but still show weakness in
translating low-frequency words belonging to
highly-inﬂected categories (e.g. verbs).
4. NMT systems have lower translation quality
on very long sentences, but do comparably
better up to a sentence length of about 60
5. The attention model for NMT does not always fulﬁll the role of a word alignment
model, but may in fact dramatically diverge.
6. Beam search decoding only improves translation quality for narrow beams and deteriorates when exposed to a larger search space.
We note a 7th challenge that we do not examine empirically: NMT systems are much less interpretable. The answer to the question of why
the training data leads these systems to decide on
speciﬁc word choices during decoding is buried in
large matrices of real-numbered values. There is a
clear need to develop better analytics for NMT.
Other studies have looked at the comparable
performance of NMT and SMT systems.
Bentivogli et al. considered different linguistic categories for English–German and Toral
and S´anchez-Cartagena compared different
broad aspects such as ﬂuency and reordering for
nine language directions.
Experimental Setup
We use common toolkits for neural machine translation (Nematus) and traditional phrase-based statistical machine translation (Moses) with common
data sets, drawn from WMT and OPUS.
Neural Machine Translation
While a variety of neural machine translation approaches were initially proposed — such as the
use of convolutional neural networks — practically all recent work has been focused on the attention-based
encoder-decoder model .
We use the toolkit Nematus2 which has been shown to give state-of-theart results at the WMT
2016 evaluation campaign .
Unless noted otherwise, we use default settings,
such as beam search and single model decoding.
The training data is processed with byte-pair encoding into subwords to
ﬁt a 50,000 word vocabulary limit.
Statistical Machine Translation
Our machine translation systems are trained using
Moses3 . We build phrasebased systems using standard features that are
commonly used in recent system submissions to
WMT .
While we use the shorthand SMT for these
phrase-based systems, we note that there are other
statistical machine translation approaches such as
hierarchical phrase-based models 
and syntax-based models that have been shown to give superior performance for language pairs such as Chinese–
English and German–English.
Data Conditions
We carry out our experiments on English–Spanish
and German–English. For these language pairs,
large training data sets are available.
datasets from the shared translation task organized
alongside the Conference on Machine Translation
(WMT)4. For the domain experiments, we use the
OPUS corpus5 .
Except for the domain experiments, we use the
WMT test sets composed of news stories, which
are characterized by a broad range of topic, formal language, relatively long sentences (about 30
words on average), and high standards for grammar, orthography, and style.
2 
3 
4 
5 
Law (Acquis)
18,128,173
Medical (EMEA)
14,301,472
Koran (Tanzil)
114,371,754
13,873,398
Table 1: Corpora used to train domain-speciﬁc
systems, taken from the OPUS repository.
corpora are GNOME, KDE, PHP, Ubuntu, and
OpenOfﬁce.
Challenges
Domain Mismatch
A known challenge in translation is that in different domains,6 words have different translations and meaning is expressed in different styles.
Hence, a crucial step in developing machine translation systems targeted at a speciﬁc use case is
domain adaptation. We expect that methods for
domain adaptation will be developed for NMT. A
currently popular approach is to train a general domain system, followed by training on in-domain
data for a few epochs .
Often, large amounts of training data are only
available out of domain, but we still seek to have
robust performance. To test how well NMT and
SMT hold up, we trained ﬁve different systems using different corpora obtained from OPUS . An additional system was trained on
all the training data. Statistics about corpus sizes
are shown in Table 1. Note that these domains are
quite distant from each other, much more so than,
say, Europarl, TED Talks, News Commentary, and
Global Voices.
We trained both SMT and NMT systems for all
domains. All systems were trained for German-
English, with tuning and test sets sub-sampled
from the data (these were not used in training). A
common byte-pair encoding is used for all training
See Figure 1 for results. While the in-domain
NMT and SMT systems are similar (NMT is better
for IT and Subtitles, SMT is better for Law, Medical, and Koran), the out-of-domain performance
for the NMT systems is worse in almost all cases,
sometimes dramatically so. For instance the Med-
6We use the customary deﬁnition of domain in machine
translation: a domain is deﬁned by a corpus from a speciﬁc
source, and may differ from other domains in topic, genre,
style, level of formality, etc.
Figure 1: Quality of systems (BLEU), when trained on one domain (rows) and tested on another domain
(columns). Comparably, NMT systems (left bars) show more degraded performance out of domain.
ical system leads to a BLEU score of 3.9 (NMT)
vs. 10.2 (SMT) on the Law test set.
Figure 2 displays an example. When translating
the sentence Schaue um dich herum. (reference:
Look around you.)
from the Subtitles corpus,
we see mostly non-sensical and completely unrelated output from the NMT system. For instance,
the translation from the IT system is Switches to
Note that the output of the NMT system is often
quite ﬂuent (e.g., Take heed of your own souls.)
but completely unrelated to the input, while the
SMT output betrays its difﬁculties with coping
with the out-of-domain input by leaving some
words untranslated (e.g., Schaue by dich around.).
This is of particular concern when MT is used for
information gisting — the user will be mislead by
hallucinated content in the NMT output.
Amount of Training Data
A well-known property of statistical systems is
that increasing amounts of training data lead to
better results. In SMT systems, we have previously observed that doubling the amount of training data gives a ﬁxed increase in BLEU scores.
This holds true for both parallel and monolingual data .
Schaue um dich herum.
Look around you.
NMT: Look around you.
SMT: Look around you.
NMT: Sughum gravecorn.
SMT: In order to implement dich Schaue .
NMT: EMEA / MB / 049 / 01-EN-Final Work
progamme for 2002
SMT: Schaue by dich around .
NMT: Switches to paused.
SMT: To Schaue by itself . \t \t
NMT: Take heed of your own souls.
SMT: And you see.
NMT: Look around you.
SMT: Look around you .
Figure 2: Examples for the translation of a sentence from the Subtitles corpus, when translated
with systems trained on different corpora.
Performance out-of-domain is dramatically worse for
24.9 26.2 26.9 27.9 28.6 29.2 29.6
21.2 22.2 23.5 24.7
26.1 26.9 27.8 28.6
Corpus Size (English Words)
BLEU Scores with Varying Amounts of Training Data
Phrase-Based with Big LM
Phrase-Based
Figure 3: BLEU scores for English-Spanish systems trained on 0.4 million to 385.7 million
words of parallel data.
Quality for NMT starts
much lower, outperforms SMT at about 15 million words, and even beats a SMT system with a
big 2 billion word in-domain language model under high-resource conditions.
How do the data needs of SMT and NMT compare? NMT promises both to generalize better (exploiting word similary in embeddings) and condition on larger context (entire input and all prior
output words).
We built English-Spanish systems on WMT
data,7 about 385.7 million English words paired
with Spanish. To obtain a learning curve, we used
512, ..., 1
2, and all of the data. For SMT, the
language model was trained on the Spanish part of
each subset, respectively. In addition to a NMT
and SMT system trained on each subset, we also
used all additionally provided monolingual data
for a big language model in contrastive SMT systems.
Results are shown in Figure 3.
NMT exhibits a much steeper learning curve, starting with
abysmal results (BLEU score of 1.6 vs. 16.4 for
1024 of the data), outperforming SMT 25.7 vs.
24.7 with 1
16 of the data (24.1 million words), and
even beating the SMT system with a big language
model with the full data set (31.1 for NMT, 28.4
for SMT, 30.4 for SMT+BigLM).
7Spanish was last represented in 2013, we used data from
 
A Republican strategy to counter the re-election
Un ´organo de coordinaci´on para el anuncio de
libre determinaci´on
Lista de una estrategia para luchar contra la
elecci´on de hojas de Ohio
Explosi´on realiza una estrategia divisiva de
luchar contra las elecciones de autor
Una estrategia republicana para la eliminaci´on
de la reelecci´on de Obama
Estrategia siria para contrarrestar la reelecci´on
del Obama .
Una estrategia republicana para contrarrestar la
reelecci´on de Obama
Translations of the ﬁrst sentence of
the test set using NMT system trained on varying
amounts of training data. Under low resource conditions, NMT produces ﬂuent output unrelated to
the input.
The contrast between the NMT and SMT learning curves is quite striking. While NMT is able to
exploit increasing amounts of training data more
effectively, it is unable to get off the ground with
training corpus sizes of a few million words or
To illustrate this, see Figure 4. With
1024 of the
training data, the output is completely unrelated to
the input, some key words are properly translated
256 of the data (estrategia for strategy, elecci´on or elecciones for election), and starting with 1
64 the translations become respectable.
Rare Words
Conventional wisdom states that neural machine
translation models perform particularly poorly on
rare words, due in part to the
smaller vocabularies used by NMT systems. We
examine this claim by comparing performance on
rare word translation between NMT and SMT
systems of similar quality for German–English
and ﬁnd that NMT systems actually outperform
SMT systems on translation of very infrequent
words. However, both NMT and SMT systems
do continue to have difﬁculty translating some
infrequent words, particularly those belonging to
highly-inﬂected categories.
For the neural machine translation model, we
use a publicly available model8 with the training
settings of Edinburgh’s WMT submission . This was trained using Ne-
8 
Figure 5: Precision of translation and deletion rates by source words type. SMT (light blue) and NMT
(dark green). The horizontal axis represents the corpus frequency of the source types, with the axis labels
showing the upper end of the bin. Bin width is proportional to the number of word types in that frequency
range. The upper part of the graph shows the precision averaged across all word types in the bin. The
lower part shows the proportion of source tokens in the bin that were deleted.
matus9 , with byte-pair encodings to allow for openvocabulary NMT.
The phrase-based model that we used was
trained using Moses , and
the training data and parameters match those described in Johns Hopkins University’s submission
to the WMT shared task .
Both models have case-sensitive BLEU scores
of 34.5 on the WMT 2016 news test set (for the
NMT model, this reﬂects the BLEU score resulting from translation with a beam size of 1).
We use a single corpus for computing our lexical frequency counts (a concatenation of Common
Crawl, Europarl, and News Commentary).
We follow the approach described by Koehn
and Haddow for examining the effect of
source word frequency on translation accuracy.10
9 
10First, we automatically align the source sentence and the
machine translation output. We use fast-align to align the full training corpus (source and reference)
along with the test source and MT output. We use the suggested standard options for alignment and then symmetrize
the alignment with grow-diag-ﬁnal-and.
Each source word is either unaligned (“dropped”) or
aligned to one or more target language words. For each target word to which the source word is aligned, we check if
that target word appears in the reference translation. If the
target word appears the same number of times in the MT output as in the reference, we award that alignment a score of
one. If the target word appears more times in the MT output
than in the reference, we award fractional credit. If the target
word does not appear in the reference, we award zero credit.
The overall average precision is quite similar
between the NMT and SMT systems, with the
SMT system scoring 70.1% overall and the NMT
system scoring 70.3%. This reﬂects the similar
overall quality of the MT systems. Figure 5 gives
a detailed breakdown. The values above the horizontal axis represent precisions, while the lower
portion represents what proportion of the words
were deleted. The ﬁrst item of note is that the
NMT system has an overall higher proportion of
deleted words. Of the 64379 words examined, the
NMT system is estimated to have deleted 3769 of
them, while the SMT system deleted 2274. Both
the NMT and SMT systems delete very frequent
and very infrequent words at higher proportions
than words that fall into the middle range. Across
frequencies, the NMT systems delete a higher proportion of words than the SMT system does. (The
related issue of translation length is discussed in
more detail in Section 3.4.)
The next interesting observation is what happens with unknown words (words which were
never observed in the training corpus). The SMT
system translates these correctly 53.2% of the
time, while the NMT system translates them correctly 60.1% of the time. This is reﬂected in Figure 5, where the SMT system shows a steep curve
We then average these scores over the full set of target words
aligned to the given source word to compute the precision for
that source word. Source words can then be binned by frequency and average translation precisions can be computed.
Unobserved
Observed Once
Named Entity
Table 2: Breakdown of the ﬁrst 100 tokens that
were unobserved in training or observed once in
training, by hand-annotated category.
up from the unobserved words, while the NMT
system does not see a great jump.
Both SMT and NMT systems actually have
their worst performance on words that were observed a single time in the training corpus, dropping to 48.6% and 52.2%, respectively; even
worse than for unobserved words. Table 2 shows
a breakdown of the categories of words that were
unobserved in the training corpus or observed only
once. The most common categories across both
are named entity (including entity and location
names) and nouns.
The named entities can often be passed through unchanged (for example,
the surname “Elabdellaoui” is broken into “E@@
lab@@ d@@ ell@@ a@@ oui” by the bytepair encoding and is correctly passed through unchanged by both the NMT and SMT systems).
Many of the nouns are compound nouns; when
these are correctly translated, it may be attributed
to compound-splitting (SMT) or byte-pair encoding (NMT). The factored SMT system also has access to the stemmed form of words, which can
also play a similar role to byte-pair encoding in
enabling translation of unobserved inﬂected forms
(e.g. adjectives, verbs). Unsurprisingly, there are
many numbers that were unobserved in the training data; these tend to be translated correctly (with
occasional errors due to formatting of commas and
periods, resolvable by post-processing).
The categories which involve more extensive
inﬂection (adjectives and verbs) are arguably the
most interesting. Adjectives and verbs have worse
accuracy rates and higher deletion rates than nouns
across most word frequencies. We show examples
in Figure 6 of situations where the NMT system
succeeds and fails, and contrast it with the failures of the SMT system. In Example 1, the NMT
system successfully translates the unobserved adjective choreographiertes (choreographed), while
the SMT system does not.
In Example 2, the
SMT system simply passes the German verb
(1) ... choreographiertes Gesamtkunstwerk ...
(2) ... die Polizei ihn einkesselte.
(1) chore@@ ograph@@ iertes
(2) ein@@ kes@@ sel@@ te
(1) ... choreographed overall artwork ...
(2) ... police stabbed him.
(1) ... choreographiertes total work of art ...
(2) ... police einkesselte him.
(1) ... choreographed complete work of art ...
(2) ... police closed in on him.
Figure 6: Examples of words that were unobserved in the training corpus, their byte-pair encodings, and their translations.
einkesselte (closed in on) unchanged into the output, while the NMT system fails silently, selecting
the ﬂuent-sounding but semantically inappropriate
“stabbed” instead.
While there remains room for improvement,
NMT systems (at least those using byte-pair encoding) perform better on very low-frequency
words then SMT systems do. Byte-pair encoding
is sometimes sufﬁcient (much like stemming or
compound-splitting) to allow the successful translation of rare words even though it does not necessarily split words at morphological boundaries.
As with the ﬂuent-sounding but semantically inappropriate examples from domain-mismatch, NMT
may sometimes fail similarly when it encounters
unknown words even in-domain.
Long Sentences
A well-known ﬂaw of early encoder-decoder
NMT models was the inability to properly translate long sentences . The introduction of the attention model remedied this problem somewhat.
But how well?
We used the large English-Spanish system from
the learning curve experiments (Section 3.2), and
used it to translate a collection of news test sets
from the WMT shared tasks. We broke up these
sets into buckets based on source sentence length
(1-9 subword tokens, 10-19 subword tokens, etc.)
and computed corpus-level BLEU scores for each.
Figure 7 shows the results. While overall NMT
is better than SMT, the SMT system outperforms
NMT on sentences of length 60 and higher. Quality for the two systems is relatively close, except
for the very long sentences (80 and more tokens).
The quality of the NMT system is dramatically
lower for these since it produces too short translations (length ratio 0.859, opposed to 1.024).
Sentence Length (source, subword count)
BLEU Scores with Varying Sentence Length
Phrase-Based
Figure 7: Quality of translations based on sentence length.
SMT outperforms NMT for sentences longer than 60 subword tokens. For very
long sentences (80+) quality is much worse due to
too short output.
Word Alignment
The key contribution of the attention model in neural machine translation 
was the imposition of an alignment of the output
words to the input words. This takes the shape
of a probability distribution over the input words
which is used to weigh them in a bag-of-words
representation of the input sentence.
Arguably, this attention model does not functionally play the role of a word alignment between
the source in the target, at least not in the same
way as its analog in statistical machine translation.
While in both cases, alignment is a latent variable
that is used to obtain probability distributions over
words or phrases, arguably the attention model has
a broader role. For instance, when translating a
verb, attention may also be paid to its subject and
object since these may disambiguate it. To further complicate matters, the word representations
are products of bidirectional gated recurrent neural networks that have the effect that each word
representation is informed by the entire sentence
But there is a clear need for an alignment mechanism between source and target words. For instance, prior work used the alignments provided
by the attention model to interpolate word translation decisions with traditional probabilistic dictionaries , for the introduction of
coverage and fertility models , etc.
But is the attention model in fact the proper
Beziehungen
angespannt
Figure 8: Word alignment for English–German:
comparing the attention model states (green boxes
with probability in percent if over 10) with alignments obtained from fast-align (blue outlines).
To examine this, we compare the soft
alignment matrix (the sequence of attention vectors) with word alignments obtained by traditional
word alignment methods.
We use incremental
fast-align to align the input and
output of the neural machine system.
See Figure 8 for an illustration. We compare
the word attention states (green boxes) with the
word alignments obtained with fast align (blue
outlines). For most words, these match up pretty
well. Both attention states and fast-align alignment points are a bit fuzzy around the function
words have-been/sind.
However, the attention model may settle on
alignments that do not correspond with our intuition or alignment points obtained with fast-align.
See Figure 9 for the reverse language direction,
German–English. All the alignment points appear
to be off by one position. We are not aware of any
intuitive explanation for this divergent behavior —
the translation quality is high for both systems.
We measure how well the soft alignment (attention model) of the NMT system match the alignments of fast-align with two metrics:
• a match score that checks for each output
if the aligned input word according to fastalign is indeed the input word that received
the highest attention probability, and
• a probability mass score that sums up the
Verh¨altnis
relationship
Figure 9: Mismatch between attention states and
desired word alignments (German–English).
probability mass given to each alignment
point obtained from fast-align.
In these scores, we have to handle byte pair encoding and many-to-many alignments11
In out experiment, we use the neural machine
translation models provided by Edinburgh12 . We run fast-align on the same
parallel data sets to obtain alignment models and
used them to align the input and output of the
NMT system. Table 3 shows alignment scores for
the systems. The results suggest that, while drastic, the divergence for German–English is an outlier. We note, however, that we have seen such
large a divergence also under different data conditions.
Note that the attention model may produce better word alignments by guided alignment training
 where supervised word alignments (such as the ones produced
by fast-align) are provided to model training.
11(1) NMT operates on subwords, but fast-align is run on
full words. (2) If an input word is split into subwords by
byte pair encoding, then we add their attention scores. (3)
If an output word is split into subwords, then we take the
average of their attention vectors. (4) The match scores and
probability mass scores are computed as average over output
word-level scores. (5) If an output word has no fast-align
alignment point, it is ignored in this computation. (6) If an
output word is fast-aligned to multiple input words, then (6a)
for the match score: count it as correct if the n aligned words
among the top n highest scoring words according to attention
and (6b) for the probability mass score: add up their attention
12 
Language Pair
German–English
English–German
Czech–English
English–Czech
Russian–English
English–Russian
Table 3: Scores indicating overlap between attention probabilities and alignments obtained with
fast-align.
Beam Search
The task of decoding is to ﬁnd the full sentence
translation with the highest probability. In statistical machine translation, this problem has been
addressed with heuristic search techniques that explore a subset of the space of possible translation.
A common feature of these search techniques is a
beam size parameter that limits the number of partial translations maintained per input word.
There is typically a straightforward relationship
between this beam size parameter and the model
score of resulting translations and also their quality score (e.g., BLEU). While there are diminishing returns for increasing the beam parameter,
typically improvements in these scores can be expected with larger beams.
Decoding in neural translation models can be
set up in similar fashion. When predicting the next
output word, we may not only commit to the highest scoring word prediction but also maintain the
next best scoring words in a list of partial translations. We record with each partial translation
the word translation probabilities (obtained from
the softmax), extend each partial translation with
subsequent word predictions and accumulate these
scores. Since the number of partial translation explodes exponentially with each new output word,
we prune them down to a beam of highest scoring
partial translations.
As in traditional statistical machine translation
decoding, increasing the beam size allows us to
explore a larger set of the space of possible translation and hence ﬁnd translations with better model
However, as Figure 10 illustrates, increasing the
beam size does not consistently improve translation quality. In fact, in almost all cases, worse
translations are found beyond an optimal beam
size setting . The optimal beam size varies from
4 (e.g., Czech–English) to around 30 (English–
Romanian).
Normalizing sentence level model scores by
length of the output alleviates the problem somewhat and also leads to better optimal quality in
most cases (5 of the 8 language pairs investigated).
Optimal beam sizes are in the range of 30–50 in
almost all cases, but quality still drops with larger
beams. The main cause of deteriorating quality are
shorter translations under wider beams.
Conclusions
We showed that, despite its recent successes, neural machine translation still has to overcome various challenges, most notably performance out-ofdomain and under low resource conditions. We
hope that this paper motivates research to address
these challenges.
What a lot of the problems have in common
is that the neural translation models do not show
robust behavior when confronted with conditions
that differ signiﬁcantly from training conditions —
may it be due to limited exposure to training data,
unusual input in case of out-of-domain test sentences, or unlikely initial word choices in beam
search. The solution to these problems may hence
lie in a more general approach of training that
steps outside optimizing single word predictions
given perfectly matching prior sequences.
Acknowledgment
This work was partially supported by a Amazon
Research Award (to the ﬁrst author) and a National Science Foundation Graduate Research Fellowship under Grant No. DGE-1232825 (to the
second author).