To be published in the proceedings of IPMI 2017
Unsupervised Anomaly Detection with
Generative Adversarial Networks to Guide
Marker Discovery
Thomas Schlegl1,2 ⋆, Philipp Seeb¨ock1,2, Sebastian M. Waldstein2,
Ursula Schmidt-Erfurth2, and Georg Langs1
1Computational Imaging Research Lab, Department of Biomedical Imaging and
Image-guided Therapy, Medical University Vienna, Austria
 
2Christian Doppler Laboratory for Ophthalmic Image Analysis, Department of
Ophthalmology and Optometry, Medical University Vienna, Austria
Abstract. Obtaining models that capture imaging markers relevant for
disease progression and treatment monitoring is challenging. Models are
typically based on large amounts of data with annotated examples of
known markers aiming at automating detection. High annotation effort and the limitation to a vocabulary of known markers limit the
power of such approaches. Here, we perform unsupervised learning to
identify anomalies in imaging data as candidates for markers. We propose AnoGAN, a deep convolutional generative adversarial network to
learn a manifold of normal anatomical variability, accompanying a novel
anomaly scoring scheme based on the mapping from image space to a latent space. Applied to new data, the model labels anomalies, and scores
image patches indicating their ﬁt into the learned distribution. Results
on optical coherence tomography images of the retina demonstrate that
the approach correctly identiﬁes anomalous images, such as images containing retinal ﬂuid or hyperreﬂective foci.
Introduction
The detection and quantiﬁcation of disease markers in imaging data is critical
during diagnosis, and monitoring of disease progression, or treatment response.
Relying on the vocabulary of known markers limits the use of imaging data
containing far richer relevant information. Here, we demonstrate that relevant
anomalies can be identiﬁed by unsupervised learning on large-scale imaging data.
Medical imaging enables the observation of markers correlating with disease
status, and treatment response. While there is a wide range of known markers
(e.g., characteristic image appearance of brain tumors or calciﬁcation patterns
in breast screening), many diseases lack a suﬃciently broad set, while in others the predictive power of markers is limited. Furthermore, even if predictive
⋆This work has received funding from IBM, FWF (I2714-B31), OeNB (15356, 15929),
the Austrian Federal Ministry of Science, Research and Economy (CDL OPTIMA).
 
To be published in the proceedings of IPMI 2017
Fig. 1. Anomaly detection framework. The preprocessing step includes extraction and
ﬂattening of the retinal area, patch extraction and intensity normalization. Generative
adversarial training is performed on healthy data and testing is performed on both,
unseen healthy cases and anomalous data.
markers are known, their computational detection in imaging data typically requires extensive supervised training using large amounts of annotated data such
as labeled lesions. This limits our ability to exploit imaging data for treatment
decisions.
Here, we propose unsupervised learning to create a rich generative model
of healthy local anatomical appearance. We show how generative adversarial
networks (GANs) can solve the central problem of creating a suﬃciently representative model of appearance, while at the same time learning a generative and
discriminative component. We propose an improved technique for mapping from
image space to latent space. We use both components to diﬀerentiate between
observations that conform to the training data and such data that does not ﬁt.
Related Work Anomaly detection is the task of identifying test data not ﬁtting the normal data distribution seen during training. Approaches for anomaly
detection exist in various domains, ranging from video analysis to remote
sensing . They typically either use an explicit representation of the distribution of normal data in a feature space, and determine outliers based on the
local density at the observations’ position in the feature space. Carrera et al. 
utilized convolutional sparse models to learn a dictionary of ﬁlters to detect
anomalous regions in texture images. Erfani et al. proposed a hybrid model
for unsupervised anomaly detection that uses a one-class support vector machine
(SVM). The SVM was trained from features that were learned by a deep belief
network (DBN). The experiments in the aforementioned works were performed
on real-life-datasets comprising 1D inputs, synthetic data or texture images,
which have lower dimensionality or diﬀerent data characteristics compared to
medical images. An investigation of anomaly detection research papers can be
found in . In clinical optical coherence tomography (OCT) scan analysis, Venhuizen et al. used bag-of-word features as a basis for supervised random
forest classiﬁer training to distinguish diseased patients from healthy subjects.
Schlegl et al. utilized convolutional neural networks to segment retinal ﬂuid
regions in OCT data via weakly supervised learning based on semantic descriptions of pathology-location pairs extracted from medical reports. In contrast to
our approach, both works used some form of supervision for classiﬁer training.
Seeb¨ock et al. identiﬁed anomalous regions in OCT images through unsupervised learning on healthy examples, using a convolutional autoencoder and a
To be published in the proceedings of IPMI 2017
one-class SVM, and explored diﬀerent classes of anomalies. In contrast to this
work, the SVM in involved the need to choose a hyper-parameter that deﬁned
the amount of training points covered by the estimated healthy region.
GANs enable to learn generative models generating detailed realistic images . Radford et al. introduced deep convolutional generative adversarial networks (DCGANs) and showed that GANs are capable of capturing
semantic image content enabling vector arithmetic for visual concepts. Yeh et
al. trained GANs on natural images and applied the trained model for semantic image inpainting. Compared to Yeh et al. , we implement two adaptations for an improved mapping from images to the latent space. We condition the
search in the latent space on the whole query image, and propose a novel variant
to guide the search in the latent space (inspired by feature matching ). In
addition, we deﬁne an anomaly score, which is not needed in an inpainting task.
The main diﬀerence of this paper to aforementioned anomaly detection work
is the representative power of the generative model and the coupled mapping
schema, which utilizes a trained DCGAN and enables accurate discrimination
between normal anatomy, and local anomalous appearance. This renders the
detection of subtle anomalies at scale feasible.
Contribution In this paper, we propose adversarial training of a generative model
of normal appearance (see blue block in Figure 1), described in Section 2.1, and
a coupled mapping schema, described in Section 2.2, that enables the evaluation of novel data (Section
2.3) to identify anomalous images and segment
anomalous regions within imaging data (see red block in Figure 1). Experiments
on labeled test data, extracted from spectral-domain OCT (SD-OCT) scans,
show that this approach identiﬁes known anomalies with high accuracy, and at
the same time detects other anomalies for which no voxel-level annotations are
available. To the best of our knowledge, this is the ﬁrst work, where GANs are
used for anomaly or novelty detection. Additionally, we propose a novel mapping
approach, wherewith the pre-image problem can be tackled.
Generative Adversarial Representation Learning to
Identify Anomalies
To identify anomalies, we learn a model representing normal anatomical variability based on GANs . This method trains a generative model, and a discriminator to distinguish between generated and real data simultaneously (see
Figure 2(a)). Instead of a single cost function optimization, it aims at the Nash
equilibrium of costs, increasing the representative power and speciﬁcity of the
generative model, while at the same time becoming more accurate in classifying
real- from generated data and improving the corresponding feature mapping. In
the following we explain how to build this model (Section 2.1), and how to use
it to identify appearance not present in the training data (Sections 2.2 and 2.3).
To be published in the proceedings of IPMI 2017
Fig. 2. (a) Deep convolutional generative adversarial network. (b) t-SNE embedding
of normal (blue) and anomalous (red) images on the feature representation of the last
convolution layer (orange in (a)) of the discriminator.
Unsupervised Manifold Learning of Normal Anatomical
Variability
We are given a set of M medical images Im showing healthy anatomy, with
m = 1, 2, . . . , M, where Im ∈Ra×b is an intensity image of size a × b. From
each image Im, we extract K 2D image patches xk,m of size c×c from randomly
sampled positions resulting in data x = xk,m ∈X, with k = 1, 2, . . . , K. During
training we are only given ⟨Im⟩and train a generative adversarial model to learn
the manifold X (blue region in Figure 2(b)), which represents the variability
of the training images, in an unsupervised fashion. For testing, we are given
⟨yn, ln⟩, where yn are unseen images of size c × c extracted from new testing
data J and ln ∈{0, 1} is an array of binary image-wise ground-truth labels,
with n = 1, 2, . . . , N. These labels are only given during testing, to evaluate the
anomaly detection performance based on a given pathology.
Encoding Anatomical Variability with a Generative Adversarial Network. A
GAN consists of two adversarial modules, a generator G and a discriminator
D. The generator G learns a distribution pg over data x via a mapping G(z) of
samples z, 1D vectors of uniformly distributed input noise sampled from latent
space Z, to 2D images in the image space manifold X, which is populated by
healthy examples. In this setting, the network architecture of the generator G
is equivalent to a convolutional decoder that utilizes a stack of strided convolutions. The discriminator D is a standard CNN that maps a 2D image to a
single scalar value D(·). The discriminator output D(·) can be interpreted as
probability that the given input to the discriminator D was a real image x sampled from training data X or generated G(z) by the generator G. D and G are
simultaneously optimized through the following two-player minimax game with
value function V (G, D) :
D V (D, G) = Ex∼pdata(x) [log D(x)] + Ez∼pz(z) [log(1 −D(G(z)))] .
The discriminator is trained to maximize the probability of assigning real training examples the “real” and samples from pg the “fake” label. The generator G
To be published in the proceedings of IPMI 2017
is simultaneously trained to fool D via minimizing V (G) = log(1 −D(G(z))),
which is equivalent to maximizing
V (G) = D(G(z)).
During adversarial training the generator improves in generating realistic images and the discriminator progresses in correctly identifying real and generated
Mapping new Images to the Latent Space
When adversarial training is completed, the generator has learned the mapping
G(z) = z 7→x from latent space representations z to realistic (normal) images
x. But GANs do not automatically yield the inverse mapping µ(x) = x 7→z
for free. The latent space has smooth transitions , so sampling from two
points close in the latent space generates two visually similar images. Given a
query image x, we aim to ﬁnd a point z in the latent space that corresponds
to an image G(z) that is visually most similar to query image x and that is
located on the manifold X. The degree of similarity of x and G(z) depends on to
which extent the query image follows the data distribution pg that was used for
training of the generator. To ﬁnd the best z, we start with randomly sampling
z1 from the latent space distribution Z and feed it into the trained generator
to get a generated image G(z1). Based on the generated image G(z1) we deﬁne
a loss function, which provides gradients for the update of the coeﬃcients of z1
resulting in an updated position in the latent space, z2. In order to ﬁnd the most
similar image G(zΓ ), the location of z in the latent space Z is optimized in an
iterative process via γ = 1, 2, . . . , Γ backpropagation steps.
In the spirit of , we deﬁne a loss function for the mapping of new images to the latent space that comprises two components, a residual loss and
a discrimination loss. The residual loss enforces the visual similarity between
the generated image G(zγ) and query image x. The discrimination loss enforces
the generated image G(zγ) to lie on the learned manifold X. Therefore, both
components of the trained GAN, the discriminator D and the generator G, are
utilized to adapt the coeﬃcients of z via backpropagation. In the following, we
give a detailed description of both components of the loss function.
Residual Loss The residual loss measures the visual dissimilarity between
query image x and generated image G(zγ) in the image space and is deﬁned by
|x −G(zγ)|.
Under the assumption of a perfect generator G and a perfect mapping to
latent space, for an ideal normal query case, images x and G(zγ) are identical.
In this case, the residual loss is zero.
To be published in the proceedings of IPMI 2017
Discrimination Loss For image inpainting, Yeh et al. based the computation of the discrimination loss L ˆ
D(zγ) on the discriminator output by feeding
the generated image G(zγ) into the discriminator L ˆ
D(zγ) = σ(D(G(zγ)), α),
where σ is the sigmoid cross entropy, which deﬁned the discriminator loss of real
images during adversarial training, with logits D(G(zγ)) and targets α = 1.
An improved discrimination loss based on feature matching In contrast
to the work of Yeh et al. , where zγ is updated to fool D, we deﬁne an
alternative discrimination loss LD(zγ), where zγ is updated to match G(zγ)
with the learned distribution of normal images. This is inspired by the recently
proposed feature matching technique .
Feature matching addresses the instability of GANs due to over-training on
the discriminator response . In the feature matching technique, the objective function for optimizing the generator is adapted to improve GAN training.
Instead of optimizing the parameters of the generator via maximizing the discriminator’s output on generated examples (Eq. (2)), the generator is forced to
generate data that has similar statistics as the training data, i.e. whose intermediate feature representation is similar to those of real images. Salimans et al. 
found that feature matching is especially helpful when classiﬁcation is the target
task. Since we do not use any labeled data during adversarial training, we do
not aim for learning class-speciﬁc discriminative features but we aim for learning
good representations. Thus, we do not adapt the training objective of the generator during adversarial training, but instead use the idea of feature matching
to improve the mapping to the latent space. Instead of using the scalar output
of the discriminator for computing the discrimination loss, we propose to use
a richer intermediate feature representation of the discriminator and deﬁne the
discrimination loss as follows:
|f(x) −f(G(zγ))|,
where the output of an intermediate layer f(·) of the discriminator is used to
specify the statistics of an input image. Based on this new loss term, the adaptation of the coordinates of z does not only rely on a hard decision of the trained
discriminator, whether or not a generated image G(zγ) ﬁts the learned distribution of normal images, but instead takes the rich information of the feature
representation, which is learned by the discriminator during adversarial training,
into account. In this sense, our approach utilizes the trained discriminator not
as classiﬁer but as a feature extractor.
For the mapping to the latent space, we deﬁne the overall loss as weighted
sum of both components:
L(zγ) = (1 −λ) · LR(zγ) + λ · LD(zγ).
Only the coeﬃcients of z are adapted via backpropagation. The trained parameters of the generator and discriminator are kept ﬁxed.
To be published in the proceedings of IPMI 2017
Detection of Anomalies
During anomaly identiﬁcation in new data we evaluate the new query image x as
being a normal or anomalous image. Our loss function (Eq. (5)), used for mapping to the latent space, evaluates in every update iteration γ the compatibility
of generated images G(zγ) with images, seen during adversarial training. Thus,
an anomaly score, which expresses the ﬁt of a query image x to the model of
normal images, can be directly derived from the mapping loss function (Eq. (5)):
A(x) = (1 −λ) · R(x) + λ · D(x),
where the residual score R(x) and the discrimination score D(x) are deﬁned by
the residual loss LR(zΓ ) and the discrimination loss LD(zΓ ) at the last (Γ th)
update iteration of the mapping procedure to the latent space, respectively. The
model yields a large anomaly score A(x) for anomalous images, whereas a small
anomaly score means that a very similar image was already seen during training.
We use the anomaly score A(x) for image based anomaly detection. Additionally,
the residual image xR = |x −G(zΓ )| is used for the identiﬁcation of anomalous
regions within an image. For purposes of comparison, we additionally deﬁne a
reference anomaly score ˆA(x) = (1 −λ) · R(x) + λ · ˆD(x), where ˆD(x) = L ˆ
is the reference discrimination score used by Yeh et al. .
Experiments
Data, Data Selection and Preprocessing We evaluated the method on clinical
high resolution SD-OCT volumes of the retina with 49 B-scans (representing an
image slice in zx-plane) per volume and total volume resolutions of 496×512×49
voxels in z-, x-, and y direction, respectively. The GAN was trained on 2D image
patches extracted from 270 clinical OCT volumes of healthy subjects, which
were chosen based on the criterion that the OCT volumes do not contain ﬂuid
regions. For testing, patches were extracted from 10 additional healthy cases
and 10 pathological cases, which contained retinal ﬂuid. The OCT volumes were
preprocessed in the following way. The gray values were normalized to range
from -1 to 1. The volumes were resized in x-direction to a size of 22µm resulting
in approximately 256 columns. The retinal area was extracted and ﬂattened to
adjust for variations in orientation, shape and thickness. We used an automatic
layer segmentation algorithm following to ﬁnd the top and bottom layer
of the retina that deﬁne the border of the retina in z-direction. From these
normalized and ﬂattened volumes, we extracted in total 1.000.000 2D training
patches with an image resolution of 64×64 pixels at randomly sampled positions.
Raw data and preprocessed image representation are shown in Figure 1. The
test set in total consisted of 8192 image patches and comprised normal and
pathological samples from cases not included in the training set. For pathological
OCT scans, voxel-wise annotations of ﬂuid and non-ﬂuid regions from clinical
retina experts were available. These annotations were only used for statistical
evaluation but were never fed to the network, neither during training nor in the
To be published in the proceedings of IPMI 2017
evaluation phase. For the evaluation of the detection performance, we assigned
a positive label to an image, if it contained at least a single pixel annotated as
retinal ﬂuid.
Evaluation The manifold of normal images was solely learned on image data
of healthy cases with the aim to model the variety of healthy appearance. For
performance evaluation in anomaly detection we ran the following experiments.
(1) We explored qualitatively whether the model can generate realistic images.
This assessment was performed on image patches of healthy cases extracted from
the training set or test set and on images of diseased cases extracted from the
(2) We evaluated quantitatively the anomaly detection accuracy of our approach
on images extracted from the annotated test set. We based the anomaly detection
on the anomaly score A(x) or only on one of both components, on the residual
score R(x) or on the discrimination score D(x) and report receiver operating
characteristic (ROC) curves of the corresponding anomaly detection performance
on image level.
Based on our proposed anomaly score A(x), we evaluated qualitatively the
segmentation performance and if additional anomalies were identiﬁed.
(3) To provide more details of individual components’ roles, and the gain by
the proposed approach, we evaluated the eﬀect on the anomaly detection performance, when for manifold learning the adversarial training is not performed
with a DCGAN but with an adversarial convolutional autoencoder (aCAE) ,
while leaving the deﬁnition of the anomaly score unchanged. An aCAE also
implements a discriminator but replaces the generator by an encoder-decoder
pipeline. The depth of the components of the trained aCAE was comparable to
the depth of our adversarial model. As a second alternative approach, denoted
as GANR, we evaluated the anomaly detection performance, when the reference
anomaly score ˆA(x), or the reference discrimination score ˆD(x) were utilized
for anomaly scoring and the corresponding losses were used for the mapping
from image space to latent space, while the pre-trained GAN parameters of the
AnoGAN were used. We report ROC curves for both alternative approaches.
Furthermore, we calculated sensitivity, speciﬁcity, precision, and recall at the
optimal cut-oﬀpoint on the ROC curves, identiﬁed through the Youden’s index
and report results for the AnoGan and for both alternative approaches.
Implementation details As opposed to historical attempts, Radford et al. 
identiﬁed a DCGAN architecture that resulted in stable GAN training on images
of sizes 64 × 64 pixels. Hence, we ran our experiments on image patches of the
same size and used widley the same DCGAN architecture for GAN training
(Section 2.1) as proposed by Radford et al.
 1. We used four fractionallystrided convolution layers in the generator, and four convolution layers in the
discriminator, all ﬁlters of sizes 5 × 5. Since we processed gray-scale images, we
utilized intermediate representations with 512−256−128−64 channels . (Best viewed in color)
of 1024 −512 −256 −128 used in ). DCGAN training was performed for 20
epochs utilizing Adam , a stochastic optimizer. We ran 500 backpropagation
steps for the mapping (Section 2.2) of new images to the latent space. We used
λ = 0.1 in Equations (5) and (6), which was found empirically due to preceding
experiments on a face detection dataset. All experiments were performed using
Python 2.7 with the TensorFlow library and run on a Titan X graphics
processing unit using CUDA 8.0.
Results demonstrate the generative capability of the DCGAN and the appropriateness of our proposed mapping and scoring approach for anomaly detection.
We report qualitative and quantitative results on segmentation performance and
detection performance of our approach, respectively.
Can the model generate realistic images? The trained model generates
realistic looking medical images (second row in Figure 3) that are conditioned by
sampling from latent representations z, which are found through our mapping
approach, described in Section 2.2. In the case of normal image patches (see
ﬁrst and second block in Figure 3), our model is able to generate images that
are visually similar to the query images (ﬁrst row in Figure 3). But in the
case of anomalous images, the pairs of input images and generated images show
obvious intensity or textural diﬀerences (see third block in Figure 3). The t-
SNE embedding (Figure 2(b)) of normal and anomalous images in the feature
representation of the last convolution layer of the discriminator that is utilized in
the discrimination loss, illustrates the usability of the discriminator’s features for
To be published in the proceedings of IPMI 2017
Fig. 4. Image level anomaly detection performance and suitability evaluation. (a)
Model comparison: ROC curves based on aCAE (blue), GANR (red), the proposed
AnoGAN (black), or on the output PD of the trained discriminator (green). (b)
Anomaly score components: ROC curves based on the residual score R(x) (green), the
discrimination score D(x) (black), or the reference discrimination score ˆD(x) (red).
(c) Distribution of the residual score and (d) of the discrimination score, evaluated on
normal images of the training set (blue) or test set (green), and on images extracted
from diseased cases (red).
anomaly detection and suggests that our AnoGAN learns a meaningful manifold
of normal anatomical variability.
Can the model detect anomalies? Figure 4(b) shows the ROC curves for image level anomaly detection based on the anomaly score A(x), or on one of both
components, on the residual score R(x), or on the discrimination score D(x).
The corresponding area under the ROC curve (AUC) is speciﬁed in parentheses. In addition, the distributions of the residual score R(x) (Figure 4(c)) and
of the discrimination score D(x) (Figure 4(d)) over normal images from the
training set and test set or over images extracted from diseased cases show that
both components of the proposed adversarial score are suitable for the classiﬁcation of normal and anomalous samples. Figure 3 shows pixel-level identiﬁcation
of anomalies in conjunction with pixel-level annotations of retinal ﬂuid, which
demonstrate high accuracy. Last column in Figure 3 demonstrates that the model
successfully identiﬁes additional retinal lesions, which in this case correspond to
hyperreﬂective foci (HRF). On image level, the red and yellow bars in Figure 3
demonstrate that our model successfully identiﬁes every example image from
diseased cases of the test set as beeing anomalous based on the residual score
and the discrimination score, respectively.
How does the model compare to other approaches? We evaluated the
anomaly detection performance of the GANR, the aCAE and the AnoGAN on
image-level labels. The results are summarized in Table 1 and the corresponding
ROC curves are shown in Figure 4(a). Although aCAEs simultaneously yield a
generative model and a direct mapping to the latent space, which is advantageous in terms of runtimes during testing, this model showed worse performance
on the anomaly detection task compared to the AnoGAN. It turned out that
aCAEs tend to over-adapt on anomalous images. Figure 4(b) demonstrates that
anomaly detection based on our proposed discrimination score D(x) outperforms
To be published in the proceedings of IPMI 2017
Table 1. Clinical performance statistics calculated at the Youden’s index of the ROC
curve and the corresponding AUC based on the adversarial score A(x) of our model
(AnoGAN ) and of the aCAE, based on the reference adversarial score ˆA(x) utilized
by GANR, or based directly on the output of the DCGAN (PD).
Sensitivity
Speciﬁcity
the reference discrimination score ˆD(x). Because the scores for the detection of
anomalies are directly related to the losses for the mapping to latent space, these
results give evidence that our proposed discrimination loss LD(z) is advantageous compared to the discrimination loss L ˆ
D(z). Nevertheless, according to the
AUC, computed based on the anomaly score, the AnoGAN and the GANR show
comparable results (Figure 4(a)). This has to be attributed to the good performance of the residual score R(x). A good anomaly detection performance (cf.
PD in Figure 4(a) and Table 1) can be obtained when the mapping to the latent
space is skipped and a binary decision is derived from the discriminator output,
conditioned directly on the query image.
Conclusion
We propose anomaly detection based on deep generative adversarial networks.
By concurrently training a generative model and a discriminator, we enable
the identiﬁcation of anomalies on unseen data based on unsupervised training
of a model on healthy data. Results show that our approach is able to detect
diﬀerent known anomalies, such as retinal ﬂuid and HRF, which have never
been seen during training. Therefore, the model is expected to be capable to
discover novel anomalies. While quantitative evaluation based on a subset of
anomaly classes is limited, since false positives do not take novel anomalies
into account, results demonstrate good sensitivity and the capability to segment
anomalies. Discovering anomalies at scale enables the mining of data for marker
candidates subject to future veriﬁcation. In contrast to prior work, we show that
the utilization of the residual loss alone yields good results for the mapping from
image to latent space, and a slight improvement of the results can be achieved
with the proposed adaptations.