Nonlinear Dynamic Boltzmann
Machines for Time-Series Prediction
Sakyasingha Dasgupta and Takayuki Osogami
IBM Research - Tokyo
{sdasgup, osogami}@jp.ibm.com
The dynamic Boltzmann machine (DyBM) has been proposed as a stochastic generative model of multi-dimensional
time series, with an exact, learning rule that maximizes the
log-likelihood of a given time series. The DyBM, however,
is deﬁned only for binary valued data, without any nonlinear hidden units. Here, in our ﬁrst contribution, we extend
the DyBM to deal with real valued data. We present a formulation called Gaussian DyBM, that can be seen as an extension of a vector autoregressive (VAR) model. This uses, in
addition to standard (explanatory) variables, components that
captures long term dependencies in the time series. In our
second contribution, we extend the Gaussian DyBM model
with a recurrent neural network (RNN) that controls the bias
input to the DyBM units. We derive a stochastic gradient update rule such that, the output weights from the RNN can also
be trained online along with other DyBM parameters. Furthermore, this acts as nonlinear hidden layer extending the
capacity of DyBM and allows it to model nonlinear components in a given time-series. Numerical experiments with synthetic datasets show that the RNN-Gaussian DyBM improves
predictive accuracy upon standard VAR by up to ≈35%. On
real multi-dimensional time-series prediction, consisting of
high nonlinearity and non-stationarity, we demonstrate that
this nonlinear DyBM model achieves signiﬁcant improvement upon state of the art baseline methods like VAR and
long short-term memory (LSTM) networks at a reduced computational cost.
Introduction
The Boltzmann machine (BM) is an artiﬁcial neural network that is motivated by the Hebbian learning rule
 of biological neural networks, in order to learn
a collection of static patterns. That is, the learning rule of
the BM that maximizes the log likelihood of given patterns
exhibits a key property of the Hebb’s rule, i.e. co-activated
neural units should be connected. Although the original BM
is deﬁned for binary values (1 for neuron ﬁring and 0 representing silence), it has been extended to deal with real
values in the form of Gaussian BM or Gaussian-
Bernoulli restricted BM 
Copyright c⃝2017, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
primarily for engineering purposes. Unlike BM, the recently
proposed dynamic Boltzmann machine (DyBM) can be used to learn a generative
model of temporal pattern sequences, using an exact learning rule that maximises the log likelihood of given timeseries. This learning rule exhibits key properties of spiketiming dependent plasticity (STDP), a variant of the Hebbian rule. In STDP, the amount of change in the synaptic
strength between two neurons that ﬁred together depends on
precise timing when the two neurons ﬁred. However, similar
to a BM, in the DyBM, each neuron takes a binary value, 0
or 1, following a probability distribution that depends on the
parameters of the DyBM. This has limited applicability to
real-world time-series modeling problems, which are often
real valued.
First Contribution: Here, we extend the DyBM to deal with
real values and refer to the extended model as a Gaussian
DyBM. Although extension is possible in the way that BM is
extended to the Gaussian BM, we also relax some of the constraints that the DyBM has required in . The primary purpose of these constraints
in was to interpret its
learning rule as biologically plausible STDP. We relax these
constraints in a way that the Gaussian DyBM can be related
to a vector autoregressive (VAR) model , while keeping the key properties of the DyBM having an exact learning rule intact. This
makes our new model speciﬁcally suited for time-series prediction problems. Speciﬁcally, we show that a special case of
the Gaussian DyBM is a VAR model having additional components that capture long term dependency of time series.
These additional components correspond to DyBM’s eligibility traces, which represent how recently and frequently
spikes arrived from one unit1 to another. This forms the ﬁrst
primary contribution of this paper.
Second Contribution: Similar to DyBM, its direct extension to the Gaussian case, can have two restrictions. Firstly,
the maximum number of units has to be equal to the dimension of the time-series being learned, and secondly, the absence of nonlinear hidden units. In our second contribution,
we extend DyBM by adding a RNN layer that computes a
1In the rest of the paper the term neuron and unit are used interchangeably.
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)
high dimensional nonlinear feature map of past input sequences (from the time-series data) to DyBM. The output
from the RNN layer is used to update the bias parameter in
Gaussian DyBM at each time. As such, we call this extension as RNN-Gaussian DyBM. The RNN is modeled similar
to an echo state network . Such that,
the weights in the RNN layer is ﬁxed randomly, and we only
update the weights from the recurrent layer to the bias layer
for Gaussian DyBM. We derive a stochastic gradient descent
(SGD) update rule for the weights, with the objective of
maximizing the log likelihood of given time-series. RNN-
Gaussian DyBM, thus allows hidden nonlinear units in the
form of the recurrent layer, where by the size of RNN layer
can be selected differently than the dimension of time-series
being modeled. This can signiﬁcantly improve the learning
of high-dimensional time-series by enabling very long temporal memory in DyBM that can also deal with nonlinear
dynamics of the data.
Evaluation: We demonstrate the effectiveness of this
nonlinear DyBM model, namely, RNN-Gaussian DyBM
through numerical experiments on two different synthetic
datasets. Furthermore, we also evaluate its performance
on more complex real time-series prediction using nonstationary multidimensional ﬁnancial time-series and nonlinear sunspot time-series prediction problems . We train the
RNN-Gaussian DyBM and let it predict the future values of
the time-series in a purely online manner with SGD . Namely, at each moment, we update
the parameters and variables by using only the latest values
of the time-series, and let the DyBM predict the next values
of time-series. The experimental results show that the RNN-
Gaussian DyBM can signiﬁcantly better the predictive performance against standard VAR methods, as well as match
(and on occasion outperform) the performance of state of the
art RNNs like long short-term memory (LSTM) networks. Furthermore, RNN-
Gaussian DyBM can be implemented in an online manner,
at a signiﬁcantly reduced cost.
Related work
In the remainder of this section, we review the prior work
related to ours. Besides the DyBM, the BM has also been
extended into temporal horizon to deal with time-series in
various ways, and these extensions have been shown to perform effectively in practice . In particular, Gaussian units have been applied in . Unlike the DyBM, however, exact learning of the logliklihood gradient without back-propagation and sampling,
in these extended models is intractable and needs to be approximated. On the other hand, the RNN-Gaussian DyBM
can be trained to maximize the log-likelihood of given timeseries without approximation, and this learning rule has the
characteristics of STDP, which is inherited from DyBM.
A large amount of the prior work has compared recurrent
neural networks (RNN) against autoregressive models .
ఋିௗ೔ǡೕݑ௜ǡ௝ǡ௞
Figure 1: (a) A Gaussian Boltzmann machine (BM) that
when unfolded in time, gives a Gaussian DyBM as T →∞.
(b) The parametric form of the weight assumed in the Gaussian BMs and its extensions.
The focus of such study, however, is on non-linearity of simple RNNs. The Gaussian DyBM formulation, ﬁrst extends
the linear VAR model but with the additional variables, related to DyBM’s eligibility traces, which take into account
the long term dependency in time-series. The addition of the
RNN layer then allows to model the nonlinear components
of the time-series.
Deriving a Gaussian DyBM
We will deﬁne a G-DyBM2 as a limit of a sequence of Gaussian BMs. Each of the Gaussian BMs deﬁnes a probability
distribution of the patterns that it generates, and an analogous probability distribution for the G-DyBM is deﬁned as
a limit of the sequence of those probability distributions.
Gaussian DyBM as unfolded Gaussian Boltzmann
machines for T →∞
Consider a Gaussian BM having a structure illustrated in
Figure 1 (a). This Gaussian BM consists of T + 1 layers
of units. Let, N be the number of units in each layer. This
Gaussian BM can represent a series of N-dimensional
patterns of length T +1. In the ﬁgure, this series of patterns
is denoted as x[t−T,t] ≡(xs)s=t−T,...,t for some time t. That is,
the δ-th layer represents the pattern, x[t−δ] ≡(x[t−δ]
)i=1,...,N,
at time t −δ for δ = 0,1,...,T.
The Gaussian BM in Figure 1(a) has three kinds of parameters, bias, variance, and weight, which determine the
probability distribution of the patterns that the Gaussian BM
2In interest of space, we refer to Gaussian DyBM as G-DyBM
and its RNN extension as RNN-G-DyBM on certain occasions.
generates. For i = 1,...,N, let bi be the bias of the i-th
unit of any layer and σ2
i be the variance of the i-th unit
of any layer. Let w[δ]
i,j be the weight between the i-th unit
of the (s + δ)-th layer and the j-th unit of the s-th layer
for (i, j) ∈{1,...,N}2, s = 0,...,T −δ, and δ = 1,...,T.
We assume that there are no connections within each layer:
namely w 
i,j = 0. With this assumption, the most recent values, x[t]
for i = 1,...,N, are conditionally independent of
each other given x[t−T,t−1].
Hence, the conditional probability density of x[t] given
x[t−T,t−1] can be represented as
p(x[t]|x[t−T,t−1]) =
j |x[t−T,t−1]),
where each factor of the right-hand side denotes the conditional probability density of x[t]
j given x[t−T,t−1] for j =
1,...,T. More speciﬁcally, x[t]
j has a Gaussian distribution
for each j:
j |x[t−T,t−1]) =
where μ[t]
j takes the following form and can be interpreted
as the expected value of the j-th unit at time t given the last
T patterns:
i,j x[t−δ]
To take the limit of T →∞while keeping the number of
parameters constant (i.e., independent of T), we assume that
the weight has the parametric form illustrated in Figure 1(b).
Here, di,j ≥1 is an integer and will represent the conduction
delay from i to j in the G-DyBM. Speciﬁcally, for δ ≥di, j,
we use the parametric form of the DyBM :
where, for each k, ui,j,k is a learnable parameter, and the decay rate, λk, is ﬁxed in range [0,1). Unlike the DyBM, we
assume no constraint on w[δ]
i, j for 0 < δ < di, j. Although these
weight could have shared weight in the G-DyBM as well, the
unconstrained weight will allow us to interpret the G-DyBM
as an extension of the VAR model in the next section.
Plugging (4) into (3) and letting T →∞, we obtain
i,j x[t−δ]
ui, j,k α[t−1]
where α[t−1]
i,j,k will be referred to as an eligibility trace and is
deﬁned as follows:
Notice that the eligibility trace can be computed recursively:
i, j,k = λk α[t−1]
[t−di,j+1]
Gaussian DyBM as extended vector autoregression
When the expression (5) is seen as a regressor (or predictor)
j , it can be understood as a model of vector autoregression (VAR) with two modiﬁcations to the standard model.
First, the last term in the right-hand side of (5) involves eligibility traces, which can be understood as features of historical values, x[−∞,t−di,j], and are added as new variables of
the VAR model. Second, the expression (5) allows the number of terms (i.e., lags) to depend on i and j through di,j,
while this number is common among all pairs of i and j in
the standard VAR model.
When the conduction delay has a common value (di,j =
d,∀i, j), we can represent (5) simply with vectors and matrices as follows:
W[δ] x[t−δ] +
where μ ≡(μ j) j=1,...,N, b ≡(b j)j=1,...,N, x[t] ≡(x[t]
j )j=1,...,N,
and α[t−1]
)j=1,...,N for k = 1,...,K are column vectors; W[δ] ≡( ˜wi, j)(i, j)∈{1,...,N}2 for 0 < δ < di, j and Uk ≡
(ui, j,k)(i, j)∈{1,...,N}2 for k = 1,...,K are N ×N matrices.
RNN-Gaussian DyBM as a nonlinear model
Having derived the G-DyBM, we now formulate the RNN-
G-DyBM, as a nonlinear extension of the G-DyBM model
by updating the bias parameter vector b, at each time using
a RNN layer. This RNN layer computes a nonlinear feature
map of the past time series input to the G-DyBM. Where in,
the output weights from the RNN to the bias layer along with
DyBM parameters, can be updated online using a stochastic
gradient method.
We consider a G-DyBM connected with a M-dimensional
RNN, whose state vector changes dependent on a nonlinear
feature mapping of its own history and the N-dimensional
time-series input data vector at time t −1. Where in, for
most settings M > N. Speciﬁcally, for RNN-G-DyBM we
consider the bias vector to be time-dependent. Where in, it
is updated at each time as:
b[t] = b[t−1] +A⊤Ψ[t]
Here, Ψ[t] is the M×1 dimensional state vector at time t of
a M dimensional RNN. A is the M ×N dimensional learned
output weight matrix that connects the RNN state to the bias
vector. Where, the RNN state is updated based on the input
time-series vector x[t] as follows:
Ψ[t] = (1−ρ)Ψ[t−1] +ρF(WrnnΨ[t−1] +Winx[t]),
Where, F(x) = tanh(x). This can however be replaced by
any other suitable nonlinear function, e.g. rectiﬁed linear
units, sigmoid etc. Here, 0 < ρ ≤1 is a leak rate hyperparameter of the RNN, which controls the amount of memory in each unit of the RNN layer. Wrnn and Win are the
M ×M dimensional RNN weight matrix and N ×M dimensional projection of the time series input to the RNN layer,
respectively. Here, we design the RNN similar to an echo
state network . Such that, the weight
matrices Wrnn and Win are initialized randomly. Wrnn is initialized from a Gaussian distribution N (0,1) and Win is
initialized from N (0,0.1). The sparsity of the RNN weight
matrix can be controlled by the parameter φ and it is scaled
to have a spectral radius less than one, for stability . For all results presented here, the RNN
weight matrix was 90% sparse and had a spectral radius of
Online training of a RNN-Gaussian DyBM
We now derive an online learning rule for the RNN-G-
DyBM in a way that the log-likelihood of given time-series
data, D, is maximized. The log-likelihood of D is given by
log p(x[t]|x[−∞,t−1]).
Here, we show the case where D consists of a single timeseries, but extension to multidimensional cases is straightforward. The approach of stochastic gradient is to update the
parameters of the RNN-G-DyBM at each step, t, according
to the gradient of the conditional probability density of x[t],
∇log p(x[t]|x[−∞,t−1]) =
∇log pk(x[t]
i |x[−∞,t−1])
where, the ﬁrst equality follows from the conditional independence (1), and the second equality follow from (2). From
(13) and (5), we can now derive the derivative with respect
to each parameter. These parameters can then be updated,
for example, as follows:
bj ←b j +η
σj ←σ j +η
ui,j,k ←ui,j,k +η
Al,j ←Al, j +η′ (x[t]
for k = 1,...,K, δ = 1,...,di,j −1, and (i, j) ∈{1,...,N},
where η and η′ are learning rates. We set, η′ < η such
that Al j is stationary while other parameters are updated.
The learning rates can be adjusted at each step according
to stochastic optimization techniques like Adam and RMSProp .
In (14)-(18), μ[t]
is given by (5), where α[t−1]
and x[t−δ]
for δ ∈[1,di, j −1], respectively, are stored and updated in
a synapse and a FIFO queue that connects from neuron i to
neuron j. It should be noted that, in the absence of the formulations in (9) and (10), this same learning procedure updates the parameters of an equivalent G-DyBM model, without the need for (18). See algorithmic description in supplementary .
Numerical experiments
We now demonstrate the advantages of the RNN-G-DyBM
through numerical experiments with two synthetic and two
real time series data. In these experiments, we use the RNN-
G-DyBM with a common conduction delay (di, j = d for any
i, j; see (8)). All the experiments were carried out with a
Python 2.7 implementation (with numpy and theano backend) on a Macbook Air with Intel Core i5 and 8 GB of memory.
In all cases we train the RNN-G-DyBM in an online manner. Namely, for each step t, we give a pattern, x[t], to the
network to update its parameters and variables such as eligibility traces (see (7)), and then let theRNN-G-DyBM predict
the next pattern, ˜x[t+1], based on μ[t+1] using (8)-(10). This
process is repeated sequentially for all time or observation
points t = 1,2,.... Here, the parameters are updated according to (14)-(18). The learning rates, η and η′, in (14)-(18) is
adjusted for each parameter according to RMSProp , where the initial learning rate was
set to 0.001. Throughout, the initial values of the parameters
and variables, including eligibility traces and the spikes in
the FIFO queues, are set to 0. However, we initialize σ j = 1
for each j to avoid division by 0 error. All RNN weight matrices were initialized randomly as described in the RNN-G
DyBM model section. The RNN layer leak rate ρ was set to
0.9 in all experiments.
Synthetic time series prediction
The purpose of the experiments with the synthetic datasets
is to clearly evaluate the performance of RNN-G-DyBM
in a controlled setting. Speciﬁcally, we consider a RNN-G-
DyBM with N DyBM units and M RNN units. The DyBM
units are connected with FIFO queues of length d and eligibility traces of decay rate λ, where d and λ are varied in the
experiments. For λ = 0, and in the absence of the RNN layer,
we have α[t] = x[t−d], and this Gaussian DyBM reduces to a
vector autoregressive model with d lags. We use this VAR
model as the baseline for performance evaluation.
Multidimensional noisy sine wave:
In the ﬁrst synthetic
task, we train the RNN-G-DyBM with a ﬁve dimensional
noisy sine-wave. Where each dimension xD is generated as:
D = sin(Dπ t/100)+ε[t], D = (1,2,3,4,5),
for each t, where ε[t] is independent and identically distributed (i.i.d) with a Gaussian distribution N (0,1). The
number of DyBM units N = 5 with M = 10 hidden RNN
(a) noisy sine, d = 2
(b) noisy sine, d = 4
(c) noisy sine, d = 7
(d) narma-30, d = 2
(e) narma-30, d = 4
(f) narma-30, d = 7
Figure 2: Mean squared error of prediction for the synthetic time series. For each step t, the mean squared error is averaged
over 100 independent runs. Decay rate λ is varied as in the legend, and the red curve (λ = 0) corresponds to the baseline VAR
model. Conduction delay d is varied across panels. (a)-(c) Prediction performance on the noisy sine task. (d)-(f) Prediction
performance on the 30th order NARMA task.
30th order nonlinear autoregressive moving average
In this task, we train the RNN-G-DyBM for
one step prediction with a one dimensional nonlinear timeseries. Namely, the 30th order NARMA which is generated as:
x[t] = 0.2x[t−1] +0.004x[t−1] 29
+1.5u[t−30] u[t−1] +0.01
Where, u[t] is i.i.d with a Gaussian distribution N (0,0.5).
As such, for future prediction, this task requires the modeling of the inherent nonlinearity and up to 30 time-steps of
memory. The number of DyBM units are N = 1 with M = 5
hidden RNN units.
Figure 2 shows the predictive accuracy of the RNN-
G-DyBM. Here, the prediction, ˜x[t], for the pattern at
time t is evaluated with mean squared error, MSE[t] ≡
s=t−50(˜x[t] −x[t])2, and MSE[t] is further averaged over
100 independent runs of the experiment. Due to the timedependent noise ε[t], the best possible squared error is 1.0 in
expectation. We vary decay rates λ as indicated in the legend
and d as indicated below each panel. We observe that in the
noisy sine task, although the prediction accuracy depends
on the choice of λ, Figure 2 (a)-(c) shows that the RNN-G-
DyBM (with λ > 0) signiﬁcantly outperforms VAR model
(λ = 0; red curves) and reduces the error by more than 30 %.
A similar performance beneﬁt is also observed in Figure 2
(d)-(f) for the 30th order NARMA prediction task. Where
in, the RNN-G-DyBM performs robustly across varying decay rates and consistently outperforms the VAR model even
Figure 3: Average root mean squared error after 20 epochs
for one-week ahead prediction plotted for varying decay
rates (λ), on the real dataset of weekly retail prices for gasoline and diesel in U.S.A. The results shown are using ﬁxed
delay strength d = 2. The star marks show the performance
of the baseline VAR model (λ = 0.0). Training error is plotted in red and test error is plotted in blue. See supplementary
results , for plots with d = 3
and d = 4.
when the lag or delays increases. As this task requires both
long memory and nonlinearity, the gain RNN-G-DyBM has
over the VAR stems from the use of the eligibility traces,
α[t] and the nonlinear RNN hidden units, instead of the lagd variable, x[t−d]. As such, even with increasing delays the
VAR model does not match the performance of RNN-G-
DyBM. The results for even longer delays can be found in
the supplementary material .
Retail-price prediction Sunspots prediction
RNN-G-DyBM (d=2)
RNN-G-DyBM (d=3)
RNN-G-DyBM (d=4)
Table 1: The average test RMSE after 20 epochs for various
models for online time series prediction tasks with the retailprice dataset and sunspot number datasets, respectively. The
delay (d) for each model is in brackets. For LSTM model
there was no delay and decay rate (λ) hyper-parameters. In
all VAR models λ = 0.0. For RNN-G-DyBM models the
best test score achieved across the entire range of decay rates
is reported.
Average runtime/epoch (s)
RNN-G-DyBM (across delays)
VAR (across delays)
Table 2: The average CPU time taken in seconds, to train
a model per epoch. The reported values are for the sunspot
nonlinear time series prediction task.
Real-data time series prediction
Having evaluated the performance of RNN-G-DyBM in a
controlled setting, we now evaluate its performance on two
real-data sets. We demonstrate that RNN-G-DyBM consistently outperforms the baseline VAR models as well as, betters in some cases, the performance obtained with the popular LSTM RNN model. We ﬁrst evaluate using a highly nonstationary multidimensional retail-price time series dataset.
Weekly retail gasoline and diesel prices in U.S.3:
dataset consists of real valued time series of 1223 steps(t =
1,...,1223) of weekly prices of gasoline and diesel (in US dollar/gallon)
with 8 dimensions covering different regions in U.S.A. We
normalize the data within the interval [0,1.0]. We use the
ﬁrst 67% of the time series observations (819 time steps) as
the training set and the remaining 33% (404 time steps) as
the test data set. We train the RNN-G-DyBM with N = 8
units, and M = 20 hidden RNN units with varying d and λ.
The objective in this task was to make one-week ahead predictions in an online manner, as in the synthetic experiments.
We once again use the VAR model as the baseline. Additionally, we also learn with LSTM RNN with 20 hidden units. We set
the relevant hyper-parameters such that they were consistent
across all the models. In all models the parameters of the network was updated online, using the stochastic optimization
method RMSProp with an initial learning rate of 0.001. LSTM was implemented in Keras
see supplementary .
3Data obtained from 
Monthly sunspot number prediction4:
In the second
task, we use the historic benchmark of monthly sunspot
number collected in Zurich from
Jan. 1749 to Dec. 1983. This is a one-dimensional nonlinear
time series with 2820 time steps. As before, we normalize
the data within the interval [0,1.0] and used 67% for training and 33% for testing the models. With goal of monthly
prediction we trained RNN-G-DyBM with N = 1 unit, and
M = 50 hidden RNN units.The LSTM model also had 50
hidden units. All other settings were same as the other realdata set.
Figure 3 shows the one-week ahead prediction accuracy
of RNN-G-DyBM on the retail-price time series for delay
d = 2. We evaluate the error using the root mean squared error (RMSE) measure averaged over 20 epochs calculated for
the normalized time series. The ﬁgure shows that the RNN-
G-DyBM (solid curves) clearly outperforms VAR (stared
points) by nearly more than 30% (depending on the settings), on both training and test predictions. However for
larger decay rates the test RMSE increases suggesting that
over-ﬁtting can occur if hyper-parameters are not selected
properly. In comparison, while directly using the G-DyBM
model on this task, the best case test RMSE was 0.0718
with d = 3, thus RNN-G-DyBM improves upon G-DyBM
by more than 21% .
As observed in Table 1, on this task the RNN-G-DyBM
with d = 3 achieved the best performance, which remarkably
beats even the LSTM model by a margin of ≈16 %. Considerable performance gain against the VAR baseline was also
observed (Table 1. columns two) using the sunspot time series. Due to the inherent nonlinearity in this data, both the
VAR and G-DyBM models perform very poorly even for
higher delays. Notably, the best case test RMSE obtained
when using the G-DyBM model was 0.1327 (with d = 3) i.e.
40% lower as compared to the best RNN-G-DyBM model.
With 50 hidden units the LSTM model performs slightly better in this case with a normalized test error of 0.07342 as
compared to the 0.0770 for the RNN-G-DyBM with d = 3
 . Visual inspection
shows little difference between RNN-G-DyBM and LSTM
prediction.).
In Table 2. we record the average CPU time taken (in seconds) to execute a single training epoch on the sunspot data,
across the three models. As observed, the RNN-G-DyBM
not only achieves comparable performance to the LSTM
but learns in only 0.7014 sec./epoch as compared to the
11.2132 sec./epoch (16 times more) for the LSTM model.
Notably, after 50 epochs LSTM achieves a best test RMSE
of 0.0674(retail-price dataset) taking ≈566 secs., while after 50 epochs the RNN-G-DyBM realises a best test RMSE
of 0.0564 in ≈35 secs, on the same task. As such, the nonlinear RNN-G-DyBM model is highly scalable in an online
learning environment. Expectedly, the VAR model without
any eligibility traces and hidden units runs much faster, albeit with signiﬁcantly lower predictive accuracy.
4Publicly available at 
Conclusion
In this paper we ﬁrst extended the dynamic Boltzmann
machine (DyBM), into the Gaussian DyBM (G-DyBM) to
model real valued time series. The G-DyBM can be seen
as an extension of vector autoregression model. We further extended this to the RNN-Gaussian DyBM in order to
model inherent nonlinearities in time series data. Experimental results demonstrate the effectiveness of the RNN-
G-DyBM model with signiﬁcant performance gain over
VAR. The RNN-G-DyBM also outperforms popular LSTM
models at a considerably reduced computational cost. Our
model is highly scalable similar to binary DyBM that was shown to give signiﬁcant performance improvement on the high-dimensional
moving MNIST task. Furthermore, unlike models requiring back-propagation, in RNN-G-DyBM each parameter can
be updated in a distributed manner in constant time with
Eqs.14-18. This update is independent of data dimension or
the maximum delay. This makes the RNN-G-DyBM model
highly robust and scalable for online high-dimensional time
series prediction scenarios.
Acknowledgments
This work was partly funded by CREST, JST.