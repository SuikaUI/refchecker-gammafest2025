Self-Supervised Learning of Pretext-Invariant Representations
Ishan Misra
Laurens van der Maaten
Facebook AI Research
The goal of self-supervised learning from images is
to construct image representations that are semantically
meaningful via pretext tasks that do not require semantic
annotations for a large training set of images. Many pretext tasks lead to representations that are covariant with
image transformations. We argue that, instead, semantic
representations ought to be invariant under such transformations.
Speciﬁcally, we develop Pretext-Invariant Representation Learning (PIRL, pronounced as “pearl”) that
learns invariant representations based on pretext tasks. We
use PIRL with a commonly used pretext task that involves
solving jigsaw puzzles. We ﬁnd that PIRL substantially improves the semantic quality of the learned image representations. Our approach sets a new state-of-the-art in selfsupervised learning from images on several popular benchmarks for self-supervised learning. Despite being unsupervised, PIRL outperforms supervised pre-training in learning image representations for object detection. Altogether,
our results demonstrate the potential of self-supervised
learning of image representations with good invariance
properties.
1. Introduction
Modern image-recognition systems learn image representations from large collections of images and corresponding semantic annotations. These annotations can be provided in the form of class labels , hashtags , bounding boxes , etc. Pre-deﬁned semantic annotations
scale poorly to the long tail of visual concepts , which
hampers further improvements in image recognition.
Self-supervised learning tries to address these limitations by learning image representations from the pixels
themselves without relying on pre-deﬁned semantic annotations. Often, this is done via a pretext task that applies a
transformation to the input image and requires the learner
to predict properties of the transformation from the transformed image (see Figure 1). Examples of image transformations used include rotations , afﬁne transformations , and jigsaw transformations . As
Pretext Image
Pretext Invariant
Representation Learning
Transform t
Standard Pretext
Representation
Predict property of t
Representation
Representation
Encourage to be similar
Figure 1: Pretext-Invariant Representation Learning (PIRL). Many
pretext tasks for self-supervised learning involve transforming
an image I, computing a representation of the transformed image, and predicting properties of transformation t from that representation. As a result,
the representation must covary with the transformation t and may not contain much semantic information. By contrast, PIRL learns representations
that are invariant to the transformation t and retain semantic information.
the pretext task involves predicting a property of the image transformation, it encourages the construction og image
representations that are covariant to the transformations.
Although such covariance is beneﬁcial for tasks such as predicting 3D correspondences , it is undesirable
for most semantic recognition tasks. Representations ought
to be invariant under image transformations to be useful for
image recognition because the transformations do
not alter visual semantics.
Motivated by this observation, we propose a method that
learns invariant representations rather than covariant ones.
Instead of predicting properties of the image transformation, Pretext-Invariant Representation Learning (PIRL) constructs image representations that are similar to the representation of transformed versions of the same image and
different from the representations of other images. We adapt
the “Jigsaw” pretext task to work with PIRL and ﬁnd
that the resulting invariant representations perform better
than their covariant counterparts across a range of vision
tasks. PIRL substantially outperforms all prior art in selfsupervised learning from ImageNet (Figure 2) and from uncurated image data (Table 4). Interestingly, PIRL even outperforms supervised pre-training in learning image representations suitable for object detection (Tables 1 & 6).
 
Number of Parameters (in millions)
Top-1 Accuracy
A:Supervised
DeepCluster
Supervised
Figure 2: ImageNet classiﬁcation with linear models. Single-crop top-1
accuracy on the ImageNet validation data as a function of the number of
parameters in the model that produces the representation (“A” represents
AlexNet). Pretext-Invariant Representation Learning (PIRL) sets a new
state-of-the-art in this setting (red marker) and uses signiﬁcantly smaller
models (ResNet-50). See Section 3.2 for more details.
2. PIRL: Pretext-Invariant
Representation Learning
Our work focuses on pretext tasks for self-supervised
learning in which a known image transformation is applied
to the input image. For example, the “Jigsaw” task divides
the image into nine patches and perturbs the image by randomly permuting the patches . Prior work used Jigsaw
as a pretext task by predicting the permutation from the perturbed input image. This requires the learner to construct
a representation that is covariant to the perturbation. The
same is true for a range of other pretext tasks that have recently been studied . In this work, we adopt
the existing Jigsaw pretext task in a way that encourages
the image representations to be invariant to the image patch
perturbation. While we focus on the Jigsaw pretext task in
this paper, our approach is applicable to any pretext task that
involves image transformations (see Section 4.3).
2.1. Overview of the Approach
Suppose we are given an image dataset,
{I1, . . . , I|D|} with In ∈RH×W ×3, and a set of image
transformations, T . The set T may contain transformations
such as a re-shufﬂing of patches in the image , image rotations , etc. We aim to train a convolutional network,
φθ(·), with parameters θ that constructs image representations vI = φθ(I) that are invariant to image transformations
t ∈T . We adopt an empirical risk minimization approach
to learning the network parameters θ. Speciﬁcally, we train
the network by minimizing the empirical risk:
ℓinv(θ; D) = Et∼p(T )
L (vI, vIt)
where p(T ) is some distribution over the transformations
in T , and It denotes image I after application of transformation t, that is, It = t(I). The function L(·, ·) is a loss
function that measures the similarity between two image
representations. Minimization of this loss encourages the
network φθ(·) to produce the same representation for image
I as for its transformed counterpart It, i.e., to make representation invariant under transformation t.
We contrast our loss function to losses 
that aim to learn image representations vI = φθ(I) that are
covariant to image transformations t ∈T by minimizing:
ℓco(θ; D) = Et∼p(T )
Lco (vI, z(t))
where z is a function that measures some properties of
transformation t.
Such losses encourage network φθ(·)
to learn image representations that contain information on
transformation t, thereby encouraging it to maintain information that is not semantically relevant.
Loss function. We implement ℓinv(·) using a contrastive
loss function L(·, ·) . Speciﬁcally, we deﬁne a matching score, s(·, ·), that measures the similarity of two image
representations and use this matching score in a noise contrastive estimator . In our noise contrastive estimator
(NCE), each “positive” sample (I, It) has N corresponding
“negative” samples. The negative samples are obtained by
computing features from other images, I′ ̸= I. The noise
contrastive estimator models the probability of the binary
event that (I, It) originates from data distribution as:
h(vI, vIt) =
s(vIt,vI′)
Herein, DN ⊆D \ {I} is a set of N negative samples that
are drawn uniformly at random from dataset D excluding
image I, τ is a temperature parameter, and s(·, ·) is the cosine similarity between the representations.
In practice, we do not use the convolutional features v
directly but apply to different “heads” to the features before
computing the score s(·, ·). Speciﬁcally, we apply head f(·)
on features (vI) of I and head g(·) on features (vIt) of It;
see Figure 3 and Section 2.3. NCE then amounts to minimizing the following loss:
= −log [h (f(vI), g(vIt))]
I), f(vI′)
This loss encourages the representation of image I to be
similar to that of its transformed counterpart It, whilst also
encouraging the representation of It to be dissimilar to that
of other images I′.
M Memory Bank
Dissimilar
Dissimilar
Figure 3: Overview of PIRL. Pretext-Invariant Representation Learning
(PIRL) aims to construct image representations that are invariant to the
image transformations t ∈T . PIRL encourages the representations of the
image, I, and its transformed counterpart, It, to be similar. It achieves this
by minimizing a contrastive loss (see Section 2.1). Following , PIRL
uses a memory bank, M, of negative samples to be used in the contrastive
learning. The memory bank contains a moving average of representations,
mI ∈M, for all images in the dataset (see Section 2.2).
2.2. Using a Memory Bank of Negative Samples
Prior work has found that it is important to use a large
number of negatives in the NCE loss of Equation 4 .
In a mini-batch SGD optimizer, it is difﬁcult to obtain a
large number of negatives without increasing the batch to
an infeasibly large size. To address this problem, we follow and use a memory bank of “cached” features. Concurrent work used a similar memory-bank approach .
The memory bank, M, contains a feature representation
mI for each image I in dataset D. The representation mI
is an exponential moving average of feature representations
f(vI) that were computed in prior epochs. This allows us to
replace negative samples, f(v′
I), by their memory bank representations, mI′, in Equation 4 without having to increase
the training batch size. We emphasize that the representations that are stored in the memory bank are all computed
on the original images, I, without the transformation t.
Final loss function. A potential issue of the loss in Equation 4 is that it does not compare the representations of untransformed images I and I′. We address this issue by using
a convex combination of two NCE loss functions in ℓinv(·):
= λLNCE(mI, g(vIt))
+(1 −λ)LNCE(mI, f(vI)).
Herein, the ﬁrst term is simply the loss of Equation 4 but
uses memory representations mI and mI′ instead of f(vI)
I), respectively. The second term does two things:
(1) it encourages the representation f(vI) to be similar to
its memory representation mI, thereby dampening the parameter updates; and (2) it encourages the representations
f(vI) and f(v′
I) to be dissimilar. We note that both the ﬁrst
and the second term use mI′ instead of f(v′
I) in Equation 4.
Setting λ = 0 in Equation 5 leads to the loss used in .
We study the effect of λ on the learned representations in
Section 4.
2.3. Implementation Details
Although PIRL can be used with any pretext task that involves image transformations, we focus on the Jigsaw pretext task in this paper. To demonstrate that PIRL is
more generally applicable, we also experiment with the Rotation pretext task and with a combination of both tasks
in Section 4.3. Below, we describe the implementation details of PIRL with the Jigsaw pretext task.
Convolutional network. We use a ResNet-50 (R-50) network architecture in our experiments . The network
is used to compute image representations for both I and
It. These representations are obtained by applying function
f(·) or g(·) on features extracted from the the network.
Speciﬁcally, we compute the representation of I, f(vI),
by extracting res5 features, average pooling, and a linear
projection to obtain a 128-dimensional representation.
To compute the representation g(vIt) of a transformed
image It, we closely follow .
We: (1) extract
nine patches from image I, (2) compute an image representation for each patch separately by extracting activations
from the res5 layer of the ResNet-50 and average pool
the activations, (3) apply a linear projection to obtain a
128-dimensional patch representations, and (4) concatenate
the patch representations in random order and apply a second linear projection on the result to obtain the ﬁnal 128dimensional image representation, g(vIt). Our motivation
for this design of g(vIt) is the desire to remain as close as
possible to the covariant pretext task of . This
allows apples-to-apples comparisons between the covariant
approach and our invariant approach.
Hyperparameters. We implement the memory bank as described in and use the same hyperparameters for the
memory bank. Speciﬁcally, we set the temperature in Equation 3 to τ = 0.07, and use a weight of 0.5 to compute the
exponential moving averages in the memory bank. Unless
stated otherwise, we use λ= 0.5 in Equation 5.
3. Experiments
Following common practice in self-supervised learning , we evaluate the performance of PIRL in
transfer-learning experiments. We perform experiments on
a variety of datasets, focusing on object detection and image
classiﬁcation tasks. Our empirical evaluations cover: (1)
a learning setting in which the parameters of the convolutional network are ﬁnetuned during transfer, thus evaluating
the network “initialization” obtained using self-supervised
learning and (2) a learning setting in which the parameters
of the network are ﬁxed during transfer learning, thus using
the network as a feature extractor. Code reproducing the
results of our experiments will be published online.
Baselines.
Our most important baseline is the Jigsaw
ResNet-50 model of . This baseline implements the co-
Network APall AP50 AP75 ∆AP75
Supervised
Jigsaw 
Rotation 
NPID++ 
PIRL (ours)
CPC-Big 
CPC-Huge 
55.2∗† 81.4∗† 61.2∗†
Table 1: Object detection on VOC07+12 using Faster R-CNN. Detection AP on the VOC07 test set after ﬁnetuning Faster R-CNN models
(keeping BatchNorm ﬁxed) with a ResNet-50 backbone pre-trained using
self-supervised learning on ImageNet. Results for supervised ImageNet
pre-training are presented for reference. Numbers with ∗are adopted from
the corresponding papers. Method with † ﬁnetunes BatchNorm. PIRL signiﬁcantly outperforms supervised pre-training without extra pre-training
data or changes in the network architecture. Additional results in Table 6.
variant counterpart of our PIRL approach with the Jigsaw
pretext task.
We also compare PIRL to a range of other selfsupervised methods.
An important comparison is to
NPID . NPID is a special case of PIRL: setting λ = 0 in
Equation 5 leads to the loss function of NPID. We found it is
possible to improve the original implementation of NPID by
using more negative samples and training for more epochs
(see Section 4). We refer to our improved version of NPID
as NPID++. Comparisons between PIRL and NPID++ allow us to study the effect of the pretext-invariance that PIRL
aims to achieve, i.e., the effect of using λ > 0 in Equation 5.
Pre-training data.
To facilitate comparisons with prior
work, we use the 1.28M images from the ImageNet 
train split (without labels) to pre-train our models.
Training details. We train our models using mini-batch
SGD using the cosine learning rate decay scheme with
an initial learning rate of 1.2×10−1 and a ﬁnal learning rate
of 1.2 × 10−4. We train the models for 800 epochs using a
batch size of 1, 024 images and using N = 32, 000 negative
samples in Equation 3. We do not use data-augmentation
approaches such as Fast AutoAugment because they
are the result of supervised-learning approaches. We provide a full overview of all hyperparameter settings that were
used in the supplemental material.
Transfer learning. Prior work suggests that the hyperparameters used in transfer learning can play an important role
in the evaluation pre-trained representations . To
facilitate fair comparisons with prior work, we closely follow the transfer-learning setup described in .
3.1. Object Detection
Following prior work , we perform objectdetection experiments on the the Pascal VOC dataset 
using the VOC07+12 train split.
We use the Faster R-
CNN C4 object-detection model implemented in Detectron2 with a ResNet-50 (R-50) backbone. We pretrain the ResNet-50 using PIRL to initialize the detection
model before ﬁnetuning it on the VOC training data. We use
the same training schedule as for all models ﬁnetuned
on VOC and follow to keep the BatchNorm parameters ﬁxed during ﬁnetuning. We evaluate object-detection
performance in terms of APall, AP50, and AP75 .
The results of our detection experiments are presented in
Table 1. The results demonstrate the strong performance of
PIRL: it outperforms all alternative self-supervised learnings in terms of all three AP measures. Compared to pretraining on the Jigsaw pretext task, PIRL achieves AP improvements of 5 points. These results underscore the importance of learning invariant (rather than covariant) image
representations.
PIRL also outperforms NPID++, which
demonstrates the beneﬁts of learning pretext invariance.
Interestingly, PIRL even outperforms the supervised
ImageNet-pretrained model in terms of the more conservative APall and AP75 metrics.
Similar to concurrent
work , we ﬁnd that a self-supervised learner can outperform supervised pre-training for object detection. We
emphasize that PIRL achieves this result using the same
backbone model, the same number of ﬁnetuning epochs,
and the exact same pre-training data (but without the labels). This result is a substantial improvement over prior
self-supervised approaches that obtain slightly worse performance than fully supervised baselines despite using orders of magnitude more curated training data or much
larger backbone models .
In Table 6, we show that
PIRL also outperforms supervised pretraining when ﬁnetuning is done on the much smaller VOC07 train+val set.
This suggests that PIRL learns image representations that
are amenable to sample-efﬁcient supervised learning.
3.2. Image Classiﬁcation with Linear Models
Next, we assess the quality of image representations by
training linear classiﬁers on ﬁxed image representations.
We follow the evaluation setup from and measure the
performance of such classiﬁers on four image-classiﬁcation
datasets: ImageNet , VOC07 , Places205 , and
iNaturalist2018 . These datasets involve diverse tasks
such as object classiﬁcation, scene recognition and ﬁnegrained recognition.
Following , we evaluate representations extracted from all intermediate layers of the pretrained network, and report the image-classiﬁcation results
for the best-performing layer in Table 2.
ImageNet results.
The results on ImageNet highlight
the beneﬁts of learning invariant features: PIRL improves
recognition accuracies by over 15% compared to its covariant counterpart, Jigsaw. PIRL achieves the highest singlecrop top-1 accuracy of all self-supervised learners that use
a single ResNet-50 model.
Parameters
Transfer Dataset
ImageNet VOC07 Places205 iNat.
ResNet-50 using evaluation setup of 
Supervised
Colorization 
Rotation 
NPID++ 
Jigsaw 
PIRL (ours)
Different architecture or evaluation setup
BigBiGAN 
DeepCluster 
CPC-Huge 
BigBiGAN-Big 
Table 2: Image classiﬁcation with linear models. Image-classiﬁcation
performance on four datasets using the setup of .
We train linear
classiﬁers on image representations obtained by self-supervised learners
that were pre-trained on ImageNet (without labels). We report the performance for the best-performing layer for each method. We measure mean
average precision (mAP) on the VOC07 dataset and top-1 accuracy on all
other datasets. Numbers for PIRL, NPID++, Rotation were obtained by
us; the other numbers were adopted from their respective papers. Numbers with † were measured using 10-crop evaluation. The best-performing
self-supervised learner on each dataset is boldfaced.
The beneﬁts of pretext invariance are further highlighted
by comparing PIRL with NPID. Our re-implementation of
NPID (called NPID++) substantially outperforms the results reported in .
Speciﬁcally, NPID++ achieves a
single-crop top-1 accuracy of 59%, which is higher or on
par with existing work that uses a single ResNet-50. Yet,
PIRL substantially outperforms NPID++.
We note that
PIRL also outperforms concurrent work in this setting.
Akin to prior approaches, the performance of PIRL improves with network size. For example, CMC uses a
combination of two ResNet-50 models and trains the linear classiﬁer for longer to obtain 64.1% accuracy. We performed an experiment in which we did the same for PIRL,
and obtained a top-1 accuracy of 65.7%; see “PIRL-ens.” in
Figure 2. To compare PIRL with larger models, we also performed experiments in which we followed by doubling the number of channels in ResNet-50; see “PIRL-c2x”
in Figure 2. PIRL-c2x achieves a top-1 accuracy of 67.4%,
which is close to the accuracy obtained by AMDIM with
a model that has 6× more parameters.
Altogether, the results in Figure 2 demonstrate that PIRL
outperforms all prior self-supervised learners on ImageNet
in terms of the trade-off between model accuracy and size.
Indeed, PIRL even outperforms most self-supervised learn-
Data fraction →
Top-5 Accuracy
Random initialization 
Jigsaw 
NPID++ 
VAT + Ent Min. 
S4L Exemplar 
S4L Rotation 
PIRL (ours)
Colorization 
CPC-Largest 
R-170 and R-11
Table 3: Semi-supervised learning on ImageNet. Single-crop top-5 accuracy on the ImageNet validation set of self-supervised models that are
ﬁnetuned on 1% and 10% of the ImageNet training data, following .
All numbers except for Jigsaw, NPID++ and PIRL are adopted from the
corresponding papers. Best performance is boldfaced.
ers that use much larger models .
Results on other datasets. The results on the other imageclassiﬁcation datasets in Table 2 are in line with the results
on ImageNet: PIRL substantially outperforms its covariant
counterpart (Jigsaw). The performance of PIRL is within
2% of fully supervised representations on Places205, and
improves the previous best results of on VOC07 by
more than 16 AP points. On the challenging iNaturalist
dataset, which has over 8, 000 classes, we obtain a gain
of 11% in top-1 accuracy over the prior best result .
We observe that the NPID++ baseline performs well on
these three datasets but is consistently outperformed by
PIRL. Indeed, PIRL sets a new state-of-the-art for selfsupervised representations in this learning setting on the
VOC07, Places205, and iNaturalist datasets.
3.3. Semi-Supervised Image Classiﬁcation
We perform semi-supervised image classiﬁcation experiments on ImageNet following the experimental setup
of . Speciﬁcally, we randomly select 1% and
10% of the ImageNet training data (with labels).
ﬁnetune our models on these training-data subsets following the procedure of . Table 3 reports the top-5 accuracy
of the resulting models on the ImageNet validation set.
The results further highlight the quality of the image representations learned by PIRL: ﬁnetuning the models on just
1% (∼13,000) labeled images leads to a top-5 accuracy of
57%. PIRL performs at least as well as S4L and better than VAT , which are both methods speciﬁcally designed for semi-supervised learning. In line with earlier results, PIRL also outperforms Jigsaw and NPID++.
3.4. Pre-Training on Uncurated Image Data
Most representation learning methods are sensitive to the
data distribution used during pre-training .
To study how much changes in the data distribution im-
Transfer Dataset
Jigsaw 
DeepCluster 
PIRL (ours)
Jigsaw 
DeeperCluster 
Table 4: Pre-training on uncurated YFCC images. Top-1 accuracy or
mAP (for VOC07) of linear image classiﬁers for four image-classiﬁcation
tasks, using various image representations. All numbers (except those for
PIRL) are adopted from the corresponding papers. Deep(er)Cluster uses
VGG-16 rather than ResNet-50. The best performance on each dataset is
boldfaced. Top: Representations obtained by training ResNet-50 models
on a randomly selected subset of one million images. Bottom: Representations learned from about 100 million YFCC images.
pact PIRL, we pre-train models on uncurated images from
the unlabeled YFCC dataset . Following , we
randomly select a subset of 1 million images (YFCC-1M)
from the 100 million images in YFCC. We pre-train PIRL
ResNet-50 networks on YFCC-1M using the same procedure that was used for ImageNet pre-training. We evaluate
using the setup in Section 3.2 by training linear classiﬁers
on ﬁxed image representations.
Table 4 reports the top-1 accuracy of the resulting classiﬁers. In line with prior results, PIRL outperforms competing self-supervised learners. In fact, PIRL even outperforms Jigsaw and DeeperCluster models that were trained
on 100× more data from the same distribution. Comparing pre-training on ImageNet (Table 2) with pre-training
YFCC-1M (Table 4) leads to a mixed set of observations.
On ImageNet classiﬁcation, pre-training (without labels) on
ImageNet works substantially better than pre-training on
YFCC-1M. In line with prior work , however, pretraining on YFCC-1M leads to better representations for image classiﬁcation on the Places205 dataset.
4. Analysis
We performed a set of experiments aimed at better understanding the properties of PIRL. To make it feasible to
train the larger number of models needed for these experiments, we train the models we study in this section for fewer
epochs (400) and with fewer negatives (N = 4, 096) than
in Section 3. As a result, we obtain lower absolute performances. Apart from that, we did not change the experimental setup or any of the other hyperparameters. Throughout
the section, we use the evaluation setup from Section 3.2
that trains linear classiﬁers on ﬁxed image representations
to measure the quality of image representations.
4.1. Analyzing PIRL Representations
Does PIRL learn invariant representations?
PIRL was designed to learn representations that are invariant to image transformation t ∈T . We analyzed whether
the learned representations actually have the desired invari-
l2 distance between unit norm representations
Proportion of Samples
Jigsaw 
Figure 4: Invariance of PIRL representations. Distribution of l2 distances between unit-norm image representations, f(vI)/∥f(vI)∥2, and
unit-norm representations of the transformed image, g(vIt)/∥g(vIt)∥2.
Distance distributions are shown for PIRL and Jigsaw representations.
ance properties. Speciﬁcally, we normalize the representations to have unit norm and compute l2 distances between
the (normalized) representation of image, f(vI), and the
(normalized) representation its transformed version, g(vIt).
We repeat this for all transforms t ∈T and for a large set of
images. We plot histograms of the distances thus obtained
in Figure 4. The ﬁgure shows that, for PIRL, an image representation and the representation of a transformed version
of that image are generally similar. This suggests that PIRL
has learned representations that are invariant to the transformations. By contrast, the distances between Jigsaw representations have a much larger mean and variance, which
suggests that Jigsaw representations covary with the image
transformations that were applied.
Which layer produces the best representations?
All prior experiments used PIRL representations that were
extracted from the res5 layer and Jigsaw representations
that were extracted from the res4 layer (which work better for Jigsaw). Figure 5 studies the quality of representations in earlier layers of the convolutional networks. The
ﬁgure reveals that the quality of Jigsaw representations improves from the conv1 to the res4 layer but that their quality sharply decreases in the res5 layer. We surmise this
happens because the res5 representations in the last layer of
the network covary with the image transformation t and are
not encouraged to contain semantic information. By contrast, PIRL representations are invariant to image transformations, which allows them to focus on modeling semantic
information. As a result, the best image representations are
extracted from the res5 layer of PIRL-trained networks.
4.2. Analyzing the PIRL Loss Function
What is the effect of λ in the PIRL loss function?
The PIRL loss function in Equation 5 contains a hyperparameter λ that trades off between two NCE losses. All prior
Top-1 Accuracy
Jigsaw 
Figure 5: Quality of PIRL representations per layer. Top-1 accuracy of
linear models trained to predict ImageNet classes based on representations
extracted from various layers in ResNet-50 trained using PIRL and Jigsaw.
Relative weight of loss terms ( ) in Equation 5
Top-1 Accuracy
Figure 6: Effect of varying the trade-off parameter λ. Top-1 accuracy
of linear classiﬁers trained to predict ImageNet classes from PIRL representations as a function of hyperparameter λ in Equation 5.
experiments were performed with λ = 0.5. NPID(++) 
is a special case of PIRL in which λ=0, effectively removing the pretext-invariance term from the loss. At λ = 1, the
network does not compare untransformed images at training
time and updates to the memory bank mI are not dampened.
We study the effect of λ on the quality of PIRL representations. As before, we measure representation quality by
the top-1 accuracy of linear classiﬁers operating on ﬁxed
ImageNet representations. Figure 6 shows the results of
these experiments. The results show that the performance
of PIRL is quite sensitive to the setting of λ, and that the
best performance if obtained by setting λ=0.5.
What is the effect of the number of image transforms?
Both in PIRL and Jigsaw, it is possible to vary the complexity of the task by varying the number of permutations of
the nine image patches that are included in the set of image
transformations, T . Prior work on Jigsaw suggests that increasing the number of possible patch permutations leads to
better performance . However, the largest value |T |
can take is restricted because the number of learnable parameters in the output layer grows linearly with the number
# of permutations of patches
Jigsaw 
Figure 7: Effect of varying the number of patch permutations in T .
Performance of linear image classiﬁcation models trained on the VOC07
dataset in terms of mAP. Models are initialized by PIRL and Jigsaw, varying the number of image transformations, T , from 1 to 9! ≈3.6 million.
of patch permutations in models trained to solve the Jigsaw
task. This problem does not apply to PIRL because it never
outputs the patch permutations, and thus has a ﬁxed number
of model parameters. As a result, PIRL can use all 9! ≈3.6
million permutations in T .
We study the quality of PIRL and Jigsaw as a function
of the number of patch permutations included in T . To facilitate comparison with , we measure quality in terms
of performance of linear models on image classiﬁcation using the VOC07 dataset, following the same setup as in Section 3.2. The results of these experiments are presented in
Figure 7. The results show that PIRL outperforms Jigsaw
for all cardinalities of T but that PIRL particularly beneﬁts
from being able to use very large numbers of image transformations (i.e., large |T |) during training.
What is the effect of the number of negative samples?
We study the effect of the number of negative samples, N,
on the quality of the learned image representations. We
measure the accuracy of linear ImageNet classiﬁers on ﬁxed
representations produced by PIRL as a function of the value
of N used in pre-training. The results of these experiments
are presented in Figure 8. They suggest that increasing the
number of negatives tends to have a positive inﬂuence on the
quality of the image representations constructed by PIRL.
4.3. Generalizing PIRL to Other Pretext Tasks
Although we studied PIRL in the context of Jigsaw
in this paper, PIRL can be used with any set of image
transformations, T .
We performed an experiment evaluating the performance of PIRL using the Rotation pretext task . We deﬁne T to contain image rotations by
{0◦, 90◦, 180◦, 270◦}, and measure representation quality
in terms of image-classiﬁcation accuracy of linear models
(see the supplemental material for details).
The results of these experiments are presented in Table 5
(top). In line with earlier results, models trained using PIRL
Number of negatives N in Equation 3
Top-1 Accuracy
Figure 8: Effect of varying the number of negative samples. Top-1 accuracy of linear classiﬁers trained to perform ImageNet classiﬁcation using
PIRL representations as a function of the number of negative samples, N.
(Rotation) outperform those trained using the Rotation pretext task of .
The performance gains obtained from
learning a rotation-invariant representation are substantial,
e.g. +11% top-1 accuracy on ImageNet. We also note that
PIRL (Rotation) outperforms NPID++ (see Table 2). In a
second set of experiments, we combined the pretext image
transforms from both the Jigsaw and Rotation tasks in the
set of image transformations, T . Speciﬁcally, we obtain It
by ﬁrst applying a rotation and then performing a Jigsaw
transformation. The results of these experiments are shown
in Table 5 (bottom). The results demonstrate that combining image transforms from multiple pretext tasks can further
improve image representations.
5. Related Work
Our study is related to prior work that tries to learn
characteristics of the image distribution without considering a corresponding (image-conditional) label distribution.
A variety of work has studied reconstructing images from a small, intermediate representation, e.g., using
sparse coding , adversarial training , autoencoders , or probabilistic versions thereof .
More recently, interest has shifted to specifying pretext tasks that require modeling a more limited set
of properties of the data distribution.
For video data,
these pretext tasks learn representations by ordering video
frames , tracking , or using
cross-modal signals like audio .
Our work focuses on image-based pretext tasks. Prior
pretext tasks include image colorization , orientation prediction , afﬁne transform prediction , predicting contextual image patches , reordering image patches , counting visual
primitives , or their combinations . In contrast, our
works learns image representations that are invariant to the
image transformations rather than covariant.
PIRL is related to approaches that learn invariant image
Transfer Dataset
ImageNet VOC07 Places205 iNat.
Rotation 
PIRL (Rotation; ours)
Combining pretext tasks using PIRL
PIRL (Jigsaw; ours)
PIRL (Rotation + Jigsaw; ours) 25.6M
Table 5: Using PIRL with (combinations of) different pretext tasks.
Top-1 accuracy / mAP of linear image classiﬁers trained on PIRL image
representations. Top: Performance of PIRL used in combination with the
Rotation pretext task . Bottom: Performance of PIRL using a combination of multiple pretext tasks.
representations via contrastive learning ,
clustering , or maximizing mutual information . PIRL is most similar to methods that learn
representations that are invariant under standard data augmentation . PIRL learns representations that are invariant to both the data augmentation and to
the pretext image transformations.
Finally, PIRL is also related to approaches that use a contrastive loss in predictive learning . These prior approaches predict missing parts of the
data, e.g., future frames in videos , or operate on
multiple views . In contrast to those approaches, PIRL
learns invariances rather than predicting missing data.
6. Discussion and Conclusion
We studied Pretext-Invariant Representation Learning
(PIRL) for learning representations that are invariant to image transformations applied in self-supervised pretext tasks.
The rationale behind PIRL is that invariance to image transformations maintains semantic information in the representation. We obtain state-of-the-art results on multiple benchmarks for self-supervised learning in image classiﬁcation
and object detection. PIRL even outperforms supervised
ImageNet pre-training on object detection.
In this paper, we used PIRL with the Jigsaw and Rotation
image transformations. In future work, we aim to extend to
richer sets of transformations. We also plan to investigate
combinations of PIRL with clustering-based approaches . Like PIRL, those approaches use inter-image statistics
but they do so in a different way. A combination of the two
approaches may lead to even better image representations.
Acknowledgments:
We are grateful to Rob Fergus, and Andrea Vedaldi for encouragement and their feedback on early versions of the manuscript; Yann LeCun for helpful discussions;
Aaron Adcock, Naman Goyal, Priya Goyal, and Myle Ott for their
help with the code development for this research; and Rohit Girdhar, and Ross Girshick for feedback on the manuscript. We thank
Yuxin Wu, and Kaiming He for help with Detectron2.