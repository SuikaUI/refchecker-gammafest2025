Neural Network Ensemble Operators for Time Series
Forecasting
Nikolaos Kourentzesa,∗, Devon K. Barrowa, Sven F. Cronea
aLancaster University Management School
Department of Management Science, Lancaster, LA1 4YX, UK
The combination of forecasts resulting from an ensemble of neural networks
has been shown to outperform the use of a single “best” network model. This
is supported by an extensive body of literature, which shows that combining
generally leads to improvements in forecasting accuracy and robustness, and
that using the mean operator often outperforms more complex methods of
combining forecasts. This paper proposes a mode ensemble operator based
on kernel density estimation, which unlike the mean operator is insensitive
to outliers and deviations from normality, and unlike the median operator
does not require symmetric distributions. The three operators are compared
empirically and the proposed mode ensemble operator is found to produce
the most accurate forecasts, followed by the median, while the mean has
relatively poor performance. The ﬁndings suggest that the mode operator
should be considered as an alternative to the mean and median operators
in forecasting applications. Experiments indicate that mode ensembles are
useful in automating neural network models across a large number of time
series, overcoming issues of uncertainty associated with data sampling, the
stochasticity of neural network training and the distribution of the forecasts.
Time Series, Forecasting, Ensembles, Combination, Mode
Estimation, Kernel Density Estimation, Neural Networks, Mean, Median
∗Correspondance: N Kourentzes, Department of Management Science, Lancaster University Management School, Lancaster, Lancashire, LA1 4YX, UK. Tel.: +44-1524-592911
Email address: (Nikolaos Kourentzes)
 
December 5, 2013
1. Introduction
With the continuing increase in computing power and availability of data,
there has been a growing interest in the use artiﬁcial Neural Networks (NNs)
for forecasting purposes. NNs are typically used as ensembles of several network models to deal with sampling and modelling uncertainties that may
otherwise impair their forecasting accuracy and robustness. Ensembles combine forecasts from the diﬀerent models that comprise them.
This paper
proposes a new fundamental ensemble operator for neural networks that is
based on estimating the mode of the forecast distribution, which has appealing properties compared to established alternatives.
Although the use of ensembles is nowadays accepted as the norm in forecasting with NNs , their performance is a function of how
the individual forecasts are combined . Improvements in the ensemble combination operators have direct impact on the resulting forecasting accuracy and the decision making that forecasts support.
This has implications for multiple forecasting applications where NN ensembles have been used. Some examples include diverse forecasting applications
such as: economic modelling and policy making , ﬁnancial and commodities trading , fast-moving consumer goods , tourism , electricity load , temperature and weather , river ﬂood and hydrological modelling , climate , and ecology to name a few. Zhang et al.
 lists multiple other forecasting applications where they have been employed successfully.
As NN ensembles are fundamental for producing accurate NN forecasts
for these various applications; hence, improvements in the construction of
the ensembles are important. In this paper, the performance of the proposed
mode operator is investigated together with the two existing fundamental
ensemble operators: the mean and the median. Two diﬀerent datasets, including 3,443 real time series, are used to empirically evaluate the diﬀerent
operators. Furthermore, ensembles of both training initialisations and sampling (bagging) are used to investigate the performance of the operators.
The proposed operator is found to be superior to established alternatives.
Moreover, the robustness and good performance of the median operator is
validated. The ﬁndings provide useful insights for the application of NNs in
large scale forecasting systems, where robustness and accuracy of the forecasts are equally desirable.
The rest of the paper is organised as follows: section 2 discusses the
beneﬁts of NN ensembles and the limitations of the established ensemble
operators. Section 3 introduces multilayer perceptrons that will be used for
this paper and section 4 discusses the three fundamental ensemble operators
and presents the proposed method for mode ensembles. Sections 5 and 6
discuss the experimental design and the results respectively, followed by a
discussion of the ﬁndings in section 7.
2. Forecasting with neural networks
Over the last two decades there has been substantial research in the use of
NNs for forecasting problems, with multiple successful applications . Adya and Collopy found that NNs outperformed established statistical benchmarks in 73% of the papers reviewed. NNs are ﬂexible
nonlinear data driven models that have attractive properties for forecasting.
They have been proven to be universal approximators , being able to ﬁt to any underlying data generating process.
NNs have been empirically shown to be able to forecast both linear and nonlinear time series of diﬀerent forms. Their
attractive properties have led to the rise of several types of NNs and applications in the literature .
While NNs powerful approximation capabilities and self-adaptive data
driven modelling approach allow them great ﬂexibility in modelling time
series data, it also complicates substantially model speciﬁcation and the estimation of their parameters. Direct optimisation through conventional minimisation of error is not possible under the multilayer architecture of NNs
and the back-propagation learning algorithm has been proposed to solve this
problem , later discussed in the context of time series
by Werbos . Several complex training (optimisation) algorithms have
appeared in the literature, which may nevertheless be stuck in local optima
 . To alleviate this problem, training of
the networks may be initialised several times and the best network model
selected according to some ﬁtting criteria. However, this may still lead to
suboptimal selection of parameters depending on the ﬁtting criterion, resulting in loss of predictive power in the out-of-sample set . Another challenge in the parameter estimation of NNs is due to the
uncertainty associated with the training sample.
Breiman in his
work on instability and stabilization in model selection showed that subset
selection methods in regression, including artiﬁcial neural networks, are unstable methods. Given a data set and a collection of models, a method is
deﬁned as unstable if a small change in the data results in large changes in
the set of models.
These issues pose a series of challenges in selecting the most appropriate
model for practical applications and currently no universal guidelines exist
on how best to do this.
In dealing with the ﬁrst, the NN literature has
strongly argued, with supporting empirical evidence, that instead of selecting
a single NN that may be susceptible to poor initial values (or model setup),
it is preferable to consider a combination of diﬀerent NN models . Naftaly et al. 
showed that ensembles across NN training initialisations of the same model
can improve accuracy while removing the need for identifying and choosing
the best training initialisation. This has been veriﬁed numerous times in the
literature . These ensembles aim
at reducing the parameter uncertainty due to the stochasticity of the training
of the networks. Instead of relying on a single network that may be stuck
to a local minima during its training, with poor forecasting performance, a
combination of several networks is used. In the case of uncertainty about the
training data, Breiman proposed Bagging (Bootstrap aggregation
and combination) for generating ensembles. The basic idea behind bagging
is to train a model on permutations of the original sample and then combine
the resulting models. The resulting ensemble is robust to small changes in
the sample, alleviating this type of uncertainty. Recent research has lead
to a series of studies involving the application of the Bagging algorithm for
forecasting purposes with positive results in many application areas . Apart from improving accuracy, using
ensembles also avoids the problem of identifying and choosing the best trained
In either case, neural network ensembles created from multiple initialisations or from the application of the Bagging algorithm, require the use
of an ensemble combination operator. The forecast combination literature
provides insights on how to best do this. Bates and Granger were
amongst the ﬁrst to show signiﬁcant gains in forecasting accuracy through
model combination. Newbold and Granger showed that a linear combination of univariate forecasts often outperformed individual models, while
Ming Shi et al. provided similar evidence for nonlinear combinations.
Makridakis and Winkler using simple averages concluded that the
forecasting accuracy of the combined forecast improved, while the variability of accuracy among diﬀerent combinations decreased as the number of
methods in the average increased. The well known M competitions provided
support to these results; model combination through averages improves accuracy . Elliott and
Timmermann showed that the good performance of equally weighted
model averages is connected to the mean squared error loss function and
under varying conditions optimally weighted averages can lead to better accuracy. Agnew found good accuracy of the median as an operator to
combine forecasts. Stock and Watson considered simple averages, medians and trimmed averages of forecast, ﬁnding the average to be the most
accurate, although one would expect the more robust median or trimmed
mean to perform better. On the other hand, McNees found no signiﬁcant diﬀerences between the performance of the mean and the median.
Kourentzes et al. showed that combining models ﬁtted on data sampled at diﬀerent frequencies can achieve better forecasting accuracy at all
short, medium and long term forecast horizons and found small diﬀerences
in using either the mean or the median.
There is a growing consensus that model combination has advantages over
selecting a single model not only in terms of accuracy and error variability,
but also simplifying model building and selection, and therefore the forecasting process as a whole. Nonetheless, the question of how to best combine
diﬀerent models has not been resolved. In the literature there are many different ensemble methods, often based on the fundamental operators of mean
and median, in an unweighted or weighted fashion. Barrow et al. argued that the distribution of the forecasts involved in the calculation of the
ensemble prediction may include outliers that may harm the performance
of mean-based ensemble forecasts. Therefore, they proposed removing such
elements from the ensemble, demonstrating improved performance. Jose and
Winkler using a similar argument advocated the use of trimmed and
winsorised means. On the other hand, median based ensembles, are more
robust to outliers and such special treatment may be unnecessary. However,
the median, as a measure of central tendency is not robust to deviations from
symmetric distributions. The median will merely calculate the middle value
that separates the higher half from the lower half of the dataset, which is not
guaranteed to describe well the location of the distribution of the forecasts
that are used to construct the ensemble.
Taking a diﬀerent perspective, ensembles provide an estimate of where
most forecasts tend to be. Mean and median are merely measures of the central tendency of the forecast distribution. In the case of normal distribution
these coincide. Outliers and deviations from normality harm the quality of
the estimation. An apparent alternative, that in theory is free of this problem, is the mode. This measure of central tendency has been overlooked in
the combination literature because of its inherent diﬃculty in estimating it
for unknown distributions. This paper exploits the properties of the mode
to propose a new fundamental ensemble operator. In the following sections
this operator is introduced and evaluated against established alternatives.
3. Multilayer Perceptrons
The most commonly used form of NNs for forecasting is the feedforward
Multilayer Perceptron. The one-step ahead forecast ˆyt+1 is computed using
inputs that are lagged observations of the time series or other explanatory
variables. I denotes the number of inputs pi of the NN. Their functional
ˆyt+1 = β0 +
(1), w = (β, γ) are the network weights with β = [β1, . . . , βH],
γ = [γ11, . . . , γHI] for the output and the hidden layers respectively. The β0
and γ0i are the biases of each neuron, which for each neuron act similarly
to the intercept in a regression. H is the number of hidden nodes in the
network and g(·) is a non-linear transfer function, which is usually either
the sigmoid logistic or the hyperbolic tangent function.
NNs can model
interactions between inputs, if any. The outputs of the hidden nodes are
connected to an output node that produces the forecast. The output node
is often linear as in eq. (1).
Figure 1: Contour plot of the error surface of a neural network. The initial (⊕) and ending
(•) weights for six diﬀerent training initialisations are marked.
In the time series forecasting context, neural networks can be perceived as
equivalent to nonlinear autoregressive models . Lags of
the time series, potentially together with lagged observations of explanatory
variables, are used as inputs to the network. During training pairs of input
vectors and targets are presented to the network. The network output is
compared to the target and the resulting error is used to update the network
weights. NN training is a complex nonlinear optimisation problem and the
network can often get trapped in local minima of the error surface. In order
to avoid poor quality results, training should be initialised several times with
diﬀerent random starting weights and biases to explore the error surface
more fully. Figure 1 provides an example of an error surface of a very simple
NN. The example network is tasked to model a time series with a simple
autoregressive input and is of the form ˆyt+1 = g (w2g (w1yt−1)), where g(·)
is the hyperbolic tangent and w1 and w2 its weights. Six diﬀerent training
initialisations, with their respective ﬁnal weights, are shown. Observe that
minor diﬀerences in the starting weights can result in diﬀerent estimates, even
for such a simple model. In order to counter this uncertainty an ensemble
of all trained networks can be used. As discussed before, this approach has
been shown to be superior to choosing a single set of estimated weights.
Note that the objective of training is not to identify the global optimum.
This would result in the model over-ﬁtting to the training sample and would
then generalise poorly to unseen data , in particular given
their powerful approximation capabilities . Furthermore, as
new data become available, the prior global optimum may no longer be an
In general, as the ﬁtting sample changes, with the availability of new
information, so do the ﬁnal weights of the trained networks, even if the
initial values of the network weights were kept constant.
This sampling
induced uncertainty can again be countered by using ensembles of models,
following the concept of bagging.
4. Ensemble operators
Let ˆymt be a forecast from model m for period t, where m = 1, . . . , M
and M the number of available forecasts to be combined in an ensemble
forecast ˜yt. In this section the construction of ˜yt using the mean, median and
the proposed mode operators is discussed. To apply any of these operators
reliably a unimodal distribution is assumed.
4.1. Mean ensemble
The mean is one of the most commonly used measures of central tendency
and can be weighted or unweighted. Let wm be the weight for the forecasts
from model m.
Conventionally 0 ≤wm ≤1 and PM
m=1 wm = 1.
ensemble forecast for period t is calculated as:
If all wm = M −1 the resulting combination is unweighted. The properties of
the mean are well known, as well as its limitations. The mean is sensitive to
outliers and unreliable for skewed distributions. To avoid some of its problems one might use a winsorised or truncuted mean .
In this case the mean behaves more closely to the median. For distributions
with ﬁnite variance, which is true for sets of forecasts, the maximum distance
between the mean and the median is one standard deviation .
4.2. Median ensemble
Similarly the median can be unweighted or weighted, although the latter
is rarely used. The median ensemble ˜yMedian
is simply calculated sorting
wmymt and picking the middle value if M is odd or the mean of the two
middle values otherwise. Although the median is more robust than the mean
it still suﬀers with non-symmetric distributions.
4.3. Mode Ensemble
The mode is deﬁned as the most frequent value in a set of data. The
mode is insensitive to outliers, in contrast to the mean and median. There is
no formula to calculate the mode of an unknown distribution for continuous
variables. There are two common ways to calculate it: either by discretising
the data and identifying the most frequent bin, or by kernel density estimation. In this work the second approach is preferred in order to avoid the
discretisation of the data. Furthermore, kernel density estimation lends itself
well to the continuous-valued nature of forecasts.
Kernel density estimation is a non-parametric way to estimate the probability density function of a random variable, in this case the forecasts. Given
forecasts of a distribution with unknown density f, we can approximate its
shape using the kernel density estimator
ˆfth(x) = (Mh)−1
where K(·) is a function with the property
K(x)dx = 1 that is called kernel
and h > 0 is its bandwidth. The kernel is often chosen to be a unimodal
symmetric density function, therefore making ˆfh(x) a density function itself,
which is often, for computational reasons, the Gaussian kernel φ(x):
Figure 2 shows an example of the calculation of kernel density. A kernel
with bandwidth h is ﬁtted around each observation and the resulting sum
approximates the density function of the sample.
A number of alternative kernel functions have been proposed in the literature, however the choice of kernel has been found to have minimal impact on
the outcome for most cases . The bandwidth of the
kernel h controls the amount of smoothing. A high bandwidth results in more
Observations
Figure 2: Example calculation of kernel density estimation.
smoothing. Therefore, the choice of h is crucial, as either under-smoothing
or over-smoothing will provide misleading estimation of the density f .
The approximation by Silverman is often used in
where ˆσ is the standard deviation of the sample of the forecasts. This approximation is often adequate for Gaussian kernels. Botev et al. propose
a methodology to automatically select the bandwidth that is free from the
arbitrary normal reference rules used by existing methods. This is preferred
in the calculation of the mode ensemble as the resulting bandwidth h allows
fast convergence and good performance of the ensemble, as it is discussed in
the results.
The value x that corresponds to the maximum density approximates the
mode of the true underlying distribution for a set of forecasts, which is also
the value of the mode ensemble ˆyMode
t+h . This is true as long as the estimated
distribution is unimodal. Although the probability of facing non-unimodal
distributions when dealing with forecasts is low, the following heuristic is
proposed to resolve such cases.
Since there is no preference between the
modes, the one closer to the previous (forecasted or actual) value is retained
as the mode.
This results in smooth trace forecasts.
(3) results in
unweigthed ˆyMode
t+h . It is trivial to introduce wm individual weights for each
For kernel density estimation to adequately reveal the underlying density
a relatively large number of observations are required. A small number of
observations will lead to a bad approximation. This is illustrated in ﬁgure 3.
It shows the mean, median, mode ensembles as well as the selected “best”
model forecast, as selected using a validation sample for four diﬀerent forecast
Furthermore, the estimated kerned density for each horizon is
It is apparent by comparing 3a and 3b that the kernel density
estimation using only 10 models is very poor.
While in 3a the shape of
the distribution is successfully approximated, in 3b there are not enough
forecasts to identify the underlying shape of the distribution of the forecasts.
Furthermore, in ﬁgure 3a it is easy to see that neither the mean, median or the
“best” model are close to the most probable value of the forecast distribution.
The mode ensemble oﬀers an intuitive way of identifying where forecasts
from diﬀerent models converge and provide a robust forecast, independent of
distributional assumptions.
5. Empirical Evaluation
5.1. Datasets
To empirically evaluate the performance of the mean, median and the
proposed mode ensemble for NNs, two large datasets of real monthly time
series are used. The ﬁrst dataset comes from Federal Reserve Economic Data
(FRED) of St. Luis.1 From the complete dataset 3,000 monthly time series
that contain 108 or more observations (9 years) were sampled. Long time
series were preferred to allow for adequate training, validation and test sets.
The second dataset comes from the UK Oﬃce for National Statistics and
contains 443 monthly retail sales time series.2 Again, only time series with
108 or more observations were retained for the empirical evaluation.
A summary of the characteristics of the time series in each dataset is
provided in table 1. To identify the presence of trend in a time series the coxstuart test was employed on a 12-period centred moving average ﬁtted to each
time series. The test was performed on the centred moving average to smooth
any eﬀects from irregularities and seasonality. To identify the presence of
seasonality, seasonal indices were calculated for the de-trended time series
and then these were tested for signiﬁcant deviations from each other by means
of a Friedman test. This procedure, based on non-parametric tests, is robust,
1The dataset can be accessed at 
 
retail-sales/january-2012/tsd-retail-sales.html.
(a) 100 models
(b) 10 models
Figure 3: Example of the distribution of NN forecasts of diﬀerent number of models,
as estimated by Gaussian kernel density estimation, for the ﬁrst four steps ahead. The
forecasts by model selection, mean, median and mode ensembles are provided.
however diﬀerent tests may provide slightly diﬀerent percentages to those in
Table 1: Dataset characteristics.
Series Length
Series Patterns
Trend-Season
The last 18 observations from each time series are withheld as test set.
The prior 18 observations are used as validation set to accommodate NNs
5.2. Experimental Design
A number of NN ensemble models are ﬁtted to each time series. Two are
based on mean, two on median and two on mode ensembles. Hereafter, these
are named NN-Mean, NN-Median and NN-Mode respectively. All combination operators are applied in their unweighted version, as the objective is to
test their fundamental performance. In each pair of ensembles, the ﬁrst is
a training ensemble, combining multiple training initialisations and the second is based on bagging, as described by Kunsch . This moving block
bootstrap samples the original time series while preserving the temporal and
spatial covariance structure, as well as the serial correlation of the time series
data. By assessing the operators using diﬀerent types of ensembles we aim
to assess the consistency of their performance. Furthermore, diﬀerent sizes
of ensembles are evaluated, from 10 members up to 100 members, with steps
of 10. Results for single NN models, based on selecting the best one, are
not provided as there is compeling evidence in the literature that ensembles
are superior .
This was validated in our experiments as well.
The individual neural networks have identical setup. Following the suggestions of the literature, if trend is identiﬁed in a time series it is removed
through ﬁrst diﬀerencing . The time series is then linearly scaled between -0.5 and 0.5 to facilitate the NN training. The inputs
are identiﬁed through means of stepwise regression, which has been shown
to perform well for identifying univariate input lags for NNs . All networks use the hyperbolic tangent transfer function for the hidden nodes and a linear output
node. The number of hidden nodes was identiﬁed experimentally for each
time series. Up to 60 hidden nodes were evaluated for each time series and
the number of hidden nodes that minimised the validation Mean Squared
Error (MSE) was chosen.
Each network was trained using the Levenberg-Marquardt (LM) algorithm.
The algorithm requires setting a scalar µLM and its increase and
decrease steps.
When the scalar is zero, the LM algorithm becomes just
Newton’s method, using the approximate Hessian matrix.
On the other
hand, when µLM is large, it becomes gradient descent with a small step size.
Newton’s method is more accurate and faster near an error minimum, so
the aim is to shift toward Newton’s method as quickly as possible. If a step
would increase the ﬁtting error then µLM is increased. Here µLM = 10−3,
with an increase factor of µinc = 10 and a decrease factor of µdec = 10−1. For
a detailed description of the algorithm and its parameters see Hagan et al.
 . MSE was used as the training cost function. The maximum training
epochs are set to 1000. The training can stop earlier if µLM becomes equal
or greater than µmax = 1010. The MSE error at the validation set is tracked
while training. If the error increases consequently for 50 epochs then training
is stopped. The weights that give the lowest validation error are selected at
the end of each training. This is common practice in the literature and helps
to achieve good out-of-sample performance, since it avoids over-ﬁtting to the
training sample .
Following the suggestions of the forecasting literature two statistical benchmarks are used in this study, namely the
naive forecast (random walk) and exponential smoothing. This is done to
assess the accuracy gains of using NNs against established simpler statistical
methods. The Naive requires no parameterisation or setup, hence is used
as a baseline that any more complex model should outperform. The appropriate exponential smoothing model is selected for each time series, depending on the presence of trend and/or seasonality using Akaike’s Information
Criterion. Model parameters are identiﬁed by optimising the log-likelihood
function . Exponential smoothing was selected
as a benchmark based on its widely demonstrated forecasting accuracy and
robustness and will be named
ETS in this work. The use of these benchmarks can help establish the relative performance of the NN models. In total, eight forecasting models are
ﬁtted to each time series, six NNs and two statistical benchmarks.
Rolling trace forecasts of 12 months are produced using each model. The
rolling origin evaluation enables collecting a large sample of forecasts and
their errors, while being robust to irregular forecast origins and outliers, thus
providing reliable error measurements. Based on the long test set, 7 trace
forecasts from t+1 up to t+12 months are collected for each time series.
The reader is referred to Tashman for a detailed description of the
evaluation scheme and its advantages.
The forecasting accuracy is assessed using the Mean Absolute Scaled Error (MASE). This is preferred due to its favourable statistical properties.
MASE is calculated for each trace forecast as:
MASE = m−1 Xm
(n −1)−1 Pn
r=2 |yr −yr−1|,
where yj and ˆyj are the actual and forecasted value for j = 1, . . . , m outof-sample observations. The denominator is the mean absolute error of the
random walk in the ﬁtting sample of n observations and is used to scale the
error. MASE, being a scaled error, permits summarising model performance
across time series of diﬀerent scale and units, which mean squared or absolute
errors cannot do, and is less biased from errors like the mean absolute percentage error and its symmetric equivalent. Another advantage of this error
is that it is very improbable that the denominator is zero, therefore making
it easy to calculate in several scenarios and robust to time series with several
values equal or close to zero . Note that the
Retail dataset contains several time series that do not permit the calculation
of conventional percentage errors, due to zero values in the denominator. To
summarise the results across the time series of each dataset the mean and
median MASE across all series are calculated.
6. Results
Table 2 presents the results for the FRED time series. Numbers in brackets refer to median MASE, while the rest to mean MASE. The table provides
results for ensembles from 10 to 100 members. The results for bagging and
training ensembles are presented separately to assess the impact of the ensemble type on the diﬀerent ensemble operators. In each row the best performing
method according to mean and median MASE is highlighted in boldface.
Table 2: Mean (Median) MASE for FRED dataset.
Ensemble Size
1.06 (0.66)
0.92 (0.64)
1.30 (0.77)
1.11 (0.87)
3.43 (0.62)
1.06 (0.65)
0.90 (0.63)
0.94 (0.65)
1.11 (0.87)
3.43 (0.62)
1.02 (0.65)
0.89 (0.62)
0.89 (0.62)
1.11 (0.87)
3.43 (0.62)
1.04 (0.65)
0.88 (0.62)
0.88 (0.61)
1.11 (0.87)
3.43 (0.62)
1.03 (0.64)
0.88 (0.62)
0.88 (0.61)
1.11 (0.87)
3.43 (0.62)
1.03 (0.64)
0.89 (0.62)
0.88 (0.61)
1.11 (0.87)
3.43 (0.62)
1.04 (0.65)
0.88 (0.62)
0.87 (0.61)
1.11 (0.87)
3.43 (0.62)
1.03 (0.65)
0.88 (0.62)
0.87 (0.61)
1.11 (0.87)
3.43 (0.62)
1.01 (0.65)
0.88 (0.61)
0.87 (0.61)
1.11 (0.87)
3.43 (0.62)
1.01 (0.65)
0.88 (0.61)
0.87 (0.61)
1.11 (0.87)
3.43 (0.62)
Training ensemble
1.05 (0.64)
0.95 (0.62)
1.17 (0.70)
1.11 (0.87)
3.43 (0.62)
1.03 (0.65)
0.93 (0.62)
0.95 (0.64)
1.11 (0.87)
3.43 (0.62)
1.01 (0.64)
0.91 (0.62)
0.90 (0.62)
1.11 (0.87)
3.43 (0.62)
1.02 (0.64)
0.91 (0.62)
0.90 (0.61)
1.11 (0.87)
3.43 (0.62)
1.02 (0.64)
0.92 (0.62)
0.89 (0.61)
1.11 (0.87)
3.43 (0.62)
1.01 (0.64)
0.91 (0.62)
0.89 (0.62)
1.11 (0.87)
3.43 (0.62)
1.01 (0.64)
0.91 (0.61)
0.89 (0.61)
1.11 (0.87)
3.43 (0.62)
1.01 (0.64)
0.91 (0.62)
0.88 (0.61)
1.11 (0.87)
3.43 (0.62)
1.01 (0.64)
0.91 (0.61)
0.88 (0.61)
1.11 (0.87)
3.43 (0.62)
1.01 (0.64)
0.91 (0.62)
0.88 (0.61)
1.11 (0.87)
3.43 (0.62)
Overall, the diﬀerence between the mean and median MASE results indicates that there are several diﬃcult time series, particularly aﬀecting the
less robust mean MASE. Focusing on the bagging results, all NN-Mean, NN-
Median and NN-Mode are more accurate than the benchmarks when considering mean MASE. Furthermore, as the ensembles increase in size their
accuracy improves. In particular, for NN-Mode after there are 30 or more
members the forecasts are very accurate. This was to be expected since the
kernel density estimation becomes reliable once there is an adequate number of observations, as discussed in section 4. For ensembles of 70 or more
members NN-Mode provides consistently the best accuracy, closely followed
by NN-Median. Note that achieving large numbers of ensemble members is
trivial with NNs, as this merely implies that more training initialisations or
bootstrapped samples are used. Therefore, the requirement of the mode operator for 30 or more ensemble members is not a limiting factor. In contrast,
NN-Mean underperforms to the extent that ETS is more accurate for median
MASE. This is an interesting ﬁnding, given how common is the mean operator for ensembles in the literature. The more robust behaviour of median
and the in-sensitive to outliers nature of the mode result in more accurate
ensemble forecasts. Looking at mean MASE, all NNs behave more robust
than ETS, the latter being severely aﬀected by outliers.
The results of the training ensembles are very similar.
Again, as the
number of members in the ensemble increases NN-Mode performs better and
is the most accurate model for 40 or more ensemble members. NN-Median
ranks second with small diﬀerences, while NN-Mean is substantially worse.
Comparing the results between bagging and training ensembles we can see
that the former is marginally more accurate for NN-Median and NN-Mode
when mean MASE is considered. However, the same is not true for NN-Mean,
indicating that the robustness and performance of this ensemble operator is
aﬀected by the type of ensemble.
Table 3 presents the results for the Retail dataset. Its structure is the
same as in table 2. The diﬀerences between mean and median MASE are
smaller than the FRED results, showing that the time series in this dataset
are better behaved. Considering the bagging results, NN-Median consistently
outperforms the statistical benchmarks, while the same is true for NN-Mode,
once there is an adequate number of members in the ensemble (again 30
or more). NN-Mode is the most accurate model with the lowest mean and
median MASE. This is followed closely by NN-Median. On the other hand,
NN-Mean often fails to outperform the benchmark ETS, although it is always
better than the Naive.
Looking at the accuracy of the training ensembles NN-Mode is overall
more accurate for mean MASE, NN-Median is the most accurate for median
MASE. Although all NN models outperform the Naive benchmark, the differences between either NN-Mode or NN-Median and ETS are very small.
NN-Mean is worse than ETS in terms of mean MASE, while occasionally it is
marginally better in terms of median MASE. Comparing accuracies between
bagging and training ensembles there are diﬀerences in favour of the former
when looking at NN-Median and NN-Mode, while the accuracy for NN-Mean
is almost identical for both types of ensembles.
Across both datasets NN-Mode and NN-Median are the most accurate
NN-Mode seems to perform better when the size of ensemble is
large enough. NN-Median has slightly lower accuracy. While large ensembles beneﬁt NN-Median, it can perform well for small ensembles too. Both
Table 3: Mean (Median) MASE for Retail dataset.
Ensemble Size
1.33 (0.96)
1.11 (0.93)
1.44 (1.10)
1.45 (1.29)
1.12 (0.97)
1.37 (0.97)
1.10 (0.94)
1.14 (0.94)
1.45 (1.29)
1.12 (0.97)
1.29 (0.96)
1.10 (0.91)
1.09 (0.92)
1.45 (1.29)
1.12 (0.97)
1.31 (0.97)
1.10 (0.91)
1.09 (0.90)
1.45 (1.29)
1.12 (0.97)
1.30 (0.97)
1.10 (0.92)
1.09 (0.89)
1.45 (1.29)
1.12 (0.97)
1.26 (0.96)
1.09 (0.91)
1.09 (0.89)
1.45 (1.29)
1.12 (0.97)
1.26 (0.96)
1.09 (0.91)
1.09 (0.90)
1.45 (1.29)
1.12 (0.97)
1.26 (0.98)
1.09 (0.90)
1.08 (0.88)
1.45 (1.29)
1.12 (0.97)
1.27 (0.97)
1.09 (0.90)
1.09 (0.87)
1.45 (1.29)
1.12 (0.97)
1.27 (0.95)
1.09 (0.91)
1.09 (0.88)
1.45 (1.29)
1.12 (0.97)
Training ensemble
1.34 (0.97)
1.14 (0.91)
1.27 (0.97)
1.45 (1.29)
1.12 (0.97)
1.33 (0.95)
1.14 (0.91)
1.14 (0.91)
1.45 (1.29)
1.12 (0.97)
1.31 (0.96)
1.13 (0.89)
1.11 (0.90)
1.45 (1.29)
1.12 (0.97)
1.28 (0.95)
1.12 (0.91)
1.11 (0.90)
1.45 (1.29)
1.12 (0.97)
1.28 (0.96)
1.12 (0.90)
1.11 (0.91)
1.45 (1.29)
1.12 (0.97)
1.29 (0.96)
1.12 (0.89)
1.11 (0.91)
1.45 (1.29)
1.12 (0.97)
1.29 (0.95)
1.13 (0.90)
1.11 (0.90)
1.45 (1.29)
1.12 (0.97)
1.29 (0.95)
1.13 (0.90)
1.11 (0.90)
1.45 (1.29)
1.12 (0.97)
1.28 (0.95)
1.13 (0.89)
1.12 (0.90)
1.45 (1.29)
1.12 (0.97)
1.28 (0.96)
1.13 (0.89)
1.12 (0.90)
1.45 (1.29)
1.12 (0.97)
these models are on average more accurate than the statistical benchmarks
ETS and Naive. On the other hand, NN-Mean provides mixed results. In
both datasets it outperforms Naive, but not always ETS. It is substantially
outperformed by both NN-Mode and NN-Median.
7. Discussion
The value of ensembles for NNs has been argued theoretically and demonstrated empirically. The combination of the models has often involved some
type of mean operator. The empirical evaluation in this paper found that the
less commonly used median operator and the proposed mode operator are
more accurate and thus preferable. The size of the ensemble was found to
be important for the accuracy of all operators. Both mode and median, for
the two datasets investigated here, seemed on average to converge for ensem-
bles of 60 or more members, with any additional members oﬀering minimal
changes in the forecasting performance. In particular, the mode, due to its
reliance on kernel density estimation, required at least 30 members. However,
after that point it was found to be on average the most accurate ensemble operator. This is illustrated in ﬁgures 4 and 5 that present the mean MASE for
diﬀerent number of ensemble members for the FRED and the Retail datasets
respectively. The results for the diﬀerent type of ensembles have been pooled
together, since they had only small diﬀerences. Note that there is little evidence that the mean ensembles had converged even with 100 members. Even
larger ensembles were not calculated due to the substantial computational
resources required, especially when the objective is to forecast a large number of time series, which is common in supply chain and retailing forecasting
applications.
Ensemble members
FRED dataset
Figure 4: Mean MASE for diﬀerent number of ensemble members for the FRED dataset.
In order to assess whether these diﬀerences are signiﬁcant or not, we
employ the testing methodology suggested by Koning et al. that is
appropriate for comparing forecasts from multiple models. The comparison
is done across all diﬀerent ensemble sizes to highlight if an operator is consistently statistically diﬀerent. First, a Freidman test is used to assess whether
the accuracy of any model is signiﬁcantly diﬀerent from the rest. Subsequently, the MCB test is used to reveal the exact ordering of the diﬀerent
operators, and any signiﬁcant diﬀerences between them. For both datasets
the mode operator was signiﬁcantly better than the median, which in turn
was signiﬁcantly diﬀerent than the mean, at 5% signiﬁcance level.
At this point, it is useful to comment on the associated computational
Ensemble members
Retail dataset
Figure 5: Mean MASE for diﬀerent number of ensemble members for the Retail dataset
cost of the NN ensembles. The main cost comes from training the networks.
Therefore, the more ensemble members that need to be trained, the less
scalable forecasting with NNs becomes, and the operator that achieves good
forecasting performance with the least amount of members is preferable.
Table 4 provides an overview of the average time required for forecasting
across all series, for each dataset. As a diﬀerent number of hidden nodes are
used for each time series, the complexity of NN training changes, requiring
diﬀerent amount of time. To keep the presentation of the values simple, we
summarise the training time over diﬀerent series into the reported average
time. The ensemble size that gave the minimum error for each operator in
ﬁgures 4 and 5 is used as reference for the comparison. The average time
in seconds, as well as the percentage diﬀerence over the time needed for the
mode ensembles, are provided in the table. The networks were trained in
parallel on an i7-3930K CPU clocked at 4.5 Ghz with 12 logical cores.
The mode operator needed the least number of ensemble members, requiring from 25% up to 200% less time than the mean or median operators
across both datasets. Therefore, apart from the signiﬁcant gains in forecasting accuracy, the proposed ensemble operator required the least computational resources. In particular for the retailing dataset, the run-time was
more than halved.
In ﬁgures 4 and 5 it is clear that similar performance is achieved for a
large range of ensemble sizes for the median operator. This allows exchanging
marginal diﬀerences in accuracy for smaller run-times, thus improving its
scalability as well. On the other hand, this is not the case with the mean
Table 4: Average computational time comparison.
operator, the accuracy of which improves with bigger ensembles.
In the experiments, two types of ensembles were considered, bagging and
training ensembles. Each one tackles a diﬀerent type of parameter uncertainty. We examined whether the performance of the operators was aﬀected
by the type of ensemble. Again median and mode had very similar performance, favouring bagging. For the mean this behaviour was not evident.
We attribute this diﬀerent behaviour to the sensitivity of the mean to extreme values, which both median and mode are designed to avoid, albeit with
diﬀerent success.
8. Conclusions
This paper evaluates diﬀerent fundamental ensemble operators. The well
known mean and the less commonly used median were compared, together
with a proposed mode operator that is based on kernel density estimation.
All these three diﬀerent operators attempt to describe the location of the
distribution of the forecasts of the members of an ensemble. However, they
deal with outlying extreme values diﬀerently, with the mean being the most
sensitive and the mode the least. Furthermore, distributional asymmetries
can aﬀect both the mean and the median, while the mode is immune.
The ﬁndings in this paper suggest that both median and mode are very
useful operators as they provided better accuracy than mean ensembles consistently across both datasets. The mode was found to be the most accurate,
followed by the median. Based on this ﬁnding, we recommend investigating
the use of the mode and median operators further in ensembles research and
applications, which have been largely overlooked in the literature that has
mainly focused on the mean.
Furthermore, this work demonstrated that mode ensembles can robustly
and accurately forecast automatically a large number of time series with
neural networks, while the commonly used mean ensembles were often outperformed by exponential smoothing forecasts. Moreover, mean ensembles
required a very large number of members, which neither mode or median
needed, with apparent implications for computational costs. In particular,
the mode operator was found to require the least computation resources,
due to the relatively small number of ensemble members that needed to be
We have already mentioned a number of applications that can beneﬁt
from improved NN ensemble forecasts, ranging from economic and business
forecasting to climate modelling. Most of these applications are characterised
by forecasting a few, yet important, time series. The improved scalability
of mode ensembles over the commonly used mean ensembles allows applying
NNs to areas that routinely require large scale automatic forecasting, which
can beneﬁt from the nonlinear modelling capabilities of NNs. One such example is retailing, where one has to forecast a large number of products,
the sales of which are aﬀected by multiple factor that interact in a nonlinear fashion, such as pricing, promotional and temperature eﬀects. The improved scalability of mode ensembles, compounded with the ever increasing
computing capabilities provides opportunities for novel important forecasting
applications of NN ensembles. This paper found signiﬁcant savings in computing time from the proposed operator, which over the complete number of
time series accounts for several hours of computations. Such reduction will
also help using NN ensembles in high frequency forecasting cycles, where the
computational speed has been a limiting factor. Future work should explore
these potentials.
The empirical evaluation, in this paper, focused on the unweighted version
of all these operators, trying to assess their fundamental properties. Although
their diﬀerences are attributed to their robustness to extreme values, future
research should extend this work to weighted versions of the operators. This
will allow considering their use on further ensemble types, such as boosting.