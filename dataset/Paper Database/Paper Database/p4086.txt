A framework for evaluating image segmentation algorithms
Jayaram K. Udupa a,*, Vicki R. LeBlanc b,c, Ying Zhuge a, Celina Imielinska b,d,e,
Hilary Schmidt b,c, Leanne M. Currie f, Bruce E. Hirsch g, James Woodburn h
a Medical Image Processing Group, Department of Radiology, University of Pennsylvania, Philadelphia, PA, USA
b Ofﬁce of Scholarly Resources, Columbia University College of Physicians and Surgeons, New York, NY, USA
c Center for Education Research and Evaluation, Columbia University College of Physicians and Surgeons, New York, NY, USA
d Department of Biomedical Informatics, Columbia University College of Physicians and Surgeons, New York, NY, USA
e Department of Computer Science, Columbia University, New York, NY, USA
f School of Nursing, Columbia University, New York, NY, USA
g Department of Neurobiology and Anatomy, Drexel University College of Medicine, Philadelphia, PA, USA
h Rheumatology and Rehabilitation Research Unit, University of Leeds, Leeds, UK
Received 27 August 2005; accepted 12 December 2005
The purpose of this paper is to describe a framework for evaluating image segmentation algorithms. Image segmentation consists of object
recognition and delineation. For evaluating segmentation methods, three factors—precision (reliability), accuracy (validity), and efﬁciency
(viability)—need to be considered for both recognition and delineation. To assess precision, we need to choose a ﬁgure of merit, repeat
segmentation considering all sources of variation, and determine variations in ﬁgure of merit via statistical analysis. It is impossible usually
to establish true segmentation. Hence, to assess accuracy, we need to choose a surrogate of true segmentation and proceed as for precision. In
determining accuracy, it may be important to consider different ‘landmark’ areas of the structure to be segmented depending on the
application. To assess efﬁciency, both the computational and the user time required for algorithm training and for algorithm execution should
be measured and analyzed. Precision, accuracy, and efﬁciency factors have an inﬂuence on one another. It is difﬁcult to improve one factor
without affecting others. Segmentation methods must be compared based on all three factors, as illustrated in an example wherein two
methods are compared in a particular application domain. The weight given to each factor depends on application.
q 2006 Elsevier Ltd. All rights reserved.
Keywords: Image segmentation; Evaluation of segmentation; Image analysis; Segmentation efﬁcacy
1. Introduction
1.1. Background
Image segmentation is the process of identifying and
delineating objects in images. It is the most crucial among all
computerized operations done on acquired images. Even
seemingly unrelated operations like image (gray-scale/color)
display, 3D visualization, interpolation, ﬁltering, and
registration depend to some extent on image segmentation
since they all would need some object information for their
optimum performance. Ironically, segmentation is needed for
segmentation itself since object knowledge facilitates
segmentation. In spite of several decades of research ,
segmentation remains a challenging problem in image
processing and computer vision.
Image segmentation may be thought of as consisting of
two related processes—recognition and delineation.
Recognition is the high-level process of determining
roughly the whereabouts of an object of interest in the
image. Delineation is the low-level process of determining
the precise spatial extent and point-by-point composition
(material membership percentage) of the object in the
image. Humans are more qualitative and less quantitative,
whereas, computerized algorithms are more quantitative
and less qualitative. Incorporation of high-level expert
Computerized Medical Imaging and Graphics 30 75–87
www.elsevier.com/locate/compmedimag
0895-6111/$ - see front matter q 2006 Elsevier Ltd. All rights reserved.
doi:10.1016/j.compmedimag.2005.12.001
* Corresponding author. Address: Medical Image Processing Group,
Department of Radiology, University of Pennsylvania, 423 Guardian Drive,
Fourth Floor, Blockley Hall, Philadelphia, PA 19104-6021, USA. Tel.: C1
215 662 6780; fax: C1 215 898 9145.
E-mail address: (J.K. Udupa).
human knowledge algorithmically into the computer has
remained a challenge. Most of the drawbacks of current
segmentation methods may thus be attributed to the latter
weakness of computers in the recognition process. We
envisage, therefore, that the assistance of humans, knowledgeable in the application domain, will remain essential in
any practical image segmentation method. The challenge
and goal for image scientists are to develop methods that
minimize the degree of this required help as much as
While algorithms for image segmentation have been in
development for several decades , the development of
systematic evaluation frameworks for these algorithms has
been lagging, particularly in medical imaging which is the
focus of this paper. The lag is perhaps the result of
problems such as limits in common data sets with which
to compare methods, difﬁculty in deﬁning the performance
metrics and statistics, and the difﬁculty in establishing true
segmentation. As early as 1977, the need for effective
evaluation of the segmentation of biological images has
been outlined . More recently, this need has been
echoed by many researchers . In , the authors
stress the need for an objective evaluation of medical
image segmentation on large sets of common clinical data,
arguing that this is a critical step towards establishing the
validity and the clinical applicability of an algorithm.
Similarly, claims that the development of an objective
approach will provide consistency in evaluation methods
by removing biases due to human factors. Many attempts
at evaluation do not address the important components that
should be present in any evaluation methodology, thus
limiting their validity and clinical applicability. Claims
about the performance of segmentation algorithms are
limited by problems such as (a) the data sets are too small,
(b) different data sets are used for different estimations of
performance, (c) the data sets are not representative of a
clinical problem, (d) appropriate ground truths (or
surrogates) are difﬁcult to determine, (e) the performance
metrics are poorly deﬁned, (f) there is poor methodology
for training and testing the algorithms, (g) large costs of
time and effort are involved in collecting and handsegmenting data, and (h) the algorithms are not compared
against other algorithms .
In light of such difﬁculties, it is not surprising that many
researchers develop complex applications (e.g. virtual
colonoscopy systems) that make use of 3D visualizations
of anatomical images derived from 3D segmentation
methods that have not been formally evaluated by a
consistent evaluation strategy (e.g. ). Many of the
researchers who do evaluate their segmentation algorithms
do so only on a limited number of components, such as cost
analysis , inter-rater reliability , overall volume
 , or the Hausdorff distance . These efforts, despite
representing a valid attempt at evaluation, exemplify the
difﬁculty in devising comprehensive and effective segmentation evaluation methodologies in this domain.
Few researchers have made attempts to develop
evaluation frameworks that incorporate many of the
performance metrics necessary for a practical and
informative evaluation of a segmentation algorithm. In
 , the authors discuss the variety of metrics that would
result in a valid estimation of the performance of an
algorithm. When comparing a segmentation method to a
ground truth segmentation of the image, argues that
there are ﬁve possible outcomes that need to be identiﬁed.
The computer algorithm can either (a) correctly segment a
region, (b) over-segment a region, (c) under-segment a
region, (d) miss a region, or (e) incorrectly segment
a noise region. Hoover et al. also developed a rigorous
framework for the evaluation of segmentation algorithms.
This involved the use of pixel-level ground truths in 30
real images. The ground truth consisted of the handsegmentation, which was reviewed by a second human
operator to catch obvious errors. Each pixel in the region
segmented by the computer algorithm was classiﬁed as
either a correct detection, an over-segmentation, an undersegmentation, a missed pixel, or noise. Four algorithms
were then compared and described on the basis of these
metrics, as well as on the basis of processing time. Zhang
 approaches evaluation of segmentation methods by
proposing analytical and empirical methods, where the
empirical methods are divided into goodness and
discrepancy measurements. The analytical methods
examine and assess the segmentation algorithms themselves by analyzing their principles and properties. The
empirical methods indirectly judge the algorithms by
evaluating
segmentation results. The weakness of this approach is
that it is intended for ‘all images’. Because of the lack of a
general theory for image segmentation, not all characteristics of segmentation can be obtained and described by
analytical studies.
We argue that a primary reason for the lack of activity
in evaluation, commensurate with the level of investigation in segmentation algorithm development, is the lack
of a framework which algorithm developers can readily
utilize, without having to spend a great deal of time, to
assess the efﬁcacy of their methods. Such a framework,
we believe, should consist of: (F1) a speciﬁcation of
readily computable, effective, and meaningful metrics of
efﬁcacy, (F2) real life image data, (F3) reference
segmentations that can be used as surrogates of true
segmentations
segmentation algorithms, and (F5) a software system
that incorporates the evaluation methods and the standard
segmentation
algorithms.
evaluation framework to refer to this quintuple of
components (F1)–(F5). It
is clear from the above
description that a comprehensive framework for the
evaluation of segmentation algorithms in the sense of
including the ﬁve components is lacking. Even the
metrics of efﬁcacy have not considered all important
J.K. Udupa et al. / Computerized Medical Imaging and Graphics 30 75–87
factors and situations that may inﬂuence the segmentation
results. For example, variations due to images acquired
on different imaging devices (brands, sites, etc.) or slight
changes in the acquisition protocol are rarely considered.
Even the variations arising from repeat acquisitions on
the same device are rarely addressed.
There are shortcomings in the metrics of efﬁcacy
commonly-used
volume of an object of interest is not a good metric
since two sets S1 and S2 of voxels may constitute very
similar volumes, although, as segmentations of the same
physical object in two different situations, the sets may
differ signiﬁcantly in terms of the extent of overlap
(S1hS2), and false positive and false negative regions
(S1KS2 and S2KS1). Factors involving efﬁciency (practical viability of the segmentation method in terms of the
extent of the various forms of human and computational
help needed) are not at all considered except for one
aspect of the computational requirements of the method.
As we shall see later on, there are several factors (both
human and computational) that inﬂuence efﬁciency that
should be considered within the evaluation framework.
Factors relating to the quality of segmentation results
have not been considered previously. These factors allow
us to take into account in the evaluation framework how
well certain salient features of the object (e.g. a site of
attachment of a ligament, a particular segment of the
object boundary), which are considered important for the
application for which image segmentation is sought, are
captured in the segmentation.
1.2. Purpose
In summary, the gaps that exist in the currently used
evaluation strategies are of two kinds: methodological and
resource related. The former represent lapses in the
evaluation techniques currently employed. The present
paper is an attempt at ﬁlling these methodological gaps.
The latter pose challenges to the image scientist since the
evaluation tasks require considerable resources (multiple
data sets with repeat acquisitions and from different sites
and brands of imagers and for different applications with
known segmentations, software), which most algorithm
developers do not possess. We are working toward
addressing these issues and nothing further will be said
about these issues in this paper. We believe that further
work is needed in each of the ﬁve components (F1)–(F5) of
the framework. The proposed evaluation methodology is
described in Section 2 and an example is presented in
Section 3 illustrating how the methodology can be utilized
in an actual application for comparing methods. Our
concluding remarks are stated in Section 4. An early
version of this paper was presented at the SPIE Medical
Imaging 2002 conference whose proceedings contained that
paper .
2. The methodology
2.1. Notation and terminology
Any method of evaluation of segmentation algorithms
has to, at the outset, specify the application domain under
consideration. We consider the application domain to be
determined by the following three entities.
T: A task; example: volume estimation of tumors.
B: A body region; example: brain.
P: An imaging protocol; example: FLAIR MR imaging
with a particular set of parameters.
An evaluation characterizing the efﬁcacy of a particular
segmentation method a for a given application domain
hT,B,Pi that signals high performance for a may tell nothing
at all about a for a different application domain hT0,B0,P0i.
For example, a particular algorithm may have high
performance in determining the volume of a tumor in the
brain on an MR image, but may have a low performance in
segmenting a cancerous mass from a mammography scan of
a breast. Therefore, evaluation must be performed for each
application domain separately. The following additional
notations are needed for our description.
Object: A physical object of interest, denoted O, in B for
which images are acquired; example: brain tumor.
Scene: A 3D (or higher-dimensional) volume image,
denoted by CZðC;fÞ, where C is a 3D (or higherdimensional) rectangular array of voxels (short for
volume elements), and f(c) denotes the scene
intensity of any voxel c in C. C may be a vectorial
scene, meaning that f(c) may be a vector whose
components represent several imaged properties.
C is referred to as a binary scene if the range of
f(c) is {0,1}.
SX: A set of scenes acquired for the same given
application domain XZhT;B;Pi for different
The word ‘segmentation’ as used in medical imaging has
two distinct meanings in two different contexts. The ﬁrst
context is provided by computer aided diagnosis (CAD).
Detection in this context refers to the act of ﬁnding via the
given scene an abnormality (such as a lesion) that may exist
in B. The answer sought in this act is mostly to the query
whether or not a particular kind of abnormality (such as a
nodule in the lung) that may exist in B is portrayed in the
scene. If the answer is ‘yes’, the purpose of the second act of
localization is to mark on the scene location(s) where the
abnormality is determined to be present. The second context
for a different meaning for ‘segmentation’ is provided by a
wide-spread and long-standing activity that begs for a name
and an acronym of its own. We refer to this for now as
CAVA, an acronym for computer aided visualization and
J.K. Udupa et al. / Computerized Medical Imaging and Graphics 30 75–87
analysis. Brieﬂy, the goal of CAVA is to develop computer
methods for aiding humans in visualizing the objects in B in
their true form, shape, and function, and to quantify the
form, shape, and function of these objects. This is usually
for the purpose of studying in vivo the normal behavior of
an organ system in B or its disease processes in their natural
course or the effects of therapy on the disease processes. The
nature of the objectives and requirements for segmentation
are quite different in CAD and CAVA, as such, we believe
that their evaluation strategies must also be different. The
problem of evaluation addressed in this paper is as related to
segmentation in CAVA.
Segmentation of an object O in a given scene acquired for
an application domain hT,B,Pi is the process of deﬁning the
region/boundary of O in the given scene. As said previously, it
consists of two related tasks—recognition and delineation.
Although recognition in CAVA is analogous to detection in
CAD, the term detection would be inappropriate to describe
the role of recognition. For example, if the task is to quantify
brain atrophy in studying a neurological disease or its
treatment effects, the high-level act of recognizing brain
parenchyma and distinguishing it from other objects in the
head that are also portrayed in the scene is very different from
seeking an answer to the detection task of ‘whether or not the
brain is there’. Similar comments are applicable to delineation
vis-a-vis localization.
We assume that the output of any segmentation
algorithm corresponding to a given scene CZðC;fÞ is a
(hard) set O3C of voxels. This set represents the region
occupied by (the support of) an object O of B in C. To
accommodate methods that output fuzzy segmentation
results, we denote the fuzzy segmentation of O in C as a
scene COZðC;fOÞ, where, for any c2C, fO(c) denotes the
degree of objectness assigned to every voxel c in O by the
segmentation method. We shall always denote a hard
segmentation in C of an object O in B by O and the
corresponding fuzzy object by CO. Our treatment throughout will be general considering fuzzy objects to be the
output of segmentation methods. Hard segmentations will
become a particular case of this general treatment.
We will use the following operations on fuzzy
segmentations. Let COxZðC;fOxÞ; COyZðC;fOyÞ; and COzZ
ðC;fOzÞ be any fuzzy segmentations deﬁned by the same
physical object O in a scene C. The cardinality jCOxj of the
fuzzy segmentation COx is deﬁned as jCOxjZP
c2C fOxðcÞ.
Fuzzy set union COzZCOxgCOy is deﬁned by, for any
c2C, fOz(c)Zmax(fOx(c), fOy(c)). Fuzzy set intersection
COzZCOxhCOy is deﬁned by, for any c2C, fOz(c)Z
min(fOx(c), fOy(c)). Fuzzy set complement COxZðC;f OxÞ of
COx is deﬁned by, for any c2C, f OxðcÞZ1KfOxðcÞ.
Fuzzy set difference COzZCOxKCOy is deﬁned by, for
fOxðcÞKfOyðcÞ;
fOxðcÞKfOyðcÞR0
otherwise:
A fuzzy masking operation COzZCOx†COy, called
inside, is deﬁned by, for any c2C:
otherwise:
Another fuzzy masking operation COzZCOx+COy called
outside, is deﬁned by, for any c2C:
fOyðcÞ Z 0
otherwise:
2.2. Outline of methodology
The efﬁcacy of any segmentation method M in an
application domain hT,B,Pi is to be measured in terms of
three groups of factors: Precision (also known as
reliability), which represents repeatability of segmentation
taking into account all subjective actions required in
producing the result; accuracy (also known as validity),
which denotes the degree to which the segmentation agrees
with truth; efﬁciency (also known as viability), which
describes the practical viability of the segmentation method.
In evaluating segmentation efﬁcacy, both recognition and
delineation aspects must be considered. Commonly, only
delineation is considered to represent the entire segmentation process. Our methodology attempts to capture both
recognition and delineation within the same framework in
the factors considered for evaluation.
Our overall approach for the evaluation method consists
of the following steps. (1) Establishing true segmentation
for delineation. (2) Establishing true segmentation for
recognition. (3) Deﬁning metrics for precision. (4) Deﬁning
metrics for accuracy. (5) Deﬁning metrics for efﬁciency. (6)
Comparison of segmentation methods by statistical analysis
of the metrics generated by the methods on the same set of
scenes acquired for a given application domain hT,B,Pi.
These steps are described in detail in the following sections.
2.3. Surrogate of truth
For real scenes (patient scenes in medical imaging), since
it is impossible to establish absolute true segmentation,
some surrogate of truth is needed. In describing this aspect,
we will treat the delineation and recognition aspects
separately in Sections 2.3.1 and 2.3.2, respectively.
2.3.1. Object delineation
Three possible choices of the surrogate for delineation
are outlined below.
(a) Manual delineation. Object boundaries are traced or
regions are painted manually by experts (see Fig. 1).
Sometimes, it is easier for experts to manually correct the
delineation produced by an algorithm. Corresponding to a
given set SX of scenes for the application domain
XZhT;B;Pi, manual delineation in either of these forms
J.K. Udupa et al. / Computerized Medical Imaging and Graphics 30 75–87
produces a set SX
of scenes representing the fuzzy
segmentations of the same object represented in the scenes
in SX in the following manner. Manual delineation produces
a hard set O for each scene C in SX. Multiple repetitions of
segmentation by multiple operators should be performed.
The fuzzy segmentation CO is produced simply by
averaging the multiple manual delineations. Manual
delineation is inherently binary; that is, it cannot specify
tissue percentages. These binary results are converted into
fuzzy segmentations by using the above strategy. However,
if only binary segmentation is desired, then the averaged
scene can be thresholded at 0.5 to output a binary scene
corresponding to each C in SX. In that case, SX
td contains
binary scenes. Another alternative is to use the method
suggested in wherein an expectation–maximization
algorithm is described for estimating the surrogate of true
delineation produced by a group of experts.
Manual delineation has several shortcomings. First,
when required to be done by expert physicians skilled in
the application domain, it is very costly because of the time
and effort needed in hand segmenting multiple data sets
multiple times. Second, it can be highly variable. For
example, intra- and inter-operator variations of over 20%
have been reported for manual outlining of multiple
sclerosis lesions in brain MRI scenes . Third, the
precision of manual delineations depends on the crispness of
boundaries, the window level settings for image display, the
computer monitor and its settings, and even on the
operator’s vision characteristics . When object
regions/boundaries are fuzzy or very complex, manual
delineation becomes ill-deﬁned. For example, in Fig. 1, it is
difﬁcult to decide what aspect of the edematous region of
the tumor should be included/excluded. Given the various
problems with other surrogates (see below) and given that
manual delineation is an accepted standard surrogate, it
makes sense to examine how we may overcome some of the
drawbacks of manual outlining and still produce a surrogate
that is governed by the underlying precept which has made
this mode of delineation to be accepted as a defacto standard
surrogate. For example, the display characteristics of
display monitors can be standardized , and the scene
intensity scale can be standardized in modalities such
as MRI wherein the intensity scales are arbitrary and do not
have a tissue-speciﬁc numeric meaning. Such strategies
make it possible to standardize window settings for scene
display to minimize the variation in displayed scenes. These
aspects require further work.
(b) Mathematical phantoms. A set of mathematical
phantoms is created to depict the application domain XZ
hT;B;Pi as realistically as possible in terms of image blur,
relative tissue contrast and heterogeneity, noise, and
background inhomogeneity in the scenes (see Fig. 2). The
starting point for this simulation is a set SX
td of binary scenes
(true delineation is known to begin with). Each scene in SX
is gradually corrupted to yield the actual set of scenes SX.
We may also start with gray scenes depicting true fuzzy
segmentations and then follow the same procedure. The
approach here is the reverse of that described above for
Fig. 2. White matter (WM) in a gray matter background, simulated by segmenting WM from real MR scenes and by setting contrast observed in real scenes and
adding blur, noise, background variation to various degree: (a) low, (b) median, and (c) high.
Fig. 1. A slice from an MR FLAIR scene of a patient’s brain. Different window settings (a) and (b) and magniﬁcation factors (c) can cause signiﬁcant variations
in the result of manual delineations, especially for fuzzy objects.
J.K. Udupa et al. / Computerized Medical Imaging and Graphics 30 75–87
manual delineation in the sense that here we start from SX
and then produce SX as compared to producing SX
td starting
from SX in the latter approach. The main shortcoming of
this approach is the questionable authenticity of the
challenges posed by the scenes in SX to segmentation
algorithms.
(c) Simulated scenes. One of the possible strategies in
this approach is to use the method of mathematical
phantoms described above to generate scenes and apply to
both the segmentations and the simulated scenes known 3D
deformations (to capture realistically the variations that
exist among patients) to generate more scenes and their
segmentations. The same method is applicable to the
method of manual segmentation also. The complete set of
scenes (original with deformed) in this case constitutes SX,
and the complete set of segmentations represents SX
main drawback of this approach is that it is difﬁcult to devise
deformations and the associated changes in intensity
characteristics that are realistic.
Another method is to emulate the process of image
acquisition as realistically as possible, starting from
mathematical phantoms which constitute object regions,
and tissue properties/labels assigned to the regions which
constitute actual object properties/labels . This
process consists of three distinct steps: (a) generating the
object geometries; (b) simulating the physical process of
data collection to generate the so called ‘projection data’
based on assumptions regarding the imaging device; (c)
reconstructing images. A weakness of this approach is that
in step (a), it is very difﬁcult to include all objects in a body
region in the mathematical phantom and at sufﬁciently high
resolution. It is also very expensive to generate a sufﬁcient
number of data sets corresponding to different imaged
subjects. Since, all objects cannot be considered with their
realistic properties, realism of the challenges posed for
segmentation cannot be guaranteed in the resulting scenes.
The object geometries generated in step (a) in this approach
constitute SX
td , and the reconstructed images generated in
step (c) constitute SX.
A third method to simulate scenes is to ﬁrst create an
ensemble of ‘cut-outs’ of object regions from actual
acquired scenes and to bury them realistically in different
scenes. Each cutout is segmented carefully by using an
appropriate segmentation method. This should not be
difﬁcult since the cutout contains just the object region
with a background tissue region only and no other
confounding tissue regions. The resulting scenes and the
segmentations constitute SX and SX
td , respectively. See
Fig. 3. A major weakness of this approach is that its
applicability is very limited, perhaps to only small objects
(such as multiple sclerosis lesions) that occur within large
regions of a co-object (such as white matter) in a relatively
independent manner.
In our methodology, SX
td and the corresponding set SX
associated with any of the above methods can be utilized.
We recommend, however, that the averaged fuzzy results
output by the manual delineation method, particularly the
method wherein the output of an algorithm is manually
corrected by human operators (rather than the results of fully
manual delineation) under standardized conditions, be used
as a surrogate of true delineation.
2.3.2. Object recognition
Evaluation strategies that are usually considered for
assessing delineation accuracy are based on treating all
aspects of the region corresponding to the surrogate of true
delineation with equal weight. In the absence of any
prerequisites, this is a correct, and only possible, stand.
However, such approaches do not address the fact that some
areas of the object may be more important than others. An
algorithm may segment an object and match 98% of the true
delineated region. The importance of that 2% difference will
depend on the importance of the regions missed in
delineation. For example, if it is a crucial landmark area,
such as a location of vascularization, or a site of attachment
of a ligament on a bone, then missing or overestimating 2%
of the volume in this region could have important
repercussions for the surgeon or therapist who needs to
know the location of vital nearby anatomic objects. This
example highlights the importance of landmark identiﬁcation and weighting in evaluating an algorithm’s recognition performance. Our approach for ensuring the inclusion
of the information related to certain key features or
landmarks related to the object (the recognition aspect) in
the surrogate used for assessing accuracy of segmentation is
as follows.
(1) Compile a list of features/landmarks (points, curves,
regions) on the object that are observable in the
acquired scene and that are vital for the application
domain XZhT;B;Pi through help from a set of experts
(radiologists, surgeons, anatomists).
(2) Have each expert assign a score to each feature to
indicate its level of importance in the application
(3) Compute an average of the scores. Normalize these to
the range . In this fashion, we generate a feature
Fig. 3. A slice (a) of a scene simulated from an acquired MRI proton density
scene of a multiple sclerosis patient’s brain and its ‘true’ segmentation (b)
of the lesions.
J.K. Udupa et al. / Computerized Medical Imaging and Graphics 30 75–87
vector F whose components have values in 
indicating their level of importance for X.
(4) Have experts delineate the critical area of these features
in scenes in SX repeatedly.
(5) Use the mean location, the spread information about the
location of the features, and the mean vector F to
generate a scene Ci
trÞ for each scene C2SX
and for each feature i. Ci
tr is a location-weighted (for
feature location) and importance-weighted (for i)
representation of feature i. In this scene, a high value
trðcÞ for a voxel c2C indicates that c is both close to
the mean location for a particular feature i and the
importance of the feature is high. These individual
tr may be combined into a composite scene Ctr
by taking an average or a fuzzy union over all i. Fuzzy
union is perhaps more appropriate and this is the
approach we have taken. From this repeated identiﬁcation of landmarks/features, we generate the scenes Ctr
capturing information about truth in recognition for
each scene C2SX. Note that Ctr does not have any
information about object delineation. It contains bright
blobs (of different shapes) only at the location of the
selected features. We denote by SX
tr the set of scenes
containing ‘truth’ in recognition for the set of scenes
2.4. Metrics of segmentation efﬁcacy
2.4.1. Assessment of precision
Two types of subjective actions need to be addressed in
evaluating segmentation precision: (1) patient positioning in
the scanner. (2) Operator input required for segmentation.
be n sets of scenes which represent n
repeat scans, registered and redigitized, of the same subjects
and for the same application domain X. In other words,
we think of SX
to represent repeat scans
corresponding to SX, with SX
1 ZSX. Let H1, H2,.,Hm be
m human operators and let M be a particular segmentation
method. Let CO1 and CO2 be fuzzy segmentations of the
same object O pertaining to the same subject in two
repeated trials. CO1 and CO2 may have resulted from one of
the following situations.
T1: The same operator segments the same object in the
same scene twice by using method M (intra-operator).
T2: Two operators segment the same object in the same
scene once by using method M (inter-operator).
T3: The same operator segments the same object once in
two corresponding scenes in SX
j ðisjÞ by using
method M (inter-scan).
For the given method of segmentation M, all possible
pairs ðCO1;CO2Þ for T1 will allow us to assess intra-operator
precision of M. Analogously, T2 and T3 correspond to the
assessment of inter-operator and repeat-scan (inter-scan)
precision. A measure of precision for method M in a trial
that produced fuzzy segmentations CO1 and CO2 for situation
Ti is given by
TiðOÞ Z jCO1 hCO2j
jCO1 gCO2j :
TiðOÞ represents the total amount of the tissue that is
common to both CO1 and CO2 as a fraction of the total
amount of tissue in the union of CO1 and CO2. PRM
values estimated over the scenes in SX
n utilizing
operators H1, H2,.,Hm characterize the intra-operator,
inter-operator, and repeat-scan (inter-scan) repeatability
(respectively for iZ1,2,3) of method M. The precision of
method M for a given situation (iZ1,2,3) can be
characterized by computing the coefﬁcient of variation or
conﬁdence intervals of the PRM
Ti values. The precision of any
two segmentation methods M1 and M2 for each Ti can be
compared by comparing the set of PRM
Ti values by using a
paired t-test.
Note that just how much the volumes of CO1 and CO2
agree (especially for T1 and T2) will not constitute a robust
measure of precision as illustrated in Fig. 4. This is because
CO1 and CO1 may have identical volumes but may constitute
substantially different delineations. However, situation T3 is
quite different from T1 and T2 in that it involves a
registration and subsequent interpolation. As demonstrated
in , because of the errors associated with the latter
processes, especially when the object has thin and subtle
features (as in peripheral cerebrospinal ﬂuid in the brain),
the overlap measure of Eq. (5) may indicate poor precision
Fig. 4. Segmented objects (muscles) obtained in two different situations Ti and Tj (b) and (c) in a slice of a CT scene of a knee (a). The two segmentations have
nearly identical volumes, still they differ substantially.
J.K. Udupa et al. / Computerized Medical Imaging and Graphics 30 75–87
even when the segmentation is of excellent quality in the
repeat scans. To avoid this problem, we suggest that, for
T3, we must use simply the volume of the object computed
from the segmentation of original repeat scans (without
subjecting repeat acquisitions to registration and then
interpolation). In this case, PRM
T3ðOÞ is deﬁned by
T3ðOÞ Z 1K jjCO1jKjCO2jj
2 jCO1j CjCO2j
A fourth metric PRM
T4 becomes necessary in large clinical
trials wherein different sites (possibly with different brands
of scanners) are utilized for image acquisition. PRM
then constitute inter-site precision, and its treatment should
be similar to that of PRM
T3. In this case, SX
represent l sets of scenes obtained for the same subjects at l
different sites for X. If X involves image acquisition on
only one scanner, then there is no need for assessing PRM
We note that for estimating the precision of any method M,
surrogates of true delineations and of recognition are not
needed. In summary, there are four precision metrics we
utilize: PRM
T3, and PRM
2.4.2. Assessment of accuracy
We consider accuracy measures separately for object
delineation and recognition.
(a) Delineation. Let SX
td be the set of scenes containing
‘true’ delineations for the scenes in SX. For any scene
CZðC;fÞ2SX, let CM
d be the fuzzy segmentation of an
object O of B in C obtained by using any method M, and let
td be the corresponding scene of ‘true’ delineation,
all under the application domain X. Let Ud be a subset of C
such that it constitutes a reference superset with respect to
which all delineated regions (true as well as false) within C
can be expressed as a fraction. Let Ud be the binary scene
representing Ud, that is, a scene with domain C and with a
scene intensity value of one for all voxels in Ud and a value
of 0 for voxels in CKUd. We shall comment on the choice
of Ud later on. The only theoretical requirement on Ud is that
any delineated region within C corresponding to O by any
segmentation
CFN ZCtdKCM
d , CFPZCM
d KCtd, CTPZCM
d hCtd, and
CTN ZUdKCM
d KCtd, where the operations between scenes
are as deﬁned in Eqs. (1)–(3). The following measures are
deﬁned to characterize the delineation accuracy of method
M under X.
True positive volume fraction;
d ðOÞ Z jCTPj
True negative volume fraction;
jUdKCtdj ;
False positive volume fraction;
jUdKCtdj ;
False negative volume fraction;
d ðOÞ Z jCFNj
The meaning of these measures is illustrated in Fig. 5 for
the binary case. TPVFM
d indicates the fraction of the total
amount of tissue in the true delineation Ctd that was covered
by method M. TNVFM
d describes the fraction of the total
amount of tissue in the reference region Ud that is truly not
in the object that was also excluded by method M. FPVFM
denotes the amount of tissue falsely identiﬁed by method M
as a fraction of the amount of tissue in Ud that is truly not in
the object. And FNVFM
d expresses the fraction of tissue in
the true delineation Ctd that was missed by method M. The
following desirable relationships among the above measures
can be easily established from Eqs. (1) and (6)–(9).
Ud Z CTPgCTNgCFPgCFN;
d ðOÞ Z 1KTNVFM
d ðOÞ Z 1KTPVFM
We note that, in view of Eqs. (10)–(12), only two of the
four measures are independent. Consequently, only two
measures (such as FPVFM
d , or TPVFM
d ) need to be speciﬁed to describe the delineation
accuracy of method M. The above measures are borrowed
from statistical decision theory as applied to observer
studies but appropriately modiﬁed to our situation of
segmentation delineation. Continuing along these lines, we
deﬁne delineation sensitivity of method M to be given by
d ðOÞ and delineation speciﬁcity to be described by
d ðOÞ. Clearly, the greater both these entities are,
the better is the delineation accuracy of method M.
Some comments are in order regarding the deﬁnition in
Eqs. (6)–(9) and the choice of Ud. We argue that any
alternative deﬁnition should satisfy Eqs. (10)–(12). Most
Fig. 5. Illustration of the accuracy factors for delineation for a binary case.
Here, Ud is assumed to be a binary scene with all voxels in the scene
domain C set to have a value 1.
J.K. Udupa et al. / Computerized Medical Imaging and Graphics 30 75–87
likely there cannot be alternatives for the numerators in Eqs.
(6)–(9). (Other measures such as those based on the
boundary of O have been used in place of the regionbased measures described above. Our comments here are
applicable to the region-based measures. The boundarybased measures will also have to be, and can be, cast
in terms of TP, TN, FP, and FN for appropriately
characterizing delineation accuracy). However, for the
denominator, other choices seem plausible; for example,
jCTPgCFPgCFNj in Eqs. (6), (8), and (9). Such a choice
has been utilized in the past for TPVF (for example in )
and for TPVF, FPVF, and FNVF . Another
alternative is to use jCtdj in the denominator of all of Eqs.
(6)–(9). All such choices lead to the situation of not
satisfying Eqs. (10)–(12), unlike the deﬁnitions in Eqs. (6)–
(9). The satisfaction of Eqs. (10)–(12) is essential for these
measures to make sense.
Coming now to the choice of Ud (and hence Ud), one
obvious possibility is to take UdZC as in Fig. 5. This has
the undesirable property that TNVFM
depend on the size of the scene domain chosen; by making C
large, these two factors can be changed arbitrarily. Another
possible choice for Ud is the body region B as it manifests in
scene C. In (macroscopic) medical imaging, this usually
corresponds to the foreground region of the scene. In
comparing methods based on the same scene data sets, these
choices may not matter much. These issues obviously
require further research and deliberation. We argue that a
standard evaluation framework (with the ﬁve components
(F1)–(F5) mentioned earlier) is essential to carry out
meaningful and exchangeable segmentation performance
evaluation measures.
Fig. 6 presents an example showing the above four
factors for the application domain of brain parenchymal
volume estimation via MRI T2 and PD scenes and by using
the fuzzy connectedness segmentation method .
(b) Recognition. At present, the existing segmentation
algorithms seem to focus mainly on delineation without
considering in their design the ability to capture salient
feature/landmark information. It is conceivable, however,
that methods can be devised to recognize important
landmarks as part of the segmentation process. In such a
case, we may formulate measures for recognition exactly
along the lines described for delineation. If CM
r is the scene
representing the landmarks recognized by method M, then
r , and FNVFM
can be deﬁned
exactly as in Eqs. (6)–(9) once an appropriate choice is
made for the reference superset Ur for recognition (and for
the corresponding scene Ur). At the current state of affairs,
such algorithms for recognition as part of the segmentation
process do not exist. Therefore, for now we suggest using
the following two measures for recognition, utilizing the
result of delineation CM
d to judge the ability of M to capture
important landmark information.
r ðOÞ Z jCtr†CM
r ðOÞ Z jCtr+ CM
where † and + are as deﬁned in Eqs. (2) and (3). The idea
here is to determine what portion of the spread region of the
landmarks/features is captured by CM
d . The total weight in
this captured region as a fraction of the total weight in Ctr
deﬁnes TPVFM
for characterizing the accuracy of the
qualitative (recognition) aspect of segmentation by method
M. Analogously, FNVFM
r speciﬁes the fraction of the total
weight in Ctr that is missed by method M. Fig. 7 illustrates
these ideas for the application domain of determining the
kinematics of the ankle joint complex via MRI . Here
we focused on the problem of segmenting one of the bones
of the joint, namely the talus ðOÞ. Two experts (BEH, JW)
compiled a set of ﬁve features which included the following:
(i) The superior surface (articular surface) of the body of the
talus. (ii) The inferior surface (posterior articular facet) of
the body of the talus. (iii) The talus head. (iv) The middle
calcaneal articular surface on the inferior surface of the neck
of the talus. (v) A distinct point deﬁning the posterolateral
edge of the sinus tarsi. We have assigned equal weighting
for all features. Subsequently SX
tr was created from the
repeated (three times) delineation provided by them for
these features on a set of ﬁve scenes. Fig. 7a shows a slice of
one of the scenes C in SX, and Fig. 7b shows the appearance
of three of these features (i)–(iii) on the corresponding slice.
Fig. 6. Assessment of accuracy of segmentation. (a) and (b) A slice of a PD and T2 MRI scene of a patient’s brain. (c) The result of fuzzy connectedness
segmentation of brain parenchyma (in 3D). (d) ‘True’ delineation obtained by manual correction of the fuzzy connectedness segmentation. For this example,
FNVFZ1.9%, FPVFZ0.2%, TPVFZ98.1%, TNVFZ99.8%.
J.K. Udupa et al. / Computerized Medical Imaging and Graphics 30 75–87
Fig. 7c displays the corresponding slice of Ctr overlayed on
the corresponding slice of CM
d . M in this case is the live-wire
method .
In summary, there are six accuracy metrics that this
framework will utilize only three of which are independent
measures: FNVFM
2.4.3. Assessment of efﬁciency
We note that for any method M, the precision and
accuracy metrics inﬂuence one another in a complex
manner in the following sense. An attempt to improve
accuracy is usually accompanied by a compromise in
precision and vice versa. As an example, consider M to
represent the method of thresholding based on a ﬁxed
threshold value, as illustrated in Fig. 8, where hT,B,Pi is the
application domain considered in Fig. 6. Obviously PRM
T2 are both one. However, with repeat scan (Fig. 8a
and b) there is much variation in the result (Fig. 8c and d)
T3 becomes 0.702. The ‘true’ delineations for the two
scans of Fig. 8a and b are shown in Fig. 8e and f,
respectively. It is clear that, although this method has high
precision (except for the third factor PRM
T3) and degree of
automation
(efﬁciency),
d Z0:142, and FPVFM
d Z0:10. A possible way of
improving accuracy of M is to modify M by having a human
operator correct the results post-hoc. This will of course
bring down both efﬁciency and precision. What most
segmentation methods strive for is to try to have as few
free parameters as possible and then to juggle among these
three groups of factors (precision, accuracy, and efﬁciency)
in setting up optimal values for the parameters. Some
segmentation methods require other forms of per-scene
human help also, such as for initialization (seed speciﬁcation, initial boundary speciﬁcation) and for any per-scene
algorithm training needed. We denote the total human time
Fig. 7. (a) One slice of the human ankle MRI scene. (b) The corresponding slice of Ctr. (c) The slice of Ctr overlayed on the corresponding slice of CM
d obtained
by using the live-wire method.
Fig. 8. (a) and (b) Two corresponding slices after registration of a pair of repeat scans (with a short time gap in between scans) of a patient’s brain. (c) and (d)
Segmentation of (a) and (b) by ﬁxed thresholding. The object of interest is brain parenchyma. (e) and (f) ‘True’ segmentation of (a) and (b) obtained as for
J.K. Udupa et al. / Computerized Medical Imaging and Graphics 30 75–87
required by M in this manner for each scene by tM
h1. Another
human time component needed, denoted tM
h2, is for any onetime (and not per-scene) algorithm training needed.
In addition to the human help required, and analogous to
the two components tM
h2, there are two components of
computational time required by M, denoted by tM
They represent, respectively, the computation time required
for segmenting each scene and the computation time
required for one-time (and not per-scene) algorithm
training. We note that tM
h1 is the most crucial among these
four time factors.
Efﬁciency of method M refers to its practical viability in
terms of the above four factors. Most methods require some
human help and the claim of ‘totally automatic’ for a
method M in an application domain hT,B,Pi is not valid
unless perfect (or high) precision and accuracy is
demonstrated for M in hT,B,Pi over a large (essentially
inﬁnite) number of scenes. It is clear therefore, that
precision, accuracy, and efﬁciency factors have a complex
interdependency, and the measurement of the efﬁciency
factors is practically highly relevant to distinguish among
methods that have otherwise comparable precision and
accuracy but vastly differing efﬁciency factors, particularly
h2. A sensible way of combining the efﬁciency
factors is via the dollar cost incurred. Assuming a trained
technician to be a standard human operator H, H’s salary g
will determine the cost per time unit, and, hence, the weight
to be given to tM
h2. Similarly, the cost (l) per unit
computer time will determine the weight to be given to tM
c2. Obviously g and l can be determined and ﬁxed
within an evaluation framework, although they will have to
be updated on a regular basis. For a given X, the overall
c2Þ, where g is a function that converts
time factors into dollar cost for the total cost incurred in
segmenting each scene in the set of scenes SX by utilizing g
and l. In summary, this framework will utilize ﬁve
efﬁciency metrics: tM
c2, and EM. For comparing
methods (Section 3), either the efﬁciency factor or directly
the time factors can be utilized.
3. How to compare methods
The procedure for comparing two methods M1 and M2
under a given hT,B,Pi consists of the following steps.
(1) Collect sets of scenes SX
n corresponding to n
repeat scans of the scenes in SX acquired for hT,B,Pi.
Produce scenes SX
tr representing surrogate of
true delineation and of recognition for the scenes in SX.
(2) Optimize the implementations of M1 and M2 for hT,B,Pi.
For methods M1 and M2, have operators H1,H2,.,Hm
repeat segmentations of scenes in SX. Have one
operator segment scenes in SX
n for methods
M1 and M2.
(3) For iZ1,2,3,4, determine all possible values of PRM1
(4) Knowing SX
tr and the segmentations of the
scenes in SX produced by M1 and M2, compute
r , for jZ1, 2.
(5) Record t
h2 for jZ1, 2 during the segmentation experiments, and from these compute the efﬁciency
metric EMj.
(6) For each method Mj, we get a set of values for each of
the 15 parameters: PR
c2 , and EMj.
Considering only two independent accuracy parameters
(or one (EMj)) among the
efﬁciency parameters, we get a set of 12 (or 9) parameters
altogether. There are several choices for the statistical
analysis of these 12 (or 9) sets of values.
(a) Do a paired t-test of the two sets of values for each
parameter for the two methods.
(b) Combine the 12 (or 9) parameters for each method Mj
by a weighted sum, the weight reﬂecting the
importance given to that parameter for hT,B,Pi and
then do a paired t-test of the resulting single parameter.
(c) Do a multivariate analysis of variance considering
all 12 (or 9) parameters to determine if there is a
statistically signiﬁcant difference in performance
between methods M1 and M2.
As an example, we display in Table 1 8 of the 12
parameters for the application domain illustrated in Fig. 7.
The two methods compared are M1Zfuzzy connectedness
(FC) , and M2Zlive-wire (LW) which is a usersteered boundary segmentation method and the steering was
provided by a non-expert in the application domain (YZ).
SX consisted of a set of 20 MRI scenes. Because of the
connective tissues (ligaments and tendons) and because
Some of the metrics computed from 20 scenes for the two methods FC and LW
0.99G0.015
0.13G0.043
0.04G0.018
0.78G0.035
0.96G0.011
0.03G0.010
0.03G0.017
0.94G0.018
Comparison
J.K. Udupa et al. / Computerized Medical Imaging and Graphics 30 75–87
cortical bone yields very little MR signal, this is a difﬁcult
segmentation problem. SX
td has been previously created for
this application domain by well trained students of podiatric
medicine via LW and subsequently scrutinized (and
corrected if necessary) by an expert (BEH). The two
numbers in the table represent the mean and the standard
deviation of the metric over SX. The p-value for a paired
t-test on each of the metrics comparing the two methods is
also listed when the difference is statistically signiﬁcant. We
note from the table that, although FC is more efﬁcient from
the consideration of operator help needed ðtM
h1Þ and more
precise ðPRM
T1Þ, it is less accurate (in both delineation and
recognition) and requires more computational time than LW
for this application domain (‘O’ indicates better than). (For
LW, the live-wire segments are computed and displayed in
real time , and therefore, tM
c1 is negligible). Clearly, the
preference for the methods in an application domain
depends very much on which of these metrics is crucial
for that domain.
4. Concluding remarks
(1) If the surrogates of truth are highly reliable (gold
standard), then it may appear that there is no need to
evaluate precision, and accuracy analysis would be
sufﬁcient. However, accuracy analysis will then have
to consider intra-operator, inter-operator, repeat-scan,
and inter-site variations. We feel that it is best to
relegate the analysis of these variations to a separate
group, namely precision. Therefore, the factors
describing precision, accuracy, and efﬁciency are all
essential in assessing the performance of segmentation
(2) A descriptive answer in terms of the various
parameters gives a more meaningful and complete
assessment of the methods than an answer to the
overall question ‘Is method M1 better than M2 under
the application domain?’
(3) Since, most segmentation methods in practice consider only delineation, we suggest that, at a minimum,
the following set of seven parameters should be
evaluated: PRM
for any given method M.
(4) General statements about the merit of segmentation
algorithms cannot be made independent of the
application domain hT,B,Pi. The evaluative results of
two methods M1 and M2 observed under one hT,B,Pi
may not foretell anything about their comparative
behavior for a different hT,B,Pi.
(5) We have proposed a method to incorporate into the
evaluation method the aspect of how well key features
(landmarks) of an object that are considered important
for hT,B,Pi are captured in the segmentation. We are able
to include this qualitative aspect of recognition also
within the same common framework of evaluation.
(6) The four components of efﬁciency are essential, tM
being the most crucial among these. There is no such
thing as ‘an automatic segmentation method’. Any
method may fail (for example, it may produce high FN
and/or FPVFM
for a particular data set) if a
sufﬁciently large set of scenes is processed, and then it
will need human intervention. ‘Automatic’ is only a
design intent and not necessarily the end result for a
segmentation method. Therefore, the phrase has no
meaning, even for a particular application domain,
unless the method’s efﬁciency is proven to be 100% (for
all four factors) over a large (essentially inﬁnite) number
ofdatasetswithacceptableprecisionandaccuracyinthe
application domain.
(7) The factors describing precision, accuracy, and efﬁciency are interdependent. To simultaneously improve
all three factors for a method is usually difﬁcult and
requires considerable research. An attempt to increase
accuracy may be accompanied by a decrease in
efﬁciency and/or precision.
(8) Once the surrogates are determined, the framework can
be easily implemented and utilized to evaluate any
image segmentation methods.
(9) A framework with the ﬁve components (F1)–(F5)
mentioned in Section 1 becomes essential to carry out
meaningful, exchangeable, and widely accepted segmentation performance evaluation. Further work is
needed in all of these components. Further research is
also needed in the deﬁnition of accuracy measures. We
have focused on region-based measures in this paper.
Boundary-based or other (even hybrid) strategies may
be more relevant in certain application domains. (For
example, when the object shape is such that its surface
areato volumeratio ishigh—that is, whenthe numberof
voxels in the boundary approaches the number of voxels
constituting the region occupied by the object—small
changes in segmentation would yield large changes in
the precision and accuracy measures. For such objects,
perhaps boundary-based measures are more appropriate). Their deﬁnition satisfying conditions similar to
those in Eqs. (10)–(12) requires further work. Most
segmentation algorithms behave like human observers
in that their performance cannot be completely
characterized by measures derived at one operating
point, which is decided by the values assigned to the
parameters of the method. There is even no systematic
approach available for setting the values of the
parameters of segmentation methods optimally.
Methods akin to ROC analysis are needed to more
completely characterize the range of behavior of the
accuracy of segmentation methods.
(10) Some comments are in order regarding the terms
‘evaluation’ vis-a-vis ‘validation’. It is wrong to call
the process ofevaluating a segmentationmethodM inan
application domain X a ‘validation process’. Since, the
level of performance of M in X is unknown, a neutral
J.K. Udupa et al. / Computerized Medical Imaging and Graphics 30 75–87
termlike‘evaluation’ismoreappropriatetodescribethe
process. Only when the evaluation is completed and if
the level of performance of M in X becomes acceptable,
itisappropriate todescribeM asbeing validatedinX. In
the phrase ‘validation process’, there is a hint of
presumption and wishful thinking. We, therefore,
suggest that evaluation be used to describe this process.
Acknowledgements
The research reported here is supported by DHHS grants
NS 37172 and AR46902. The authors are grateful to Gul
Moonis for Fig. 1, Punam Saha for Figs. 2 and 3, and to
Laszlo Nyu´l for Figs. 6 and 8.