Dropout improves Recurrent Neural Networks for
Handwriting Recognition
Vu Pham∗†, Th´eodore Bluche∗‡, Christopher Kermorvant∗, and J´erˆome Louradour∗
∗A2iA, 39 rue de la Bienfaisance, 75008 - Paris - France
† SUTD, 20 Dover Drive, Singapore
‡LIMSI CNRS, Spoken Language Processing Group, Orsay, France
Abstract—Recurrent neural networks (RNNs) with Long
Short-Term memory cells currently hold the best known results
in unconstrained handwriting recognition. We show that their
performance can be greatly improved using dropout - a recently
proposed regularization method for deep architectures. While
previous works showed that dropout gave superior performance
in the context of convolutional networks, it had never been applied
to RNNs. In our approach, dropout is carefully used in the
network so that it does not affect the recurrent connections, hence
the power of RNNs in modeling sequences is preserved. Extensive
experiments on a broad range of handwritten databases conﬁrm
the effectiveness of dropout on deep architectures even when the
network mainly consists of recurrent and shared connections.
Keywords—Recurrent Neural Networks, Dropout, Handwriting
Recognition
INTRODUCTION
Unconstrained ofﬂine handwriting recognition is the problem of recognizing long sequences of text when only an
image of the text is available. The only constraint in such a
setting is that the text is written in a given language. Usually
a pre-processing module is used to extract image snippets,
each contains one single word or line, which are then fed
into the recognizer. A handwriting recognizer, therefore, is
in charge of recognizing one single line of text at a time.
Generally, such a recognizer should be able to detect the
correlation between characters in the sequence, so it has more
information about the local context and presumably provides
better performance. Readers are referred to for an extensive
review of handwriting recognition systems.
Early works typically use a Hidden Markov Model (HMM)
 or an HMM-neural network hybrid system , for
the recognizer. However, the hidden states of HMMs follow
a ﬁrst-order Markov chain, hence they cannot handle longterm dependencies in sequences. Moreover, at each time step,
HMMs can only select one hidden state, hence an HMM
with n hidden states can typically carry only log (n) bits of
information about its dynamics .
Recurrent neural networks (RNNs) do not have such limitations and were shown to be very effective in sequence
modeling. With their recurrent connections, RNNs can, in
principle, store representations of past input events in form
of activations, allowing them to model long sequences with
complex structures. RNNs are inherently deep in time and can
have many layers, both make training parameters a difﬁcult
optimization problem. The burden of exploding and vanishing
gradient was the reason for the lack of practical applications
of RNNs until recently , .
Lately, an advance in designing RNNs was proposed,
namely Long Short-Term Memory (LSTM) cells. LSTM are
carefully designed recurrent neurons which gave superior performance in a wide range of sequence modeling problems. In
fact, RNNs enhanced by LSTM cells won several important
contests , , and currently hold the best known
results in handwriting recognition.
Meanwhile, in the emerging deep learning movement,
dropout was used to effectively prevent deep neural networks
with lots of parameters from overﬁtting. It is shown to be
effective with deep convolutional networks , , ,
feed-forward networks , , but, to the best of
our knowledge, has never been applied to RNNs. Moreover,
dropout was typically applied only at fully-connected layers
 , , even in convolutional networks . In this work,
we show that dropout can also be used in RNNs at some certain
layers which are not necessarily fully-connected. The choice of
applying dropout is carefully made so that it does not affect the
recurrent connections, therefore without reducing the ability of
RNNs to model long sequences.
Due to the impressive performance of dropout, some extensions of this technique were proposed, including DropConnect , Maxout networks , and an approximate approach
for fast training with dropout . In , a theoretical
generalization bound of dropout was also derived. In this work,
we only consider the original idea of dropout .
Section II presents the RNN architecture designed for
handwriting recognition. Dropout is then adapted for this
architecture as described in Section III. Experimental results
are given and analyzed in Section IV, while the last section is
dedicated for conclusions.
RECURRENT NEURAL NETWORKS FOR HANDWRITING
RECOGNITION
The recognition system considered in this work is depicted
in Fig. 1. The input image is divided into blocks of size
2 × 2 and fed into four LSTM layers which scan the input
in different directions indicated by corresponding arrows. The
output of each LSTM layer is separately fed into convolutional
layers of 6 features with ﬁlter size 2 × 4. This convolutional
layer is applied without overlaping nor biases. It can be
seen as a subsampling step, with trainable weights rather
than a deterministic subsampling function. The activations of
4 convolutional layers are then summed element-wise and
squashed by the hyperbolic tangent (tanh) function. This
process is repeated twice with different ﬁlter sizes and numbers
 
Input image
Block: 2 x 2
Features: 2
Convolutional
Input: 2x4
Stride: 2x4
Features: 6
Sum & Tanh
Features: 10
Convolutional
Input: 2x4
Stride: 2x4
Features: 20
Sum & Tanh
Features: 50
Fully-connected
Features: N
N-way softmax
The Recurrent Neural Network considered in this paper, with the places where dropout can be applied.
of features, and the top-most layer is fully-connected instead of
convolutional. The ﬁnal activations are summed vertically and
fed into the softmax layer. The output of softmax is processed
by Connectionist Temporal Classiﬁcation (CTC) .
This architecture was proposed in , but we have adapted
the ﬁlter sizes for input images at 300 dpi. There are two
key components enabling this architecture to give superior
performance:
Multidirectional LSTM layers . LSTM cells are
carefully designed recurrent neurons with multiplicative gates to store information over long periods and
forget when needed. Four LSTM layers are applied in
parallel, each one with a particular scaning direction.
In this way the network has the possibility to exploit
all available context.
CTC is an elegant approach for computing the Negative Log-likelihood for sequences, so the whole architecture is trainable without having to explicitly
align each input image with the corresponding target
In fact, this architecture was featured in our winning entry
of the Arabic handwriting recognition competition OpenHaRT
2013 , where such a RNN was used as the optical model in
the recognition system. In this paper, we further improve the
performance of this optical model using dropout as described
in the next section.
DROPOUT FOR RECURRENT NEURAL NETWORKS
Originally proposed in , dropout involves randomly
removing some hidden units in a neural network during
training but keeping all of them during testing. More formally,
consider a layer with d units and let h be a d-dimensional
vector of their activations. When dropout with probability p
is applied at this layer, some activations in h are dropped:
htrain = m ⊙h, where ⊙is the element-wise product, and m
is a binary mask vector of size d with each element drawn
independently from mj ∼Bernoulli (p). During testing, all
units are retained but their activations are weighted by p:
htest = ph. Dropout involves a hyper-parameter p, for which
a common value is p = 0.5.
Dropout is only applied to feed-forward connections in RNNs. The
recurrent connections are kept untouched. This depicts one recurrent layer
(hi) with its inputs (xi), and an output layer (yi) which can comprise full or
shared connections. The network is unrolled in 3 time steps to clearly show
the recurrent connections.
We believe that random dropout should not affect the
recurrent connections in order to conserve the ability of RNNs
to model sequences. This idea is illustrated in Fig. 2, where
dropout is applied only to feed-forward connections and not
to recurrent connections. With this construction, dropout can
be seen as a way to combine high-level features learned
by recurrent layers. Practically, we implemeted dropout as a
separated layer whose output is identical to its input, except
at dropped locations (mj = 0). With this implementation,
dropout can be used at any stage in a deep architecture,
providing more ﬂexibility in designing the network.
Another appealing method similar to dropout is DropConnect , which drops the connections, instead of the hidden
units values. However DropConnect was designed for fullyconnected layers, where it makes sense to drop the entries
of the weight matrix. In convolutional layers, however, the
weights are shared, so there are only a few actual weights.
If DropConnect is applied at a convolutional layer with k
weights, it can sample at most 2k different models during training. In contrast, our approach drops the input of convolutional
layers. Since the number of inputs is typically much greater
than the number of weights in convolutional layers, dropout
in our approach samples from a bigger pool of models, and
presumably gives superior performance.
In , dropout is used to regularize a bi-directional RNN,
but the network has only one hidden layer, there are no
LSTM cells involved, and there is no detail on how to apply
dropout to the RNN. In , dropout is used in a convolutional
neural network but with a smaller dropout rate because the
typical value p = 0.5 might slow down the convergence and
lead to higher error rate. In this paper, our architecture has
both covolutional layers and recurrent layers. The network is
signiﬁcantly deep, and we still ﬁnd the typical dropout rate
p = 0.5 yielding superior performance. This improvement
can be attributed to the way we keep recurrent connections
untouched when applying dropout.
Note that previous works about dropout seem to favor
rectiﬁed linear units (ReLU) over tanh or sigmoid for
the network nonlinearity since it provides better covergence
rate. In our experiments, however, we ﬁnd out that ReLU can
not give good performance in LSTM cells, hence we keep tanh
for the LSTM cells and sigmoid for the gates.
EXPERIMENTS
A. Experimental setup
Three handwriting datasets are used to evaluate our system: Rimes , IAM and OpenHaRT containing
handwritten French, English and Arabic text, respectively. We
split the databases into disjoint subsets to train, validate and
evaluate our models. The size of the selected datasets are
given in Table I. All the images used in these experiments
consist of either isolated words (Section IV-B) or isolated
lines (Section IV-C). They are all scanned at (or scaled to)
300 dpi, and we recall that the network architecture presented
in section II is designed to ﬁt with this resolution.
THE NUMBER OF ISOLATED WORDS AND LINES IN THE
DATASETS USED IN THIS WORK.
Validation
Evaluation
1 For OpenHaRT, only a subset of the full available data was used in the experiments
on isolated word.
To assess the performance of our system, we measure the
Character Error Rate (CER) and Word Error Rate (WER).
The CER is computed by normalizing the total edit distance
between every pair of target and recognized sequences of
characters (including the white spaces for line recognition).
The WER is simply the classiﬁcation error rate in the case of
isolated word recognition, and is a normalized edit distance
between sequences of words in the case of line recognition.
The RNN optical models are trained by online stochastic
gradient descent with a ﬁxed learning rate of 10−3. The objective function is the Negative Log-Likelihood (NLL) computed
by CTC. All the weights are initialized by sampling from a
Gaussian distribution with zero mean and a standard deviation
of 10−2. A simple early stopping strategy is employed and no
other regularization methods than dropout were used. When
dropout is enabled, we always use the dropout probability
B. Isolated Word Recognition
1) Dropout at the topmost LSTM layer: In this set of
experiments, we ﬁrst apply dropout at the topmost LSTM layer.
Since there are 50 features at this layer, dropout can sample
from a great number of networks. Moreover, since the inputs
of this layer have smaller sizes than those of lower layers due
to subsampling, dropout at this layer will not take too much
time during training.
Previous work suggests that dropout is most helpful
when the size of the model is relatively big, and the network
suffers from overﬁtting. One way to control the size of the
network is to change the number of hidden features in the
recurrent layers. While the baseline architecture has 50 features
at the topmost layer, we vary it among 30, 50, 100 and 200.
All other parameters are kept ﬁxed, the network is then trained
with and without dropout.
For each setting and dataset, the model with highest
performance on validation set is selected and evaluated on
corresponding test set. The results are given in Table II.
It can be seen that dropout works very well on IAM and
Rimes where it signiﬁcantly improves the performance by
10 −20% regardless of the number of topmost hidden units.
On OpenHaRT, dropout also helps with 50, 100 or 200 units,
but hurts the performance with 30 units, most likely because
the model with 30 units is underﬁtted.
Fig. 3 depicts the convergence curves of various RNN
architectures trained on the three datasets when dropout is disabled or enabled. In all experiments, convergence curves show
that dropout is very effective in preventing overﬁtting. When
dropout is disabled, the RNNs clearly suffer from overﬁtting
as their NLL on the validation dataset increases after a certain
number of iterations. When dropout is enabled, the networks
are better regularized and can achieve higher performance on
validation set at the end. Especially for OpenHaRT, since its
training and validation sets are much larger than IAM and
Rimes, 30 hidden units are inadequate and training takes a
long time to converge. With 200 units and no dropout, it seems
to be overﬁtted. However when dropout is enabled, 200 units
give very good performance.
2) Dropout at multiple layers: Now we explore the possibilities of using dropout also at other layers than the topmost
LSTM layer. In our architecture, there are 3 LSTM layers,
hence we tried applying dropout at the topmost, the top two
and all the three LSTM layers.
Normally when dropout is applied at any layer, we double
the number of LSTM units at that layer. This is to keep the
same number of active hidden units (on average) when using
dropout with p = 0.5 as in the baseline where all hidden units
are active. We remind that the baseline architecture consists of
LSTM layers with 2, 10 and 50 units, so it would correspond to
an architecture of 4, 20 and 100 units when dropout is applied
at every layer. Since most of free parameters of the networks
concentrate at the top layers, doubling the last LSTM layer
almost doubles the number of free parameters. Therefore we
also have several experiments where we keep the last LSTM
layer at 50 units with dropout. Besides, in order to avoid
favouring the models trained with dropout because they have
greater capacity, we also test those big architectures without
Their performance are reported in Table III. Since we double the size of LSTM layers, the modeling power of the RNNs
is increased. Without dropout, the RNNs with more features at
EVALUATION RESULTS OF WORD RECOGNITION, WITH AND
WITHOUT DROPOUT AT THE TOPMOST LSTM HIDDEN LAYER
LSTM cells
Bold numbers indicate the best results obtained for a given database and a given
conﬁguration.
TABLE III.
EVALUATION RESULTS OF WORD RECOGNITION, WITH
DROPOUT AT MULTIPLE LAYERS
# LSTM layers
with dropout
2, 10, 100
2, 20, 100
(no dropout)
4, 20, 100
1 (topmost)
2, 10, 100
2, 20, 100
4, 20, 100
lower layers generally obtain higher performance. However we
observed overﬁtting on Rimes when we use 4 and 20 features
at the lowest LSTM layers. This makes sense because Rimes
is the smallest of the three datasets. With dropout, CER and
WER decrease by almost 30-40% on a relative basis. We found
that dropout at 3 LSTM layers is generally helpful, however
the training time is signiﬁcantly longer both in term of the
number of epochs before convergence and the CPU time for
each epoch.
C. Line Recognition with Lexical Constraints and Language
Note that the results presented in Table III can not be directly compared to state-of-the-art results previously published
on the same databases , , since the RNNs only output
unconstrained sequences of characters. A complete system
for large vocabulary handwriting text recognition includes a
lexicon and a language model, which greatly decrease the
error rate by inducing lexical constraints and rescoring the
hypotheses produced by the optical model.
In order to compare our approach to existing results, we
trained again the best RNNs for each database, with and
without dropout, on lines of text. The whitespaces in the
annotations are also considered as targets for training.
Concretely, we build a hybrid HMM/RNN model. There is
a one-state HMM for each label (character, whitespace, and the
blank symbol of CTC ), which has a transition to itself and
an outgoing transition with the same probability. The emission
probabilities are obtained by transforming the posterior probabilities given by the RNNs into pseudo-likelihood. Speciﬁcally,
the posteriors p(s|x) are divided by the priors p(s), scaled by
some factor κ : p(s|x)
p(s)κ , where s is the HMM state, i.e. a
character, a blank, or a whitespace, and x is the input. The
priors p(s) are estimated on the training set.
We include the lexical contraints (vocabulary and language
model) in the decoding phase as a Finite-State Transducer
(FST), which is the decoding graph in which we inject
the RNN predictions. The method to create an FST that is
compatible with the RNN outputs is described in . The
whitespaces are treated as an optional word separator in the
lexicon. The HMM is also represented as an FST H and is
composed with the lexicon FST L, and the language model G.
The ﬁnal graph HLG is the decoding graph in which we
search the best sequence of words ˆ
W = arg max
W [ω log p(X|W) + log p(W) + |W| log WIP]
where X is the image, p(X|W) are the pseudo-likelihoods,
p(W) is given by the language model, ω and WIP are
the optical scaling factor – balancing the importance of the
optical model and the language model – and the word insertion
penalty. These parameters, along with the prior scaling factor
κ, have been tuned independently for each database on its
validation set.
For IAM, we applied a 3-gram language model trained
on the LOB, Brown and Wellington corpora. The passages of
the LOB corpus appearing in the validation and evaluation sets
were removed prior to LM training. We limited the vocabulary
to the 50k most frequent words. The resulting model has a
perplexity of 298 and OOV rate of 4.3% on the validation set
(329 and 3.7% on the evaluation set).
For Rimes, we used a vocabulary made of 12k words from
the training set. We built a 4-gram language model with modiﬁed Kneser-Ney discounting from the training annotations.
The language model has a perplexity of 18 and OOV rate of
2.6% on the evaluation set.
For OpenHaRT, we selected a 95k words vocabulary containing all the words of the training set. We trained a 3gram language model on the training set annotations, with
interpolated Kneser-Ney smoothing. The language model has
a perplexity of 1162 and OOV rate of 6.8% on the evaluation
The results are presented in Tables IV (Rimes), V (IAM)
and VI (OpenHaRT). On the ﬁrst two rows, we present the
error rates of the RNNs alone, without any lexical constraint.
It can be seen that dropout gives from 7 to 27% relative
improvement. The third rows present the error rates when
adding lexical constraints without dropout. In this case, only
valid sequences of characters are outputed, and the relative improvement in CER over the systems without lexical constraints
is more than 40%. On the 4th row, when dropout and lexical
constraints are both enabled, dropout achieves 5.7% (Rimes),
19.0% (IAM) and 4.1% (OpenHaRT) relative improvement in
CER, and 2.4% (Rimes), 14.5% (IAM) and 3.2% (OpenHaRT)
relative improvement in WER. Using a single model and closed
vocabulary, our systems outperform the best published results
for all databases. Note that on the 5th line of Table V, the
system presented in adopts an open-vocabulary approach
RESULTS ON RIMES
MDLSTM-RNN
+ Vocab&LM
Messina et al. 
Kozielski et al. 
Messina et al. 
Menasri et al. 
RESULTS ON IAM
MDLSTM-RNN
+ Vocab&LM
Kozielski et al. 
Kozielski et al. 
Espana et al. 
Graves et al. 
Bertolami et al. 
Dreuw et al. 
RESULTS ON OPENHART
* MDLSTM-RNN
* + dropout
+ Vocab&LM
Bluche et al. 
Bluche et al. 
Kozielski et al. 
* The error rates in the ﬁrst 2 lines are computed
from the decomposition into presentation forms
and are not directly comparable to the remaining of
the table.
TABLE VII.
NORM OF THE WEIGHTS, FOR DIFFERENTLY TRAINED
Baseline Dropout Baseline Dropout Baseline Dropout
The ﬁrst 2 lines correspond to weights in the topmost LSTM layer (before
dropout, if any) and the last 2 lines correspond to classiﬁcation weights in topmost
linear layer (after dropout, if any).
Nb updates / 1M
nb topmost hidden units:
50 (valid)
50 (train)
50 with dropout
100 with dropout
200 with dropout
Convergence Curves on OpenHaRT. Plain (resp. dashed) curves show
the costs on the validation (resp. training) dataset.
and can recognize out-of-vocabulary words, which can not be
directly compared to our models.
D. Effects of dropout on the Recurrent Neural Networks
In order to better understand the behaviour of dropout in
training RNNs, we analyzed the distribution of the network
weights and the intermediate activations. Table VII shows the
L1 and L2 norm of the weights of LSTM gates and cells in
the topmost LSTM layer (referred to as ”LSTM weights”),
and the weights between the topmost LSTM layer and the
softmax layer (”Classiﬁcation weights”). It is noticeable that
the classiﬁcation weights are smaller when dropout is enabled.
We did not use any other regularization method, but dropout
seems to have similar regularization effects as L1 or L2 weight
decay. The nice difference is that the hyper-parameter p of
dropout is much less tricky to tune than those of weight decay.
On the other hand, the LSTM weights tend to be higher
with dropout, and further analysis of the intermediate activations shows that the distribution of LSTM activations have
a wider spread. This side effect can be partly explained
by the hypothesis that dropout encourages the units to emit
stronger activations. Since some units were randomly dropped
during training, stronger activations might make the units
more independently helpful, given the complex contexts of
other hidden activations. Furthermore, we checked that the
LSTM activations are not saturated under the effect of dropout.
Keeping unsaturated activations is particularly important when
training RNN, since it ensures that the error gradient can be
propagated to learn long-term dependencies.
The regularization effect of dropout is certain when we
look into the learning curves given in Fig. 3, where it shows
how overﬁtting can be greatly reduced. The gain of dropout
becomes highly signiﬁcant when the network gets relatively
bigger with respect to the dataset.
CONCLUSION
We presented how dropout can work with both recurrent
and convolutional layers in a deep network architecture. The
word recognition networks with dropout at the topmost layer
signiﬁcantly reduces the CER and WER by 10-20%, and the
performance can be further improved by 30-40% if dropout
is applied at multiple LSTM layers. The experiments on
complete line recognition also showed that dropout always
improved the error rates, whether the RNNs were used in
isolation, or constrained by a lexicon and a language model.
We report the best known results on Rimes and OpenHaRT
databases. Extensive experiments also provide evidence that
dropout behaves similarly to weight decay, but the dropout
hyper-parameter is much easier to tune than those of weight
decay. It should be noted that although our experiments were
conducted on handwritten datasets, the described technique is
not limited to handwriting recognition, it can be applied as
well in any application of RNNs.
ACKNOWLEDGEMENT
This work was partially funded by the French Grand
Emprunt-Investissements
PACTE project, and was partly achieved as part of the Quaero
Program, funded by OSEO, French State agency for innovation.