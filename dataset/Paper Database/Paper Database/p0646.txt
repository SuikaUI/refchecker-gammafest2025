Large Scale Fine-Grained Categorization and Domain-Speciﬁc Transfer
Yin Cui1,2∗
Yang Song3
Andrew Howard3
Serge Belongie1,2
1Department of Computer Science, Cornell University
2Cornell Tech
3Google Research
Transferring the knowledge learned from large scale
datasets (e.g., ImageNet) via ﬁne-tuning offers an effective
solution for domain-speciﬁc ﬁne-grained visual categorization (FGVC) tasks (e.g., recognizing bird species or car
make & model). In such scenarios, data annotation often
calls for specialized domain knowledge and thus is difﬁcult
to scale. In this work, we ﬁrst tackle a problem in large scale
FGVC. Our method won ﬁrst place in iNaturalist 2017 large
scale species classiﬁcation challenge. Central to the success of our approach is a training scheme that uses higher
image resolution and deals with the long-tailed distribution of training data. Next, we study transfer learning via
ﬁne-tuning from large scale datasets to small scale, domainspeciﬁc FGVC datasets. We propose a measure to estimate
domain similarity via Earth Mover’s Distance and demonstrate that transfer learning beneﬁts from pre-training on a
source domain that is similar to the target domain by this
measure. Our proposed transfer learning outperforms ImageNet pre-training and obtains state-of-the-art results on
multiple commonly used FGVC datasets.
1. Introduction
Fine-grained visual categorization (FGVC) aims to distinguish subordinate visual categories. Examples include
recognizing natural categories such as species of birds , dogs and plants ; or man-made categories
such as car make & model . A successful FGVC
model should be able to discriminate categories with subtle
differences, which presents formidable challenges for the
model design yet also provides insights to a wide range of
applications such as rich image captioning , image generation , and machine teaching .
Recent advances on Convolutional Neural Networks
(CNNs) for visual recognition have fueled remarkable progress on FGVC . In general, to achieve reasonably good performance with CNNs,
∗Work done during internship at Google Research.
Figure 1. Overview of the proposed transfer learning scheme.
Given the target domain of interest, we pre-train a CNN on the
selected subset from the source domain based on the proposed domain similarity measure, and then ﬁne-tune on the target domain.
one needs to train networks with vast amounts of supervised
data. However, collecting a labeled ﬁne-grained dataset often requires expert-level domain knowledge and therefore
is difﬁcult to scale. As a result, commonly used FGVC
datasets are relatively small, typically containing around 10k of labeled training images. In such a scenario, ﬁne-tuning the networks that are pre-trained on large
scale datasets such as ImageNet is often adopted.
This common setup poses two questions: 1) What are
the important factors to achieve good performance on large
scale FGVC? Although other large scale generic visual
datasets like ImageNet contain some ﬁne-grained categories, their images are usually iconic web images that
contain objects in the center with similar scale and simple
backgrounds. With the limited availability of large scale
FGVC datasets, how to design models that perform well
on large scale non-iconic images with ﬁne-grained categories remains an underdeveloped area. 2) How does one
effectively conduct transfer learning, by ﬁrst training the
network on a large scale dataset and then ﬁne-tuning it
on domain-speciﬁc ﬁne-grained datasets? Modern FGVC
methods overwhelmingly use ImageNet pre-trained networks for ﬁne-tuning. Given the fact that the target ﬁnegrained domain is known, can we do better than ImageNet?
This paper aims to answer the two aforementioned problems, with the recently introduced iNaturalist 2017 large
scale ﬁne-grained dataset (iNat) . iNat contains 675,170
 
training and validation images from 5,089 ﬁne-grained categories. All images were captured in natural conditions with
varied object scales and backgrounds. Therefore, iNat offers a great opportunity to investigate key factors behind
training CNNs that perform well on large scale FGVC. In
addition, along with ImageNet, iNat enables us to study
the transfer of knowledge learned on large scale datasets
to small scale ﬁne-grained domains.
In this work, we ﬁrst propose a training scheme for
large scale ﬁne-grained categorization, achieving top performance on iNat. Unlike ImageNet, images in iNat have
much higher resolutions and a wide range of object scales.
We show in Sec. 3.1 that performance on iNat can be improved signiﬁcantly with higher input image resolution.
Another issue we address in this paper is the long-tailed
distribution, where a few categories have most of the images . To deal with this, we present a simple yet
effective approach. The idea is to learn good features from
a large amount of training data and then ﬁne-tune on a
more evenly-distributed subset to balance the network’s efforts among all categories and transfer the learned features.
Our experimental results, shown in Sec. 3.2, reveal that we
can greatly improve the under-represented categories and
achieve better overall performance.
Secondly, we study how to transfer from knowledge
learned on large scale datasets to small scale ﬁne-grained
domains. Datasets are often biased in terms of their statistics on content and style . On CUB200 Birds , iNat
pre-trained networks perform much better than ImageNet
pre-trained ones; whereas on Stanford-Dogs , ImageNet
pre-trained networks yield better performance. This is because there are more visually similar bird categories in iNat
and dog categories in ImageNet. In light of this, we propose a novel way to measure the visual similarity between
source and target domains based on image-level visual similarity with Earth Mover’s Distance. By ﬁne-tuning the networks trained on selected subsets based on our proposed
domain similarity, we achieve better transfer learning than
ImageNet pre-training and state-of-the-art results on commonly used ﬁne-grained datasets. Fig. 1 gives an overview
of the proposed training scheme.
We believe our study on large scale FGVC and domainspeciﬁc transfer learning could offer useful guidelines for
researchers working on similar problems.
2. Related Work
Fine-Grained Visual Categorization (FGVC). Recent
FGVC methods typically incorporate useful ﬁne-grained
information into a CNN and train the network end-toend. Notably, second order bilinear feature interactions was
shown to be very effective . This idea was later extended to compact bilinear pooling , and then higher
order interactions .
To capture subtle visual
differences, visual attention and deep metric
learning are often used. Beyond pixels, we also
leverage other information including parts , attributes , human interactions and text descriptions . To deal with the lack of training data in
FGVC, additional web images can be collected to augment
the original dataset . Our approach differs
from them by transferring the pre-trained network on existing large scale datasets without collecting new data.
Using high-resolution images for FGVC has became increasingly popular . There is also a similar trend
in ImageNet visual recognition, from originally 224 × 224
in AlexNet to 331 × 331 in recently proposed NAS-
Net . However, no prior work has systematically studied the effect of image resolution on large scale ﬁne-grained
datasets as we do in this paper.
How to deal with long-tailed distribution is an important problem in real world data . However, it is
a rather unexplored area mainly because commonly used
benchmark datasets are pre-processed to be close-to evenly
distributed . Van Horn et al. pointed out that the
performance of tail categories are much poorer than head
categories that have enough training data. We present a simple two-step training scheme to deal with long-tailed distribution that works well in practice.
Transfer Learning.
Convolutional Neural Networks
(CNNs) trained on ImageNet have been widely used for
transfer learning, either by directly using the pre-trained
network as a feature extractor , or ﬁne-tuning
the network . Due to the remarkable success of
using pre-trained CNNs for transfer learning, extensive
efforts have been made on understanding transfer learning . In particular, some prior work loosely
demonstrated the connection between transfer learning and
domain similarity. For example, transfer learning between
two random splits is easier than natural / man-made object splits in ImageNet ; manually adding 512 additional relevant categories from all available classes improve
upon the commonly used 1000 ImageNet classes on PAS-
CAL VOC ; transferring from a combined ImageNet
and Places dataset yields better results on a list of visual
recognition tasks . Azizpour et al. conducted a useful study on a list of transfer learning tasks that have different similarity with the original ImageNet classiﬁcation task
(e.g., image classiﬁcation is considered to be more similar
than instance retrieval, etc.). Our major differences between
their work are two-fold: Firstly, we provide a way to quantify the similarity between source and target domain and
then choose a more similar subset from source domain for
better transfer learning. Secondly, they all use pre-trained
CNNs as feature extractors and only train either the last
layer or use a linear SVM on the extracted features, whereas
we ﬁne-tune all the layers of the network.
3. Large Scale Fine-Grained Categorization
In this section, we present our training scheme that
achieves top performance on the challenging iNaturalist
2017 dataset, especially focusing on using higher image resolution and dealing with long-tailed distribution.
3.1. The Effect of Image Resolution
When training a CNN, for the ease of network design and
training in batches, the input image is usually pre-processed
to be square with a certain size. Each network architecture
usually has a default input size. For example, AlexNet 
and VGGNet take the default input size of 224 × 224
and this default input size cannot be easily changed because the fully-connected layer after convolutions requires
a ﬁxed size feature map. More recent networks including
ResNet and Inception are fully convolutional, with a global average pooling layer right after convolutions. This design enables the network to take input
images with arbitrary sizes. Images with different resolution induce feature maps of different down-sampled sizes
within the network.
Input images with higher resolutions usually contain
richer information and subtle details that are important to
visual recognition, especially for FGVC. Therefore, in general, higher resolution input image yields better performance. For networks optimized on ImageNet, there is a
trend of using input images with higher resolution for modern networks: from originally 224 × 224 in AlexNet to
331 × 331 in recently proposed NASNet , as shown in
Table 3. However, most images from ImageNet have a resolution of 500 × 375 and contain objects of similar scales,
limiting the beneﬁts we can get from using higher resolution inputs. We explore the effect of using a wide range
of input image sizes from 299 × 299 to 560 × 560 in iNat
dataset, showing greatly improved performance with higher
resolution inputs.
3.2. Long-Tailed Distribution
The statistics of real world images is long-tailed: a few
categories are highly representative and have most of the
images, whereas most categories are observed rarely with
only a few images . This is in stark contrast to the
even image distribution in popular benchmark datasets such
as ImageNet , COCO and CUB200 .
With highly imbalanced numbers of images across categories in iNaturalist dataset , we observe poor performance on underrepresented tail categories. We argue that
this is mainly caused by two reasons: 1) The lack of training
data. Around 1,500 ﬁne-grained categories in iNat training
set have fewer than 30 images. 2) The extreme class imbalance encountered during training: the ratio between the
number of images in the largest class and the smallest one is
Input Res.
AlexNet , VGGNet , ResNet 
Inception 
ResNetv2 , ResNeXt , SENet 
NASNet 
Table 1. Default input image resolution for different networks.
There is a trend of using input images with higher resolution for
modern networks.
Category id sorted by number of images
Image frequency
iNat train
Subset for further fine-tuning
Figure 2. The distribution of image frequency of each category in
the whole training set we used in the ﬁrst stage training and the
selected subset we used in the second stage ﬁne-tuning.
about 435. Without any re-sampling of the training images
or re-weighting of the loss, categories with more images in
the head will dominate those in the tail. Since there is very
little we can do for the ﬁrst issue of lack of training data,
we propose a simple and effective way to address the second issue of the class imbalance.
The proposed training scheme has two stages. In the ﬁrst
stage, we train the network as usual on the original imbalanced dataset. With large number of training data from all
categories, the network learns good feature representations.
Then, in the second stage, we ﬁne-tune the network on a
subset containing more balanced data with a small learning
rate. The idea is to slowly transfer the learned feature and let
the network re-balance among all categories. Fig. 2 shows
the distribution of image frequency in iNat training set that
we trained on in the ﬁrst stage and the subset we used in the
second stage, respectively. Experiments in Sec. 5.2 verify
that the proposed strategy yields improved overall performance, especially for underrepresented tail categories.
4. Transfer Learning
This section describes transfer learning from the networks trained on large scale datasets to small scale ﬁnegrained datasets. We introduce a way to measure visual similarity between two domains and then show how to select a
subset from source domain given the target domain.
4.1. Domain Similarity
Suppose we have a source domain S and a target domain
T . We deﬁne the distance between two images s ∈S and
t ∈T as the Euclidean distance between their feature representations:
d(s, t) = ∥g(s) −g(t)∥
where g(·) denotes a feature extractor for an image. To better capture the image similarity, the feature extractor g(·)
needs to be capable of extracting high-level information
from images in a generic, unbiased manner. Therefore, in
our experiments, we use g(·) as the features extracted from
the penultimate layer of a ResNet-101 trained on the large
scale JFT dataset .
In general, using more images yields better transfer
learning performance. For the sake of simplicity, in this
study we ignore the effect of domain scale (number of images).
Speciﬁcally, we normalize the number of images
in both source and target domain. As studied by Chen et
al. , transfer learning performance increases logarithmically with the amount of training data. This suggests that
the performance gain in transfer learning resulting from the
use of more training data would be insigniﬁcant when we already have a large enough dataset (e.g., ImageNet). Therefore, ignoring the domain scale is a reasonable assumption
that simpliﬁes the problem. Our deﬁnition of domain similarity can be generalized to take domain scale into account
by adding a scale factor, but we found ignoring the domain
scale already works well in practice.
Under this assumption, transfer learning can be viewed
as moving a set of images from the source domain S to the
target domain T . The work needed to be done by moving
an image to another can be deﬁned as their image distance
in Eqn. 1. Then the distance between two domains can be
deﬁned as the least amount of total work needed. This definition of domain similarity can be calculated by the Earth
Mover’s Distance (EMD) .
To make the computations more tractable, we further
make an additional simpliﬁcation to represent all image features in a category by the mean of their features. Formally,
we denote source domain as S = {(si, wsi)}m
i=1 and target
domain as T = {(tj, wtj)}n
j=1, where si is the i-th category in S and wsi is the normalized number of images in
that category; similarly for tj and wtj in T . m and n are
the total number of categories in source domain S and target domain T , respectively. Since we normalize the number
of images, we have Pm
i=1 wsi = Pn
j=1 wtj = 1. g(si) denotes the mean of image features in category i from source
domain, similarly for g(tj) in target domain. Using the de-
ﬁned notations, the distance between S and T is deﬁned as
their Earth Mover’s Distance (EMD):
d(S, T ) = EMD(S, T ) =
i=1,j=1 fi,jdi,j
i=1,j=1 fi,j
Northern cardinal
Ragdoll (0.2)
Indigo bunting
Boeing 777
Tesla Model S
Feature Space
Figure 3. The proposed domain similarity calculated by Earth
Mover’s Distance (EMD). Categories in source domain and target domain are represented by red and green circles. The size of
the circle denotes the normalized number of images in that category. Blue arrows represent ﬂows from source to target domain by
solving EMD.
where di,j = ∥g(si) −g(tj)∥and the optimal ﬂow fi,j
corresponds to the least amount of total work by solving the
EMD optimization problem. Finally, the domain similarity
is deﬁned as:
sim(S, T ) = e−γd(S,T )
where γ is set to 0.01 in all experiments. Fig. 3 illustrates
calculating the proposed domain similarity by EMD.
4.2. Source Domain Selection
With the deﬁned domain similarity in Eqn. 2, we are able
to select a subset from source domain that is more similar
to target domains. We use greedy selection strategy to incrementally include the most similar category in the source
domain. That is, for each category si in source domain S,
we calculate its domain similarity with target domain by
sim({(si, 1)}, T ) as deﬁned in Eqn. 3.
Then top k categories with highest domain similarities will be selected.
Notice that although this greedy way of selection has no
guarantee on the optimality of the selected subset of size k
in terms of domain similarity, we found this simple strategy
works well in practice.
5. Experiments
The proposed training scheme for large scale FGVC is
evaluated on the recently proposed iNaturalist 2017 dataset
(iNat) . We also evaluate the effectiveness of the our
proposed transfer learning by using ImageNet and iNat as
source domains, and 7 ﬁne-grained categorization datasets
as target domains. Sec. 5.1 introduces experiment setup.
Experiment results on iNat and transfer learning are presented in Sec. 3 and Sec. 5.3, respectively.
5.1. Experiment setup
iNaturalist.
The iNatrualist 2017 dataset (iNat) 
contains 675,170 training and validation images from 5,089
natural ﬁne-grained categories. Those categories belong to
13 super-categories including Plantae (Plant), Insecta (Insect), Aves (Bird), Mammalia (Mammal), and so on. The
iNat dataset is highly imbalanced with dramatically different number of images per category. For example, the largest
super-category “Plantae (Plant)” has 196,613 images from
2,101 categories; whereas the smallest super-category “Protozoa” only has 381 images from 4 categories. We combine
the original split of training set and 90% of the validation set
as our training set (iNat train), and use the rest of 10% validation set as our mini validation set (iNat minival), resulting
in total of 665,473 training and 9,697 validation images.
We use the ILSVRC 2012 splits of
1,281,167 training (ImageNet train) and 50,000 validation
(ImageNet val) images from 1,000 classes.
Fine-Grained Visual Categorization. We evaluate our
transfer learning approach on 7 ﬁne-grained visual categorization datasets as target domains, which cover a wide
range of FGVC tasks including natural categories like bird
and ﬂower and man-made categories such as aircraft. Table
2 summarizes number of categories, together with number
of images in their original training and validation splits.
Network Architectures
We use 3 types of network architectures: ResNet , Inception and SENet .
Residual Network (ResNet). Originally introduced by
He et al. , networks with residual connections greatly
reduced the optimization difﬁculties and enabled the training of much deeper networks. ResNets were later improved
by pre-activation that uses identity mapping as the skip connection between residual modules . We used the latest
version of ResNets with 50, 101 and 152 layers.
Inception. The Inception module was ﬁrstly proposed
by Szegedy et al. in GoogleNet that was designed
to be very efﬁcient in terms of parameters and computations, while achieving state-of-the-art performance. Inception module was then further optimized by using Batch Normalization , factorized convolution and residual
connections as introduced in . We use Inceptionv3 , Inception-v4 and Inception-ResNet-v2 as representatives for Inception networks in our experiments.
Squeeze-and-Excitation (SE). Recently proposed by
Hu et al. , Sequeeze-and-Excitation (SE) modules
achieved the best performance in ILSVRC 2017 . SE
module squeezes responses from a feature map by spatial
average pooling and then learns to re-scale each channel of
FGVC Dataset
Flowers-102 
CUB200 Birds 
Aircraft 
Stanford Cars 
Stanford Dogs 
NABirds 
Food101 
Table 2. We use 7 ﬁne-grained visual categorization datasets to
evaluate the proposed transfer learning method.
Inc-v3 299
Inc-v3 448
Inc-v3 560
Table 3. Top-5 error rate on iNat minival using Inception-v3 with
various input sizes. Higher input size yield better performance.
the feature map. Due to its simplicity in design, SE module
can be used in almost any modern networks to boost the performance with little additional overhead. We use Inceptionv3 SE and Inception-ResNet-v2 SE as baselines.
For all network architectures, we follow strictly their
original design but with the last linear classiﬁcation layer
replaced to match the number of categories in our datasets.
Implementation
We used open-source Tensorﬂow to implement and
train all the models asynchronously on multiple NVIDIA
Tesla K80 GPUs.
During training, the input image was
randomly cropped from the original image and re-sized to
the target input size with scale and aspect ratio augmentation . We trained all networks using the RMSProp optimizer with momentum of 0.9, and the batch size of 32. The
initial learning rate was set to 0.045, with exponential decay
of 0.94 after every 2 epochs, same as ; for ﬁne-tuning
in transfer learning, the initial learning rate is lowered to
0.0045 with the learning rate decay of 0.94 after every 4
epochs. We also used label smoothing as introduced in .
During inference, the original image is center cropped and
re-sized to the target input size.
5.2. Large Scale Fine-Grained Visual Recognition
To verify the proposed learning scheme for large scale
ﬁne-grained categorization, we conduct extensive experiments on iNaturalist 2017 dataset. For better performance,
we ﬁne-tune from ImageNet pre-trained networks. If training from scratch on iNat, the top-5 error rate is ≈1% worse.
We train Inception-v3 with 3 different input resolutions
(299, 448 and 560). The effect of image resolution is presented in Table 3. From the table, we can see that using
higher input resolutions achieve better performance on iNat.
Inc-v3 299
Inc-v3 560
Inc-v4 560
Inc-ResNet-v2 560
Network and input image size
Top-5 error rate (%)
Figure 4. Top-5 error rate on iNat minival before and after ﬁnetuning on a more balanced subset. This simple strategy improves
the performance on long-tailed iNat dataset.
The evaluation of our proposed ﬁne-tuning scheme for
dealing with long-tailed distribution is presented in Fig. 4.
Better performance can be obtained by further ﬁne-tuning
on a more balanced subset with small learning rate (10−6
in our experiments). Table 4 shows performance improvements on head and tail categories with ﬁne-tuning.
Improvements on head categories with ≥100 training images
are 1.95% of top-1 and 0.92% of top-5; whereas on tail categories with < 100 training images, the improvements are
5.74% of top-1 and 2.71% of top-5. These results verify
that the proposed ﬁne-tuning scheme greatly improves the
performance on underrepresented tail categories.
Table 5 presents the detailed performance breakdown of
our winning entry in the iNaturalist 2017 challenge . Using higher image resolution and further ﬁne-tuning on a
more balanced subset are the key to our success.
5.3. Domain Similarity and Transfer Learning
We evaluate the proposed transfer learning method by
pre-training the network on source domain from scratch,
and then ﬁne-tune on target domains for ﬁne-grained visual categorization. Other than training separately on ImageNet and iNat, we also train networks on a combined ImageNet + iNat dataset that contains 1,946,640 training images from 6,089 categories (i.e., 1,000 from ImageNet and
5,089 from iNat). We use input size of 299 × 299 for all
networks. Table 6 shows the pre-training performance evaluated on ImageNet val and iNat minival. Notably, a single
network trained on the combined ImageNet + iNat dataset
achieves competitive performance compared with two models trained separately. In general, combined training is better than training separately in the case of Inception and Inception SE, but worse in the case of ResNet.
Based on the proposed domain selection strategy deﬁned
in Sec. 4.2, we select the following two subsets from the
combined ImageNet + iNat dataset: Subset A was chosen
by including top 200 ImageNet + iNat categories for each
of the 7 FGVC dataset. Removing duplicated categories resulted in a source domain containing 832 categories. Subset
B was selected by adding most similar 400 categories for
Head: ≥100 imgs
Tail: < 100 imgs
Table 4. Top-1 and top-5 error rates (%) on iNat minival for
Inception-v4 560. The proposed ﬁne-tuning scheme greatly improves the performance on underrepresented tail categories.
Inc-v3 299
Inc-v3 560
25.4 (+ 4.5)
8.6 (+ 2.0)
Inc-v3 560 FT
22.7 (+ 2.7)
6.6 (+ 2.0)
Inc-v4 560 FT
20.8 (+ 1.9)
5.4 (+ 1.2)
Inc-v4 560 FT 12-crop
19.2 (+ 1.6)
4.7 (+ 0.7)
18.1 (+ 1.1)
4.1 (+ 0.6)
Table 5. Performance improvements on iNat minival. The number
inside the brackets indicates the improvement over the model in
the previous row. FT denotes using the proposed ﬁne-tuning to
deal with long-tailed distribution. Ensemble contains two models:
Inc-v4 560 FT and Inc-ResNet-v2 560 FT with 12-crop.
CUB200, NABirds, top 100 categories for Stanford Dogs
and top 50 categories for Stanford Cars and Aircraft, which
gave us 585 categories in total. Fig. 6 shows top 10 most
similar categories in ImageNet + iNat for all FGVC datasets
calculated by our proposed domain similarity. It’s clear to
see that for CUB200, Flowers-102 and NABirds, most similar categories are from iNat; whereas for Stanford Dogs,
Stanford Cars, Aircraft and Food101, most similar categories are from ImageNet. This indicates the strong dataset
bias in both ImageNet and iNat.
The transfer learning performance by ﬁne-tuning an
Inception-v3 on ﬁne-grained datasets are presented in Table
7. We can see that both ImageNet and iNat are highly biased, achieving dramatically different transfer learning performance on target datasets. Interestingly, when we transfer networks trained on the combined ImageNet + iNat
dataset, performance are in-between ImageNet and iNat
pre-training, indicating that we cannot achieve good performance on target domains by simply using a larger scale,
combined source domain.
Further, in Fig. 5, we show the relationship between
transfer learning performance and our proposed domain
similarity. We observe better transfer learning performance
when ﬁne-tuned from a more similar source domain, except
Food101, on which the transfer learning performance almost stays same as domain similarity changes. We believe
this is likely due to the large number of training images in
Food101 (750 training images per class). Therefore, the target domain contains enough data thus transfer learning has
very little help. In such a scenario, our assumption on ignoring the scale of domain is no longer valid.
ImageNet val
iNaturalist minival
Separate Train
Combined Train
Separate Train
Combined Train
ResNet-50 
ResNet-101 
ResNet-152 
Inception-v3 
Inception-ResNet-v2 
Inception-v3 SE 
Inception-ResNet-v2 SE 
Table 6. Pre-training performance on different source domains. Networks trained on the combined ImageNet + iNat dataset with 6,089
classes achieve competitive performance on both ImageNet and iNat compared with networks trained separately on each dataset. ∗indicates
the model was evaluated on the non-blacklisted subset of ImageNet validation set that may slightly improve the performance.
Stanford Dogs
Flowers-102
Stanford Cars
ImageNet + iNat
Subset A (832-class)
Subset B (585-class)
Table 7. Transfer learning performance on 7 FGVC datasets by ﬁne-tuning the Inception-v3 299 pre-trained on different source domains.
Each row represents a network pre-trained on a speciﬁc source domain, and each column shows the top-1 image classiﬁcation accuracy by
ﬁne-tuning different networks on a target ﬁne-grained dataset. Relative good and poor performance on each FGVC dataset are marked by
green and red, respectively. Two selected subsets based on domain similarity achieve good performance on all FGVC datasets.
Domain similarity (e
Transfer learning performance (%)
Oxford Flowers
Stanford Cars
Stanford Dogs
ImageNet+iNat
Subset A (832-class)
Subset B (585-class)
Figure 5. The relationship between transfer learning performance
and domain similarity between source and target domain. Each
line represents a target FGVC dataset and each marker represents
the source domain. Better transfer learning performance can be
achieved by ﬁne-tuning the network that is pre-trained on a more
similar source domain. Two selected subsets based on our domain
similarity achieve good performance on all FGVC datasets.
From Table 7 and Fig. 5, we observe that the selected
Subset B achieves good performance among all FGVC
datasets, surpassing ImageNet pre-training by a large margin on CUB200 and NABirds. In Table 8, we compare our
approach with existing FGVC methods. Results demonstrate state-of-the-art performance of the prposed transfer
learning method on commonly used FGVC datasets. Notice
that since our deﬁnition of domain similarity is fast to compute, we can easily explore different ways to select a source
domain. The transfer learning performance can be directly
estimated based on domain similarity, without conducting
any pre-training and ﬁne-tuning.
Prior to our work, the
only options to achieve good performance on FGVC tasks
are either designing better models based on ImageNet ﬁnetuning or augmenting the dataset by collecting
more images . Our work, however, provides a novel
direction of using a more similar source domain to pre-train
the network. We show that with properly selected subsets
in source domain, it is able to match or exceed those performance gain by simply ﬁne-tuning off-the-shelf networks.
6. Conclusions
In this work, we have presented a training scheme that
achieves top performance on large scale iNaturalist dataset,
by using higher resolution input image and ﬁne-tuning to
deal with long-tailed distribution.
We further proposed
a novel way of capturing domain similarity with Earth
Mover’s Distance and showed better transfer learning performance can be achieved by ﬁne-tuning from a more similar domain. In the future, we plan to study other important
factors in transfer learning beyond domain similarity.
Acknowledgments. This work was supported in part by a
Google Focused Research Award. We would like to thank
our colleagues at Google for helpful discussions.
Stanford Dogs
Flowers-102
Stanford Cars
Apis mellifera: Insecta
Campsis radicans: Plantae Chamerion angustifolium:
Cornus florida: Plantae
n11939491: daisy
Gaillardia pulchella:
Digitalis purpurea:
Helianthus annuus:
Rudbeckia hirta: Plantae
n11939491: pot
Mimus polyglottos:
Setophaga coronata:
cinerascens: Aves
Sayornis phoebe: Aves
Sporophila torqueola:
Tyrannus vociferans:
Setophaga coronata
auduboni: Aves
Myiarchus crinitus:
Setophaga coronata:
n01560419: bulbul
n02105412: kelpie
n02099712:
Labrador_retriever
n02098105:
soft-coated_wheaten_ter
n02094114:
Norfolk_terrier
n02096437:
Dandie_Dinmont
n02096294:
Australian_terrier
n02099849:
Chesapeake_Bay_retrieve
n02106662:
German_shepherd
n02095570:
Lakeland_terrier
n02097474:
Tibetan_terrier
n04285008: sports_car
n03100240: convertible
n03770679: minivan
n02974003: car_wheel
n02814533: beach_wagon
n04037443: racer
n03459775: grille
n02930766: cab
n03670208: limousine
n03769881: minibus
n02690373: airliner
n04552348: warplane
n04592741: wing
n04008634: projectile
n03773504: missile
n04266014: space_shuttle
n03976657: pole
n02704792: amphibian
n04336792: stretcher
n03895866:
passenger_car
n07579787: plate
n07711569:
mashed_potato
n04263257: soup_bowl
n04596742: wok
n07614500: ice_cream
n07584110: consomme
n07836838:
chocolate_sauce
n07871810: meat_loaf
n03400231: frying_pan
n07880968: burritor
Mimus polyglottos:
cinerascens: Aves
Setophaga coronata:
Sporophila torqueola:
Sayornis phoebe: Aves
Setophaga coronata:
Sayornis saya: Aves
Passerina caerulea:
Pheucticus
ludovicianus: Aves
Fringilla coelebs: Aves
Target Domain
Top 10 most similar categories from a source domain of ImageNet + iNat (blue: ImageNet categories; red: iNat categories)
Figure 6. Examples showing top 10 most similar categories in the combined ImageNet + iNat for each FGVC dataset, calculated with our
proposed domain similarity. The left column represents 7 FGVC target domains, each by a randomly chosen image from the dataset. Each
row shows top 10 most similar categories in ImageNet + iNat for a speciﬁc FGVC target domain. We represent a category by one randomly
chosen image from that category. ImageNet categories are marked in blue, whereas iNat categories are in red.
Stanford Dogs
Stanford Cars
Subset B (585-class): Inception-v3
Subset B (585-class): Inception-ResNet-v2 SE
Krause et al. 
Bilinear-CNN 
Compact Bilinear Pooling 
Zhang et al. 
Low-rank Bilinear Pooling 
Kernel Pooling 
RA-CNN 
Improved Bilinear-CNN 
MA-CNN 
Table 8. Comparison to existing state-of-the-art FGVC methods. As a convention, we use same 448 × 448 input size. Since we didn’t ﬁnd
recent proposed FGVC methods applied to Flowers-102 and NABirds, we only show comparisons on the rest of 5 datasets. Our proposed
transfer learning approach is able to achieve state-of-the-art performance on all FGVC datasets, especially on CUB200 and NABirds.