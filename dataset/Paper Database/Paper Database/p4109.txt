Mach Learn 81: 21–35
DOI 10.1007/s10994-010-5198-3
Large scale image annotation: learning to rank with joint
word-image embeddings
Jason Weston · Samy Bengio · Nicolas Usunier
Received: 30 April 2010 / Accepted: 20 June 2010 / Published online: 27 July 2010
© The Author(s) 2010
Abstract Image annotation datasets are becoming larger and larger, with tens of millions
of images and tens of thousands of possible annotations. We propose a strongly performing
method that scales to such datasets by simultaneously learning to optimize precision at k
of the ranked list of annotations for a given image and learning a low-dimensional joint
embedding space for both images and annotations. Our method both outperforms several
baseline methods and, in comparison to them, is faster and consumes less memory. We also
demonstrate how our method learns an interpretable model, where annotations with alternate
spellings or even languages are close in the embedding space. Hence, even when our model
does not predict the exact annotation given by a human labeler, it often predicts similar
annotations, a fact that we try to quantify by measuring the newly introduced “sibling”
precision metric, where our method also obtains excellent results.
Keywords Large scale · Image annotation · Learning to rank · Embedding
1 Introduction
The emergence of the web as a tool for sharing information has caused a massive increase
in the size of potential datasets available for machines to learn from. Millions of images on
web pages have tens of thousands of possible annotations in the form of HTML tags (which
Editors: José L. Balcázar, Francesco Bonchi, Aristides Gionis, and Michèle Sebag.
J. Weston ()
Google, New York, USA
e-mail: 
Google, Mountain View, USA
e-mail: 
N. Usunier
Université Paris 6, LIP6, Paris, France
e-mail: 
Mach Learn 81: 21–35
can be conveniently collected by querying search engines, Torralba et al. 2008a), tags such
as in www.ﬂickr.com, or human-curated labels such as in www.image-net.org . We therefore need machine learning algorithms for image annotation that can scale
to learn from and annotate such data. This includes: (i) scalable training and testing times,
and (ii) scalable memory usage. In the ideal case we would like a fast algorithm that ﬁts
on a laptop, at least at annotation time. For many recently proposed models tested on small
datasets, e.g. Makadia et al. , Guillaumin et al. , it is unclear if they satisfy
these constraints.
In this work we study feasible methods for just such a goal. We consider models that learn
to represent images and annotations jointly in a low dimension embedding space. Such embeddings are fast at testing time because the low dimension implies fast computations for
ranking annotations. Simultaneously, the low dimension also implies small memory usage.
To obtain good performance for such a model, we propose to train its parameters by learning
to rank, optimizing for the top annotations in the list, e.g. optimizing precision at k. Unfortunately, such measures can be costly to train. To make training time efﬁcient we propose the
WARP loss (Weighted Approximate-Rank Pairwise loss). The WARP loss is related to the
recently proposed Ordered Weighted Pairwise Classiﬁcation (OWPC) loss which has been shown to be state-of-the-art on (small) text retrieval tasks. WARP
uses stochastic gradient descent and a novel sampling trick to approximate ranks resulting
in an efﬁcient online optimization strategy which we show is superior to standard stochastic
gradient descent applied to the same loss, enabling us to train on datasets that do not even ﬁt
in memory. Moreover, WARP can be applied to our embedding models (in fact, to arbitrary
differentiable models) whereas the OWPC loss, which relies on SVMstruct cannot.
The model we propose learns the semantic structure of both word sequences (annotations) and images, and we observe semantically consistent rankings due to semantically
similar annotations (and hence also images) appearing close in the embedding space. Given
tens of thousands of possible labels it is unlikely that a human annotator will have assigned
all the correct possible labels to an image, hence we cannot always be sure if a prediction
is wrong. Moreover, when a prediction is wrong it would be nice to know if it is completely wrong (“motorcar” instead of “tuna”) or semantically almost right (“tuna” instead
of “salmon”). To quantify this we propose a novel metric, the sibling precision, which gives
credit for semantically similar predictions to the correct one by using a collected dataset of
similarities between annotations.
Overall the novelty of the paper is:
(i) reporting image annotation results on a larger scale than ever previously reported
(10 million training examples and 100 thousand annotations);
(ii) showing for the ﬁrst time the utility of optimizing precision at k for image annotation;
(iii) proposing a large scale algorithm for (approximately) optimizing precision at k (WARP
(iv) showing that our embedding model yields low memory usage, fast computation time,
and learns the semantic structure of similarly annotated images as measured by the
sibling precision;
(v) showing that using an embedding model trained with the WARP loss yields better performance than any known competing approach for this task.
The structure of the paper is as follows. Section 2 deﬁnes the embedding models that
we employ. Section 3 deﬁnes the WARP loss and shows how to train our models with it.
Section 4 describes how we perform our evaluation, including the newly proposed sibling
precision metric. Section 5 details prior related work, Sect. 6 describes experiments conducted on large scale datasets, and Sect. 7 concludes.
Mach Learn 81: 21–35
2 Joint word-image model
We propose to learn a mapping into a feature space where images and annotations are both
represented. The mapping functions are therefore different, but are learnt jointly to optimize
the supervised loss of interest for our ﬁnal task, that of annotating images. We start with
a representation of images x ∈Rd and a representation of annotations i ∈Y = {1,...,Y},
indices into a dictionary of possible annotations. We then learn a mapping from the image
feature space to the joint space RD:
ΦI(x) : Rd →RD.
while jointly learning a mapping for annotations:
ΦW(i) : {1,...,Y} →RD.
These are chosen to be linear maps, i.e. ΦI(x) = V x and ΦW(i) = Wi, where Wi indexes
the ith column of a D × Y matrix, but potentially any mapping could be used. In our work,
we use sparse high dimensional feature vectors of bags-of-visual terms for image vectors
x and each annotation has its own learnt representation (even if, for example, multi-word
annotations share words).
Our goal is, for a given image, to rank the possible annotations such that the highest
ranked annotations best describe the semantic content of the image. We consider the following model:
fi(x) = ΦW(i)⊤ΦI(x) = W ⊤
where the possible annotations i are ranked according to the magnitude of fi(x), largest
ﬁrst, and our family of models have constrained norm:
i = 1,...,d,
i = 1,...,Y.
which acts as a regularizer in the same way as is used in lasso . In the
next section we describe the kind of loss function we employ with our model, and thus
subsequently the algorithm to train it.
3 Weighted approximate-rank pairwise (WARP) loss
We consider the task of ranking labels i ∈Y given an example x. In our setting labeled pairs
(x,y) will be provided for training where only a single annotation yi ∈Y is labeled correct.1
Let f (x) ∈RY be a vector function providing a score for each of the labels, where fi(x) is
the value for label i.
A class of ranking error functions was recently deﬁned in Usunier et al. as:
1However, the methods described in this paper could be generalized to the multi-label case, naively by averaging the loss over all positive labels.
Mach Learn 81: 21–35
where ranky(f (x)) is the rank of the true label y given by f (x):
fi(x) ≥fy(x)
where I is the indicator function, and L(·) transforms this rank into a loss:
with α1 ≥α2 ≥··· ≥0.
This class of functions allows one to deﬁne different choices of L(·) with different minimizers. Minimizing L with αj =
Y−1 would optimize the mean rank, α1 = 1 and αj>1 = 0
the proportion of top-ranked correct labels, and larger values of α in the ﬁrst few positions
optimize the top k in the ranked list, which is of interest for optimizing precision at k or
mean average precision (MAP). For example, given two images, if one choice of function
ranks their true labels at position 1 and position 100 respectively, and another function both
at position 50, then a choice of αj =
Y−1 prefers these functions equally, whereas a choice
of αj = 1/j prefers the ﬁrst function, which gives superior precision at 1.
The authors of Usunier et al. used this loss (calling their method OWPC) in an
SVMstruct formalism to train on (small) text retrieval datasets, showing experimentally that
the choice of αj = 1/j yields state-of-the-art results measuring precision at k. We hence
adopt the same choice but are interested in a method that can: (i) train embedding models
of the form (1) which cannot be trained using SVMstruct; and (ii) can be trained efﬁciently
online when the data will not even ﬁt into memory. In the following section we show how
this can be done by employing a novel sampling trick to make stochastic gradient descent
(SGD) feasible for optimizing precision at k for arbitrary differentiable models, including
our embedding model formulation.
Online learning to rank
The loss (4) is equal to:
I(fi(x) ≥fy(x))
ranky(f (x))
with the convention 0/0 = 0 when the correct label y is top-ranked. Using the hinge loss
instead of the indicator function to add a margin and make the loss continuous, err can be
approximated by:
|1 −fy(x) + fi(x)|+
where |t|+ is the positive part of t and rank1
y(f (x)) is the margin-penalized rank of y:
1 + fi(x) > fy(x)
The overall risk we want to minimize is then:
Risk(f ) =
Mach Learn 81: 21–35
An unbiased estimator of this risk can be obtained by stochastically sampling in the following way:
1. Sample a pair (x,y) according to P (x,y);
2. For the chosen (x,y) sample a violating label ¯y such that 1 + f ¯y(x) > fy(x).
This chosen triplet (x,y, ¯y) has contribution:
f (x),y, ¯y
1 −fy(x) + f ¯y(x)
to the total risk, i.e. taking the expectation of these contributions approximates (8) because we have probability 1/rank1
y(f (x)) of drawing ¯y in step (2) (or a contribution of
0 if rank1
y(f (x)) = 0) which accounts for the denominator of (6).
This suggests for learning we can thus perform the following stochastic update procedure over the parameters β that deﬁne a family of possible
functions f ∈F:
βt+1 = βt −γt
∂err(f (x),y, ¯y)
where γt is the learning rate.
Weighted approximate ranking
To perform the SGD described above we still have two
problems that make this procedure inefﬁcient:
(i) In step (2), we need to compute the values fi(x) for i = 1,...,Y to know which labels
¯y are violators, which is expensive for large Y .
(ii) rank1
y(f (x)) in (10) is also unknown without computing fi(x) for i ∈Y, which again is
expensive.
We propose to solve both problems with the following approach: for step (2), we sample
labels i uniformly with replacement until we ﬁnd a violating label.
Now if there are k = rank1
y(f (x)) violating labels, the random variable Nk which counts
the number of trials in our sampling step follows a geometric distribution of parameter
(i.e. Pr(Nk > q) = (1 −
Y−1)q). Thus k =
E[Nk]. This suggests that the value of rank1
in (9) may be approximated by:
y(f (x)) ≈
where ⌊.⌋is the ﬂoor function and N the number of trials in the sampling step.
Remark 1 We intuitively presented the sampling approach as an approximation of the SGD
step of (10). In fact, the sampling process gives an unbiased estimator of the risk (8) if we
consider a new function ˜L instead of L in (6), with:
Elementary calculations show that ˜L satisﬁes (5). So our approach optimizes a new ranking
Mach Learn 81: 21–35
Algorithm 1 Online WARP loss optimization
Input: labeled data (xi,yi), yi ∈{1,...,Y}.
Pick a random labeled example (xi,yi)
Let fyi(xi) = ΦW(yi)⊤ΦI(xi)
Set N = 0.
Pick a random annotation ¯y ∈{1,...,Y} \ yi.
Let f ¯y(xi) = ΦW( ¯y)⊤ΦI(xi)
N = N + 1.
until f ¯y(xi) > fyi(xi) −1 or N ≥Y −1
if f ¯y(xi) > fyi(xi) −1 then
Make a gradient step to minimize:
N ⌋)|1 −fy(xi) + f ¯y(xi)|+
Project weights to enforce constraints (2)–(3).
until validation error does not improve.
Remark 2 The ﬂoor function in the approximation rank1
y(f (x)) ≈⌊Y−1
N ⌋makes it useless
to continue sampling after Y −1 unsuccessful trials. Thus, an SGD step requires less than
rank1y(f (x)),Y −1) computations of scores fi(x) on average. If the true annotation
happens to be at the top of the list for that particular image our method is as slow as standard
SGD (i.e. computing the rank (7) explicitly). However at the start of training when there are
many errors this is rarely the case. Moreover, on difﬁcult datasets (as in our case), many
correct labels will never have a rank close to one. Hence, our method results in the huge
speedups observed later in experiments (see Fig. 2).
Training our models
To summarize, our overall method which we call WSABIE (Web
Scale Annotation by Image Embedding, pronounced “wasabi”) consists of the joint wordimage embedding model of Sect. 2 trained with the WARP loss of Sect. 3. The mapping
matrices V and W are initialized at random with mean 0, standard deviation
d , which is a
common choice, e.g. as implemented in the Torch Machine Learning library2 (which is the
software we used for our experiments). Note, the initial weights are rescaled if they violate
the constraints (2)–(3). Pseudocode for training with WARP loss is given in Algorithm 1.
We use a ﬁxed learning rate γ , chosen using a validation set (a decaying schedule over time
t is also possible, but we did not implement that approach). The validation error in the last
line of Algorithm 1 is in practice only evaluated after every hour on a subset of the validation
set for computational efﬁciency.
4 Evaluation and sibling precision
We measure in the experimental section the standard metrics of precision at the top k of the
list (p@k) and mean average precision (MAP) for the algorithms we compare, which give
more credit if the true annotation appears near the top of the list of possible annotations.
2 
Mach Learn 81: 21–35
On our datasets, we have between ten or a hundred thousand annotations, some of which
can be semantically rather close to each other. In the extreme case, two different labels can
be synonyms, translations or alternative spellings. Our model tries to capture this structure
of the annotation set through the projection in the embedding space.
To evaluate the ability of our model to learn the semantic relations between labels from
images, we propose a new metric called the sibling precision at k (psib@k). Suppose we
have some ground-truth in the form of a matrix S, where Sij ∈ is a measure of semantic
similarity between labels i and j (Sij = 1 means that the words are semantically equivalent).
Then, for a ranking yr = (yr
Y), psib@k is deﬁned as:
When S is the identity matrix we recover the usual p@k loss. Otherwise, psib@k is a relaxation of p@k, as off-diagonal elements of S give credit when a prediction is semantically
close to the true label. psib also measures the ability of the model to discover the semantic
structure by considering the similarity of all the ﬁrst k predicted labels.
In order to build S, we proceed as follows. We suppose we have a database of known
relations between annotations of the form isa(yc,yp) where yp is a parent concept of yc, e.g.
isa(“toad”,“amphibian”). We then deﬁne two annotations as siblings if they share a “parent”:
if i = j ∨∃k : isa(i,k) ∧isa(j,k),
otherwise.
In the databases we consider in Sect. 6, ImageNet already has reliable isa relations annotated
in WordNet, and for Web-data we have obtained a similar but noisier proprietary set based
on occurrences of patterns such as “X is a Y” on web pages. The median numbers of siblings
per label are reported in Table 1.
5 Related approaches
The problem of image annotation, including the related task of image classiﬁcation, has
been the subject of much research, mainly in the computer vision literature. However, this
research mostly concentrates on tasks with a rather small number of classes, in part due to
the availability of appropriate databases. Well known databases such as Caltech-256 and Pascal-VOC have a limited number of categories,
ranging from 20 to 256. More recently, projects such as the TinyImage database and ImageNet have started proposing larger sets of annotated
images with a larger set of categories, in the order of 104 different categories. Note that
for now, even with these new large datasets, published results about image annotation or
classiﬁcation have concentrated on subsets pertaining to a few hundred different categories
or less only, e.g. Torralba et al. , Fergus et al. . Much research in the literature
has in fact concentrated on extracting better image features, then training independently
simple classiﬁers such as linear or kernel SVMs for each category .
An alternative approach, championed by Makadia et al. , Torralba et al. ,
Guillaumin et al. , Wang et al. , and others, is to use k-nearest neighbor in the
image feature space. This has shown good annotation performance, in particular as the size
Mach Learn 81: 21–35
of the training set grows. On the other hand, as the data grows, ﬁnding the exact neighbors becomes infeasible in terms of time and space requirements. Various approximate approaches have thus been proposed to alleviate this problem, ranging from trees to hashes,
but can suffer from being fast but not precise, or precise but slow.
Embedding words in a low dimensional space to capture semantics is a classic (unsupervised) approach in text retrieval which has been adapted for image annotation before,
for example PLSA has been used for images but has been
shown to perform worse than (non-embedding based) supervised ranking models like PAMIR
 . Embedding for image retrieval (rather than annotation) using
KCCA was also explored in Zhou et al. . Other related work includes learning embeddings for supervised document ranking and for semi-supervised
multi-task learning .
Several loss functions have also recently been proposed to optimize the top of the ranked
list. The most similar to our work is the so-called OWPC loss of Usunier et al. , which
is similar to (6) except that the weight given to each pair of labels (y,i) depends on the rank
of the incorrect label i rather than the rank of y. In its original formulation, the algorithm
of Usunier et al. relies on SVMstruct for the optimization, which cannot be used to
train our embedding models. Moreover, even if one tries to train our models with SGD on
the OWPC loss, each step would necessarily be more costly as it would require additional
sampling steps to compute or approximate the rank of the incorrect label. This argument also
applies to the loss functions proposed for other algorithms such as ListNet 
or SVMmap because the contribution to these losses of a single annotation
is tightly bound to the scores of all other annotations. Thus, to our knowledge none of these
existing methods would scale to our setup as they either cannot be trained online, or do not
avoid computing fi(x) for each i ∈Y as the WARP loss does.
In terms of the sibling precision metric, WordNet has been widely used to calculate distances between concepts in the ﬁeld of natural language processing, and has been used for
image annotation to build voted classiﬁers . In our work we
are concerned with measuring missing annotations and would consider using other similarity
measures for that, not just from WordNet.
6 Experiments
6.1 Datasets
ImageNet dataset
ImageNet is a new image dataset organized according
to WordNet . Concepts in WordNet, described by multiple words or word
phrases, are hierarchically organized. ImageNet is a growing image dataset that attaches
quality-controlled human-veriﬁed images to these concepts. We split the data into 2.5 M
images for training, 0.8 M for validation and 0.8 M for testing, removing duplicates between
train, validation and test by throwing away test examples which had too close a nearest
neighbor training or validation example in feature space. The most frequent annotation only
appears 0.04% of the time.3
3This is a commonly measured sanity check in case there is an annotation that occurs rather often, artiﬁcially
inﬂating precision.
Mach Learn 81: 21–35
Table 1 Summary statistics of
the datasets used in this paper
Statistics
Number of training images
Number of test images
Number of validation images
Number of labels
Median number of siblings per label
Web-data dataset
We had access to a very large proprietary database of images taken from
the web, together with a very noisy annotation based on anonymized user click information,
processed similarly to ImageNet. The most frequent annotation appears 0.01% of the time.
Table 1 provides summary statistics of the number of images and labels for the ImageNet
and Web-data datasets used in our experiments.
6.2 Image representation
In this work we focus on learning algorithms, not feature representations. Hence, for all
methods we try we use a standard bag-of-visual-terms type representation, which has a
sparse vector representation. In particular, we use the bag-of-terms feature setup of Grangier
and Bengio , which was shown to perform very well on the related task of image
ranking. Each image is ﬁrst segmented into several overlapping square blocks at various
scales. Each block is then represented by the concatenation of color and edge features. These
are discretized into a dictionary of d = 10,000 blocks, by training k-means on a large corpus
of images. Each image can then be represented as a bag of visual words: a histogram of the
number of times each visual word was present in the image, yielding vectors in Rd with
an average of d¯∅= 245 non-zero values. It takes on average 0.5 seconds to extract these
features per image (and via sampling the pixels this is invariant to the resolution).
6.3 Baselines
We compare our proposed approach to several baselines: approximate k-nearest neighbors
(k-NN), one-versus-rest large margin classiﬁers (One-Vs-Rest) of the form fi(x) = w⊤
trained using the Passive Aggressive algorithm , or the same models
trained with a ranking loss instead, which we call PAMIRIA as it is like the PAMIR model
used in Grangier and Bengio but applied to image annotation rather than ranking.
For all methods, hyperparameters are chosen via the validation set.
We tested approximate k-NN (ANN) because k-NN is not feasible. There are many ﬂavors of approximation . We chose the following: a random
projection at each node of the tree is chosen with a threshold to go left or right that is the
median of the projected training data to make the tree balanced. After traversing p nodes
we arrive at a leaf node containing t ≈n/2p of the original n training points from which we
calculate the nearest neighbors. Choosing p trades off accuracy with speed.
6.4 Results
The results of comparing all the methods on ImageNet and Web-data are summarized in
Table 2. Further detailed plots of precision and sibling precision for ImageNet are given
in Fig. 1. WSABIE outperforms all competing methods. We give a deeper analysis of the
results, including time and space requirements in subsequent sections.
Mach Learn 81: 21–35
Table 2 Summary of test set results on ImageNet and Web-data. Precision at 1 and 10, Sibling Precision at
10, and Mean Average Precision (MAP) are given
Approx. k-NN
One-vs-Rest
Fig. 1 Precision@k and Sibling Precision@k on ImageNet for various methods
Word-image embeddings
Example word embeddings learnt by WSABIE for Web-data are
given in Table 3 and some example image annotations are given in Table 8. Overall, we observe that the embeddings capture the semantic structure of the annotations (and images are
also embedded in this space). This explains why the sibling precision of WSABIE is superior
to competing methods, which do not attempt to learn the structure between annotations. We
also observe that WSABIE is relatively insensitive to the embedding dimension size D as
shown in Table 4. Note that although the numerical performance in terms of p@k appears
low, this is actually a worst case result as there are many labels that are actually either correct
or almost correct, which is not captured by p@k. (However, this is captured to some degree
by psib@k.) Overall, we believe the performance of our method actually gives a practically
useful system.
We compared different models trained with either WARP or AUC optimization
(via the margin ranking loss |1 −fy(x) + f¯x(y)|+ as is used in PAMIR Grangier and Bengio
2008). The results given in Table 5 show WARP consistently gives superior performance.
We also compared training time using WARP (with (1), D = 100), AUC or a standard implementation of SGD for (4) where the rank (7) is computed explicitly rather than using our
approximation method (note, in that case updates can be made for all violations for a given
image at once) which we call OWPC-SGD. For all methods we report results using their best
learning rate γ as measured on the validation set. Figure 2 shows after 36 hours WARP and
AUC are well trained, but AUC does not perform as well, and OWPC-SGD has hardly got
anywhere. Hence, the trick of approximating the rank introduced in Sect. 3 is very important
for our task.
Mach Learn 81: 21–35
Table 3 Nearest annotations in the embedding space learnt by WSABIE on Web-data. Translations (e.g.
delphin, rosen) and synonyms or misspellings (beckam, mt fuji) have close embeddings. Other annotations
are from similar visual images, e.g. Alessandro Del Piero is a soccer player. Annotations in italics are in our
known sibling set
Annotation
Neighboring annotations
barack obama
barak obama, obama, barack, barrack obama, bow wow, george bush
david beckham
beckham, david beckam, alessandro del piero, del piero, david becham
santa claus, papa noel, pere noel, santa clause, joyeux noel, tomte
delphin, dauphin, whale, delﬁn, delﬁni, baleine, blue whale, walvis
cattle, shire, dairy cows, kuh, horse, cow, shire horse, kone, holstein
rosen, hibiscus, rose ﬂower, rosa, roze, pink rose, red rose, a rose
abies alba, abies, araucaria, pine, neem tree, oak tree, pinus sylvestris
mount fuji
mt fuji, fuji, fujisan, fujiyama, mountain, zugspitze, fuji mountain
eiffel tower
eiffel, tour eiffel, la tour eiffel, big ben, paris, blue mosque, eifel tower
i pod, ipod nano, apple ipod, ipod apple, new ipod, ipod shufﬂe
f 18, euroﬁghter, f14, ﬁghter jet, tomcat, mig 21, f 16, euroﬁghter
Table 4 Changing the
embedding size on ImageNet.
Test Error metrics when we
change the dimension D of the
embedding space used in
Embedding dimension
Table 5 WARP vs. AUC
optimization. For each model,
WARP consistently improves
fi(x) = ΦW (i)⊤ΦI (x)
fi(x) = ΦW (i)⊤ΦI (x)
fi(x) = w⊤
fi(x) = w⊤
Computational expense
A summary of the test time and space complexity of the various
algorithms we compare is given in Table 6 (not including cost of pre-processing of features)
as well as concrete numbers on the particular datasets we use using a single computer, and
assuming the data ﬁts in memory (for WSABIE we give values for D = 100). In particular
k-NN would take 255 days to compute the test error on ImageNet and 3913 days for Web,
corresponding to 26 seconds and 103 seconds per image respectively, making it infeasible to
use. In comparison, PAMIRIA takes 0.07 and 0.5 seconds to compute per image and requires
1.2 GB and 8.2 GB respectively. WSABIE takes 0.02 and 0.17 seconds, and requires far less
memory, only 12 MB and 82 MB respectively. In summary, WSABIE can be feasibly run
on a laptop using limited resources whereas k-NN requires all the resources of an entire
cluster. Moreover as k-NN has time and space complexity O(n · d¯∅), where n is the number
of training examples and d¯∅is the number of non-zero features, as n increases its use of
Mach Learn 81: 21–35
Fig. 2 Training time: WARP vs. OWPC-SGD & AUC
Table 6 Algorithm time and space complexity. Time and space complexity needed to return the top ranked
annotation on a single test set image, not including feature generation. Prediction times (d = days, h = hours)
and memory requirements for the whole test sets are given for both ImageNet and Web-data datasets. We
denote by Y the number of classes, n the number of training examples, d the image input dimension, d¯∅the
average number of non-zero values per image, D the size of the embedding space, and p the depth of the tree
for approximate k-NN
Time complexity
Space complexity
Test time and memory usage
O(n · d¯∅)
O(n · d¯∅)
Approx. k-NN
O((p + n/2p) · d¯∅)
One-vs-Rest
O(Y · d¯∅)
O(Y · d¯∅)
O((Y + d¯∅) · D)
O((Y + d) · D)
resources only gets worse, whereas the other algorithms do not depend on n at test time.
WSABIE has a second advantage that it is hardly impacted if we were to choose a larger and
denser set of features than the one we use, as it maps these features into a D dimensional
space and the bulk of the computation is then in that space.
Approximate nearest neighbor and nearest neighbor
Approximate nearest neighbor
(ANN) performed rather poorly, and computing true nearest neighbor is infeasible for our
problems due to prohibitive computational expense and use of memory. ANN, however,
can be tuned depending on the test time vs accuracy tradeoff one wishes to achieve. We
investigated some parameter choices for ImageNet. In our implementation in the experiments reported in Table 2 we get 1.55% precision at 1 when we traverse a tree until we have
≈20,000 points at the leaf. Note, this takes around two days to compute which should be
compared to other methods in Table 6. If we have ≈2,500 points at each leaf (so it is much
faster) this goes down to 0.89%. If we have ≈40,000 points at each leaf, we still have only
1.94%. In summary, we cannot get ANN to perform well on our large scale datasets.
Despite exact nearest neighbor being prohibitively expensive, we used a very large cluster
of machines to compute it on our test set to evaluate its accuracy. For ImageNet we get 4.5%
p@1 and for Web-data we get 0.3% p@1 (the latter is quoted in Table 2). For Web-data this
is still poor, but for ImageNet this result is good 81: 21–35
Table 7 Ensemble models combining different annotation approaches, on the ImageNet data. WSABIE-300
means it was trained with an embedding of size 300. Hyperparameters λ1, λ2, λ3 are chosen on the validation
set using grid search
Ensemble model
WSABIE-300
λ1WSABIE-100 + λ2WSABIE-200
λ1WSABIE-100 + λ2WSABIE-300
λ1WSABIE-100 + λ2WSABIE-200 + λ3WSABIE-300
Table 8 Examples of the top 10 annotations of three compared approaches: PAMIRIA, One-vs-Rest and
WSABIE, on the Web-data dataset. Annotations in bold are the true labels, and those in italics are so-called
One-vs-Rest
bora, free willy, su, orka,
worldwide,
island, universal remote
bottlenose dolphin
surf, bora, belize, sea
tahiti, delﬁni, surﬁng,
delﬁni, orca, dolphin,
whale, sea world
air show, st augustine,
architecture, streetlight, doha
tower, sierra sinn, lazaro
eiffel tower, tour eiffel,
snowboard,
sky, empire state building, luxor, eiffel, lighthouse, jump, adventure
eiffel tower, statue, eiffel, mole antoneliana, la
tour eiffel, londra, cctv
tower, big ben, calatrava,
tokyo tower
depardieu,
obama, freddie vs jason,
dragana, shocking, falco
craig, obama, barack
pharrell williams, 50
cent, barrack obama,
barrack obama, barack
obama, barack hussein
obama, barack obama,
james marsden, jay z,
being noisier and having less points per class). It can thus be understood that ANN is hard
to get to work well for ImageNet via the following analysis: if we retrieve the ﬁrst nearest
neighbor correctly only 50% of the time, and otherwise we get the second neighbor without
fail 100% of the time the p@1 degrades to 3.4%. For 25% instead of 50%, we get 2.8%. If
we only have a 25% chance of catching any particular neighbor (not just the ﬁrst) we get
2.0%. For a 10% chance instead, we get 1.4%. Retrieving the ﬁrst neighbor, at least on this
dataset, is very important for achieving good accuracy, a phenomena that has been observed
on other datasets, see e.g. Table 3 of Makadia et al. . As our feature space is high
dimensional it is not surprising we do not retrieve the exact neighbor often, in line with
previous literature, see e.g. Table 3 of Torralba et al. .
Ensemble models
Ensembles of models are known to provide a performance boost
 , so we linearly combined various pre-trained WSABIE annotation models
with different embedding sizes. We estimated the weights of the linear combination using
the validation set in order to minimize the overall cost. Table 7 shows a summary of the
Mach Learn 81: 21–35
results. In fact, ensembles of our model do give an impressive boost in performance. Combining 100, 200 and 300 dimensional models provides the best overall performance, while
still yielding a scalable solution, both in terms of memory and time.
7 Conclusions
We have introduced a scalable model for image annotation based upon learning a joint representation of images and annotations that optimizes top-of-the-list ranking measures.
To our knowledge this is the ﬁrst data analysis of image annotation (considering all
classes at once, with millions of data points) at such a scale. We have shown that our embedding model trained with the WARP loss is faster, takes less memory, and yields better
performance than any known competing approach for this task. Stochastic gradient descent
is the standard method for optimizing non-convex models such as our embedding model,
but SGD applied to the loss function of interest is too slow, and the novelty of our training
algorithm is to use an approximation of the rank, which is otherwise slow to compute, via a
sampling trick that makes such optimization feasible for the ﬁrst time. In fact, to the best of
our knowledge this is the largest scale optimization of a rank-dependent measure attempting
to maximize the precision at the top positions of the ranking reported on any dataset (not
just for image annotation).
Finally, evaluation using the novel sibling precision metric shows that our embedding
method learns more of the semantic structure of the annotation space than competing methods, which we believe helps lead to its good performance in other metrics.