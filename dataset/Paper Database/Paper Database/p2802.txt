Proceedings of the SIGDIAL 2017 Conference, pages 201–206,
Saarbr¨ucken, Germany, 15-17 August 2017. c⃝2017 Association for Computational Linguistics
The E2E Dataset: New Challenges For End-to-End Generation
Jekaterina Novikova, Ondˇrej Duˇsek and Verena Rieser
School of Mathematical and Computer Sciences
Heriot-Watt University, Edinburgh
j.novikova, o.dusek, 
This paper describes the E2E data, a
new dataset for training end-to-end, datadriven natural language generation systems in the restaurant domain, which is
ten times bigger than existing, frequently
used datasets in this area. The E2E dataset
poses new challenges: (1) its human reference texts show more lexical richness and
syntactic variation, including discourse
phenomena; (2) generating from this set
requires content selection. As such, learning from this dataset promises more natural, varied and less template-like system
utterances. We also establish a baseline on
this dataset, which illustrates some of the
difﬁculties associated with this data.
Introduction
The natural language generation (NLG) component of a spoken dialogue system typically has
to be re-developed for every new application domain. Recent end-to-end, data-driven NLG systems, however, promise rapid development of
NLG components in new domains: They jointly
learn sentence planning and surface realisation
from non-aligned data . These approaches
do not require costly semantic alignment between
meaning representations (MRs) and the corresponding natural language (NL) reference texts
(also referred to as “ground truths” or “targets”),
but they are trained on parallel datasets, which
can be collected in sufﬁcient quality and quantity using effective crowdsourcing techniques, e.g.
 . So far, end-to-end approaches to NLG are limited to small, delexi-
NL reference
name[Loch Fyne],
eatType[restaurant],
food[French],
priceRange[less than £20],
familyFriendly[yes]
Loch Fyne is a family-friendly
restaurant providing wine and
cheese at a low cost.
Loch Fyne is a French family
friendly restaurant catering to
a budget of below £20.
Loch Fyne is a French
restaurant with a family setting
and perfect on the wallet.
Table 1: An example of a data instance.
calised datasets, e.g. BAGEL , SF Hotels/Restaurants ,
or RoboCup . Therefore, end-to-end methods have not been able to
replicate the rich dialogue and discourse phenomena targeted by previous rule-based and statistical
approaches for language generation in dialogue,
e.g. .
In this paper, we describe a new crowdsourced
dataset of 50k instances in the restaurant domain
(see Section 2).
We analyse it following the
methodology proposed by Perez-Beltrachini and
Gardent and show that the dataset brings
additional challenges, such as open vocabulary,
complex syntactic structures and diverse discourse
phenomena, as described in Section 3. The data
is openly released as part of the E2E NLG challenge.1 We establish a baseline on the dataset in
Section 4, using one of the previous end-to-end
approaches.
The E2E Dataset
Flower platform and quality-controlled following
Novikova et al. . The dataset provides infor-
1 
InteractionLab/E2E/
Figure 1: Pictorial MR for Table 1.
Example value
verbatim string
The Eagle, ...
dictionary
restaurant, pub, ...
familyFriendly
priceRange
dictionary
cheap, expensive, ...
dictionary
French, Italian, ...
verbatim string
market square, ...
dictionary
riverside, city center, ...
customerRating
enumerable
1 of 5 (low), 4 of 5 (high), ...
Table 2: Domain ontology of the E2E dataset.
mation about restaurants and consists of more than
50k combinations of a dialogue-act-based MR and
8.1 references on average, as shown in Table 1.
The dataset is split into training, validation and
testing sets (in a 76.5-8.5-15 ratio), keeping a similar distribution of MR and reference text lengths
and ensuring that MRs in different sets are distinct.
Each MR consists of 3–8 attributes (slots), such as
name, food or area, and their values. A detailed
ontology of all attributes and values is provided
in Table 2. Following Novikova et al. , the
E2E data was collected using pictures as stimuli
(see example in Figure 1), which was shown to
elicit signiﬁcantly more natural, more informative,
and better phrased human references than textual
Challenges
Following Perez-Beltrachini and Gardent ,
we describe several different dimensions of our
dataset and compare them to the BAGEL and SF
Restaurants (SFRest) datasets, which use the same
Table 3 summarises the main descriptive
statistics of all three datasets. The E2E dataset
is signiﬁcantly larger than the other sets in terms
of instances, unique MRs, and average number
of human references per MR (Refs/MR).2 While
having more data with a higher number of references per MR makes the E2E data more attractive for statistical approaches, it is also more challenging than previous sets as it uses a larger number of sentences in NL references (Sents/Ref; up
to 6 in our dataset compared to typical 1–2 for
other sets) and a larger number of slot-value pairs
in MRs (Slots/MR). It also contains sentences of
about double the word length (W/Ref) and longer
sentences in references (W/Sent).
Lexical Richness:
We used the Lexical Complexity Analyser to measure various dimensions of lexical richness, as shown in Table 4.
We complement the traditional measure of lexical
diversity type-token ratio (TTR) with the more robust measure of mean segmental TTR (MSTTR)
 , which divides the corpus into successive segments of a given length and then calculates
the average TTR of all segments. The higher the
value of MSTTR, the more diverse is the measured
text. Table 4 shows our dataset has the highest
MSTTR value (0.75) while Bagel has the lowest
one (0.41). In addition, we measure lexical sophistication (LS), also known as lexical rareness,
which is calculated as the proportion of lexical
word types not on the list of 2,000 most frequent
words generated from the British National Corpus. Table 4 shows that our dataset contains about
15% more infrequent words compared to the other
We also investigate the distribution of the top 25
most frequent bigrams and trigrams in our dataset
(see Figure 2).
The majority of both trigrams
(61%) and bigrams (50%) is only used once in
the dataset, which creates a challenge to efﬁciently
train on this data. Bigrams used more than once
in the dataset have an average frequency of 54.4
(SD = 433.1), and the average frequency of trigrams used more than once is 19.9 (SD = 136.9).
For comparison, neither SFRest nor Bagel dataset
contains bigrams or trigrams that are only used
once. The minimal frequency of bigrams is 27 for
Bagel (Mean = 98.2, SD = 86.9) and 76 for SFrest
(Mean = 128.4, SD = 50.5), for trigrams the minimal frequency is 24 for Bagel (Mean = 63.5, SD
= 54.6) and 43 for SFRest (Mean = 67.3, SD =
18.9). Infrequent words and phrases pose a chal-
2Note that the difference is even bigger in practice as the
Refs/MR ratio for the SFRest dataset is skewed: for speciﬁc
MRs, e.g. goodbye, SFRest has up to 101 references.
unique MRs
1.82 (1–101)
1.05 (1–4)
1.02 (1–2)
Table 3: Descriptive statistics of linguistic and computational adequacy of datasets.
No. of instances is the total number of instances in the dataset, No. of unique MRs is the number of distinct MRs, Refs/MR is
the number of NL references per one MR (average and extremes shown), Slots/MR is the average number of slot-value pairs
per MR, W/Ref is the average number of words per MR, W/Sent is the average number of words per single sentence, Sents/Ref
is the number of NL sentences per MR (average and extremes shown).
Figure 2: Distribution of the top 25 most frequent bigrams and trigrams in our dataset (left: most frequent
bigrams, right: most frequent trigrams).
Table 4: Lexical Sophistication (LS) and Mean
Segmental Type-Token Ratio (MSTTR).
lenge to current end-to-end generators since they
cannot handle out-of-vocabulary words.
Syntactic Variation and Discourse Phenomena:
We used the D-Level Analyser to evaluate syntactic variation and complexity of human
references using the revised D-Level Scale . Figure 3 show a similar syntactic variation in all three datasets. Most references in all the
datasets are simple sentences (levels 0 and 1), although the proportion of simple texts is the lowest for the E2E NLG dataset (46%) compared
to others (47-51%).
Examples of simple sentences in our dataset include: “The Vaults is an
Indian restaurant”, or “The Loch Fyne is a moderate priced family restaurant”. The majority of
our data, however, contains more complex, varied syntactic structures, including phenomena explicitly modelled by early statistical approaches
 . For ex-
Figure 3: D-Level sentence distribution of the
datasets under comparison.
ample, clauses may be joined by a coordinating
conjunction (level 2), e.g. “Cocum is a very expensive restaurant but the quality is great”. There
are 14% of level-2 sentences in our dataset, comparing to 7-9% in others.
Sentences may also
contain verbal gerund (-ing) phrases (level 4), either in addition to previously discussed structures
or separately, e.g. “The coffee shop Wildwood
has fairly priced food, while being in the same
vicinity as the Ranch” or “The Vaults is a familyfriendly restaurant offering fast food at moderate
prices”. Subordinate clauses are marked as level 5,
e.g. “If you like Japanese food, try the Vaults”.
The highest levels of syntactic complexity involve
Table 5: Match between MRs and NL references.
O: Omitted content, A: Additional content, C: Content fully
covered in the reference.
sentences containing referring expressions (“The
Golden Curry provides Chinese food in the high
price range.
It is near the Bakers”), non-ﬁnite
clauses in adjunct position (“Serving cheap English food, as well as having a coffee shop, the
Golden Palace has an average customer rating and
is located along the riverside”) or sentences with
multiple structures from previous levels. All the
datasets contain 13-16% of sentences of levels 6
and 7, where Bagel has the lowest proportion
(13%) and our dataset the highest (16%).
Content Selection:
In contrast to the other
datasets, our crowd workers were asked to verbalise all the useful information from the MR
and were allowed to skip an attribute value considered unimportant.
This feature makes generating text from our dataset more challenging
as NLG systems also need to learn which content to realise. In order to measure the extent of
this phenomenon, we examined a random sample of 50 MR-reference pairs. An MR-reference
pair was considered a fully covered (C) match
if all attribute values present in the MR are verbalised in the NL reference.
It was marked as
“additional” (A) if the reference contains information not present in the MR and as “omitted”
(O) if the MR contains information not present
in the reference, see Table 5.
40% of our data
contains either additional or omitted information.
This often concerns the attribute-value pair eat-
Type=restaurant, which is either omitted (“Loch
Fyne provides French food near The Rice Boat. It
is located in riverside and has a low customer rating”) or added in case eatType is absent from the
MR (“Loch Fyne is a low-rating riverside French
restaurant near The Rice Boat”).
Baseline System Performance
To establish a baseline on the task data, we use
TGen , one of the re-
BLEU 
NIST 
METEOR 
ROUGE-L 
CIDEr 
Table 6: TGen results on the development set.
cent E2E data-driven systems.3
TGen is based
on sequence-to-sequence modelling with attention
(seq2seq) .
In addition
to the standard seq2seq model, TGen uses beam
search for decoding and a reranker over the top k
outputs, penalizing those outputs that do not verbalize all attributes from the input MR. As TGen
does not handle unknown vocabulary well, the
sparsely occurring string attributes (see Table 2)
name and near are delexicalized – replaced with
placeholders during generation time (both in input
MRs and training sentences).4
We evaluated TGen on the development part of
the E2E set using several automatic metrics. The
results are shown in Table 6.5 Despite the greater
variety of our dataset as shown in Section 3, the
BLEU score achieved by TGen is in the same
range as scores reached by the same system for
BAGEL (0.6276) and SFRest (0.7270). This indicates that the size of our dataset and the increased
number of human references per MR helps statistical approaches.
Based on cursory checks, generator outputs
seem mostly ﬂuent and relevant to the input MR.
For example, our setup was able to generate long,
multi-sentence output, including referring expressions and ellipsis, as illustrated by the following
example: “Browns Cambridge is a family-friendly
coffee shop that serves French food. It has a low
customer rating and is located in the riverside area
near Crowne Plaza Hotel.”
However, TGen requires delexicalization and does not learn content
selection, forcing the verbalization of all MR attributes.
3TGen is freely available at 
UFAL-DSG/tgen.
4Detailed system training parameters are given in the supplementary material.
5To measure the scores, we used slightly adapted versions
of the ofﬁcial MT-Eval script (BLEU, NIST) and the COCO
Caption metrics (METEOR, ROUGE-
L, CIDEr). All evaluation scripts used here are available at
 
Conclusion
We described the E2E dataset for end-to-end,
statistical natural language generation systems.
While this dataset is ten times bigger than similar,
frequently used datasets, it also poses new challenges given its lexical richness, syntactic complexity and discourse phenomena. Moreover, generating from this set also involves content selection. In contrast to previous datasets, the E2E data
is crowdsourced using pictorial stimuli, which
was shown to elicit more natural, more informative and better phrased human references than
textual meaning representations . As such, learning from this data promises
more natural and varied outputs than previous
“template-like” datasets.
The dataset is freely
available as part of the E2E NLG Shared Task.6
In future work, we hope to collect data with further increased complexity, e.g. asking the user to
compare, summarise, or recommend restaurants,
in order to replicate previous rule-based and statistical approaches, e.g. . In addition, we will experiment with
collecting NLG data within a dialogue context,
following , in order to
model discourse phenomena across multiple turns.
Acknowledgements
This research received funding from the EPSRC
projects DILiGENt (EP/M005429/1) and MaDrIgAL (EP/N017536/1). The Titan Xp used for this
research was donated by the NVIDIA Corporation.