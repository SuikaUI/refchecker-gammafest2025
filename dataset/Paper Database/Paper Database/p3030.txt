Journal of the Physical Society of Japan
Detection of phase transition via convolutional neural networks
Akinori Tanaka1 ∗and Akio Tomiya2 †
1Interdisciplinary Mathematical and Computational Collaboration Team, RIKEN, Wako 351-0198, Japan
2Key Laboratory of Quark & Lepton Physics (MOE) and Institute of Particle Physics, Central China Normal
University, Wuhan 430079, China
A Convolutional Neural Network (CNN) is designed to study correlation between the temperature and the spin con-
ﬁguration of the 2 dimensional Ising model. Our CNN is able to ﬁnd the characteristic feature of the phase transition
without prior knowledge. Also a novel order parameter on the basis of the CNN is introduced to identify the location of
the critical temperature; the result is found to be consistent with the exact value.
Studies of phase transition are connected to various areas
among theoretical/experimental physics.1–7) Calculating order
parameters is one of the conventional ways to deﬁne phases
and phase transitions. However, some phases like topological
phases8) do not have any clear order parameters. Even if there
are certain theoretical order parameters like entanglement entropy,9,10) they are diﬃcult to measure in experiments.
Machine learning (ML) techniques are useful to resolve
this undesirable situation. In fact, ML techniques have been
already applied to various problems in theoretical physics:
ﬁnding approximate potential surface,11) a study of transition in glassy liquids,12) solving mean-ﬁeld equations14) and
quantum many-body systems,13,15) a study of topological
phases.16)
Especially, ML techniques based on convolutional neural network (CNN) have been developing since the recent
groundbreaking record17) in ImageNet Large Scale Visual
Recognition Challenge 2012 (ILSVRC2012),18) and it is applied to investigate phases of matters with great successes
on classiﬁcations of phases in 2D systems19–21) and 3D systems.22,23) It is even possible to draw phase diagrams.23)
In these previous works, however, one needs some informations of the answers for the problems a priori. For example, to
classify phases of a system, the training process requires the
values of critical temperatures or the location of phase boundaries. This fact prevents applications of the ML techniques to
unknown systems so far.
The learning process without any answers is called unsupervised learning. Indeed, there are known results on detecting the phase transitions based on typical unsupervised learning architectures called autoencoder which is equivalent to
principal component analysis32) and its variant called variational autoencoder.?) These architectures encode informations of given samples to lower dimensional vectors, and it
is pointed out that such encoding process is similar to encoding physical state informations to order parameters of the systems. However, it is not evident whether the latent variables
provide the critical temperature.
We propose a novel but simple prescription to estimate the
critical temperature of the system via neural network (NN)
based on ML techniques without a priori knowledge of the
order parameter. Throughout this letter, we focus on the fer-
∗ 
† 
‘(Color online)’ Plots of weight matrix components in convolutional neural network (left) and fully connected neural network (right). Horizontal axis corresponds to the inverse temperature β. Vertical axis m corresponds to components connected to hidden nodes in NN.
System size
Critical temperature of 2D Ising model on the square lattice extracted from CNN (3 from top). L →∞stands for thermodynamic limit, and
its value is exact one, βExact
romagnetic 2D Ising model on a square lattice mainly based
on the following three reasons. First, this system is one of the
simplest solvable systems24,25) which has a phase transition,
and it is easy to check the validity of our method. Second, this
system can be generalized to other classical spin systems like
Potts model, XY model and Heisenberg model, so our method
can be applied to these other systems straightforwardly. Third,
this model is a good benchmark for new computational methods.26,27)
Using TensorFlow (r0.9),28) we have implemented a
neural network to study the correlation between spin conﬁgurations and discretized inverse temperatures. We ﬁnd that NNs
are able to capture features of phase transition, even for simpler fully-connected (FC) NN, in the weight W without any
information of order parameters nor critical temperature. Fig.
1 reminds us of the discovery of the “cat cell” in the literature29) in which the model recognizes images of cats without
having explicitly learned what a cat is.
We examine the boundary structure in Fig. 1 by deﬁning
order parameter, and estimate the inverse critical temperature
by ﬁtting distribution of the order parameter (Table I).
First, we explain the details of our NNs If the reader is
 
J. Phys. Soc. Jpn.
not familiar with machine learning based on NN, we suggest
reading literatures reviewing it, e.g.33,34) Our NN model is designed to solve classiﬁcation problems. It is constructed from
a convolution layer and a fully connected layer, so it can be
regarded as the simplest model of a CNN (Fig. 2). The whole
deﬁnition is as follows.

Ising conﬁg on L × L lattice.

Convolution[N2
f -ﬁlter, (s,s)-stride, C-channels]
ReLU activation
RHidden = L2/s2×C
( Fully connected
 N = O

The ﬁrst transformation in (1) is deﬁned by convolution including training parameters Fa
ij called ﬁlters, rectiﬁed linear
activation called ReLU, and ﬂatten procedure:
i, j=1 σ(sX+i)(sY+j)Fa
→max(0, Σa
11, . . . uC
21, . . . , uC
21, . . . ] = [um].
The second transformation is deﬁned by fully-connected layer
including training parameters Wm
I called weights, and softmax
activation:
fully-connected
J=1 ezJ = βCNN
We classify conﬁgurations into N classes labeled by I in
the ﬁnal step. This N is related to the inverse temperature β
through (7). Because of the following facts, βCNN
∈ and
= 1, we can interpret βCNN
as a probability for classifying given state {σxy} to I-th class in a classiﬁcation problem.
In total, we have two types of parameters:
i j in convolution,
I in fully connected layer.
" a = 1, . . . ,C
i, j = 1, . . . , Nf
" m = 1, . . . , L2/s2 × C
I = 1, . . . N
In later experiment, these parameters will be updated. In the
ﬁrst trial, we take NN without convolution (12). In this case,
the parameters F are not the ﬁlters in (4) but weights.
We need training set for optimizing the above parameters
(4) in CNN. We call it TL where L indicates the size of the
square lattice. The deﬁnition is
= Tmin + nδ
n=0,...,(Nconf−1)
where {σ(n)
xy } is the generated conﬁguration under the Ising
Hamiltonian on the square lattice
σ(x+1),y + σx,(y+1)
and inverse temperature βn using the Metropolis method.
Tmin (max) is the minimum (maximum) temperature for the
‘(Color online)’ A schematic explanation for the network (1). In
this ﬁgure, we take 3 ﬁlters Fa (a = 1, 2, 3). An Ising spin conﬁguration
{σxy} generated by MCMC is passed to 3 hidden lattices via the convolution
process + ReLU activation. After making them ﬂatten, they are connected to
N nodes via fully-connected weight W. In the end, it returns βCNN
softmax activation.
target Ising system. The temperature resolution is deﬁned by
δ = (Tmax −Tmin)/Nconf where Nconf is the number of samples.
⃗β is the discretized inverse temperature deﬁned by
clN : β →⃗β =

(1, 0, . . . , 0, 0)
(0, 1, . . . , 0, 0)
for β ∈[0,
(0, 0, . . . , 1, 0)
for β ∈[ N−3
(0, 0, . . . , 0, 1)
This is called the one-hot representation, and enables us to
implement the inverse temperature β into the NN directly. We
use index I or J to represent component of the vector ⃗β as
already used in .
Now let us denote our CNN, explained in (1) as F(F,W)
need error function that measures the diﬀerence between the
output of the CNN and the correct discretized inverse temperature. In our case, the task is classiﬁcation, so we take cross
entropy as the error function:
E(⃗βCNN, ⃗β) = −
βI log βCNN
where ⃗βCNN = F(F,W)
CNN ({σxy}) and ({σxy}, ⃗β) ∈TL.
Roughly speaking, the parameters w
updated via w
w −ϵ∇wE with small parameter ϵ.
More precisely speaking, we adopt a sophisticated version
of this method called Adam30) implemented in TensorFlow
(r0.9)28) to achieve better convergence.
Our neural network, F(F,W)
CNN , learns the optimal parameters
i j} and W = {Wm
I } in (4) through iterating the optimization of the cross entropy (8) between the answer ⃗β and
the output ⃗βCNN constructed from stochastically chosen data
({σxy}, ⃗β) ∈TL.
Require: CNN F(F,W)
CNN (1); an Ising dataset TL (5)
for Num of iterations do
Choose ({σxy}, ⃗β) ∈TL randomly
⃗βCNN = F(F,W)
CNN ({σxy})
loss = E(⃗βCNN, ⃗β)
Update F, W via AdamOptimizer(loss)
As we show later, the weight matrix W inheres well approximated critical temperature after 10,000 iterations in (9).
Here, we prepare TL (5) by using the Metropolis method
J. Phys. Soc. Jpn.
‘(Color online)’ Heat maps of Wm
I and Fij for the CNN with one
ﬁlter. In case (A), there always exist two distinct regions (black and gray). In
case (B), there is no such clear decomposition.
with the parameters
Tmin = 0.1, Tmax = 5.0, Nconf = 104.
The max and min values for T mean that 0.2 < β < 10, where
β is the inverse temperature. Note that the known value form
the phase transition is Tc ∼2.27 or βc ∼0.44. This means that
conﬁgurations in our training data TL extend from the ordered
phase to the disordered phase. In all, we prepare three types
of training set,
TL=8, TL=16, TL=32.
We apply negative magnetic ﬁeld weakly to realize the unique
ground state at zero temperature. As a result, almost all con-
ﬁgurations at low temperature (β ≫βc ∼0.44) are ordered to
{σxy} = {−1, −1, . . . , −1}.
Before showing the CNN result, let us try a somewhat
primitive experiment: training on NN without the convolution
layer, i.e. a fully connected NN.

I = {σxy| Ising conﬁg on L × L lattice.}
Fully connected F
 Hidden=80
( Fully connected W
 N=100 = O

We retain the error function and optimizer, i.e. the cross entropy (8) and AdamOptimizer(10−4). We align the heat map
for the weight W trained by using TL=8 in right side of Fig.
1. After 10,000 iterations, NN does detected the phase transition. So this NN is suﬃcient for detecting ising phase transition, but we cannot answer why this NN captures it. To answer
it, we turn to our main target: CNN below.
Next, we take N = 100, N f = 3 and C = 1 and use the
TL=8 training set. Once we increase the number of iterations
to 10,000, we get two possible ordered ﬁgures. We denote
them case (A) and case (B) respectively as shown in Fig. 3.
Case (A) is characterized by P
ij Fij > 0, and we can observe two qualitatively diﬀerent regions in the heat map of the
weight W, black colored region (0.48 ≲β) and gray colored
‘(Color online)’ Heat maps of Wm
ij with ﬁve ﬁlters.
region (β ≲0.48). The boundary is close to the critical temperature βc ∼0.44. Case (B) is characterized by P
i j Fi j < 0,
and values in the heat map for W are in gray colored region
and almost homogeneous. We will discuss later the reason
why only case (A) displays phase transition.
We now turn to the multi-ﬁlter case with N = 100, Nf =
3,C = 5 and L = 8. The results for all heat maps after 10,000
iterations are shown in Fig. 4. The stripe structure in the heat
map of W corresponds to its values connecting to the convoluted and ﬂatten nodes via ﬁve ﬁlters, (A), (A), (B), (B),
(B) respectively. Empirically speaking, the number of ﬁlters
should be large to detect the phase transition because the probability for appearance of (A) increases with increased statistics.
From the experiments, we know that our model (1) seems
to discover the phase transition in the given training data for
the Ising model. In order to verify this statement, we would
like to extract the critical temperature from our CNN after
the training. As a trial, we ﬁx the parameters of the CNN as
follows: the number of ﬁlters, channel and stride are N f = 3,
C = 5 and s = L/4 respectively. The number of classiﬁcations
is taken as N = 100 as well as we did in previous section.
First, we plot heat maps for the weight matrix W in CNN
trained by TL=8, TL=16, TL=32. For every lattice size, we observe a domain-like structure with a boundary around β ∼
0.44. However, the heat map does not give the location of
the boundary quantitatively. We propose an order parameter
based on the weight matrix Wm
and estimate the critical temperature. The result is shown in
Fig. 5. To quantify the boundary in the heat map for W, we
deﬁne critical temperature extracted by CNN βCNN by ﬁtting
Wsum(β) with the following function: ˜Wsum(β) = a tanh(c(β −
βCNN)) −b, where a, b, c and βCNN are ﬁtting variables and
βCNN indicates the location of the jump. This function is motivated by the magnetization of the Ising model using the mean
ﬁeld approximation. Table I shows the ﬁt results both for CNN
and FC. Our results show that βCNN matches the critical temperature to 2 – 8 % accuracy. Compared to it, βFC shows less
Let us conclude this letter. We have designed simple neural networks to study correlation between conﬁguration of the
2D Ising model and inverse temperature, and we have trained
them by SGD method implemented by TensorFlow. We have
found that the weight W in neural networks captures a feature
J. Phys. Soc. Jpn.
Wsum, L=32
Wsum, βc = 0.451887
‘(Color online)’ Wsum(β) for L = 32. The horizontal axis is the inverse temperature β which is translated using (7). We ﬁt Wsum(β) data by
the following smooth function: a tanh[c(β −βCNN)] −b (Green curve), where
a, b, c, βCNN are ﬁtting parameters, and regard the determined ﬁtting parameter βCNN as a critical temperature determined by CNN.
of phase transition of the 2D Ising model, and deﬁned a new
order parameter Wsum(β) in (13) via trained neural networks
and have found that it can provide the value of critical inverse
temperature.
Why are our neural networks able to ﬁnd a feature of phase
transition? There is an intuitive explanation thanks to CNN
experiments. The ﬁlter with Nf = 3 in case (A) has a typical
average around 0.1 ∼0.2. This is close to the convolution with
ﬁlter Fi j = 1/N2
f which is equivalent to a real space renormalization group transformation, and the ﬁlters reﬂect local magnetization which is related to a typical order parameter and
it enables CNN to detect the phase transition. As an analog
of this, FC NN might realize the real space renormalization
group transformation in inside.
Our NN model has potential to investigate other statistical
models. For example, it was reported that CNNs can distinguish phases of matters, topological phases in Z2 gauge theories,19) phases in the Hubbard model22) and Potts model.31)
It is interesting to apply our design of neural networks to
these problems and see whether the NN can discover nontrivial phases automatically, as we did in this letter.
Acknowledgment
We would like to thank to K. Doya, K. Hashimoto, T.
Hatsuda, Y. Hidaka, M. Hongo, B. H. Kim, J. Miller, S. Nagataki, N. Ogawa,
M. Taki and Y. Yokokura for constructive comments and warm encouragement. We also thank to D. Zaslavsky for careful reading this manuscript. The
work of Akinori Tanaka was supported in part by the RIKEN iTHES Project.
The work of Akio Tomiya was supported in part by NSFC under grant no.
1) Kenneth G Wilson and John Kogut. The renormalization group and the
ϵ expansion. Physics Reports, Vol. 12, No. 2, pp. 75–199, 1974.
2) Joseph Polchinski. Eﬀective ﬁeld theory and the fermi surface. arXiv
 
3) K Intriligator and N Seiberg. Lectures on supersymmetric gauge theories and electric-magnetic duality. Nuclear Physics B-Proceedings Supplements, Vol. 45, No. 2-3, pp. 1–28, 1996.
4) Roman Pasechnik and Michal ˇSumbera. Phenomenological review
on quark-gluon plasma: concepts vs observations. arXiv preprint
 
5) PC Hohenberg and AP Krekhov. An introduction to the ginzburg–
landau theory of phase transitions and nonequilibrium patterns. Physics
Reports, Vol. 572, pp. 1–42, 2015.
6) Qijin Chen, Jelena Stajic, Shina Tan, and Kathryn Levin. Bcs–bec
crossover: From high temperature superconductors to ultracold super-
ﬂuids. Physics Reports, Vol. 412, No. 1, pp. 1–88, 2005.
7) Bernhard Kramer, Tomi Ohtsuki, and Stefan Kettemann. Random network models and quantum phase transitions in two dimensions. Physics
reports, Vol. 417, No. 5, pp. 211–342, 2005.
8) Xiao-Gang Wen. Topological orders in rigid states. International Journal of Modern Physics B, Vol. 4, No. 02, pp. 239–271, 1990.
9) Alexei Kitaev and John Preskill. Topological entanglement entropy.
Physical review letters, Vol. 96, No. 11, p. 110404, 2006.
10) Michael Levin and Xiao-Gang Wen. Detecting topological order in a
ground state wave function. Physical review letters, Vol. 96, No. 11, p.
110405, 2006.
11) J¨org Behler and Michele Parrinello. Generalized neural-network representation of high-dimensional potential-energy surfaces. Physical review letters, Vol. 98, No. 14, p. 146401, 2007.
12) Samuel S Schoenholz, Ekin D Cubuk, Daniel M Sussman, Efthimios
Kaxiras, and Andrea J Liu. A structural approach to relaxation in glassy
liquids. Nature Physics, 2016.
13) Louis-Franc¸ois Arsenault, Alejandro Lopez-Bezanilla, O Anatole von
Lilienfeld, and Andrew J Millis. Machine learning for many-body
physics: The case of the anderson impurity model. Physical Review B,
Vol. 90, No. 15, p. 155136, 2014.
14) Louis-Franc¸ois Arsenault, O Anatole von Lilienfeld, and Andrew J Millis. Machine learning for many-body physics: eﬃcient solution of dynamical mean-ﬁeld theory. arXiv preprint arXiv:1506.08858, 2015.
15) Giuseppe Carleo and Matthias Troyer. Solving the quantum manybody
networks. arXiv
 
16) Dong-Ling Deng, Xiaopeng Li, and S Das Sarma. Exact machine learning topological states. arXiv preprint arXiv:1609.09060, 2016.
17) Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.
18) Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev
Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla,
Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large
Scale Visual Recognition Challenge.International Journal of Computer
Vision (IJCV), Vol. 115, No. 3, pp. 211–252, 2015.
19) Juan Carrasquilla and Roger G Melko. Machine learning phases of matter. arXiv preprint arXiv:1605.01735, 2016.
20) Peter Broecker, Juan Carrasquilla, Roger G Melko, and Simon Trebst.
Machine learning quantum phases of matter beyond the fermion sign
problem. arXiv preprint arXiv:1608.07848, 2016.
21) Tomoki Ohtsuki and Tomi Ohtsuki. Deep learning the quantum phase
transitions in random two-dimensional electron systems. Journal of the
Physical Society of Japan, Vol. 85, No. 12, p. 123706, 2016.
22) Kelvin Ch’ng, Juan Carrasquilla, Roger G Melko, and Ehsan Khatami.
Machine learning phases of strongly correlated fermions.arXiv preprint
 
23) Tomi Ohtsuki and Tomoki Ohtsuki. Deep learning the quantum phase
transitions in random electron systems: Applications to three dimensions. arXiv preprint arXiv:1612.04909, 2016.
24) Lars Onsager. Crystal statistics. i. a two-dimensional model with an
order-disorder transition. Physical Review, Vol. 65, No. 3-4, p. 117,
25) Yˆoichirˆo Nambu. A note on the eigenvalue problem in crystal statistics.
Broken Symmetry: Selected Papers of Y Nambu, Vol. 13, No. 1, p. 1,
26) Pankaj Mehta and David J Schwab. An exact mapping between the
variational renormalization group and deep learning. arXiv preprint
 
27) Ken-Ichi Aoki, Tamao Kobayashi, and Hiroshi Tomita. Domain wall
renormalization group analysis of two-dimensional ising model. International Journal of Modern Physics B, Vol. 23, No. 18, pp. 3739–3751,
28) Martın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng
Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeﬀrey Dean, Matthieu
Devin, et al. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
29) Quoc V Le. Building high-level features using large scale unsupervised
learning. In 2013 IEEE international conference on acoustics, speech
J. Phys. Soc. Jpn.
and signal processing, pp. 8595–8598. IEEE, 2013.
30) Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
31) Chian-De Li, Deng-Ruei Tan, and Fu-Jiun Jiang. Applications of neural
networks to the studies of phase transitions of two-dimensional potts
models. arXiv preprint arXiv:1703.02369, 2017.
32) Lei Wang. Discovering phase transitions with unsupervised learning.
arXiv preprint arXiv:1606.00318, 2016.
33) Hermann Kolanoski.Application of artiﬁcial neural networks in particle
physics. In International Conference on Artiﬁcial Neural Networks, pp.
1–14. Springer, 1996.
34) Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. Nature, Vol. 521, No. 7553, pp. 436–444, 2015.