Forecasting economic and ﬁnancial time-series
with non-linear models
Michael P. Clements
Department of Economics,
University of Warwick
Philip Hans Franses
Econometric Institute,
Erasmus University Rotterdam.
Norman R. Swanson∗
Department of Economics,
Rutgers University.
October 1, 2003
In this paper we discuss the current state-of-the-art in estimating, evaluating, and selecting among non-linear forecasting models for economic and ﬁnancial time series. We review
theoretical and empirical issues, including predictive density, interval and point evaluation
and model selection, loss functions, data-mining, and aggregation. In addition, we argue
that although the evidence in favor of constructing forecasts using non-linear models is
rather sparse, there is reason to be optimistic. However, much remains to be done. Finally,
we outline a variety of topics for future research, and discuss a number of areas which have
received considerable attention in the recent literature, but where many questions remain.
∗Corresponding Author: Norman R Swanson, Department of Economics, Rutgers University, 75 Hamilton
Street, New Brunswick, NJ, USA. The authors wish to thank Valentina Corradi and Dick van Dijk for helpful
conversations, and are grateful to Jan De Gooijer for reading and commenting on the manuscript. Swanson
gratefully acknowledges ﬁnancial support from Rutgers University in the form of a Research Council grant.
Introduction
Whilst non-linear models are often used for a variety of purposes, one of their prime uses is for
forecasting, and it is in terms of their forecasting performance that they are most often judged.
However, a casual review of the literature suggests that often the forecasting performance of
such models is not particularly good. Some studies ﬁnd in favour, but equally there are studies
in which their added complexity relative to rival linear models does not result in the expected
gains in terms of forecast accuracy. Just over a decade ago, in their review of non-linear time
series models, De Gooijer and Kumar concluded that there was no clear evidence in
favour of non-linear over linear models in terms of forecast performance, and we suspect that
the situation has not changed very much since then. It seems that we have not come very far
in the area of non-linear forecast model construction.
We argue that the relatively poor forecasting performance of non-linear models calls for
substantive further research in this area, given that one might feel uncomfortable asserting that
non-linearities are unimportant in describing economic and ﬁnancial phenomena. The problem
may simply be that our non-linear models are not mimicing reality any better than simpler
linear approximations, and in the next section we discuss this and related reasons why a good
forecast performance ‘across the board’ may constitute something of a ‘holy grail’ for non-linear
We discuss the current state-of-the-art in non-linear modelling and forecasting, with particular emphasis placed on outlining a number of open issues. The topics we focus on include
joint and conditional predictive density evaluation, loss functions, estimation and speciﬁcation,
and data-mining, amongst others. As such, this paper complements the rest of the papers in
this special issue of the International Journal of Forecasting.
The rest of the paper is organized as follows. In Section 2 we discuss why one might want
to consider non-linear models, and a number of reasons why their forecasting ability relative to
linear models may not be as good as expected. In Section 3 we discuss recent theoretical and
methodological issues to do with forecasting with non-linear models, many of which go beyond
the traditional preoccupation with point forecasts to consider the whole predictive density.
Section 4 highlights a number of empirical issues, and how these are dealt with in the papers
collected in this issue. Concluding remarks are gathered in Section 5.
Why consider non-linear models?
Many of us believe that linear models ought to be a relatively poor way of capturing certain
types of economic behaviour, or economic performance, at certain times. The obvious example
would be a linear (e.g., Box-Jenkins ARMA) model of output growth in a Western economy
subject to the business cycle, where the properties of output growth in recessions are in some
ways quite diﬀerent from expansions . Output growth non-linearities can be characterised by the presence of two or
more regimes (e.g. recessions and expansions), as can ﬁnancial variables (periods of high and
low volatility). Other types of non-linearity might include the possibility that the eﬀects of
shocks accumulate until a process “explodes” (self-exciting or catastrophic behaviour), as well
as the notion that some variables are relevant for forecasting only once in a while (e.g., only
when oil prices increase by a large amount do they have a signiﬁcant eﬀect on output growth,
and therefore become useful for forecasting output growth).
In macroeconomics and ﬁnance theory a host of non-linear models are already in vogue. For
example, almost all real business cycle models are highly non-linear. As well as which, bond
pricing models, diﬀusion processes describing yield curves, and almost all other continuous time
ﬁnance models are non-linear. The predominance of non-linear models in economics and ﬁnance
is not inconsistent with the use of linear models by the applied practitioner, as such models
can be viewed as reasonable approximations to the non-linear phenomenon of interest. Thus,
from the perspective of forecasting, there is ample reason to continue to look at non-linear
models. As our non-linear model estimation, selection and testing approaches become more
sophisticated, one might expect to see their forecast performance improve commensurately. It
is thus not surprising that non-linear models, ranging from regime-switching models, to neural
networks and genetic algorithms, are receiving a great deal of attention in the literature.
On a more cautionary note, we review a number of factors which might count against
the aforementioned improvement in the relative performance of non-linear models.
after De Gooijer and Kumar, Granger and Teräsvirta in their review of smooth transition autoregressive (STAR) models of US
industrial production, argue that the superior in-sample performance of such models will only
be matched out-of-sample if that period contains ‘non-linear features’. Similarly, Tong argues strongly that for non-linear models ‘how well we can predict depends on
where we are’ and that there are ‘windows of opportunity for substantial reduction in prediction
errors’. This suggests that an important aspect of an evaluation of the forecasts from non-linear
models relative to the linear AR models is to make the comparison in a way which highlights
the favourable performance of the former for certain states, especially if it is forecasts in those
states which are most valuable to the user of the forecasts. Clements and Smith compare
the forecasting performance of empirical self-exciting threshold autoregressive (SETAR) models
and AR models using simulation techniques which ensure that past non-linearities are present
in the forecast period. See Tiao and Tsay for a four-regime TAR model applied to
US GNP, and Boero and Marrocu for an application of regime-speciﬁc evaluation to
exchange rate forecasts.
In addition, in the context of exchange rate prediction, Diebold and Nason give a
number of reasons why non-linear models may fail to outperform linear models. One is that
apparent non-linearities detected by tests for linearity are due to outliers or structural breaks,
which cannot be readily exploited to improve out-of-sample performance, and may only be
detected by careful analysis along the lines of Koop and Potter , for example. They also
suggest that conditional-mean non-linearities may be a feature of the data generating process
(DGP), but may not be large enough to yield much of an improvement to forecasting, as well
as the explanation that they are present and important, but that the wrong types of non-linear
models have been used to try and capture them.
There is a view that, because some aspects of the economy or ﬁnancial markets do indeed
display non-linear behaviour, then neglecting these features in constructing forecasts would
leave the end-user uneasy, feeling that the forecasts are in some sense “second best” . This
follows from the belief that a good model for the in-sample data should also be a good out-ofsample forecasting model. As an example, an AR(2) model for US GNP growth may yield lower
average squared errors than an artiﬁcial neural network with one hidden unit, but, knowing that
the AR(2) model is incapable of capturing the distinct dynamics of expansions and contractions,
is unlikely to engender conﬁdence that one has a model that captures the true dynamics of the
economy. But this is the crux of the matter — a good in-sample ﬁt (here, capturing the distinct
dynamics of the business cycle phases) does not necessarily translate in to a good out-of-sample
performance relative to a model such as an AR. The sub-section below illustrates, and see also
Clements and Hendry for a more general discussion.
We take the view that, if one believes the underlying phenomenon is non-linear, it is worth
considering a non-linear model, but warn against the expectation that such models will always
do well — there are too many unknowns and the economic system is too complex to support the
belief that simply generalising a linear model in one (simple!) direction, such as adding another
regime, will necessarily improve matters. That said, it is natural to be unhappy with models
which are obviously deﬁcient in some respect, and to seek alternatives. The subsequent sections
of this paper review some of the recent developments in model selection and empirical strategies,
as well as the remaining papers in this issue, which take up the challenge of forecasting with
non-linear models.
We end this section with two short illustrations of some of the diﬃculties that can arise.
In the ﬁrst, there is distinct regime-switching behaviour, but this does not contribute to an
improved forecast performance.
In the second, we discuss the relationship between output
growth and the oil price.
Markov-switching models of US output growth
Clements and Krolzig present some theoretical explanations for why Markov-Switching
(MS) models may not forecast much better than AR models, and apply their analysis to post
War US output growth. The focus is on a two-regime Markov-Switching (MS) model:
(pq) =  (∆vq−1 −(pq−1)) + rq
where rq ∼FK[0
r]. The conditional mean (pq) switches between two states:
if pq = 1 ("expansion" or "boom")
if pq = 2 ("contraction" or "recession")
The description of a MS-AR model is completed by the speciﬁcation of a model for the stochastic
and unobservable regimes on which the parameters of the conditional process depend. Once a
law has been speciﬁed for the states pq, the evolution of regimes can be inferred from the data.
The regime-generating process is assumed to be an ergodic Markov chain with a ﬁnite number
of states pq = 1 2 (for a two-regime model), deﬁned by the transition probabilities:
mfg = Pr(pq+1 = g | pq = f)
∀f g ∈{1 2}
The model can be rewritten as the sum of two independent processes:
∆vq −v = q + wq
where v is the unconditional mean of ∆vq, such that B[q] = B[wq] = 0. While the process wq
is Gaussian:
wq = wq−1 + q
the other component, q, represents the contribution of the Markov chain:
q = (2 −1)q
where q = 1 −Pr(pq = 2) if pq = 2 and −Pr(pq = 2) otherwise. Pr(pq = 2) = m12.(m12 + m21) is
the unconditional probability of regime 2. Invoking the unrestricted VAR(1) representation of
a Markov chain:
q = (m11 + m22 −1)q−1 + sq
then predictions of the hidden Markov chain are given by:
bQ+e|Q = (m11 + m22 −1)ebQ|Q
where bQ|Q = B[Q |VQ ] = Pr(pQ = 2|VQ ) −Pr(pQ = 2) is the ﬁltered probability Pr(pQ = 2|VQ )
of being in regime 2 corrected for the unconditional probability. Thus, the conditional mean of
∆vQ+e is given by c
∆vQ+e|Q −v which equals:
bQ+e|Q + bwQ+eQ
(2 −1)(m11 + m22 −1)ebQ|Q + e h
∆vQ −v −(2 −1)bQ|Q
e ¡∆vQ −v
¢ + (2 −1)
(m11 + m22 −1)e −ei
The ﬁrst term in (4) is the optimal prediction rule for a linear model, and the contribution
of the Markov regime-switching structure is given by the term multiplied by bQ|Q , where bQ|Q
contains the information about the most recent regime at the time the forecast is made. Thus,
the contribution of the non-linear part of (4) to the overall forecast depends on both the
magnitude of the regime shifts, |2 −1|, and on the persistence of regime shifts m11 + m22 −1
relative to the persistence of the Gaussian process, given by .
Clements and Krolzig estimate m11+m22−1 = 065, and the largest root of the AR polynomial
to be 064, so that the second reason explains the success of the linear AR model in forecasting.
Since the predictive power of detected regime shifts is extremely small, m11 + m22 −1 '  in
(4), the conditional expectation collapses to a linear prediction rule. Heuristically, the relative
performance of non-linear regime-switching models would be expected to be better the more
persistent the regimes. When the regimes are unpredictable, we can do no better than employing
a simple linear model. See also Krolzig , and Dacco and Satchell for an analysis
of the eﬀects of wrongly classifying the regime the process will be in.
A number of authors, such as Sensier, Artis, Osborn and Birchenhall , attempt to
predict business cycle regimes, and the transition probabilities in (3) can be made to depend
on leading indicator variables, as a way of sharpening the forecasting ability of these models.
Franses, Paap and Vroomen utilise information from extraneous variables to determine
the regime in a novel approach to predicting the US unemployment rate.
Output growth and the oil price
Of obvious interest in the literature is the relationship between oil prices and the macroeconomy.
To what extent did the OPEC oil price rises contribute to the recessions in the 70’s, and
might one expect reductions in prices to stimulate growth, although perhaps to a lesser extent?
Hamilton originally proposed a linear relationship between oil prices and output growth
for the US. This was challenged by Mork , who suggested that the relationship was
asymmetric, in that output growth responds negatively to oil price increases, but is unaﬀected
by oil price declines. With the advantage of several more years of data, Hooker showed
that the linear relationship proposed by Hamilton appears not to hold from 1973 onwards
(the date of the ﬁrst oil price hike!). However, he also cast doubt on the simple asymmetry
hypothesis suggested by Mork . More recently, Hamilton proposes relating output
growth to the net increase in oil prices over the previous year, and constructs a variable that is
the percentage change in the oil price in the current quarter over the previous year’s high, when
this is positive, and otherwise takes on the value zero. Thus, increases in the price of oil which
simply reverse previous (within the preceding year) declines do not depress output growth.
Recently, Hamilton has used a new ﬂexible non-linear approach and
Dahl and Hylleberg ) to characterize the appropriate non-linear transform of the oil price.
Raymond and Rich have investigated the relationship between oil prices and the
macroeconomy by including the net increase in the oil price in an MS model of US output for
the period 1951 — 1995. They are interested in whether the recurrent shifts between expansion
and contraction identiﬁed by the MS model remain when oil prices have been included as an
explanatory variable for the mean of output. Raymond and Rich conclude that ‘while
the behavior of oil prices has been a contributing factor to the mean of low growth phases of
output, movements in oil prices generally have not been a principal determinant in the historical
incidence of these phases   ’ investigate
whether oil prices can account for the asymmetry in the business cycle using the tests proposed
in Clements and Krolzig .
Clearly, the process of discovery of (an approximation to) the form of the non-linearity in
the relationship between output growth and oil price changes has taken place over two decades,
is far from simple, and has involved the application of state of the art econometric techniques.
Perhaps this warns against expecting too much in the near future using ‘canned’ routines and
Theoretical and methodological issues
There are various theoretical issues involved in constructing non-linear models for forecasting.
In this section we outline a number of these. An obvious starting point is which non-linear
model to use, given the many possibilities that are available, even once we have determined
the purpose to which it is to be put (here, forecasting). The diﬀerent types of models often
require diﬀerent theoretical and empirical tools and van Dijk et al. ). For example, closed form solutions exist for the
conditional mean forecast for an MS process, but not for a threshold autoregressive process,
requiring simulation or numerical methods in the latter case. Certain theoretical properties,
such as stability and stationarity, and the persistence of shocks, are not always immediately
The choice of model might be suggested by economic theory, and often by the requirement
that the model is capable of generating the key characteristics of the data at hand. Of course,
the issue of which characteristics often arises.
For example, Pagan and Harding
and Pagan argue that non-linear models should be evaluated in terms of their ability to
reproduce certain features of the classical cycle, rather than their ability to match the stationary
moments of the detrended growth cycle.1
An approach to tackling these issues is to use predictive density or distributional testing, as
a means of establishing which of a number of candidate forecasting models has distributional
features that most closely match the historical record. This could include, for example, ﬁnding
out which of the models yields the best distributional or interval predictions. For example,
in ﬁnancial risk management interest often focuses on predicting a particular quantile (as the
Value at Risk, VaR) but alternatively the entire conditional distribution of a variable may be
of interest. Over the last few years, a new strand of literature addressing the issue of predictive
density evaluation has arisen , Christoﬀersen , Christoﬀersen, Hahn
and Inoue , Clements and Smith , Diebold, Gunther and Tay (henceforth DGT), Diebold, Hahn and Tay , Giacomini and White and Hong ).
The literature on the evaluation of predictive densities is largely concerned with testing the
null of correct dynamic speciﬁcation of an individual conditional distribution model. At the
same time, the point forecast evaluation literature explicitly recognises that all the candidate
models may be misspeciﬁed and White ). Corradi
and Swanson draw on elements from both types of papers in order to provide a test
for choosing among competing predictive density models which may be misspeciﬁed. Giacomini and White tackle a similar problem, developing a framework that allows for the
evaluation of both nested and nonnested models.
Many of the above papers seek to evaluate predictive densities by testing whether they
have the property of correct (dynamic) speciﬁcation. By making use of the probability integral
transform, DGT suggest a simple and eﬀective means by which predictive densities can be
evaluated. Using the DGT terminology, if mq(vq|Ωq−1) is the “true” conditional distribution of
vq|Ωq−1, then the probability of observing a value of vq no larger than that actually observed
is a uniform random variable on [0 1]. Moreover, if we have a sequence of predictive densities,
then the resulting sequence of probabilities should be identically independently distributed.
Goodness of ﬁt statistics can then be constructed that compare the empirical distribution
1As an aside, these papers have shown that the durations and amplitudes of expansions and contractions of
the classical cycle can be reasonably well reproduced by simple random walk with drift models, where the ratio
of the drift to the variance of the disturbance term is the crucial quantity. Non-linear models appear to add little
over and above that which can be explained by the random walk with drift.
function of the probabilities to the 45 degree line, possibly taking into account that mq(vq|Ωq−1)
contains parameters that have been estimated , Diebold, Hahn and Tay
 and Hong ).
The approach taken by Corradi and Swanson diﬀers from the DGT approach as they
do not assume that any of the competing models are correctly speciﬁed. Thus, they posit that
ii models should be viewed as approximations to some unknown underlying data generating
process. They proceed by “selecting” a conditional distribution model that provides the most
accurate out-of-sample approximation of the true conditional distribution, allowing for misspeciﬁcation under both the null and the alternative hypotheses. This is done using an extension
of the conditional Kolmogorov test approach of Andrews due to Corradi and Swanson
 that allows for the in-sample comparison of multiple misspeciﬁed models. More specifically, assume that the objective is to form parametric conditional distributions for a scalar
random variable, vq+1 given some vector of variables, Wq = (vq  vq−p1+1 Uq  Uq−p2+1)
q = 1  Q where p = max{p1 p2}, and U is a vector Additionally, assume that f = 1  k
models are estimated. Now, deﬁne the ob
ropfsb j-estimator for the parameter vector associated with model f as:
bfq = arg min
nf(vg Wg−1 f)
O ≤q ≤Q −1 f = 1  k
f = arg min
f∈Θf B(nf(vg Wg−1 f))
where nf denotes the objective function for model f Following standard practice (such as in
the real-time forecasting literature), this estimator is ﬁrst computed using O observations, then
O + 1 observations, then O + 2, and so on until the last estimator is constructed using Q −1
observations, resulting in a sequence of M = Q −O estimators. In the current discussion, we
focus on 1-step ahead prediction, so these estimators are then used to construct sequences of M
1-step ahead forecasts and associated forecast errors.
Now, deﬁne the group of conditional distribution models from which we want to make a selection as C1(r|Wq †
1)  Ck(r|Wq †
k) and deﬁne the true conditional distribution as C0(r|Wq 0) =
Pr(vq+1 ≤r|Wq) Hereafter, assume that nf(vq Wq−1 f) = −ln cf(vq|Wq−1 f) where cf(·|· f) is
the conditional density associated with Cf f = 1  k, so that †
f is the probability limit of a
quasi maximum likelihood estimator (QMLE) If model f is correctly speciﬁed, then †
Now, C1(·|· †
1) is taken as the benchmark model, and the objective is to test whether some
competitor model can provide a more accurate approximation of C0(·|· 0) than the benchmark.
Assume that accuracy is measured using a distributional analog of mean square error. More
precisely, the squared (approximation) error associated with model f f = 1  k is measured
in terms of the average over R of B
Cf(r|Wq †
f) −C0(r|Wq 0)
 where r ∈R, and R is
a possibly unbounded set on the real line. The hypotheses of interest are:
C1(r|Wq †
1) −C0(r|Wq 0)
Ch(r|Wq †
h) −C0(r|Wq 0)
C1(r|Wq †
1) −C0(r|Wq 0)
Ch(r|Wq †
h) −C0(r|Wq 0)
(r)ar , 0
(r) ≥0 and
(r) = 1 r ∈R ⊂< R possibly unbounded. Note that for a given
r we compare conditional distributions in terms of their (mean square) distance from the true
distribution. The statistic is:
WMr(1 h) (r)ar
WMr(1 h) =
1{vq+1 ≤r} −C1(r|Wqb1q)
1{vq+1 ≤r} −Ch(r|Wqbhq)
Here, each model is estimated via QMLE, so that in terms of the above notation, nf = −ln cf
where cf is the conditional density associated with model f and bfq is deﬁned as bfq =
arg max f∈Θf
g=p ln cf(vg Wg−1 f)
O ≤q ≤Q −1 f = 1  k.
For further details,
please refer to Corradi and Swanson .
Clearly, the above approach can be used to evaluate multiple non-linear forecasting models.
Now, assume that focus centers on evaluating the joint dynamics of a non-linear prediction
model, say in the form of a real business cycle (RBC) model. Corradi and Swanson 
develop a statistic based on comparison of historical and simulated distributions. As the RBC
data are simulated using estimated parameters (as well as previously calibrated parameters),
the limiting distribution of their test statistic is a Gaussian process with a covariance kernel
that reﬂects the contribution of parameter estimation error. This limiting distribution is thus
not nuisance parameter free, and critical values cannot be tabulated. In order to obtain valid
asymptotic critical values, they suggest two block bootstrap procedures, each of which depends
on the relative rate of growth of the actual and simulated sample size. In addition, the standard
issue of singularity that arises when testing RBC models is circumvented by considering a
subset of variables (and their lagged values) for which a non singular distribution exists. For
example, in the case of RBC models driven by only one shock, say a technology shock, their
approach can be used to evaluate the model’s joint CDF of current and lagged output, including
autocorrelation and second moment structures, etc. In the case of models driven by two shocks,
say a technology and a preference shock, the approach can be used for the comparison of the
joint CDF of current (and lagged) output and hours worked, say. In general, their testing
framework can be used to address questions of the following sort: (i) For a given RBC model,
what is the relative usefulness of diﬀerent sets of calibrated parameters for mimicing diﬀerent
dynamic features of output growth? (ii) Given a ﬁxed set of calibrated parameters, what is the
relative performance of RBC models driven by shocks with a diﬀerent marginal distribution?
More speciﬁcally, consider j RBC models, and assume that the variables of interest are
output and lagged output. Now, set model 1 as the benchmark model. Let ∆log Uq, q =
1  Q denote actual historical output (growth rates), and let ∆log Ugk, g = 1  j and
k = 1  P denote the output series simulated under model g where P denotes the length
of the simulated sample. Denote ∆log Ugk(bgQ ), k = 1  P g = 1  j to be a sample of
length P drawn (simulated) from model g and evaluated at the parameters estimated under
model g where parameter estimation is done using the Q available historical observations.
Further, let Vq = (∆log Uq ∆log Uq−1) Vgk(bgQ ) = (∆log Ugk(bgQ ) ∆log Ugk−1(bgQ )), and
let C0(r; 0) denote the distribution of Vq evaluated at r and Cg(r; †
g) denote the distribution
of Vgk(†
g) where †
g is the probability limit of bgQ  taken as Q →∞ and where r ∈R ⊂
<2 possibly unbounded.
As above, accuracy is measured in terms of squared error.
squared (approximation) error associated with model g g = 1  j is measured in terms of
the (weighted) average over R of
g) −C0(r; 0)
 where r ∈R, and R is a possibly
unbounded set on <2. Thus, the rule is to choose Model 1 over Model 2 if
1) −C0(r; 0)
2) −C0(r; 0)
(r)ar = 1 and
(r) ≥0 for all r ∈R ⊂<2 For any evaluation point, this measure
deﬁnes a norm and is a typical goodness of ﬁt measure. Note that within our context, the
hypotheses of interest are:
C0(r; 0) −C1(r; †
C0(r) −Cg(r; †
C0(r) −C1(r; †
C0(r) −Cg(r; †
(r)ar , 0
Thus, under E0, no model can provide a better approximation (in a squared error sense) to the
distribution of Vq than the approximation provided by model 1 If interest focuses on conﬁdence
intervals, so that the objective is to “approximate” Pr(r ≤Vq ≤r) then the null and alternative
hypotheses can be stated as:
1) −C1(r; †
−(C0(r; 0) −C0(r; 0))
g) −Cg(r; †
−(C0(r; 0) −C0(r; 0))
1) −C1(r; †
−(C0(r; 0) −C0(r; 0))
g) −Cg(r; †
−(C0(r; 0) −C0(r; 0))
If interest focuses on testing the null of equal accuracy of two distribution models ), we can
simply state the hypotheses as:
C0(r; 0) −C1(r; †
C0(r) −Cg(r; †
C0(r) −C1(r; †
C0(r) −Cg(r; †
(r)ar 6= 0
In order to test E0 versus E> the relevant test statistic is
QWQP, where: 2
WgQP(r) (r)ar
0 versus E0
0 versus E00
> can be tested in a similar manner.
1{Vq ≤r} −1
1{V1k(b1Q ) ≤r}
1{Vq ≤r} −1
1{Vgk(bgQ ) ≤r}
with bgQ an estimator of †
g that satisﬁes Assumption 2 below.
See Corradi and Swanson
 for further details.
Another measure of distributional accuracy available in the literature 
and Vuong ), is the KLIC, according to which we should choose Model 1 over Model 2
B(log c1(Vq; †
2) −log c2 have shown that
the best model under the KLIC is also the model with the highest posterior probability. The
above approach is an alternative to the KLIC that should be viewed as complementary in some
cases, and preferred in others. For example, if we are interested in measuring accuracy over a
speciﬁc region, or in measuring accuracy for a given conﬁdence interval, this cannot be done
in an obvious manner using the KLIC, while it can easily done using our measure. As an
illustration, assume that we wish to evaluate the accuracy of diﬀerent models in approximating
the probability that the rate of growth of output is say between 05% and 15% We can do so
quite easily using the squared error criterion, but not using the KLIC. Furthermore, we often
do not have an explicit form for the density implied by the various models we are comparing.
Of course, model comparison can be done using kernel density estimators, within the KLIC
framework. However, this leads to tests with nonparametric rates ).
On the other hand, comparison via our squared error measure of accuracy is carried out using
empirical distributions, so that resulting test statistics converge at parametric rates.
3Recently, Giacomini proposes an extension which uses a weighted (over Vq) version of the KLIC, and
Kitamura suggests a generalization for choosing among models that satisfy some conditional moment
restrictions.
Point forecast production and evaluation continues to receive considerable attention, and
can perhaps be viewed as a leading indicator for the predictive density literature. There are now
a host of tests based on traditional squared error loss criteria, but in addition tests based on
directional forecast accuracy and sign tests, tests of forecast encompassing, as well as measures
and tests based on other loss functions. Swanson and White survey a number of the
important contributions, which include Chao, Corradi and Swanson , Chatﬁeld ,
Clark and McCracken , Clements and Hendry , Diebold and Chen , Diebold
and Mariano , Hansen , Hansen, Heaton and Luttmer , Hansen and Jeganathan , Harvey, Leybourne and Newbold , Linton, Maasoumi and Whang
 , McCracken , Pesaran and Timmerman , Stekler ,
West and West and McCracken , among others. A number of these papers
emphasise the role of parameter estimation uncertainty in testing for equal forecast accuracy,
or that one model forecast encompasses another, as well as the consequences of the models
being nested. Nevertheless, much remains to be done with regard to making these tests applicable to non-linear models, and constructing tests using non-linear and/or nondiﬀerentiable
loss functions, as well as allowing for parameter estimation error and misspeciﬁcation when the
comparisons involve non-linear models.
The above paragraph notes the role of the loss function in determining how accuracy is
to be assessed.
However there is also the issue of whether the in-sample model estimation
criterion and out-of-sample forecast accuracy criterion should be matched. For example, it is
only recently that much attention has been given to the notion that the same loss function
used in-sample for parameter estimation is often that which should be used out-of-sample for
forecast evaluation. Much remains to be done in this area, although progress has been made,
as discussed in Christoﬀersen and Diebold , Clements and Hendry , Granger
 , and Weiss . That said, it is often diﬃcult to come up with asymptotically
valid inferential strategies using standard estimation procedures (that essentially minimize onestep ahead errors) for many varieties of non-linear models, from smooth transition models to
projection pursuit and wavelet models. In some contexts it is even diﬃcult to establish the
consistency of some econometric parameter estimates, such as cointegrating vectors in certain
non-linear cointegration models, and threshold parameters in some types of regime-switching
models. In sum, estimation and in-sample inference of non-linear forecasting models remains
a potentially diﬃcult task, with much work remaining to be done.
Nevertheless, there are
many recent papers that propose novel approaches to estimation, such as the variety of new
cross-validation related techniques in the area of neural nets.
A general problem with non-linear models is the ‘curse of dimensionality’ and the fact that
such models tend to have a large number of parameters (at least relative to the available number
of macroeconomic data points) — how to keep the number of parameters at a tractable level?
This sort of issue is relevant to the speciﬁcation of many varieties of non-linear models, including
smooth transition models ) and neural network models
 ). A counter to the fear that such models may
be overﬁtting in-sample — in the sense of picking up transient, accidental connections between
variables — is of course to compare the models on out-of-sample performance, or on a ‘holdout’ sample. The ‘data-snooping procedures’ developed by White , and used in Sullivan,
Timmermann and White , can also be used, but this would appear to be
an open area, with much room for advance. For example, the extension of the data-snooping
methodology to multivariate models awaits attention, where the sheer magnitude of the problem
can quickly grow out of hand.
Finally, although not germane to forecasting, some models have parameters that are readily
interpretable, whilst others are less clear. The study by De Gooijer and Vidiella-i-Anguera
 extends ‘non-linear cointegration’ to allow the equilibrium, cointegrating relationship to
depend upon the regime. Diﬃcult issues arise when cointegration is no longer ‘global’, and
the theory-justiﬁcation for the long-run relationship is less clear. Clements and Galvão 
review speciﬁcation and estimation procedures in systems when cointegration is ‘global’, and
evaluate the forecasting ability of non-linear systems in the context of interest rate prediction,
building on earlier contributions by Anderson , van Dijk and Franses and Kunst
 , inter alia. Using a variety of forecast evaluation methods, De Gooijer and Vidiella-i-
Anguera establish the superiority of their model from a forecasting perspective.
Another new model is developed by Franses, Paap and Vroomen , who propose a
model in which a key autoregressive parameter depends on a leading indicator variable. This
model captures the notion that some variables are relevant for forecasting only once in a while,
and it mimics some of the ideas put forward in Franses and Paap . The autoregressive
parameter is constant unless a linear function of the leading indicator plus a disturbance term
exceeds a certain threshold level. The authors discuss issues relating to estimation, inference
and forecasting, and the relationship of their model to existing non-linear models. The model
is applied to forecasting unemployment, and is shown to be capable of capturing the sharp
increases in unemployment in recessions, and to provide competitive forecasts compared to
alternative models.
A number of possible approaches are available to account for the possibility that the parameters in forecasting models are changing over time. Taylor uses adaptive exponential
smoothing methods that allow smoothing parameters to change over time, in order to adapt to
changes in the characteristics of the time series. More speciﬁcally, he presents a new adaptive
method for predicting the volatility in ﬁnancial returns, where the smoothing parameter varies
as a logistic function of user-speciﬁed variables. The approach is analogous to that used to
model time-varying parameters in smooth transition GARCH models. These non-linear models
allow the dynamics of the conditional variance model to be inﬂuenced by the sign and size of
past shocks. These factors can also be used as transition variables in the new smooth transition
exponential smoothing approach. Parameters are estimated for the method by minimising the
sum of squared deviations between realised and forecast volatility. Using stock index data, the
new method gives encouraging results when compared to ﬁxed-parameter exponential smoothing and a variety of GARCH models.
Bradley and Jansen propose a model in which the dynamics that characterise stock
returns are allowed to diﬀer in periods following a large swing in stock returns — that is, a
non-linear state-dependent model.
Their approach allows them to test for the existence of
non-linearities in returns, and to estimate the size of the shock that is required to cause the
non-linear behaviour.
Empirical issues
In this section we discuss various practical issues. In contrast to linear models, the design of
non-linear models for actual data and the estimation of parameters is less straightforward.
How should we select a model?4 Should all the observations be used, or should models be
4There is a growing literature on nonparametric and semi-parametric forecasting methods and models. Traditionally this literature has focused on the conditional mean, but a number of recent papers have looked at
estimated and/or evaluated against speciﬁc (dynamic) features of the historical record? Should
we split the sample into in- and out-of-sample periods, and if so, where should the split occur?
Boero and Marrocu provide evidence related to some of these questions.
analyse the out-of-sample performance of SETAR models relative to a linear AR and a GARCH
model using daily data for the Euro eﬀective exchange rate. Their evaluation is conducted
on point, interval and density forecasts, unconditionally, over the whole forecast period, and
conditional on speciﬁc regimes.
Their results show that the GARCH model is better able
to capture the distributional features of the series and to predict higher-order moments than
the SETAR models. However, their results also indicate that the performance of the SETAR
models improves signiﬁcantly conditional on being in speciﬁc regimes.
In a related study,
Corradi and Swanson focus on various approaches to assessing predictive accuracy. One
of the main conclusions of their study is that there are various easy to apply statistics that
can be constructed using out-of-sample conditional-moment conditions, which are robust to
the presence of dynamic misspeciﬁcation. Because estimated models are approximations to the
DGP and likely to be mis-speciﬁed in unknown ways, tests that are robust in this sense are
obviously desirable. They provide an illustration of model selection via predictive ability testing
involving the US money-income relation, and demonstrate the relevance of the various testing
Next, which non-linear models should be entertained in any speciﬁc instance? This question
can be answered by looking at the theoretical properties of the models, as well as the speciﬁc
properties of the data under scrutiny. For example, Dahl and Hylleberg give a nice review
of four non-linear models, namely, Hamilton’s ﬂexible non-linear regression model , artiﬁcial neural networks and two versions of the projection pursuit regression model.
The forecasting performance of these four approaches is compared for U.S. industrial production
and an unemployment rate series, in a ‘real-time forecasting’ exercise, whereby the speciﬁcation
of the model is chosen, and the parameters re-estimated at each step, as the forecast origin
moves through the available sample. The paper provides some guidance for model selection,
nonparametric estimation of other aspects of conditional ditributions, such as quartiles, intervals and density
regions: see e.g., De Gooijer, Gannoun and Zerom , Matzner-Løber, Gannoun and De Gooijer and
Samanta , and the references therein. Developments in these areas look set to continue apace with the
more parametric approaches considered in this issue.
and evaluates the resulting forecasts using standard MSE-related criteria as well as directionof-change tests. Interestingly, they ﬁnd evidence that some of the ﬂexible non-linear regression
models perform well relative to the non-linear benchmark.
Further, how can we reliably estimate the model parameters? Can we address the problem
whereby we only ﬁnd local optima? How can we select good starting values in non-linear optimization? Can we design methods to test the non-linear models, based on in-sample estimation
and in-sample data? How should we aggregate and analyse our data? Some of these questions are examined in Van Dijk and Franses , who consider daily, weekly and monthly
data, and ﬁnd distinct models for diﬀerent temporal aggregation levels. But, what happens
if we aggregate over the cross-section dimension? Marcellino argues that cross-section
aggregation of countries, with constant weights over time, may produce ‘smoother’ series better suited for linear models, while aggregation with time-varying weights (and the presence
of common shocks) is more likely to generate a role for non-linear modelling of the resultant
series. Marcellino ﬁts a variety of non-linear and time-varying models to aggregate EMU
macroeconomic variables, and compares them with linear models. He assesses the quality of
these models in a real-time forecasting framework. It is found that often non-linear models
perform best.
Of course the bottom line is: Are there clear-cut examples where actual forecast improvements are delivered by non-linear models? Provision of such examples might serve to allay the
fears held by many sceptics. Three nice recent examples of this sort are Clements and Galvão
 , Sensier et al. and Gencay and Selcuk , all of whom show the relevance
of non-linear models for forecasting. The latter paper suggests that the use of non-linearities
is crucial for making sensible statements about the tail behaviour of asset returns. In particular, Gencay and Selcuk investigate the relative performance of Value-at-Risk (VaR)
models with the daily stock market returns of nine diﬀerent emerging markets. In addition to
well-known modeling approaches such as the variance-covariance method and historical simulation, they employ extreme value theory (EVT) to generate VaR estimates and provide the
tail forecasts of daily returns at the 0.999 percentile along with 95 percent conﬁdence intervals
for stress testing purposes. The results indicate that EVT based VaR estimates are more accurate at higher quantiles. According to estimated Generalized Pareto Distribution parameters,
certain moments of the return distributions do not exist in some countries. In addition, the
daily return distributions have diﬀerent moment properties in their right and left tails. Therefore, risk and reward are not equally likely in these economies. The other two papers consider
macroeconomic variables. Sensier et al. examine the role of domestic and international
variables for predicting classical business cycles regimes in four European countries, where the
regimes are classiﬁed as binary variables. One ﬁnding is that composite leading indicators and
interest rates of Germany and the US have substantial predictive value. Clements and Galvao
 test whether there is non-linearity in the response of short and long-term interest rates
to the spread. They assess the out-of-sample predictability of various models and ﬁnd some
evidence that non-linear models lead to more accurate short-horizon forecasts, especially of
the spread. And, as mentioned, De Gooijer and Vidiella-i-Anguera report more marked
gains to allowing for non-linearities.
Concluding remarks
In this paper we have summarized the state-of-the-art in forecast construction and evaluation
for non-linear models, and in selection among alternative non-linear prediction models. We
conclude that the day is still long oﬀwhen simple, reliable and easy to use non-linear model
speciﬁcation, estimation and forecasting procedures will be readily available.
Nevertheless,
there are grounds for optimism. The papers in this issue suggest that careful application of
existing techniques, and new models and tests, can result in signiﬁcant advances in our understanding. Supposing that the world is inherently non-linear, then as computational capabilities
increase, more complex models become amenable to analysis, allowing the possibility that future generations of models will signiﬁcantly outperform linear models, especially if such models
become truly multivariate.
Much remains to be done in the areas of speciﬁcation, estimation, and testing, with important issues of non-diﬀerentiability, parameter-estimation error and data-mining remaining to
be addressed. In addition, a further area for research (both empirical and theoretical) is the
following. Suppose one has data with trend components, seasonality, non-linearity and outliers.
How should one proceed? This is a complex issue, due to the possible inter-reactions between
these elements, e.g., neglecting outliers may suggest non-linearity )
and the trend can be intertwined with seasonality, as in periodic models of seasonality for an up-to-date survey). Much has been said about modelling each
of these characteristics in isolation, but developing coherent strategies for such data remains an
important task. It will be interesting to see to what extent developments in these areas give
rise to tangible gains in terms of forecast performance — we remain hopeful that great strides
will be made in the near future.
Biographies
Michael P. Clements is a Reader in the Department of Economics at the University of
Warwick. His research interests include time-series modelling and forecasting. He has published
in a variety of international journals, has co-authored two books on forecasting, and co-edited
(with David F Hendry) of A Companion to Economic Forecasting 2002, Blackwells. He is also
an editor of the International Journal of Forecasting.
Philip Hans Franses is Professor of Applied Econometrics and Professor of Marketing Research, both at the Erasmus University Rotterdam. He publishes on his research interests, which
are applied econometrics, time series, forecasting, marketing research and empirical ﬁnance.
Norman Swanson, a 1994 University of California, San Diego Ph.D. is currently Associate
Professor at Rutgers University. His research interests include forecasting, ﬁnancial- and macroeconometrics. He is currently an associate editor of the Journal of Business and Economic
Statistics, the International Journal of Forecasting, and Studies in Non-linear Dynamics and
Econometrics. He has recently been awarded a National Science Foundation research grant
entitled the Award for Young Researchers, and is a member of various professional organizations,
including the Econometric Society, the American Statistical Association, and the American
Economic Association. Swanson has recent publications in Journal of Development Economics,
Journal of Econometrics, Review of Economics and Statistics, Journal of Business and Economic
Statistics, Journal of the American Statistical Association, Journal of Time Series Analysis,
Journal of Empirical Finance, and International Journal of Forecasting, among others. Further
details about Swanson, including copies of all papers cited above, are available at his website: