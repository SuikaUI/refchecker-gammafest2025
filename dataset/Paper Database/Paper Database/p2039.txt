Attention-Based Recurrent Neural Network Models for Joint Intent Detection
and Slot Filling
Bing Liu1, Ian Lane1,2
1Electrical and Computer Engineering, Carnegie Mellon University
2Language Technologies Institute, Carnegie Mellon University
 , 
Attention-based encoder-decoder neural network models have
recently shown promising results in machine translation and
speech recognition. In this work, we propose an attention-based
neural network model for joint intent detection and slot ﬁlling,
both of which are critical steps for many speech understanding
and dialog systems. Unlike in machine translation and speech
recognition, alignment is explicit in slot ﬁlling. We explore different strategies in incorporating this alignment information to
the encoder-decoder framework. Learning from the attention
mechanism in encoder-decoder model, we further propose introducing attention to the alignment-based RNN models. Such
attentions provide additional information to the intent classiﬁcation and slot label prediction. Our independent task models
achieve state-of-the-art intent detection error rate and slot ﬁlling
F1 score on the benchmark ATIS task. Our joint training model
further obtains 0.56% absolute (23.8% relative) error reduction
on intent detection and 0.23% absolute gain on slot ﬁlling over
the independent task models.
Index Terms: Spoken Language Understanding, Slot Filling,
Intent Detection, Recurrent Neural Networks, Attention Model
1. Introduction
Spoken language understanding (SLU) system is a critical component in spoken dialogue systems. SLU system typically involves identifying speaker’s intent and extracting semantic constituents from the natural language query, two tasks that are often referred to as intent detection and slot ﬁlling.
Intent detection and slot ﬁlling are usually processed separately. Intent detection can be treated as a semantic utterance
classiﬁcation problem, and popular classiﬁers like support vector machines (SVMs) and deep neural network methods 
can be applied. Slot ﬁlling can be treated as a sequence labeling
task. Popular approaches to solving sequence labeling problems include maximum entropy Markov models (MEMMs) ,
conditional random ﬁelds (CRFs) , and recurrent neural networks (RNNs) . Joint model for intent detection and
slot ﬁlling has also been proposed in literature . Such joint
model simpliﬁes the SLU system, as only one model needs to
be trained and ﬁne-tuned for the two tasks.
Recently, encoder-decoder neural network models have
been successfully applied in many sequence learning problems
such as machine translation and speech recognition .
The main idea behind the encoder-decoder model is to encode
input sequence into a dense vector, and then use this vector to
generate corresponding output sequence. The attention mechanism introduced in enables the encoder-decoder architecture to learn to align and decode simultaneously.
In this work, we investigate how an SLU model can bene-
ﬁt from the strong modeling capacity of the sequence models.
Attention-based encoder-decoder model is capable of mapping
sequences that are of different lengths when no alignment information is given. In slot ﬁlling, however, alignment is explicit,
and thus alignment-based RNN models typically work well. We
would like to investigate the combination of the attention-based
and alignment-based methods. Speciﬁcally, we want to explore
how the alignment information in slot ﬁlling can be best utilized
in the encoder-decoder models, and on the other hand, whether
the alignment-based RNN slot ﬁlling models can be further improved with the attention mechanism that introduced from the
encoder-decoder architecture. Moreover, we want to investigate
how slot ﬁlling and intent detection can be jointly modeled under such schemes.
The remainder of the paper is organized as follows. In section 2, we introduce the background on using RNN for slot ﬁlling and using encoder-decoder models for sequence learning.
In section 3, we describe two approaches for jointly modeling
intent and slot ﬁlling. Section 4 discusses the experiment setup
and results on ATIS benchmarking task. Section 5 concludes
2. Background
2.1. RNN for Slot Filling
Slot ﬁlling can be treated as a sequence labeling problem, where
we have training examples of
(x(n), y(n)) : n = 1, ..., N
and we want to learn a function f : X →Y that maps an
input sequence x to the corresponding label sequence y. In slot
ﬁlling, the input sequence and label sequence are of the same
length, and thus there is explicit alignment.
Figure 1: ATIS corpus sample with intent and slot annotation.
RNNs have been widely used in many sequence modeling
problems . At each time step of slot ﬁlling, RNN reads a
word as input and predicts its corresponding slot label considering all available information from the input and the emitted
output sequences. The model is trained to ﬁnd the best parameter set θ that maximizes the likelihood:
 
where x represents the input word sequence, yt−1
represents
the output label sequence prior to time step t. During inference, we want to ﬁnd the best label sequence y given an input
sequence x such that:
ˆy = arg max
2.2. RNN Encoder-Decoder
The RNN encoder-decoder framework is ﬁrstly introduced in
 and . The encoder and decoder are two separate RNNs.
The encoder reads a sequence of input (x1, ..., xT ) to a vector c.
This vector encodes information of the whole source sequence,
and is used in decoder to generate the target output sequence.
The decoder deﬁnes the probability of the output sequence as:
where yt−1
represents the predicted output sequence prior to
time step t. Comparing to an RNN model for sequence labeling, the RNN encoder-decoder model is capable of mapping sequence to sequence with different lengths. There is no explicit
alignment between source and target sequences. The attention
mechanism later introduced in enables the encoder-decoder
model to learn a soft alignment and to decode at the same time.
3. Proposed Methods
In this section, we ﬁrst describe our approach on integrating
alignment information to the encoder-decoder architecture for
slot ﬁlling and intent detection. Following that, we describe
the proposed method on introducing attention mechanism from
the encoder-decoder architecture to the alignment-based RNN
3.1. Encoder-Decoder Model with Aligned Inputs
The encoder-decoder model for joint intent detection and slot
ﬁlling is illustrated in Figure 2. On encoder side, we use a bidirectional RNN. Bidirectional RNN has been successfully applied in speech recognition and spoken language understanding . We use LSTM as the basic recurrent network
unit for its ability to better model long-term dependencies comparing to simple RNN.
In slot ﬁlling, we want to map a word sequence x =
(x1, ..., xT ) to its corresponding slot label sequence y
(y1, ..., yT ). The bidirectional RNN encoder reads the source
word sequence forward and backward. The forward RNN reads
the word sequence in its original order and generates a hidden
state fhi at each time step. Similarly, the backward RNN reads
the word sequence in its reverse order and generate a sequence
of hidden states (bhT , ..., bh1). The ﬁnal encoder hidden state
hi at each time step i is a concatenation of the forward state fhi
and backward state bhi, i.e. hi = [fhi, bhi].
The last state of the forward and backward encoder RNN
carries information of the entire source sequence. We use the
last state of the backward encoder RNN to compute the initial
decoder hidden state following the approach in . The decoder is a unidirectional RNN. Again, we use an LSTM cell
as the basic RNN unit. At each decoding step i, the decoder
state si is calculated as a function of the previous decoder state
si−1, the previous emitted label yi−1, the aligned encoder hidden state hi, and the context vector ci:
si = f(si−1, yi−1, hi, ci)
(Slot Filling)
(Slot Filling)
(Slot Filling)
Figure 2: Encoder-decoder model for joint intent detection and
slot ﬁlling. (a) with no aligned inputs. (b) with aligned inputs.
(c) with aligned inputs and attention. Encoder is a bidirectional
RNN. The last hidden state of the backward encoder RNN is
used to initialize the decoder RNN state.
where the context vector ci is computed as a weighted sum of
the encoder states h = (h1, ..., hT ) :
k=1 exp(ei,k)
ei,k = g(si−1, hk)
g a feed-forward neural network. At each decoding step,
the explicit aligned input is the encoder state hi. The context
vector ci provides additional information to the decoder and can
be seen as a continuous bag of weighted features (h1, ..., hT ).
For joint modeling of intent detection and slot ﬁlling, we
add an additional decoder for intent detection (or intent classiﬁcation) task that shares the same encoder with slot ﬁlling
decoder. During model training, costs from both decoders are
back-propagated to the encoder. The intent decoder generates
only one single output which is the intent class distribution of
the sentence, and thus alignment is not required. The intent decoder state is a function of the shared initial decoder state s0,
which encodes information of the entire source sequence, and
the context vector cintent, which indicates part of the source
sequence that the intent decoder pays attention to.
3.2. Attention-Based RNN Model
The attention-based RNN model for joint intent detection and
slot ﬁlling is illustrated in Figure 3. The idea of introducing attention to the alignment-based RNN sequence labeling model
(Slot Filling)
Figure 3: Attention-based RNN model for joint intent detection
and slot ﬁlling. The bidirectional RNN reads the source sequence forward and backward. Slot label dependency is modeled in the forward RNN. At each time step, the concatenated
forward and backward hidden states is used to predict the slot
label. If attention is enabled, the context vector ci provides information from parts of the input sequence that is used together
with the time aligned hidden state hi for slot label prediction.
is motivated by the use of attention mechanism in encoderdecoder models. In bidirectional RNN for sequence labeling,
the hidden state at each time step carries information of the
whole sequence, but information may gradually lose along the
forward and backward propagation. Thus, when making slot label prediction, instead of only utilizing the aligned hidden state
hi at each step, we would like to see whether the use of context
vector ci gives us any additional supporting information, especially those require longer term dependencies that is not being
fully captured by the hidden state.
In the proposed model, a bidirectional RNN (BiRNN) reads
the source sequence in both forward and backward directions.
We use LSTM cell for the basic RNN unit. Slot label dependencies are modeled in the forward RNN. Similar to the encoder
module in the above described encoder-decoder architecture,
the hidden state hi at each step is a concatenation of the forward state fhi and backward state bhi, hi = [fhi, bhi]. Each
hidden state hi contains information of the whole input word
sequence, with strong focus on the parts surrounding the word
at step i. This hidden state hi is then combined with the context
vector ci to produce the label distribution, where the context
vector ci is calculated as a weighted average of the RNN hidden states h = (h1, ..., hT ).
For joint modeling of intent detection and slot ﬁlling, we
reuse the pre-computed hidden states h of the bidirectional
RNN to produce intent class distribution. If attention is not
used, we apply mean-pooling over time on the hidden
states h followed by logistic regression to perform the intent classiﬁcation. If attention is enabled, we instead take the
weighted average of the hidden states h over time.
Comparing to the attention-based encoder-decoder model
that utilizes explicit aligned inputs, the attention-based RNN
model is more computational efﬁcient. During model training,
the encoder-decoder slot ﬁlling model reads through the input
sequence twice, while the attention-based RNN model reads
through the input sequence only once.
4. Experiments
ATIS (Airline Travel Information Systems) data set is
widely used in SLU research.
The data set contains audio
recordings of people making ﬂight reservations. In this work,
we follow the ATIS corpus1 setup used in . The
training set contains 4978 utterances from the ATIS-2 and
ATIS-3 corpora, and the test set contains 893 utterances from
the ATIS-3 NOV93 and DEC94 data sets. There are in total 127
distinct slot labels and 18 different intent types. We evaluate
the system performance on slot ﬁlling using F1 score, and the
performance on intent detection using classiﬁcation error rate.
We obtained another ATIS text corpus that was used in 
and for SLU evaluation. This corpus contains 5138 utterances with both intent and slot labels annotated. In total there
are 110 different slot labels and 21 intent types. We use the
same 10-fold cross validation setup as in and .
4.2. Training Procedure
LSTM cell is used as the basic RNN unit in the experiments.
Our LSTM implementation follows the design in . Given
the size the data set, we set the number of units in LSTM cell as
128. The default forget gate bias is set to 1 . We use only
one layer of LSTM in the proposed models, and deeper models
by stacking the LSTM layers are to be explored in future work.
Word embeddings of size 128 are randomly initialized and
ﬁne-tuned during mini-batch training with batch size of 16.
Dropout rate 0.5 is applied to the non-recurrent connections 
during model training for regularization. Maximum norm for
gradient clipping is set to 5. We use Adam optimization method
following the suggested parameter setup in .
4.3. Independent Training Model Results: Slot Filling
We ﬁrst report the results on our independent task training models. Table 1 shows the slot ﬁlling F1 scores using our proposed
architectures. Table 2 compares our proposed model performance on slot ﬁlling to previously reported results.
Table 1: Independent training model results on ATIS slot ﬁlling.
(a) Encoder-decoder NN
79.66 ± 1.59
with no aligned inputs
(b) Encoder-decoder NN
95.38 ± 0.18
with aligned inputs
(c) Encoder-decoder NN
95.47 ± 0.22
with aligned inputs & attention
BiRNN no attention
95.37 ± 0.19
BiRNN with attention
95.42 ± 0.18
In Table 1, the ﬁrst set of results are for variations of
encoder-decoder models described in section 3.1. Not to our
surprise, the pure attention-based slot ﬁlling model that does
not utilize explicit alignment information performs poorly. Letting the model to learn the alignment from training data does
not seem to be appropriate for slot ﬁlling task. Line 2 and line
3 show the F1 scores of the non-attention and attention-based
encode-decoder models that utilize the aligned inputs.
1We thank Gokhan Tur and Puyang Xu for sharing the ATIS data
attention-based model gives slightly better F1 score than the
non-attention-based one, on both the average and best scores.
By investigating the attention learned by the model, we ﬁnd that
the attention weights are more likely to be evenly distributed
across words in the source sequence. There are a few cases
where we observe insightful attention (Figure 4) that the decoder pays to the input sequence, and that might partly explain
the observed performance gain when attention is enabled.
Figure 4: Illustration of the inferred attention when predicting
the slot label for the last word “noon” in the given sentence.
Darker shades indicate higher attention weights. When word
“noon” is fed to the model as the aligned input, the attention
mechanism tries to ﬁnd other supporting information from the
input word sequence for the slot label prediction.
The second set of results in Table 1 are for bidirectional
RNN models described in section 3.2. Similar to the previous
set of results, we observe slightly improved F1 score on the
model that uses attentions. The contribution from the context
vector for slot ﬁlling is not very obvious. It seems that for sequence length at such level (average sentence length is 11 for
this ATIS corpus), the hidden state hi that produced by the bidirectional RNN is capable of encoding most of the information
that is needed to make the slot label prediction.
Table 2 compares our slot ﬁlling models to previous approaches. Results from both of our model architectures advance
the best F1 scores reported previously.
Comparison to previous approaches. Independent
training model results on ATIS slot ﬁlling.
CNN-CRF 
RNN with Label Sampling 
Hybrid RNN 
Deep LSTM 
RNN-EM 
Encoder-labeler Deep LSTM 
Attention Encoder-Decoder NN
(with aligned inputs)
Attention BiRNN
4.4. Independent Training Model Results: Intent Detection
Table 3 compares intent classiﬁcation error rate between our
intent models and previous approaches.
Intent error rate of
our proposed models outperform the state-of-the-art results by
a large margin.
The attention-based encoder-decoder intent
model advances the bidirectional RNN model. This might be
attributed to the sequence level information passed from the encoder and additional layer of non-linearity in the decoder RNN.
4.5. Joint Model Results
Table 4 shows our joint training model performance on intent
detection and slot ﬁlling comparing to previous reported results. As shown in this table, the joint training model using
Comparison to previous approaches. Independent
training model results on ATIS intent detection.
Recursive NN 
Boosting 
Boosting + Simpliﬁed sentences 
Attention Encoder-Decoder NN
Attention BiRNN
encoder-decoder architecture achieves 0.09% absolute gain on
slot ﬁlling and 0.45% absolute gain (22.2% relative improvement) on intent detection over the independent training model.
For the attention-based bidirectional RNN architecture, the join
training model achieves 0.23% absolute gain on slot ﬁlling and
0.56% absolute gain (23.8% relative improvement) on intent
detection over the independent training models. The attentionbased RNN model seems to beneﬁt more from the joint training.
Results from both of our joint training approaches outperform
the best reported joint modeling results.
Comparison to previous approaches. Joint training
model results on ATIS slot ﬁlling and intent detection.
Intent Error (%)
RecNN+Viterbi 
Attention Encoder-Decoder
NN (with aligned inputs)
Attention BiRNN
To further verify the performance of our joint training models, we apply the proposed models on the additional ATIS data
set and evaluate them with 10-fold cross validation same as
in and . Both the encoder-decoder and attention-based
RNN methods achieve promising results.
Joint training model results on the additional ATIS
corpus using 10-fold cross validation.
Intent Error (%)
TriCRF 
CNN TriCRF 
Attention Encoder-Decoder
NN (with aligned inputs)
Attention BiRNN
5. Conclusions
In this paper, we explored strategies in utilizing explicit alignment information in the attention-based encoder-decoder neural
network models. We further proposed an attention-based bidirectional RNN model for joint intent detection and slot ﬁlling.
Using a joint model for the two SLU tasks simpliﬁes the dialog
system, as only one model needs to be trained and deployed.
Our independent training models achieved state-of-the-art performance for both intent detection and slot ﬁlling on the benchmark ATIS task. The proposed joint training models improved
the intent detection accuracy and slot ﬁlling F1 score further
over the independent training models.
6. References
 P. Haffner, G. Tur, and J. H. Wright, “Optimizing svms for complex call classiﬁcation,” in Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP’03). 2003 IEEE International
Conference on, vol. 1.
IEEE, 2003, pp. I–632.
 R. Sarikaya, G. E. Hinton, and B. Ramabhadran, “Deep belief nets
for natural language call-routing,” in Acoustics, Speech and Signal
Processing (ICASSP), 2011 IEEE International Conference on.
IEEE, 2011, pp. 5680–5683.
 A. McCallum, D. Freitag, and F. C. Pereira, “Maximum entropy
markov models for information extraction and segmentation.” in
ICML, vol. 17, 2000, pp. 591–598.
 C. Raymond and G. Riccardi, “Generative and discriminative algorithms for spoken language understanding.” in INTERSPEECH,
2007, pp. 1605–1608.
 K. Yao, B. Peng, Y. Zhang, D. Yu, G. Zweig, and Y. Shi, “Spoken language understanding using long short-term memory neural networks,” in Spoken Language Technology Workshop (SLT),
2014 IEEE.
IEEE, 2014, pp. 189–194.
 G. Mesnil, Y. Dauphin, K. Yao, Y. Bengio, L. Deng, D. Hakkani-
Tur, X. He, L. Heck, G. Tur, D. Yu et al., “Using recurrent neural
networks for slot ﬁlling in spoken language understanding,” Audio, Speech, and Language Processing, IEEE/ACM Transactions
on, vol. 23, no. 3, pp. 530–539, 2015.
 B. Liu and I. Lane, “Recurrent neural network structured output prediction for spoken language understanding,” in Proc. NIPS
Workshop on Machine Learning for Spoken Language Understanding and Interactions, 2015.
 D. Guo, G. Tur, W.-t. Yih, and G. Zweig, “Joint semantic utterance classiﬁcation and slot ﬁlling with recursive neural networks,” in Spoken Language Technology Workshop (SLT), 2014
IEEE, 2014, pp. 554–559.
 P. Xu and R. Sarikaya, “Convolutional neural network based
triangular crf for joint intent detection and slot ﬁlling,” in Automatic Speech Recognition and Understanding (ASRU), 2013
IEEE Workshop on.
IEEE, 2013, pp. 78–83.
 I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence
learning with neural networks,” in Advances in neural information
processing systems, 2014, pp. 3104–3112.
 W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, “Listen, attend and
spell,” arXiv preprint arXiv:1508.01211, 2015.
 D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” arXiv preprint
 
 T. Mikolov, S. Kombrink, L. Burget, J. H. ˇCernock`y, and S. Khudanpur, “Extensions of recurrent neural network language model,”
in Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE
International Conference on.
IEEE, 2011, pp. 5528–5531.
 K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau,
F. Bougares, H. Schwenk, and Y. Bengio, “Learning phrase representations using rnn encoder-decoder for statistical machine translation,” arXiv preprint arXiv:1406.1078, 2014.
 A. Graves, N. Jaitly, and A.-r. Mohamed, “Hybrid speech recognition with deep bidirectional lstm,” in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE,
2013, pp. 273–278.
 S. Hochreiter and J. Schmidhuber, “Long short-term memory,”
Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.
 X. Zhang, J. Zhao, and Y. LeCun, “Character-level convolutional
networks for text classiﬁcation,” in Advances in Neural Information Processing Systems, 2015, pp. 649–657.
 C. T. Hemphill, J. J. Godfrey, and G. R. Doddington, “The atis
spoken language systems pilot corpus,” in Proceedings, DARPA
speech and natural language workshop, 1990, pp. 96–101.
 G. Tur, D. Hakkani-Tur, and L. Heck, “What is left to be understood in atis?” in Spoken Language Technology Workshop (SLT),
2010 IEEE.
IEEE, 2010, pp. 19–24.
 M. Jeong and G. Geunbae Lee, “Triangular-chain conditional
random ﬁelds,” Audio, Speech, and Language Processing, IEEE
Transactions on, vol. 16, no. 7, pp. 1287–1302, 2008.
 W. Zaremba, I. Sutskever, and O. Vinyals, “Recurrent neural network regularization,” arXiv preprint arXiv:1409.2329, 2014.
 R. Jozefowicz, W. Zaremba, and I. Sutskever, “An empirical exploration of recurrent network architectures,” in Proceedings of
the 32nd International Conference on Machine Learning (ICML-
15), 2015, pp. 2342–2350.
 D. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014.
 B. Peng and K. Yao, “Recurrent neural networks with external memory for language understanding,” arXiv preprint
 
 G. Kurata, B. Xiang, B. Zhou, and M. Yu, “Leveraging sentencelevel information with encoder lstm for natural language understanding,” arXiv preprint arXiv:1601.01530, 2016.
 G. Tur, D. Hakkani-T¨ur, L. Heck, and S. Parthasarathy, “Sentence
simpliﬁcation for spoken language understanding,” in Acoustics,
Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on.
IEEE, 2011, pp. 5628–5631.