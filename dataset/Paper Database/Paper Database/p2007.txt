Fine-Grained Visual Comparisons with Local Learning
Aron Yu and Kristen Grauman
University of Texas at Austin
 , 
Given two images, we want to predict which exhibits a
particular visual attribute more than the other—even when
the two images are quite similar. Existing relative attribute
methods rely on global ranking functions; yet rarely will
the visual cues relevant to a comparison be constant for all
data, nor will humans’ perception of the attribute necessarily permit a global ordering. To address these issues,
we propose a local learning approach for ﬁne-grained visual comparisons. Given a novel pair of images, we learn a
local ranking model on the ﬂy, using only analogous training comparisons. We show how to identify these analogous
pairs using learned metrics. With results on three challenging datasets—including a large newly curated dataset for
ﬁne-grained comparisons—our method outperforms stateof-the-art methods for relative attribute prediction.
1. Introduction
Beyond recognizing objects (or activities, scenes, emotions, etc.), a computer vision system ought to be able to
compare them. A promising way to represent visual comparisons is through attributes, which are mid-level properties that appear across category boundaries and often vary
in terms of their perceived strengths. For example, with
a model for the relative attribute “brightness”, the system
could judge which of two images is brighter than the other,
as opposed to simply labeling them as bright/not bright.
Attribute comparisons open up a number of interesting
possibilities. In biometrics, the system could interpret descriptions like, “the suspect is taller than him” . In
image search, the user could supply semantic feedback to
pinpoint his desired content: “the shoes I want to buy are
like these but more masculine” . For object recognition,
human supervisors could teach the system by relating new
objects to previously learned ones, e.g., “a mule has a tail
longer than a donkey’s” . In texture recognition,
relative attributes could capture the strength of base properties . For subjective visual tasks, users could teach the
system their personal perception, e.g., about which human
faces are more attractive than others .
Fine-Grained
Figure 1: A global ranker may be suitable for coarse ranking tasks, but
ﬁne-grained ranking tasks require attention to subtle details—and which
details are important may vary in different parts of the feature space. We
propose a local learning approach to train comparative attributes based on
ﬁne-grained analogous pairs.
While a promising direction, the standard ranking approach tends to fail when faced with ﬁne-grained visual
comparisons, in which the novel pair of images exhibits
subtle visual attribute differences. While the learned function tends to accommodate the gross visual differences that
govern the attribute’s spectrum, it cannot simultaneously account for the many ﬁne-grained differences among closely
related examples, each of which may be due to a distinct set
of visual cues. For example, what makes a slipper appear
more comfortable than a high heel is different than what
makes one high heel appear more comfortable than another;
what makes a mountain scene appear more natural than a
highway is different than what makes a suburb more natural
than a downtown skyscraper (Figure 1). Furthermore, by
learning a single global function to rank all data, existing
methods ignore the reality that visual comparisons need not
be transitive; if human viewers perceive A ≻B ≻C ≻A,
one global function cannot adequately capture the perceived
ordering (Figure 2).
We contend that the ﬁne-grained comparisons are actually critical to get right, since this is where modeling relative
attributes ought to have great power. Otherwise, we could
just learn coarse categories of appearance (“bright scenes”,
“dark scenes”) and manually deﬁne their ordering.
We propose a local learning approach to the ﬁne-grained
visual comparison problem.
Rather than learn a single
global function to predict how pairs of examples relate,
we learn local functions that tailor the comparisons to the
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.
Figure 2: Visual comparisons need not be transitive. An observer rates A
as more comfortable than B, and B > C, but A < C.
neighborhood statistics of the data. In particular, given a
novel test pair of images and the attribute along which they
should be compared, we ﬁrst identify analogous training
pairs using a learned metric. We then train a ranking function on the ﬂy using only those nearby pairs, and apply it
to the test case. While simple, our framework directly addresses the ﬂaws that hinder existing methods. By restricting training pairs to those visually similar to the test pair,
the learner can zero in on features most important for that
kind of comparison. At the same time, by not insisting on a
single global function to relate all images, we mitigate the
impact of inconsistencies in visual comparisons.
To our knowledge, no prior work speciﬁcally explores
ﬁne-grained visual comparisons, and all prior methods assume a single global function is sufﬁcient .
Furthermore, while local learning methods have been explored for classiﬁcation and information
retrieval problems , our idea for learning local
ranking functions with comparisons is new. A key question
is how to identify neighboring training pairs; we show how
to learn pairs that appear analogous to the input, accounting
for the attribute-speciﬁc visual similarities.
On three challenging datasets from distinct domains, our
approach improves the state of the art in relative attribute
predictions. We also introduce a new large dataset of 50,000
Zappos shoe images that focuses on ﬁne-grained attribute
comparisons. Our results indicate that more labeled data is
not necessarily preferable to isolating the right data.
2. Related Work
Comparing attributes has gained attention in the last several years. The “relative attributes” approach learns a global
linear ranking function for each attribute . It is extended
to non-linear ranking functions in by training a hierarchy of rankers with different subsets of data, then normalizing predictions at the leaf nodes. In , rankers trained for
each feature descriptor (color, shape, texture) are combined
to produce a single global ranking function. Aside from
learning to rank formulations, researchers have applied the
Elo rating system for biometrics , and regression over
“cumulative attributes” for age and crowd density estimation . All the prior methods produce a single global function for each attribute, whereas we propose to learn local
functions tailored to the comparison at hand.
The basic idea in local learning is to concentrate
the learning algorithm on training instances that are most
similar to the test example.
Primarily two formulations
have been studied. In the ﬁrst, the system identiﬁes the K
training examples nearest to the test input, trains a model
with only those examples, and applies it to the test case.
For example, this paradigm is employed for neural network
classiﬁcation , linear regression , and SVM classiﬁcation . In the second strategy, the system learns a feature space mapping (e.g., with LDA) with only those instances close to the test example , thereby tailoring
the representation to the input. In a similar spirit, local metric learning methods use example-speciﬁc weights 
or a cluster-speciﬁc feature transformation , then apply
nearest neighbor classiﬁcation. For all these prior methods, a test case is a new data point, and its neighboring examples are identiﬁed by nearest neighbor search (e.g., with
Euclidean distance). In contrast, we propose to learn local
ranking functions for comparisons, which requires identifying analogous neighbor pairs in the training data.
In information retrieval, local learning methods have
been developed to sort documents by their relevance to
query keywords . They take strategies quite
similar to the above, e.g., building a local model for each
cluster in the training data , projecting training data onto
a subspace determined by the test data distribution , or
building a model with only the query’s neighbors .
Though a form of ranking, the problem setting in all these
methods is quite different from ours. There, the training examples consist of queries and their respective sets of ground
truth “relevant” and “irrelevant” documents, and the goal is
to learn a function to rank a keyword query’s relevant documents higher than its irrelevant ones. In contrast, we have
training data comprised of paired comparisons, and the goal
is to learn a function to compare a novel query pair.
The fact that humans exhibit inconsistencies in their
comparisons is well known in social choice theory and preference learning . In all the global models above, intransitive constraints would be unaccounted for and treated as
noise. While the HodgeRank algorithm also takes a
global ranking approach, it estimates how much it suffers
from cyclic inconsistencies, which is valuable to know how
much to trust the ﬁnal ranking function. However, that approach does not address the fact that the features relevant
to a comparison are not uniform across a dataset, which we
ﬁnd is critical for ﬁne-grained comparisons.
Work on ﬁne-grained visual categorization aims to recognize objects in a single domain, e.g., bird species . While such problems also require making distinctions
among visually close instances, our goal is to compare attributes, not categorize objects.
3. Approach
Our local learning approach addresses the relative comparison problem on a per attribute basis. As training data
for the attribute of interest A (e.g., “comfortable”), we are
given a pool of ground truth comparisons on pairs of images. Then, given a novel pair of images, our method predicts which exhibits the attribute more, that is, which of the
two images appears more comfortable.
In the following, we ﬁrst present a brief overview of Relative Attributes (Section 3.1), as it sets the foundation
as a state-of-the-art global ranking approach. Then we introduce our local ranking approach (Section 3.2), followed
by our idea to select ﬁne-grained neighboring pairs with
metric learning (Section 3.3).
3.1. Ranking for Relative Attributes
The Relative Attributes approach treats the attribute
comparison task as a learning-to-rank problem. The idea
is to use ordered pairs of training images to train a ranking function that will generalize to new images. Compared
to learning a regression function, the ranking framework
has the advantage that training instances are themselves expressed comparatively, as opposed to requiring a rating of
the absolute strength of the attribute per training image.
Each attribute is learned independently.
Let xi ∈ℜd denote the image descriptor for image i,
such as a GIST descriptor or a color histogram. The algorithm is given a set of training image pairs OA = {(i, j)},
in which each ordered pair (i, j) denotes that image i displays the attribute A more than image j. Let RA be a linear
ranking function:
RA(x) = wT
The goal is to learn the parameters wA ∈ℜd so that the
ordering RA assigns to the training pairs agrees with OA as
much as possible. That is, ∀(i, j) ∈OA : wT
By itself, the problem is NP-hard, but introduces slack
variables and a large-margin regularizer to approximately
solve it. The learning objective is:
A(xi −xj) ≥1 −ξij; ∀(i, j) ∈OA
where the constant C balances the regularizer and ordering
constraints. The objective can also be seen as a paired classiﬁcation problem, where, rather than predict the class label
of an individual xi, we want to predict the label “more” or
“less” for a pair (i, j) based on the difference in their visual
features. The margin one wants to maximize is the distance
between the nearest ranked points. While uses this linear formulation, it is also kernelizable and so can produce
non-linear ranking functions.1
1The objective in further adds a set of “similar” training pairs that
should receive similar ranks. We found they did not impact results for
either global or local methods on all our datasets, and so we omit them.
By projecting images onto the resulting hyperplane wA,
we obtain a 1D global ranking for that attribute, e.g., from
least to most “comfortable”. Given a test pair (xp, xq), if
RA(xp) > RA(xq), the method predicts image p has the
attribute “more” than image q, and “less” otherwise.
Our local approach draws on this particular ranking formulation, which is also used in both and in the hierarchy of to produce state-of-the-art results. However,
we note that our local learning idea would apply similarly
to alternative ranking methods.
3.2. Local Learning for Visual Comparisons
Existing methods train a global ranking function using
all available constraints OA, with the implicit assumption
that more training data should only help better learn the target concept. While such an approach tends to capture the
coarse visual comparisons, it can be difﬁcult to derive a single set of model parameters that adequately represents both
these big-picture contrasts and more subtle ﬁne-grained
comparisons (recall Figure 1). Indeed, in our early exploration applying Relative Attributes , we were impressed
by the qualitative results at either end of an attribute’s spectrum, but we could not make sense of its ﬁner-grained predictions. For example, for a dataset of shoes, it would map
all the sneakers on one end of the “formal” spectrum, and all
the high heels on the other, but the ordering among closely
related high heels did not show a clear pattern.
The solution is not simply a matter of using a higher
capacity learning algorithm. While a low capacity model
can perform poorly in well-sampled areas, unable to suf-
ﬁciently exploit the dense training data, a high capacity
model can produce unreliable (yet highly conﬁdent) decisions in poorly sampled areas of the feature space . Different properties are required in different areas of the feature space. Furthermore, in our visual ranking domain, we
can expect that as the amount of available training data increases, more human subjectiveness and ordering inconsistencies will emerge, further straining the validity of a single
global function.
Thus, we propose a local learning approach for attribute
ranking. The idea is to train a custom ranking function tailored to each novel pair of images Xq = (xr, xs) that we
wish to compare. We train the custom function using just a
subset of all labeled training pairs, exploiting the data statistics in the neighborhood of the test pair. In particular, we
sort all training pairs OA by their similarity to (xr, xs),
then compose a local training set O′
A consisting of the top
K neighboring pairs, O′
A = {(xk1, xk2)}K
k=1. (We explain
in the next section how we deﬁne similarity between pairs.)
Then, we train a ranking function using Eq. 2 on the ﬂy, and
apply it to compare the test images.
Such a ﬁne-grained approach helps to eliminate ordering constraints that are irrelevant to the test pair. For in-
(a) Local Approach
(b) Global Approach
Figure 3: Given a novel test pair (blue △) in a learned metric space, our
local approach (a) selects only the most relevant neighbors (green #) for
training, which leads to ranking test image 2 over 1 in terms of “sporty”.
In contrast, the standard global approach (b) uses all training data (green
# & red ×) for training; the unrelated training pairs dilute the training
data. As a result, the global model accounts largely for the coarse-grained
differences, and incorrectly ranks test image 1 over 2. The end of each
arrow points to the image with more of the attribute (sporty). Note that the
rank of each point is determined by its projection onto w.
stance, when evaluating whether a high-topped athletic shoe
is more or less “sporty” than a similar looking low-topped
one, our method will exploit pairs with similar visual differences, as opposed to trying to accommodate in a single
global function the contrasting sportiness of sneakers, high
heels, and sandals (Figure 3).
One might wonder if we could do as well by training
one global ranking function per category—i.e., one for high
heels, one for sneakers, etc., in the example above. This
would be another local learning strategy, but it is much too
restrictive. First of all, it would require category-labeled
examples (in addition to the orderings OA), which may be
expensive to obtain or simply not apropos for data lacking
clear-cut category boundaries (e.g., is the storefront image
an “inside city scene” or a “street scene”?). Furthermore,
it would not permit cross-category comparison predictions;
we want to be able to predict how images from different
categories compare in their attributes, too.
3.3. Selecting Fine-Grained Neighboring Pairs
A key factor to the success of the local rank learning
approach is how we judge similarity between pairs. Intuitively, we would like to gather training pairs that are somehow analogous to the test pair, so that the ranker focuses on
the ﬁne-grained visual differences that dictate their comparison. This means that not only should individual members
of the pairs have visual similarity, but also the visual contrasts between the two test pair images should mimic the
visual contrasts between the two training pair images. In
addition, we must account for the fact that we seek comparisons along a particular attribute, which means only certain aspects of the image appearance are relevant; in other
words, Euclidean distance between their global image descriptors is likely inadequate.
To fulﬁll these desiderata, we deﬁne a paired distance
function that incorporates attribute-speciﬁc metric learning.
Let Xq = (xr, xs) be the test pair, and let Xt = (xu, xv)
be a labeled training pair for which (u, v) ∈OA. We deﬁne
their distance as:
DA (Xq, Xt) = min
A ((xr, xs), (xu, xv)) ,
A ((xr, xs), (xv, xu))
A is the product of the two items’ distances:
A ((xr, xs), (xu, xv)) = dA(xr, xu) × dA(xs, xv).
The product reﬂects that we are looking for pairs where
each image is visually similar to one of those in the novel
pair. If both query-training couplings are similar, the distance is low.
If some image coupling is highly dissimilar, the distance is greatly increased.
The minimum in
Eq. 3 and the swapping of (xu, xv) →(xv, xu) in the
second term ensure that we account for the unknown ordering of the test pair; while all training pairs are ordered
with RA(xu) > RA(xv), the ﬁrst or second argument of
Xq may exhibit the attribute more. When learning a local
ranking function for attribute A, we sort neighbor pairs for
Xq according to DA, then take the top K to form O′
When identifying neighbor pairs, rather than judge image distance dA by the usual Euclidean distance on global
descriptors, we want to specialize the function to the particular attribute at hand. That’s because often a visual attribute does not rely equally on each dimension of the feature space, whether due to the features’ locations or modality. For example, if judging image distance for the attribute
“smiling”, the localized region by the mouth is likely most
important; if judging distance for “comfort” the features describing color may be irrelevant. In short, it is not enough
to ﬁnd images that are globally visually similar. For ﬁnegrained comparisons we need to focus on those that are similar in terms of the property of interest.
To this end, we learn a Mahalanobis metric:
dA(xi, xj) = (xi −xj)T MA(xi −xj),
parameterized by the d×d positive deﬁnite matrix MA. We
employ the information-theoretic metric learning (ITML)
algorithm , due to its efﬁciency and kernelizability.
Given an initial d × d matrix MA0 specifying any prior
knowledge about how the data should be compared, ITML
produces the MA that minimizes the LogDet divergence
UT-Zap50K (pointy)
OSR (open)
PubFig (smiling)
FG-LocalPair
FG-LocalPair
FG-LocalPair
Figure 4: Example ﬁne-grained neighbor pairs for three test pairs (top row) from the datasets tested in this paper. We display the top 3 pairs per query.
FG-LocalPair and LocalPair denote results with and without metric learning (ML), respectively. UT-Zap50K pointy: ML puts the comparison focus on the
tip of the shoe, caring less about the look of the shoe as a whole. OSR open: ML has less impact, as openness in these scenes relates to their whole texture.
PubFig smiling: ML learns to focus on the mouth/lip region instead of the entire face.
Dℓd from that initial matrix, subject to constraints that similar data points be close and dissimilar points be far:
Dℓd(MA, MA0)
dA(xi, xj) ≤c
(i, j) ∈SA,
dA(xi, xj) ≥ℓ
(i, j) ∈DA.
The sets SA and DA consist of pairs of points constrained
to be similar and dissimilar, and ℓand c are large and small
values, respectively, determined by the distribution of original distances. We set MA0 = Σ−1, the inverse covariance
matrix for the training images. To compose SA and DA,
we use image pairs for which human annotators found the
images similar (or dissimilar) according to the attribute A.
Figure 4 shows example neighbor pairs. They illustrate
how our method ﬁnds training pairs analogous to the test
pair, so the local learner can isolate the informative visual
features for that comparison.
Note how holistically, the
neighbors found with metric learning (FG-LocalPair) may
actually look less similar than those found without (Local-
Pair). However, in terms of the speciﬁc attribute, they better
isolate the features that are relevant. For example, images
of the same exact person need not be most useful to predict
the degree of “smiling”, if others better matched to the test
pair’s expressions are available (last example). In practice,
the local rankers trained with learned neighbors are substantially more accurate, as we will show in Section 5.
3.4. Discussion
Learning local models on the ﬂy, though more accurate
for ﬁne-grained attributes, does come at a computational
cost. The main online costs are ﬁnding the nearest neighbor pairs and training the local ranking function. For our
datasets, with K = 100 and 20,000 total labeled pairs, this
amounts to about 3 seconds. There are straightforward ways
to improve the run-time. The neighbor ﬁnding can be done
rapidly using well known hashing techniques, which are applicable to learned metrics .
Furthermore, we could
pre-compute a set of representative local models. For example, we could cluster the training pairs, build a local model
for each cluster, and invoke the suitable model based on a
test pair’s similarity to the cluster representatives. We leave
such implementation extensions as future work.
While global rankers produce comparable values for all
test pairs, our method’s predictions are test-pair speciﬁc.
This is exactly what helps accuracy for subtle, ﬁne-grained
comparisons, and, to some extent, mitigates the impact of
inconsistent training comparisons. For an application requiring a full ordering of many images, one could feed our
predictions to a rank aggregation technique , or apply a
second layer of learning to normalize them, as in .
4. Fine-Grained Attribute Zappos Dataset
We introduce a new UT Zappos50K dataset (UT-
Zap50K2) speciﬁcally targeting the ﬁne-grained attribute
comparison task. The dataset is ﬁne-grained due to two factors: 1) it focuses on a narrow domain of content, and 2) we
develop a two-stage annotation procedure to isolate those
comparisons that humans ﬁnd perceptually very close.
The image collection is created in the context of an online shopping task, with 50,000 catalog shoe images from
Zappos.com. For online shopping, users care about precise
2UT-Zap50K dataset and all related data are publicly available for
download at vision.cs.utexas.edu/projects/finegrained
Figure 5: Example pairs contrasting our predictions to the Global baseline’s. In each pair, top item is more sporty than bottom item according to ground
truth from human annotators. (1) We predict correctly, Global is wrong. We detect subtle changes, while Global relies only on overall shape and color. (2)
We predict incorrectly, Global is right. These coarser differences are sufﬁciently captured by a global model. (3) Both methods predict incorrectly. Such
pairs are so ﬁne-grained, they are difﬁcult even for humans to make a ﬁrm decision.
visual differences between items. For instance, it is more
likely that a shopper is deciding between two pairs of similar men’s running shoes instead of between a woman’s high
heel and a man’s slipper. The images are roughly 150×100
pixels and shoes are pictured in the same orientation for
convenient analysis. For each image, we also collect its
meta-data (shoe type, materials, manufacturer, gender, etc.)
that are used to ﬁlter the shoes on Zappos.com.
Using Mechanical Turk (mTurk), we collect ground truth
comparisons for 4 relative attributes: “open”, “pointy at the
toe”, “sporty”, and “comfortable”. The attributes are selected for their potential to exhibit ﬁne-grained differences.
A worker is shown two images and an attribute name, and
must make a relative decision (more, less, equal) and report
the conﬁdence of his decision (high, mid, low). We repeat
the same comparison for 5 workers in order to vote on the
ﬁnal ground truth. We collect 12,000 total pairs, 3,000 per
attribute. After removing the low conﬁdence or agreement
pairs, and “equal” pairs, each attribute has between 1,500 to
1,800 total ordered pairs.
Of all the possible 50K2 pairs we could get annotated,
we want to prioritize the ﬁne-grained pairs. To this end,
ﬁrst, we sampled pairs with a strong bias (80%) towards
intra-category and -gender images (based on the meta-data).
We call this collection UT-Zap50K-1. We found ∼40% of
the pairs came back labeled as “equal” for each attribute.
While the “equal” label can indicate that there’s no detectable difference in the attribute, we also suspected that
it was an easy fallback response for cases that required a
little more thought—that is, those showing ﬁne-grained differences. Thus, we next posted the pairs rated as “equal”
(4,612 of them) back onto mTurk as new tasks, but without
the “equal” option. We asked the workers to look closely,
pick one image over the other, and give a one sentence rationale for their decisions. The rationale functions as a speed
bump to slow workers down so that they think more carefully about their decisions. We call this set UT-Zap50K-2.
Interestingly, the workers are quite consistent on these
pairs, despite their difﬁculty. Out of all 4,612 pairs, only
278 pairs had low conﬁdence or agreement (and so were
pruned). Overall, 63% of the ﬁne-grained pairs (and 66%
of the coarser pairs) had at least 4 out of 5 workers agree on
the same answer with above average conﬁdence. This consistency ensures we have a dataset that is both ﬁne-grained
as well as reliably ground truthed.
Compared to an existing Shoes attribute dataset with
relative attributes , UT-Zap50K is about 3.5× larger, offers meta-data and 10× more comparative labels, and most
importantly, speciﬁcally targets ﬁne-grained tasks.
5. Experiments
To validate our method, we compare it to two state-ofthe-art methods as well as informative baselines.
We evaluate on three datasets: UT-Zap50K, as
deﬁned above, with concatenated GIST and color histogram
the Outdoor Scene Recognition dataset 
(OSR); and a subset of the Public Figures faces dataset 
(PubFig). OSR contains 2,688 images (GIST features) with
6 attributes, while PubFig contains 772 images (GIST +
Color features) with 11 attributes. See Supp File for more
We use the exact same attributes, features, and
train/test splits as .
We run for 10 random train/test splits, setting aside
300 ground truth pairs for testing and the rest for training.
We cross-validate C for all experiments, and adopt the same
C selected by the global baseline for our approach. We use
no “equal” pairs for training or testing rankers. We report
accuracy in terms of the percentage of correctly ordered
pairs, following . We present results using the same
labeled data for all methods.
For ITML, we use the ordered pairs OA for rank training to compose the set of dissimilar pairs DA, and the set of
“equal” pairs to compose the similar pairs SA. We use the
default settings for c and l in the authors’ code . The setting of K determines “how local” the learner is; its optimal
setting depends on the training data and query. As in prior
work , we simply ﬁx it for all queries at K = 100.
Values of K = 50 to 200 give similar results. See Supp File
for more details.
We compare the following methods:
• FG-LocalPair: the proposed ﬁne-grained approach.
• LocalPair: our approach without the learned metric
(i.e., MA = I). This baseline isolates the impact of tailoring the search for neighboring pairs to the attribute.
Global 
FG-LocalPair
Table 1: UT-Zap50K-1 dataset results for coarser pairs.
Global 
FG-LocalPair
Table 2: UT-Zap50K-2 dataset results for ﬁne-grained pairs.
• RandPair: a local approach that selects its neighbors
randomly. This baseline demonstrates the importance
of selecting relevant neighbors.
• Global: a global ranker trained with all available labeled pairs, using Eq. 2. This is the Relative Attributes
Method . We use the authors’ public code.
• RelTree: the non-linear relative attributes approach
of , which learns a hierarchy of functions, each
trained with successively smaller subsets of the data.
Code is not available, so we rely on the authors’ reported numbers (available for OSR and PubFig).
Zappos Results
Table 1 shows the accuracy on UT-
Zap50K-1. Our method outperforms all baselines for all
attributes. To isolate the more difﬁcult pairs in UT-Zap50K-
1, we sort the test pairs by their intra-pair distance using the
learned metric; those that are close will be visually similar for the attribute, and hence more challenging. Figure 6
shows the results, plotting cumulative accuracy for the 30
hardest test pairs per split. We see that our method has substantial gains over the baselines (about 20%), demonstrating
its strong advantage for detecting subtle differences. Figure
5 shows the qualitative results.
We proceed to test on even more difﬁcult pairs. Whereas
Figure 6 focuses on pairs difﬁcult according to the learned
metric, next we focus on pairs difﬁcult according to our human annotators. Table 2 shows the results for UT-Zap50K-
2. We use the original ordered pairs for training and all
4,612 ﬁne-grained pairs for testing (Section 4). We outperform all methods for 3 of the 4 attributes. For the two
more objective attributes, “open” and “pointy”, our gains
are sizeable—14% over Global for “open”. We attribute this
to their localized nature, which is accurately captured by our
learned metrics. No matter how ﬁne-grained the difference
is, it usually comes down to the top of the shoe (“open”) or
the tip of the shoe (“pointy”). On the other hand, the subjective attributes are much less localized. The most challenging one is “comfort”, where our method performs slightly
worse than Global, in spite of being better on the coarser
# of Test Pairs
Cumulative Accuracy (%)
Global 
FG−LocalPair
# of Test Pairs
Cumulative Accuracy (%)
"Comfortable"
Figure 6: Accuracy for the 30 hardest test pairs on UT-Zap50K-1.
pairs (Table 1). We think this is because the locations of the
subtleties vary greatly per pair.
Overall, local learning outperforms the state-of-the-art
global approach on the Zappos images.
Scenes and PubFig Results
We now shift our attention
to OSR and PubFig, two commonly used datasets for relative attributes . The paired supervision for these
datasets originates from category-wise comparisons ,
and as such there are many more training pairs—on average over 20,000 per attribute.
Tables 3 and 4 show the accuracy for PubFig and OSR,
respectively. Figure 7 shows representative precision recall
curves, using |R(xi) −R(xj)| as a measure of conﬁdence.
On both datasets, our method outperforms all the baselines. Most notably, it outperforms RelTree , which to
our knowledge is the very best accuracy reported to date
on these datasets. This particular result is compelling not
only because we improve the state of the art, but also because RelTree is a non-linear ranking function. Hence, we
see that local learning with linear models is performing better than global learning with a non-linear model. With a
lower capacity model, but the “right” training examples, the
comparison is better learned. Our advantage over the global
Relative Attributes linear model is even greater.
On OSR, RandPair comes close to Global. One possible
cause is the weak supervision from the category-wise constraints. While there are 20,000 pairs, they are less diverse.
Therefore, a random sampling of 100 neighbors seems to
reasonably mimic the performance when using all pairs. In
contrast, our method is consistently stronger, showing the
value of our learned neighborhood pairs.
While metric learning (ML) is valuable across the board
(FG-LocalPair > LocalPair), it has more impact on Pub-
Fig than OSR. We attribute this to PubFig’s more localized
attributes. Subtle differences are what makes ﬁne-grained
comparison tasks hard. ML discovers the features behind
those subtleties with respect to each attribute. Those features could be spatially localized regions or particular image
cues (GIST vs. color). Indeed, our biggest gains compared
to LocalPair (9% or more) are on “white”, where we learn
to emphasize color bins, or “eye”/“nose”, where we learn to
emphasize the GIST cells for the part regions. In contrast,
the LocalPair method compares the face images as a whole,
RelTree 
Global 
FG-LocalPair
Table 3: Accuracy comparison for the PubFig dataset. FG-LocalPair denotes the proposed approach.
RelTree 
Global 
FG-LocalPair
Table 4: Accuracy comparison for the OSR dataset.
"LargeSize"
Global 
FG−LocalPair
"DiagonalPlane"
"NarrowEyes"
"Forehead"
Figure 7: Precision-recall for OSR (top) and PubFig (bottom).
and is liable to ﬁnd images of the same person as more relevant, regardless of their properties in that image (Figure 4).
6. Conclusion
Fine-grained visual comparisons have many compelling
applications, yet traditional global learning methods can fail
to capture their subtleties. We proposed a local learningto-rank approach based on analogous training comparisons,
and we introduced a new dataset specialized to the problem.
With three attribute datasets, we ﬁnd our idea improves the
state of the art. In future work, we plan to explore ways
to pre-compute local models to reduce run-time and investigate generalizations to higher-order comparisons.
Acknowledgements
We thank Mark Stephenson for his
help creating the UT-Zap50K dataset. This research is supported in part by NSF IIS-1065390 and ONR YIP.